
 16.4-T.ops Direct Numerical Simulation of Turbulence by a Fourier Spectral Method on the Earth Simulator 
Mitsuo Yokokawa1 , Ken ichi Itakura2, Atsuya Uno2, Takashi Ishihara3 and Yukio Kaneda3 1 Earth Simulator 
Research and Development Center Japan Atomic Energy Research Institute 6-9-3, Higashi-Ueno, Taito-ku, 
Tokyo 110-0015, Japan 2 Earth Simulator Center, Japan Marine Science and Technology Center 3173-25 Showa-machi, 
Kanazawa-ku, Yokohama 236-0001, Japan {itakura,uno}@es.jamstec.go.jp 3 Graduate School of Engineering, 
Nagoya University, Chikusa-ku, Nagoya 464-8603, Japan {ishihara,kaneda}@cse.nagoya-u.ac.jp Abstract The 
high-resolution direct numerical simulations (DNSs) of incompressible turbulence with numbers of grid 
points up to 40963 have been executed on the Earth Simulator (ES). The DNSs are based on the Fourier 
spectral method, so that the equation for mass conservation is accurately solved. In DNS based on the 
spectral method, most of the computation time is consumed in calculating the three-dimensional (3D) Fast 
Fourier Transform (FFT), which requires huge-scale global data transfer and has been the major stumbling 
block that has prevented truly high-performance computing. By implementing new methods to e.ciently perform 
the 3D-FFT on the ES, we have achieved DNS at 16.4 T.ops on 20483 grid points. The DNS yields an energy 
spectrum exhibiting a wide inertial subrange, in contrast to previous DNSs with lower resolutions, and 
therefore provides valuable data for the study of the universal features of turbulence at large Reynolds 
number. *  1 Introduction Direct numerical simulation (DNS) of turbulence provides us with detailed 
data on turbulence that is free of experimental uncertainty. DNS is therefore not only a powerful means 
for .nding directly applicable solutions to problems in practical application areas that involve turbulent 
phenomena, but also for advancing our under­standing of turbulence itself the last outstanding unsolved 
problem of classical physics, and a phenomenon that is seen in many areas which have societal impacts. 
Su.ciently high levels of computational performance are essential to the DNS of turbulence. If we don 
t have this, we are only able to simulate turbulence with insu.cient resolution or for low or moderate 
values of the Reynolds number Re, which represents the degree of non-linearity of .ow in a turbulent 
system. However, Currently Grid Technology Research Center, National Institute of Advanced Industrial 
Science and Technology e-mail: m.yokokawa@aist.go.jp *0-7695-1524-X/02 $17.00 (c) 2002 IEEE one will 
then be missing the essence of the turbulence. For example, our recent experience has shown that DNS 
with resolution of only around 5123 grid points or so results in a signi.cant overestimate of the Kolmogorov 
constant, which is one of the most important constants in the theory of turbulence. To obtain asymptotically 
correct higher-order statistics on small-scale eddies for large Re, which today forms the core of much 
of the e.ort in turbulence research, the required degree of resolution for the DNS of incompressible 
turbulent .ow is estimated as at least 20483 or 40963 grid points. The computer that runs a DNS of this 
type must have (M) enough memory to accommodate the huge number of degrees of freedom, (S) high enough 
speeds to run the DNS within a tolerable time, and (A) high levels of accuracy such that it is possible 
to resolve the motion of small eddies that have velocity amplitudes much smaller than those of the energy-containing 
eddies. The Earth Simulator (ES) provides a unique opportunity in these respects. On the ES, we have 
recently achieved DNS of incompressible turbulence under periodic boundary conditions (BC) by a spectral 
method on 20483 grid points with double-precision arithmetic, and DNS on 40963 grid points with the application 
of time integration with single-precision arithmetic; double-precision arithmetic was used to obtain 
the convolutional sums for evaluating the nonlinear terms. Being based on a spectral method, our DNS 
accurately satis.es the law of mass conservation as is explained below (§ 3); this is not achieved by 
a conventional DNS based on a .nite-di.erence scheme. Such accuracy is of crucial importance in the study 
of turbulence, and in particular for the resolution of small eddies. On the other hand, the execution 
of DNS code that implements the Fourier spectral method is de.nitely suitable as a way of evaluating 
the performance of such newly developed distributed memory systems as the ES from the viewpoints of computational 
performance, bandwidth from node to node, and I/O capabilities. DNS code is a very good benchmark program 
of the ES. It has also been employed in the .nal adjustment of the hardware system. The maximum number 
of degrees of freedom used in the evaluation is O(1011). The computational speed for DNSs was measured 
by using up to 512 processor nodes of the ES to simulate runs with di.erent numbers of grid points. The 
best sustained performance of 16.4 T.ops was achieved in a DNS on 20483 grid points. An overview of the 
ES and of the numerical methods applied in the DNS, along with results for system­performance evaluation 
and DNS are presented in this paper. 2 Overview of the Earth Simulator 2.1 Structure The ES is a parallel 
computer system of the distributed-memory type, and consists of 640 processor nodes (PNs) connected by 
640 × 640 single-stage crossbar switches. Each PN is a system with a shared memory, consisting of 8 vector-type 
arithmetic processors (APs), a 16-GB main memory unit (MMU), a remote access control unit (RCU), and 
an I/O processor. The peak performance of each AP is 8G.ops. The ES as a whole thus consists of 5120 
APs with 10 TB of main memory and the peak performance of 40T.ops[1]. Each AP consists of a 4-way super-scalar 
unit (SU), a vector unit (VU), and main memory access control unit on a single LSI chip. The AP operates 
at a clock frequency of 500MHz with some circuits operating at 1GHz. Each SU is a super-scalar processor 
with 64KB instruction caches, 64KB data caches, and 128 general­purpose scalar registers. Branch prediction, 
data prefetching and out-of-order instruction execution are all employed. Each VU has 72 vector registers, 
each of which can has 256 vector elements, along with 8 sets of six di.erent types of vector pipelines: 
addition/shifting, multiplication, division, logical operations, masking, Processor Node #0 Processor 
Node #1 Processor Node #639 Figure 1: System con.guration of ES and load/store. The same type of vector 
pipelines works together by a single vector instruction and pipelines of di.erent types can operate concurrently. 
The VU and SU support the IEEE 754 .oating-point data format. The RCU is directly connected to the crossbar 
switches and controls inter-node data communications at 12.3GB/s bidirectional transfer rate for both 
sending and receiving data. Thus the total bandwidth of inter-node network is about 8TB/s. Several data-transfer 
modes, including access to three-dimensional(3D) sub-arrays and indirect access modes, are realized in 
hardware. In an operation that involves access to the data of a sub-array, the data is moved from one 
PN to another in a single hardware operation, and relatively little time is consumed this processing. 
The overall MMU is divided into 2048 banks and the sequence of bank numbers corresponds to increasing 
addresses of locations in memory. Therefore, the peak throughput is obtained by accessing contiguous 
data which are assigned to locations in increasing order of memory address. The fabrication and installation 
of the ES at the Earth Simulator Center of the Japan Marine Science and Technology Center was completed 
by the end of February 2002 (Fig. 2)[2]. 2.2 Parallel programming on the ES If we consider vector processing 
as a sort of parallel processing, then we need to consider three-level parallel programming to attain 
high levels of sustained performance for the ES. The .rst level of parallel processing is vector processing 
in an individual AP; this is the most fundamental level of processing by the ES. Automatic vectorization 
is applied by the compilers to programs written in conventional Fortran 90 and C. The second level is 
that of shared-memory parallel processing within an individual PN. Shared-memory parallel programming 
is supported by microtasking and OpenMP. The microtasking capability is similar in style to that one 
which has been provided for a Cray supercomputer, and the same function is realized for the ES. Microtasking 
is applied in two ways; one (AMT) is automatic parallelization by the compilers and the other (MMT) is 
the manual insertion of microtasking directives before target do loops. The third level is distributed-memory 
parallel processing that is shared among the PNs. A distributed­memory parallel programming model is 
supported by the Message Passing Interface (MPI). The performance of this system for the MPI put function 
of the MPI-2 speci.cation was measured[3]. The maximum throughput  Figure 2: A model of the ES system 
in the gym-like building. The building is 50m ×65m ×17m and has two stories; it includes a seismic isolation 
system. and latency for MPI put are 11.63GB/s and 6.63 µsec, respectively. Only 3.3 µsec is required 
for barrier syn­chronization; this is because the system includes a dedicated hardware system for global 
barrier synchronization among the PNs.  3 Numerical Methods 3.1 Basic Equations and Spectral Method 
The problem of incompressible turbulence under periodic boundary conditions (BC) is one of the most canon­ical 
problems in the study of turbulence, and has in fact been extensively studied. It keeps the essence of 
turbulence nonlinear convection, pressure and dissipative mechanisms due to the viscosity while being 
free of such extra complexities as those due to the .uid s compressibility, which often make the reliability 
of DNS less transparent. We here consider the .ow of an incompressible .uid as described by the Navier-Stokes 
(NS) equations, .u +(u ·.)u =-.p +..2u + f, (1) .t under a periodic boundary condition with period 2p, 
where u =(u1, u2, u3) is the velocity .eld, p is the pressure, and f is the external force that satis.es 
.· f =0, the .uid density is assumed to be unity. The pressure term p can be eliminated by the incompressibility 
condition: .·u =0. (2) Let us rewrite (1) in the form .u =u ×. -..+..2u + f, (3) .t 1 where . = rotu 
= (.1,.2,.3) is the vorticity and .= p + 2 u2. Then, taking the divergence of (3) and using (2), we obtain 
.2.= .·[u ×.]. (4) In DNS of turbulence, the accurate solution of equations of this type, i.e., Poisson 
s equation, is important, because a violation of the equation(s) implies a violation of mass conservation, 
one of the most fundamental laws of physics. However, it is not, in general easy to accurately solve 
Poisson s equation by using a .nite­di.erence (FD) scheme. In fact, most of the cpu time consumed in 
solving a DNS by FD is known to be consumed in solving Poisson s equation, and the cpu time increases 
rapidly with the required accuracy level. This di.culty can be overcome by using the Fourier spectral 
method, where (3) is written as d k ·7s(k) + .k27u(k) = 7s(k) -k + 7f(k), (5) dt k2 7s(k) = -(ux×.)(k), 
where we have used (4). In (5), k is the wave vector, k = |k|, and the hat7denotes the Fourier coe.cient. 
For example, 7u(k) is de.ned by u(x) =7u(k)exp ik ·x, k<KC where KC is a cut-o. wavenumber in the DNS 
(see below). In the spectral method, the inverse Poisson operator is expressed simply by -1/k2, so its 
evaluation can be accurate to within the limit imposed by the numerical round-o. error. In DNS by a spectral 
method, most of the cpu time is consumed for the evaluation of the nonlinear terms in NS equations, which 
are expressed as convolutional sums in the wave vector space. As is well-known, the sum can be e.ciently 
evaluated by using an FFT. In order to achieve high levels of e.ciency in terms of computational speed 
and numbers of modes retained in the calculation, we use the so-called phase-shift method, in which a 
convolutional sum is obtained for a shifted grid system as well as for the original grid v system. This 
method allows us to keep all of the wave vector modes which satisfy k < KC = 2N/3, where N is the number 
of discretized grid points in each direction of the Cartesian coordinates, thus greatly increasing the 
number of the retained modes. The nonlinear term in (3) or (5) can be evaluated without aliasing error 
by 18 real 3D-FFTs[4]. Our code ( hereafter called Code-.) based on (5) and the standard 4-th order Runge-Kutta 
(R-K) method for advancing time, was written in Fortran 90, and 25N3 main dependent variables are used 
in the program. The total number of lines of code, excluding comment lines, is about 3,000. The required 
amount of memory for N = 4096 and double-precision data was thus estimated to be about 12.5 TB, which 
is beyond the capacity of the ES. However, executing DNS with N = 4096 is of great interest from the 
view-point of studying turbulence. A closer inspection motivated by this consideration showed that the 
number of variables to be accommodated in the DNS may be reduced to 22N3 by rewriting the NS equations 
(1) in divergence form, i.e., as .ui .. + (uiuj) = - p + ..2ui + fi, (6) .t .xj .xi and that the memory 
requirement can be further reduced by (a) limiting the time integration by the R-K method to relevant 
wave-vector modes or (b) reducing the precision of the arithmetic from double to single. DNS for N = 
4096 by 512 PNs of the ES is then possible. Unlike method (a), method (b) is easy to implement and makes 
the computation faster, so we were tempted to try method (b). Before applying method (b) to allow us 
to execute DNS with N = 4096, it is desirable that we have some idea on the potential e.ects of the arithmetic 
precision on the results. We therefore performed preliminary DNSs with N = 1024 in (i) double-precision 
arithmetic, (ii) single-precision arithmetic, and (iii) single-precision arithmetic for time integration 
in the spectral space and double-precision arithmetic for the convolutional sums used in evaluating the 
nonlinear term. Method (iii) is interesting because, while the nonlinear coupling in the NS equations 
is treated more accurately by (iii) than by (ii), the memory capacity of the ES allows DNS for N = 4096 
with method (iii). In comparing the results of the DNS test runs, we con.rmed that the di.erence between 
the results is not signi.cant, at least in terms of such low-order statistics as the energy spectrum 
presented below (the details of the comparison will be reported elsewhere). There are slightly fewer 
operations per time step in Code-. than in the code (hereafter called Code-div) based on the divergence 
form (6); this di.erence, however is O(N3), which is much less than the number O(N3 log2 N) of operations 
for one 3D-FFT, [see Eqs. (7) and (8)], since the respective programs have the same numbers of 3D-FFTs 
per time step. Accordingly, the speed of computation is slightly faster with Code-..For example, the 
respective cpu times per 100 time steps in execution by 512 nodes of simulations with N = 2048 and double-precision 
arithmetic are 321 sec for Code-w and 330 sec for Code-div. We con.rmed that the results of simulation 
by the two bodies of code are the same, to within the limit of machine accuracy. With regard to the simulation 
runs with N = 2048, we below present only the results for Code-.. Regarding the run with N = 4096, we 
only used Code-div, in which we apply the Runge-Kutta-Gill method for advancing time to save on memory 
usage. 3.2 Parallelization Since the 3D-FFT accounts for more than 90% of the computational cost of 
executing the code for a DNS of turbulence by the Fourier spectral method, the most crucial factor in 
maintaining high levels of performance in the DNS of turbulence is the e.cient execution of this calculation. 
In particular, in a parallel implementation, the 3D-FFT has a global data dependence because of its requirement 
for global summation over PNs, so data is frequently moved about among the PNs during this computation. 
Vector processing is capable of e.ciently handling the 3D-FFT as decomposed along any of the three axes. 
Applying the domain decomposition method to assign the calculations for respective sets of several 2D 
planes to individual PNs, then having each PN execute the 2D-FFT for its assigned slab by vector processing 
and microtasking is an e.ective approach. The FFT in the remaining direction should then be performed 
after transposition of the 3D array data. Domain decomposition in the k3 direction in wave-vector space 
and in the y direction in physical space was implemented in the code. We achieved a high-performance 
FFT by implementing the following ideas/methods on the ES. (1) Data Allocation Let nd be the number of 
PNs, and let s consider the 3D-FFT for N3 real-valued data, which we will call u.In wave-vector space, 
the Fourier transform u of the real u is divided into a real part uR and an imaginary part u I , each 
of which is of size N3/2. These data are divided into nd data sets, each of which is allocated to the 
global memory region (GMR) of a PN, where the size of each data set is (N + 1) × N × (N/2/nd). Similarly, 
the real data of u are divided into nd data sets of size (N + 1) × (N/nd) × N, each of which is allocated 
to the GMR of the corresponding PN. Here the symbol ni in n1 × n2 × n3 denotes the data length along 
the i-th axis, and we set the length along the .rst axis to (N + 1), so as to speed up the memory throughput 
by avoiding memory-bank con.ict. (2) Parallelization by Microtasking For e.ciently performing the N 
×(N/2/nd) 1D-FFTs of length N along the .rst axis, the data along the sec­ond axis is divided up equally 
among the 8 APs of the given PN. This division can be e.ciently achieved by using the microtasking function 
which is provided by the ES. However, we decided to achieve this in prac­tice by the method referred 
to as MMT in subsection §2.2, i.e., the manual insertion of the parallelization directive *cdir parallel 
do before target do-loops, which directs the compiler to apply microtasking to parallelize the do-loop. 
We did this because we had found that the use of automatic parallelization (AMT) by the compiler did 
not draw out the bene.ts of parallel execution. (3) Radix-4 FFT Though the peak performance of an AP 
of the ES is 8 G.ops, the bandwidth between an AP and the memory system is 32 GB/s. This means that only 
one double-precision .oating-point datum can be supplied for every two possible double-precision .oating-point 
operations. The ratio of the number of times memory is accessed to the number of .oating-point data operations 
for the radix-2 FFT is 1; the memory system is thus incapable of supplying su.cient data to the processors. 
This bottleneck of memory access in the kernel loop of a radix-2 FFT function degrades the sustained 
levels of performance on the overall task. Thus, to obtain e.ciently calculation of the 1D-FFT within 
the ES, the radix-4 FFT must replace the radix-2 FFT to the extent that this is possible. This is because 
of the lower ratio of the number of memory accesses to the number of .oating-point data operations in 
the kernel loop of the radix-4 FFT, so the radix-4 FFT better .ts the ES. The 1D-FFT along the 2nd axis 
can be performed by vectorization and the function of microtasking is applied to the 1st axis by dividing 
the do-loops. (4) Data Transposition by Remote Memory Access Before performing the 1D-FFT along the 
3rd axis, we need to transpose the data from the domain decom­position along the 2nd axis to the one 
along the 3rd axis. The remote memory access (RMA) function is capable of handling the transfer of data 
which is required for this transposition. RMA is a means for the direct transfer of data from the GMR 
of a PN of the ES to the GMR of pre-assigned PN. One then does not need to make copies of the data, i.e., 
data is copied to neither the MPI-library nor the communications bu.er region of the OS. In a single 
cycle of RMA transfer, N × (N/nd ) × (N/2/nd) data are transferred from each of the nd PNs to the other 
PNs. The data transposition can be completed with (nd - 1) such RMA-transfer operations, after which 
N × (N/nd) × (N/2) data will have been stored at each target PN. The 1D-FFT for the 3rd axis is then 
executed by dividing the do-loop along the 1st axis so as to apply microtasking.   4 Performance and 
DNS Results 4.1 Performance of Parallel Computation on Multi-nodes We have measured the sustained performance 
for both the double-precision and single-precision versions of code-. by changing the number N3 of grid 
points, setting N values of 128, 256, 512, 1024, and 2048. The corresponding numbers of PNs taken up 
in the ES are 64, 128, 256, and 512; see Table 1 for the correspondences between number of PNs and number 
of grid points which we tested. The calculation time for 100 time steps of the Runge-Kutta integration 
was measured by using the MPI function MPI wtime. The time taken in initialization and I/O processing 
are excluded from the measurement because these values are negligible in comparison with the cost of 
Runge-Kutta integration, which increases with the number of time steps. Table 1: Performance in T.ops 
of the computations with double [single] precision arithmetic as counted by the hardware monitor on the 
ES. The numbers in ( -) denote the values for computational e.ciency, CE . The number np of APs in each 
PN is a .xed 8. N3 \ nd 512 256 128 64 20483 13.7(0.43)[15.3(0.48)] 6.9(0.43)[7.8(0.49)] -- 10243 11.3(0.35)[11.2(0.35)] 
6.2(0.39)[7.2(0.45)] 3.3(0.41)[3.7(0.47)] 1.7(0.43)[1.9(0.48)] 5123 - 4.1(0.26)[4.0(0.25)] 2.7(0.34)[3.0(0.38)] 
1.5(0.38)[1.7(0.43)] 2563 -- 1.3(0.16)[1.2(0.15)] 1.0(0.26)[1.1(0.28)] 1283 - -- 0.3(0.07)[0.3(0.07)] 
Table 2: Performance in T.ops as calculated for the same cases in Table 1 by using the analytical expressions 
for numbers of operations N3 \ nd 512 256 128 64 20483 14.6[16.4]7.4[8.4] -- 10243 12.2[12.1] 6.7[7.7] 
3.5[4.0] 1.8[2.1] 5123 - 4.4[4.3] 3.0[3.3] 1.7[1.9] 2563 -- 1.4[1.3] 1.1[1.2] 1283 - -- 0.3[0.3] The 
number of .oating-point operations in the measurement range, which is needed in calculation of the sustained 
performance, may be monitored by either (a) a hardware counter in the ES or (b) some analytical method 
for obtaining the number of operations. The hardware counter obtains total amounts of .oating-point data 
operations that have been processed in vector operations. Generally, numbers of operations are greater 
for vector than for scalar processors, because IF statements in do loops in the former case may require 
extra number of .oating-point operations due to masking operations. Since there are few IF statements 
in most of the subroutines for calculating 3D-FFTs, the number measured by the hardware counter is appropriate 
for the calculation of sustained performance. Method (b) is based on the fact that the number of operations 
in a 1D-FFT with both radix-2 and 4 against the number of grid points N is N(5p + 8.5q), where N is represented 
by 2p4q and p = 0 or 1 according to the value of N[5]. Considering that the DNS code has 72 real 3D-FFTs 
per time step and summing up the number of operations over the measurement range by hand, we obtain the 
following analytical expressions of the number of operations: 459N3 log2 N + (288 + 16p)N3 , if p = 0, 
(7) 459N3 log2 N + (369 + 16p)N3 , if p = 1. (8) Results of these expressions can be used as reference 
values on the number of operations in comparing the sustained performance of code. Tables 1 and 2 show 
results obtained by methods (a) and (b), respectively. N3 and nd in the tables indicate the numbers of 
grid points and PNs used in each run, respectively. The number np of APs in each PN is .xed to 8 in the 
execution that produced Tables 1 and 2 by the use of MMT. The total number of APs used in each run is 
thus 8 × nd. In the measurement runs, the data allocated to the GMR of each PN are transferred by the 
RMA function MPI put, and the tasks in each PN are divided up among the APs by the use of manual microtasking 
(MMT). The maximum sustained performance of 16.4 T.ops as calculated by method (b) was for the problem 
size of 20483 with 512 PNs; 15.3 T.ops (again the maximum) was obtained for the same case in evaluation 
by method (a). Table 1 shows that, for a .xed np = 8, the sustained performance is better for single 
precision than for double precision code for larger values of N and smaller values of nd. Though the 
numbers of .oating-point operations in the code are the same regardless of the precision, only half as 
much data is transferred in the transposition for 3D-FFT in the former case. This is presumably the reason 
for the better performance of the single precision code for larger values of N. Note here that when one 
uses AMT, the compiler automatically determines the division of the do-loop for microtasking and the 
number np of available APs within 1 = np = 8. In our experience with AMT, pre-assigning np to np = N/nd/2 
has not greatly improved the performance over that for the case with no pre-assignment. This implies 
that one needs to use MMT to achieve high levels of performance for .xed values of N and nd. In fact, 
the highest levels of performance are achieved by the manual insertion of microtasking directives that 
obtain equal utilization of all 8 APs. If AMT were used instead of MMT, only 2 APs in each PN would be 
utilized to execute each nested do-loop. In Table 1 we see that the performance in T.ops increases almost 
linearly with nd or the total number of APs (8 × nd), and the computational e.ciency CE , de.ned as (sustained 
performance)/(theoretical peak performance) increases with N given a .xed 8 × nd. This implies that the 
RMA transfer is working very e.ciently despite the huge amount of data being transferred among the PNs, 
so that communications do not become a bottleneck. A close inspection of the CE value reveals that it 
is over 0.6 for some examples of the radix-4 FFT, but is generally only about 0.3 for the radix-2 FFT. 
Our achievement of CE values in the 0.4 - 0.5 range for larger values of N is presumably due to our strategy 
of using the radix-4 FFT as much as possible, as was explained in §3. The CPU times for Code-. to go 
through one time-step of the 4th-order R-K method, when N = 1024 and 2048, np = 8, and nd = 512, are 
0.435 and 3.21 seconds, respectively. It may be interesting to compare this with the performance of other 
code; for example, on the supercomputer Fujitsu VPP5000/56 installed at the Computation Center of Nagoya 
University, code developed by the authors (which has been used in various studies of turbulence; see 
the cited example [6]) achieved the CPU time of 160 seconds for the DNS with N = 1024. The CPU times 
to calculate a single eddy-turn-over time T de.ned as T = L/U (where U and L are the characteristic velocity 
and length scale of energy containing eddies, respectively), were 23 and 268 minutes in the runs with 
N = 1024 and 2048, respectively, when we were using np = 8 and nd = 512. The time of 23 minutes for N 
= 1024 is much smaller than that of 60 hours achieved with the 32 PEs ( ~ 307 G.ops) of the VPP500/56 
[8]. This comparison demonstrates the strong performance of the code we have developed for the ES, in 
terms of wall-clock measures of computation time. For reference, it may be also interest to note that 
the CPU time of the ES for one time step of the Runge­Kutta-Gill method in single precision for the run 
with N = 4096, np = 8, and nd = 512 was 30.7 seconds and the CPU time for a single eddy-turn-over time 
was estimated as 43 hours. The memory capacity of 7.2 TBytes was required for this execution. Simulations 
of this size are only truly practicable on the ES. 4.2 DNS Results All of the runs, except the one with 
N = 4096, were performed with double-precision arithmetic and continued until the time t 5T. Single-precision 
arithmetic was used in the run with N = 4096, except for the 3D-FFT (method (iii) in subsection 3.1), 
and this run was continued until t 0.7T. Table 3 shows some characteristic parameter values for the 
runs, where R. is the so-called Taylor-scale Reynolds number. In experimental and numerical data on fully 
developed turbulence, it is traditional to use R. instead of the Reynolds number Re = UL/., since the 
former is easier to measure [7]. The relation R. . Re1/2 has been shown to hold for large-Reynolds-number 
turbulence. The R. values 732 and 1217 for N = 2048 and 4096, are much higher than those in previous 
DNSs [8, 9]; for example, R. = 460 for N = 1024 in the DNS of [8]. One of the most important features 
of real turbulence at large Reynolds number is the existence of a wide gap, the so-called inertial subrange, 
between the scale on which the energy is and the scale on which it has to be dissipated. These scales 
are characterized by L (the so-called integral length scale) and the Kolmogorov length scale ., respectively. 
In order to study the possible universal features of turbulence by DNS, simulating a wide enough inertial 
subrange is of crucial importance. In this respect, Table 3 shows that the scale ratio L/. is more than 
103 for N = 2048 and 4096,which is much greater than the ratios in previous DNSs. These simulation runs 
on the ES may thus have provided us with data which is indispensable to turbulence research. Among these 
are data on the energy spectrum E(k). According to Kolmogorov s theory (K41)[10], which remains a major 
source of inspiration for turbulence research, the energy spectrum in the inertial subrange at su.ciently 
large Reynolds number must obey a power law of the form E(k) = CK 2/3k-5/3 , (9) where CK is a universal 
constant and . is the mean rate of energy dissipation per unit mass. Figure 3 shows ¯ the compensated 
energy spectra of the form E(k) = k5/3E(k)/2/3 for four di.erent values of N. Equation (9) indicates 
that the compensated spectra in Fig. 3 must be .at, i.e., constant independent of k, in the wavenumber 
range that corresponds to the inertial subrange. One can estimate CK from this constant level. To obtain 
a reliable estimate of CK in this way, the inertial range has to be wide enough, i.e., N must be large 
enough. Otherwise, the estimate will be of questionable validity. As a matter of fact, values for CK 
of 2.0 ~ 2.2 on the basis of data from DNSs have been reported in literature, but we must remember that 
these estimates have been based on DNSs with N = 512. In contrast, Figure 3 shows that with increases 
in N (or Re), CK converges well to a constant CK = 1.6 ~ 1.7, which is in good agreement with existing 
large-scale experimental data.[11] Table 3: Parameters in the DNS; R.: Taylor micro scale Reynolds number, 
.: kinematic viscosity, L: integral length scale, .: Kolmogorov length scale. N R. .(×10-4) .t(×10-4) 
L . 512 257 2.8 10.0 1.02 0.00395 1024 471 1.1 6.25 1.28 0.00210 2048 732 0.44 4.0 1.23 0.00105 4096 
1217 0.173 2.5 1.21 0.00053 4 4 3.5 3.5 3 3 2.5 2.5  1 1 0.5 0.5 0 0 k. k. Figure 3: Compensated energy 
spectra as obtained in DNSs with 5123 ,10243 ,20483 and 40963 grid points. (a) Spectra from DNSs with 
5123 and 10243 grid points, and (b) spectra from the four DNSs. Moreover, a close inspection of the spectra 
for N = 2048 and 4096 in Fig. 3 (b) suggests a feature that is not seen or not clearly seen in DNSs with 
N = 1024; the compensated spectra for N = 2048 and 4096 suggest a slight slope in the inertial range, 
i.e., roughly in the range 0.004 <k.<0.03 (in Fig.3, we show (a) and k 5/3 E(k)/e2/3 (b) separately to 
make the di.erence between the spectra with N = 1024 and N = 2048 clearly visible). This implies that 
the energy spectrum has a power law form E(k) . k-awith abeing slightly di.erent from the K41 value, 
a= 5/3. The detection of such a deviation, if it does in fact exist, is possible only with a DNS that 
has high enough resolution. It has been known that K41 agrees quite well with experiments for low-order 
statistics, but the agreement is poor for high-order statistics. The source of this disagreement is believed 
to be the intermittence of turbulence, and modern work on turbulence has been focused to a large extent 
on the problem of intermittency (cf. Frisch [7]). One of the most fundamental measures of this intermittence 
is the departure, the so-called intermittence factor, which is de.ned as the degree to which the exponent 
.p di.ers from the K41 value, p/3. Here, .p is the scaling exponent of the structure function Sp , which 
is de.ned as the n-th order moment of the di.erence dur between the velocities at two points x and x 
+ r; . p Sp =|dur|p. r, (10) in the inertial subrange satisfying .« r « L, where we have ignored the 
componental dependence of the velocity .eld. Reliable data on the exponent .p are indispensable to the 
study of the intermittence problem. Simulating a wide enough inertial subrange where Sp scales like (10) 
is, however, known to be di.cult, es­pecially when the value of p is large and the DNS does not have 
a large enough N. The ranges achieved in DNS to date have been too narrow to obtain reliable estimates 
of .p. This di.culty is apparent even when the p value is as low as p = 2. Let s consider the compensated 
spectra of Fig. 3. The DNS with N = 2048 and 4096 suggests a slope, i.e., the di.erence from the K41 
scaling (note that .2 = 2/3 as obtained by K41 is equivalent to E(k) . k-5/3); however, it is di.cult 
to detect this e.ect in results for the DNS with N = 1024. The results of a DNS with as high a resolution 
as is possible are thus of fundamental interest. In this regard, the present DNS is expected to provide 
valuable data for the study of the universal features of turbulence for large values of the Reynolds 
number, and particularly the scaling properties in the inertial subrange, which has been the objective 
in numerous studies of turbulence. Analysis on this point of the DNS data behind this report is now under 
way. 11 k 5/3 E(k)/e2/3 2 2 1.5 1.5 [Visualization] In general, a DNS generates such a huge amount of 
data that e.ciently dealing with the data is very im­portant for the understanding of the physics of 
the phenomena it represents. For this purpose, the importance of visualization techniques is increasing 
with the amounts of data. Figures 4 to 7 show examples of the visual­ization of the simulated turbulence 
.eld, i.e., snapshots of the vorticity .eld in the DNS with N = 2048. The impression these images give 
is quite di.erent from those for, e.g., N = 512. The literature seems to indicate a widespread belief 
that the structure of individual small eddies is important in determining the inertial range structure. 
Although this point of view may be consistent with .gures produced by DNSs with N = 512, Figs. 4 7, where 
N = 2048, suggest that this is unlikely to be the case. The di.erence between the individual small eddies 
and the inertial subrange structure in Figs. 4 7 may remind us of the di.erence between leaves and a 
forest; the structure at the former scale may be totally di.erent from that at the latter. Much remains 
to be studied and explored in such visualizations.  5 Summary For the DNS of turbulence, the Fourier 
spectral method has the advantage of accuracy, particularly in terms of solving the Poisson equation, 
which represents mass conservation and is to be solved accurately for a good resolution of small eddies. 
However, the method requires frequent execution of the 3D-FFT, the computation of which requires global 
data transfer. In order to achieve high performance in the DNS of turbulence on the basis of the spectral 
method, e.cient execution of the 3D-FFT is therefore of crucial importance. By implementing new methods 
for the 3D-FFT, on the ES as was explained in §3, we have accomplished the high-performance DNS of turbulence 
on the basis of the Fourier spectral method. This is presumably the world s .rst DNS of incompressible 
turbulence with 20483 or 40963 grid points. The DNS provides valuable data for the study of the universal 
features of turbulence at large Reynolds number. The numbers of degrees of freedom 4 × N3 (4 is the number 
of degrees of freedom (u1, u2, u3, p) in terms of grid points, and N3 is the number of grid points) are 
about 3.4 × 1010 in the DNS with N = 2048, and 2.7 × 1011 in the DNS with N = 4096. The sustained speed 
is 16.4 T.ops. To the authors knowledge, these values are the highest in any simulation so far carried 
out on the basis of spectral methods in any .eld of science and technology. Acknowledgments The authors 
would like to express their deepest condolences in connection with the late Mr. Hajime Miyoshi, who initiated 
and directed the ES project. They would also like to thank Dr. Tetsuya Sato, the director of the Earth 
Simulator Center, for his warm encouragement of this study. The authors would also like to thank Mr. 
Minoru Saito of NEC Informatic Systems, Ltd. for his contribution in developing the code and to thank 
all the members of ESRDC and ESC who were engaged in the development of the ES for valuable discussions 
and comments. References <RefA>[1] <SinRef><author>M.Yokokawa</author>, <title>Present Status of Development of the Earth Simulator, Innovative 
Architecture for Future Generation High-Performance Processors and Systems</title>, <booktitle>IEEE PR01309</booktitle>, <pages>pp.93-99 </pages>(<date>2000</date>).</SinRef> 
[2] <SinRef><author>T.Sato</author>, <author>S.Kitawaki</author>, <author>M.Yokokawa</author>, <title>Earth Simulator Running</title>, <booktitle>Proceedings of ISC 2002</booktitle>, <location>Heidelberg</location>, <date>June 
20-22 (2002).</date> </SinRef>[3] <SinRef><author>H.Uehara</author>, <author>M.Tamura</author>, and <author>M.Yokokawa</author>, <title>An MPI Benchmark Program Library and Its Application 
to the Earth Simulator</title>, <booktitle>Proceedings of ISHPC 2002, LNCS </booktitle><volume>2327</volume>, <pages>pp. 219-230 </pages>(<date>2002</date>). </SinRef>[4] <SinRef><author>C.Canuto</author>, <author>M.Y.Hussaini</author>, 
<author>A.Quarteroni</author>, and <author>T.A.Zang</author>, <title>Spectral Methods in Fluid Dynamics</title>, <publisher>Springer-Verlag </publisher>(<date>1988</date>). </SinRef>[5] <SinRef><author>C.V.Loan</author>, 
<title>Computational Frameworks for the Fast Fourier Transform</title>, <journal>SIAM </journal>(<date>1992</date>). </SinRef>[6] <SinRef><author>T.Ishihara</author>, <author>K.Yoshida</author>, and 
<author>Y.Kaneda</author>, <title>Anisotropic Velocity Correlation Spectrum at Small Scales in a Homogeneous Turbulent Shear 
Flow</title>, <journal>Phys. Rev. Lett. </journal><volume>88</volume>, <pages>154501 </pages>(<date>2002</date>). </SinRef>[7] <SinRef><author>U.Frisch</author>, <title>Turbulence</title>, <publisher>Cambridge University Press </publisher>(<date>1995</date>). </SinRef>
[8] <SinRef><author>T. Gotoh</author>, <author>D. Fukayama</author>, and <author>T. Nakano</author>, <title>Velocity .eld statistics in homogeneous steady turbulence ob­tained 
using a high-resolution direct numerical simulation</title>, <journal>Phys. Fluids. </journal><volume>14</volume>, <pages>pp. 1065-1081</pages>, (<date>2002</date>). </SinRef>[9] <SinRef><author>T. Ishihara </author>
and <author>Y. Kaneda</author>,<title> High resolution DNS of incompressible homogeneous forced turbulence Time dependence of 
the statistics </title>, <booktitle>Proceedings of the International Workshop on Statistical Theories and Computational 
Approaches to Turbulence</booktitle>, <editor>Kaneda </editor>and <editor>Gotoh </editor>(eds.) <publisher>Springer</publisher>, <pages>pp. 179</pages>, (<date>2002</date>). </SinRef>[10] <SinRef><author>A. N. Kolmogorov</author>, <title>The 
local structure of turbulence in incompressible viscous .uid for very large Reynolds number</title>, <journal>Dokl. Akad. 
Nauk SSSR </journal><volume>30</volume>, <pages>pp. 9-13</pages>, (<date>1941</date>), <journal>Dissipation of energy in locally isotropic turbulence, Dokl. Akad. Nauk 
SSSR</journal>, <volume>32</volume>, <pages>pp. 16-18</pages>, (<date>1941</date>). </SinRef>[11] <SinRef><author>K. R. Sreenivasan</author>, <title>On the universality of the Kolmogorov constant</title>,<journal> Phys. 
Fluids </journal><volume>7</volume>, <pages>pp. 2778-2784</pages>, (<date>1995</date>). </SinRef></RefA>Figure 4: Intense-vorticity isosurfaces showing the region where |.| 
>.¯+ 4s; . is the vorticity, and .¯and s are the mean and standard deviation of |.|, respectively. The 
size of the display domain is (59842 × 1496)., periodic in the vertical and horizontal directions. . 
is the Kolmogorov length scale and R. = 732 (see Table 3). Figure 5: A closer view of the inner square 
region of Fig. 4; the size of the display domain is (29922 × 1496).. Figure 6: The same isosurfaces 
as in Fig. 4; a closer view of the inner-square region of Fig. 5. The size of the display domain is (14962 
× 1496).. Figure 7: The same isosurfaces as in Fig. 4; a closer view of the inner-square region of Fig. 
6. The size of the display domain is (7482 × 1496).. 
			
