
 A Data-parallel Algorithm for Three-dimensional Delaunay Triangulation and its Implementation Y. Ansel 
Teng * Francis Sullivan t Isabel Beichl $ Enrico Puppo ~ Abstract dimensions has been studied extensively 
in the field of computational geometry. Algorithms, both sequential In this paper, we present a parallel 
algorithm for and parallel ones, have been designed and carefully constructing the Delaunay triangulation 
of a set of implemented. However, in three-dimensional space, vertices in three-dimensional space. The 
algorithm much less has been studied due to the difficulty in an­achieves a high degree of parallelism 
by starting the al yzing the properties of three dimensions, and even construction from every vertex 
and expanding over all fewer algorithms are implemented. Because 3-d De­open faces thereafter. In the 
expansion of open faces, launay triangulation is as useful as its 2-d counterpart the search is made 
faster by using a bucketing tech-in practice, we take on the task of designing a practi­nique. The algorithm 
is designed under a data-parallel cal parallel algorithm for three-dimensional Delaunay paradigm. It 
uses segmented list structures and virtual triangulation. processing for load-balancing. As a result, 
the algo-The goals in our design of this algorithm include not rithm achieves a fast running time and 
good scalability only a good asymptotic complexity in terms of both over a wide range of problem sizes 
and machine sizes. time and work, but also efficiency and scalability in We also incorporate a topological 
check to eliminate its implementation. Additionally, it should be robust inconsistencies due to degeneracies 
and numerical er-in the face of numerical errors and degeneracies. rors. The algorithm is implemented 
on Connection We will review some of the basics in the next sec-Machines CM-2 and CM-5, and experimental 
results tion and then describe our algorithm in Sections 3 are presented. to 6. The algorithm is implemented 
on Connection Machines CM-2 and CM-5 and experimental results are presented in Section 7. 1 Introduction 
 In the last decade, with the advent of commercial 2 Basics parallel computers, parallel processing has 
evolved es­sentially from a theoretical stage to a practical stage. 2.1 Geometric definitions Howeverj 
widespread application of parallel process­ing is impeded by the fact that most parallel programs Let 
S be a set of points in X?3, called sites. The achieve satisfactory performance only for a very lim- 
Delaunay triangulation of S is a partition of their con­ited range of problems. It is especially true 
for discrete vex hull by a set of tetrahedral T whose vertices are algorithms whose data are usually 
organized in an ir­ the sites, S, and such that the circumsphere of every regular structure. tetrahedron 
cent ains no sites in its interior. Delaunay triangulation is a basic example of such Discussion of 
these geometric objects is usually un­an irregular structure that has applications in many der the general 
position assumption, that is, we as­fields of computing. It is particularly useful in mesh sume no four 
sites are co-planar and no five sites are generation for solving fluid dynamics problems using co-spherical. 
Under this assumption, a unique set T finite element methods. Delaunay triangulation in two exists for 
every point set S, which is also the dual structure of the Voronoi diagram. College Park, MD 20742. 
In this paper, we consider the problem of construct­t Supercornputing Research Center, Bowie, MD 20715. 
*Center for Automation Research, University of Maryland, ing T for a given S in parallel. Throughout 
the paper,$computing md Applied Mathematics Laboratory, National n and k denote the sizes of S and T 
respectively. We Institute of Standsrd and Technology, Gaithersburg, MD 20899. !Istituto per la Matematica 
Applicata, Consiglio Nazionale first assume general positions for simplicity in the dis­delle Ricerche, 
Via L.B. Alberti, 4-16132 Geneva, ITALY. cussion, and later we will show how to accommodate Permission 
to copy without fee all or pm of this tmtemal is granted, pmridsd that the ccpies are not made or distributed 
for dkci commercial advanlag.a the ACM copyrigh{ naice and the title of tk publication and ils tie sppear,and 
notice is given that ccpymg is by permission of the 112 Association for CcxqMing Machinery. To copy 
cihenvi.se, or m republish, @ 1993 ACM 0-8186-4340-4/93/0011 $1.50 requires a fee andhr specific rennission, 
situations where this assumption does not hold. 2.2 Previous work Due to its importance in both theory 
and practice, there are numerous publications on Delaunay triangu­lation and Voronoi diagram. See [1] 
for an extensive survey. The method we will be using falls into the class of incremental construction 
methods. This is probably the most popular method that has been implemented so far due to its relative 
basic idea is to construct point at a time and to cordingly. Two different general method: ease of implementation. 
the structure by adding modify the triangulation approaches exist under The one ac­this Interpolation: 
added to the in this interior approach, of an existing the points simplex. are The simplex is split 
and an operation called flipping is employed to update the structure so that it is again a Delaunay triangulation 
for the points inserted so far. 3-d algorithms [9, 11] and a 2-d parallel algorithm [12] are implemented 
based on this idea. However, it is not clear how to insert points in parallel for the 3-d case. Extrapolation: 
extrapolation differs from interpo­lation in that partial results are always a subset of the final structure. 
New points are chosen to expand the structure toward its exterior. There are a few algorithms for 3 or 
higher dimensions in this class, such aa [3, 6]. Saxena and others [13] proposed a parallel 3-d algorithm 
with VLSI im­plementation under this approach, which has a control framework similar to ours. Other 
powerful methods exists mainly for the 2-d case, such as divide-and-conquer, plane-sweeping, and local 
improvement. However, their generalization to 3-d is mostly unclear, and the implementations are complicated 
or subject to numerical instability. Since the number of tetrahedral in a 3-d Delaunay triangulation 
is EI(nz) in the worst case, an 0(n2) al­gorithm will be worst-case optimal, and it has been achieved 
in [2]. However, for most input data, there will only be O(n) tetrahedral in the triangulation. Therefore, 
an algorithm with better expected com­plexity and/or output-sensitive complexity is still de­sirable. 
It has been proven, in particular, that O(n) is the expected number of tetrahedral for uniformly dis­tributed 
random points independently by Bernal and Dwyer [4, 6]. They both proposed sequential algo­rithms that 
achieve expected linear time complexity for random points.  2.3 Data-parallelism The algorithm we propose 
is designed under the data-parallel paradigm [10] in which parallelism is ex­pressed by applying a single 
operation to a set of data objects in parallel. For the computational primitives, we use the scan-vector 
model by Blelloch [5], The scan­vector model is defined as a set of primitives that oper­ate on arbitrarily 
y long vectors of atomic values. These primitives include elementwise arithmetics, permuta­tion among 
elements, and scan operations. The scan-vector model is distinguished by the set of scan primitives that 
perform a prefix operation with a wide class of operators, such as addition, multipli­cation, maximum, 
minimum, and all the binary logic operators. A simple variation of the add-scan will pro­vide the copy 
and enumerate operations along the vec­tor. The scan operations can also be conducted in a segmented 
fashion, where the segments are defined us­ing a flag vector whose values are set to 1 at the head of 
each segment. A set of high-level data-parallel operations can be implemented using these primitives 
as the build­ing blocks. These composite operations, including puck, append, distribute, sort, union, 
and concurrent read/write, will be extensively used in our algorithm, and we refer the reader to [5] 
for their implementa­tions. When an operation involves several vectors of equal length, these vectors 
will be considered as a list of data objects each consisting of a few variables. A mechanism called context 
indicates the active/inactive status of each data object. Contextualization is per­formed by where statements, 
and only the selected data objects will execute the instructions in the scope of the where statements. 
In this model, we apply oper­ations to vectors of arbitrary length but in reality the machine size is 
fixed. We assume a virtual processing mechanism in the implementation such that the ma­chine will automatically 
partition each vector among its physical processors. The scan-vector model provides a set of powerful 
operations for the algorithm designer while maintain­ing simplicity and flexibility in the instruction 
set so that it can be implemented efficiently on different ma­chine topologies. The complexity of an 
algorithm for a specific topology is incorporated through the cost of the implementation of these primitives. 
 2.4 Terminology and Represent at ion Throughout the paper, we assume that the input sites are normalized 
in a unit cube. We use the term face to refer to a facet with an associated orientation. Input sites 
are stored in a list where each element con­t ains the three coordinates of a site. A d-simplex is represented 
as a d-tuple of vertices, but only the in­dices of the site list are actually stored. For a face ~ in 
the form of (a, b, c), orientation is defined by the vector product ab x ac, and the positive half-space, 
Hp(f) is defined as the half-space on this side of the face. Parallel incremental extrapolation Our algorithm 
is based on the idea of incremen­ tal extrapolation. In the sequential case of such algo­ rithms, an 
initial Delaunay facet is usually found by gifl wrappin~ that is, start with a site a, usually the one 
nearest the center of the unit cube, and find the site b that is nearest to a. (a, b) is a Delaunay edge. 
Then find the site c that forms the smallest circumcir­ cle with (a, b). By properties of the Voronoi 
diagram, (a, b,.) is a Delaunay facet. We refer to the ezparz­ sion of a face as the operation that searches 
for a site on the positive side of the face to form a Delaunay tetrahedron. A null vertex is returned 
if there are no vertices on its positive side, making this a boundary face. A face is an open face if 
it is a Delaunay face but has not been expanded; it is a dead face otherwise. The algorithm maintains 
an open face list which is initialized to both sides of the first facet. Then it proceeds by expanding 
the faces in the open face list into tetrahedral and inserting new open faces back to the open face list 
until the list becomes empty. If we start with only one site, the algorithm will exhibit only a limited 
degree of parallelism due to the limited degree of face-adjacency among the tetrahedral. We observe that, 
under the general position assumption, the input sites should determine a unique Delaunay triangulation 
and we should obtain this triangulation no matter where we start the construction. There­fore, we can 
start the triangulation from all sites and achieve a very high degree of parallelism by expanding all 
open faces at the same time. Let OFL be the open face list, and TL be the tetrahedron list. Both lists 
are maintained as sets, i.e., there are no duplicate items in the lists. The control structure of the 
algorithm is outlined aa follows: Algorithm Parallel Extrapolation begin for each vertex find the first 
Delaunay facet by gift wrapping; initialize OFL to the initial faces; while OFL is not empty expand all 
open faces into tetrahedral; append the new tetrahedral to TL; append the new outward faces to OFL; purge 
OFL: delete opposite-duplicates; end while end Algorithm Let T be the set of tetrahedral in the Delaunay 
tri­angulation. Consider the face-adjacency graph G in­duced by T, that is, the graph whose nodes correspond 
to the tetrahedral in T, and whose edges connect pairs of tetrahedral that share a common facet. Let 
1 be the set of initial faces and define the depth function for a tetrahedron t if thas aface in 1 d~(t) 
= 1 rnin(d~(u) + 1) otherwise { where u is a neighbor oft in the face-adjacency graph. It is easy to 
shown that the algorithm performs a breadth-first search in G: in the i-th iteration, all tetrahedral 
with depth i will be traversed. Since the induced face-adjacency graph is finite and connected, d~(t) 
is finite for all tG T. Therefore, all tetrahe­dral will be discovered in a finite number of iterations, 
and each tetrahedron should appear only once in the tetrahedron list. 4 Face expansion The key to the 
performance of this algorithm is the search in face expansions. We describe the details of face expansions 
in this section. A similar technique can also be applied to the gift-wrapping. 4.1 Fundamental calculation 
There are two numerical primitives in a face expan­sion: the Orientation test and the InSphere test. 
Ori­entation determines whether a fourth point, is on the positive side of a triple (a, b, c) and InSphere 
tests if a fifth point is inside the sphere determined by 4 given points, (a, b, c, d). Both primitives 
can be formulated as matrix evaluations, which are costly if performed directly. We follow the formulation 
in [3] and show that both primitives can be implemented with one QR decomposition per face so that the 
computation in the inner loop is minimized. For the Orientation primitive, a point d is in Hp((a, b, 
c)) if the determinant of the 3 x 3 matrix A=[b a, c a, d a] is positive, where the vectors b a, c 
 a and d a are column vectors. Using the QR decomposition [14], we In the affine plane containing a, 
b, c, the center of the circle determined by these vertices is may write: uniquely determined. Hence, 
in the equations above, A=QR Z1 and Z2 are determined by a, b, and c and only Z3 where Q = [ql qz q3] 
is an orthogonal 3 x 3 matrix with depends on d. In fact, columns qi {i = 1, 2, 3} and R is an upper 
triangular 3 x 3 matrix, that is: <d = ;ql ~q2 +?3   =[ :H:I ~qz tcirc = +!?1+(93, a) q3 Then {ql, 
qz, q3} is an ordered orthonormal basis for 7?3 having the following properties: Hence, the signed distance 
of <d from &#38;cirC is just 1. {m) ~z} iS an orthonormal b~is for the plane spanned by {b a, c a}. 
SD(d) = sign(rl,l * rz,z * rs,s) *(~ + (qs, a)),  2. q3 is orthogonal to this plane.  Therefore, for 
the face expansion, a QR decomposition is performed in the beginning for the matrix [b a, c a] 3. The 
determinant of A is rl,l * rz,z * r3,3. of each face. Then, for each site considered, we only For the 
InSphere primitive, instead of computing carry out the necessary computation to obtain r3,3 whether a 
fifth point is in the circumsphere of four for the Orientation primitive and Z3 for the InSphere points, 
we compute a signed dist ante function. For a primitive. face (a, b, c), let <Circ be the center of the 
circumcircle determined by these three points. A fourth point d, 4.2 Bucketing together with (a, b, c) 
will determine a circumsphere. Let cd be the center of this sphere. Define the signed A bucketing technique 
is employed in our algorithm distance function of d with respect to (a, b, c), as to reduce the search 
complexity. Although in the worst case it may be necessary to check all sites in abx ac order to expand 
a face, in most cases the tetrahedronSD(d) :f (~~ &#38;~rc) . Ilab x acll can be found by just looking 
at sites near the face. If we arrange the sites so that we can find all the sites It can be shown that 
finding a site din Hp((a, b, c)) near a given point immediately, the search will be verythat forms an 
empty sphere is equivalent to finding the efficient. This notion of closeness has been exploited d that 
minimizes SD(d). To get a convenient expres­ by using bucketing in several previous works [3, 6]. All 
sion for SD(d), we arrange the sphere equation in a these works use a regular partition for simplicity, 
and matrix form: we adopt this scheme for the same reason. The unit cube is partitioned into sub-cubes 
of the same size, or cells. The number of cells is proportional to the size of the input so that the 
expected number of sites in each cell is constant. The cell in which each site is located  [Rx il=-[lll] 
is computed in the initialization step and the sites are where p, q,s are the coordinates of the center 
(d and reordered so that each cell contains sites with consec­p is the quantity pz + qz + S2 r2. Subtracting 
row 1 utive indices. Therefore, the bucketing structure is of this equation from the other rows gives: 
simply a list of index pairs each recording the range of site indices for a cell. Ilbllz-11412 A cell 
is opened if all the sites in the cell are RTZ=-IIcllz-Ilallz searched. For each face, the idea of face 
expansion [1 Ildllz-llallz with bucketing is performed by first opening the cell in which ~circ is located 
and then iteratively openingwhere cells that are most likely to contain the best site. The Z = [Zl Z2 
Z# = 2QT~d search terminates when all cells intersecting the best Since Q is an orthogonal matrix, once 
the first two circumsphere or the positive half-space are opened. columns are computed, the third column, 
qs, is With an appropriate strategy to determine the cell to open faces, we would encounter load-balancing 
prob­lems twofold: first, each face may need to search a different number of cells, which would cause 
a vari­ation in the number of iterations before the search terminates; second, each cell may contain 
a different number of sites, which would cause a variation in the time needed to complete a search iteration. 
be opened in each iteration, this routine should per­ form very well for most faces. However, if we ap­ 
plied this routine in parallel directly to expand all The general strategy in dealing with these problems 
is to convert the time variation into a resource vari­ation and use virtual processing to balance the 
load of each physical processor. That is, we evaluate the load for each component of the problem and 
allocate the resources proportionally. The algorithm is modi­fied so that several cells may be opened 
for a face at one iteration. We take the strategy that allows more cells to be opened as the search iterates, 
and open all cells once a preset limit is exceeded. This enables the amount of work to be adaptive to 
the different needs of each face while it guarantees a limit on the number of iterations. Two auxiliary 
structures are employed to cope with the two levels of imbalance. In order to balance the variation in 
the number of cells to be opened among the faces, a number of requests are allocated for each face, each 
being responsible for opening a cell for the given face and each request is thus identified by a face­cell 
pair; similarly, in order to balance the variation in the number of sites in each cell, a number of warrants 
are allocated for each request. It is the warrant that actually performs the search for the fourth vertex 
us­ing the formulation in Section 4.1. For eficiency, a warrant will perform the search over several 
sites for the same face. The number of sites for which a war­rant is responsible is determined in advance 
and it is usually set to the expected number of sites in a cell. Therefore, each warrant is identified 
by its cor­responding face and the range of site indices that will be searched by this warrant. These 
structures are implemented by segmented lists using the distribute operation described in Sec­tion 2.3. 
Figure 1 illustrates the relation among these structures. In the request, list, a segment of requests 
is allocated for each face; likewise, in the warrant list, a segment of warrants is allocated for each 
request and the warrants corresponding to the same face occupy consecutive segments and thus form a super-segment. 
These super-segments enable fast computation of the minimal signed distance over all sites for the same 
face using the scan primitives. open-face list fl f2 f3 f4 fs ............  II I / request list / Iwla 
II...... ...... I l-I warrant list l\\\ ti+ \ f313f3 13f3 f3 f3 r3 cl c1 C2 (23C3 C3 C3 C4 ...... ............ 
 S1S2 S1 S1 S2S3S4 S1 +- * Figure 1: Implementing the auxiliary structures using segmented lists. In 
this example, four cells are to be searched for the open face f3. A segment of four elements is allocated 
for f3 in the request list. Each request is identified by the face id and a cell id. Then, a segment 
of warrants is allocated for each request, whose length depends on the number of sites in the corresponding 
cell. The heavy dots indicate the heads of segments, while the arrowed line emitting from each dot marks 
a segment. Now we need a strategy to determine the cells to be opened in the initial step and the subsequent 
itera­tions in order to achieve good performance using these structures. Since a face may open O(n) cells 
in one iteration, it is very important to make sure that the corresponding cell of each request can be 
computed ef­ficiently. Therefore, we maintain the volume searched after each iteration as a bo~, which, 
in the following discussion, is defined as a set of cells forming a paral­lelepipeds. A box can be represented 
by the ranges of coordinates along the three axes, thus has a constant­size representation in spite of 
its volume. We define dead box of a face as the box whose constituent cells are already opened for that 
face, and new boz as the box whose constituent, cells will have been opened af­ter the next search. The 
difference between the new box and the dead box are the cells to be opened in the next search. Initially, 
we set the dead box of a face to empty and the new box to the containment box of the face and ~Civc. 
The containment box of a set of points is defined as the minimal box that contains these points and is 
obtained by finding the extremal coordinates among the given points. After each iter­ation, we compute 
the center and the radius of the circumsphere determined by the face and the best site using the formula 
in Section 4.1. The containment box of the circumsphere is then computed and compared with the dead box. 
If further search is necessary, the new box is obtained by extending the dead box in the directions where 
the gaps between the boxes are large. The face expansion scheme using bucketing is sum­marized as follows: 
Algorithm Face Expansion begin for all open faces newbox := containmentJox({a, b, c, ~Circ }); deadbox 
:= 0; done := false; min.sd := m; d := null; while not globally done, where not done compute the number 
of cells to open; aUocate the request list; for each request compute its corresponding cell; obtain the 
range of site indices of the cell; allocate the warrant list; for each warrant W S := the range of its 
site indices; w-min.sd := min{SD(p) I p 6 WSnp E HP}; end for each end for each min-sd := min{mjn-sd, 
w-min.sd} for all warrants of the face; d := p that contributes to this minimum; deadbox := newbox; newboz 
:= containment-box( circumsphere((a, b, c, d))); if newboz ~ deadbox or (unit cube n Hp) ~ deadbox done 
:= true; else adjust newbox; end while end for all end Algorithm Robustness Robustness is a serious 
issue in geometric algo­rithms due to degeneracies and numerical errors. In their primitive form, many 
geometrical algorithms simply assume that there is neither degeneracy nor numerical error. As a result, 
when these methods are used on data that is degenerate or nearly degener­ate, they either fail completely 
or else give incorrect results. Another problem that may occur in a par­allel algorithm is the inconsistency 
among the partial results obtained by different processors. In the degen­erate cases, a Delaunay cell 
may be partitioned into tetrahedral in many ways, and each processor may in­dependently decide to take 
a different one. This will not only generate intersecting tetrahedral, but may also lead the algorithm 
into infinite loop by repeat­edly generating the same tetrahedral. Our approach in obtaining robustness 
while main­t aining efficiency consists of three elements. Besides using the numerically stable methods, 
such as the QR decomposition, we also apply a perturbation to the input points to resolve ambiguities 
in the InSphere primitive, and a topological check is employed to de­tect and eliminate the inconsistency 
among the dis­tributed results. 5.1 Resolving ambiguities One simple method to resolve the ambiguities 
is to add a small amount of random noise to the sites. However, this is not a good way to eliminate 
degen­eracies, because the added noise generates many extra tetrahedral, even where there is no ambiguity. 
A bet­ter approach is to perturb the input, only when neces­sary. An important method of this type, called 
Simu­lation of Simplicity, has been devised by Edelsbrunner and Miicke [7]. Here perturbations are applied 
when degeneracies are detected, and symbolic methods are used. Fewer false features are introduced and 
there is usually no after-the-fact removal of zero volume tetra­hedral. Detection of degeneracy is implemented 
by us­ing extended-precision integers to achieve the effect of exact arithmetic. This, in addition to 
the fact that the order of the sites must be fixed for the test, results in costly computations. In order 
to resolve the ambiguities in the InSphere primitive while maintaining efficiency, we perform a random 
linear transformation: a single perturbation matrix A = I+qkl is computed, where M is a random matrix 
and q is a small constant. and the input points v are replaced by points x = Aw. Since matrix A is the 
same for all input points, all the affine relations implied by the original data are preserved. It is 
proved in [3] that with probability 1, degen­eracy exists after the transformation only if all input 
points are co-planar or the matrix .4 is singular. In practice, the entries of h! are chosen at random 
in [0, 1], and the probability that A is singular is very small with a small q. As the ambiguities of 
the In-Sphere primitive are resolved by the perturbation, De­launay cells with more than four points 
will be par­titioned consistently among the processors. Since co­planarity is preserved in our perturbation, 
the Orien­tation primitive will prevent the generation of zero­volume tetrahedral. Therefore, the algorithm 
should output a desirable triangulation of the space. 5.2 Consistency check The perturbation eliminates 
degeneracies in real arithmetic. But in the case of floating point arith­metic, there is a finite probability 
that there will still be degeneracies with respect to the precision of the arithmetic. It is very difficult 
to detect and elimi­nate such degeneracies by checking geometric prop­erties since such computation will 
also be subject to numerical errors. Therefore, topological consistency is checked to further reduce 
the impact of such problems. The consistency check is based on the following topological property of 
a triangulation: each face of the triangulation should have at most one tetrahedron on each side of the 
face. In a degeneracy involving five co-spherical points, there are two different ways to partition the 
convex hull of these five points. Two tetrahedral from the two different partitions will always coincide 
on one face. By detecting that two tetrahedral are coincident on the same side of a face, we can detect 
such degeneracy and eliminate it by discarding all but one tetrahedron that are coincident, on the same 
side of a face. For degeneracies involving only five points, it suf­fices to check only the faces of 
the new tetrahedral. If more than five points are involved in such degeneracy, the situation will be 
much more complicated. In such cases, intersecting tetrahedral may be generated with­out violating the 
topological consistency. However, as the different ways of partitioning should agree on the convex hull, 
the inconsistency will at least be detected at a later iteration and localized within the convex hull 
if we check all the faces in the triangulation. Although the degeneracy will not be eliminated by discarding 
the extra tetrahedral, we will see that these situations rarely occur with double-precision floating-point 
arith­metic. Based on the above discussion, we check the topo­logical consistency after each face expansion 
by main­taining a dead face dictionary. The dictionary is ini­tialized as empty. After each face expansion 
iteration, new dead faces are inserted into the dictionary. Du­plicates in the dictionary will be detected 
and the cor­responding tetrahedral will be eliminated. Complexity analysis The complexity of the algorithm 
is determined by two factors: the number of face expansion iterations and the number of sites searched 
in face expansion steps. Using the graph distance function, dT(), defined in Section 3, the number of 
face expansion iterations is mac{dT(t) I t G 2 }. According to [13], if the algo­ rithm terminates after 
m steps, there will be Q(m2) tetrahedral in the final structure. Since the output size of a 3-d Delaunay 
triangulation can be 0(n2) in the worst case, the algorithm may need O(n) iterations to generate all 
the tetrahedral. However, such worst cases rarely happen in prac­tice. We observed empirically that if 
the input sites are uniformly distributed random points, the number of tetrahedral incident to a site 
has a distribution sim­ilar to a normal distribution with mean 24. If we con­sider only tetrahedral incident 
to a site and start the face expansion from one initial face, the expected num­ber of iterations should 
be a small constant close to four. In the Parallel Extrapolation algorithm, a tetra­hedron can be generated 
through a path from any ini­tial face, thus it is very unlikely that dT(t) is large for any t.We can 
randomize the choice of initial faces ( instead of the one whose circumcircle is smallest ) to ensure 
this low probability even if the output size is larger than O(n). As for the cost of face expansion, 
we conjecture that each face will search only an expected constant num­ber of sites for uniformly distributed 
random points. Although we will not give a rigorous proof, Dwyer [6] has proved linear expected time 
for a sequential algo­rithm that, performs face expansion in a similar fash­ion. The major differences 
between his work and ours are: 1. Dwyer s complexity analysis is done with sites uniformly distributed 
in a unit ball instead of a unit cube. However, since the difference between a unit ball and a unit cube 
is a small percentage of both in 3 dimensions, we conjecture that his result can apply to our problem 
space. 2. His algorithm uses a priority queue to arrange the order in which the cells are opened. When 
a Delaunay tetrahedron is discovered, all cells that intersect the circumsphere and the positive half­space 
must have been opened. Our algorithm ba­sically opens all cells in the containment box of the cells his 
algorithm opens. Again, in 3-d, the ratio between the number of extra cells and the number of necessary 
cells is bounded by a con­stant.  Based on the above discussion, a face expansion in our algorithm should 
search only an expected con­stant number of points for uniformly distributed ran­dom input, and it should 
only take a. small number of face expansion iterations. Considering the costs of the scan-vector primitives, 
our algorithm should have an expected time complexity poly-logarithmic in n using O(n) processors for 
such input sets in many scalable network models. 7 Experimental result Our algorithm has been implemented 
in the C* lan­guage and tested on Connection Machines CM-2 and CM-5. For the CM-2, it is compiled using 
the Version 6.0.3 compiler and tested on a 16 K-processor model. Single-precision floating point arithmetic 
is used due to the lack of double-precision hardware support in the tested machine. For the CM-5, it 
is compiled us­ing the Version 7.0 beta compiler and tested on a 32­node system. Double-precision floating-point 
arith­ metic is used on the CM-5 unless specified otherwise. According to the vendor, this compiler is 
a test ver­sion that does not utilize the vector units. It has not had the benefit of optimization or 
performance tuning and, consequently, is not necessarily representative of the performance of the full 
version of this software. Table 1 shows the CM-5 running time and other information for several 3-d data 
sets. The data sets are divided into three groups: 1. uniformly-distributed random points, which are 
named by the prefix rand and a suffix indicating the sizes of the sets; 2. points sampled from 3-D objects, 
as indicated by their names. Some of these files are also used in [8]; 3. polymer data, which are named 
by the prefix polymer and a suffix indicating the sizes of the sets. Among the columns, n and k are the 
number of input points and the number of output tetrahedral respectively: the col­umn time shows the 
running time of the main loop, and time/k is the cost per tetrahedron. Figure 2 shows the running time 
for uniformly dis­tributed random points. The figure is drawn in double logarithmic scale, and a reference 
curve is plotted to shown the slope of a linear relation in such scale. The running times for the random 
points in Table 1 are plotted as the CM-5 double precision curve. The slope is indeed very close to that 
of the linear relation curve. As a contrast, a CM-5 brute-force curve is plotted for which exhaustive 
search is used in all face expansions without the auxiliary processor structures. It illustrates a curve 
with quadratic growth rate. The break-even point for bucketing is about 500 sites. Also the running time 
using a 16 K-processor CM-2 is plot­ted. Its growth rate is even slower than the linear relation since 
the machine is not fully utilized with most of the problem sizes. This curve illustrates the Time (see) 
11 103: O(n) CM5 brute force + 10 : 1 ;00 1000 10000 Number of sites Figure 2: The running times for 
random points Number of tetrahedral ~05 104 103 102 10 r lC 1 1 1 1 12345678 910 Iteration Figure 3: 
The number of tetrahedral generated at each iteration. achievement of a poly-logarithmic time bound with 
unlimited resources. In the complexity analysis, we conjectured the low likelihood that dT (t) is large 
for any tetrahedron t. The column entitled depth in Table 1 shows the number of face expansion iterations 
performed for each data set, i.e., the largest dT(t).None of the data sets requires more than 10 iterations 
and most of them are done after 6 or 7 iterations. Figure 3 shows the distri­bution of dT(t) for random 
points. It is confirmed that most tetrahedral are discovered by the fourth iteration and the maximal 
d~(t) seems to grow in a logarithmic rate with the minimal-circle initialization. It is also interesting 
to notice that, for all the input sets, the total number of tetrahedral is roughly six times the Data 
set n k depth time time/k FE search/FE (see) (ins) (sites) rand.125 125 676 6 2.42 3.5 1095 81.7 rand.250 
250 1489 6 4.40 2.9 2311 69.2 rand.500 500 3037 6 7.14 2.3 4693 65.4 rand.1000 1000 6361 7 11.84 1.8 
9732 73.5 rand.2000 2000 12947 7 26.10 2.0 19717 85.3 rand.4000 4000 26252 8 61.17 2.3 39812 80.1 rand.8000 
8000 52895 8 125.31 2.4 80120 88.6 rand.16000 16000 106693 9 299.77 2.8 161060 88.6 dodecahedron 20 40 
5 0.65 16.3 104 20.0 torus 256 1328 6 4.34 3.2 2346 140.7 ml 318 1967 6 7.91 4.0 2974 134.0 cube 343 
1296 5 6.54 5.0 2417 133.7 tori----800 5889 7 17.87 3.0 9150 156.3 -. spiral 1248 8799 7 28.62 3.2 13818 
182.0 m2 1366 8938 7 27.58 3.1 13571 121.7 bust 2630 17249 6 91.33 5.2 26338 324.3 16909 fract2 2704 
10 182.56 10.7 26323 724.4 11111, I I:-.. .I 1 4~00 1 2755(] I I liss-5-8 6 I 113.22 I 4.1 I 43U84 I 
24(i.u I teapot 4668 27081 6 246.37 9.0 43778 617.4 phonel 6070 39429 8 371.19 9.4 59739 645.7 polynler.278 
278 1670 6 5.15 3.1 2538 97.0 rmlvmer.500-------500 2904 71 6.64 2.2 4516 61.1 , -4 II t1t, , Dolvmer.932 
. J 932 6 16.82 3.1 8524 141.9 1 -5424 1t,#,, 1 Table 1: Experimental results for3Ddata sets on32-node 
CM-5 using double precision arithm etic. number of the sites. rand.4000 rand.16000 The last two columns 
in Table l, labeled FE and #PE time speedup time speedup search/FE , are the number of face expansions 
per­32 55.03 1.0 266.9 1.0 formed by the algorithm and the average number of 128 15.31 3.59 77.83 3.43 
points searched in each face expansion respectively. 256 11.63 4.73 43.89 6.08 The number of face expansions 
is usually 50% more than the number of the tetrahedral due to the convex hull faces and those tetrahedral 
that are discovered by Table 2: Scalability over different machine sizes. The several open faces in the 
same iteration. In these ex-times are in seconds. periments, the unit cube is partitioned in a way such 
Performance Computing Center using various number that the computation cost of the site search and the 
of processors. Table 2 lists the running time and the overhead of managing the intermediate structures 
are speedup with respect to the 32-node running time. balanced. The number we obtained is an average 
of Substantial speedup is obtained with increased num­ eight sites per cell. Therefore, according to 
the col­umn search/FE , an average of 10 cells are opened ber of processors, although it is limited by 
the scala­ bility of the network connecting the processors as well in each face expansion for the random 
points. This as the constant overhead such as resource allocation means that most face expansions are 
completed after and instruction decoding. the second search iteration, and it confirms our conjec­ture 
that a constant number of searches are expected As for the robustness of the program, it has not in 
a face expansion. encountered any inconsistency while using double- Another important goal of our algorithm 
is good precision arithmetic. With single-precision arithmetic, scalability over a wide range of machine 
sizes. We the consistency checking helps to overcome the de­ tested our algorithm on the CM-5 at the 
Army High generacies in all cases involving random points and Data set n k inconsistency rand.4000 4000 
26239 2 rand.16000 16000 106879 5 torus 256 1351 27 cube 343 too many II spiral 1248 8795 8 bust 2630 
-too many fract2 2704 -too many liss-5-8 4200 27545 19 teapot 4668 -too many phonel 6070 too many Dolvmer.932 
932 5432 10 Table 3: Robustness with the consistency check in single­precision arithmetic, polymer data. 
For some highly degenerate data sets that involve hundreds of points symmetrically sam­pled from a sphere 
or a circle, the single-precision arithmetic is unable to localize the inconsistency and therefore unable 
to finish within a reasonable amount of time. The result is shown in Table 3. Conclusion In this paper, 
we presented a parallel algorithm for constructing Delaunay triangulation of a set of points in three-dimensional 
space. The algorithm achieves a high degree of parallelism by starting the construc­tion from every vertex 
and expanding over all open faces thereafter. In the expansion of the open faces, the search is made 
faster by using a bucketing tech­nique. The algorithm is designed under a data-parallel paradigm. It 
uses segmented list structures and vir­tual processing for load-balancing. As a result, the algorithm 
achieves a fast running time and good scal­ability over a wide range of problem sizes and machine sizes. 
We incorporated a topological check for elimi­nating inconsistencies due to degeneracies and numer­ical 
errors. The algorithm is implemented on Connec­tion Machines CM-2 and CM-5 and the experimental results 
confirmed our prediction of a good performance over a wide variety of input data sets. Acknowledgement 
The first author would like to thank Larry Davis and David Mount for their valuable discussions and suggestions 
during the project development. We also like to thank E.P. Mucke for his help in obtaining some of the 
test data sets. References <RefA>[1] <SinRef><author>F. Aurenhammer</author>. <title>Voronoi diagram a survey of a fundamental geometric data 
structure</title>. <journal>ACM Comp­uting survey</journal>, <volume>23(3)</volume>:<pages>345-405</pages>, <date>1991</date></SinRef>. [2] <SinRef><author>F. Aurenhammer </author>and <author>H. Edelsbrunner</author>. <title>An optimaJ 
algorithm for constructing the weighted Voronoi dia­gram in the plane</title>. <journal>Pattern Recognition</journal>, <volume>17</volume>:<pages>251-257</pages>, 
<date>1984</date></SinRef>. [3]<SinRef><author> I. Beichl </author>and<author> F. Sullivan</author>. <title>Fast triangulation via empty spheres</title>. <note>manuscript</note>, <date>1992</date></SinRef>. [4] <SinRef><author>J. Bernal</author>. 
<title>On the expected complexity of the 3­dimensional Voronoi diagram</title>. <tech>Technical Report NISTIR-4321</tech>, <institution>National 
Institute of Standards and Technology</institution>, <date>1990</date></SinRef>. [5] <SinRef><author>G. Blelloch</author>. <title>Vector models for data-parallel cornput­x 
ng</title>. <publisher>MIT Press</publisher>, <date>1990</date></SinRef>. [6] <SinRef><author>R. A. Dwyer</author>. <title>Higher-dimensional Voronoi diagrams in linear expected time</title>. <journal>Discrete 
and Computational Geometry</journal>, <volume>6</volume>:<pages>343-367</pages>, <date>1991</date></SinRef>. [7] <SinRef><author>H. Edelsbrunner </author>and <author>E. Miicke</author>. <title>Simulation of sin~­plicity: 
a technique to cope with degenerate cases in geometric algorithms</title>. <journal>ACM Transactions on Graph­ics</journal>, <volume>9(1):</volume><pages>66 
104</pages>, <date>1990</date></SinRef>. [8] <SinRef><author>H. Edelsbrunner </author>and <author>E. Mucke</author>. <title>Three dimensional al­pha shapes</title>. <tech>Technical Report UIUCDCS-R-92-1734</tech>, 
<institution>Dept. of Compnter Science, University of Illinois at Urbana-Champion</institution>, <date>1992</date></SinRef>. [9] <SinRef><author>H. Edelsbrunner </author>and <author>N. 
Shah</author>. <title>Incremental topologi­cal flipping works for regular triangulations</title>. <booktitle>In Proc. 8th ACM Sgmpos:urn 
on Computational Geometrv</booktitle>, <pages>pages 43-52</pages>, <date>1992</date></SinRef>. [10] <SinRef><author>W. Hillis </author>and <author>G. Steele</author>. <title>Data parallel algorithms</title>. 
<journal>Communications of the ACM</journal>, <volume>29</volume>:<pages>1170 1183</pages>, <date>1986</date></SinRef>. [11] <SinRef><author>H. Inagaki</author>, <author>K. Sugihara</author>, and <author>N. Sugie</author>. <title>Numerically 
robust incremental algorithm for constructing three­dimensionid Voronoi diagrams</title>. <booktitle>In Proc. Fourth Cana­dian 
Conference on Computational Geometry</booktitle>, <pages>pages 334-339</pages>, <date>1992</date></SinRef>. [12] <SinRef><author>E. Puppo </author><author>et al</author>. <title>Parallel terrain triangulation</title>. 
<booktitle>In Proc. Fifth Interraational Sympos~um on Spatial Data Handling</booktitle>, <pages>pages 632-641</pages>, <date>1992</date></SinRef>. [13] <SinRef><author>S. Saxena</author>, 
<author>P. Bhatt</author>, and <author>V. Prasad</author>. <title>Efficient VLSI parallel algorithm for Delaunay triangulation on or­thogonal 
tree network in two and three dimensions</title>. <journal>IEEE Trans. on Computers</journal>, <volume>39(3)</volume>:<pages>400-404</pages>, <date>1990</date></SinRef>. [14] <SinRef><author>G. Stewart</author>. 
<title>Introduction to Matrix Computations</title>. <publisher>Academic Press</publisher>, <location>New York</location>, <date>1973</date></SinRef>.</RefA>  
			
