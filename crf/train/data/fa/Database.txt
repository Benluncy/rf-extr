
 pDatabase: Parallelism in a Memory-Mapped Environment (Research Summary) Peter A. Buhr, Anil K. Goel, 
Naomi Nishimura, and Prabhakar Ragde Department of Computer Science, University of Waterloo, Waterloo, 
Ontario, Canada N2L 3G1 {pabuhr,akgoel, nishi,plragde}@ uwaterloo.ca Abstract The goal of the ~Database 
project is to understand the be­haviour of data structures and their algorithms, both parallel and sequential, 
in a memory-mapped environment. Memory mapping allows primary-memory pointer-based data struc­tures to 
be stored on secondary storage, and subsequently traversed and modified, without transforming the embedded 
point ers. The project incorporates a collaboration between practitioners and theoreticians and has produced 
a toolkit to support a parallel memory-mapped environment, ext en­sive concurrence y tools, an analytical 
model for the system vaEdated by experiments, and preliminary results involving parallel database join 
algorithms as well as sequential B­tree and R-tree data structures. Future work includes aug­menting 
the toolkit, developing and testing more parallel algorithms tuned for efficiency in a memory-mapped 
envi­ronment, and extending the current model to cover more general algorithms and capture more low-level 
details of the physical machine. Introduction Successful parallel environments must take advantage of 
not only process parallelism (multiple CPUS) but also data par­allelism (data partitioned across multiple 
disks). Data par­allelism is particularly important; currently it is the most effective mechanism to 
deal with the increasing disparity between CPU and 1/0 performance. Traditional techniques for building 
and managing data structures on secondary stor­age involve constructing complex, often application-specific, 
buffer management subsystems using explicit 1/0 calls. To facilitate code reuse and rapid prototyping 
and to decrease programming complexity, 1/0 mechanisms should be made simpler and more transparent, particularly 
in parallel envi­ronments. Manipulating persistent data structures presents further difficulties due 
to the incompatibility between addresses in primary memory (pointers) and secondary storage (disk block 
addresses); pointers can not normally be stored di­ rectly on secondary storagefor further usage, and 
creat­ing and manipulating complex structures without pointers Permission to make digital/hard copies 
of all or part of this material for personal or classroom use is granted without fee provided that the 
copies are not made or distributed for profit or commercial advantage, the copy­right notice, the title 
of the publication and its date appear, and notice is given that copyright is by permission of the ACM, 
Inc. To copy otherwise, to republish, to post on servers or to redistribute to lists, rsquires apecitic 
permission and/or fee. SPAA 96, Padua, Italy @ 1996 ACM o_89791..8W_6/96fM ..$3.50 is both cumbersome 
and expensive. One approach is to ex­tend primary storage practices and tools to apply to sec­ondary 
storage, thereby crest ing a single-level s tore, which eliminates the need for expensive execution-time 
conversions of structured data between primary and secondary storage, while allowing the power of a general 
purpose programming language to be extended to secondary storage. The goal is to achieve substantial 
performance advantages over conven­tional file access [6, 7, 13, 23, 16, 20] in both the development 
and execution of akorithms. Of particular in~erest is the implementation of a single­level store by memory-mapping, 
that is, the use of virtual memory to map data stored on secondary storage into pri­mary storage so that 
the data is directly accessible by the processors instructions [30, 11, 29]. Instead of accessing data 
on disk by explicit read and write routine calls, all 1/0 is performed implicitly by the operating system 
during normal address dereference through the virtual memory hardware. Deferring 1/0 until data is actually 
referenced and taking advantage of available memory-management hardware sig­ nificantly reduces complexity 
of persistent data structures while decreasing the runtime cost.  2 pDatabase Project We are developing 
,uDatabase [11, 9], a C-H--based toolkit for building persistent data structures using a single-level 
store implemented by memory mapping. Our single-level store and others, such as Objectstore [16], Texas 
[25], and QuickStore [29], are differentiated by their approaches to the address cons is tencg problem. 
This problem results when a pointer-based data structure built at a particular location in memory can 
not subsequently be reloaded at that location because it is otherwise occuDied. A commonlv used solution 
. is pointer swizzling, which consists of placing the data struc­ture at a new location in memory and 
modifying its pointers to reflect the new location of data. Swizzling requires the ability to locate 
all embedded pointers and modify them, which adds to the runtime cost. The performance loss is es­pecially 
significant for operations that incur high overhead in data preparation, such as sequential scans, where 
the data is accessed only once, and operations that implicitly fetch and prepare data multiple times 
in large data structures us­ing small primary storage. As well, applications that access multiple structures 
simultaneously cause a large number of address collisions resulting in more pointer swizzling. ~Database 
adopts a different solution to the address con­sistency problem, called the exact positioning of data 
ap­preach. We employ a novel technique that allows application of an old solution, hardware segmentation, 
to the problem: each hardware segment is an address space starting at a vir­ tual zero, in which a persistent 
data structure can be built, stored, and subsequently retrieved and modified. Multiple segments can be 
simultaneously accessed in a single appli­ cation because each segment has its own non-conflicting ad­ 
dress space. When a segment is mapped into memory, point­ ers wu%n the segment do not require modification; 
pointers outside the segment may require modification, but in gen­ eral, these pointers represent a small 
percentage of the total number of pointers in a data structure. 2.1 Concurrency Tools We have developed 
extensive concurrency tools for use in building highly concurrent data structures, involving both processor 
and data parallelism, and have found that a single­level store allows many existing concurrent algorithms 
and data structures to be used directly for persistent data. Basic concurrency capabilities are provided 
in a C++-based toolklt, called pC+E[8, 12]. ~Database uses pC+t to allow a pro­grammer to build whatever 
form of concurrency is appropri­ate. Using a spectrum of pC++ tools, processor parallelism may be specified 
at a low level, where a lock is used to pro­tect data, or at a high level, where a light-weight server 
task controls access to data. As well, the granularity of data ac­cess may be individual fields, entire 
objects, or groups of objects. In general, processor parallelism, e.g., concurrent access to a B-tree, 
involves trade-offs between maximizing concurrency and the administrative overhead of fine-grain locking. 
While achieving near-optimal concurrency is often data-structure specific, we believe it is possible 
to provide general concurrency tools that are applicable to broad classes of dat a structures to simplify 
application development. Some of these general concurrency tools are provided as part of the pDatabase 
toolkit. Data parallelism deals with the CPU-I/O bottleneck by partitioning data across multiple disks 
and then accessing the data in parallel [19, 28]. The transition between the two toolklts occurs at thk 
point with pDatabase providing the alility to partition a single-level store transparently across multiple 
disks with p+ providing parallelism even while 1/0 operations are occurring. As for processor parallelism, 
we have developed general tools, which support a variety of partitioning schemes and parallel algorithms, 
that form part of the ~Database toolkit. 2.2 Algorithm Design The novel persistence facility of pDatabase 
and concurrency capabilities of VGH yield new degrees of freedom for design­ing complex and highly-parallel 
database algorithms: tradi­ tional algorithms must be re-examined in thk new frame­ work, For example, 
we have been able to improve perfor­mance of algorithms by making use of spatial ordering in­formation 
implicit in the pointers defining data structure relations. As well, there is a need for partitioning 
schemes [22] to distribute data across multiple disks to achieve max­imum parallelism; performance is 
a function of the dynamic and possibly concurrent access pattern, which is often inde­pendent of the 
static structure of the data. A natural starting point for the stucly of clatabase algo­rithms is the 
join operation as it is . . . relevant to relational, extensible, and object-orient ed database systems 
ahke [14]. Joining is a merging of data from two collections of data ob­jects, R and S, where an R object 
contains a join attribute that refers to an S object, and data from each is combined to form the join. 
In pointer-based join algorithms [24, 17, 10], the join attribute is a virtual pointer to objects in 
S. We have designed and analyzed parallel pointer-based versions of nested loops, sort-merge, and Grace 
join algorithms. As stat ed above, it is possible to exploit the implicit ordering of objects in S via 
the virtual pointers to eliminate the usual sorting or hashing of S in sort-merge and hash-based joins, 
respectively.  2.3 Analytical Modelling We have developed an analytical model of our memory­mapped environment, 
providing quantitative predictions of performance. The model acts as a high-level filter to predict general 
performance behaviour; only those algorithms that look promising from the model need to be more fully 
tested. More importantly, a quantitative model is an essential tool for subsystems such as a database 
query optimizer which use static analysis to construct a fast implementation. Since prediction of actual 
running time rather than char­acterizing asymptotic complexity was the goal of our model, our work is 
closer to models developed to explain empiri­cal results [24, 21, 18] than extensions to theoretical 
mod­els handling such features as 1/0 complexity, block transfer, and hierarchical memory [2, 1, 4, 15, 
3, 26, 27]. As modelling the physical machine closely and accurately is essential, we have, for example, 
used a measured function to model disk transfer times, to account for all of the following factors. In 
a persistent memory-mapped environment, all 1/ O is per­formed by means of normal virtual memory mechanisms. 
Thus, a read page fault needs to be processed before ex­ecution can continue, whereas writ e-backs can 
be handled in parallel with program execution. Further, 1/0 costs are affected by whether data on the 
disk(s) are accessed sequen­tially or randomly. The total span of disk over wtilch random accesses take 
place is also a factor in determining the amor­tized cost of 1/0. Full details of the model can be found 
in [10]. 2.4 Practical Relevance Besides developing a theoretical framework for studying memory-mapped 
systems, the ~Database project addresses several import ant practical issues, Some of these issues are 
reduction in programming complexity while maintain­ing or improving execution speed, accurate cost estimation, 
exploitation of data and process parallelism with memory­mapping to address the increasing CPU-IO disparity, 
and effective tools for teaching concurrency and parallelism in persistent systems.  3 Preliminary Results 
and Future work To validate our model and analysis, experiments were run that performed full joins on 
two relations R and S with 102,400 objects each [10]. R and S were partitioned across disks with one 
R and one S partition on each disk. All exper­iments were mm on a Sequent Symmetry with 10 processors, 
which uses a simple page replacement algorithm (see [11] for testbed details). The execution environment 
was strictly controlled so the results were not influenced by any outside activitv. The amount of memorv 
for the experiment s ad­dress s pace and the global cache were tightl~ controlled as well. Fig. 1 shows 
the predicted and measured elapsed times for running the various join algorithms with varying amounts 
of available memory. The dlscontinuities in the sort-merge graph occur when additional merging phases 
are required. The curve in the Grace graph at low memory levels results from thrashing caused by the 
page replacement algorithm. As is evident from the graphs, our model does an excel­ ~ 2000 i m e 1600 
i n : 1200 c s 800 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Ratio of primary to secondary memory (a) Nested Loops 
T 700 Model -Experiment . ..- A 650  17 I 1 e i n s 550 e : 500 5 ,. ..,... .... ...= I III 1­ 0.01 
0.02 0.03 0.04 0.05 Ratio of primary to secondary memory (b) Sort Merge r IIII I II I Model * _ ~ 460 
-. Experiment . . . . m n ~ 380 c .. ... .... .. 340 s IIIIII 0.02 0.03 0.04 0.05 0.06 0.07 0.08 Ratio 
of primary to wmondary memory (c) Grace F@we 1: Experimental Results lent job of predicting performance 
for the various join al­gorithms in almost all conditions. In particular, there is a close match between 
prediction and actual performance for nest ed loops and sort-merge. For Grace, our approximation for 
1/0 caused by thrashing at low memory levels is reason­ably accurate; there is scope for further refinement 
of this approximation. A major part of the difference between pre­ diction and actual behavior at low 
memory levels comes from the overhead introduced by the particular page replacement st rat egy used by 
the operating system. Future studies with the model include speedup and scaleup measurements, changing 
the nature of the joining relations and a comparative analysis of various algorithms. Our analysis of 
the join algorithms highlights an inherent drawback in memory-mapped single-level stores: the lack of 
control over btier management on the part of the database application results in incorrect decisions 
being made at times by the underlying page replacement strategy. While accePt­ing this inefficiency, 
we demonstrated two approaches for achieving predictable behaviour, an essential property in a database 
system. With single-level stores becoming more common, it is our hope that future research and develop­ment 
in operating system architecture will make it feasible for database applications to exercise more control 
over the replacement st rat egies used (see [5]). There is also scope for further improvement in the 
design of our model, especially in the modelling of the underlying paging behaviour, Future work will 
involve extending our model to other memory­ mapped environments, allowing us to perform comparative 
 studies, It will also be an int cresting exercise to explore the . . apphcablhty of our model in t radlt 
ionzd environments. References <RefA>[1]<SinRef><author>Aggarwal, A.</author>, <author>Alpern, B.</author>, <author>Chandra, A. K.</author>, and <author>Snir, M.</author><title>A Model for 
Hierarchical Memory </title>. In <booktitle>ACM S Z OC</booktitle>, pages <pages>305-314</pages>, <date>May 1987</date></SinRef>. [2] <SinRef><author>Aggarwal, A.</author>, <author>Chandra, A. K.</author>, and <author>Snir, 
M.</author> <title>Commu­nication Complexity of PRAMs </title>. <journal>Theoretical Comput</journal>. ,$cz.. <volume>71(1)</volume>:<pages>3-28</pages>, <date>Jan. 1990</date></SinRef>. [3] <SinRef><author>Aggarwal, 
A.</author> and <author>Vitter, J. S.</author> <title>The Input/Output Complexity of Sorting and Related Problems </title>. <journal>Corn­mun</journal>. <publisher>ACM</publisher>, <volume>31(9)</volume>:<pages>1116 
1127</pages>, <date>Sept. 1988</date>. </SinRef>[4] <SinRef><author>Alpern, B.</author>, <author>Carter, L.</author>, <author>Feig, E.</author>, and <author>Selker, T.</author> <title>Uni­form Memory Hierarchies </title>. 
<journal>Algortthmica</journal>, <volume>12(2/3):</volume><pages>72­109</pages>, <date>August/September 1994</date>.</SinRef>[5] <SinRef><author>Appel, A. W. </author>and <author>Li, K. </author><title>Virtual Memory Primitives 
for User Programs </title>. In <booktitle>4thInternational Conference on Architectural Support for Programming Languages 
and Operating Systems</booktitle>, pages <pages>96-107</pages>, <date>Apr. 1991</date>. </SinRef>[6] <SinRef><author>Atkinson, M. P.</author>, <author>Bailey, P. J., </author><author>Chisholm, K. J., </author>
<author>Cock­shott, P. W.,</author> and <author>Morrison, R. </author><title>An Approach to Persis­tent Programming </title>. <journal>The Computer Journal</journal>,<volume> 26(4) </volume>
:<pages>360­365</pages>, <date>NOV. 1983</date>. </SinRef>[7] <SinRef><author>Atkinson, M. P. </author>and <author>Morrison, R. </author><title>Procedures as Per­ .;.t.m~ Da</title>&#38;a OL&#38;&#38;J~. 
<publisher>A Cl-f Traz</publisher>.. &#38;03. .La=3. f+~., <volume>7(4):</volume><pages>539-559</pages>, <date>Oct. 1985</date>. </SinRef>[8] <SinRef><author>Buhr, P. A., </author><author>Ditchfield, G</author>., <author>Stroobosscher, 
R. A</author>., <author>Younger, B. M</author>., and <author>Zarnke, C. R. </author><title>uC++: Con­currency in the Object-Oriented Language C++</title> . <booktitle>Software 
Practice and Experience</booktitle>, <volume>22(2)</volume>:<pages>137 172</pages>, <date>Feb. 1992</date>. </SinRef>[9] <SinRef><author>Buhr, P. A,</author> and <author>Goel, A. K.</author> <title>~Database Annotated 
</title></SinRef>[23] <SinRef><author>Shekita, E. </author>and <author>Zwilling, M. </author><title>Cricket: A Mapped, Reference Manual, Version 1.0 . Technical Report 
Persistent Object Store </title>. In <editor>Dearle, A.</editor><editor> et al</editor>., edi-Unnumbered: Available via ftp from<url> plg.uwaterloo.ca 
tors</url>, Implementing Persistent Object Bases: Principles in <url>pub/uDatabme/uDatabase .ps,gz</url>, <institution>Department of 
Com-and Practise</institution>, <booktitle>Proceedings of the Fourth International put er Science</booktitle>, <publisher>University of Waterloo</publisher>, <location>Waterloo</location>, 
On-Workshop on Persistent Object Systems , pages <pages>89 </pages>tario, Canada, N2L 3GI, June 1995. <pages>102</pages>. Morgan Kaufmm, 
<date>1990</date>. </SinRef>[10] <SinRef><author>Buhr, P. A</author>., <author>Goel, A. K</author>., <author>Nishimura, N</author>., and <author>Ragde</author>, </SinRef>[24]<SinRef><author> Shekita, E. J. </author>and <author>Carey, M. J</author>, <title>A 
Performance Evalua- P. Parallel Pointer-Based Join Algorithms in Memory tion of Pointer-Based Joins </title>. 
In <booktitle>ACM SIGMOD</booktitle>, pages Mapped Environments . To appear in the Proceedings <pages>300-311</pages>,<date> June 1990</date>. oj the I%h 
IEEE International Con~erence on Data En­ </SinRef>[25] <SinRef><author>Singhal, V.</author>, <author>Kakkad, S. V</author>,, and <author>Wilson, P.R. </author>Texas: gineering, 
Feb. 1996. An Efficient, Portable Persistent Store . In Albano, A. </SinRef>[11] <SinRef><author>Buhr, P. A</author>., <author>Goel, A. K</author>., and 
<author>Wai, A. </author><title>pDatabase : A and Morrison, R., editors, Persistent Object Systems, Toolld for Constructing Memory 
Mapped Databases</title> . pages <pages>11 33</pages>, <date>Sept. 1992</date>. <publisher>Springer-Verlag</publisher>. In<editor> Albano, A. </editor>and <editor>Morrison, R., </editor>editors, 
<booktitle>Persistent Object Sgsteras</booktitle>, pages <pages>166-185</pages>, <date>Sept. 1992</date>. <publisher>Springer­ </publisher></SinRef>[26] <SinRef><author>Vltter, J. S. </author>and <author>Shriver, E. A. 
M. </author><title>Algorithms for Par­allel Memory, I: Two-Level Memories </title>. <booktitle>Atgorithmica</booktitle>, Verlag. <volume>12(2/3)</volume>:<pages>110-147</pages>,<date> August/September 
1994</date>. </SinRef>[12]<SinRef> <author>Buhr, P. A. </author>and <author>Stroobosscher, R. A,</author> <title>PC++ Anno­t ated Reference Manual</title>, Version 4.4 . Technical 
Report </SinRef>[27]<SinRef> <author>Vitter, J. S. </author>and <author>Shriver, E. A. M. </author><title>Algo­ rithms for Parallel Memory, II: Hierarchical Multi-Unnumbered:</title> 
Available via ftp from <url>plg.uwaterloo.ca </url>Level Memories . <booktitle>Algorzthmica</booktitle>, <volume>12(2/3)</volume>:<pages>148-169</pages>, Au­in pub/uSystem/uC+ 
+.ps.gz, <institution>Department of COmPuter gust/September 1994. Science, University of Waterloo</institution>, <location>Waterloo, Ontario, 
Canada</location>, N2L 3G1,<date> Sept. 1995</date>. </SinRef>[28] <SinRef><author>Weikum, G., </author><author>Zabbak, P.</author>, and<author> Scheuermann, P. </author><title>Dy­ namic File Allocation 
in Disk Arrays </title>. In <booktitle>ACM SIG­ </booktitle></SinRef>[13] <SinRef><author>Copeland, G., </author><author>Franklin, M., </author>and <author>Weikum, G, </author><title>Uni-MOD</title>, pages <pages>406-415</pages>, 
<date>June 1991</date>. form Object Management . In <booktitle>Advances in Database Technology Proc</booktitle>. <institution>European Conference on 
Database </institution></SinRef>[29] <SinRef><author>White, S. J. </author>and <author>DeWkt, D. J. </author><title>QuickStore: A HighTechnology</title>, pages <pages>253-268</pages>,<date> Mar. 1990</date>. 
Performance Mapped Object Store . In ACM SIG- MOD, pages 395-406, May 1994. </SinRef>[14] <SinRef><author>Graefe, G. </author><title>Sort-Merge-Join: 
An Idea whose Time has(h) Passed? </title>, In <booktitle>IEEE International Conference on </booktitle></SinRef>[30] <SinRef><author>Wilson, P. R. </author><title>Pointer Swizzling 
at Page Fault Data Engineering</title>, page <pages>406</pages>, <date>February 1994</date>. <title>Time: Efficiently Supporting Huge Address Spaces 
on Standard Hardware </title>. Computer Architecture News, Red-Blue Pebble Game . In <booktitle>ACM STOC</booktitle>, pages <pages>326</pages>-<volume>19(4):</volume>6-13, 
<date>June 1991</date>. <pages>333</pages>, 1981. </SinRef>[15] <SinRef><author>Hong, J.-W. </author>and <author>Kung, H. T. </author><journal>1/0 Complexity: The </journal></SinRef>[16] <SinRef><author>Lamb, C., </author><author>Landis, G., </author>
<author>Orenstein, J., </author>and <author>Weinreb, D. </author><title>The Objectstore Database System </title>. <journal>Comrrvun</journal>. <publisher>ACM</publisher>, <publisher>34(10):</publisher><pages>50-63</pages>, <date>oct. 1991. 
</date></SinRef>[17] <SinRef><author>Lkmven, D. F.</author>, <author>DeWitt, D. J</author>., and <author>Mehta, M. </author><title>Pointer-Based Join Techniques for Object-Oriented Databases 
</title>. In <booktitle>International Conference on Parallel and Dwtributed Information Systems</booktitle>, <date>Jan. 1993</date>. </SinRef>[18] <SinRef><author>Martin, 
T. P., </author><author>Larson, P.-A., </author>and <author>Deshpande, V. </author><title>Parallel Hash-Based Join Algorithms for a Shared-Everything Environment 
</title>. <journal>IEEE Transactions on Knowledge and Data Engineering</journal>, <volume>6(5):</volume><pages>750-763</pages>, <date>Oct. 1994</date>. </SinRef>[19] <SinRef><author>Patterson, D. A., 
Gibson, G</author>., and <author>Katz, R. H.</author> <title>A Case for Redundant Arrays of Inexpensive Disks(RAID)</title> . <date>In ACM SIGMOD, pages 
109-116, June 1988</date></SinRef>. [20]<SinRef> <author>Richardson, J. E.</author>, <author>Carey, M. J. </author>,and <author>Schuh, D. T. </author><title>The Design of the E Programming 
Language </title>. <publisher>ACM Trans</publisher>. Prog. Lang. Syst,, <volume>15(3):</volume><pages>494-534</pages>, <date>July 1993</date></SinRef>. [21] <SinRef><author>Schneider, D. A.</author> and <author>DeWitt, 
D. J.</author> <title>A Perfor­mance Evaluation of Four Parallel Joins Algorithms in a Shared-Nothing Multiprocessor 
Environment </title>. In <booktitle>ACM SIGMOD</booktitle>, pages <pages>110-121</pages>, <date>June 1989</date></SinRef>. [22]<SinRef> <author>Seeger, B,</author> and <author>Larson, P.-A.</author> <title>Multi-Disk B-trees</title> 
. In <booktitle>ACM SIGMOD</booktitle>, pages <pages>436-445</pages>, <date>June 1991</date>. 
</SinRef>			
</RefA>
