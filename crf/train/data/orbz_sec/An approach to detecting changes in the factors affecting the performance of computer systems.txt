
 An Approach to Detecting Changes in the Factors Affecting the Performance of Computer Systems Robert 
Berry and Joseph Hellerstein IBM Research Division T.J. Watson Research Center Yorktown Heights, NY 10598 
 Abstract Resolving intermittent performance problems in computer systems is made easier by pinpointing 
when a change occurs in the system s perforrnance­determinin g factors (e.g., workload composition, con­figuration). 
Since we often lack direct measurements of performance factors, this paper presents a proce­dure for 
indirect~ detecting such changes by analyzing performance characteristics (e.g., response times, queue 
lengths). Our procedure employs a widely used clustering algorithm to identify candidate change points 
(the times at which performance factors change), and a newly developed statistical test (based on an 
AR(1) time series model) to determine the sig­tilcance of candidate change points. We evaluate our procedure 
by using simulations of M/M/1, FCFS queueing systems and by applying our procedure to measurements of 
a mainframe computer system at a large telephone company. These evaluations suggest that our procedure 
is effective in practice, especially for larger sample sizes and smaller utilizations. We further conclude 
that indirectly detecting changes in performance factors appears to be inherently dii%cult in that the 
sensitivity of a detection procedure depends on the magnitude of the change in performance char­acteristics, 
which often has a nonlinear relationship with the change in performance factors. Thus, a change in performance 
factors (e.g., increased service times) may be more readily detected in some situ­ations (e.g., very 
low or very high utilizations) than in others (e.g., moderate utilizations), A key insight here is that 
the sensitivity of the detection procedure can be improved by choosing appropriate measures of performance 
characteristics. For example, our experi­ence and analysis suggest that queue lengths can be more sensitive 
than response times to changes in arrival rates. 1. Introduction Due to the stochastic service times 
in computer formance are common. variations are due to nature of inter-arrival and systems, variations 
in per-Sometimes, however, these changes in the system s performance-deterrninin g factors Permission 
to copy without fee all or part of this material is granted provided that the copies ara not made or 
distributed for direct commercial advantage, the ACM ocopyright notice and the title of the publication 
and its date appear, and notice is given that copying is by permission of the Association for Computing 
Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. ~ 1991 ACM 
089791 -392 -2191 /000510039 . ..S1 .50 (hereafter, just performance factors), such as cotilg­ uration, 
workload composition, scheduling algorithms, or tuning parameters. Detecting changes in perform­ ance 
factors is an essential part of distinguishing true performance problems from normal variability. In 
addition, knowing when performance factors change may provide enough information to identify the underlying 
cause of a performance problem. For example, we studied one mainframe computer system in which memory 
usage increased starting at 9:00 and continued increasing until 10:30, at which time memory usage returned 
to normal. By comparing user activity with the time period during which this performance problem occurred, 
we were able to iden­ tfi the underlying cause -­requested additional memory This paper describes detecting 
abrupt changes in the performance of computer approach to data taken a progam that repeatedly without 
releasing it. a statistical approach to the factors that determine systems, and applies this from a 
computer system running the IBM Machine/System Product Option (VM/SP HPO). factors can be detected are 
collected (on-line) operating system Virtual with the High Performance Changes in performance either 
sequentially as the data or in toto after all measure­ ments are available (off-line). Our interest is 
mostly with the latter since an off-line approach permits doing a more thorough analysis, and off-line 
tuning provides a wider range of corrective actions. The times at which performance factors change are 
called change points. One approach to change point detection is to look for changes in the measured values 
of performance factors. Unfortunately, many performance factors are unknown, except to the sys­tem s 
designers. For example, although operating system schedulers are often modeled as simple disci­plines 
(e.g. round robin, head-of-line priority sched­uling), the actual implementations are very complex. How 
then can we measure deviations from a sched­uling discipline for which we have little under­standing? 
Furthermore, even if we know a performance factor, we may not have measurements of it. For example, commonly 
used measurement reporting facilities for VM/SP HPO do not provide time serial information for input/output 
service times, and they provide very limited time serial information for input/output rates. Because 
of the foregoing, we cannot in general detect changes in performance factors by direct meas­urement. 
Instead, an indirect approach is used: We measure the s~stem s performance characteristics (e.g., response 
times, queue lengths), and attempt to detect 39 changes in performance factors from changes in per­formance 
characteristics. 1 ,,I  II I 1., ,,,, t8I, 200 !lOrl Boo i%IiNAvOtd Figure 1. Series of Response Times 
in a VM/SP HPO System For example, in Figure 1 we see an abrupt increase in response time around observations 
300 through 450 and another such change begins near observation 750. h turns out that these are change 
points resulting fi-om data dependent software errors that increased CPU utilizations. 0 200 Do 800 OBSERVATION 
 Figure 2. Series of Response Times in a Stationary M/M/l, FCFS Queueing System Fignre 2 plots response 
time for another system. As before, there are several sequences of above-average response times, which 
suggests that multiple change points are present. Here, however, our visual analysis is wrong: These 
data come from a stationary M/M/1, FCFS queueing system. The foregoing indicates the difllculties with 
analyzing a single performance charac­teristic. In practice, the analysis is even more demanding since 
multiple computer systems must be analyzed, and for each, there may be a large number of performance 
characteristics to examine. Clearly, an automated approach is desirable. One commonly used approach to 
problem detection is based on thresholds: If an observation exceeds a threshold value, an alert is generated 
(e.g., [1], [12]). For computer systems, choosing mean­ingful thresholds is often dif33cult since this 
choice depends on hardware, software, workload, and tuning parameters, all of which change frequently 
in large installations. (Maxion [11] proposes heuristic tech­niques to circumvent these difllculties, 
but provides no formal basis for his approach.) Another approach to detecting changes in performance 
factors is to employ techniques from manufacturing quality control, such as Shewhart and CUSUM tests 
(e.g., [5], [10], [13]). However, these are on-line proce­dures and so do not usually take into account 
the quantities of data available when off-line evaluations are done. Also, these procedures assume that 
obser­vations are independent and identically distributed (iid), usually from a normal distribution. 
Such assumptions are too restrictive for measurements of computer systems, which are rarely normally 
distrib­uted and are often strongly correlated. One final approach to change point detection is based 
on cluster analysis, which provides an off-line approach to detecting multiple changes in performance 
factors. A commonly used clustering algorithm is proposed by Fisher [8], which uses a least squares approach 
to form optimal partitions. A variety of tests have been proposed to determine the statistical si@lcance 
of partitions formed in this manner (e.g., [4], [6], and [7]). However, all assume that observations 
are iid normals. Also, to the best of our knowledge, existing tests assume a form of the Fisher Algorithm 
(the unrestricted case) that is inappropriate for detecting change points in time serial observations. 
In this paper, we present a relatively simple and fairly robust procedure for detecting abrupt changes 
in performance factors. This procedure alternates between identifying candidate change points and testing 
their statistical signMcance, Our approach to identifying candidate change points is described in section 
2; we use the Fisher Clustering Algorithm since it is a natural approach for detecting multiple change 
points (in that it maximizes within-partition homogeneity) and is widely used. Section 3 describes how 
we test the statistical sign&#38;cance of change points; our approach considers the effects of autocorrelations 
and appears to work well even for data that are not normally distributed. Section 4 pro­vides an evaluation 
of our approach, including an in­depth analysis for M/M/ 1, FCFS queueing systems. Our conclusions are 
contained in section 5. 2. Detection Procedure Our approach to ident@ing candidate change points is 
based on the Fisher Clustering Algorithm. This algorithm partitions a time series of N observations so 
a to minimize the total of each partition s adjusted sum of squares, which is the sum of squared differ­ences 
between observations and the mean value for their partition. (Fisher [8] discusses an implementa­tim 
of this algorithm.) More precisely, let X, be the value of the i-th observation. We denote a parti­tioning 
of a time series by either P or Q. A k-parti­tioning is a k-tuple of indexes that specifj the beginning 
of each partition. So if P = (Pl, ... . P~), P, is the index of the fwst observation in the i-th pati­tion. 
Since PI = 1, the change points corresponding to this partitioning are Pz, ... . Pk. Let [nz..n] denote 
the subseries consisting of the rn-th through n-th observations, and define to be the average value for 
this subseries. The adjusted sum of squares for the partition consisting of the subseries [m..rz] is 
A~Q[rn..n] = y (xi X[n z..n])? i=m We identfi change points by selecting partitions to minimize the 
sum of ~sQ across all partitions. Thus, for the k-partitioning P of a series that has length N, we &#38;fme 
k 1 ~p = ~ ~SQ[P~..Pi + ~ 1]+ ~SQ[Pk..Nl. [=1 For a given k, we want to fmd an optimal P in the sense 
that there is no k-partitioning P such that Dp < Dp. For example, suppose we have the series 95 105 510 
490 Consider the 2-partitioning Q = (1, 3). Then DQ = ~~Q[l..2] + Z4~Q[3..4] = (95 100)2 + (105 -100)2+ 
(510 -500)2 + (490 -500)2 = 250 Furtherrno~, Q is an optimal 2-partitioning, with k I = 1 change points} 
and this change point is Q,= 3. Our detection procedure is hierarchical, as in [61. In part, this is 
motivated by computational con­siderations, since finding optimal k-partitions becomes intractable for 
large values of N and even moderate values of k. Another reason for using a hierarchical FindChangePoints: 
first..last]; (jkrt < last) 1. Compute the optimal 2-partitioning: Q = (1, Qz) As~~rst,.kzst] 2. Compute 
T = As~~r$t..Qz 1]+ A~QIQz..lust]  3. If T exceeds the critical value,  add Q2 to the list of change 
points;  FindChangePoints ~r.rt..Qz 1];  FindChangePoints [Qz..la.rt];   A~Q = adjusted sum of squares 
Figure 3. Procedure for Finding Change Points approach is to simplify our test for the statistical sig­nificance 
of a partitioning. When k =2, a partitioning is si@lcant if the distribution of observations within one 
partition diflers from that within the other parti­tion. When k >2, the situation is more complex since 
we need to identify which (if any) of the k parti­tions has a distribution that difiers from the other 
par­titions. While statistical tests like analysis of variance do address this kind of problem, these 
tests assume that the partitions have a common variance. Such an assumption is inappropriate for detecting 
change points in queueing systems since commonly measured performance characteristics (e.g., response 
time and queue length) have a variance that is a function of performance factors (e.g., arrival rates 
and service times). Our detection procedure, FindChangePoints, identfles a candidate change point by 
computing the optimal 2-partitioning of [1. .N1 . We denote this par­titioning by Q = (1, Qz), where 
Qz is the candidate change point. To evaluate the statistical signMcance of Qz, we use information readily 
available from the clustering algorithm. Speciilcally, we consider the test statistic Dp T= (1) Q where 
P is the 1-partitioning of [1. .N1. Qz is signif­icant if T is larger than a critical value. (Section 
3 dis­cusses how to obtain critical values for T.) If Q2 is significant, FindChangePoints is then applied 
to the partitions [1. .Qz . 1] and [QZ..NJ in the same manner as [1..Nl . As summarized in Figure 3, 
our proce­dure continues recursively until there is no signitlcant change point within the current partitions, 
We now illustrate our procedure by applying it to the data in Figure 1. Since we have not yet pre­sented 
the critical values for T, we use zero for the critical values and only show the fwst four levels of 
partitions. (In section 3, we return to this example and determine which candidate change points are 
sta­tistically sigrMcant.) The data in Figure 1 consist of 958 observations, procedure, FindChangePoints, 
is calle$with ~1% series [1,,958]. Applying the frost step of this procedure, we compute u =25 F u 2.0 
 91.5 gl.o ~ 0.5 K. 200 400 S(M *I ~o LEVEL I II Figure 4. Candidate Identification Algorithm Applied 
to Response Times in a VM/SP HPO System the optimal 2-partitioning: Q = (1, 736); hence, the probability 
of false positives that is considered accept­fist candidate change point is observation 736. Next, able 
(e.g., .05, .01). We seek a critical value, Tc, such we compute ~ DP = As~[l..958] = 140.22, and that 
P(T > Tc \ no change point) < a. 11~ = As.~[1..735] + As~[736..958] = 92.32 + 24.15; To fmd critical 
values for T, we need to know hence, T = DP/D~ = 1.204. Since our critical value for its distribution. 
Readers familiar with analysis of vari-T is O, we add 736 to our list of change points and ance (ANOVA) 
may notice that T 1 is the ratio of then apply FkdChangePoints to [1..735] and the between-sum-of-squares 
to the within-sum-of­[736..958]. Figure 4 shows the results of successive squares; so the product, (N 
 2)(T 1) has an ~l,~.z applications of FhdChangePoints. The top part of distribution, If the A NO VA 
assumptions hold. this figure is the original time series; below this plot Figure 5 compares the F distribution 
for are vertical lines that indicate the candidate change (N 2)(T 1) with the empirical distribution 
of T points ident~led. The figure also displays T values, obtained by simulating M/M/1, FCFS queueingand 
the recursion level at which candidate change systems. (These simulations use 100 replications and points 
are identfled. an initial transient of 60,000 customers.) The left plot is for N = 100, and the right 
is for N = 1000; the solid line is the F distribution, and the dotted lines are the 3. Statistical Significance 
of a Change queueing simulations for the values of utilization (p)  Point indicated. These plots clearly 
indicate that the ~1,~ _ ~ This section presents a test for the statistical signif-distribution has a 
poor fit with the simulation results, icance of candidate change points, as required in step Even for 
low values of utilization, the empirical dis­3 of FhdChangePoints. This test is based on the sta-tributions 
from the queueing simulations lie far to the tistic T in 13q. (1). (Similar statistics are used in [7] 
right of the F distribution, indicating that the actual and [4].) The purpose of this section is to fmd 
crit -level of false positives would greatly exceed a. For ical values for T. example, when N = 1000 
and p = .1, the critical value obtained from the F distribution for a = .05 isThe choice of critical 
values depends on what approximately 1.0025, which corresponds to a proba­we consider to be an acceptable 
level of false positives. bility of false positives in the queueing system ofIn our context, a false 
positive occurs if a candidate approximately .95! change point is accepted as si~lcant when there is 
no change in performance factors. Translating this Why does the F distribution provide such a into the 
language of statistical inference, our null poor fit? The answer lies in the following assump­hypothesis, 
HO, is that there is no change point (i.e., tions that underlie this distribution: observations are identically 
distributed). Let a be the N=1OO F(I ,98) :..,.,................ ~[~ ,.......-+ p=.9 ................... 
............. .,,,,,... -1.0 1.1 1.2 T 1.3 1.4  Figure 5. Comparison of F dis~ibution with Empirical 
T Distributions for M/M/l, FCFS Partitions are chosen arbitrarily. observations are normally distributed. 
 Observations are independent.  The first assumption clearly does not hold since the Fisher Clustering 
Algorithm chooses partitions that maximize the statistic T. The second and third assumptions are also 
questionable. For example, in an M/M/1, FCFS queueing system, response times are exponentially distributed 
and positively correlated (especially at larger utilizations). We could handle non-normal, autocorrelated 
data in the same manner as is done with the a.mi.lysis of simulation output and the CUSUM and Shewhart 
Tests: by batching. Spe­tilca.lly, for N observations and a batch size of B, we could consider the NIB 
batch averages. Such an approach has three shortcomings for detecting changes in performance factors. 
First, we may lack suflcient measurements to obtain a large enough batch size to eliminate the autocomelations. 
Second, averaging may so diminish the factors that we make the time coarse, thereby for diagnosing For 
example, if intervals and a points would be minutes instead the signii3cance Algorithm. We If we are 
lation, how parameterized? reasons for the from normatity effect of a change in performance fail to detect 
it. Third, batching can granularity of our detection quite diminishing the value of this analysis intermittent 
performance problems. we have one minute measurement batch size of twenty, then change detected with 
a granularity of twenty of one minute. The foregoing suggests to finding critical values for Engelman 
and Hartigan [7], normals, use simulation to of partitions use a similar that an analytic approach T 
is dtilcult. Indeed, who only consider iid obtain critical values for obtained by the Fisher approach. 
to obtain critical values from simu­ should these simulations be Our experience is that of the three 
F distribution s poor fit, deviations is of the least concern. So to obtain Tc from simulation, we hope 
to use the normal dis­ 43 N=1OOO o...... F(1,995) ,......----  -r /­ ... q o p=.1;.: ,. ,.. ..: 
 L[ . q ,. 0 ~ ; ~ ,... * ../ 0 1. ..: ~ /,/ p=.5 . ,.. ....................... ......... ...... 
............-. . . ...  ..... ,.....: : ~: ............- P=.B . .......- ,.. ,., .... I ......... 
 0 ,.. 1I 1tI+11 1.(XI5 1.010 1.015 1.020 1.025 T tribution to generate data that has similar to that 
of a large class of Our approach is based on a time [15]) that relates the performance a dynamic behavior 
queueing systems. series analysis (e.g., characteristics of the i + l-st departure from a queueing system 
to the performance characteristics of the i-th departure. Let Xi, ~ be a measure of performance characteristics 
as seen by the i+ 1-st departure. For queue lengths, we expect Xi+ 1 to be very close to Xi. If Xi+ 1 
is a response time, the situation is, in general, more complex. For example, the i + I-st departure from 
a queueing system with round-robin service has a waiting time that depends on its service time, which 
may be quite different from the i-th departure. However, for FCFS queueing systems, a customer s wait 
is independent of its service requirements; so the i + l-st response time should be close to i-th (espe­ 
cially at higher utilizations), A time series model provides an algebraic relationship between successive 
observations. As noted in [14], the dynamic behavior of response times in FCFS queueing systems and queue 
lengths in general is fairly well expressed by the one parameter autoregressive time series model, denoted 
by AR(1): xi+l ~=~(~t ~)+ai+lj (2) where C) is the lag one autocorrelation (with I ~ I < 1) and the 
ai are distributed as iid normals with mean O and variance a?. This model assumes that the X are covariance 
stationary; that is, they have the same mean (p) and E{(X, ~)(Xi + ~ p)} is the same for all t hence, 
observations have the same variance. We denote this common variance by Oz, where 02 = oa2/( 1 4)2. Thus, 
we revise our null hypothesis to HO: Observations in both partitions are from the same AR(1) time series 
We note in passing that there are existing tech­niques for testing if observations are consistent with 
a specific time series model (e.g., the Q statistic [15]). E{A~Q[l..N_J= NP2 NE{~2] However, these techniques 
assume a covariance sta­tionary series, and so it is questionable how effective they would be for evaluating 
change points. Alterna­tively, since HO assumes that both partitions contain = AR(1) series, we could 
test for their having the same AR(1) parameter. However, we doubt that existing statistics are applicable 
for one of the same reasons that the F statistic is inadequate for our situ­ation. Partitions are not 
chosen arbitrarily. Equation (2) suggests that a maximum of four parameters need be considered in our 
simulation of T N, K, ~, and O.2. However, a four dimensional table of critical values is extremely cumbersome. 
To simplify matters further, we use an analytical model to deter­mine the key parameters, and only vary 
these parame­ters in the table of critical values. We identify the key parameters by deriving an approximation 
for the expected value of T when HO holds, which we denote by ?.. Let Q = (1, Q z) be an arbitrary 2-partitioning. 
We defiie We are not interested in how accurately ~o predicts T, instead, we only want to know which 
parameters appear in the resulting expression for ~ as well as the directional effects of these parameters. 
We begin by obtaining an approximation for the expected value of the square of the sample mean of an 
AR( 11 series. For notational simplicity, let X = X[ l..~, where N is the number of observations. N 1 
.+ p2+T (N k)E(&#38;Yj + J (3) k=l ) (z N 1 1+4 2$ #k , P2+N(1 +) 22( x) k=O where pz is the second 
moment of an observation. We simplify Eq. (3) by considering situations in which N is huge and/or ~ is 
moderate, in which case the summation in Eq. (3) is much smaller than N since 1+1< 1.s0 2 1++ E{7 }=p2 
+ ~ ~ , (4) which is a special case of Lemma 3 in Chapter 20 of [21. The expected value of an adjusted 
sum of squares is (N-W ) (A different form of this result appears in [9].) From Eq. (5) we obtain and 
E{DQ,] = E{A~Q[l..Q 2 11+&#38;Q[~ 2uNl} 2(N 2W so, ,0=  2(N-W ( 1++ 02 (6) 21 4 ) N(l 4) (l+@) N(l 
~) 2(1+ +) Where: N 2 +< N-+2 Thus, ~o only depends on N and ~. Further, Eq. (6) indicates that +0 is 
increasing in ~ and decreasing in N; we expect T to behave in a similar manner. Since fo is unaffected 
by p and Oaz, we consider an AR(1) series in which p = O and o.Z = 1. We obtain a distribution for T 
for specfied values of N and ~ by using simulation to produce N successive observations in an AR(1) series 
with parameter ~, and then compute T for this series. Repeating this process many times permits us to 
construct an empirical dis­tribution for T, which we refer to as FP~: F for opti­mally partitioned AR(1). 
To assess the adequacy of critical values obtained in this manner, we consider the empirical distribution 
of T obtained from time series consisting of M/M/1, FCFS response times. In each case, we identify the 
appropriate FPA, which depends on N and C). We consider N = 100 and N = 1000, and obtain ~ by using an 
approximation given in [3]: +=1 (1 ,0)2. (7) Figure 6 plots our results for four cases. For the most 
part, F~A (solid lines) is close to the T values obtained from the queueing simulations (dotted lines), 
and certainly the fit is much better than that obtained  {K!r 1.1 1.2 1s3 t 1.2 1.4 1.0 l.e 20  E(K 
1.0051.0101.0151.020 1.025 1.0?. 1.04 1.00 Figure 6. Comparison of FPA distribution with Empirical T 
Distributions for M/M/l, FCFS usihg the F distribution. Further, these plots are con­sistent with Eq. 
(6) in that the probability mass shifts to the right (i.e., T increases) as @ increases, and it shifts 
to the left as N increases. We obtained similar results for M/D/ 1, FCFS. Figure 7 contains the (mean) 
u = .05 critical values for T (the soIid dots) obtained from FPA simu­lations; these values have a maximum 
coefficient of variation of .03. We consider values of ~ ranging from .05 to .99, in increments of .05. 
The solid lines m this figure are a function that has been fitted to the simulation results. The fit 
is quite good --R% .98; so, the fitted function, which is given in Eq. (8), should suffice to obtain 
critical values for interpolations over the range of simulation results plotted. 573 30745 In(Tc(N, 
&#38; 1)= 5.2942+ ~ ~ (8) ,, + 5.8427$ 12.372$2+ 11.102$3, where ~ is the sample autocorrelation 
of the series (see [15]), 100s N< 1000, and .05< ~ s .99. (If ~ <.05, we use $ = .05; if ~ >.99, we use 
~ = .99.) Let Y be ln(Tc(N, ~) 1). T is si@lcant if it exceeds T=(N, ~)= ey + 1. We now apply our test 
to the data in Figure 1 and Figure 2. For Figure 1 we use the candidate change points ident~led in Figure 
4. The fu~t candi­date change point occurs at observation 736; N = 958, .$= .52, and T= 1.204> TC(958,.52) 
= 1.030. So 736 is a signiilcant change point. We now consider the two subseries [1. .735] and [736. 
.958]. For the latter, $ = .36 and T= 1.095. Since T does not exceed T~223,.36), this partition is not 
included in the list of signMcant change points, and we do not consider any of the change points identfled 
in Figure 4 that are contained in [736..958] (i.e., 743, 762, 769, 798, 939, 949, and 956), For [1..735], 
$ = .44 and .. ~\ k . . -.09  ~+ . II, ,7L1 , = .05 200 400 600 BOO 1Ood N Figure 7. Critical Values 
for Test at a = .05 Level, with Fitted Curves T= 1.053> Tc(735,.44) = 1.032; so 181 is a si~lcant change 
point, and the partitions [1.. 180] and [181 ..735] must be analyzed. It turns out that 52 is not a significant 
change point (~ = .14); so we ignore candidate change points in [1.,180]. However, 432 (~= .48), 334 
(~= .47), and 580 (~= .23) are signif­ icant change points. Since no further invocations of FindChangePoints 
identfles a signii3cant change point, the procedure terminates and the set of signiilca.nt change points 
is {736, 181, 432, 334, 580}. The par­titions formed by this set are displayed in Figure 8. A For the 
data in Figure 2, ~ = .87, N = 958, and T = 1,077. From Eq. (8), we determine that Tc(958, .87)= 1.18. 
Since T< Tc, no si~lcant change point is detected. This is consistent with our expectations since the 
data come from a stationary queueing system. 4. Evaluation of Approach This section evaluates the effectiveness 
of our proce­dure for detecting changes in performance factors. Section 4.1 evaluates our significance 
test by an in­depth study of M/M/1, FCFS queueing systems, and section 4.2 conducts a similar analysis 
of our algo­rithm for identifying candidate change points, Section 4.3 evaluates our detection procedure 
in total by ana­lyzing data from a VM/SP HPO system. In section 4.4, we discuss some considerations that 
may make the problem we address inherently difi3cult. 4.1 Evaluation of Significance Test How effective 
is our test for the statistical sigdcance of change points? That is, given that a change point exists 
and is correctly identfled by the clustering algo­rithm, what is the probability that our test rejects 
the null hypothesis? This is referred to as the power of the test. With a powerful test, small changes 
in per­formance factors cause a large drop in P(Accept Ho), t 1,11 1 1 =U p~ 1 ml Emil1 U3d # , u 200 
Soo Boo &#38;B~EIWATION Figure 8. Significant Change Points Detected in the Response Times of a VM/SP 
HPO System with stall larger changes causing this probability to fall to zero. To fully evaluate our 
test, a wide range of queueing systems should be investigated. Our analysis here is much more modest. 
We analyze in-depth a simple queueing system: M/M/1, FCFS. For this system, the performance factors consist 
of 2, the mean arrival rate, and ~, the mean service time. In the sequeI, we normalize ~ by 2 (i.e., 
2 = 1), so the magni­tude of $ can be interpreted as utilization (p). A change in performance factors 
consists of a change in utilization, denoted by Ap. We simulate M/M/1, FCFS queueing systems in which 
there is at most one change in utilization; this change affects the final fN arrivals at the queueing 
system, where Os f <1. We refer to the initial ( 1 flN observations as the base period and the final 
fiV observations as the shifted peliod-Throughout, our measure of performance characteristics is response 
time. There are four parameters in our analysis: N, p, j and Ap, We use simulation and critical values 
obtained from Eq. (8) to compute 900/0 cotildence intervals for P(Accept HO I N, p, f, Ap). Due to space 
constraints, we only present results for a limited range of the parameters studied. These results are 
contained in Figure 9, and consist of four plots: two levels for N and p, with f = .5. In each plot, 
the vertical axis is P(Accept HO I N, p ,f Ap), and the horizontal axis is Ap. We note that when Ap = 
O, the cotildence inter­vals contain ahnost always .95. Since the critical values in Eq. (8) are for 
a = .05, this is consistent with P(Accept Ho I no change point) = 1 a = .95. Also note that, as expected, 
the probability of accepting HO decreases as Ap increases. How do N and p affect the probability of a 
false positive (i.e., accepting HO when a change in perform­ance factors occurs)? The results in Figure 
9 (as well as those in our other studies) indicate that for Ap >0 the probability of accepting Ho increases 
with p and decreases with N. Further study is required to deter­mine the reason for these influences, 
but our intuition oIo.I W M 0,3 00I 0,14 0.2 0.3 Figure 9. Evaluation of Significance Test for M/M/l, 
FCFS: f= .5, 90% Confidence Intervals is that much of it relates to auto correlations. From Eq. (7), 
we know that ~ increases with utilization. So, if the probability of accepting HO increases with auto 
correlations, then this probability y increases with p, which is what occurs. As for N, our intuition 
is that autocorrelations decrease as N increases. Hence, the probability of accepting HO should decrease 
as N increases, which is what happens. If the power of our test degrades as auto correlations increase, 
then we expect our test to work better in practice than the M/M/1 simulations suggest. The reason is 
that measurements of com­puter systems are typically aggregated before they are reported. For example, 
observations in the VM/SP HPO system from which the Figure 1 data are taken consist of samples that are 
aggregated over a 30 second interval and this system has roughly 2,000 CPU transactions a minute. The 
effect of these aggre­gations on autocorrelations is dramatic. The average CPU utilization in this VM/SP 
HPO system is approximately .9; however, for stationary periods the autocorrelations of response time 
are typically under .4, whereas an autocon-elation of .99 would be expected if Eq. (7) applied. Inverting 
this equation, we note that an autocorrelation of .4 corresponds to a utilization of approximately .2 
in an M/M/1, FCFS queueing system (with unaggregated response times). From Figure 9, we see that our 
test works well when o = .2, especially for large N. 4.2 Evaluation of Candidate Identification Given 
that a change point exists, how close to the true change point is the one selected by our candidate ident~lcation 
algorithm? We refer to this property -­having the candidate change point be very close to the true change 
point --as accuracy. This section evalu­ ates the accuracy of our algorithm for identifying can­didate 
change points. We begin by studying response times in M/M/1, FCFS queueing systems, as in section 4.1. 
Accuracy is assessed by measuring J, the fraction of observa­ found by our candidate identification 
algorithm for N=IOO {E(E o ON:, ooti4 f 0.s O.a 1.0 0 !,E$OE . . . . . 0.2 0.4f 0.6 O.a 1.0 Figure 
10. Cumulative Distribution of Fraction of Shified Intervals for M/M/l, FCFS, f = .5 tions that the algorithm 
detects as being shifted. We want fmfl where as before f is the fraction of shifted .>, . observations 
(a simulation parameter). Figure 10 con­tains several plots of the cumulative distribution of ~ for f= 
.5, N= 100, 1000, p = .2, .4, and Ap = ,1, .3. Note that for most of the cases the probability mass tends 
to be close to the true f value; this indicates that the algorithm works well for many of the simulation 
runs. Furthermore, accuracy improves with Ap. However, as with our significance test, our candidate identification 
algorithm works best when N is large and p is small. Looking at Figure 10 in more detail, we note that 
the probability mass to the left of the true ~ cor­responds to late or delayed detections, and the mass 
to the right of the true f reflects early detections. We attribute late detections primarily to the time 
required for the change in performance factors to be reflected in performance characteristics, due to 
queueing delays and performance transients. We attribute early detections to the effect of autoconelations. 
which can cause runs of above-average values (e.g., see Figure 2). From Eq. (7), we know that autocorrelations 
(and hence runs) increase with p. So with larger p our candidate ident~lcation algorithm may mistakenly 
perceive these runs as change points, especially if Ap is small. We conclude that, like our si~lcance 
test, our candidate identflcation ahzorithm works best when N is large and p is small. fiso, we speculate 
that a key reason for accuracy decreasing as p increases is that autocorrelations increase with p. 4.3 
Change Points in a VM/SP HPO System -Although detailed, the analysis in sections 4.1 and 4.2 is quite 
simplistic in that it only considers M/M/1 queueing systems. For a more realistic evaluation, we must 
analyze production computer systems. Figure 8 shows the signii3cant change points data taken from a 
VM/SP HPO system. Recall from section 1 that we expect change points near observa­tions 300, 450, and 
750. Five si~lcant change points are found: 181, 334, 432, 580, and 736. The change points 334, 432 and 
736 correspond to our expectations for change points near 350, 450, and 750. But why are there significant 
change points at 181 and 580? The change point at 181 may, in part, be explained by the presence of another 
non-stationarity in our data: cyclic behavior (every twenty observa­ tions) due to a high overhead application 
that polls for work. As a result of this cyclic behavior, vari­ ability increases dramatically, especially 
in the early part of the series. Since the adjusted sum of squares is very similar to commonly used 
estimates of popu­lation variance and the candidate identflcation algo­rithm seeks to minimize the adjusted 
sums of squares, this variability may cause the algorithm to pick obser­vation 181 where variability 
appears to increase abruptly, Doing so may also result in a very large T value, thus giving a significant 
result. At present, we have no explanation for the change point at 580; however, it may reflect a problem 
that was not identi­fied by the personnel at the VM/SP HPO installation. 4.4 Difficulty of the Problem 
The limitations of our procedure for detecting changes in the performance factors of computer systems 
are not due entirely to the procedure itseti, in some ways, the problem addressed appears to be inherently 
di.ff3cult. One such inherent d~lculty is that we attempt to detect changes in performance factors indirect~. 
That is, since we often lack meas­urements of the factors themselves, we detect changes in performance 
factors from changes in performance characteristics. Thus, the sensitivity of a detection procedure (e.g., 
the accuracy of our candidate identfl­cation algorithm and the power of our statistical test) depends 
on the change in performance characteristics, and the variability of the measure chosen, Unfortu­nately, 
many performance factors, such as arrival rates and service times, have a nonlinear relationship with 
commonly used measures of performance character­istics, such as queue lengths and response times. Thus, 
a change in performance factors may be more readily detected @ some situations than in others. To illustrate 
the foregoing, we use a simple ana­lytic model. As in section 4.1, we consider an M/M/ 1, FCFS queueing 
system that experiences a single change in performance factors, and our measure of performance characteristics 
is response time. Let A2 denote the change in mean arrival rate, and As denote the change in mean service 
time. So, p + Ap = (2 + AA)(s + AJ). Let R denote the response time before a shift has occurred, and 
R denote the response time after a shift has occurred. For Al, As> O, we expect the power of our procedure 
to increase as the R /R increases. For a steady-state, FCFS, M/M/1 queueing system, R S+AS l p = (9) 
R s l p Ap How does R /R vary with p? We use a numer­ical example to answer this question. Let J = 1, 
and let s take on small, moderate, and large values: .05, .5, .85. So, p = .05, .5, .85. We consider 
a single change in performance factors: As= .1, A,I=O; SO Ap=,l. Doing the necessary arithmetic, we learn 
that for p = .5, R /R = 1.5; however, for p = .05 and .85, R/R = 3.35. Thus, if measurements are taken 
so that the variance of the test statistic remains constant, we expect changes in performance factors 
to be more readily detected at very low or very high utilizations than at moderate utilizations. Some 
of the difficulties of indirect detection can be overcome by considering appropriate performance characteristics. 
Since end-users perceive performance problems by changes in response time, our initial inclination was 
to focus on response time as well. However, our experience with analyzing VM Systems suggests that queue 
lengths are often more sensitbe than response times to changes in performance factors. To provide some 
analytic just~lcation for this seemingly counter-intuitive result, we consider a general queueing system 
in steady state, and compare the power of a statistical test based on queue lengths to one based on response 
times. (Throughout, we assume that AA, As > O.) Let P ~ be the statistic for queue length, and VR be 
the response time statistic. We assume that V~ and V~ are unbiased; that is, if the mean queue length 
is L and the mean response time is R, then E(VL.)= L and E(VR)= R. Let L , R be the mean values for queue 
length and response time after a shifthas occurred and the queucing system has reached steady state; 
let VL,, V~, denote the corre­sponding test statistics. We expect that the test based orI queue length 
will be more powerful than the one based on response time if the following two condi­tions hold: (lo) 
(11) It turns out that under our assumptions Eq. (10) aIways hoMs. To see this, recall that the test 
statistics are unbiased; so Eq. ( 10) is equivalent to L R Let A = 1 + AA. Since we only consider steady-state 
measurements, Little s result applies and so ~,R, >&#38;. ,lR -R this holds whenever AA >0. Unfortunately, 
the condi­tiorI in Eq. (11) is not so easily addressed analytically; indeed, we know from numerical studies 
that this con­dition does not hold in general. However, the fact that Eq. ( 10) holds under very general 
conditions is consistent with our experience that changes in per­formance factors are often more readily 
detected by tests based on queue length as opposed to tests based on response times. We speculate that 
another inherent difficulty with detecting changes in performance factors is the presence of autocorrelations. 
Although further study is required, our intuition suggests that larger autocorrelations decrease the 
power of our statistical test and diminish the accuracy of our candidate iden­tii3cation algorithm. Thus 
far, our strategy has been to consider approaches that are robust to auto correlations. Another strategy 
is to transform the measurements in a manner that eliminates autocomelations. If the data are truly AR(1), 
we could remove the effects of autocorrelations by ana­lyzing the residuals (i.e., the q). However, 
the AR(1) model is only an approximation, and so sigdcant errors may be introduced by attempting to remove 
autocomelations if this approximation is inaccurate. Another ditZculty with using the AR(1) model to 
remove autocorrelations is that the presence of change points typically implies a change in covariance 
struc­ture, since auto correlations increase with utilization. Thus, we must identfi the change points 
before we remove the autocorrelations, and hence the candidate ident~lcation algorithm must still operate 
on autocorrelated data. A final issue with an AR(1) approach to removing autocorrelations is that it 
requires estimating ~, and existing estimators have a large variance. Currently, we are investigating 
approaches to removing autocorrelations that do not require knowing the covariance structure of the data, 
5. Conclusions Pinpointing when performance factors change (e.g., when a CPU-intensive workload is added) 
can be of great assistance when analyzing intermittent perform­ance problems. Since we often lack measurements 
of performance factors, this paper presents a procedure for indirectly detecting abrupt changes in performance 
factors by analyzing performance characteristics (e.g., response times, queue lengths). Our procedure 
alter­nates between identifying candidate change points (the times at which performance factors change) 
and testing their statistical si~lca.nce. Our algorithm for identifying candidate change points is based 
on a widely used clustering algorithm; our statistical test uses information easily obtained from this 
algorithm. Since time serial observations of queueing systems often have large autocon-elations, our 
statistical test models the dynamic behavior of performance charac­teristics as an AR(1) time series, 
which is appropriate for queue lengths in general and response times if service is FCFS. Using the AR(1) 
model and an ana­lytic approximation, we parametrize simulations from which critical values are obtained 
for our statistical test. The effectiveness of our procedure is evaluated by using simulations of M/M/1, 
FCFS queueing systems and by applying our procedure to measure­ ments of a VM/SP HPO system running 
a workload from a large telephone company. These evaluations m~st that our procedure works well in practice, 
especially for larger sample sizes and smaller utiliza­ tions. Although reasonably effective, our procedure 
could be improved in several ways. For example, our algorithm for identifying candidate change points 
could be more robust to the presence of multiple kinds of changes in performance factors, such as cyclic 
behavior. Also, it might be desirable to simuhane­ously consider multiple performance characteristics 
(e.g., queue length and response time), which would require using a multi-dimensional clustering algorithm. 
In addition, our statistical test should be extended to consider a larger class of queueing systems (e.g., 
by considering other time series models). However, even with these improvements, there are some fundamental 
limits to how well we can do since indirectly detecting changes in performance factors appears to be 
inher­ently d.iflicult . Specflcally, the sensitivity of a detection procedure depends on the magnitude 
of the change in performance characteristics, which often has a nonlinear relationship with the change 
in perform­ance factors. Thus, changes in performance factors (e.g., increased service times) may be 
more readily detected in some situations (e.g., very low or very high utilizations) than in others (e.g., 
moderate utiliza­tions). A key insight here is that the sensitivity of the detection procedure can be 
improved by choosing appropriate measures of performance characteristics. For example, our experience 
and analysis suggest that queue lengths can be more sensitive than response times to changes in arrival 
rates. Acknowledgements We wish to thank Philip Heidelberger, Marty Schatzoff, Peter Welch, William W. 
White, Emmanuel Yashchin and the conference referees for their helpful comments.  References <RefA>1. l+. 
Pat Artis. Using Expert Systems for Ana­lyzing RMF Data. Proceedings of the Com­puter Measurement Group, 
653-657, 1985. 2. Patrick Billingsley. Convergence of Proba­bility Measures. John Wiley &#38; Sons, 
1971. 3. D. Craven. Serial Dependence of a Markov Process. Journal of the Australian A4athemat­ical 
Society, 5:299-314, 1966. 4. Richard O. Duda and Peter E. Hart. Pattern Classl~cation and Scene Ana/ysis, 
John Wiley &#38; Sons, 1973.  5. Martin Dybeck. Taking Process Automation One Step Further: SPC. Proceedings 
of the Annual Controi Engineering Conference, 6:643-651, May 1987. 6. A. W. F. Edwards and L. L. Cavalli-Sforza. 
A Method for Cluster Analysis, Biometrics, 21:362-375, 1965. 7. L. Engehnan and J. A. Hartigan. Percentage 
Points of a Test for Clusters. .Journal of the American Statistical Association, 64:1647-1648, 1969. 
 8. W. D. Fisher. On Grouping for Maximum Homogeneity. Journal of the American Sta­tistical Association, 
53:789-798, December 1958. 9. David Goldsman, Keebom Kang, and Robert Sargent. Large and Small Sample 
Compar­isons of Various Variance Estimators, Proceedings of the 1986 Winter Simulation Conference, 278-284, 
1986. 10. The Quality Institute. Process Control, Capability and Improvement. IBM Corpo­ration. May 
1986. (SR1 1-3055) 11. Roy A. Maxion. Anomaly Detection for Diagnosis. Proceedings of Fault Tolerant 
Computing Systems, 20:20-27, June 1990. 12. George Nagy and Rolf Iserrnan. Feature Extraction on Binary 
Patterns Process Fault Detection Based on Modeling and Estimation Methods -A Survey, IEEE Transactions 
on Systems Science and Cybernetics Automatic, 20(4):387-404, Pergamon Press Ltd., October 1984. 13. 
W. A. Shewhart. Statistical Method from the Viewpoint of Quality Control. The Graduate School, Department 
of Agriculture, Washington D. C., 1939. 14. Harold J. Steudel and S. M. Wu. A Time Series Approach to 
Queueing Systems with Applications for modeling job-shop in-process inventories. Management Science, 
23:745-755, March 1977. 15. Walter Vandaele. Applied Time Series and Box-Jenkins Models. Academic Press, 
Inc., 1983.   
			</RefA>
