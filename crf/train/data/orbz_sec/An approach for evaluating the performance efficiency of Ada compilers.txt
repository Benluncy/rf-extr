
 AN APPROACH FOR EVALUATING THE PERFORM.ANCEEFFICIENCY OF ADA ® COMPILERS Mitchell J. Bassman Computer 
Sciences Corporation Gerald A. Fisher, Jr.* Computer Sciences Corporation Anthony Gargaro Computer 
Sciences Corporation BACKGROUND AND RATIONALE There have been several studies that have reported upon 
the performance of the early Ada language (US DoD 1983) compilers. These studies have been oriented to 
quantitative performance testing relying on approaches proven useful in evaluating previous contemporary 
language compilers. These approaches include writing (I) a set of small well- established numerical benchmarks 
(Harbaugh &#38; Forakis 1984), (2) a sample of representative programs from the application domain (Rosenberg 
1984), and (3) a synthetic benchmark (Weicker 1984) in Ada and other high order languages, viz., FORTRAN 
and Pascal, and comparing the resulting compila- tion and execution times. All three approaches yield 
incomplete data. The quantitative measures are not refined to a level of detail at which remedial action 
might be suggested to the compiler implementor or user so that improved results might be obtained. Even 
the synthetic benchmark approach, which provides a model of the application domain based on an analysis 
of the most frequently used language constructs, fails to pro- vide information sufficient for understanding 
the differences in results, thereby allowing diverse interpretations. The key requirements for evaluating 
the performance efficiency of the code generated by an Ada compiler are: i. To provide quantitative 
data on overall performance effi- ciency for a particular application domain. 2. To provide quantitative 
data on performance efficiency that promotes an informed interpretation of the above data.  3. To provide 
a qualitative assessment of the code generated by the compiler with respect to its immediate and future 
use.  4. To counteract any specific effects or interactions, not  Ada ® is a registered trademark 
of the U.S. Government, Ada Joint Program Office. *Author's present address is the IBM Thomas J. Watson 
Research Center. explicitly required, that may invalidate an evaluation of code efficiency. APPROACH 
 None of the approaches reported in the literature satisfies all of the stated requirements. Quantitative 
performance data can be derived by testing a representative sample of programs from the applica- tion 
domain. This is an established reliable method for quantitatively assessing overall time and space performance 
characteristics of the gen- erated object code when the intended application domain is well under- stood. 
Confidence in the results obtained from this testing methodology depends on the adequacy of the program 
sample. However, this method does not provide any insight into, or explanation of, the resulting perfor- 
 mance efficiency of the object code. Quantitative performance testing of the important language features 
that combine to form programs can provide an analytic model of the object code performance for an application 
based upon a specific com- bination of language features. Derivation of an accurate model requires 
exhaustive evaluation of the compiler-generated code for all relevant language features. Since the individual 
abstract features have numerous variations, and since code quality depends upon interaction of the fea- 
 tures, overall performance evaluation may be impractical. The results obtained from this method can, 
however, provide the needed insight into code generation inefficiencies observed during subsequent application 
 program testing. Since the object code efficiency may be evaluated with respect to many different application 
domains, test cases should be developed for all language features if possible. Qualitative assessment 
of the generated code must be per- formed by inspection and hence is subjective. Visual inspection may 
be required to resolve inconsistencies raised by quantitative measurements and to confirm initially 
that the tests have properly compensated for any Ada-specific effects. Establishment of an approach 
for developing language test cases requires consideration for the goals of the compilation process. 
 In a compiler, code is generated from an intermediate representation of a program. In this representation 
the program is decomposed into fundamen- tal language functional components, for example, units for control 
struc- tures, expressions, addressing, etc. For the most part, code is gener- ated for each functional 
unit independently of the others. To improve quality, information about the locality and interaction 
of functional units is gathered and is used to guide the generation of code. This process is called optimization 
or code improvement (Aho &#38; Ullman 1978). With a consideration for the code generation process and 
recognizing the potential need to compare the efficiency of the code gen- erated by the Ada compiler 
with that generated by a compiler for an alternative high order language (HOL), the following approach 
is used: i. Test cases for fundamental language features are pro- grammed in Ada, the HOL, and assembly 
language. The space and time measurements derived from executing these pro- grams are compared in order 
to diagnose possible deficien- cies in the Ada or HOL basic code generation schemes. 2. Test cases for 
code improvement are written in Ada and the HOL. The space and time measurements derived from execut- 
ing these programs are used to determine whether or not classical code improvement techniques are implemented 
by the compiler.  3. Representative applications are programmed in Ada, the HOL, and assembly language. 
The space and time measure- ments derived from executing these programs are compared to obtain an overall 
performance evaluation of code gener- ation efficiency.  This approach provides both overall performance 
efficiency data and also the necessary insight into code generation and code improve- ment techniques 
required to diagnose possible inefficiencies in the object code for fundamental Ada language features. 
The approach is quantitative; except for an initial concept validation, there should be no need to exam- 
ine the generated code. The language features tests and application tests are coded in the target computer's 
assembly language to provide (I) a common base for comparison and (2) the ability to compensate for language- 
specific effects, e.g., constraint checking. The code improvement tests are not coded in assembly language 
since the pertinent result is whether or not a specific optimization is performed in generating the object 
code. A choice may then be made between two compilers that achieve simi- lar object code efficiency, 
one having excellent basic code generation and one having a sophisticated optimizer. The Ada and HOL 
compilers are assumed to generate correct code. Since the alternative HOL compiler could be another Ada 
language compiler, this approach can also be used to compare the efficiency of code generated by multiple 
Ada compilers for the same target. A by-product of compiling and executing the test cases and application 
programs is an assessment of the usability of the compil- ers in terms of host resource requirements. 
 Although an evaluation of the efficiency of Ada run-time sys- tem features, e.g., tasking, input/output, 
exception handling, and memory management, is an important consideration in the determination of whether 
 or not to use Ada for a particular application, those features are typi- cally implemented by calls 
to target run-time library routines rather than by the inline generation of target machine instructions. 
The empha- sis of this approach methodology is on evaluating the time and space efficiency of the generated 
object code; therefore, the run-time system is specifically excluded. The evaluation of the Ada run-time 
system is being addressed in on-going research (Ruane 1984). TEST CONSTRUCTION The initial tests developed 
using this approach were intended to support an evaluation of the performance efficiency of the code 
gener- ated by an Ada compiler being developed for use in a mission-critical system. The selected application 
benchmark test for overall performance analysis was characterized by complex data structures, iterative 
control constructs, and fixed point numeric computation. The language feature tests and code improvement 
tests initially developed were restricted to those that would be supported by the evolving and partially 
implemented Ada compiler. All tests were designed in Ada and then recoded in the alternative HOL and 
assembly language. Pascal (Language Resources, Inc. 1982) was the alternative HOL; however, since the 
tests were designed in Ada, they are not biased toward Pascal. The baseline assembly language was that 
for the Motorola 68000 (Motorola 1984). The remainder of this paper will report on the approach methodology 
in this context, concen- trating upon the language features and code improvement tests. A taxonomy of 
the fundamental features of the Ada language and of classical optimization techniques was used as a 
basis from which to construct the initial tests. The language features taxonomy includes: I. Control 
flow 2. Data reference  3. Expression evaluation ~  4. Subprogram calls  5. Type representation. 
The code improvement techniques include:  I. Local (basic block) optimizations 2. Loop optimizations 
 3. Global optimizations   4. Target-dependent optimizations. Each major category is further decomposed 
into functional categories. The control flow categories, for example, comprise conditional state- ments, 
case statements, loop statements, and transfers of control. Simi-  larly, local optimization categories 
include value propagation, constant folding, expression simplification, and common subexpression elimination. 
 Corresponding to each functional category within the taxonomy is a single generic package containing 
the test procedures for all rele- vant features. The generic package for loop control tests includes 
tests for a loop statement without an iteration scheme, a loop with a while iteration scheme, and several 
variants of loops with a for iteration scheme. It is important to recognize that this approach does not 
measure the efficiency of the Ada implementation of generics. The use of generic packages facilitates 
the instantiation of the Ada tests for different data types. Tests written in assembly language and in 
the alternative HOL are explicitly coded for the different types. Language Features For each Ada language 
feature, a quantitative measure of its space and time cost is obtained for code generated by the Ada 
compiler and the HOL compiler. The cost is determined by compiling and executing the test, both with 
and without the feature present, and computing the difference. The efficiency of the Ada implementation 
of a language fea- ture can be deduced by comparing its cost with the corresponding assem- bly language 
cost. The efficiency of the HOL implementation is deduced in a similar way. Each test comprises a pair 
of procedures: a test version and a control version. The test version makes use of the feature under 
eval- uation. The control version must have exactly the same execution time and space requirements 
except for the use of the specific language fea- ture. Example 1 demonstrates the straightforward use 
of this approach in evaluating the cost of using a loop statement with a while iteration scheme. Example 
1: procedure While_Loop is begin Let(Global, Ident(Zero)); while Global = Zero loop --executed only 
once Let(Global, Ident(One)); end loop; end While_Loop; procedure While_Loap_Control is begin Let(Global, 
Ident(Zero)); --while Global = Zero loop --executed only once Let(Global, Ident(One)); --end loop; end 
WhileLoopControl; The object of the test is to evaluate the cost of using the loop construct itself. 
It is essential to isolate the fundamental feature in order to prevent unexpected interactions with other 
features and to ensure that un- wanted optimizations do not effect the measurements. The number of itera- 
tions must be known a priori to the test constructor, but not to the Ada compiler. If the number of iterations 
were small and could be determined by the compiler, a loop unrolling optimization might be performed. 
If the loop body comprised only an assignment to an otherwise unused variable, the dead variable and 
the dead assignment might be eliminated resulting in a loop with a null body that might also be eliminated 
as dead code. In the example, the conditional expression compares the value of the global variable Global 
with the value of Zero, a generic formal object of mode in. Function Ident has the effect of an identity 
func- tion, and procedure Let has the effect of an assignment statement. Those effects cannot be deduced 
by the compiler. The compiler must, therefore, generate code for the loop construct. Since the value 
of Global is reset to One within the loop, the loop will be executed exactly once. Global, Let, and Ident 
are defined within a generic support package, which must be instantiated for the applicable generic actual 
parameters. The loop statement with the while iteration scheme provides a simple example of the construction 
methodology used to develop the lan- guage feature tests. More complex tests must be developed to evaluate 
 the cost of using other language features, e.g., a case statement. Exam- pies of such tests have been 
shown previously (Bassman et al. 1985). Code Improvements The purpose of investigating the code improvement 
techniques is to determine whether or not the Ada compiler and the HOL compiler em- ploy classical optimization 
techniques (Davis et al. 1978; MacLaren e~ al. 1982; Goes et al. 1983). Construction of the code improvement 
tests is similar to that of the tests for language features. Each test consists of a pair of sub- programs 
for Ada and the HOL. The test version provides the compiler with the opportunity for optimization. The 
control version is specifically written to prohibit optimization. The only functional difference between 
the two subprograms is whether or not the optimization technique is ap- plicable. Some code improvement 
techniques are applied to save storage space, while others are used to reduce execution time requirements. 
Use of the optimization technique is indicated by a difference between the measured space and/or time 
values for the two versions. Example 2 provides a compiler with an opportunity to perform a local value 
propagation optimization that may lead to the elimination of a load operation. New values are computed 
and stored in Global 1 and i Global 2. In the test version those values are reused immediately in the 
 subsequent assignment statements, offering a compiler the opportunity to retain the values in separate 
registers and thereby eliminating the need to reload them prior to expression evaluation. The opportunity 
is removed in the control version by inserting a call to procedure Break Basic Block. Three additional 
support procedures have been introduced to support construction of the code improvement tests. The call 
to Init Globals guarantees that the global variables have initial values. Update_Globals guarantees 
that the assignments to Global_4 and Global 5 are not dead in the control version. Break Basic Block 
is declared by renaming Update Globals. Its alternative name is introduced to clarify the usage of 
the procedure call in the control version, The call to Break_Basic Block immediately preceding Update 
Globals in the test ver- sion is required so that execution time and storage space requirements for 
the two versions will be equivalent except for the possible applica- tion of the code improvement technique. 
 Example 2: procedure Load Elimination is begin Init Globals: Global_l := Global_4 + Global_5; Global_2 
:= Global 4 - Global_5; --Break BasicBlock; Global 4 := Global 1 - Global_3; Global 5 := Global_2 - 
Global_3; Break Basic Block; --comment out in control version Update Globals; end LoadElimination; 
procedure Load Elimination Control is begin Init Globals; Global I := Global 4 + Global 5; Global--2 
:= Global--4 - Global--5; Break Basic Block; --comment out in test version Global 4 := Global 1 - Global 
3; Global--5 := Global--2 + GlobalZ3; --Break Basic Block;-- Updat~Glob~is: end LoadElimination_Control; 
 TEST COMPILATION AND EXECUTION Compilation of the performance evaluation tests is a host- dependent 
activity. Similarly, test execution is target-dependent. Al- though this paper makes no formal recommendations 
for a standard approach to compiling and executing the tests, future work to refine this part of the 
methodology is anticipated. All of the tests developed to date have been successfully compiled by at 
least one validated Ada compiler. At the time of this writing, not all of the tests had been processed 
by the partial Ada compiler that was the subject of the initial application study. To be completely successful, 
the approach requires a stable, validated compiler. The following data are collected: I. Space utilized 
by test version (STAda, STHoL, STAs M) 2. Space utilized by control version (SCAda, SCHo L, SCAs M) 
 3. Time utilized by test version (TTAda, TTNo L, TTAs M)  4. Time utilized by control version (TCAda, 
TCHo L, TCAs M)  For each of the language features tests, the following results are analyzed: I. Memory 
space usage (SAda, SNOL, SAS M) 2. Execution time usage (TAd a, TMO L, TAS M)  3. Ada space efficiency 
(SAda/SAs M)  4. HOL space efficiency (SHoL/SAs M)  5. Ada time efficiency (TAda/TAS M)  6. HOL time 
efficiency (THoL/TAs M) The memory space usage for an Ada language feature, SAda, is computed as the 
space required for the object code of the test version minus that required for the control version: 
SAd a = STAd a -SCAd a Only the difference, which gives the space cost of using the language feature, 
is significant. The raw data include the overhead of the code used to inhibit unwanted optimizations. 
Other time and space usage results are derived similarly. Since the purpose of the code improvement tests 
is to deter- mine whether or not the optimization techniques are used, the following results are analyzed: 
 I. Space improvement measure (Ada) (SCAd a - STAd a >0) 2. Space improvement measure (HOL) (SCHo L 
- STHo L > 0)  3. Time improvement measure (Ada) (TCAd a - TTAd a > 0)  4. Time improvement measure 
(HOL) (TGHo L - TTHO L > 0)  Data for the analysis of compiler usability are collected during test 
compilation and execution. Examples of quantitative host resource utilization measures are the following: 
 I. Compilation time 2. Disk spaceutilization  3. Main memory utilization.  A qualitative assessment 
is provided for the following factors: I. Command usage 2. Diagnostics  3. Documentation  4. Listings. 
  SUMMARY OF RESULTS The results from the initial use of this approach are incom- plete and are presented 
only to establish concept validation to guide and refine future work. Although compilation and execution 
experience has been useful in reporting upon the performance efficiency characteristics of an Ada and 
a Pascal compiler, insufficient quantitative data are cur- rently available to publish an accurate comparison. 
Furthermore, the available Ada compiler had not reached a. level of maturity at which such a comparison 
would be constructive. v ~ . ~ While the test/control pairs were designed to evaluate the quality of 
compiler-generated code, compilation efficiency measures were also collected in the expectation that 
useful results might be derived. Experience with the language feature tests and code improve- ment tests 
have indicated that the results of their executions will pro- vide useful information in understanding 
the execution results of the application benchmarks without resorting to an intensive study of the generated 
code for the application. Concept Validation The approach is valid only if the test/control pairs can 
be constructed in such a way as to nullify undesirable side effects from both language specific characteristics 
and the interactions of the en- closing language constructs. Examination of the code generated for the 
tests compiled using an Ada and a Pascal compiler has shown that the pertinent code was successfully 
isolated so that the performance efficiency of language features can be measured and code improvement 
techniques detected. An objective in designing the tests was to prevent unwanted optimizations. Although 
the camouflaging techniques might have unneces- sarily complicated the tests by introducing extraneous 
functionality, analysis of preliminary results has reduced that concern. It is pre- mature, however, 
to claim that all unwanted optimizations were success- fully prevented until the tests have been compiled 
using a larger sample of compilers. Use of Generics The use of Ada generics was an important decision 
of the con- struction methodology. The underlying assumption was that the perform- ance efficiency measures 
would not be compromised by any side effect of using a generic instantiation rather than its nongeneric 
counterpart. Since there is limited experience in projecting the efficiency of generic instantiations, 
especially in the presence of shared bodies that may pro- duce adverse effects on code efficiency (Bray 
1984), a straightforward experiment was performed to investigate the assumption. The test/control pairs 
for the loop control language feature tests were modified to refer- ence objects of an INTEGER subtype. 
The results of compiling and execut- ing those tests were compared with those obtained from compiling 
and executing the equivalent test/control pairs created by generic instanti- ation. This comparison 
revealed that there was no difference in the data that would be used for performance efficiency measures. 
 At least one validated Ada compiler (Verdix Corporation 1985) supports an option to control the sharing 
of generic bodies when optimal code is required. It is expected that future compilers will provide a 
similar option (Digital Equipment Corporation 1984) so that there is minimal risk of invalidating performance 
measures by the use of generics in the construction methodology. Language Features Test execution data 
from versions of the language features tests written in Ada, Pascal, and MC68000 assembly language have 
been collected and analyzed. The results have confirmed the expected effi- ciency of assembly language 
for very small programming exercises and the value of using it as a baseline when evaluating an achievable 
objective for HOL compilers. Code Improvement Test compilation and execution data for Ada and Pascal 
code improvement tests have identified differences between the two compilers in detecting opportunities 
for optimizations. In some instances only partial optimization was performed when there was the potential 
for fur- ther code improvement after the initial optimization was detected, e.g., additional expression 
simplification and dead code elimination. Inspection of the generated code has confirmed that all test/ 
 control pairs are sufficiently sensitive for the code improvement oppor- tunities to be offered to 
the Ada and Pascal compilers. Problems Encountered Although no serious flaws in the approach have been 
encoun- tered, distorted measurements were computed for some of the tests. These invalid measurements 
occurred because the test and control subprograms were enclosed in the same compilation unit and occupied 
a contiguous ad- dress space when loaded for execution. A consequence of this is that, depending upon 
the target ISA, references to the same global object from within the two subprograms may cause a displacement 
addressing scheme to force generation of different length instructions since the relative displacements 
are different from the test and control subprograms. This problem can be eliminated by ensuring that 
the test and control subpro- grams are loaded identically for execution. CONCLUSIONS This paper has 
suggested an approach for evaluating the per- formance efficiency of code generated by Ada compilers. 
The approach can be used either to compare the efficiency of Ada and other HOL compilers or the relative 
efficiency of multiple Ada language compilers for the same target. Experience with this approach has 
been limited to one par- ticular application and the use of two Ada compilers and one Pascal com- piler. 
Although the approach appears to be sound, it is premature to report conclusive results until more extensive 
experience has been acquired and the data assimilated from using additional compilers. It is anticipated 
that all existing language features and code improvement tests will be compiled and executed using at 
least two production quality Ada compilers in the near future. Current results indicate that the approach 
achieves a modest and systematic advance over some commonly used, less formal benchmarking approaches. 
This conclusion is based upon the observation that the separate classes of tests have provided complementary 
information and assessments of the compilers that have been under evaluation. The lan- guage features 
and code improvement tests fulfilled the expectation that they would provide both quantitative measures 
of the generated code and insight into the results from the application program test. There is a need 
for further results from diverse Ada and HOL compilers. The analysis of these results would assist in 
refining the tests and perhaps identifying potential flaws in the approach. One pos- sible criticism 
of the approach is that the simplicity, or feature trivi- alization, of the tests may not yield sufficient 
insight into overall compiler performance efficiency since language feature interactions are artificially 
controlled by the tests. For Ada, in particular, the com- plexities of these interactions may have significant 
ramifications on code generation, requiring that this approach incorporate a less trivial testing methodology 
in order to increase the sensitivity of the tests to these interactions. ACKNOWLEDGMENTS The authors 
wish to acknowledge the contribution of Charles H. Sampson (CSC, San Diego, CA) in the development of 
the test approach. REFERENCES <RefA>Aho, A. and Ullman, J. (1978). Principles of Compiler Design. Addison- 
Wesley, Reading, Massachusetts. Bassman, M.J. et al. (1985). Evaluating the Performance Efficiency of 
Ada Compilers. In Proceedings of the Washington Ada Symposium, ed. J. Johnson. ACM. Bray, G. (1984). 
Sharing Code among Generic Instances of Ada Generics. In Proceedings of the SIGPLAN '84 Symposium on 
Compiler Construction. SIGPLAN Notices, 19, 6, pp. 276-284. Davis, M. et al. (1978). Optimization Panel 
Report, USAF Standard Compiler Workshop. Digital Equipment Corporation (1984). VAX Ada Technical Summary 
(Preliminary). Goos, G. et al. (1983). An Optimizing Ada Compiler. University of Karlsruhe. Harbaugh, 
S. and Forakis, J. (1984). Timing Studies using a Synthetic Whetstone Benchmark, Ada Letters, ~, 2, pp. 
23-34. Language Resources, Inc. (1982). Pascal Programming Language Specification, #DPO02. MacLaren, 
M. et al. (1982). Engineering a Compiler, VAX-II Code Generation and Optimization. Digital Press. Motorola 
(1984) MC68000 16/32-Bit Micro-processor Programmer's Reference Manual. Prentice-Hail, Inc., Englewood 
Cliffs, New Jersey. Rosenberg, M. (1984). Comparison of Ada Code Efficiency with Other Languages, AdaTEC 
National Meeting, February. Ruane, M. (1984). An Empirical Approach to the Evaluation of Run-Time Environments, 
SIGAda National Meetings July. U.S. Department of Defense (1983). Reference Manual for the Ada Programming 
Language, ANSI/MIL-STD-1815A. Verdix Corporation (1985). Private Communication, January. Weicker, R. 
(1984). Dhrystone: A Synthetic Systems Programming Benchmark, Commun. ACM, 21, 6, pp. 1013-1030.</RefA> 
			
