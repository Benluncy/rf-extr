
 A Database Management Capability for Ada* Arvola Chan Sy Danberg Stephen Fox Terry Landers Anil Nori 
John M. Smith Computer Corporation of America 4 Cambridge Center Cambridge, MA 02142 INTRODUCTION 
The data requirements of mission-critlcal defense systems have been increasing dramatically. Command 
and control, intelligence, logistics, and even weapons systems are being required to integrate, process, 
and share ever increasing volumes of information. To meet this need, systems are now being specified 
that incorporate database management subsystems for handling storage and retrieval of information. Indeed, 
it is expected that a large number of the next ge~eratlon of mission-crltical systems will contain embedded 
database management systems. Since the use of Ada has been mandated for most of these systems, it is 
important to address the issues of providing data- base management capabilities that can be closely 
coupled with Ada. Under sponsorship by the Naval Electronics Sys- tams Command and the Defense Advanced 
Research Pro- Jects Agency, Computer Corporation of America has been investigating these issues in the 
context of a comprehensive distributed database management pro- Ject. The key deliverables of this project 
are three closely related prototype systems implemented in Ada. I. LDM (local data manager): an advanced, 
cen- tralized database management system that sup- ports a semantically rich data model designed This 
project ls supported Jointly by the Advanced Research Projects Agency of the Department of De- fense 
(DARPA) and the Naval Electronics Systems Command (NAVELEX) under contract N00039-82-C-0226. The views 
and conclusions contained in this paper are those of the authors and should not be inter- preted as 
necessarily representing the official policies, either expressed or implied, of DARPA, NAVELEX, or 
the U.S. Government. Ada is a Registered Trademark of the U.S. Govern- ment (AJPO) COPYRIGHT 198~ BY 
THE ASSOCIATION FOR COMPUTING ~CHINERY, INC. Permission to copy without fee all or pars of this material 
is graflte~ provided that the copies ere not made or distributed for direct Â¢0mercial advantage, the 
ACN copyright notice and the titlt of the pub)icatfon end its date appmer, and notice is given that copying 
is by permission of the Asaoc[atlon for Computing /4achlnery. To copy otherwise, or to republish, requires 
 fee and/or specific permission. to improve user productivity. It can be used either stand alone or as 
an integral part of the other two prototype systems. 2. DDM (distributed data manager): a homogeneous 
distributed database management system built on top of a collection of LDMs in a computer network. It 
supports the transparent distri-bution and replication of data in order to provide efficient access and 
high availabil-ity. 3. Multibase: a retrieval-only system that pro-vides a uniform interface through 
a single query language and database schema to data in preexisting, heterogeneous, distributed data-bases. 
It utilizes LDM for managing its local workspace during the processing of a global  query. All three 
systems are designed to support Ident- lea1 interfaces for interactive use and for use through application 
programs written in Ada. Fun- damentally, they support a "semantic" data model that captures more application 
semantics than con- ventlonal data models. The interactive language is called Daplex. Daplex has been 
designed to be an Ada compatible database sublar~uage. The syntax of many of its constructs for data 
definition and data manipulation has been borrowed from Ada. The application programming interface is 
called Ada- plex. It consists of an expresslon-level integra- tion of Daplex's data manipulation constructs 
with Ada. This paper identifies the set of requirements for a modern database management capability for 
Ada that has driven our design for the aforementioned prototype systems. It provides an overview of the 
Daplex and Adaplex languages, and a summary of the functional capabilltles and technical innovations 
we have incorporated in the LDM, DDM, and Multlbase systems. REQUIREMENTS Providing a database management 
capability for Ada is not an easy task. Our goal is to provide a complete set of modern database management 
capabil- ities which are oonaletent with the style and phi-losphy of Ada and which are well integrated 
with the Ada language and its support environments. This section summarizes the major requirements of 
a database management capability for Ada. These requirements can be grouped into three general areas: 
classes of databases that must be supported, operating environments, and compatibil- ity with Ads. 
 Classes 9_~ databases Ads programs will need to access three classes of databases. The first class 
consists of central- ized databases. These databases reside at a single location and are managed by a 
DBMS that executes on a single computer. The second class consists of distributed databases. These databases 
can be fragmented, distributed, and replicated across a number of (possibly geographically separated) 
sites. They are managed by a DBMS that executes on a number of computers that are connected by ~ com- 
munications network. Distributed databases provide improvements in reliability, survivability, and expandability 
over centralized databases. The third class is pre-existing databases. These are databases (possibly 
centralized or distributed) that are managed by existing DBMSs. These DBMSs are not implemented in Ada. 
They provide different sets of functional capabilities and support dif- ferent interface languages. An 
important require- ment for an Ada database capability is to provide a single Ada interface to all of 
the above classes of databases. In other words, the particular class of database being accessed should 
be transparent to the Ada database application programmer. ~_alzng ~nLtcmumn~= An Ada DBMS must be 
able to operate effectively in both an Ada programming support environment (APSE) to facilitate the development 
of Ada data- base application programs, and in an Ada run time environment to support the execution of 
these pro- grams. To provide for the needs of these two environments, the DBMS must have two operating 
modes: shared and embedded. Shared mode is nor- really used in an APSE. A single copy of the DBMS supports 
the simultaneous development of multiple Ada database application programs in this mode. The interface 
between the application programs and the DBMS is a loosely-coupled one, each being exe- cuted as a separate 
Ada program. Thus, each appli- cation program can be changed without impacting the DBMS or other application 
programs. Embedded mode is typically used in a run tlme environment. Once the application programs have 
stabilized, they can be loaded together wlth the DBMS into a single Ads program. The appllcatlons and 
the DBMS then operate as separate Ads tasks that synchronize and communicate vla rendezvous, thereby 
achieving a higher degree of interface efficiency. Embedded mode is less flexible than shared mode since 
a change to one application causes the other applica- tions and the DBMS to be rellnked. wlth Ada Ada 
has made a large contribution to improving program integrity through strong type checking at compile 
time and constraint checking at run time. It Is important that an Ada DBMS provides the same degree of 
integrity on the Ada program data that it manages. An Ada DBMS should support all of the Ads data types, 
including derived types, subtypes, and type attributes. It should also support the same degree of run 
tlme constraint checking. Note that thls cannot be easily (or efficiently) accomplished by simply providing 
an Ada interface to an existing (non-Ada) DBMS. Let us illustrate this with a sim- ple example. Suppose 
an Aria programmer wants to store a set of employee records in a database. The Ada type definitions for 
this record may look like: type YEARS is new INTEGER range 0.50;' type EMPLOYEE is record NAME : STRrNG(I..30): 
YEARS_OF_SERVICE : YEARS; SALARY :INTEGER: end record; Suppose that the Ada programmer writes a program 
that contains a transaction that adds one to the YEARSOFSERVICE component of each employee record. There 
are two ways to process this transaction. One way is to retrieve the YEARS_OFSERVICE com- ponent for 
each record in the database and return it to the application program, add one and then store it back 
in the database. This is a very inefficient way of processing since it results in a lot of data being 
sent from the DBMS to the appli- cation program and then back again. A much more efficient method is 
to have the DBMS perform the update directly. That is, the application program can instruct the DBMS 
to add one to the YEARSOFSERVICE component of each record. This results in no data being returned to 
the applica- tion program. However, the DBMS must now take the responsibility of insuring that all new 
values of YEARSOFSERVICE remain within the specified range. It is not acceptable for the DBMS to blindly 
change each value of YEARSOFSERVICE, only to have the application programs that retrieve the data at 
a later tlme discover that some values have become illegal. To our knowledge, no existing DBMS pro- vides 
this degree of Ada-compatible run time con- straint checking. DAPLEX Data models and associated query 
languages have evolved significantly over the past two decades. The early hierarchical models were superseded 
by the network and relational models. The latter are in turn being superseded by so-called semantic data 
models. Our overall DBMS project is based on a semantically rlch data model called Daplex which combines 
and extends the key features of earlier data models. For example, Daplex's modelling con- structs are 
a strict superset of those found in the relational model. Daplex is designed to enhance the effectiveness 
and usability of database systems by capturing more of the meaning of an application environment than 
is possible wlth conventional data models. It descrlbea a database In terms of the kinds of entities 
that exist in the application environment, the classifications and groupings of these entities, and the 
structural interconnections among them. The semantic knowledge captured In Daplex is not only meaningful 
to end users, but is also usable by the database system and database administrator for the purposes of 
query and physi- cal schema optimization. For example, knowledge of the nature of relationships between 
types of enti- ties (i.e., whether they are one-to-one, many-to-one, or many-to-many) can be used to 
control the appropriate clustering of entities of different types that are likely to be accessed together, 
both in a centralized and in a distributed environment. The basic modelling constructs in Daplex are 
entities and functions. Entities correspond to conceptual objects. Entities are classified into entity 
types, based on the generic properties they possess. Functions represent properties of concep- tual objects. 
Each function, when applied to an entity of appropriate type, yields a single pro- perty associated with 
that entity. Such a property is represented by either a single value or a set of values. These values 
can be simple, being drawn from Ada supported scalar types and character strings, or composite, consisting 
of references to entities stored in the database. We illustrate these constructs with an example. Consider 
a university database modelling stu- dents, instructors, departments, and courses. Fig- ure I is a graphical 
representation of the defini- tion of such a database. The rectangles depict entity types. The labels 
within the rectangles depict functions that range over Ada scalar and string types. The single-headed 
and double-headed arrows represent single-valued and set-valued func- tions that map argument entity 
types to result types. The double-edged arrows indicate isa (sub- type) relationships. PERSON SSN NAME 
,s/ AGE SA I INSTRUCTOR COURSE STUDENT [ ADVISOR TITLE I GPA )R _ SALARY ROOM CREDITS I COORSES-TAUO"T 
I ENROLLMENTS DEPT DEPT  ~ DEPARTMENT I NAME FLOOR  Figure I. A Daplex Database One major difference 
between Daplex and the relational model is that referential integrity con- straints [Date81], which are 
extremely fundamental in database applications but not easily specifiable in a relational environment, 
are directly captured. For example, when a student is inserted into the database, the database system 
will ensure that it is assigned a valid instructor, i.e., one that is existent in the database. Likewise, 
when an instructor is to be removed from the database, the database system will see to it that no dangling 
references result, i.e., there are no more students in the database who have the instructor in question 
as advisor. Another important semantic notion captured in Daplex is that of a hierarchy of overlapping 
entity types. In relational systems, a real-world entity that plays several roles in an application environ- 
ment is typically represented by tuples in a number of relations. In the university application enviror~ent, 
we might have an instructor entity named John Doe and a student entity also named John Doe. In this case, 
it might be desirable to impose the constraint that the age of John Doe as an instructor should agree 
with the age of John Doe as a student. One possible strategy in a relational system is to represent this 
information only once by having a relation person that stores the age information, and relying on joining 
operations to determine the age information for students and instructors. In Daplex, we can specify that 
stu- dent and instructor are subtypes of person whereby we can utilize Daplex's function inheritance 
seman- tics to simplify the formulation of queries and updates. Figure 2 shows a relational equivalent 
of the university database. Figures 3 and 4 show a Daplex query and its equivalent in SQL [DATE84]. The 
intent of this query is to print the names of all students taking a class held at room "F320" and PERSON 
(SSN, NAME, AGE) STUDENT (SSN, ADV-SSN) INSTRUCTOR (SSN, DEPT) COURSE (ROOM, CREDITS) ENROLLMENTS (SSN, 
TITLE) COURSES_TAUGHT (SSN, TITLE) Figure 2. A Relational Schema for each S in STUDENT where "F320" 
is in ROOM(ENROLLMENTS(S)) and DEPT(AOVISOR(S)) = CS loop PRINT(NAME(S)); end loop; Figure 3. A Daplex 
Query SELECT PERSON.NAME FROM PERSON, STUDENT, ENROLLMENTS, COURSE, INSTRUCTOR WHERE PERSON.SSN = STUOENT.SSN 
AND PERSON.SSN = ENROLLMENTS.SSN AND ENROLLMENTS.TITLE = COURSE.TITLE AND COURSE.ROOM = "F320" AND STUDENT.ADV-SSN 
= INSTRUCTOR.SSN AND INSTRUCTOR.OEPT = CS Figure 4. An Equivalent SQL Query  taught by an instructor 
in the "CS" department. Notice how explicit join terms have to be intro- duced in the SQL query, which 
tend to obscure rea- dability. On the other hand, the absence of such constructs from the Daplex query 
allows the query to be read in a more or less English-like manner. A complete description of the Duplex 
data model and access language can be found in [SLRR84]. ADAPLEX Database environments for popular 
programming languages, notably C, PL/I, COBOL, and Pascal, have resulted in extensions to the host programming 
language. At the outset, it was not clear whether Ada would also need to be extended to accommodate database 
applications. This is because Ada con- tains important new features not found in previous widely-used 
languages. In particular, Ada's pack- age construct offers the potential for defining a database extension 
within the language itself. There have actually been a number of proposals for coupling database management 
capabilities to Ada through the package construct [HTVN81, NOKI8S, VINE83]. However, we feel that such 
approaches sacrifice usability and data integrity for not extending Ada [SCDF85]. Since our goal is to 
design the best Ada compatible language environment for developing database application programs, it 
is our desire to express as much of the database environment in Ada as possible, although not at the 
expense of database capabilities and ease of use. Two major capabilities that must be provided by a 
database programming environment are schema definition (for describing the contents of the database) 
and transaction definition (for specify- ing operations on the stored data). In order to support database 
applications programming in Ada, it is necessary to couple the DBMS to an Ada pro- gramming support environment. 
One possible approach for achieving such a coupling is illus- trated in Figure 5. Notice that both schema 
defin- ition and transaction definition are separated from the Ada application program. Schema Transaction 
Ada Definition Definition Program Schema Transaction Ada Compiler Optimizer Compiler Schema Library Transaction 
Library Program Library t t t Ada DBMS ~ Run time System Figure 5. Coupling a DBMS with an Ada Programming 
Support Environment This separation works for database schema defin- ition since the output of the schema 
compiler can be logically thought of as an Ada package contain- ing type definitions representing a database 
 schema. The separation of transaction definition from application program is less natural because parameters 
must be passed from the application pro- gram to the DBMS and transaction results must be bound to application 
program variables. In the course of our project, two approaches for handling transaction definition 
have been con- sidered. The first approach is simillar to the one used for schema definition. A transaction 
defini- tion is passed to the transaction optimizer which generates an Ada package that implements (i.e. 
calls the DBMS to execute) the transaction. The package is then loaded with the application pro- gram. 
This approach, however, leaves the applica- tions programmer with a rather complicated inter- face. The 
programmer must learn a transaction definition language which is quite distinct from Ada. Besides, parameter 
passing between the appli- cation program and the package that implements the transaction is cumbersame. 
Since Ada is a strongly typed language, it might be necessary to use an intermediate representation like 
character strings for passing certain parameters. This has a number of drawbacks. First, the programmer 
must expli- citly encode and decode these strings. Second, compile time type checking cannot be performed 
on the contents of these strings. In general, such a parameter passing mechanism can be quite ineffi- 
cient. These difficulties lead us to adopt a second approach which permits the application programmer 
to embed transaction definitions directly in an Ada program. The result is an integrated language, called 
Adaplex, which provides a tight coupling between Ada and our transaction definition language. No changes 
were made to existing Ada constructs. The new constructs that were added are treated in an Ada compatible 
manner. The coupling is achieved at the expression level. Applications programmers are free to use Ada 
expressions, con- trol structures, and subprogram calls within a transaction definition. Because of Adaplex's 
uni- form syntax and semantics, we expect it to be very easy to learn and use by trained Ada programmers. 
 For portability reasons, a preprocessor is used to decompose applications programs written in Ada- plex 
into a transaction part and an Ada program part. The transaction part is forwarded to the transaction 
optimizer and the Ada part to the Ada compiler. The preprocessor is a very powerful tool. It provides 
the same integrity checking across the application program/DBMS interface that the Ada compiler provides 
for an Ada program. The schema compiler, transaction optimizer, preprocessor, and DBMS form the minimum 
set of pro- gram development tools required for the database environment. Their combined configuration 
is shown in Figure 6. Any one of the Multlbase, LDM, DDM systems can be substituted in place of the box 
labelled DBMS. Provided all these tools are writ- ten in Ada, database schemas, application programs, 
and databases may be ported between Aria installa- tions. Fundamentally, Adaplex adds two constructs 
to Ada, the ~ declaration and the ALg/LIg ~tate- m.~.nJ~. These constructs provide for schema Integrated 
Application database UNIVERSITY is Program I Preprocessor Schema Transaction Ada Definition Definition 
Program To Ada Schema i I Transaction CompilerCompiler Optimizer DBMS 1 Figure 6. Configuration of Adaplex 
Programming Tools definition and transaction definition respectively. A database declaration specifies 
the data objects in a database, the types of those data objects, and their consistency/integrity requirements. 
Database declarations are processed by the schema compiler. Figure 7 shows the database declaration for 
the university database that was depicted graphically in Figure I. In addition to the type and subtype 
declarations, several constraint statements have been specified. overlap INSTRUCTOR with STUDENT; indicates 
that it is legal for a PERSON entity to be both a STUDENT and INSTRUCTOR simultaneously. unique TITLE 
within COURSE; indicates that all COURSE entities must have unique TITLEs. A database is similar to 
a package since it is a related collection of data and type declarations. However, a database differs 
from a package in three principal ways. First, there are explicit proto- cols within Adaplex for several 
independent main programs to share the use of a database. Second, a strong discipline is imposed on the 
specifications allowed in a database declaration. Third, database declarations are developed interactlvely 
via the schema compiler, and they are stored for future reference in the schema library. An atomic statement 
specifies a compound opera- tion which must be indivisibly executed with respect to a database. The preprocessor 
extracts transactions from atomic statements for processing by the transaction optimizer. Figure 8 shows 
an Ada code fragment containing an atomic statement. type DEPT NAME is (CS, EE, MA); type YEARS is new 
INTEGER range 0 .. 120; UNKNOWNAGE : constant YEARS := 0; type COURSE is entity TITLE : STRING (1 .. 
6) ROOM : STRING() .. 5); CREDITS : INTEGER range I .. 4; end entity type PERSON is entity NAME : STRING 
(1., 30); AGE : YEARS := UNKNOWN_AGE; SSN : INTEGER; end entity; subtype INSTRUCTOR is PERSON entity 
DEPT :DEPT_NAME; COURSES_TAUGHT : set of COURSE; end entity; subtype STUDENT is PERSON entity DORM : 
STRING (I 10); , ADVISOR : INSTRUCTOR withnull; ENROLLMENTS : set of COURSE; end entity; overlap INSTRUCTOR 
with STUDENT; unique TITLE within COURSE; end UNIVERSITY; Figure 7. An Adaplex Database Declaration 
 This transaction creates a new COURSE entity and indicates that the course will be taught by the with 
UNIVERSITY; use UNIVERSITY; AIDD_COURSE: declare NEW_COURSE : COURSE; atomic NEW_COURSE : = new COURSE 
(TITLE = > "CS-101", ROOM = > GET_ROOM(CS), CREDITS = > 3); include NEWCOURSE Into COURSES_TAUGHT (I 
in INSTRUCTOR where NAME (I) = "Adam Jones"); exception when UNIQUENESSCONSTRAINT = > PUT_LINE("Duplicate 
course name"); end atomic; Figure 8. An Adaplex Database Transaction instructor named Adam Jones. Notice 
that the data- base type declarations are made visible by the with and use statements, The expression 
level integra- tion of Daplex and Ada is illustrated by calling an Ada subprogram, GET._ROOM, to generate 
a value to assign to the ROOM function. Since COURSEs are constrained to have unique TITLEs, it is possible 
that the create statement may fail. An exception handler is included to cleanly handle this error. An 
atomic statement is similar to a block in the sense that it is a compound stat~ent that has associated 
declarations and exception handlers.  However, an atomic statement differs from a block in three ways. 
First, atomic statements are exe- cuted indivisibly with respect to databases. Second, strong disciplines 
are imposed on the con- tents, nesting, parallel execution, and exception handling of atomic statements. 
Third, atomic statements are transformed by the preprocessor to extract database transactions. A complete 
description of the Adaplex language can be found in [SFL83]. A detailed discussion on our rationale for 
developing Adaplex can be found in [SFL83, SCDF85]. LDM LDM is a general purpose system for defining, 
storing, retrieving, updating, sharing, and pro- tecting formatted information. While its users may be 
geographically distributed, LDM and its data must be centrally located. LDM is designed to pro- vide 
all the functions typically found in a modern database system, including: * logical and physical database 
definition,  logical and physical database reorganization,  a fully integrated data dictionary facility, 
  an authorization mechanism for controlling data- base access,  optimized selection of access paths 
for transac- tions,  interference-free concurrent access by multiple users/transactions,  automatic 
recovery from transaction failures, software crashes, and media failures,  a dumping utility for taking 
a consistent snapshot of the entire database,  a reload utility for restoring a database to a previously 
saved state.  LDM's main design objectives are transportabil- ity and high performance. Transportability 
is achieved by the use of Ada as the implementation language and by using a modular system architecture 
which is greatly facilitated by Ada's packaging construct and separate compilation mechanism. A description 
of LDM's component architecture can be found in [CFLR81 ]. High performance, on the other hand, requires 
the introduction of a number of technical innovations in the areas of physical data structuring, query 
optimization, concurrency con- trol, and recovery management as identified below. LDM is designed to 
provide complete physical data independence. It supports flexible physical structuring options so that 
a database administra- tor can tailor the physical representation of a database according to applieatlon 
requlrements [CDFL82]. LDM employs special data structures for the efficient maintenance of referential 
integrity and other contralnts associated with type overlaps in a generalization hierarchy. It also 
provides a wide range of options for the clustering of enti- ties that belong to a generalization hierarchy. 
LDM supports dynamic data structures (namely, linear hashing [LARSS0] and B-trees [COMRV9]) to eliminate 
the need for pe riodic reorg anization. In order to suppert the efflelent traversal of interentity references, 
LDM implements a pointer validation scheme that minimizes the updating costs associated with the use 
of dynamic data structures. The design of LDM is geared towards the process- ing of repetitive transactions 
in a database appli- cations programming environment. Transactions are compiled, thereby permitting the 
costs for parsing, authorization checking, and access path optimiza- tion to be amortized over multiple 
execution. LDM is also designed to optimize a much larger class of queries than relational systems. In 
particular, we have developed efficient strategies for processing queries with outer joins and nested 
quantifiers [RCDF82, DAYA83A]. At the same time, the amount of effort that LDM will expend to optimize 
a transac- tion template can be controlled by a user (in the form of a pragma). Thus, a user can ensure 
that the effort for optimizing a given transaction tem- plate is commensurate with the savings that can 
be expected to accrue over repeated execution. LDM implements an integrated concurrency control and 
recovery mechanism which has the advantage of improving concurrency while simplifying transaction and 
system recovery. Specifically, LDM implements a multiversion mechanism that allows each read-only transaction 
to see a consistent snapshot of the database without having to synchronize with update transactions [CFLN82]. 
The essence of this mechan- ism is that update transactions create new versions of data objects without 
overwriting their previous versions. An efficient scheme is used to determine the appropriate version 
of different data objects each read-only transaction should see, and to iden- tify those old versions 
that can be garbage col- lected. Since database dumps can be considered as read-only transactions that 
access the entire data- base, they can also be taken non-intrusively (i.e., without requiring the quiescence 
of concurrent updates). In addition to being a stand-alone centralized database system, LDM also functions 
as an integral part of DDM and Multibase. DDM DDM is a homogeneous distributed database system built 
on top of a collection of LDMs running at different sites connected by a computer network. From the end-users' 
point of view, DDM performs precisely the same operations supported by LDM. This is because all complexities 
introduced by fragmentation, distribution, and replication of a database are hidden from end-users. Users 
access a distributed and replicated database in DDM just as they would access a centralized database 
in LDM. In a distributed environment, a copy of LDM and a copy of DDM are installed on each of several 
com- p-ters in a computer network where data is distri- buted / replicated. Each LDM is responsible 
for managing all locally stored data at its resident site. Each DDM cooperates with all other DDMs in 
 the network in order to hide the distribution and replication of data from end users and applica- tions. 
As a truly distributed system, DDM delivers the benefits of improved processing capacity, com- munications 
efficiency, survivability, and modular upward scaling. DDM provides the following impor- tant facilities. 
 * An integrated global schema that encompasses data stored at all sites. DDM maintains a glo- bal directory 
in order to keep track of the dis- tribution and replication of data. It automati- cally maps transactions 
on the global schema into subtransactions on data stored at indivi- dual LDMs.  * Complete physical 
data independence. The data- base administrator is free to tune parameters involving the physical distribution, 
replica- tion, and representation of the stored data, without affecting the external view of the data- 
base.  * Mutual consistency of replicated data. Users deal with logical data only. Propagation of updates 
to redundant copies of updated data is managed by the system.  * Atemicity of distributed transactions. 
DDM guarantees than no partial effects of one tran- saction will be seen by another. If a transac- tion 
is unable to complete, all of its effects on the database are autsmatically undone.  * Continued operation 
in spite of site failures. Users can continue to perform retrieval and update operations, even though 
some copies may be temporarily inaccessible. These latter copies are brought up to date by the system 
before being used for processing subsequent transactions.  * Dynamic integration of new sites. No quiescence 
of on-going activities is needed for reconfi- guration of the system.  As in LDM, our main design objectives 
for DDM are transportability and performance. Again, we have introduced a number of technical innovations 
in the areas of data allocation, query optimiza- tion, concurrency control, and recovery management in 
order to obtain good performance. These are summarized below. DDM supports flexible database fragmentation 
and allocation that can be used to improve locality of reference and efficiency of query processing [CDFR83]. 
Each database managed by DDM is option- ally divided into a number of groups of data frag- ments, based 
on the likelihood of their being used together. Each group of data fragments constitutes a unit for allocation 
and may optionally be repli- cated at as many sites as desired. For a repli- cated fragment group, two 
kinds of copies are dis- tinguished. Online copies are used for processing transactions. Off line copies 
serve as warm stand- bys that can quickly (and automatically) be upgraded to online status in order 
to retain a desired degree of resiliency as sites storing online copies fail. When specifying the replica- 
 tion parameters for a fragment group, a database administrator indicates the number of desired online 
copies and those sites whose copies are to be kept online preferrably. DDM will then strive to keep 
those copies at the preferred sites online, but dynamically bringing copies stored at other sites online 
to maintain the desired level of resi- liency when necessary. Unlike previous systems, DDM is designed 
to take into consideration database fragmentation and replication in its selection of strategies for 
pro- cessing transactions [CDFGS3]. Whereas most previ- ous studies on distributed query optimization 
assume the distribution of joins over unions, DDM will consider the options of using left distribu- tion, 
right distribution, or no distribution at all when processing queries that involve such opera- tions. 
DDM treats each fragment group as an integral data unit during the optimization process. Both compile 
time and run time optimization are performed. Compile time optimization seeks to identify a good order 
for processing the high level data manipulation operations on fragment groups without binding operations 
and copies to sites. This is because the choice of which copy of a frag- ment group to use for processing 
a transaction can- not be made until the availability of sites at run time is known. By dividing the 
optimization into two stages, DDM maximizes the amount of preanalysis done at compile time while ensuring 
the validity and optimality of the generated access plans. DDM's concurrency control mechanisms are 
exten- sions of those used in LDM. Again~ a multi-version mechanism is used to eliminate conflicts between 
read-only and update transactions [CG85]. In addi- tion to improving parallelism~ this mechanism greatly 
facilitates the taking of global check- points. Such a checkpoint may be necessary if one wants to reset 
a distributed database to a previous globally consistent state after the log data in one or more sites 
is damaged. With respect to replica control, DDM provides a balance between synchroni- zation overhead 
and failure resiliency. Essen- tially, updates are propagated to online copies synchronously. Offline 
copies are only updated in a background hatched fashion. Because DDM is designed for distributed command 
and control applications, survivability is a very important issue. A special transaction commit algorithm 
is used to ensure that distributed tran- sactions are terminated in a timely fashion, even in the presence 
of site failures, so that resources at the remaining operational sites can be fully utilized (without 
being tied down by incomplete transactions). DDM is designed to recover automat- ically from total failures 
wherein all of the sites coordinating a transaction or all of the sites storing replicated copies of 
a fragment group fail simultaneously. Previous systems have treated such failures as catastrophes and 
required human inter- vention for recovery. In order to speed up the availability of data at a recovering 
site, DDM employs an incremental site recovery strategy. Essentially, the fragment groups stored at the 
recovering site are prioritized and brought up to date one at a time (with the assistance of other replication 
sites). As soon as a fragment group is brought online, it can be used for processing new transactions 
without having to wait for the recovery of other fragment groups. MULTIBASE Multibase is designed to 
provide a logically integrated, retrieval-only, user interface to a physically nonintegrated environment 
containing pre-existing databases. These databases may reside on different types of database management 
systems,  at different physical locations, and on different types of hardware. Before local databases 
can be accessed through Multibase, the local host systems must be connected to a communications network. 
This network can be local or geographically distributed. After Multi- base has been eonnected to the 
same communications network, a global user can access data in the local databases through Multlbase using 
a single query language. Each local site maintains autonomy for local database updates. Local applications 
can continue to operate using the existing local inter- faces, as before. Multibase presents the end 
user or application program with the illusion of a sing/e, integrated, non-distributed database. Specifically, 
Multibase assumes the following responsibilities:  providing a global and consistent picture of the 
available data,  knowing the locations for the database items,  transforming a query expressed in 
the global query language into a set of subqueries expressed in the different languages supported by 
the target systems,  formulating an efficient plan for executing a sequence of subqueries and data 
movement steps,  implementing an efficient plan for accessing the data at a single target site,  
* moving the results of the subqueries among the sites,  resolving incompatibilities between the data- 
bases (such as difference in naming conventions and data types),  resolving inconsistencies in copies 
of the same information that are stored in different data- bases, and  combining the retrieved data 
to correctly answer the original request.  Multibase has three key design objectives: gen- erality, 
compatibility, and extensibility. To satisfy the first objective, Multibase has been designed to be a 
general tool, capable of providing integrated access to various database systems used for different applications. 
Multibase has not been engineered to be an interface for a specific appli- cation area. The second requirement 
of Multibase is that it co-exists and be compatible with exist- ing database systems and applications. 
No changes or modifications to local databases, DBMSs, or application programs are necessary to interface 
Multibase with systems already in operation. The local sites retain full autonomy for maintaining the 
databases. All local access and application programs can continue to operate without change under Multlbase. 
The third design objective is that it must be relatively easy to couple a new local system into an existing 
Multibase configura- tion. All these objectives are achieved by designing a modular architecture for 
Multibase and by making the system largely "description driven" [LR82]. Multibase's modular architecture 
isolates those parts of the system that deal with specific aspects of a local system. Because of this, 
a Multlbase configuration can be expanded to include a new DBMS in a short period of time and with little 
impact on the existing Multibase software. Descriptions are used throughout Multibase to tailor general 
modules for specific applications, users, and databases. These descriptions are written by the database 
administrator(s) who is responsible for tailoring a Multibase configuration. The component architecture 
of MUl tibase is illustrated in Figure 9. There are two types of modules: a global data manager (GDM) 
and a local DAPLEX Global Query ~ ~ Result I Global Data Manager I (GDM) Is j/ D,.<Ex \ % // ~ Single 
Site Query ~ "~",~""'h  Interface I Interface N I-o-. 1-o-, I (LDI) (LOl) Data~ ~ Local Query l ~Data 
I I Figure 9. Multibase Component Architecture  database interface (LDI). All global aspects of a query 
are handled by the GDM. All specific aspects of a local system are handled by an LDI. There is one LDI 
for each local host DBMS accessed by Multi- base. The GDM makes use of LDM as an internal DBMS to manage 
its workspace. The LDM is used to store the results of the Daplex single-site queries which are processed 
by the LDIs and to perform all the required steps of the final query for combining and formatting the 
data. We like to point out that Multibase does not provide the capability to update data in the local 
databases or to synchronize read operations across several sites. This is because implementing global 
concurrency control mechanisms for read or update operations would have necessitated the global pro- 
cess to request and control specific resources offered by the local systems (i.e., locking local database 
items) as required to ensure consistency across the databases. However, most systems do not make available 
to an external process the services necessary to implement global concurrency control. Since Multibase 
is designed to operate without requiring modifications to existing systems, the tools necessary to ensure 
consistency across data- bases are not globally available. Thus, autonomy of database update is maintained 
locally, and Mul- tibase provides the global user with the same level of data consistency that the local 
host DBMSs pro- vide to each local database user. In addition to the highly modular and descrip- tion 
driven architecture, the design of Multlbase has required research in the areas of schema integration, 
global query optimization, and local querTy optimization. Our results in each of these areas have been 
reported in [KG81, DAYA84a], [DAYA83b, GY84, DAYA84b], and [DG82] respectively. STATUS Designs of the 
Daplex and Adaplex languages are complete. Prototype versions of Multibase and LDM which support most 
of the described capabilities have been implemented. Implementation of DDM is well underway. To date, 
the systems contain approximately 500,000 lines of Ada source code. Most of the implementation was done 
in an Ada- subset using an Ada-to-Pascal translator [SOFT81]. We are currently converting the systems 
to full Ada using the DEC VAX Ada compiler [DEC85]. The ini- tial target environment for all three systems 
is VAX VMS. The current systems support an interac- tive version of Adaplex (i.e., Daplex). Implemen- 
tation of the preprocessor necessary to provide the Adaplex interface is expected to begin in the near 
future. An intensive evaluation and tuning phase will also commense once we have completed the conversion 
process. REFERENCES <RefA>[CDFG83b] A. Chan, U. Dayal, S. Fox, N. Goodman, D. Hies, D. Skeen. "Overview 
of an Ada Compatible Distributed Database Manager." ACM SIGMOD Conference yroceedin~s, 1983. [CDFL82] 
A. Chan, S. Fox, S. Danberg, W. Lin, A. Nori, D. Hies. "Storage and Access Structures to Support a Semantic 
Data Model." ~ Confer- ence p~y_ggfig~gg, 1982. [CDFR83] A. Chan, U. Dayal, S. Fox, D. Ries. "Sup- porting 
a Semantic Data Model in a Distributed Database System." ~ Conference ~y_gggg~j~g~, 1983. [CFLN82] A. 
Chan, S. Fox, W. Lin, A. Nori, D. Hies. "The Implementation of an Integrated Concurrency Control and 
Recovery Scheme." ACM SIGMOD Confer- ence Proce@ding, 1982. [CFLR81] A. Chan, S. Fox, W. Lin, D. Hies. 
"The Design of an Ada Compatible Local Database Manager." Technical Report CCA-81-09, Computer Corporation 
of America. [CG85] A. Chan, R. Grsy. "Implementing Distributed Read-only Transactions." To appear inn 
Tran- sactions ~]~ ~ ~g~g~J~.~Y.~AR~, Vol. SE-11, No. I, February 1985. [DATE81] C. Date. "Referential 
Integrity." ]~ Conference Proceedin~s~ 1981. [DATE84] C. Date. ~ Guide .~g ~, Addison Wesley, 1984 
[DAYA83a] U. Dayal. "Processing Queries with Quan- tlflers: A Horticultural Approach." ACM PODS Conference 
Proceedinzs~ 1983. [DAYA83b] U. Dayal. "Processing Queries over Gen- eralization Hierarchies in a Multldatabase 
Sys- tem.".~~ Proceedings, 1983. [DAYA84a] U. Dayal, H Hwang. "View Definition and Generalization for'Database 
Integratio n in Mul- tibase: A System for Heterogeneous Distributed Databases." ~ ~Y_~lgggJ~.~g ~ Software 
~gi/ig~ES4Rg, Vol. SE-10, No. 4, November 1984. [DAYA84b] U. Dayal. "Query Processing in a Multida- 
tabase System." in Ouerv ~/.~ggE~i~Ig.~ Databa~ ~y_~, (W. Elm, D. Batory, D. Reiner, edi- tors), Springer 
Verlag, 1984. [DEC85] Digital Equipment Corporation. "Developing Ada Programs on VAX/VMS." ~985. [DG82] 
U. Dayal, N. Goodman. "Query Optimization for CODASYL Database Systems." ~ Conference Proceedi~a, 1982. 
 [GSCD83] N. Goo~am, D. Skeen, A. Chan, U. Dayal, S. Fox, D. Hies. "A Recovery Algorithm for a Distributed 
Database Management System." ~Q~ PODS Conference Proceedln~s, 1983.  [GY84] D. Goldhirsh, L. Yedwab. 
"Processing Read- Only Queries over Views with Generalization." Conference Prooeedin~s~ 1984. [HD84] 
H. Hwang, U. Dayal. "Using SemiouterJoins to Process Queries in a Multidabase System." ACM PODS ~onference~y.R.qfig~4~g~, 
1984. [HTVN81] J. Holland, K. Tai, M. Van Name. "An Ada Relational Database Interface Using Abstract 
Data Types." TH 81-07, North Carolina State University, 1981. [KG81] R. Katz, N. Goodman. "View Processing 
in Multibase --A Heterogeneous Database System." in Entitv-Relationshin APProach .~R ~_QP_~J~4Rn Modelling 
~ .~, (P. Chen, editor), ER Institute, Saugus, CA, 1981.  [LARSg0] Larson, P., "Linear Hashing with 
Partial Expansions," ~ Conference Proceedimzs, 1980. [LR82] T. Landers, R. Rosenberg. "An Overview of 
Multlbase." in Distributed Databases, (H. Schneider, editor), North Holland Publishing Company, 1982. 
 [NOKI83] Nokia Data Terminal Systems. "MPS 10 Database Management System Functional Descrip- tion." 
Version 1.0, June 1983. [RCDF82] D. Ries, A. Chart, U. Dayal, S. Fox, W. Lin, L. Yedwab. "Decompilatlon 
and Optimization of Adaplex: A Procedural Database Language." Technical Report, Computer Corporation 
of Amer- ica, 1982. [SCDF85] J. Smith, A. Chan, S. Danberg, S. Fox, A. Nori. "A Tool Kit for Database 
Programming in Ada." To appear in ~ Ada Conference ~/_9_~_~lllg~, 1985. [SFL83] J. Smith, S. Fox, T. 
Landers. "Adaplex: Rationale and Reference Manual." Technical Report, Computer Corporation of America, 
1983.  [SLRR84] S. Fox, T. Landers~ D. Ries, R. Rosenberg. "Daplex User's Manual." Technical Report 
CCA- 84-01, Computer Corporation of America, March 1 984. [SOFT81 ] SofTech, Inc. "Interim Ada- to-Pascal 
Translation Tool Language Reference Manual." TP 124, September 7981. [SWK76] M. Stonebraker, E. Wong, 
P. Kreps. "The Design and Implementation of INGRES." ACM Tran- sactions ~ll Database ~y_~_~_, Vol. I, 
No. 3, September 1 976. [VINE83] D. Vines, Jr. "An Interface to an Existing DBMS from Ada (IDA)." GTE 
Network Systems, 1983.</RefA>
			
