
 A NEURAL NETWORK FOR SPEEDY TRIALS Ray IL Hashemil,2, There~ M. Schaferl, William G. Hinson 2, and John 
R. Talburt3 1Department of Computer and Information Science University of Arkansas at Little Rock 2801 
S. University Ave. Little Rock, AR. 72204 E-mail:nnhashemi@ualr.edu 2National Center for Toxicological 
~h Jefferson, AR. 72079 3Acxiom Ccapmation 301 Industrial Blvd. Conway, AR. 72032-7103 Key words: Computerized 
sentencing, Speedy trial, Computer in courts, Neural Networks, Intelligent Systems. AIk IBAC In recent 
years, the case loads of judges have increased, while speedy trial laws place a time limit between the 
defendant's arrest and trial dates. Because of this time constraint, it seems that for m eases, judges 
pass sentences based on a set of certain factors (paum~) not based on the iudividual merRs of each case. 
Patteras may be learned by a neural network. In this paper, we investigate the credibility of the neural 
netw(sk aPlmmch as a viable tool in the sentencing process and we show its superimity oyez the ID3 approach. 
INTRODUCTION The goal of circuit judges is to serve justice in a timely manner to the defen0a_ ngs assigned 
to {heir Â¢otlat. In leCent years, however, the case load of judges has inereased, while speedy trial 
laws place a time limit between the defendant's arrest and trial dates. In 1993 alone, the number of 
felony cases, for example, filed in the State of Arkansas increased by 2356 cases over the 31,656 felonies 
fdedin 1992. Tbe nmber of cases the judges were able to terminate only increased by 268 over the 1992 
[1, 5] level. The docket backlog increases, while mote new cases are added into the system. Because of 
time conslraints for having a speedy trial, it is not known if judges passed sen~ based on the individual 
merits of eac, h case ~ based on a set of certain factors on mini3f cases. The latter seems to be exercised 
based on our initial review of 246 minor cases. This means that patte~s exist among sentences passed 
by circuit court judges. Since the artificial neural network (ANN) technolo~ leads itself easily to learn 
petterm, it may be used in conrts for handling mimx cases. In thi~ paper, we investigate the credibility 
of the neural network approach as a viable tool in the sentencing process. To do so, we train an ANN 
by the backpropagatim paradigm, and tbea test its prediction capability by applying the testing data 
set against it. We also compare the lxediction power of the neural network with another classification 
technique called ID3. ID3 techaique is a decision tree which is built using the same trainingset and 
is tested asainst tbe same testing dataset. A brief background about neural networks amt ID3 is wovided 
in Section 2. Section 3 covec~ the analysis of the ~ data. In section 4 the deveiopamat of the artificial 
neural network used in this investigation is discussed. The remits and fuuut research are covered in 
secttoas 5 and 6, respecevciy. BACKGROUND A brief description of a neural network and the bmckpropagation 
training paradigm along with the ID3 classification approach is presented here. The details regm'ding 
neural new.ks and l~,.klm:q~l,mi~ my be found in [ 3, 2] and detaiis of ID3 may be found in [st]. Neural 
metworb A neural netwot~ is ~ of a set of ~ neuron (nodes) grouped in a umber of layers. The first and 
the last layers within a neural net are called input and output layers, respectively. ~ inner layers 
are known as hidden layers. The output of each neuron in a given layer (except the output layer) is fed 
as an input to every attifieial nemou of the "next layer. The input to the input layer is not the output 
of any layer. Figure 1 shows a three-layer "network. Input ~ OutputLayer L=y~ Layer Figure 1: A three-layer 
network Node Hi in layer Ln has four lm3perties. These properties are an input vector, Ii = [il ..... 
ik], an output, ai, an activation function L and a training rate Ixi. The input vector mimics the signals 
received by the neuron Ni from all the neurons (k neurons) in the previous layer. To each element of 
input vectm" a weight is associated, which collectively makes a weight vector, Vi = [Vl ..... Vk]. The 
weight vector mimics the strengths of synoptic connections between the neuron Ni and the other neurons. 
The inner product of Ii.Vi = s = ~ ij vj (for j = 1 ..... k) represents the total weighted inputs (signals) 
received by node Hi. The activation function f determines the level of excitation for the node Ni. The 
activation functiou is the same fog all tbe nodes of a neural netwogg. "['ne output a i ai ffi f(s) = 
(Eq. 1) f(Ii.Vi). When the activation function is a sigmoid function: 1 l+e "s The training rate is 
a coegficient ( 0 < ~ < 1) used to train the node Ni and may be the same fog all the nodes within a neural 
network. Fog the nodes N1 ..... Nj in layer I4 the weight vectors of V 1 ..... Vj form a matrix of J 
COl,nnng and K rows, W n. If vecto~ In is the input to the layer Ln then, the output for the layer Ln 
is a vectog An = [al, ..., ai] and is calculated as: An =Y(ln.Wn ) (Eq.2 __) If the neural network is 
composed of L1 ..... L m layers, then the output for" each layer is calculated based on (2). The output 
of layer L i is fed as input to the next layer, Li+ 1. This process continues until the final output 
vector is im3duced. The process of taking un input und sending it through all the layers of a neural 
net to generate the final output vector is geferted to as a forward pass . A neural net may be trained 
using a training pmadism to genera~ the desired output vectog fog each individual input vector of a tminmg 
dataset. The training paradigm of our interest is bactpropagationin which, fog a given input vector, 
the output vector is generated in a forward pass. Then the difference between the output vectog and the 
desired output vector is backpropagated through the network (from the output layer to the input layer) 
to modify the weight matrices for the entire net. This process is refened as a reverse pass. In the reverse 
pass, the training of the network takes place. By training we mean that all the weights in weight matrices 
will be modified based ou the A rule [2]. Fog e~am~e, the weight vj associated to input ij fog the node 
Nj in the ~lillllitJaX~is chansed to a new weight v~, v'] - vj + Aj (Eq. 3). The value of &#38;j is calculated 
using &#38;j = ttj 8j ij (Eq. 4), where $tj is the training rate fog the node Nj, 6j is calculated as 
asj (tj - aj ) = aj (1-aj) ( tj-aj ) (5) in which y is the activation function, s is the sum of the weighted 
input, aj" is the output, and tj is the desired output fog node Nj, aiKI ij is am input to the Node Nj 
and the weight assoeiated to this input will be modified. When 8j is less than or equal to a tolerance 
level, then node Nj has learned the iqmt pattern. We also ~ tlg weight mauices related to tbe inl~ut 
vectogs of the nodes in hidden laye~ based on (Eq. 3) and (Eq. 4). Since a desired output is not known 
fog a given node in an inner layer, then 8 fog the node is calculated differently, as follows: Let Mk 
be a node in an inner layer (the layer immediately before the output layer). Let the output of Mk be 
amk, which is fed as input to all h nodes of the output layer. Let the weights associated with these 
h inputs m~tf, up the weight vector G- [81, ..., gh]. Let the d's calculated for the h nodes of the output 
layer mAt'~ up the vector D=[B1 ..... &#38;el. The inneg Weduct of G and D is : h GD = a = ~ gi 8i (Eq. 
5) and i=l  ~mk=amk(1-amDa. (Eq. 6) As is shown above, the 8's for the nodes in inner layer Lj are calculated 
based on the 8's fog the nodes in layer Lj+I. After a neural net is trained, it is te.qed agaiast the 
~ of a testing dataset, which have not been seea by the net. For these records, the desired output is 
known. The output generated for each ~ of this dataset is checked against the desired output far that 
~ ff there is a match, then it is concluded the trained neural network caa recognize (classify) the 
record correctly. ID3 In this approach, a decision tree is built based on the records of a trainin$ set. 
Let the fields (attribetes) w~thin a record of the 13"aining set be fl, .... fn, fn+l. The figst n attributes 
are referred to as condition attributes, and the last attribete is referred to as the decision (classification) 
amamte. This meres, based on the condition amibutes of a given record, a classification is given to that 
recot~ Let the possible values for the attribute fi be the vect~ Vi = {Vl ..... Vk}. All the condition 
attributes ate a ~!e for serving as the rcot node of the decision tree. Let fi be selected as the root 
node. The number of lnnches emana_ted from the mot node is equal to K (number of possible values for 
the attribute fi). As a result, the records of the tr~inin s set are gripped into K nodes (N 1, .... 
Nk) based on their vahie in field fi. If all the records in node Nj have the same value for their classification 
attribute (fn+l), men node Nj is ueated as a ~af node in the decision ue.e. If this is not the case, 
we choose another attribute, fm, (fro is different from fi) from the condition attributes of the records 
in Nj, to further separate the reamis of this node by expanding the decision tree through r~q)eating 
the above wocess. In the final decision tree, all the records of a given leaf node have the identical 
classification value. The goal is to build a shorter decision tree by selecting proper condition attributes 
to serve as mot node and other intermediate nodes. To do so, the selection ~ udw~s place in two steps. 
In step one, the entropy for all candidate attributes of a given node is calculated [3]. Let the entropy 
for the attributes of fl ..... fn be E1 ..... En, respectively. In step two, we select the attribute 
fj, as tue nod~ for which Ej ffi MAX(El ..... En). After the decision tree is built, it is tested against 
the testing dstaseL The attribute of a testing datatset's reo3~ which is the same attribute as the attribute 
used fee the decision tree's mot, is identified. Based on the value that this atUibute carries, the record 
will follow one of the Ix'anches emanated from the root to reach tbe next node. For the next node in 
the tree the same wocess will be repeated, until tbe rec~d reaches one of the leaf nodes. If the value 
for the classification attn%ute of the record is the same as the classification value of the recmds in 
the leaf node, then the record is classified correctly; otherwise, there is a mL~-classificatim of the 
record. COURT DATA ANALYSIS Any corot case is smmmxled by a set of facts and a set of artifacts. For 
example, in a case that a husband is murdered by his wife, the facts are the husband was mm-de~ at a 
certain time, with a cea~n assault weapon, by a o~gtain person, etC. Tl~ aglifa~ are the wife ~ a Im3ken 
leg and a Im3ken rib three yem's ago resuhing from her husband's abuse. The artifacts play a major role 
in convicting a suspect in the court of law, and lawyeN are trying to use them generously in the court 
to weaken or suengtben a case presented to the jury. ldeatification and then generalization of the artifacts 
for all real life cases are impossible. "l'ne artifacts about a case greatly decrease if a jury is not 
involved. The artifact decrea~s even mete if the case is a minor" one. To minimize the artifact, we limited 
ourselves to the minor cases in which a jury was not involved. In addition, we limited ourselves to first 
time offenders to further eliminate tbe artifacts smronnding a case. The minor Case we chose to study 
was "burglary". Each ~ of this data is composed of two sets of attributes, the condition set and the 
decision set. Seven attributes regarding the defendant and judge constitute the condition set. These 
seven attribmes are: (1)the nember of years the judge has been serving on the bench; (2)the percentage 
of criminal cases to total cases heard; (3)the race of the judge; (4)the defendant's race; (5)tbe defendant's 
abe; (6)the defendam's sex; and (7)the defet~tant's plea. Since all judges in our dataset were male, 
the sex of the judge was excluded from the dataseL F~ all attributes of the condition set, the possible 
values and the meaning of each value were given in Table 1. Valm Attri~tm Valuta No. etYmm J~le. S.osd 
On Bm~h I 2 3 < 2yem~ 9-10 yi~s > lOymm Total Cams 1 2 3 <2r~ 2r~-76% >71~, se.~ ~t I 2 3 4 5 6 <21 2/-262S~0 
31-86 M,.40 >40 Rsmef a.de. I 2 C~ BIm~ Ram or' I 2 CmKmian BIm~ 8,ad 1 2 Flmall Ptm Fmtwsd I 2 $ Table 
I: Possible values and their meanings for the attributes of the condition set. The decision set is composed 
of only one amibute: the sentence given by the judge. Once a defendant is convicted, the sentence can 
consist of any c~nblnation of j~ time, pet ~an of tbej~ time saspmded, pmbatio~ fine, and restitution 
to the victim. The majority of the court cases fell into one of three categcgie.s: jail time sea-red, 
the entire jail time suspended and the defendant given a fine and probation, or the defendant given only 
a fine and restitution. The above categ~,s were~ted as the decisions 1, 2, and 3, respectively. Our study 
was limited to these three outcomes. In the court dataset, there were records, for which the values of 
condition attributes were identical but the decision values were different. These conflicting records 
were identified and removed from the dataset. As a result, the dataset contained conflict flee records. 
The number of records for the decision values of 1, 2, and 3 were 126, 79, and 41, respectively. Of the 
records in the data set, ten percent of records (chosen randomly) for each decision value were set aside 
for testing. As a result, our testing set had 25 records: thirteen with decision 1, eight with decision 
2, and four with decision 3. Among the remaining records in the dataset, the number of records with decision 
3 was the smallest, 37. ~fore, we randamly collected 37 records for decision values of 1 and 2 to make 
our training dataset along with the 37 records for decision 3, the total of 111 records. Having the same 
number of records for every decision value avoids training the system in favor of one decision value, 
thus making the network unbiased. TOPOLOGY OF THE NETWORK Since the "linear separable" constraint limits 
the representation ability of a two layer network, we started with a three layer netwm~ The choice of 
the number of nodes in each layer was crucial. Too many nodes can result in the network memorizing, not 
learning, the training set. At the same time, the system cannot learn if there me insufficient nodes. 
The number of nodes for the input layer is dictated by the input data representation. Since the numbeg 
of attributes in the condition set was seven, the input layer had sevm nodes. The number of nodes in 
the output layer was lluee from which the nodes 1, 2, and 3 were fired for the decision valuesof I, 2, 
and 3, respectively. In order to determine the nmber of nodes in the hidden layer,we used a trial and 
mor approach. We stmed with 6 nodes in the hidden layer. The net was trained by the training dataset 
and was tested against the testing dataset. The same process was repeated while the number of nodes in 
the bidden layer was increased by one. We stoped at the 14 nodes for the hidden layer because testing 
was deteriorating. Throughout the trial and error wocess we chose the sigmoid function as the activation 
function because our studywas concerned with lem~sing from avea~ges and not the exceptions. The results 
obtained from the trial and error process revealed that the best ntmaber of nodes for the hidden layer 
was 14. Under this configuration, slightly moce than 50% of the records of the testing dataset were classified 
correcdy. This classification was not satisfacte~. To improve the Classifgationof the network, a second 
hidden layer was added to the net. Again, the umber of nodes for these two.hidden layers were detemmined 
using the trial and error approach as follows, For each selected umber of nedes (in the range of 6 through 
14) for the first hidden layer, we tried a several different number of nodes (6 throngh 15) for the sexomi 
hidden layer one at a time. Table 2 shows the best number of nodes in the second hidden layer for every 
selected number of nodes for the  first hidden la~,er. , erNodlmim 12q~ bmtm. *fNedm am~J~ ~i~d I 8 
6 m ]0 f 0 ,IS m 8 W m 9 D It M 1o gO m 1.1 15 lJ m 14 I I0 i ii Table 2. Tbe results of trial and 
~'or Wocess for detemmining the nember Of nodes in both hidden layers in a four-layer netual netw{wk. 
The correct classification of the testing dataset by a 4- layer neural net (68%) was improved over the 
classifr.ation of the same dataset by a three-layer neural network (52%). However, the results were still 
not satisfactory. To further improve the classification of the testing ~t~t, we applied several remedies. 
We changed the range of initial random weights from (0 tlueugh 1) to (-0.5 through +0.5). In addition, 
a momenuma was used in the weight changes of the netwmk, which caused little training improvement, and 
was detrimental when the nmmentum coemcient exceeded 03. Mm~x~ver, a nenmn bias was added to every node 
Of all the layms (except nedes in the input layer). Neuron bias was used to force changes in weight values, 
when the input to the related node was zero. All of these remedies made slight improvement in the classifr~en 
Of the testing dataset. As a last resort, to improve the ~ classification of the dataset, we turned 
to bit patterns for representing input data. Attributes with mote than two possible values needed to 
be represented by more than one node. For instance, the age of the defendant could have six possible 
values, from oBe to six. The minimqm of three bits were needed to represent all the possible values of 
the defendant's age. This means that three nodes in the input layer were needed to accept the defendanCs 
abe value. The values of the algribotes "No. of Yeats Judge Served On Bench', "Percentage Criminal to 
Total Cases', aud "Plen Entered" were reWer~aCed by two bits and the values for the rest of the attributes 
were ~ted oely by one node. Using binary representation of values in a given input vector, the number 
of input nodes (one node per bi0 was increased to twelve. Table 3 shows The results of and ergof II'o~g 
fog ~ining t~ nwllbeg of nodes in both bidden layers of a fom'-layef neural network . -.. . . rep data 
in No~ No. ~ No. e( No Noof Nodmin Nodmin Nedm ia lk~rd~ ~m~ the lnlMat tl~ Pir~ Trs/ni~ mt T~t~l l~yw 
l.H&#38;]m ~w~bY t~ m'~ Layw Layw 12 6 8 8O 8 12 7 8 43 8 12 8 10 67 IB 10 W 18 12 10 6 104 21 12 11 
7 98 19 8 W 19 7 9? 17 !o 14 8 97 17 Table 3. The results of trial and error wocessing for determining 
the number of nodes in both hidden layers of a four-layer neural network representing the input data 
in bin~ W. SUMbIARY AND RF~ULTS The best configuration for our uctwork was composed of four layers. 
The input layer, the first hidden layer, the second hidden layer, and the output layers, which had 12, 
10, 6, and 3 nodes, respectively. The net used the sigmoid function as its activation fuuction. Our training 
data set had a total of 111 records. "l~e testing data set h~_ 25 records. The number of records for 
each decision in the testing set were not the same and they had the same proportion as they had in the 
original court data. The network learned 94% of the records in the training set and gave the correct 
decision for 84% of the records in the testingset. The detailed results for our best configuration _ 
are illustrated in Table 4. To show the significance of the result&#38; the same data was analyzed using 
the ID3 ~. This ~ was only able to correctly identify 17 out of 25 re, o~ds in the training set, 68%. 
The comparisoa between the two methods was also shown in Table 4. The neural network was coded in C ++ 
and was implemented on a VAX/VMS timeshanag envirmment. The ID3 ap~oach Was coded in C aBd was imnleineilted 
on the same platform. CONCLUSION AND ~ RESEARCH The results in Table 4 revealed that the use of a neural 
network for a computerized sentencing system is l~3mising. Such a system coupled with a human ex~ (a 
judge) will inerease the thronshput of a court. Also, the results revealed that there is a long way to 
reach to the point that such a system be used as a stand alone system because the 84% correct classification 
of any testing dataset is less than ~rfect for the court of law. ~sa ~.ea~sa No.e/' No.of N~of No.e( 
 Mothod Deeiaoe lt,m~b lk~nl, Total lt,msq, iI,=edsNot Toes Tmimd Cot q~laed P.*cqmmd 8 ~t 11 2 18 1 
(t'/%) (~t) (e6~t) (lr~t) 2 ~t 8 ~ 8 2 8 Newal (te~t) (tMt) (78~t) (2Set) Not M 1 ~ 4 0 4 $ (v~)) (8~) 
Ooo~) (o~) 10t 7 111 Zl 4 S 'Ibtal (04qt) (8~) (84qt) (10q~) ll| ~t 8 ~ 11 $ 13 1 (nqt) (~t) (uct) (l~t) 
u~ 1 ~ff 1 $ 4 (W~) (~,) (~) (7~) Total 10t 7 in 17 8 ~x (~,) (~) (~) (~) Table 4. The detailed results 
for the neural net and ID3. Regarding the court data, four issues are under investigation. These issues 
are having (a) data for judges of different sex, (b) combined data for different minor cases, (c) a reco~ 
o( the defendant's lxevioes ccmvictions, and (d) a much larger court datasec Also, to perfuct the system, 
the process of using such an automated sentencing in parallel with a judge sentencing is under conskleratim. 
  CKNOWL DG  The first antl~x (RIO had appointment to the Oak Ridge Institute for Science and Education 
(ORISE) at the National Center for Toxicological Research at the time that this paper was prepm~ REFERENCES 
<RefA> 1. <SinRef><author>~nnn~l</author> <title>Rel~ft of the jU4r~n.~.~ Jtulieiarv Fi.v.t~_al Year</title>.<institution> Administrative Office of the Courts of 
Arkansas</institution>, <location>Little Rock</location>, AI~ <date>1993 </date></SinRef>2. <SinRef><author>C_~ S., </author>"<title>Classical and Instrumental Learning by Neural Networks</title>", 
<booktitle>Pgo?xess in Theoretical Biology</booktitle>. Vol <volume>3</volume>, pp. <pages>51-141</pages>, <location>New York</location>, <publisher>Academic Press</publisher>, <date>1974</date></SinRef>. 3. <SinRef><author>Hertz J., </author><author>Krogh 
A., </author>and<author> l~imef R., </author>"<title>Introdect/on to the Theory of Neural Computation</title>", L f ~ Volume in The Santa Fe Institute 
Studies In <booktitle>The Sciences of Comnlexitv</booktitle>. Vol <volume>I</volume>, <publisher>Addison Wesley</publisher>, <date>1991</date></SinRef>. 4. <SinRef><author>Quinlan R. J., </author>"<title>Discovering Rules 
by Inductioa Fnan Large Collections of Examples'</title>, <journal>Expert Sy~,ym~ in the Mioro-Electrenic Age</journal>, <location>Fxlinhurg </location>
<publisher>University Press</publisher>. Edinbur8, pp <pages>168-201</pages>, <date>1979</date></SinRef>.  5. <SinRef><author>St~tigtie~! </author><title>Snnnlement to the 1992-1993 Annual Renort 
of the Ark~ng</title>.~<location><institution> J,uliei~'v Adi~ve Office of the Courts of Arkansas</institution></location>,<location> LiWe Rock, AR</location>., <date>1993</date></SinRef>. 6. <SinRef><author>Wasserman 
P., N</author>&#38;#169;<author>ur~i Comnutine</author>. <title>Theory and Prattle</title>. <journal>Van Noswmd ReinhoM</journal>, pp <pages>43-59</pages>, <date>1989</date></SinRef></RefA>.  472  
			
