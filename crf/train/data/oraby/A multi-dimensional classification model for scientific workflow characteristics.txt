
 A Multi-Dimensional Classi.cation Model for Scienti.c Work.ow Characteristics Lavanya Ramakrishnan 
Lawrence Berkeley National Lab Berkeley, CA lramakrishnan@lbl.gov ABSTRACT Work.owshavebeenused to modelrepeatabletasksoroper­ations 
in manufacturing, business process, and software. In recent years, work.ows are increasingly used for 
orchestra­tionof sciencediscovery tasks thatusedistributedresources and web services environments through 
resource models such as grid and cloud computing. Work.ows have disparate re­quirements and constraints 
that a.ects how they might be managed in distributed environments. In this paper, we present a multi-dimensional 
classi.cation model illustrated bywork.ow examples obtained through a survey of scientists fromdi.erentdomains 
includingbioinformaticsandbiomed­ical, weather and ocean modeling, astronomy detailing their data and 
computational requirements. The survey results and classi.cation model contribute to the high level under­standing 
of scienti.c work.ows. Categories and Subject Descriptors A.1[Introductory]: Survey General Terms Design 
 Keywords Cloud, Grid, Scienti.c Work.ows 1. INTRODUCTION Work.owsandwork.owconceptshavebeenusedto 
model a repeatable sequence of tasks or operations in di.erent do­mains includingthe scheduling of manufacturing 
operations, businessprocess management[Taylor etal.2006],inventory management, etc. The advent of internet 
and web services has seen the adoption of work.ows as an integral component of cyberinfrastructure for 
scienti.c experiments [Deelman and Gil 2006, Atkins 2002]. The availability of distributed Permission 
to make digital or hard copies of all or part of this work for personal or classroom use is granted without 
fee provided that copies are not made or distributed for pro.t or commercial advantage and that copies 
bear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on 
servers or to redistribute to lists, requires prior speci.c permission and/or a fee. WANDS 2010 Indianapolis, 
Indiana USA Copyright 2010 ACM 978-1-4503-0188-6 ...$10.00. Beth Plale Indiana University Bloomington, 
IN plale@cs.indiana.edu  resourcesthroughgridand cloud computing modelshas en­abledusers tosharedataand 
resourcesusing work.ow tools and other user interfaces such as portals. Work.ow tools are in use today 
in various cyberinfras­tructureprojectstosatisfy theneedsofaspeci.cscience problem [Deelman et al. 2003, 
Altintas et al. 2004, Con­dor DAGMan , Taylor et al. 2004, Oinn et al. 2006]. This has resulted in innovative 
solutions for work.ow planning, resource management, provenance generation, etc. Each scienti.c work.ow 
has di.erent resource require­ments and constraints associated with them. Work.ows can vary in their 
size, resource requirements, constraints, amount of user intervention, etc. For example, applica­tion 
work.ows with stringent timeliness constraints such as for weather prediction or economic forecasting 
are now in­creasingly run in distributed resource environments. These work.ows require a clear understanding 
of their work.ow requirements to manage user s deadline constraint with the variability of underlying 
resources. Additionally the par­allelism of the work.ow and its storage needs often a.ect design choices. 
However, there is a limited description and understanding of usage, performance and characteristics of 
scienti.c work.ows. This is a major road block to reuse of existing technologies and techniques and innovation 
of new work.ow approaches. Inthispaper,wepresent aqualitativeclassi.cation model of work.ow characteristics. 
These characteristics help clas­sify or bin work.ow types enabling broader engagement and applicability 
of solutions. We discuss work.ow exam­ples fromdi.erentdomains:bioinformaticsandbiomedicine, weather 
and ocean modeling, astronomy, etc and cast them in the context of our model. The work.ow examples have 
been obtained throughsurveyingdomain scientists and com­puter scientists who composed and/or run these 
work.ows. Each ofthesework.owshasbeen modeledusing oneof sev­eral work.ow tools and/or through scripts. 
For each work­.ow we specify the running time of applications and input and output data sizes associated 
with each task node. Run­ning time of applications and data sizes for a work.ow de­pend on a number of 
factors including user inputs, speci.c resource characteristics and run-time resource availability variations 
[Kramer and Ryan 2003]. Thus our numbers are approximate estimates for typical input data sets that are 
representativeof thegeneral characteristicsof thework.ow. The remainder of the paper is organized as 
follows. Sec­tion 2 discusses related work. Section 3 presents the classi­.cation model that is drawn 
from the survey of work.ows presented in Section 4. In section 5 we tie the model and survey together 
with a taxonomy. The paper concludes in Section 6. 2. RELATED WORK New work.ow tools have been developed 
to represent and run scienti.c processes in a distributed grid environment. Work.ow tools such as Kepler 
[Altintas et al. 2004, Lud­scher et al. 2005], Taverna [Oinn et al. 2006], Pegasus [Deel­man etal.2003, 
Deelman etal.2003],Triana[Churches etal. 2006] allow users to compose tasks (i.e., analysis, modeling, 
synthesis, mapreduce, and data-driven) and services into a logical sequence. These toolsoftenaredevelopedinthecon­text 
of one or more speci.c application domains and have various features to allow users to compose and interact 
with work.ows through a graphical interface, provides seamless access to distributed data, resources 
and web services. Yu and Buyya provide a taxonomy for scienti.c work.ow sys­tems that classify systems 
based on four elements of a grid work.ow systems -a) work.ow design, b) work.ow schedul­ing,c)faulttoleranceandd)data 
movement[YuandBuyya 2005]. Thain et al. characterize a collection of scienti.c batch-pipelined workloads 
on processing, memory, and I/O demands, and the sharing characteristics of the workloads. However there 
has been no detailed study of characteristics of complex scienti.c work.ows and representing a qualita­tive 
classi.cation model to capture the features of a work­.ow.  3. CLASSIFICATION MODEL Work.ows vary signi.cantly 
in their characteristics and their computational anddata requirements. Work.ows vary in their size, structure 
and usage. A number of the bioinfor­maticswork.owsoftenhave tasks that arebasedonquerying largedatabasesinorderofminutesforthetask 
execution. In other cases we see each of the tasks of the work.ow require computation time on the order 
of hours or days on mul­tiple processors. In some cases sub-parts of the work.ow might also present di.erent 
characteristics. It is critical to understand these characteristics to e.ectively manage these work.ows. 
We present a multi-dimensional work.ow char­acterization model that considers the following a) Size b) 
Resource Usage c) Structural Pattern d) Data Pattern and e) Usage Scenarios. 3.1 Size The size of the 
work.ow is an important characteristic since a work.ow might have from a small number of tasks tothousandsof 
tasks. Thesizeofthework.owsthat arede­ployedtodayin mostproduction environmentsarerelatively small. The 
largest work.ows in our survey contain about a couple of hundred independent tasks. The Avian Flu (Fig­ure 
11) and PanSTARRS(Figures 13 and 14) work.ows has over a thousand nodes but the computation at each node 
is expected to take only a few minutes to an hour. Scientists express a need to run larger sized work.ows 
but are often limited by available resources or work.ow tool features that might be needed to support 
such large-scale work.ows. In addition to the total number of tasks in a work.ow it is also important 
to consider the width and length ofthe work­.ows. The width of the work.ow (i.e. maximum number of parallel 
branches) determines the concurrency possible and the length of the work.ow characterizes the makespan 
(or turnaround time) of the work.ow. We observe that in our work.ow examples, the larger sized work.ows 
such as the Motif work.ow (Figure 8) and the astronomy work.ows (Figures13 and14) thewidthofthework.ow 
issigni.cantly larger than the length of the work.ow. Thus we de.ne three properties in the size dimension 
­ Total Number of Tasks. de.nes the total number of tasks in the work.ow.  Number of Parallel Tasks. 
de.nes the maximum number of parallel tasks in any part of the work.ow.  Longest Chain. de.nes the number 
of tasks in the longest chain of the work.ow.   3.2 Resource Usage In addition to the structure and 
pattern of a work.ow it is important to understand the computational requirements. In thepresented work.ow 
examples we observe that compu­tational time required by the work.ows can vary from a few seconds to 
several days. A number of the bioinformat­ics work.ows depend on querying large databases and have small 
compute times. Some examples include the Glimmer work.ow (Figure 6), Gene2Life (Figure 7), caDSR (Fig­ure 
12). Similarly the initial parts of the LEAD forecast work.ow(Figures1 and2) andtheLEADdata mining work­.ows 
(Figure 3) have small computational load. A number ofthework.ows includingthe forecastingpartsofthe LEAD 
work.ow,Pan-STARRS work.ows (Figures13 and14),SCOOP (Figure 4), SNS (Figure 15), Motif (Figure 8), NCFS 
(Fig­ure 5) have medium to large sized compute requirements. Max task processor width. is the maximum 
con­current numberofprocessors requiredbythework.ow.  Computation time. is the total computational time 
required by the work.ow.  Data Sizes. is the data size of the work.ow inputs, outputs and intermediate 
data products.  3.3 Structural Pattern Each work.ow might include one or more patterns. Our goal is 
to capture the dominant pattern seen in the work­.ow. The work.ows that we surveyed depict the basic 
con­trol .ow patterns such as sequence, parallel split, synchro­nization [van der Aalst et al. 2003]. 
The parallel split­synchronization pattern has similarities to the map-reduce programming paradigm. A 
number of work.ows divide the work units into distinct work units and the results are then combined -Motif 
work.ow (Figure 8), Pan-STARRS work­.ows (Figures 13 and 14). Thus we classify our work.ows into the 
following patterns: Sequential. consists of tasks that follow one after the other.  Parallel. consists 
of multiple tasks that can be run at the same time.  Parallel-split. one task s output feeds to multiple 
tasks.  Parallel-merge. multiple tasks merge into one task.  Parallel-merge-split. bothparallel-merge 
andparallel­split.  Mesh. task dependencies are interleaved.  3.4 Data Pattern The work.ows are associated 
with di.erent types of data including input data, backend databases, intermediate data products, output 
data products. A large number of the bioinformaticsapplicationsoftenhavesmall input and small dataproductsbutoften 
rely onhugebackenddatabases that are queried as part of task execution. These work.ows re­quire that 
thedatabasesbepre-installedonvarioussitesand resource selection is often based on selecting the resources 
where thedata mightbeavailable.Work.owssuch as LEAD (Figures 1 and 2), SCOOP (Figure 4), NCFS (Figure 
5) andPan-STARRS work.ows(Figures13 and14)have fairly large sized input, intermediate and output data 
products. The Glimmer work.ow (Figure 6) has similar sized input and output data products but its intermediate 
data prod­ucts are smaller. In today s production environments work­.owsoftencompressdataproducts toreduce 
transfertimes through intermediate scripts etc. When scheduling work­.ows on resources, a number of data 
issues need to be con­sidered including theavailability of therequireddataaswell asthedatatransfer timeofbothinput 
and outputproducts. We classify the work.ows as Data reduction. where the output data is smaller than 
the input data of the work.ows.  Data production. where the ouput data is larger than the input data 
of the work.ow.  Data processing. where input data is processed but data sizes do not change dramatically. 
  3.5 Usage scenarios It is often important to understand the use case scenar­ios for the work.ows. 
Work.ows are used in a number of di.erent scenarios -a new work.ow might be initiated in response to 
dynamic data or a number of work.ows might be launched as part of an educational workshop. In addi­tion, 
the user might want to specify constraints to adjust the number of worklows to run based on resource 
availabil­ity [Ramakrishnan et al. 2007]. Interactive Work.ows Scienti.c explorations often re­quire 
a human-in-the-loop as part of the work.ow. The typical mode of usage of science cyberinfrastructure 
is where a user logs into theportal and launches a work.ow for some analysis. The user selects apre-composed 
work.ow andsup­plies the necessary data for the run. The user might also want the ability to pause the 
work.ow at the occurrence of a prede.ned event, inspect intermediate data and make changes during work.ow 
execution. Event-driven Work.ows A number of scienti.c work­.owsget triggeredby newlyarrivingdata. Multipledynamic 
eventsand theirscale might needprioritiesbetweenusers for appropriate allocation of limited available 
resources. Re­sources must be allocated to meet deadlines. Additionally, to ensure successful completion 
of tasks, we might need to replicate some of the work.ow tasks for increased fault tol­erance. It is 
possible that with advance notice of upcoming weather events, we might want to anticipate the need for 
resources and try to procure them in advance. The weather forecasting, storm surge modeling (Figure 4), 
.ood-plain mapping(Figure5) andtheastronomy work.ows(Figures13 and 14) are launched with the arrival 
of data. User Constrained Work.ows An advanced user might want to provide a set of constraints (e.g. 
time deadline or budget) on a work.ow. Scienti.c processes such as weather prediction, .nancial forecasting 
have a number of parame­ters and computing an exact result is often impossible. To improve con.dence 
in the result, it is often necessary to run a minimal number of the work.ows. There is a need to run 
multiple work.ows (i.e. work.ow sets)that need to be scheduled together. Thus for work.ow sets, users 
specify that they minimally require M out of N work.ows to com­plete by the deadline.  4. WORKFLOW SURVEY 
EXAMPLES Table 1 presents an overview of the work.ow survey. The survey includes work.ows from diverse 
scienti.c domains and cyberinfrastruture projects. In the following sections, we identify the project 
from which the work.ow is drawn, the work.ow and usage model as available at the time of the survey. 
For each of the work.ows, we also provide a DAG representation of the work.ow annotated with computation 
anddatasizes. Sections4.1describes theweatherand ocean modeling work.ows and Sections 4.2 describes the 
bioinfor­maticsandbiomedicinework.ows. Sections4.3describe the astronomy and neutron science. 4.1 Weather 
and Ocean Modeling In the last few years the world has witnessed a number of severe natural disasters 
such as hurricanes, tornadoes, .oods, etc. The models used to study weather and ocean phenomenon use 
considerable and diverse real-time obser­vational data, static data, and parameters that are varied to 
study the possible scenarios for prediction. In addition the models must be run in a timely manner and 
information disseminated to groups such as disaster response agencies. This creates the need for large 
scale modeling in the areas of meteorology and ocean sciences, coupled with an inte­grated environment 
for analysis,prediction and information dissemination. 4secs 338secs 0.2MB 0.2MB 147MB 88secs 243MB 
78secs 19MB 206MB 4570secs/16 processors 2422MB Figure 1: LEADNorthAmerican Mesoscale(NAM) ini­tialized 
forecast work.ow. The work.ow processes ter­rain and observation data to produce weather forecasts. Domain 
Project Website Tool Weather and Ocean Modeling Linked Environments for Atmo­spheric Discovery (LEAD) 
TeraGrid Science Gateway http://portal.lead. project.org xbaya, GPEL, Apache ODE Southeastern Coastal 
Ocean Ob­serving and Prediction Program (SCOOP) http://www.renci.org/ focusareas/disaster/ scoop.php 
[Scripts] North Carolina Floodplain Mapping Program [Scripts] Bioinformatics and Biomedical North Carolina 
Bioportal, TeraGrid Bioportal Science Gateway http://www.renci.org/ focusareas/biosciences/ motif.php 
Taverna MotifNetwork http://www. motifnetwork.org/ Taverna National Biomedical Computation Resource (NBCR), 
Avian Flu Grid, Paci.c Rim Application and Grid Middleware Assembly http://nbcr.sdsc.edu/ http://gemstone. 
mozdev.org http: //www.pragma-grid.net/ http://avianflugrid. pragma-grid.net/ http: //mgltools.scripps.edu/ 
Kepler, Gem­stone, [Scripts] and Vision cancer Biomedical Informatics Grid (caBIG) http://www.cagrid.org/ 
Taverna Astronomy Pan-STARRS http://pan-starrs.ifa. hawaii.edu/public/, http://www.ps1sc.org/ Neutron 
Sci­ence Spallation Neutron Source (SNS), Neutron Science TeraGrid Gateway (NSTG) http://neutrons.ornl. 
gov/ Table 1: Work.ow Survey Overview. Survey is of work.ows used in domains from meteorology and ocean 
modeling, bioinformatics and biomedical work.ows, astronomy and neutron science 4.1.1 Mesoscale Meteorology 
The LinkedEnvironments forAtmospheric Discovery(LEAD) [Droegemeier et al. 2005] is a cyberinfrastructure 
project in support ofdynamicand adaptiveresponse tosevereweather. A LEAD work.ow is constrained by execution 
time and ac­curacy due to weather prediction deadlines. The typical inputs to a work.ow of this type 
are real time observational data and static data such as terrain information [Droege­meier et al. 2005, 
Plale et al. 2006] both of which are pre­processed and then used as input to one or more ensemble of 
weather models. The model outputs are post-processed by a data mining component that determines whether 
some ensemble set members must be repeated to realize statisti­cal bounds on prediction uncertainty. 
Figures 1, 2 and 3 show the work.ows available through the LEAD portal and include weather forecasting 
and data mining work.ows [Li etal.2008].Each work.ow taskisannotatedwithcomputa­tion timeandtheedgesofthedirectedacyclicgraph(DAG) 
are annotated with .le sizes. The weather forecasting work­.owsare largelysimilarand vary only in theirpreprocessing 
or initialization step. While the data mining work.ow can be run separately today, it can trigger forecast 
work.ows and/or steer remote radars for additional localized data in regions of interest [Plale et al. 
2006]. SoutheasternCoastalOcean ObservingandPrediction(SCOOP) program is creating an open-accessgrid 
environment for the southeastern coastal zone to help integrate regional coastal observing and modeling 
systems [SCOOP Website , Ra­makrishnan et al. 2006]. Storm surge modeling requires assembling input meteoro­logical 
and other data sets, running models, processing the output and distributing the resulting information. 
In terms of modes of operation, most meteorological and ocean mod­els can be run in hindcast mode, as 
an after fact of a major storm or hurricane, for post-analysis or risk assessment, or in forecast mode 
for prediction to guide evacuation or oper­ational decisions [Ramakrishnan et al. 2006]. The forecast 
mode is driven by real-time data streams while the hindcast mode is initiated by a user. Often it is 
necessary to run the model with di.erent forcing conditions to analyze forecast accuracy. This results 
in a large number of parallel model runs, creating an ensemble of forecasts. Figure 4 shows a .ve member 
ensemble run of the tidal and storm-surge AD-CIRC [Luettich et al. 1992] model. For increased accuracy 
of forecast the number of concurrent model runs might be increased. ADCIRC is a .nite element model that 
is par­allelized and using the MPI message passing model. The work.ow has a predominately parallel structure 
and the re­sults are merged in the .nal step. The SCOOP ADCIRC work.ows are launched according to 4.1.2 
Storm surge modeling the typical six hour synoptic forecast cycle used by the Na­ SoutheasternUniversitiesResearchAssociation 
s(SURA)  4secs 338secs 0.2MB 0.2MB 147MB 240secs 243MB 78secs 19MB 206MB 4570secs/16 processors 2422MB 
Figure 2: LEADARPS DataAnalysisSystem (ADAS) initialized forecast work.ow. The work.ow processes terrain 
and observation data to produce complex 4D assimilated volume that initializes a weather forecast model. 
1KB 2 MB 35secs 1KB 4KB 66secs 1KB 129secs 9KB 5KB Figure 3: LEAD Data Mining work.ow. The work­.ow processes 
Doppler radar or model generated fore­cast data to identify regions where weather phenomenon might exist 
in the future.  tional Weather Service and the National Centers for Envi­ronmental Prediction (NCEP). 
NCEP computes an atmo­spheric analysis and forecast four times per day at six hour intervals. Each of 
the member runs i.e. each branch of the work.ow gets triggered when wind .les arrive through Local Data 
Manager (LDM)[Unidata Local Data Manager (LDM)], an event-driven data distribution system that se­lects,captures, 
managesanddistributes meteorologicaldata products. The outputs from the individual runs are syn­thesized 
to generate the work.ow output that is then dis­tributed through LDM. In the system today each arriving 
ensemble member is han­dled separately through a set of scripts and Java code [Ra­makrishnan etal.2006].Theresourceselectionapproach[Lan­deretal.2008]makesareal-timedecisionforeach 
modelrun and uses knowledge of scheduled runs to load-balance across availablesystems. However thisapproachdoesnothaveany 
means of guaranteeing desired QoS in terms of completion time. 275 MB 900 secs/ 16 processors 162MB 
  Figure 4: SCOOP work.ow. The arriving wind data triggers the ADCIRC model that is used for storm-surge 
prediction during hurricane season. 4.1.3 Floodplain Mapping TheNorthCarolinaFloodplainMappingProgram 
[North CarolinaFloodplainMappingProgram,Blanton etal.2008] is focusedondeveloping accuratesimulationof 
stormsurges in the coastal areas ofNorthCarolina. Thedeployed system today consists of a four-model system 
that consists of the Hurricane Boundary Layer (HBL) model for winds, Wave-Watch III and SWAN for ocean 
and near-shore wind waves, andADCIRCforstormsurge. The modelsrequiregoodcov­erageof theparameterspacedescribing 
tropical stormchar­acteristicsinagivenregionforaccurate.oodplain mapping and analysis. Figure 5 shows 
the dynamic portion of the work.ow. Forcing winds for the model runs are calculated by the Hurricane 
Boundary Layer(HBL) model that serve as inputs to the work.ow. The HBL model is run on a lo­cal commodity 
Linux cluster. Computational and storage requirements for these work.ows are fairly large requiring careful 
resource planning. An instance of this work.ow is expected to run for over a day. The remainder of the 
work­.ow runs on a supercomputer.  4.2 Bioinformatics and Biomedical work.ows Thelast fewyearshaveseenlargescaleinvestmentsincy­berinfrastructure 
forbioinformatics andbiomedicalresearch. The infrastructure allows users to access databases and web 
534MB 810MB 11 hr /256 CPUs 13hr/8CPUs 34MB 3hr/192CPUs  4hr/160CPUs 4.5hr/256CPUs 6501MB Figure 5: 
NCFS work.ow. A multi-model work.ow used to model the storm surges in the coastal areas of North Carolina. 
servicesthrough work.owtoolsand/orportal environments. We surveyed three major projects in the United 
States -North Carolina Bioportal, cancer Biomedical Informatics Grid (caBIG), and National Biomedical 
Computational Re­source(NBCR) togainabetter understanding of thework­.ows. Signi.cant number of these 
work.ows involve small computationbutinvolveaccess tolarge-scaledatabasesthat must be preinstalled on 
available resources. While the typ­ical use cases today have input data sizes in the order of megabytes, 
it is anticipated that in the future data sizes might be in the gigabytes. 4.2.1 Glimmer The North Carolina 
Bioportal and The TeraGrid Biopor­tal Science Gateway [Ramakrishnan et al. 2006] provides access to about 
140 bioinformatics applications and a num­ber of databases. Researches and educators use the applica­tions 
interactively for correlation, exploratory genetic anal­ysis, etc. The Glimmer work.ow shown in Figure 
6 is one such example work.ow that is used to .nd genes in micro­bial DNA. The Glimmer work.ow is sequential 
and light on both compute and data consumption.  4.2.2 Gene2Life Let us consider the Gene2Life work.ow 
used for molec­ular biology analysis through the North Carolina Biopor­tal. This work.ow takes an input 
DNA sequence, discovers genes that match the sequence. It globally aligns the re­sults and attempts to 
correlate the results based on organ­ismand function. Figure7depicts thestepsof thework.ow and the corresponding 
output at each stage. In this work­.ow the user provides a sequence that can be a nucleotide or an amino 
acid. The input sequence performs two paral­lel BLAST [Altschul et al. 1990] searches, against the nu­cleotide 
and protein databases respectively. The results of the searches are parsed to determine the number of 
identi­ 8.8 MB 2 seconds 27KB 1 seconds 1.6 MB 5 seconds 1.35 MB 90 seconds 9.9 MB Figure 6: Glimmer 
work.ow. A simple work.ow used in educational context to .nd genes in microbial DNA. .ed sequences that 
satisfy the selection criteria. The outputs trigger the launch of ClustalW, a bioinformatics application 
that is used for the global alignment process to identify re­lationships. These outputs are then passed 
through parsi­monyprogramsforanalysis. Thetwoapplicationsthat may be available for such analysis are 
dnapars and protpars. In the last step of the work.ow plots are generated to visual­ize the relationships, 
using an application called drawgram. This work.ow has two parallel sequences. 0.1 MB 0.1 MB 1 MB 1 MB 
180 seconds 180 seconds 0.1 MB0.1 MB 0.1MB 0.1MB 300 seconds 4 KB 30 seconds 30 seconds 4 KB 30 seconds 
30 seconds Tree Files 35 KB (ps and .pdf files) Figure 7: Gene2Life work.ow. The work.ow is used for 
molecular biology analysis of input sequences. The dotted arrows show the intermediate products from 
this work.ow that are required by the user and/or might be used to drive other scienti.c processes. 
4.2.3 Motif Network TheMotifNetworkproject[Tilson etal.2007,Tilson etal. 2007], a collaboration between 
RENCI and NCSA, is a soft­ware environment to provide access to domain analysis of genome sized collections 
of input sequences. The MotifNet­work work.ow is computationally intensive. The .rst stage of thework.owassembles 
inputdataandprocesses thedata that is then fed into InterProScan service. The concurrent executionsofInterProScanarehandled 
throughTavernaand scripts. Theresultsofthedomain scanning steparepassed to a parallelized MPI code for 
the determination of domain architectures. The motif work.ow has a parallel split and merge paradigm 
where preprocessing spawns a set of paral­lel tasks that operate on subsets of the data. Finally, the 
results from the parallel tasks are merged and fed into the multi-processor application. 13 MB 30secs 
100 KB 5400 secs N=135  500 KB 60secs 3600 secs/ 71MB 256 processors 599 MB 599 MB 1432 MB Figure 
8: Motif work.ow. A work.ow used for mo­tif/domain analysis of genome sized collections of input sequences. 
 4.2.4 MEME-MAST is to facilitate biomedical research by harnessing advanced computational and information 
technologies. The MEME­MAST(Figure 9)work.owdeployedusingKepler [Ludscher et al. 2005, Altintas et al. 
2004] allows users to discover sig­nals or motifs in DNA orprotein sequences and then search the sequence 
databases for the recognized motifs. This is a simple work.ow often used for demonstration purposes. 
The work.ow is a sequential work.ow similar to Glimmer. 4.2.5 Molecular Sciences An important step in 
the drug-design process is under­standingthe three-dimensional atomicstructuresofproteins and ligands. 
Gemstone, a client interface to a set of com­putational chemistry and biochemistry tools, provides the 
biomedical community access to a set of tools that allows users to analyze and visualize atomic structures. 
Figure 10 shows an example molecular science work.ow. The work.ow runs in an interactive mode where each 
step of the work.ow is manuallylaunchedbytheuseroncethepreviouswork.ow taskhas.nished. The.rst fewstepsofthework.owinvolve 
downloadingthedesiredproteinandligandfrom theProtein Data Bank (PDB) database and converting it to a 
desired format. Concurrent preprocessing is done on the ligand us­ing the Babel and LigPrep services. 
Finally GAMESS and 100 KB 60 seconds 150 KB 60 seconds 200 KB Figure 9: MEME-MAST work.ow. A simple 
demon­stration work.ow used to discover signals in DNA se­quences. 100 KB 60 seconds ThegoalofNationalBiomedicalComputationResource(NBCR) 
120 KB 60 seconds 2 MB 140 KB 5 minutes 5 minutes 2.2 MB 175 KB 10 minutes 50MB APBS are used to analyze 
the ligand and protein. The re­sults are .nally visualized using the QMView which is done as an o.ine 
process. First few steps have small data and small compute and .nally produce megabytes of data.  4.2.6 
Avian Flu The Avian Flu Grid project is developing a global infras­tructure for the study of Avian Flu 
as an infectious agent and as a pandemic threat. Figure 11 shows a work.ow that is used in drug design 
to understand the mechanism of host selectivity and drug resistance. The work.ow has a number of small 
preprocessing steps followed by a .nal step where upto 1000 parallel tasks are spawned. The data products 
from this work.ow are small. 150KB 2 minutes 1 KB 4 minutes 50 KB 200KB 7.5MB 30 minutes N=1000 200KB 
Figure 11: Avian Flu work.ow. A work.ow used in drug design to study the interaction of drugs with the 
environment has a large fan-out at the end.  4.2.7 caDSR The cancer Biomedical Informatics Grid(caBIG) 
is a vir­tual infrastructure that connects scientists with data and tools towards a federated cancer 
research environment. Fig­ure 12 shows a work.ow using the caDSR (Cancer Data StandardsRepository)andEVS(EnterpriseVocabularySer­vices) 
services[CaGridTavernaWork.ows] to.ndallthe concepts related to a given context. The caDSR service is 
used to de.ne and manage standardized metadata descrip­tors for cancer research data. EVS in turn facilitates 
ter­minology standardization across the biomedical community. This work.ow is predominantly a query type 
work.ow and the compute time is very small in the order of seconds. 4.3 Astronomy and Neutron Science 
In this subsection we consider scienti.c work.ow examples from the astronomy and neutron science community. 
4.3.1 Astronomy work.ow The goal of the Pan-STARRS s (Panoramic Survey Tele-scopeAndRapidResponseSystem)project 
[etal.2005] is a continuous survey of the entire sky. The data collected by thecurrentlydeployedprototype 
telescope PS1 willbeused todetecthazardousobjects intheSolarSystem,and other astronomical studies including 
cosmology and Solar System astronomy. TheastronomydatafromPan-STARRSis man­agedby the teamsatJohnHopkinsUniversity 
andMicrosoft 5 seconds 10MB 5 seconds 15MB 5 seconds 10MB 5 seconds 15MB  Researchthroughtwowork.ows. 
The .rstPSLoadwork.ow (Figure 13) stages incoming data .les from the telescope pipeline and loads them 
into individual relational databases each night. Periodically the online production databases that can 
be queried by the scientists, are updated with the databases collected over the week by the PSMerge work­.ow(Figure 
14). The infrastructure to support the PS1 telescope data is still under development. Both the Pan-STARRS 
work.ows are data intensive but require coordi­nation and orchestration of resources to ensure reliability 
and integrity of the data products. The work.ows have a high degree of parallelism through substantial 
partitioning of data into small subsets. 4.3.2 McStas work.ow Neutron science research enables study 
of structure and dynamics of molecules that constitute materials. Neutron 100 MB 100 MB SourceSNSat 
OakRidgeNationalLaboratory connect large neutron science facilities that contain instruments with com­putationalresourcessuch 
astheTeraGrid [Lynchetal.2008]. TheNeutronScienceTeraGridGatewayenables virtual neu­tron scattering experiments. 
These experiments simulate a beam line and enables experiment planning and experi­mental analysis. Figure 
15 shows a virtual neutron scat­tering work.ow using McStas, VASP, and nMoldyn. VASP and nMoldyn are 
used for molecular dynamics calculations and McStas is used for neutron ray-trace simulations. The work.ow 
is computationally intensive and currently runs on ORNL supercomputing resources and TeraGrid resources. 
Thework.ow s initialstepsrun foranumberofdaysand are followed by additional compute intensive steps. 
The work­.ow is sequential and data set sizes remarkably small. 5. DISCUSSION Thispaperpresentswork.owsgathered 
throughsurveying of six scienti.c domains. The survey demonstrates similari­ties anddi.erences of work.ows 
along axes of structuralpat­tern, data pattern, usage, compute time and data sizes. We summarize the 
results of the survey in Tables 2 and 3. The total numberof tasks, lengthoflongest chainand widestde­greeofparallelizationdescribe 
thestructureofthework.ow. The work.ows in our survey vary from a handful of tasks to thousands of components. 
The maximum processor width of ataskgives an indicationof computational needsand can be useful in resource 
planning. A number of the work.ows are simple, requiring a single processor per task. However others, 
including motif, .ood-plain mapping and McStats often require multiple processors for parallel data process­ing 
either in a message passing (MPI) style application or tightly coupled parallel application. The computation 
and data sizes convey a median time and size magnitude. The majority of work.ows work with megabytes 
to gigabytes of data. However a few work.ows such as PanSTARRS Merge can yield gigabyte to terabyte databases 
as outputs. The classi.cation we propose can be used to study work­.ow types in greater detail. For 
example, Table 4 is an attempt understandtherelationshipbetweencomputational anddatasizes. Ourwork.owsurveyconsistsof 
alargenum­ber of work.ows that have small data sizes and these tend to vary from taking a few seconds 
to minutes (e.g. LEAD Data Mining) to hours (e.g., Storm surge) to days (e.g., Mc­Stats).Work.owswith 
mediumtolargedatasets (i.e., from megabytes to terabytes) tend to take longer time to process from hours 
to days as seen by the second and third rows of the table. 6. CONCLUSIONS Thispaperreportsonasurveyof 
work.ows fromphysical and natural sciences that vary in structure, and computa­tional and data requirements. 
The work.ows vary signif­icantly in their structure, user constraints associated with them and environments 
in which they might run. Our pro­posed work.ow classi.cation model helps us understand the characteristics 
of the work.ows and can serve as a founda­tion for design of next-generation work.ow technologies. 7. 
ACKNOWLEDGEMENTS Thisworkis fundedinpartbytheNationalScienceFoun­dation under grant OCI-0721674 and the 
Linked Environ­ments for Atmospheric Discovery funded by National Sci­enceFoundation underCooperativeAgreementATM-0331480 
(IU).Thisworkwassupportedbythe Director, O.ceofSci­ence, of the U.S. Department of Energy under Contract 
No. DE-AC02-05CH11231. The authors would like to thank the people who con­tributed towork.owsurveyincludingSureshMarruandthe 
entire LEAD team, Brian Blanton, Howard Lander, Steve Thorpe, Je.rey Tilson, Sriram Krishnan, Luca Clementi, 
Ravi Madduri, Wei Tan, Cem Onyuksel, Yogesh Simmhan, Sudharshan Vazhkudai, Vickie Lynch. The authors 
would also like to thank Kelvin K. Droegemeier for explaining var­ious concepts of mesoscale meteorology 
in great detail and Jaidev Patwardhan for feedback on the paper. 8. REFERENCES <RefA>[Altintas et al. 2004] 
Altintas, I., Berkley, C., Jaeger, E., Jones, M., Ludscher, B., and Mock, S. 2004. Kepler: An Extensible 
System for Design and Execution of Scienti.c Work.ows. [Altschul et al. 1990] Altschul, S. F., Gish, 
W., W. Miller, E. M., and Lipman, D. 1990. Basic Local Work.ow Name Structural Pat­tern Data Pattern 
Usage LEAD Weather Forecasting Sequential Data production Interactive, Event-driven, User Constrained 
LEAD Data Mining Sequential Data reduction Event-driven Storm Surge Parallel-merge Data reduction Interactive 
, Event-driven, User Constrained Flood-plain mapping Mesh Data production Event-driven Glimmer Sequential 
Data processing Interactive Gene2Life Parallel Data reduction Interactive Motif Parallel-split Data production 
Interactive MEME-MAST Sequential Data processing Interactive Molecular Sciences Parallel-merge Data production 
Interactive Avian Flu Parallel-split Data processing Interactive caDSR Sequential Data processing Interactive 
PanSTARRS Load Parallel-split­merge Data processing Event-driven PanSTARRS Merge Parallel-split­merge 
Data processing Event-driven McStats Sequential Data processing Interactive Table 2: Work.ow Survey 
Pattern and Usage Summary. The table captures the structural, data and usage patterns. Work.ow Name Total 
no. of tasks Longest Chain Max Width Max task processor width Comp. Data sizes LEAD Weather Forecasting 
6 4 3 16 hours megabytes to gi­gabytes LEAD Data Min­ing 3 3 1 1 minutes kilobytes Storm Surge 6 2 5 
16 minutes-hours megabytes Flood-plain map­ping 7 4 2 256 days gigabytes Glimmer 4 4 1 1 minutes megabytes 
Gene2Life 8 4 2 1 minutes kilobytes to megabytes Motif 138 4 135 256 hours megabytes to gi­gabytes MEME-MAST 
2 2 1 1 minutes kilobytes Molecular Sci­ences 6 5 2 1 minutes megabytes Avian Flu ~ 1000 3 1000 1 minutes 
kilobytes to megabytes caDSR 4 4 1 1 seconds megabytes PanSTARRS Load ~ 1600 -41000 4 800 -40000 1 minutes 
megabytes PanSTARRS Merge ~ 4900 -9700 4 4800 -9600 1 hours gigabytes to ter­abytes McStats 3 3 1 128 
days kilobytes to megabytes Table 3: Work.ow Survey Summary. The table captures the structural, computational 
and data character­istics of the work.ows. Comp. Time | Data Size seconds to min­utes hours days Kilobytes 
to Megabytes LEAD Data Mining, Glim­mer, Gene2Life, MEME-MAST, Molecular Sci­ences, Avian Flu, caDSR, 
PanSTARRS Load Storm surge McStats Megabytes to Gigabytes LEAD Weather Forecasting, Motif Flood-plain 
map­ping Gigabytes to Terabytes PanSTARRS Merge Table 4: Comparison of Data Sizes of Work.ows with Computation 
Time. Alignment Search Tool. Journal of Molecular Biology 214, 1-8. [Atkins 2002] Atkins, D. 2002. A 
Report from the U.S. National Science Foundation Blue Ribbon Panel on Cyberinfrastructure. In CCGRID 
02: Proceedings of the 2nd IEEE/ACM International Symposium on Cluster Computing and the Grid. IEEE Computer 
Society, Washington, DC, USA, 16. [Blanton et al. 2008] Blanton, B., Lander, H., Luettich, R. A., Reed, 
M., Gamiel, K., and Galluppi, K. 2008. Computational Aspects of Storm Surge Simulation. Ocean Sciences 
Meeting. [CaGridTavernaWork.ows] CaGridTavernaWork.ows. CaGrid Taverna Work.ows. http://www.cagrid.org/wiki/CaGrid:How-To: 
Create\_CaGrid\_Workflow\_Using\_Taverna. [Churches et al. 2006] Churches, D., Gombas, G., Harrison, 
A., Maassen, J., Robinson, C., Shields, M., Taylor, I., and Wang, I. 2006. Programming Scienti.c and 
Distributed Work.ow with Triana Services. Concurrency and Computation: Practice and Experience(Special 
Issue: Work.owin Grid Systems) 18, 10, 1021 1037. [CondorDAGMan] CondorDAGMan.CondorDAGMan. http://www.cs.wisc.edu/condor/dagman/. 
[Deelman et al. 2003] Deelman, E., Blythe, J., Gil, Y., and Kesselman, C. 2003. Work.ow Management in 
GriPhyN. Grid Resource Management, J. Nabrzyski, J. Schopf, and J. Weglarz editors, Kluwer. [Deelman 
et al. 2003] Deelman, E., Blythe, J., Gil, Y., Kesselman, C., Mehta, G., Vahi, K., Lazzarini, A., Arbree, 
A., Cavanaugh, R., and Koranda, S. 2003. Mapping Abstract Complex Work.ows onto Grid Environments. Journal 
of Grid Computing, Vol. 1, No. 1,. [Deelman and Gil 2006] Deelman, E. and Gil, Y. 2006. Report from the 
NSF Workshop on the Challenges of Scienti.c Work.ows. Work.ow Workshop. [Droegemeier et al. 2005] Droegemeier, 
K. K., Gannon, D., Reed, D., Plale, B., Alameda, J., Baltzer, T., Brewster, K., Clark, R., Domenico, 
B., Graves, S., Joseph, E., Murray, D., Ramachandran, R., Ramamurthy, M., Ramakrishnan, L., Rushing, 
J. A., Weber, D., Wilhelmson, R., Wilson, A., Xue, M., and Yalda, S. 2005. Service-Oriented Environments 
for Dynamically Interacting with Mesoscale Weather. Computing in Science and Engg. 7, 6, 12 29.  [et 
al. 2005] et al., N. K. 2005. Pan-STARRS Collaboration. American Astronomical Society Meeting 206. [Kramer 
and Ryan 2003] Kramer, W. and Ryan, C. 2003. Performance Variability of Highly Parallel Architectures. 
International Conference on Computational Science. [Lander et al. 2008] Lander, H. M., Fowler, R. J., 
Ramakrishnan, L., and Thorpe, S. R. 2008. Stateful Grid Resource Selection for Related Asynchronous Tasks. 
Tech. Rep. TR-08-02, RENCI, North Carolina. April. [Li et al. 2008] Li, X., Plale, B., Vijayakumar, N., 
Ramachandran, R., Graves, S., and Conover, H. 2008. Real-time Storm Detection and Weather Forecast Activation 
through Data Mining and Events Processing. Earth Science Informatics. [Ludscher et al. 2005] Ludscher, 
B., Altintas, I., Berkley, C., Higgins, D., Jaeger, E., Jones, M., Lee, E., Tao, J., and Zhao, Y. 2005. 
Scienti.c Work.ow Management and the Kepler System. [Luettich et al. 1992] Luettich, R., Westerink, J. 
J., and W.Scheffner, N. 1992. ADCIRC: An Advanced Three-dimensional Circulation Model for Shelves, Coasts 
and Estuaries; Report 1: Theory and Methodology of ADCIRC-2DDI and ADCIRC-3DL. Technical Report DRP-92-6, 
Coastal Engineering Research Center, U.S. Army Engineer Waterways Experiment Station, Vicksburg, MS. 
[Lynch et al. 2008] Lynch, V., Cobb, J., Farhi, E., Miller, S., and Taylor, M. 2008. Virtual Experiments 
on the Neutron Science TeraGrid Gateway. TeraGrid. [NorthCarolinaFloodplainMapping Program] North Carolina 
Floodplain Mapping Program. North Carolina Floodplain Mapping Program. http://www.nc.oodmaps.com/. [Oinn 
et al. 2006] Oinn, T., Greenwood, M., Addis, M., Alpdemir, M. N., Ferris, J., Glover, K., Goble, C., 
Goderis, A., Hull, D., Marvin, D., Li, P., Lord, P., Pocock, M. R., Senger, M., Stevens, R., Wipat, A., 
and Wroe, C. 2006. Taverna: Lessons in Creating a Work.ow Environment for the Life Sciences: Research 
Articles. Concurr. Comput. : Pract. Exper. 18, 10, 1067 1100. [Plale et al. 2006] Plale, B., Gannon, 
D., Brotzge, J., Droegemeier, K. K., Kurose, J., McLaughlin, D., wilhelmson, R., Graves, S., Ramamurthy, 
M., Clark, R. D., Yalda, S., Reed, D. A., Joseph, E., and Chandrashekar, V. 2006. CASA and LEAD: Adaptive 
Cyberinfrastructure for Real-time Multiscale Weather Forecasting. IEEE Computer 39, 66 74. [Ramakrishnan 
et al. 2006] Ramakrishnan, L., Blanton, B. O., Lander, H. M., Luettich, R. A., Jr, Reed, D. A., and Thorpe, 
S. R. 2006. Real-time Storm Surge Ensemble Modeling in a Grid Environment. In Second International Workshop 
on Grid Computing Environments (GCE),Heldin conjunctionACM/IEEE Conference for High Performance Computing, 
Networking, Storage and Analysis. [Ramakrishnan et al. 2006] Ramakrishnan, L., Reed, M. S., Tilson, J. 
L., and Reed, D. A. 2006. Grid Portals for Bioinformatics. In Second International Workshop on GridComputingEnvironments 
(GCE), Held in conjunction with ACM/IEEE Conference for High Performance Computing, Networking, Storage 
and Analysis. [Ramakrishnan et al. 2007] Ramakrishnan, L., Simmhan, Y., and Plale, B. 2007. Realization 
of Dynamically Adaptive Weather Analysis and Forecasting in LEAD. In In Dynamic Data Driven Applications 
Systems Workshop(DDDAS)inconjunctionwith ICCS(Invited). [SCOOPWebsite] SCOOPWebsite.SCOOPWebsite. http://scoop.sura.org. 
[Taylor et al. 2004] Taylor, I., Shields, M., and Wang, I. 2004. Resource Management for the Triana Peer-to-Peer 
Services. In Grid Resource Management, J. Nabrzyski, J. M. Schopf, and J. W¸eglarz, Eds. Kluwer Academic 
Publishers, 451 462. [Taylor et al. 2006] Taylor, I. J., Deelman, E., Gannon, D. B., and Shields, M. 
2006. Work.ows for e-Science: Scienti.c Work.ows for Grids. Springer. [Tilson et al. 2007] Tilson, J., 
Blatecky, A., Rendon, G., Mao-Feng, G., and Jakobsson, E. 2007. Genome-Wide Domain Analysis using Grid-enabled 
Flows. Proceedings 7th International Symposium on BioInformatics and BioEngineering, IEEE. [Tilson et 
al. 2007] Tilson, J., Rendon, G., Mao-Feng, G., and Jakobsson, E. 2007. MotifNetwork: A Grid-enabled 
Work.ow for High-throughput Domain Analysis of Biological Sequences:Implications for Study of Phylogeny, 
Protein Interactions, and Intraspecies Variation. Proceedings 7th International Symposium on BioInformatics 
and BioEngineering, IEEE. [UnidataLocalDataManager (LDM)] UnidataLocal Data Manager (LDM). Unidata Local 
Data Manager (LDM). http://www.unidata.ucar.edu/software/ldm/. [van der Aalst et al. 2003] van der Aalst, 
W. M. P., Ter, Kiepuszewski, B., and Barros, A. P. 2003. Work.ow Patterns. Distributed andParallel Databases 
14, 1 (July), 5 51. [Yu and Buyya 2005] Yu, J. and Buyya, R. 2005. A Taxonomy of Scienti.c Work.ow Systems 
for Grid Computing. SIGMOD Rec. 34, 3 (September), 44 49.</RefA>    
			
