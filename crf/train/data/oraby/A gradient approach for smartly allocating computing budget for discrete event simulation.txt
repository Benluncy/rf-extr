
 A GRADIENT APPROACH FOR SMARTLY ALLOCATING COMPUTING BUDGET FOR DISCRETE EVENT SIMULATION Chun-Hung 
Chen Hsiao-Chang Chen Department of Systems Engineering University of Pennsylvania Philadelphia, PA 19104-6315, 
U.S.A. ABSTRACT Simulation plays a vital role in analyzing many discrete­event systems. Usually, using 
simulation to solve such problems can be both expensive and time consuming. We present an effective approach 
to smartly allocate computing budget for discrete-event simulation. This approach can smartly determine 
the best simulation lengths for all simulation experiments and significantly reduce the total computation 
cost for obtaining the same contldence level. Numerical testing shows that our approach can obtain the 
same simulation quality with one-tenth the simulation effort. INTRODUCTION In order to efficiently manage 
and operate large man-made systems such as communication networks, traffic sys­tems, and automated manufacturing 
plants, it is often necessary to apply extensive simulation to study their performance since no closed-form 
analytical solutions exist for such problems. Collectively, these types of systems are known as Discrete 
Event Systems (DES) (Ho 1991). Unfortunately, using simulation to solve such problems can be both expensive 
and time consum­ing due to their massive search space and their evolution in time according to complex 
man-made rules and the influence of random occurrences. In industry, with pres­sure to meet certain system 
specifications and only a limited budget to carry out necessaty simulations, the limitations of traditional 
simulation technology can either delay a projector force it to go over budget. Suppose we want to compare 
n different discrete-event systems (designs or alternatives), we do T simulation replications for all 
n designs (or alternatives). Totally, we need nT simulation replications. The simulation results become 
more accurate when T increases. If the accuracy requirement is not low (T is not small), and if the total 
number of designs in a decision problem is not small (n is large), then nT can be very large, which may 
Liyi Dai Department of Systems Science and Mathematics Washington University St. Louis, MO 63130, U.S.A. 
 easily make total simulation cost extremely high and preclude the feasibility of a simulation approach. 
To reduce total simulation time, one can either develop more efficient simulation technology or use faster 
computers to reduce the simulation time of each simulation experiment. In this paper, we present another 
approach to improve the overall simulation efficiency. Our ideas are as follows. Intuitively, some bad 
&#38; signs can be discarded before completing all of the T rep­lications. We don t have to waste efforts 
on simulating bad designs and so reduce overall simulation time. Then the question is how to systematically 
do this? When? And which designs? Ideally, we want to optimally choose the number of simulation replications 
for all A signs to minimize the total simulation cost, while ob­taining the desired confidence level. 
In fact, this question is equivalent to optimally decide which designs will receive computing budget 
for continuing simulation. Figure 1 illustrates the ideas by comparing a typical solution to this problem 
with the conventional approach using equal simulation lengths. Chen (1995) formulates this question and 
obtained promising preliminary results using very simple heuristics. In this paper, we will further discuss 
it and compare two approaches, one of which utilizes the gradient information. To optimally allocate 
computing budget, first of all, one must have an efficient way to estimate the confi­dence level based 
on the results of the completed simula­tion. Further, one must have easy ways to anticipate how the confidence 
level will change if some computing budget is allocated and additional simulation replications are completed. 
Goldsman and Nelson (1994) provide an excellent survey on current approaches (e.g., Goldsman, Nelson, 
and Schmeiser (1991), Gupta and Panchapakesnn (1979), and Law and Kelton (1991)) to estimating simulation 
confidence level. In addition, Bechhofer, Santner, and Goldsman (1995) give a systematic and more detail 
discussion on this issue. Those approaches are mainly suitable for problems not having large number of 
designs 398 I Number ofsimulation replications - ~z=--==~ _ Withequal number ofsimulation replications 
With Intelligent Computing Budget Allocation Figure l: Compmison of Simulation Budget Allocations for 
Obtaining the Same Confidence Level (e.g., Goldsman and Nelson (1994) suggest 2 to 20 designs). For 
real-life DES problems, the number of designs can grow up to numerous orders of magnitude easily. Chen 
(1996) presents a feasible way to quantify cotildence level for large DES simulation (the large refers 
to large search space). Further, when the approach in Chen (1996) is applied the sensitivity information 
of the confidence level with respect to simulation replication numbers can be easily obtained, which 
will provide the basis to determine how to allocate computing budget among designs in this paper. Section 
2 describes the notation used in this paper and a brief overview of Chen ( 1996) s approach to quantify 
confidence level for problems with large search space. In Section 3, we will define the optimal computing 
budget allocation problem. Two major difficulties in solving this problem will be pointed out. Since 
it is difficult to find an optimal solution for the computing budget allo­cation problem, and it is impractical 
to spend lot of time in finding the optimal solution, we propose a sequential approach to overcome these 
two difficulties in Sections 4 and 5. We call this approximation smart computing budget allocation scheme. 
Numerical testing in Section 6 shows that using this approach to smartly allocate computing budget can 
reduce the total computation time by about ten times for a 1000-design example. Section 7 concludes this 
paper. 2 A FEASIBLE APPROACH TO QUANTIFY CONFIDENCE LEVEL FOR PROBLEMS WITH LARGE SEARCH SPACE Chen 
(1996) provides a simple approximation approach to quantify confidence level for problems with large 
search space and also provide some useful sensitivity information of the confidence level with respect 
to simu­lation replication numbers, which will provide the basis to determine how to allocate computing 
budget among designs in this paper. Denote n: the total number of designs, T the length of simulation, 
the number of replication, or the total number of samples, ~j(t): the t-th sample of the performance 
lmeasure of design j, ~j(T): the sample mean of design j, namely, ~i (T) = ~ ~~j(t), and t J, the performance 
measure, or more specifically, the mean performance measure of design j, i.e., the mean of ~j (t). Assume 
that i) ~j (t) is i.i.d. for all t, ii) the simulations for designs i and j, i # ,j, are inde­ pendent. 
Thus, ~i (t)and ~j (t) are independent. For steady-state simulation, the sample ~J(t)may not be independent 
of ~j (s) for s # t. One possible way is to place the raw data in a few large batches, and work with 
these few batches as if they were independent (Banks, Carson, and Nelson 1995). As the strong law of 
large numbers, with probability 1, ~j(T) +Jj, as T+oo. Without infinitely long simulations or infinite 
number of simulations replication, the sample mean ;~j (T) is an approximation to Jr We refer to the 
sample mean ~j (T) from one finite simulation experiment as an observed performance measure for a particular 
design s simulation. Let Oj be the index of the design having the j-th largest observed performance measure. 
With these notations, we have 7., >702>..>7. . The traditional optimization approaches are to find the 
design with maximum performance measure. (Without loosing generality, we only consider maximization prob­ 
lems in this paper.) However, even the simulation cost for a good estimation of ~ could be very high, 
especially for complicated systems. Instead of insisting on picking the best design, Ordinal Optimization 
(Ho, Sreenivas, and Vakili 1992) concentrates on finding good enough designs and reduces the required 
simulation time dramati­ cally. Comparing the observed performance measures at short simulation length 
T, we can select the observed best design (o,) or the observed top-r designs (01, oZ, , . . . or), and 
then ask what is the probability that at least one of the observed top-r designs actually belongs in 
top-k. This is crucial to Ordinal Optimization, although estima­ tion of such a probability is very difficult 
for problems having large n. Chen (1996) adopts the Bayesian model to analyze such confidence probability. 
Under the Bayesian model, J, is treated as a random variable and has a prior dk.ribu­tion which describes 
the knowledge or the subjective belief about .lj before any sampling. The posterior dis­tribution is 
updated after we observe the samples { ~j (t), t=l,..,T}. The posterior distribution p(~ I { ~j (t), 
 t=1,..,T} ) summarizes the statistical properties of .1, given the prior knowledge and sampling information. 
When simulation stops, the statistical properties is de scribed by the posterior distributions. We can 
estimate the probability that J, is in some specific region, e.g., Pr{ .lj>O I {~j(t),t=l,.., T }}, or 
compare two designs, e.g., Pr{.Ji-~>0 I {~, (t),~j(t)j@l,..,T} }. For nota­ tional simplicity, we denote 
~j as the posteriori J~l { ~j(t), t=l,..,T}. Namely, Pr{ ~j>O} represents Pr{ Jj >0 I { ~j(t), t= 1, 
... T}}. The posterior distribution p( ~j) illustrates what value ~ may be, based on samples { ~j(t), 
t= 1,..,T) and the prior knowledge. With some normal assumptions, the posterior .jj s Jj I {~j(t), t 
= 1,2,.-, T} _ N(~$~j(t), :) t Chen (1996) also shows that the Confidence Probability CP1 = Pr{At least 
one of designs o,, Oz, ... or actually belongs in top-k} 2 Approximated Confidence Probability ACP1 = 
fiPr{jol > j.,}, j=r+k and that CP2 = I%{ The true performance of the observed best design is not worse 
than/3 fraction of the performance of the true best design) 2 Approximated Confidence Probability ACP2 
= fi ~{jol > Pjoj }. ,=2 While CP1 and CP2 are very difficult to obtain, ACP1 and ACP2 can be computed 
very easily, and therefore will be used to approximate CP1 and CP2, respectively. Numerical testing shows 
that they can provide reasona­bly good approximation. Furthermore, since ACP1 and ACP2 are lower bounds 
of CP1 and CP2, we are sure that confidence level is sufficiently high when ACP1 or ACP2 is high enough. 
Although the definitions of CP 1 and CP2 are different, the formulas for ACPI and ACP2 are quite similar. 
For easy explanation, without loss of generality, we will only consider the simple case that ACP = fiPr{j., 
> j.j ] j=z in the latter discussion, i.e., how to smartly allocate computing budget for obtaining satisfactory 
ACP. 3 PROBLEM DEFINITION We follow the problem formulation given by Chen (1995). Let Tj be the simulation 
length, or the number of samples, of design j. If simulations are performed on a sequential computer 
and with simulation length T for all designs, the computation cost can be approximated by T1 + T2 +.. 
~+ T. = nT. However, to ensure that ACP is larger than some value, we don t need to restrict our­selves 
to T1 = T2 = . . . = T., and may choose different simulation lengths for different designs. This means 
T, may not be equal to Tj for i #j. Furthermore, we can choose Tj for all j such that the total computation 
cost is minimized, while guaranteeing that ACP is g-eater than some satisfactory level. More specifically, 
we are considering the following problem. (P) nlill(T1+ T2+... +T~), T~,...,T s.t. ACP 2 (a satisfactory 
level). There are two major difficulties in solving (P): Difficult y 1. ACP( T1, T2, ..., Tn) can be 
com­puted only after doing simulations until T1, T2,. , T.. Before performing simulations until T1, 
T2, , T n, how can we predict or estimate the ACP at T1, T2, , Tn ? Difficulty 2. T1, T2, ..., T. are 
integers. Even if we have techniques to estimate ACP at T], T2, , Tn, an extremely large combinatorial 
space must be searehed to find a solution to (P), especially when n is large. Note that the purpose of 
solving (P) is to further ~­duce computation cost for obtaining a desired confidence level. We should 
not exert too much effort solving (P) during simulation. Otherwise, the additional cost of solving (P) 
will cancel the benefits of computing budget allocation. Hence, we need to solve (P) very efficiently, 
even if this means obtaining a sub-optimal solution. Efficiency is more crucial than optimality in this 
appli­cation. 4 A SEQUENTIAL APPROACH This section presents a sequential procedure to overcome the difficulties 
in solving (P). To Optimally allocate computing resource, it is equivalent to determine which designs 
we should do more simulation. We will sequentially deeide it, although this is usually not an optimal 
solution any more. Before doing simulation there is neither knowledge about ACP nor a basis for choosing 
Tj. First, all &#38; signs are simulated until length tO to obtain statistical information about sample 
means and sample variances. Then we try to determine how to further allocate comput­ing budget using 
available statistical information. When simulation is stopped at tO, the posterior distribution of design 
j is  ~j t=1,2, ,to} ~j(to)=Jj l{~j(t), 2 -N(; ~j (t), g :) At this moment, we have some ideas about 
each design and then can deeide which designs are worthy of being allocated more computing budget. To 
determine how to further allocate computing budget, we have to be able to know how the ACP will change 
if some computing budget is allocated to some designs (Difficulty 1). More specifically, based on statistical 
information at to, we want to anticipate the posterior distribution at to+ A, where A is a positive integer. 
To do this, we assume the sample mean and variance at toarenear those at to+ A, and approximate the posterior 
distribution at to + A using the estimated posterior distribution Allocating Computing Budget r+ (~~ 
~j (t), ) to+A Note that the denominator of variance portion is to+ A rather than to.This approximation 
will be satisfactory assuming to is large enough and if A is not too large. On the other hand, we don 
t want to choose to too large, or we will defeat the purpose of this approach. Using the estimated posterior 
distributions, we can estimate the ACP at to+A using the statistical information at to, and call it the 
Estimated ACP or EACP. Similarly, when simulation proceeds until T,, T2,..., Ti_l, T,, Till,..., T., 
we can alsD use the available information to estimate how ACP will change if design i is given additional 
budget A, i.e., EACP(T1, T2,..., Ti_l, Ti+A, Ti+l, Tn), Tn). This is X­ complished by using the estimated 
posterior distribution for design i. Now it is feasible to predict the ACP when the change of Ti s are 
not large. A possible sequential approxima­tion approach to solving (P) is as follows. Since ACP will 
become larger as simulation proceeds, we sequen­tially add computing budget by b each time until ACP 
reached some satisfactory level (say P,,,). In order to minimize to the total computation cost, at each 
step, this budget b is allocated among some designs such that the EACP is maximized. Thus, at step k, 
k= 1,2,3,.., (P-k) rniIX EACP(Tf +z~, T;+ Z;, , Tj +7;), 71,..,7: St. 7;+ Z;+.. +r; = b and z! 20 for 
all i. More specifically, the sequential algorithm is A Sequential Al~orithm for Smart ComDuting Budget 
Allocation (SCBA) Step O. PERFORM SIMULATION until length to for all designs, k+ O, T:= T~=... _ Tk~ 
= to. Step 1. If ACP(T~, T;, . . . . T;) ~ Psat, stoP, otherwise, go to Step 2. Step 2. Solve (P-k), 
T:+] =T~+f, fori= 1, ... n, k+ k+l, Step 3. PERFORM SIMULATION until (T:, T:,..., T:); go to Step 1. 
Remarks: 1. Obviously, the computing budget allocated by this sequential approach is not the optimal 
way. As we dis­ cussed before, efficiency is more important than optimal­ ity. Otherwise, the additional 
cost of determining com­ puting budget allocation may cancel its benefits. 2. b is the one-time increment 
of simulation budget. Small b means small step size and therefore will increase the total number of solving 
(P-k). On the other hand, large b may waste computing budget and result in a larger ACP. 5 SOLVING PROBLEM 
(P-k) The next question is how to efficiently solve (P-k). While solving (P-k) is easier than solving 
(P), again, efficiency is much more important than optimality here. In this paper, we test two quick 
and dirty approaches to allocate the given computing budget b for obtaining large increment of ACP. In 
the first approach, m designs are chosen and then the computing budget is equally distributed to them 
(each design has blm ). For each design, we calculate the an­ ticipated increment of ACP if computing 
budget blm is allocated to it. Then those designs are chosen if their anticipated ACP increments are 
among top-m. Approach 1. Choose a positive integer m, and let A = b I m (assume A is an integer) Step 
1. For i =1, ... n, calculate Di = EACP(Tf, T;,..., Tf_l, Tz~+A, T}+l, , T;) -ACP(T$, T), ... T,!l, 
Tl~, T,~l,... ,T~). Step 2. Find the set S(m)= { i : Di is within the top-highest-m} Step 3. z: = A, 
for all i IES(m). Approach 2. In the second approach, instead of equally allocating computing budget 
among some m designs, we apply steepest-descent method (Imenberger 1984) to solve (P-k). We do the following 
approximation to estimate the gradient of ACP with respect to T,. Lemma 1. Suppose the random variable 
X -N(A, d), where A >0. Then Pr{X<O} s exp( 2: <pf> Using Chemoff bounds (Ross 1994), we have ~2t2 Pr{X<O} 
< in$M(t) = in; exp(Y + pt). Choose t = ~, we have the minimum, i.e., o Chen, and Dai 2 Pr{X<O} S exp( 
~). Q.E.D. With this lemma, ACP = fi Pr{~O1 > ~Oj } = fi(l pr{~ol ~Oj < 0}) j=z j= n A~j > 1 exp( 
z ) = ACP*, I-( 201j 1 j=z CY2 0:, where A,j = ~01 ~Oj and ~~j =?+ ~. 01 01 For notation simplicity, 
let s=o,. The gradient of ACP* with respect to Ti are as follows: ifi #s, 6 A~iT:of A;i ACp*= exp ~Ti 
2(T/7; + Ti@2 o 2(: +:) [1 SZ · fi l-exp(- t z ) j=l 2(:+:) j#.s,]*l [1 sJ ifi=s, &#38;ACp* z s ( () 
h A~i 1 exp( ?j ~) J=2.]*oi 02 2(;+ TOj s J To avoid spending much time in iteratively finding the solution 
of (P-k), we only do a very limited numbers of iterations when applying steepest-descent method. Dif­ferent 
numbers of iterations are tested in Section 6. Unifil ,7] LL Cl: Unif12,18] ; C2 Exp(o.12)  :-n-o 4sno 
Figure 2: 10-Node Network with Priority, 6 NUMERICAL TESTING Two examples are tested. Example 1 is a 
steady-state simulation, and Example 2 is a terminating simulation. Example 1. We consider a 10-node 
network (Please see Figure 2). Such a network could be the model for a large number of real-world systems, 
such as a manufacturing system, and a communication or a traffic network. For details about this example, 
please refer to Chen and Ho (1995). We consider the problem of optimally allocating 22 buffer units among 
the 10 different nodes for maximizing the throughput. Priority, interruption, blocking and multi-classes 
are included in this network. We denote the buffer size of node i by B,. We set some constraints for 
symmetry reasons: BO= BI = Bz = B3, B4 = Bc, and B~ = B,. In addition, B8 &#38; B9 > 1. These constraints 
limit our search space to n=1000 different configurations. The Standard Clock method (Chen and Ho 1995 
and Vakili 1991), which is an efficient technique for DES simulation, is used to simulate this system. 
The computation cost for one design is roughly proportional to the number of clock ticks (for Standard 
Clock method, one event is generated at each clock tick). We define the computation cost as ~ ~ [the 
number of clock ticks when the 1000 j=, simulation of design j is stopped), 1 The is used to rescale 
the cost. The computation 1000 cost for determining the smart computing budget alloca­tion is so small 
as compared with the simulation cost that we ignore this portion. In this testing, we consider CP1 = 
Pr{At least one of the observed top-3 designs actually belongs in top-3} and CP2 = Pr{The true per­formance 
of the observed best design is not worse than 99.6% of the performance of the true best design}. We test 
Approach 1 and set to= 5,m =25,andb =125. We Unitl 1,7] - Interruption, and Shared Server repeat this 
testing 50 times. Each run has a different random seed. We consider the computation costs for different 
satisfactory confidence levels. Tables 1 and 2 contain the testing results for CP1 and CP2 respectively. 
The computation costs in these two tables are the average costs in the 50 testing runs. Table 1: Speedup 
with the SCBA Method for ACPI P~at without with Speedup SCBA SCBA --l 50% 24200 3952.5 6.12 60% 29100 
4378.7 6.64 :>.78 70% 45700 4670.0 80% 65600 5775.0 11.35 3 Table 2: Speedup with the SCBA Method for 
ACP2 Psat without with Speedup SCBA SCBA 3 50% 23000 3737.5 6.15 60% 29400 4105.0 7.16 7070 37300 4480.0 
8.32 ~).98 80?io 54400 5446.2 a Example 2. To further compare these two approaches, we test a simple 
single-node queue. The interatival time is -Uniform[O. 1, 1.9]. We consider 10 designs with different 
service times, which are Uniform[O. 1, 1.85­i*O.05] for design i, i=l ,..,10. Suppose we are interested 
in the average system time of the customers served between time O and time 10. Although the derivations 
in Sections 2 -5 focus on maximization, we only need to reverse their inequality signs in order to apply 
to this minimization problem. We set b = 12. and tO = 10. 10,000 independent experiments are done to 
estimate the average cost for using different approaches. We consider CP = Pr{The observed best design 
is actuallly the true best design }. Table 3 shows the average totnl numbers of simulation replications 
for obtaining the confidence level P,,, when setting tO= 10, and Table 4 is for tO= 5. Table 3: Average 
Total Number of Simulation Replications for Different SCBA Approaches (TO=lO) I P~~t II60% I 70% I 80% 
I 90% I A. 1 m=4 172.4 230.3 350.0 608.4 A. 1 m=3 170.5 225.9 322.5 542.7 A. 1 m=2 170.0 226.5 322.5 
511.9 A. lm=l 176.6 228.9 324.6 509.6 a A .2 litrn 167.2 221.1 324.1 525.9 A. 2 2 itrns 162.2 212.0 
307.7 498.6 A .2 4 itrns 163.3 213.4 313.4 511.6  q Without SCBA 327.1 488.2 796.4 1458. Table 4: Average 
Total Number of Simulation Replications for Different SCBA Approaches (TO=5) P~at II60% i 70% I 80% I 
90% App. l(m =4) II 127.6 I 186.3 I 305.6 I 576.9 App. 2 (1 itrn) II 130.1 I 185.9 I 290.4 I 496.2 122.3 
175.3 273.1 467.3 125.9 185.3 285.5 492.1  $%3&#38;l Without SCBA 305.2 475.5 790.0 1462. From Tables 
3 and 4, we have the following observations: · tO may affect the performance quite significantly, particularly 
when P,,t is small. How to choose an appropriate tO is problem-specific. This remains to be investigated. 
· Different choices of m s obtain different perform­ances. It is interesting to note that large m works 
better for low P,a,, while small m performs well for high P,,,. We conjecture that there exist some better 
ways which dynamically change m through simulation. · For the steepest descent method, two iterations 
in each sequential optimization step works better than others. We need more testing to justify this. 
Ide ally, we may gradually change the number of itera­tions through simulation to optimize the performance. 
· The time savings factor of using SCBA increases as P,., increases. This makes sense since we have more 
space to manipulate the allocation of computing budget when P,,,, the cotildence level requirement, is 
higher. When to= 5, Approach 2 with two iterations can reduce computation effort by 6870 for P,,t = 9070. 
We believe that this time savings factor will be even larger if higher confidence level is required. 
Since ACP is a lower bound of the confidence level CP and we use ACP to determine computing budget allocation, 
people may be concerned with the ending CP (actual confidence). In this testing, CP = Pr{The ob­served 
best design is actually the true best design} can be obtained numerically by calculating (total number 
of simulation experiments in which the observed best de sign is actually the true best design) / 10,000. 
Table 5 shows the numerical results of CPS for to= 5. Table 5: CP for Different SCBA Approaches (tO=5) 
Our approaches stop simulation when ACP is no less than P,,,. We anticipate that the ending CP will be 
higher than P,,, since ACP is a lower bound of CP. Table 5 shows that CPS are not much higher than ACPS 
except the case in which SCBA is not used. The reason is that without using SCBA, all designs receive 
computing budget so that the simulation quality im­ provement is higher at each step. Consequently the 
ending CPS can be much higher than required level.  7 CONCLUDING REMARKS In this paper we present two 
approaches to smartly allo­cate computing budget for DES simulation. Preliminary numerical testing shows 
that we can significantly tiuce total computation cost. For real-time application problems, we have only 
a limited computing budget to carry out simulation. The SCBA can be applied to these problems to maximize 
the utilization of limited budget and obtain higher confidence level. In particular, from Table 1, the 
computation cost is 65,600 units for ensuring ACP1 > 80% without SCBA. On the other hand, with SCBA the 
computation cost is only 5,775 units. This implies that 5,775 SCBA computation units can obtain the same 
simulation quality as 65,600 computation units without SCBA. Application of the SCBA algorithm can obtain 
the same simulation quality with one-tenth the simulation effort. In this paper, we compare two simple 
approaches us­ing an example. The gradient approach performs slightly better than the other. While which 
approach is surprier needs more testing to justify, we fiirmly believe that there exists some more sopfisticaed 
way to accomplish better performance. We will test more examples in the future. The approaches using 
second order information is also one of the ongoing research topics. ACKNOWLEDGMENTS The work in this 
paper is partially supported by the University of Pennsylvania Research Foundation. The author would 
like to thank Dr. Russell R. Barton, Dr. Douglas J. Morrice, and the anonymous referees for their helpful 
suggestions and valuable comments. REFERENCES <RefA>Banks, J., and J. S. Carson, B. L. Nelson. 1995. Discrete-Event 
System Simulation, Prentice-Hall. Bechhofer, R. E., T. J. Santner, and D. M. Goldsman. 1995. Design and 
Analysis of Experiments for Statistical Selection, Screening, and Multiple Comparisons, John Wiley &#38; 
Sons, Inc. Casella, G., and R. L. Berger. 1990. Statistical Inference, Wadsworth. Chen, C. H. 1995. An 
Effective Approach to Smartly Allocate Computing Budget for Discrete Event Simulation, Proceedings of 
the 34th IEEE Conference on Decision and Control, 2598-2605. Chen, C. H. 1996. A Lower Bound for the 
Correct Subset-Selection Probability and Its Application to Discrete Event System Simulations, To appear 
on IEEE Transactions on Automatic Control. Chen, C. H., and Y. C. Ho. 1995. An Approximation Approach 
of the Standard Clock Method for General Discrete Event Simulation, IEEE Transactions on Control Systems 
Technology, Vol. 3, #3, 309-317. Goldsman, G., and B. L. Nelson. 1994. Ranking, Selection, and Multiple 
Comparison in Computer Simulation, Proceedings of the 1994 Winter Simulation Conference, 192-199. Goldsman, 
G., B. L. Nelson, and B. Schmeiser. 1991. Methods for Selecting the Best System, Proceedings of the 1991 
Winter Simulation Conference, 177-186. Gupta, S. S. and S. Panchapakesan. 1979. Multiple Decision Procedures: 
Theory and Methodology of Selecting and Ranking Populations, John Wiley. Allocating Computing Budget 
405 Ho, Y. C. (Editor). 1991. Discrete Event Dynamic Systems, IEEE Press. Ho, Y. C., R. S. Sreenivas, 
and P. Vakili. 1992. Ordinal Optimization of DEDS , Journal q~ Discrete Event Dynamic Systems, 2, #2, 
61-88. Law, A. M. and W. D. Kelton. 1991. Simulation Modeling &#38; Analysis, McGraw-Hill, Inc. Luenberger, 
D. G. 1984. Linear and Nonlinear Programming, Addison-Wesley. Ross, S. 1994. A First Course in Probability, 
Prentice Hall. Vakili, P. 1991. A Standard Clock Technique for Efficient Simulation, Operations Research 
Letters, Vol. 10, 445-452.</RefA> AUTHOR BIOGRAPHIES CHUN-HUNG CHEN is an Assistant Professor of Systems Engineering 
at the University of Penmylvania, Philadelphia, PA. He received his Ph.D. (degree in Simulation and Decision 
from Harvard University in 1994. His research interests cover a wide range of areas in Monte Carlo simulation, 
optimal control, :stochastic decision processes, ordinal optimization, perturbation analysis, and their 
applications to manufacturing sys­tems. Dr. Chen won the 1994 Harvard IJniversity Eliahu I. Jury Award 
for the best thesis in the field of control. He is also one of the recipients of the 1992 MasPar Parallel 
Computer Challenge Award. HSIAO-CHANG CHEN is a Ph.D. candidate at the Systems Engineering Department, 
University of Penn­sylvania. He received a double B.S. degree in Mathemat­ics and Computer Science from 
the Eastern Michigan University in 1992, and he received an M.S. degree in Systems Science and Mathematics 
from Washington University, St. Louis in 1994. He is working on devel­oping efficient approaches for 
discrete-event simulation. LIYI DAI is an assistant professor in the Department of Systems Science and 
Mathematics at Washington University, MO. He received the M.S. degree from the Institute of Systems Science, 
Academia Sinica, Beijing, China, in 1986, and the Ph.D. degree from Harvard University in 1993. His research 
interests include discrete event dynamic systems, simulation, stochastic optimization, communication 
systems, and singular systems. He has coauthored over 30 papers in various journals and is the author 
of Singular Control Systems (Berlin: Springer-Verlag, 1989). Dr. Dai is listed in Who s Who among Asian 
Americans and is a recipient of the NSF CAREER award. 
			
