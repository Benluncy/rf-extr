
 A Framework For Simple Sorting Algorithms On Parallel Disk Systems (Extended Abstract) Sanguthevar Rajasekaran 
Dept. of CISE, Uniersity of Florida Gainesville FL 32611 Abstract In this paper we present a simple 
parallel sorting algorithm and illustrate two applications. The al- gorithm (called the (1, m)-merge 
sort (LMM)) is an extension of the bitonic and odd-even merge sorts. Literature on parallel sorting is 
abundant. Many of the algorithms proposed, though being theoret- ically important, may not perform satisfactorily 
in practice owing to large constants in their time bounds. The algorithm to be presented in this pa- 
per, due partly to its simplicity, results in small constants. We present an implementation for the parallel 
 disk sorting problem. The algorithm is asymptoti- cally optimal (assuming that N is a polynomial in 
M, where N is the number of records to be sorted and M is the internal memory size). The under-lying 
constant is very small. This algorithm has a better performance than the disk-striped merge-sort (DSM) 
algorithm when the number of disks is large. Our implementation is as simple as that of DSM (requiring 
no fancy data structures or prefetch techniques.) Permission to make digital or hard copies of all or 
part of this work for personal or classroom use is granted without fee provided that copies are not made 
or distributed for profit or commercial advantage and that copies bear this notice and the fidl citation 
on the first page. TO copy otherwise, to republish, to post on servers or to redistribute to lists, requires 
prior specific permission and/or a fee. SPAA 98 Puerto Vallarta Mexico Copy+@ ACM 1998 O-89791-989-0/98/ 
6...$5.00 As a second application, we prove that we can get a sparse enumeration sort on the hypercube 
that is simpler than that of the classical algorithm of Nas- simi and Sahni [14]. We also show that Leighton 
s columnsort algorithm is a special case of LMM. 1 Introduction Sorting is perhaps one of the most widely 
stud-ied problems of computing. Numerous asymptoti-cally optimal sequential algorithms have been dis- 
covered. Asymptotically optimal algorithms have been presented for varying parallel models as well. The 
classical algorithm of Batcher [5] was nearly optimal. The celebrated paper of Ajtai, Komlos and Szemeredi 
[3] gave the first asymptotically op-timal logarithmic time deterministic parallel algo-rithm for sorting. 
Reischuk s randomized algo-rithm for the PRAM [20] and the Flashsort of Reif and Valiant [19] were asymptotically 
optimal and randomized. Some of the follow-up algorithms in-clude Leighton s column sort [13], Cole s 
optimal algorithm for the PRAM [7], etc. These sorting re-sults have been employed in the design of numerous 
other parallel algorithms also. Since sorting is a fundamental problem, it is im- perative to have efficient 
algorithms to solve it. Though the literature on sorting is vast, many of these algorithms have huge 
constants in their run times, making them inferior in practice to asymp- totically inferior algorithms. 
For a survey of paral- lel sorting algorithms the reader is referred to [18]. This paper is motivated 
by a desire to seek prac- tical algorithms. In particular, we are interested in the development of sorting 
algorithms that will have small underlying constants. We introduce a variant, we call the (Z,m)-merge 
sort (LMM), of the bitonic and odd-even merge sort algorithms. To demonstrate its applicability, we present 
two il- lustrative implementations. The first implementation is for the parallel disk sorting problem. 
This problem also has been ex- tensively studied on several related models. The model we use is the one 
suggested by Vitter and Shriver in their pioneering paper [22]. A known lower bound for the number of 
I/O read steDs for parallel disk sorting is1 fi (&#38; [a]). Here N is the number of records to be sorted 
and M is the internal memory size. Also, B is the block size and D is the number of parallel disks used. 
There ex-ist several asvmptotically optimal algorithms that make 0 (&#38; [&#38;$$]) I/O read steps (see 
e.g., p5,1,4]j. --' .-. Our implementation results in an asymptotically optimal algorithm under the assumption 
that N is a polynomial in M. This assumption is easily met in practice. For instance in today s PC market, 
M is typically of the order of megabytes. Disk sizes are of the order of gigabytes. So, it is perhaps 
safe to assume that N < M3. In particular, the number of I/O read steps needed in our algorithm is no 
more than $J 1.m + 11 2. This complex-ity bound is n&#38;t dependent -on t&#38;e abovementioned assumptions. 
If N = MC, for some constant c, and B is small (e.g., M is a polynomial in B) then this bound is O(&#38; 
[ai j. Our implementation is very simple and requires no fancy data structures. The internal memory re-quirement 
is only 3DB. We illustrate with exam-ples that when D is large, LMM performs better than DSM. We also 
believe that when D is large LMM has the potential of comparing favorably to the simple randomized algorithm 
(SRM) recently proposed by Barve, Grove, and Vitter [6]. In addition, we prove that LMM algorithm can 
be used to solve the sparse enumeration sort on the hypercube. Such an implementation is somewhat conceptually 
simpler than Nassimi and Sahni s al-gorithm [14]. In Section 2 we give a description of the (Z,m)-merge 
sort and prove its correctness. In Section 3 we present details of our parallel disk sorting im-plementation. 
Section 4 compares the three algo-rithms DSM, SRM, and LMM. Section 5 is devoted to sparse enumeration 
sort. In section 6 we relate LMM with the column sort algorithm. Section 7 concludes the paper. 2 The 
(Z,m)-merge Sort (LMM) Let Icl,lcz,.. . , lc, be a given sequence of n keys. Assume that n = 2h for some 
integer h. The odd-even merge sort [12, lo], the &#38;tonic sort [5], and the periodic balanced merge 
sort [8] are all very similar. We use the term odd-even merge sort to refer to these algorithms. These 
algorithms have a common theme (up to some slight variations). The odd-even mergesort algorithm employs 
the odd-even merge algorithm repeatedly to merge two sequences at a time. To begin with it forms f sorted 
sequences of length two each. Next, it merges two sequences at a time so that at the end f sorted sequences 
of length 4 each will remain. This process of merging is continued until only two sequences of length 
$ each are left. Finally these two sequences are merged. Step 1. Let U = u~,u~,...,u~ and v = q,v2,...,vq 
be the two sorted sequences to be merged. Unshufie U into two, i.e., partition U into two: u odd = Ul,U3,*-*, 
~~-1 and U,,,, = 212, u4,. . . , uq. Similarly partition V into Vodd and vevem Step 2. Now recursively 
merge U&#38;d with Throughout this paper we use log to denote logarithms to the base 2 and In to denote 
natural logarithms. Vodd. Let X = 51, ~2,. . . , zq be the result. Also merge V,,,, with I&#38;,,,. Let 
Y = Yl,Y27. . ,yq be the result. Step 3. Shufle X and Y, i.e., form the sequence: Z=xl,y1,x2,y2 ,..., 
xcq,yq. Step 4. Perform one step of compare-exchange operation, i.e., sort successive subsequences of 
length two in 2. In other words, sort yr, x2; sort yz, x3; and so on. The resultant sequence is the merge 
of U and V. One can use the zero-one principle to prove the correctness of the above merge algorithm 
(see e.g., [12] or [9]). A n extension of this idea has been employed by Thompson and Kung [21] to design 
an asymptotically optimal algorithm for sorting on the mesh. Their algorithm, called the s2-way merge, 
partitions the given n-element sequence to be sorted into s2 evenly sized parts (for some ap- propriate 
function s of n), recursively sorts each part, and merges the s2 sorted parts. In order to merge s2 sorted 
sequences, the sequences are un-shuffled into two components, namely the odd and even components. Each 
component is merged re-cursively, the results are shuffled, and some local sorting is done. Effectively, 
the problem of merg- ing s2 sequences is reduced to two subproblems, where each subproblem is that of 
merging s2 sub- sequences. The subsequences now will be of length one-half of the length of the original 
sequences. The base case is that of merging s2 sequences of length one each. This case is handled by 
a different algo-rithm. LMM is a generalization of the s2-way merge sort. Here also the sequence to be 
sorted is par- titioned into 1 parts (for some appropriate 1). Each part is recursively sorted. To merge 
these 1 se- quences, the sequences are unshuffled into m com- ponents (instead of two). More details 
follow. Algorithm LMM Step 1. Let K = Ici,kz,. . . ,Ic, be the sequence to be sorted. Partition K into 
 1 evenly sized parts. Let these parts be K = k(i-l)n/l+l, k(i-l)n/l+2, ---7 kin/l, for i = 1,2,..., 
1. Sort each part recursively. Let the sorted sequences be Vi, Us,. . . , Ul. Step 2. Merge VI, U2,. 
. . , Ul using Al- gorithm (1, m)-merge. Algorithm (1, m)-merge Step 1. Let the sequences to be merged 
be u. = u! u? ,ur, for 1 5 i 2 1. 1; r is 6rn% use a base case al-gorithm. Otherwise, unshuffle each 
Vi into m parts. In particular, partition Vi into U~,Uf,...,U~, where Vi = ul ut+m = u2 $+m 2) a , - 
- -1 'Us i,i ,...; and so on. Step 2. Recursively merge U!, Ui, . . . , U/, for 1 5 j _< m. Let the merged 
sequences be Xj = xl x? xlyim, for 1 5 j < m. 3 J J Step 3. Shuffle X1, X2, . . . , X,, i.e., form 
the sequence 2 = zl,zi,. . . ,xk, /1 1rJm x:,x; )...) x&#38; )...) x1lrJm, xlT 2 m,***, Xm - Step 4. 
It can be shown that at this point the length of the dirty sequence (i.e., un- sorted portion) is no 
more than Zm. But we don t know where the dirty sequence is located. We can cleanup the dirty se-quence 
in many different ways. One way is described below. Call the sequence of the first Zm elements of 2 as 
21; the next Zm elements as 22; and so on. In other words, 2 is parti- tioned into .Zi,&#38;,. . . , 
Z,,,. Sort each one of the &#38; s. Followed by this merge 21 and 22; merge 23 and 24; etc. Finally merge 
22 and 23; merge 24 and 25; and so on. Proof of correctness. Note that it suffices to prove the correctness 
of the merge algorithm. We prove the correctness of Algorithm (Z,m)-merge using the zero-one principle. 
Since the algorithm is oblivious, the zero-one principle holds. Assume that the sequence to be sorted 
consists of only zeros and ones. Let the number of zeros in Vi be Zi, for 1 5 i 5 1. The minimum number 
of zeros contributed by any Vi to any Xj (1 < i 5 1; 1 < j 5 m) is 121. The maximum number of zeros contributed 
by any Ui to any Xj is [%I. Thus the minimum number of zeros in any Xj is Zmin = &#38; I%]. The maximum 
number of zeros in any Xj is Zm,, = Ci=r [z]. The difference between zmaz and Zmi, can be at most 1. 
This in turn means that when the Xj s are shuffled, the length of the dirty sequence (i.e., the unsorted 
portion) can be at most Zm. The fact that Step 4 cleans up the dirty sequence is also easy to see. This 
completes the proof of correctness. 0 Observation. The odd-even mergesort is nothing but LMM with I = 
m = 2. Thompson and Kung s [21] s2-way merge sort is a special case of LMM with 1= s2 and m = 2. 3 Parallel 
Disk Sorting The problem of external sorting has been widely ex-plored owing to its paramount importance. 
With the widening gap between processor speeds and disk access speeds, the I/O bottleneck has become 
critical. Parallel disk systems have been introduced to alleviate this bottleneck. Several models for 
parallel disks have been inves- tigated. The model employed in this paper is the one introduced in one 
of the pioneering papers of Vitter and Shriver [22]. In this model there are D distinct and independent 
disk drives. The disks can simultaneously transmit a block of data. A block consists of B records. If 
A4 is the internal memory size, then one usually requires that M 1 2DB. For the algorithms presented 
in this paper, a choice of M = 3DB suffices. Of this, only 2DB amount of memory is used to store data 
to be currently oper-ated on. In the other portion, we store prefetched data in order to overlap computation 
and data ac-cess. From hereon, M is used to refer to only DB. The problem of disk sorting was first studied 
by Aggarwal and Vitter in their foundational paper [2]. In the model they considered, each I/O opera-tion 
results in the transfer of D blocks each block having B records. A more realistic model was en- visioned 
in [22]. Several asymptotically optimal al-gorithms have been given for sorting on this model. Nodine 
and Vitter s optimal algorithm [16] involves solving certain matching problems. Aggarwal and Plaxton 
s optimal algorithm [l] is based on the Sharesort algorithm of Cypher and Plaxton. Vitter and Shriver 
gave an optimal randomized algorithm for disk sorting [22]. All these results are highly nontrivial and 
theoretically interesting. However, the underlying constants in their time bounds are high. In practice 
the simple disk-striped mergesort (DSM) is used [6], even though it is not asymptoti- cally optimal. 
DSM has the advantages of simplic- ity and a small constant. Data accesses made by DSM is such that at 
any I/O operation, the same portions of the D disks are accessed. This has the effect of having a single 
disk which can transfer DB records in a single I/O operation. An g-way mergesort is employed by this 
algorithm. To start with, initial runs are formed in one pass through the data. At the end the disk has 
N/M runs each of length M. Next, &#38; runs are merged at a time. Blocks of any run are uniformly striped 
across the disks so that in future they can be accessed in par- allel utilizing the full bandwidth. Each 
phase of merging involves one pass through the data. There hases and hence the total number of are ltj-&#38;sp 
passes made by DSM is &#38;$$&#38;. In other words, the total number of I/O read operations performed 
by the algorithm is &#38; (1 + m). The con-stant here is just 1. The known lower bound on the number 
of passes for parallel disk sorting is 0 (m). If one as- sumes that N is a polynomial in M and that B 
is small (which are readily satisfied in practice), the lower bound simply yields IR(1) passes. All the 
abovementioned optimal algorithms make only O(1) passes. So, the challenge in the design of par- allel 
disk sorting algorithms is in reducing this con-stant. If M = 2DB, the number of passes made by DSM is 
1 + log(N/M), which indeed can be very high. Recently, several works have been done that deal with the 
practical aspects. Pai, Schtier, and Var- man [17] analyzed the average case performance of a simple 
merging algorithm, employing an ap-proximate model of average case inputs. Barve, Grove, and Vitter [6] 
have presented a simple ran-domized algorithm (SRM) and analyzed its perfor- mance. The analysis involves 
the solution of certain occupancy problems. The expected number RSRM of I/O read operations made by their 
algorithm is such that N In(N/M) In D In In In D 1+1n/c R.SRMI DB I+--l+------+~+w 1nkD kInlnD In In 
D (2 Each run will be striped across the disks. If R 2 D, the starting disk for the ith run is i mod 
D, i.e., the zeroth block of the ith run will be in disk i mod D; its first block will be in disk (i 
+ 1) mod D; and so on. This will enable us to access, in one I/O read operation, one block each from 
D distinct runs and hence obtain perfect disk parallelism. If R < D, the starting disk for the ith run 
is ig. (Assume without loss of generality that D divides R.) Even now, we can obtain f blocks from each 
of the runs in one I/O operation and hence achieve perfect disk parallelism. In practice the value of 
B will be much less than M. For example, if 3 > fl, then the number of read passes made by our algorithm 
is no more than 2w + 1)2. But for the sake of com- ( pleteness, we also consider the case g 5 a. In either 
case, we show that the number of read passes made by our algorithm is upper bounded by [e + 11 . Like 
all the algorithms The algorithm merges R = kD runs at a time, for some integer k. When R = fl(Dlog D), 
the expected performance of their algorithm is opti- mal. However, in this case, the internal memory 
needed is fl(BD log D) . They have also compared SRM with DSM through simulations and shown that SRM 
performs better than DSM. The algorithm presented in this paper is asymp- totically optimal under the 
assumptions that N is a polynomial in M and B is small. The al-gorithm is an implementation of the (Z,m)-merge 
sort. The algorithm is as simple as DSM. We do not need any fancy data structures or prefetching techniques. 
The standard overlapping of computa- tions and I/O operations can be done. The inter-nal memory requirement 
is only 3DB. We demon- strate with examples that our algorithm makes less number of passes than DSM when 
D is large. Our algorithm merges R runs at a time, for some appropriate R. Since our algorithm is also 
based on merging in phases, we have to specify how the runs in a phase are stored across the D disks. 
Let the disks as well as the runs be numbered from zero. h the literature,. our algorithm also forms 
initial runs of length M each in one read pass through the data. After this, the runs will be merged 
R at a time. Throughout, we use T(u, w) to denote the number of read passes needed to merge u sequences 
of length 2) each. 3.1 Some Special Cases We begin by looking at some special cases. Con-sider the problem 
of merging &#38;? runs each of length M, when $$ _> a. Here R = &#38;f. This merging can be done using 
Algorithm (1,m)-merge with 1 = m = m. Let U1,U2,...,U47 be the sequences to be merged. In Step 1, each 
Vi gets unshuffled into &#38;% parts so that each part is of length a. This unshuffling can be done in 
one pass. In Step 2, we have &#38;? merges to do, where each merge in- volves &#38; sequences of length 
m each. Observe that there are only M records in each merge and hence all the mergings can be done in 
one pass through the data. Step 3 involves shuffling and Step 4 involves cleaning up. The length of the 
 dirty sequence is (m) 2 = M. These two steps can be combined and finished in one pass through the data. 
The idea is to have two successive Zi S (c.f. Algorithm (Z,m)-merge) (call these Zi and Zi+r) at any 
time in the main memory. We can sort Zi and Zi+r and merge them. After this Zi is ready to be shipped 
to the disks. Zi+2 will then be brought in, sorted, and merged with Zi+l. At this point Zi+i will be 
shipped out; and so on. Note that throughout we can maintain perfect disk parallelism. Thus we get: 
Lemma 3.1 T(fi, M) = 3, if $$> a. Now consider the case of merging 3 runs each of length M, when g < 
m. To solve this problem, employ Algorithm (Z,m)-merge with 1 = m = 9. Note that we have assumed M = 
DB. Let the sequences to be merged be hU2,*..,UM/B* Step 1 can be done in one pass. Each Ui gets partitioned 
into M/B parts each of length B. Thus there are M/B merging problems, where each problem has to merge 
M/B sequences each of length B. Since the total number of records in any problem is M, these merging 
problems can be solved in one pass. Finally, Steps 3 and 4 can also be done in one pass since the length 
of the dirty sequence is 5 M2/B2 < M. As a result we have Lemma 3.2 2 ($f,M) = 3, if $$ < a. 3.2 The 
General Algorithm Now we are ready to present the general version of the parallel disk sorting algorithm. 
Here also we will present the algorithm in two cases, one for $$ 2 m and the other for s < m. In either 
case, initial runs are formed in one pass at the end of which N/M sorted sequences of length M each remain 
to be merged. If 3 2 a, we employ Algorithm (Z,m)-merge with Z = m = &#38;? and R = &#38;!f. Let K denote 
m and let fi = K2c. In other words, c= w. It is easy to see that T(K2C, M) = T(K, M) + T(K, KM) + *. 
. + T(K, K2C- itq (2) The above relation basically means that we start with K2c sequences of length 
M each; we merge K at a time to end up with K2c-1 sequences of length KM each; again merge K at a time 
to end up with K2c-2 sequences of length K2M each; and so on. Finally we ll have K sequences of length 
K2c-1M each which are merged. Each of these mergings are done using the Algorithm (Z,m)-merge with l=m=&#38;iT. 
Let us compute T(K, KiM) for any i. We have K sequences of length KiM each. Let these se- quences be 
VI, Us, . . . , UK. In Step 1, each Uj is unshuffled into K parts each of size Ki- M. This takes one 
pass. Now there are K merging prob-lems, where each merging problem involves K se-quences of length KimlM 
each. The number of passes needed is T(K, Ki- M). In Steps 3 and 4, the length of the dirty sequence 
is 5 K2 = M. Clearly, this takes only one pass. Therefore, T(K, KiM) = T(K, Ki-lM) + 2. Expanding this 
out we see, T(K, KiM) = 2i + T(K, M) = 2i + 3. We have made use of the fact that T(K, M) = 3 (c.f. Lemma 
3.1). Substituting this into Equation 2, we get 2c-1 T(K2C, M) = c (2i + 3) = 4c2 + 4c i=O where c = 
w. If N 2 M3, the above merging cost is 5 24 passes. We have the following Theorem 3.1 The number of 
read passes needed to sort N records is 1 + 4 (w) + 4w, if 9 2 a. This number of passes is no more log( 
N/M) than log(min{a,M/B}) 12 Now consider the case $$ < d%. Algorithm (Z,m)-merge will be used with 
1 = m = g and R = g. Let Q denote g and let g = Qd. That is, d = w. As before we have T(Qd, M) = T(Q, 
W+T(Q, QM)+. . .+T(Q, Qd-'M) (3) In order to compute T(Q,QM) for any i, note that we have Q sequences 
of length QiM each. Let Ur,&#38;,..., UQ be these sequences. In Step 1, each Uj is unshuffled into Q 
parts each of size Q- &#38;f. This takes one pass. Now there are Q merging problems. Each merging problem 
has Q sequences of length QiblM each. The number of passes needed to perform all these mergings is T(Q,Qi-iM). 
Steps 3 and 4 can be performed in one pass since the length of the dirty sequence is < Q2 < M. Therefore, 
T(Q, QiM) = T(Q, Qi- M) + 2. Expansion of this gives T(Q, QiM) = 2i + T(Q, M) = 2i + 3. We have made 
the substitution T(Q, M) = 3 (c.f. Lemma 3.2). Equation 3 now becomes d-l T(Qd, M) = c(2i + 3) = d2 + 
2d i=o where d = log N M ii&#38;i&#38;* Theorem 3.2 The number of read passes needed to sort N records 
is upper bounded by [iT!$$%g+1127 Pi+** Theorems 3.1 and 3.2 readily yield Theorem 3.3 We cat sort N 
records in 5 [m + I] read passes over the data, maintaining perfect disk parallelism. In other words, 
the total number of I/O read operations needed is I &#38; [,-+I]~. Observation. In Algorithm (1, m)-merge, 
both 1 and m have to be 5 $$ in order to achieve perfect disk parallelism.   4 A Comparison of DSM, 
SRM, and LMM DSM is not asymptotically optimal. For example, if M = 2DB, DSM makes 1 + log(N/M) = fl(log 
N) passes over the data. If R = kD for some constant k, then SRM is also not asymptotically optimal. 
Also, under the assumption M = O(DB) (which is the case for LMM), SRM is not asymptotically optimal. 
However, if R = kD log D, then the ex-pected performance of SRM is optimal. This will mean that the size 
of M has to be fi(BD 1ogD). On the other hand, LMM is asymptotically opti-mal assuming that N is a polynomial 
in M and B is not very large. Both DSM and LMM are deterministic and need only a reasonable amount of 
internal memory. Both are very simple and easy to implement. No ad- ditional data structures or prefetching 
techniques are used. Performance analyses are quite simple. On the other hand, SRM is randomized. Its 
in-ternal memory requirement is 2RB + 4DB + RD. SRM stores additional (though small) information in each 
block and maintains a forecasting data structure. The analysis of SRM involves the so-lution of certain 
occupancy problems. Now we show that LMM can indeed perform bet-ter than DSM when D is large with two 
examples. A fair comparison of LMM with SRM will require simulations. Since for R = D, the number of 
passes made by SRM is not optimal, we speculate that LMM might compare favorably to SRM when D is large. 
In the following examples we won t invoke The-orem 3.3 but rather specialize LMM to get the best possible 
performance. These examples illus-trate that we can get better results than promised by Theorem 3.3. 
In this context LMM should be thought of as a framework for designing parallel disk sorting algorithms. 
 Example 4.1 The first example considered is one with i = 23g; M = 226; B = 21°. Here M/B = 216. We need 
to merge 213 sequences of length 226 each. We apply a (2 13, 213)-merge on the sequences. There will 
be 213 merging problems where each problem involves 213 sequences each of length 213. Thus the mergings 
in Step 2 need only one pass. Step 1 takes one pass. Steps 3 and 4 together need one pass. So LMM takes 
a total of three passes. The number of read passes made by the algorithms for various disk sizes are 
shown in Table 1. I 1 D 1 DSM 1 LMM fl m Table 1. An Example Example 4.2 The next example we consider 
has N = 242; M = 226; B = 21°. We have to merge 216 sequences each of length 226. T(216, 226) = 5! (213,226) 
+ T(23,23g). Both 2 (213,226) and T(23, 23g) can easily be seen to be 3 each using (213, 213) and (23, 
216)-merges, respectively. Table 2 displays the comparison of DSM and LMM for this example. Cl 1 D 1 
DSM 1 LMM fl I. 215 16 6 212 4 6 2 3 6 Table 2. Another Example  5 Sparse Enumeration Sort In this section 
we present a somewhat simpler algo- rithm than that of the classical algorithm of Nas- simi and Sahni 
[14] for sparse enumeration sort. The problem is to sort n elements on an nl+ lk- processor hypercubic 
network, for any k > 0. The algorithm of [14] has a time bound of O(k logn). Our algorithm also has the 
same asymptotic per-formance. 5.1 Some Definitions A hypercube of dimension d, denoted ?fd has 2d processors. 
Each processor can be labeled with a d-bit binary number. A processor labeled u is con- nected to those 
processors with label u such that u and TJ differ in exactly one bit. If we fix some i bits and vary 
the remaining bits of a d-bit binary number, the corresponding pro- cessors fOrIn a subcube ?&#38;j-i 
in &#38;!d.  5.2 The Algorithm The main result of this section needs the following Theorem 5.1 2m keys 
can be sorted on a hyper- cube of size n = 22m in O(logn) time. Proof. 3tzrn can be thought of as consisting 
of 2m copies of 7-l,. Input is given in one of these 3tm s. Broadcast the input so that each 31, has 
a copy of the input. This takes O(logn) time. Each ?l, computes the rank of one input key using the prefix 
sums algorithm. Prefix computation takes O(log n) time. Finally route the keys to their sorted posi-tions. 
This can be done in O(logn) time as well. Cl Theorem 5.2 We can sort n keys in O(klogn) time on a hypercube 
of size nl+ lk. Proof. Let S(n) be the time needed to sort n keys. Let T(zl,v) be the time needed to 
merge u lists of length w each. Also let E = $. To begin with group the elements with n + el-ements in 
each group. Sort each group recursively in time S(n - ). Followed by this sort, merge the resultant n 
sorted lists, each of length n - , em-ploying Algorithm (1, m)-merge with 1 = m = r-8. Let Q = n . Thus 
it follows that S(n) = S (s) +T (Q, i) = S(Q2k-1)+T(Q,Q2k-1) (4) In order to compute T(Q, Qj), for any 
j, consider Step 1 of Algorithm (Z,m)-merge. After unshuf-fling, each Ui will be partitioned into Q parts 
with &#38;j-l elements each. Step 2 will thus take time T(Q, &#38;j-l). Unshuffling in Step 1 and shuffling 
in Step 3 take time O(logQ). In Step 4 the length of the dirty sequence is 5 Q2 and hence Step 4 can 
be completed in O(logQ) time as well (employing Theorem 5.1). As a result, we have T(Q, Qj) = T(Q, &#38;j-l) 
+ O(log Q). Expanding this out, we get T(Q, Qj) = O(j log Q). Substituting this back in Equation 4, we 
see that 2k-1 S(n) = S(Q)+ c W log Q) j=l = O(logQ) +O(k210gQ) = O(klogn) cl  6 Columnsort As A Special 
Case Columnsort [13, 121 has been employed on vari-ous parallel models of computing. This algorithm can 
be described as follows. Let ICI, k2,. . . , k, be the n given numbers. These numbers are thought of 
as forming a matrix M with r rows and s columns.(with r 2 s2). There are 7 steps in the algorithm: 1) 
Sort, the columns in increasing order; 2) Transpose the matrix preserving the dimension as r x s. I.e., 
pick the elements in column major order and fill the rows in row major order; 3) Sort the columns in 
increasing order; 4) Rearrange the numbers applying the reverse of the permutation employed in step 2; 
5) Sort the columns in a way that adjacent columns are sorted in reverse order; 6) Apply two steps of 
odd-even transposition sort to the rows; and 7) Sort each column in increasing order. At the end of step 
7, it can be shown that, the numbers will be sorted in column major order. Columnsort is thus easily 
seen to be a special case of LMM where Z = m = s (with n 2 s3). In Step 1 of columnsort, the input is 
grouped into s parts and each part is sorted (c.f. Step 1 of Algorithm LMM). In Step 2, the sorted subse-quences 
are s-way unshuffled (c.f. Step 1 of Al- gorithm (Z,m)-merge). In Step 3, the unshuffled subsequences 
are sorted instead of being recursively merged (c.f. Step 2 of Algorithm (Z,m)-merge). In Step 4 shuffling 
is performed (c.f. Step 3 of Algorithm (Z,m)-merge). Steps 5, 6, and 7 of columnsort perform Step 4 of 
Algorithm (Z,m)- merge. Note that the crucial differences between LMM and the columnsort are: 1) LMM 
is recursive and the columnsort is not; and 2) The columnsort has 1 = m but LMM is general. In fact in 
Example 4.2, we have I# m. It is also noteworthy that Kunde s algorithm for the mesh [II] somewhat resembles 
the columnsort algorithm. 7 Conclusions We have introduced a new sorting scheme called the (1, m)-merge 
sort,. An implementation of this algo-rithm for the parallel disk sorting problem yields an asymptotically 
optimal algorithm that performs better than DSM and possibly SRM algorithms when the number of disks 
is large. This algorithm is as simple as DSM requiring no nontrivial data structures or prefetching techniques. 
We strongly believe that our algorithm will perform well in prac- tice. We also believe that implementations 
of our sorting scheme to other models will result in sim- ilar performances. We have also shown that 
the columnsort of Leighton is a special case of (1, m)- merge sort.  Acknowledgement This research was 
supported in part, by an NSF Award CCR-95-03-007 and an EPA Grant R-825-293-01-0. The author thanks Peter 
J. Varman for introduc- ing him to the problem of parallel disk sorting and some fruitful discussions. 
The author also thanks the reviewers for their thoughtful comments. References <RefA>[l] A. Aggarwal and C. 
G. Plaxton, Optimal Parallel Sorting in Multi-Level Storage, Proc. Fifth Annual ACM Symposium on Discrete 
Algorithms, 1994, pp. 659-668. A. Aggarwal and J. S. Vitter, The In- PI put/Output Complexity of Sorting 
and Re-lated Problems, Communications of the ACM 31(9), 1988, pp. 1116-1127. M. Ajtai, J. Komlb and E. 
Szemeredi, An PI O(n logn) Sorting Network, Proc. 15th An-nual ACM Symposium on Theory of Comput- ing, 
1983, pp. l-9. L. Arge, The Buffer Tree: A New Technique PI for Optimal I/O-Algorithms, Proc. 4th Inter- 
national Workshop on Algorithms and Data Structures (WADS), 1995, pp. 334-345. PI K. Batcher, Sorting 
Networks and their Appli-cations, Proc. AFIPS Sprang Joint Computing Conference 32, 1968, pp. 307-314. 
PI R. Barve, E. F. Grove, and J. S. Vitter, Sim-ple Randomized Mergesort on Parallel Disks, Parallel 
Computing 23(4-5), 1997, pp. 601-631. PI R. Cole, Parallel Merge Sort, SIAM Journal on Computing 17(4), 
1988, pp. 770-785. PI M. Dowd, Y. Perl, L. Rudolph, and M. Saks, The Periodic Balanced Sorting Network, 
Jour-nal of the ACM 36, 1989, pp. 738-757. PI E. Horowitz, S. Sahni, and S. Rajasekaran, Computer Algorithms, 
W. H. Freeman Press, 1998. PO1 D. Knuth, Searching and Sorting, The Art of Computer Programming, Volume 
3, Addison- Wesley, 1973. Pll M. Kunde, Block Gossiping on Grids and Tori: Deterministic Sorting and 
Routing Match the Bisection Bound, Proc. First European Sym-posium on Algorithms, 1993, pp. 272-283. 
PI T. Leighton, Introduction to Parallel Al-gorithms and Architectures: Arrays-Bees-Hypercube, Morgan-Kaufmann 
Publishers, 1992. WI T. Leighton, Tight Bounds on the Complex-ity of Parallel Sorting, IEEE Transactions 
on Computers C34(4), 1985, pp. 344-354. WI D. Nassimi and S. Sahni, Parallel Permuta-tion and Sorting 
Algorithms and a New Gen- eralized Connection Network, Journal of the ACM 29(3), 1982, pp. 642-667. M. 
H. Nodine and J. S. Vitter, Greed Sort: P51 Optimal Deterministic Sorting on Parallel Disks, Journal 
of the ACM 42(4), 1995, pp. 919-933. WI M. H. Nodine and J. S. Vitter, Large Scale Sorting in Parallel 
Memories, Proc. Third An-nual ACM Symposium on Parallel Algorithms and Architectures, 1990, pp. 29-39. 
V. S. Pai, A. A. Schaffer, and P. J. Varman, WI Markov Analysis of Multiple-Disk Prefetching Strategies 
for External Merging, Theoretical Computer Science 128(2), 1994, pp. 211-239. S. Rajasekaran, Sorting 
and Selection on In- PI terconnection Networks, DIMA CS Series in Discrete Mathematics and Theoretical 
Com-puter Science 21, 1995, pp. 275-296. J.H. Reif and L.G. Valiant, A Logarithmic PI Time Sort for 
Linear Size Networks, Journal of the ACM 34(l), 1987, pp. 60-76. PO1 R. Reischuk, Probabilistic Parallel 
Algorithms for Sorting and Selection, SIAM Journal of Computing 14(2), 1985, pp. 396-409. WI C.D. Thompson 
and H.T. Kung, Sorting on a Mesh Connected Parallel Computer, Commu-nications of the ACM20(4), 1977, 
pp. 263-271. P21 J. S. Vitter and E. A. M. Shriver, Algorithms for Parallel Memory I: Two-Level Memories, 
Algorithmica 12(2-3), 1994, pp. 110-147.</RefA> 
			
