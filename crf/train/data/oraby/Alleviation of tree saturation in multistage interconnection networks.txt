
 Alleviation of Tree Saturation in Multistage Interconnection Networks Matthew Farrens Brad Wetmore 
Division of Computer Science Division of Computer Science University of California University of California 
Davis, CA 95616 Davis, CA 95616 tek (916) 752-%78 tel: (916) 752-2149 fax: (916) 752-4767 fax: (916) 
752-4767 emaik farrens@cs.ucdavis.edu email: wetmore@cs.ucdavis. edu Abstract This paper presents an 
examination of two distinct but complementary extensions of previous work on hot spot contention in multistage 
interconnection networks. The first extension focuses on the use of larger queues at the memory modules 
than are traditionally studied. The second extension explores a simple feedback damping scheme, which 
we refer to as bleeding, which allows selected processors to ignore feedback information. The impact 
of memory queue size, feedback thres­ hold value, and bleeding on system performance (specifically maximum 
bandwidth per processor) is evaluated by analyzing the results of extensive network simulations. These 
results indicate that combining these approaches can significantly improve the effective bandwidth of 
a multistage interconnection network. 1. Introduction In order to achieve the goal of teratlop computing 
speeds by the end of the century, the number of process­ing nodes in a multiprocessor will have to increase 
sub­stantially. However, as the number of processors grows, so does the impact of two problems inherent 
to multipro­cessors how can memory and processors be efficiently connected, and how can the side effects 
of memory con­tention be controlled? There are several ways to interconnect a large number of processors. 
A shared bus memory is the least expensive, but is inappropriate with many processors and memory modules; 
of traffic far exceeds the capacity of a single stantially more costly solution is a crossbar architecture 
for systems the amount bus. A sub­ network in which every processor has a connection to every memory 
module via a set of intersecting wires. Such a network requires at least N2 crosspoint switches (where 
N is the number of processors and memory modules), an impracti­cal solution for large values of N. A 
popular compromise is to use multistage intercomection networks consisting of log~N levels of kxk switches. 
Since these networks are This work was supported by the National Science Founda­tion under Grant CCR-90-1 
1535. inexpensive bandwidth Allison Woodruff Division of Computer Science University of California Davis, 
CA 95616 tel: (916) 752-8878 fax: (916) 752-4767 email: woodmff@cs.ucdavis. edu enough to be feasible, 
and provide sufficient for many processors and memory modules to communicate simultaneously [Sieg85], 
they will be the focus of this paper. Unfortunately, sharing memory leads to memory contention. Furthermore, 
the larger the number of proces­sors, the greater the degree of contention lLee89]. The result in multistage 
interconnection networks is an undesirable phenomenon lmown as hotspots 13WN085]. Hot spots are locations 
(nodes within the network, memory modules, or specific addresses) which receive a nonuniform distribution 
of traffic. This concentration of traffic can be caused by events such as references to shared or global 
variables (e.g. for barrier synchronization or operations on semaphores l@e85, TaYe90]); block transfers 
[Thom861; a coincidental concentration of requests to a single memory module lJ.ee85, NoPf85]; or a coincidental 
concentration of traffic through an internal switch (nonuniform traffic spots, or NUTS l?AKu90]) which 
can occur even if the requests to memory are evenly distributed. The model used in this study assumes 
that queues exist between processors and switches, switches and switches, and switches and memory modules, 
in order to buffer requests (see Figure 1). When nonuniform traffic occurs, the become full and can not 
accept which are blocked will remain hot spot. These queues at these and the effects will propagate queue(s) 
at the hot spot(s) further requests; requests in the switches feeding the switches can also fill, etc., 
backward through the net­ work, forming a tree of nodes whose queues are ftil. Hot spots degrade performance 
of requests to both hot and cold modules. Since a hot module or queue has several pending requests but 
can only service one at a time, the mean service time for these requests is increased. Requests to cold 
memory modules ( cold requests) can encounter full queues; therefore, the latency of the cold requests 
is increased, and the level of conges­tion in the network is heightened PoHa89, SCS090]. This degradation 
is called tree saturation and is compounded by the fact that its onset can be rapid DG&#38;f86] and may 
take a long time to alleviate. In addition, it takes very lit­ tle nonuniform traffic to induce this 
condition lJ%No85, Ston87].  @ 1991 ACM O-89791-459 -7191P~ $01.50 Processor + N +1111+ P --=mH emeryI 
I 2X2 2x2 4 4 000 Switch Switch P Processor -+ N +Illb I in 44 000 o 000 0 000 0 ­ ~L!-Jq 2X2 q 2X2 
000 000 Switch Switch P Processor ~ N +Illb I +4 ­ !7 q Figure 1. Components of a Multistage Interconnection 
Network 2. Previous Approaches requests to the corresponding (hot) memory module. Once the queue length 
falls below the threshold, proces- Several methods of eontmlling tree saturation have sors are informed 
that they may once again issue requests been proposed. Most, however, have been impractical in to the 
formerly hot mcxlule. terms of technology or expense. Three of these methods are described below. This 
can considerably deerease network congestion by limiting requests which would contribute to tree 2.1. 
Combining saturation, and therefore improve the performance of requests to cold modules. Unfortunately, 
because of the One commonly discussed approach to reducing tme inherent Irttencies involved in the network, 
a new batch of saturation is to combine requests to the same address. requests may not arrive before 
the hot module services allBoth hardware [GGKM83, LeK86, PfNo85] and software of its queued tvquests 
(leaving the memory module idle). [TaYe90, YeT87] solutions have been proposed. These Scott and Sohi 
label this undershoot, an analogy from approaches reduce the number of requests to a hot classic control 
systems theory [ScSo90]. memory location, as well as minimize network traffic. Another problem with feedback 
occurs when However, combining has several disadvantage~ several of the processors have saved a number 
of requests large hardware expense 1; lack of sealability in practical for a hot module. When the processors 
are notifted that applications lLee89]; and failure to alleviate tree satura­ the module is again aeeepting 
requests, the stored tion arising from causes other than excessive competition requests will be simultaneously 
released by the proces­for a single address. sors, possibly flooding the network. This re-creation of 
tree saturation is termed overshoot [SCS089, SCS0901. 2.2. Feedback The technique of feedback can be 
used to control 2.3. Limiting ttee saturation as follows: a threshold value for the queues A limiting 
scheme assumes the presence of a global at all memory modules is chosen. When a queue s thres­ arbiter 
which controls the number of requests to each hold is reached, all processors are notified to hold memory 
module that enter the network per cycle. Limit­ing can be used by itself as a scheme to control tree 
lLee et al found that the hardware cost is even Klgher than saturation, although it is difficult to predict 
how many originally suspected, because suggested methods of combining requests to allow per cycle to 
maximize bandwidth and are ineffective, necessitating even more sophisticated hardware minimize the idle 
time of the memory modules. to achieve favorable results [LeK86]. This scheme can be enhanced by using 
limiting in conjunction with feedback to temper both undershoot and overshoot. Undershoot will be avoided 
because there is a constant, albeit reduced, flow of requests to a hot module. Overshoot will be avoided 
because requests to the hot memory module will be released gradually. Unfor­tunately, the drawback to 
limiting is a direct result of its most prominent characteristic: it assumes globat coordi­nation, a 
complex and expensive solution. 3. Extensions to Previous Work This paper examines two methods of controlling 
tree saturation the use of feedback with larger queues at the memory modules, and the use of a damping 
mechan­ism which releases requests gradually into the network. These methods should reduce tree saturation 
occurring from nonuniform traffic to a given memory module, but should not affect tree saturation due 
to nonuniformity of requests to switches internal to the network.  3.1. Larger Queues at the Memory 
Modules Several authors have suggested that larger queues at the memory modules could reduce the negative 
effects of nonuniform traffic. Stone has suggested using large queues at the memory modules in regular 
multistage interconnection networks (i.e. networks without feed­back) [Ston87], Although it has been 
shown that simply increasing memory queue sizes is not an effective method of controlling tree saturation 
[GGKM83, Lee85], this paper shows that this technique is significantly more use­ful when used in conjunction 
with feedback. 3.2. Damping Because hardware costs and complexity of a global arbiter are prohibitive, 
some other form of controlling requests to a hot module is desirable. A better solution [SCS090] is to 
allow each processor to independently limit its requests to hot and/or cold memory modules, using some 
method that (with a high degree of probabil­ity) will reduce tree saturation. By using the queue status 
information, for example, it is possible to limit requests to the hot modules, rather than stopping them 
completely as would occur in a straight feedback scheme. This paWr explores a simplification of the limiting 
scheme proposed by Scott and Sohi, who suggest that whenever a modute is hot, at most 1 request to a 
hot module and 2 requests to a cold module be allowed to enter the network each cycle [SCS089]. Requests 
are damped as follows: at each network cycle, a number of processors are selected and allowed to submit 
a request to the hot module, if they have one pend­ing. Cold requests are not restricted from entering 
the network. We call this technique bleeding. The motivation for this approach stems from a desire to 
keep a small number of hot requests in the network at all times in order to reduce the possibility of 
undershoot. It rdso provides a more gradual release of blocked requests into the net­work, reducing overshoot. 
4. The Simulator In this study we used a slightly modified version of the model-driven simulator used 
by Scott and Sohi, thus allowing our results to be directly compared to their work [SCS089, SCS090]. 
  4.1. The Simulation Model The simulation model assumes an equal number of processom (P) and memory 
modules (M); in this study M and P are set to 256. The processors and memory modules are connected by 
a standard Omega network with 2x2 crossbar blocking switches. The number of lev­els in our network is 
log2256 = 8, implying a total of 128x8 = 1024 switches (nodes). Each node is capable of receiving one 
request from each node input per cycle, and will place that request in a FIFO queue in time for the next 
cycle queues can accept up to two inputs per cycle, as in LeK86, Lee89]. Each queue feeds the next stage 
of the network, and can hold up to four elements. The queue that lies between the final stage of the 
network and the memory modules is called the memory queue (tnq) and has a variable length. A queue length 
of four is the same length selected by several previous studies [KuPf86, PfNo85, SCS090]. The appropriateness 
of a small queue size is shown by both Lee and Gottlieb, who suggest increasing the queue size beyond 
size five 13-ee85] or size eight [GGKM83] does not help alleviate tree saturation. (However, neither 
study takes feedback into account.) The model also assumes two complete interconnec­tion networks, one 
to propagate requests from the proces­sors to the memory modules, and one for the reverse trip. Each 
processor/memory module has a network interface which interacts with both the forward and the reverse 
net­works and is responsible for requests being released or received. Each processor may make at most 
one request per network cycle, via its processor network interface (PM), and there is no limit on the 
number of requests a processor may have outstanding. The memory modules are assumed to have a latency 
equal to one network cycle. In order to force hot spots to exist within the net­work the simulator directs 
a user-selectable proportion of the requests to a designated memory module. This module is called the 
hot module, although it may be either hot or cold at any point during the simulation. The model does 
not differentiate the percentage of time spent in hot spot versus uniform traffic conditions. It has 
been noted that hot spots are transient @K86], but the amount of time spent in different states has not 
been fully explored, and is not examined here. We assume a steady-state representation. Further, the 
model is some­what optimistic in forcing only one hot spot at a time. It will allow more than one hot 
spot to arise, but this will only happen if the random distribution of uniform requests leads to an additional 
hot spot. In practice there may be nonuniform requests to several modules simul­taneously, although Pfister 
and Norton suggest that typi­cally the~ are only one or two memory modules hot simultaneously rfNo85]. 
 5. Simulation Results The simulator was run under a variety of condi­tions, varying parameters such 
as memory queue size, feedback threshold, hot rates, degree of bleeding, etc. In discussing our results, 
we use the following variables, chosen to allow comparison with previous models ~085, SCS089, SCS090]: 
N: The number of processors and memory modules. n: The number of levels in the network. (n= log.JV). 
P~: The fraction of hot processors: those processors making nonuniform requests to a hot memory module. 
h: The percentage of nonuniformity of requests from the hot processors. h is also known as the hot rate. 
The uniform requests from all processors are distri­buted evenly among all memory modules, including 
the hot module. Tf: The threshold value for feedback. When the number of elements in a module s queue 
exceeds this threshold, processors are notified that that module has become hot. mq The size of the queue 
at the last level of the net­work. Each processor will attempt to offer a request dur­ing each network 
cycle. If the network cannot accept the request, either because the Iirst-stage queue has no room, or 
because the request is to a forbidden hot spot, the pro­cessor must wait to resubmit. For each processor, 
effec­tive maximum bandwidth is calculated by the following formuk Mw mum Bandwidth= Total Number of 
Requests TIMExN where TIME is the simulated number of network cycles elapsed, accumulated from the first 
cycle in which the request was generated. The maximum bandwidth of a network can only approach 1.0, even 
if there are no hot spots, because collisions in both the nodes and the memory modules occur even under 
uniform traffic [DiJu81]. In these simulations, the term relative bandwidth refers to the performance 
of the modified network under study compared to that of a standard Omega network with no modifications. 
Relative Bandwidth = Bandwidth of Modrjied Network Bandwidth of Unmodified Network Therefore, if the 
maximum bandwidth of a modilied network is .76, and that of an unmodified Omega network is .38, the relative 
bandwidth of the net­work under study is 2.0 (meaning the new method effec­tively doubled the bandwidth). 
5.1. Effects of Increasing the Memory Module Queue Size Figure 2 shows the results of varying the size 
of the queue at the memory module (the nzq) with no feedback mechanism provided. The dashed line represents 
the theoretical degradation which would occur if tree satura­tion did not affect the bandwidth [SCS090], 
i.e. if all degradation could be attributed to the fact that the hot memory module can process only one 
request per cycle. As expected, increasing the size of the memory queue provides virtually no improvement 
in bandwidth. Without feedback, a large queue experiences the same problems that smaller queues do; the 
only difference is the amount of time before saturation occurs. As shown in lJ.A%5] and [GGKM83], employing 
larger memory queues without supplying some kind of feedback mechan­ism does not eliminate saturation 
conditions (although it might be useful for reducing the effects of transient hot spots). In order to 
test how well feedback would work in conjunction with larger memory queues, another set of simulations 
was performed. Figure 3 shows the results of simulations with a hot rate of 2~0 and various memory queue 
sizes and threshold values. Data is presented for the most effective threshold values, T~l,2,3,4. In 
Figure 3, the line of mq = 4 can be directly com­pared with Scott and Sohi s work, and is consistent 
with their results. From the figure we see that larger values of mq yielded relative bandwidths greater 
than those of Scott and Sobi. As expected, the relative bandwidth is close to 1.0 for high values of 
pk. In this case, a hot memory module s latency, and not tree saturation, is the bottleneck. The relative 
bandwidth for nearly uniform traffic (low values of I h) never passes 1.0. This stems from the fact that 
if no processors are directing their requests to a particular module, the requests will be approximately 
uniform over the memory space. Tem­porary hot spots in random modules may occur, but a sin­gle module 
should not receive a disproportionate amount of traffic. The additional buffer space provided by a large 
memory queue will rarely be used, and larger queues will not substantially improve system bandwidth. 
M a x i m u m 1.0 0.8­ 1 0 mq=d + mq=32 A mq=128 i O mq.d + mq=32 -., A mq=128 . . . . . . ..> --------­*r 
---------------------------------------­ . . -. .. ___________________. --------­ ... .. . . ... . .. 
-.. .. ... =-------------------------­::..__-----_--­ 0.0 ~ 1 d 0.0 0.2 0.4 ().6 ().8 1.0 0.0 0.2 0.4 
().6 0.8 1.0 fraction of hot processors fraction of hot processors (a) (b) Figure 2. Maximum Bandwidth 
per Processor in a Regular (a)h=2%, (b)h=8% Omega Network The lower values of Tf, 1 and 2, give relative 
bandwidths substitidly below 1.(), evidencing undershoo~ under uniform traffic conditions, lower thres­hold 
values tend to decrease network bandwidth since a random access pattern could declare a module hot when 
in fact it was experiencing a temporary locality of refer­ence. In this case, requests that would not 
degrade the network are forbidden to enter, reducing relative bandwidth to values lower than 1.0. Conversely, 
the higher the threshold, the more time will elapse before requests are forbidden to enter the net­work. 
Note that even under the most intensive nonuni­form request conditions, the net gain in a module s mq 
length is only one request per cycle since the mq is fed by only two switches and the module processes 
one request per cycle. Since the onset of tree saturation is rapid, pro­cessors may be not be warned 
to stop issuing requests to the hot module until saturation is nearly complete. For example, if half 
of the 256 processors are making nonuni­form requests with a hot rate of 8Y0, an average of ten requests 
are generated to the hot module every network cycle. By the time a threshold of 8 is reached, approxi­mately 
80 requests have been issued to the hot module, meaning the network has been clogged with requests to 
a single module. Having a large mq does not solve this problem entirely, since only two requests can 
enter an mq per network cycle. Therefore, as we increase the threshold value, we also limit the bandwidth 
for the middle values of P*; simulation shows higher thresholds yield lower throughput. Unfortunately, 
as previously described, lower thresholds give false hot readings which imoduce more undershoot. These 
effects can be seen in Figure 2: Tf. 4 generates no undershoot, but has lower relative bandwidth improvements 
than the other threshold values. Therefore, Tf = 3 seems optimal, demonstrating little undershoot and 
yielding the greatest relative bandwidth improvements. Figure 3 also shows that only small amounts of 
bandwidth improvement are achieved for middle values of ph. This can be directly attributed to the low 
hot rate. With a hot rate of only 2%, tree saturation occurs less fre­quently and is less severe than 
with higher hot rates. In Figure 2, the distance between the actual curve and the theoretical curve (the 
dashed line) represents the rmm for itnpmvement if all tree saturation is removed. This mar­gin is much 
larger for h=8% than for h=2%. Figure 4 shows the results of set of simulations with a higher hot rate 
(h = 8%). We chose to simulate this higher hot rate on the basis of current trends in multipro­cessing 
systems. As the number of processors in a system becomes larger, it is anticipated that the severity 
of tree saturation caused by a given hot rate will increase, lead­ing to decreased network performance 
[SCS090]. By using the higher hot rates in smaller systems, we can esti­mam-the pe;ormance of lower hot 
rates in largers ystems. ---------------------------------------------------~--------------------------------------------------­ 
2.0 R e 1 a t 1.5 +""------------------------------------------------­i >-x­v e B I a I n d w 0.5-----------------------------------------------------" 
+mq. f# o mq=32 t +mq=4 0 mq=32 : omq. a vmq=64 omq=8 Vmq. ~ Amq=16 x mq =128 Amq =16 x mq =128k 0.0, 
,,t,t ,1 ,, ,r1 0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9 fraction of hot processors Ilaction of hot processors 
(a) (b) ._-......-_ .......--_.. -.--- ----------------------­ 2.0 R e 1 a t 1.5 i v e 1.0 B ---+3 
a n d w 0.5 +mq=4 o rnq =32 +mq=4 o rnq =32 : omq.a Vmq.~ o mq. a Vmq. i$f timq =16 x mq = 128 Amq=]6 
x mq = 128:0.0 *,, 1 0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9 fraction of hot processors fraction of hot 
processors (c) (d) Figure 3. Maximum Bandwidth relative to a regular Omega network, using straight feedback, 
h=2%. (a) TF1, (b) I&#38;4(c) Tf=3, (d) Tf=4. As in Figure 3, the relative bandwidth is unaffected It 
is also interesting to note that as the aueue sizes by mq size for-highest values of Ph. However. the 
effects increase exponentially, the relative bandwidth only of undershoot are less pronounced since fewer 
false hot increases (@proximately) linearly. The reason for the readings occur than for h=2%. Additionally, 
the results smalter return may be that as the larger queues are able to for the middle value of PA are 
much more impressive for handle the request overflows, the number of times and this higher hot rate, 
showing a relative bandwidth degree to which the network becomes saturated increase of over 3.0. decreases. 
Since there is less saturation, the amount of improvement gained by adding additional queue elements 
. -.. . -.. drops. 1 heretore, at some point, it may become cost-5.2. Using Bleeding in Conjunction with 
Feedback to ineffedive to further increase the size of the mq. In fact, Control Undershoot and Overshoot 
preliminary runs with larger queue sizes were performed, Overshoot and undershoot can be seen as results 
of with no appreciable difference in results observed. the bursty nature of the model when feedback is 
used to abruptty stop and start requests from being released into the network. A better idea is to promote 
a smoother flow of requests by allowing a small number of hot requests to 4.0 --------------------------------------------------- 
RT e 1 3.5 --------------------------------------------------­t a ----------------------L-?-~--_-_---_--_---_-_­ 
t 3.0- -------------------w-+%-----------------­i/ v \-----------­ e B a n ,.-----------------------------------­d 
w 1.o-­ 1 _________________________________________________ +rnq.4 0 mq =32 : 0.5­ omq. s v re q.@ o 
mq.8 v mq. ~ : Amq=]6 x mq = 128 Amq =16 x mq = 128 0.0 B 1 1 0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 ().9 
fraction of hot processors fraction of hot processors (a) (b) 4.0 --------------------------------------------------- 
 R T e 1 3.5 --------------------------------------------------­a t t -------------------------&#38;-----------------------­ 
/x­ i x v e d B a n d w 1.0 +mq.d 0 mq =32 : omq.8 Vmq. ~ : Amq =16 x mq =128 ,, , rt i-EEdzL  0.1 
0.3 0.5 0.7 ().9 0.1 0.3 0.5 ().7 0.9 fraction of hot processors fraction of hot processors (c) (d) Figure 
4. Maximum Bandwidth relative to a regular Omega network, using straight feedback, h=8%. (a) TF1, (b) 
Tp2, (c) Tf=3, (d) Tf=4. ignore the thresholding scheme and enter the network every cycle. This bleeding 
of requests into the network has the additional advantage that it decreases the latency of these hot 
requests which are permitted to enter the net­work during hot spot conditions. The most promising such 
scheme is a round robin one in which, during each cycle, a specdic number of hot processors may issue 
a blocked hot request. Cold requests are not limited, and may be released by any pro­ cessor. Figure 
5 shows the results obtained by a bleeding scheme in which one processor per cycle is permitted to make 
a request to the hot module, if it has one pending. The results show a marked improvement over the straight 
feedback scheme. This is partially because overshoot is decreased. Although the degree of undershoot 
seems to be unaffected by the addition of bleeding when traffic is uniform, undershoot is lessened for 
the middle and higher values of f h, contributing to the increased relative bandwidth. Threshold size 
does not have the impact on perfor­mance at the middle values of ph that it did in the straight feedback 
scheme. Because additional requests are in transit, the queues rarely empty to leave the memory module 
idle. There is obviously an optimal number of requests to bleed to the network per network cycle. Performance 
plummets when the simulation allows two hot requests to enter the network per cycle, eliminating most 
of the beneficial effects of feedback because the network can not clear and tree saturation is enhanced. 
The optimal number appears to be one request per cycle, since that is the latency of the memory module 
in servicing requests. 6. Hardware Requirements One of the goals of this study was to examine tree saturation 
limiting schemes which could be implemented easily and inexpensively (unlike combining, which has been 
estimated to increase hardware cost by a factor of between 6 and 32 W085], or global limiting, which 
not only increases hardware cost and design difficulty, but also potentially lengthens the critical path 
for submitting requests to the network). A brief estimate of the hardware complexity of the approaches 
examined in this paper are presented below. 6.1. Feedback Measuring the size of queues to see if they 
have exceeded their threshold value can be implemented trivi­ally. A bus to allow the memory/network 
interface to communicate with the processor/network interface (PNI) each time a module changes temperature 
is slightly more complicated; since multiple transitions can occur per cycle [SCS090], the memory/network 
interface may have to wait for the bus. However, the bus will only requhe O(logN) wires to uniquely identify 
the memory module whose temperature has changed. Finally, a buffer with a list of hot modules can be 
implemented at the PNIs (if the scheme allows more than one module to be designated hot at any given 
time). The size of the buffer is dependent on the number of hot spots expected to occur simultaneously. 
 6.2. Large Queues at Memory Modules Increasing the size of the queues at the level of switching nodes 
closest to the memory modules entails minimal cost in actual components. While the memory queues will 
be slightly different than the other queues, the design and fabrication of this additional component 
should be trivial. 6.3. Damping The bleeding scheme presented in this paper can be implemented in a 
straightforward manner. Each PNI keeps an internal counter which increments on each syn­chronous network 
clock pulse and runs horn O to N-1. On each network cycle, the PNIs compare the processor id and internal 
counter to either permit or forbid a request from entering the network. 7. Future Work There are a number 
of topics yet to be investigated. Hot spots on the return network should be examined in non-combining 
networks; in UK86], it was discovered that with combining, return networks needed larger queues because 
requests came out in bursts. Further, the effects of caching have yet to be examined, although Pfister 
and Norton suggest that caching does not help to alleviate tree saturation related to global variables 
(since they can not be cached) rfNo85]. In addition, an examimtion of the literature reverds that the 
frequency of occurrence of hot spots and degree of hot rate have not been explored in detail. A study 
of the percentage of time spent in various hot spot condi­tions, for different values of Ph and h, would 
facilitate performance analysis of practical applications of any tree saturation mechanism. Additional 
research is also needed regarding the behavior of tree saturation (e.g. onseg fre­quency, effect on bandwidth) 
in larger networks. We are currently investigating a variation of bleed­ing in which each processor is 
allowed to issue a request to a hot module with a given probability, for example l/N. We are also trying 
to ascertain the optimal mq size. Addi­tionally, we are looking at an extension of feedback designed 
to eliminate undershoot and overshoot, which is to use different threshold values for temperature transi­tions. 
- 4.0 ---------------------------------------------------- R TT x. e 1 3.5I&#38;-------------------%-/-+-<-----------------------------. 
-~---: a t 3.-) -------------_--_/_------::-.:-= ~------_-_---. 2.5 ----------;~-+,.- e 2.0 -------~ 
~ ~----------------------------------- B vi r -----T---­ at ~&#38;---------------­+Y- Y -----------­ 
q- ------------------------------------------­ n 15t--r----------------------------------------Y-,{ 
d 1/4 / w1.0Lh ~mq.d 0 mq =32 +mq=4 o ~=32 r--------------------------- -- Y ; 0.5-omq=B V mq. @# o 
mq. B vmq=64 Amq =16 x mq =128 Amq =16 x mq =128 ; 0.0, It 0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9 4.0 
T----"R e 1 a t i v e B a n d w : 0.5 i .-\ U.u: fraction of hot processors (a) ----------------------------------------------­ 
fraction of hot processors (b) ~___ -----------------------------------------------­-~%-------------------­ 
+mq=4 omq. Amq=lb a o mq V mq x mq =32 .&#38;f = 128 # r --------------------------------------------------­+mq.4 
1 o mq. s Amq=]6 1 0 V x mq mq. mq =32 ~ = 128 t 0.1 0,3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9 fraction of 
hot processors fraction of hot processors (c) Figure 5. (d) Maximum Bandwidth retative to a regular Omega 
network, straight (a) T~l, (b) Tf2, (c) Tf=3, (d) T+. feedback + bleeding, h=8%. 8. Conclusions The 
techniques examined in this paper have shown that chamatic improvements in the overall bandwidth of multistage 
interconnection networks can be made with relatively little expense. Simulations have shown that by using 
a larger memory queue size in conjunction with feedback, relative bandwidth can improve by up to a factor 
of three. The addition of a simple bleeding scheme boosts performance even further, yielding relative 
bandwidth of over 3.7. This is especially significant given the minimal hardware complexity of the described 
methods. 9. Acknowledgements We would like to express our profound gratitude to Steven ScotI and Gurindar 
Sohi, for loaning us a copy of their network simulator and providing guidance while we were designing 
our simulations. Their help was invalu­able. We would also like to thank Ron Maeder for his time and 
efforts. References <RefA>[DiJu81] D. M. Dias and J. R. Jump, Analysis and Simulation of Buffered Delta Networks 
, IEEE Transactions on Computers, vol. C­ 30:4 (April 1981),pp. 273-282. [GGKM83] A. Gottlieb, R. Grishman, 
C. P. Kruskal, K. P. McAuliffe, L. Randolph and M. Snir, The NYU Ukracomputer -Designing an MIMD Shared 
Memory Parallel Computer , IEEE Transactions on Computers, vol. C-34:1O (February 1983), pp. 175-189. 
 [KuPf86] M. Kumar and G. F. Ptister, The Onset of Hot Spot Contention , International Conference on 
Parallel Processing(1986), pp. 28-34. MKu90] T. Lang and L. Kurisaki, Nonuniform Tmffic Spots (NUTS) 
in Multistage Interconnection Networks , Journal of Parallel and Distributed Computing(1990), Pp. 55-67. 
l@e85] R. Lee, On Hot Spot Contention , ACM Computer Architecture News, vol. 13:5 (1985), pp. 15-20. 
DK86] G. Lee, C. P. Kruskal and D. J. Kuck, The Effectiveness of Combining in Shared Memory Pamllel Computers 
in the Presence of Hot Spots , International Conference on Parallel Processing(1986), pp. 35-41. lLee89] 
G. Lee, A Performance Bound of Multistage Combining Networks , ZEEE Transactions on Computers, vol. C-38:10 
(October 1989),pp. 1387-1395. llloPf85] A. Norton and G. F. Pfister, A Methodology for Predicting Multiprocessor 
Performance , International Conference on Parallel Processing(1985), pp. 772-881. l_PfNo85] G. F. Mister 
and V. A. Norton, Hot Spot Contention and Combining in Multistage Interconnection Networks , IEEE Transactions 
on Computers, (October 1985),pp. 943-948. vol. C-34:10 roHa89] A. Pombortis and C. Halatsis, Behaviour 
of Circuit-Switched Multistage Networks in Presence of Memory Hot Spot , Electronic Letters, vol. 25:13 
(June, 1989), pp. 833­834. [SCS089] S. L. Scott and G. S. Sohi, Using Feedback to Control Tree Saturation 
in Multistage Interconnection Networks , Proceedings of the 15th Annuul Symposium on Computer Architecture(1989), 
pp. 167-176. [SCS090] S. L. Scott and G. S. Sohi, The Use of Feedbaek in Multiprocessors and Its Application 
to Tree Saturation Control , IEEE Transactions on Parallel and Distributed Systems, vol. 1:4 (October 
1990), pp. 385-399. [Sieg85] H. J. Siegel, Interconnection Networks for Large-Scale Parallel Processing: 
Theory and Case Stuales, Lexington BOOkS, Lexington, MA, (1985). [Ston87] H. S. Stone, High-Performance 
Architecture, Addison-Wesley Company, Reading, MA, (1987). Computer Publishing [TaYe90] P. Tang and P. 
Yew, Software Combining Algorithms for Distributing Hot-Spot Addressing , Journal of Parallel and Distributed 
Computing, VO1. 10 (1990), PP. 130-139. [Thom86] R. H. Thomas, Behavior of the Butterfly Parallel Processor 
in the Presence of Memory Hot Spots , International Conference on Parallel Processing(1986), pp. 46-50. 
[YeT87] P. Yew, N. Tzeng and D. H. Lawrie, Distributing Hot-Spot Addressing in Large-Scale Multiprocessors 
, IEEE Transactions on Computers, vol. C-364 (Aprd 1987), pp. 388-395.</RefA>   
			
