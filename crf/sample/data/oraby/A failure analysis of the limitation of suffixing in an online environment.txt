
 A Failure Analysis on the Limitations of Suffixing in an Online Environment Donna Harman Lister Hill 
Center for Biomedical Communications National Library of Medicine Bethesda, Maryland, 20209 Abstract 
The interaction of suffixing algorithms and ranking techniques in retrieval performance, particularly 
in an online environment, was investigated. Three general propose suffixing algorithms were used for 
retrieval on the Cranfield 1400, Medlars, and CACM collections, and the results analysed with several 
standard evaluation measures. An examination of the retrieval performance using suffixing suggested two 
modifications to ranking techniques: variable weighting of word vari- ants and selective stemming depending 
on query length. The experi- mental data is presented, and the limitations of suffixing in an online 
environment is discussed. 1. Introduction Traditional statistically-based keyword retrieval systems have 
been the subject of experiments for over thirty years. The use of simple key- word matching as a basis 
for retrieval can produce very acceptable results, and the addition of ranking techniques based on the 
frequency of a given matching term within a document collection and/or within a given document adds considerable 
improvement [SPARCK-JONES72; SALTON83]. Many other experimental methods have been implemented to improve 
performance beyond that of straight-forward ranking tech- niques. These methods fall into one of four 
categories: 1) enhancement of the query or document terms, 2) use of a more complex ranking or term weighting 
technique, 3) modification of the retrieval space, such as by document clustering, and 4) interaction 
with the user. The IRX (Information Retrieval Experiment) project [BENSON86] has been investigating how 
the performance bounds of traditional statistically- based keyword retrieval systems might be extended. 
Some of the ini- tial research was in ranking algorithms, both in determining what fac- tors are important 
in ranking, and in effectively combining these fac- tors to maximize performance [HARMAN86]. Subsequently, 
work has been done using the most common method of query/document enhancements: the conflation of word 
variants using stemming. This area was selected both because of the wide- spread use of stemming, and 
because the interaction of ranking tech- niques and stemming techniques has not received much experimental 
attention. Word stems have been used interehangably with full words, Permission to copy without fee all 
or part of this material is granted provided that the copies are not made or distributed for direct commer- 
cial advantage, the ACM copyright notice and the title of the publica- tion and its date appear, and 
notice is given that copying is by permis- sion of the Association for Computing Machinery. To copy otherwise, 
or to republish, requires a fee and/or specific permission. &#38;#169; 1987 ACM 08979t-232-2/87/0006/0102--75Â¢ 
and little effort has been made to modify the ranking techniques to ban- die the problems posed in ranking 
when using a stemmer. The flexibil- ity of the IRX retrieval system permits easy modification of the 
ranking routines to use all information flom a query, allowing a thorough inves- tigation into the limits 
of retrieval using suffixing. In addition to doter- mining the performance bounds of suffixing, the appropriateness 
of suffixing in an online environment was examined. Evaluation of suffixing has usually been done with 
batch retrieval systems, using standard recall/precision techniques. It is not obvious that word variant 
conflation, which is basically a recall device, is useful in an interactive environment. The conflation 
of word variants using suffixing algorithms is one of the earliest enhancements to statistical keyword 
retrieval systems [SALTON68], and has become so standard a part of most experimental systems that many 
papers neglect to mention the use of suffixing, or to indicate which algorithm was used. Suffixing was 
originally done for two principle reasons: the large reduction in storage required by a retrieval dictionary 
[BELL79], and the increase in performance due to the use of word variants. Recent research has been more 
concerned with performance improvement than with storage reduction. Most research done on suffixing has 
been in the area of new suffixing algorithms, particularly algorithms aimed at specific collec- tions 
or subject areas. Major work has been done in the area of medi- cal English [PACAK78]. A technique for 
developing a stemming algorithm for a specific collection was done for CITE, the NLM end- user interface 
to the CATLINE book catalog file [ULMSCHNEIDER83]. Another technique for specific corpuses which was 
developed by Hafer and Weiss [HAFER74] used statistical properties of successor and predecessor letter 
variety counts. Several stemming algorithms have become widely used in the research community. A basic 
algorithm conflating singular and plural word forms, referred to as an "S" stemming algorithm, is commonly 
used for minimal stemming. The SMART system uses a version of the Lovins stemmer lLOVINS68] that removes 
over 260 different suffixes, producing considerable word variant conflation. Porter [PORTER80] developed 
a stemming algorithm that looks for about 60 suffixes in a multistep process, producing word variant 
conflation intermediate _ between a simple singular-plural technique and Lovins algorithm. The Lovins 
and Porter algorithms were compared [LENNON81] using several collections, including the Cranfield collection 
(titles only) and were found to make very little difference in retrieval performance. The research reported 
on in this paper addresses both the issue of adapting ranking techniques to better handle word variant 
conflation, and the issue of the appropriate use of stemmers in an interactive environment. !07 2. Methodology 
Abstracts and floes from the Cranfield 1400 collection, with 225 queries, comprised the test collection 
for this study. Additionally, the Medlars collection with 30 queries and 1033 documents, and the CACM 
collection with 64 queries and 3204 documents were used to provide information about the variation of 
stemming performance across different subject areas and test collections. The IRX retrieval system used 
in this study parses each query into noncommon terms. When a stemming algorithm is used, the noncom- 
mon terms are mapped to a list of all words contained in the document collection that have the same word 
stem (as defined by the particular stemming algorithm in effect). These additional word variants are 
then added to the query, and a list of all documents containing one or more of the query terms serves 
as input to the ranking algorithm. The ranking algorithm uses term weighting; that is, a document is 
ranked according to a normalized score formed by summing the weights of the terms occurring in the document 
that match terms in the query. This is equivalent to a weighted inner product between the query and the 
documenL These weights are based on both the impor- tance of the term within a given document (as measured 
by the log 2 of the frequency of the term in that document), and on the importance of a term within the 
entire collection (as measured by either its noise value or its inverse document frequency weight). The 
ranking equations are given in the Appendix, and details of the ranking method can be found in [HARMAN86]. 
Three stemming algorithms were investigated: the Lovins stem- ming algorithm, the Porter stemming algorithm, 
and an "S" stemming algorithm. These three algorithms were selected both because of their general acceptance, 
and because they have different levels of word conllation, a variable that was expected to be significant 
in stemming performance. The development of a new stemming algorithm was not considered at this stage 
of research. 3. Evaluation Two types of evaluation methods were used: standard recall/precision measures, 
with SMART system averaging [SAL- TON83], and a method more suited to an interactive retrieval environ- 
menL Since the evaluation was to reflect needs of online users, the fol- lowing hypothetical interactive 
system was assumed to be in place: 1) the user constructs a query and inputs it to the system; 2) the 
system, using the best ranking scheme available, returns a ranked list of the top document titles, using 
the full words from the query; 3) the user makes relevancy decisions on these documents, either based 
on the rifles, or by machine browsing the documents; and 4) if more documents are desired by the user, 
the system attempts to provide both some advice and several "user-aids" in a multiple- window interface. 
This type of interactive system would allow the users to scan titles of a group of documents a screenful 
at a time, so that the ranking of individual documents within the screenful is not as important as the 
total number of relevant titles within a screen. Also, the number of relevant in the first few screens 
is far more important for the user than the number of relevant in the last screenfuls. Therefore, a measure 
was selected which evaluates performance at given rank cutoff points such as those corresponding to a 
screenful of document titles. The E meas- ure [RLISBERGEN79] is a weighted combination of recall and 
preci- sion that evaluates a set of retrieved documents at a given cutoff, ignor- ing the ranking within 
that set. The measure may have weights of 0.5, 1.0 and 2.0 which correspond, respectively, to attaching 
half the impor- tance to recall as to precision, equal importance to both, and double importance to recall. 
A lower E value indicates a more effective per- formance. A second measure applicable to the interactive 
environment is the number of queries that retrieve no relevant documents by a given cutoff. This is important 
because many types of user aids, such as relevance feedback, require relevant documents to work well. 
A third meast~e, the total number of relevant documents retrieved by a given cutoff, was also calculated. 
Cutoffs of ten and thirty documents were used~ with ten reflecting a minimum number a user might be expected 
to scan, and thirty being an assumed upper limit of what a user would scan before query modification. 
These measures were all used in [CROFI'83] as complementary measures to the standard recall/precision 
evaluation. 4. Results 4.1 Initial stemming results The three stemmers were used in the traditional suffixing 
manner. All terms mapping to the same root for a given stemmer were treated as identical, that is, the 
root was assigned a number of postings corresponding to the total number of postings for all terms mapping 
to that root, the frequencies of all terms mapping to the root in a given document were summed, and the 
noise value of the root was set to the maximum noise value of any term mapping to that root. (See Appendix 
for the definition of the noise value.) This creates an environment for retrieval using stemming that 
is equivalent to the traditional vector based systems such as SMART. The results for the Cranfield collection 
are given in Table 1. It can be seen that there is a very significant increase in performance (42.3%) 
from a simple ranking technique (number of term matches between the queries and the documents) to a technique 
using term weighting (fuLl words and the noise rafiking algorithm, see Appendix). None of the three suffixing 
techniques, however, achieve any further major improvement. A standard recall/precision evaluation shows 
an average precision improvement over no stemming of 1.6% for Lovins, 4.2 for Porter, and 5.3% for the 
S stemmer, using the noise ranking technique, and 2.7% for Lovins, 7.3% for Porter, and 6.3% for the 
S stemmer using the IDF ranking technique. The E measures (for all weights) indicate no substantial difference 
between full word retrieval and retrieval using suffixing. The measure of the number of queries that 
fail at the 10- and 30-document cutoff does show that suffixing con- sistently results in fewer failures. 
The measure of total relevant retrieved at certain cutoffs shows lower totals at the 10-document cut- 
off, but slight improvements for all stemmers at the 30-document cut- off. The Medlars collection (Table 
2) indicates a 4.5% improvement in average precision over full word retrieval using the Lovins stemmer, 
a 0.6% using Porter, and a 1.7% improvement using the S stemmer, with the noise ranking technique, and 
4.7% for Lovins, 0.0% for Porter, and 1.3% for the S stemmer using the IDF ranking technique. The E meas- 
ures show even less improvement, with some degradations in perfor- mance. No queries fail for Medlars 
using full word retrieval, so this measure cannot be used to measure any improvements (the Lovins stemmer, 
however, fails on 1 query at the 10-document cutoff). The total relevant retrieved is slightly increased 
with stemming, but only at the 30-docurnent cutoff. The CACM collection (Table 3) shows more variation 
in perfor- mance for suffixing, especially using the IDF ranking technique. The improvement using the 
IDF ranking technique is 11.1% for Lovins, a substantial improvement, with much less improvement for 
the Porter algorithm, and a degradation in performance for the S stemmer. The E measures show consistent 
improvements for the Lovins stemmer at both cutoffs. The other stemmers, however, display poorer perfor- 
mance using the E measure. The number of queries that fail does not change much for stemming, but also 
does not change much after many more documents are retrieved, so this measure is of little use for evaluation 
of stemming. For the last measure, the number of relevant documents re,eyed, only the Lovins stemmer 
consistently improved performance at both the 10- and 30-document cutoffs. TABLE 1 Retrieval Performance 
for Crantield 225 Number best noise ranking best idf ranking of full Lovins Porter full Lovins Porter 
Matches words words Average precision for 3 intermediate points of recall 0.265 0.377 0.383 0.393 0.397 
0.368 0.378 0.395 0.391 % Precision Change 42.3 44.5 48.3 49.8 38.8 42.6 49.1 47.5 E, 0.5, 10 docs 0.78 
0.71 0.71 0.71 0.71 0.72 0.71 0.71 0.71 E, 1.0, 10 docs 0.77 0.68 0.69 0.68 0.69 0.69 0.69 0.69 0.69 
E, 2.0, 10 docs 0.74 0.64 0.64 0.64 0.64 0.65 0.65 0.64 0.64 E, 0.5, 30 docs 0.87 0.84 0.84 0.84 0.84 
0.84 0.84 0.84 0.84 E, 1.0, 30 docs 0.82 0.79 0.79 0.78 0.78 0.79 0.79 0.78 0.79 E, 2.0, 30 docs 0.73 
0.67 0.67 0.67 0.67 0.68 0.68 0.67 0.67 Number ofqueries that fail At 10 documents retrieved 34 21 17 
16 19 21 19 16 20 At 30 documents retrieved 15 9 8 6 5 10 8 7 7 Total relevant retrieved At 10 documents 
retrieved 473 650 638 648 644 628 631 643 644 At 30 documents retrieved 784 945 954 962 957 938 942 953 
946 TABLE 2 Retrieval Performance for Medlars collection Number bestnoiseranking best idf ranking of 
full Lovins Porter full Lovins [ Porter Matches words words Average precision for 3 intermediate points 
of recall 0.445 0.538 0.562 0.541 0.545 0.535 0.560 0.535 0.542 ,% Precision Change 20.9 26.3 21.6 22.5 
20.2 25.8 20.2 21.8 E, 0.5, 10 does 0.57 0.51 0.51 0.52 0.51 0.51 0.51 0.51 0.50 E, 1.0, 10 does 0.66 
0.61 0.61 0.62 0.61 0.61 0.61 0.61 0.60 E, 2.0, 10 does 0.71 0.66 0.66 0.67 0.67 0.66 0.66 0.67 0.66 
E, 0.5, 30 docs 0.63 0.57 ~57 0.56 0.56 0.58 0.57 0.58 0.57 E, 1.0, 30 docs 0.61 0.53 0.53 0.53 0.53 
0.55 0.54 0.55 0.54 E, 2.0, 30 does 0.56 0.48 0.47 0.47 0.47 0.50 0.48 0.49 0.49 Number of queries that 
fail At 10 documents retrieved 1 0 1 0 0 0 1 0 0 At 30 documents retrieved 0 0 0 0 0 0 0 0 0 Total relevant 
retrieved At 10 documents retrieved 168 189 189 187 189 192 192 189 193 At 30 documents retrieved 330 
388 387 390 390 378 386 376 382 TABLE 3 Retrieval Performance for CACM collection Number best noise ranking 
best idf ranking of full Lovins Porter full Lovins Porter Matches words words i Average precision for 
3 intermediate points of recall % Precision Change 0.187 0.313 67.4 0.333 78.1 0.309 65.2 0.305 63.1 
0.316 69.0 : 0.351 87.7 0.323 72.7 0.301 61.0 E, 0.5, 10 docs 0.84 0.75 0.74 0.76 0.79 0.75 0.73 I 0.75 
0.78 E, 1.0, 10 does 0.85 0.76 0.76 0.78 0.79 0.76 0.75 0.77 0.79 E, 2.0, 10 docs E, 0.5, 30 docs E, 
1.0, 30 docs 0.84 0.87 0.85 0.73 0.82 0.79 0.74 0.82 0.79 0.76 0.82 0.79 0.77 0.84 0.81 0.73 0.82 0.79 
0.73 0.81 0.78 0.75 0.81 0.78 0.77 0.84 0.81 E, 2.0, 30 docs 0.80 0.72 0.72 0.72 0.75 0.72 0.71 0.71 
0.75 Number of queries that fail At 10 documents retrieved 19 13 14 16 19 13 14 16 18 At 30 documents 
retrieved 17 13 12 12 15 13 12 12 15 Total relevant retrieved At 10 documents retrieved 99 158 170 155 
138 157 175 161 140 At 30 documents retrieved 201 283 291 290 255 281 307 297 257 104 The lack of major 
improvement for stemming is not because the retrieval is unaffected by stemming. Table 4 shows the number 
of queries that have an improvement or degradation in performance as measured by the number of relevant 
documents retrieved by either the top ten documents or the top thirty documents. Both the noise and the 
IDF ranking methods show improvements over simple matching, with an average of 5 queries showing an improvement 
for each query show- ing a decremenL However, for all the stemming techniques, the number of queries 
showing improvments in performance is nearly equalled (and surpassed in many cases), by the number of 
queries showing degradation in performance. The only exception to this is the CACM collection's use of 
the Lovins stemmer, which correlates with the unusually high performance of the Lovins stemmer for the 
CACM col- lection. In general, the Lovins stemmer produces both more improve- ment and more degradation 
in performance than either the Porter stem- mer or the S stemmer, with the Porter stemmer performance 
closer to that of the Lovins stemmer than to the S stemmer. In addition to the averaged evaluation measures 
reported above, several queries were analyzed individually in order to understand how document ranks 
were affected by the stemming algorithm. Query 16 from the Cranfield collection is an example of this 
type of analysis. Query 16 document rank document no suffixing Porter S 498 1 1 1 106 2 7 2 266 29 9 
26 196 36 79 39 Query 16 has four relevant documents, ranked at 1, 2, 29 and 36 without suffixing. The 
first of these relevant documents, document 498, is the source document for this query, and contains 
17 matching terms (7 unique terms). It also contains 4 terms that have word vari- ants, and therefore 
increases its ranking score when stemming is used. The second of the relevant documents, document 106, 
contains no words that have word variants; thus its ranking score does not change when stemming is used. 
The document, however, is not only short, but contains 10 terms (5 unique) that match terms in the query, 
giving it a high ranking score. The third relevant document, document 266, con- taius 8 matching terms 
(3 unique), and 8 word variants of these matches. Its ranking score increases significantly using stemming, 
especially for the Porter algorithm, which includes two word variants not included for the Lovins algorithm. 
The fourth relevant document, document 196, also contains 8 terms (4 unique) that match query terms, 
plus 1 word variant. Its ranking score therefore increases slightly for all stemming algorithms. The 
actual rank of a relevant document, however, depends not only on its own ranking score, but also on the 
ranking scores of the non- relevant, and therefore the ranks of the relevant documents are dif- ferent 
for each stemmer. The S stemmer, which adds the fewest word variants and hence the least additional non-relevant 
documents, ranks the relevant documents at 1, 2, 26, and 39. The high ranking score for document 106 
was not affected by additional non-relevant documents. Document 266 had a higher ranking score using 
the S stemmer (more singular/plural variants), and hence improved its raank from 29 to 26. The slight 
increase in ranking score for document 196 did not ccompen- sate for larger ranking scores of non-relevant 
documents, and hence its rank dropped slightly from 36 to 39. The Porter stemmer adds 29 word variants 
for retrieval, and therefore many more non-relevant documents receive higher ranks. Document 106, with 
no increase in its high rank- ing score, drops from rank 2 to rank 7, and document 196 drops from 36 
to 79. Document 266 increases its ranking score because of the two additional word variants using the 
Porter algorithm, and hence moves up in rank from 29 to 9. TABLE 4 Number of queries showing Number of 
queries showing improvement in performance decrement in performance at cutuff=10 at cutoff=30 at cutoff=10 
at cutoff=30 Crantield (225 queries) noise vs match 118 idf vs match 115 Lovins vs full word, noise ranking 
46 Porter vs full word, noise ranking 50 S stemming vs full word, noise ranking 35 Medlars (30 queries) 
noise vs match 17 idf vs match 17 Lovins vs full word, noise ranking 8 Porter vs full word, noise ranking 
7 S stemming vs full word, noise ranking 8 CACM (57querier noise vs match 35 idf vs match 35 Lovins vs 
full word, IDF ranking 24 Porter vs full word, IDF ranking 17 S stemming vs full word, IDF ranking 12 
 103 98 50 46 42 21 20 62 55 43 24 21 57 52 40 20 20 8 9 7 3 3 12 9 7 38 38 23 17 16 6 7 9 14 13 4 4 
13 12 17 Query 16 points out the major problem using stemmers--nonrelevant documents receive higher 
ranking scores, often surpassing those of the relevant. The main reason for this is the large number 
of terms being effectively added to the query (the word variants). Table 5 shows the average number of 
terms in a query for the various collec- tions, and the average effective number of terms added by each 
stem- mer. The Lovins stemmer produces a much larger number of word variants mapping to a given root, 
and expands the query by a factor of 3 on average. The Porter stemmer in general produces fewer word 
variants for a given term, and hence adds fewer words to the query. The S stemmer, which only conflates 
singular/plural variants, adds the least terms, expanding only by a factor of 1.5 on average. The increased 
number of terms leads to a much larger number of documents being retrieved, and this requires greater 
discrimination from the rank- ing algorithm. TABLE 5 effective number total number of of terms documents 
retrieved Cranfield no stemming I0 736 Lovins stemmer 36 961 Porter stemmer 26 873 S stemmer 15 835 
Medlars no stemming 11 296 Lovins stemmer 39 442 Porter stemmer 30 389 S stemmer 18 355 CACM no stemming 
13 894 Lovins stemmer 49 1121 Porter stemmer 41 1394 S stemmer 21 1239  4.2 Reweighting term expansions 
to improve ranking The first effort to improve performance was a set of experiments in adjusting term 
weights of stemmed results. It seemed intuitively help- ful to use the stemmers as recall devices, increasing 
the pool of poten- tially relevant documents, but modifying the ranking mechanism to enhance precision. 
One method for increasing precision could be to assign less weight to those terms added by the stemmer, 
since they presumably are not as precise as the word form used in the query. It is possible in the IRX 
system to assign different term weights to each word variant for a given root, rather than treat all 
variants as if they were the same term. Instead of grouping all word variants for a given term as described 
earlier, each term variant was just added to the query, with its individual number of postings, noise, 
and document frequen- cies. An experiment was performed in which the variants of a term added by the 
stemmer were downweighted. Three different down- weightings were used: half the initial term weight, 
one-quarter the ini- tial term weight, and the initial term weight divided by the total number of term 
variants for a given term using the given suffixing algorithm. This allowed the suffixing technique to 
retrieve the extra documents that contained terms that were word variants, but gave less weight to these 
terms in ranking. None of the weighting schemes improved performance substan- tially. The three weighting 
schemes behaved similarly, with the weighting of half the initial term weight performing the best, both 
for all stemmers and for all collections. The Cranfield collection showed a slight improvement in performance 
at the lower cutoff (ten documents), but a worse performance by thirty documents. The Medlars collection 
showed the reverse, and no improvements at all appeared for the CACM collection. An examination of the 
number of changes in retrieval performance on a query-by-query basis showed that the number of queries 
having a degradation in performance is lowered (so this technique does lower the number of non-relevant 
that have higher ranks) but the number of queries showing an improvement in perfor- mance is also lowered. 
The end result is minimal overall change in any averaged evaluation measure. This strongly indicates 
that the assump- tion that term vari~ts are interchangable in retrieval is correct; that is, downweighting 
a term because it is not the original query term elim- inates relevant documents as well as nonrelevant 
documents. 4.3 Selective stemming A second effort to improve the suffixing performance was based on 
controlling the number of terms used for retrieval by using stemming for only short queries. Short queries 
were defined as queries having nine or fewer (noncommon) terms, which accounted for about 54% of the 
Cranfield query collection. Stemming algorithms were used on the short queries, but no stemming was used 
on queries having ten or more terms. An additional term number cutoff of twelve was used. The use of 
the suffixing algorithms on only the shorter queries did not improve overall performance. The results 
using the Lovins stem- mer were slightly better than no stemming, and slightly worse than the initial 
runs using the Lovins stemmer (for the Cranfield collection). The results using the Porter stemmer shows 
performance better than no stemming, but slightly worse than the initial Porter stemmer results. Again, 
the number of queries having decrements in performance was lowered, along with the number of queries 
showing improvements in performance. Thus the number of terms in the original query is not predictive 
of whether the query performance can be improved by stem- ming. 5. Conclusion The use of the three general 
purpose stemming algorithms did not result in improvements in information retrieval in the three test 
collec- tions examined. Although individual queries were affected by stem- ming, the number of queries 
with improved performance tended to equal the number with poorer performance, thereby resulting in little 
overall change for the entire test collection. Similarly, attempts to improve performance by changing 
the weighting of stemmed terms which did not occur in the query, or by restricting stemming to short 
queries, did not affect overall performance. It is useful to consider the implications of these results 
for an online environment. Even when stemming did improve queries, the improve- ments were frequently 
at document ranks beyond which an average user might browse, or they involved minor rank changes within 
a ten or thirty document cutoff and consequently would not substantially enhance onhne performance, Additionally 
stemming causes a significant increase in query processing time. In IRX the processing time is related 
to the number of terms in a query. For the average query (ten terms), the time is approximately doubled 
using the Lovins stemmer, which constitutes a noticeable response delay to an online user. Nevertheless, 
the stemming of query terms is intuitive to many users, is more convenient than specifically using truncation 
and wild- card characters in queries, and is often necessary for helping queries retrieve relevant documents 
in the top ten or thirty documents. 106 One approach in an interactive system would be to offer users 
a method of stemming us an aid to query formulation. The system could inform the user of all terms in 
the database that are stemmed variants of the query terms, including statistical information on these 
variants, and allow the user to select which variants are pertinent to the query. This approach, as compared 
to automatic stemming, has the advantage of increasing recall through the use of additional appropriate 
terms without introducing superfluous terms. The current generation of workstations and window environments 
is particularly well-suited to this interactive approach of query refinement and is the direction of 
future work on the NLM IRX project. Acknowledgments Thanks to the rest of the IRX project team, Dennis 
Benson, Larry Fitzpatrick, Charles Goldstein, and Rand Huntzinger, who make this research possible. In 
particular, thanks to Larry Fitzpatrick for his patient help with UNIX and to Dennis Benson for his many 
helpful comments on this paper. The system code for the evaluation section of IRX was graciously given 
by the SMART project. References <RefA>[BELL79] Bell C. and Jones K.P.,"Toward Everyday Language Information 
Retrieval Systems via Minicomputers", Journal of the American Society for Information Science, Vol. 30, 
No. 5, November 1979, pp. 334-338. [BENSON86] Benson D.,Goldstein C.M., Fitzpatrick L., Williamson D. 
and Huntzinger R., "Developing Tools for Online Medical Reference Works", Proceedings of Medinfo 86, 
Elsevier Science Publishers B.V., Amsterdam. 1986., pp 558-560. [CROFT83] Croft W.B., "Experiments with 
Representation in a Document Retrieval System", Information Technology: Research and Development 2 (1983), 
pp. 1 -21. [HAFER74] Hafer M. and Weiss S., "Word Segmentation by Letter Successor Varieties", Information 
Storage and Retrieval 10, 1974, pp. 371-385. [HARMAN86] Harman D., "An Experimental Stody of Factors 
Important in Document Ranking", Proceedings of the 1986 ACM Conference on Research and Developments in 
information Retrieval, Pisa, 1986. [LENNON81] Lennon M., Peirce D., Tarry B. and Willett P., "An Evaluation 
of Some Conflation Algorithms for Information Retrieval", Journal of Information Science 3, 1981, pp. 
177-188. [LOVINS68] Lovins J.B., "Development of a Stemming Algorithm", Mechanical Translation and Computational 
Linguistics I 1, March 1968, pp. 22-31. [PACAK78] Pacak M.G. and Pratt A.W., "Identification and Transformation 
of Terminal Morphemes in Medical English, Part II", Methods of Information in Medicine 17, 1978, pp 95-100. 
 [PORTERS0] Porter M.F., "An Algorithm for Suffix Stripping", Program 14, Vol. 14, July 1980,pp. 130-137. 
 [RIJSBERGEN79] VanRijsbergen C.J., information Retrieval, Butterworths, London, 1979. [SALTON68] Salton 
G., The SMART Retrieval System--Experiments in Automatic Document Processing, Prentice-Hall, Englewood 
Cliffs, N.J. 1968. [SALTON83] Salton G., McGill M., Introduction to Modern Infornlation Retrieval, McGraw-Hill 
Book Company, New York, 1983. [SPARCK-JONES72] Sparck-Jones, K.,"A Statistical Inteq~retation of Term 
Specificity and Its A19plication in Retrieval", Journal of Documentation, Vol. 28, No. 1, March 1972, 
pp. 11-20. [ULMSCHNEIDER83] Lllmschneider J. and Doszkocs T., "A Practical Stemming Algorithm for Online 
Search Assistance", Online Review 7(4), 1983, pp 301-315. -I07</RefA> Appendix  Ranking using the IDF weighting 
measure M (log 2 Freq.~ x IDF~ rankj k=l tog 2 M ~'~ where M = the number of terms in the document j 
Freq. = the frequency of term k in document j IDFk~ = the inverse document frequency weight of term k 
 IDF k = log 2 ---~---+I NumDk where N = the number of documents in the document collection NumD k = 
number of documents in the collection that contain one or more instances of term k Ranking using the 
inverse noise measure M (log 2 Freq~ x noiseO rankj k=Ix'~ 10g2 M where M = the number of terms in 
the document j Freq..~ = the frequency of term k in document j noise k = the inverse noise weight of 
term k N Freq~ TFreqk noise k = inverse of ~ ~ log 2 i=1 tt, req~ Fn~.i~ where N = the number of documents 
in the document collection Fr~. the frequency of term k in document i TFrfflk the total frequency of 
term k in the document collection 108 
			
