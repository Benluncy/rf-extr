
 A Novel Framework of Register Allocation for Software Pipelining Qi Ning Guang R. Gao School of Computer 
Science McGill University Montreal, Quebec Canada H3A 2A7 email: ning@cs.mcgill.ca gao@cs.mcgill.ca Abstract 
Although software pipelining has been proposed as one of the most important loop scheduling methods, 
simultaneous scheduling and register allocation is less understood and re­mains an open problem [28]. 
The objective of this paper is to develop a unified algorithmic framework for concurrent scheduling and 
register allocation to support time-optimal software pipelining. A key intuition leading to this surpris­ingly 
simple formulation and its efficient solution is the as­sociation of maximum computation rate of a program 
graph with its criticai cgcles due to Reiter)s pioneering work on Karp-Miller computation graphs [29]. 
In particular, our method generalizes the work by Callahan, Carr and Kennedy on scalar expansion [6], 
the work by Lam on modular variable expansion for soft ware pipelined loops [20], and the work by Rau 
et al on register allocation for modulo scheduled loops [28]. ­ 1 Introduction Software pipelining has 
been proposed as one of the most im­portant fine-grain loop scheduling methods. It determines a parallel 
schedule with a periodic pattern which may over­lap instructions of a loo~ bodv from different iterations. 
S~ftware pipelining can be applied to high-performance pipelined processor architectures, super-scalar 
and VLIW architectures [2, 3, 4, 12, 13, 20, 27, 31]. Although much progress has been made in finding 
time­optimal schedules for software pipelining, simultaneous scheduling and register allocation is less 
understood and re­mains an open problem. In terms of instruction scheduling for RISC processor architectures, 
it is well recognized that Permission to copy without fee all or part of this material is granted provided 
that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice 
and the title of the publication and its date appear, and notice is given that copying is by permission 
of the Association for Computing Machinery. To oopy otherwise, or to republish, requires a fee and/or 
specific permission. ACM-20th PoPL-1/93-S.C., USA e 1993 ACM 0.89791 .561 -5/93 /0001 /0029 . ..$1 .50 
performing register allocation before instruction scheduling (postpass scheduling) may introduce new 
constraints due to the reuse of registers, which may limit possible reordering of the instructions, as 
reported in [15, 17]. On the other hands, if instruction scheduling is done before (and independent of) 
register allocation (prepass scheduling) [5], more registere than necessary may be needed, which may 
cause unneces­sary register spills and severely degrade the performance of the resulting code. Warren 
has described a technique which applies instruction scheduling twice: once before and once after the 
register allocation [32]. The objective of this paper is to develop a unified scheduling-allocation framework 
for concurrent scheduling and register allocation. We propose a framework in which register allocation 
for software pipelining is solved in two stepa. Step 1: Time-Optimal Scheduling and Minimal Buffer Allocation: 
The first step determines the time-optimal schedule for a software pipelined loop and allocates symbolic 
registers organized as FIFO buffers, one buffer for each variable defined in the loop. Intuitively, such 
a buffer is used to extend the lifetimes of the corresponding loop variable gen­erated in successive 
iteration, permitting multiple it­erations to be overlapped in concurrent executions. We show that the 
minimum buffer allocation and the time­optimal scheduling problem can be formulated together as an integer 
programming problem called the Optimal Scheduling and Buffer Allocation (OSBA) problem. An efficient 
polynomial time solution is presented baaed on a transformation of the OSBA problem into rnin-cost jlow 
problem. A key intuition leading to this surpris­ingly simple formulation and its efficient solution 
is the association of maximum computation rate of a program graph with its critical cycles due to Reiter 
s pioneering work on Karp-MiUer computation graphs [29]. Step 2: Nlapping buffers to physical registers: 
The second step is to map the symbolic registers of the FIFO buffers into physical registers. Since a 
schedule is de­rived from the solution of the OSBA problem, a color­ing algorithm can be applied to minimize 
the number of physical registers required to implement the buffers. In particular, we can apply a recently 
proposed method based on coloring of cyclic interwd graphs [16]. Code generation schemes with or without 
special hardware support are discussed. We have implemented the OSBA algorithm. In experi­ ments using 
our method on the working example used in Rau et. al. [28], a reduction of register usage from 28 to 
21 is achieved a 25% improvement in register use without compromising the speed of the schedule, The 
general problem of allocating minimum number of registers to support a class of loop schedules of certain 
rate, possibly optimal rate, is proven to be NP-complete in [25]. However the two step approach proposed 
here is a practi­ cal way of attacking this hard problem, and it has a time complexity 0(n3 log n). Since 
the register coloring methods are described in other works, our discussion centers on the buffer allocation 
step and the code generation schemes. We organize the subse­quent sections as follows. In Section 2 we 
present a simple example loop to motivate the concept of buffers and the two step approach for concurrent 
scheduling and register allocw tion. In Section 3 we formulate the Optimal Scheduling and Buffer Allocation 
(OSBA) problem. A polynomial time so­lution is presented in Section 4. The proof of the theorem in this 
section can be found in [25, 24]. In Section 5 we address the code generation problem. In Section 6, 
we explain how to reduce the register requirement further through coloring algorithms for backend code 
generation. This is the second step in our register allocation scheme. In Section 7 we com­pare our approach 
with related works. We also include an appendix which describes the example in Rau et al s paper [28], 
and our solution. 2 Motivation In this section, we motivate our work by studying register allocation 
for a simple loop L, shown in Table 1, under soft­ware pipelining. Although a high leyel language represen­tation 
of the loop is chosen here, it is intended only to give a simple description of our technical framework. 
There is no difficulty in applying our framework to the lower level representation of the code. As shown 
in Table 1, the loop L cent ains three instructions in its b,ody. It also contains m Table 1: An example 
loop L. a loop carried dependence of distance 2 from S3 to S1. For instance, the value c[i] generated 
by S3 in iteration i is only used two iterations later by statement S1 in iteration i + 2. The other 
data dependence in the loop are all within the same iteration. The Data Dependence Graph (DDG) of L is 
shown in Figure 1, where the positive number beside (s3, SI ) indicates its dependence distance. Figure 
1: Data dependence graph of the example loop L. Under software pipelining, the iterations are permitted 
to overlap so that the subsequent iterations may start before the previous iterations finish. Since there 
exist loop carried dependence, there should be a proper initiation delay inter­ val P between successive 
iterations so that when the next iteration starts P cycles after the previous iteration started, no loop 
carried dependence are violated. In our example, assuming that the delay for Add is 1 cycle and the delay 
for Multiply is 2 cycles, then a delay of P = 2 cycles between the starting times of two consecutive 
iterations is optimal in the sense that the scheduled loop achieves the maximum speedup. A possible maximum 
speedup schedule is shown in Table 2, in which sz,1 indicates the instruction sz in iteration 1. Note 
in Table 2 that iteration 3 starts after iteration 1 pro­duces c[l], and iteration 4 starts after iteration 
2 produces c[2], etc. So the loop carried dependence is not violated. The schedule exploits parallelism 
since there are two instructions scheduled in parallel at cycle 3 (5, 7 etc.). Under the above time-optimal 
schedule, S1 is executed twice before its suc­cessor S3 is scheduled for the first time in cycle 3. Hence, 
in order to support the schedule, it is natural to provide a stor­age buffer of size more than one (if 
necessary) between the generator S1 and its successors sz, ss. To enforce the correct order of the values 
produced, the buffer should behave like a first-in-first-out (FIFO) queue. Let us examine this in some 
detail. Suppose that a FIFO buffer of two symbolic registers {ao, al } is allocated to a, such that a. 
is the tail and al is the head of the queue, Each of the other variables is allocated a buffer of size 
1, which is a single register. For convenience we will use the variable name as its register allocated 
if the buffer size is one. When a[l] is produced by S1 in cycle O, it is put into the tail a. of the 
buffer. At cycle 2, a new iteration starts and a[2] is produced before a[l] is used at cycle 3. At this 
moment, the new value can not be written into ao, otherwise it would overwrite a value (a[l]) which is 
still needed in cycle 30 iteration 1 iteration 2 iteration 3 iteration 4 0 SI,I: a[l]=X+c[-1] 1 $2,1: 
b[l]=a[l]*F 2 s1,z: a[2]=X+c[O] 3 s3,1: c[l]=a[l]+b[l] SZ,Z: b[2]=a[2]*F 4 SI,3: a[3]=X+c[l] 5 s3,2: 
c[2]=a[2]+b[2] s2,3: b[3]=a[3]*F 6 SI,4: a[4]=X+c[2] 7 S3,3: c[3]=a[3]+b[3] s2,4: b[4]=a[4]*F !3 s..: 
cr41=flr41+br41 , , ,., , -., ,=--..L J III I{I I Table2: Apossible maximum speedup schedule. 3. Now 
since deallocated two registers and organized them as a FIFO buffer, we can continue to write to the 
buffer at a. al the tail at cycle 2. We assume that a mechanism is provided such that a buffer can shift 
its contents towards its head so (3+-+7-P(T)that the tail (ao ) is ready to get a new value. In our case, 
we can assume that the old value in ao is shifted to al at beginning of cycle 2. So at cycle 2, a[l] 
is in al and a[2] is in ao. S* Conceptually, the pseudo code buffer-shift in Table 3 plays the role 
of such shifting. In practice, the buffer mech­ 3 {ao, al} is the buffer for anism can be implemented 
either by explicit shifting using register moves, or by implicit shiftings based on a modulo S1. S2 reads 
from the tail addressing scheme. These schemes will be discussed in Sec­ a. and S3 reads from tion 5. 
the head al. Let us see how the successors sz, .93 of sl access the buffer of a in Table 3. Since S2,1 
is scheduled at cycle 1, when it reads the value of a[l], we can assume it is still in register Figure 
2: A multiple-head buffer. ao. Therefore, the actual code for SZ,I in iteration 1 could be b = ao * F. 
On the other hand, if we assume that the buffer shifts its contents at cycle 2, then at cycle 3 when 
S3,1 is executed, the value of a[l] is shifted to al in the buffer, hence .93,] should read from al. 
Therefore the code for S3,1 loop, they are not shown in Figure 3. should be c = al + b. Hence S2,, always 
reads from the tail a. As shown in Figure 3, the live range of c does not overlap and S3,* always reads 
from the head al. This phenomenon is with the live range of al, so they can share the same physical caused 
by the different scheduled timings of the successors register. Thus, the mapping of buffers to physical 
registers of a producer. We call such a buffer muitiple-head FIFO (step 2 of our method) is somewhat 
similar to the traditional queue because the successors read the contents of the buffer register allocation 
problem. In Section 6, we describe how to at different places. The multiple-head buffer is illustrated 
in apply the register allocation method based on cyclic interval Figure 2. graphs for this step. After 
this procedure the code does Here the concept of FIFO buffer plays an important role, not need the extra 
register c, which is replaced by a 1, as as it captures the notion of lifetime of a loop variable ex-shown 
in Table 4 where the symbolic register names e.g. al tended into successive iterations. The repeating 
pattern now represent physical registers, and where the sign II means between cycles 2 and 3 in Table 
3 is, in fact, the time-optimal execute in parallel with . schedule for the software pipelining we expect 
to derive. We wiU choose this repeating pattern as our new loop body, which is the first step of our 
method. 3 Formulation of the Optimal Scheduling and When the new loop body and the buffer allocation 
have Buffer Allocation (OSBA) Problem: Step 1 been decided, we notice that the buffers allocated to indi­vidual 
instructions can share same physical registers if their In this section we give a mathematical formulation 
of the live ranges in the schedule are non-overlapping. In our ex-scheduling and buffer allocation problem. 
Section 3.1 intro­ample loop L and its given schedule, the live ranges of the duces the concept of periodic 
schedules and optimcd com­variables in the repeating pattern are indicated in Figure 3. put ation rate. 
Section 3.2 presents an integer programming Since the live ranges of F and X are the whole range of the 
formulation of the Optimal Scheduling and Buffer Allocation iteration 1 iteration 2 Iteration 3 iteration 
4 0 buffer-shift: al = ao sl,l:aO=X+c 1 sz,l:b=aO*F 2 buffer-shift: al = ao  sl,z:aO=X-Fc 3 ss,l:c=al+b 
s2,2:b=ao*F 4 buffer-shift: al = ao sl,J:aO =X-kc 5 s3,2:c=al+b sz,S:b-=ao*F 6 buffer-shift: al = ao 
sl,*:aO=X+c 7 sa,s:c=al+b sz,b:b=aO*F 8 9 s3,4:c=a1+b Table 3: Code generated for loop L. prologue code 
p <---­ :----­ ;.. 2 al p -------+-__; ----;-epilogue code ! ~:1 !E Table 4: Code from repeating pattern. 
3---- UUU­ ___________-------------. ! (a) Life ranges drawn from ! , -----------------, \ tie repeatin-pattern. 
,;; , Legend: (OSBA) problem. 1 ----------------------I I : H Definition point. \ : x Last use point. 
] 3.1 Time-Optimal SchedulesI--------------------I We assume that a data dependence graph (DDG) supple­mented 
with delay information is used to represent a loop. In a DDG, nodes represent the instructions in a program. 
An arc from node i to node j indicates that there is a data dependence between them. Loop carried dependence 
are e H indicated by arcs with positive numbers beside them indi­cycle 3 cating their dependence defined 
below. If node i produces a result at the current iteration and the result will be used k iterations 
later by node j, then we say that the arc (i,j)  +&#38;i@ has a dependence distance k, and we use m,~ 
to indicate it. So for a data dependence not crossing iterations, the m,j is~(b) Life ranges drawn as 
circular: O. d, (called delay) is used to indicate the number of clock ,arcs on a cycle. I1 ------------------------------4 
cycles node i needs to finish its execution. Definition 3.1 A Timed DDG is a tupie (N, E, m, d) where 
Figure 3: Live ranges of the variables for code generated by N is the set of nodes, E is the set of arcs, 
m == {m,J, d(i,,i) E ASC scheme. Ej is the dependence distance vector on arc set E, and d = {d,, Vi EN) 
is the delay function on node set N. Later on in the paper, we WN simply US~~DDG to mean Timed DDG whenever 
no confusion can be caused. We will consider a class of software pipelining schemes called periodic 3.2 
Optimal Scheduling and Buffer Allocation To­ schedules. Definition 3.2 Let P be a positive integer. A 
schedule is called periodic with period P if for-any node i, the tames i is scheduled for its first instance, 
second instance, third instance,... are t,, t, + P,t, + 2P, . . .. Time t, is called the initial execution 
time for node i. It is obvious that the computation rate of a periodic sched­ule with period P is ~. 
Reiter [29] has shown that it is always possible to have an periodic schedule with optimal period P which 
equals the reciprocal of the maximum com­putation rate of the dependence graph. When a periodic schedule 
achieves this maximum computation rate, we call it time-optimal. Themain objective ofthispaper isto finda 
time-optimal periodic schedule which uses aminimumnum­ber of registers. The next lemma characterizes 
the feasible periodic schedules. Lemma 3.1 (Reiter [29]) The initial execution times t, are feasible 
foraperiodic schedzde with period P if and only if they satisfy the following ineqwdities: t~ >t, +d% 
Pm,J, V(i, j) GE (1) where d, is the delay of node i, P the period, and m;l the dependence distance 
for arc (i,j). The maximum computation rate of a DDG (loop) is the maximum average number of executions 
of its nodes per clock cycle, restricted only by the data dependence in the graph. For example, the loop 
represented in Figure 1 has a maximum computation rate of ~, meaning that every node can be executed 
once in every two time cycles. The follow­ing theorem characterizes the maximum rate one can obtain. 
Proofs can be found in [29, 26]. Theorem 3.1 (Reiter [29]) Given a DDG=(N, E, m, d) which represents 
u loop. The maximum computation rate of every node in the graph is the critical ratio, min p,mc J , for 
all cycles C in the DDG, J EEC d The cycles that reach the min are called critical cycles. The critical 
ratio can be computed efficiently by the al­gorithm of solving the minimum cost-to-time ratio problem 
[21]. In this paper, we assume P (the reciprocal of criti­cal ratio) to be an integer. In the case when 
a fraction is derived for the period, we can always unrolled the loop a proper number of times and the 
resulting loop will have an integer period P, as illustrated in [25] or [24]. For loops with­bout loop 
carried dependence or its DDG does not contain dependence cycles, the criticaJ ratio is undefined, and 
can be considered as positive infinity. In this case we choose any positive number as period P which 
is always feasible. How­ever a better choice of P should be obtained by analyzing the size of the loop 
body and the available resources. In general if the loop body is large and the resource is limited, the 
period should be larger. gether In general, there are many time-optimal schedules. One of our goals 
in this paper is to find the best schedule t, such that it will need the least number of registers. In 
this section we look at the problem of providing the minimum number of buffers so that successive iterations 
can be initi­ ated at the desired optimal rate. We do not assume that a fixed schedule is given here. 
Instead we will find one that can achieve both time-optimality and space-optimality. The time-optimal 
property is enforced by using an optimal period while not fixing the timings of the individual nodes. 
A schedule may produce different numbers of results at different times during the execution. So our first 
objective is to minimize the maximum of the number of buffers re­quired at different times during the 
execution. Let K be a given time instance. We want to know what should be the size of the buffer for 
node i at time K. This size depends on the timings of i s successors and also on the reservation scheme 
for the registers. Here we take the most conserva­tive assumption that a register is reserved at the 
issue time of the instruction. However our analysis can be applied to other reservation schemes with 
only minor modification. For instance, if we assume that a register is reserved only at the output stage 
of the pipeline, then a modified formulation can be obtained. See [25, 24] for details. Let us consider 
one node j of s~ch successors, so that (i, j) is an arc from node i to node j. Suppose node i has started 
its IcI -t h inst ante execution and the ( kl + 1)-t h inst ante has not been started. Hence we have 
the following relation: t*+ P(kl l)<K<t, +Pkl. (2) Suppose that at time K node j has started executing 
its kz-th instance, and the (k-z + 1)-th instance has not been start ed. Hence we have the following 
relation: tj +P(k2 1) ~ h < tj +PkQ. (3) Therefore at time K, the number of results produced by node 
i which have not yet been consumed by node j is ICI + m,~ kQ. Therefore we should allocated a buffer 
of size at least kl k2 + m,~ to node i. Next we calculate the bounds on kl and kZ in terms of t,and 
t~. From inequalities (2) and (3), we can transform them into: h t, <,l<K=L+,, (4) P P and K tj <k2<=+l. 
(5) P P Let us consider (4) first. There is only one integer value in the half-open-half-closed interval: 
K t, A t, (~, ~ +1]. (6) Hence kl equals ~ + 1 if the latter is an integer, oth­erwise ICI equals integer 
ceiling of the lower bound, i.e. /cl = [=1. But in both cases, it is easy to verify that the following 
is always true: (7) Similarly for /cz, it is always true that 4 Solution of the OSBA Problem K tj+l 
lm=( ~ 1. (8) At time instance K, the number of results produced by node i which has not yet been consumed 
by node j is: kl+rn,j-k2=[ -:+ll+m,, -F:+ll. (9) Let b, be the buffer size for node i Then we should 
con­sider all the immediate successors of node i for the bounds in (9). When we consider all the outcoming 
arcs from node i, we have: b,~( -:+ll+?W,-(K-;+ll, V(i, j) e E,VK e z+. (lo) where Z+ is the set of all 
positive integers. The constraints in (10) are too complicated to handle in two aspects. First it must 
be true for all the positive values of K. This condition can be reduced to consider K for P con­secutive 
values only, because of the periodicity of our sched­ule. Secondly, the right hand sides of the inequalities 
involve integral ceiling function which is not a linear function. In order to formulate the problem into 
an integer linear pro­gramming formulation, some approximation must be taken to deal with the integral 
c&#38;ling function. Our approach here is to approximate k] in (7) by (11) and to approximate kz in 
(8) by (12) With these approximations in (11) and (12) we can get a simplified constraints on b, as the 
following: bi >k~+m,j k~ = ~ +1+ m,J li-~+l, V(i,j) cE,VK cZ+ .~+m,~+l, Y(i, j) c E. (13) Let us notice 
that in (13) the time instance parameter K haa disappeared. That is due to the result of the approxima­tion. 
Now we are ready to formulate our optimal schedule and buffer allocation problem into an integer linear 
program problem by combining the constraints for feasible schedules and the constraints for buffer sizes, 
as follows: Optimal Scheduling and Buffer Allocation (OSBA) Problem: subject to Pb,+t, tj~P(m,j +1) 1, 
V(i,j) ~E t3 t, >d, Pm,J, V(i, j) e E t,, b, integers, Vi c N. (14) In last section we obtained an integer 
programming formu­lation (14) of the OSBA problem. In this section we inves­tigate its solution. Here 
we do a variable substitution: b; = Pb, (15) and transform the formulation (14) into the following form: 
OSBA Problem with Variable Substitution (15): min ~,e~ b: subject to b:+tt tj >P(mtj+l) l, v(t,j)e E 
t] ti >d, Pm,J, V(i, j) c E b;, t, integers, Vi c N. (16) We state the following theorem which is essential 
to solve (16). The proof can be found in [25, 24], Theorem 4.1 The constraint matrix in (16) is totally 
uni­rnoduiar, that is to gay, each of its square submatrices has a determinant equal to either O or 1 
or 1. The right hand sides of (16) are all integral numbers. From linear programming theory [30] that 
if the constraint matrix is totally unimodular and the right hand sides are in­tegral, then the integer 
programming problem can be solved as a linear programming problem. The optimal linear pro­gramming solution 
is guaranteed to be integral. Hence we obtain the following corollary: Corollary 4.1 When (16) is solved 
as a linear program­ming problem (dropping the integer requirement), the sohJ­tion obtained is always 
integral, i.e. it is the integer pro­gramming solution of (16). To solve (16), we can use general linear 
programming al­gorithms like simplex methods [9], or ellipsoid method [19] or interior point methods 
[18]. Here we present an more efficient algorithm in next subsection. 4.1 More Efficient Algorithm for 
Solving OSBA In this subsection, we show a more efficient algorithm to solve OSBA problem as expressed 
in (16). The algorithm is a number of transformations of the problem to the minimum cost flow problem. 
Since the minimum cost flow problem can be solved more efficiently by the so called combinatorial algorithms, 
this will imply that our original (16) can also be solved more efficiently. Let us first write down the 
linear programming dual of (16): x subject to A,J=l, ViEN (17) 3CJ+(,) max -~ (PmiJ di)~ij Aij >0, ~tj 
>0, V(i, j) E E. (i,j)r3E where 6+ (i), 6-(i) are the sets of immediate successors and subject to immediate 
predecessors of i, respectively. If we reorganize the variables in the objective function, then the objective 
function can be written as: jC6i_(i) je6-(i) (i,j)GE (t,J)6E gi~ unrestricted, m,~ >0, V(i, j) c E. 
 Formulation (22) is not yet a minimum cost flow problem. Next we show that (22) can be further reduced 
to a minimum cost flow problem. But let us first consider the following problem: subject to (23) jG6+(i) 
jC6+(i) jC6-(i) Note that the [P l)n in (18) is a constant. The variables .fij ~ l, V(i, j) c E. in 
the constraints can also be rearranged. After these rear­rangement, the dual problem can be written in 
the following The first set of constraints in (23) gives a lower limit on form: the sum of output flow 
for each node. The second set of con­straints is the conservation restriction for the flow meaning that 
flow coming into a node must equal to the flow com­ing out of that node. If the first set of constraints 
have not appeared in (23), then it is the ordhmry minimum cost net­ subject to work flow problem. We 
will show that how we can split the Alj=l,VieN (19) x nodes in the graph to make the current formulation 
fitting ~G6+(i) into the ordinary minimum cost flow problem. Actuslly, we can replace each node i in 
the original graph by two nodes i and i . The origiual input arcs to node i jG6+(i) jC6-(i) are now 
directed to node i . The original output arcs from Atj >0, ~lj ~ 0, V(i, j) 6 E. node i are now going 
out from node i . We also add a new arc from node ii to node i . Now consider the ordinary Now we do 
a variable substitution: minimum cost flow problem on this split graph. Let N be g,j = r,j A, ,V(i, 
j) c E. (20) the set of i nodes and N be the set of i nodes. We use E to denote the set of arcs in the 
split graph. With thw variable substitution, the objective function in It is easy to see that the following 
minimum cost flow (18) becomes: problem (24) is equivalent to (23). subject to (i,j)~E iEN jG6+(i) tJe6+(u) 
VG6-(U) f; 2 -l, V(%W) G E . where we define the cost coefficients in the objective function (i,j)eE 
iCN by Note that the last term in (21) is also a constant. Then ifu=i 6N andv=eN e, , the formulation 
(19) becomes the following with the variable d;v = fm j - ifu=i G N andv=i e N #. substitution 20: { 
Lemma 4.1 Formulation (24) and formulation (23) are equivalent, that is, 9aVetI an Optimal sdut~on }(U,V)EEI 
{fth Of (~4) then the {ftj }(1.?)cE defined @ the fo~l@w formula is an optimai solution of (.23): andv=jl~N 
andi #j, fl~ = f;., if u= i EN Similarly, given an optimal solution {fij }(t,j)eE of (~~), the the following 
defined {f&#38; }(ti,v)6E/ iS an oPtimal solution of (24): The proof of the lemma is straightforward. 
It is well known that the minimum cost flow can always obtain an optimal integer flow if all the capacity 
constraints on the arcs are integral [30]. The capacity constraints on the arcs in (23) and (24) are 
integral, therefore they have optimal integral solutions. Actually the efficient out-of-kilter algorithm 
(see [21]) and its variants will give such an optimal integer solution when it is applied to (24). Now 
we are ready to proof the equivalence between (17) and (23). Lemma 4.2 Given an optimal integra! solution 
of (23 ) we can transform it to obtain an optimal feasible solution for (17) and vice versa. Proofi Since 
(17) is equivalent to (22), we only need to show the equivalence of (22) and (23). Let {f,, }(,,,)e~ 
be an optimal integral solution of (23). First let g,~ = f,j, V(i, j) c E. Because of the first set of 
constraints in (23), and the fact that ~,~ S are integers, we have either ~ f,, = -1, (25) jc6+(i) or 
~ f,, > o. (26) 3G6+(I) If (25) happens, we let m,j = O,Vj c 6+ (i). If (26) happens, we let ~jj = 1 
+ gjj, for a particularly given j, c 6+ (i) and ~i~ = O for all other j c f+(i) {ji}. It is ewy to verify 
that such defined {gij, ~tj } is a fe~ible solution of (22). Furthermore, since X,j does not appeu in 
the objective function in (22), this solution wiU have the same objective value as that of (23) defined 
by the given fij. Next we prove the reverse. Suppose that {g,j, Xtj } is an optimal solution of (22). 
Simply let fil = gi~, V(2, j) c E will give a feasible solution of (23). Again because ~,j does not appear 
in the objective fnnction in (22), snch defined ~.j will give the same objective value of (23) as gij, 
~ij gives for (22). Since the feasible solutions for both problems have the same objective value, one 
of them is optimal implies that other must be optimal. a Theorem 4.2 The problem (16) can be solved in 
O(n3 log n) time, where n is the number of nodes in the graph representing the loop. Proofi Recall that 
(17) is the dual of (16). Therefore it is equivalent to solve either of them. Lemma 4.2 proved the equivalence 
bet ween (17) and (22). Lemma 4.1 proved the equivalence between (22) and (23). Therefore (17) is equiv­alent 
to (23) which is a minimum cost flow problem. Using the Out-of-Kilter method in [21] to solve the minimum 
cost flow problem, the algorithm for our case has a complexity 0(w3 log n). Hence our transformation 
procedures never use more than 0(n3 log n) time, we can conclude here that (17) can be solved in 0(n3 
log n) time.  4.2 Back Substitution Our method reduces the problem to a minimum cost flow problem on 
a network, which has a 0(n3 log n) algorithm. The t, variables give the optimal schedule of the nodes. 
The b: variables have to be substituted back by the formula (27) However such b, s may not always be 
integral since we have done a divide operation in (27). If we simply round the bi s to their integral 
ceilings, a suboptimal solution may result. We can resolve this problem by noticing that by this time 
the schedule is already produced. By fixing the schedule to be the t, s produced by the solution of OSBA, 
we can recal­culate the kl and kz by (7) and (8) only for P consecutive time inst antes of K, starting 
from the time it enters a steady state. It turns out that the starting time of the steady state can be 
the cycle time when the first iteration finishes its lat­est instructions. We use Q to denote the cycle 
time when the latest instruction finishes. Considering all the successors of node i, we should allocate 
a buffer of size R, determined by the following formula (28): Vje6+(i), for fC=Q, Q+l,..., Q+(P -1), 
} (28) where 6+ (i) is the set of immediate successors of i. For our example loop in Section 2, its OSBA 
problem is: min bl+bz+ bs subject to 2bl+tl tz~l 2bl + tl -t3~l 2b2+iz-ts>l (29) 2bs+ts tl>5 tz tl~l 
t3 $1>1 t3 t2~2 tl t3~ 3 Solving (29) for loop L, we obtain the following scheduling and guidelines for 
the buffer sises: tl=o, t2=l, t3 =3, (30) b1=2jb2=;, ba=l. (31) If we round up the solution for the 
hi s in (31), we would end up with 5 registers. However if we use (28) to calculate the real need for 
the registers that support the schedule, then we can have the following allocation: R1=2, R2=I, R3=I. 
(32] which uses only 4 registers. The actusl schedule for the loop L is shown in (2) on page 3. In (2), 
notice that by the time node 53 is first scheduled, its predecessor node S1 hss been executed 2 times. 
That is why we allocated a FIFO buffer of size 2 for node sl. Code Generation In this section we discuss 
the code generation problem baaed on our solution of the schedule and register allocation prob­lem in 
Section 4. The unique sspect of our code generation method is the buffer registers allocated to each 
node as a FIFO (First-In­First-Out) queue which haa multiple heads. If we allocated more than one register 
to a node, arranging them as a FIFO queue can make sure that the results are consumed in the same order 
as they are produced. ConceptusJly, the new result produced by a node should always be written to the 
tail of the corresponding FIFO buffer, while the successors of the node should read the re­sults at the 
proper places of the buffer. However, we must note that it is not always true that the successors should 
read the results from the same head of the FIFO buffer. In some cases it is possible a successor should 
read the result from a place in the FIFO buffer other than the head. In other word, the FIFO buffer should 
have multiple heads. The intuition is that these successor nodes are, in general, executing at dif­ferent 
time points in the software pipelined schedule. And the buffer shifts its contents each time a new value 
is pro­duced by the associated node. Therefore the successors need to read from dfierent places of the 
buffer. Hence, a FIFO with multiple heads is required. Such a multiple-head buffer was illustrated in 
Figure 2 in Section 2. In the rest of this section, we illustrate two schemes to generate code which 
implements a FIFO buffer using regis­ters. The tradeoff of dedicated hardware architecture sup ports 
will also be discussed. Scheme I: Access Stationary Coding (ASC). In this scheme, the FIFO buffer between 
a producer node and its successor nodes is directly accessed using fixed reg­ister assignment for the 
tail and the heads. This sasign­ment is stationary , and will remain the same during the entire execution. 
On the other hand, the data in the FIFOS are explicitly shifted each time the producer node is writing 
a new value to its tail. The shift­ing can be realized by issuing multiple register move instructions, 
or by special architecture support for reg­ist er shifts. Scheme II: Data Stationary Coding (DSC). In 
this scheme, instead of letting the registers in a buffer to shift their contents, we simply let the 
next iteration write to the next position in the corresponding FIFO buffer. Thus, data are kept stationary, 
while accesses to the registers of a FIFO buffer are performed with the modulo addressing method. 5.1 
Scheme I: Access Stationary Coding Under Access Stationary Coding (ASC) scheme, the code generated with 
register shifting for our example loop L is given in Table 5. In the table, at cycle time O (or cycles 
2, 4, 6, etc) the FIFO buffer of two registers allocated to node S1 (for variable a) shifts its contents, 
and a new value is written into its tail a.. We assume that at the beginning of the cycle, all the old 
content are read off from the registers, and at the end of the same cycle new contents are written back 
into the registers. Therefore al = aO, = X + c have a. the effect of shifting the old contents in ao 
to the register al and the new result is written into the tail at the same aO clock cycle. It is safe 
to overwrite al at this moment because the schedule and the supporting buffer allocation guarantee that 
the old content of al is no longer used. We always align the shifting operation at the point when the 
corresponding instruction is issued. To ensure that the successors also read the correct results from 
the right places, we have to calculate the positions for them to read in the FIFO buffer. We have seen 
the use of multiple-head buffer in Section 2. Here we give a lemma to calculate the positions for the 
successors to access the data from the buffers: Lemma 5.1 Let (i,j) be an dependence arc in the DDG, 
that is to say, that node i is the producer and node j is a consumer (successor). The formula to calculate 
the position from which node j should read in the FIFO buffer of node i is: (33) Proof of Lemma 5.1: 
Node i writes the result to the tail of its FIFO buffer at time ti+ (K -1)* P in iteration K. This result 
will be read by node j in iteration K + rn,j. Node j in iteration K+ m,j is scheduled at cycle tj+ (K-1 
+ WZ,J) * P. Therefore the time difference between the production and the consumption is: [tJ+(K-l+7?l:J) 
*P]-[ti+(K-l)*P] tj-ti+?71ij *P. During this time interval, there are p -t,j?nt,*P 1-1= [+1+%-1 many 
register shiftings for the butfer allocated to node i. Also note that the above formula is independent 
of iteration K. Hence node j (in any iterations) should read from the buffer position indexed by the 
above formula which is (33). Cl As an example, we use formula (33) to calculate the posi­tions node 32 
and node S3 read from the FIFO buffer of size 3 allocated for node S1. For node S2, we have tz tl indezl,z 
= ( p l+7nl,2-1   = [L# 1+o.1 =o, so node .SZshould read from ao iteration 1 iteration 2 iteration 
3 iteration 4 J o buffer-shift: al =ao sl,l:aO=X+c 1 sz,l:b=aO*F 2 buffer-shift: al = a. sl,z:aO=X+c 
3 ss,l:c=al+b sz,z; b=aO*F 4 buffer-shift: al = a. sl,3:a0=X+c 5 sa,z:c=al+b sz,a:b=aO*F 6 buffer-shift: 
al = ao sl,l:aO=X-+c 7 ss,a:c=al+b sz,4:b=ao*F 8 9 sa,d:c=al+b Table 5: Code generated by ASC scheme. 
 For node S3, we have prologue code [ indezl,s = ( ~3; q+rn,,3-1 p] ­ epilogue code = 1, J so node 
S3 should read from al. If we examine line by line horizontally at the code in Table Table 7: Code from 
the repeating pattern of the ASC code. 5, we can discover a repeating pattern of the code from cycles 
2 to 3, as shown in Table 6. So far, we assume that the register shifting operationiteration i iteration 
i+ 1 I can be implemented using register moves (copying) in con­ventional architectures. However, it 
is also possible that a processor architecture supports register shifting directly in ~ hardware. Such 
support allows the ALUs be devoted to other computation functions, thus improving performance. Table 
6: The repeating pattern from the ASC code. 5.2 Scheme II: Data Stationary Coding The Data Stationary 
Coding (DSC) scheme proposed here is intended to avoid explicit register shiftings in the previousWe 
will use this repeating pattern as our new loop body. ASC scheme. Instead of letting the registers to 
shift theirThe original loop is now transformed into an new parallel contents, we simply let the next 
iteration write to the next loop body plus a prologue (lines O to 1) and an epilogue. The position in 
the corresponding FIFO buffer. For the successorimportant fact is that the new loop body is only of P(=2) 
nodes, we can not simply use formula (33) to calculate thecycles, which means that in every 2 cycles 
a new iteration positions to read in the FIFOS. Instead we lmust use modulo will start. That is the optimal 
rate we can obtain. The new addressing according to the following lemma. parallel loop is shown is Table 
7, in which the II sign means execute in parallel with . Lemma 5.2 For a dependence arc (i,j), if in 
the current Generally, let iteration node i is writing to Q, (where Q, is the index) of its buffer, then 
node j in current iteration should read from position: index~j = (Qi -~iJ) mod Rij (34) then the pattern 
is formed from time instances T P + 1 to T. where R, is the bufler size. Proof of Lemma 5.2: Suppose 
that the current iteration 6 Reduce Register Requirement Further by is K. Then node i writes to the position 
(K 1) mod R, since the data are not to be moved. In current iteration A , node j should read the result 
produced by node i in iteration K m,~. Therefore node j in current iteration should read from position 
(K m,j 1) mod R,. Substitute K 1 with Q:, we obtain the formula (34), The code generated using modulo 
addressing scheme is shown in Table 8. For example, at cycle P(i 1), we have the instruction a(,-l) 
mod z = X + c for iteration i. iteration i I m Table 8: Code for one iteration generated by DSC scheme. 
Its expanded version is shown in Table 9 on the next page. In Table 9 the repeating pattern is from 
cycle 2 to cycle 3, which is shown in Table 10. iteration i iteration i+l SI,,+I : a =X+c dz S3,, : c=a 
 1-1 )modztb 2, +1 : b = at mod 2 * F Table 10: Repeating pattern from the I)SC code. We can see from 
Table 10 that the pattern derived by using the DSC scheme contains less instructions than that of the 
ASC scheme, which is due to the elimination of the register shifting operations. The new parallel loop 
body is shown in Table 11. prologue code II for i= 1to n-P do I a,mod, =x+c c=af,_l~modz+bllb=atmod 
2*F enddo erilome code Table 11: New loop body from the DSC code repeating pat­tern. Coloring: Step 
2 In Section 5 we showed how to generate code from a re­peating pat tern. That finished the first step 
of our register allocation scheme. At this point, the FIFO buffer sizes and the schedule are all determined. 
However we still have the chance to share the buffer elements if their live ranges do not overlap with 
each other for this fixed schedule. Hence second step of our register allocation scheme is to apply the 
conventional coloring algorithm(s) to further reduce the reg­ister requirement. For each of the instructions 
that we allocated a buffer of size 1, the register may be t bought as a symbolic register, Each such 
symbolic registers may be reused. For an instruc­tion we allocated a buffer of size more than 1, the 
chance to reuse the registers in the buffer is only for the head register in the FIFO because all the 
other registers are live in the entire range of the loop cycle. In our example loop L, suppose that we 
use the ASC scheme to generate code, then we choose the repeating pat­tern in (6), and draw the live-range 
diagram of the variables in Figure 3 in Section 2. We can draw the interference graph according to the 
cir­cular arcs in Figure 3, and color the interference graph with 3 colors. For instance, the following 
is a legal coloring with 3 colors: color-1 = {al, c}, color_2= {b}, color.3 = {aO}. Therefore the actual 
number of registers required for the re­peating pattern is 3 for these 3 colors, plus 2 extra for loop 
invariants X and F, which totals 5 registers. In general we can use the coloring aigorit hms [8, 7, 16] 
to obt tin the mini­mum number of registers used in the new loop body. In this paper, we apply a recent 
method of cyclic interval graph coloring [16]. After the coloring algorithm the final code for the repeat­ing 
pattern is shown in Table 4 in Section 2, in which c is replaced by al since they have the same color. 
The coloring algorithm can also be applied to the code produced by DSC scheme. However the live ranges 
of the registers in a buffer of size more than one may last for sev­eral repeating patterns (new iterations) 
because there are no explicit register-shiftings now. For instance, the live ranges of the variables 
in Table 11 generated by the DSC scheme is shown in Figure 4. In the picture no two variables can be 
colored the same color. Hence the code already uses mini­mum number of registers and we do not need to 
change the code again for this example. 7 Related Work The early work by Aiken and Nicolau [2, 3, 4] 
did not con­sider register allocation problem. In a recent paper, Nicolau et al [23] considered the register 
allocation problem by re­naming for the compact ion-by-percolation based algorithms. Ebcioglu et al have 
proposed the technique of enhanced soft­ ware pipelining with resource constraints [12, 13, 11, 22]. 
However, they did not consider the minimum register allo­ cation problem as discussed in this paper. 
In Lam s work on software pipelining [20], an interesting scheme called raodrdo variable expansion is 
proposed to al­ iteration 1 iteration 2 iteration 3 iteration 4 0 sl,l:aO=X+c 1 sz,l:b=aO*F 2 sl,z:al=X+c 
3 ss,l:c=ao+b sz,z:b=a~*F 4 sl,3:a0=X+c 5 ss,z:c=al+b sz,s:b=aO*F 6 sl,4:al=X+c 7 sa,s:c=ao+b s2,4:b=a1*F 
8 9 s3,4:c=a1+b Table 9: Expanded code from DSC scheme. at a fixed initiation interval. The register 
allocation prob­lem is formulated as a bin-packing problem of vector lifetimes on a cylinder surface. 
A heuristic aJgorithm has been pro­ ----:­ ----.­ posed for the register allocation and has been demonstrated 
c~ %mod2~ to be quite effective by experimental results. However, the paper did not attempt to describe 
a complete concurrent b ----------------b---­:: scheduling-allocation strategy for software pipelining. 
In fact we have calculated the number of registers needed by our method for the example in Rau et al 
[28]. Our method --A ----o---- . _ --­uses 21 registers, while Rau et al claimed that 28 was needed b 
in their case, a 25~o improvement in register usage without  ----W - b compromising the speed. Other 
related work can be found in [14, 10]. Figure 4: Life range intervals for code generated by DSC scheme. 
8 Conclusions and Future Work Many conventional register allocation algorithms are based on the coloring 
of interference graphs representing overlap­low a scalar loop variables be expanded to use more than 
ping relations of the life ranges of program variables given one location so that the unnecessary precedence 
constraints a sequential execution schedule [8, 7, 1]. In this paper,due to scalar variable in different 
iterations can be removed. we formulate the register allocation problem as part of the However modulo 
variable expansion is only performed after joint schedule-allocation problem. Intuitively, we are usingthe 
schedule has been fixed. The work described in in this register allocation as a constraint to the software 
pipelin­paper can be considered as an extension to modulo variable ing scheduling process to derive, 
among all time-optimalexpansion in the sense that it is incorporated in a unified schedules, the ones 
which have the potential to use the min­framework of time-optimaJ scheduling, and minimizes the imum 
number of registers, Once such a schedule is fixed, we amount of storage for scalar variable expansion 
and array can use conventional methods to further minimize the regis­variable shrinkage. ter requirements. 
This paper has provided a new framework Callahan, Carr and Kennedy have studied re~ister allo­ for simultaneous 
scheduling and register allocation for soft­cation for subscripted variables. In their method, array 
 ware pipelining. We plan to extend this framework to real references which are live across several iterations 
are rec­ applications in our future work. One scenario is that the ognized and a source-to-source transformation 
called scalar number of available registers is given. Another scenario is replacement is performed such 
that they can be handled by that the loop body may contain conditionals. coloring-based register allocators. 
However, their work is aimed at sequential loop execution and does not consider loop scheduling such 
as software pipelining. We have shown References in [25, 24] that our OSBA formulation (14) includes 
Calla­han et al s result as a special case. <RefA>[I] A. V. Aho, R. Sethi, and J. D. Unman. Compilers Principles, 
Techniques, and Tools. Addison. WesleYIn a recent paper by Rau et al [28], a method of regis­ ter allocation 
for software pipelining was presented. In t-his Publishing Co., 1986. met hod, register allocation is 
performed after the so-called [2] A. Aiken. Compaction-based parallelization. (PhD the­modular scheduling 
phase. Successive iterations are initiated sis), Technical Report 88 922, Cornell University, 1988. 
[3] A. Aiken and A. Nicolau. Optimal loop parallelization. In Proceedings of the 1988A CM SIGPLAN Conference 
on Programming Languages Design and Imp!ementa­tiorz, June 1988. [4] A. Aiken and A. Nicolau. A realistic 
resource­constrained software pipelining algorithm. In Proceed­ing of the Third Workshop on Programming 
Languages and Compilers for Parallel Computing, Irvine, CA, Au­gust 1990. [5] D. G. Bradlee, S. J. Eggers, 
and R. R. Henry. Integrat­ing register allocation and instruction scheduling for RISCS. International 
Conference on Architectural Sup­port for Programming Languages and Operating .Sys­tenas (ASPLOS IV), 
pages 122-131, April 1991. [6] David Callahan, Steve Carr, and Ken Kennedy. Improv­ing register allocation 
for subscripted variables. Pro­ceedings of the SIGPLA N 90 Conference on Program­ming Language Design 
and Implementation, June 1990. White Plains, NY. [7] G. J. Chaitin. Register allocation &#38; spilling 
via graph coloring. ACM SIGPLAN Syrnp. on Compiler Con­struction, pages 98 105, 1982. [8] G. J. Chaitin, 
M. Auslander, A. Chandra, J. Cocke, M. Hopkins, and P. Markstein. Register allocation via coloring. 
Computer Languages 6, pages 47-57, January 1981.  [9] V. ChvataL Linear Porgranwning, W. Ii, Freeman 
and Company., 1983. [10] E. Duesterwald, R. Gupta, and M.L. Sofia. Register pipelining: An integrated 
approacn to register alloca­tion for scalar and subscripted variables. Technical re­port, Department 
of Computer Science, University of Pittsburgh, 1991. [11] K. Ebcioglu and T. Nakatani. A new compilation 
technique for parallelization loops with unpredictable branches on a VLIW architecture. Technical report, 
IBM, 1990. [12] K. Ebcio~lu. A compilation technique for software pipelining of loops with conditional 
jumps. In Proceed­ings oj the 20th Annual Workshop on Microprograrn­rning, December 1987. [13] K. Ebcio~lu 
and A. Nicolau. A global resource­constrained parallelization technique. In Proceedings of the ACM SIGARCH 
International Conference on Su­percomputing, June 1989. [14] Christine Eisenbeis, WiUiam Jalby, Daniel 
Windheiser, and Francois Bodin. A strategy for array management in local memory. In Third Workshop on 
Programming Languages and Compiters for Parallel Computing. Uni­versit y of California, Irvine, 1990. 
To be published by Pitman/MIT Press. [15] P. B. Gibbons and S. S. Muchnick. Efficient instruc­tion scheduling 
for a pipelined architecture. In Proceed­ings of the ACM Symposium on Compiler Construction, pages 11 
16, PaJo Alto, CA, June 1986. [16] L. Hendren, G.R. Gao, E. Altman, and C. Mnkerji. A register allocation 
framework based on hierarchical cyclic interval graphs. Lecture Notes in Computer Sci­ence 641, pages 
176 191, October 1992. [17] J. Hennessy and T. Gross. Postpass code optimization of pipelined constraints. 
ACM Transactions on Pro­gramming Languages and Systems, 5(3):422-448, July 1983. [18] N. Karmarkar. A 
new polynomiaJ-time algorithm for linear programming. Combinatorics, 4:373-395, 1984. [19] L. G. Khachian. 
A polynomial algorithm in linear pro­gramming. Scwiet Math. Doldadg, 20:191 194, 1979. [20] Monica Lam. 
Software pipelining: An effective schedul­ing technique for VLIW machines. In Proceedings of the 1988 
ACM SIGPLAN Conference on Programming Languages Design and Implementation, pages 318 328, Atlanta, GA, 
June 1988. [21] Eugene L. Lawler. Combinatorial Optimization: Net­works and .kfatroids. Saunders College 
Publishing, Ft Worth, TX, 1976. [22] T. Nakatani and K. Ebcioglu. Using a lookahaed win­dow in a compaction 
based parallelizing compiler. In Proceedings of the 23rd A nrmal Workshop on Micropro­gramming and Microarchitectures, 
pages 57 68, 1990. [23] A. Nicolau, R. Potasman, and H. Wang. Register al­location, renaming and their 
impact on fine-grained parallelism. In U. Banerjee et al., editor, Languages and Compilers for Parallel 
Computing, Lecture Notes in Computer Science 589, pages 359 373, Santa Clara, California, 1992. Springer-Verlag. 
[24] Q. Ning and G.R. Gao. A novel framework of reg­ister allocation for soft ware pipelining. Technical 
Re­port ACAPS Technique Memo 42, School of Computer Science, McGill University, Montreal, Quebec, Canada, 
1992. [25] Qi Ning. Optimal Register Allocation to Support Time-Optimal Scheduling for Loops. PhD thesis, 
in prepa­ration, School of Computer Science, McGill University, 1992. [26] C. V. Ramamoorthy and G. S. 
Ho. Performance evac­uation of asynchronous concurrent systems using Petri Nets. IEEE Transactions on 
Computers, pages 440­448, September 1980. [27] B. R. Rau and C. D. Glaeser. Some scheduling tech­niques 
and an easily schedulable horizontal architecture for high performance scientific computing. In Proceed­ings 
of the L/th Annual Workshop on Microprogram­ming, pages 183 198, 1981. [28] B.R. Rau, M. Lee, P.P. Tirumalai, 
and M.S. Schlansker. Register allocation for modulo scheduled loops: Strate­gies, algorithms and heuristics. 
In Proceedings oj SIG-PLAN 92 Conf. on Programming Language Design and Implementation, San Francisco, 
CA, 1992. [29] R. Reiter. Scheduling parallel computations. .70urnal oj ACM, 15:590 599, October 1968. 
[30] A. Schrijver. Theorg of Linear and Integer Program­ming. John Wiley and Sons, 1986. [31] R. F. Touzeau. 
A FORTRAN compiler for the FPS­164 scientific computer. In Proceedings of the ACM SIGPLAN 84 Symposium 
on Compiler Construction, pages 48-57, June 1984. [32] H.S. Warren. Instruction scheduling for the IBM 
RISC System/6000 processor. IBM J. Res. Develop., 34(l), January 1990. </RefA>Appendix A Example from Rau Et 
Al s Paper In this appendix, we look at the example loop given in Rau Et Al s paper [28]. The loop is 
shown in Table 12. fori=l tondo s=s+a[i] a[i] = .9* s* a[i] enddo n Table 12: Example from Rau et al 
s paper. Its low level representation, like 3-address code, is shown in Table 13. ur33 = VT33 + VT32 
% vr33 is address of a[i] % zm34 = loadrn(or33) % vr34 = a[i]% V?-35 = ur35 + v r34 %vr35=s% vr36 = wr35 
+ vr35 vr37 = VT36 + rw34 % vr37 = new a~] % store(rw37, m(vr33)) branch ifisn Table 13: Low level intermediate 
represent ation. We will focus on the low level representation and generate code for it. The data dependence 
gr,aph of the low level presentation is shown in Figure 5. The delay for Add and Store is 1, the delay 
for Multiply is 2 and delay for Load is 13. There is only one directed cycle in the dependence graph, 
which is a self-loop from node a to a. The B-ratio of it is 1. Therefore we can generate a schedule with 
period of 1. However since Rau et al used 2 as their period, we will also use 2 as our period for generating 
the schedule and the register allocation. The OSBA formulation of the low level representation is in 
35. Figure 5: Data dependence graph of the intermediate repre­sent ation. minba+bb+bc+bd+be subject to 
a ba+ta tb>l ba+ta tf~4 ba+ta ta>4 bb+tb tc>2 bt, +tb-te>2 bc+tc td>2 bd+td te>2 (35) be+t. tf>2 tb ta>13 
tf ta> l ta t. > l t. tb~ls t. tb>ls td t. >l t. td> 2 tf te>2 Solving (s5) we obtain the following 
schedule and buffer allocation: t.=l, tb=o, t.=13, td=14, t.= 16, tf =18; (36) ba=; ,bb=9, bC=; ,bd=2, 
b.= 2. (37) With the technique (28) in Section 4, we obtain the fol­lowing buffer allocation: R.=lO>Rt, 
=8, Rc=l, Rd=l, Re= 1. (38) Since the life ranges of the variables in the repeating pat­tern all overlap, 
our second step of coloring algorithm is not necessary now. So there is a total of 21 register allocated, 
while Rau et al used 28 registers in [28].  
			
