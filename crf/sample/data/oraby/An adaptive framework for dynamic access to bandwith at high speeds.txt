
 An Adaptive Framework for Dynamic Access to Bandwidth at High Speeds Kerry W. Fendick Manoel A. AT&#38;T 
Bell Crawfords Holmdel, NJ ABSTRACT Emerging technologies for high-speed data network provide users with 
explicit feedback about congestion. Users, however, have questioned whether feedback can help in avoiding 
congestion in wide-area networks that operate at speeds of multiple megabits-or gigabits-per­second. 
In this paper we analyze a class of adaptive schemes with delayed feedback, where adaptive sources interact 
with each other as well as with non-adaptive sources through a single queue. We model the network as 
a stochastic, delay-differential equation and the rate at which an adaptive source transmits data as 
a fluid. Our model accounts for different levels of traffic burstiness introduced by the non-adaptive 
sources and for distinct propagation delays for different adaptive sources. We show how the performance 
increasing network speeds the bandwidth available Simulation results show of the system scales with for 
quite general fluctuations in to the adaptive sources, the accuracy of the model s predictions as a function 
of key parameters, This paper should raise the expectations of users about the potential effectiveness 
of responding to congestion notification. I. Introduction Emerging technologies for high-speed data networks 
(e.g., frame and cell relay) are offering users dynamic access to unprecedented amounts of bandwidth. 
Since end-to-end throughputs degrade rapidly as data loss grows inside the network, end users benefit 
most tlom this bandwidth if they can maximize their rates without suffering data loss in the process. 
Their success in operating in such a regime seems to hinge on their ability to adapt to the conditions 
of the network, With that in mind, some of these new technologies were designed with the capability of 
providing network access points with explicit feedback about congestion inside the network. Permission 
to copy without fee all or part of this material is granted provided that the copies are not made or 
distributed for dhCt Commercial adVanIa!#3, The ACM copyright notice end the title of the publication 
and its date appear, and notica is givan that copying is by permission of the Association for Computing 
Machinery. To copy otharwise, or to republish, raquires a fee and/or spacific permission. SIGCOMM 93 
-Ithaca, N.Y., USA /9/93 @ 1993 ACM 0-89791-61 9-0193/000910127...$1 .50 Rodrigues Laboratories Corner 
Road 07733-3030 Users of high-speed networks, however, have questioned whether explicit or implicit feedback 
can help in avoiding congestion. At speeds of multiple rnegabits­or gigabits-per-second, congestion in 
the network can arise over shorter time scales than those at which end protocols typically operate. This 
congestion can dissipate just as rapidly, even before feedback about it reaches the users, Users therefore 
face the jiltering problem of estimating them in tie can neither users will distributed knowledge of 
different the expected amount of bandwidth available to face of noise in this bandwidth, which they predict 
nor affect. Furthermore, different attempt to solve this problem with their own algorithms; each must 
do so without direct of the actions taken by the others and iu spite feedback delays. We show in this 
paper that the dual goals of keeping losses low and utilizations high can be achieved using network feedback 
if both the network and the users do their share. There are at least four major tasks required from the 
network to achieve these goals. First, it must control the amount of burstiness in traffic. Second, the 
network must police tm.ffic, dropping some if necess~. Third, the network must have large enough buffers 
to accommodate the allowed randomness in traffic. Finally, the network must send feedback productively; 
the feedback should contain more information about the trend in usage of buffers and bandwidth than about 
transient congestion introduced by the allowed randomness. The users, as we show in this paper, will 
find a large set of effective adaptive controls available when the network provides the proper controls. 
For a simple, one-node network with ncmnegligible propagation delays, we derive a particular stochastic 
delay-differential equation (WIDE) as a model appropriate for large transmission rates. We then derive 
a set of policies that optimizes performzmce for this model (in a senm described later). The SD13E studied 
here, which describes the buffer content of tbe single network queue, represents several types of randomness 
in the bandwidth available to adaptive sources. It models non. stationary fluctuations in the available 
bandwidth, m well as some effects of loss priorities at the queue. 127 The class of adaptive algorithms 
studied here is not new, and its performance in less random environments is described in Fendick, Mitra, 
Mitrani, Rodrigues, Seery, and Weiss[l], Fendick, Rodrigues, and Weiss[2], and Fendick and Rodrigues[3]. 
Similar algorithms also have studied in Elwalid[4], Mitra[5], Mitra and Seery[61 [71[81and Mukherjee 
and Strikwerda[9], In this paper we show through analysis and simulation that the scaling for adaptive 
schemes with network speed obtained in Fendick, Rodrigues and Weiss[z] also holds when several adaptive 
and non-adaptive sources interact through a single queue (a more detailed analysis can be found in Fendick 
and Rodrigues[3]). The main contribution of this paper is to describe the performance of these algorithms 
when facing quite general types of random fluctuations in bandwidth (e.g., as caused by nonadaptive traffic 
controlled by loss priority mechanisms at a network queue). Our results attest to the robust behavior 
of these algorithms, when the network polices traffic properly. The paper is organized as follows: In 
Section II, we verify through simulations that more complex adaptive systems scale with speed as predicted 
in 2], prompting some additional questions. In Section III we describe a network model that captures 
the more complex characteristics of the simulated adaptive system and derive the approximating SDDE, 
In Section IV, we introduce the class of adaptive schemes studied here. In Section V we state and interpret 
our analytical results and the conditions under which they hold. In Section VI we present simulation 
results that validate these results for some examples. In Section VII we conclude with some remarks about 
the implications of our results for potential users of high speed networks. II. Simulation Examples We 
begin by using simulations to study how a simple adaptive algorithm performs in a queue that serves more 
than one adaptive source as well as random, nonadaptive traffic. We use the scaling of parameters for 
the adaptive algorithm suggested in Fendick, Rodrigues, and Weiss[2]. Although the analysis there pertains 
to a queue that serves a single, deterministic, adaptive source, the scaling recommended there was largely 
motivated by the need for the adaptive algorithm to perform well in more complex environments, We now 
test this scaling in such environments, If the results from Fendick, Rodrigues, and Weiss[2] do extend 
to cover them, then we will observe queue lengths ofO(I31 2 J and queue utilizations of 0(1 -B-1 2) x 
100% when the trunk transmission rate p for the queue is large. 1. Description of the System We study 
the model of a virtual-circuit network depicted in Figure 1. The network consists of a single queue that 
serves fixed-lengthed cells at rate B from both adaptive and non­adaptive sources. Each adaptive source 
I transmits cells at rate Li (t) at time t, Data from adaptive source i arrives at the queue after a 
fixed delay ~i,l. The queue is served according to the First-in-First-Out discipline and generates feedback 
to source I at rate ri about the state x(t) of its shared buffer. The feedback arrives at the transmitter 
of the adaptive source after an additional fixed delay 7,.2, and the adaptive source then changes its 
rate based on this feedback. Non-Adaptive Traffic 1 ~1: m 1: G1 ~1. m 1: ~1: 3 Queue ~~~ kil~ Figure 
1. The System Model. 2. Description of the Sources We assume that the adaptive algorithm for source chooses 
a new rate Li every r; 1 units of time and transmits deterministically-spaced cells at this new rate 
until the next update, The new rate chosen at timer of an update is related to the queue length at time 
t T/,2 and the previous rate by k~(?) = IIMX 122i22ki(f-r~l)+Fi(X(t-Ti,2 )), hi , li .(2.1) [{ }] If 
X(t -~i,2) in (2.1) is below a threshold B, then Li increases at time t according to the function ~i, 
but subject to the constraint that it does not exceed hi> O; otherwise, it decreases according to ~i, 
but subject to the constraint that it does not go below li >0. We further assume that Fi in (2.1) scales 
with increasing trunk rates through positive scale parameters a i and y such that 19Q LLU Fi(x) = ai 
.f(; -b), (2.2) where f and b .B/y do not scale with the trunk rate. Using the scaling in Fendick, Rodrigues, 
and Weiss[*], we let kifi ~i= (2.3) rl and B = yb = g~, (2.4) where ~1and g are positive constants 
that do not scale with the trunk rate 13.For the simulation examples, we also let f(Y) = -Y, (2.4) so 
that Fi (x) decreases linearly as x increases. In our analytical treatment, however, we will consider 
more general decreasing functions$ To model non-adaptive traffic, we assume that the queue also servers 
a single on-off source with fixed-length busy periods and random idle periods, as depicted in Figure 
2. mmn .*. la mm .*O -time 1I 11 @b --~- exp(~- u) fi- Figure 2. Model for the Nonadaptive Source We 
assume that the non-adaptive source transmits messages of size rn cells. The arrival times of cells in 
the message are separated by gaps of length 13-1. Following a gap of length p-1 after the last cell of 
the message, the source remains idle for a exponentially distributed period with mean length fJ-1w, and 
then begins to transmit the next message of m cells. Later, we will consider more general models of non-adaptive 
traffic. 3. Examples A and B In all example in this paper, time is measured in units of round-trip propagation 
delay, and we let ~i,l = ~i,z = 0.5. For Example A, set ~ = 320 ad assume that the queue serves two adaptive 
sources with identical p~~ettXS: 1, = 64, hi = 320, ri = 2.0, ki = 0.35777, ~d g = 0.805. To specify 
tie non-adaptive source, let m = 2 and o = 18.0 for an arrival rate to the queue of 32, or 10% of the 
trunk s rate. Figure 3 shows the resulting queue lengths x(r) for OSJSSOO.The solid horizontal line in 
Figure 3 at a level of about 14 cells is the threshold B. l he average throughput for the queue is 92,5% 
of the trunk rate for 0<K800. The buffer size z for Example A corresponds to the top of the plot in Figure 
3, so that no cells overflow the buffer during the length of the simulation. We use Example A as a point 
of reference for other examples in this paper. In Example B, we multiple the trunk rate and the bounds 
on the adaptive rates from Example A by ten, so that @= 3200, Ii = 640, ~d hi = 3200 fOr i=l,2. We now 
let rj = 4.0. (The choice of f3=3200 ~d ~i,l =ti,z =0.5 corresponds to an ATM network with 53-byte cells, 
a 45-Mbps trunk, and a 30 msec round-trip delay.) For fixed o, the parameters of our model for non-adaptive 
traffic automatically scale with (3to keep the transmission rate of non-adaptive traffic equal to a fixed 
proportion of the trunk rate. We therefore use o = 18.0 as in Example A to maintain a non-adaptive arrival 
rate of 10% alf the trunk rate. The constants ~i and g do not change, but ai is resealed according to 
(2.3) and B according to (2.4). Figure 4 shows the new queue lengths x(t) for OIStSXKL As in Figure 3, 
the threshold B a pears as a horizontal line. By (2.4), the value for B is ? 10 times higher than its 
value in Example A. Comparing Figures 3 and 4, we see that queue lengths also scale by a factor of about 
G, since the peak queue lengths increase in proportion to B. Therefore, no cells overflow the buffer 
during the length of the simulation run in Figure 4, if the buffer size is increased from Example A to 
Example B in proportion to the buffer threshold B. The average through]?ut for the queue in Example B 
is 97.9% of the trunk rate for osts 1000, meaning that the proportion of the potential throughput lost 
because of idleness decreases by a factor of (1 -.979)/( 1-.925) -0.28. This is close to 11~ z 0.316. 
Hence, both the queue lengths and the throughputs in this example scale as predicted ii Fendick, Rodrigues, 
and Weiss[*]. 4. New Questions These examples reassure us that our previous results do extend to more 
complex environments. They also suggest some new questions: 1. Can we explain why our previous results 
extend to more complex environments using some analytical framework? 2. How should the parameters of 
the control scheme depend on the amount of non-adaptive baffic or its burstiness? 3. Would our previous 
results still hold if the nonadaptive traffic changed with the trunk rate p in ways different from the 
simple resealing used in the above examples? 4. Can networks control the characteristics of non­adaptive 
traffic to assure good performance for adaptive traffic?  The remainder of this paper attempts to answer 
these questions. Ill . Model of the Queue We now describe the behavior of a single network queue carrying 
both adaptive and non-adaptive traffic. In Section 111.1, we show that, at high networks speeds, the 
nonadaptive traffic will have a component that behaves like a 13rowniarr motion. In Section 111.2, we 
introduce our final model with additional types of randomness. 1. The Brownkn Component of the Nonadaptive 
Traffic Over time, as transmission rates in data networks increase, the characteristics of nonadaptive 
traffic offered to the network can scale in a variety of ways. In this section, we focus on one such 
scaling, in which the mean rate of the nonadaptive traffic increases in a jixed proportion p to the transmission 
rate p at a network queue. Let g~(t)denote the cumulative number of cells offered to the queue up to 
time zby non-adaptive sources. We assume that q(t) = m) (3.1) for some non-decreasing process E( ) independent 
of O, such that ~~(;) = pf for all t >0. This is the scaling used for the non-adaptive traffic in going 
from Example A to Example B in Section H. It gives the desired scaling properties for the mean rate, 
since E~p(t)=p!3r for all t. We allow ~ to have a fairly general covariance structure, but assume that 
any correlation between the number of cells arriving in disjoint intervals becomes small as the time 
between the intervals becomes large. Such processes, which include the most comonly used open-network 
models of data traffic, are called weakly dependent. Then, = p~ci, (3.2) where c~ is a dimensionless 
parameter that describes the burstiness of the nonadaptive traffic. In Section VI, we describe how to 
calculate cj for the simulation examples there and in Section H, as well as for more general models. 
Let y(t) = I% -KP(o. We call the formal derivative of y the residual bandwidth available to the adaptive 
sources. The residual bandwidth may be negative, at which time the buffer content of the queue would 
increase or ovefiow even without the adaptive traffic. From the standpoint of an adaptive source separated 
from the queue by a fixed propagation delay (which does not scale with ~), we may write where 7 is a 
predictable trend and ? has the property that E $(f) ~(s)l $ (u) for 09As = o for all t 2 s 20. [ 1 
The process $ is, in principle, unpredictable. By strong­approximation theorems from Csorgo and Rdvdsz[lO], 
$(t) = (1 p)fJt + o(l) us ~ -+CC and (3.4) $(f) = uW(t) + O(B1 2) us B --) co, (3.5) where w(t) is a 
standard Brownian motion. At an intuitive level, the scaling assumed for the process =P in (3.1) compresses 
time for the process ~ as p increases, so that events that occur far apart in ~ will occur close together 
in .gP when D is large. Because we have assumed that ~ is weakly dependent, the correlation between the 
number of cells counted by gb in disjoint intervals (separated by a fixed time) will becomes small as 
13becomes large. The random component of gP then looks more and more like a Brownian motion, which has 
uncorrelated increments. The input process EP supplies all the burstiness in y, so that the same Brownian 
component appears in y for large L By (3.2), u = o(@ 2) as B+o. (3.6) Also note that the Bmwnian motion 
w(t) in (3,5) does not depend on 13.Hence, we interpret (3,4) and (3.5) as saying that the amount of 
predictable residual bandwidth at any time tgrows like P for large B, and the unpredictable noise in 
the residual bandwidth like u = o(f11 2). We next describe the buffer content of a queue sewing n adaptive 
sources with rates ki for i = 1,... ,tt as well as the nonadaptive traffic. We do not specify the algorithm 
yet for choosing the adaptive rates ii, but we assume as in Section II that the queue at time t feels 
the effects of the rates ?+i(si) determined at times S, = t ti,l, Using the convention that the processor 
at the queue continues to run at its transmission rate p even when the queue is empty, we let L(I) denote 
the amount of wasted bandwidth by time ~. Similarly, we assume that the offered traffic continues to 
arrive when the queue equals its maximum value Z, and we let U(I) denote the amount of offered traffic 
that overflows the buffer by time t. Finally, we assume that y(o) =L(0) = u(o) =0. Then, for t>o, ;the 
buffer content X(I) is related to the other processes defined above via x(t) = ~ j ki(S- t f,~)dS -~(t) 
+ L.(t)U(t) - i=lb . ~ j ki(.S-Zi,~)dS -~(t) -~(t) + L(t) -U(t). (3.7)i=lo If z=o(p ) for large B, 
then x(~) = 0(p 2) in (3.7), since osx(t)sz, Likewise, by (3.5) and (3,6), the term on the right-hand 
side of (3.7) due to the unpredictable noise in the residual bandwidth is O@1 2). On the other hand, 
by (3.4), the term on the right-hand side of (3.7) due to the predictable residual bandwidth is o@). 
Whether ~(t) and u(t)in (3.7) are 0(131 2) or o(p) depends on the characteristics of the terms in (3.7) 
due to the adaptive sources. The adaptive sources attempt to consume the predictable residual bandwidth 
and thus offset its O(D) size. One important consequence of (3.3)-(3.7) is that the amount of throughput 
lost at the queue as quantified by the sum L + u necessarily is at least 0( f1112) for large 13 when 
the buffer size is O(B 2 ), because of the noise?. This noise is, in principle unpredictable, so the 
best an adaptive control can hope to do is to match the predictable part ~ of y. Using (3.4)-(3.5), we 
rewrite (3.7) as II x(t)= ~ ~ki($-~i,l)d -(1-p)~t + oW(t) + L(t) (3.8) i=lo u(t) + O(pz) a f3+c0. This 
representation will form the basis of our complete model of the queue, described next. 2. The Complete 
Model of the Queue If it were not for the term 0@l/2), equation (3.8) would describe a stochastic delay-differential 
equation (SDDE) with reflecting (regulating) boundaries, suggesting what may be a good model for large 
values of 13. In Fendick and Rodrigues[3], we prove convergence of (3.8) to such an equation. However, 
the purpose of this paper is not to give formal proofs of convergence. Thus, we will model the system 
directly as a SDDE. Furthermore, we show that with a properly chosen adaptive algorithm, the performance 
of the SDDE is robust to several types of random perturbations that are much larger than O(I31 2). One 
such perturbation that we study causes the queue length to jump unpredictably, which is a particularly 
difficult phenomena for an adaptive control to handle. We therefore feel that our tinal model of the 
queue not only captures the essential characteristics of (3.8), but also stresses the abilities of the 
adaptive controls beyond (3.8). Our basic model, without the additional random fluctuations, is given 
by n X(t)= ~jki($ Ti,l)dS (l-p)~t + OW(t) /=10 + L(t) U(t) for I 20. (3.9) For our first addition to 
the right-hand side of (3.9), we introduce a continuous random process@ deiined by @(t,x(t)) = QI (t)-@z(t) 
+ @~(t,x(t)) , where @~ and Q2 are arbitrary continuous, nondecreasing random processes and t cl~(t,x(t)) 
= J+(s,x(s))ds o for random rates $(t,x(t))a).regard @1 and @l We as introducing additional random fluctuations 
in the bandwidth available to adaptive sources, and @s as an additional input from low-priority traffic 
sources. We allow @s to depend on the current queue length x(t), so that we can model the use of buffer 
threshc~lds at the queue, above which some input is dropped before it ever enters the queue; see Doshi 
and Jagemxm[l 1] and Fendick and Hemandez-Valencia[12] for similar applications of state-dependent processes. 
Note that low priority traffic does not contribute directly towards the loss of high priority traffic 
recorded by u(t), although it does affect the value ofx(t). The scaling that we assume for the nonadaptive 
stream gb in Section III. 1 implies that burst sizes are 0(1) as 13increases, To model nonadaptive traffic 
for which burst sizes grow with p we further extend. (3.9) by introducing a jump process J(t,x), which 
is constant between jumps. The jumps produce sudden changes in the queue-length process. We characterize 
J(r,x) through a jump rate r( ) and a jump-size density fi(, ). The probability of a jump in the interval 
(t,t +At] is equal to r(x(t)) + o(At) for small At, where r(. ) is some positive, continuous function 
of the queue length X. Given that a jump occurs at t, its size Y has probability density it(x(t -), y). 
This defines a time-inhomogeneous batch r Poisson process. We assume that J fi(x, Y)ffY = 1 for some 
positive value r that does not depe~d on X. Although this treatment of the jump process is not as general 
as our treatment of the nonadaptive traffic g~, batch Poisson traffic is a particularly difficult type 
of nonadaptive traffic for an adaptive source to face. It results in sudden, unpredictable changes in 
the queue. Our model for J allows both positive and negative jumps. We view the negative jumps and some 
of the positive jumps as surrogates for the error term in (3.8) that we drop to go to (3,9). Other positive 
jumps may represent batch arrivals of large files to the queue, We will always regard the process J as 
low-priority traffic, so that we do not wish to include the portion of J that overflows the buffers in 
our account u(t) of data loss due to overflow. Without loss of generality, we therefore assume that a 
positive jump never overflows the buffer, although it may completely fill it. Thus, if a positive jump 
occurs at time t, we assume that it(x(t ), w -x(r -)) = o for w exceeding the buffer size Z. Similarly, 
if a negative jump occurs at time t, we assume that ti(x(t -), w) = o for w less than (-x(t -)). This 
means that a negative jump does not push on the lower boundary of the queue and thus does not directly 
contribute to the throughput loss L(1) due to idleness. The negative jumps will still contribute indirectly 
to idleness of the queue, since the other traffic may keep x(t) near the boundary for a period following 
a negative jump to the boundary. With the addition Of J and Q, we define our tinal model of the network 
queue by .X(t) = ~ -(1-p)pf + CJw(t) (3.10) i=lo n JL~(S-T/,l)dI + @(t,x(t)) + J(t,x(t)) + L(t) -U(r) 
for t 20, where x(0) =czI(o,x(o))=J(o, x(o)) =L(o)=u(o)=o and OSX()<Z. In (3.10), L(t) and u(t) retain 
their interpretation as the amount of throughput lost because of idleness and buffer overflow, respectively. 
IV. An Approximation for the Adaptive Algorithm We now need an adaptive algorithm that captures the essential 
characteristics of (2.1). Rather than working directly with (2.1), we work with an approximation to it 
for large ri, o if ki(t)=l, snd X(t T/,Z)>~ +kf(l)= r f i( )=hi ad ( -T 2)<B (4.1) / Ff(X(t-Tj,2))r~ 
otherwise. 1 If r, grows with fJ then we also may view (4.1) as an approximation to (2.1) for large 
p. We define Fi in (4.1) as in (2.2). We do not need to restrict ourselves to the linear function f in 
(2.4), however, We assume only that f is a continuously differentiable, non-increasing function with 
f(0) =O. The values of the parameters ai, ri, Zi, and hi in (2.2) and (4. 1) all may depend on p (the 
average amount of bandwidth available to the adaptive sources). We assume that ~li<(l-p)~< ~hi, so that 
the sum of the rates of all i=l i-l adaptive sources can vary over a range that includes (1-p) f! (the 
mean amount of bandwidth available to them). While the parameters li and hi in (4.1) may scale with B 
as well, our results do not depend on their values or on the particular scaling used. The number n of 
adaptive sources may scale with 13,while the propagation delays zi,1 and ~i,2 do not. Finally, we assume 
that z = y(b +c) is the buffer size of the queue, where c >0 does not scale with&#38; A ~scussion of 
how a scheme like (4.1) relates to other adaptive schemes can be found in Fendick, Rodrigues and Weiss[2]. 
V. The Results We next discuss the performance of the adaptive algorithm (4.1) in optimizing throughput 
for the adaptive sources while controlling throughput loss at the queue (3,10), To optimize throughputs, 
we must restrict the sizes of parameters for the adaptive algorithm and of the non-adaptive fluctuations 
in the queue s bandwidth. To state our results, let ~ = lim t-1m(t) denote the t-)ca average amount of 
throughput lost per unit time in (3.10) due to idleness, and U= ,~mt-1Eu(t) denote the average amount 
of throughput lost per unit time due to overflow of the buffer. Similarly, let ii = limt-lE jli(S)dS, 
I+ca o and tiz = /~&#38; t-1 E @2(f). We assume that, for large f3, y = O(u) and (5.1) (5.2) (5.3) 62 
= O(f# 2), and (5.4) r( ) = O(l), (5.5) where u= O(P1 2) by (3.2), Later we describe how the network 
may control nonadaptive traffic to achieve (5.3)­(5.5). Theorem 1, ZJfor large 13,(5.1)-(5.5) hold and 
@(t,x(f)) = 0(~1 2) for any t, (5.6) then the system a!q%ed by (3.10) and (4.1) satisjies z = o(o), 
(5.7) ti = O(U), and (5.8) ~Xi -(l-p)~ = o(tJ), (5.9)i=l Furthermore, (5,7) and (5.8) are tight, in that 
either U> 0(6) or Z 2 O(U). (5.10) Theorem 2. 2 for large II, (5.1)-(5.5) hold and @(t,x(t)) = 0(~1/2 
) whenever x(t) > B, (5.11) then (.5.7), (5.8) and (5,10) still hold for the system dejined by (3,10) 
and (4.1), and jji, +5-(1-p)~ = o(u), (5.12) i=l The proofs of Theorems 1 and 2 follow precisely the 
same lines as the proof of Theorem 1 and its corollaries in Fendick and Rodrigues[31, but uses the greater 
generality of Ito s formula discussed in Chapter 4 of Harrison[13] and Section 1.5 of Kushner and Dupuis[14]. 
As we shall discuss, Theorem 1 describes performance without loss priorities at the queue, and Theorem 
2 discusses performance with loss priorities, Because U2 increases with the parameter .~ in (3.2) that 
describes the burstiness of the Brownian component of the non-adaptive traffic, (5.7)-(5,10) and (5.12) 
imply that throughput loss due to idleness or overflow will increase as cl increases. This is attributable 
to the jump term J in (3.10); without it, we could compensate for any change in c~ through the scale 
parameter y for the buffer size z =Y (b +c) and buffer threshold B =Y b. We view c~ as an 0(I) parameter 
as p increases, however, so that the dominant effect of an increase in 13will be a decrease in throughput 
loss as a proportion of the trunk rate. By (3.6), (5.7)-(5.9), and (5,12), bandwidth utilization will 
approaches 100% and the percentage of offered throughput lost because of overflowing buffers will approach 
070 as 13increases, This is accomplished with a buffer of size 0(~1/2 ), so that queueing delays will 
approach O as 13increases. The 0(131/2) scaling of the throughput loss and the buffer size constitutes 
what Mitra[]5] calls the moderate usage regime, because it maximizes power (throughput + response time) 
as 13+~: see also Section 4 of Fendick and Rodrigues[3]. Theorem 1 addresses questions (1) and (2) of 
Section 11.4. Theorems 1 and 2 address question (3) there. We next discuss the above results in the context 
of a broader architecture for congestion control. In Sections V,3 and V.4, we address question (4) of 
Section 2.4. 1. Generating Feedback By (5.1), the Brownian component of the nonadaptive traffic should 
determine the scale of both the buffer size z = Y (b +.) and the buffer threshold B =Y b. As Section 
III. 1 shows, the Brownirm component will arise ffom non-adaptive traffic that speeds up in proportion 
to the trunk speed but retains the same burst structure. The threshold y must increase as the amount 
of residual bandwidth available to the adaptive sources increases and as the burstiness parameter c~ 
in (3.2) increases. In Section VI, we compute CI for some examples,, 2. Designing the Adaptive Algorithm 
Equation (5.2) describes the proper scaling of the gain parameter aj in (2.2) with respect to the feedback 
rate ri and the transmission rate p. By (5.2), the choice of the feedback rate ri should determine the 
scale of the gain parameter a ~ in (2.3). A knowledge of the total number n of other active adaptive 
sources also contributes to a good choice of ~i, but if this information is not available, then the choice 
of Ui should depend only on characteristics of source i. By results in Elwalid[4], Mitra and !leery[s], 
and Mukherjee and Strikwerda[g], the relative sizes of the gain parameters used by different adaptive 
sources also determine the proportion of residual bandwidkh that each receives. Therefore, by policing 
adaptive traffic to limit each adaptive source to its fair share of residual bandwidth, a network can 
encourage the use of gain parameters in the adaptive algorithms of sources consistent with (5.2). 3. 
Policing Traffic Theorem 1 shows that the given adaptive controls can tolerate different types of nonadaptive 
traffic in the queue and still keep the queue in the moderate usage regime. As p increases, they can 
tolerate a component of the nonadaptive traffic arriving at a rate of 0(13) as described in Section III. 
1 provided that its bursts retain an O( 1) size. They can tolerate arrival rates of only o@l/2 ) from 
the less constrained contributions of nonadaptive trafjlc @ and J in (3.9); see (5.3)-(5.5). For example, 
the jump process J can contribute bursts of size 0@l 2 ), potentially filling the entire buffer of the 
queue but, according to (5.5), it should send them only at an O( 1) rate. We can view these results in 
terms of a leaky-bucket mechanism used by a network to police traffic; see for example the algorithm 
proposed by the ATM forum[lsl. As p increases over time in a network, the network can support leaky-bucket 
policing mechanisms that will admit traffic at an o(p) rate, but only for bucket sizes of O( 1). It can 
support leaky-bucket mechanisms with bucket sizes that will admit bursts of size o(131/2), but only if 
the mechanism will sustain admission rates of no more than 0(131 2). Such a leaky-bucket mechanism is 
consistent, for example, with (5.5), The bucket will allow a burst of size O@l 2 ), but then will require 
an O( I ) time to refill before it will allow another such burst. Note that the number of non-adaptive 
sources with bucket sizes and rates that both are O@1 2) must not increase with p, because the total 
mean arrival rate from such sources also must not exceed O(p lz). 4. Loss Priorities Theorem 2 shows 
that the given adaptive algorithms can still control throughput loss due to idleness and overflow at 
a queue where nonadaptive traffic can arrive at arbitrary rates when the queue length is less than some 
buffer threshold DSB, so long as the nonadaptive traffic arrives at a rate of o@l 2 J whenever the queue 
length exceeds B. The network can enforce this behavior by dropping cells from nonadaptive sources whenever 
the queue exceeds D, accepting only an 0(13 /2) rate. The low-priority non-adaptive traffic can eat into 
the amount of throughput received by the adaptive sources, however, as (5.12) shows. Nonetheless, intuitively 
we see that, through the choice of D, we can control the amount of bandwidth consumed by low-priority 
non-adaptive sources. VI. Simulations Examples, Burstier Traffic We now return to the simulation experiments 
of Section II and test the accuracy of our results in predicting the scaling of queue performance as 
the burstiness of traffic increases. In contrast to (2.4), we now let B = y b = g r ~C~ (6.1) where 
c~ is defined as in (3.2) and g is a positive constant. This is consistent with (5.1) by (3.2), since 
p is fixed, We continue to use the adaptive and nonadaptive sources described in Section 11.2, 1. Calculating 
the Burstiness Parameter The model for the non-adaptive source in Section 11.2 is a special case of 
the model in Section IV-C of Fendick, Saksena, and Whitt[17] where the sizes of messages, spaces between 
cells of the same message, and spaces between different messages each are independent and identically 
distributed sequences of random variables characterized by their first two moments. Suppose that the 
queue serves p non-adaptive sources. Let mj and mj [c~j + 1I denote the fist two moments of the message 
size, and let Vj ~d vj [C;j + 1] denote the first two moments of the space between cells of the same 
burst. Also let mj and COJdenote the first two moments of the [c;, + 1I idle time between messages, Let 
For the special case of the model of non-adaptive traffic in Section II, p=1, mp = m, c~p = O, mp = p-lo, 
Cfp = 1, where ei=m, vt/(mlvl+o, ) is the proportion of time that source i is busy. Then, by (26) of 
Fendick, Saksena, and whitt[171,  Vp = D-1, and Cfp = o. Then, (6.2) becomes c~ = m(l 9)2. (6.3) wheree 
= m~-l/(m@- +OP- ) = m/(m+m). 2. Examples C and D Examples C and D use the same number of adaptive sources 
(n= 2) with the same propagation delays (tl,l =7,,2 =0.5) and the same parameters for the adaptive algorithms 
(1, = 64, h, = 320, r, = 2.0, ~d ~i = 0.35777) as in Example A, The single nonadaptive source offers 
cells to the queue with the same average rate of 32, but with a longer mean message size m = so cells 
and a correspondingly longer mean idle period UP-l =450.0x~-l =1.40625. For Example C, we scale the buffer 
threshold B as in (6.1) to compensate for the larger sizes of bursts from the nonadaptive source. For 
Example C, e = 0.1 in (6.3), which is the same as for Example A. Therefore, by (6.3), the increase in 
the message size from m =2 in Example A to m= 50 in Example C results in a proportionate increase from 
c~=l.62 in Example A to c~=25xl.62=40.5 in Example C, Using (6,3) and (6.1), with g =g/fi=O.6325 so that 
(6.1) agrees with (2.4) for Example A, we find that we need B .72 cells for Example C, which is&#38; 
times higher than in Example A. Figure 5 shows the queue lengths .(t) from Example C for o<r<goo. Here 
too, tie horizon@] line shows the value of the buffer threshold B. The peak queue lengths increase from 
Example A to Exam le C in proportion to B and thus by a factor of about ? 25. This is consistent with 
(5.1), since (5. 1) implies that the buffer size z =Y (b + C) should increase in proportion to the buffer 
threshold B =Y b. The throughput measured for the queue in Example C is 92.2% of the trunk rate, extremely 
close to the 92.5% observed in Example A. With this scaling of B, the systems in Examples A and C indeed 
operate in qualitatively similar regimes. Example D explores the consequences of not scaling the buffer 
threshold with the burstiness. For Example D, we use the same parameters as in Example C, but use the 
buffer threshold B. 14.4 from Example A. Figure 6 shows the resuhing queue lengths x(r) for 0.stS800. 
The peak queue lengths are smaller than in Example C but still much larger than in Example A. The cost 
of not scaling B shows up in the throughput for the queue, which now is only 81.1 % of the trunk rate. 
This loss in throughput from Example C to Example D reflects a decreased effectiveness in the feedback 
generated by the network for the adaptive sources. Without scaling B, the system in Examples D operates 
in a qualitatively different regime than the systems in Examples A and C. VII. Conclusions Our results 
describe the asymptotic performance of adaptive schemes with delayed feedback as network rates become 
large, which is precisely the regime of greatest concern to many users of data networks when trying to 
decide on the proper response to congestion notification from the network. This paper brings good news 
to these users that should raise their expectations about the potential effectiveness of responding to 
congestion notification, It identifies roles for the network and for adaptive sources to play to use 
bandwidth efficiently without producing unacceptable loss of data. Specifically, it shows that if the 
network does a proper job of controlling the level of burstiness from nonadaptive sources, sizing buffers, 
and providing feedback information, then all adaptive sources need to do is to scale their adaptive scheme 
appropriately. The analytical results hold for a large class of adaptive algorithms. The results hold 
even if the available resources are shared with less controlled traffic, as long as it is treated with 
lower priority than the adaptive traffic. Acknowledgements We thank Judy Seery for sharing simulation 
code with us that we used for the examples in Sections II and VI. REFERENCES <RefA>1. K. W. Fendick, D. Mitra, 
I. Mitrani, M. A. Rodrigues, J. B. Seery and A. Weiss, An approach to high performance, high speed data 
networks, IEEE Communications Magazine, pp. 74-82, October 1991. 2. K. W. Fendick, M. A. Rodrigues, 
and A. Weiss, Analysis of a rate-based feedback control strategy for long haul data transport, to appear 
in Performance Evaluation. 3. K. W, Fendick and M. A. Rodrigues, Asymptotic analysis of adaptive rate 
control for diverse sources with delayed feedback, submitted for publications.  4. A, Elwalid, Adaptive 
rate-based congestion control for high-speed wide area networks: stability and optimal design, preprint. 
5. D. Mitra, Asymptotically optimal design of congestion control for high speed data netwclrks, IEEE 
Trans. Commun., 40, No. 2, pp. 301-311, Feb. 1992. 6, D. Mitra and J.B Seery, Dynamic adaptive windows 
for high speed &#38;ta networks: theory and simulations, Proc, ACM SIGCOMM, pp. 30-40, 1990. 7. D. Mitra 
and J. B, Seery, Dynamic adaptive windows for high speed data networks with multiple paths and propagation 
delays, INFOCOM91. 8. D, Mitra and J. B, Seery, New adaptive algorithms for windows and rates in high 
speed, wide area networks based on periodic, 1-bit, explicit feedback, preprint, 9. A. Mukherjee and 
J. C. Strikwerda, Analysis of dynamic congestion control protocols-a Foldcer-Pkmk approximation, ACM 
SIGCOMM, Zurich, 1991  10. M. Csorgo and P. Rdvcfsz, Strong Approximations in Probability and Statistics, 
Academic Press, New York, 1981,  11, Doshi, B. T, and Jagerman, D. L. (1986) An M/G/l queue with class 
dependent balking (reneging), Teletrafic Analysis and Computer Perj+orrnance Evaluation, O. J. Boxma, 
J. W. Cohen, H. C. Tijms (editors), Elsevier. 12. K. W. Fendick and E, Hemandez-Valencia, A Brownian 
flow model for a cell-relay switch with shared buffers and loss priorities, submitted for publication, 
 13. J. M. Harrison, Brownian Motion and Stochastic Flow Systems, Wiley, 1985. 14. H. J. Kushner and 
P. G. Dupuis, Numerical Methoak for Stochastic Control Problems in Continuous Time, Springer-Verlag, 
1992. 15. D. Mitra, Asymptotically optimal design of congestion control for high speed data networks, 
lEEE Trans. Commun,, 40, No. 2, pp. 301-311, Feb. 1992. 16. UNI Specification Technical Working Group, 
Traffic Management Baseline Text, ATM Forum/92-239R5, December 14, 1992. 17. K. W. Fendick, V. R. Saksena, 
and W. Whitt Dependence in packet queues, IEEE Trans. Comm., Vol. 37, No. 11, 1173-1183, 1989.</RefA>  L 0 
Figure 200 4W Unw 3. The Queue-Length Example A. - @xl Sca Process for Figure I + -o 200 5. The Queue-Length 
Example C. 4CU bra m Process II for -# ml I I 0 w lW tulle Figure 4. The Queue-Length Example B. 15J 
process 4 m for Figure 1 [ 0 m 400 mm 6. The Queue-Length Example D. m Process m for 136  
			
