
 Some text in this electronic article is rendered in Type 3 or bitmapped fonts, and may display poorly 
on screen in Adobe Acrobat v. 4.0 and later. However, printouts of this file are unaffected by this problem. 
We recommend that you print the file for best legibility. AParallelLinearAlgebraServerforMatlab-likeEnvironments 
GregMorrow and RobertvandeGeijn DepartmentofComputerSciences TheUniversityofTexasatAustin Austin,Texas78712 
fmorrow,rvdgg@cs.utexas.edu AnExtendedAbstractSubmittedtoSC98 1Introduction MathematicalsoftwarepackagessuchasMathematica,Matlab,HiQandothersallowscientistsandengineers 
toperformcomplexanalysisandmodelingintheirfamiliarworkstationenvironment.However,thesesystems arelimitedinthesizeofproblemthatcanbesolved,becausethememoryandCPUpoweroftheworkstation 
arelimited.Obviously,thebeneftoftheinteractivesoftwareislostiftheproblemtakestwoweeksto run.Withtheadventofinexpensive\beowulf"-typeparallelmachines[4],andtheproliferationofparallel 
computersingeneral,itisnaturaltowonderaboutcombiningtheuser-friendlinessandinteractivityofthe commercially-availablemathematicalpackageswiththecomputingpoweroftheparallelmachines. 
Wehaveimplementedasystem,whicwecallPLAPACKServerInterface(\PSI"),allowsauser hthat ofoneofthesupportedmathematicalpackagestoexportcomputationallyintenseproblemstoaparallel 
computerrunningPLAPACK.Theinterfaceconsistsofasetoffunctionsthattheusercallsfromwithinthe mathematicalpackage.Thesefunctionsallowcreating,flling,querying,manipulating,andfreeingmatrix, 
vector,andscalarobjectsontheparallelcomputer.BothmemoryandCPUpowerscalelinearlywiththe numberofprocessingelementsintheparallelmachine.Thus,PSIallowstheinteractivesoftwarekages 
pactobreakthebondsoftheworkstationtosolveeverlargerandmorecomplexproblems. PSIisnotthefrstattempttoexploitparallelismfromwithininteractivemathematicalsoftwarepackages. 
MultiMatlab[6](fromtheCornellTheoryCenter)andtheMATLABtoolbox[5](fromUniversityofRostock, Germany)areextensionsoftheMatlabintrepreterthatessentiallyrunoneachnodeoftheparallelmachine. 
AsimilarproductforMathematica[10]isavailablefromMathconsultinSwitzerland.Compiler-based systemssuchasFALCON[9](fromUniversityofIllinois),Otter[8](fromOregonStateUniversity)and 
CONLAB[7](fromUniversityofUmea,Sweden)startwithMatlabscriptflesandusecompilertechnology tocreateexplicitmessage-passingcodes,whichthenexecuteessentiallyindependentlyofthescript'soriginal 
interactivesoftwareplatform.Thislistisnotexhaustive,butshouldgivetheideathattherearemany approachestothisproblem.OurapproachismostsimilartotheMultiMATLABandMATLABToolbox 
approaches,withoneimportantdiference.InPSI,thethird-partysoftware(MATLAB,Mathematica,etc.) runsonaworkstation,whiletheparallelcomputationisperformedonacompletelyseparatemassively 
1 parallelmachine.AllparallelcommunicationishandledfromwithinPLAPACK. Thispaperisorganizedasfollows.Section2givesabriefoverviewofPLAPACKandoftheinteractive 
mathematicalpackages.Section3discussessomeimplementationdetailsofPSI.Section4showswhatPSI 1PMIalsorunson\beowulf"networksofworkstations,inwhichcasethethird-partysoftwarerunsononenode,and 
PLAPACKrunsontheremainingnodes. Permission to make digital/hard copy of all or part of this work for 
personal or classroom use is granted without fee provided that copies are not made or distributed for 
profit or commercial advantage, the copyright notice, the title of the publication and its date appear, 
and notice is given that copying is by permission of ACM, Inc. To copy otherwise, to republish, to post 
on servers or to redistribute to lists, requires prior specific permission and/or a fee. 1 SC 98, Orlando, 
FL (c) 1998 IEEE 0-89792-984-X/98/0011 $3.50 lookslikefromauser'spointofview.Section5detailssomemeasurementsofPSI'sperformanceonaparallel 
system.Finally,section6givessomeconcludingremarks. 2OverviewofPLAPACKandtheinteractivepackages ThissectiongivesabriefdescriptionofPLAPACKandoftheinteractivepackagesthatPSIconnectsto 
PLAPACK. 2.1PLAPACK PLAPACK(ParallelLinearAlgebraPackage)isanobject-orientedsystemfordoingdenselinearalgebraon 
parallelcomputers[3,1,2].ItiswritteninC,andusestheMessagePassingInterface(MPI)forcommunica­tion.Itisdistinguishedbythefactthattheprogrammerisnotexposedtoerror-proneindexcomputations. 
Instead,theconceptofa\view"intoamatrixorvectorisusedtoallowforindex-freeprogramming,even ofhighlycomplexalgorithms.PLAPACK'shigh-levelabstractionanduser-friendlinessdonotcomeatthe 
expenseofhighperformance.ThePLAPACKcomplexLUfactorizationalgorithm,forexample,achieves over390MFLOPSperPEon16PE'softheCrayT3E-600(300MHz). 
 2.2X-lab InteractivemathematicspackagessuchasMatlab(fromtheMathworks,www.mathworks.com),Mathematica 
(fromWolframResearch,www.wolfram.com),HiQ(fromNationalInstruments,www.natinst.com),and othersallowtheirusersaccesstosophisticatedmathematicsinaninteractive,workstationenvironment. 
Thepackagestypicallyincludefunctionalityforlinearalgebra,curve-ftting,diferentialequations,signal processingandsophisticatedgraphics,aswellasmanyotherareasoffunctionality. 
BecausePSIcanconnectwithanyoftheseproducts,andinanattempttobeeven-handed,throughout thetextwewillrefertotheinteractivepackageas\X-lab."Thisisintendedtorefertoanyoftheabove 
products. 3Implementation ThissectionbriefydescribestheimplementationofPSI.Webeginbydiscussingthebasicmechanismof 
communicationinPSI.Thenwedescribethe\third-party"partoftheprogram,i.e.thepartofthePSI softwareassociatedwithaparticularplatform(Matlab,Mathematica,etc.).Next,wedetailthePLAPACK 
sideoftheinterface.Finally,wegivesomeremarksaboutsoftwarelayeringinPSI. 3.1Overviewofimplementation 
 Figure1showsadiagramofthePSIimplementation.Thethird-partysoftware(\X-lab")communicatesvia itsinherentinterfacetoasetofCroutines.TheseroutinesthencommunicateviaTCPwithaPLAPACK 
server.Theservereitherrunsonothernodesofthesamemachineoronacompletelyseparateparallel machine.ThePLAPACKserverthenusesthePLAPACKapplicationinterfaceandMPIroutinestocom­municatewiththeotherPLAPACKnodes.Forsendingresultsbacktothethirdpartysoftware,theprocess 
isreversed. 3.2The\X-lab"side Eachpieceofthird-partysoftwarethatisacandidateforaPLAPACKinterfacemustbeabletocalluser­suppliedCcode,andmustalsobeabletopassdatabackandforthtothatCcode.(Inpractice,ifuserC 
codeiscallablewithinapackage,thenthereisalwaysawaytopassdatabackandforth.) X-lab PMI  PLAPACK PLAPACK 
PLAPACK PLAPACK PMI   TCP/IP workstation parallel computer Figure1:DiagramofthePSImachinelayout. Giventhatthethird-partyprogrampossessesthefeaturesdescribedabove,thefollowingoutlineshows 
howitprocessesPSIcommands. 1.TheusercallsaPSIfunctionfromwithinX-lab 2.X-labinvokesacalltothePSIcode,passingwhateverparametersarenecessaryforthisparticular 
function 3.ThePSIcodewritesaheadertothecommandbufer,followedbydataitemstobetransferred,where applicable 
4.ThePSIcodesendsthecommandtothePLAPACKserver 5.ThePSIcodewaitsforthereturnmessagefromthePLAPACKserver 
6.ThePSIcodeperformsanypost-processingrequired,andeitherreturnstherelevantdatatoX-lab, or,ifnoreturndataisrequired,returnsanintegerwhosevaluesignifesthesuccessorfailureofthe 
requestedoperation 3.3PLAPACKside ThePLAPACKsideofPSIisaparallelapplicationthatplaystheroleofacomputeserver:itsetsupthe 
PLAPACKenvironment,thenwaitsforcommandsfromtheclient(whichisthethird-partysideofPSI.) Withintheparallelapplication,oneprocessingelement(PE)playsaspecialrole,calledthemasterPE,and 
therestofthePE'sareslaves.ThefollowingisanoutlineofhowthePLAPACKsideprocessescommands. 1.ThemasterPEreadsacommandfromtheTCPsocketintoalocalcommandbufer 
2.ThemasterPEreadsthecommandinformationfromthebufer'sheaderarea,andbroadcastsany relevantinformationtotheslavePE's 
3.Inthecaseofarequesttoplacedataintoaparallelobjectortoretrievedatafromaparallelobject, themasterPEusesthePLAPACKapplicationinterfacetotransferthedatatoorfromtheintended 
PE's 4.ThePLAPACKPE'sperformwhateverparallelcomputationisrequiredbythecommand 5.ThemasterPEwritesanyreturninformation,errormessages,andanerrorreturncodetotheheader 
ofthecommandbufer 6.Inthecaseswherenumericaldata(e.g.matrixelements)needtobetrasferredbacktothethird-party 
software,thedataarewrittenbythemasterPEintothedataareaofthecommandbufer 7.Thecommandissentbacktotheclient 
 3.4Softwarelayering PSIisintendedtobeageneral-purposeinterface.Thatis,itshouldbeabletoplugintoavarietyofthird­partypackages,andshouldbeabletorunonavarietyofparallelmachines.Portabilityoftheparallel 
programiseasy:sincePLAPACKitselfishighlyportable(requiringonlyCandMPI),thispartofPSI's portabilityisautomatic. 
Becauseofthequirksandidiosyncrasiesofthethird-partyinterfacespecifcations,therearecertain functionsthatmustbereimplementedforeachintendedthirdpartyplatform.Wehaveattemptedtolayer 
oursoftwareinsuchawaythatthesenon-portablepartsofthecodeareisolatedandsmall. Figure2showsadiagramofthePSIlayering.Thereisasetof\core"functionsthatcontaintheworkings 
ofPSI.Foreachclientplatform,thereisathinwrappertothesefunctionsthattranslatesdatafromthe client'sformattoPSI'sformat.Thislayeralsotakescareofsettingupanyconstantsthatneedtobedeclared 
withintheinteractiveenvironment,andisresponsibleforimplementinga\printf"-likefunctiontoreport errorsandwarnings. 
InadditiontothePSIcore,thereareasetofcommunicationsprimitivesthatareusedbyboththe clientandtheserver.Thesecommunicationsfunctionsareessentiallyplatform-independent,requiringonly 
aBerkeleySocketsimplementation. Figure2:SoftwarelayeringinPSI. 4UsingPSI ThissectiondescribesPSIfromauser'spointofview.Whatdoesshedotoinitiateasession'Whatdoes 
sheseefromwithinherthirdpartymathematicalsoftware'Whatdoesasampleapplicationlooklike'  4.1Theparallelapplication 
ThePLAPACKsideofPSIisjustlikeanyotherparallelMPIexecutable.Hence,whateverstepsarenecessary tolaunchaparallelprogramonyourcomputermustbefollowed.Weassumeherethatthe\mpirun"facility 
isavailablefortheparallelmachine. Letussupposethatwewishtohave16PE'sinvolvedintheparallelsideofPSI.Thenwewouldlaunch 
thePSIplapack.xexecutable,andspecify16processorsonthecommandline.Thiswouldlooksomething likethefollowing 
%mpirun-np16PSI_plapack.x PLAPACKsoftwareinterfacewaitingforconnection PLAPACK/Matlabinterfacefromsimoom.cs.utexas.eduona4by4mesh 
Oncetheparallelexecutableisrunning,theuserisfreetostartPSIfromwithinhermathematical software.(Actually,theorderinwhichthetwosidesofPSIareinitiatedisimmaterial.However,aPSIopen[] 
callfromwithinX-labwillblockuntiltheparallelexecutableisstarted.) 4.2Withinthemath-environment Fromwithinathirdpartymathematicalpackage(which,forconcreteness,wewillagainrefertoasX-lab), 
PSIisaccessedthroughasetofcommands,allprefacedwiththeletters\PSI."Thesecommandsfallinto threebasiccategories. 
Thefrstcategroryconsistsofcommandstoinitialize,fnalize,andmanipulatetheenvironment.Examples ofcommandsinthiscategoryarePSIOpen[],PSIClose[],andPSIVerbose[]2. 
ThesecondclassofcommandsinPSIperformparallelobjectmanipulations.Thepurposeofthese commandsistocreate,free,query,andfllparallelmatrices,multivectors,andmultiscalars.Examplesof 
commandsinthiscategoryarePSICreateObject[],PSIFreeObject[],PSIAxpyToObject[],PSIAxpyFromOb­ject[].Thelattertwofunctionsareusedtoputvaluesintoaparallelobjectandtogetvaluesfromaparallel 
object,respectively. ThethirdclassofcommandsinPSIcausesomeactiontobetakenonparallelobjectsthatalreadyexist 
withinthePSIparallelapplication(i.e.objectsthathavealreadybeencreatedandflledwithdatavalues.) ExamplesofthesecommandsarePSILU[]andPSIGemm[],whichperformLUfactorizationandmatrix 
multiplication,respectively. 4.3Asampleapplication Thissectionpresentsasampleapplication.ThisprogramwouldbeexecutedfromwithinaMatlabsession, 
andofcoursewouldrequireacopyofthePSIparallelapplicationtoberunningaswell.Thisexample performsthefollowingsteps. 
1.OpenthePSIinterface 2.Createparallelobjects:amatrix,amultivector,andamultiscalar 3.Fillthematrixandvectorwithdatavalues 
4.Performagenerallinearsolve(LUfactorizationofthematrix,triangularsolveswiththevectoras righthandside,LUpivotsstoredinthemultiscalar) 
5.Retrievethedatafromthevector 6.ClosethePSIinterface ItisimportanttonotethatthecallstotheMatlab\rand()"functionwouldinarealcodebereplaced 
byameaningfulroutinethatcomputesasubmatrixofthematrixinquestion,andreturnsitasaMatlab matrix.Forexample,inaboundaryelementcode,onecoulduseaMatlabM-flescripttocomputethe 
interactionmatrixbetweentwoelements.Loopingoverpairsofelements,eachcontributionisaddedtothe globalmatrixasitisgenerated.Thusthetaskofmatrixgeneration(whereusersarelikelytowanttouse 
Matlab-centrictechnology)staysinsideMatlab,whilethesimplenumber-crunchingoffactoringthematrix isshippedoftotheparallelmachine. 
Figure3showstheaboveprogramfromwithintheMatlabversionofPSI. 5Performance ThissectiongivessomeperformancemeasurementsforthePSIsystem.Weshowresultsfortwoplatforms. 
ThefrstsystemiscomposedofanSGIworkstationconnectedtoNPACI'sUniversityofTexasT3E.The lattermachineisaCrayT3E-600,whichconsistsof56300MHzprocesssingelements,eachwith128Mbof 
2Theexactsemanticsofthesecallsisplatform-dependent.ThecallsasshowninthetextareMathematica-styleSeeFigure 
3forthesemanticsoftheMatlab-stylecommands. matrixSize=1000; axpySize=100; PSI('open','t3e-utexas.npaci.edu'); 
% %Createtheparallelobjects % A=PSI('create',PSImatrix,PSIdouble,matrixSize,matrixSize); x=PSI('create',PSImvector,PSIdouble,matrixSize,1); 
y=PSI('create',PSImvector,PSIdouble,matrixSize,1); alpha=PSI('create',PSImscalar,PSIdouble,1,1); beta=PSI('create',PSImscalar,PSIdouble,1,1); 
piv=PSI('create',PSImscalar,PSIint,1,matrixSize); % %Problemset-up:FillAandxwithvalues,thenlety=Ax. %Also,setupscalarsalphaandbeta. 
% fori=1:axpySize:matrixSize-1, forj=1:axpySize:matrixSize-1, aTmp=rand(axpySize,axpySize); PSI('addto',A,axpySize,axpySize,i-1,j-1,aTmp); 
 end end fori=1:axpySize:matrixSize, xTmp=rand(axpySize,1); PSI('addto',x,axpySize,1,i-1,0,xTmp); end 
alphaIn=1.0; PSI('addto',alpha,1,1,0,0,alphaIn); betaIn=0.0; PSI('addto',beta,1,1,0,0,betaIn); % %Performamatrix/vectormultiplicationtosetyproperly 
% trans='N'; PSI('gemv',trans,alpha,A,x,beta,y); % %dothesolve % PSI('gesv',A,piv,y); % %retrievethevectorsandfindthenormofthedifference 
% norm(PSI('getfrom',y,matrixSize,1,0,0)-PSI('getfrom',x,matrixSize,1,0,0)) PSI('close'); Figure3:APSIprogram 
7 machine latency(sec) bandwidth(Mbyte/sec) beowulf 0.009 2.7 SGIwithT3E 0.03 0.5 Table1:LatencyandbandwidthofthePSIinterface 
memory.ThesecondsystemisaPentiumII-based\beowulf"systemthatconsistsof16300MHzPentium IIworkstations,eachwith256Mbofmemory,connectedwitha100Mbitethernetnetwork. 
Weconcentrateonpropertiesoftheinterfaceitself,ratherthanonthepropertiesoftheparallelexe­cutable.(TheparallelexecutableissimplyaPLAPACKprogramindisguise,andperformancenumbersfor 
PLAPACKareavailableintheliteratureandfromthePLAPACKwebpage.)Wedo,however,showsome speedupvaluestogetanideaofoverheadsinherentintheinterface. 
ThemainperformancemetricsforPSIconcernthespeedofthecommunicationconnection.Inparticular, wemeasurethelatencyoftheconnection(thetimerequiredtogetazero-lengthmessagebackandforthto 
theparallelexecutable)andtheinversebandwidth(thetimeperbyteofdatasenttoorreceivedfromthe parallelexecutable.)Inadditiontothesemeasurements,wealsoprovideaprofleofthesampleapplication 
describedintheprevioussection.  5.1Latency ThelatencyofthePSITCPconnectionismeasuredbytimingaround-tripmessage,wherethecommand 
involvedrequiresnoprocessingontheparallelside.Table1showssomemeasuredlatencies.Thelatencies areclearlysmallenoughtonotprecludeinteractivity. 
 5.2Bandwidth ThebandwidthofthePSIconnectionismeasuredbytimingtheactionofputtingdataintoaparallelobject, 
andbytimingtheactionofretrievingdatafromaparallelobject.Thespeedoftheseoperationsisdependent uponthespecifcsoftheparallelmesh,andthespecifcsofthenetworkconnectionbetweentheworkstation 
andtheparallelmachine.WereporttypicalvaluesforthemeasuredbandwidthinTable1. Noticethatthebeowulfconnectionisseveraltimesfasterthentheworkstationtot3econnection.This 
illustratesthebeneftofrunningtheinteractivesoftwareononeofthenodesoftheparallelmachine.The bandwidthoftheworkstationtot3econnectionistypicaloftheparticularnetworkweused.Forexample,an 
ftpconnectionbetweenthesamemachinesrunsataround0.7Mbyte/sec,comparedtothePSIbandwidth of0.5Mbyte/sec. 
 5.3Performanceofthesampleapplication Thissectionpresentsdataconcerningtheperformanceofthesampleapplicationdescribedintheprevious 
sectionofthispaper.First,inFigure4,weshowaprofleoftheapplication(generatedfromwithin Matlab,theenvironmentwhereweranthePSIprogram)Thisplotshowsabargraph,withthevertical 
axisrepresentingtimeinseconds,andthehorizontalaxisrepresentingmatrixsize.Thesenumberswere generatedwithafourprocessorconfguration.Second,inFigures5and6wepresentperformancenumbers 
foraparticularkernel(LUfactorizationwithrowpivoting,followedbyforwardandbackwardsubstitution). Thehorizontalaxisisthematrixsize,andtheverticalaxisistotalMFLOPS(millionsoffoatingpoint 
operationspersecond).Figure5showstheperformanceofbareMatlabrunningononenodeofthebeowulf forcomparison. 
Itisimportanttonotethattheperformancenumberspresenteddonotincludethetimetofllthematrices. Forlargeproblemsandwitharelativelyslownetwork,thecommunicationofthematrixelementsfromthe 
X-labworkstationtotheparallelmachinecandominatetheexecutiontime.Thebeowulfperformswellin Figure4:ProfleofaPSIapplication.Verticalaxisistimeinseconds. 
thissense,becauseitsdedicatednetworkhasroughly5timesthebandwidthofourcampusnetwork.This problemwiththeworktationtoMPPconnectionwouldbemitigatedbysimplyincreasingthebandwidthof 
thenetworkconnection. 6Conclusionsandfuturework ThispaperhasdescribedthePLAPACKServerInterface,whichisasoftwareconnectionallowingauserto 
plugaparallelcomputerintothebackoftheirfavoriteinteractivemathematicalsoftware.Wehavegiven somedetailsoftheimplementation,useandperformanceofPSI.Asyet,wehaveonlyrealizedasmallsubset 
ofwhatcanbedonewiththisinterface.First,theinterfacecanbeextendedtosupportothersoftware packages,subjectonlytotheconstraintsrefereredtointheimplementationsectionofthispaper.Second, 
weintendtoincorporatemoreoftheuniquefeaturesofPLAPACK(forexample,theuseof\views"into matricesandvectors)intoPMI.Finally,weareinterestedinexperimentingwith\realapplications."That 
is,wewishtotakeanexistingMatlaborMathematicaapplicationcodeandparallelizeitthroughPSI. Pleasesendanyinquiriestoplapack@cs.utexas.edu,orseeourwebsiteat 
www.cs.utexas.edu/users/plapack. total MFLOPS beowulf LU performance 1500.0 1000.0 500.0 0.0 Figure5:TotalMFLOPSoftheLUkernel:singlenodeMatlabvs.MatlabwithPSIon16processors 
 T3E LU performance 6000.0 5000.0 4000.0 3000.0 2000.0 1000.0 0.0 total MFLOPS Figure6:TotalMFLOPSoftheLUkernel:PSIbetweenandSGIworkstationand16processorsofaT3E. 
 Acknowledgements ThisworkwassponsoredinpartbytheIntelResearchCouncil.ThePLAPACKprojectwassponsored inpartbytheParallelResearchonInvariantSubspaceMethods(PRISM)project(ARPAgrantP-95006), 
theNASAHighPerformanceComputingandCommunicationsProgram'sEarthandSpaceSciencesProject (NRAGrantsNAG5-2497andNAG5-2511),andtheEnvironmentalMolecularSciencesconstructionproject 
atPacifcNorthwestNationalLaboratory(PNNL)(PNNLisamultiprogramnationallaboratoryoperated byBattelleMemorialInstitutefortheU.S.DepartmentofEnergyunderContractDE-AC06-76RLO1830). 
WewouldliketoacknowledgetheuseofcomputerfacilitiesattheTexasInstituteforComputationaland AppliedMathematicsintheUniversityofTexasatAustin,andalsoattheUniversityofTexas'sT3Ethrough 
theNationalPartnershipforAdvancedComputationalInfrastructure. References <RefA>[1]P.Alpatov,G.Baker,C.Edwards,J.Gunnels,G.Morrow,J.Overfelt,R.vandeGeijn,Y.-J.J.Wu, 
"PLAPACK:ParallelLinearAlgebraPackage,"inProceedingsoftheSIAMParallelProcessing Conference,1997. [2]P.Alpatov,G.Baker,C.Edwards,J.Gunnels,G.Morrow,J.Overfelt,R.vandeGeijn,Y.-J.J.Wu, 
"PLAPACK:ParallelLinearAlgebraPackageDesignOverview,"inProceedingsofSC97,1997. [3]RobertvandeGeijn,UsingPLAPACK:ParallelLinearAlgebraPackage,TheMITPress,1997. 
[4]D.J.Becker,T.Sterling,D.Savarese,J.E.Dorband,U.A.Ranawake,andC.V.Packer.BEOWULF: Aparallelworkstationforscientifccomputation.InProceedingsofthe1995InternationalCon­ferenceonParallelProcessing(ICPP),pages11-14,1995. 
[5]Pawletta,S.,Drewelow,W.,Duenow,P.,Pawlette,T.,andSuesse,M.\AMATLABtoolboxfor DistributedandParallelProcessing,"inProceedingsoftheMATLABConference95,Cambridge, 
MA(1995). [6]A.E.Trefethen,V.S.Menon,C.C.Chang,G.J.Czajkowski,C.Myers,L.N.Trefethen,\MultiMAT­LAB:MATLABonmultipleprocessors,"TechnicalReport96-239,CornellTheoryCenter,Ithaca,NY 
(1996). [7]P.Drakenberg,P.Jacobson,B.Kagstrom,\ACONLABcompilerforaDistributedMemoryMulticom­puter,"inProceedingsoftheSixthSIAMConferenceonParallelProcessingforScienti.c 
Computation,Volume2(1993),pp.814-821. [8]M.J.Quinn,A.Malishevsky,N.Seelam,Y.Zhao,\PreliminaryresultsfromaparallelMATLAB 
compiler,"inProceedingsoftheIEEEInternationalParallelProcessingSymposium(1998), pp.81-87. [9]L.DeRoseandD.Padua,\AMATLABtoFortran90Translatoranditsefectiveness,"inProceedings 
ofthe10thACMInternationalConferenceonSupercomputing(May1996). [10]R.A.Maeder,\DemonstrationprogramsfromkeynotelecturesgivenbyR.MaederatIMS'97(Second 
InternationalMathematicaSymposium),Rovaniemi,Finland,June29-July4,1997,"availableat http://www.mathconsult.ch/math/stuff/ims.html. </RefA>
 
			
