
 50 GFlops Molecular Dynamics on the Connection Machine 5 Peter S. Lomdahl, Pablo Tamayo, Niels Gr@nbech-Jensen, 
and David M. Beazley Theoretical Division and Advanced Computing Laboratory Los Alamos National Laboratory, 
Los Alamos, NM 87545 Abstract We present timings and performance numbers for a new short range three 
dimensional (3D) molecul­ar dynamics (MD) code, SPaSM, on the Connection Machine-5 (CM-5). We demonstrate 
that rurM with more than 108 particles are now possible on massively parallel MIMD computers. To the 
best of our knowl­edge this ti at least an opder of magnitade more par­ticles than what has previously 
been reported. Typical production runs show sustained performance (includ­ing communication) in the range 
of 47-50 GFlops on a 1024 node CM-5 with vector units (VUS). The speed of the code scales linearly with 
the number of proces­sors and with the number of particles and shows 95% parallel eficiency in the speedup. 
1 Introduction The use of molecular dynamics (MD) [1] to study dynamical properties of solids and liquids 
has been known for decades, but it is only the recent prolif­eration of powerful massively parallel computers 
that begins to makes detailed studies of realktically sized systems possible. A cube of material 1000 
atoms on the side measures roughly 0.5pm x 0.5pm x 0.5pm ­while this may seem like a very small piece 
of mate­rial -it contains 109 atoms. Solving Newton s equa­ tions for a billion interacting atoms still 
represents a formidable problem for MD. However, realistic calcu­lations in materials science require 
system sizes in this range if the dynamics of defects like dislocations and grain-boundaries is to be 
studied. An additional prob­lem is presented by the short time scale that is ac­cessible by the MD method, 
which is typically tens or maybe hundred of nano-sec. at best. Ideally one would like to use the MD method 
for second long sim­ulations with at least 109 atoms. While this goal is still very far away, there is 
substantial current interest in the development of fast MD algorithms[2, 3,4,5, 6, 7] which allow for 
the simulation of at least million atom systems. We have developed an new scalable MD algo­rithm based 
on a message-passing multi-cell approach which allows for simulating at least 108 particles in­teracting 
via a relative short range potential. We have implemented the algorithm in a code, SPaSM (scalable ~mallel 
short-range Molecular dynamics), on the Connection Machine 5 (CM-5) and demon­strated that simulations 
with tens of millions of atoms can now be performed routinely. In addition, it is clear that simulations 
with 10s particles are now possible at a sustained rate of 50 GFlops. To our knowledge the performance 
numbers are the best reported to date for any MD simulation and may well be the highest for any 3D production 
code implementing a substantial amount of unstructured communication. In prelimi­nary 2D studies, we 
simulated the fracture dynamics of a piece of material with 2 million atoms that is being pulled apart 
in a tensile experiment. We are currently using SPaSM to study dynamic fracture physics with millions 
of atoms in 3D. 2 MD Simulations The MD method concerns the solution of Newton s equation of motion for 
IV interacting particles. This general N-body problem involves the calculation of IV(N 1)/2 pair interactions 
in order to compute the total force on any given particle: here ri indicates the instantaneous position 
and ~ the msss of particle i. The complexity of the force calculation is simplified considerably if the 
potential Vij (r) has a finite range of interaction. This is a rea­sonable approximation of the atomistic 
interactions in many solids and fluids. In our timings here we have Permission 10 copy without fee aff 
or part of this materiaf B granted, provided that the copies are not made or dis!nfxued fm direct zcial 
 advantage, the ACM copyright notice and the titfe of the pubfkation and its date appear, and notice 
is gwen that ccpying is by fxmnission d the 520 Association for Computing Machinery. To copy cUmr wise, 
or to repubfish, @ 1993 ACM 0-8186-4340-4/93/0011 $1.50 requires a fee andior speL~lc permission. used 
the Lennard-Jones 6-12 (LJ) potential Here a and c are the usual LJ parameters. The poten­tial is cut-off 
at r~~=, i. e. no particles interact beyond this range. We include here timings and performance numbers 
for two values of ~maa. We stress that while more complicated and accurate potentials which in­clude 
many-body effects are available, the amount of work needed in the force calculation is represented well 
by the LJ potential especially when the cut-off is rma= = 50. The number of interacting neighbors for 
each particle depends on the value of the cut-off distance Tma= and the particle density p. Our code 
is written in ANSI C with explicit calls to the CMMD message-passing library. The kernel of the force 
calculation is coded in the CM-5 vector unit (VU) assembler language, CDPEAC, and consists of approximately 
60 lines of code. All our calculations were performed in doable precision. Multi-cell algorithm Our 
algorithm has been described in detail in[7]. Here we briefly outline its main features, illustrating 
the algorithm in 2D, but it extends naturally to 3D. Space is considered to be a rectangular region with 
periodic boundary conditions. Each Processin g Node 16 Node CM-5:!, ........................ ;: I ::: 
I :: V-I > Tin== j------i--i--4 ------~---j---{--­ ;----%3-!--­ .... ................+..+.-w-{---..._. 
 ......................_ 8::: 9::: 10:: 11:: ::: ::: ::: ::: l...:.. J...:...l...:.. L.. J... I...:..:...:...l...:...!...J...I 
Em ---?a %3-?-­ %3-: - -Figure 1. Processor layout and force calculation. This region is subdivided 
into large cells that are assigned to the processing nodes (PNs) on the CM-5. The region assigned to 
each PN is further subdivided Figure 2. 3D interaction path into small cells with dimensions slightly 
larger than the cutoff distance. Particles are assigned to a par­ticular cell geometrically according 
to the particle s coordinates. In Fig. 1, solid lines represent proces­sor boundaries while dashed lines 
represent the cells created on each PN. For large simulations, many thou­sands of cells per PN may be 
created (this does not explicitly depend on the number of PNs being used). Associated with each cell 
is a small block of memory for storing a sequential list of particles. To compute the forces for particles 
in a cell, we first compute all of the interactions between particles in that cell. Af­terwards, forces 
between particles in neighboring cells are calculated by following an interaction path that visits neighboring 
cells. The path in 2D is shown in Fig. 1 and in 3D in Fig. 2. As we follow the path, accelerations are 
accumulated by the original cell and any visited cells (using Newton s third law). To calcu­late all 
of the forces, this procedure is carried out on all cells on all of the PNs. Cells will accumulate accel­erations 
from their lower neighbors when they calcu­late their interactions. Whenever the interaction path crosses 
a processor boundary, message passing is used to communicate particle data. After all forces have been 
calculated, the particle positions are updated. Since our algorithm is geometrically based, all of the 
data structures must be updated to account for posi­ tional changes. The particle coordinates are checked 
and if a particle is in the wrong cell it is moved to the proper cell. If the new cell is on a different 
PN, asyn­ chronous message passing is used to send the particle to its new PN. Each PN checks for incoming 
parti­ cles and places them in the proper cell when received. Since a large number of cells may be created 
on each PN (even for moderately sized systems) hundreds or even thousands of message-passing calls may 
be re­ Parallel Memory SPARC (Holds many cells) 1 W Cell 2 m CCU2 u 8 particles from Cell 2 loaded 
into Vector Registers Calculate Accelerations 11 L.Q..-l L-Q.--l  t!F=13+13 SPARC Parallel Memory 
Memory (Holds accel. from many cells) Figure 3. Calculating forces on the VUS. quired for each time 
step. The amount of commu-trolling an 8 Mbyte bank of memory. The 32 Mbytes nication and calculation 
for 3D is substantial due to of memory on each PN can be accessed by both the an increased number of 
neighboring cells and a more SPARC processor and the VUS, but the memory is dl­ complicated interaction 
path. vialed into two separate areas for this purpose. SPARC memory is memory that has been allocated 
for use by the SPARC processor. All usual SPARC opera­ 4 Using The Vector Units and Parallel tions perform 
normally in thki area. Parallel memory is a special memory allocation that allows the four Memory VUS 
to perform simultaneous load/store operations. On each processing node, the CM-5 has four vec-Each VU 
allocates an identically structured memory tor units (VUS) that perform fast vector arithmetic region 
in the 8 Mbyte memory bank that they con­in a SIMD mode. Each VU has a peak speed of 32 trol. When loads 
or stores are performed, each VU Mflops for a combined speed of 128 Mflops per node. accesses its particular 
bank. This allows the VUS to In addition to performing arithmetic operations, the operate on four different 
data sets in a SIMD mode. VUS also act as memory controllers with each VU con-The SPARC processor can 
access any particular bank of parallel memory, but the VUS can not directly ac­cess SPARC memory. Transferring 
data between the two memory regions can be done using special instruc­tions, but accessing SPARC memory 
from the VUS is slow and should be avoided as much as possible. As a general rule, any operations involving 
the VUS must use parallel memory for optimal performance. To access the VUS, we have implemented the 
force calculation in CDPEAC (the assembler language for controlling the CM-5 vector units). The kernel 
of the force calculation takes two cells of particles and calcu­lates the resulting accelerations between 
the particles. This calculation is described in detail in Fig. 3. First, the particle coordinates from 
the two cells are copied from SPARC memory and replicated across all four VUS in parallel memory. Eight 
particles from cell 2 are then loaded onto all four VUS. We then loop over all of the particles in cell 
1 and calculate the acceler­ations between these particles and the eight particles loaded from cell 2. 
At each step, four different parti­cles from cell 1 are loaded (a different particle on each VU). This 
allows the VUS to calculate 32 interactions simultaneously. Once all particles in cell 1 have been processed, 
the next set of eight particles from cell 2 is loaded and the process is repeated. The calculation continues, 
calculating 32 interactions per step, until all accelerations have been calculated. Afterwards, the resulting 
accelerations are gathered from parallel memory and saved back to SPARC memory. All internal data structures 
in our code are stored in SPARC memory. This allows the data to be easily accessible to functions that 
do not require the VUS. Whenever particles are used in the force calculation, they are copied to parallel 
memory. For simulations with a cutoff of Tmaz = 5cr, each cell may contain several hundred particles 
and most of the time is spent calculating interactions in the force kernel. The extra overhead associated 
with copying the particles from SPARC memory to parallel memory is small so we pay a minimal penalty 
for using SPARC memory in this case. For simulations with a smaller cutoff such as m== = 2.5c, the number 
of cells per processor increases dramatically while the number of particles per cell decreases. In this 
case, it is more difficult to keep the VUS busy and our use of parallel memory becomes more critical. 
To obtain better performance with a smaller in­teraction cutoff Tma=, several modifications have been 
made. The main performance problem is that of load­ing the particles into parallel memory from SPARC 
memory. If we use the same scheme developed for large cutoff distances, each cell is loaded into par­allel 
memory whenever needed in the force calcula­tion. This results in each cell being loaded to par­allel 
memory as many as 14 times (once when calcu­lating self-interactions and 13 times when neighbor­ing cells 
calculate their interactions). To reduce the amount of loading, a parallel memory caching scheme has 
been implemented. A buffer for holding a col­lection of cells is allocated in parallel memory. Each time 
a cell is encountered in the force calculation, this buffer is checked to see if that cell has already 
been loaded. If not, it is loaded to parallel memory and the previous contents (if any) saved back to 
SPARC memory (including accelerations). The loading pro­cess operates according to a FIFO scheme and 
even­tually new cells will begin to replace previously loaded cells in the cache. As the force calculation 
proceeds, previously loaded cells will no longer be necessary in the calculation (after they have remained 
in the cache sufficiently long) and can be saved back to SPARC memory without having to be reloaded. 
This prop­erty is due to the fact that the interaction path has a finite range and will not see cells 
that were loaded much earlier in the calculation. By making the cache sufficiently large, each cell will 
be loaded to parallel memory only once and a substantial improvement in performance is obtained. 4.5 
I1II t i 8192000 Parlicles 4.0 1024 PNs g * ~ 3.5 .-E 1-k 1 ,5 t  d 0.0 20.0 40.0 60.0 80.0 100.0 
% of memory cached Figure 4: Effect of caching on iteration time In our code, the amount of memory available 
for caching can be adjusted. This gives us increased flexi­bility since our code can be optimized for 
memory (by using a small cache) or for speed (by using a larger cache). This allows us to run small simulations 
at increased speed or large simulations with more than 10s particles by simply adjusting our memory usage. 
Caching has significantly improved our code perfor­mance for simulations with a cutoff of r 2.5u. ?nax 
 The speedup obtained by cachkg for a particular sim­ulation is shown in Fig. 4. As more memory is added 
to the cache, the iteration time drops rapidly. In the figure we also see that little speedup is gained 
by caching more than 30% of the cells. For very large simulations, we have found that even adding a small 
cache of 1-2 ?70 of the cells can dramatically improve performance. 5 Timings and performance In Table 
1 (~~a= = 5c7) we summarize the timings for runs on a v&#38;iety of CM-5 processor partitions with different 
number of particles, i f, in the range from 1 million to 131 million. The particles, in each case, were 
arranged in a uniform 3D cubic lattice at con­stant density p = N/n3 = 1. With this density and interaction 
cutoff, each particle has approximately 520 interacting neighbors. This configuration is unstable and 
will undergo a phase change where the particles rearrange in an face-center-cubic (fee) configuration. 
This choice of initial conditions thus guarantees that the particles are moving between processors and 
real­istic inter-processor communication is involved. In the table, the update time per time step and 
the corresponding GF1oP rates are given (the numbers in parenthesis are the GFlop rates.) The GFlop rates 
were obtained by counting the total number of inter­actions between the particles during a time step. 
Each interaction involves a force calculation with 42 float­ing point operations in the CDPEAC kernel 
(counting multiply, add, and compare as one operation each and divide as five)[8]. The GFlop numbers 
are calculated by multiplying the total interaction count by 42 and dlvidlng this number with the time 
for a time step (measured with CM4D_node_tirner_elapsed). This procedure was then repeated and averaged 
over many time steps. Our numbers thus include both compu­tation and inter-processor communication and 
reflects the speed of realistic production runs.1 In Table 2 we summarize our recent timings for runs 
with a cutoff of Tmax = 2.5u. The update time per time step and the GFlop rates are given. All runs were 
performed using parallel memory caching. In each case, of the cells were cached except for the %~o run 
with 131 million particles that used a 3% cache. With a cutoff of r maz = 2.50, each particle has ap­ 
proximately 65 interacting neighbors. Since each par­ ticle has fewer neighbors, it is more dWicult to 
keep the VUS busy during the force calculation. Consequently, the GFlop rates are lower, However, our 
best timing 1The b=e kernel of the force calculation (with no Comm~­cation or SPARC memory operations) 
runs at 68.5 GFlops. for Tma= = 2.5c is our run with 65 million particles on 1024 PNs. In this case, 
the update time is 16.55 seconds which corresponds to 250 mzn~sec. per par­ticle update. To the best 
of our knowledge, this is the best reported time to date [6]. Using minimal paral­lel memory cachg, we 
were also able to simulate 180 million particles with an update time of 55.6 seconds. 100.0 1024000 Particles 
I e.. i (),, ~ 10.0 100.0 1000.0 Number of Processors Figure 5: MD update time vs. number of processors 
In Fig. 5 we illustrate the scaling properties of our code. The data are for runs with 1 million particles 
and two different values of the cut-off rma= = 5U and Tmaz = 2.5u. A near linear dependence is found 
for both values of the cut-off. This is of course also evident from the numbers in Tables 1 and 2. It 
should also be noted that our algorithm scales linearly with the number of particles for a fixed number 
of processors. Tin.. = 2.5u Particles Comp. Comm. Comp. Comm. 1024000 82.5% 17.5% 78.0% 22.0% 4096000 
88.5% 11.5% 77.2T0 23.8% 16384000 91.9% 8.1% 84.8~o 15.2% 65536000 93.2% 6.8% 89.5~o 10.5% 131072000 
94.4% 5.6yo 90.8~o 9.2% Table 3: Timing breakdown for 1024 PNs In Table 3 the breakdown of computation 
and com­munication time is given. For the larger simulations, each processor may have many thousan-ds 
of cells. As a result, calculating the accelerations may require sev­eral thousand message passing calls. 
Dependhg on the value of ~~~~, each message passing call may in­volve a transfer of 800-10000 bytes. 
Despite the large amount of communication, our algorithm is dominated by the calculation of forces for 
both values of rma=. In Processors Particles 64 128 256 512 1024 1024000 47.94 (1:) 25.41 (2.9) 12.56 
(5.8) 6.79 (10.7) 4.30 (16.9) 1.66 (43.7) 2048000 94.39 (1.5) 48.39 (3.0) 24.43 (5.9) 12.75 (11.4) 6.92 
(21.0) 3.16 (47.0) 4096000 186.83 (1.6) 95.28 (3.1) 47.98 (6.0) 25.57 (11.4) 14.00 (20.7) 6.17 (47.0) 
8192000 - 188.00 (3.1) 95.11 (6.1) 50.63 (11.5) 28.12 (20.6) 12.13 (47.8) 16384000 - 185.77 (6.2) 95.52 
(12.2) 52.60 (22.1) 23.68 (49.0) 32768000 - 190.81(12.2) 101.55(22.9) 46.71 (49.7) 65536000 - 204.91(22.6) 
92.91 (50.0) 131072000 - 163.01(50.7) Table 1: Update times per time step in sec. (GFlops in parenthesis). 
Cut-ofE r~a= =50 Processors Particles 32 64 128 256 512 1024 1024000 8.90(0.81) 4.51(1.61) 2.32 (3.12) 
1.26 (5.74) 0.72 (10.07) 0.44 (16.55) 2048000 - 8.96(1.62) 4.44 (3.26) 2.46 (5.90) 1.36 (10.65) 0.74 
(19.54) 4096000 8.79 (3.29) 4.81 (6.03) 2.67 (10.84) 1.36 (21.27) 8192000 - 16.83(3.44) 8.81 (6.58) 4.80 
(12.07) 2.47 (23.50) 16384000 16.95 (6.84) 8.74 (13.26) 4.49 (25.82) 32768000 16.90 (13.72) 8.54 (27.14) 
65536000 - 16.55(28.01) 131072000 - 34.26(27.06) Table2: Update times pertimestepin sec. (GFlops in 
parenthesis). Cut-ofE ~~ac =2.5cr. the table, computation time includes the calculation of forces and 
the numerical integration. The commu­nication time includes all message passing during the interaction 
calculation and the time to redistribute the particles after each time step. Wltha cutoffof rma== 5u, 
thespeedup fromarun with4million particlesona 32nodeCM-5 tothesame run on a 1024 node CM-5 is over afactor 
30 and cor­responds to 95% parallel efficiency. We were also re­cently able to run SPaSM on a 4 PN CM-5 
with 1 mil­lion particles. The update time here was 367 sec. The speedup achieved with same run on 1024 
PNs is a fac­tor 221, representing 86% parallel efficiency. Our best performance number, 50.7 GFlops, 
represents 40% of the theoretical peak performance of 128 GFlops on a 1024 node CM-5. The 50.7 GF1OPS 
also represents a cost /performance number of 1.95 GFlops/$Million. Conclusion We have demonstrated that 
three-dimensional multi-million particle MD simulations can now be per­formed routinely on massively 
parallel MIMD com­puter systems. To demonstrate the practicality of the algorithm, we present a few time 
frames from a mil­lion particle impact simulation. The simulated par­ticles have an interaction cut-off 
of Tma= = 2.5cr and the time step of the integration is At = 0.01 time units. The system is initiated 
with one block of par­ticles in a fcc lattice with 200x200x 25 (106) atoms, and a projectile in a fcc 
lattice with the dimensions 20a x 20cr x 40 layers (14000) atoms. The projectile has a velocity of 10 
towards the block (-l.3 times the sound velocity in the lattice). This initial condition is shown in 
Fig. 6a. In Fig. 6b we show the system at t= 2.5. The projectile has made contact with the block and 
has partially penetrated. The hexagonal nature of the lattice is seen to dominate the phonons emitted 
on the surface of the block, even though the projectile makes contact with a square (20x20) shape. Finally, 
in Fig. 6c, we show the particles at t= 5, where part of the projectile has been absorbed in the block. 
Other parts of the projectile have disintegrated into almost free particles. We are currently using SPaSM 
to perform other multi-million atom simula­ tions. The impact simulation shows the inherently un­ structured 
nature of MD simulations. In principle, every particle is free to move to any location within the system. 
This may require a substantial amount of communicant ions and data management. However, we have been 
able to achieve high performance by care­fully analyzing the problem and mapping it to the ar­chitecture 
of the CM-5. Our algorithm is dominated by computation with communication requiring only 5­20% of the 
overall time. Our algorithm scales linearly with the number of processors and the number of par­ticles. 
With an interaction cutoff of Tma= = %, runs with a sustained calculation rate of 50 Gflops can be performed 
on a 1024 PN CM-5. With a smaller cut­off of rma= = 2.5u, we can achieve an update time of 250 nan~seconds 
per particle. This fast update time allows us to run large MD simulations that have been impossible to 
perform in the past. Using all of the memory of the CM-5, we have also been able to sim­ulate more than 
180 million particles in 3D. WhHe we may not be able to model 1 billion particles on cur­ rent machines, 
this goal now seems withb reach as next generation machines become available. Acknowledgments We thank 
the Advanced Computing Laboratory for generous support, in particular M. Krogh and D. Rich provided major 
assistance. We also express our appre­ciation to D. Dahl, A. Greenberg, C. Lobron, A. Main­waringl and 
L. Tucker from TMC for valuable sugges­tions regarding CDPEAC and CMMD. References <RefA>[1] <SinRef><title>Computer Simulations 
of Liquids</title>, <author>M. P. Allen </author>and <author>D. J. Tildesley</author>. <publisher>Clarendon Press</publisher>, <location>Oxford </location>(<date>1987</date>)</SinRef>. [2] <SinRef><author>A. I. Mel</author> &#38;k, <author>R. 
C. Giles</author>, and<author> H. Gould</author>, <title>Com­puters in Physics</title>, <date>May/June 1991</date>, p. <pages>311</pages></SinRef>. [3] <SinRef><author>P. Tamayo</author>, <author>J. P. Mesirov</author>, and 
<author>B. M. Boghosian</author>, <title>Proc. of Supemomputing 91</title>, <publisher>IEEE Computer So­ciety </publisher>(<date>1991</date>), <pages>p. 462</pages></SinRef>. [4] <SinRef><author>B. L. Holian </author><author>et 
al. </author><title>Phys. Rev. A</title> <pages>43,2655 </pages>(<date>1991</date>). </SinRef>[5] <SinRef><author>R. C. Giles </author>and <author>P. Tamayo</author>, <title>Proc. of SlfPCC 92</title>, <journal>IEEE Computer Society </journal>
(<date>1992</date>), p. <pages>240</pages></SinRef>. [6] <SinRef><author>S. Plimpton </author>and <author>G. Heffelfinger</author>, <title>Proc. of SH­PCC 92</title>, <publisher>IEEE Computer Society </publisher>(<date>1992</date>), 
<pages>p. 246</pages></SinRef>. [7] <SinRef><author>D. M. Beazley</author> and <author>P. S. Lomdahl</author>, <title>Afessage-Passing Multi-Cell Molecular Dynamics on the Connection 
Machine </title><volume>5</volume>, <publisher>Parall. Comp.(in press)</publisher> (<date>1993</date>) </SinRef>. [8] <SinRef><title>DPEA C Reference Manual</title>, <institution>Thinking Machines Corporation </institution>
(<date>1992</date>),<location> Cambridge, Massachusetts</location></SinRef></RefA>.   
			
