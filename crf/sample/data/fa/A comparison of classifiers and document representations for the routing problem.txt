
 A Comparison of Classifiers and Document Representations for the Routing Problem Hinrich Schutze David 
A. Hunt Jan O. Pedersen* Werox Palo Alto Research Center hank Xerox Research Center 3333 Coyote Hill 
Road 6 Chemin de Maupertuis Palo Alto, CA 94304, USA 38240 Meylan, France {schuetze,pedersen} @?parc.xerox.com 
hull@ xerox.fr URL: ftp://parcftp.xerox. com/pub/qca/SIGIR95 .ps Abstract In this paper, we compare 
learning techniques based on statistical classification to traditional methods of relevance feedback 
for the document routing problem. We consider three classification tech­niques which have decision rules 
that are derived via explicit error minimization linear discriminant analysis, logistic regression, and 
neuraf networks. We demonstrate that the classifiers perform 10­15% better than relevance feedback via 
Rocchio expansion for the TREC-2 and TREC-3 routing tasks. Error minimization is difficult in high-dimensional 
feature spaces because the convergence process is slow and the models ~e prone to overfitting. We use 
two different strategies, latent semantic in­dexing and optimaJ term selection, to reduce the number 
of features. Our results indicate that features based on latent semantic indexing are more effective 
for techniques such as linear discriminant anaf­ysis and logistic regression, which have no way to protect 
against overfitting. Neural networks perform equally well with either set of features and can take advantage 
of the additional information avail­able when both feature sets are used as input. Overview Document 
routing can be described as a problem of statistical text classification. Documents are to be assigned 
to one of two cate­gories, relevant or non-relevant, and a large sample of judged docu­ments is available 
for training. This paper will compare traditional relevance feedback approaches to routing with classification 
based on explicit error minimization. A central problem in routing is the high dimensionality of the 
native feature space, where there exists one potentiaJ dimension for each unique term found in the collection, 
typically hundreds of thousands. Standard classification techrtiques cannot deaf with such a large feature 
set, since computation of the solution is not tractable and the results become unreliable due to the 
lack of sufficient train­ing data. One solution is to reduce dimensionafity by using sub­sets of the 
original features or transforming them in some way. An­other approach does not attempt dimensionality 
reduction, but in­stead employs a learning algorithm without explicit error minimiza­ tion. Relevance 
feedback via Rocchio expansion, which has been widely used in IR, is an example of such an approach. 
We will ex­ permission to make digitah~rd copies of ;L1l or patl of this motcriid without fee is granted 
provicicd [hot the copies orc not Ina(ie or distributed for profit or commcrci:d mlv:mt:lgc, lhu ACM 
c{)pyright/ server notice., the title of tlw puhlic:ltioo aod its d:llc :Ipl)c:ir, :md notice is given 
th~t copyright c; h!y permission of (Iw Associ:lti{~n I or Computing Mmchinmy, inc. (ACF.1 ). 7-<>u)py 
Oll)crwise, to rCIIUlll ISh, rquiws spccilic p{:rlrlisstoll to post on servers or to rdistrihulc 10 lists, 
andlor fee. SIGIR 95 Seattle WA USA c 1995 ACM O-8979 1-714-6 /95/07 .$:1 .S[) amine two different forms 
of dimensionality reduction, Latent Se­mantic Indexing (IS) and optimal term selection, in order to inves­tigate 
which form of dimensionafity reduction is most effective for the routing problem. In routing, the system 
uses a query and a list of documents that have been identified as relevant or not relevant to construct 
a clas­sification rule that ranks unlabeled documents according to their likelihood of relevance. We 
examine a number of different meth­ods of generating the document classified relevance feedback via query 
expansion (QE), linear discnminant analysis (LDA), logis­tic regression (LR), linear neural networks 
(LNN), and non-linear neural networks (NNN). The mathematical description of the clas­sification rule 
is generally expressed as a function ~(x), where z is a vector of feature variables. The traditional 
approach to relevance feedback [30] defines $(z) = q * z, where q, the feedback query, is a weighted 
combination of the onginaf query vector and the vec­tors of the relevant (and perhaps non-relevant) documents. 
Methods which use this functional form (QE, LDA, LR, and LNN) are known as linear classifiers. We also 
look at NNN s to investigate whether adding anon-linear component to the basic model improves perfor­mance. 
The classification techniques proposed above have significant advantages over query expansion. They perform 
explicit error min­imization using an underlying model with enough generality to take full advantage 
of the information contained in a large sample of rel­evant documents. In contrast, query expansion uses 
a limited prob­abilistic model that assumes independence between features and the model parameters are 
often fit in a heuristic manner based on term frequency information from the corpus. This paper will 
demon­strate that these advantages translate directly into improved retrieval performance for the routing 
problem. We use the Tipster collection and the TREC-2 and TREC-3 routing tasks to test classifiers and 
representations [15, 16]. There are some risks associated with using more general models of the relevant 
document space. On the surface, one might expect that learning algorithms that use more parameters ardor 
a larger feature space will have an easier time capturing the distinction be­tween relevant and non-relevant 
documents (cf. Buckley s recent experiments that show better performance with increasing number of terms 
[4]). However, the improved performance is only guaran­teed for the training data, which is simply a 
sample from the under­lying population of relevant documents which may not adequately characterize its 
true distribution. The more general the model, the more effort it will expend on fitting to specific 
features of the train­ing documents that will generalize to the full relevant population. A classification 
technique is said to suffer from overjitting when it im­proves performance over the training documents 
but reduces perfor­ mance when applied to new documents, when compared to another method. There is thus 
a fundamental trade-off between a large fea­ture space with a restrictive learning algorithm and fewer 
features with a more general learning algorithm. In the past [15], evidence has suggested that a weak 
learning rule (query expansion) and a high-dimensional feature space (terms) optimizes performance. We 
will demonstrate that the alternative approach is likely to prove su­perior in the long run. Sections 
2 and 3 describe and motivate our dimensionality re­duction strategies and classification techniques. 
Sections 4 and 5 present expenmentrd set-up and experimental results. Section 6 an­alyzes results in 
detail and section 7 states our conclusions. Dimensionality Reduction In our work, we will examine two 
major approaches to dimension­ality reduction, loosely described as feature selection and reparam­eterization. 
In feature selection, a subset of the most important fea­tures are selected from the full feature space 
for use by the learning algorithm. Most previous work on classification in IR has relied ex­clusively 
on this method of dimension reduction. Reparameteriza­tion is the process of constructing a new document 
representation by taking combinations and transformations of the original feature variables. In our experiments, 
the most important features are assessed by applying a X2-measure of dependence to a contingency table 
con­taining the number of relevant and non-relevant documents in which the term occurs (IV,+ and ZVn+, 
respectively), and the number of relevant and non-relevant documents in which the term doesn t oc­cur 
(IV.. and N._, respectively). N(N.+N. N, N.+)2 2=(N,++ Nr_)(Nm+ + N.-)(iV,+ + Nn+)(Nr-+ N.-) For each 
query, only documents in the local region are considered (see below). We settled on the X2-statistic 
as the selection criterion after initial experiments comparing it with term selection according to raw 
frequency of occurrence and according to the ratio of rele­vant and non-relevant documents a term occurs 
in. These altern­ative measures were less effective than the Xz -test. The underlying assumption in using 
the Xz -test is that features whose frequency de­pends heavily on whether they occur in a relevant or 
non-relevant document (defined by a high X2-score for constant total frequency) will be useful for measuring 
the distinction between these two cat­egories. For reparametenzation, we use Latent Semantic Indexing 
(LSI) [8], a technique that represents features and documents by a low­dimensionrd linear combination 
of orthogonal indexing variables. Our use of LSI differs in two important aspects from [8]. We com­pute 
a separate representation of terms and documents for each query, focusing on the documents which are 
most likely to be rel­evant [19]. We refer to this technique as Local LSI since it is only applied to 
a region of the document space that is in the neighbor­hood of the query. A second imovation is that 
the LSI representa­tions are not used to construct a query which is analyzed using the vector space model. 
Rather, they are used as input parameters to a learning algorithm [19, 34]. LSI works by applying a matrix 
decomposition to the term by document matrix of the collection, which generates a large number of orthogonal 
LSI factors. A small number of the most important factors are then selected to approximate the covariance 
structure of the full collection. We use SVDPACK, a sparse SVD algorithm, for our computations [3], Even 
though this algorithm does not need to calculate all orthogonal factors. it is still difficult to compute 
the LSI solution for the TREC collection, since it contains over a million terms and documents. A proper 
model of the full TREC collection would probably re­quire many hundreds of LSI factors, far more than 
could be success­fully modeled by learning algorithms. Furthermore, these factors are capturing the structure 
of the document collection as a whole and are not tuned for particular queries. Previous work has shown 
that LSI is more successful when applied to a local region on a quety specific basis [19]. Dumais [9] 
also applies LSI to the routing task, but uses the judged documents for all the queries to generate her 
re­duced representation, a method that corresponds roughly to taking the union of the local LSI regions 
for each query. We compute a separate LSI representation for each query us­ing only the documents contained 
in the local region (defined in the next section), retaining the 100 most important factors. 1 These 
fac­tors should capture the most important local structure, which will be crucial in separating relevant 
documents from nearby non-relevant documents. This approach differs from the one in [ 19] in that the 
lo­cal region now contains both relevant and non-relevant documents, which was found to be more effective 
than using only relevant doc­uments [20]. 2.1 Discussion LSI captures the theme (or latent semantic 
structure [8]) of a document by analyzing the patterns of cooccurrence between terms. 2 Focusing on 
the theme of a document addresses the prob­lems of synonymy and near-synonymy: In a term-based representa­tion 
scheme, documents that are about the same theme but describe it with different vocabulary are represented 
in away that hides their thematic similarity. This makes it difficult to obtain an accurate measurement 
of relevance. LSI avoids some of this problem by rep­resenting the theme of a document rather than specific 
terms. At first sight, synonymy seems a minor problem in the rout­ing context where a training set is 
available. A classifier can be trained to recognize that each of several different ways of express­ing 
a particular theme indicates relevance. Indeed, if there area few terms that provide reliable evidence 
for estimating relevance, then the use of LSI is not necessary. For example, consider a mail filter for 
TREC topic 133, which is about the Hubble Space Telescope. It can do an excellent job by relying on the 
single term Hubble , and an LSI analysis will make it more difficult for a classifier to get at the correct 
information, the presence or absence of the term Hub­ble . However, if there is a great number of terms 
which rdl contribute a small amount of critical information, then the combination of ev­idence is a major 
problem for a term-based classifier. Consider the example of another TREC query, Topic 124 about Alternatives 
to Traditional Cancer Therapies . There are articles about many dif­ferent alternative cancer therapies 
in the Tipster collection: gene therapy, immunization, vitamin A therapy, umbilical blood trans­fusion, 
etc. Each therapy has terms that are unique to it. so that the joint vocabulary of relevant terms is 
too large for a learning algo­rithm based on error minimization (given the small number of pos­itive 
examples typical in Tipster). There are more than a thousand terms that contribute helpful information, 
for example carcinogen­esis and terminally-ill have ranks 1010 and 1018, respectively.3 Therefore, LSI 
serves as a means of data compression, captur­ing the important information contained in a large number 
of terms with a much smaller number of factors. This is particularly useful for eliminating the redundancy 
in word features that is due to term dependence, since LSI factors are constructed to be orthogonal. 
By creating a compact representation of documents, LSI reduces over­fitting while still modeling the 
important structure contained in het­erogeneous queries like topic 124 just described. 1These computmons 
took less than 5 mmutes per query 2 We use theme rather than topic to avoid contlmon with the TREC queries 
which iue also called topics 3However, it ISdifficult to assess by intuition only bow useful a given 
term IS. For exsmple, carcmogenesis could be perfectly comelated with a term Iugher up m the fist. in 
wtuch case it would not contibute information, Non-linear term-based classifiers can also detect dependencies 
and are an alternative to the particular analysis of term correlations performed by LSI. However, if 
the amount of training data is com­paratively small, a more general classifier may fail to model non­linear 
dependencies correctly. In our experiments, the more compli­cated models we have tested don t achieve 
any gain in performance compared to LSI. The disadvantage of LSI is that the full discriminatory power 
of some of the underlying terms maybe lost for queries that crucially depend on particulm highly informative 
terms. Term-based meth­ods excel for this kind of query, for example the above mentioned TREC Topic 133 
on the Hubble Space Telescope. Our experiments will compare the performance of features based on variable 
selec­tion to those generated by Latent Semantic Indexing and determine which are more effective for 
learning algorithms. 3 Learning Algorithms Previous approaches to routing and text categorization [24] 
have used classification trees [33, 22], Bayesiarr networks [6], Bayesian classifiers [22, 23], rules 
induction [1], nearest-neighbor techniques [25, 36], logistic regression [5], least-square methods [11], 
discnm­inant analysis [19], and neural networks [32, 34]. The majority of these algorithms require that 
the number of feature variables be re­stricted in some way. The issue of how best to accomplish this 
di­mensionality reduction is one that has been neglected in the research on learning algorithms in information 
retrieval. We compare three different classification rdgorithms, linear dis­criminant analysis, logistic 
regression, and neural networks to a baseline constructed by query expansion. The baseline classifica­tion 
vector q is the vector sum of the relevant documents, using con­ventional term weighting and document 
normalization strategies. This is equivalent to Rocchio expansion when one assigns a weight of zero to 
the query and the non-relevant documents. In previous ex­periments, we found no evidence that negative 
feedback improved performance. The other classification rules are obtained by error minimization of an 
explicit underlying model, but use different models and opti­mization techniques. LDA can be derived 
from a normal model for the distribution of relevant and non-relevant documents in feature space (although 
that is not how it is derived here) and models feature dependence explicitly by using the covarirmce 
matrix of each doc­ument class. It has a closed form solution that it obtained by inver­sion of the covariance 
matrix, as described below. Logistic regres­sion and linear NN s are based on a binomial model of document 
relevance, which has an iterative solution obtained via numerical optimization. Logistic regression uses 
the Newton-Raphson tech­nique while neural networks rely an backpropagation (gradient de­scent). 3.1 
Linear Discriminant Analysis Linear Discriminant Analysis (LDA) for the two-group problem can be derived 
as follows [13]. Suppose that one has a sample of data from two groups with nl and nZ members, with mean 
vectors 51 and zz and covariance matrices SI and SZ respectively. The goal is to find the linear combination 
of the variables that maximizes the separation between the groups. A reasonable optimization criterion 
is to maximize the separation between the vector means, scaling to reflect the structure in the pooled 
covariance matrix. In other words, choose a so that: (2 stands for transpose) G*= argmax a~(z~ -Z2) a 
V z% is maximized, where (rtI +rzz 2)S = (rtl l)SI + (rtz 1)S2. Since S is positive definite, we can 
define the Cholesky decom­position of S = RTR. Let b = Ra, then the formula above be­ which is maximized 
by choosing b m R -1(21 %2), which means then that a* = R-lb = S-l (ZI Z2 ). Therefore, the one dimen­ 
. T sionrd space defined by y = a z should cause the group means to be well separated. This approach 
can be generalized to more than two groups and it can be extended to create a non-linear classifier by 
modeling a separate covariance matrix for each group. LDA has already been applied to the routing problem 
by Hull [19]. In order to produce a non-linear classifier, one can estimate a separate covarirmce matrix 
for each group, rather than using a pooled estimate of the covariance matrix S, an approach known as 
Quadratic Discnminant Analysis (QDA). However, QDA is only effective when the number of elements in each 
group is significant] y larger than the number of feature variables, which is almost never the case for 
the routing problem because relevant documents are rel­atively rare, There is a more well-behaved alternative 
known as Regularized Discriminant Analysis (RDA) [10]. RDA uses a pair of shrink­age parameters to create 
a very general family of estimators for the group covariance matrices. Rather than choosing between the 
pooled (LDA) and unpooled (QDA) covariance matrices, it looks at a weighted combination of them. RDA 
selects the optimal val­ues for the shrinkage parameters based on cross-validation over the training 
set. However, previous experiments have not found much benefit to applying RDA to the routing problem 
[20]. 3.2 Logistic Regression Logistic regression is a statistical technique for modeling a binary response 
variable by a linear combination of one or more predictor variables, using a logit link function: g(7r) 
= log(7r/(1 7r)) and modeling variance with a binomial random variable, i.e., the de­pendent variable 
log(~/ (1 z)) is modeled as a linear combination of the independent variables. The model has the form 
g(z) = Z@ where T is the estimated response probability (in our case the proba­bility of relevance), 
X. is the feature vector for document i, and /3 is the weight vector which is estimated from the matrix 
of feature vec­tors. The optimal value of/3 is derived using maximum likelihood [26] and the Newton-Raphson 
method of numerical optimization. Logistic regression has been used for text retrieval in previous experiments 
[5, 12, 32]. Our approach is similar but all our fea­ture variables are query-specific. i.e. we do not 
make use of general properties that are common to all queries in the collection. For the document routing 
problem, where large quantities of training docu­ments are available for each query, such information 
is likely to be of limited value.  3.3 Neural Networks A neural network (NN) is a network of units, 
some of which are designated as input and output units. Neural networks are trained by backpropagation: 
the activation of each input pattern is propa­gated forward through the network, and the error produced 
is then backpropagated and the parameters changed so as to reduce the er­ror [28]. The strength of neural 
networks is that they are robust. i e., they have the ability to tit a wide range of distributions accurately. 
For example, any member of the exponential family can be modeled where t, is the relevance for document 
z and o, is the estimated rel­ evance (or activation of the output unit) for document i. The defi­ nition 
of the sigmoid is equivalent to z = 109( f(z)/(1 i(~))), which is the same as the Iogit link function. 
This means that linear neural networks (architecture (a) in Figure 1) and logistic regression  / j ..,!,. 
 , both perform maximum likelihood estimation of the same model. The main difference lies in the optimization 
algorithm, Newton-- LSIreprewrtahm termrepesentshcn LSIrqxeswtatm tam Fefrefamatm a)linearmural mtwark 
b)rmr-linaar naural!wtvmk Figure 1: Linear and non-linear neural network. [29]. Unfortunately, this 
capacity leads to the danger of overjit­ ting. Neural netw orks can produce a model which fits the training 
data too precisely and does not generalize to the full population. In previous experiments, we found 
that logistic regression performed poorly when used with large numbers of features variables, and the 
most likely culprit is overfitting.4 Our neural networks protect against overtitting by using a val­ 
 idation set. Two thirds of the training data is used for model se­ lection, while the remaining third 
is set apart for validation. At each iteration, the parameters of the model are updated and the er­ ror 
on the validation set computed. Training continues until the er­ ror on the validation set goes up, which 
indicates that overfitting has set in. This procedure establishes the number n of iterations of training 
that improve generalization. The final parameters of the model are then computed by training on the entire 
training set for n iterations. We chose this procedure rather than systematic cross­ validation since 
the latter would have been computationally too ex­ pensive. For the validation procedure described above, 
it is usefid to have an optimization strategy that changes the parameters by small amounts at each iteration 
so that it does not overshoot the optimaJ point and overfit the training data. Backpropagation (gradient 
de­scent), as implemented in our neural networks, acts in just this faah­ion. The architectures of the 
neural networks used in our exper­iments are shown in Figure 1. There is only one output unit whose activation 
models probability of relevance. The linear network con­sists only of input and output units. The non-linear 
network addi­tionally has two blocks of 3 hidden units each of which are con­nected to both input and 
output units. (The figure shows the network architectures for dual input (MI and terms). The architectures 
with only one input realize only the corresponding half of the architec­tures.) In both architectures, 
all input units are directly comected to the output unit. Relevance for a document is computed by set­ting 
the activations of the input units to the document s representa­tion and propagating the activation through 
the network to the out­put unit, then propagating the error back through the network, using a gradient 
descent algorithm [28]. We chose the sigmoid: as the activation function ~ for the units of the network, 
It can be shown [29] that in this case backpropagation minimizes the same error as the logistic regression, 
the cross-entropy error: L= ~t,logoz+(l ti)log(l Ot) 4Table1confirmstlusresult precisionfor logisticregressiondecreaseswhen 
more fearures are added. Raphson for the logistic regression and backpropagation for neural networks. 
Apart from gradient descent, another difference between logis­ tic regression and neural networks is 
that the latter have a non-linear extension (architecture (b) with hidden units in Figure 1). Hidden 
units can be interpreted as feature detectors that estimate the prob­ ability of a feature being present 
in the input. This estimate is then propagated to the output unit and can contribute to a better estimate 
of relevance. We focus on the learning aspect of neural networks, in particular explicit error minimization. 
In contrast, other work on neural net­ works in IR has been closely related to the vector space model 
[35] or relevance feedback [2]. Kwok s work in [21] bears most simi­ larity with our approach. However, 
apart from the standard learn­ ing algorithm we use, our input consists of reduced representations (either 
by feature selection or reparametenzation). This representa­ tional scheme substantially reduces training 
time, and is less prone to overfitting, because there are fewer parameters. An interesting innovation 
of Kwok s approach that we are planning to integrate into our model is the non-random initialization 
of weights, which reflects prior knowledge about terms and documents. In summary, there are two reasons 
why we use neural networks as a statistical technique for routing. First we would like to protect against 
overfitting. Linear neural networks and logistic regression have the same probabilistic model, but validation 
combined with gradient descent (used to train neural networks) is better suited to avoid overfitting. 
Secondly, we would like to explore the use of non-linear classifiers in routing. In analogy to the way 
that non­ linear RDA generalizes linear LDA, linear neural networks have a simple non-linear extension: 
neural networks with hidden units, corresponding to feature detectors. 4 Experimental Set-Up We use the 
Tipster corpus for our experiments. It consists of 3.3 gigabytes of text in over one million documents 
from several dif­ferent sources: newswire, patents, scientific abstracts, and the Fed­eral Register [14]. 
There are also 200 Tipster queries, detailed state­ments of information need that are crdled topics. 
We preprocess the corpus using the TDB system [7], performing document parsing, tokenization including 
stemming using a two­level finite-state morphology, and removal of terms froma951 word stop-list. Our 
terms consisted of single words and two-word phrases that occur over five times in the corpus (where 
phrase is defined as an adjacent word pair, not including stop words). This process produced over 2.5 
million terms. We also breakup documents into chunks of about 250 terms, called text-tiles [17]. Only 
the tile with the highest proximity to the topic (i.e. the highest correlation in the vector space model) 
is selected and used for all subsequent experi­ments (both in training and test). For our routing runs, 
we replicate the routing setup at the second and third TREC conferences. Disks 1 and 2 (about two gigabytes) 
are the training set for our run, Disk 3 (about one gigabyte) is the test set. Each combination of classifier 
and input representation is run for two sets of topics: 5 1 100 (corresponding to the routing task in 
TREC 2 [15]) and 101 150 (corresponding to the routing task in TREC 3 [16]). Our goals in these experiments 
are (1) to demon­strate that classification techniques work better than query expan­sion, (2) to find 
the most effective classification technique for the classifier input II Precision for Topics 51-100 precision 
for Topics 101-150 average I average % ch~ge at 100 average % change at 100 change baseline exmmsion 
II 0.3678 +0.0% 0.4710 0.3705 +0.0% 0.4194 +0 Lil 0.3240 -11.9% 0.4210 0.3268 -11.8% 0.3908 -12 200 terms 
0.3789 +3.OYO 0.4824 0.3712 +0.2% 0.4440 +2 LSI + 200 terms 0.3359 -8.7% 0.4426 0.3358 -9.4% 0.3928 -9 
logistic LSI 0.3980 +8.2Y0 0.5108 0.4057 +9.5?Z0 0.4802 +9 regression 200 terms 0.3654 -0.7% 0.4788 0.3637 
-1.8% 0.4434 LSI + 200 terms 0.3494 -5.0% 0.4652 0.3457 -6.7 % 0.4168 :: LDA LSI 0.4139 +12.5% 0.5166 
0.4230 +14.2% 0.4870 +13 200 terms 0.3966 +7.8% 0.4916 0.3841 +3.7% 0.4586 +6 LSI + 200 terms 0.3973 
+8.0% 0.5034 0.3910 +5.5% 0.4616 i-7 linear LSI 0.4098 +11 .4% 0.5094 0.4211 +13.7% 0.4830 +13 network 
0.4121 +1 1.2% 0.4742 +13 0.4302 +16. 1~0 0.4908 +16 non-linear 0.4208 +13.6% 0.4834 +13 network 0.4115 
+11.1% 0.4740 +13 I LSI + 200 terms II 0.4251 +15.6% 0,5204 II 0.4318 +16.5% 0.4882 II +16 Table 1: 
Non-interpolated average precision, precision at 100 documents and improvement over expansion for routing 
runs on TREC data. routing problem, and (3) to make sure that our comparison between LSI and term-based 
methods is not based on the idiosyncrasies of a particular learning algorithm. 4.1 Query-specific screening 
of the collection The sheer size of the TREC collection makes it difficult to apply learning methods 
to the full training set from a purely computational standpoint. Furthermore, all documents are not of 
equal vahte for training. Relevant documents are relatively rare, which means that they are much more 
valuable for training than non-relevant docu­ments. These considerations motivate an initial screening 
of docu­ments before applying our classification algorithms. For each query, we apply an initial screening 
process designed to identify decuments that are clearly not relevant so that they can be excluded from 
further analysis. We define the local region for a query as the 2000 nearest documents, where similarity 
is mea­sured using the imer product score to the Rocchio-expansion of the initial uery vector [4], corresponding 
to our baseline feedback ? algorithm. The documents in the local region are then used as the training 
set for the learning algorithms. The documents in this re­gion for which relevance judgments do not exist 
are treated as not relevant. There are a number of advantages to training over the local re­gion. First, 
the size of the training set is substantially reduced, so it is possible to attack the problem using 
computationally intensive learning algorithms. Second, the density of relevant documents is much higher 
in the local region than in the collection as a whole. Third, the non-relevant documents selected for 
training are those which are most difficult to distinguish from the relevant documents. These non-relevant 
documents are clearly among the most valuable ones to use as training data for a learning algorithm. 
The screening process is also applied to the test set before eval­uation to avoid extrapolating beyond 
the region defined by the train­ing set. A threshold derived from the training set is applied to all 
documents in the test set. Documents with a query-correlation higher than the threshold are automatically 
ranked ahead of those that fall outside the local region. 5 Only the one-thousand highest-weighted terms 
were used which may partl y ex­plain why our pafom.nce IS not as good as the one m [4], 5 Experimental 
Results Table 1 presents routing results for 5 different classifiers and 4 dif­ferent representations. 
The representations are: a) relevance feedback via query expansion b) LSI (100 factors from a query-specific 
local LSI) c) 200 terms (200 highest ranking terms according to X2 -test) d) LSI + terms (100 LSI factors 
and 200 terms). The classifiers are: a) baseline (the vector space model: documents ranked accord­ing 
to proximity to query vector for LSI , 200 terms , and LSI + 200 terms and proximity to expanded query 
vector for expansion ) b) logistic regression c) linear NN (architecture (a) in Figure 1) d) non-linear 
NN (architecture (b) in Figure 1) e) LDA (linear discriminant analysis) The run expansion was tf-idf 
weighted [31], and terms in the baseline runs were idf-weighted. Inverse document frequency (idf) weights 
are derived from the entire training set, not from the local region. All other runs on terms were not 
weighted: the input was 1 if the term occurred in the document and O otherwise. This strat­egy was motivated 
by poor results for runs in which terms were weighted according to frequency of occurrence and a desire 
to let the learning algorithms select the proper weight for each term. These experimental results are 
analyzed using ANOVA and the Friedman Test [18] to measure their statistical significance. ANOVA determines 
that one method is significantly better than an­other if the average difference in performance is large 
compared to its variability, correcting for differences between queries. The Friedman test conducts a 
similar analysis, but it uses only the rank­ing of the methods within each query. From Table 1 we can 
draw the following conclusions: Classification vs. Expansion. More advanced learning algo­rithms increase 
performance by 10 to 15 percent over query expan­sion. LDA and neural networks perform significantly 
better than the baseline experiments, regardless of representation. Logistic regres­sion only performs 
better when using an LSI representation (signif­icant difference N .02). LSI vs. Selected terms. LDA 
and logistic regression work sig­nificantly better with LSI features than with term features. Neurat 
networks work equally well with either LSI or term-based features, and significantly better with a combination 
of LSI and term-based features (significant difference x .01). Logistic Regression vs. Other Classifiers. 
For LSI features, logistic regression is less effective than the other learning algo­rithms according 
to the Friedman Test, although the magnitude of the difference is small. For word or combined features 
logistic re­gression performs a lot worse than either LDA or neural networks. Linear vs. Non-linear neural 
networks. The results suggest that there is no advantage to adding non-linear components to the neurat 
network. (see Section 6 for discussion) LDA vs. Neural networks. For LSI features, LDA and neu­ral networks 
perform about the same. Neural networks are superior to LDA for the other representations. The best neural 
network per­formance (combined features) is slightly better than the best LDA performance (LSI features), 
but not enough to be statistically sig­nificant. The sharp observer will note that the magnitude of the 
signifi­cant difference changes, depending on the experiment. This occurs because the variability between 
learning algorithms is greater than the variability between representations. Therefore, comparisons be­tween 
experimental runs using the same learning algorithm can de­tect the significance of a smatler average 
difference. The most important conclusion is that advanced learning algo­rithms capture structure in 
the feature data that was not obtained from query expansion. It is also interesting that the linear neural 
net­work works better than logistic regression, since they are using ex­actly the same model. This indicates 
that the logistic model is over­fitting the training data, and the ability of the neural network to stop 
training before convergence is an important advantage. NN s can also benefit from the additional information 
available by combining the word and LSI features unlike the other classification techniques. Evidence 
of overfitting for logistic regression can be found by ob­serving that performance decreases when going 
from LSI or term features to a combined representation. Using a more general feature space should only 
increase performance over the training set, yet it hurts performance in the final evaluation. The price 
for better pro­tection against overfitting in neural networks is their slower speed of convergence, since 
backpropagation (gradient descent) requires more time to converge than Newton-Raphson. Linear discriminant 
analysis also suffers from overfitting, which explains why it works most successfully with the compact 
LSI rep­resentation. One might be able to improve perforrmmce for word­based features by applying regularized 
discriminant analysis [10], which uses cross-validation to adjust for this problem. However, we did not 
conduct such so experiment here, due to the prohibitive computational cost of cross-validation for large 
IR problems. Pre­vious work [20] suggests that RDA does not improve performance when applied to the LSI 
representation. To the best of our knowl­edge, the results given here for LDA and neural networks are 
at least as good as the best routing results published for TREC-2 [4] and TREC-3 [27]. Selection of the 
best routing technique in an operational sys­tem may depend on efficiency as well as IR performance. 
When computed using a Spare 10, the neural network sohrtion requires 3 hours per query, logistic regression 
requires 2-10 minutes per query, LDA requires 0.5-5 minutes, and query expansion (limited to 1000 terms) 
requires considerably less than a minute. This does not in­clude the time to compute the LSI solution 
which is less than 5 min­utes. However, there are several other important factors. One gen­erally assumes 
that the routing query is a standing profile which can be computed once in advance. and is not subject 
to the same time constraints which apply to other search problems. The experimental set-up of the TREC 
routing problem is un­usual in that all the relevance judgments in the training set are presented initially 
rather than coming in graduatly over time. Iter­ative algorithms (and query expansion) are well-equipped 
to deal with new training data as the new solution can be computed from the previous optimal setting 
of the parameters, and convergence times should be much reduced. There also exist updating algorithms 
which can be used to compute a revised solution for linear discrimi­nant analysis. However, the LSI solution 
must be recomputed from scratch, and it is unclear how neural networks would protect against overfitting 
in this context. 6 Query Analysis While the average performance scores presented in the previous section 
are quite informative, they do not provide a complete pic­ture of the experimental results. Similar average 
scores can conceal large differences in performance for individual queries. In this sec­tion, we examine 
the experimental results in more detail on a query by query basis in order to gain a better understanding 
of the ob­served differences between methods and representations. We focus on three specific issues. 
First, when do our classifica­tion techniques perform better (or worse) than relevance feedback via query 
expansion? Second, does the optimal choice of represen­tation depend on some characteristic of the query? 
Third, while lin­ear and non-linear neural networks perform equally well on aver­age, perhaps there are 
individual queries where non-linearity can be helpful. Query expansion vs. Linear neural network. Table 
2 ex­amines the difference between query expansion and the linear neu­ral network with terms as input; 
and presents the queries with the largest differences between the two methods. The neural network performs 
better than expansion in71 of the 100 queries with an av­erage improvement of .047. Note that despite 
the high standard de­viation of .090, the average difference between expansion and the neural network 
(as well as LDA) is significant according to both ANOVA and Friedman test. We hypothesized that the queries 
where expansion was more successful than learning algorithms might be ones where the use of feature selection 
resulted in a loss of information. We tested this hypothesis by looking at the baseline scores for these 
queries using expansion and word based features. However, there was no correla­tion between poor performance 
of the neural networks and poor per­formance of the feature selection algorithms. So far, we have been 
unable to find any patterns that indicate which characteristics of the query (or its relevant documents) 
make it more (or less) amenable to learning atgorittuns. LSI vs. Term Features. Table 3 compares performance 
of the linear neural network for LSI and terms. The queries with the largest differences between the 
two methods are presented. Average precision for LS1 is better for 56 queries and worse for 39 queries 
with 5 ties. Although there is virtually no difference in average per­formance (-0.001 O), the differences 
for individual topics are large: There are 24 topics with a difference of more than s~o. We analyzed 
the top ten documents of four of the topics (51, 133,72, 134) for both representations to determine possible 
reasons for the large individual differences. Topic 51 Airbus Subsidies specifies that relevant articles 
de­scribe either government assistance or a dispute between a Euro­pean and an American manufacturer. 
The term-based method did a better job at capturing this condition in the decision rule. It ranked TREC 
topic A expansion terms 134 The Human Genome Proiect 0.1283 0.5833 0.4550 140 Political Impact of Islarn~c 
Fundamentalism 0.0846 0.2773 0.1927 143 Why Protect U.S. Farmers? 0.0633 0.5686 0.5053 68 Health Hazards 
from Fine-Diameter Fibers 0.0557 0.7203 0.6646 73 Demographic Shifts Across Boundaries 0.0390 0.4568 
0.4178 92 International Military Equipment Sales -0.1815 0.1239 0.3054 144 Management Problems at&#38;e 
United Nations -0.2291 0.1334 0.3625 61 Israeli Role in Iran-Contra Affair -0.2470 0.1871 0.4341 133 
Hubble Space Telescope -0.2914 0.3763 0.6677 51 Airbus Subsidies -0.6363 0.2271 0.8634 mean (100 topics) 
-0.0473 0.3692 0.4165 std. dev. (100 topics) 0.0902 0.2197 0.2159 Table 2: Query expansion vs. linear 
network with terms as input. 10 topics with the greatest differences in non-interpolated average precision. 
 TREC topic A LSI terms 134 The Human Genome Proiect 0.1501 0.6051 0.4550 72 Demographic Shifts in th~ 
U.S. 0.0945 0.4471 0.3526 124 Alternatives to Traditional Cancer Therapies 0.0923 0.5396 0.4473 136 Diversification 
by Pacific Telesis 0.0812 0.5204 0.4392 63 Machine Translation 0.0786 0.5642 0.4856 131 McDonnell Douglas 
Contracts for Militarv Aircraft I -0.0741 0.0494 0.1235 144 Management Pr:blems at the United Nati~ns 
-0.1004 0.2621 0.3625 61 Israeli Role in Iran-Contra Affair -0.1309 0.3032 0.4341 133 Hubble Space Telescope 
-0.1630 0.5047 0.6677 51 Airbus Subsidies -0.2606 0.6028 0.8634 mean (100 topics) -0.0010 0.4155 0.4165 
std. dev. (100 topics) 0.0519 0.2138 0.2159 Table 3: Linear network with input LSI vs. selected terms. 
10 topics with the most marked differences in non-interpolated average precision. TREC topic A non-linear 
linear 61 Israel and Iran-Contra 0.0345 0.4686 0.4341 122 RDT&#38;E of New Cancer Fighting Drugs 0.0309 
0.5240 0.4931 106 U.S. Control of Insider Trading 0.0299 0.2204 0.1905 144 Management Problems at the 
United Nations 0.0192 0.3817 0.3625 77 Poacfing 0.0144 0.5005 0.4861 139 Iran s Islamic Revolution I 
-0.0093 0.1861 0.1954 138 Iranian Support for Lebanese Hostage-takers -0.0152 0.2041 0.2193 60 Merit-Pay 
vs. Seniority -0.0364 0.1962 0.2326 107 Japanese Regulation of Insider Trading -0.0375 0.2051 0.2426 
124 Alternatives to Traditional Cancer Therapies -0.0536 0.3937 0.4473 mean (100 topics) -0.0002 0.4163 
0.4165 std. dev. (100 topics) 0.0240 0.2152 0.2159 Table 4: Linear vs. Non-linew Neural Networks. 10 
topics with the most marked differences in non-interpolated average precision. many relevant articles 
higher than the LSI-based method because they contained good indicators for subsidies or trade conflicts. 
Ex­amples of documents and highly weighted terms indicating subsi­dies or trade conflicts: AP900907-0243: 
u.s.-manufacturer, airbus­industrie; SJMN91 -06169017: competitive-advantage; SJMN91-­06176191: u.s.-aircraft, 
airbus-industrie. Conversely, the LSI­based classifier ranked non-relevant documents high that were about 
government involvement, but not about the precise kind of involvement required by Topic 51, namely subsidies. 
Examp­les include: AP9OO33O-O1O7 and AP900501 -0186 (corruption charges concerning one of Airbus deals 
with India), and SJMN91 ­06320105 (Taiwan government in talks with McDonnell Douglas). Apparently, only 
the exact term features succeeded at differentiat­ing different kinds of government involvement. Relevance 
to Topic 133 Hubble Space Telescope seems to de­pend on a small number of highly weighted terms like 
bubble-­telescope or defect (many articles are about the Hubble s defec­tive mirror). Since theme-based 
features don t capture helpful in­formation in this case, LSI is at a disadvantage in this example. Topic 
72 is about demographic shifts in the U.S. with economic impact. The condition that there be rut economic 
impact of the shift can be expressed in many different ways. In particular, if there are many numbers 
in a text, that is a good indication for economic data. Two such articles that were ranked higher in 
the LSI-based scheme are SJMN9 1-06113204 (population growth in the San Francisco Bay Area) and AP900611 
-0055 (job growth in Crdifornia). In gen­eral, the 200 top-ranked terms included in the term-based represen­tation 
seem insufficient for this topic. For example, the following highty relevant sentence does not contain 
any of these terms: The nation grew to 249.6 million people in the 1980s as more Arnen­cans left the 
industrial and agricultural heartlands for the South and West. Consequently, only the LSI classifier 
ranked the relevant document AP901227-0006 that contains it high. For Topic 72, non-relevant articles 
were ranked high by the term-based classifier even if they did not mention economic conse­quences. The 
following three articles about reapportionment don t cover economic implications, but still receive high 
ranks of 5, 7, and 8: SJMN91-06005094 (Massachusetts loses seat), SJMN91 ­06293056 (Montana loses seat 
to California), SJIVJN91-06137238 (Redistricting in San Francisco Bay Area). There were not enough clues 
in the pool of 200 terms to make reliable decisions as to whether an article covered economics or not. 
Insufficient coverage of the relevant vocabula.ty also seems to explain the poor performance of term-based 
classification for Topic 134 The Human Genome Project . For example, document ZF32-037-119 is about a 
donation by Microsoft to the University of Washington. Due to a passing reference to the Human Genome 
Initiative, it contains many indicators that mislead the term-based classifier which gives it rank 5. 
The LSI-based representation cap­tures the medicrd rather than microbiological theme of the article and 
gives it rank 31. In summary, if there is a small number of good terms that re­liably indicate relevance, 
term-based methods are superior to LSI since an LSI-based classifier can infer the presence of these 
individ­ual terms only indirectly from the LSI features that are linear combi­nations of all terms. In 
contrast, if the number of indicators is large, than LSI is superior because it can integrate information 
from many terms. Linearity vs. Non-linearity. Table 4 looks at the difference in performance between 
linear and non-linear neural networks. The non-linear network performs better than the linear network 
in 43 of the 100 queries (with 11 ties) and the average difference between the methods is basically zero. 
The standard deviation of the differences is only .024 and the extreme differences are relatively small. 
The differences are distributed in a fashion which suggests that they are only the result of noise. To 
obtain further evidence, we examined the most extreme top­ics (61 and 124) a bit more close] y. For topic 
61, there are 35 rel­evant documents in the local region. The non-linear network ranks 19 of them higher 
and there are 3 ties. For topic 124, there are 68 rel­evant documents in the local region. The linear 
network ranks 32 of them higher and there are 3 ties. Neither of these results are close to being statistically 
significant according to the sign test. Since there is no difference in within-query performance for 
even the most ex­treme topics, it is safe to conclude that the non-linear component to the neural network 
provides absolutely no advantage, even for in­dividual queries. For large numbers of input variables, 
there is often no advantage to modeling non-linetity, because there is insufficient training data (even 
in the IR context). 7 Conclusions In this paper, we compare two approaches to document routing, relevance 
feedback via query expansion and statistical classifica­tion with error minimization. We show that advanced 
classifica­tion algorithms perform 10-15% better than relevance feedback on the Tipster document collection. 
Since learning algorithms based on error minimization and numerical optimization are computation­ally 
intensive and prone to overtitting in a high dimensional fea­ture space. it is necessary to apply some 
method of dimensionrd­ity reduction. We examine two different approaches, latent seman­tic indexing and 
feature selection of terms using a Xz -test of non­independence. Our experiments indicate that latent 
semantic indexing is more effective for classification techniques such as linear discriminant analysis 
and logistic regression, which have no way to protect against overtitting. Neurat networks perform equally 
well with ei­ther set of features and can take advantage of the additional infor­mation available when 
both terms and LSI factors are used as input. We also provide evidence that non-linear extensions of 
the classi­fiers (RDA and non-linem neural networks) do not improve perfor­mance, probably because there 
is not enough information in the Tip­ster data collection to accurately learn complex models. Past evidence 
[15] has suggested that a weak learning algorithm (relevance feedback) and a high-dimensional feature 
space (terms) optimizes performance. We interpret the results in this paper as ev­idence that the alternative 
approach, complex learning algorithms and a reduced feature space is both practical and beneficial for 
the routing problem. Acknowledgments. We are indebted to Michael Berry for SVD-PACK, to Marti Hearst 
for implementing the text-tiling algorithm, to Jerry Friedman for advice about regularized discnminant 
analy­sis, and to John Tukey for helpful comments. We would also like to thank three SIGIR reviewer for 
their excellent comments. References <RefA>[1] <SinRef><author>C. Apte</author>, <author>F. Damerau</author>, and <author>S.M. Weiss</author>. <title>Towards language in­dependent 
automated learning of text categorization models</title>. <booktitle>In Proc. 17th Int 1 Conference on R&#38;Din IR (SIGIR)</booktitle>, 
pages<pages> 23 30</pages>, <date>1994</date></SinRef>. [2] <SinRef><author>Rik K. Belew</author>. <title>Adaptive information retrieval: Using a con­nectionist representation 
to retrieve and learn about docu­ments</title>. <booktitle>In Proceedings of SIGIR </booktitle><volume>89</volume>. pages <pages>11 20</pages>, <location>Cambridge MA</location>. <date>1989</date>. </SinRef>
 [3] <SinRef><author>Michael W. Berry</author>. <title>Large-scale sparse singular value compu­tations</title>. <journal>The International Journal of 
SuperComputer Applica­tions</journal>, <volume>6(l)</volume>: <pages>13-49</pages>, <date>1992</date></SinRef>. [4] <SinRef><author>Chris Buckley</author>,<author> Gerard SaJton</author>, and <author>James Allan</author>. <title>The 
effect of adding relevance information in a relevance feedback envi­ronment</title>.<booktitle> In Proceedings of SIGIR </booktitle>
<volume>94</volume>, pages <pages>292-300</pages>, <date>1994</date></SinRef>. [5] <SinRef><author>Wm. S. Cooper</author>, <author>Aitao Chen</author>, and <author>Fredric C. Gey</author>. <title>Full text re­trieval based 
on probabilistic equations with coefficients fitted by logistic regression</title>. pages <pages>57-66</pages>, <date>1994</date>. In </SinRef>[15]. 
[6] <SinRef><author>W. B. Croft</author>, <author>J. Callan</author>, and <author>J. Broglio</author>. <title>Tree-2 routing and ad­hoc retrieval evahtation using the 
INQUERY system</title>. <date>1994</date></SinRef>. In [15]. [7] <SinRef><author>Douglass R. Cutting</author>, <author>Jan O. Pedersen</author>, and <author>Per-Kristian Halvorsen</author>. 
<title>An object-oriented architecture for text retrieval</title>. <booktitle>In Conference Proceedings of RIAO 91, Intelligent 
Text and Image Handling</booktitle>, <location>Ba~elona, Spain</location>, pages <pages>285 298</pages>, <date>April 1991</date>. Also available as <booktitle>Xerox PARC technical 
report SSL</booktitle>­<pages>90-83</pages></SinRef>. [8] <SinRef><author>S. Deerwester</author>, <author>S. Dumais</author>, <author>G. Fumas</author>, <author>T. Landauer</author>, and <author>R. Hadsrnan</author>. <title>Indexing by latent 
semantic analysis</title>. <journal>Journal of the American Society for Information Science</journal>,<volume> 41 (6)</volume>:<pages>391 407</pages>, <date>1990</date></SinRef>. [9] 
<SinRef><author>Susan T. Dumais</author>. <title>Latent semantic indexing (lsi) and tree-2</title>. <booktitle>In The Second Text REtneval Confewtce (TREC-2)</booktitle>, 
pages <pages>105­115</pages>, <date>1993</date></SinRef>. [10] <SinRef><author>Jerome H. Friedman</author>. <title>Regularized discriminant analysis</title>. <journal>Jour­nal of the American 
Statistical Association</journal>, <volume>84(405) </volume>:<pages>165-1 75</pages>, <date>1989</date></SinRef>. [11] <SinRef><author>Norbert Fuhr</author>. <title>Optimum polynomial retrieval funcions 
based on the probability ranking principle</title>. <title>ACM Transactions on Information Systems</title>,<volume> 7(3): </volume><pages>183 204</pages>, <date>1989</date></SinRef>. 
[12] <SinRef><author>Norbert Fuhr </author>and <author>U. Pfeifer</author>. <title>Probabilistic information re­trieval as a combination of abstraction, 
inductive learning, and probabilistic assumptions</title>. <journal>ACM TOIS</journal>, <volume>12(1)</volume>,<date> Jan 1994</date></SinRef>. [13] <SinRef><author>R. Gnanadesikam </author><title>Methods 
for Statistical Data Analysis of A4ultivariate Observations</title>. <publisher>John Wiley &#38; Sons</publisher>, <location>New York</location>, <date>1977</date></SinRef>. [14] 
<SinRef><author>Donna Harman</author>. <title>Overview of the first tree conference</title>. <booktitle>In Pro­ceedings of SIGIR </booktitle><volume>93</volume>, <date>1993</date></SinRef>. [15] <SinRef><editor>Doma Harman</editor>, 
editor. <title>Proceedings of the 2nd Text Retrieval Conference (TREC-2)</title>, <date>1994</date></SinRef>. [16] <SinRef><editor>Doma Harman</editor>, editor, <title>Proceedings 
of the 3rd Text Retrieval Conference (TREC-3), </title><date>1995</date>. to appear</SinRef>. [17] <SinRef><author>Marti A. Hearst</author>. <title>Multi-paragraph 
segmentation of expository discourse</title>. <booktitle>In Proceedings of the 32nd Meeting of the Associ­ation for Computational 
Linguistics</booktitle>, <date>June 1994</date></SinRef>. [18] <SinRef><author>David Hull</author>. <title>Using statistical testing in the evaluation of re­trieval performance</title>. 
<booktitle>In Proc. of the 16th A CM7SIGIR Confer­ence</booktitle>, pages <pages>329 338</pages>, <date>1993</date></SinRef>. [19] <SinRef><author>David Hull</author>. <title>Improving text retrieval 
for the routing problem using latent semantic indexing</title>. <booktitle>In Proceedings of SIGIR </booktitle><volume>94</volume>, pages <pages>282 289</pages>, <date>1994</date></SinRef>. 
[20] <SinRef><author>David A</author>. <title>Hull. Information Retrieval using Statistical Classi­fication</title>. <note>PhD thesis</note>, <institution>Stanford University</institution>, 
1995</SinRef>. [21] <SinRef><author>K. L. Kwok</author>. <title>Experiment with a component theory of prob­abilistic information retrieval based 
on single terms as doc­ument components</title>.<journal> ACM Transactions on Information Sys­tems</journal>, <volume>8(4):</volume><pages>363 386</pages>, <date>1990</date></SinRef>. 
[22] <SinRef><author>David Lewis </author>and <author>Marc Ringuette</author>. <title>A comparison of two learning algorithms for text categorization</title>. 
<booktitle>In Symposium on Document Analysis and Information Retrieval</booktitle>. <publisher>University of Nevada</publisher>, <location>Las Vegas</location>, <date>1994</date></SinRef>. [23] 
<SinRef><author>David D. Lewis</author>. <title>An evaluation of phrasal and clustered rep­resentations on a text categorization task</title>. 
<booktitle>In SIGIR</booktitle>, pages <pages>37 50</pages>, <date>1992</date></SinRef>. [24] <SinRef><author>David D. Lewis </author>and <author>Philip J. Hayes</author>. <title>Special issue on text cate­gorization. 
guest editorial</title>. <journal>ACM Transactions on Information Systems</journal>, <volume>12(3):</volume><pages>23 1</pages>, <date>1994</date></SinRef>. [25] <SinRef><author>Bnj Masand</author>, <author>Gordon Linoff</author>, 
and <author>David Waltz</author>. <title>Classifying news stones using memory based reasoning</title>. <booktitle>In SIGIR</booktitle>, pages <pages>59-65</pages>, <date>1992</date></SinRef>. [26] 
<SinRef><author>P. McCullagh </author>and <author>J.A. Nelder</author>. <title>Generalized Linear Models, chapter 4</title>, pages <pages>101-123</pages>. <publisher>Chapman and Hall</publisher>, 
<volume>2</volume>nd edition, <date>1989</date></SinRef>. [27] <SinRef><author>S. E. Robertson</author>, <author>S. Walker</author>, <author>S. Jones</author>, <author>M. M. Hancock-Beaulieu</author>, and <author>M. Gatford</author>. 
<title>Okapi at tree-3</title>. <booktitle>In Text Retrieval Conference </booktitle><volume>3</volume> (preproceedings), <date>1994</date></SinRef>. [28] <SinRef><author>D. E. Rumelhart</author>,<author> G, E. Hinton</author>, 
and <author>R. J. Williams</author>. <title>Learning internal representations by error propaga­tion</title>. In <editor>David E. Rumelhart</editor>, <editor>James 
L. McClelland</editor>, and <editor>the PDP Research Group</editor>, editors, <booktitle>Parallel Distributed Process­ing. Explorations in 
the Microstructure of Cognition</booktitle>. Volume<volume> I: </volume>Founakztions. <publisher>The MIT Press</publisher>, <location>Cambridge MA</location>, <date>1986</date></SinRef>. [29] <SinRef><author>David 
E. Rumelhart</author>, <author>Richard Durbin</author>, <author>Richard Golden</author>, and <author>Yves Chauvin</author>. <title>BackPropagation The basic theory</title>. In 
<editor>Yves Chauvin </editor>and <editor>David E. Rumelhart</editor>, editors, <booktitle>Back-propagation: Theory, Architectures, and Applications</booktitle>. 
<publisher>Lawrence Erlbaum</publisher>, <location>Hillsdale NJ</location>, <date>1995</date></SinRef>. [30] <SinRef><author>Gerard Sahon </author>and <author>Chris Buckley</author>. <title>Term-weighting approaches 
in automatic text retrieval</title>.<journal> Information Processing and Man­agement</journal>, <volume>24(5):</volume><pages>5 13-523</pages>, <date>1988</date></SinRef>. [31] <SinRef><author>Gerard 
Sahon </author>and <author>Chris Buckley</author>. <title>Improving retrieval perfor­ mance by relevance feedback</title>. <journal>Journal of the American 
Soci­etyfor Information Science</journal>,<volume> 41 (4)</volume>:<pages>288 297</pages>, <date>1990</date></SinRef>. [32] <SinRef><author>Hirrrich Schiitze</author>, <author>Jan O. Pedersen</author>, and 
<author>Marti A. Hearst</author>. <title>Xerox TREC 3 report: Combining exact and fuzzy predictors</title>. <date>1995</date>. In [16], to appear. 
</SinRef>[33] <SinRef><author>Richard M. Tong </author>and <author>Lee A. Appelbaum</author>. <title>Machine learning for knowledge-based document routing (a reprot 
on the tree-2 experiment). </title>pages <pages>253 264</pages>, <date>1994</date></SinRef>. In [15]. [34] <SinRef><author>Enk Wiener</author>, <author>Jarr Pedersen</author>. and <author>Andreas 
S. Weigend</author>. <title>A neural network approach to topic spotting</title>. <booktitle>In Fourth Annual Sym­posium on Document Analysis 
and Information Retrieval</booktitle>, <location>Las Vegas NV</location>, <date>1995</date></SinRef>. To appear. [35] <SinRef><author>Ross WWinson </author>and <author>Philip Hingston</author>. <title>Using 
the cosine mea­sure in a neural network for document retrieval</title>. <booktitle>In SIGIR</booktitle>, pages <pages>202 21 0</pages>, <location>Chicago</location>, <date>1991</date></SinRef>. 
[36]<SinRef> <author>Yiming Yang</author>. <title>Expert network: Effective and efficient learning form human decisions in text categorization 
and retrieval</title>. <booktitle>In Proceedings of SIGIR </booktitle><volume>94</volume>, pages <pages>13 22</pages>, <date>1994</date></SinRef></RefA>.  
			
