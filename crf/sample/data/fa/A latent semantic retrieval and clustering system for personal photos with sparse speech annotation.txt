
 A Latent Semantic Retrieval and Clustering System for Personal Photos with Sparse Speech Annotation 
Yi-Sheng Fu, Winston H. Hsu, Lin-Shan Lee National Taiwan University, Taipei, Taiwan mayaplus@speech.ee.ntu.edu.tw, 
winston@csie.ntu.edu.tw, lslee@gate.sinica.edu.tw ABSTRACT In this demo we present a user-friendly latent 
semantic retrieval and clustering system for personal photos with sparse spontaneous speech tags annotated 
when the photos were taken. Only 10% of the photos need to be annotated by spontaneous speech of a few 
words regarding one or two semantic categories (e.g. what or where), while all photos can be effectively 
retrieved using high­level semantic queries in words (e.g. who, what, where, when) and clustered by the 
semantics as well. We use low-level image features to construct the relationships among photos, but train 
semantic models using Probabilistic Latent Semantic Analysis (PLSA) based on fused speech and image features 
to derive the topics of the photos. The sparse speech annotations serve as the user interface for the 
whole personal photo archive, while photos not annotated are automatically related by fused features 
and semantic topics of PLSA.  Categories and Subject Descriptor: H3.3 [Information Search and Retrieval]: 
Clustering, Retrieval Models General Terms: Algorithms, Experimentation, Management Keywords: Photo retrieval, 
clustering, fused speech and image features, Probabilistic Latent Semantic Analysis (PLSA) 1. INTRODUCTION 
Content-based image retrieval using image features is very successful [1] but not satisfactory for personal 
photos, because users prefer high-level semantic descriptions of photos that use words as indices or 
queries, such as who, where, when, what (objective/events) and so on. This desired scenario can be in 
general achieved by tags entered via networks. But such tags are usually freely entered and not associated 
with any type of ontology or categorization, therefore often inaccurate or ambiguous [2]. In addition, 
for personal photos many tags are personal and thus have to be annotated by the users themselves, such 
as mammy and catty , or my little house . Tagging is also time-consuming. It usually takes 5 to 6 seconds 
for providing a tag in Flickr and ~15 seconds by other designated games (e.g., ESP games); thus most 
users hesitate to tag photos. Automatic tagging such as semantic concept detection [3] is promising, 
but still suffers from relatively low accuracy currently [4]. The above problem is still difficult even 
if the users can annotate photos with spontaneous speech when the photo is taken, because the query and 
tags of its relevant photos may use different sets of words; for example, the annotation may describe 
location (where), but the user may look for a person (who). Assume that photo annotation can be formulated 
into six categories: who, what (object and event), when, where, and others. Copyright is held by the 
author/owner(s). SSCS'09, October 23, 2009, Beijing, China. ACM 978-1-60558-762-2/09/10. When labeling 
a photo, users typically select only one or two categories. As a result, related photos may not be labeled 
using similar words (e.g. some may be labeled by where and some by who), and the relationships among 
words in different categories cannot be trained using latent topics. In addition, tagging each photo 
when it is taken is boring even with spontaneous speech. It will be desired if only very few photos need 
to be tagged. Also, very often a given query results in a large number of photos. For efficient browsing 
it will be highly desired if the large number of retrieved photos can be classified based on meaningful 
groups (or topics ) rather than just sorted by scores. Since different queries result in different sets 
of retrieved photos, pre-defined ontology cannot be used here. Unsupervised clustering for search results 
is therefore in needs. Considering all the above problems, here we propose a user­friendly latent semantic 
retrieval and clustering approach for personal photos with sparse spontaneous speech annotation using 
fused speech and image features. We use image features to derive the relationships among photos, since 
these features are the universal language describing photos. We train semantic models with Probabilistic 
Latent Semantic Analysis (PLSA) using fused speech and image features to analyze the topics of these 
photos. In this PLSA modeling, a document is constructed for each photo by visual words for the image 
and audio words for the spontaneous speech annotation. We have already developed a preliminary retrieval 
system based on this idea [5]. In this paper we will exploit the use of visual words with spontaneous 
speech and investigate feasibility for search result clustering [5]. The "visual words" are constructed 
based on the hessian affine scale­invariant feature transform [6] and the audio words include N­grams 
on the Position Specific Posterior Lattices (PSPL) [7] based on words and subwords (syllables and characters 
here for Mandarin speech) constructed from lattices obtained in spontaneous speech recognition. PSPL 
has been a well known approach for retrieving spontaneous speech information, since the 1-best results 
may include many errors. Using PSPL here is not too difficult here, since the speech tags are very sparse, 
and each tag includes only very few words. So the computation and memory load is not high. For photos 
without speech annotation, the audio words are simply blank.  2. SYSTEM ARCHITECHTURE 2.1 Overview 
of the proposed approach As shown in Fig. 1, the proposed approach includes a preparation phase (left 
part) and a retrieval and clustering phase (right part). Visual words and audio words are first generated 
for each photo (Blocks (B) and (C), lower left of the figure) in the photo archive (Block (A), upper 
left corner). These two types of words are then  Fig. 1. The proposed approach: preparation phase (document 
construction for photos and PLSA model training) and retrieval and clustering phase (based on PLSA) fused 
to construct a document for each photo (Block (D), middle). These documents and their words are then 
used to train a PLSA topic model (Blocks (E)(F)(G), upper middle). The user query then includes only 
very few semantic words in text form. PLSA gives the desired photos (Block(H), right middle), which are 
then further clustered based on the PLSA topics (Block (I), lower right corner).  2.2 Probabilistic 
latent semantic analysis (PLSA) Model Training Probabilistic latent semantic analysis (PLSA) [8] uses 
a set of latent topic variables, {zk ,k = 1,2,..., K}, to characterize the word­ document co-occurrence 
relationships given a set of words, {wj , j = 1,2,....., M} , and a set of documents ,{di , i = 1,2,....N} 
, assuming the document di and word wj are both independently conditioned on an associated latent topic 
zk . The joint probability of an observed pair (di , wj ) is then expressed by the following equations: 
P(wj , d ) =P(di )P(wj | di ) (1) i K P(wj | di ) =.P(wj | zk )P(zk | di ) (2) k=1 All PLSA model parameters 
can be trained using EM algorithm by maximizing a total likelihood function. With the latent topic variables, 
retrieval and clustering can then both be based on the topics rather than the words, so topically relevant 
documents using different set of words can be properly retrieved and clustered. For photos here, however, 
because very few photos are annotated by speech, the obtained latent topics are based primarily on image 
semantics, i.e., photos of the same latent topic look similar, even if they are annotated by tags of 
different categories (e.g. some by where and some by who). In this experiment, we adopt 10K visual words 
and around 60K audio words. 2.3 Latent semantic retrieval and clustering The input query is in text 
form, represented as a sequence of observed words or subword units (syllables and characters here in 
Mandarin Chinese), and the relevance score with respect to each photo is then calculated based on the 
probabilities with respect to Fig. 2. The user interface and an example output for an input query ..(all 
together) . The three rows of photos here show the top three photos in each of the first three clusters 
automatically generated after retrieval. They are all photos for all people, but respectively in the 
opera house , in restaurants , and on the street as the three clusters here. Among the nine photos only 
the second of the first cluster has a speech tag of ...... (all in opera house together) . All other 
eight photos are not tagged at all, but can be properly retrieved and clustered each PLSA topic. The 
central idea of PLSA-based latent semantic retrieval is that a query and a document may have a high relevance 
score even if they do not share any words in common, as long as they share the same latent topics. The 
retrieval model is detailed in [5]. Similarly, the retrieved photos are further clustered based on the 
probabilities for the latent topics. The user interface of the system and an example output are shown 
in Fig 2. The evaluation of clustering requires more investigation. We are now evaluating the system 
performance in more details.   3. REFERENCES <RefA>[1] <SinRef><author>John R. Smith</author>, <author>Shih-Fu Chang</author>, <title>VisualSEEk: a fully 
automated content-based image query system</title>, <journal>ACM Multimedia </journal><date>1996</date></SinRef>. [2] <SinRef><author>L. Kennedy </author><author>et al, </author><title>How Flickr helps 
us make sense of the world: Context and Content in community-contributed media collections</title>, <journal>ACM Multimedia</journal>, 
<pages>pp. 631-640</pages>, <date>2007</date></SinRef>. [3] <SinRef><author>Milind Naphade </author><author>et al</author>, <title>Large-Scale Concept Ontology for Multimedia</title>, <journal>IEEE Multimedia 
Magazine</journal>, <date>2006</date></SinRef>. [4] <SinRef><author>Rong Yan</author>, <author>et al</author>, <title>A Learning-based Hybrid Tagging and Browsing Approach for Efficient 
Manual Image Annotation </title>, <journal>CVPR </journal><date>2009</date>. </SinRef>[5] <SinRef><author>Yi-Sheng Fu</author>, <author>Chia-Yu Wan</author>, and <author>Lin-Shan Lee</author>, "<title>Latent semantic 
retrieval of personal photos with sparse user annotation by fused image/speech/text features</title>", <journal>ICASSP </journal>
<date>09</date></SinRef>. [6] <SinRef><author>Josef Sivic </author>and <author>Andrew Zisserman</author>, "<title>Video Google: a text retrieval approach to object matching 
in videos</title>", <journal>ICCV' </journal><date>03</date></SinRef>. [7] <SinRef><author>C. Chelba </author>and <author>A. Acero</author>, <title>Position specific posterior lattices for indexing speech, 
in ACL</title>, <journal>Ann Arbor</journal>, <date>2005</date>, <pages>pp. 443-450</pages></SinRef>. [8] <SinRef><author>T. Hofmann</author>, <title>Probabilistic latent semantic indexing</title>, <journal>Proc. ACM 
SIGIR Conf.R&#38;D in Informational Retrieva</journal>,<date>1999</date></SinRef></RefA>.   
			
