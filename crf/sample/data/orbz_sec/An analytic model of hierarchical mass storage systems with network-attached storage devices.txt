
 An Analytic Model of Hierarchical Mass Storage Systems with Network-Attached Storage Devices Daniel 
A. Menasc6 Odysseas I. Pentakalos Yelena Yesha Dept. of Computer Science Dept. of C.S. and E.E. George 
Mason University University of Maryland Baltimore County Fairfax, VA 22030 Baltimore, MD 21228 menasce@cs.gmu.edu 
odysseas@cs.umbc. edu yeyesha(lcs.umbc. edu Abstract Network attached storage devices improve 1/0 per­formance 
by separating control and data paths and elim­inating host intervention during data transfer. Devices 
are attached to a high speed network for data trans­fer and to a slower network for control messages. 
Hi­erarchical mass storage systems use disks to cache the most recently used files and tapes (robotic 
and manually mounted) to store the bulk of the files in the file system. This paper shows how queuing 
network models can be used to assess the performance of hierarchical mass stor­age systems that use network 
attached storage devices. The analytic model validated through simulation was used to analyze many different 
scenarios. 1 Introduction Most current mass storage systems at national laborato­ ries and supercomputer 
centers are based on the server attached storage model in which all storage devices are attached to a 
single machine using high speed busses and 1/0 channels. A request for access to a storage object arrives 
at the server which transfers the data, through the server s main memory, to or from the de- Permission 
to make digital/hard copy of part or all of this work for personal or classroom use is granted without 
fee provided that copies are not made or distributed for profit or commercial advantage, the copyright 
notice, the title of the publication and its date appear, and notice is given that copying is by permission 
of ACM, Inc. To copy otherwise, to republish, to post on servers, or to redrstr(bute to lists, requires 
prior specific permission and/or a fee. SIGMETRICS 96 5/96 PA, USA e 1996 ACM 0-89791 -793 -6/96 /0005 
. .. S3.50 vices attached to the server. As the arrival rate of re­quests increases, the server may become 
a bottleneck since all data needs to flow through its main memory. To overcome this problem the new paradigm 
of net­work attached storage has been emerging in the de­sign of mass storage system hierarchies [1]. 
Under this paradigm, storage devices are directly attached to the network. Hierarchical mass storage 
systems, such as the UNI-TREE [2] and HPSS [3], use a large disk cache to store the files that have been 
used more recently, and roboti­cally mounted tapes to store the bulk of the files in the file system. 
The two main operations on a mass storage system are file storage (put) and retrieval (get). Files stored 
in the mass storage system are placed on the disk cache and migrate to the tapes if they are not used 
for some specified time. A get may be satisfied from the disk cache (a cache hit) or from the tape via 
the disk cache (a cache miss). In previous works [4], the authors have used queu­ing network (QN) based 
models to assess the perfor­mance of hierarchical mass storage systems that use host-attached devices. 
In this paper we show how queu­ing network models can be used to study the perfor­ mance of hierarchical 
mass storage systems that use network-attached storage devices. Approximate Mean Value Analysis (MVA) 
is used as a solution technique. The approximations introduced here are needed to cap­ ture the simultaneous 
resource possession [5] that occurs when a file has to be transferred from a tape read/write station 
to a network attached disk. Since the tape has to be mounted during the whole transfer, the tape read/write 
station is used concurrently with the disk and the high speed network used for the transfer. Sim­ ulation 
of a simpler subsystem was used to validate our approximation. Simulation of the complete system with 
all its workloads and devices was used to validate the complete analytic model. The rest of the paper 
is organized as follows, Section two describes the general operation of network-attached storage devices 
including HIP PI, IPI-3, and the trans­fer protocols considered in this paper. Section three describes 
the QN model, the workload characterization, the model parameters and service demand equations, and the 
approximation used to model tape to disk trans­ fers. Section four describes the model validation and 
section five provides numerical results. Concluding re­ marks are given in section six.  2 NAD Operation 
A typical architecture of a NAD based mass storage system is shown in Fig. 1. It consists of a File Server 
SforageUnitControlNetwork Figure 1: Network attached device mass storage system with a Host Attached 
(HA) Storage Device, a Storage Access Control Network (SACN), a Storage Unit Con­trol Network (SUCN), 
a High Speed Data Network, and a number of Network-Attached (NA) Storage Servers. The File Server manages 
the storage of files in the stor­age system. It is responsible for locating files within the system, 
performing name resolution, authorization, and manages the flow of files among the different levels of 
the mass storage system. Clients issue gets and puts through the SACN. The File Server controls the oper­ation 
of the NADs through the SUCN. Finally, data flows between the File Server and a NAD through the High 
Speed Data Network. In many cases, the Storage Access Control Network and the Storage Unit Control Network 
are the same Ethernet network, whereas the High Speed Data Network is a point-to-point HIPPI connection. 
Associated with each NA Storage Server is a Mover process. The mover runs on a processor that controls 
the NA devices in the server and controls the flow of data between the two devices involved in a trans­fer. 
In the model described in section 3, we assume that the disk cache is implemented by a Network Attached 
Disk Server (NADS) with several disks and that the tape level of the hierarchy is implemented by a Net­work 
Attached Tape Server (NATS) with several robot­ically mounted tape read/write stations. There are al­ready 
commercially available RAID-5 NA disks [6]-[7] but there are no NA Tape Drives on the market. How­ever, 
they are expected to become available pretty soon. 2.1 HIPPI and IPI-3 protocols The High-Performance 
Parallel Interface (HIPPI) is aL simplex point-to-point interface with a data transfer rate of 800 Mbit/see 
and 1600 Mbit/see based on a 32 and 64 bit word size, respectively. Before data can be transferred, the 
source must first establish a connection with the destination by placing the address of the desti­nation 
on the bus. Once the connection has been estab­ lished, a packet of data can be transferred in the form 
of multiple bursts. The source indicates that it is ready to transfer a burst and the destination must 
send an acknowledgement, thus allowing for flow control. After all data is transferred, the connection 
is broken. Note that a file transfer over a HIPPI switch may imply many rounds of connect, transfer, 
and disconnect. Among other fields, the HIPPI frame includes two data fields D1 and Dz. DI is intended 
for transferring user control information. By limiting the size of the header plus the Dl area to 1024 
bytes, the command can be transferred to the destination within the first burst and be delivered and 
processed while the data stored within the Dz area is still being transferred using additional bursts 
[8]. The Intelligent Peripheral Interface (IPI-3) [7] is a device independent protocol which defines 
a generic command set for data transfer to and from magnetic and optical disk drives, allowing vendors 
to use a sin­gle device driver for communicating with a number of devices. 2.2 Transfer Protocols This 
section describes third-party protocols for execut­ing data transfer transactionsin amass storage system 
with NADs. Three protocols will be described based on whether the transfer is device-to-client or device-to­ 
device and whether it is source or sink initiated. A device-to-client protocol is used when data is transferred 
between the File Server (a client in this case to the NA Disk Server) and aNA Disk device, whereas a 
device­to-device protocol is used when thetransfer is between a NA Disk device and a NA Tape device. 
In all pro­tocols the control messages are sent over the Storage Unit Control Network (SUCN) while the 
data transfer messages are sent over the High Speed Data Network (HSDN). We assume that, as in most existing 
sites with NADs, the SUCN and the SACN are implemented by a single local area network. Also, the HSDN 
in this case is assumed to be a HIPPI network. The following ab­breviations are used heretofore: File 
Server (FS), File Server Disk Mover (FSDM), NA Disk Mover (NADM), and NA Tape Mover (NATM). Read from 
a NA Disk to the File Server. The se­ quence of messages in this case is: 1) a user sends a read request 
to the FS over the SUCN. 2) the FS processes and forwards the request to the NADM over the SUCN. This 
request indicates the direction of data flow and the name of the file to be transferred. 3) the NADM 
pre­ pares for the move, assigns a Transfer Identifier (TID) that the FSDM will use to identify the transfer, 
and sends a response back to the FS over the SUCN con­ taining the TID assigned. 4) the FS sends an IPI 
third­ party read request to the FSDM. This read request in­ dicates the direction of data flow, the 
amount of data to be transferred, and the TID. Steps 5 and 6 are re­ peated for each block until all 
the data has been sent. 5) the FSDM prepares for the move and then sends an IPI-3 read command message 
within the DI area of the HIPPI frame over the HIPPI channel. 6) the NADM when ready, sends an IPI-3 
response within a HIPPI frame. The D1 area contains the transfer notification message and the Dz area 
contains a data block. 7) once the file has been transferred, both movers send comple­ tion messages 
back to the FS. Write from the File Server to an NA Disk. When files are stored into the hierarchical 
mass storage system they are always stored in the disk cache first. The se­quence of messages in this 
case is: 1) a user sends a write request to the FS over the SUCN. 2) the FS processes and forwards the 
request to the FSDM. This request indicates the direction of data flow and the amount of data to be transferred. 
3) the FSDM prepares for the move, assigns a TID that the initiator will use to iden­tify the transfer, 
and sends a response back to the FS containing the TID. 4) the FS sends a write request to the NADM over 
the SUCN indicating the direction of data flow, the amount of data to be transferred, the TID to be used 
to identify the transfer. Steps 5 and 6 are repeated for each block until all the data has been transferred. 
5) the NADM prepares for the move and sends an IPI-3 read command message within the D1 area of the HIPPI 
frame over the HIPPI channel. 6) the FSDM, when ready, sends an IPI-3 response within an HIPPI frame. 
The DI area contains the transfer notifi­cation message and the Dz area contains a data block. 7) once 
the file has been transferred, both movers send completion messages back to the FS. Read from a NA Tape 
to a NA Disk. When a file is not found in the disk cache, it has to be moved from the tape level to the 
disk cache before it can be retrieved by a user. The sequence of messages in this case is: 1) a user 
sends a read request to the FS over the SUCN for a file stored in a tape device. 2) the FS processes 
and forwards the request, which indicates the direction of data flow and the name of the file, to the 
NATM over the SUCN. 3) the NATM prepares for the move, assigns a TID that the NADM will use to identify 
the transfer, and sends a response back to the FS over the SUCN containing the TID assigned. 4) the FS 
sends a request, which indicates the direction of data flow and the TID, to the NADM over the SUCN. Steps 
5 and 6 above are repeated for each block until all the data for the requested file has been sent. 5) 
the NADM sends an IPI-3 read command to the NATM over the HIPPI channel. The IPI-3 command resides within 
the D1 area and the Dz area is empty. 6) the NATM, when ready to transmit a block, sends the block of 
data in an IPI-3 response with the transfer notification message in the D1 area and the data block in 
the Dz area. 7) both movers send completion messages back to the FS over the SUCN. 182 Table 1: Workload 
characteristics class File Size Frequency of in MB Occurrence gl 1.2 33.8% gz 19.6 9.9% 93 78.9 4.2% 
g4 220.6 1.4% pl 1.7 42.3% p2 34.8 3.3% P3 77.7 3,9% p4 144.1 1.2%  3 NAD Performance Model We first 
present the characteristics of the workload used to drive the NAD performance model, then we discuss 
the actual QN model, and equations used to compute the service demand parameters, as well as a modelling 
approximation required to handle tape to disk transfers. 3.1 Workload Characterization The workload defined 
by the authors in a previous study [4] for a host-attached based hierarchical mass storage system was 
used to drive the QN model devel­oped here. The data source for workload characteriza­tion was the log 
of ftp get and put requests for a period of ten days. First, histograms were generated depicting the 
number of get and putrequests for each hour of the day. Based on these histograms, it was determined 
that the interactive load on the system achieved its peak be­tween 9:00 am and 6:00 pm for all days considered. 
A k-means clustering algorithm [9] was used to determine the appropriate number of classes for each type 
of re­ quest (get or put) and the average file size for each class. Data for ten days of get and put 
requests was used for the characterization. Table 1 shows the file size of each class and the frequency 
of occurrence of each class out of a total of 3691 requests observed during the measure­ment period. 
Four classes of get requests (gl-g4) and four classes of put requests (PI-PA) were chosen. 3.2 The Model 
The performance model consists of a closed multiple class QN with load dependent devices. Each class 
in the model is associated with one of the get or put classes described in subsection 3.1. Since transfers 
from a NA time use the HIPPI switch or the disk, we had to make some modifications to the MVA equations 
to account for this instance of simultaneous resource possession as described later. User workstations 
are represented by a delay device which models the user think time. The SACN and the SUCN are assumed 
to be implemented by the same Ethernet LAN which is modeled as a loacl­dependent device since its service 
rate depends on the load imposed on it [9]. The File Server is modeled by a CPU device and by a host-attached 
disk device. A queueing device represents the HIPPI switch whiclh connects the NADs to the File Server 
and only allows a single point-to-point connection between any of th,e three ports shown in Fig. 1. Each 
NA server (NA disk and NA tape) is represented by two types of queuing devices: one represents the CPU 
where the mover op­erating system and local file system runs and the other represents the actual NA device. 
There are as many queuing devices of the latter type as there are networlc attached devices in a NA server. 
 3.3 Input Parameters Using the centroids for each class determined in sec­tion 3.1, the think time 
Zc of class c was measured from the FTP log file and the inter-arrival time was computed. using the average 
time between arrivals of the same class for each of the classes considered. The values in seconds computed 
were Z~, = 30.0, Z9, = 96.1, Z9~ = 246.7, Zgb = 735.7, 2P, = 23.3, 2P, = 308.5, 2P, = 246.7, and 2P, 
= 881.0. Table 2 describes the remaining input parameters to the model with their values. 3.4 Computation 
of Service Demands This section describes the equations used to compute the service demands for each 
device of the QN model using the parameters described in section 3.3. The no­tation D~,c stands for the 
service demand (i.e., total service time) of class c requests at device i. The service demand at the 
delay device that repre­sents user workstations for class c is equal to the think time Zc. The service 
demand, DL,4N,C, for class c at the Ethernet LAN depends on the class and on the hit ratio, ph, at the 
disk cache because the number of messages Table 2: Model input parameters Parameter Description Value 
Frame Size size of an Ethernet frame 1518 bytes TRatee Ethernet transfer rate 10 Mbps Block Sized NA 
and HA disk block size 8Kbvtes BlockSize+ I NA ta~e block size I 16Kbvtes II I CtrlSi.ze size of a HIPPI-FP 
frame with an emtpy D2 area 1024 b~tes DataSize~ I D~ area size of a HIPPI frame containing a dk.k block 
BlockSized DataSizet 1 D, area size of HIPPIa frame containing a tape block BlockSizet Tltateb I transfer 
rate of the [PPIHI channel 100 Mbvtes/sec .F . ,,. I CPU time at the FS per 64~--d .­ -.. Seek I average 
seek time of the NA and HA dkks 3Latency TRated t mOweT average latency of the NA and HA disks transfer 
rate of the NA and HA disks time needed to process a transfer request by the mover 6 msec 8MBytes/sec 
884.6 msec Mount ., average time for a tape dismount, wedfollov bv. a mount , 71.9 sec Seekt average 
seek time to locate a file in the tape 20 sec TRate+ transfer rate of a NA tave read/write station 6.0 
MB/see transmitted over the LAN depends on the protocol used to service the request (see section 2.2). 
Thus, DLAN,, for c 6 {gl, g2, g3, g4} is: DLAN,. = {z% NFramesd + FrameSize (1 ph) NFramesJ ~Rate e 
where Nl%amesd is the number of frames transmitted when the request is served by an NA disk (disk cache 
hit) and NFramest is the number of frames transmit­ted when the request has to be served from the NA 
tape (disk cache miss). The values of NFramesd and NFramest can be obtained by inspecting the protocols 
described in section 2.2. For a request served by an NA disk, one frame contains the request, two frames 
setup the transfer, and one frame indicates transfer comple­tion. So, NFramesd = 4. Notice that messages 
trans­mitted internally to the File Server are ignored since the time needed to send them is negligible 
if compared with the other times involved. For a request served by an NA tape, one frame contains the 
request, three frames setup the transfer, two frames indicate completion of the NA tape to NA disk transfer, 
two frames initiate the NA disk to HA disk transfer, and one frame indi­cates transfer completion. Thus, 
NFramest = 9. The service demand ~LAN,c for c = {PI, P2, P3, P4} is: FrameSize DLAN,C = NFramesW ~Rate 
e For a put request, NFramesW = 3: one frame contains the request, one frame initiates the transfer, 
and one frame completes the transfer. The service demands of the HIPPI device also de­pend on the class 
and on the disk cache hit ratio because the number of messages transmitted depends on the file size of 
the class making the request and on whether the file needs to be staged to a NA disk before being trans­ferred 
to the HA disk. The service demand DHIPPI,C for c E {91,92,93,94] is: DHIPPI,. = p~ @~PP1,c+(l-P~) (%IPPI,. 
+~g;PPI,.) where Dg~ppl,c is the portion of the service demand due to a transfer from the NA disk to 
the file server HA disk when there is a disk cache hit and Dj$lppl,C is the portion of the service demand 
due to a transfer from a NA tape read/write station to a NA disk in the case of a disk cache miss. The 
expressions for D#~ppl,C and D~lppl,C are: FileSizeC 2 CtrlSize -1-DataSized %FW>C = r BlockSized 1 
TRateh FileSizec 2 CtrlSize i-DataSizet Dj$IPpI,. = BIOC~S~zei l Rateh (1) The service demand equation 
for put requests does not depend on the hit ratio of the NA disk since files are always written to the 
disk cache. So, the service demand DHIPPI,C for c ~ {PI, P2 , P3 , P4} is: FileSizec 2 6 trlSi.ze + DataSized 
DHIPP1,= = ~BlockSized] l Rateh The service demand equation DCPU,C at the CPU device for all classes 
c is Dcpu,c = tcpu,ftpo + tcpu,fs + tcpu,ita + 1 DataSized +tcpu,hb [fi;;&#38;;;d Notice that the hit 
ratio does not affect the part of the equation that depends on the file size since, in the case of a 
disk cache miss, the NA tape to NA disk transfer is a device to device transfer and does not impose any 
load on the CPU of the File Server. The service demand equation for the HA disk also does not depend 
on the hit ratio of the NA disk for the same reason described before. The service demand DHAD.C at the 
HA disk device HAD for all classes c is The function #(c) gives the average time spent in seek and latency 
per block read/written for class c files at a disk. We assume that the first block of the file will re­ 
quire an average seek and an average latency while the remaining blocks will require a seek and latency 
with probability p,l (we assume p~l to be 0.1 in all our ex­ periments). Thus, The service demand DNADM,C 
for the NA disk mover (NADM) depends only on the class of the re­ quest. If the read request is a hit 
at the NA disk cache then only one transfer takes place whereas on a miss the file goes from the NA 
tape to the NA disk and from c ~ {91)92,93)94} is: DNADM,C = ph tmOVeT +2(1 ph) tmover= (2-ph) tmov,,r 
The service demand DNADM,C for put classes does nc)t depend on the hit ratio since a put request involves 
a single transfer by the NADM and is given by: DNADM,C = tmovev. The service demand for the NA dhik 
server depends both on the class of the request and on the disk cache hit ratio. The service demand DNAD,C 
at each disk of the lVA disk server for class c E {gl, g2, g3, g4} is a function of the service demand 
D~AD,c of class c requests when there is a disk cache hit and, the service demand D~AD,,, when there 
is a miss in the disk cache. So, DNAD,C = p~ D~AD,C + (1 ph) where, assuming that requests are equally 
likely to use any of the iVd disks of the NA disk server. The factor 2 in Eq. 2 comes from the fact that 
a miss requires both an NA tape to NA disk transfer and an NA disk to HA disk transfer. The service demand 
DNAD,C at device NA disk fOr c ~ {PI ,p2, p3, p4} is: since a write always requires a transfer from the 
HA disk to the NA disk. The service demand DNATM,C at the NA tape mover device (NATM) depends both on 
the class of the request and on the hit ratio and is given by the equation below for class c E {91,92,93,94}. 
DNATM,C = (1 -ph) tmover A request arrives at the NA tape mover only if the file is not located at the 
NA disk cache. The service demand DNATM,C at device NATiW for c G {PI, P2, P3, P4} is zero since puts 
never go explicitly to the NA tape device. The service demand DNAT,. for the NA tape (NAT) device depends 
on the hlt ratio and on the class of the request and is given by: DNAT,C = (1 ph) [Mw~t + Seekt + Fi2eSize 
+Bk3xe (Bkmksiz:t 11 (3) for c c {gl, gz, g~, g~}. The next subsection discusses how tape to disk transfers 
are modeled.  3.5 Modeling Tape to Disk Transfers As already mentioned, a file transfer from a NA tape 
to a NA disk exhibits simultaneous resource possession [5] since once a tape is mounted for a tape to 
disk transfer, the tape read/write station cannot be used for any other transfers until all blocks of 
the file have been transferred. Thus, the tape read/write station is held by a file trans­fer request 
at the same time that the HIPPI switch or the disk are used during the transfer. This is illustrated 
in Fig. 2 that shows the tape residence time as a func­tion of time intervals at the tape read/write 
station, HIPPI switch, and disk for a sequence of n blocks being transferred from the tape to the disk. 
To model this tqe residencetime Q+Ss ss Tape. ......................... ............................ 
..... _ .............t (1) , ;(2) 1 i(3)1 (n)I I II t t Ill I1 : :11I II I I1,.. 1 I 1, 1 111 1 1, 
!, 1 Ill I I , Q+s QJ [I&#38;q W; : [Q+3 : IUPPI r I (1) ,I(1) (2)t ;(3) (n)I !1 II 11i 1I 1 ... ;!1I1 
1 I1 1 II II 81! 11 1I I II 1I1 Disk w Q+s Q+s (n) 0) (2) S. servicetime Q+S. queuingplusservieetime 
(i)= i.ti block transferred Figure 2: Simultaneous resource possession situation, we modified the MVA 
residence time equa­tion at the tape devices to account for the fact that the tape is held by a transfer 
while the disk and the HIPPI switch are used. The residence time at the tape is equal to the effec­tive 
service demand at the tape plus the waiting time to acquire the tape drive for the first time. The effec­tive 
service demand at the tape read/write station is defined aa the sum of the actual service demand at the 
tape DNAT,C, given by equation 3, plus the disk service demand, plus the total waiting time at the disk 
for all blocks, plus the service demand at the HIPPI switch, plus the total waiting time at the HIPPI 
switch for all blocks. The waiting time at the disk is equal to the disk service demand multiplied by 
the average number of re­quests found at the disk upon arrival of a tape to disk transfer request. Using 
Arrival Theorem [10] like argu­ments, this number is equal to the steady state queue length at the disk 
when we remove all customers that cannot be found at the disk queue by the arriving re­quest. The same 
argument can be made for the HIPPI switch. If there is more than one tape read/write sta­ tion, an arriving 
request at the disk may find customers of all classes queuing for the disk. If there is only one tape 
read/write station, all requests that need to use both the tape and the disk will be queued at the tape 
and there will be no queuing at the disk due to these requests. Requests that do not need the tape will 
queue at the disk. So, the residence time, R~AT,C (N) for class c ~ {91, gz, gs, ga} requests at a NA 
tape read/write station is given by, RkAT,.(~) = [DNAT,. + RhADmia. ,.(N) + R~1pp1mi=8 ,C(fi)][l + ENAT(fi 
-It)] (4) where N is the vector of customer population per class in the QN model, fii (N IC) stands 
for the average queue length at device i when there is one less cus­ tomer of class c in the QN, R~AD~,8, 
,C(fi) is the por­ tion of the residence time of class c requests at a NA disk due to tape to disk transfers 
generated by disk cache misses, and R~lPP1mi.8 ,C(l?) is the portion of the residence time of class c 
requests at the HIPPI switch due to tape to disk transfers generated by disk cache misses. So, the MVA 
equations for R~AD~%8, ,C(N) and kIPPImi$$ ,C(fi) are: R~xD~,,~,C(fi) = [(1 Ph) NjD c] [1 +?7NAD(17 -i.)] 
 R&#38;IPPIm,aa,c(fl) = [(l-ph) @IpPI,C] [l+EHIPPI(fi-iC)] where D~lPP1,c and D~~~,c are given by Eq. 
1 md 2, respectively. As mentioned before, we assume that re­quests are equally likely to use any of 
the IVd disks of the NA disk server, Therefore, fijvAD (N) is the average queue length at any one of 
them. The transfer time (response time) equation also has to be modified to eliminate the terms R&#38;ADm,,= 
,.(fi) and RkIpPIm{~, ,C(~) that would otherwise be counted twice.  Numerical Results Two process 
oriented simulation models were developed using CSim to validate the accuracy of both the approx­imation 
for modeling tape to disk transfers, and the analytic model of the entire hierarchical mass storage system. 
The approximation was validated using three classes for a multiprogramming level varying from 1 to 15. 
The error was less than 10 % for all cases but two, where it was 1170. The complete model simulated in­cluded 
a file server with four disks, one NA disk server with 96 disks, four NA tape servers with six tape drives 
each, a user workstation, a HIPPI switch, and a LAN network. The customer population in this simulation 
also varied from 1 to 15. The error in this case was less than 10% for all cases. The validation simulations 
with tables showing all the results can be found in [12]. The validated model was used to study several 
scenarios. Three studies are conducted with the use of the an­alytical model. The first shows how the 
transfer time for put and get classes varies with the disk cache hit ratio. The second shows how the 
transfer time varies as the load of get and put classes increases. Finally, the third study shows the 
impact of increasing the number of disks at the NA disk server. In all the studies dis­cussed the transfer 
time is defined as the sum of the residence times at all devices except the delay device. Viirying the 
Hit Ratio. In Figs. 3 and 4 we show how the transfer time varies with the disk cache hit ratio for get 
and put classes, respectively, An analysis of the model output shows that for low values of the hit ratio, 
the bottleneck is the NA disk cache, followed by the NA Transfer time (see) versus p~ T 16000 94 +-­ 
14000: 93 -1­n 12000 s 92 -E-­f 10000 91 ++­e 8000 r 6000 T 4000 i 2000 m e0 0 0.2 0.4 0.6 0.8 1 Ph 
Figure 3: Transfer time of get classes vs p~ tape, and then by the FS disk, It should be remembered 
that even when the hit ratio is very small, the NA disk is still used by all get requests since files 
have to be staged at the NA disk before being transferred to the FS. Af­ter pfi exceeds 0.7, the second 
bottleneck shifts to the FS disk and the third becomes the NA tape. Finally at = 1.0, the bottleneck 
becomes the FS disk foilowed by the NA disk. The tape plays no role at this point since it is not used 
for this value of the hit ratio. The output of the model also reveals that i) the congestion Ph factor 
(defined as the ratio between residence time and service demand) of a get request at the FS disk increases 
continuously with the hlt ratio, ii) the congestion factor at the NA tape device decreases continuously 
with the hlt ratio, and iii) the congestion factor at the NA dkk increases up to a value of ph equal 
to 0.6 and then starts to decrease. This happens because after this point, the File Server disk congestion 
increases to a point where more and more requests tend to accumulate there as op­posed to accumulating 
at the NA disk, thereby lowering the queuing time at the NA disk. For the put classes (Fig. 4), the transfer 
time always increases with the hit ratio. The congestion has a similar kind of behavior for the FS and 
NA disks for the put classes. For values of Ph up to 0.6, the congestion factor for both NA disk and 
FS disk increases. After this point, the decrease in the congestion factor of the NA disk is not sufficient 
to offset the increase in the congestion factor of the FS disk. Therefore, the transfer time increases 
with p~ in the entire range from O to 1.0. For both Figs. 3 and 4 we used a load factor ac (defined m 
the maximum number of outstanding requests of class c) equal to 15 Figure 4: Transfer time of put Ckis.%s 
Vs Ph Transfer time (see) versus p~ T 6000 P4 + : n 5000 P3 + s f 4000 P2 PI + * e r 3000 2000 B T i 
1000 m e O 0.2 0.4 0.6 0.8 1 Ph Transfer time (see) versus a T 3500 : 3000 i 2500 f 2000 e r 1500 ~ 
1000 i 500 m eo 05 10152025 cx~ Figure 5: Transfer time of get classes vs load factor for all classes. 
Varying the Load Factor. Figures 5 and 6 display the variation of the transfer time as a function of 
the load factor ac, respectively. The experiment was repeated for each class c, varying the population 
of class c from 1 to 25 while keeping the population of the other classes fixed at 1. As expected, the 
transfer time increases steadily with the load factor. The hit ratio assumed in both figures is 0.3. 
Since the hit ratio is reasonably small (but typical of mass storage systems according to our measurements), 
most get requests will need to go to the NA tape. Smaller file requests (classes gl through 9s) will 
tend to queue at the NA tape behind large file requests (class g4) since the tape has to be allocated 
to a single request for the entire duration of the transfer. This behavior is portrayed in Fig. 5. Since 
put requests do not use the NA tape, the transfer time curves are more evenly spread out in the graph 
of Fig. 6. The Figure 6: Transfer time of put classes vs load factor Transfer time (see) versus a T 1000 
900 800 700 f 600 e r 500 400 300 T 200 i m 100 e 0 0 5 10 15 20 25 a~ Transfer time (see) vs. no. of 
NA disks T 12000 r 94 -+­; 10000 93 + s 92 -Et­f 8000 91 * e rR 12345678910 Number of NA Disks Figure 
7: Transfer time of get classes vs # of NA disks transfer time is largely influenced by the congestion 
at the NA disk for the put classes. Varying the Number of NA Disk Server Disks, Figure 7 shows the variation 
of the transfer time as a function of the number of disks in the NA disk server for get classes. The 
hit ratio is assumed to be 0.3 and the load factor is 15. For classes g3 and g4, the per­centage of time 
spent at the NA disk drops significantly (from 49% to 2% for g3 and from 66% to 3% for g4) as the number 
of disks increases from one to two and the percentage of time spent at the NA tape increases (from 46% 
to 64% for g, and from 27% to 44% for g.). However, the increase in NA tape residence time is not sufficient 
to offset the decrease in congestion at the NA disk. Therefore the total transfer time decreases. Af­ter 
the number of NA disks increases beyond two, there is no significant variation in the percentages of 
times spent at each device, and therefore, the total transfer While classes g3 and g4 spend most of 
their time at the NA disk when the number of disks is equal to one, the bottleneck for the small file 
classes (gl and g2) is the NA tape since they have to queue at the tape behind large file transfer requests. 
Therefore, an increase in the number of disks from one to two, aggravates the bottleneck at the tape 
and increases the transfer time. After two disks, the distribution of the transfer time among the NA 
disk and the NA tape remains pretty much constant. When the number of disks increases from one to two, 
the bottleneck shifts to the FS disk, and the transfer time does not change any longer with an increase 
in the number of NA disks. Put classes have a similar behavior as get classes when the number of NA disk 
server disks va-ies. Concluding Remarks This paper presented a very accurate analytic model of a hierarchical 
mass storage system using network attached storage devices. The model is based on approximations to standard 
multiclass MVA. The approximations deal with the simultaneous resource possession that occurs when files 
are transferred between network attached tapes to network attached dkks across a HIPPI switch. The approximations 
were first validated with the use of simulation and proved to be accurate at a 10% level. The workload 
used to drive the model was derived from a workload characterization study conducted at a large scientific 
installation that uses a mass storage system with host-attached storage devices. The model devel­oped 
here was used to analyze different situations such as the variation of the disk cache hit ratio, system 
load, and number of disks in the network attached disk server. The results of other analyses are not 
shown here due to space limitations. Acknowledgements Daniel Menasc6 was partially supported by CESDIS. 
We thank Chris Wood (Maximum Strategy, Inc.) for providing documentation on the GEN-5 line of products 
and Sanjeev Setia (GMU) for suggesting us to develop an analytic model for NADs.<RefA> [1] R. Katz, High-Performance 
Network and Channel Based Storage , Proceedings of the IEEE, vol. 80, no. 8, pp. 1238 1261, August 1992. 
[2] Convex Computer Corporation, Unitree++ Sys­ tem Administration Guide, First Edition, Conve~ Press, 
Richardson, TX, 1993. [3] D. Teaff, D. Watson, and B. Coyne, The Archi­tecture of the High Performance 
Storage System , in Proc. of the Goddard Conf. on Mass Storage (!! Technologies, College Park, MD, March 
1995. [4] 0. Pentakalos, D. Menasc6, M. Halem, and Y. Yesha, An Approximate Performance Model of a Unitree 
Mass Storage System , ibid. [5] S. Agrawal, Metamodeling: A Study of Approxima­tions in Queueing Models, 
The MIT Press, Cam­bridge, MA, 1985. [6] C. Wood, Client/Server Data Serving for High-Performance Computing 
, in L/th IEEE Sympo­sium on Mass Storage Systems, Monterey, CA, September 1995, pp. 107-119. [7] Maximum 
Strategy, Inc., GEN-5 XL Svstem Guide/IPI-3 Manual Revision 1.00, January 1995. [8] D. Tolmie, High-Performance 
Parallel Interface (HIPPI) , in High Performance Networks: Tech­nology and Protocols, A. Tantawy, Ed., 
pp. 131 156. Kluwer Academic Publishers, Boston, 1994. [9] D. Menasc6, V. Almeida, and L. Dowdy, Capac­ity 
Planning and Performance Modeling: From Mainframes to Client-Server Systems, Prentice-Hall, Englewood 
Cliffs, NJ, 1994. [10] K. Sevcik and I. Mitrani, The Distribution of Queuing Network States at Input 
and Output In­stants , JACM, vol. 28, no. 2, April 1981. [11] H. Schwetman, CSIM: A C-based, process 
ori­ ented simulation language , in Proc. of the 1991 Winter Simulation Conference, 1991, pp. 387-396. 
 [12] D. Menasc6, O. Pentakalos, and Y. Yesha, An An­ alytic Model of Hierarchical Mass Storage Systems 
with Network-Attached Storage Devices , Tech. Rep. TR-95-164, CESDIS, November 1995.  
</RefA>			
