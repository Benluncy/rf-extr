
 A Decentralized Communication Efficient Distributed Shared Memory Legand L. Burge III * Mitchell L. 
Neilsen Computer Science Department Oklahoma State University Stillwater, Oklahoma 74078 Abstract An 
important class of DSM implementations is one which uses cache memories to improve efficiency. Brown, 
Afek, and Merritt proposed cache-consistency protocols that provide a lower level of coherence. Such 
protocols are useful for applications that do not re-quire strict consistency among all sites in a distributed 
system [1, 4]. Masaaki, Zhou, Singh, and Neilsen pro- posed more efficient algorithms that enforce the 
same level of coherence as Brown's protocol [9, 10]. These protocols each require a single processor 
centralized memory to enforce sequential consistency. This pro- vides the advantage of a simple implementation 
and a clean correctness proof. However, a single copy of shared memory could become a bottleneck. Typically, 
if remote accesses to shared memory is costly, this would also decrease performance and object availabil- 
ity. In this paper, we present a decentralized cache- consistency protocol for DSM. We prove that our 
pro- tocol enforces sequential consistency. Keywords: Distributed algorithm, sequential consis- tency, 
distributed shared memory, memory coherence, da.tabase These paradigms allow multicomputers to communi- 
cate through Distributed Shared Memory (DSM). Distributed Shared Memory is an attractive ab-straction 
because it provides processes with uniform access to local and remote information. This uni-formity of 
access simplifies programming, eliminat-ing the need for separate mechanisms to access lo-cal state and 
remote state information. Several tech- niques have been proposed to allow multicomputers to communicate 
through Distributed Shared Memory [2, 3, 5, 6, 8, 9, 10, 11]. Each technique provides its own level of 
coherence. An important class of DSM implementations is one which uses cache memories to improve efficiency. 
Brown, Afek, and Merritt pro-posed cache-consistency protocols that provide a lower level of coherence. 
Such protocols are useful for appli- cations that do not require strict consistency among all sites in 
a distributed system [1, 4]. Mizuno, Zhou, Singh, and Neilsen proposed more efficient algorithms which 
enforce the same level of coherence as Brown's protocol [9, 10]. These protocols use the abstraction 
of a single copy of shared memory to enforce sequential consistency. This provides the advantage of a 
simple implementation and a clean correctness proof, ttow-ever, a single copy of shared memory could 
become a bottleneck. Typically, if remote accesses to shared memory is costly, which would decrease performance 
and object availability.  Introduction Initially, researchers strictly followed the common parMlel programming 
paradigms: shared variables (for tightly coupled systems) and message passing (for loosely coupled systems). 
More recently, ef-forts to combine the advantages of multiprocessors (easy to program) and multicomputers 
(easy to build) have lead to communication paradigms that simu-late shared memory on multicomputer systems 
[11]. *This work was supported in part by the National Science Foundation under Grant CDA-9421776 "Permission 
to make digital/hard copy of all or part of this material without fee is granted provided that copies 
are not made or distributed for profit or commercial advantage, the ACM copyright/server notice, the 
title of the publication and its date appear, and notice is given that copying is by pernfission of the 
Association for Computing Machinery, Inc.(ACM). To copy otherwise, to republish, to post on servers or 
to redistribute tO lists, requiresprior specific permission and/or a fee." In this paper, we present 
a decentralized cache-consistency protocol for DSM which provides the same level of coherence as the 
protocols presented in [1, 4, 9, 10]. Our protocol distributes the shared ob- jects among all processors 
in the network providing an increase in performance and object availability. Our protocol is not dependent 
on the system architecture, therefore allowing the algorithm to scale to a large number of processors 
more efficiently than the pro- tocols in [1, 4, 9, 10]. As memory cost decreases and the cost of communication 
become more expensive, we show that the increase in memory performance/cost of our protocol is minimal 
as compared to the reduction 358 in communication cost. Lastly, we prove that our pro- tocol satisfies 
a formulation of sequential consistency. 2 Memory Coherence A major concern in designing and programming 
efficient DSM applications is in avoiding conflicts in the use of shared memory. In order to maintain 
an appropriate performance level, often multiple copies of shared data are maintained. All updates are 
per- formed on a primary copy and all reads are performed on a local copy that is cached. The value of 
a pri-mary copy is replicated to remote cached copies once an update occurs. Replication introduces the 
prob- lem of having inconsistent copies of the same logical data. Complications also arise because the 
operations on shared data may not be instantaneous. A mem- ory consistency model defines certain restrictions 
on the use of shared memory. Applications that adhere to these restrict'ions are given guarantees about 
the coherence of that memory. Lamport proposed the idea of sequential consis- tency, which states that 
a shared memory system is sequentially consistent if each execution of the mem- ory system is equivalent 
to a sequential execution of the same set of operations [7]. Sequential consistency is appropriate for 
applications such as distributed databases which require all operations to be serial- izable. Most proposed 
distributed shared memory im- plementations are based on cache-coherency protocols for multiprocessor 
systems which use different varia- tions of this notion [1, 4, 9, 10]. 3 Definitions In this section, 
we review definitions of consistency on which our implementation is based. Some of the definitions and 
notations introduced in this section fol- low [9, 10]. A shared memory system consists of a set of processors 
P and a memory M. Each processor in P may execute a sequence of read and write operations on objects 
in M. A write operation by processor i on an object x is denoted by wi(x)v, where v is the value written 
on x by this operation. A read operation on x by i is denoted by ri(x)u, where u is the value of x returned 
by this operation. For simplicity, we assume that all values written by read and write operations are 
distint. An execution history of a shared memory system is a poset/_] = (U, -*u), where U is a set of 
read and write operations and --*u is an irreflexive and antisym- metric relation on U; that is, --*~r 
is a partial order on U. In the following we give some definitions: * We say that an execution history 
b" = (U,--*u) is processor-ordered if the operations of each pro- cessor in U are totally ordered by 
--*u. * An execution history S = (S,--~s) is a sequential history if it is processor-ordered and ---~s 
is a total order. * A sequential history S = (S, --~s) is legal if for every read operation r(x)v in 
S, there exists a write operation w(x)v such that w(x)v Ms r(x)v and there does not exist a write operation 
w(z)u such that w(x)v -'*s w(x)u -"s r(x)v. * A restriction of I)" = (V, --*v) to the set U, where U 
C V, is an execution history ~r = (U, "~v) such that for any operations o and o' in U, o --*v o' iff 
 O--~v 0 t We define U I i to be the restriction of history gr to the set of operations performed by 
i.  Two execution histories S and ~" are equivalent if for every processor i, S I i = U ] i.  Two execution 
histories S = (S,--*s) and (f = (U,--*u) are result-equivalent if S = U; that is, corresponding read 
operations return the same value and corresponding write oper- ations write the same value on both S 
and ~'. For example, wl(x)l,w2(~)2,r2(x)l and Wl(X)l,r2(x)l,w2(x)2 are result-equivalent but not equivalent. 
 (J = (V, -"~v) respects V = (V, -*v) if V C U and for any two operations o and d in V, if o ~v o' then 
o "--~u o t.  Definition 1: A memory M is consistent if for each of its execution histories H, there 
exists a legal sequen- tial execution history H?R = (WR, -*wR), where WR is the set of all read and write 
operations in H, such that/t and PIfR are equivalent. Definition 2: A memory M is consistent if for each 
of its execution histories H, there exists a legal se- quential history W = (W, :-~w), where W is the 
set of all write operations in H, such that the following property holds for each processor i. (a) Let 
W/~ = W U R~, where R~ is the set of read operations performed by processor i in /:/. Then, there exists 
a legal sequential history Vv~R = (WR, ~wR) such that W~R~ respects I~ and H I i = WR/I i. It has been 
shown in [9] that Definitions 1 and 2 are equivalent. Definition 1 considers a sequential his- tory for 
the entire system. This consists of the read and write operations issued by all processors. Defini-tion 
2 considers a sequential history for each processor i. This consist of the write operations issued by 
all the processors and the read operations issued only by processor i. We will use the definitions later, 
to prove that our protocol satisfies a formulation of sequential consistency. Overview of Protocol The 
previously proposed protocols in [1, 4, 9, 10], each require the abstraction of a single processor cen- 
tralized memory to enforce the real-time ordering on writes. In particular, [1, 4, 9, 10] assume the 
archi- tecture consists of a set of processors connected by a shared bus. In this paper, we consider 
a larger scale system architecture in which computers are logically fully-connected and communicate over 
costly point-to- point links. Each processor contains two threads of control, a Processor Manager and 
an Objec~ Manager, which share a single address space. This address space con- tains state information 
to capture causal relations of read/write operations to an object, and to notify a processor of invalid 
objects. Each processor initially owns a set of objects, and no two processors own the same object. An 
owner of an object, owns the most consistent version of an object, as updates to an object are only allowed 
to be processed by the owner of that object. This allows the real-time ordering on write operations to 
an object to be preserved; but only with respect to the owner of the object. Therefore, all other processors 
only maintain local cache copies. Each pro- cessor manager communicates with the owner of an object for 
a read/writ e request, if: 1. During a read to an object not currently owned, the value in the cache 
is invalid. 2. A write operation is issued to an object not owned by the current process.  Otherwise, 
the read/write operation is performed lo- cally. 4.1 Implementation In the next section, we describe 
two management schemes used to keep track of the owner of an object. A primary problem with distributed 
manager schemes is the initial distribution of objects. An optimal solu- tion would be to distribute 
an object to a processor who accesses the object most frequent. 4.1.1 Fixed Distributed Manager The fixed 
distributed manager scheme distributes the central manager's role to every processor in the sys- tem, 
thereby avoiding a single processor bottleneck sit- uation. In this scheme, every processor keeps track 
of owners of a predetermined set of objects (determined by a mapping function H) [8]. The primary difficulty 
in such a scheme is choosing an appropriate mapping from objects to processors. If we assume there are 
M objects in the system and I = {1, ..., M}. H is defined as a hashing function such that H(p) = p rood 
N where p E I and N is the number of processors. There- fore, when processor i requests to access an 
object p, processor i contacts the object manager H(p), and the protocol proceeds as in the centralized 
protocol in [9, 10]. 4.1.2 Dynamic Distributed Manager In the dynamic distributed manager scheme, every 
processor keeps track of the ownership of an object in its local cache. This is maintained through the 
use of the vector Prob [8]. The value Prob[o] contains the owner of object o. As processors that frequently 
access an object can cause the object to migrate, this value can either be the true owner or the probable 
owner of an object. This value is used as a hint to locate the true owner of an object. When a processor 
wants to perform a remote op- eration on some object o, it sends a request to the processor i indicated 
by the Prob[o] field. Upon re-ceipt of the request, if processor i is the true owner of the object the 
algorithm proceeds as in the centralized protocol described in [9, 10]. Otherwise, processor i forwards 
the request to the processor indicated in its Prob[o] field. This continues until the true owner of the 
object is found. The hint in the Prob[o] is up- dated after every remote operation to object o. The dynamic 
distributed manager algorithm requires at most (N -1) forwarding request messages to locate an owner 
of an object-in a system containing N pro- cessors. In the optimal case, only one extra message is require 
to forward a request; assuming the hint of the probable owner is correct. Because, the hints are updated 
as a side effect of different migration policies, the average number of messages required should be much 
less. 4.1.3 Migration Policies Our dynamic distributed manager scheme allows ob- jects to migrated between 
processors. This introduces the notion of a migration policy which could upgrade or degrade the performance 
of the protocol due to the locality of reference. There are three policies that can be used: Random Policy, 
Threshold Policy, and a Heuristic Policy. * Random Policy -The random policy is a simple migration scheme 
that uses no state information. An object o is simply migrated to process i after process i request a 
remote operation on object o. The problem with this approach is that use-less object migration can occur 
when an object is migrated to a processor that doesn't access it frequently.   Threshold Policy -The 
problem of useless ob-ject migration under the random policy can be avoided by maintaining statistical 
information of an object most frequently accessed by a proces- sor. Based on locality of reference, this 
strategy chooses the best processor to engage in migration of an object. This is very costly in terms 
of mem- ory. Each processor must maintain a threshold vector T of size N × M; where N denotes the num- 
ber of processors and M denotes the number of objects. Moreover, Tip, o] contains the expected number 
of accesses by processor p on object o.  Heuristic Policy - This policy uses heuristic infor- mation 
to make the best prediction on the loca- tion to migrate an object.  4.2 Data Structures Each processor 
manages the following data struc- tures: Let N denote the number of processors and M denote the number 
of objects. 1. Memory area C[M]. Ci[M] contains the values cached at processor i. . One-dimensional array 
Causal[M], used to cap- ture causal relations among write operations. Causali [o] keeps the version number 
of the most recent write on object o of processor i. . A set of valid cache objects valid. The set validi 
is initialized to the objects owned by processor i. For the dynamic scheme, the following data struc- 
ture is also maintained. . One-dimensional array Prob[M]. Entry Probi[o] contains the hint of the owner 
of object o by pro- cessor i. 5 Description of Protocol In this section, we provide the actual description 
of the protocol using a syntax similar to the C pro- gramming language. We denote all elements in a one 
dimensional array R by R[.]. Note that all operations on an object local or remote are executed atomically. 
Therefore, simultaneous updates to local memory by the processor manager or the object manager are syn- 
chronized. 5.1 Variable Definitions In this section, we define and motivate all variables used in the 
our description of the protocol. 1. Let z be an integer value denoting the object to access in the cache 
. 2. Let v represent any data structure or block of data structures to be stored in shared memory. 
3. Let valid represent a set of integer values to de- note the valid objects stored in the local cache. 
 . Let Causal be an integer vector used to cap-ture causal relations among write operations to a shared 
object. 5. Let C be a vector of the type v to represent the shared objects maintained in the cache. 
6. Let i and j be integer values to denote the pro- cessor id. 7. Let Prob be an integer vector used 
to denote the owner of an object. 8. Let pj denote processor j.  5.2 Fixed Distributed Manager In this 
section we provide a description of the decen- tralized protocol using the fixed distributed manager 
scheme. OBJECT MANAGER at pi: Process [write, j, x, v, Causalj[*] ] message from pi :: Ci[z] = v; increment(Cau 
s ali [x]); Invalidate( ln v alidi ); validi = (validi --Invalidi) U {x}; send[Causali[*]] message to 
pj Process [read, j, x, Causal9[*] ] message from pj :: Invalidate( I nv alidi ); validi = (validi --Invalidi) 
O {z}; send[Ci[x], Causali[*]] message to p$ Procedure Invalidate(var Invalid) :: Invalid = $; For eachy 
EM, y#xdo if (Causali[y] < Causalj[y]) then Causali[y] = O; lnvalidi = lnvalidi U {y} endif enddo PROCESS 
MANAGER at Pi: write (x, v) :: if (H(x) ¢ i) then send[write, i, x, v, Causali[*]] message to Pz/(~) 
receive[Causalj[*]] message from PH(.) Invalidate(Inv alidi ); Causali[z] =Causalj[x]; validi = validl 
-Invalidi; else increment ( Causali[x]); endif validl = validi U {x}; C~[z] = v; read(x) :: if x ~ 
Validi then send [read, j, x, Causali[*] ] message to PH(x) receive Iv, Causalj[*] ] message from PH(z) 
Invalidate(Invalids); Causali[x] =Causalj[z]; validi = (validi -lnvalidl) O {x}; C~[x]= v; endif return(Ci[x]); 
 5.3 Dynamic Distributed Manager In this section we provide a description of the decen- tralized protocol 
using the dynamic distributed man- ager scheme. OBJECT MANAGER at pi: Process [write/forwardw,j, x, 
v, Causalj[*] ] message from pj :: if (Probi[z] == i) then checkthreshold(x,j); c,[x] = v; increment( 
C au "s al i [z ] ) ; Invalidate( lnvalidl ); validi = (validi -Invatidi) u {x}; send[Causali[*], Probi[x]] 
message to pj else send[] orwardw , j, x, v, Causal j[*]] message to p probi[x ] Process [read/forwardr, 
j, x,Causalj[*] ] message from pj :: if (Probi[z] == i) then checkthreshold(x,j); Invalidate( Invalidi 
); validl = (validl -Invalidi) u {z}; send[Ci[x], Causali[*], Probi[z]] message to pj else send[]orwardr, 
j, z, Causalj[*]] message to pprobi[z ] Procedure Invalidate(var Invalid) :: Invalid = ~; For cathy 
6 M, y ~zdo if (Causali[y] < Causalj[y]) then Causali[y] = 0; Invalidi = Invalidi U {y}; endif enddo 
 Procedure checkthreshold(x, j) :: increment(Ti[j, x]); ifTi[j,x]> t then Probi[x] = j; endif Procedure 
resetthreshold(z) :: if (Probi[x] == i) then for each j E N do Tdj, x] = O; enddo endif PROCESS MANAGER 
at pi: write (x,v) :: if (Probi[x] ~ i) then send[write, i, x, v, Causali[*]] message to pprobi[= ] receive(Causalj[*], 
owner) message from pprobi[z ] Probi[x] = owner; Invalidate(Inv alidl ); resetthreshold(z); Causali[x] 
=Causalj[x]; validl = validl -Invalidi; else increment ( Causali[x]); endif validi = validi U {x}; 
Ci[z] = v;  read(z) :: if X ~ Validl then send [read, j, x, Causali[*] ] message to pprobi[::] receive 
Iv, Causalj[*], owner ] message from pprobi[x ] Probi[x] = owner; Invalidate(lnvalidi ); resetthreshold(z); 
Causali[z] =Causalj[x]; validl = (validl -Invalidi) U {z}; C~[z] = v; endif return(Ci[z]); 5.4 Cost 
Performance Our decentrMized protocol requires one round of message exchange for a write operation if 
the current process is not the owner of the object; otherwise the value is written to the local cache 
(Table 1). A read operation requires one round of message exchange if the value in the local cache is 
not valid and the cur- rent process is not the owner of the object; otherwise the value is read from 
the local cache. We provide a protocol similar to [9, 10], that does not require an atomic broadcast 
capability. Message [4, 1] {9] [lo] Decentralized Reads 0 if valid 1 if -valid 0 if owner 1 if "~ valid 
0 if valid I if -~ owner Writes N 1 0 if owner 1 if -~ owner Forwards N/A N/A Best case: 1 Worst case: 
N -1 Table 1. Communication Cost Performance In particular, if we consider performing all read op- erations, 
our protocol provides about the same level of performance as the protocols in [9, 10]. Although on the 
average, our protocol requires slightly fewer messages than the single shared memory protocols in [9, 
10]. This is because remote reads to objects owned by a process can be performed locally; were as, it 
must always be performed remotely using the protocols in [9, 10]. As shown in Table 2, our protocol increases 
the performance in communication at the cost of more memory. Assumption,: K ---~ ~ of object, N -~-~ 
of processors Protocol Procesl Memory Size [1] Processor IN Queue Unbounded OUT Queue Unbounded Cache| 
| K Shared Memory Cache[ ] K [4] II P ........ Q .... Unbounded H Cache| ] K Shared Memory Cache[ ] 
K [S] Proce**or v.lld{ ] K Cache| ! K SMem M[ ] K Cache.Vet[ ][ ] N×K c.u,.l[ ] K llO] Processor Valid[ 
] K Cache K SMem N X K binary vector c,...i[ I K Processor Cl l K Manall:er v,J,.d[ I K I Decent ral|sed 
c.u,.l[ ] K *'*'+ II M*nager i Probl ] K Decentralized v&#38;lidl l K C.u,.l[ ] K I °"'"-I] ........ 
I K °" Table 2. Memory Cost Performance 6 Proof of Program Correctness In this section we prove that 
our protocol preserves the real-time ordering on write operations and allows the same set of sequentially 
consistent executions as [4, 9, 10]. The following proof is based on [9]. Theorem: The implementation 
is consistent; that is, it satisfies Definition 2. Assumptions: 1 No two processors initially own an 
object simul- taneously. 2. All operations on an object local or remote are executed atomically. 3. 
Assume , for all processors, that all objects o owned by a processor i make up the distributed shared 
memory. If we let DSM = hlUh2U...Uhg; where N is the number of processors. Since DSM is the global shared 
memory, all writes are performed only on objects maintained in DSM. More formally, let P = (Pl,P2, ..-,PM) 
be a set of processors, O = (ol, o2, ..., OM) be a set of objects maintained in the system, H = (hi, 
h2, ..., hg) be a set of objects owned by each processor such that V(pi) 6 P, 3(hiOhj = 0) Since all 
updates to an object o E hi are performed only by processor Pi, this maintains the strict ordering among 
writes for each hi E DSM.  Proof: Let Ho be an execution history of the protocol for object o. In order 
to show the implementation is consistent, by Definition 2, we have to show that: (i) We can construct 
a sequential history I~o = (Wo, --*Wo), where Wo is the set (>fall write opera- tions in Ho, (This preserves 
the real-time ordering on writes for each object separately) (ii) For each processor j we can construct 
a legal se- quential history WoRj = (WoRj,"'*WoRj), where WoR i = WoURj and Rj is the set of read opera- 
tions performed by processor ] on object o in/~o, such that WoRj respects Wo and/~o I i = WoR i I i iff 
process i owns o.  Now let l~o = (Wo, "-~Wo) be a history such that if o ando' are operations in Wo 
then o -'*Wo d, if o is processed before o' by the owner of the object de- noted OWNERÙ. Because the 
OWNER° processes the write operations sequentially, l@o- (Wo,--~wo) is 363  a sequential history. We 
will show (ii) by constructing a legal sequential history W~Rj = (WoRj, ~WoR¢) as follows: (a) For any 
two operations o and d in WoRI which access OWNERo, o --*Won, o' if o is processed before, o'. (b) For 
any two operations oJ and o] performed by processor j, o~ --*Woa~ o], if o~ is processed before  oy. 
 (c) Let rj~,rj~,...,rj~ be a sequence of consecu-tive local read operations by process j ( thus, vii 
~Won~ rj~ ~Wotb "'" "-*Wot~¢ rjN due to the ordering enforced by (b)). Let o~ be an opera-tion by any 
processor z which accesses OWNER° and immediately follows oj at OWNER° (thus, oj -*Woa¢ o~ due to the 
ordering enforced by (a)). Then, rjN -'-'Woa¢ Oj. From (a) and the fact that all operations in Wo access 
the OWNERo, we have that WoRj = (WoRj, --*Won¢ ) respects Wo. From (b),/Io I J = W[,R¢ I J. Finally, 
we will show that W~Rj is legal. Proof: Assume that WoRj is not legal for any proces- sor j. Then there 
must exist a read operation r(z)v such that w(x)v -'*Wow~sa~ w(z)u ~wo~s~ r(z)v and there does not exist 
w(z)s such that There are three cases to consider: Case 1: Operation r(x) performed by processor i, ac- 
cesses OWNER~ = i. Then the last value written by any processor j is the most recent write. Therefore, 
history w(z)v --*WowN~R=I~j W(Z)U "-~WowN~=R~ ne er occurs. Case 2: Operation r(z) accesses OWNER~, 
and pro- cessor i is not the owner. Clearly, from the proto- col, w(z)u writes u to COWNr~R=[z], and 
r(x) is per- formed after w(z)u to OWNER~. Thus, r(x) does not return v, and the history w(z)v "-*WowNER=R~ 
never occurs. Case 3: Operation r(z) is a local read and proces- sor i is not the owner of z. Let oj 
be the last operation before r(x) by processor j which accesses some shared object . There are three 
cases to consider: . oj ~WowNER=Rj r(x)v: In this case, assume the owner of the object is processor k. 
There are two cases to consider: (la) w(x)u is issued by processor j: Then w(z)u sets Ck[x] = u, Causalk[x] 
is incremented, and Cj[z] = u, Causalj[z] = Causalk[z], and validj[x] = 1. Since there does not exist 
w(z)s ordered by "--~wknj in between w(x)u and r(z), the values of validj [z] and Cj [x] stay unchanged 
at least until r(z) is performed. Thus, r(x) locally reads value u from Cj[x], and history never occurs. 
(Ib) w(x)u is not issued by processor j: Execu-tion of w(x)u sets Ck[z] = u and Causalk[z] is in- cremented. 
After oj accesses the OWNER~ = k, validj [z] = 1 at processor j. Since r(x) is a local read, validj [z] 
must be 1 when r(z) is performed by processor j. This means validj [x] has been changed to 1 before oi 
is completed. From the protocol, validj Ix] can be changed to 1 only if a read or write operation on 
 by processor j is performed at OWN.ER~ = k. By the assump- tion, there does not exist w(x)s ordered 
in be- tween w(x)u and r(x) by "~WowNsR=Rj" There-fore, there must be a read operation by processor j 
which reads Ck[x] at OWNER~ = k between w(x)u and oj, including oi. This read operation also sets validj 
[z] = 1 and Cj[z] = u. Thus, r(x) returns u, and history w(z)v "~WovcN~R=R¢ W(X)U "~Wow~ER=R~ r(x)v never 
occurs. . oj is w(x)u: Then, the operation w(x)u sets validj [z] = 1 and Cj[z] = u. Since there is no 
operation by processor j which accesses an object at OWNERx = k between w(x)u and r(x),r(x) returns u. 
Thus, history w(x)v -'*WowNsa=aj w(x)u WowN .. r(x)v never occurs. . oj "~WowN~R=I~ W(Z)U: In this case, 
rule (lc) above orders r(x) in between oj and w(x)u. Hence, a history w(z)v "+)VowN~a=l~j w(x)u -'~WowNsn= 
Rj r(z)v never occurs. 7 Conclusion In this paper, we presented a decentralized cache- consistency protocol 
for DSM which manages objects distributed among all processors in the system. Our protocol preservers 
the real-time ordering on write operations, and allows the same set of sequentially consistent executions 
as [4, 9, 10] without requiring atomic broadcast or multicast. In contrast, we prove that our protocol 
enforces sequential consistency . Al- though our protocol requires additional state informa- tion, the 
tradeoff of memory cost to communication cost provides reduction in overall communication per- formance. 
8 Future Work Future work in protocol performance would be to reduce memory cost using a method simalar 
to [10]. Other future work would be to look at fault-tolerance issues that could effect the performance 
of the protocol such as transient failures. References <RefA>[1] <SinRef><author>Y. Afek</author>,<author> G. Brown</author>, and <author>M. Merritt</author>. <title>A lazy 
cache algorithm</title>. <booktitle>In Proceedings of the ACM Symposium on Parallel Algorithms and Architectures</booktitle>, <pages>pages 
209-222</pages>. <publisher>ACM</publisher>, <date>1989</date></SinRef>. [2] <SinRef><author>M. Ahamad</author>, <author>R. ttutto</author>, and <author>R. John</author>. <title>Implement- ing and programming causal distributed 
shared memory</title>. <booktitle>In Proceedings of the IEEE Interna- tional Conference on Distributed Computing Sys- tems</booktitle>, 
<pages>pages 274-281</pages>. <publisher>IEEE</publisher>, <date>1991</date></SinRef>. [3] <SinRef><author>It. E. Bal </author>and <author>A. S. Tanenbaum</author>. <title>Distributed programming with shared data</title>. 
<journal>Computer Lan. guages</journal>, <volume>16</volume>:<pages>129-146</pages>, <date>1991</date></SinRef>. [4] <SinRef><author>G. Brown</author>. <title>Asynchronous multicaches</title>. <journal>Dis. tributed Computing</journal>, 
<volume>4</volume>:<pages>31-36</pages>, <date>1990</date></SinRef>. [5] <SinRef><author>M. Feeley </author>and <author>H. Levy</author>. <title>Distributed shared mem- ory with versioned objects</title>. <tech>Technical 
Report TR- 92-03-01</tech>, <institution>Department of Computer and Engineer- ing , University of Washington</institution>, <date>1992</date></SinRef>. [6] <SinRef><author>K.. 
Gharachorloo</author>, <author>D. Lenoski</author>,<author> J. Laudon</author>,<author> P. Gib- bons</author>, <author>A. Gupta</author>, and <author>J. Hennessy</author>. <title>Memory con- sistency and 
event ordering in scalable shared memory multiprocessors</title>. <booktitle>In Proceedings of the iTth Annual Symposium 
on Computer Architec- ture</booktitle>, <pages>pages 15-26</pages>. <publisher>Computer Architecture News</publisher>, <date>1990</date></SinRef>. [7] <SinRef><author>L. Lamport</author>. <title>tIow to make 
a multiprocessor com- puter that correctly executes multiprocess pro- grams</title>.<journal> IEEE Transactions of Computers</journal>, 
<volume>28</volume>:<pages>690-691</pages>, <date>1979</date></SinRef>. [8] <SinRef><author>K. Li </author>and <author>P. Iiudak</author>. <title>Memory coherence in shared virtual memory systems</title>. <journal>A CM Transactions 
on Computer Systems</journal>,<volume> 7</volume>:<pages>321-359</pages>, <date>1989</date></SinRef>. [9] <SinRef><author>M. Mizuno</author>, <author>G. Singh</author>, <author>M. Raynal</author>, and <author>M. Neilsen</author>. <title>Communication 
efficient distributed shared mem- ories</title>. <tech>Technical Report TR-CS-93-3</tech>, <institution>Department of Computer and Information 
Sciences, Kansas State University</institution>, <date>1993</date></SinRef>. [10] <SinRef><author>M. Mizuno</author>, <author>G. Singh</author>, and <author>James Z. Zhou. </author><title>A sequentially 
consistent distributed shared mem-ory</title>. <tech>Technical Report T1;~-CS-93-4</tech>, <institution>Department of Computer and Information 
Sciences, Kansas State University</institution>, <date>1993</date></SinRef>. [11] <SinRef><author>M. Stumm and S. Zhou</author>. <title>Algorithms implement- ing distributed 
shared memory</title>. <journal>IEEE Computer</journal>,<volume> 23</volume>:<pages>54-64</pages>, <date>1990</date></SinRef>.</RefA> 365   
			
