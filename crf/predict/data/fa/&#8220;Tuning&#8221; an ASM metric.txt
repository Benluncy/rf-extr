
 Tuning an ASM Metric: A Case Study in Metric ASM Optimization t Hal Ber~hel. $ David Roach $ George 
B;logh, $ Carroll Hyatt T University of Arkansas $ Acxiom Corporation Abstract Wc present an approximate 
string matching case study. An optimized version of the edit distance algorithm is described which has 
proven more accurate for a particular commercial application than the existing (benchmark) algorithm. 
The cvoluhon and nature of the optimization are detailed and test results are presented. Introduction 
 String matching refers to the activity of associating strings of symbols with one another. fiuct String 
Matching (ESM) refers to the set of proee-dures which associate identical string tokens with one another 
[1]. Approximate String Malching (ASM) refers to a set of procedures which associate non-identical strings 
with one another on the basis of some criterion (or set of criteria) of similarity [2]. Classical Approximate 
String Makhing is the traditional ASM approach which requires that each symbol be nondczomposabie. To 
illustrate, classical ASM on strings of text would not analyze the properties of the symbols (e. g., 
symbol set from which they arc drawn, typeface and size, etc.). Historically, most Classical ASM has 
been probabi&#38;Jic in the sense that the result of string comparison was an estimate of likelihood 
that the two strings were the same. In contrast to probabilistic methods, aiomalic methods [3][4] directly 
encode the relevant definitions of similarity and thereby have uniform match probabilities of 1. A third 
method, comprised of edit distance algorithms, determine the minimal number of editing steps which transpose 
one string into another. Any similarity relation which may exist betwtzn strings may be conveniently 
described in terms of Faulk categories [5]: posi~ional similarity, or the degree to which matching symbols 
sre in the same position in their respective strings [6]; ordinal similarity, or the degree to which 
the matching symbols are in the same order [7J; and rnaterkd sinrih-w-ity, or the degree to which two 
strings are made up of the same symbol tokens [8­ 13]. Algorithms may, and frequently do, rely on one 
or more of these categories. Those which use only one category of mess urcmcnt are called single-relation 
measures, while others are nruiIiple-relalion measures. The advantage of single-relation similarity measures 
for ASM applications is that the algorithms are usually straightforward and easy to implement. However, 
this advantage comes at a considerable expense: single-relation measures only look at one aspect of similarity 
and, as a consequence, are limited to relativcl y unsophisticated applications. Beeause of this, multi­rclation 
measures have dominated ASM research since the mid­1970 s. Two major categories of multi-relation techniques 
exist: metric and non-metric. Metric techniques are real-valued difference functions which satisfy, among 
other things, the property of triangular inequality Permission to copy without fee ail or part of this 
matarial ia granted provided that tha copies are not made or distributed for direct commercial advantage, 
the ACM copyright notice and the title of the publication and ita date appear, and notice is given that 
copying is by permission of the Association for Computing Machinery. To copy otharwise, or to republish, 
requires a fee and/or specific permission. @1992 ACM O-89791.502.X/92/0002/0131 ...$1.50 (i.e., if string 
1 and string 2 are different as are strings 2 and 3, then the difference between strings 1 and 3 is less 
than or equal to the sum of the other differences). One popular metric ASM tuhnique is the Levenshtein 
method [14], which has been applied extensively (e. g., [15][16]; see also [2] and [17J). Non­mctric 
techniques arc also quite popular. One leading non­metric teehnique is n-gram anafysis [4][7][18][19[20]. 
As wc mentioned in the first paragraph, classical ASM has been based on approximations or estimates of 
similarity. In the mid­1980 s, another automated approach was developed by Berghel, et al [3][4]. This 
method direetly encodes the set-theortiical definition of similarity in use into the computer program 
within a logic programming paradigm. The advantage of this approach is that it is maximally effedive 
as measured by standard information-theoretic measures [21][22]. That is, it doea not err in adjudging 
similarity, however it is known that this aectsracy may come at the expense of efficiency when implemented 
within a logic programming paradigm. In this paper, we report on a case study in ASM optimization based 
upon the Levenshtein metric.  The Levenshtein Metrie An ASM similarity measure is a real-valued difference 
function, d, over character strings, which satisfies the following conditions d(sl ,s2) >0 :] d(sl ,s2) 
= 0 <-> SI=S2 3) d(sl ,s2) = d(s2,sl) 4) d(sl ,s2) + d(s2,s3) > d(sl ,s3) for arbitrary character strings 
s 1,s2,s3. A popular similarity measure which is both multi-relation and metric is the so-called Levenshtein 
Metric , named atler the pioneer in coding theory who first suggested ita use [14]. This measure has 
been applied to spefling correction by [1S][16]. We now describe this teehnique based upon the presentation 
in Hall and Dowlirsg [2]. Let d(ij) be an multi-relation measure of similarity with respect to strings 
S1=ctez...ci and Sz=c lcz...j j. We defuse it recursively as follows: d(O,O) = O d(ij) = min[d(ij-1) 
+ 1, d(i-lj)+ 1, d(i-1 J-1) +v(ci,c j), d(i-2J-2) +v((ei-I ,c j) +v(cj,c j-1) + 1] where,   @ cxo -> 
Ci= c; and V(e,,Cj) l <->ei+cj. In this case, we extract a measure of similarity between two strings 
by creating a dinxtcd graph for all nodes (i j), with 131 horizontal and vertical edges weighted 1 (the 
penalty for a mismatch) and the weights of diagonal edges determined by v (see Figure 1). Figura1: MatrixPaths 
GA RVEY A v r?. R Y Intuitively, since penalties are cumulative, the more dissimilar strings will have 
the longest paths. Since the difference measure, d, satisfies conditions 1)-4), above, it qualifies as 
a legitimate metric. Note here that the terms of the minimization function amwoximate the Damerau condhions 
as set forth in [23] (i.e., string difference in terms of one missing character, one additional character, 
a substitution of a chamcter and transposition of two characters). To illustrate, assume that the shortest 
path weight is 1 for two strings whose lengths differ by 1. Futiher, assume that thk path weight resulted 
from the minimizing term d(i-1 .j) + 1. We would take this to mean that the shofier string is related 
to the longer by accidental omission of a chamcter. Objective The objective of this research was to develop 
methodological and ~roceduml guidelines which could regulate the tuning of metric ASM algorithms so that 
they could equal or exced the effectiveness of other generic ASM algorithms with respect to a test dataset. 
Specific details of the test dataset and benchmark algorithm are irrelevant to this repofi since our 
objective is the development of guidelines which would apply of ASM algorithms as such and in general. 
The dataset of intereat consisted of 2,500 pairs of last names. The names in the name-pair were slightly 
different from one another. For example, one might be Vareha and the other Vahera . For each pair, a 
(manual) determination was made whether the strings were sufficiently similar to justify the conclusion 
that one might likely be a corruption of the other. The determination was 3-valued: definitely a match 
, ? and definitely not a match . Then the performance of the benchmark algorithm was compared with these 
evaluations. The algorithm s output was also 3-valued: yes , ? and no . Obviously, for each namepair 
nine situations could result. There was no variable weighting assigned to a mismatch, assigning a yes 
with a no was penalized the same as asaigning a ? to a yes . Figure 2 assesses the accuracy of the benchmark 
algorithm in terms of these nine possibilities as measured by the number of mismatches. The legend identifies 
the patterns that correspond to the conclusions of the algorithm. For instance, the blackened area represents 
the number of instances where the algorithm said there was a match. The other attems correspond to the 
number of instances in which the a gorithm f said there was a mismatch or a questionable case. The x-axis 
represents percentages of the algorithm s conclusions that were Y s, N s, and ? s. These are plotted 
against the three categories of manual responses on the y-axis. For example, the upper bar Figure2: Effactivmess 
of benchmark elWfithm by calegory ~N c j? A Ly Oloti 30 40 !ioabioioeolixl PERCENTAGE indicates that 
the algorithm: (1) says N when it should say N = 79% of the time, (2) says ? when It should say N = 19% 
of the time, and (3) says Y when it should say N * 2% of the time. The other two bara compare the algorithm 
s output to the expected outputa of? and Y, respectively. We describe the geneml procedure which enabled 
us to tune the Levenshtein metric so that the effectiveness exceeded the benchmark algorithm. We refer 
to the resulting algorithm as TM ( tuned metric ). Methodology Figure 3 identifies the mnge of options 
considered in the search for a more effective ASM hhnique. Figure&#38; M@r DecMon Points (1..uulwMJl 
Q  12!Ek_l MEW NcN~ mCl 2. VALUBD 3.vAKe9 c1m 1 If 4 mlm#naJ nmmntm mCl Only Classical ASM techniques 
were considered since axiomatic procedures have so far only been implemented as declamtive (vs. proceduml) 
progmms, and thus have such different performance charactcriatics that comparison would be problematic. 
Algorithms representing all three single=measure techniques (see introduction) were considered, although 
they me not shown in the figure. While singkelation measures may offer advantages, they must be used 
with caution, On the one extreme, material similarity tends to be too loose (anagmms 132 arc materially 
similar, for example), while on the other, positional tends to be too restrictive ( abedef and bcdef 
are totalIy dissimilar, positionally). The ampromise, ordinal similarity, has similar problems ( wilson 
and wsn are similar). For these reasons, single-relation measures are usually most appropriate in special-purpose 
applications for which the nature of the string differences is well understood, The application under 
discussion was not such an application. The properties of a metric algorithm, particularly triangular 
inequality, were needed for further processing. As a component of a VLDB system, the ASM routines which 
we developed had to provide output scores which were amenable to fuzther partitioning of the data. Metric 
ASM algorithms make it possible to partition the data without a priori knowledge about orthographical 
make-up (vs., e.g., n-gram analysis which derives its utility fmm the non-uniform distribution of n-grams 
in the data which must be known in advance). In the present context, all ASM activity must be done on 
the fly . Thus, we compare-d the Lcvenshteirt algorithm (see above) to the benchmark algorithm. Implemented 
according to standard practice [2][15][16], the Levenshtein algorithm compared strings with respeet to 
omission, insertion, substitution, and transposition of characters, Both algorithms were initially set 
up to return a binary match value (Yes or No). In this mode, it is trivial to optimize algorithms in 
terms of critical threshold: the cutoff point is adjusted to maximize agreement. On this basis, the Levenshtein 
algorithm had a slight edge in terms of accuracy (69% correet to 63%). However, manual comparison of 
the results with the test dataset indicated a major problem with the two-valued approach: the accuracy 
could not appreciably increase unless both algorithms could better distinguish bdween degrees of similarity. 
On the two-valued account, a miss was as good as a mile . However, the test dataset revealed many cases 
of very similar strings which differed in subtle ways which were not detectable by the algorithms. In 
order to catch such pairs it was decided to concentrate on only three-valued variations of the algorithms 
(Yes, No and Ma be). The advantage was that as long as a high percentage o { the problematic string pairs 
were flagged as possible matches, futiher processing (depending upon the application and context) could 
be used to resolve the matter. The alternative would be to continuously tune the algorithms to accommodate 
each new subtlety, which would both be impractical and reduce the efficiency of the programs. In their 
ternary versions, the accuracy advantage shifted (69% for the benchmark algorithm and 65% for Levenshtein), 
but did not increase overall. Once again, visual inspection was called for. Examination revealed that 
the nature of the remaining failures were such that a significant fraction could be overcome by means 
of two simple adjustments. It was found that by normalizing the match vaiues with respect to the lengths 
of the strings (so that a ontiharacter difference between strings of len h two would be penalized more 
than a one-character dif rerence between strings of length ten) and by handling truncations (e.g., WILL 
vs. WILLIS) indepen dently of the ASM algorithms through pre-proeeasing, the overall accuracy increased 
by 10% for both systems. It was now simply a matter of refinement. The benchmark algorithm is also an 
multi-relation ASM teehnique which is three-valued and employs normalization and truncation according 
to the same algorithm. Although it was not a concern, we found that the general orientation of the benchmark 
algorithm was prudent: we were unable to fmd alternative orientations (i.e., with respect to Figure 3) 
which were more effective. Having determined that within the range of our comparisons the Levenshtein 
algorithm was the most effective alternative to the benchmark algorithm, the next objective became optimization. 
Levcnshtein metrics have two parameters which ean be tuned. First, variations in the penalty set are 
possible. These values weigh the character differences according to the nature of the difference. In 
its standard form, the Levenshtein algorithm (cf. [2]) recognizes missin$, extra, substituted and transposed 
characters (classical typutg errors) and penalizes them equally with unity (i.e., the standard penalty 
set is <1,1,1,1>, corresponding to missing, extra, substituted and transposed characters, respectively). 
Figure 4 illustrates the effect that the penalty sel may have on the aeeuracy of the algorithm, with 
the other parameter held constant. Fiwre 4: Mismatch rate for varying penatties (optimizad) --­ y 7C0 
Sam Msm A ~ 4m cm HZCO ~ lm o :tflttlz Iizl 1122121112121221 t2222f 112t12?1212t2c m11n1222zt 2222 PENALTIES 
 The s=ond parameter which may be adjusted involves the thresholds at which distinctions between strings 
are made. In the three-valued framework, the distinction must be made between matches and questionable 
matches, and questionable matches and definite non-matches. Thus, there are two thresholds to contend 
with. Obviously, with two equalfy impmtant variables to consider, one will have to be held constant while 
the algorithm is optimized with respect to the other, The question of the order in which the parameters 
are optimized redly comes to a question of an estimate of the coarseness or fineness of the parameter 
s changes. In the case under review, the coarser measure is the penalty set -we could effectively deactivate 
one quarter of the algorithm by substituting a penalty of O for 1 for any of the error types. As Figure 
4 shows, the <1,1,1,1> sel of penalties for <omission, insertion, substitution, transposition >, respectively, 
performed comparatively well with respect to other combinations of values. <2,2,2,2> yielded a few less 
mismatches but not enough to justify its use over the traditional penalty assignments. Holding the penalties 
constant at <1,1,1,1>, an optimimtion algorithm was employed to adjust first the lower and then the upper 
thresholds until mismatches began to increase. Figure 5 shows that the optimal lower and upper bounds 
for our test datasti are 0.12 and 0.19 -the bounds at which the fewest mismatches occur. Figure 5 Effaets 
of threshold changes on aceuraey E0ma2  Results Figure 6 plots the accuracy of TM with respeet to various 
combinations of matches and mismatches that occur as was done with the benchmark algorithm in Figure 
2. 133 Figure 6: Effaetiwrress of TM algorithm by category . m . ?8 yes I F .­ ­--­ 1 0 10203040506070 
So 90 100 PERCENTAGE  For example, the upper bar indicates that the algorithm: (1) says N when it should 
say N ~ 90% of the time, (2) says ? when it should say N = 9% of the time, and (3) says Y when it should 
say N = 1% of the time. The other bars compare the cases in which the algorithm outputs ? or Y to what 
It should output. The actual results are tabulated below. The fwst Y, N, or ? represents the algorithm 
s output, and the second represents the desired output, i.e., the actual match value which was determined 
manually. 2,500 string pairs were tested. There are 1,257 non-matching pairs, 614 matching pairs, and 
629 pairs that are border-line cases. Benchmark Algorithm Mismatches: 784 Match Type Ratio Percentage 
Y-Y 4581614 74,59 Y-N 17/ 1257 1.35 Y-? 1921629 30,52 N-Y 381614 6.19 N-N 998 I 1257 79.40 N-? 1771629 
28.14 ?-y 1181614 19.22 ?-N 242 I 1257 19.25 ?-? 260 I 629 41.34 TM AIgorithm Mismatches: 579 Match Type 
Ratio Percentage Y-Y 4771614 77,69 Y-N 3 I 1257 0.24 +-+ 79 t629 12.56 N-Y 401614 6.51 N-N 1139 / 1257 
90.61 N-? 2451629 38.95 ?-y 971614 15.80 ?-N 115 / 1257 9.15 ~-? 3051629 48.49 The difference in the 
number of mismatches (205) represents a 26% improvement in accuracy over the benchmark algorithm, In 
addition to mismatch ratios, TM was evaluated using the standard information theoretic measures of precision, 
recaU, fallout, and generality [21]. These measures are calculated for each of the three match categories 
Y, ?, and N. A match occurs when the algorithm s match value equals the actual match value. A mismatch 
and missed match occur when the algorithm s match value does not equal the actual match value. For instance, 
if the algorithm says Y when it should say ?, a mismatch has occurred with respect to the Y cate ory 
and a missed match has occurred with respect to the ? category. Ideally, an algorithm would have 100% 
precision and recall and zero fallout. Generality represents an average that needs only remain constant 
when two algorithms are compared. These measures can be expressed in terms of matches, mismatches, and 
missed matches as follows: Precision = matches / (matches + mismatches) Recall = matches / (matches + 
missed matches) 134 Fallout = mismatches / (mismatcha + total number that are neither matches, miamatchea, 
nor missed matches) Generality = (matches + missed matches)/ (matches + missed matches + mismatches + 
the total number that are neither matchea, mismatches, nor missed matches), i.e., the total number of 
string pairs compared If we let [A-B] represent the ratio of the number of times the algorithm output 
A to the number of times it should have output B, then we can define the measures for each category (Y, 
N, and ?) as follows: prccision(YES) = ~-~/ (~-yl + N-N] + [Y-?]) precision(?) = [?-?]/ ([?-?] + [?-~ 
+ [?-N]) precision = ~-N]/ (W-N] + (N-Yj + ~-?]) rccall(YES) = V-yl / (~-~ + [?-YJ + N-YJ) recall(?) 
= [?-?] / ([?-?] + ~-?] + ~-?]) recall(NO) = ~-N]/ (N-N] + W-N] + [?-N]) fallout(YES) = (~-?] + N-N])/ 
{(W-?] + W-N]) + ON-N] + W-?] + [?-?] + [?- Nl)) . .,, , fallout(?) = ([?-Yl + [?-N])/ [~~~ + [?-N]) 
+ (~-~ + W-N] + [Y-v + ~­ -. . faUout(NO) = (~-?] + ~-yl) /  {(rN-?] + rN-Yl) + (N-YI + ry-?l + [?-YI 
+ [?-?])} generality(YES) = (~-Yl + {[?-Yl + ~-~))/ (w-w + {[?-m + m-m)) + (~-?] + ~-N]) + (m-N] + ~-?] 
+ [?-?] + [?-N]) generality(?) = ([?-?) + {~-?] + ~-?]})/ ([?-?] + {w-?) + ~-?])) + ([?-~ + [?-N]) + 
(NNI + NV + lY-YI+ N-N] i generality = (N-N] + {[?-N] + W-N] ) / (~-N] + {[?-N] + W-N]}) + (0+?1 + m-n) 
+ ([Y-m + -Iy-?j + [?-?]+ [?-Yl) As indicated by Figures 7-9, precision increased for the Y and ? categories 
and decreased only slightly for the N category, recall increased for the Y and N categories and decreased 
slightly for the ? category, and fallout decreased for the Y and ? categories while increasing slightly 
for the N category. Figure 7 Precision by category O.w r 0.s0 0.70 0.s0 0.s0 0.40 m aandlmsfk 0.s0 
n 0,20 0.10 O&#38;l . . .. YES ?-UI The actual values are tabulated below:  Benchmark Algorithm precision(y): 
0.77 prcziaion(?): 0.48 precision(N) :O.82 recall(Y): 0.73 rwall(?): 0.52 recalI(N): 0.80 faUout(Y): 
0.07 fallout(?): 0.19 faUout(N): 0.18 generality(Y): 0.25 generality(?): 0.25 generality(N): 0.50 Figure 
&#38; Recall by cate~ry 0.80 0,s0 0.70 0.s0 0.50 0.40 0.24 0.20 0.10 0.00 YES ?N) Figure 9: Fallout 
by catqpy 0.25 0.20 0.15 O.lc 0.05  dill 0.00 1 YES ? Kr TM Algorithm precision(Y): 0.85 precision(fi: 
0.59 precision(N) :O.80 recall(Y): 0.78 recall(?): 0.48 recall(N): 0.91 fallout(Y): 0.04 fallout(?): 
O.11 fallout(N): 0.23 generality(y): 0.25 generality(?): 0.25 generahty(N): 0.50 There was a net increase 
of 17% in precision and a net increase in reeall of 12%. The fallout decreased by 6%. The decrease in 
precision with respect to the N category deserves comment. Figure 10 depicts the breakdown of matches 
and mismatches within the category for both benchmark and tuned metric algorithms. Figure 10. Malches 
and mismatches by algcmthrn(N ealegory) We note that while TM actually identified 141 more of the unmatched 
pairs than the benchmark, this advantage was offset by only half as many mismatches. This illustrates 
the sensitivity of the precision measure to mismatches. In order to understand the underlying cause, 
it is helpful to determine the nature of the mismatches. TM mismatched 96 times in this category in situations 
where the benchmark algorithm made the correct assessment. The list in Figure 11 reveals typical differences. 
Figure 11, Cases where TM mismatched with e value of 0.20 (N-Y &#38; N­?) when the benchmark alaorithm 
matched {Y-Y &#38; 7-?) N-V N-? CORACORREA ALVARADG ALVAREZ C OROCORRAO AJUASARJZA PAFS PE.4SE CAHILL 
HILL PHILIJPS PHILP CANALES CANAIJZO CARREIROCURRIER CHARETIE CHARTER CHRISTENSON CHRISTt COATES COTE 
FEELY FOLEY FERRARtFERREtRA ITSCHERFISCHMAN GAGNE GAGON GENDRJZAUGENDRON GOULD GUILD HENDERSON HENDRICKSON 
L4J?.NAJANES KEELY JOLEY KIAMA KULMA IAFAIAM L4FIAMME tANGILLE IANGLEY MARJOMAURO MATEO MATOS MCKENNA 
MCKINNEY QUEEN QUINE RAZMORAMOS RASTANI RISTAINO REGAN RGEON SALWA SUWA SANDREW SAUNDERS SILVERMAN S1LVERST13N 
STEEN STONE Swm SWEtz In our opinion, further tuning was unwarranted for the results of TM more closely 
approximated our intuition than the test dataset. However, the tuning procedure would be the same as 
employed above. Conclusion The tunability of the Levenshtein metric makes it possible to tailor it to 
specific applications. Using a representative sample of the production data it can be adjusted to adapt 
to the types of corru ions that occur in that data. We have found that it signi lcantly r improves on 
an existing algorithm in a context where corrupted name pairs must be matched. Acknowledgements This 
research was supported in part by a grant from the ACXIOM corporation and grant # ASTA 91-A-02 from the 
Arkansas Science and Technology Authority. References <RefA>[1] <SinRef><author>Knuth, D. </author>(<date>1973</date>). <title>SortinR and Searching</title>. <booktitle>Reading</booktitle>: 
<publisher>Addison-Wesley</publisher></SinRef>. [2] <SinRef><author>Hall, P. </author>&#38; <author>Dowliig, G. </author>(<date>1980</date>). <title>Approximate String Matching</title>. <journal>Comuutin~ Survevs</journal>, 
<volume>12</volume>, <pages>381-402</pages></SinRef>. [3] <SinRef><author>Berghel, H. </author>(<date>1987</date>). <title>A Logical Framework for the Correction of Spelling Errors in Eiedronic 
Documents</title>. <journal>Info. Proc. and Mrzmt</journal>.. <volume>23</volume>,<pages>477-494</pages></SinRef>. [4] <SinRef><author>Berghel, H. </author>&#38; <author>Andreu, C. </author>(<date>1988</date>). <title>TALISMAN: A Prototype 
Expert System for the Detection and Correction of Spelling Errors</title>. <booktitle>Proc. 1988 ACM SvmD. on Small Svstems </booktitle>
(<pages>pp. 107-113</pages>). </SinRef>[5] <SinRef><author>Fautk, R. </author>(<date>1964</date>). <title>An Inductive Approach to Language Translation</title>. <journal>Communications of 
the ACM</journal>, <volume>7</volume>, <pages>647­ 653</pages></SinRef>. 135 [6] <SinRef><author>Glantz, H. </author>(<date>1957</date>). <title>On the Recognition of Information with a Digital Computer</title>. 
<journal>Journal of the ACM</journal>, <volume>4</volume>, <pages>178-188</pages></SinRef>. [7] <SinRef><author>Angell, R.</author>,<author> Freund, G:</author>, &#38; <author>Willett, P. </author>(<date>1983</date>). <title>Automatic Spelling 
Correetlon using a Trigram Similarity Measure</title>. <journal>Information Processing and Management</journal>, <volume>19</volume>, <pages>255-261</pages>.</SinRef> [8] 
<SinRef><author>Odell, M.</author> &#38;<author> Russell, R. </author>(<date>1918, 1922</date>).<title> U.S Patents </title>1,261,167 (<date>1918</date>) and 1,435,663 (<date>1922</date>). </SinRef>[9] <SinRef><author>Alberga, 
C.</author> (<date>1967</date>). <title>String Similarity and Misspellings</title>. <journal>Communications of the ACM</journal>. <volume>10</volume>,<pages>302-313</pages>. </SinRef>[10] <SinRef><author>Blair, C. </author>
(<date>1960</date>). <title>A Program for Correcting Spelling Errors</title>. <journal>Information and Control</journal>, <volume>3</volume>, <pages>60-67</pages>. </SinRef>[11] <SinRef><author>Davidson, L. </author>
(<date>1962</date>). <title>Retrieval of Misspelled Names in an Airlines Passenger Record System</title>. <journal>Communications of the ACM</journal>, 
<volume>5</volume>, <pages>169-171</pages>. </SinRef>[12] <SinRef><author>Muth, F. </author>&#38;<author> Tharp, A. </author>(<date>1977</date>). <title>Correcting Human Error in Alphanumeric Terminal Input</title>. 
<journal>Information Processing and Management</journal>. <volume>13</volume>, <pages>329-337</pages>. </SinRef>[13] <SinRef><author>Pollock, J. </author>&#38;<author> Zamora, A. </author>(<date>1984</date>), <title>Automatic 
Spelling Correction in Scientific and Scholarly Text</title>. <journal>CACM</journal>, <volume>27</volume>, <pages>358-368</pages>. </SinRef>[14] <SinRef><author>Levenshtein, V. </author>(<date>1966</date>). 
<title>Binary Codes Capable of Correcting Deletions, Insertions and Reversals</title>. <journal>~ Phvs, Dokl</journal>. .<volume>10</volume>, <pages>707-710</pages></SinRef>. [15] 
<SinRef><author>Lowrance, R. </author>&#38; <author>Wagner, R. </author>(<date>1975</date>). <title>An Extension of the String-to-String Correction Problem</title>. <journal>Journal 
of the ~, </journal><pages>177-183</pages>. </SinRef>[16] <SinRef><author>Wagner, R. </author>&#38; <author>Fischer, M. </author>(<date>1974</date>). <title>The String-to-String Correction Problem</title>. 
<journal>Journal of the ACM</journal>. <volume>21</volume>, <pages>168­ 178</pages></SinRef>. [171 <SinRef><author>Manber, U. </author>(<date>1989</date>). <title>Introduction to Als!orithms: a Creative Amxoach</title>. 
Reading: <publisher>Addison-Wesley</publisher>. </SinRef>[18] <SinRef><author>Fu, K. </author>(<date>1976</date>). <title>Error-Correcting Parsing for Syntactic pattern Recognition</title>. 
In <editor>Klinger</editor>, <editor>et al </editor>(Eds.), w Structures, Comuuter Graphics and Pattern Rccoznition. <location>New York</location>: <publisher>Academic 
Press</publisher></SinRef>. [19] <SinRef><author>Unman, J. </author>(<date>1977</date>). <title>A Binary N-Gram Technique for Automatic Correction of Substitution, Deletion, 
Insertion and Reversal Errors in Words</title>. <journal>~ Comuuter Journal</journal>, <volume>20</volume>, <pages>141-147</pages>. </SinRef>[20] <SinRef><author>Zamora, E., </author><author>Pollock, J., </author>
&#38; <author>Zamora, A.</author> (<date>1981</date>). <title>The Use of Trigram Analysis for SpelIing Error Detection</title>. <journal>Information Processing 
and Management</journal>. <volume>17</volume>, <pages>305­ 316</pages></SinRef>. [21] <SinRef><author>Salton, G. </author>&#38; <author>McGill, M. </author>(<date>1983</date>). <title>Introduction to Modem Information 
Retrieval</title>. <location>New York</location>: <publisher>McGraw Hill</publisher></SinRef>. [22] <SinRef><author>Salton, G. </author>(<date>1990</date>). <title>Automatic Text Processing. Cambridge</title>, <location>MA</location>: <publisher>MIT 
Press</publisher></SinRef>. [23]<SinRef> <author>Damerau, F. J. </author>(<date>1964</date>). <title>A Technique for Computer Detection and Correction of Spelling Errors</title>. 
<journal>Communications of the ACM</journal>. <volume>7</volume>, <pages>171-176</pages>.  </SinRef></RefA>
			
