
 3D Galatea: Entry of three-dimensional moving points from multiple perspeetive views Steven A. MacKay, 
Richard E. Sayre, and Michael J. Potel Department of Biophysics and Theoretical Biology Department of 
Radiology University of Chicago Chicago, minois 60637  Abstract We describe an interactive graphics 
system for the entry of three-dimensional moving points from multiple perspective views. This work represents 
a major extension of Galatea, our system for graphics-assisted 2D motion analysis. 3D Galatea permits 
reconstruction of 3D time-dependent positions from 2D entries in two or more perspective views. The system 
supports a general approach for calibrating perspective views. This method, based on work of Sutherland, 
uses a known 3D reference object to calibrate completely arbitrary perspective projections. A somewhat 
restricted class of perspective views may be calibrated without an explicit calibration object using 
another approach developed from photogrammety. In 2D Galatea, we have used an animated graphics overlay 
onto the source image to give the analyst feedback regarding current and previous data entries. This 
capability is extended in 3D Galatea by overlaying auxiliary lines, which are the backprojections of 
previous 2D entries from one view into other views. This concept amounts to a fourth interpretation of 
the well-known Roberts homogeneous matrix equation describing perspective projections of 3D space into 
a 2D image. The auxiliary line is useful in locating a point which is obscured in one of the images, 
or in determining the correspondence of projected points as seen in different views, which may be ambiguous 
or easily confused. CR Categories and Subject Descriptors. 1.3.6 [Computer Graphics] : Methodology and 
Techniques-Interaction techniques. 1.4.8 [Image Proecssing] : Scene Analysis-Range data; Stereo; Time-varying 
imagery. J.3 [Life and Medical Sciences] -Biology; Health. Permission to copy without fee all or part 
of this material is granted provided that the copies are not made or distributed for direct commercial 
advantage, the ACM copyright notice and the title of the publication and its date appear, and notice 
is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, 
or to republish, requires a fee and/or specific permission. (~) 1982 ACM 0-89791-076-1/82/007/0213 $00.75 
 Introduction Many scientific problems require the acquisition and analysis of three-dimensional (3D) 
x,y,z coordinate positions. Such application arise in biomechanics (joint movement), medicine (blood 
vessel or tumor positions), anatomy (bone structure), behavioral ecology (fish schools or bird flocks), 
and many engineering fields (automotive, aerospace, and aerial survey). Three-dimensional input devices 
are useful if the object of interest can be measured directly, and is of appropriate size. However, most 
scientific problems requiring 3D data entry have as their primary source planar images (photographs, 
movie films, X-ray films, video images, etc.). These planar images are two-dimensional perspective projections 
of the original 3D objects, and the input problem becomes one of inferring three-dimensional positions 
from these planar projections. In various application areas, ad hoc solutions to this problem are the 
rule. Techniques of photogrammetry or stereology [ 24,21] and devices such as stereo comparators are 
adapted to work for particular multiple view situations, such as stereo pairs, orthogonal pairs or other 
fixed geometry imaging situations. However, a general solution requires the ability to support arbitrary 
perspective views arising from non-specific, possibly moving camera positions, inference of view descriptions 
subsequent to imaging, reconstructions from more than 2 views, and high quality interactions and well-defined 
procedures for the analyst. The work reported here describes an interactive animated computer graphics 
system for tracking points through time in three-dimensions from multiple perspective views. This system, 
called 3D Galatea, is an extension of our work with the Galatea system for entering time dependent 2D 
data [3,10,14,13,12]. The principal mathematical formulations on which the system is based are familiar 
to computer graphics practitioners from their use in data output, in the generation of perspective displays. 
Our system illustrates how these same formulations are the basis for data input techniques, for 3D data 
describing phenomenon sampled through perspective views. Our data acquisition system provides several 
advances on previous entry systems. First, it provides a completely generalized approach to the calibration 
of the perspective view [ 22,19,20]. Our system makes use of a calibration object which is imaged in 
the same system as the unknown object. When data entry begins, positions in the calibration object image 
are entered and used to infer a mathematical description of the perspective views. The resulting calibrated 
views may be completely arbitrary -orthogonal, stereo, oblique, whatever. The views are inferred a posteriori, 
that is, the imaging system may be  positioned arbitrarily, with the resulting perspective views calibrated 
after-the-fact. Moreover, the reconstruction geometry is not restricted to two views, but may be comprised 
of any number of views, with more views increasing the accuracy of the inferred 3D positions. A somewhat 
restricted class of perspective views can be calibrated up to a scale factor without an explicit calibration 
object if special conditions exist in the imaging system. If two views are produced with identical cameras, 
a single moving camera, or a moving object, and the location of the projection of the optical axis on 
the image is known, then the relationship between the views may be determined by entering the corresponding 
pairs of image points from a number of unknown object points [ 5]. A second advance of our system is 
that it makes use of an animated computer graphics display superimposed onto the film image, using a 
projection crt and series of mirrors [13]. The computer is interfaced to both the film projector and 
a data tablet, and the generated graphics display shows changes in both devices. Thus, when the analyst 
moves the cursor on the data tablet, he sees its location displayed on the film image. Also, as the film 
projector runs, the animated display changes in frame-by-frame synchrony. In effect, the system acts 
like "dynamic tracing paper" [i0]. This allows the accuracy of the entries to be constantly checked and 
corrected if necessary. The graphics overlay of previously entered pointsmakes it easy for the analyst 
to avoid omission or duplication of points and to resolve ambiguities in the orientation and correspondence 
of points. The most powerful capability provided by the graphics overlay is the display of backprojections 
of positions from one view onto other views. When the position of a point is entered in one 2D perspective 
view, it is constrained to lie on a line in 3D space (Figure la). Using our 3D reconstruction mathematics, 
the equation of this line can be computed and its 2D projection superimposed onto other views using the 
graphics overlay. The point in question must lie along these "auxiliary lines" in the other views. The 
auxiliary lines greatly facilitate locating the corresponding positions in the other views. For example, 
one of the primary applications of this system is the study of the 3D motion of the heart. The points 
of interest are the branch points of large and small coronary arteries, and these bifurcations are occasionally 
on-edge or obscured in one view. But, as long as the bifurcation can be entered in any of the views, 
its position can be entered in the other views as the intersection of the auxiliary line and the large 
vessel. This significantly increases the number of usable points, since points that cannot be entered 
for just a few frames in any view are rendered useless for other data entry methods. The auxiliary line 
technique also is of immense value in identifying which are the corresponding points in the two views 
even when they are clearly seen. Without the auxiliary line, this determination is usually the most time- 
consuming aspect of multi-view data entry and very easy to confuse, especially in applications where 
there is little external information (anatomy, connectivity, or other structure) to guide the correspondence. 
 Possibly the most important aspect of our system is that it provides a high quality interactive graphics 
context for the difficult problem of working with multiple views of time-dependent 3D objects. The analyst 
is presented with 2D moving points, 3D moving points, and perspective view descriptions as graphical 
data types [ 13] to be manipulated. By supporting these abstractions as primitives, the system provides 
a natural conceptual framework for manipulating image views and seeing the relationship between his 2D 
interactions and the 3D object space. In this paper, we will first discuss the overall methodology of 
3D data entry, including the mathematical basis for the 3D reconstructions. Then, we will describe the 
details of how the system is used to calibrate the 3D perspective views and reconstruct 3D positions 
from 2D entries in the perspective views. Finally, we will summarize some of our applications of the 
resulting 3D time-series data. Methodology Our method of entering 3D positions by marking 2D locations 
in multiple perspective views involves four major steps (shown in Fig. 1 a-e). (1) The perspective views 
are calibrated. In the general approach, a calibration object containing points whose 3D positions are 
known is imaged with the same system used to image the unknown object. The analyst enters the 2D projected 
points of this object using the computer system. Fig. lb shows what the analyst sees while he is calibrating 
the view. In the center of the figure is the source image, a frame from an X-ray movie, with an image 
of a typical calibration object, a plexiglas cube with imbedded metal shot and rods at known 3D positions. 
Superimposed on the source image is the computer graphics image, containing command menus and status 
information which are described below. The superimposed cross shows the position of the cursor, which 
is on top of one of the cube points. (2) After determining the view transformations, the analyst enters 
points from the first view. Fig. lc shows what the analyst sees during first view data entry. The source 
image is a frame from a X-ray movie film of a human heart. Radiopaque dye is injected into the coronary 
arteries to make them visible as the light curves in the film. The overlaid cross indicates a point being 
entered at the bifurcation of two vessels at the center of the image. (3) After entering points from 
the first view, the analyst enters points from subsequent views, aided by the auxiliary line, which is 
calculated from the view transformations and the coordinates of a 2D point in the first view. Fig. id 
shows a second view of the same coronary arteries as in Fig. lc, with the auxiliary line and other computer 
graphics overlaid. The auxiliary line intersects the bifurcation near the right edge of the image, indicating 
to the analyst the point entered from the first view. (4) Once projected points of at least two views 
of an unknown 3D point are entered, the 3D coordinates of the point can be reconstructed. The geometric 
relationship between the 2D points [UlVl] , [u2v 2] and the 3D point [xyz] appears in Fi~. la. ~mce much 
of our interest lies in examining the movement of points in three dimensions, the above process is carried 
out for sequences of 2D points entered over many frames of film, resulting in many time series of 3D 
points. These line series can be utilized in a number of analyses. Fig. le is a frame from a stereo pair 
animation of heart wall motion, with the 3D points connected by lines which approximate the coronary 
arteries, and vectors representing the 3D velocity of each of the wall points.  Theory For a particular 
perspective view, the relationship between a 3D point [x,y,z] and its projected point [u,v] is given 
by the matrix equation: [xyz 1] [T] =k [uv 1] , (I) where k is a scaling factor, [T] is the 4 x 3 transformation 
matrix which describes the perspective view, and [x y z 1] and [u v 1] are homogeneous coordinate representations 
of the 3D and 2D points, respectively [17,22,18,8]. The view described by [T] can represent arbitrary 
linear operations on 3D space. Any nonlinear aspects such as projection onto a curved surface (e.g., 
the face of a cathode ray tube) must be handled by polynomial corrections (see below). But otherwise, 
the perspective matrix can describe the complete transformation from actual 3D positions imaged from 
any arbitrarily placed camera to an arbitrarily oriented film plane. Furthermore, the method will also 
correctly handle the subsequent projection of this film image onto an arbitrarily oriented screen, and 
data entry with an arbitrarily oriented 2D digitizing device. Roberts' [17] initial interpretation of 
equation (1) was that it described the projection of known 3D points into 2D. Sutherland [22] provided 
two additional interpretations. If many values of Ix y z] and the corresponding [u v] are known, then 
IT] can be computed in a process called view calibration. Sutherland's second interpretation was that 
if the projected [u v] coordinates of a 3D point are measured for at least two calibrated views, then 
the 3D coordinates of that point can be reconstructed. We provide a fourth interpretation of (1) [ 19,20], 
which allows us to calculate the position of the backprojected auxiliary line if one [u v] and two views 
are known. View ealibration To determine the perspective transformation for a view, the 2D projected 
positions [u:,v:] of points in the calibration object with known 3D lo~a{ions [xl,y~,z i] are entered. 
Each point entered must satisfy equatlor~ (5): [xiYiZi 1] IT] =k [u iv i I] (2) This is a series of 
three linear equations, which can be reduced to two equations by solving for the scaling factor k and 
substituting: (tll -t13ui)xi + (t21 - t23ui)Y i + (t31 - t33ui)z i + (t41 - t43u i) = 0 (t12 - t13vi)xi 
+ (t22 - t23vi)Yi + (3) (t32 - t33vi)z i + (t42 - t43v i) = 0 th th where t.. is the element of [T] 
in the i row and the j column. II Each of the equations (3) represents a plane in three dimensions, and 
the intersection of the two planes in (3) is a 3D line passing through li,.Yi,Z i and ui, vi, shown as 
one of the dashed lines in Fig. If the 2D projections of n 3D points in the calibration object are measured, 
the n pairs of equations of the form of (3) may be solved simultaneously for the unknowns t... This system 
of equations can be represented [~ matrix form as: [A] [T c] = t43[B] (4) where [A] is a 2n x 11 matrix, 
[T c] is an 11 element column matrix containing eleven of the twelve elements of the transformation matrix 
[T] , and [B] is a 2n column matrix. The twelfth matrix element t.^ is an overall scale term and may 
be taken to be equal ~o J 1 without loss of generality. This system of equations is in ii unknowns (the 
elements of [T_], therefore, if 6 or more 2D points are measured, then ~he system is overdetermined and 
can be solved in a least squares sense by using a generalized inverse [4] : [T c] = ([A]t[A])-I[A]t[B] 
(5) which gives the elements of the transformation matrix [T] [22,18]. Several methods are available 
for calibrating a smaller class of perspective views without resorting to a special calibration object 
[23]. In the particular method we have used, the principal restriction imposed upon the views is that 
view 1 can be transformed into view 2 by simply a 3D translation of the center of projection followed 
by a 3D rotation about the new center of projection [5,7]. Any scale changes brought about by different 
camera systems, or subsequent projection must be known independently. Also, the principal point, the 
point on the image plane where the optical axis of the system falls, must be known, as is the case in 
photogrammetric systems. In spite of these restrictions, this method is useful for situations, such as 
surveying, where it is not possible to place a calibration object in the field of view. The method requires 
that 8 corresponding pairs of image points be entered from any unknown object points. The 2D coordinates 
of these 8 points in each of the two views allows the translation and rotation operations to be calculated. 
This calibrates the two perspective views to within a scale factor, which can be resolved if some absolute 
distance is known in the object of interest. 3D Point Reconstructions After the transformation matrices 
[S] for view 1 and [T] for view 2 have been determined, the projected 2D points [ul,v I] and [u2,v 2] 
in each of the two views correspondin~to-the 3D pomt of interest can be measured and the coordinates 
of the 3D point [x, y, z] reconstructed. It is possible to write 2 pairs of equations like (3), which 
determine two lines in three dimensions (the two dashed lines in Fig. la). If the elements of [S] are 
denoted by s i. and those of [T] by tij, then these equations can be v~ritten in matrix form as: s11-s13u 
I s21-s23u1 s31-s33u 1 xl s43u1-s41 s12-s13vl s22-s23v1 s32-s33v 1 yJ s43v1-s42 (6) t11-t13u 2 t21-t23u 
2 t31-t33u 2 zl t43u2-t41 t12-t13v 2 t22-t23v 2 t32-t33v2 This matrix equation represents a system of 
4 equations in 3 unknowns [x,y,z] and thus, as in (5) above, the solution is found in a least squares 
sense by a generalized inverse. If more than 2 views are available, the additional views just add more 
rows to equation (6), and the least-squares solution for [x,y,z] is that much better determined.  Computer 
Graphics Calculation of auxiliary lines Fig. la shows another way equation (6) can be interpreted. If 
the two views [S] and [T] are known, and a 2D projected point [ul,vl] in view I ([S]) are entered, then 
a line is defined*in'three dimensions which passes through [ul,vl] and the 3D point [x,y,z], as shown 
by one of th6 d~shed lines in Fig. la. This 3D line can then be backprojected into the second view ([T]) 
as a 2D line on which [u~,v~] must lie. This projected line can also be calculate0 fFom equation (6), 
which represents an overdetermined system of 4 equations in 3 unknowns [x,y,z]. If one of the rows of 
(6) was eliminated, it would still be possible to solve for [x,y,z]. If u I and v I have been specified 
in the first view, then u 2 ~ selected as, say, the u coordinate of the left edge of the film image. 
This system of 3 equations is solved for [x,y,z] , which is projected through transform [T] to produce 
[ug, vg], the intersection of the auxiliary line with the left e~gd'of the film image. This process is 
repeated for any of the other three edges of the film image to give the position of the auxiliary line 
projected into view 2. Each additional view generates another auxiliary line. All the auxiliary lines 
will, given correct data entry, intersect at the projection of the 3D point of interest. Use of graphical 
overlays From the above discussion, one can see that to calibrate a view, the analyst must enter the 
locations of the projections of several 3D points, and must define at least 2 such views in order to 
correctly reconstruct unknown 3D points from the locations of their 2D projections. Clearly, the positions 
of 2D projected points must be entered correctly in both the calibration and reconstruction phases in 
order to compute the 3D points correctly. Thus, the system should provide information to the analyst 
about the accuracies of the calibration and of the 3D reconstructions as the 2D data entries proceed. 
The other requirement for correct 3D reconstruction is that the 2D projected points entered from different 
views must correspond to the same 3D point. Our experience has shown that in many instances the points 
of interest are obscured or ambiguous in one of the views. Therefore, it is extremely useful for the 
system to provide information about the possible positions of a 2D projected point in one view given 
the location of a 2D projected point from another view. To solve both the accuracy problem and the correspondence 
problem, we have used an animated computer graphics display superimposed onto the film image and synchronized 
with the film projector [13,6]. The source image (movie film, photographic slide, etc.) is front-projected 
onto a large (110 x 120 cm) screen, and a specially-constructed projection crt (based on a RCA 4862 projection 
kinescope and Schmidt reflective optics) is used to overlay the graphics image directly onto the film 
image. The projection crt is driven by a vector graphics display processor (DEC VT-11), which produces 
a points- and--lines graphics image. This image is updated in real- time by the system minicomputer (DEC 
PDP-II/40 with 56 kilobytes of memory and two 2.4 megabyte disk cartridges). The analyst uses a data 
tablet (Summagraphics) to move a cursor in the graphics overlay for entering positions and controlling 
the system. As the film projector runs, the animated display changes in frame-by-frame synchrony with 
the film image. For example, as a moving point is tracked by the analyst, the graphics overlay superimposes 
the corresponding moving data point being recorded in each frame. Additionally, the entire collection 
of previously entered points are  Volume 16, Number 3 July 1982 continually displayed and updated as 
the film is advanced, helping the analyst enter more points without inadvertant omission or duplication. 
The graphics overlay permits other useful displays such as connectivity of points, allowing the analyst 
to use the structure of the object of interest as an aid in locating, identifying, and tracking points. 
The graphics overlay provides a major contribution toward resolving the correspondence problem by superimposing 
auxiliary lines onto the source image. After a point is entered in one view, the location of the auxiliary 
line is calculated as described above, and is used by the analyst in locating the position of the corresponding 
projected point in the other view (see Fig. id). Frequently, the auxiliary line passes through an obvious 
point and the correspondence is known immediately. Sometimes there may be more than one candidate point 
on the auxiliary line, but the correct one can usually be resolved easily by gross structural considerations, 
or by running the film and observing what point stays on the auxiliary line. If a third view is available, 
it can be used to resolve such ambiguities, since the two auxiliary lines will intersect at or near the 
correct point. Of course, the auxiliary line is of further help to the analyst in indicating whether 
an accurate 3D position is being obtained. User interface In addition to serving as an aid to the data 
entry, the graphics overlay provides the medium by which the analyst interacts with and controls the 
execution of the system. The graphics display is divided into two areas, a large data area and a small 
strip along the top edge which contains the command menu (Fig. 1b-d). The bottom part of the display 
contains continually updating information regarding the system status, including the location of the 
digitizing cursor, the number of points already entered, the frame number of the film projector, the 
current view, the current data instance, and other items of interest to the analyst. This status information 
is drawn in a part of the data area so this region is still open for data entry. The analyst initiates 
actions and controls the execution of the system through the command menu. The menu contains 35 buttons 
on 6 rows, but to reduce the amount to space taken from the data area, only 1 row is visible at a time. 
The other rows are accessed by hitting the leftmost button in each row (marked ROLL). The placement of 
buttons on rows is organized so that all buttons on a row have a similar function or are used during 
the same phase of data entry. During the view calibration phase, row 1 (visible in Fig. ib) is used. 
The buttons on the row allow the analyst to select the current calibration point (INC CP#, DEC CP#), 
to delete mistaken entries (DEL CP#, DEL PT), and to use the entered 2D points to infer a view transformation 
(INFER). When the analyst hits the button marked INFER, information about the 2D points just entered 
and the transformation matrix just calculated is displayed. The system will also reproject all the 3D 
calibration points according to the inferred transformation matrix and superimpose them onto the film 
image using the graphics overlay. This aids the analyst in assessing the error associated with individual 
point entries and allows detection of the most common types of mistakes made in the calibration process, 
which are misidentification of 3D points and poor entry of a specific point. If these mistakes are committed, 
then specific points will show large individual errors, which the analyst corrects by reentering the 
offending point. The reprojection of all the 3D calibration points after a subset  of them has been 
entered is often of great value in helping the analyst locate and properly identify the remaining, possibly 
more difficult to find, calibration points. Row 2 allows the analyst to control the transition between 
view calibration and unknown point entry. Command buttons in this row enable the analyst to turn on and 
off the graphics display (BLANK/SHOW), to save the current transformation matrix (NEXT VIEW), and to 
terminate calibration entry (CAL DONE). The command buttons in rows 3-6 are used during the actual entry 
of 3D points. The data entry is organized by instances [ ii]. A 2D data instance consists of one u,v 
coordinate pair in each frame of a particular view. Thus, the analyst uses one 2D instance to track one 
projected point as it moves in one view and uses the instance number to distinguish that moving point 
from all others. Two or more corresponding 2D instances are then used along with the view calibration 
information to reconstruct a 3D moving points instance [ 13]. The system supports two tools for entering 
instances. The first is a marking tool: data is entered on cursor depressions in the data area, and is 
used for entering static points. The second tool is a tracking tool, used for following moving objects. 
This tool is first turned on (using a command button) and the point of interest is followed with the 
cursor as the film moves. A u,v coordinate pair will be entered on every film projector frame change 
until the tool is turned off (using another command button). The graphics display changes from blinking 
to steady and the cursor changes its appearance when the tool is turned on to inform the analyst of the 
change. Row 3 of the menu (visible in Fig. ic) deals mainly with the active instance, the 2D instance 
currently being entered. By hitting buttons in this row, the analyst is able to: turn the active instance 
on or off (INST ON/OFF); write the active instance to a file and prepare the system for entering a new 
instance or re-entering an old instance (NEXT INST, PREV INST); delete all data previously entered for 
the active instance (DEL INST); and terminate system execution (EXIT). For entry of time dependent data, 
a temporal calibration must be performed in addition to the spatial calibration discussed above. If the 
views are obtained on the same piece of movie film or videotape (e.g., by using a mirror), then the temporal 
calibration is trivial. But, if the views are recorded on different pieces of film (e.g., by different 
cameras), then temporal calibration consists of determining when the same real time point occurs in each 
view so that a time base can be assigned to each film clip. The system monitors projector frame changes 
to determine the time to associate with each data entry, so control of the temporal aspect of data entry 
amounts to control of the projector frame number. Row 4 of the menu gives the analyst this control by 
allowing him to: set the current frame number to zero, or to one of the time bases (FR#=0), FR#=BI, FR#=B2); 
and, increment or decrement the frame number (INC FR#, DEC FR#), which is useful for changing the frame 
number without moving the film in the projector. Row 5 of the command buttons deals mainly with the auxiliary 
instances, which are previously entered 2D instances from other views. Data from these are used to calculate 
the position of auxiliary lines which will appear across the screen. In addition to the auxiliary lines, 
a continuously updating single line (visible below the command buttons in Fig. ld) shows the 3D coordinates 
(as determined from the position of one of the auxiliary points and the current cursor position) and 
an error measure (the distance from the cursor position to one of the auxiliary lines). If an auxiliary 
line is off the screen, or if there is no auxiliary data for a certain frame, warning messages to that 
affect will appear on the screen in place of this single line display. By hitting buttons in row 5, the 
analyst is able to: activate the auxiliary line features (AUX ON); change which auxiliary instances are 
being used (INC AUX, DEC AUX); and, produce a set of 3D points (MAKE 3D). Row 6 of the command buttons 
allows the analyst to select the view being entered. In the default configuration, the view number is 
determined by the 2D instance number (for example, if two views are being used, all odd 2D instances 
are assumed to be from view 1, and all even instances from view 2). The analyst can override the default 
using the command buttons on row 6 and: establish a relationship between frame number and view number 
(FR# = VIEW) (e.g., all data in a certain range of frames is assumed to be from a particular view); or 
independently set the view number (INC VIEW, DEC VIEW, SET VIEW); or restore the default relationship 
between instance numbers and view numbers (INST# = VIEW). Aeeuraey of entered data The accuracy with 
which 3D points can be entered using this system was checked with the following experiment. X-ray images 
of an 8 cm calibration cube with 15 pieces of lead shot (diameter 0.125 cm), were taken from two approximately 
orthogonal views. The two views of this cube were calibrated using some of the visible points as known 
points and two transformation matrices were constructed. The rest of the 15 points were then entered 
as unknown points and 3D coordinates were calculated. The average ~istance between the points as measured 
directly on the cube and as entered using this system was less than 0.03 cm, smaller than the size of 
the shot. Although this experiment indicates that the accuracy of this data entry system can be very 
high, several sources of error have been identified in this method, and steps have been taken to reduce 
their impact. One principal problem arises from geometric distortions introduced by the projection crt 
in the film analysis system, and by certain types of imaging equipment (e.g. video image intensifiers). 
These devices introduce pincushion distortions into the image, causing straight lines to appear curved 
inward. This distortion can be corrected by placing a grid on the large front screen or in the imaging 
system. The relation between the measured distorted coordinates and the known undistorted coordinates 
is estimated by a 3rd order polynomial, where the coefficients of the polynomial are chosen to minimize 
the squared distance between the polynomial estimate of the undistorted position and the actual position 
[16]. This polynomial is then used to compute the undistorted positions of 2D points from the distorted 
coordinates entered using the system. At its maximum, in the corners of the projection crt, the distortion 
is less than 7% of the size of the image. Another potential source of error involves registration of 
the film image from frame to frame. Slight shifts in frame registration could be magnified into large 
errors in the space of the unknown object. Some of our preliminary work was performed using a 16mm film 
projector without pin registration, and such registration shifts were noticed. We solved this problem 
by acquiring a pin-registered 35mm film projector (Vanguard Film Analyzer), which has great spatial resolution 
and registration accuracy. Another solution supported by the data entry system is to enter fiducial marks 
and correct for the frame-by-frame 2D misregistration. Applieations Because our 3D data entry system 
permits the use of arbitrary perspective projections, a number of different imaging systems can be utilized 
which generate the multiple views in a variety of ways, depending on the requirements of the application. 
For example, it is not necessary to use two camera systems to record two views simultaneously. By using 
a mirror (Figure 2a), two views of the object can be recorded by one camera, eliminating the need to 
synchronize multiple movie cameras or to provide timing events in the film. A similar technique to the 
use of a mirror is to use the shadow of the object as the second view, where the sun or a light source 
provides a different center of projection [2,9]. If the user supplies a connectivity between the points 
in the object, the system can produce an animated stick figure, which is a useful characterization of 
human or animal figures for locomotion studies. Figure 2b shows the stick figure as a stereo pair from 
an overhead view, which is different from either original view. The data entry system also supports the 
use of more than two views. Two views of any point are all that are required to reconstruct 3D coordinates, 
but often some object points are obscured in one or more of the views, and multiple views may be necessary 
to visualize all the points. If multiple views are used, easier verification of entries is possible, 
since the data entry system will produce more than one auxiliary line. If the entries of the previous 
views are correct, the auxiliary lines will intersect at the point of interest. If the auxiliary lines 
do not agree, the different 2D positions can be reentered, and their auxiliary lines examined in the 
other views, with an overall consensus reached through successive refinement. The use of multiple views 
is illustrated in Figure 3. These four images were produced by moving a single camera around this static 
object. Figure 3a shows the entry of the first view, 3b-d the second, third, and fourth views. Each new 
view has an additional auxiliary line to aid entry. The analyst can verify that previous views were entered 
correctly because the auxiliary lines intersect at the point of interest (clearly seen in Fig. 3c). Once 
2D entries in the three views have been made, each view can be examined to confirm that the 2D point 
entered on the point of interest in the film falls on the intersection of the two auxiliary lines. The 
object used for calibrating the views need not be specially constructed. If the 3D coordinates of some 
points on the object of interest are known, they may be used as calibration points, and the calibrated 
views then may be used to measure the 3"19 locations of other points on the object. In Figure 4, the 
object of interest is the city of Chicago, specifically the 3D coordinates of the tops of the city's 
major buildings. The 3D coordinates of 8 downtown buildings were determined using a street map (for the 
x-y position) and an almanac (for the building height, or z coordinate). The tops of these buildings 
as seen in photographs taken from five locations around Chicago were entered as calibration points, and 
the calibrated views were used to find the 3D coordinates of the tops of other buildings. Figs. 4a-e 
are five views of the city showing 4 auxiliary lines intersecting at the op of Sears Tower, Chicago's 
tallest building. For more than two years, the primary application of this data entry system has been 
the analysis of 3D motion of human and animal hearts. In this case the imaging system is a pair of X-ray 
source/image intensifier/movie camera trains which produce two synchronized, approximately orthogonal 
views. In human hearts, branch points of the coronary arteries (Fig. lb,c) have been the principal objects 
tracked, typically one to three dozen points for one hundred to three hundred film frames [ 1,15]. The 
animal studies have been performed in dogs, using surgically implanted endocardial and epicardial metallic 
markers. We have been able to study the speeds and directions in which different regions of the heart 
wall move (Fig. le), and show that these motions are affected in diseased hearts and by drug interventions. 
 Summary We have described an interactive computer graphics system used for entering the 3D positions 
of moving points from arbitrary multiple perspective views. This system has several features which allow 
large amounts of 3D data to be gathered quickly and accurately. The graphical overlay, especially the 
auxiliary line, have proven essential for the recognition of corresponding points in the different views 
which is the most difficult task for the human analyst. By supporting 2D data instances, 3D data instances, 
and perspective view descriptions as basic system primitives, 3D Galatea provides a rich graphics environment 
for relating 2D interactions and 3D data input. Aeknowledgments This work has been supported by grants 
from the Goldblatt Cancer Research Foundation, NIH grants CA- 14599 and HD-07136, and by the Galatea 
Computer Graphics Facility of the University of Chicago. The assistance and comments of Dr. Jonathan 
M. Rubin and Dr. Alex M. Aisen, Department of Radiology, University of Chicago, are appreciated.  Referenees 
 <RefA>1. <SinRef><author>Aisen, A. M.,</author> <author>A1-Sadir, J., </author><author>MacKay, S. A., </author><author>Potel, M. J., </author><author>Rubin, J. M., </author>and <author>Sayre, R. E. </author><title>Quantitative 
ventricular wall motion analysis from biplane coronary angiograms</title>. <booktitle>Proc. IEEE Computers in Cardioloffy</booktitle>, 
(<date>1980</date>), <pages>443-446</pages></SinRef>. 2. <SinRef><author>Cullen, J. M., </author><author>Shaw, E., </author>and <author>Baldwin, H. A. </author><title>Methods for measuring the three-dimensional 
structure of fish schools</title>.<journal> Animal Behaviour </journal><volume>13</volume>, (<date>1965</date>), <pages>534-543</pages></SinRef>. 3. <SinRef><author>Futrelle, R. P.</author> and<author> Potel, M. J. 
</author><title>The system design for Galatea, an interactive real-time graphics system for movie and video analysis.</title> 
<journal>Computers and Graphics </journal><volume>1</volume>, (<date>1975</date>), <pages>115-121</pages></SinRef>. 4. <SinRef><author>Graybill, F. A. </author><title>Introducton to Matrices with Applications 
in Statistics</title>. <location>Belmont, CA.: </location><publisher>Wadsworth Publishing </publisher>Co. (<date>1969</date>). </SinRef>5. <SinRef><author>Longuet-Higgins, H. C.</author> <title>A computer algorithm 
for reconstructing a scene from two projections</title>. <journal>Nature</journal> (<location>London</location>) <volume>293</volume>, (<date>1981</date>), <pages>133-135</pages>.</SinRef> 6. <SinRef><author>MacKay, S. 
A. </author><title>A system for graphics assisted entry of 3D points</title>. <booktitle>Proc. IEEE 4th Int'l. Computer Software and Applications 
Conf~ (COMPSAC'<volume>80</volume>)</booktitle>, (<date>1980</date>), <pages>128-134</pages></SinRef>.  7. <SinRef><author>Merchant, J.</author> <title>Exact area registration of different views of 
a common object scene</title>. <journal>Optical Engineering </journal><volume>20</volume>, (<date>1981</date>), <pages>424-436</pages></SinRef>. 8. <SinRef><author>Newman, W. M.</author> and <author>Sproull, R. F. </author>
<title>Principles of Interactive Computer Graphics</title>, (<volume>2</volume>nd edition). <location>New York</location>: <publisher>McGraw-Hill</publisher>, (<date>1979</date>).  </SinRef>9. <SinRef><author>Partridge, 
B. L. </author>and<author> Cullen, J. M. </author><title>A low cost interactive coordinate plotter</title>. <journal>Behavior Research Methods and Instrumentation </journal>
<volume>9</volume>, (<date>1977</date>), <pages>473-479</pages></SinRef>. 10.<SinRef> <author>Potel, M. J. </author>and <author>Sayre, R. E. </author><title>Interacting with the Galatea film analysis system</title>. 
<journal>ACM Computer Graphics i__</journal><volume>00 </volume>, (<date>1976</date>), <pages>52-59</pages></SinRef>. 11. <SinRef><author>Potel, M. J. </author>and <author>Sayre, R. E. </author><title>Data environment for 
a laboratory film analysis system</title>. <booktitle>Proc. IEEE 1st Int'l. Computer Software and Applications Conf. (COMPSAC 
'<volume>77</volume> t)</booktitle> (<date>1977</date>), <pages>800-806</pages></SinRef>. 12. <SinRef><author>Potel, M. J., </author><author>MacKay, S. A., </author>and <author>Sayre, R. E. </author><title>Galatea, an interactive computer 
graphics system for movie and video analysis</title>. <booktitle>Proc. 15th Int'l. Cong. on High Speed Photography and Photonics</booktitle>, 
<publisher>in press</publisher>, (<date>1982</date>). </SinRef>13. <SinRef><author>Potel, M. J., </author><author>Sayre, R. E., </author>and <author>MacKay, S. A. </author><title>Graphics tools for interactive motion 
analysis</title>. <booktitle>Comput. Graphics Imag. Proc. </booktitle><volume>1_44</volume>, (<date>1980</date>), <pages>1-23</pages></SinRef>. 14. <SinRef><author>Potel, M. J., </author><author>Sayre, R. E., </author>and <author>Robertson, 
A. </author><title>A system for interactive film analysis</title>. <journal>Comput. Biol. Med. </journal>_<volume>9</volume>, (<date>1979</date>), <pages>237-256</pages></SinRef>. 15.<SinRef> <author>Potel, M. J., </author>
<author>Rubin, J. M., </author><author>MacKay, S. A., </author><author>Aisen, A. M., </author><author>Al-Sadir, J., </author><author>Sayre, R.E. </author><title>Evaluation of heart wall motion 
in 3D using biplane coronary angiograms</title>. <journal>Investigative Radiology </journal><volume>1__66</volume>, (<date>1981</date>), <pages>5423</pages></SinRef>. 16. <SinRef><author>Pratt, W. 
K. </author><journal>Digital Image Processing</journal>. <location>New York</location>: <publisher>Wiley</publisher>, (<date>1978</date>). </SinRef>17. <SinRef><author>Roberts, L. G. </author><title>Homogeneous matrix representation 
and manipulation of n-dimensional constructs</title>. <journal>Document MS </journal><volume>1405</volume>, <institution>Lincoln Laboratory, MIT</institution>, <location>Cambridge, MA., </location>
(<date>1965</date>). </SinRef>18. <SinRef><author>Rogers, D. F. </author>and <author>Adams, J. A. </author><title>Mathematical Elements for Computer Graphics</title>. <location>New York</location>: <publisher>McGraw-Hill</publisher>, 
(<date>1976</date>). </SinRef>19.<SinRef> <author>Rubin, J. M. </author>and <author>Sayre, R. E. </author><title>A computer-aided technique for overlaying cerebral angiograms 
onto computed tomograms</title>.<journal> Investigative Radiology </journal><volume>13</volume>, (<date>1978</date>), <pages>362-367</pages></SinRef>. 20. <SinRef><author>Sayre, R. E., </author><author>Rubin, J. M., </author>
<author>Duda, E. E., </author>and <author>Patronas, N. J.</author> <title>Quantitative three-dimensional angiograms: application, including augmentation 
of computed tomograms</title>.<booktitle> Proc. IEEE Conf. on Computer Applications in Radiology</booktitle>, (<date>1979</date>),<pages> 95-102</pages>.</SinRef> 21. <SinRef><editor>Slama, 
C. C., </editor><editor>Theurer, C., </editor><editor>Henrikson, S. W., </editor>(eds). <title>Manual of Photogrammetry. American Society of Photogrammetry</title>, 
<location>Falls Church, VA., </location>(<date>1980</date>). </SinRef>22. <SinRef><author>Sutherland, I. E. </author><title>Three-dimensional data input by tablet</title>. <booktitle>Proc. IEEE 
</booktitle><volume>62</volume>, (<date>1974</date>), <pages>454-461</pages>. </SinRef>23. <SinRef><author>Unman, S. </author><title>The Interpretation of Visual Motion</title>. <publisher>MIT Press</publisher>: <location>Cambridge and London</location>, 
(<date>1979</date>). </SinRef>24. <SinRef><author>Underwood, E. E. </author><title>Quantitative Stereology</title>. Reading, <location>MA</location>.: <publisher>Addison-Wesley</publisher>, (<date>1970</date>).  </SinRef></RefA>
			
