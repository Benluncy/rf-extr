
 e-approximate linear programs: new bounds and computation* Daniel Bienstock t Abstract ~Ve describe 
on-going computational experiments with a code for approximately solving "block-angular" linear pro- 
grams. Our algorithm is based on the potential function method, as developed by Plotkin, Shmoys and Tardos 
[8] for producing e-approximate solutions to linear programs of this form. Our implementation integrates 
a new technique for strengthening Lagrangians that produces provably stronger lower bounds, a technique 
for accelerating convergence by "exactly" solving restricted linear programs, and a simple method for 
fine-tuning the potential function. On large problem instances, our implementation yields large speed- 
ups over commercial Linear Programming codes and com-peting codes. Background. We consider linear programs 
of the form z* = rain {cTx : Ax < b, x e P = p1 x p2 x . x pM}, (LP) where each pi C_ R ~ is a polytope. 
For simplicity we will assume that b > 0, c >_ 0 and that LP is feasible. Given e > 0, the method in 
[8] (also see [4], [5], [6]) will produce, under appropriate assump- tions, a vector x E P satisfying 
cTx <_ (1 + e)z*, Ax < (1 + e)b. To produce this result, given B > 0 [8] considers the system of inequalities 
FEAS(B) defined by {cTx<_B, Ax<_b, xeP} and either (a) proves FEAS(B) is infeasible, or (b) produces 
x E P with cTx < (1 + e.)B, Ax < (1 + e)b (an e-feasible solution). Using binary search over B to guess 
z* yields the final algorithm. Consider a system FEAS(B), and denote by aix < bi the i th constraint 
in Ax < b (1 < i < m); and write ao = c T, bo = B. To handle FEAS(B) as desired [8] constructs the (exponential) 
potential function ~I,(x) = ~o~e a(~-l} (where c~ > 0) and iterates through points x l, x2, --. E P. 
At iteration k, the method solves the linear program min {[V'I~(:rk)lTy : y e P} (GRAD), with solution 
yk and sets x TM = /39 k + (1 - ~)x k, where 0 _< ~ _< 1 is chosen so that x TM minimizes potential in 
the segment [x k, yk]. research wins partially funded by NSF awards NCR- 9706029, CDA-9726385 and 29132-5538 
(CORC) t Columbia University. THEOREM 1.1. [8], [7]. Under appropriate assump-tions, the potential reduction 
algorithm finds an e-feasible solution or pTvves that FEAS(B) is infeasible, in a polynomial number of 
iterations. Note that we have not explained how infeasibility is proved. Also note that problem GRAD 
breaks up into independent linear programs over the different pi. In fact each iteration can be restricted 
to a single block pi i.e., in GRAD all the coordinates of y, except those in one block P~, are fixed 
at the values that z k takes. We will refer to such a restricted iteration as a block optimization. If 
the block pi is chosen in round- robin fashion, or randomly, the resulting algorithm is still guranteed 
to l)olynoInially converge. Next we outline the elements in our implementa- tion. See [1] for details. 
 2 Improving lower bounds. Suppose the matrix A in LP has m rows, let Â¢r E R~', and write L(x, Tr) = 
(c t + ~rT A)x --7rTb. Writing L*(Tr) = minzepL(x, Tr), it is known that L*(Tr) < z*. However, it may 
be the case that x* E P with L(x*,rr) = L*(rr) satisfies cTx * > L*0r). This leads to the linear program 
(where B E R): F,~(B) = min {L(x, Tr) : cTx < B, x E P}, a Lagrangian with a "budget" side-constraint. 
We can show the following: LEMMA 2.1. z* >>_ maxBeRmin{B,F,~(B)} = nlax~_> 1 miIlzEp L(.., ) ond this 
quo..,, con be % computed efficiently by solving linear programs over P. We can improve oil the lower 
bounds on z* used in [3]. To show that FEAS(B) is infeasi- ble we have to show that A* > 1, where A* 
= min{A : 3xEPwithctx-AB<0, Ax-Ab<0}. Given 0 < 7r E R m, and 0 < v E R, then mm'~P {("C+~rA)~} A. if 
 [8] ,~* _> ,,~+~ = Thus > 1, FEAS(B) is infeasible. In fact, we will have that AB < z* [8]. This bound 
can be improved somewhat, as follows. We have AB + (A -1)- 7 - "rb = min~ep {(cT+ ~@A)x-~-~-b}. The right-hand 
side minimizes a Lagrangian, and therefore is a lower bound on z*. Since A > 1, the left-hand side is 
strictly larger than AB. Moreover, in 2 our implementation we use the above Lemma to further strengthen 
the lower bound. 3 Accelerating convergence. In many practical applications (multicommodity flows, routing 
problems, set partitioning, etc.) it is the case that the number of variables in LP is much larger than 
the number of constraints, and in the optimal solution only a small fraction of the variables will take 
positive value. Experimentally, it seems that the exponential potential method quickly converges to a 
"good" set of variables (but not necessarily to good values for them). We attempt to exploit these facts 
as follows. At iter- ation k (where k is a multiple of M, the number of blocks in LP), let Z(k) be the 
set of indices j such that ,k ,k xj < 10 -6 ~-'~i xi Then (a) Solve LP~ with the additional constraint 
that xj = 0 for all j E Z(k). Let v k be its optimal solution, and 7r k the optimal dud1 variables. 
(b) Set x TM = /3v k + (1 -[3)x k, where 0.< 3 < 1 is chosen so that x TM has minimum potential. (c) 
Compute max-t>lmin~ep{L(x,~'-~-~)}, a lower bound on z*. Some comments: (1) The linear program in (a) 
is in general much easier than LP because of the much smaller column count, (2) step (b) serves to accelerate 
convergence because v k is "feasible" (according to the  underlying linear program solver) and usually 
has low objective value, and therefore, low potential, (3) step (c) may be seen ms a way of refining 
the dual variables produced by the LP solver. 4 Fine-tuning the potential function. The quantity a used 
in the exponential potential should be of the order of t,~(am) [7] to guarantee polynomial- time convergence. 
But this value is too large for practical purposes. [3] used a heuristic to keep a manageably large; 
also see [2]. In our implementation we use a different heuristic for controlling ~: when the algorithm 
is converging (when the restricted LP discussed above produces a better solution) we decrease c~; when 
the iterates have decreasing infeasibility for Ax <__ b but increasing objective value, we increase a. 
More details will be provided in the full paper.  5 Computational experiments. Our implementation is 
general-purpose; as input it takes a general LP fornmlation where "blocks" have been delimited. All linear 
programs, including the "block" iterations, are solved using a black-box LP solver (in this case Cplex 
6.5) ~s general linear programs, e.g. no combinatorial information is exploited. In the talk Instance 
rmfgenl rmfgen2 multigridl nl 27352 156891 271039 n 116403 670052 818943 nonz 257443 2009281 2414742 
Cplex sec. 187 9940 > 50000 Cplex 1% sec. 133 4868 1200 e-approx see. 115 1292 230 Table 1: sample results 
 we will present results with several classes of linear programs. Briefly, we report here on experiments 
with pure multicommodity flow probleIns using instances discussed in [3]. In [3] the block optimizations 
are solved as (single commodity) network flow problems, and we solve them as LPs, nevertheless our implementation 
is as nmch as a factor of five faster than that in [3], the primary reason being that our iteration count 
is as much as a factor of thirty times smaller. Table 1 shows some experimental results on a 667 MHz 
ev5 Alpha computer, and using CPLEX 6.5 to solve linear programs. References <RefA>[1] <SinRef><author>D. Bienstock</author>, <title>Approximately 
solving large-scale linear programs. I: Strengthening lower bounds and acceler- ating convergence</title>. <tech>CORC 
Report</tech> <date>1999-1</date>. Available for download from <url>http://www.ieor.columbia.edu/'dano</url></SinRef>. [2] <SinRef><author>D. Bienstock</author>, <title>Experiments 
with a network design algorithm using e-approximate linear programs </title>(<date>1996</date>), <note>submitted</note></SinRef>. [3] <SinRef><author>A. Goldberg</author>, 
<author>J. Oldham</author>, <author>S. Plotkin </author>and <author>C. Stein</author>, <title>An Implementation of a Combinatorial Approximation Algorithm for 
Minimum Multicommodity Flow</title>, <journal>IPCO</journal> <date>1998</date></SinRef>. [4] <SinRef><author>M.D. Grigoriadis </author>and <author>L.G. Khachiyan </author>(<date>1991</date>), <title>F~ut approximation 
schemes for convex programs with many blocks and couping constraints</title>, <journal>SIAM Journal on Optimization 4 
</journal>(<date>1994</date>) <pages>86-107</pages>. </SinRef>[5] <SinRef><author>M.D. Grigoriadis </author>and <author>L.G. Khachiyan</author>, <title>An exponential- function reduction method for 
block-angular convex programs</title>, <journal>Networks</journal> <volume>26 </volume>(<date>1995</date>) <pages>59-68</pages>. </SinRef>[6] <SinRef><author>M.D. Grigoriadis </author>and <author>L.G. Khachiyan</author>, <title>Approximate 
minimum-cost nmlticonmmdity flows in ()(e.-'2KNM) time</title>, <journal>Mathematical Programming </journal><volume>75 </volume>(<date>1996</date>), <pages>477-482</pages>. </SinRef>
[7] <SinRef><author>S. Plotkin </author>and <author>D. Karger</author>, <title>Adding multiple cost con- straints to combinatorial optimization problems, 
with applications to multicommodity flows</title>, <booktitle>In Proceedings of the 27th Annual ACM Symposium on Theory 
of Computing</booktitle>, (<date>1995</date>), <pages>18-25</pages></SinRef>. [8] <SinRef><author>S. Plotkin</author>, <author>D.B. Shmoys </author>and <author>E. Tardos</author>, <title>Fast approxi- mation algorithms 
for fractional packing and covering problems</title>, <booktitle>Proc. 32nd Annual IEEE Syrup. On Foun- dations of Computer 
Science</booktitle>, (<date>1991</date>), <pages>495-504</pages></SinRef></RefA>.  
			
