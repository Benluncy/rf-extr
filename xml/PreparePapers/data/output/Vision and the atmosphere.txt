
 International Journal of Computer Vision 48(3), 233 254, 2002 c &#38;#169; 2002 Kluwer Academic Publishers. 
Manufactured in The Netherlands. Vision and the Atmosphere SRINIVASA G. NARASIMHAN AND SHREE K. NAYAR 
Computer Science Department, Columbia University, New York, NY, USA srinivas@cs.columbia.edu nayar @cs.columbia.edu 
Received January 31, 2001; Revised October 16, 2001; Accepted December 4, 2001 Abstract. Current vision 
systems are designed to perform in clear weather. Needless to say, in any outdoor application, there 
is no escape from bad weather. Ultimately, computer vision systems must include mechanisms that enable 
them to function (even if somewhat less reliably) in the presence of haze, fog, rain, hail and snow. 
We begin by studying the visual manifestations of different weather conditions. For this, we draw on 
what is already known about atmospheric optics, and identify effects caused by bad weather that can be 
turned to our advantage. Since the atmosphere modulates the information carried from a scene point to 
the observer, it can be viewed as a mechanism of visual information coding. We exploit two fundamental 
scattering models and develop methods for recovering pertinent scene properties, such as three-dimensional 
structure, from one or two images taken under poor weather conditions. Next, we model the chromatic effects 
of the atmospheric scattering and verify it for fog and haze. Based on this chromatic model we derive 
several geometric constraints on scene color changes caused by varying atmospheric conditions. Finally, 
using these constraints we develop algorithms for computing fog or haze color, depth segmen­tation, extracting 
three-dimensional structure, and recovering clear day scene colors, from two or more images taken under 
different but unknown weather conditions. Keywords: physics based vision, atmosphere, bad weather, fog, 
haze, visibility, scattering, attenuation, airlight, overcast sky, scene structure, defog, dehaze 1. 
Computer Vision and the Weather the most magni.cent visual experiences known to man, including, the colors 
of sunrise and sunset, the blueness Virtually all work in computer vision is based on the of the clear 
sky, and the rainbow (see Minnaert (1954) premise that the observer is immersed in a transparent and 
Henderson (1977)). The literature on this topic has medium (air). It is assumed that light rays re.ected 
by been written over the past two centuries. A summary of scene objects travel to the observer without 
attenuation where the subject as a whole stands would be too am­or alteration. Under this assumption, 
the brightness of bitious a pursuit. Instead, our objective will be to sieve an image point depends solely 
on the brightness of a out of this vast body of work, models of atmospheric single point in the scene. 
Quite simply, existing vision optics that are of direct relevance to computational vi­sensors and algorithms 
have been created only to func-sion. Our most prominent sources of background mate­tion on clear days. 
A dependable vision system how-rial are the works of McCartney (1975) and Middleton ever must reckon 
with the entire spectrum of weather (1952) whose books, though dated, serve as excellent conditions, 
including, haze, fog, rain, hail and snow. reviews of prior work. The study of the interaction of light 
with the atmo-The key characteristics of light, such as its intensity sphere (and hence weather) is widely 
known as atmo-and color, are altered by its interactions with the atmo­spheric optics. Atmospheric optics 
lies at the heart of sphere. These interactions can be broadly classi.ed into three categories, namely, 
scattering, absorption and emission. Of these, scattering due to suspended particles is the most pertinent 
to us. As can be expected, this phenomenon leads to complex visual effects. So, at .rst glance, atmospheric 
scattering may be viewed as no more than a hindrance to an observer. However, it turns out that bad weather 
can be put to good use. The farther light has to travel from its source (say, a surface) to its destination 
(say, a camera), the greater it will be effected by the weather. Hence, bad weather could serve as a 
powerful means for coding and conveying scene structure. This observation lies at the core of our investigation; 
we wish to understand not only what bad weather does to vision but also what it can do for vision. Surprisingly 
little work has been done in computer vision on weather related issues. An exception is the work of Cozman 
and Krotkov (1997) which uses the scattering models in McCartney (1975) to com­pute depth cues. Their 
algorithm assumes that all scene points used for depth estimation have the same inten­sity on a clear 
day. Since scene points can have their own re.ectances and illuminations, this assumption is hard to 
satisfy in practice. Research in image processing has been geared to­wards restoring contrast of images 
degraded by bad weather. Note that bad weather effects depend strongly on the depths of scene points. 
Hence, simple contrast enhancement techniques such as histogram equaliza­tion and contrast stretching 
do not suf.ce here. Oakley and Satherley (1998) use separately measured range data and describe an algorithm 
to restore contrast of at­mospherically degraded images based on the principles of scattering. However, 
they approximate the distribu­tion of radiances in the scene by a single gaussian with known variance. 
Kopeika (1998) and Yitzhaky et al. (1998) restore image contrast using weather predicted atmospheric 
modulation transfer function and an a pri­ori estimate of the distance from which the scene was imaged. 
The goal of our work is to lay the foundation for interpreting scenes from one or more images taken un­der 
bad weather conditions. We discuss various types of weather conditions and their formation processes. 
We summarize two models of atmospheric scattering attenuation and airlight that are most pertinent to 
us. Using these models, we develop algorithms that re­cover complete depth maps of scenes without requiring 
any prior information about the properties of the scene points or atmospheric conditions. Next, we study 
the color effects of atmospheric scat­tering. A new model that describes the appearance of scene colors 
under bad weather is presented and ver­i.ed for fog and haze. Based on this color model, we develop several 
geometric constraints on scene-color changes, caused by varying atmospheric conditions. Using these constraints, 
we present methods to recover structure as well as clear day scene colors from im­ages taken under poor 
weather conditions. All of these methods only require changes in weather conditions and accurate measurement 
of scene irradiance, and not any prior information about the scene points or weather conditions. 2. Bad 
Weather: Particles in Space Weather conditions differ mainly in the types and sizes of the particles 
involved and their concentrations in space. A great deal of effort has gone into measur­ing particle 
sizes and concentrations for a variety of conditions (see Table 1). Given the small size of air molecules, 
relative to the wavelength of visible light, scattering due to air is rather minimal. We will refer to 
the event of pure air scattering as a clear day (or night). Larger particles produce a variety of weather 
conditions which we will brie.y describe below. Haze. Haze is constituted of aerosol which is a dis­persed 
system of small particles suspended in gas. Haze has a diverse set of sources including volcanic ashes, 
foliage exudation, combustion products, and sea salt (see Hidy (1972)). The particles produced by these 
sources respond quickly to changes in relative humidity and act as nuclei (centers) of small water droplets 
when the humidity is high. Haze particles are larger than air molecules but smaller than fog droplets. 
Haze tends to produce a distinctive gray or bluish hue and is certain to effect visibility. Table 1. 
Weather conditions and associated particle types, sizes and concentrations (adapted from McCartney (1975)). 
Condition Particle type Radius (µm) Concentration (cm-3) Air Haze Molecule Aerosol 10-4 10-2 1 1019 103 
10 Fog Cloud Rain Water droplet Water droplet Water drop 1 10 1 10 102 104 100 10 300 10 10-2 10-5 Vision 
and the Atmosphere 235 Fog. Fog evolves when the relative humidity of an air parcel reaches saturation. 
Then, some of the nuclei grow by condensation into water droplets. Hence, fog and certain types of haze 
have similar origins and an increase in humidity is suf.cient to turn haze into fog. This transition 
is quite gradual and an inter­mediate state is referred to as mist. While perceptible haze extends to 
an altitude of several kilometers, fog is typically just a few hundred feet thick. A practical distinction 
between fog and haze lies in the greatly reduced visibility induced by the former. There are many types 
of fog (ex., radiation fog, advection fog, etc.) which differ from each other in their formation processes 
(Myers, 1968). Cloud. A cloud differs from fog only in existing at higher altitudes rather than sitting 
at ground level. While most clouds are made of water droplets like fog, some are composed of long ice 
crystals and ice­coated dust grains. Details on the physics of clouds and precipitation can be found 
in Mason (1975). For now, clouds are of less relevance to us as we restrict ourselves to vision at ground 
level rather than high altitudes. Rain and snow. The process by which cloud droplets turn to rain is 
a complex one (Mason, 1975). When viewed up close, rain causes random spatial and tem­poral variations 
in images and hence must be dealt with differently from the more static weather con­ditions mentioned 
above. Similar arguments apply to snow, where the .akes are rough and have more complex shapes and optical 
properties (Koenderink and Richards, 1992; Ohtake, 1970). Snow too, we will set aside for now. 3. Mechanisms 
of Atmospheric Scattering The manner in which a particle scatters incident light depends on its material 
properties, shape and size. The exact form and intensity of the scattering pattern varies dramatically 
with particle size (Minnaert, 1954). As Figure 1. A particle in the path of an incident light wave abstracts 
and reradiates incident energy. It therefore behaves like a point source of light. The exact scattering 
function is closely related to the ratio of particle size to wavelength of incident light. (Adapted from 
Minnaert (1954)). seen in Fig. 1, a small particle (about 1/10 ., where . is the wavelength of light) 
scatters almost equally in the forward (incidence) and backward directions, a medium size particle (about 
1/4 .) scatters more in the forward direction, and a large particle (larger than .) scatters almost entirely 
in the forward direc­tion. Substantial theory has been developed to derive scattering functions and their 
relations to particle size distributions (Mie, 1908; Hulst, 1957; Chandrasekhar, 1960; Chu and Hogg, 
1968; Rensch and Long, 1970; Nieto-Vesperinas and Dainty, 1990). Figure 1 illustrates scattering by a 
single particle. Clearly, particles are accompanied in close proximity by numerous other particles. However, 
the average sep­aration between atmospheric particles is several times the particle size. Hence, the 
particles can be viewed as independent scatterers whose scattered intensities do not interfere with each 
other. This does not imply that the incident light is scattered only by a single particle. Multiple scatterings 
take place and any given particle is exposed not only to the incident light but also light scattered 
by other particles. A simple analogy is the inter-re.ections between scene points. In effect, multi­ple 
scattering causes the single scattering functions in Fig. 1 to get smoother and less directional. Now, 
consider the simple illumination and detection geometry shown in Fig. 2. A unit volume of scatter­ing 
medium with suspended particles is illuminated with spectral irradiance E(.) per cross section area. 
The radiant intensity I (.,.) of the unit volume in the direction . of the observer is (see McCartney 
(1975)): I (.,.) = ß(.,.)E(.), (1) where, ß(.,.)isthe angular scattering coef.cient. The radiant intensity 
I (.,.)isthe .ux radiated per unit solid angle, per unit volume of the medium. The irradi­ance E(.) is, 
as always, the .ux incident on the volume Figure 2. A unit volume of randomly oriented suspended particles 
illuminated and observed. per unit cross-section area. The total .ux scattered (in all directions) by 
this volume is obtained by integrating over the entire sphere: f(.) =ß(.)E(.), (2) where, ß(.) is the 
total scattering coef.cient. It rep­resents the ability of the volume to scatter .ux of a given wavelength 
in all directions. It is generally as­sumed that the coef.cient ß(.) is constant (homoge­neous medium) 
over horizontal paths. To satisfy this constraint, we will restrict ourselves to the case where the observer 
is at (or close to) ground level and is in­terested not in the sky but other objects on (or close to) 
ground level. Also, we will assume that the atmo­sphere is more or less homogeneous in the scene of interest. 
3.1. Attenuation The .rst mechanism that is relevant to us is the at­tenuation of a beam of light as 
it travels through the atmosphere. This causes the radiance of a scene point to fall as its depth from 
the observer increases. Here, we will summarize the derivation of the atten­uation model given in McCartney(1975). 
Consider a collimated beam of light incident on the atmospheric medium, as shown in Fig. 3. The beam 
is assumed to have unit cross-sectional area. Consider the beam pass­ing through an in.nitesimally small 
sheet (lamina) of thickness dx. The fractional change in irradiance at location x can be written as: 
dE(x,.) =-ß(.) dx. (3) E(x,.) By integrating both sides between the limits x =0 and x =d we get: -ß(.)d 
E(d,.) =Eo(.)e, (4) where, Eo(.) is the irradiance at the source (x =0). This is Bouguer s exponential 
law of attenuation (Bouguer, 1729). At times, attenuation due to scattering is ex­pressed in terms of 
optical thickness, T =ß(.)d. The utility of Bouguer s law is somewhat limited as it as­sumes a collimated 
source of incident energy. This is easily remedied by incorporating the inverse-square law for diverging 
beams from point sources: -ß(.)d Io(.)e E(d,.) = , (5) d2 where, Io(.) is the radiant intensity of the 
point source. This is Allard s law (Allard, 1876). (See Hardy (1967) for an analysis of the applicability 
of the inverse square criterion for sources of various sizes.) In deriving Allard s law, we have assumed 
that all scattered .ux is removed from the incident energy. The fraction of energy that remains is called 
direct transmis­sion and is given by expression (5). We have ignored the .ux scattered in the forward 
direction (towards the ob­server) by each particle. Fortunately, this component is small in vision applications 
since the solid angles sub­tended by the source and the observer with respect to each other are small 
(see Middleton (1949)). In the re­mainder of the paper, we refer to the terms direct trans­mission model 
and attenuation model interchangeably. Finally, in some situations such as heavy fog, the exponential 
law may not hold due to signi.cant mul­tiple scatterings of light by atmospheric particles. We will assume 
here that once light .ux is scattered out of a column of atmosphere (seen by a pixel, say), it does not 
re-enter the same column (or only an insignif­icant amount does). Multiple scattering can also cause 
blurring in the image of a scene. In other words, the .ux scattered out of an atmospheric column (visible 
to a pixel) enters another column (seen by a neighboring pixel). In this work, we do not model the blurring 
effects of multiple scattering. 3.2. Airlight A second mechanism causes the atmosphere to behave like 
a source of light. This phenomenon is called airlight (Koschmieder, 1924) and it is caused by the scatter­ing 
of environmental illumination by particles in the atmosphere. The environmental illumination can have 
several sources, including, direct sunlight, diffuse sky­light and light re.ected by the ground. While 
attenua­tion causes scene radiance to decrease with pathlength, airlight increases with pathlength. It 
therefore causes the apparent brightness of a scene point to increase with depth. We now build upon McCartney 
s (1975) derivation of airlight as a function of pathlength. Consider the illumination and observation 
geometry shown in Fig. 4. The environmental illumination along the observer s line of sight is assumed 
to be constant but unknown in direction, intensity and spectrum. In effect, the cone of solid angle d.subtended 
by a sin­gle receptor at the observer s end, and truncated by a physical object at distance d, can be 
viewed as a source of airlight. The in.nitesimal volume dV at distance x from the observer may be written 
as the product of the cross section area, d.x2, and thickness dx: dV =d.x2 dx. (6) Irrespective of the 
exact type of environmental illumi­nation incident upon dV , its intensity due to scattering in the direction 
of the observer is: dI (x,.) =dV kß(.) =d.x2 dxkß(.), (7) where, ß(.) is the total scattering coef.cient 
and the proportionality constant k accounts for the exact na­ture of the illumination and the form of 
the scattering function. Vision and the Atmosphere 237 If we view element dV as a source with intensity 
dI (x,.), the irradiance it produces at the observer s end, after attenuation due to the medium, is given 
by (5): -ß(.)x dI (x,.)e dE(x,.) = . (8) x2 We can .nd the radiance of dV from its irradiance as: -ß(.)x 
dE(x,.) dI (x,.)e dL(x,.) == . (9) d. d.x2 By substituting (7) we get dL(x,.) =kß(.)e-ß(.)x dx. Now, 
the total radiance of the pathlength d from the observer to the object is found by integrating this expression 
between x =0 and x =d: -ß(.)d L(d,.) =k1 -e. (10) If the object is at an in.nite distance (at the horizon), 
the radiance of airlight is maximum and is found by setting d =8 to get L8(.) =k. Therefore, the radiance 
of airlight for any given pathlength d is: -ß(.)d L(d,.) = L8(.)1 -e. (11) As expected, the radiance 
of airlight for an object right in front of the observer (d = 0) equals zero. Of great signi.cance to 
us is the fact that the above expression no longer includes the unknown factor k. Instead, we have the 
airlight radiance L8(.) at the horizon, which is an observable. 3.3. Overcast Sky Illumination Allard 
s attenuation model in (5) is in terms of the radiant intensity of a point source. This formulation does 
not take into account the sky illumination and its re.ection by scene points. We make two simpli­fying 
assumptions regarding the illumination received by a scene point. Then, we reformulate the attenuation 
model in terms of sky illumination and the BRDF of scene points. Usually, the sky is overcast under foggy 
conditions. So, we use the overcast sky model for environmen­tal illumination (Gordon and Church, 1966; 
Moon and Spencer, 1942). We also assume that the irradiance at each scene point is dominated by the radiance 
of the sky, and that the irradiance due to other scene points is not signi.cant. In Appendix A, we show 
that the attenuated irradiance at the observer is given by, -ß(.)d L8(.).(.)e E(d,.) =g . (12) d2 where 
L8(.) is the horizon radiance. .(.) represents the sky aperture (the cone of sky visible to a scene point), 
and the re.ectance of the scene point in the di­rection of the viewer. g represents the optical settings 
of the camera (aperture, for instance). Note that we refer to (5) as the direct transmission model while 
dealing with images of light sources taken at night. However, while dealing with images of scenes taken 
during day­light, we refer to (12) as the direct transmission model.  4. Depths of Light Sources from 
Attenuation Consider the image of an urban setting taken at night (see Fig. 5). Environmental illumination 
of the scene due to sunlight, skylight and re.ected ground light are minimal and hence airlight can be 
safely ignored. The bright points in the image are mainly sources of light such as street lamps and windows 
of lit rooms. On a clear night, these sources are visible to a distant ob­server in their brightest and 
clearest forms. As haze or fog sets in, the radiant intensities of the sources dimin­ish due to attenuation. 
Our goal here is to recover the relative depths of the sources in the scene from two images taken under 
different (unknown) atmospheric conditions. Since environmental illumination is negligible at night, 
the image irradiance of a light source in the scene can be expressed using the attenuation model (5) 
as: -ß(.)d Io(.)e E(d,.) =g , (13) d2 where, Io(.) is the radiant intensity of the source, d is the 
distance between the source and the camera and the constant gain g accounts for the optical parameters 
(aperture, for instance) of the camera. It is important to note that ß(.) is the total scattering coef.cient 
and not the angular one. We are assuming here that the lines of sight are not too inclined and hence 
all lines of sight pass through the same atmospheric conditions. This removes all dependence on the exact 
form of the scattering function; the attenuation is determined by a single coef.cient ß(.) that is independent 
of viewing direction. If the detector of the camera has spectral response s(.), the .nal image brightness 
recorded is determined as: -ß(.)d Io(.)e EI=s(.)E(d,.) d.=gs(.) d.. d2 (14) For the visible light spectrum, 
the relationship be­tween the scattering coef.cient ß, and the wavelength ., is given by the inverse 
power law (analogous to Rayleigh s law for small air particles): Constant ß(.) = , (15) .. where . .[0,4]. 
Fortunately, for fog and dense haze, . 0 (see Middleton (1952) and McCartney (1975)). In these cases, 
ß does not change appreciably with wavelength. Furthermore, since the spectral bandwidth of the camera 
is rather limited (visible light range for a gray-scale camera, and even narrower spectral bands when 
the camera is color), we will assume the scatter­ing coef.cient ß(.) to be constant over this bandwidth. 
Then, we have: -ßd -ßd ee EI=gs(.)I (.) d.=gI I. (16) d2d2 Now consider two different weather conditions, 
say, mild and dense fog. Or, one of the conditions could be clear with ß =0. In either case we have two 
different attenuation coef.cients, ß1 and ß2. If we take the ratio of the two resulting image brightness 
values, we get: EI 1 -(ß1-ß2)d R ==e. (17) EI 2 Using the natural log, we obtain: RI=ln R =-(ß1 - ß2)d. 
This quantity is independent of the sensor gain and the radiant intensity of the source. In fact, it 
is nothing but the difference in optical thicknesses (DOT) of the source for two weather conditions. 
In the at­mospheric optics literature, the term DOT is used as a quantitative measure of the change in 
weather con­ditions. Now, if we compute the DOTs of two different light sources in the scene (see Fig. 
5) and take their ra­tio, we determine the relative depths of the two source locations: RI di i = (18) 
RI j dj Hence, the relative depths of all sources (with unknown radiant intensities) in the scene can 
be computed from two images taken under unknown but different haze or fog conditions. Since we may not 
entirely trust the DOT computed for any single source, the above calculation may be made more robust 
by using: RI i di = (19) _j=N _j=N RI j=0 jj=0 dj By setting the denominator on the right hand side to 
an arbitrary constant we have computed the depths of all sources in the scene up to a scale factor. Figure 
6 shows experimental results on the recovery of light sources from night images. This experiment and 
all subsequent ones are based on images acquired using a Nikon N90s SLR camera and a Nikon LS-2000 slide 
scanner. All images are linearized using the ra­diometric response curve of the imaging system that is 
computed off-line using a color chart. Figure 6(a) shows a clear day image of a scene with .ve lamps. 
This image is provided only to give the reader an idea of where the lamps are located in the scene. Figures 
6(b) and (c) are clear night and foggy night images of the same scene. The above algorithm for depth 
estima­tion was used to recover the locations of all .ve light sources up to a scale factor. Figure 6(d) 
shows different perspectives of the recovered coordinates of the lamps in three-dimensional space. The 
poles and the ground plane are added only to aid visualization of the results.  5. Structure from Airlight 
Under dense fog and close by objects or mild fog and distant objects, attenuation of object brightness 
is se­vere and airlight is the main cause of image irradiance. Also, in the case of dense haze around 
noon, airlight dominates. In such cases, airlight causes object bright­ness to increase with distance 
from the observer. Here, Vision and the Atmosphere 239 we present a simple method for computing scene 
struc­ture from a single airlight image. A different but re­lated method for computing depth cues was 
proposed by Cozman and Krotkov (1997). Let a scene point at depth d produce airlight radiance L(d,.). 
If our camera has a spectral response s(.), the .nal brightness value recorded for the scene point is: 
EI(d) = gs(.)L(d,.) d., (20) where, g accounts for the constant of proportionality between scene radiance 
and image irradiance (Horn, 1986). Substituting the model for airlight given by (11) we get: -ß(.)d EI(d) 
= gs(.)L8(.)1 -ed. (21) where, L8(.) is again the radiance of airlight at the horizon. As before, we 
will assume that the scatter­ing coef.cient ß(.) is more or less constant over the spectral band of the 
camera. This allows us to write: - EI(d) =E8(1 -eßd ). (22) Let us de.ne: E8-EI(d) S = . (23) E8 By substituting 
(22) in the above expression and taking the natural logarithm, we get: SI=ln S =-ßd. (24) Hence, the 
three-dimensional structure of the scene can be recovered up to a scale factor (the scattering coef.cient 
ß) from a single image. Clearly, at least a small part of the horizon must be visible to obtain E8. If 
so, this part is easily identi.ed as the bright­est region of the image. If there is a strong (direc­tional) 
sunlight component to the illumination, scat­tering would be greater is some directions and airlight 
could be dependent on viewing direction. This problem can be alleviated by using the horizon brightness 
E8 that lies closest to the scene point under consideration. Figure 7 shows the structure of an urban 
setting com­puted from a hazy image taken around noon, and the structure of a mountain range computed 
using a foggy image. Given that some of the objects are miles away  Figure 6. Relative depths from brightness 
decay of point sources at night. (a) A scene with .ve light sources (street lamps). This image is shown 
only to convey the relative locations of the sources to the reader. (b) An image of the scene taken on 
a clear night. (c) An image of the scene taken on a foggy night. The three-dimensional coordinates of 
the .ve sources were computed from images (b) and (c). (d) Rotated graphical illustrations used to demonstrate 
the computed lamp coordinates (small bright spheres). The lamp poles and the ground plane are added only 
to aid visualization. from the camera, such scene structures are hard to com­pute using stereo or structure 
from motion. An interest­ing study of the visibility of distant mountains taking into account earth s 
curvature can be found in Porch (1975).  6. Dichromatic Atmospheric Scattering Thus far, we have not 
taken into account the chro­matic effects of atmospheric scattering. Furthermore, we have described attenuation 
and airlight separately. However, in most situations the effects of both attenua­tion and airlight coexist. 
In the remainder of the paper, we discuss the chromatic effects of atmospheric scat­tering that include 
both attenuation and airlight, and hence develop a general framework for analyzing color images taken 
in bad weather. For this, we .rst present a new model that describes the appearance of scene colors in 
poor visibility conditions. As we know, attenuation causes the radiance of the surface to decay as it 
travels to the observer. In addition, if the particle sizes are comparable to the wavelengths Vision 
and the Atmosphere 241  Figure 7. Structure from one image taken under dense fog/haze. (Left column) 
(a) Image of an urban scene taken under noon haze. (b) Depth map of the scene computed using the image 
in (a). (c) A three-dimensional rendering of the scene. (Right column) (d) Image of a mountain range 
taken under foggy conditions. (e) Depth map computed from the image in (d). (f) A three-dimensional rendering 
of the scene. Some of the objects in these scenes are several kilometers away from the camera. of the 
re.ected light, the spectral composition of the re­.ected light can be expected to vary as it passes 
through the medium. For fog and dense haze, these shifts in the spectral composition are minimal (see 
Middleton (1952) for details), and hence we may assume the hue of direct transmission to be independent 
of the depth of the re.ecting surface. The hue of airlight depends on the particle size distribution 
and tends to be gray or light blue in the case of haze and fog. Therefore, the .nal spectral distribution 
E(d,.) received by the ob­server is a sum of the distributions D(d,.) of directly transmitted light and 
A(d,.) of airlight, which are de­termined by the attenuation model (12) and the airlight model (11) respectively: 
E(d,.) =D(d,.) +A(d,.), -ß(.)d e D(d,.) =gL8(.).(.), (25) d2 -ß(.)d A(d,.) =g 1 -eL8(.). Here, L8(.) 
is the radiance of the horizon (d =8), and g is a constant that accounts for the optical settings of 
the imaging system. .(.) represents the re.ectance properties and sky aperture of the scene point. We 
refer to the above expression as the dichromatic atmospheric scattering model. It is similar in its spirit 
to the dichro­matic re.ectance model (Shafer, 1985) that describes the spectral effects of diffuse and 
specular surface re­.ections. A fundamental difference here is that one of our chromatic components is 
due to surface and vol­ume scattering (transmission of re.ected light) while the other is due to pure 
volume scattering (airlight). If a chromatic .lter with a spectral response f (.) is in­corporated into 
the imaging system, image irradiance is obtained by multiplying (25) by f (.) and integrating over .: 
E( f )(d) = D( f )(d) + A( f )(d). (26) In the case of a color image detector several such .lters (say, 
red, green and blue) with different sensitivities are used to obtain a color measurement vector. The 
dichromatic model can then be written as: E(d) = D(d) + A(d) (27) where, E = [E( f1),E( f2),....E( fn 
)]T . As we men­tioned earlier (see (15)), for fog and haze, the depen­dence of the scattering coef.cient 
ß(.) on the wave­length (within the small bandwidth of the camera) of light tends to be rather small. 
Therefore, except in the case of certain types of metropolitan haze, we may as­sume the scattering coef.cient 
to be constant with re­spect to wavelength (ß(.) = ß). Then, expression (26) may be simpli.ed as: E( 
f )(d) = pI(d)D( f ) + qI(d)A( f ) , (28) where: D( f ) = gf (.)L8(.).(.) d., A( f ) = gf (.)L8(.) d., 
(29) -ßd e pI(d) = , qI(d) = (1 - e-ßd ). d2 Here, D( f ) is the image irradiance due to the scene point 
without atmospheric attenuation and A( f ) is the image irradiance at the horizon in the presence of 
bad weather. We are assuming here that the clear and bad weather have illuminations with similar spectral 
distributions. Hence, the color measurement given by (27) can be rewritten as: E(d) = pI(d)D +qI(d)A. 
Since the inten­sity of illumination (or magnitude of the illumination Figure 8. Dichromatic atmospheric 
scattering model. The color E of a scene point on a foggy or hazy day, is a linear combination of the 
direction D of direct transmission color, and the direction A of airlight color. spectrum) at a scene 
point is expected to vary between clear and bad weather, it is more convenient to write: E(d) = m|L8|pI(d)D 
+ n|L8|qI(d)A (30) where D and A are unit vectors and m and n are scalars. |L8| is the magnitude of the 
illumination spectrum. For convenience, the dichromatic model is re-written as: E = pD + qA , (31) where 
p is the magnitude of direct transmission, and q is the magnitude of airlight (see Fig. 8). From (30) 
we have, E8re-ßd p = , q = E8(1 - e-ßd ). (32) d2 where E8= n|L8|, is termed as the sky intensity and 
r = m/n is a function that depends on the properties of the scene point (re.ectance and sky aperture). 
For our analysis, the exact nature of r is not important; it suf.ces to note that r does not depend on 
the weather condition ß.1 This simpli.ed dichromatic scattering model will prove useful in the coming 
sections when we attempt to recover scene structure and remove weather effects from images. It is easy 
to see that the simpli.ed dichromatic model (31) is linear in color space. In other words, D ,A and E 
lie on the same dichromatic plane in color space. As Vision and the Atmosphere 243  Figure 9. For fog 
and haze, the transmittance (e(-ß(.)d)) does not vary appreciably with wavelength within the visible 
spectrum. The plots were generated using the atmospheric transmission software MODTRAN 4.0, with a .xed 
viewing geometry (distance, d and viewing directions are .xed). stated earlier, we impose the restriction 
that the hue of illumination under various weather conditions remains the same although its intensity 
can vary. It follows that the unit vectors D and A do not change due to dif­ferent atmospheric conditions 
(say, mild fog and dense fog). Therefore, the colors of any scene point, observed under different atmospheric 
conditions, lie on a single dichromatic plane (see Fig. 10(a)). We performed simulations using the atmospheric 
transmission software MODTRAN 4.0 (Acharya et al., 1999) to verify that the scattering coef.cient does 
not vary with wavelength within the visible spectrum (0.4µ 0.7µ). Figure 9 shows plots of transmittance 
(e-ß(.)d ) for a particular viewing geometry in fog and haze respectively. The distance from the observer 
to the scene was .xed at d = 0.2 km and the viewing direc­tion was .xed at 5 degrees off the ground plane. 
The plots show that the variation in ß is very small within the visible spectrum. Experiments with real 
scenes (shown in Fig. 17) were performed to verify this model under three differ­ent fog and haze conditions. 
The sky was overcast in all these conditions. The images used contained around half a million pixels. 
The dichromatic plane for each pixel was computed by .tting a plane to the colors of that pixel, observed 
under the three atmospheric condi­tions. The error of the plane-.t was computed in terms of the angle 
between the observed color vectors and the estimated plane. The average absolute error (in de- Figure 
10. (a) Dichromatic plane geometry and its veri.cation. The observed color vectors Ei of a scene point 
under different (two in this case) foggy or hazy conditions lie on a plane called the dichromatic plane. 
(b) Experimental veri.cation of the dichromatic model with two scenes imaged under three different foggy 
and hazy conditions, respectively. The error was computed as the mean angular deviation (in degrees) 
of the observed scene color vectors from the estimated dichromatic planes, over half a million pixels 
in the images. grees) for all the pixels in each of the two scenes is shown in Fig. 10(b). The small 
error values indicate that the dichromatic model indeed works well for fog and haze.  7. Weather Removal 
and Structure using Chromatic Decomposition Consider color images of a scene taken under clear weather 
and foggy or hazy weather. Assume that the clear day image is taken under environmental illumi­nation 
with similar spectral characteristics as the bad weather image. If not, a white patch in the scene may 
be used to apply the needed color corrections. The sky in the bad weather image reveals the direction 
of the airlight color A . The direction of the color D of each scene point is revealed by the clear weather 
image. Therefore, Eq. (31) can be used to decompose the bad weather color E at each pixel into its two 
components and determine the airlight magnitude q(d). The result­ing airlight image is then used to compute 
a depth map as described in Section 5. Figure 11 shows ex­perimental results obtained using the above 
decompo­sition method. Figure 12 demonstrates a simple form of weather removal by defogging windows of 
buildings. In computing depth from the airlight component, we have assumed that the atmosphere itself 
is uniformly illuminated. Consider a pathlength that extends from a point on a building to an observer. 
Clearly, atmospheric points closer to the building see less of the sky due to occlusion by the building. 
This effect increases towards the foot of the building. Some of the errors in the com­puted structure 
can be attributed to this illumination Figure 11. Structure from chromatic decomposition. (a) Clear 
day image of buildings. (b) Foggy day image of the same scene. (c) The direct transmission component 
(brightened) estimated by the chro­matic decomposition algorithm. Black and gray points (windows) are 
discarded due to lack of color. (d) Depth map of the scene com­puted from the airlight component (depths 
of window areas are inter­polated). (e) A three-dimensional rendering of the computed depth map. Figure 
12. Demonstration of fog removal. (a) A clear day image of a building taken under an overcast sky. The 
color directions (and not magnitudes) of scene points (non-window regions) are recorded as true colors 
or clear day colors. (b) A foggy day image of the same scene again captured under an overcast sky. Note: 
Even though both images in (a) and (b) were taken on overcast days (ie., spectral composition of the 
daylight on both days are more or less identical), the horizon brightnesses (and/or camera exposure parameters) 
can vary. (c) The true colors recorded were used to decompose the foggy image into direct transmission 
and airlight components. The airlight component was subtracted from the window regions to demonstrate 
a simple form of weather removal. occlusion effect (see Appendix B for a more detailed treatment). Finally, 
there are certain limitations to this type of decomposition. First, we cannot decompose (31) if both 
the airlight and scene points have the same color. Also, this algorithm for chromatic decomposition is 
re­strictive since it requires a clear day image of the scene. In the remainder of the paper, we develop 
more general constraints and algorithms to compute structure as well as recover clear day colors, without 
requiring a clear day image of the scene.  8. Computing the Direction of Airlight Color The direction 
of airlight (fog or haze) color can be simply computed by averaging a patch of the sky on a foggy or 
hazy day (as was done in Section 7), or from scene points whose direct transmission color is black.2 
However, these methods necessitate either (a) the inclusion of a part of the sky (which is more prone 
to color saturation or clipping) in the image or (b) a clear day image of the scene with suf.cient black 
points to yield a robust estimate of the direction of airlight color. Here, we present a method that 
does not require either the sky or a clear day image, to compute the direction of airlight color. Figure 
13 illustrates the dichromatic planes for two scene points Pi and Pj , with different direct transmis­sion 
colors D (i) and D ( j). The dichromatic planes Qi Vision and the Atmosphere 245 and Qj are given by 
their normals, = E(i) × E(i) Ni 12 , (33) = E( j) × E( j) Nj . 12 Since the direction A of the airlight 
color is the same for the entire scene, it must lie on the dichromatic planes of all scene points. Hence, 
A is given by the intersection of the two planes Qi and Qj , Ni × Nj A = . (34) lNi × Nj l In practice, 
scenes have several points with different colors. Therefore, we can compute a robust intersec­tion of 
several dichromatic planes by minimizing the objective function E =(Ni · A )2 . (35) i Thus, we are 
able to compute the color of fog or haze using only the observed colors of the scene points under two 
atmospheric conditions, and not relying on a patch of the sky being visible in the image. We veri.ed 
the above method for the two scenes shown in Fig. 17. First, the direction of airlight color was computed 
using (35). Then, we compared it with the direction of the airlight color obtained by averag­ing an unsaturated 
patch of the sky. For the two scenes, the angular deviations were found to be 1.2. and 1.6. respectively. 
These small errors in the computed di­rections of airlight color indicate the robustness of the method. 
9. Dichromatic Constraints for Iso-depth Scene Points In this section, we derive a simple constraint 
for scene points that are at the same depth from the observer. This constraint can then be used to segment 
the scene based on depth, without knowing the actual re.ectances of the scene points and their sky apertures. 
For this, we .rst prove the following lemma. Lemma. Ratios of the direct transmission magni­tudes for 
points under two different weather conditions are equal, if and only if the scene points are at equal 
depths from the observer. Proof: Let ß1 and ß2 be two unknown weather con­ditions with horizon brightness 
values E81 and E82. Let Pi and Pj be two scene points at depths di and dj , from the observer. Also, 
let r(i) and r( j) represent sky apertures and re.ectances of these points. From (32), the direct transmission 
magnitudes of Pi under ß1 and ß2, can be written as (i)-ß1di (i)-ß2di rere (i) E81(i) E82 p= , p= . 1 
d22 d2 ii Similarly, the direct transmission magnitudes of Pj under ß1 and ß2, are ( j)-ß1dj ( j)-ß2dj 
( j) E81 re( j) E82 re p= , p= . 1 d22 d2 jj Then, we immediately see that the relation: (i) 2( j) E82 
pp 2 -(ß2-ß1)d ==e, (36) (i)( j) ppE81 11 holds if and only if di =dj =d. So, if we have the ratio of 
direct transmissions for each pixel in the image, we can group the scene points according to their depths 
from the observer. But how do we compute this ratio for any scene point without knowing the actual direct 
transmission magnitudes? Consider the dichromatic plane geometry for a scene point P, as shown in Fig. 
14. Here, we denote a vector by the line segment between its end points. Let p1 and p2 be the unknown 
direct transmission magnitudes of P under ß1 and ß2, respectively. Similarly let q1 and q2 be the unknown 
airlight magnitudes for P under ß1 and ß2. We de.ne a magnitude |OAt |on the airlight vector such that 
E2 At lE1 O. Also, since the direction of di­rect transmission color for a scene point does not vary 
due to different atmospheric conditions, E1 A1 lE2 A2. Here A1 and A2 correspond to the end points of 
the airlight magnitudes of P under ß1 and ß2, as shown in Fig. 14. Thus, 1E1 OA1 ~1E2 At A2. This implies, 
p2 q2 -|OAt ||E2 At | == . (37) p1 q1 |E1 O| Since the right hand side of (37) can be computed using 
the observed color vectors of the scene point P,we can compute the ratio ( p2/p1) of direct transmission 
magnitudes for P under two atmospheric conditions. Therefore, from (36), we have a simple method to .nd 
points at the same depth, without having to know their re.ectances and sky apertures. Let us now consider 
the numerical stability of the direct transmission ratio (37). Under heavy fog/haze (or when the dynamic 
range of the sensor is low), the direct transmission magnitudes are low and their ratio could be unstable. 
In such cases, the ratio constraint can be supported by another constraint for depth segmentation we 
describe brie.y. Consider the dichromatic planes of two different scene points as illustrated in Fig. 
15. It can be shown (using the geometric analysis in Fig. 14) that the shaded triangles are similar if 
and only if the two scene points are at equal depths from the observer. Therefore, the constraint for 
two scene points to be iso-depth is given in terms of observables, E(1) E(2) E(1) E(2) l. (38) 11 22 
Using the constraints in (36) and (38) a sequential la­beling like algorithm can be used to ef.ciently 
segment scenes into regions of equal depth. 10. Scene Structure from Two Bad Weather Images We extend 
the direct transmission ratio constraint given in (36) one step further and present a method to con­struct 
the complete structure of an arbitrary scene, from two images taken under poor weather conditions. From 
(36), the ratio of direct transmissions of a scene point P under two atmospheric conditions, is given 
by p2 E82 -(ß2-ß1)d = e. (39) p1 E8 1 Note that we have already computed the left hand side of the above 
equation using (37). Taking natural loga­rithms on both sides, we get E8p2 (ß2 -ß1)d =ln 2 -ln . (40)E8p1 
1 So, if we know the horizon brightness values, E81 and E82, then we can compute the scaled depth (ß2 
-ß1)d at P. As before, (ß2 -ß1)d is just the difference in optical thicknesses (DOT) for the pathlength 
d, under the two weather conditions. 10.1. Estimation of E81 and E82 The expression for scaled depth 
give in (40), includes the horizon brightness values, E81 and E82. These two terms are observables only 
if some part of the sky is visible in the image. However, the brightness values within the region of 
the image corresponding to the sky, cannot be trusted since they are prone to intensity saturation and 
color clipping. Here, we estimate E81 Vision and the Atmosphere 247 and E82 using only points in the 
non-sky region of the scene. Let q1 and q2 denote the magnitudes of airlight for a scene point P under 
atmospheric conditions ß1 and ß2. Using (32), we have - - q1 =E81(1 -eß1d ), q2 =E82(1 -eß2d ). (41) 
Therefore, E8-q2 E8 22 -(ß2-ß1)d = e. (42) E81 -q1 E81 Substituting (39), we can rewrite the previous 
equation as p2 p1 = q2 -c q1 , where, c =E82 - p2 p1 E81 . (43) Comparing (43) and (37), we get c =|OAt 
|(see Fig. 14). hence, the expression for c in (43) repre­sents a straight line equation in the unknown 
param­eters, E81 and E82. Now consider several pairs of (i) (i)(i) {c, (p/p)}corresponding to scene points 
Pi ,at 21 different depths. Then, the estimation of E81 and E82 is reduced to a line .tting problem. 
Quite simply, we have shown that the horizon brightnesses under dif­ferent weather conditions can be 
computed using only non-sky scene points. Since both the terms on the right hand side of (40) can be 
computed for every scene point, we have a simple algorithm for computing the scaled depth at each scene 
point, and hence the complete scene structure, from two images taken under different atmospheric conditions. 
 10.2. Experimental Results We now present results showing scene structure recov­ered from both synthetic 
and real images. The synthetic scene we used is shown on the left side of Fig. 16(a) as a 200 ×200 pixel 
image with 16 color patches. The colors in this image represent the direct transmission or clear day 
colors of the scene. We assigned a random depth value to each color patch. The rotated 3D struc­ture 
of the scene is shown on the right side of Fig. 16(a). Then, two different levels of fog (ß1/ß2 =0.67) 
were added to the synthetic scene according to the dichro­matic model. To test robustness, we added noise 
to the foggy images. The noise was randomly selected from a uniformly distributed color cube of dimension 
10. The Figure 16. Experiments with a synthetic scene. (a) On the left, a 200 ×200 pixel image representing 
a synthetic scene with 16 color patches, and on the right, its rotated 3D structure. (b) Two levels of 
fog (ß1/ß2 =0.67) are added to the synthetic image according to the dichromatic model. To test robustness, 
noise is added by random selection from a uniformly distributed color cube of dimension 10. (c) The recovered 
structure (3 ×3 median .ltered). resulting two foggy (and noisy) images are shown in Fig. 16(b). The 
structure shown in 16(c) is recovered from the two foggy images using the technique we de­scribed above. 
Simulations were repeated for the scene in Fig. 16(a) for two relative scattering coef.cient values (ß1/ß2), 
and three different noise levels. Once again, the noise was randomly selected from a uniformly distributed 
color cube of dimension .. Table 2 shows results of sim­ulations for two parameter sets {ß1/ß2, E81 , 
E8}= 2 {0.5, 100, 255}and {0.67, 200, 400}. The computed values for E81 , E82, and the percentage RMS 
error in the recovered scaled depths, computed over all 200 ×200 pixels are given. These results show 
that our method for recovering structure is robust for rea­sonable amounts of noise. Experiments with 
two real scenes under foggy and hazy conditions are shown in Fig. 17. The .rst of the two scenes was 
imaged under two foggy conditions, and is shown in 17(a). The second scene was imaged Table 2. Simulations 
were repeated for the scene in Fig. 16(a), for two sets of parameter values, and three different noise 
levels. Noise was randomly selected from a uniformly distributed color cube of dimension .. Noise (.) 
0 5 10 15 Actual values {ß1/ß2, E81 , E82 }={0.5, 100, 255} Estimated E81 100 108.7 109.2 119.0 Estimated 
E82 255 262.7 263.6 274.0 Depth error (%) 0.0 7.14 11.7 15.3 Actual values {ß1/ß2, E81 , E82 }={0.67, 
200, 400} Estimated E81 200 204.3 223.7 249.5 Estimated E82 400 403.8 417.5 444.2 Depth error (%) 0.0 
12.3 15.3 17.8 under two hazy conditions as shown in 17(c). Figure 17(b) and (d) shows the corresponding 
re­covered depth maps.  11. Clear Day Scene Colors As we stated in the beginning of the paper, most 
out­door vision applications perform well only under clear weather. Any discernible amount of scattering 
due to fog or haze in the atmosphere, hinders a clear view of the scene. Earlier we presented a simple 
form of weather removal that requires a clear day image of the scene (see Figs. 11 and 12 in Section 
7). In this section, we compute the scene colors as they would appear on a clear but overcast day from 
two bad weather images. More precisely, we compute the direct transmission colors of the entire scene 
using minimal a priori scene information. For this, we .rst show that, given addi­tional scene information 
(airlight or direct transmission vector) at a single point in the scene, we can compute the clear day 
colors of the entire scene from two bad weather images. Consider the dichromatic model given in (31). 
The color of a scene point Pi under weather condition ß is, E(i)(i)D (i)(i) =p+qA, (44) where p(i) is 
the direct transmission magnitude, and q(i) is the airlight magnitude of Pi . Suppose that the direction 
D (i) of direct transmission color for a single point Pi is given. Besides, the direction A of airlight 
color for the entire scene can be estimated using (35). Therefore, the coef.cients p(i) and q(i) can 
be computed using (44). Furthermore, the optical thickness ßdi of Pi can be computed from (32). Vision 
and the Atmosphere 249 Since we have already shown how to compute the scaled depth of every scene point 
(see (40)), the relative depth dj /di of any other scene point Pj with respect to Pi can be computed 
using the ratio of scaled depths. Hence, the optical thickness and airlight for the scene point Pj , 
under the same atmospheric condition are given by ßdj =ßdi (dj /di ), (45) ( j) - q=E8(1 -eßdj ). Finally, 
the direct transmission color vector of Pj can be computed as ( j)D ( j)( j) p=E( j) -qA. (46) Thus, 
given a single measurement (in this case, the direction of direct transmission color of a single scene 
point), we have shown that the direct transmission and airlight color vectors of any other point, and 
hence the entire scene can be computed. But how do we specify the clear day color of any scene point 
without actually capturing the clear day image? For this, we assume that there exists at least one scene 
point whose direct transmission color D lies on  Figure 19. [(a) and (c)] Clear day scene colors recovered 
from the two foggy and hazy images shown in Fig. 17(a) and (c) respectively. The colors in some of the 
dark window interiors are dominated by airlight and thus their clear day colors are computed to be black. 
The images are median .ltered to reduce noise and brightened for display purposes. [(b) and (d)] Actual 
clear day images of the scenes are shown for qualitative comparison. Note: The clear day images on the 
right and the bad weather images (Fig. 17) were captured on different days. Some differences between 
actual and recovered clear day colors are due to the different spectral distributions of illumination 
in the scene, during image acquisition. the surface of the color cube (including origin or black) and 
we wish to identify such point(s) in the scene auto­matically. Consider the R-G-B color cube in Fig. 
18. If the clear day color of a scene point lies on the surface Vision and the Atmosphere 251  of the 
color cube, then the computed q is equal to the airlight magnitude q of that point. However, if it lies 
within the color cube, then clearly q > q. For each point Pi , we compute q (i) and optical thickness 
f ß1di . Note that fmay or may not be the correct optical ß1di thickness. We normalize the optical thicknesses 
of the scene points by their scaled depths (DOTs) to get f ß1di a i = . (47) (ß2 - ß1)di For scene points 
that do not lie on the color cube sur­face, a i is greater than what it should be. Since we have assumed 
that there exists at least one scene point whose clear day color is on the surface of the cube, it must 
be the point that has the minimum a i . So, q (i) of that point is its true airlight. Hence, from (45), 
the airlights and direct transmission colors of the entire scene can be computed without using a clear 
day image. For robust­ness, we use k least a i Is. We call this the Color Cube Boundary Algorithm. Figure 
19 illustrates experiments with real scenes. Usually in urban scenes, window interiors have very little 
color of their own. Their intensities are solely due to airlight and not due to direct transmission. 
In other words, their direct transmission color is black (the origin of the color cube). We detected 
such points in the scene using the above technique and recovered the clear day colors of foggy and hazy 
scenes. A second result is shown in Figs. 20 and 21.  12. Summary Research in atmospheric optics has 
been around for over two centuries. The physical processes that govern the effects of atmospheric scattering 
on scene appear­ance are well established. This article is just an initial attempt at understanding and 
exploiting the manifesta­tions of weather in order to interpret, recover and ren­der scenes under various 
atmospheric conditions. We summarized existing models in atmospheric optics and proposed new ones, keeping 
in mind the constraints faced by most vision applications. We presented sev­eral simple algorithms for 
recovering scene structure from one or two bad weather images and demonstrated that bad weather can be 
put to good use. Using scene structure, algorithms to remove weather effects were developed. We intend 
to use these results as building blocks for developing more advanced weather-tolerant vision techniques. 
Potential applications of this work are in outdoor surveillance, navigation, underwater ex­plorations 
and image based rendering.  Appendix A: Direct Transmission Under Overcast Skies We present an analysis 
of the effect of sky illumina­tion and its re.ection by a scene point, on the direct transmission from 
the scene point. For this, we make two simplifying assumptions on the illumination re­ceived by scene 
points. Usually, the sky is overcast un­der foggy conditions. So we use the overcast sky model (Gordon 
and Church, 1966; IRIA, 1978) for environ­mental illumination. We also assume that the irradiance of 
each scene point is dominated by the radiance of the sky, and that the irradiance due to other scene 
points is not signi.cant. See Langer and Zucker s work (1994) for a related analysis. Consider the illumination 
geometry shown in Fig. 22. Let P be a point on a surface and n be its normal. We de.ne the sky aperture 
. of point P, as the cone of sky visible from P. Consider an in­.nitesimal patch of the sky, of size 
d. in polar angle and df in azimuth as shown in Fig. 22. Let this patch subtend a solid angle d. at P. 
For overcast skies, Moon (Moon and Spencer, 1942) and Gordon (Gordon and Church, 1966) have shown that 
the radiance of the in.nitesimal cone d., in the direction (.,f)is given by L(.,f) = L8(.)(1 + 2 cos 
.)d., where d. = sin .d.df. Hence, the irradiance at P due to the entire aperture .,isgivenby E(.) = 
L8(.)(1 + 2 cos .) cos . sin . d. df, (48) where cos . accounts for foreshortening (Horn, 1986). If R 
is the BRDF of P, then the radiance from P toward the observer can be written as Lo(.) = L8(.) f (.)R(.,f,.) 
d. df, (49) where f (.) = (1 + 2 cos .) cos . sin .. Let s be the projection of a unit patch around P, 
on a plane per­pendicular to the viewing direction. Then, the radiant intensity of P is given by Io(.) 
= sLo(.). Since L8(.) is a constant with respect to . and f, we can factor it out of the integral and 
write concisely as Io(.) = L8(.).(.), (50) where .(.) = s f (.)R(.,f,.) d. df. (51) The term .(.) represents 
the sky aperture and the re­.ectance in the direction of the viewer. Substituting for Io(.) in the direct 
transmission model in (5), we obtain -ß(.)d L8(.).(.)e E(d,.) = g , (52) d2 where g represents the optical 
setting of the camera (exposure, for instance). We have thus formulated the direct transmission model 
in terms of overcast sky il­lumination and the re.ectance of the scene points. Appendix B: Illumination 
Occlusion Problem In deriving the expression for the radiance due to airlight in Section 3.2, we assumed 
that the atmosphere is illuminated uniformly regardless of the type of illu­mination. This is not always 
true since not all points in the atmosphere see the same solid angle of the sky. In fact, the scene itself 
occludes part of the sky hemisphere visible to a point in the atmosphere. For explanation purposes, consider 
a scene with a single building. The solid angle subtended at any point in the atmosphere by the sky is 
called its sky aperture. As seen in Fig. 23, this solid angle decreases as the distance increases from 
the observer for any given pathlength. Similarly, the solid angle is smaller for points near the bottom 
of the building. We now present a simpli.ed analysis of this effect. We assume that the atmosphere is 
illuminated mainly by overcast skylight (ground light is ignored here). Then, the irradiance received 
by any point in the atmosphere is given by (see Eq. (48)), = E(hemisphere) - E(occluded) E , f. E(occluded) 
= L8(1 + 2 cos .) -f 0 × cos . sin .d.df, (53) p p/2 E(hemisphere) = L8(1 + 2 cos .) -p 0 × cos . sin 
.d.df, where E(hemisphere) is the irradiance the point would receive from the entire sky hemisphere (as 
if there were no occlusions). Eoccluded is the irradiance the point would have received from the occluded 
part. . and f denote the polar and azimuth of the occluded region. The above equation simpli.es to 7p- 
7fcos2 .(3 + 4 cos .) E = L8 . (54) 3 To correct for the radiance of airlight in Section 3.2, we multiply 
by the fraction of irradiance received by each point and rewrite the airlight radiance (10) of a pathlength 
d as L(d,.) d fcos2 .(3 + 4 cos .) -ß(.)d = k 1 - e- k p × ß(.)e-ß(.)x dx. (55) 0 Note here that both 
. and fdepend on the depth from the observer x (see Fig. 23). In other words, the integral in the previous 
equation depends on the exact extent of occlusion by the scene. In our experiments, we have as­sumed 
uniform illumination of the atmosphere and thus some of the errors in the depth maps can be attributed 
to this effect. Vision and the Atmosphere 253 Acknowledgments This work was supported in parts by a 
DARPA/ONR HumanID Contract (N00014-00-1-0916), an NSF Award (IIS-99-87979), and a DARPA/ONR MURI Grant 
(N00014-95-1-0601). The authors thank Jan Koenderink of Utrecht University for pointers to early work 
on atmospheric optics. The authors also thank Yoav Schechner for the discussions on this topic that helped 
improve the paper. Some of the results pre­sented in this paper have appeared in the proceedings of the 
IEEE International Conference on Computer Vision 1999 (Nayar and Narasimhan, 1999), IEEE Conference on 
Computer Vision and Pattern Recog­nition 2000 (Narasimhan and Nayar, 2000) and SPIE Conference on Human 
Vision and Electronic Imag­ing 2001 (Narasimhan and Nayar, 2001). Notes 1. We do not handle situations 
where wet materials may appear darker than dry materials. 2. Sky and black points take on the color 
of airlight on a bad weather day.  References Acharya, P.K., Berk, A., Anderson, G.P., Larsen, N.F., 
Tsay, S.C., and Stamnes, K.H. 1999. Modtran4: Multiple scattering and BRDF upgrades to modtran. In SPIE 
Proc. Optical Spectroscopic Tech­niques and Instrumentation for Atmospheric and Space Research III, p. 
3756. Allard, E. 1876. Memoire sur l intensite et la portee des phares. Dunod: Paris. Bouguer, P. 1729. 
Traite d optique sur la gradation de la lumiere. Chandrasekhar, S. 1960. Radiative Transfer. Dover Publications: 
New York. Chu, T.S. and Hogg, D.C. 1968. Effects of precipitation on propa­gation at 0.63, 3.5 and 10.6 
microns. The Bell System Technical Journal. Cozman, F. and Krotkov, E. 1997. Depth from scattering. In 
Pro­ceedings of the 1997 Conference on Computer Vision and Pattern Recognition, vol. 31, pp. 801 806. 
Gordon, J. and Church, P. 1966. Overcast sky luminances and di­rectional luminous re.ectances of objects 
and backgrounds under overcast skies. Applied Optics, 5:919. Hardy, A.C. 1967. How large is a point source? 
Journal of Optical Society of America, 57(1). Henderson, S.T. 1977. Daylight and its Spectrum. Wiley: 
New York. Hidy, G.M. 1972. Aerosols and Atmospheric Chemistry. Academic Press: New York. Horn, B.K.P. 
1986. Robot Vision. The MIT Press: Cambridge, MA. IRIA. 1978. The Infrared Handbook. Infrared Information 
and Anal­ ysis Center, Environmental Research Institute of Michigan. Koenderink, J.J. and Richards, W.A. 
1992. Why is snow so bright? Journal of Optical Society of America, 9(5):643 648. Kopeika, N.S. 1998. 
A System Engineering Approach to Imaging. SPIE Press. Koschmieder, H. 1924. Theorie der horizontalen 
sichtweite. Beitr. Phys. Freien Atm., 12:33 53, 171 181. Langer, M.S. and Zucker, S.W. 1994. Shape from 
shading on a cloudy day. JOSA-A, 11(2):467 478. Mason, B.J. 1975. Clouds, Rain, and Rainmaking. Cambridge 
University Press: Cambridge. McCartney, E.J. 1975. Optics of the Atmosphere: Scattering by Molecules 
and Particles. John Wiley and Sons: New York. Middleton, W.E.K. 1949. The effect of the angular aperture 
of a telephotometer on the telephotometry of collimated and non-collimated beams. Journal of Optical 
Society of America, 39:576 581. Middleton, W.E.K. 1952. Vision Through the Atmosphere. University of 
Toronto Press. Mie, G. 1908. A contribution to the optics of turbid media, espe­cially colloidal metallic 
suspensions. Ann. of Physics, 25(4):377 445. Minnaert, M. 1954. The Nature of Light and Color in the 
Open Air. Dover: New York. Moon, P. and Spencer, D.E. 1942. Illumination from a non-uniform sky. Illum 
Eng., 37:707 726. Myers, J.N. 1968. Fog. Scienti.c American, pp. 75 82. Narasimhan, S.G. and Nayar, S.K. 
2000. Chromatic framework for vision in bad weather. In Proceedings of the IEEE Conference on Computer 
Vision and Pattern Recognition. Narasimhan, S.G. and Nayar, S.K. 2001. Vision and the weather. In Proceedings 
of SPIE Conference on Human Vision and Electronic Imaging VI, p. 4299. Nayar, S.K. and Narasimhan, S.G. 
1999. Vision in bad weather. In Proceedings of the 7th International Conference on Computer Vision. Nieto-Vesperinas, 
M. and Dainty, J.C. 1990. Scattering in Volumes and Surfaces. North-Holland: New York. Oakley, J.P. and 
Satherley, B.L. 1998. Improving image quality in poor visibility conditions using a physical model for 
degradation. IEEE Trans. on Image Processing,7. Ohtake, T. 1970. Factors affecting the size distribution 
of rain­drops and snow.akes. Journal of Atmospheric Science, 27:804 813. Porch, W.M. 1975. Visibility 
of distant mountains as a measure of background aerosol pollution. Applied Optics, 14. Rensch, D.B. and 
Long, R.K. 1970. Comparative studies of extinction and backscattering by aerosols, fog, and rain at 10.6 
and 0.63 microns. Applied Optics, 9(7). Shafer, S. 1985. Using color to separate re.ection components. 
Color Research and Applications, pp. 210 218. Van De Hulst. 1957. Light Scattering by Small Particles. 
John Wiley and Sons: New York. Yitzhaky, Y., Dror, I., and Kopeika, N.S. 1998. Restoration of altmospherically 
blurred images according to weather-predicted atmospheric modulation transfer function. Optical Engineering, 
36.  
			