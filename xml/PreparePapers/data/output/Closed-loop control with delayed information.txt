
 Closed-Loop Control with Delayed Information Eitan Altman and Philippe Nain INRIA 2004, Route des Lucioles 
06565 Valbonne Cedex, France altman@sophia.inria.fr nain@kophia.inria.fr Abstract The theory of Markov 
Control old type, where the threshold is seen to depend on the Model with Perfect State Information (MCM-PSI) 
re-last action. quires that the current state of the system is known to the decision maker at decision 
instants. Otherwise, one speaks of Markov Control Model with Imperfect 1 Introduction State Information 
(MCM-ISI). In this article, we introd­uce a new class of MCM-ISI, where the information The theory of 
Markov Control Model with Perfect State on the state of the system is delayed. Such an informa- Information 
(MCM-PSI) requires that the current state tion structure is encountered, for instance, in high-speed 
of the system is known to the decision maker at deci­ data networks. sion instants (Bertsekas [I], Ross 
[8], Schal [9]). While this assumption may be realistic or may provide good In the first part of this 
article, we show that by en­approximations in some cases, it is no longer true, for larging the state 
space so as to include the last known instance, in modern communication networks. The rea­ state as well 
as all the decisions made during the travel son is that the propagation delay in such networks is no 
time of the information, we may reduce a MCM-IISI to longer negligible with respect to the packet transmis­ 
a MCM-PSI. In the second part of this paper, this re­sion time. As a result, the state of the system 
may have sult is applied to a flow control problem. Considered is changed considerably by the time it 
is known by the de­ a discrete time queueing model with Bernoulli arrivals cision maker. This, in turn, 
may yield serious problems and geometric services, where the intensity of the ar­when closed-loop controls 
are used. rival stream is controlled. At the beginning of slot f + 1, t=o,l,2,. . ., the decision maker 
has to select the prob-Control problems with imperfect state information ability of having one arrival 
in the current time slot from have already received much attention in the literature. the set {pl, p2}, 
O < pz < pI < 1, only on the basis of When the control is centralized the standard assump­the queue-length 
and action histories in [0, t]. The aim tion in this setting is that the decision maker instead is to 
optimize a discounted throughput/delay criterion. of having perfect knowledge of the state has only ac-We 
show that there exists an optimal policy of a thresh­ cess to an observation of this state (see e.g., 
Bertsekas [1], Hernafidez-Lerma [3]). When the control is decen­ *The work of this author was supported 
by the France-Israel tralized (i.e., there are several decision makers), the so- Scientific Cooperation 
in Computer Science and Engineering un­der grant 3321190. called Delayed Sharing of State Information 
(DSSI) pat­tern provides another instance of imperfect state infor- Permission to copy without fee all 
or part of this material is granted provided that the copies are not mada or distributed for mation structure 
(Grizzle and Marcus [2], Hsu and Mar­direct commercial advantage, the ACM copyright notice end the cus 
[4], Schoute [10]). In that case, each decision maker title of the publication and its date appear, and 
notice is given informs the others about his observation with a delay. that copying is by permission 
of the Association for Computing Mechinery. To copy otherwise, or to republish, requiree a fee However, 
in each case the imperfect state information and/or specific permission. (or partially observable) problem 
is seen to reduce to 1992 ACM SIG METRICS &#38; PERFORMANCE 92-6 /92/R.l., lJSA 01992 ACM 0-89791-508-919210005101 
93....$1.50 193, Performance Evaluation Review, Vol. 20, No. 1, June 1992 a perfect state information 
(or completely observable) problem by enlarging the state space. The first objective of this work is 
to introduce a new type of imperfect information structure, the so-called N-Step Delayed State Information 
(N-SDSI) structure. We assume a discrete-time MCM with a single decision maker. The N-SDSI pattern assumes 
that the state of the system at time t c IN := {O, 1,2,. . .} is not known by the decision maker until 
time t+ N, where N >1 is an arbitrary integer. However, we assume that all the past actions are known 
with no delay. Therefore, and as opposed to the DSSI pattern briefly described above, the N-SDSI pattern 
is such that the decision maker has no information whatsoever on the current state of the system. Alike 
the previous studies on imperfect state in­formation, we show that this MCM-ISI can be converted into 
a MCM-PSI by enlarging the state space. However, because of the simple structure of the N-SDSI pattern 
the new state of the system remains easy to handle. The second objective is to solve a flow control prob­lem 
with a 1-SDSI pattern (see Stidham [11] for a sur­vey on flow control models). We consider a discrete 
time queueing model with Bernoulli arrivals and geo­metric services, where the intenstiy of the arrival 
stream is controlled. At the beginning of slot t + 1, t c IN, the decision maker has to select the probability 
of hav­ing one arrival in the current time slot from the set {Pi, PZ}, O S P2 < PI < 1, only on the 
basis of the queue-length and action histories in [0, t].We show that there exists a policy of a threshold 
type that optimizes a discounted throughput/delay criterion over an infi­nite horizon. More precisely, 
we show that there exists a mapping 1 : {pl, p2} ~ ~U {+cm} such that the prob­ability pl (resp. p2) 
is chosen at time t + 1 if z < l(a) (resp. z > l(a)), where x is the queue-length at time t and a is 
the action chosen at time t,respectively. We further show that /(pi) < l(p2) < l(pl) + 1. The paper is 
organized as follows. In Section 2, we briefly recall the basic features of a MCM-PSI. Then, the N-SDSI 
structure is introduced, and we show how the related MCM-ISI can be transformed into a MCM- PSI. These 
results are then used in Section 3 to solve the flow control problem in the case when N = 1. Con­ cluding 
remarks are given in Section 4. 2 Markov Control Model with Delayed Information Let IR denote the set 
of all real numbers. Let g be any mapping defined on some set E such that E = El x E2. With a slight 
abuse of notation, g (e 1, e2) will stand for g(e) foranye=(el, ez) 6E, ei EEi, i= 1,2. 2.1 Markov Control 
Model with Perfect State Information A MCM-PSI (see Schal [9]) is the collection of the fol­lowing objects: 
a state space X, an action space A, a transition law q and a cost function C : X x A -+ Et. In the following, 
we shall assume that the set X is denu­merable and that the set A is finite (a more general set­ting 
can be found in Hern6mdez-Lerma [3] and in Schal [9]). We shall also assume without loss of generality 
that in every state all the actions in A are available to the decision maker. Let ?(X) and P(A) be the 
set of all probability measures on the Borel a-algebra of X and A, respec­tively. The transition law 
q is a transition probability q : X x A --+ 7(X), where g(o ] x, a) is the distribution of the next state 
visited by the system if the system is in state x E X and action a E A is taken. A control policy (or 
simply a policy) u is defined as a sequence of conditional probabilities Ut : Ht ~ P(A), Ho := X and 
Ht+l = Ht x(A xX) for all tc IN,such that Ut (ht; e) assigns probability y one to the set A for all ht 
~ Ht, t c IN. Let U be the class of all policies. A stationary policy is a policy such that ut(ht; .) 
is concentrated at the point a(zt) for all ht = (%~, ao,..., zt_l,at_l,q) E Ht, t c IN, where a is a 
measurable mapping from X to A. In other words, a stationary policy is a nonrandomized policy that only 
depends on the current state of the system. Given some initial distribution p. on X and some transition 
law q, any policy u defines a probability mea­sure on the product space (X x A)m endowed with the product 
a-algebra (see Schal [9]). On this probability y space, let us define the random variables Xt and At 
that 194. Performance Evaluation Review, Vol. 20, No, 1, June 1992 describe the state of the system 
at time tand the action taken at time t,respectively, for all t c IN. A standard objective in this setting 
is to solve the following control problem (e.g., see Ross [8]): (P@): Minimize VP(X, u) over U for all 
z 6 X, where Vp(~, u) := E ~ /3tC(Xt, Ai)l Xo = z , [ tEIN 1 u E U, where ,6 is a discount factor in 
(O, 1). The quan­tity VP (z, u) is the expected /?-discounted cost incurred over an infinite horizon. 
Let VP(~) := infue ~ V@(~, u). Let H be the set of all real-valued function on X. The solution to the 
problem PO may be obtained by using the following basic result of the MCM-PSI theory (cf. Bertsekas[l, 
Chapter 4]): Theorem 2.1 Assume that the cost function C is non­ negative. Then, for every /? G (O, 1), 
Vp satisjies the Dynamic Progmmming (DP) equation for all z G X, where the DP operator To : 7-t -+ W 
as given by Tp~(o) := min C(o, a) + ~ ~ j(z ) q(z I e,a) . aeA{ Zfex } (2) Further, the stationary policy 
which selects an action minimizing the right-hand side of (1) for all x f= X solves the problem I?P. 
 2.2 The N-SDSI Pattern We now consider the case when the state of the sy,tem at time tis not known to 
the decision maker until time t + N for all t E IN, where N is a given integer. How­ever, we will still 
assume that all past actions are known without delay at any decision epoch. It is worth noting that this 
setting differs from the imperfect information setting usually encountered in the literature (see Bertsekas 
[1, Chapter 3] and Herniindez-Lerma [3, Chapter 4]), where at each stage tthe decision maker receives 
some observation about the current state of the system, that may be corrupted by some random disturbance. 
Alike to the standard imperfect information setting we shall show in this section that, by enlarging 
the state space so as to include the last known state and the suc­cessive actions taken during the delay 
period, the con­trol problem with the N-SDSI pattern can be reformu­lated into the MCM-PSI framework. 
Define Y := X x AN. We first introduce the notion of N-SDSI policies. Definition 2.1 A N-SDSI-policy 
u is defined as a se­quence of transition probabilities ut : I<t --i 7(A) with KN :=Y andKt+l =Ki x(A 
xX) for allt>N, such that ut (kt; e) assigns probability one to the set A foranykt GKt, t= N, N+l,... 
Let UN be the set of all N-SDSI-policies. A natural objective is then to minimize over UN the cost criterion 
v@N(y,u) := E ~ @-N C (z~_N,At) I Zo = !J , [ t>N 1 (3) for all y E Y, where Zt :=( Xt, At, At+l, ..., 
At+N-l), t G IN, and where Cl is a measurable mapping from Y x A to R In (3) E. is the expectation operator 
associated with the probability space defined by a policy u E UN, given some initial distribution p on 
Y and the transition law q. Let us now show that the solution to the MCM­1S1 problem (3) may be obtained 
by solving a related MCM-PSI problem. This MCM-PSI is constructed as follows (see Sec­tion 2.1). Let 
Y, A, q , C and U be its state space, action space, transition law, cost function and set of policies, 
respectively, where q (y ly;a) :=l{(aj, aj,....&#38;r)= (a~,..., aN, a)} q(d l~;al)j (4) 195. Performance 
Evaluation Review, Vol. 20, No. 1, June 1992 for all y = (x, al, a2,..., aN) e Y, y = (Ll? ,a(,aj,..., 
a~)~Y,a~A. Given the initial distribution p on Y and the transi­tion law q (or, equivalently, see (4), 
given the transition law q), any policy u G U defines a probability y measure on (Y x A)m endowed with 
the product u-algebra. Let l; and A; be random variables describing the state of this new system at time 
tand the action taken at time t, respectively, for all t C IN. For any vector kt EY x(A xX)i, tE IN,such 
that kt=(20, ao, . . ..a~_l. a~, zlja~+t_l,zt), zt), define the mappings gt : Y x (A x X)t -+ Y x (Ax 
Y)~ as 9t(kt) = (yO, aO, . . .,w-l, at-l, yt), with yi = (ri, at, ai+l, . . ..ai+N_l )fori=O,l,..., t. 
The following lemma holds: Lemma 2.1 Let U = {uj}tcm C U and u = {ut+~}tem E UN be such that U:(* Igt(h)) 
= 4+N(0 Ifh)) (5) for all kt e Y x(A xX) , t E IN. Then, the pro­ cesses {Yi)Aj; t c IN} and {Zt, At+N; 
t E IN} have the same distribution under policies u and u, respectively. Proof. The proof is left to 
the reader (hint: show by induction in t that {Y,, A;; O < s < t} and {Z,, A,+N; O s s < t E ~} have 
the same distribution under policies u and u, respectively, for all tc IN). ~ Applying Theorem 2.1 to 
the MCM-PSI {Yt, tE IN},it is seen that there exists a stationary policy u = {u: }Cm in U that solves 
the following problen.: (Qe): Minimize J@(y, u) over U for all y ~ Y, where ~~(y,u) := E ~ ptc (Yt, 4; 
)lYo = Y , (6) [ teni 1 Let a be a measurable mapping from Y to A such that ~~(olyO, aO, . . . ,Yt) = 
l(a (yt)= .), teIN. (7) We have the following proposition: Proposition 2.1 The poiicy v = {~;+N}@N 6 
UN such that ?)~(.lk~) = l(~*(Zt, @,. ... (2t+N_l) o), (8) for all kt GY x(A xX)t wdh kt=(Zo, UO, . . 
.. U_l)UN.Z1,l, . . . ,Ut+N_l, *t), minimz~t?svpN(y, u) over UN, for all y ~ Y. Proof. Let u be an arbitrary 
policy in UN. Let U c U be such that (5) holds for all tc IV. Then, by Lemma 2.1, (3), (6), Jp(y, u ) 
= vpN(Y, u), for all y~Y. On the other hand, JP(Y) U*) < J@(y, u ), for all y c Y, since u* solves the 
problem Qp. It remains to show that Jp (y, u*) = V@N(y, v*) for all y G Y. This result again follows 
from Lemma 2.1 since it is easily seen from (7) and (8) that policies u* and v* satisfy (5). ~ It other 
words, we have shown that a policy that minimizes the cost VPN(O, u) over UN may be obtained by solving 
the problem Q6. To conclude this section, let us briefly discuss the problem of minimizing the ,B-discounted 
cost over all stages t E IN. Assume that the initial distribution p. is known. Then, once Jp is known, 
the following optimization problem may be addressed: u 6 U . Let .lp(y) := inf~eu, Jp(y, u). 196, Performance 
Evaluation Review, Vol. 20, No. 1, June 1992 lXo=z, Aj=ij, O<j< N-l] forally =(~, a)e Y=lNx A,A CA. In(9), 
. .>2pJ-l  +/3NJ/3(z,io,. ), 1} where r is a measurable mapping from Y to IR. This optimization problem 
will not be investigated in this article. In the remainder of this paper, we will assume that the initial 
distribution p. is not known. 3 A Flow Control Problem with Delayed Information The results in Section 
2.1 are illustrated through the study of a simple flow control problem with the 1-SDSI pattern (i.e., 
N = 1). Throughout this section /3 is a fixed number in (O, 1). 3.1 The Queueing Model Considered is 
a discrete-time single-server queueing model. The service times form a sequence of i.i.d. ran­dom variables 
distributed according to a geometric dis­tribution with known parameter O ~ b < 1. We assume that at 
most one customer may join the system in every time slot. This arrival (if any) is assumed to occur at 
the beginning of the time slot (synchronized arrivals). At the beginning of each time slot, the decision 
maker chooses in the set A := {Pi, PZI}, O < P2 < PI < 1, the probability of having one arrival in this 
time slot. Therefore, if action pi is chosen at time t E IN then a customer will enter the system at 
time t with the prob­ability pi, i = 1, 2. We further assume that a customer that enters an empt y system 
may leave the system ( with the probability y b) at the end of this same time slot. Let Xt ~ IN denote 
the number of customers in the system at time t, t c IN. We assume that the state of the system is known 
with a delay of one unit of time or, equivalently, we assume that the 1-SDSI pattern is used. Our objective 
is to minimize, over U1, the cost cri­terion V; (o, u) defined in (3) in the case when C (y, A) := c(z) 
+~a, (9) C(Z) is any real-valued n ondecreasmg convex function on IN and -y is an arbitrary constant. 
Observe that the assumptions placed on c ensure that C t is bounded be­ low. Therefore, we shall assume 
in the following that without loss of generality the cost function C is non­negative. The choice of the 
cost function C is discussed below. From the results in Section 2.2, we know that the solution to this 
control problem is obtained by solving the problem Q@, defined in (6), where the transition law q is 
given by (see (4)) a~, ifz~l, z =x+l; (lo) /(Y I Y;A) = { 1 a~, ifz=x =o; a6, ifz=O, z?=l; o, if Iz 
z l > 1, [ if y = (z , a ) ~ Y is such that a = A, and zero other­wise. Let us now comment on the definition 
of the cost C (see (9)). If c is nonnegative and if y <0 then the cost Cl reduces to a cost frequently 
encountered in the literature on flow control models with completely ob­servable states (see for instance 
Stidham [11]). In that case, C(Z) can be interpreted as a holding cost per unit time, and ~a as a reward 
related to the acceptance of an incoming customer. Then, the problem Qp involves a trade-off between 
low expected response time and low throughput on one hand (e.g., always choose action PZ) and high expected 
response time and high throughput on the other hand (e.g., always choose act ion pl ). A similar problem 
in the case of completely observable states was addressed by Ma and Makowski [6] under the additional 
requirements that the expected response time must be bounded from above. Also note that the problem Q9 
becomes trivial when c is nonnegative and y ~ O. In that case, the optimal action is always to choose 
p2. The remainder of Section 3 is devoted to identifying a stationary policy in U that solves the problem 
Q@ in the case when the cost function and the transition law are given by (9) and (10), respectively. 
197 Performance Evaluation Review, Vol. 20, No. 1, June 1992 3.2 Preliminary Results Further not ation 
are needed at this point. Let K be the set of all real-valued functions on Y. Let SA : K x Y -+ Et be 
the operator defined as S~(~,y) := ~ j(y )q (y [ y; A), (11) y eY forall~6K, y6Y, A~A. It is easily seen 
from (10) and (11) that SA(.f, O,a) := (1 -a~)f(O, A)+a6f(l, A); (12) SA(.f,~,a) := tibf($ l,A) + (ab 
+ ti6)f(0, A) +a~f(z + 1, A), (13) forallz>l, acA, AEA. Let T@ : K -+ K be the DP operator associated 
with the problem Qp. It is easily seen from (2), (9) and (11) that TP.f(Y) = C(Z)+ 7 a +~min{$, (j, $, 
a), %,(f, ~, a)}, (14) for all y = (zja) CY. It is usually difficult to directly determine the opti­mal 
policy from the DP equation J8 = T@J8 (see (l)). An alternative approach is to use the well known value 
zteration algorithm (see Ross [8]). A key result of this approach is the following (see Bertsekas [1, 
Sec. 5.4, Prop. 12]): Proposition 3.1 -Let ~0 be the .zero function on K. Then, lim Tjfo(y) = Jp(y), 
y E Y. (15) n-co Proposition (3. 1) will be used in the proof of Theo­rem 3.1. 3.3 The Value Iteration 
Algorithm Ap­preach The following notation will be used throughout this sec­tion. We shall say that f 
G K satisfies assumption 198. Performance Evaluation R1 if ~(x, pl) f (z, pz) is monotone increasing 
in x (i.e., j is supermodular); equivalently, f(~, pl) .f(z-l,pl) 2 .f(*, P2)-.f(x-l, P2), ~ 2 1; R2 
if $(z + 1, pz) f (x, pl) is monotone increasing in x; equivalently, f(z, p~) f(x l, pl) < f(r+l, pz) 
f(%, pa), x >1. A mapping h : IN -+ JR will be said to satisfy assumption R3 if h(x) is integer-convex 
in z; equivalently, h(z+l) h(x) ~ h(x) h(z 1), z ~ 1; R4 if h(l) ~ h(0). The proof oft he main result 
(see Theorem 3.1) relies upon the following technical lemmas: Lemma 3.1 Let h : IN --i IR be a nondecreastng 
func­ tton. Then, for ail x ~ 1, Proof. For i = 1,2, x z 1, define We must show that 172(3 + 1) z I?l(z) 
for x z 1. We have F2(Z+ 1) Fl(x) =p26[h(2 +2) /l(z+ 1)] +(1 (pit+ fib)) [h(z + 1) h(z)] +Flb[h(z) 
 h(z l)]. The proof is then concluded by using the increasingness of h together with the fact that pl~ 
+ ~2b ~ 1. ~ Lemma 3.2 Let f 6 K be such that f satwjies RI and R2. Then, f (o, a) satiafies R3 for all 
a E A. Proof. Let f 6 K be such that f satisfies R1 and R2. We have, for x~ 1, f(%+l, p,) f(fc,p,) [f(fc, 
p,)-f(z l,p,)] 2 f(~+ l,P1) f(~,m) [f(~ + 11P2) f(~, P2)l,   > 0, Review, Vol. 20, No. 1, June 1992 
where the first (resp. second) inequality follows since ~ satisfies R2 (resp. RI). On the other hand, 
f(~+ 1,P2) -f(*, P2) -[f(~,P2) -f(~-1,P2)I 2 f(~, Pl) .t-(~ I, PI) U($,P2) f(* 1, P2)I, > 0, where 
again the first (resp second) inequality follows since ~ satisfies R2 (resp. Rl). This establishes the 
proof. ~ Lemma 3.3 Let f E K be such that f(o, a) satisjies R3 and R4 for all a E A. Thenj SA(f, l,a) 
~ sA(f,lo, U) forall AEA, aEA. Proof. Let ~ E K be such that f(o, a) satisfies R3 and R4 for all a G 
A. From the definition of SA ( f, z, a) (see (12)-(13)) it is seen that for A G A, a c A, s~(f, l,a) 
 SA(f, O,a) = a~[f(2, A) f(l, A) (16) -(f(l,A) -f(O,A))] +(1 d) [f(l,A) f(O, A)] , which is nonnegative 
since f(o, a) satisfies R3 and R4, which concludes the proof. ~ Lemma 3.4 Let f E K be such that f satisfies 
RI. Then, S(,) (f,., a) satisjies RI for all a g A. Proof. Let f E K be such that f satisfies R1 and 
fix aEA. Since f satisfies Rl, it is seen for x z 2 that %( fj~]a) $,(f, x I,a) = ab[f(x l,Pl) f(~ 
 z,lZ)l + (ah+ tiG)[f(z, p~) f(z l,p~)] +a~[f(z +1, pl) f(z, pi)], ~ Gb[f(z I,pz) f(z 2,p2)] + (ah+ 
ti~)[f(z, p~) f(x l,p~)] + ab[f(x+ l,p~) f(z, p~)],  = $,( f,z, a) SP, (f, z l,a). On the other 
hand, using (16) we see that Spl(fj 1, a) Spl(f) 0, a) = aF[f(2, pl) f(l, Pl)l +(1 db a~)[f(l,pl) 
 f(O, pi)], ~ a6[f(2, p~) f(l, p~)] +(1 iib-al)[f(l, pz) f(O, p2)], = sP2(f! l)a) Sp, (f) O)a), 
by using the assumption that f satisfies R1 and the fact that tib+a~< 1. ~ Lemma 3.5 Let f E K and A 
~ A besuch that (i) f satisfies RI and R2, and (ii) f(o, A) satisjies R4. Then, SA(f, e, e) satwjies 
R1 and R2. Proof. Let f G ~ and A c A be such that conditions (i) and (ii) in the lemma hold. We first 
establish that SA(f, e, e) satisfies R1. For z z 1, we have s~(f, ~,~1) SA(f, $,P2) = (?1- F2)bf(~ -1A) 
+ ((PI pz)b + (P1 l%)~)f(~, A) + (P1 P2)~f((~ + l,A)))   (P1 -P2) {~[f($+ l,A) -f(~, A)] + b[f(z, 
A) f(z I,A)]} , and the latter is increasing in x since pl > pz and since by Lemma 3.2 f (o, a) satisfies 
R3 for all a ~ A. It remains to show that the monotonicity property also holds at the boundary. We have 
SA(f, l,pl) SA(f, l,p2) [SA(f, O,p,) -SA(f, O,pz)] = (Pl Pz) [b(f(l,A) -f(O>A)) +~(f(2, A) -f(l, A) 
-(f(l, A) -f(O, A))] , which is nonnegative since f(o, A) satisfies R4 and since (see Lemma 3.2) f(o, 
a) satisfies R3 for alla E A. This completes the proof that SA ( f, .,.) satisfies RI. Let us now show 
that SA ( f,.,.) satisfies R2. For z ~ 2, it is easily seen that SA(f,$,p~) SA(f,~ l,pl) 199. Performance 
Evalual ion Review, Vol. 20, No. 1, June 1992 . .. .-/.. \. \SA(.t, ~+ 1,~2) L5A(j,~,p2)j == Plb[f(z 
 l,A) ~(x 2, A)] +(Pl~ +Pl~) [f(x, A) f(~ I, A)] +P16 [f(z + I,A) f(z, A)l p2b [f(z, A) f(x l, 
A)] -(pzb +F@ [.f(~ + l,A) -f(~, A)l p2J[f(x+2, A) f(z+ I, A)], p26[f(z + Z,A) f(~ + l,A) -(f(x + 
l,A) -f(x, A))] -(1 -pl~-@b)[f(x + l,A) -j(z, A) -( f(r, A) -f(~ -l,A))] plb[f(x, A) f(~ l,A) -(.f(~-I, 
A)-~(z-2, A))],  < 0, where the above inequality again follows from the fact that ~(., a) satisfies 
R3 for all a c A. It remains to examine the case when x = 1. We have SA(~,l,~l) sA(.f, o,Pl) [SA(~,2,~~) 
-S~(~, l)PZ)]  p~~ [~(3, A) ~(2, A) -(~(2, A) -$(1, A))I -(1 -pl~ -~,b) [f(2, A) -~(1, A) -( f(l, A) 
-f(o,A))l plb [f(l, A) f(O, A)] , which is nonnegative since ~(., A) satisfies both condi­tions R3 
by Lemma 3.2 and R4 by assumption. Hence, SA (~,., e) satisfies R2, which concludes the proof. E Lemma 
3.6 Assume that f satzsfies RI. Then, for every a E A, (i) there exists i(a) 6 lNU {+co} such that SP, 
(f, ~ja) SP, (f, x,a) < 0, for O < z < J(a); %1(-f}x)a) sP2(f>~>a) 2 0> for~ 2 ~(a) Moreover, (ii) /(pl) 
< l(p2) and ( iii) l(p2) < l(P1) + 1. Proof. Let f G K be such that f satisfies RI. The property (i) 
is a direct consequence of the fact that S(.)(~, e, a) satisfies RI for all a E A (see Lemma 3.4), where 
l(a) := inf{x ~ O : SP, (~, x,a) ~ SP, (~, x,a)}, a c A. Let us now turn to the proof of (ii). For z 
~ 1 we have $,( f!~!Pl) sp,(.f,~,?h) -[spl(f, Z,P2) -$,(f) ~!P2)l Flb[.f(x l,P1) ~($ 1,P2)] +(plb 
+ ~1~)[.f(~,pl) .f($, P2)1 +Pl~[f(~+ l) Pl) .f(x+l)P2)l F2b[f(Z l,pl) ~($ 1,P2)] -(p2b + F2~)[f(x,P1) 
-.f($,P2)1 FJ2b[f(ir +1, Pl)-f(x+l, P2)lj (Pl -W) {~[f(~+ lP1) -f(x+ 1P2) -(f(r,P,) -f(x,P2))l +b [.t 
(~, pl) f(~, P2) (f(x l,P1) -f(z -l,p,))]} , 0, (17) since pl > p2 and since .f satisfies RI. For 
x = Owe have .%(f)o!Pl) &#38;2( f)o, Pl) -[$, (f, o,P2) -sp2(f,01P2)l = (PI -P2) ~[f(ljPl) -f(l,P2) 
-(f(o,P,) -f(ojP2))l, > 0, (18) since ~ satisfies RI. Therefore, it is seen from (17) and (18) that SP, 
(f, Z,p~) SP,(f, ~,Pl) Z s~,(f, ~,P2) -Sp, (f, X, P2) for all z z O, which proves (ii). It remains to 
establish (iii). For x ~ 1, we have sP, (f)~l Pl) sP2(f)~)Pl) -[sp, (f, $ + 1,P2) -Sp,(f, z+ 1,P2)I 
 ~lb[f(x l,p~) f(z 1,p2] +(plb+ Plj)[f(r, pl) f(z, P2)] + plb[f(%+ l,P1) f($+ 1,P2)] P2b[f($, pl) 
 f(~, P2)] -(P2b + @O[f(* + l>P1) -f(~ + 1)P2)I  200. Performance Evaluation Review, Vol. 20, No. 1, 
June 1992 p~~[f(x + 2,p~) f(z +2, p~)] < 0. (19) The above inequality is obtained by applying Lemma 
3.1 with h(z) :=~(z, pl) ~(z, p2) and by noting that h is nondecreasing since ~ satisfies RI. On the 
other hand, %lU>O,P1) %2 U)O,P1) -[sp, (f, 1,P2) -Sp,(f, 1,P2)I P23 [f(2, P1) f(2, P2) -(f(l,f?,) -f(l,P2))l 
-(1 -p,b -pal)) [f(l,p,) -j(l,pa) -(f(o,p,) -f(o,P2))l , < 0, (20) since ~ satisfies R1. Combining (19) 
and (20) yields Z(P2) < 1(P1 ) + 1, which completes the proof. ~ Lemma 3.7 Let f G K be such that f satisfies 
RI and R2, and f(o, a) satisfies R4 for alla c A. Then, T))f satisfies RI and R2, and Tpf(e, a,) satisfies 
R4 for all aGA. Proof. Let f E K be such that f satisfies RI and IR2, and f(o, a) satisfies R4 for all 
a c A. We first establish that TO f satisfies R1. This is equivalent to showing that G(z) := /3-1 (Tpf(x, 
pl) Tpf(x, p2) -[Tpf(z + l,pl)-Tpf(x+ l,p,)]), = min{Sp, (f, $,pl), Sp2(fj~, Pi)} (21) rein {SP, (f, 
x,p2), Sp, (f, ~jP2)} -[min{SP1(f, z+ l,pl),.Sp. (f, ~+ l,P1) min{.$$, (f, z + l,p2), Sp, (~, ~ + 1, 
P2)}I , is nonpositive for all z ~ O. Recall the definition of /(pi) for i = 1,2(see Lemma 3.6). Also 
recall that /(pl) ~ i(pz) ~ i(pl) + 1. We shall distinguish the following cases: (1) z > /(pZ); (2) max(O, 
/(p~) -1)< z < i(p2); (3) x < max(O,/(pi) -2).  Case (l): z > /(pa). We have, cf. (21), G(z) = rnin{Sp, 
(f, ~,pl), SP, (f, ~,Pl)} %,(f) $!232)  -[sp,(f, x+ l, PI) -Sp,(f, t+ 1, P2)]}, < sp2(f, z,Pl) %,( 
f)~!P2) -[sp2(f, z+l, Pl)-sp2(f, z+l,P2)], which is nonpositive since Spz ( f, e,e) satisfies R1 from 
Lemma 3.5. Case (2): max(O,/(pi) 1) < z < J(p2). We have for all z c IN, cf. (21), G(z) = min {sP,(f, 
z,P1),sp~(f,z,n)} (22) Spl(f, 3,P2) [Sp,(f, ~+ l,P1) min{Spl(f, z + l, P2),.S p, (f, ~ + 11P2)}I. For 
z ~ 1, we have from (13), (22), G(x) s sp,(f, ~,d sp,(f, x,m) -[sp, (f, x+ l,P1) Sp,(f, $+ 1,P2)] , 
(Pl P2) {b[f(~>pa) -f(~ -l,P1) -(f(x + 1,P2) -f(~) Pl))l +t[f(~+ l) P2) f(~, Pl) -(f(z+2,P2) -f(~ + l, 
PI))I} , which is nonpositive since f satisfies R2 and since pl > P2 . On the other hand, we see from 
(12), (13), (22) that G(0) < sp,(f, O,P1) sp,(f>0~P2) [Sp,(f, l,P1) Sp,(f, 1,P2)I, = -(PI -Pa){ b[f(l,zn) 
-f(o,zm)] +6[f(2, p2) f(ljpz) (f(l, pl) f(o, pi)]}, which is nonpositive since f satisfies R2 and 
since f(o, a) satisfies R4 for all a GA. 201. Performance Evaluation Review, VO1. 20, No. 1, June 1992 
Case (3): 2< max(O, l(p~) 2). We obtain from (21) G(z) = &#38;,(.f, z,Pl) Sp, ($, ~1P2) -[sp, (f, z+l, 
Pl)-sp, (f, ~+1, P2)]) which is nonpositive for all z c IN since Sp, ($, e, e) sat­isfies RI by Lemma 
3.5. This concludes the proof that G(x) ~ O for all z c IN. Hence, T@~ satisfies R1. Let us now show 
that Tp~(o, a) satisfies R4. We have, cf. (14), ~- [T@~(l, a) -Tpf(o)u)l  = B- (41) -40))+ rnin{sp,(f, 
lju), sp,(f, I, U)} rnin{sp,(f, O,u),sp,(f,o,u)}, u 6 A. (23) Since SA(j, 1, a) > SA(~, O, a) by Lemma 
3.3 and since c(l) ~ c(O) by assumption, we readily deduce from (23) that T@~(o, a) satisfies R4. It 
remains to establish that T@f satisfies R2. This amounts to showing that H(z) := ~-1 (TPf(~ + l,p2) T6f(z, 
p2) -[Tpf(z, p,) -Tflf(x -l,p,)]), is nonnegative for all z ~ 1. We have for x~ 1, H(z) = /3-1 [C(x + 
1) c(x) (c(z) C(z 1))] +min{SP, (~, $+ l,p2),5 P, (f,*+ 1,P2)} min{spl (f, ~,p2)jsp, (fjxl P2)} -[min{spl 
(f, x, Pi), sp2(flxjPl)} min{SPl(~, x l,pI), Sp, (~, ~ l, Pi)}] , ~ min{SP, (~, z+ l,p2), sp2(.f,~+ 
1,P2)} min{Sp, (f, z, P2), sp, (fj~l P2)} -[rein {Spl(f, X,PI), Sp, (f, z,n)} min{SP, (~, z l,pl),spz(.f,~ 
 lIP1)}I, (24) where (24) follows from the convexity of the cost func­tion c. For z ~ l(pl) + 1, we have 
from (24) H(z) 2 Sp, (f, x + 1,P2) $,(.f)*, P2) -[&#38;( f,x, Pl) -Sp, (f, x -l,P1)] , > 0, The first 
inequality holds because /(p2) < l(pl) + 1. The last inequality holds because SP2 (~, e, e) satisfies 
R2 (cf. Lemma 3.5). It remains to cover the case when 1 ~ z ~ i(pl ). Assume first that I(pl) = 1. We 
have for x = 1, cf. (24), H(l) 2 SP, (~, 2,P2) min{spl ($, 1,P2), SP, U, 1,P2)} [min{.$l (f, l,n),sp,(fj 
l, IJ1)} -%,( f,o,n)], Sp, (f, 2)P2) Sp,u, 1,P2) [sp2(f,l, Pl) sp, (j-, o,Pl)] , ~26[~(s,~z) $(2, pl) 
 (~(2, p2) ~(1, pi))] +(1 PIT p2b) [f(2, p2) f(l, pl) -(.f(l,P2) -f(o,pl))] +Flb [f(ljP2) -f(o,P2)l, 
which is nonnegative since f satisfies R2 and since ~(., a) satisfies R4, Assume now that l(pl) ~ 2. 
For 1< z < l(pl) 1, it is easily seen from (24) that H(x) 2 Spl(f, x + 1,P2) Sp, (f, X>P2) [sp, (f, 
x,Pl) (f, z Sp, l,P1)] , which is nonnegative since SP, ( f, e,.) satisfies R2 (cf. Lemma 3.5). For 
z = /(pl), we have H(z) > Sp, (f, x + 1,P2) szl, (.f>3)P2) [sp, (f, iz, pl)-s p, (f, bl, pi)], pzb[f(x,pz) 
 f(z l,pl)] +(pzb +Pzt) [f(x + l,pz) f(x, p~)] +p2T[f(x + 2,p2) f(z + l,pl)] p~b[f(iz l,p2) f(z 2, 
pi)] -(Plb+PlJ)[f($,P2) -f(x -l,P1)] plb[f(z+ l,pz) f(z, p~)], 202. Performance Evaluation Review, 
Vol. 20, No. 1, June 1992 which is nonnegative by Lemma 3.1 with h(x) := jl(x + I,p2) f(z, pl) (note 
that h is nondecreasing since j satisfies R2). Assume now that z = /(pl) 1. When l(pl) = l(pz) this 
case is seen to reduce to the case x = l(pl). When l(p2) = /(pl) + 1, we have H(z) 2 sp, (f, ~+ 1)P2) 
min{sPl (~, ~,p2), Sp, (~l$JP2)} -[min{Spl(f, ~,Pl), SP,(~, ~,Pl)} $ll(fj~ l, PI)I> 2 Sp, (f, r+ 1)P2) 
 5 p, (.f, ~!P2) -[spl(.f, *,Pl) Sp, (f, x l,P1)] , which is nonnegative since SP, (~, e, e) satisfies 
R2, which concludes the proof. ~ We are now ready to prove the main result of this article. Theorem 3.1 
There exists an optimal threshold policy for the problem Q@. More precisely, for evey a E A there exists 
i(a) 6 ~U{+m} such that when the system is m state y = (x, a), x E JN, then the opttmal actton is pl 
ifx < l(a), and p2 otherwise. Further, KP1) < ~(P2) s ~(Pl) + 1. Proof. Define L7 c K to be the set 
of all real-valued functions on Y such that f satisfies RI and R2, and ~(o, a) satisfies R4 for alla 
e A. Since the zero function .fo on K belongs to g, it is seen from Lemma 3.7 that T;.fo GG, n> 1. (25) 
 Since ~ is closed under pointwise limits, we see from (25) and Proposition 3.1 that ~p E ~, which in 
turn implies from Lemma 3.6 that for every a c A there exists l(a) c IN u {+m} such that SP, (Jp, r,a) 
 SP, (Jp, Z, a) < 0, for z < i(a); spl(~pj~)a) SP,(J~, ~,a) > 0, for z 2 ~(a). Hence, it is seen from 
(11), (14) and Theorem 2.1 that the optimal action when the system is in state Y = (~, a) C Y is PI if 
z < l(a) and pz otherwise. The last part of the theorem also follows from Lemma 3.6. 4 Concluding Remarks 
The existence of an optimal policy of a threshold type has been shown for a simple flow control problem 
in the case when the decision maker has only access to delayed state information (the so-called 1-SDSI 
pattern). High-Speed Data Net works (HSDN s) provide a good instance where such N-SDSI information patterns 
are useful. In­deed, because of the very high throughputs that can be handled by HSDN s, the state of 
the network may have changed considerably by the time the messages on the state of the network reach 
the decision-maker. Therefore, only cent rols based on delayed information patterns may be used as far 
as closed-loop controls are concerned. Extensions of our model to arbitrary values of N and to more general 
arrival processes (e. g., batch ar­rivals, Markov Arrival Processes, cf. Neuts [7], Lucan­toni [5]) are 
however needed so as to capture the basic characteristics of the traffic in HSDN S (e.g., burstiness). 
These issues, as well as the long-run average version of the control problem considered in this article, 
are the object of ongoing research. Acknowledgements: The authors are grateful to A. Khamisy, N. Shimkin 
and to Prof. S. Stidham Jr. for useful discussions during the course of this work.  References [1]Bertsekas, 
D. P., Dynamic Programming. Deter­ministic and Stochastic Models. Prentice Hall, Inc. Englewoods Cliff, 
1987. [2] Grizzle, J. W. and S. I. Marcus, A decentral­ized control strategy for multiaccess broadcast 
net­works, ) Large Scale Systems, Vol. 3, pp. 75-88, 1982. [3] Hern6ndez-Lerma, O., Adaptive Markov Control 
Processes. Springer-Verlag, New York, 1989. [4] Hsu, K. and S. I. Marcus, Decentralized control of finite 
state Markov processes, IEEE Trans. Aut. Contr., Vol. AC-27, No. 2, pp. 426-431, Apr. 1982. 203. Performance 
Evaluation Review, Vol. 20, No. 1, June 1992 [5] Lucantoni, D. M., New results on the single server with 
a batch arrival Markovian arrival process , Stochastic Models, Vol. 6, pp. 1-46, 1991. [6] Ma, D.-J. 
and A.M. Makowski, Optimality results for a simple flow control problem, in Proc. 26th Conf. indecision 
and Control, pp. 1852-1857, Los Angeles, CA, USA, Dec. 1987. [7] Neuts, M. F., Structured Stochastic 
Matrices of the M/G/l Type and Their Applications. Marcel Dekker Inc., New York, 1989. [8] ROSS, S. M., 
Introduction to Stochastic Dynamic Programming. Academic Press, New York, 1983. [9] Schal, M., Conditions 
for optimality in dynamic programming and for the limit of the n-stage opti­mal policies to be optimal 
, Z. Wahrschezniichkeit­sth., Vol. 32, pp. 179-196, 1975. [10] Schoute, F. C. , Decentralized control 
in packet switched satellite communication ), IEEE Trans. Aut. Contr., Vol. AC-23, No. 2, pp. 362-371, 
Apr. 1978. [11] Stidham, S., Optimal Control of Admission to a Queueing System, IEEE Trans. Aut. Contr., 
Vol. AC-30, No. 8, pp. 705-713, Aug. 1985. 204. Performance Evaluation Review, Vol. 20, No. 1, June 
1992 
			