
 A Coding Theorem for Distributed computation Sridhar Rajagopalan* Leonard Schulmant University of California, 
Berkeley. 1 Introduction This paper is concerned with the question of whether it is possible to sustain 
computation in the presence of noise; and if so, at what cost in efficiency and reliability. Shannon 
in his classical coding theorem showed that data can successfully and efficiently be transmitted in a 
noisy environment [27]. The outstanding features of the theorem are first, that if a large block of data 
is to be transmitted over a noisy channel, the transmission need slow down by only a constant factor 
(relative to the noiseless case) and second, that one can achieve an error probability exponentially 
small in the length of the block of data. Data transmission is an elementary step in computation: this 
is overt in dktributed or parallel computation, but is also true of sequential computers, where almost 
invariably, several logical units are involved. Thk leads to the question: if a computation is being 
performed by several processors linked in a network, and if the communications in this network are unreliable, 
what is the effect on the efficiency and reliability with which this computation can be performed? In 
the main result of the present paper, we address this question by providing a network analog of Shannon 
s coding theorem. We show that a dk.tributed protocol which runs on a network of degree d can be simulated, 
if the links of that network are noisy, with a slow-down factor proportional to log(d + 1) and with probability 
of error exponentially small in the length of the original protocol. In this we extend a recent result 
of Schulman[26] in which such a coding theorem was shown for any interactive proto­col involving two 
processors. The physical context for this work is simply that noise is an inherent characteristic of 
physical systems; and, in partic­ular, of communications hmdware. As computing systems grow, so does 
the necessity of dealing with such noise either by overbuilding the hardware to make it error-free; by 
introducing redundant hardware; or by implementing error­correction in software. *Supported by NSF grants 
# lR1 91-20074 and GGR-931oz14, Email: sridhar@cs.Berkeley. EDU. t SuPpOrted by an NSF Postdoctoral Fellowship. 
Email: schulman@cs.Berkeley. EDU. Permission to copy without fee all or part of this materfal is granted 
provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright 
notice and the title of the publication and its date appear, and notice is given that copying is by permission 
of the Association of Computing Machinery. To copy otherwise, or to republish, requires a fee and/or 
specific permission. STOC 84-5/94 Montreal, Quebec, Canada @ 1994 ACM 0-89791 -663-8/94/0005..$3.50 790 
In contrast to physical computers, machine-based mod­els of computing typically take noise-free devices 
as their basis: e.g. Turing machines, cellular automata, formal cir­cuits, and parallel and dktributed 
systems. Much work has been driven by the need to bridge this gap between model and reaM y. The first 
investigate ions in thk direction were by Shannon in 1948 [27], for the problem of data transmis­sion 
over noisy links; and by von Neumann in 1956 [34], for the problem of computation by circuits with unreliable 
components. This paper is a further contribution toward bridging the gap between noiseless models, and 
noisy reality. The model we address is closest to a distributed system (or a single computer with several 
processors). In this case, the indl­vidual processors are comput ationally macroscopic; corre­spondingly 
in our model the processors are capable of ex­tensive computing. Further, we assume that all noise in 
our system occurs on the communication links between the pro­cessors, and not in their internal comput 
at ions. (This is a useful model because the error rate on a long communication link may be significantly 
worse than that inside a localized processor.) 1.1 Background We very briefly review some of the extensive 
literature re­lating noisy and noiseless models. The foundational results of both Shannon and von Neumann 
were essentially positive statements. Shannon in his foundational work described the notion of the capacity 
of a communication link, and showed that data can be transmitted with high reliability across the link 
at any rate below capacity. He also showed a converse to thk coding theorem, namely that communication 
fails at any rate above capacity; but the striking technological implication lay in the coding theorem 
itself, which showed that modulo codkg theory , one may as well think of data transmission in a noise-free 
model. The Shannon theory applies to a wide class of channels; its essential character­istics however 
are already present in the case of a thay symmetric channel, one in which in each transmission, inde­pendently 
of all prior events, the transmitted bit is fllpped wit h fixed probability y E < 1/2. Therefore in this 
paper we will focus on this noise model. Von Neumann s work, in conjunction with later work by Dobrushin 
and Ortyukov [3, 4], Pippenger [17], Pip­penger Stamoulis and Tsitsiklis [20], Gal [9] and Reischuk and 
Schmeltz [23], shows that a logarithmic factor over­head in size is always sufficient, and sometimes 
necessary, for the simulation of noiseless by noisy circuits. (Here it is understood that each circuit 
component malfunctions in­dependently with constant probability. Pippenger haa also explored more general 
noise models [19].) Furthermore, von Neumann showed that only a constant factor overhead in depth was 
needed in going from noiseless to noisy circuits; that a factor greater than 1 was, in fact, sometimes 
neces­sary was demonstrated by Pippenger [18], and the form of the lower bound was later improved by 
Feder [6] and Evans and Schulman [5]. The noise-resilient circuits described by von Neumann and in the 
subsequent literature, are not embeddable with­out significant edge dilation in finite dimension. Since 
longer wires are more likely to malfunction, the question naturally arose of whether computation can 
persist in the more restric­tive model of noisy cellular automata. Affirmative answers in this regard 
were supplied by Taylor [28], Toom [30, 31, 32], Tsirel son [33], Gics [7] and G6CS and Reif [8]. There 
is an essential difference between the circuit and cellular automata results, on the one hand, and the 
studies in information and coding theory on the other. In the former models no component of the system 
is assumed to be noise­less: an entire reliable computation must be synthesized out of individual unreliable 
components. In the latter situation computations are aasumed to be reliable, but communica­tions are 
assumed to be error prone. The two suppositions correspond naturally to a fine-grained modeling of a 
system and a coarse grained modeling of it, Our work is in the latter line: The study of what is feasible 
when encoding, decoding and other local computation can be implemented cheaply and reliably at each processor 
in the course of a protocol, One case of this question was proposed by El Gamal and studied by Gallager 
[10]. They considered a complete network of noisy links among N processors, and a restricted communication 
model: in each round, a processor is allowed to broadcast some bit to the entire system. (The noise events 
on each channel are independent.) Each processor starts the computation with some private bit, and the 
goal of the computation is for all processors to learn all the bitsl. The cost of the computation is 
measured as the total number of broadcasts. The naive protocol for this problem requires O(N log N) broadcasts, 
but Gallager showed how to accom­plish the computation with only O(N log log N) broadcasts. It is a very 
interesting question whether this is best possible. We are concerned in this paper with communication 
aa a resource for computation. An essential aspect of this set­ting is the interactive communication 
that is needed between processors. In contrast, in information theory and coding theory communication 
has been viewed entirely in terms of the data transmission problem (even in those papers which studied 
the benefits for data transmission, of feedback from the recipient to the transmitter). The interactive 
model for communication complexity, in which the input to a problem is split between two processors linked 
by a noiseless channel, was introduced by Yao [35]. The model was devised to meaeure the cost incurred 
in de­parting from the single-processor model of computation. It has since been intensively investigated 
(e.g. [36, 16, 29, 11]; and see [12] for a survey). The effect of noise upon this model was studied by 
Schul­man [24, 25, 26], who proved an analog of Shannon s coding theorem showing that any interactive 
protocol for a pair 1Actually the stated goal was just to compute the parity of the bits, but Gallager 
s protocol as well ae others we are aware of, solve the harder problem with the same effort, and no lower 
bound is known to separate the two problems. of processors, designed for a noiseless communication link, 
could be simulated on a noisy link with only constant com­munication overhead. This result opened the 
way to the following basic question which was poseci in [24, 251: can the coding theorem for interactive 
protocols be extended to the efficient simulation of noiseless distributed protocols, on networks with 
noise? 1.2 Overview of this Paper We answer the above question by providing a compiler for distributed 
protocols which, given the description of a pro­tocol which runs on a certain network wilth noiseless 
links, converts the protocol into one which runs on the same net­work but with noisy links. Our result 
is: Theorem 1.1 Any protocol which runs in time T on an N-processor network of degree d having nciseless 
communi­cation channels, can, if the channels are in fact noisy (each a binary symmetric channel of capacity 
C, O < C < 1), be simulated on that network in time O(T&#38; log(d + 1) + &#38; log N). The probability 
of failure of the protocol is e QtTj. By comparison, Shannon s original coding theorem states that a 
block of T bits of data can be transmitted across such a channel in time O(T &#38; ), with error probability 
e-Q(TJ; and Schulman s paper describes a compiller for arbitrary noiseless communication protocols of 
length T between two processors, which guarantees a running time of O(T&#38;) and error probability e 
 (T). There are two principal ideas used in obtaining our re­sults. The first is that in our protocol, 
errors in the system that have a space-like separation that occur closer in time than the distance between 
them in the network cause no more delay to the protocol than a single error. This al­lows us to make 
an argument of a type that haa come to be known as a delay sequence mgument . This type of ar­gument 
haa been successfully employed in various contexts: for instance by Aleliunas [1], Luby [13], and Martel, 
Sub­ramonian and Park [14] in parallel communications; and by Ranade in routing [22] and load balancing 
[21]. The second is a coding technique, tree codes, introduced in [26]. Two major difficulties remain 
concerning theorem 1.1. The first is that, while there is an existence proof for the type of codes employed 
in our proof -the tree codes no explicit construction of such codes is known. The second is that, even 
given an explicit tree code, the computational overhead involved in certain decoding steps of our protocol, 
is exponential in T. Both of these difficulties parallel those which remained following the original 
proof of Shannon s coding theorem. There is one very special case of the general network problem, however, 
in which we can provide {aprotocol which is not burdened by these computational clifficulties. The problem 
is this: transmit B bits from end to end of a chain of N processors. Of course in the absence of noise 
this can be accomplished in time N + B 2. The network here is as simple as can be, and so is the problem 
statement, just data transmission. Remarkably, no time-efficient solution seems to have been provided 
for this problem until very recently when Nisan and Parnaa [15] analyzed the case in which the processors 
are connected by binary erasure channels. (Chan­nels which sometimes output Error but never the wrong 
bit.) Specifically, Nisan and Parnas showed that if the bks are pipelined through the chain, then except 
for an event of probability e Q IN+ B), the last bit will arrive at the end of the chain within time 
O(N + B). We address the case in which (as in the rest of our paper) the channels are bi­nary symmetric. 
In thk case, even the problem of efficiently transmitting a single bit (B = 1) from end to end appears 
to be not entirely trivial. Let the probability of error in the channels be (1 -6)/2 for some O < d ~ 
1. As indicated, our first result concerning the chain broadcast problem is an efficient protocol: Theorem 
1.2 A B bit message can be transmitted from end to end of an arra~ of N processors interconnected by 
binary symmetric channels in time O((B + N). e t og (B+ N))). All computations necessary for the protocol 
are polynomial time in B + N. The probability of an ewor in transmission is at $2(B+N) most e Our second 
result concerning this problem is a lower bound showing that the noise on the channels does force the 
broadcast to be slower than it would be in the absence of noise: Theorem 1.3 Suppose that each input 
bit 0,1 is equally likely. Then for any -y <1, for sufficiently large values of n, any bit which is computed 
at processor n at time (n l)y/&#38; + 1 can be equal to the input bit with probability at most 1/2 + 
o(l). 2 The Network Model Consider a network ~ with processor set V and intercon­nections E executing 
a synchronous dktributed protocol IL We use the letter d to denote the maximum degree of any vertex. 
In II each processor repeats the following three steps in sequence in each time step: Receive a bit 
on each incoming link.  DO a local computation.  . Transmit a bit on each outgoing link. The bit transmitted 
by processor p on link (p, q) at the end of time step t is received at processor q at the inception of 
time step t + 1. Formally a protocol II on a network Af is a collection of functions {IIp}, one for each 
processor p in the network. Let indeg(p) and outdeg(p) denote the in-degree and out-degree of processor 
p. (How many processors can send messages to, and receive messages from, p. Typically the communication 
links are bidirectional but we do not require this.) Each function l_IP maps indeg(p) binary strings 
a! (t), ...>afndeg(P) (t),of length to an outdeg(p)-tuple each t, of bits: m(t) = (rq (t),.(t)) .. Irutdeg(p) 
Each string a?(t) is the history of all bits that have been received at processor p by time talong the 
i%h incoming edge to p. Each bit II;(t) is the response of the processor along it s j th outgoing edge. 
3 The Simulation Protocol In this section we describe a simulation protocol Z which turns an sxbitrary 
network protocol II, designed for use on a noiseless network, into one which runs successfully with high 
probability even if the channels are noisy. 3.1 Communication using Tree-Coded Channels In X, as we 
will describe it, processors wiil use three symbols, {0, 1, bkp}, to communicate with each other. Two 
steps will be required to turn such a symbol into a binary string for transmission on a channel: 7 Encoding 
of the symbol using a ternary tree code X: Encoding of the tree code letter into a binary string, using 
a code for data transmission. We will speak abstractly of a tree-coded channel as one in which transmissions 
are encoded using these two steps, and decoded using a matching two step process. In order to explain 
this process we briefly describe tree codes. For more detail on this subject see [26]. A ternary tree 
code is a ternary tree (each internal node has 3 children) whose edges are labelled with characters chosen 
from S, a finite alphabet. To encode a string z E {O, 1, bkp}* of length tusing a tree code T, we set 
T(x) to be the concatenation of the characters found on the simple path from the root, to the vertex 
of the tree representing x. If s = (S1...Sm) and r = (rl...r~) are words of the same length over S, say 
that the dktance D(s, r) between s and r is the number of positions i in which St # ri (Hamming distance). 
A ternary tree of depth n is a rooted tree in which every internal node has 3 children, and every leaf 
is at depth n (the root is at depth O). Definition 3.1.1 A ternary tree code over alphabet S and of depth 
n is a ternary tree of depth n in which every arc of the tree is labeled with a character from the alphabet 
S subject to the following condition. Let VI and vz be any two nodes at some common depth h in the tree, 
and whose least common ancestor is at some depth h-1. Let W(vl) and W(V2) be the concatenation of the 
letters on the arcs leading from the root to VI and vz respectively. Then D(W(V1), W(vz)) > 1/3. (See 
figure 1). The key fact regarding tree codes is: Lemma 3.1.2 ([26]) There is a finite s such that an 
al­phabet S of size s sufices to label a ternary tree code of any depth. Figure 1: The figure exhibits 
the distinguishing property of tree codes. The strings V1 and vz may differ very little, but their tree 
code representations differ in 1/3 character positions where 1 is the height of the subtree pictured 
here. Thus the fist step of the encoding described above, is accomplished as follows: let z ~ {0, 1, 
bkp} be the paat history of transmissions from processor p to processor q, and 3.2 The Protocol suppose 
that processor p now wishes to send a character a E {O, 1, bkp}. Then the product of this step of the 
encoding is the letter of S which is inscribed on the arc between the vertices T(z) and 7_(za), In the 
second step of the encoding, the letter of S is encoded as a binary string using a data transmission 
code x : S + {O, 1}~. We will refer to the parameter k aa the dilation of the tree-coded channel. (We 
will later specify the range for k that is useful for our purpose.) Finally this resulting string is 
transmitted across the binary noisy channel. Decoding of a tree-coded channel is accomplished aa fol­lows: 
1: Decode the received k-bit string into a letter of S using the decoding algorithm for the data transmission 
code. x 7-1: Decode the tree code using the following maximum likelihood criterion: If ~ is tie received 
tre~ code string (the concatenation of all letters of S decoded using X-l ) in the present and previous 
rounds) then de­code to the string 2= T-1 (j) which maximizes P(jlz) among all strings x E {O, 1, bkp} 
*. Define an edge character emor to be the event that a tree code character is corrupted during transmission 
across an edge, i.e. that for a transmitted character a E S, the noise is such that X 1(X(a) + noise) 
# a. Observe that the prob­ability of an edge character error decreaaes exponentially in k. Say also 
that a processor character error (which we will occasionally refer to simply as a character error) occurs 
at processor p at time tif p makes an edge character error in decoding any of the characters it has received 
in that round from its various neighbors. In addition to the character errors, there is another type 
 of error we need to consider. Whether or not p makes a character error in decoding the character received 
from a nei hbor q in the current round, it may make an error in ~ T-. Often these events will occur 
together, but they need not: the tree code may be incorrectly decoded in spite of correct reception of 
the current character, because of the lingering effect of previous incorrect receptions; while the tree 
code may be correctly decoded in spite of a current incorrect reception, because the map from the tree 
code al­ phabet to {O, 1, bkp} is not injective. If p makes an error in T-1 in round tfor the message 
from one of its neighbors, we will say that an edge tree code error has occurred on that edge at time 
t;and if p makes such an error in round ton the message from any of its neighbors, then we will say that 
a processor tree code error (or just a tree code error) has occurred at processor p at time t. Hence 
if there is no pro­ cessor tree code error at (p, t),then p haa correctly decoded the tree codes for 
all of its neighbors at time t. Observe that successive edge character errors on an edge (q, p) are 
almost independent eventsz. On the other hand, successive edge tree code errors are not independent. 
To the contrary, they are determined by many of the same channel noise events, and so they may be significantly 
correlated. Understanding the behavior of the protocol in spite of these correlations will require some 
attention. Communications in E are executed through the tree coded channel mechanism, and Z can now be 
specified entirely in terms of transmissions and receptions of characters in {0,1, bkp}. Processors running 
Z will in every time step either sim­ulate one time step of II, or erase one previously simulated step 
of H. If the processor goes one step forward, then a data bit (O or i) is transmitted on on each outgoing 
channel. If the processor takes a step back, then the bkp character is transmitted on every channel. 
More formally, if p has out­ degree outdeg(p), Ep (t) is either a binary outdeg(p)-tuple or the outdeg~)-tuple 
(bkp, bkp. 0., bkp). The direction of movement is governed. by the notion of consistency which we will 
describe below, Observe that there is a well defined notion at each processor p and time tof the current 
time step of II being simulated. We will refer to this as the apparent time AT(p, t).If B(p, t)d~fnumberthe 
of times p has backed up before time t,then AT(p, ­ t)d~ft  2B(p, t). A string over {O, 1, bkp} is parsed 
to a string over {O, 1} in the following way: every bkp character erases the last preceding un-eraaed 
{O, 1} character (much like the Back Space key on most keyboards) . For instance the string 0, l, O, 
bkp, 1,0, I, bkp, l, bkp, bkp, O is parseal to 0,1,1,0. If p has transmitted a sequence Z(p g) @ {O, 
1, blsp} or received a sequence 2(9 P) ~ {O, 1, bkp} then the parsed versions of these strings, WJ(p 
q) and ti(~ p) in {O, 1}, will be referred to as the transcripts of p s communications to and from processor 
q in l_I; the collection of these transcripts, for all in-and out­neighbors of processor p, will be referred 
to as the transcript of II at p. The action taken by p in the succeeding round of X will be determined 
entirely by this transcript (and in particular, not by any further information available in the strings 
Z(p q) or ~(q p) )4. Notice also, that the length of each outgoing transcript from p at time t is exactly 
the same and is equal to AT(p, t). We will say that the transcript at (p, t)is consistent if for every 
time r and every out-neighbor q of p, the ~ th character oft he output transcript w P g) equals the character 
specified by the protocol II, given the prefix c~fthe transcript up through time ~ 1. There are two 
ways in which the transcript at (p, t) might be inconsistent. The first is that the current transcript, 
as decoded from the tree codes using max-likelihood decoding, disagrees with the transcript decoded in 
past rounds. (Be­cause of either past or present decoding errors.) Therefore some of the actions taken 
in past time steps may be differ­ent from what they should have been baaed upon the current transcript. 
The second way can occur if one (of p s neighbors has backed up in II. In this case p may have in the 
paat More formally, parsing follows the grammar rule (01l)bkp H A where A is the empty string and I 
allows selection It is easily seen that repeated application of the grammar rule on 2(Q,P) and Z(P,Q) 
results in a unique minimum length string over {O, 1}. 4The various strings and the relationships between 
them are de­scribed by the following dis,gram: 2They are determined quences of noise events by disjoint, 
and on the channel; therefore the only independent, reason the se­edge z 7_-code~hannel character errore 
are not entirely independent is that the distances parse +~ protocol J.p~rse among various codewords 
of x vary, and so the probabilities of var­ w ti iou? error, depend slightly on which mermage ia sent.. 
This Blight non-independence will not affect the analysie and one may as well The strings w, O are over 
{O, 1}, z and 2 are over {,0, 1, bkp} think of the edge character errors as independent. issued some 
messages of II, which are now un-substantiated since the incoming data required to determine them, has 
been erased or modhied. Observe that this second kind of inconsistency can occur even if p has made no 
decoding er­rors. We can now state our protocol. Simulation Protocol 2: If the transcript is consistent, 
trans­mit the data bits indicated by II; otherwise back up. To see that this completely specifies Z we 
need only ob­serve that AT(P, t) s tmod 2 and therefore a set of data bits is specified by II unless 
one the the incoming transcripts is shorter by 2 than AT(p, t)(the length of p s outgoing tran­scripts), 
in which case the transcript is inconsistent and p will choose to back up. 4 Mechanics of the Protocol 
In order to understand the protocol we will need to clas­ sify the events that can occur at a processor 
in any round, into several types. The function ACTION (P, t)which takes as argument a (processor, time) 
pair, will describe this clas­ sificat ion. Note that the classification is one we make for purpose of 
analysis: it is not one that the processors can determine while they axe running the protocol. Many of 
the interesting phenomena in our protocol arise already in the case of protocols on a bidirectional linear 
ar­ray, and can therefore be easily illustrated (figures 2, 3). In such an array each processor in each 
time step exchanges a codeword of x with each of its two neighbors. We first consider the possibility 
that a processor tree code error occurred at processor p at time t.In this case if p decided to back 
up, we set ACTION(P, t) = bx. On the other hand if p decided to transmit data of II, and if any of that 
data disagrees with the data that would indeed be transmitted in II at (p, t) on a noiseless network, 
then we set ACTION (P, t) = x. Events of type x are depicted in figure 2 at (6, 2), (4,5) and (10, 5); 
and in figure 3 at (11, 6). An event of type bx is depicted in figure 3 at (7, 4). Processors_ ~,!: ii:: 
 I ,,, ,., :!! T Time v Figure 2: The figure depicts the operation of Z when three errors have occurred. 
There is one pair of time-like errors. A tree-code failure is not the only reason for the trans­mission 
of incorrect data. It may also be that one of the neighbors of p transmitted wrong data in the preceding 
round and consequently, p transmitted erroneous data. If such an event occurs specifically, if there 
was no pro­cessor tree code error at (p, t)but nevertheless p transmit­ted erroneous data (which can 
only occur for the indicated reason), we say that p made a propagated error, and set ACTION (P, t) = 
y. A number of such errors are indicated in figures 2 and 3. Consider the position (6, 3) in figure 2. 
Since ACTION (6, 3) is not an x or bx, processor 6 decodes each incoming message correctly. Therefore 
it detects that the data it transmitted at the end of the last time step does not correspond to the preceding 
incoming data stream: thus, an inconsistency in its transcript. Hence processor 6 backs up one step. 
Now consider the position (7, 4). At this stage processor 7, too, observes an inconsistency in its transcript. 
The inconsis­tency arises because processor 7 has transmitted 3 messages of II to its neighbors even 
though it has received one mes­sage from 6. Thus, processor 7 s response string is not a prefix of the 
correct response string (which is of length 2), and therefore processor 7 now backs up one step. Both 
of these actions are denoted by b in figure 2. In general, we set ACTION(P, t) = b if (a) There is no 
tree code error at p at time t,and p backs up; (b) For at least one of p s out­neighbors q, there was 
a character in the outgoing transcript ~@ ,QJ(t 1) which was incorrect. (I.e. the character did not 
agree with that which would have been transmitted in a noiseless run of II.) The reader can now verify 
that figure 2 is a faithful representation of the execution of the protocol, for the given pattern of 
errors. Processors_ .. .. Time . . . . I Y ; . . . . . . . b Y . . ,,, . . . T-: : - bj .: ~,: , :. 
t Figure 3: This figure shows an instance of a spurious backup. If condition (a) but not (b) is satisfied, 
we set ACTION (p, t)= by. Consider figure 3. Here an erroneous backup, bx, occurs at position (7, 4), 
and causes a series of by backups. All of these backups are spurious, and erase the good simulation steps 
that occurred in the positions marked by a thick dot, . In this example those steps are promptly re-simulated 
in the unmarked rounds following the spurious backups. Observation 4.0.1 If a processor does not make 
a tree code error (in which case ACTION is either x or bx), does not back up (in which case ACTION is 
either b, by or bx), and is not transmitting data based on erroneous information (in 794 which case 
ACTION is y) then it is necessaril~ successfully simulating a round of II. In that case we say that ACTION 
= progress. We leave the figures blank in such progress positions. We now define the real progress RP(p, 
t),the key measure of the progress of our simulation. Unlike AT(p, t), it is not one that can be determined 
by the processors while they are running the protocol. The real progress RP (p, t ) is simply the number 
of steps of the original protocol II that have been correctly simulated by processor p at time t.Thus 
for example in figure 2, the real progress at position (4,5) is 3. By way of contrast AT(4, 5) =5. In 
figure 3, RP(5, 5) = 5, and RP (5,6) is 4. This decrease in the value of the real progress is due to 
the spurious back up which erased a step of meaningful computation. A consequence of the definitions 
is that RP@, t)< AT(p, t), 5 Analysis 5.1 Overview We will ability, prove the real our result progress 
by at sha owing that with high processor at a given prob­time t (measured in rounds during each of which 
a single tree code character is transmitted over each channel), is within a con­stant factor of t. There 
are two key components to our analysis. The first is to associate the delay at processor p at time t 
with a specific, time-like sequence of processor tree code errors in the history cone of (p, t). We say 
that (p, t) and (p , t ) are time-iike if a signal leaving processor p at time i! can reach processor 
p by time t (or vice versa): thus (p, t)and (p, t ) are always time-like, and (p, t)and (p, t + 1) are 
time-like if p and p are neighbors. If (p, t)and (p , t ) are not time-like we say they are space- We. 
A set {(pi, tl),. . (pm, t~)} are time-like or space-like if every two of them are. The history cone 
at (p, t)consists of all time-like predecessors of (p, t),and (p, t) itself. The second component of 
our analysis is to argue that with high probability, the number of processor tree code errors (henceforth 
abbreviated as T-errors) on any single time-like path is small. Since ~-errors are not independent of 
each other, we accomplish this objective by associating with any large time-like sequence of ~-errors 
a large number of nearly time-like processor character errors. Since char­acter errors are essentially 
independent, a Chernoff bound can then be established. These two components are summarized in the following 
two lemmas: Lemma 5.1.1 If for a processor p, RP(p, t) = t t?,then there is a time-like sequence of at 
least -!?/2 T-errors in the time history cone ofp at t. Lemma 5.1.2 Choose the dilation k of the Shannon 
code ~ to be clog(d + I)/C, for a sufficiently large constant c. For any ficed time-like sequence {(pi, 
i) : 1 ~ i ~ t}, the probability that there are more than ~ ~-errors in the time­ like sequence is less 
than ~2(d~l)), . Since the number of time-like sequences in a simulation of length t is at most (d+ 1)*N 
where d is the largest in-degree of any Processor, the probability of any time-like sequence having more 
than t/4 errors is at most (d+l) N/(2(d+l)) = N2-t and the main theorem 1. I follows if>] is run for 
twice as many rounds as II. The second component of the proof is accomplished by extending a method 
used in [26], In this pa,per, most of our efforts will be devoted to the first component: associating 
delay with a time-like sequence of tree-cods errors. 5.2 Simulation Delays and Tree Code Errors In this 
section we link simulation delays and time-like se­quences of tree code errors. Let X(p, t)denote the 
size of longest time-like sequence of x s and bx s in the history cone of (p, t).Our main task is to 
ahow: Proposition 5.2.1 For any processor p and any moment in time r, ~ ~ RP(p, ~) + X(p, ~) + B(p, ~) 
It will then be relatively straightforward to obtain lemma 5.1.1 from this proposition. Proofi We prove 
the proposition by induction on time ~. We start with ~ = O in which case all the terms of the inequality 
are O. For the induction we may assume that for time t< T + 1 and for any processor p in the network, 
t< RP(p, t) + X(p, t) + B(p, t).We now show ~+ls RP(p, ~+l)+X(p, ~+l) +B(p,7+ l). The proof is by case 
analysis according to the classification ACTION (P, T + 1) described in section 4. There are four easy 
cases which we deal with first. If ACT ION(P, T + 1) = progress, then RP(p, ~ + 1) = RP(p, ~) + 1. and 
the other quantities are unchanged from (p, ~) (except that X may increase); the induction follows. If 
ACTIOI@ I-+ 1) = x then X(P, r + 1) 2 X(P, T) + 1 and again, the (other quantities are unchanged from 
(p, T) the induction follows. Similarly if ACTION(P, T+ 1)= bthen B(p, ~+ 1)= B(p, ~)+ 1and the other 
quantities are unchanged from (p, ~) except that X might possibly increase; the induction follows. Finally 
if ACTION(P, r+l) = bx then both X(p, ~) and B(p, r) increase by 1 while RP(P, ~) may decreaae. However 
RP(p, r) can decrease by at most 1 so the induction follows. We now consider the two nontrivial cases, 
beginning with ACTION(P, T + 1) = y. Lemma 5,2.1 Let ACTION(D, t+l) = Y. !l hen the following two conditions 
hold. - ­ 1. For every processor q such that (q, p) 6 N, B(q, t) B(p, t). 2. There is a q such that 
(q, p) G N and RP(q, t) RP(p, t).  Consequently, there is a q such that (q, p) ~ N, and RP(q, t) + B(q, 
t) < Rp(p, t) + B(p, t) Proof: Since ACTION@, t+ 1) is neither an x nor a ~x, there was no tree code 
error at processor p at time t+,1. Hence for each incoming edge (q, p) 6 N, tO(g Pl = W(9J J. Therefore 
if (1) were violated, p would have backed up; while if (2) were violated, the data transmitted on each 
outgoing channel would have been correct. 0 For the q provided by the lemma we have: RP(q, ~) + 13(q, 
r) < RP(p, r) + B(P, T) = RP(p, ~ + 1) +B(p, ~+ 1)  Further since (q, p) 6 ~, X(q, ~) S X(p, ~ + 1). 
Conse­quently, RP(q, ~) + X(q, ~) + B(q, ~) < RP(p, ~+l)+X(p,7 +l)+B(p, ~+ 1) Applying the induction 
hypothesis at q at timer, we obtain: Which completes the induction in this case due to the strict inequality. 
The final case is that ACTION(P, 7+1) = by. Here B(P) -r) increases by 1 while RP (p, I-) decreases by 
1. Note that X(p, T) does not necessarily increase. However we can es­tablish: Lemma 5.2.2 Let ACTION(P, 
t+ 1) = by. Then there is a processor q such that (q, p) = ~ and RP(q, t) < RP(p, t)-1. Proofi The classification 
by implies that RP(P, t)= AT(P, t); and that AT (q, t) < AT (p, t). Because of the parity condi­tion 
on AT it follows that AT(q, t)< AT(P, t) 1, and since RP(q, t)< AT(q, t),the lemma follows. 0 Let q be 
as provided by the lemma, and consider two cases. In the first case suppose that B(q, T) < B(p, ~) + 
1. Then RP(q, ~) + X(q, T) + B(q, T) < RP(p, ~) + X(P,~ + 1) + B(P,7) Since RP(p, T) + B(p, T) = RP(p, 
~ + 1) + B(P,7 + 1), we find RP(q, T) + X(q, T) + B(q, T) < RP(P, T+l)+X(P, T+l)+B(P, T+ 1) Applying 
the inductive hypothesis at q at time T, we obtain: T< RP(zJ, T+l)+X(P, T+ l)+ B(P, T+l) which completes 
the induction for the subcase B(q, ~) s B(p, ~)+ 1. In the second case B(q, T) > B(p, T) + 1. As observed 
in the proof of the lemma 5.2.2, RP(p, t) = AT(P, t);therefore RP(P, T) = T 2B(p, T) At (q, 7-) we know 
only that RP(q,7) ~ T 2B(q, T) Subtracting we have: RP(p,7) + 2B(p, ~) ~ RP(q, ~) + 2B(q, ~), and applying 
the assumption B(q, T) > B(p, T) + 1, RP(p, T) + B(p, T) > RP(~, T) + B(q, ~) + 1. A~ in the previous 
case we now use the fact that RP(p, ~) + B(P, T) = RP(P,7 + 1) + B(P, T + 1), and find: RP(q, T) + X(q, 
T) + B(q, T) < RP(p, T+l)+X(p, T+l)+B(p, T+ 1) Which, applying the inductive hypothesis at q at time 
7, settles this last case of the induction for the proposition. l In order to obtain lemma 5.1.1 we observe: 
Corollary 5.2.3 For any processor (p, t), RP(p, t) ~ t 2X(p, t) Proofi Using the proposition to substitute 
for X we have: RP(p, t) t+ 2X(p, t) ~ t RP(p, t) -2B(p, t) As already noted, the latter quantity is 
always nonnegative. 0 The corollary implies lemma 5.1.1, the first of the two main results needed for 
theorem 1.1, because -?/2 = (t RP(p, t))/2 s X(p, t).  5.3 Example Figure 4 shows a large simulation 
and, by way of example, a critical path of tree code errors, which justifies the delays that have occurred 
in the protocol. Observe that in the four easy cases, the critical path leads upward, to the same processor 
at the previous moment; while in the two cases y and by the critical path leads up to a neighboring processor 
at the previous moment. Processors ~ I1 Figure 4: A time-like sequence of 6 T-errors that account for 
the 12 timesteps of lost computation. Errors of type x are indicated by x, errors of type bx are indicated 
by +, and the shaded boxes indicate backup steps of type b or by.  5.4 Tree Code Errors and Character 
Errors In this section we sketch the proof of the second main lemma, 5.1.2. The difficulty in establishing 
this lemma is that T­errors are not independent. Our proof is an extension of a method used in [26]. 
Proof of lemma 5.1.2. We begin by taking the union bound over the (~f~) < 2t different ways in which 
t/4 tree code errors could appear on this time-like path. Now fix an error pattern on the path. We need 
the following observa­tion: Observation 5.4.1 By suitable choice of the constant c in lemma 5.1.2, the 
probability of occurrence of any set of n processor character errors can be bounded by exp( c n) for 
arbitrarily large c{. ProoE Although, for reasons noted earlier, character errors are not exactly independent, 
nevertheless the conditional probability of any character error, given any information about the occurrence 
of other chrwacter errors, is bounded by dexp( Ck) ~ exp( c ). 0 l?ix any processor p, and suppose the 
fixed error pattern assigns 1 specific r-errors to that processor. We will show that the probability 
of this occurring is exponentially small in 1; moreover, by lemma 5.4.1, this will be true conditional 
on any events at the other processors. The lemma 5.1.2 will follow. With reference to the argument in 
[26]. The 1 tree code errors must be contained within the union of the error in­tervals at p. There is 
a disjoint collection of error intervals covering at least 1/2 of the tree code errors. If the union 
of these disjoint intervals is of length m, the probability of their occurrence is exponentially small 
in m. If m is sub­stantially larger than 1/2 there are many ways to select such a set of intervals, but 
this union bound is dominated by the exp(-fl(m)) probability of occurrence of any particular set of intervals. 
0 6 Data Broadcast on a Chain of Processors The protocol described above is baaed on tree codes. As mentioned 
earlier, significant difficulties remain concerning the computational feasibility of this protocol. In 
this section we provide a computationally efficient pro­ tocol for a special case of the problem. We 
study the sim­ plest of network topologies, the chain, and the simplest of network tasks, the broadcast 
of data. Let the first processor in a chain of N processors have B bits to be communicated to all the 
other processors. In a noiseless setting, by pipelin­ ing, this task can be carried out in time N + B 
 2. The naive strategy of sending the block in error correct­ ing code would take @(B. N) time since 
each intermediate processor needs to receive the entire codeword before it can proceed any further. One 
could divide the block of size B into smaller blocks of size b. The running time would then reduce to 
O (N . b + B). Thk reduction in running time, however, comes at the cost of increasing the failure proba­ 
bility from roughly e-n(B) to e-n(b). Moreover even if we settle for a constant failure probability, 
the multiplicative time overhead cannot be reduced below log N in thk way. 6.1 Protocol In this section 
we describe a simple protocol that achieves speed without compromising the reliability of execution. 
The basic idea here is to encode the message block in error cor­ recting code; then divide the codeword 
into small chunks, and send each chunk to the end of the chain by successively transmitting it across 
subchains of the chain, each time us­ ing the protocol recursively. Finally after all the chunks have 
reached the last processor, they are combined and the message is obtained by using the decoding algorithm 
for the error correcting code. Observe that the computation in this method is domi­ nated by the final 
decoding of the error-correcting code, and thus the method is computationally essentially as efficient 
as any two-processor data transmission, Let &#38;f = rna.x{B, N}. Without loss of generality the number 
of processors and the message length are both M. The protocol PM is described by the following four 
steps. 1. Encode the M bits in a 25~o error correcting code &#38;. This results in a constant factor 
blow-up in the mes­sage size. Let a >4 be such that IE(M)I = cdl. 2. Partition &#38;(M) into chunks 
of bits, each of size m = log2 M. There are dbf/m such chunks. 3. Use protocol Pm to transfer each chunk 
of m bits across a subchain of length m. At the end of the subchain, use Pm again to send the chunk to 
the end of the next subchain of length m; and so on until the end of the chain. Pipeline the chunks one 
behincl the other,  4. Finally when every chunk has arrived at the end of the chain, invert the error-correcting 
code using the chunks of data provided by the recursive protocols. 6.2 Analysis Theorem 6.1 A B bit 
message can be tmnsmitted across an array of N processors interconnected by binary symmetric channels 
in time O((B + N). eo( og B+ N)) ). The probability of an error occurring in transmission is at most 
e-n(N+B). Proofi The base case for the analysis, constant M, is trivial and requires only the appropriate 
selection of constants. In the remainder of the proof we assume that M is large enough for asymptotic 
inequalities to hold. There are two issues to be resolved: the running time and the error probability. 
We consider the running time first: The total running time haa two components. The first is the processor 
broadcast time, bM. This is the length of time each processor is occupied in broadcasting bits. (Thus 
in pipelining of B bits in a noiseless chain this would be B.) The second is pipeline latency, ~M. This 
is the delay between the moment the first bit is transmitted at the first processor, and when the first 
bit is received at the last processor. (Thus in pipelining in a noiseless chain of length N this would 
be N 1.) The runtime of our protocol iS bivf -t ~M. It is convenient now to define the function L*(M) 
to be the number of applications of the function log2 necessary be­fore M is brought below some constant 
threshold. Observe that L (M) is within a constant factor of log* (M). We show by induction that b&#38;i 
5 MaL* w; and (for any @ > 0), iM < M(a +~) ~. For the first of these, observe that bM = b~aM/m = ma 
* M-laM/m 0= MCYL*M. For the second, observe that iM = (b~ +l~)M/m 1~< (b~ +&#38;) M/m = (mcrL*~ + m(a 
+ ~) L*m)M/m = M(aL*M-l + (a+/3)L*M-1) < M(a+@L*M. We next show by induction that the error probability 
qM is bounded by ezp( M). By induction the probability of any of the recursive protocols erring is exp( 
m), and by taking the union bound over the number olf subchains, we can bound the probability of a chunk 
arriving incorrectly at the last processor by ezp(-m)M/m. The protocol PM can err only if at least a 
quarter of the aM/nl chunks arrive incorrectly at the l~t processor, so by the Ghernoff bound, qM ~ [(4q~)l 
4(4(1 q~)/3)s/4]aM/~ ~f f/4 (43)314 )~M)mM/m < qm t j < (k -1)/6, it is easily shown that the largest 
term in the summation is that for which j = 1. Hence ~ e-aM/4(4(1/3)3i4)eM/~ < e-~. ~(k, t) ~ (t-k +l)d 
(l-c$) -k :=: . () Now pzwameterize t(k) = (k l)v/6+ 1. 7 Lower bound Corollary 7.0.2 For any T <1, 
h,+.+m f (k, t(k)) = O. We demonstrate the following lower bound on the problem of broadcasting one bit 
from start to end of a chain of length Proof: The Chernoff bound shows that n, in which each link is 
a binary symmetric channel, erring with probability (1 6)/2 on each transmission: CF-l(l J)t(k)-k $~~1 
<e-2(k- )a(l-T)2/T. () Theorem 7.1 Suppose that each input bit 0,1 is equallg ­likeiy. Then for an~ 
~ <1, for suficientlg la~ge values of Thus n, any bit computed at processor n at time (n l)~fc$ + 1 
is f (k, t) S (k -1)(-Y -6)e-2(k-1)6(1-7)2 jT. equal to the input bit with probabiiit~ at most 1/2+ o(1). 
This converges to O in the limit k + 00. 0 The argument is through information theory. We will show that 
the information available at processor n regardhxg the input value, at the time described above, tends 
to O in 8 Discussion n. The implication for error probability follows by standard The question of explicitly 
constructing tree codes is a beauti­ arguments. ful problem which remains unresolved. This question may 
be We begin with a lemma regamling information. This is formalized by asking for an algorithm which in 
time poly(n) a slightly modified version of a lemma of Pippenger [18]. names the letter of S on any specified 
edge of a tree of depth Let X be random variable which we interpret as data . Let Y be a random vaxiable 
which depends on X through n. In addition, it would be highly desirable to make the some arbitrary channel. 
We will think of Y as referring to computations associated with the protocol for theorem 1.1, the entire 
sequence of receptions at some processor k of the effective. The max-likelihood criterion we employ for 
de­ chain, up through time t. Let Z = (21, 22) be a random codhg tree codes requires a computational 
overhead that is variable which depends on Y through a binary symmetric exponential in the communication 
complexity. channel which errs with probability (1 -6)/2. We think of Z as the entire sequence of receptions 
at processor k + 1 of the chain, up through time t+1. 22 denotes just the last blt References received 
over the channel, at time t+1, while 21 denotes the entire sequence of previous receptions across the 
channel. [1] R. Aleliunae. Randomized parallel communication. In Pro­ ceedings of the .$@posium on the 
Principles of Distributed Systems., 1982, pages 60 72. Lemma 7.0.1 I(X; Z) < (1 6)1(X; 21)+ 61(X; Y). 
[2] T. M. Cover and J. A. Thomaa. Elements of Information Proof: Let H be a binary random variable which 
is 1 w. Theory. Wiley, 1991. prob. 1 cf. Let R be a binary random variable which is 1 w. prob. 1/2. 
21 is equivalent to a binary vaxiable defined [3] R. L. Dobrushin and S. I. Ortyukov. Lower bound for 
the re­ as follows: If H = 1 then it is equaf to R, otherwise it is dundancy of self-correcting arrangements 
of unreliable func­tional elements. Prob. Inf. Trans., 13:59 65, 1977. equal to the bit sent by Y. In 
the former case 1(X; ZIH = 1) = 1(X; 21); in the [4] R. L. Dobrushin and S. I. Ortyukov. Upper bound 
for the re­latter case we use just the data processing lemma, to obtain dundancy of self-correcting arrangements 
of unreliable func-I(X; Zlkf = o) < I(X; Y). tional elements. Prob. Inf. Trans., 13:203-218, 1977. 1(X; 
z) < 1(X; ZH) < (1 d) I(x; zl) + JI(X; Y). 0 [5] W. Evans and L. J. Schulman. Signal propagation, with 
ap- The inequality obtained above implies that the informa­plication to a lower bound on the depth of 
noisy formulas. In tion at processor k at time t can be bounded by the function Proceedings of the 34th 
Annual Symposium on Foundationsf(k, t) defined in the following way: of Computer Science, pages 594-603, 
1993. f(o, t)= lvt. [6] T. Feder. Reliable computation by networks in the pres­ence of noise. IEEE Tbarwactions 
on Information Theorg, j(k, t+ 1)= (1 d)f(k,t) +df(k l,t)o 35(3):569-571, May 1989. This function is: 
 [7] P. G&#38;cs. Reliable computation with cellular automata. J. t (k 1) Computer and System Sciences, 
32:15-78, 1986. f(k, t)=6 ~ dk- (l d) -~-(k- );:; . [8] P. G&#38;cs and J. Reif. A simple three-dimensional 
real-time J=l () reliable cellular array. J. Computer and ,!@stem Sciences, 36:125-147, 1988. Each term 
in this summation is the probability that, in a [9] A. Gal. Lower bounds for the complexity of reliable 
boolean Bernoulli process where Heads come up with probability d, circuits with noisy gates. In Proceedings 
of the 3Znd Annual exactly k 1 Heads came up in the first t j trials. For Symposium on Foundations of 
Computer Science, pages 594-601, 1991. [10] R. G. Gallager. Finding parity in a simple broadcast net­work. 
IEEE Trans. Inform. Theory, 34(2): 176-180, March 1988. [11] Lipton and Sedgewick. Lower bounds for VLSI. 
In Proceed­ings of the 13th Annual Symposium on Theory of Comput­ing, pages 300-307, 1981. [12] L. Lowiaz. 
Communication complexity: A survey. In Ko­rde et al, editor, Algorithms and Combinatom cs. Springer-Verlag, 
1990. [13] M. Luby. On the paralIei complexity of symmetric connec­tion networks. U. Toronto CS Dept. 
Tech, Report # 214, 1988. [14] C. Martel, R. Subramonian, and A. Park. Asynchronous PRAMs are (almost) 
ss good as Synchronous PRAMe. In Proceedings of the 31st Annual Symposium on Foundations of Computer 
Science, pages 590 599, 1990. [15] N. Nisan Personal Communication. [16] C. H. Papadimitriou and M. Sipser. 
Communication com­plexity. In Proceedings of the ldth Annual S~mposium on Theory of Computing, pages 
196-200, 1982, [17] N. Pippenger. On networks of noisy gates. In Proceedings of the 26th Annual Symposium 
on Foundations of Computer Science, pages 30-36, 1985. [18] N. Pippenger. Reliable computation by formulas 
in the pres­ence of noise. IEEE Transactions on Information Theory, 34(2):194 197, March 1988. [19] N. 
Pippenger. Invariance of complexity measures for net­works with unreliable gates. J. A CJ4, 36:531 539, 
1989. [20] N. Pippenger, G. D. Stamoulis, and J. N. Tsitsiklis. On a lower bound for the redundancy of 
reliable networks with noisy gates. IEEE lkansactions on Information Theory, 37(3):639-643, 1991. [21] 
A,G. Ranade, A Simpler Analysis of the Karp-Zhang Par­allel Branch-and-Bound Method. University of California, 
TR, UCB\CSD 90/586, 1990. [22j A.G. Ranade, How to emulate shared memory. JCSS, 42(3):307-326, June, 
1991. [23] R. Reischuk and B. Schmeltz, Reliable computation with noisy circuits and decision trees 
a general n log n lower bound. In Proceedings of the 32nd Annual Symposium on Foundations of Computer 
Science, pages 602-611, 1991. [24] L. J. Schulman. Communication in the Presence of Noise. PhD thesis, 
Massachusetts Institute of Technology, 1992. [25] L. J. Schulman. Communication on noisy channels: A 
cod­ing theorem for computation. In Proceedings of the 33rd Annual Symposium on Foundations of Computer 
Science, pages 724-733, 1992. [26] L. J. Schulman. Deterministic coding for interactive com­munication. 
In Proceedings of the 25th Annual Symposium on Theory of Computing, pages 747 756, 1993. [27] C. E. Shannon. 
A mathematical theory of communication. Bell S~stem Tech. J., 27:379-423; 623-656, 1948. [28] M. C. Taylor. 
Reliable information storage in memories de­signed from unreliable components. Bell System Tech. J., 
47(10):2299-2337, 1968. [29] C. D. Thompson. Area-time complexity for VLSI. In Pro­ceedings of the 1 
Ith Annual Symposium on Theory of Com­puting, pages 81-88, 1979. [30] A. L. Toom. Nonergodic multidimensional 
systems of au­tomata. Problems Inform. Transmission, 10:239 246, 1974. [31] A. L. Toom. Stable and attractive 
trajectories in multicom­ponent systems. In R. L. Dobrushin, editor, Irfulticomponent Systems, volume 
6 of Adv. Pro bab., pages 549 575. Dekker, 1980. [32] A. L. Toom. Estimates for the measure, describing 
the behavior of stochastic systems with local interaction. In Kryukov Dobruehin and Toom, editors, Interactive 
Markov Processes and Their Application to the Mathematical Mod­eUing of Biological Systems. Acad. Sci. 
US!3R, 1982. [33] B. S. Tsirel son. Reliable information storage in a system of locally interacting unreliable 
elements. In Interacting Markov Processes in Biologg, volume 653 of Lecture Notes in Mathematics. Springer-Verlag, 
1978. [34] J. von Neumann. Probabilistic logics and the synthesis of re­liable organisms from unreliable 
components. In C. E. Shan­non and J. McCarthy, editore, Automata S,tudies. Princeton University Press, 
1956. [35] A. C. Yao. Some complexity questione related to distributive computing. In Proceedings of 
the 11th Annual Symposium on Theor~ of Computing, pages 209 213, 1979. [36] A. C. Yao. The entropic limitations 
on VL131 computations. In Proceedings of the 13th Annual S~mposium on Theor~ of Computing, pagee 308-311, 
1981.   
			