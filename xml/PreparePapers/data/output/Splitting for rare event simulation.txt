
 Proceedings of the ed. J. M. Charnes, 1996 Winter D. J. Morrice, Simulation Conference D. T. Brunner, 
and J. J. Swain SPLITTING FOR RARE EVENT SIMULATION: ANALYSIS OF SIMPLE CASES Paul Glasserman Columbia 
University New York, NY 10027 Philip Heidelberger IBM T.J. Watson Research Center Yorktown Heights, NY 
10598 Perwez Shahabuddin Columbia University New York, NY 10027 Tim Zajic &#38; Columbia University New 
York, NY 10027 IBM T.J. Watson Research Center Yorktown Heights, NY 10598 ABSTRACT makes the event of 
interest occur frequently is not enough; how it is made to happen more frequently An approach to rare 
event simulation uses the tech-is also very important. For example, an arbitrary nique of splitting. 
The basic idea is to split sample change of measure that makes the rare event happen paths of the stochastic 
process into multiple copies more frequently may give an estimator with an infi­when they approach closer 
to the rare set; this in-nite variance. Thus the main problem in importance creases the overall number 
of hits to the rare set for sampling is to come up with an appropriate change a given amount of simulation 
time. This paper ana-of measure for the rare event simulation problem in lyzes the bias and efficiency 
of some simple cases of hand. This may be difficult or almost impossible for this method. complicated 
models. Hence, even though importance sampling works very well for a large class of stochas­ tic models, 
the scope of application of importance 1 INTRODUCTION sampling is limited to systems with nice structure. 
Estimations of the small probabilities of rare events This paper deals with an alternate approach to 
rare are required in the design and operation of many en-event simulation that uses the simulation technique 
gineering systems. Consider the case of a telecommu-of splitting (see, e.g., Hammersley and Handscomb 
nications network. It is customary to model such sys-1965). In standard simulation, the stochastic process 
tems as a network of queues, with each queue having being simulated, spends a lot of time in a region 
of a buffer of finite capacity. Information packets that the state space which is far away from the rare 
set arrive to a queue when its buffer is full are lost. The of interest, i.e, from where the chance of 
it entering rare event of interest may be the event of a packet be-the rare set is extremely low. In 
splitting a region ing lost. Current standards stipulate that the proba-of the state space that is closer 
to the rare set is bility of packet loss should not exceed 10-9. Or in a defined. Each time the process 
reaches this region, reliability model of a space craft computer, we may be from the far away region, 
many identical copies of interested in estimating the probability of the event this process are generated. 
This way we get more that the system fails before the mission completion. instances of the stochastic 
process spending time in Naturally, one would want this to be extremely low. a region where the rare 
event is more likely to oc-The main problem with using standard simulation to cur. The boundary between 
the far away region and estimate such small probabilities is that a large num-the closer region is called 
a threshold. The above ber of events have to be simulated in the model be-described a case with one-threshold; 
one can easily fore any samples of the rare event may occur. Hence extend it to the case where we have 
multiple thresh­special simulation techniques are needed to make the olds. This approach to rare event 
simulation was in­events of interest occur more frequently. troduced in Kahn and Harris (1951) and used 
later in Importance sampling is a technique that has been Bayes (1970) and Hopmans and Kleijnen(1979). 
Re­widely used for this purpose. The reader is referred cently it was revisited in a significant way 
by Vil16n­to Heidelberger (1995) and Shahabuddin (1995) for Altamirano and Vil16n-Altamirano (1991), 
Vil16n­some surveys. In importance sampling, the stochas-Altamirano et al. (1994) and Vil16n-Altamirano 
and tic model is simulated with a new probability dy-Vill&#38;-Altamirano (1994), who used it for estimat­namics 
(called a change of measure), that makes the ing the probability of rare events in computer and events 
of interest occur more frequently. The sample communication systems. They called their version of value 
is then adjusted to make the final estimate unbi-this approach RESTART. A software package called ased. 
However, choosing any change of measure that ASTRO (Vil16n-Altamirano and Vil16n-Altamirano 302 1994) 
was created that implements their method. They also did some approximate efficiency analysis that gave 
some insights into threshold selection and number of split paths generated at each threshold. But aformal 
andthorough analysis was lacking. Glasserman, Heidelberger, Shahabuddin, Zajic (1996) (henceforth referred 
teas GHSZ) describe a unifying class of models and implementation condi­ tions under which this type 
of method is provably effective and even optimal (in an asymptotic sense) for rare event simulation. 
The theory of branching processes (see, e.g., Harris 1989) was used to derive the unbiasedness and efficiency 
results. Experimental results supporting the theoretical analysis and explor­ing the robustness of the 
splitting method, are also reported in GHSZ. In this paper we introduce and de­rive some biasedness and 
efficiency results that sup­plement those in that paper. We begin with a simple setting, and give conditions 
under which the splitting method is optimal. We then give reasons why devi­ations from this simple setting 
result in difficulties. Some of these have been handled in GHSZ, whereas others are currently being investigated. 
Some analyt­ical results on the optimal selection of thresholds are introduced next. Finally we give 
an analysis of the bias introduced in one impIementation of this method that truncates sample paths to 
save simulation effort. 2 A SIMPLE SETTING Consider the problem of estimating -Y == P(A), think­ingof 
A as a rare event. Let A = A~ o A~_l . ..0 Al be a nested sequence of events which we think of as intermediate 
thresholds. Let pl = P(AI) and 1%+1 = P(Ai+IIAi), i = I,.. .)k 1; then We think of k increasing to infinity 
and y -+ O (this would happen, for example, if pi = p for all i, where p is some fixed constant between 
O and 1). To motivate the above setting, consider a single server queueing system with a finite buffer 
1?. Define the state of the system to be the number of jobs in the queue. The problem may be to estimate 
the prob­ability y that starting from state O, the system reaches state B before visiting O. We can think 
of this event as the event A. Estimating probabilities of this type are crucial to the simulation based 
estimation of per­formance measures like the steady state probability of packet loss (see, e.g., Heidelberger 
1995). Clearly, if the overall arrival rate is smaller than the overall service rate (which is a requirement 
for the stability of the queue), and B is large, then the event A is a rare event. Suppose now that we 
place k 1 inter­mediate thresholds between O and B (with B being the kth threshold). Let Ai be the event 
that start­ing from state O, the number of jobs in the system reaches threshold i before reaching O. 
Then clearly Ai+l c Ai and we have an example of the setting mentioned in the previous paragraph. Suppose 
that for each i we can generate ni Bernoulli random variables with parameter pi, all independent of each 
other. These are the building blocks of a splitting estimator in this simplle setting. From each successful 
Bernoulli outcome at stage i, we generate ni+l stage-(i + 1) Bernoulli. Thus, at the first stage we have 
Bernoulli 11, lz, . . ..lnl. the jth of these, if successful, spawns l~l,lja,...,lj~,, and so on. The 
estimator is . nl nk Ik= 1 .nk,x zli, li,~. nl. . Zl=l i~=l It is easy to show that Ik is an unbiased 
estimator. By conditioning on ~k we mean conditioning on the outcomes of all Bernoulli up to stage k. 
Then =E l.nk ,5  ! l~l li,~k-,nl. . $1=1 ~k-1=1( ~ E(l,,,.,,l~k-,). i,=l ) Doing this iteratively we 
get that ~(~k) = PI . . .pk = yk . Returning to the simple queueing example intro­duced above, a Bernoulli 
random variable might be the indicator that a simulated process reaches the next threshold from the current 
one, witlhout visit­ing state O. In particular, the pi should be consid­ered unknown, so the ni+l Bernoulli 
from each of the successful outcomes at sta~e i would typically be generated implicitly by simu&#38;ing 
n~+l ~s~mpl&#38; of the underlying process starting from stage i, until it either hits threshold i + 
1, or it hits O (see Figure 1). If a sample path hits threshold i + 1 before hitting O, then the corresponding 
Bernoulli random variable is set to 1: else it is set to zero. Of course, since the Bernoulli are all 
independent, the queuing process must satisfy the assumption that the dynam~c~ of the process after it 
hits the it h threshold, is independent of the past and depends only on i. A simple example where this 
is true is the M/M/l/N queue. Proof. The three cases are related to the convergence of the sum on the 
right side of (1). By the root test, Threshold 3 the series co x~=, (Pl~l) l pj  (Pjnj) (3) Threshold 
2 Threshold 1 Figure 1: Splitting with Three Thresholds and Two Split Subpaths at Each Intermediate Threshold. 
We now calculate the variance of this estimator: PI  PkPk+l(l Pk+l) = P:+lu: + 721 ...n~+l This recurrence 
relation can besolved to get which can also be written as k l pj a:=(pl. . (1) p~)2~(P1n1) . ..(Pjn3) 
 The next result examines the behavior of this variance as k increases. u;=~((pl . . .pk)2); (2) (ii) 
if co < lim inf~-m ~~=1 log(n,p~), u; =~(k(pl. ..pk)2); (iii) if lim pk+,&#38; Pk < 1 and lim infj~m 
$~~=110g(niPi) < Q, (p, -. .pk)2 = O((T:). converges if 1/j l pj lim sup j+CO [ (Plnl) (pjnj) 1 < 1; 
i.e., if 1 lim sup ~ log(l pj) + ~log <0. j~w .7 ( n~p$)] [ i=l The condition in (i) is equivalent to 
which ensures convergence of (3) and proves (2). The reverse inequality in (iii) similarly ensures divergence 
of (3). For (ii), notice that $j =0( ) (P,n;:!(pjnj) if limsup(l -pj)fi (+) < , j4cG i=l which holds 
under the condition in (ii). 0 The conditions in this lemma simplify in the im­portant special case that 
the pi approach some limit p and all ni equal some n for sufficiently large i. In this setting, the three 
cases in the lemma can be replaced with np > 1, np = 1, and np < 1. The corresponding results for this 
special cases may be found in GHSZ. In case (i) of the lemma, the second moment of the estimator is also 
O((pl . . .pk)z), because the first moment is pl . . .pk. Nonnegativity of variance makes this the best 
possible rate of decrease for the sec­ond moment. In contrast, straightforward simulation (corresponding 
to a single Bernoulli with parameter .p~ ) hasvariancePi (Pl pk)[l -(PI -pk)] = o(Pl pk) per replication 
and a second moment of the same or­der. We now supplement results for the variance with an assessment 
of the computational effort. We as­sume, for simplicity, that the work per sample is con­st ant across 
stages. (In many cases this may not be true. For example, in many queueing models the expected cost of 
simulating a trial from threshold i grows proportionally with i. This is because, with positive probability 
bounded away from zero, the sys­tem never reaches threshold i+ 1 and therefore many trials consist of 
simulating the queue until it empties again. However, these other cases can be handled similarly and 
lead to similar conclusions. ) Then the expected work is proportional to the expected num­ber of samples, 
which is nl + (nlp+z + ...+ (nlpl .. .nk-lpk l)nk y; ~:=1piw+l.  ~1  For the expected work we have: 
Lemma 2 (i) If lim sup.j-m $ Z;=l Mwi+l) > 0, the ezpected work per run grows exponentially in k. (22) 
If~~l log(pini+l) < m, then the expected work per run is O(k). (iii) If lim Supj+m $ ~~zl log(pi%+l) 
<0, then the ezpected work per run is O(l). Proof. For case (i), note that . k 1 so a positive limsup 
for this expression indicates expo­nent ial growth of expected work. The expected work is O(k) if converges, 
and a sufficient condition for this is the condition in case (ii) above. The condition in case (iii) 
above ensures that the series  Fik+l j=O i=l converges, by the root test. 0 As in Lemma 1, the conditions 
here can be replaced with np>,=,or<1inthecaseofpi ~pand fixed .i = n. The corresponding results for these 
special cases may also be found in GHSZ. The work-normalized variance, balancing compu­tational effort 
and estimator variance, is the product of the variance and the expected work per run; see Glynn and Whitt 
(1992) for full justification of this criterion. Combining Lemmas 1 and 2 yields a con­ dition for optimal 
splitting: Theorem 1 If j co < lim inf~ 10g(p~.i) ial and ~log(pini+~) < CO, i=l then Ik is asymptotically 
eficient in the sense that logO(k2(pl ~ Pk)2) = ~. $n&#38; 10, EII~] We interpret this result to mean 
that splitting is most effective when ni % l/p~. GHSZ discuss the use of a random number of splits in 
order to get the expected number of subpaths equal to I/pi when pi is not the reciprocal of an integer. 
The analysis above is based on a very simple model of splitting in which the success probabilities pi 
are constant at each threshold, regardless of what may have happened at previous thresholds. Consider 
esti­mating the probability that a Markov chain reaches some rare set before returning to its initial 
state. We label the initial state O and assume it is recurrent. Imagine introducing intermediate thresholds 
in the state space of the Markov chain and splitting each path that reaches a threshold into some number 
of subpaths. In general, the probability that the chain will reach the ith threshold before O, given 
that it has reached the (i l)th threshold before O, will de­pend on the state of the chain when it reached 
the (i l)th threshold. The assumption of constant pi would hold if, say, there were just one state through 
which the (i l)th threshold could be reached; but, more typically, pi would vary depending on the entry 
state. The case where we have a fixed and finite number of entry states into each threshold, and the 
probabil­ity dynamics of the process is homogeneous (in some limiting sense) with respect to the thresholds, 
is fur­ther analyzed in GHSZ. To get a sense of the pos­sible impact of the variability of the pi s in 
a more general setting (i.e., uncountably infinite number of entry states into thresholds), we consider 
a simple two-threshold problem. Our objective is to estimate Y = P1P2 where, now, PZ = E~2], with jz 
stochas­ tic. The mechanism we have in mind is this: each path has probability pl of reaching the first 
thresh­ old; upon reaching that threshold, its ofispring are randomly assigned a sample of fi2 as their 
common second-stage success probability y. This accurately rep­ resents the Markovian setting described 
albove. The estimator is 306 Glssserman with each lil N Bernoulli(pl ) and each ~~, =1 lili, conditionally 
Binomial(n2, ~z), independent for dif­ferent il. Let N2 = ~~,z=l L,i, for some il. Then Var[fV2] = n2p2(l 
 P2) + (n; n2)Var@2]. Proceeding as we did for (1), we get Var[lz] = &#38; {n~p~n~p~(l p,) + n~p~Var[Nz] 
} . This becomes P~Pl(l PI) + P1P2(1 P2) + Pl(nl n2)VarE21 nl nlnz nln~ The last term gives the effect 
of a random j52 com­ pared with a fixed p2. To get a sense of its im­ pact, divide through by 72 = pfp~, 
and suppose that ni x I/pi, i = 1, 2. The contribution of the first two terms is then 0(1) whereas the 
new variability y term contributes 0(n~Var~2]) = 0( Var~2]/p~). This simple observation has important 
implications for the effectiveness of multithreshold splitting proce­dures: splitting will be most effective 
if there is little variability in the success probability at each thresh­old. This further suggests (at 
least heuristically) that the thresholds should be chosen in a way that is con­sistent with the most 
likely path to a rare set. For then each subpath will draw a success probability close to that for the 
most likely path, resulting in little variation across subpaths. Understanding the large deviations behavior 
of a rare event may there­fore be useful in designing a splitting procedure. 3 OPTIMAL PARTITIONS We 
now return to the simple setting from the start of Section 2. In particular, the pi are constant at each 
threshold and we want to estimate ~ ~ ~k = pl . . . pk, with pl = P(A1) and pi = p(Ai lAi -1), continuing 
to think of k a cm and yk ~ O. We consider the problem of choosing the intermediate events A., Al, . 
. . . Ak _ 1 and make two observations: choosing these events so that the pi converge aa i ~ m has an 
asymptotic op­timality property, and there is a connection between being able to choose the thresholds 
so that the pi con­verge and being able to analyze the large deviations behavior of a rare event. We 
begin by examining the optimal choice of Pi, ..., pk for fixed ~. Based on the analysis in Sec­tion 2, 
we restrict attention to the case ni = l/pi, ignoring the integrality constraint on the ni. In this case, 
the variance becomes k (7: = 72X(1 pj), j=l et al. and the expected work per run becomes Our objective 
is then to minimize k l pi 9(Pl, ..-,Pk) ~ Pj i ,j subject to pl . . . pk = 7. Rewriting the constraint 
and appending it with a Lagrange multiplier yields %=+ (zlOgpj The first-order conditions -~ $(l-pj)-~~+~=O, 
i=l,..., k, Pi P,Pi j=,j=~ and ~~=1 log pj logy = O, are solved by taking pi = p z 71fk and A = k/p. 
Moreover, the objective g is convex because it is a sum of terms (1 pi)/pj, each of which is convex 
in (pi, pj ) (or simply pi in case j = i). Thus, it is optimal to make the pi equal. It is now a simple 
matter to conclude that parti­tioning so that the pi converge is asymptotically op­timal (at least among 
schemes with ni = l/pi). For each k let q[k), . . . . qk k) be any probabilities multiply­ing toy~. We 
claim that ifpk* pask+ co,then dpl, . ..>pk) lim sup k, (k) ~1 ~~OJ g(q~ ,...,qk ) In light of the optimization 
carried out above, 9(Y,Yk,~ ...)Y; k)<9(4! ..J)) for all k. In addition, we now argue that (4) whenever 
the pi converge. To see this, notice that Also, since ~~=1 Pj = Yk, 7kIlk + p, so I/k ,..., &#38;] = 
(1 7;/k)+ + Q, ig(% P-Yk 307 as k + co, which verifies (4). We conclude that choosing the pi so that 
they converge to a limit is asymptotically as effective (as k ~ 00) as using the optimal partition at 
each k. What does choosing the pk to converge entail for the sets Ak ? We now point out that the availability 
of a convergent pk sequence is related to the Ak satisfy­ing a limit theorem of the large deviations 
type. More specifically, if the pk converge then the P(A~ ) have a logarithmic limit; and if the ~(Ak 
) have an asymp­totically exponential decay, then the pk converge. For ifpk -pthen And if P(A~) -Ce- 
k, for some a >0, as k ~ co through integer values, then P(Ak) _O k=P(Ak_l) +e This gives another sense 
in which knowing something about the large deviations behavior of a rare event could be useful in designing 
a splitting procedure. Knowing the large deviations behavior should be use­ful in setting thresholds 
for which the resulting pk converge. 4 TRUNCATION BIAS As mentioned before, in many queueing models the 
expected cost of simulating a trial from threshold i grows proportionally with i. This is because, with 
positive probability bounded away from zero, the sys­ tem never reaches threshold i + 1 and therefore 
many trials consist of simulating the queue until it emp­ ties again. As such unsuccessful trials do 
not con­ tribute positive weight to the estimation of (k, it seems wasteful to devote significant computing 
re­ sources to them. Therefore, it is desirable to throw away trials that have dropped many thresholds 
from the starting threshold and thus are very unlikely to reach the next highest threshold. However, 
doing so introduces some bias in the estimator. In this section we analyze this truncation bias for a 
simple exam­ ple, which should nevertheless yield insight into more complex situations. We assume there 
is a truncation threshold d. If a trial started at threshold i where i < d, then we simulate the sample 
path the same way as in the case without truncation. If a trial started from threshold i, where i > d, 
ever drops to threshold (i d), that trial is counted as a failure and discarded. We analyze this bias 
for the simplest possible queueing system, the M/M/l queue. We let A and p denote the arrival and service 
rates, respectively, and define p = A/p < 1. We assume ~ + p = 1. The embedded discrete time Markov chain 
is a random walk with increments that take on the value +1 with probability J and -1 with probability 
p. In this case we let the thresholds correspond to queue sizes of 1, 2, . . .k. To estimate the bias 
we first need to calculate pi, which is the probability that the queue length ever reaches (i+ 1) before 
emptying, given that the initial queue length is i. Such probabilities are known from analysis of the 
gambler s ruin problem; see pages 344-348 of Feller (1968). More generally, if rj : F{hit O before n! 
start at j}, then Specializing (5) to j = i and n = (i+ 1), we have pi = p[l p*]/[1 p*+l]. Now let 
pj denote the probability of reaching threshold i+ 1 before threshold i d, given an initial queue length 
of i. Splitting using truncation yields an unbiased estimate of y; = ~~~11 pi. Note that p; = pi for 
O< i < d. The formula for p{ for i > d is determined from the right-hand-side of (5) with j = d and n 
= (j+ 1): p; = p[l pd]/[l pd+l]. We wish to compare ?j to -W and in particular wish to know how d = dk 
should be chosen so that ? i/Tk -+ 1 as k --+ CCL i. ,k -iii:= [l:-jqk-d-l,ii,+$ (6) The product term 
on the right-hand-side of (6) tele­scopes to [1 p~]/[1 p~+l] which a 1 provided both k and d + co. 
Thus we require [(1 -~~)/(1 p;;)~k-~ + 1. This will be true provided (1 p ) -~ 1 or, equivalently, 
(k d) log(l --pd) ~ O. Using the Taylor series expansion log(l e) = c for small c, we then require 
that kpd + O (since dpd -+ O as d -+ co). That is, we require that d + co and that k not grow too quickly 
with respect to d, specifically: k = O(~-d). (7) In an asymptotically optimal splitting procedure the 
expected cost to simulate all of the offspring from a single trial from threshold O without truncation 
is of order w = k2. With truncation, this is reduced to order WI = d x k. Thus w/wf = k/d can grow arbitrarily 
large and still satisfy (7), i.e., by appro­ priately choosing the truncation threshold we obtain significant 
computational savings without introduc­ ing significant bias. As a numerical example, when Equation 6 
is computed with p = 0.5, k = 20 and d = 5, Yi /W = 0.81 representing a truncation bias of about 2070, 
but when d is increased to 10, y; /yk increases to 0.996. Even with k = 50 (and d = 10), ? ; /yk = 0.98, 
representing only 2% bias. ACKNOWLEDGEMENT This work is supported by NSF grants DMI-94-57189 and DMS-9508709. 
REFERENCES Bayes, A.J. 1970. Statistical techniques for simula­tion models. The Australian Computer Journal 
2: 180-184. Feller, W. 1968. An Introduction to Probability The­ory and Its Applications, Volume I, Third 
Edition. New York: John Wiley &#38; Sons, Inc. Glssserman, P., P. Heidelberger, P. Shahabuddin, and T. 
Zajic. 1996. Multilevel Splitting for Es­timating Rare Event Probabilities. IBM Research Report, Yorktown 
Heights, New York. Glynn, P.W ., and W. Whitt. 1992. The Asymp­totic Efficiency of Simulation Estimators. 
Opera­tions Research 40: 505 520. Hammersley, J., and D. Handscomb. 1965. Monte Carlo Methods. Methuen 
&#38; Co. Ltd., London. Harris, T. 1989. The Theory of Branching Processes. Dover, New York. Heidelberger, 
P. 1995. Fast simulation of rare events in queueing and reliability models. ACM Trans­actions on Modeling 
and Computer Simulation 5, 43-85. Hopmans, A. C. M., and J.P.C. Kleijnen. 1979. Im­port ante sampling 
in system simulation: A practi­cal failure? Mathematics and Computing in Simu­lation XXI :209-220. Kahn, 
H., and T.E. Harris. 1951. Estimation of Par­ticle Transmission by Random Sampling. Na-tionai Bureau 
of Standards Applied Mathematics Series 12, 27-30. Shahabuddin, P. 1995. Rare Event Simulation in Stochastic 
Models. In Proceedings of the 1995 Win­ter Simulation Conference, 178-185, IEEE Press. Vil16n-Altamirano, 
M., and J. Vil16n-Alt amirano. 1991. RESTART: A method for accelerating rare events simulation. In Proceedings 
of the 13th In­ternational Teletra@c Congress, Queuing perfor­mance and control in ATM, 71-76, North 
Holland Publishing Company. Vil16n-Altamarino, M., A. Martinez Marron, J. Gamo and F. Fernandez-Cuesta. 
1994. Enhancement of accelerated simulation method RESTART by con­sidering multiple threshholds. In Proceedings 
of the Idth International Teletra@c Congress, The funda­mental role of teletrafic in the evolution of 
telecom­munication networks, 787-810, Elsevier. Vil16n-Altamarino, M., and J. Vil16n-Alt amirano. 1994. 
RESTART: A straightforward method for fast simulation of rare events. In Proceedings of the 1994 Winter 
Simulation Conference, 282-289, IEEE Press. AUTHOR BIOGRAPHIES PAUL GLASSERMAN is a Professor in the 
Man­ agement Science division of the Columbia University Graduate School of Business. Prior to joining 
the Columbia faculty he was a Member of Technical Staff in the Operations Research department of AT&#38;T 
Bell Laboratories in Holmdel, NJ. He holds a Ph.D. from Harvard University and an A.B. from Princeton 
Uni­ versity. PHILIP HEIDELBERGER received a B.A. in mathematics from Oberlin College in 1974 and a Ph.D. 
in Operations Research from Stanford Univer­sity in 1978. He has been a Research Staff Member at the 
IBM T.J. Watson Research Center since 1978. While on sabbatical in 1993-1994, he was a visit­ing scientist 
at Cambridge University and at ICASE, NASA Langley Research Center. He is the Editor-in-Chief of A CM 
TOMA CS, was Program Chairman of the 1989 Winter Simulation Conference, and Program Co-Chairman of the 
ACM Sigmetrics/Performance 92 Conference. He is a Fellow of both the IEEE and the ACM. PERWEZ SHAHABUDDIN 
is an Assistant Pro­fessor at the Industrial Engineering and Operations Research Department at Columbia 
University, New York, NY, since Fall 1995. He is currently on a leave of absence from the IBM T.J. Watson 
Research Cen­ter, Yorktown Heights, NY, where he has been a Research Staff Member since 1990. He received 
his B.Tech in Mechanical Engineering from the Indian Institute of Technology, Delhi, in 1984, followed 
by a M.S. in Statistics and a Ph.D in Operations Research from Stanford University in 1987 and 1990, 
respec­tively. From 1984 to 1985 he worked as a system analyst at Engineers India Limited, India. Currently 
he is serving as an Associate Editor for IEEE Trans­actions on Reliability and is on the Editorial Board 
of IIE Dansactions-Operations Engineering, He is a re­cipient of a 1996 National Science Foundation Career 
Award. TIM ZAJIC received a Ph.D. in Operations Re­search from Stanford University in 1993. From 1994 
to 1995 he was a visitor at the Centro de Investigaci6n en Matem6ticas (CIMAT) in Guanajuato, M6xico. 
He is currently a postdoctoral fellow at the Depart­ment of Industrial Engineering and Operations Re­search 
at Columbia University and the IBM T.J. Wat­son Research Center.  
			