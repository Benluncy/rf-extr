
 Note: This is version 1.0 of the Elements of Geometry Processing course notes. Some updates will be 
posted later. Please check the website of the course for the latest version of the notes and additional 
material (slides, softwares . . . ) : http://alice.loria.fr/WIKI/index.php/Graphite/TheElements About 
the instructors Bruno L´evy INRIA, centre Nancy Grand Est, rue du Jardin Botanique, 54500 Vandoeuvre, 
France Email: bruno.levy@inria.fr http://alice.loria.fr/index.php/bruno-levy.html Bruno L´evy directs 
the ALICE project team in INRIA Nancy Grand-Est and in the LORIA lab. His research topic is numerical 
geometry, with applications in numerical simulation, real-time rendering and scienti.c visualization. 
Some of his results (e.g., LSCM) are used in several commercial and open-source soft­wares. He defended 
his Ph.D. thesis in 1999 and received the Gilles Kahn / Acadmie des Sciences SPECIF national award in 
2000. Then he did a post-doc in Stanford. He was program co-chair of ACM SPM in 2007 and 2008, and program 
co-chair of ACM/EG SGP in 2010. He is associate editor of TVCG (IEEE) and Graphical Models (Elsevier). 
He was awarded a grant from the European Research Council in 2008, received the Lorraine regional young 
re­searcher award in 2010 and the INRIA national young researcher award in 2011. Hao (Richard) Zhang 
School of Computing Science Simon Fraser University, Burnaby, Canada V5A 1S6 Email: haoz@cs.sfu.ca http://www.cs.sfu.ca/ 
haoz Hao (Richard) Zhang co-directs the Graphics, Usability, and Visualization Lab at Simon Fraser University, 
Canada, where he is an associate professor in the School of Computing Science. He received his Ph.D. 
from the Dynamic Graphics Project (DGP), Department of Computer Science, University of Toronto in 2003 
and M. Math and B. Math degrees from the University of Waterloo. His research interests include geometry 
processing, shape analysis, and computer graphics. He was a winner of the Best Paper Award from SGP 2008 
and the Most Cited Paper Award for the journal Computer-Aided Design in 2010. Course description Summary 
statement: In this course, you will learn some elements of geom­etry processing with meshes, in particular 
how to reconstruct a valid 3D mesh from raw data, and how to extract high level information from it. 
Abstract: Geometry processing is a fast-growing area of research that designs e.cient algorithms for 
the acquisition, reconstruction, analysis, manipulation, simulation and transmission of 3D models. This 
course covers di.erent aspects of Geometry Processing, related with the reconstruction of high-level 
informa­tion from raw data. The .rst part of the course explains how starting with a point set (e.g. 
acquired with a 3D scanner), one can reconstruct a valid mesh, and then recover higher-level information 
(symmetry, structuration into parts). The second part is related with mesh-based computations (e.g. UV 
mapping and deformations) that need to de.ne a function space over the mesh. We will introduce .nite 
elements, spectral function bases and some of their applications. Background: Some elements of this course 
appeared in the book Polygon Mesh Processing (Botsch et.al, CRC Press), in the Geometric Modeling based 
on Polygon Meshes course (Botsch et.al, SIGGRAPH 07, Eurographics 08), the Mesh Parameterization, Theory 
and Practice course (Hormann et.al, SIG-GRAPH 07), and the Mesh Processing course (L´evy, invited course, 
ECCV 08), the Spectral Mesh Processing course (L´evy and Zhang, SIGGRAPH ASIA 09 and SIGGRAPH 10). Intended 
audience: Researcher in the areas of geometry processing, shape analysis, and computer graphics, as well 
as practitioners who want to learn more about how to implement these new techniques and what are the 
applications. Prerequisites: Attendees need to be familiar with at least one programming language, notions 
of geometry, linear algebra, data structures and algorithms. Course syllabus  (5 min) Introduction 
[L´evy]  (45 min) Low level -Reconstruction, Meshing and Sampling [L´evy]  (45 min) High level -Recovering 
Structure and Symmetry [Zhang]  (15 min) break  (45 min) Function Spaces I -Finite Elements Modeling 
[L´evy]  (45 min) Function Spaces II -Spectral Mesh Processing [Zhang]  (5 min) Wrapup, conclusion, 
Q&#38;A  Geometry processing is a fast-growing area of research that designs e.cient algorithms for 
the acquisition, reconstruction, analysis, manipulation, simu­lation and transmission of 3D models. This 
course covers di.erent aspects of Geometry Processing, related with the reconstruction of high-level 
infor­mation from raw data. The .rst part of the course explains how starting with a point set (e.g. 
acquired with a 3D scanner), one can reconstruct a valid mesh, and then recover higher-level information 
(symmetry, structuration into parts). The second part is related with mesh-based computations (e.g. UV 
mapping and deformations) that need to de.ne a function space over the mesh. The current version of this 
document (ver. 1.0, Oct. 1st 2011) focuses on spectral mesh processing, and introduces .nite elements 
with this speci.c point of view. Some updates posted later on the website of the course http: //alice.loria.fr/WIKI/index.php/Graphite/TheElements 
will include new chapters related with reconstruction, sampling, and structure recovery. Spectral mesh 
processing involves the use of eigenvalues, eigenvectors, or eigenspace projections derived from appropriately 
de.ned mesh operators to carry out desired tasks. Early work in this area can be traced back to the seminal 
paper by Taubin [Tau95a] in 1995, where spectral analysis of mesh geometry based on a graph Laplacian 
allows us to understand the low-pass .ltering approach to mesh smoothing. Over the past .fteen years, 
the list of geometry processing applications which utilize the eigenstructures of a variety of mesh operators 
in di.erent manners have been growing steadily. Most spectral methods for mesh processing have a basic 
framework in com­mon. First, a matrix representing a discrete linear operator based on the topo­logical 
and/or geometric structure of the input mesh is constructed, typically as a discretization of some continuous 
operator. This matrix can be seen as incor­porating pairwise relations between mesh elements, adjacent 
ones or otherwise. Then an eigendecomposition of the matrix is performed, that is, its eigenvalues and 
eigenvectors are computed. Resulting structures from the decomposition are employed in a problem-speci.c 
manner to obtain a solution. We will look at the various applications of spectral mesh processing towards 
the end of the course notes (Section 5), after we provide the intuition, moti­ vation, and some theory 
behind the use of the spectral approach. The main motivation for developing spectral methods for mesh 
processing is the pursuit of Fourier analysis in the manifold setting, in particular, for meshes which 
are the dominant discrete representations of surfaces in the .eld of computer graph­ics. There are other 
desirable characteristics of the spectral approach including e.ective and information-preserving dimensionality 
reduction and its ability to reveal global and intrinsic structures in geometric data [ZvKDar]. These 
should become clear as we discuss the applications. We shall start the course notes with a very gentle 
introduction in Section 1. Instead of diving into 3D data immediately, we .rst look at the more classic 
case of processing a 2D shape represented by a contour. The motivating application is shape smoothing. 
We reduce the 2D shape processing problem to the study of 1D functions or signals specifying the shape 
s contour, naturally exposing the problem in a signal-processing framework. Our choice to start the coverage 
in the discrete setting is intended to not involve the heavy mathematical formu­lations at the start 
so as to better provide an intuition. We present Laplacian smoothing and show how spectral processing 
based on 1D discrete Laplace oper­ators can perform smoothing as well as compression. The relationship 
between this type of spectral processing and the classical Fourier transform is revealed and a natural 
extension to the mesh setting is introduced. Having provided an intuition, we then instill more rigor 
into our coverage and more formally present spectral mesh processing as a means to perform Fourier analysis 
on meshes. In particular, we deepen our examination on the connec­tion between the continuous and the 
discrete settings, focusing on the Laplace operator and its eigenfunctions. While Section 2 provides 
some theoretical background in the continuous setting (eigenfunctions of the Laplace-Beltrami operator) 
and establishes connections with other domains (machine learning and spectral graph theory). Section 
3 is concerned with ways to discretize the Laplace operator. Since spectral mesh processing in practice 
often necessitates the computation of eigenstructures of large matrices, Section 4 presents ways to make 
that process e.cient. Finally, we discuss various applications of spectral mesh processing in Section 
5. For readers who wish to obtain a more thorough coverage on the topic, we refer to the survey of Zhang 
et al. [ZvKDar]. A gentle introduction Consider the seahorse shape depicted by a closed contour, shown 
in Figure 1. The contour is represented as a sequence of 2D points (the contour vertices) that are connected 
by straight line segments (the contour segments), as illustrated by a zoomed-in view. Now suppose that 
we wish to remove the rough features over the shape of the seahorse and obtain a smoothed version, such 
as the one shown in the right of Figure 1.  Figure 1: A seahorse with rough features (left) and a smoothed 
version (right). (a) Two steps of midpoint smoothing. (b) Laplacian smoothing. (c) 1D Laplacians (red). 
Figure 2: One step of Laplacian smoothing (b) is equivalent to two steps of midpoint smoothing (a). The 
1D discrete Laplacian vectors (c) are in red. 1.1 Laplacian smoothing A simple procedure to accomplish 
this is to repeatedly connect the midpoints of successive contour segments; we refer to this as midpoint 
smoothing. Fig­ures 2(a) illustrates two such steps. As we can see, after two steps of midpoint smoothing, 
each contour vertex vi is moved to the midpoint of the line seg­ment connecting the midpoints of the 
original contour segments adjacent to vi. Speci.cally, let vi-1 =(xi-1,yi-1), vi =(xi,yi), and vi+1 =(xi+1,yi+1) 
be three consecutive contour vertices. Then the new vertex v i after two steps of midpoint smoothing 
is given by a local averaging, 11 11 111 v i =(vi-1 + vi)+ (vi + vi+1)= vi-1 + vi + vi+1. (1) 22 22 
424 The vector between vi and the midpoint of the line segment connecting the two vertices adjacent to 
vi, shown as a red arrow in Figure 2(b), is called the 1D discrete Laplacian at vi, denoted by d(vi), 
1 d(vi)= (vi-1 + vi+1) - vi. (2) 2 As we can see, after two steps of midpoint smoothing, each contour 
vertex is displaced by half of its 1D discrete Laplacian, as shown in Figure 2(c); this is referred to 
as Laplacian smoothing. The smoothed version of the seahorse in Figure 1 was obtained by applying 10 
steps of Laplacian smoothing. 1.2 Signal representation and spectral transform Let us denote the contour 
vertices by a coordinate vector V , which has n rows and two columns, where n is the number of vertices 
along the contour and the two columns correspond to the x and y components of the vertex coordinates. 
Let us denote by xi (respectively, yi) the x (respectively, y) coordinate of a vertex Figure 3: Left: 
The x-component of the contour coordinate vector V , X, can be viewed as a 1D periodic signal de.ned 
over uniform samples along a circle. Right: it is shown by a 1D plot for the seahorse contour from Figure 
1.  vi, i =1,...,n. For analysis purposes, let us only consider the x component of V , denoted by X; 
the treatment of the y component is similar. We treat the vector X as a discrete 1D signal. Since the 
contour is closed, we can view X as a periodic 1D signal de.ned over uniformly spaced samples along a 
circle, as illustrated in Figure 3(a). We sort the contour vertices in counterclockwise order and plot 
it as a conventional 1D signal, designating an arbitrary element in X to start the indexing. Figure 3(b) 
shows such a plot for the x-coordinates of the seahorse shape (n = 401) from Figure 1. We can now express 
the discrete 1D Laplacians (2) for all the vertices using an n × n matrix L, called the 1D discrete Laplace 
operator, as follows: . . 11 -- 1 0 ... ... 0 22 ...... ...... 11 - 1 - 0 ... ... 0 22 . . ... . . . 
. ... . . . . ... . . d(X)= LX = X. (3) 11 -- 0 ... ... 0 1 22 11 - 0 ... ... 0 - 1 22 The new contour 
X resulting from Laplacian smoothing (1) is then given by . .. .. . 111 x 1 0 ... ... 0x1 424 ...... 
x n-1 ...... = ...... 111 ...... ...... ...... x 2 . . . 0 ... ... 0 x2 . . . xn-1 424 X = .. . . . 
.. .. . . . .. .. . . . .. = SX. (4) 1 4 1 2 1 4 0 ... ... 0 111 x n 0 ... ... 0xn 424 The smoothing 
operator S is related to the Laplace operator L by 1 S = I - L. 2 To analyze the behavior of Laplacian 
smoothing, in particular what happens in the limit, we rely on the set of basis vectors formed by the 
eigenvectors of L. This leads to a framework for spectral analysis of geometry. From linear algebra, 
we know that since L is symmetric, it has real eigenvalues and a set of real and orthogonal set of eigenvectors 
which form a basis. Any vector of size n can be expressed as a linear sum of these basis vectors. We 
are particularly interested in such an expression for the coordinate vector X. Denote by e1, e2,...,en 
the normalized eigenvectors of L, corresponding to eigenvalues .1, .2, ..., .n, and let E be the matrix 
whose columns are the eigenvectors. Then we can express X as a linear sum of the eigenvectors, x1 .. 
. . .. .. E11 E1n E11 ... E1n nX = eix i E21 ... ... x 1 + ... + ... E2n ... ... x n = ... E21 ... E2n 
... ... ... ... x2 = E X. = i=1 En1 Enn En1 ... Enn xn (5) The above expression represents a transform 
from the original signal X to a new signal X in terms of a new basis, the basis given by the eigenvectors 
of L. We call this a spectral transform, whose coe.cients X can be obtained by X = ETX, where ET is the 
transpose of E, and for each i, the spectral transform coe.cient T x i = e · X. (6) i That is, the spectral 
coe.cient xi is obtained as a projection of the signal X along the direction of the i-th eigenvector 
ei. In Figure 4, we plot the .rst 8 eigenvectors of L, sorted by increasing eigenvalues, where n = 401, 
maching the size of the seahorse shape from Figure 1. The indexing of elements in each eigenvector follows 
the same contour vertex indexing as X, the coordinate vector; it was plotted in Figure 3(b) for the seahorse. 
It is worth noting that aside from an agreement on indexing, the Laplace operator L and the eigenbasis 
vectors do not depend on X, which speci.es the geometry of the contour. L, as de.ned in equation (3), 
is completely determined by n, the size of the input contour, and a vertex ordering. As we can see, the 
eigenvector corresponding to the zero eigenvalue is a constant vector. As eigenvalue increases, the eigenvectors 
start to oscillate as sinusoidal curves at higher and higher frequencies. Note that the eigenvalues of 
L repeat (multiplicity 2) after the .rst one, hence the corresponding eigenvec­tors of these repeated 
eigenvalues are not unique. One particular choice of the eigenvectors reveals a connection of our spectral 
analysis to the classical Fourier analysis; this will be discussed in Section 1.5.  1.3 Signal reconstruction 
and compression With the spectral transform of a coordinate signal de.ned as in equation (5), we can 
now look at compression and .ltering of a 2D shape represented by a contour. Analogous to image compression 
using JPEG, we can obtain compact ... ... ... ... Figure 4: Plots of .rst 8 eigenvectors of the 1D discrete 
Laplace operator (n = 401) given in equation (3). They are sorted by the eigenvalue .. representations 
of a contour by retaining the leading (low-frequency) spectral transform coe.cients and eliminating the 
rest. Given a signal X as in equation (5), the signal reconstructed by using the k leading coe.cients 
is k X(k) = eix i,k = n. (7) i=1 This represents a compression of the contour geometry since only k out 
of n coe.cients need to be stored to approximate the original shape. We can quantify the information 
loss by measuring the L2 error n n ||X - X(k)|| = || eix i|| = x 2 i , i=k+1 i=k+1 which is simply the 
Euclidean distance between X and X(k). The last equality T is easy to obtain if we note the orthogonality 
of the eigenvectors, i.e., e·ej =0 i whenever i Also, since the ei s are normalized, eT = j. · ei = 1. 
i In Figure 5, we show some results of this type of shape reconstruction (7), with varying k for the 
seahorse and a bird shape. As more and more high­frequency spectral coe.cients are removed, i.e., with 
decreasing k, we obtain smoother and smoother reconstructed contours. How e.ectively a 2D shape can be 
compressed this way may be visualized by plotting the spectral transform coe.cients, the xi s in (6), 
as done in Figure 6. In the plot, the horizontal axis represents eigenvalue indexes, i =1,...,n, which 
roughly correspond to frequencies. One can view the magnitude of the xi s as the energy of the input 
signal X at di.erent frequencies. A signal whose energies are sharply concentrated in the low-frequency 
end can be e.ectively compressed at a high compression rate, since as a consequence, the total energy 
at the high-frequency end, representing the reconstruction error, k =3.k =5.k = 10.k = 20.k = 30.k 
1 n. Original. 2 Figure 5: Shape reconstruction via Laplacian-based spectral analysis. Figure 6: Plot 
of spectral transform coe.cients for the x component of a contour. Left: seahorse. Right: bird. The models 
are shown in Figure 5. is very low. Such signals will exhibit fast decay in its spectral coe.cients. 
Both the seahorse and the bird models contain noisy or sharp features so they are not as highly compressible 
as a shape with smoother boundaries. This can be observed from the plots in Figure 6. Nevertheless, at 
2:1 compression ratio, we can obtain a fairly good approximation, as one can see from Figure 5.  1.4 
Filtering and Laplacian smoothing Compression by truncating the vector X of spectral transform coe.cients 
can be seen as a .ltering process. When a discrete .lter function f is applied to X, we obtain a new 
coe.cient vector X ', where X '(i)= f(i) · X (i), for all i. The .ltered signal X' is then reconstructed 
from X ' by X' = EX ', where E is the matrix of eigenvectors as de.ned in equation (5). We next show 
that Laplacian smoothing is one particular .ltering process. Speci.cally, when we apply the Laplacian 
smoothing operator S to a coordinate vector m times, the resulting Figure 7: First row: .lter plots, 
(1 - 1 .)m with m =1, 5, 10, 50. Second row: 2 corresponding results of Laplacian smoothing on the seahorse. 
coordinate vector becomes, nn 11 1 X(m) L)mXL)m .i)m = SmX =(I - =(I - eix i = ei(1 - xi. (8) 22 2 i=1 
i=1 Equation (8) provides a characterization of Laplacian smoothing in the spec­ tral domain via .ltering 
and the .lter function is given by f(.) = (1 - 1 .)m . 2 A few such .lters with di.erent m are plotted 
in the .rst row of Figure 7. The corresponding Laplacian smoothing leads to attenuation of the high-frequency 
content of the signal and hence achieves denoising or smoothing. To examine the limit behavior of Laplacian 
smoothing, let us look at equation (8). Note that it can be shown (via the Gerschgorin s Theorem [TB97]) 
that the eigenvalues of the Laplace operator are in the interval [0, 2] and the smallest eigenvalue .1 
= 0. Since . . [0, 2], the .lter function f(.) = (1 - 1 .)m 2 is bounded by the unit interval [0, 1] 
and attains the maximum f(0) = 1at . = 0. As m .8, all the terms in the right-hand side of equation (8) 
will vanish except for the .rst, which is given by e1x 1. Since e1, the eigenvector corresponding to 
the zero eigenvalue is a normalized, constant vector, we have v vv 111 nn n ]T . e1 = [ Now taking the 
y-component into consideration, we ... 1v n [ x1 y 1]. Finally, noting that get the limit point for Laplacian 
smoothing as TT x 1 = e· X and y1 = e· Y , we conclude that the limit point of Laplacian 11 smoothing 
is the centroid of the set of original contour vertices. 1.5 Spectral analysis vs. discrete Fourier 
transform The sinosoidal behavior of the eigenvectors of the 1D discrete Laplace operator (see plots 
in Figure 4) leads one to believe that there must be a connection between the discrete Fourier transform 
(DFT) and the spectral transform we have de.ned so far. We now make that connection explicit. Typically, 
one introduces the DFT in the context of Fourier series expansion. Given a discrete signal X =[x1 x2 
... xn]T, its DFT is given by n X (k)= 1 X(k)e -i2p(k-1)(j-1)/n,k =1, . . . , n. n j=1 And the corresponding 
inverse DFT is given by n X(j)= X (k)e i2p(j-1)(k-1)/n,j =1, . . . , n, k=1 or n X = X (k)gk, where gk(j)= 
e i2p(j-1)(k-1)/n,k =1, . . . , n. k=1 We see that in the context of DFT, the signal X is expressed as 
a linear combina­tion of the complex exponential DFT basis functions, the gk s. The coe.cients are given 
by the X(k) s, which form the DFT of X. Fourier analysis, provided by the DFT in the discrete setting, 
is one of the most important topics in math­ematics and has wide-ranging applications in many scienti.c 
and engineering disciplines. For a systematic study of the subject, we refer the reader to the classic 
text by Bracewell [Bra99]. The connection we seek, between DFT and spectral analysis with respect to 
the Laplace operator, is that the DFT basis functions, the gk s, form a set of eigenvectors of the 1D 
discrete Laplace operator L, as de.ned in (3). A proof of this fact can be found in Jain s classic text 
on image processing [Jai89], where a stronger claim with respect to circulant matrices was made. Note 
that a matrix is circulant if each row can be obtained as a shift (with circular wrap-around) of the 
previous row. It is clear that L is circulant. Speci.cally, if we sort the eigenvalues of L in ascending 
order, then they are .k = 2 sin2 plk/2J,k =2, . . . , n. (9) n The .rst eigenvalue .1 is always 0. We 
can see that every eigenvalue of L, except for the .rst, and possibly the last, has a multiplicity of 
2. That is, it corresponds to an eigensubspace spanned by two eigenvectors. If we de.ne the matrix G 
of DFT basis as Gkj = ei2p(j-1)(k-1)/n,1 = k, j = n, then the .rst column of G is an eigenvector corresponding 
to .1 and the k-th and (n +2 - k)-th columns of G are two eigenvectors corresponding to .k, for k =2,...,n. 
The set of eigenvectors of L is not unique. In particular, it has a set of real eigenvectors; some of 
these eigenvectors are plotted in Figure 4. 1 f(.)= . 1+(.d)2m f(.) = [(1 - s.)(1 + µ.)]m . Hello [Chu97]. 
 Bless [SM97].  1.6 Towards spectral mesh transform Based on the above observation, one way to extend 
the notion of Fourier analysis to the manifold or surface setting, where our signal will represent the 
geometry of the surfaces, is to de.ne appropriate discrete Laplace operators for meshes and rely on the 
eigenvectors of the Laplace operators to perform Fourier analysis. This was already observed in Taubin 
s seminal paper [Tau95b]. To extend spectral analysis of 1D signals presented in Section 1.2 to surfaces 
modeled by triangle meshes, we .rst need to extend the signal representation. This is quite straightforward: 
any function de.ned on the mesh vertices can be seen as a discrete mesh signal. Typically, we focus on 
the coordinate signal for the mesh, which, for a mesh with n vertices, is an n × 3 matrix whose columns 
specify the x, y, and z coordinates of the mesh vertices. The main task is to de.ne an appropriate Laplace 
operator for the mesh. Here a crucial di.erence to the classical DFTs is that while the DFT basis func­tions 
are .xed as long as the length of the signal in question is determined, the eigenvectors of a mesh Laplace 
operator would change with the mesh connec­tivity and/or geometry. Formulations for the construction 
of appropriate mesh Laplace operators will be the subjects of Sections 2 and 3. Now with a mesh Laplace 
operator chosen, de.ning the spectral transform of a mesh signal X with respect to the operator is exactly 
the same as the 1D case for L in Section 1.2. Denote by e1, e2,...,en the normalized eigenvectors of 
the mesh Laplace operator, corresponding to eigenvalues .1 = .2 = ... = .n, and let E be the matrix whose 
columns are the eigenvectors. The vector of spectral transform coe.cients X is obtained by X = ETX. And 
for each i, we T obtain the spectral coe.cient by xi = e· X via projection. i As a .rst visual example, 
we show in Figure 8 some results of spectral re­construction, as de.ne in (7), of a mesh model with progressively 
more spectral coe.cients added. As we can see, higher-frequency contents of the geometric mesh signal 
manifest themselves as rough geometric features over the shape s surface; these features can be smoothed 
out by taking only the low-frequence spectral coe.cients in a reconstruction. A tolerance on such loss 
of geometric features would lead to a JPEG-like compression of mesh geometry, as .rst pro­posed by Karni 
and Gotsman [KG00a] in 2000. More applications using spectral transforms of meshes will be given in Section 
5.   2 Fourier analysis for meshes The previous section introduced the idea of Fourier analysis applied 
to shapes, with the example of a closed curve, for which the frequencies (sine waves) are naturally obtained 
as the eigenvectors of the 1D discrete Laplacian. We now study how to formalize the idea presented in 
the last subsection, i.e. porting this setting to the case of arbitrary surfaces. Before diving into 
the heart of the matter, we recall the de.nition of the Fourier transform, that is to say the continuous 
version of the discrete signal processing framework presented in (a) Original. (b) k = 300. (c) k = 
200. (d) k = 100. (e) k = 50. (f) k = 10. (g) k = 5. (h) k = 3. Figure 8: Shape reconstruction based 
on spectral analysis using a typical mesh Laplace operator, where k is the number of eigenvectors or 
spectral coe.cients used. The original model has 7,502 vertices and 15,000 faces. Subsections 1.2 and 
1.4. 2.1 Fourier analysis As in Taubin s article [Tau95b], we start by studying the case of a closed 
curve, but staying in the continuous setting. Given a square-integrable periodic func­tion f : x . [0, 
1] . f(x), or a function f de.ned on a closed curve parameter­ized by normalized arclength, it is well 
known that f can be expanded into an in.nite series of sines and cosines of increasing frequencies: . 
8 . H0 =1 H2k+1 f(x)= f kHk(x) ; = cos(2kpx) (10). H2k+2 k=0 = sin(2kpx) where the coe.cients f k of 
the decomposition are given by: . 1 f k =< f,Hk >= f(x)Hk(x)dx (11) 0 and where < .,. > denotes the inner 
product (i.e. the dot product for func­tions de.ned on [0, 1]). See [Arv95] or [Lev06] for an introduction 
to func­tional analysis. The Circle harmonics basis Hk is orthonormal with respect to < .,. >: <Hk,Hk 
>= 1, <Hk,Hl >=0 if k = l. The set of coe.cients f k (Equation 11) is called the Fourier Transform (FT) 
of the function f. Given the coe.cients f k, the function f can be reconstructed by applying the inverse 
Fourier Transform FT-1 (Equation 10). Our goal is Figure 9: The Fiedler vector gives a natural ordering 
of the nodes of a graph. The displayed contours show that it naturally follows the shape of the dragon. 
 now to explain the generalization of these notions to arbitrary manifolds. To do so, we can consider 
the functions Hk of the Fourier basis as the eigenfunctions of -.2/.x2: the eigenfunctions H2k+1 (resp. 
H2k+2) are associated with the eigenvalues (2kp)2: .2H2k+1(x)- = (2kp)2 cos(2kpx) = (2kp)2H2k+1(x) .x2 
This construction can be extended to arbitrary manifolds by considering the generalization of the second 
derivative to arbitrary manifolds, i.e. the Laplace operator and its variants, introduced below. Before 
studying the continuous theory, we .rst do a step backward into the discrete setting, in which it is 
easier to grasp a geometric meaning of the eigenfunctions. In this setting, eigenfunc­tions can be considered 
as orthogonal non-distorting 1D parameterizations of the shape, as explained further. 2.2 The discrete 
setting: Graph Laplacians Spectral graph theory was used for instance in [IL05] to compute an ordering 
of vertices in a mesh that facilitates out-of-core processing. Such a natural ordering can be derived 
from the Fiedler eigenvector of the Graph Laplacian. The Graph Laplacian L =(ai,j) is a matrix de.ned 
by: ai,j = wi,j > 0 if(i, j) is an edge , ai,i = - j wi,j ai,j = 0 otherwise where the coe.cients wi,j 
are weights associated to the edges of the graph. One may use the uniform weighting wi,j = 1 or more 
elaborate weightings, computed from the embedding of the graph. There are several variants of the Graph 
Laplacian, the reader is referred to [ZvKDar] for an extensive survey. Among these discrete Laplacians, 
the so-called Tutte Laplacian applies a normalization in each row of L and is given by T =(ti,j ), where 
ti,j = wi,j> 0 if(i, j) is an edge j wi,j ti,i = -1 ti,j = 0 otherwise The Tutte Laplacian was employed 
in the original work of Taubin [Tau95a], among others [GGS03a, Kor03]. The .rst eigenvector of the Graph 
Laplacian is (1, 1 ... 1) and its associ­ated eigenvalue is 0. The second eigenvector is called the Fiedler 
vector and has interesting properties, making it a good permutation vector for numerical computations 
[Fie73, Fie75]. It has many possible applications, such as .nding natural vertices ordering for streaming 
meshes [IL05]. Figure 9 shows what it looks like for a snake-like mesh (it naturally follows the shape 
of the mesh). More insight on the Fiedler vector is given by the following alternative def­inition. The 
Fiedler vector u =(u1 ...un) is the solution of the following con­strained minimization problem: , Minimize: 
F (u)= utLu = i,j wi,j (ui - uj)2 (12) ,, 2 Subject to: i ui =0 and =1 i ui In other words, given a graph 
embedded in some space, and supposing that the edge weight wi,j corresponds to the lengths of the edges 
in that space, the Fielder vector (u1 ...un) de.nes a (1-dimensional) embedding of the graph on a line 
that tries to respect the edge lengths of the graph. This naturally leads to the question of whether 
embedding in higher-dimen­sional spaces can be computed (for instance, computing a 2-dimensional embed­ding 
of a surface corresponds the the classical surface parameterization prob­lem). This general problem is 
well known by the automatic learning research community as a Manifold learning problem, also called dimension 
reduction. One of the problems in manifold learning is extracting from a set of input (e.g. a set of 
images of the same object) some meaningful parameters (e.g. camera orientation and lighting conditions), 
and sort these images with respect to these parameters. From an abstract point of view, the images leave 
in a high­dimensional space (the dimension corresponds to the number of pixels of the images), and one 
tries to parameterize this image space. The .rst step constructs a graph, by connecting each sample to 
its nearest neighbors, according to some distance function. Then, di.erent classes of methods have been 
de.ned, we quickly review the most popular ones: Local Linear Embedding [RS00a] tries to create an embedding 
that best approximates the barycentric coordinates of each vertex relative to its neighbors. In a certain 
sense, Floater s Shape Preserving Parameterization (see [FH04]) is a particular case of this approach. 
Isomap [TdSL00] computes the geodesic distances between each pair of ver­ tex in the graph, and then 
uses MDS (multidimensional scaling) [You85] to com­ pute an embedding that best approximates these distances. 
Multidimensional scaling simply minimizes an objective function that measures the deviation be­tween 
the geodesic distances in the initial space and the Euclidean distances in the embedding space (GDD for 
Geodesic Distance Deviation), by computing the eigenvectors of the matrix D =(di,j ) where di,j denotes 
the geodesic dis­tance between vertex i and vertex j. This is a multivariate version of Equation 12, 
that characterizes the Fiedler vector (in the univariate setting). Isomaps and Multidimensional scaling 
were used to de.ne parameterization algorithms in [ZKK02], and more recently in the ISO-charts method 
[ZSGS04], used in Mi­ crosoft s DirectX combined with the packing algorithm presented in [LPM02]. At 
that point, we understand that the eigenvectors play an important role in determining meaningful parameters. 
Just think about the simple linear case: in PCA (principal component analysis), the eigenvectors of the 
covariance matrix characterize the most appropriate hyperplane on which the data should be pro­jected. 
In dimension reduction, we seek for eigenvectors that will .t non-linear features. For instance, in MDS, 
these eigenvectors are computed in a way that makes the embedding space mimic the global metric structure 
of the surface, captured by the matrix D =(di,j) of all geodesic distances between all pairs of vertices 
in the graph. Instead of using the dense matrix D, methods based on the Graph Laplacian only use local 
neighborhoods (one-ring neighborhoods). As a consequence, the used matrix is sparse, and extracting its 
eigenvectors requires lighter computa­tions. Note that since the Graph Laplacian is a symmetric matrix, 
its eigenvec­tors are orthogonal, and can be used as a vector basis to represent functions. This was 
used in [KG00b] to de.ne a compact encoding of mesh geometry. The basic idea consists in encoding the 
topology of the mesh together with the coef­.cients that de.ne the geometry projected onto the basis 
of eigenvectors. The decoder simply recomputes the basis of eigenvectors and multiplies them with the 
coe.cients stored in the .le. A survey of spectral geometry compression and its links with graph partitioning 
is given in [Got03]. Spectral graph theory also enables to exhibit ways of de.ning valid graph embeddings. 
For instance, Colin­de-verdi`ere s number [dV90] was used in [GGS03b] to construct valid spherical embeddings 
of genus 0 meshes. Other methods that use spectral graph theory to compute graph embeddings are reviewed 
in [Kor02]. Spectral graph theory can also be used to compute topological invariants (e.g. Betti numbers), 
as explained in [Fei96]. As can be seen from this short review of spectral graph theory, the eigenvectors 
and eigenvalues of the graph Laplacian contain both geometric and topological information. However, as 
explained in [ZSGS04], only using the connectivity of the graph may lead to highly distorted mappings. 
Methods based on MDS solve this issue by considering the matrix D of the geodesic distances between all 
possible pairs of vertices. However, we think that it is also possible to inject more geometry in the 
Graph Laplacian approach, and understand how the global geometry and topology of the shape may emerge 
from the interaction of local neighborhoods. This typically refers to notions from the continuous setting, 
i.e. functional analysis and operators. The next section shows its link with the Laplace-Beltrami operator, 
that appears in the wave equation (Helmholtz s equation). We will also exhibit the link between the so-called 
stationary waves and spectral graph theory. 2.3 The Continuous Setting: Laplace Beltrami The Laplace 
operator (or Laplacian) plays a fundamental role in physics and mathematics. In Rn, it is de.ned as the 
divergence of the gradient: .2 . = div grad = V.V = .x2 i i Intuitively, the Laplacian generalizes the 
second order derivative to higher di­mensions, and is a characteristic of the irregularity of a function 
as .f(P ) measures the di.erence between f(P ) and its average in a small neighborhood of P . Generalizing 
the Laplacian to curved surfaces require complex calculations. These calculations can be simpli.ed by 
a mathematical tool named exterior cal­culus (EC) 1 . EC is a coordinate free geometric calculus where 
functions are considered as abstract mathematical objects on which operators act. To use these functions, 
we cannot avoid instantiating them in some coordinate frames. However, most calculations are simpli.ed 
thanks to higher-level considerations. For instance, the divergence and gradient are known to be coordinate 
free oper­ators, but are usually de.ned through coordinates. EC generalizes the gradient by d and divergence 
by d, which are built independently of any coordinate frame. Using EC, the de.nition of the Laplacian 
can be generalized to functions de.ned over a manifold S with metric g, and is then called the Laplace-Beltrami 
operator: 1.. . = div grad = dd = |g||g| .xi.xi i where |g| denotes the determinant of g. The additional 
term|g| can be inter­preted as a local scale factor since the local area element dA on S is given by 
 dA =|g|dx1 . dx2. Finally, for the sake of completeness, we can mention that the Laplacian can be extended 
to k-forms and is then called the Laplace-de Rham operator de.ned by . = dd + dd. Note that for functions 
(i.e. 0-forms), the second term dd vanishes and the .rst term dd corresponds to the previous de.nition. 
Since we aim at constructing a function basis, we need some notions from functional analysis, quickly 
reviewed here. A similar review in the context of light simulation is given in [Arv95]. 1 To our knowledge, 
besides Hodge duality used to compute minimal surfaces [PP93], one of the .rst uses of EC in geometry 
processing [GY02] applied some of the fundamental notions involved in the proof of Poincar´e s conjecture 
to global conformal parameterization. More recently, a Siggraph course was given by Schroeder et al. 
[SGD05], making these notions usable by a wider community. 2.3.1 Functional analysis In a way similar 
to what is done for vector spaces, we need to introduce a dot product (or inner product) to be able to 
de.ne function bases and project func­tions onto those bases. This corresponds to the notion of Hilbert 
space, outlined below. Hilbert spaces Given a surface S, let X denote the space of real-valued functions 
de.ned on S. Given a norm I.I, the function space X is said to be complete with respect to the norm if 
Cauchy sequences converge in X, where a Cauchy sequence is a sequence of functions f1,f2,... such that 
limn,m->8 Ifn -fmI = 0. A complete vector space is called a Banach space. The space X is called a Hilbert 
space in the speci.c case of the norm de.ned v by IfI = < f,f >, where < .,. > denotes an inner product. 
A possible i de.nition of an inner product is given by < f,g >= f(x)g(x)dx, which yields S the L2 norm. 
One of the interesting features provided by this additional level of structure is the ability to de.ne 
function bases and project onto these bases using the inner product. Using a function basis (Fi), a function 
f will be de.ned by , f = aiFi. Similarly to the de.nition in geometry, a function basis (Fi) is orthonormal 
if IFiI = 1 for all i and < Fi, Fj >= 0 for all i = j. Still following the analogy with geometry, given 
the function f, one can easily retrieve its coordinates ai with respect to an orthonormal basis (Fi) 
by projecting f onto the basis, i.e. ai =< f, Fi >. Operators We now give the basic de.nitions related 
with operators. Simply put, an op­erator is a function of functions (i.e. from X to X). An operator L 
applied to a function f . X is denoted by Lf, and results in another function in X. An operator L is 
said to be linear if L(.f)= .Lf for all f . X, . . R. An eigenfunction of the operator L is a function 
f such that Lf = .f. The scalar . is an eigenvalue of L. In other words, the e.ect of applying the operator 
to one of its eigenfunctions means simply scaling the function by the scalar .. A linear operator L is 
said to be Hermitian (or with Hermitian symmetry)2 if < Lf,g >=< f,Lg > for each f, g . X. An important 
property of Hermitian operators is that their eigenfunctions associated to di.erent eigenvalues have 
real eigenvalues and are orthogonal. This latter property can be easily proven as follows, by considering 
two eigenfunctions f, g associated with the di.erent eigenvalues ., µ respectively: < Lf,g > = < f,Lg 
> < .f,g > = < f,µg > . < f,g > = µ < f,g > 2the general de.nition of Hermitian operators concerns complex-valued 
functions, we only consider here real-valued functions. which gives the result (< f,g >= 0) since . 
= µ. As a consequence, considering the eigenfunctions of an Hermitian operator is a possible way of de.ning 
an orthonormal function basis associated to a given function space X. The next section shows this method 
applied to the Laplace-Beltrami operator. Before entering the heart of the matter, we will .rst consider 
the historical perspective. 2.3.2 Chladni plates In 1787, the physicist Ernst Chladni published the 
book entitled Discoveries Concerning the Theories of Music . In this book, he reports his observations 
ob­tained when putting a thin metal plate into vibration using a bow, and spreading sand over it. Sand 
accumulates in certain zones, forming surprisingly complex patterns (see Figure 10). This behavior can 
be explained by the theory of stationary waves. When the metal plate vibrates, some zones remain still, 
and sand naturally concentrates in these zones. This behavior can be modeled as follows, by the spatial 
component of Helmholtz s wave propagation equation: .f = .f (13) In this equation, . denotes the Laplace-Beltrami 
operator on the considered object. In Cartesian 2D space, . = .2/.x2 + .2/.y2 . We are seeking for the 
eigenfunctions of this operator. To better understand the meaning of this equa­tion, let us .rst consider 
a vibrating circle. This corresponds to the univariate case on the interval [0, 2p] with cyclic boundary 
conditions (i.e. f(0) = f(2p)). In this setting, the Laplace-Beltrami operator simply corresponds to 
the second order derivative. Recalling that sin(.x) '' = .2sin(.x), the eigenfunctions are simply sin(Nx), 
cos(Nx) and the constant function, where N is an integer. Note that the so-constructed function basis 
is the one used in Fourier analysis. From the spectrum of the Laplace-Beltrami operator, it is well 
known that one can extract the area of S, the length of its border and its genus. This leads to the question 
asked by Kac in 1966: Can we hear the shape of a drum ? [Kac66]. The answer to this question is no : 
one can .nd drums that have the same spectrum although they do not have the same shape [Cip93] (they 
are then referred to as isospectral shapes). However, the spectrum contains much information, which led 
to the idea of using it as a signature for shape matching and classi.cation, as explained in the shape 
DNA approach [RWP05a]. We are now going now to take a look at the eigenfunctions. Mathematicians mostly 
studied bounds and convergence of the spectrum. However, some results are known about the geometry of 
the eigenfunctions [JNT]. More precisely, we are interested in the so-called nodal sets, de.ned to be 
the zero-set of an eigenfunction. Intuitively, they correspond to the locations that do not move on a 
Chladni plate, where sand accumulates (see Figure 10). A nodal set partitions the surface into a set 
of nodal domains of constant sign. The nodal sets and nodal domains are characterized by the following 
theorems: 1. the n-th eigenfunction has at most n nodal domains 2. the nodal sets are curves intersecting 
at constant angles  Besides their orthogonality, these properties make eigenfunction an interesting 
choice for function bases. Theorem 1 exhibits their multi-resolution nature, and from theorem 2, one 
can suppose that they are strongly linked with the geometry of the shape. Note also that these theorems 
explain the appearance of Chladni plates. This may also explain the very interesting re-meshing results 
obtained by Dong et. al [DBG+05], that use a Morse-smale decomposition of one of the eigenfunctions. 
In the case of simple objects, a closed form of the eigenfunctions can be derived. This made it possible 
to retrieve the patterns observed by Chladni in the case of a square and a circle. For curved geometry, 
Chladni could not make the experiment, since sand would not remain in the nodal set. However, one can 
still study the eigenfunctions. For instance, on a sphere, the eigenfunction correspond to spherical 
harmonics (see e.g. [JNT]), often used in computer graphics to represent functions de.ned on the sphere 
(such as radiance .elds or Bidirectional Re.ectance Distribution Functions). In other words, on a sphere, 
the eigenfunctions of the Laplace-Beltrami operator de.ne an interesting hierar­chical function basis. 
One can now wonder whether the same approach could be used to create function bases on more complex geometries. 
In the general case, a closed form cannot be derived, and one needs to use a numerical approach, as explained 
in the next section.   Discretizing the Laplace operator The previous section mentioned two di.erent 
settings for spectral analysis : the discrete setting is concerned with graphs, matrices and vectors; 
 the continuous setting is concerned with manifolds, operators and func­tions.  The continuous setting 
belongs to the realm of di.erential geometry, a pow­erful formalism for studying relations between manifolds 
and functions de.ned over them. However, computer science can only manipulate discrete quantities. Therefore, 
a practical implementation of the continuous setting requires a dis­cretization. This section shows how 
to convert the continuous setting into a discrete counterpart using the standard Finite Element Modeling 
technique. In Geometry Processing, another approach consists in trying to de.ne a discrete setting that 
mimics the property of the continous theory. As such, we will also show how spectral geometry processing 
can be derived using Discrete Exterior Calculus (DEC). The eigenfunctions and eigenvalues of the Laplacian 
on a (manifold) surface S, are all the pairs (Hk,.k) that satisfy: -.Hk = .kHk (14) The - sign is here 
required for the eigenvalues to be positive. On a closed curve, the eigenfunctions of the Laplace operator 
de.ne the function basis (sines and cosines) of Fourier analysis, as recalled in the previous section. 
On a square, they correspond to the function basis of the DCT (Discrete Cosine Transform), used for instance 
by the JPEG image format. Finally, the eigenfunctions of the Laplace-Beltrami operator on a sphere de.ne 
the Spherical Harmonics basis. In these three simple cases, two reasons make the eigenfunctions a function 
basis suitable for spectral analysis of manifolds: 1. Because the Laplacian is symmetric (< .f,g >=< 
f, .g>), its eigen­functions are orthogonal, so it is extremely simple to project a function onto this 
basis, i.e. to apply a Fourier-like transform to the function. 2. For physicists, the eigenproblem (Equation 
14) is called the Helmoltz equa­ tion, and its solutions Hk are stationary waves. This means that the 
Hk  v are functions of constant wavelength (or spatial frequency) .k = .k. Hence, using the eigenfunctions 
of the Laplacian to construct a function ba­sis on a manifold is a natural way to extend the usual spectral 
analysis to this manifold. In our case, the manifold is a mesh, so we need to port this construc­tion 
to the discrete setting. The .rst idea that may come to the mind is to apply spectral analysis to a discrete 
Laplacian matrix (e.g. the cotan weights). How­ever, the discrete Laplacian is not a symmetric matrix 
(the denominator of the ai,j coe.cient is the area of vertex i ' s neighborhood, that does not necessarily 
correspond to the area of vertex j s neighborhood). Therefore, we lose the sym­metry of the Laplacian 
and the orthogonality of its eigenvectors. This makes it di.cult to project functions onto the basis. 
For this reason, we will clarify the relations between the continuous setting (with functions and operators), 
and the discrete one (with vectors and matrices) in the next section. In this section, we present two 
ways of discretizing the Laplace operator on a mesh. The .rst approach is based on Finite Element Modeling 
(FEM) such as done in [WBH+07], and converge to the continuous Laplacian under certain conditions as 
explained in [HPW06] and [AFW06]. Reuter et al. [RWP05b] also use FEM to compute the spectrum (i.e. the 
eigenvalues) of a mesh, which provides a signature for shape classi.cation. The cotan weights were also 
used in [DBG+06b] to compute an eigenfunction to steer a quad-remeshing process. The cotan weights alone 
are still dependent on the sampling as shown in Fig­ure 18-B, so they are usually weighted by the one 
ring or dual cell area of each vertex, which makes them loose their symmetry. As a consequence, they 
are improper for spectral analysis (18-C). An empirical symmetrization was pro­ posed in [Lev06] (see 
Figure 18-D). The FEM formulation enables to preserve the symmetry property of the continuous counterpart 
of the Laplace operator. It is also possible to derive a symmetric Laplacian by using the DEC formu­lation 
(Discrete Exterior Calculus). Then symmetry is recovered by expressing the operator in a proper basis. 
This ensures that its eigenfunctions are both ge­ometry aware and orthogonal (Figure 18-E). Note that 
a recent important proof [WMKG07] states that a perfect discrete Laplacian that satis.es all the prop­ 
erties of the continuous one cannot exist on general meshes. This explains the large number of de.nitions 
for a discrete Laplacian, depending on the desired properties. 3.1 The FEM Laplacian In this subsection, 
we explain the Finite Element Modeling (FEM) formulation of the discrete Laplacian. The Discrete Exterior 
Calculus (DEC)formululation, presented during the course, is described in the next subsection. We have 
chosen to include in these course nodes the full derivations for the FEM Laplacian for the sake of completeness. 
To setup the .nite element formulation, we .rst need to de.ne a set of basis functions used to express 
the solutions, and a set of test functions onto which the eigenproblem (Equation 14) will be projected. 
As it is often done in FEM, we choose for both basis and test functions the same set Fi(i =1 ...n). We 
use the hat functions (also called P1), that are piecewise-linear on the triangles, and that are such 
that Fi(i) = 1andFi(j) = 0if i = j. Geometrically, Fi corresponds to the barycentric coordinate associated 
with vertex i on each triangle containing i. Solving the .nite element formulation of Equation 14 , 
n relative to the Fi s means looking for functions of the form: Hk = HikFi i=1 which satisfy Equation 
14 in projection on the Fj s: .j, < -.Hk , Fj >= .k <Hk , Fj > or in matrix form: -Qhk = .Bhk (15) where 
Qi,j =< .Fi , Fj >, Bi,j =< Fi , Fj > and where hk denotes the vector [H1 k,...Hk]. The matrix Q is called 
the sti.ness matrix, and B the mass matrix. n This appendix derives the expressions for the coe.cients 
of the sti.ness matrix Q and the mass matrix B. To do so, we start by parameterizing a triangle t =(i, 
j, k) by the barycentric coordinates (or hat functions) Fi and Fj of a point P . t relative to vertices 
i and j. This allows to write P = k +Fiej - Fj ei (Figure 14). This yields an area element dA(P )= ei 
. ej dFidFj =2|t|dFidFj , where |t| is the area of t, so we get the integral: 11-Fi FiFj dA =2|t| FiFjdFidFj 
= P .t Fi=0 Fj =0 1 121 |t| |t| Fi(1 - Fi)2dFi = |t|- += 234 12 Fi=0 which we sum up on the 2 triangles 
sharing (i, j) to get Bi,j =(|t| + |t ' |/12. We get the diagonal terms by: 11-Fi F2 i dA =2|t| F2 i 
dFj = P .t Fi=0 Fj =0 1 11 |t| 2|t| Fi 2(1 - Fi)dFi =2|t|- = 34 6 Fi=0 which are summed up over the set 
St(i) of triangles containing i to get Bi,i = , (|t|)/6. t.St(i) To compute the coe.cients of the sti.ness 
matrix Q, we use the fact that d and d are adjoint to get the more symmetric expression: Qi,j =< .Fi 
, Fj >=< ddFi , Fj >=<dFi,dFj >= VFi .VFj S In t, the gradients of barycentric coordinates are the constants 
: . -eei.ej i VFi = VFi .VFj = 2|t| 4|t|2 Where e. denotes ei rotated by p/2 around t s normal. By integrating 
on t we i get: ei.ej ||ei||.||ej|| cos(ßij ) cot(ßij) VFi .VFjdA == = 4|t| 2||ei||.||ej|| sin(ßij )2 
t Summing these expressions, the coe.cients of the sti.ness matrix Q are given by: 2 e i = VFi .VFi = 
Qi,i 4|t| t.St(i) t.St(i) Qi,j = VFi .VFj =1 cot(ßij ) + cot(ß ' ijt.tl 2 Note that this expression is 
equivalent to the numerator of the classical cotan weights. cotan(ßi,j ) + cotan(ß ' /2 Qi,j =i,j) , 
Qi,i = - j Qi,j (16) Bi,j =(|t| + |t ' |) /12 , Bi,i =(t.St(i) |t|)/6 where t, t ' are the two triangles 
that share the edge (i, j), |t| and |t ' | denote their areas, ßi,j , ß ' denote the two angles opposite 
to the edge (i, j) in t and i,j t ' , and St(i) denotes the set of triangles incident to i. To simplify 
the computations, a common practice of FEM consists in replac­ing this equation with an approximation: 
 -Qhk = .Dhkor - D-1Qhk = .hk(17) where the mass matrix B is replaced with a diagonal matrix D called 
the lumped mass matrix, and de.ned by: Di,i = Bi,j =( |t|)/3. (18) jt.St(i) Note that D is non-degenerate 
(as long as mesh triangles have non-zero areas). FEM researchers [Pra99] explain that besides simplifying 
the computations this approximation fortuitously improves the accuracy of the result, due to a can­cellation 
of errors, as pointed out in [Dye06]. The practical solution mechanism to solve Equation 17 will be explained 
further in the section about numerics. Remark: The matrix D-1Q in (Equation 17) exactly corresponds 
to the usual discrete Laplacian (cotan weights). Hence, in addition to direct derivation of triangle 
energies [PP93] or averaged di.erential values [MDSB03], the discrete Laplacian can be derived from a 
lumped-mass FEM formulation. As will be seen further, the FEM formulation and associated inner product 
will help us understand why the orthogonality of the eigenvectors seems to be lost (since D-1Q is not 
symmetric), and how to retrieve it. Without entering the details, we mention some interesting features 
and de­grees of freedom obtained with the FEM formulation. The function basis F onto the eigenfunction 
problem is projected can be chosen. One can use piecewise polynomial functions. Figure 16 shows how the 
eigenfunctions look like with the usual piecewise linear basis (also called P1 in FEM parlance) and degree 
3 polynomial basis (P3). Degree 3 triangles are de.ned by 10 values (1 value per vertex, two values per 
edge and a central value). As can be seen, more complex function shapes can be obtained (here displayed 
using a fragment shader). It is also worth mentioning that the boundaries can be constrained in two di.erent 
ways (see Figure 17). With Dirichlet boundary conditions, the value of the func­ tion is constant on 
the boundary (contour lines are parallel to the boundary). With Neumann boundary condistions, the gradient 
of the eigenfunction is par­allel to the boundary (therefore, contour lines are orthogonal to the boundary). 
 3.2 The DEC Laplacian For a complete introduction to DEC we refer the reader to [DKT05], [Hir03] and 
to [AFW06] for proofs of convergence. We quickly introduce the few notions and notations that we are 
using to de.ne the inner product < .,. > and generalized second-order derivative (i.e. Laplacian operator). 
A k-simplex sk is the geometric span of k + 1 points. For instance, 0, 1 and 2-simplices are points, 
edges and triangles respectively. In our context, a mesh can be de.ned as a 2-dimensional simplicial 
complex S, i.e. a collection of nk k-simplices (k =0, 1, 2), with some conditions to make it manifold. 
A discrete k-form .k on S is given by a real value .k(sk) associated with each oriented k­simplex (that 
corresponds to the integral of a smooth k-form over the simplex). The set Ok(S) of k-forms on S is a 
vector-space of dimension nk. With a proper numbering of the k-simplices, .k can be assimilated to a 
vector of size nk, and linear operators from Ok(S) to Ol(S) can be assimilated to (nk,nl) matrices. 
The exterior derivative dk :Ok(S) . Ok+1(S) is de.ned by the signed adjacency matrix: (dk)sk,sk+1 = ±1 
if sk belongs to the boundary of sk+1, with the sign depending on their respective orientations. DEC 
provides Ok(S) with a L2 inner product: <.1 k,.2 k >=(.1 k)T *k .k (19) 2 where *k is the so-called Hodge 
star. As a matrix, the Hodge star is diagonal ** with elements |s |/|sk| where s denotes the circumcentric 
dual of simplex sk, kk and |.| is the simplex volume. In particular, for vertices, edges and triangles: 
|e *| (*0)vv = |v * | ;(*1)ee = = cot ße + cot ße ' ;(*2)tt = |t|-1 |e| where ße and ß ' denote the two 
angles opposite to e. e The codi.erential dk :Ok(S) . Ok-1(S) is the adjoint of the exterior deriva­tive 
for the inner product: <dk.1 k,.k-1 >=<.1 k,dk-1.k-1 > 22 which yields dk = - *-1 dT *k. k-1 k-1 Finally 
the Laplace de Rham operator on k-forms is given by: .k = dk-1dk + dk+1dk. In this paper, we are only 
interested in the Laplacian . = .0, i.e. the Laplace de Rham operator for 0-forms . Since d0 and d2 are 
unde.ned (zero by convention), . = d1d0 = - *-1 dT *1 d0 and its coe.cients are given by: 01  ijWeighted 
cotan (cot(ßij)+cot(ß ' ))/Ai is not symmetric which does not allow for ij correct reconstruction (C). 
Only symmetric weights (cot(ßij )+cot(ß ' ij ))/AiAj is fully mesh-independent (E). cot(ßij) + cot(ß 
' ).ij = -ij ;.ii = - .ij * |v | i j For surfaces with borders, if the edge ij is on the border, the 
term cot(ß ' ij ) * vanishes and the dual cell v is cropped by the border. This matches the FEM i formulation 
with Neumann boundary conditions (not detailed here). Remark: The matrix . corresponds to the standard 
discrete Laplacian, except for the sign. The sign di.erence comes from the distinction between Laplace-Beltrami 
and Laplace de Rham operators. The so-de.ned Laplacian . apparently looses the symmetry of its contin­uous 
counterpart (.ij =.ji). This makes the eigenfunction basis no longer orthonormal, which is problematic 
for our spectral processing purposes (Fig­ure 18-C). To recover symmetry, consider the canonical basis 
(fi) of 0-forms: fi = 1 on vertex i and fi = 0 on other vertices. This basis is orthogo­nal but not normal 
with respect to the inner product de.ned in Equation 19 (<fi,fi >=(fi)T * 0 fi = 1). However, since the 
Hodge star * 0 is a diagonal matrix, one can easily normalize (fi) as follows: -1/2 ¯ fi = *0 fi ¯ In 
this orthonormalized basis ( f¯i), the Laplacian . is symmetric, and its coef­.cients are given by: cot 
ßij + cot ß ' -1/21/2 ij . = ¯*0 .*0 ; .¯ij = - (20) ** |v ||v | ij  4 Computing eigenfunctions Now 
that we have seen two equivalent discretizations of the Laplacien, namely FEM (Finite Elements Modeling) 
and DEC (Discrete Exterior Calculus), we will now focus on the problem of computing the eigenfunctions 
in practice. An implementation of the algorithm is available from the WIKI of the course (see web reference 
at the beginning of this document). Computing the eigenfunctions means solving for the eigenvalues .k 
and ¯¯ eigenvectors Hk for the symmetric positive semi-de.nite matrix .: .¯H¯k = .kH¯k However, eigenvalues 
and eigenvectors computations are known to be ex­tremely computationally intensive. To reduce the computation 
time, Karni et al. [KG00c] partition the mesh into smaller charts, and [DBG+06b] use mul­ tiresolution 
techniques. In our case, we need to compute multiple eigenvectors (typically a few thousands). This is 
known to be currently impossible for meshes with more than a few thousand vertices [WK05]. In this section, 
we show how this limit can be overcome by several orders of magnitude. To compute the solutions of a 
large sparse eigenproblems, several iterative algorithms exist. The publicly available library ARPACK 
(used in [DBG+06b]) provides an e.cient implementation of the Arnoldi method. Yet, two charac­teristics 
of eigenproblem solvers hinder us from using them directly to compute the MHB for surfaces with more 
than a few thousand vertices: .rst of all, we are interested in the lower frequencies, i.e. eigenvectors 
with associated eigenvalues lower than .2 . Unfortunately, iterative solvers m performs much better for 
the other end of the spectrum. This can be explained in terms of .ltering as lower frequencies correspond 
to higher powers of the smoothing kernel, which may have a poor condition number; secondly, we need to 
compute a large number of eigenvectors (typically a thousand), and it is well known that computation 
time is superlinear in the number of requested eigenpairs. In addition, if the surface is large (millions 
of vertices), the MHB does not .t in system RAM. 4.1 Band-by-band computation of the MHB We address both 
issues by applying spectral transforms to the eigenproblem. To get the eigenvectors of a spectral band 
centered around a value .S , we start ¯¯ by shifting the spectrum by .S, by replacing . with . - .SId. 
Then, we can swap the spectrum by inverting this matrix in .SI = (.¯- .SId)-1 . This is called the Shift-Invert 
spectral transform, and the new eigenproblem to solve is given by: .SI H¯k H¯k = µk ¯ It is easy to check 
that . and .SI have the same eigenvectors, and that their eigenvalues are related by .k = .S +1/µk. Applying 
an iterative solver to .SI will return the high end of the spectrum (largest µ s), corresponding to a 
band ¯ of eigenvalues of . centered around .S . It is then possible to split the MHB computation into 
multiple bands, and obtain a computation time that is linear in the number of computed eigenpairs. In 
addition, if the MHB does not .t in RAM, each band can be streamed into a .le. This gives the following 
algorithm: (1) .S . 0; .last . 0 (2) while(.last <.2 )  m (3) compute an inverse .SI of ( .¯- .S Id) 
 (4) .nd the 50 .rst eigenpairs ( H¯k ,µk) of .SI (5) for k =1 to 50 (6) .k . .S +1/µk (7) if (.k 
>.last) write(H¯k,.k)  (8) end //for (9) .S . max(.k)+0.4(max(.k) - min(.k)) (10) .last . max(.k) 
 (11) end //while Before calling the eigen solver, we pre-compute .SI with a sparse direct solver (Line 
3). Note that .¯- .SId may be singular. This is not a problem since the spectral transform still works 
with an inde.nite factorization. The factorized .¯- .SId is used in the inner loop of the eigen solver 
(Line 4). To factorize .¯-.SId, we used sparse direct solvers (TAUCS, SUPERLU). For large models (millions 
of vertices), we used the sparse OOC (out-of-core) symmetric inde.nite factorization [MIT06] implemented 
in the future release of TAUCS, kindly provided by S. Toledo. We then recover the . s from the µ s (Line 
6) and stream-write the new eigenpairs into a .le (Line 7). Since the eigenvalues are centered around 
the shift .S, the shift for the next band is given by the last computed eigenvalue plus slightly less 
than half the bandwidth to ensure that the bands overlap and that we are not missing any eigenvalue (Line 
9). If the bands do not overlap, we recompute a larger band until they do. Note that this is di.erent 
from the shift-invert spectral transform imple­mented by ARPACK, dedicated to iterative solvers. Ours 
makes use of the factorization of the matrix, resulting in much better performances.  Applications The 
eigendecomposition of a discrete mesh Laplace operator provides a set of eigenvalues and eigenvectors, 
which can be directly used by an application to accomplish di.erent tasks. Moreover, the eigenvectors 
can also be used as a basis onto which a signal de.ned on a triangle mesh is projected. The resulting 
spectral transform coe.cients can be further analyzed or manipulated. Here we discuss a subset of the 
applications which utilize the spectral transform or the eigenvectors of mesh Laplace or more general 
linear mesh operators. 5.1 Use of eigenvectors Eigenvectors are typically used to obtain an embedding 
of the input shape in the spectral domain, which we call a spectral embedding. After obtain­ing the eigendecomposition 
of a speci.c operator, the coordinates of vertex i in a k-dimensional spectral embedding are given by 
the i-th row of matrix Vk =[v1,..., vk], where v1,..., vk are the .rst k eigenvectors from the spec­trum 
(possibly after scaling). Whether the eigenvectors should be in ascending or descending order of eigenvalues 
depends on the operator that is being used. The choice of k also varies between applications. For planar 
mesh parameteriza­tion, k is typically 2, while in other applications, a single eigenvector possessing 
desirable properties may be selected for a task. In computer vision and machine learning, spectral methods 
usually employ a di.erent operator, the so-called a.nity matrix [SM00, Wei99]. Each entry Wij of an a.nity 
matrix W represents a numerical relation, the a.nity, between two data points i and j, e.g., pixels in 
an image, vertices in a mesh, or two face models in the context of face recognition. Note that the a.nity 
matrix di.ers from the Laplacian in that a.nities between all data pairs are de.ned. Therefore this matrix 
is not sparse in general. In practice, this non-sparse structure implies more memory requirements and 
more expensive computations. The use of a.nity matrices for spectral mesh processing has received wide 
appeal in computer graphics as well, e.g., for mesh segmentation. 5.1.1 Parameterization and remeshing 
In the context of mesh parameterization, spectral methods have the interest­ing property of connecting 
local entities in a way that lets a global behavior emerge. This property can be used to compute a good 
base complex [DBG+06a], [HZM+08], or to directly de.ne a parameterization [ZSGS04], [MTAD08], as ex­ 
plained below. Figure 19 shows that Mullen et.al s spectral parameterization achieves a result comparable 
to ABF++. The interesting point is that it uses a simple and elegant formulation of the problem (just 
compute the Fiedler vector of the LSCM matrix). Besides corresponding to a Fourier basis on meshes , 
eigenfunctions can be used to de.ne parameterizations. For instance, in the context of data analysis, 
LLE (Local Linear Embedding) [RS00b] computes an embedding of a graph that preserves metric relations 
in local neighborhoods. Similarly, the MDS method (multidimensional scaling) [You85] computes an embedding 
that best approximates all the geodesic distances between the vertices of a graph. Mul­tidimensional 
scaling simply minimizes an objective function that measures the deviation between the geodesic distances 
in the initial space and the Euclidean distances in the embedding space (GDD for Geodesic Distance Deviation), 
by computing the eigenvectors of the matrix D =(di,j ) where di,j denotes the geodesic distance between 
vertex i and vertex j. Isomaps and Multidimensional scaling were used to de.ne parameterization algorithms 
in [ZKK02], and more recently in the ISO-charts method [ZSGS04]. Spectral analysis also has the interesting 
property of de.ning an orthogonal basis. This property can be used to avoid the degenerate con.gurations 
encountered by linear conformal param­eterization methods [LPRM02], [DMA02]. Those linear methods use 
vertex pinning to avoid the trivial constant solution, whereas the spectral method introduced in [MTAD08] 
computes the .rst solution orthogonal to the trivial one, that is to say, the eigenvector associated 
with the .rst non-zero eigenvalue. However, the spectral parameterization methods listed above still 
need to partition the mesh into charts. More recently, Dong et al. used the Laplacian to decompose a 
mesh into quadrilaterals [DBG+05, DBG+06a], in a way that facil­ itates constructing a globally smooth 
parameterization. As shown in Figure 20, their method .rst computes one eigenfunction of the Laplacian 
(the 38th in this example), then extract the Morse complex of this function, .lters and smoothes the 
Morse complex, and uses it to partition the mesh into quads. These quads are parameterized, and inter-chart 
smoothness can be further optimized us­ing global relaxation [KLS03, THCM04]. More recently, a generalization 
was proposed [HZM+08], still based on Laplacian eigenfunctions, and steered by a guidance vector .eld. 
 5.1.2 Clustering and segmentation One of the most well-known techniques in this regard is spectral clustering 
[BN03, KVV00, NJW02]. Interested readers should refer to the recent survey by von Luxburg [vL06] and 
the comparative study by Verma and Meil.a [VM03]. Ng et al. [NJW02] presented a clustering method where 
the entries of the .rst k eigen­vectors corresponding to the largest eigenvalues of an a.nity matrix 
are used to obtain the transformed coordinates of the input data points. Additionally, the embedded points 
are projected onto the unit k-sphere. Points that possess high a.nities tend to be grouped together in 
the spectral domain, where a sim­ple clustering algorithm, such as k-means, can reveal the .nal clusters. 
Other approaches di.er only slightly from the core solution paradigm, e.g., in terms of the operator 
used and the dimensionality of the embedding. The ubiquity of the clustering problem makes spectral clustering 
an ex­tremely useful technique. Kolluri et al. [KSO04] used spectral clustering to determine the inside 
and outside tetrahedra in the context of a Delaunay-based surface reconstruction algorithm. Liu and Zhang 
[LZ04] performed mesh seg­ mentation via spectral clustering. Basically, an a.nity matrix is constructed 
where the a.nity measure depends on both geodesic distances and curvature information. Next, the eigenvectors 
given by the eigendecomposition of this matrix guide a clustering method, which provides patches of faces 
that de.ne the di.erent segments of the mesh returned by the segmentation algorithm. It is shown by example 
that it can be advantageous to perform segmentation in the spectral domain, e.g., in terms of higher-quality 
cut boundaries. In a follow-up work, Zhang and Liu [ZL05] presented a mesh segmentation approach based 
on a recursive 2-way spectral clustering method. Only the .rst two largest eigenvectors of the a.nity 
matrix are computed. This provides a one-dimensional embedding of mesh faces given by the quotient of 
the entries of the two eigenvectors. The most salient cut in this embedding is located, given by a part 
salience measure [HS97]. The cut provides a segmentation of the faces into two parts. This process is 
recursively repeated in order to obtain a hierarchical binary partitioning of the input mesh. In yet 
another follow-up work, Liu and Zhang [LZ07] proposed an algorithm for mesh segmentation via recursive 
bisection where at each step, a sub-mesh embedded in 3D is spectrally projected to 2D and then a contour 
is extracted from the planar embedding. Two operators are used in combination to com­pute the projection: 
the well-known graph Laplacian and a geometric operator designed to emphasize concavity. The two embeddings 
reveal distinctive shape semantics of the 3D model and complement each other in capturing the struc­tural 
or geometrical aspects of a segmentation. Transforming the shape analysis problem to the 2D domain also 
facilitates segmentability analysis and sampling, where the latter is needed to identify two samples 
residing on di.erent parts of the sub-mesh. These two samples are used by the Nystr¨om method in the 
con­struction of a 1D face sequence for .nding an optimal cut, as in [ZL05]. Similar to the technique 
presented in Section 4, the Nystr¨om method is designed to ef­.ciently compute eigenvectors of large 
matrices. However, it relies on sampling and extrapolation and only approximates the eigenvectors. Recently, 
de Goes et al. [dGGV08] presented a hierarchical segmentation method for articulated bodies. Their approach 
relies on the di.usion distance, which is a multi-scale metric based on the heat kernel and computed 
from the eigenvectors of the Laplace-Beltrami operator. The di.usion distance is used to compute a bijection 
between medial structures and segments of the model. The medial structures yield a means to further re.ne 
the segmentation in an iterative manner and provide a full hierarchy of segments for the shape. Huang 
et al. [HWAG09] also performed hierarchical shape decomposition via spectral analysis. However, the operator 
they use encapsulates shape geometry beyond the static setting. The idea is to de.ne a certain deformation 
energy and use the eigenvectors of the Hessian of the deformation energy to characterize the space of 
possible deformations of a given shape. The eigenmodes corresponding to the low-end of the spectrum of 
the Hessian capture low-energy or in their for­mulation, more rigid deformations, called typical deformations. 
The optimal shape decomposition they compute is one whose optimal articulated (piecewise rigid) deformation 
de.ned on the parts of the decomposition best conforms to the basis vectors of the space of typical deformations. 
As a result, their method tends to identify parts of a shape which would likely remain rigid during the 
typical deformations. 5.1.3 Shape correspondence and retrieval Depending on the requirement of the problem 
at hand, the mesh operator we use to compute a spectral embedding can be made to incorporate any intrinsic 
measure on a shape in order to obtain useful invariance properties, e.g., with respect to part articulation 
or bending. In Figure 21, we show 3D spectral embeddings of a few human and hand models obtained from 
an operator derived from geodesic distances over the mesh surfaces. As geodesic distance is bending­tolerant, 
the resulting embeddings are normalized with respect to bending and can facilitate shape retrieval under 
part articulation [EK03, JZ07]. Elad and Kimmel [EK03] used MDS to compute bending-invariant signa­ 
tures for meshes. Geodesic distances between mesh vertices are computed by fast marching. The resulting 
spectral embedding e.ectively normalizes the mesh shapes with respect to translation, rotation, as well 
as bending transformations. The similarity between two shapes is then given by the Euclidean distance 
be­tween the moments of the .rst few eigenvectors, usually less than 15, and these similarity distances 
can be used for shape classi.cation. Inspired by works in computer vision on spectral point correspondence 
[SB92], Jain and Zhang [JZvK07] relied on higher-dimensional embeddings based on the eigenvectors of 
an a.nity matrix to obtain point correspondence between two mesh shapes. The .rst k eigenvectors of the 
a.nity matrix encoding the geodesic distances between pairs of vertices are used to embed the model in 
a k-dimensional space; typically k = 5 or 6. After this process is performed for two models, the two 
embeddings are non-rigidly aligned via thin-plate splines and the correspondence between the two shapes 
is given by the proximity of the vertices after such alignment. Any measure for the cost of a correspondence, 
e.g., sum of distances between corresponding vertices, can be used as a similarity distance for shape 
retrieval. One of the key observations made in [JZvK07] is the presence of eigen­ vector switching due 
to non-uniform scaling of the shapes. Speci.cally, the eigenmodes of similar shapes do not line up with 
respect to the magnitude of their corresponding eigenvalues, as illustrated in Figure 22. As a result, 
it is unreliable to sort the eigenvectors according to the magnitude of their respec­tive eigenvalues, 
as has been done in all works on spectral correspondence so far. Jain and Zhang [JZvK07] relied on a 
heuristic to unswitch the eigenmodes and thin-plate splines to further align the shapes in the spectral 
domain [JZvK07]. Recent work of Mateus et al. [MCBH07] addressed the issue using an alignment by the 
Expectation-Maximization (EM) algorithm instead. The method by Leordeanu and Hebert [LH05] focuses on 
the global char­ acteristics of correspondence computation and aims at .nding consistent cor­respondences 
between two sets of shape or image features, where the spectral approach has also found its utility. 
They build a graph whose nodes represent possible feature pairings and edge weights measure how agreeable 
the pairings are. The principal eigenvector of an a.nity matrix W , one corresponding to the largest 
eigenvalue, is inspected to detect how strongly the graph nodes belong to the main cluster of W . The 
idea is that correct correspondences are likely to Figure 22: Eigenvector plots for two similar shapes, 
both with 252 vertices. The entries in an eigenvector are color-mapped. As we can see, there is an eigenvector 
switching occurring between the .fth and sixth eigenvectors. Such switching is di.cult to detect from 
the magnitude of the eigenvalues. The .rst 8 eigenvalues for the two shapes are [205.6, 11.4, 4.7, 3.8, 
1.8, 0.4, 0.26, 0.1] and [201.9, 10.9, 6.3, 3.4, 1.8, 1.2, 0.31, 0.25], respectively. establish links 
among each other and thus form a strongly connected cluster. To de.ne informative shape descriptors, 
another possibility consists in using the heat di.usion equation and the heat kernel. Heat di.usion is 
governed by the following di.erential equation : .f = k.f .t where k is a constant. This equation admits 
a solution that can be written as : f(t, x)= K(t, y, x)dy where K(t, x, y) denotes the heat kernel. Intuitively, 
considering that a Dirac of heat was emitted from point x at time t = 0, the heat kernel K(t, x, y) indicates 
the quantity of heat that can be measured at point y after time t. The heat kernel admits an expression 
as an (in.nite) sum of eigenfunctions : 8 K(t, x, y)= e -.itfi(x)fi(y) i=0 Using this expression, the 
idea of using the auto-di.usion function was simul­taneously proposed in [SOG] and [GBAL]. The auto-di.usion 
function corre­sponds to the amount of heat that remains at point x after time t for a given t, in other 
words K(t, x, x). This de.nes a scalar .eld on the surface that prov­ably has good properties to be used 
as a shape descriptor [SOG]. In particular, protrusions correspond to extrema of the function. The value 
of t acts as a smoothing parameter (smaller values of t preserve most details, and higher val­ues of 
t tend to a constant function). Another application of the auto-di.usion function is to compute a Morse-Smale 
complex for shape segmentation [GBAL].  5.1.4 Global intrinsic symmetry detection Ovsjanikov et al. 
[OSG08] propose an approach to detect the intrinsic sym­metries of a shape which are invariant to isometry 
preserving transformations. They show that if the shape is embedded into a signature space de.ned by 
the eigenfunctions of the Laplace-Beltrami operator, then the intrinsic symmetries are transformed into 
extrinsic Euclidean symmetries (rotations or re.ections). However, it is possible to restrict the search 
of symmetries only to re.ections, avoiding the search for rotational symmetries, a task that can be hard 
in high­dimensional space. This result allows to obtain the intrinsic symmetries by .rst computing the 
eigenvectors of the operator, then embedding the shape into the signature space, and .nally .nding point-to-point 
correspondences between symmetric points. The signature adopted is derived from the GPS embedding of 
Rustamov [Rus07].  5.2 Use of spectral transforms The spectral mesh transforms are closely related 
to the Fourier transform that is the foundation of signal processing theory. Conceivably, any application 
of the classical Fourier transform in signal or image processing has the potential to be realized in 
the mesh setting. In geometry processing, the mesh signal considered is often the embedding function 
that speci.es the 3D coordinates of each vertex. This signal serves to represent mesh geometry. 5.2.1 
Geometry compression Karni and Gotsman [KG00c] proposed an approach to compress the geometry of triangle 
meshes. Firstly, the set of eigenvectors of the Tutte Laplacian is computed. Next, the mesh vertex coordinates 
are projected into the spectral space spanned by the computed eigenvectors. Part of the coe.cients obtained 
by this transformation is eliminated in order to reduce the storage space required for mesh geometry. 
The coe.cients related to the eigenvectors associated to larger eigenvalues are .rstly removed, which 
would correspond to high frequency detail, when following an analogy with Fourier analysis. The main 
drawback of this method is that many eigenvectors need to be computed. Karni and Gotsman propose to partition 
the mesh into smaller sets of vertices. Although that alleviates the problem of computing the eigenvectors 
for large matrices, it still requires a good partitioning of the mesh for the e.ciency of the compression, 
and artifacts along the partition boundaries are evident when higher compression rates are employed. 
 5.2.2 Watermarking Ohbuchi et al. [OTMM01, OMT02] also employed the spectral transform ap­ proach, but 
to insert watermarks into triangle meshes. In their method, the eigenvectors of the graph Laplacian are 
used as the basis for the projection. After transforming the geometry of the mesh and obtaining the spectral 
coe.­cients, a watermark is inserted into the model by modifying coe.cients at the low-frequency end 
of the spectrum. In this way, modi.cations on the geometry of the mesh are well-spread over the model 
and less noticeable than if they were directly added to the vertex coordinates. This method also requires 
the compu­tation of eigenvectors of the Laplace operator, which is prohibitive in the case of large meshes. 
Mesh partitioning is again used to address this problem. 5.2.3 Fourier descriptors 2D Fourier descriptors 
have been quite successful as a means to characterize 2D shapes. Using eigendecomposition with respect 
to the mesh Laplacians, one can compute analogous Fourier-like descriptors to describe mesh geometry. 
However, there have not seen such mesh Fourier descriptors being proposed for shape analysis so far. 
There have been methods, e.g., [VS01], which rely on 3D Fourier descriptors for 3D shape retrieval. In 
this case, the mesh shapes are .rst voxelized and 3D Fourier descriptors are extracted from the resulting 
volumetric data. We suspect that the main di.culties with the use of mesh Fourier descriptors for shape 
matching include computational costs and the fact that when the eigenmodes vary between the two mesh 
shapes to be matched, it becomes doubtful whether their associated eigenspace projections can serve as 
reliable shape descriptors. Also, even when the shapes are very similar, eigenvector switching, as depicted 
in Figure 22, can occur when the eigenvectors are ordered by the magnitude of their eigenvalues.  References 
[AFW06] D. N. Arnold, R. S. Falk, and R. Winther. Finite element exterior calculus, homological techniques, 
and applications. Acta Numerica 15, 2006. [Arv95] James Arvo. The Role of Functional Analysis in Global 
Illumina­tion. In P. M. Hanrahan and W. Purgathofer, editors, Rendering Techniques 95 (Proceedings of 
the Sixth Eurographics Workshop on Rendering), pages 115 126, New York, NY, 1995. Springer-Verlag. [BN03] 
M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. Neural 
Computations, 15(6):1373 1396, 2003. [Bra99] Ronald N. Bracewell. The Fourier Transform And Its Applications. 
McGraw-Hill, 1999. [Chu97] F. R. K. Chung. Spectral Graph Theory. AMS, 1997. [Cip93] Barri Cipra. You 
can t always hear the shape of a drum. What s Happening in the Mathematical Sciences, 1, 1993. [DBG+05] 
S. Dong, P.-T. Bremer, M. Garland, V. Pascucci, and J. C. Hart. Quadrangulating a mesh using laplacian 
eigenvectors. Technical report, June 2005. [DBG+06b] Shen Dong, Peer-Timo Bremer, Michael Garland, Valerio 
Pascucci, [DBG+06a] S. Dong, P.-T. Bremer, M. Garland, V. Pascucci, and J. C. Hart. Spectral mesh quadrangulation. 
ACM Transactions on Graphics (SIGGRAPH 2006 special issue), 2006. and John C. Hart. Spectral surface 
quadrangulation. In SIG-GRAPH 06: ACM SIGGRAPH 2006 Papers, pages 1057 1066, New York, NY, USA, 2006. 
ACM Press. [dGGV08] Fernando de Goes, Siome Goldenstein, and Luiz Velho. A hierarchi­cal segmentation 
of articulated bodies. Computer Graphics Forum (Symposium on Geometry Processing), 27(5):1349 1356, 2008. 
[DKT05] Mathieu Desbrun, Eva Kanzo, and Yiying Tong. Discrete di.eren­tial forms for computational modeling. 
Siggraph 05 course notes on Discrete Di.erential Geometry, Chapter 7, 2005. [DMA02] Mathieu Desbrun, 
Mark Meyer, and Pierre Alliez. Intrinsic pa­rameterizations of surface meshes. In Proceedings of Eurographics, 
pages 209 218, 2002. [dV90] Y. Colin de Verdiere. Sur un nouvel invariant des graphes et un critere de 
planarite. J. of Combinatorial Theory, 50, 1990. [Dye06] Ramsey Dyer. Mass weights and the cot operator 
(personal com­munication). Technical report, Simon Fraser University, CA, 2006. [EK03] A. Elad and R. 
Kimmel. On bending invariant signatures for sur­faces. IEEE Trans. Pattern Anal. Mach. Intell., 25(10):1285 
1295, 2003. [Fei96] J. Feidman. Computing betti numbers via combinatorial laplacians. In Proc. 28th Sympos. 
Theory Comput., pages 386 391. ACM, 1996. [FH04] M. S. Floater and K. Hormann. Surface parameterization: 
a tuto­rial and survey. Springer, 2004. [Fie73] Miroslav Fiedler. Algebraic connectivity of graphs. Czech. 
Math. Journal, 23:298 305, 1973. [Fie75] Miroslav Fiedler. A property of eigenvectors of nonnegative 
sym­metric matrices and its application to graph theory. Czech. Math. Journal, 25:619 633, 1975. [GBAL] 
Katarzyna Gebal, Andreas Baerentzen, Henrik Aanaes, and Ras­mus Larsen. Shape analysis using the auto 
di.usion function. Com­puter Graphics Forum (Proc. of Symp. on Geom. Proc.). [GGS03a] C. Gotsman, X. 
Gu, and A. She.er. Fundamentals of spherical parameterization for 3d meshes. ACM Trans. Graph., 22(3):358 
363, 2003. [GGS03b] C. Gotsman, X. Gu, and A. She.er. parameterization for 3d meshes, 2003. Fundamentals 
of spherical [Got03] Craig Gotsman. On graph partitioning, spectral analysis, and dig­ital mesh processing. 
In Shape Modeling International, pages 165 174, 2003. [GY02] X. Gu and S.-T. Yau. Computing conformal 
structures of surfaces. Communications in Information and Systems, 2(2):121 146, 2002. [Hir03] Anil Hirani. 
Discrete exterior calculus. PhD thesis, 2003. [HPW06] Klaus Hildebrandt, Konrad Polthier, and Max Wardetzky. 
On the convergence of metric and geometric properties of polyhedral sur­faces. Geom Dedicata, 2006. [HS97] 
D. D. Ho.man and M. Singh. Salience of visual parts. Cognition, 63:29 78, 1997. [HWAG09] Qixing Huang, 
Martin Wicke, Bart Adams, and Leonidas J. Guibas. Shape decomposition using modal analysis. 28(2):to 
ap­pear, 2009. [HZM+08] Jin Huang, Muyang Zhang, Jin Ma, Xinguo Liu, Leif Kobbelt, and Hujun Bao. Spectral 
quadrangulation with orientation and alignment control. ACM Transactions on Graphics (SIGGRAPH Asia conf. 
proc., 2008. [IL05] Martin Isenburg and Peter Lindstrom. Streaming meshes. In IEEE Visualization, page 
30, 2005. [Jai89] A. K. Jain. Hall, 1989. Fundamentals of Digital Image Processing. Prentice [JNT] Dmitry 
Jakobson, Nikolai Nadirashvili, and John Toth. Geometric properties of eigenfunctions. [JZ07] Varun Jain 
and Hao Zhang. A spectral approach to shape-based retrieval of articulated 3D models. Computer Aided 
Design, 39:398 407, 2007. [JZvK07] Varun Jain, Hao Zhang, and Oliver van Kaick. Non-rigid spectral correspondence 
of triangle meshes. International Journal on Shape Modeling, 13(1):101 124, 2007. [Kac66] Mark Kac. Can 
you hear the shape of Monthly, 73, 1966. a drum? Amer. Math. [KG00a] Z. Karni and C. Gotsman. Spectral 
compression of mesh geometry. In Proc. of ACM SIGGRAPH, pages 279 286, 2000. [KG00b] Zachi Karni and 
Craig Gotsman. Spectral compression of mesh geometry. In SIGGRAPH, pages 279 286, 2000. [KG00c] Zachi 
Karni and Craig Gotsman. Spectral compression of mesh geometry. In SIGGRAPH 00: Proceedings of the 27th 
annual conference on Computer graphics and interactive techniques, pages 279 286, New York, NY, USA, 
2000. ACM Press/Addison-Wesley Publishing Co. [KLS03] A. Khodakovsky, N. Litke, and P. Schr¨oder. Globally 
smooth pa­rameterizations with low distortion. ACM TOG (SIGGRAPH), 2003. [Kor02] Y. Koren. On spectral 
graph drawing, 2002. [Kor03] Y. Koren. On spectral graph drawing. In Proc. of the International Computing 
and Combinatorics Conference, pages 496 508, 2003. [KSO04] Ravikrishna Kolluri, Jonathan Richard Shewchuk, 
and James F. O Brien. Spectral surface reconstruction from noisy point clouds. In Proc. of Eurographics 
Symposium on Geometry Processing, pages 11 21, 2004. [KVV00] R. Kannan, S. Vempala, and A. Vetta. On 
clustering -good, bad, and spectral. In FOCS, pages 367 377, 2000. [Lev06] Bruno Levy. Laplace-beltrami 
eigenfunctions: Towards an algo­rithm that understands geometry. In IEEE International Confer­ence on 
Shape Modeling and Applications, 2006. [LH05] Marius Leordeanu and Martial Hebert. A spectral technique 
for correspondence problems using pairwise constraints. In Interna­tional Conference of Computer Vision 
(ICCV), volume 2, pages 1482 1489, October 2005. [LPM02] Bruno Levy, Sylvain Petitjean, and Nicolas Ray 
Nicolas Jerome Maillot. Least squares conformal maps for automatic texture atlas generation. In ACM, 
editor, SIGGRAPH conf. proc, 2002. [LPRM02] B. L´evy, S. Petitjean, N. Ray, and J. Maillot. Least squares 
confor­mal maps for automatic texture atlas generation. In Proc. of ACM SIGGRAPH 02, pages 362 371, 2002. 
[LZ04] R. Liu and H. Zhang. Segmentation of 3D meshes through spectral clustering. In Paci.c Graphics, 
pages 298 305, 2004. [LZ07] Rong Liu and Hao Zhang. Mesh segmentation via spectral em­bedding and contour 
analysis. Computer Graphics Forum (Special Issue of Eurographics 2007), 26:385 394, 2007. [MCBH07] Diana 
Mateus, Fabio Cuzzolin, Edmond Boyer, and Radu Horaud. Articulated shape matching by robust alignment 
of embedded rep­resentations. In ICCV 07 Workshop on 3D Representation for Recognition (3dRR-07), 2007. 
[MDSB03] Mark Meyer, Mathieu Desbrun, Peter Schr¨oder, and Alan H. Barr. Discrete di.erential-geometry 
operators for triangulated 2­manifolds. In Hans-Christian Hege and Konrad Polthier, editors, Visualization 
and Mathematics III, pages 35 57. Springer-Verlag, Heidelberg, 2003. [MIT06] Omer Meshar, Dror Irony, 
and Sivan Toledo. An out-of-core sparse symmetric inde.nite factorization method. ACM Transactions on 
Mathematical Software, 32:445 471, 2006. [MTAD08] Patrick Mullen, Yiying Tong, Pierre Alliez, and Mathieu 
Desbrun. Spectral conformal parameterization. In ACM/EG Symposium of Geometry Processing, 2008. [NJW02] 
A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral clustering: anal­ysis and an algorithm. In Neural Information 
Processing Systems, volume 14, pages 849 856, 2002. [OMT02] R. Ohbuchi, A. Mukaiyama, and S. Takahashi. 
A frequency-domain approach to watermarking 3D shapes. Computer Graphics Forum, 21(3):373 382, 2002. 
[OSG08] Maks Ovsjanikov, Jian Sun, and Leonidas Guibas. Global intrinsic symmetries of shapes. Computer 
Graphics Forum (Symposium on Geometry Processing), 27(5):1341 1348, 2008. [OTMM01] R. Ohbuchi, S. Takahashi, 
T. Miyazawa, and A. Mukaiyama. Wa­termarking 3D polygonal meshes in the mesh spectral domain. In Proc. 
of Graphics Interface, pages 9 18, 2001. [PP93] Ulrich Pinkall and Konrad Polthier. Computing discrete 
minimal surfaces and their conjugates. Experimental Mathematics, 2(1), 1993. [Pra99] G. Prathap. Towards 
a science of fea: Patterns, predictability and proof through some case studies. Current Science, 77:1311 
1318, 1999. [RS00a] S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear 
embedding. Science, 290:2323 2326, 2000. [RS00b] Sam Roweis and Lawrence Saul. Nonlinear dimensionality 
reduc­tion by locally linear embedding. Science, 290(5500):2323 2326, Dec 2000. [Rus07] R. M. Rustamov. 
Laplace-beltrami eigenfunctions for deformation invariant shape representation. In Proc. of Eurographics 
Sympo­sium on Geometry Processing, pages 225 233, 2007. [RWP05a] M. Reuter, F.-E. Wolter, and N. Peinecke. 
Laplace-beltrami spectra as shape-dna of surfaces and solids. CAD Journal, 2005. [RWP05b] Martin Reuter, 
Franz-Erich Wolter, and Niklas Peinecke. Laplace­spectra as .ngerprints for shape matching. In SPM 05: 
Proceedings of the 2005 ACM symposium on Solid and physical modeling, pages 101 106, New York, NY, USA, 
2005. ACM Press. [SB92] L. S. Shapiro and J. M. Brady. Feature-based correspondence: an eigenvector approach. 
Image and Vision Computing, 10(5):283 288, 1992. [SGD05] P. Schr¨oder, E. Grinspun, and M. Desbrun. Discrete 
di.erential geometry: an applied introduction. In SIGGRAPH Course Notes, 2005. [SM97] J. Shi and J. Malik. 
Normalized cuts and image segmentation. In Proc. of IEEE Conference on Computer Vision and Pattern Recog­nition, 
pages 731 737, 1997. [SM00] Jianbo Shi and Jitendra Malik. Normalized cuts and image segmen­tation. IEEE 
Trans. on Pattern Analysis and Machine Intelligence, 22(8):888 905, 2000. [SOG] Jian Sun, Maks Ovsjanikov, 
and Leonidas Guibas. A concise and provably informative multi-scale signature based on heat di.usion. 
Computer Graphics Forum (Proc. of Symp. on Geom. Proc.). [Tau95a] G. Taubin. A signal processing approach 
to fair surface design. In Proc. of ACM SIGGRAPH, pages 351 358, 1995. [Tau95b] Gabriel Taubin. A signal 
processing approach to fair surface design. In SIGGRAPH 95: Proceedings of the 22nd annual conference 
on Computer graphics and interactive techniques, pages 351 358, New York, NY, USA, 1995. ACM Press. [TB97] 
Lloyd N. Trefethen and David Bau. SIAM, 1997. Numerical Linear Algebra. [TdSL00] J. B. Tenenbaum, V. 
de Silva, and J. C. Langford. A global geo­metric framework for nonlinear dimensionality reduction. Science, 
290:2319 2323, 2000. [THCM04] M. Tarini, K. Hormann, P. Cignoni, and C. Montani. Polycube­maps. ACM TOG 
(SIGGRAPH), 2004. [vL06] Ulrike von Luxburg. A tutorial on spectral clustering. Technical Report TR-149, 
Max Plank Institute for Biological Cybernetics, August 2006. [VM03] D. Verma and M. Meila. A comparison 
of spectral clustering algo­rithms. Technical Report UW-CSE-03-05-01, University of Wash­ington, 2003. 
[VS01] D. V. Vrani´c and D. Saupe. 3D shape descriptor based on 3D Fourier transform. In Proc. EURASIP 
Conf. on Digital Signal Processing for Multimedia Communications and Services, 2001. [WBH+07] Max Wardetzky, 
Miklos Bergou, David Harmon, Denis Zorin, and Eitan Grinspun. Discrete quadratic curvature energies. 
Computer Aided Geometric Design (CAGD), 2007. [Wei99] Y. Weiss. Segmentation using eigenvectors: A unifying 
view. In Proc. of the International Conference on Computer Vision, pages 975 983, 1999. [WK05] Jianhua 
Wu and Leif Kobbelt. E.cient spectral watermarking of large meshes with orthogonal basis functions. In 
The Visual Computer, 2005. [WMKG07] Max Wardetzky, Saurabh Mathur, Felix Kalberer, and Eitan Grin­spun. 
Discrete laplace operators: No free lunch. Eurographics Symposium on Geometry Processing, 2007. [You85] 
F. W. Young. Multidimensional scaling. Encyclopedia of Statistical Sciences, 5:649 658, 1985. [ZKK02] 
Gil Zigelman, Ron Kimmel, and Nahum Kiryati. Texture map­ping using surface .attening via multidimensional 
scaling. IEEE Transactions on Visualization and Computer Graphics, 8(2), 2002. [ZL05] H. Zhang and R. 
Liu. Mesh segmentation via recursive and visually salient spectral cuts. In Proc. of Vision, Modeling, 
and Visualiza­tion, 2005. [ZSGS04] Kun Zhou, John Snyder, Baining Guo, and Heung-Yeung Shum. Iso-charts: 
Stretch-driven mesh parameterization using spectral analysis. In Symposium on Geometry Processing, pages 
47 56, 2004. [ZvKDar] Hao Zhang, Oliver van Kaick, and Ramsay Dyer. Spectral mesh processing. Computer 
Graphics Forum, 2009, to appear. http: //www.cs.sfu.ca/~haoz/pubs/zhang cgf09 spect survey.pdf.  
			