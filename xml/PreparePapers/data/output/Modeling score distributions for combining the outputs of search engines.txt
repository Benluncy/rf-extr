
  inq101: non-relevant distribution for query 151 us from dicussing the details of the EM algorithm 
and the update equations used. The reader is referred to [3, 15] for a good intro­duction to EM. 20 15 
10 5 0 normalized score normalized frequency Figure 1: Histogram and shifted exponential .t to non-relevant 
data for query 151 INQUERY (inq101) documents was small, the .t was bad. However, we believe this is 
not because the Gaussian was a bad .t but because we don t have enough relevant documents to compute 
the statistics in these cases. The exponential was also a good .t to the non-relevant data. We have so 
far been able to .t parametric forms to the score dis­tributions given relevance information. When running 
a new query, however, relevance information is not available. Clearly, it would be useful to .t the score 
distributions of such data. A natural way to do this is to .t a mixture model of a shifted exponential 
and a Gaus­sian to the combined score distribution. This approach is discussed  The procedure needs 
an initial estimate of the component den­sities and mixing parameters. Given that, it rapidly converges 
to a solution. Using EM to .t the data gives the mixture .t shown in Figure 3. The .gure plots the mixture 
density as well as the com­ponent densities for the exponential and Gaussian .ts. For com­parison Figure 
2 shows the exponential and Gaussian .ts to the non-relevant and relevant data. Comparing the two .gures, 
it ap­pears that the strategy of interpreting the Gaussian component of the mixture with the relevant 
distribution and the exponential com­ponent of the mixture with the non-relevant distribution is a rea­sonable 
one. We should note that the correspondence between the mixture components and the .ts to the relevant/non-relevant 
data is not always as good as that shown here but in general it is a reason­able .t. The same technique 
was used to model the result of query 151 for the Cornell Smart vector space engine. Similar results 
were obtained as shown in Figures 4 and 5. This model has been .tted to a large number of search engines 
on TREC-3 and TREC-4 data including probabilistic engines like INQUERY and CITY and a vector space engine 
(SMART) as well as Bellcore s LSI engine. The .t appears to be better for good search engines (engines 
with a higher average precision in TREC­3) and worse for those with a lower average precision. The model 
has also been able .tted to document scores for searches on IN-QUERY and SMART indexing a Chinese database. 
inq101: query 151 (cnt(r/n): 83/917) in the next section. 25 3.1 Mixture Model Fit 20 Consider the situation 
where a query is run using a search en­gine. The search engine returns scores but there is no relevance 
in­formation available. We show below that in this situation, a mixture model consisting of an exponential 
and a Gaussian may be .tted to the score distributions. We can then identify the Gaussian with the distribution 
of the relevant information in the mixture and the ex­ponential with the distribution of the non-relevant 
information in the mixture. Essentially this allows us to .nd the parameters of the relevant and non-relevant 
distributions without knowing relevance information apriori. The density of a mixture model p(x)can be 
written in terms of the densities of the individual components p(xlj)as follows: [3, 15] . p(x)=P(j)p(xlj) 
(1)  where j identi.es the individual component, the P(j)are known as . . mixing parameters and satisfy 
.. P(j)=l,0:P(j):l.In the present case, there are two components, an exponential density with mean A 
p(xll)=Aexp(-Ax) (2) and a Gaussian density with mean Jand variance ;. p(xl2)= l 2; exp(­(x-J). 2;. ) 
(3) A standard approach to .nding the mixing parameters and the parameters of the component densities 
is to use Expectation Maxi­mization (EM) [3, 15]. This is an iterative procedure where the Ex­pectation 
and Maximization steps are alternated. Space precludes normalized frequency 15 10 5 0 0 0.2 0.4 0.6 0.8 
1 normalized score Figure 2: Exponential .t to non-relevant data and Gaussian .t to relevant data for 
query 151 INQUERY (inq101) 3.2 Computing Posterior Probabilities Using Bayes Rule one can compute the 
probability of relevance given the score as P(sorelrel)P(rel)P(rellsore) P(sorelrel)P(rel)P(sorelnonrel)P(nonrel) 
(4) where P(rellsore)is the probability of relevance of the document given its score, P(sorelrel)and 
P(sorelnonrel)are the proba­bilities of score given that the document is relevant and score given that 
the document is non-relevant respectively. P(rel) and P(nonrel) are the prior probabilities of relevance 
and non-relevance. In our model, P(sorelrel)is given by the Gaussian component of the mixture while P(sorelnonrel)is 
given by the exponential re - )= - )= )= - )= - )= the Poisson distribution shows a decreasing distribution. 
Although, expansion the score of the relevant documents decreases and the this is not the same as an 
exponential distribution, it does have the range also decreases which is consistent with this prediction. 
For same general shape as an exponential (rapid monotonic decrease). the 50 queries from TREC-3 for INQUERY 
(inq101) and SMART (crnlea), we plotted the mean scores of the relevant data versus the number of query 
terms (including expanded queries). A small sta­tistical decrease with the number of query terms was 
observed for INQUERY and SMART. The .gures are not produced here because of a lack of space. 5. COMBINING 
SEARCH ENGINES The posterior probabilities obtained by using the model discussed above has many possible 
applications. For example the probabil­ities could be used to select a threshold for .ltering documents 
or for combining the outputs in distributed retrieval. Here we discuss one possible application which 
involves combining the outputs of different search engines on a common database to improve results. It 
would be of considerable use to combine the output of dif­ferent search engines. In this section we discuss 
how the score of search engines may be combined while taking into account the ac­number of events tual 
score distributions. In general the range of search engine scores may vary quite a bit -Figure 8: The 
Poisson distribution for different values of A. for example, one engine may have scores ranging from 
0 to 1 while It is much harder to derive the score distributions if the query consists of two or more 
terms. This is because the actual scores of search engines are complicated functions. However, there 
is empir­ical evidence that the major contribution to the scores is provided by the number of matching 
terms [11]. We also note that Robert­son and Walker [17] motivated a tf-i.fscoring function from the 
2-Poisson model. We assume .rst that the score is proportional to the number of matching words and provide 
an intuitive argument for queries with two or more terms. For simplicity we will con­sider the case where 
the query has just two terms but the argument applies in general. In this case we can assume that the 
two terms say oil and spill can be clubbed together to create a single term another can have scores ranging 
from -20 to 150. Other approaches to combining score distributions have focused on normalizing the range 
of the scores and then combining them by simple techniques like linear combination or by taking the minimum 
and maximum scores. However, range normalization does not take into account the actual distribution of 
the scores. Consider, for example, the model of the scores discussed previously where the scores of the 
relevant documents follow a normal distribution and the scores of the non-relevant follow an exponential 
distribution. Also consider two different search engines which have different parameter values for these 
distributions. A simple (linear) range normalization and combination does not clearly suf.ce. Recall-Precision 
Plot - oil spill . Then the AP(the average frequency of a term over 1 relevant documents) for joint 
occurrences of this term oil spill is much lower than the APfor either oil or spill . In other words 
the chances that the terms oil spill occur together is much less 0.8 than that of .nding oil or spill 
. When the query contains two terms, it is reasonable to assume that the An(i.e. the average fre­ quency 
over non-relevant documents) does not change much as it 0.6 essentially re.ects the background probabilities 
of the word. The Poisson model for the shape of the relevant and non-relevant distributions that we 
have derived applies to both probabilistic en­gines like INQUERY and vector space engines like SMART. 
For vector space engines the number of matching terms is given by the dot product of two vectors -one 
representing the query and the other the document. Further, this model is language indepen­dent (as long 
as word frequencies in any language have an approx­imately 2-Poisson like distribution). Thus, we predict 
that a mix­ture of exponential and Gaussian distributions will .t a much larger class of text search 
engines operating on different languages. The model in this section although intuitive can be used to 
make a prediction. The model predicts that on a statistical basis as the number of query terms is increased 
the overall Ashould decrease and hence the mean and variance of the Gaussian .tting the relevant documents 
should also decrease. Note that for any particular query the mean score can be arbitrary. However when 
a large enough sample of queries is considered, the mean query should decrease with the number of query 
terms. It is a well known fact in information retrieval that with query precision % 0.4 0.2 0  Figure 
9: Recall precision graphs for combining inq101 and crnlea using different techniques (see text). Data 
from TREC-3 There are a number of possible ways the probabilities can be used to combine the search engines. 
We propose two alternative schemes for combination. The .rst involves averaging the prob­abilities. This 
is optimal in the sense of minimizing the Bayes error if the search engines are independent [19]. Of 
course the outputs of search engines are not necessarily independent. In the following .gures and discussions, 
data are taken from TREC-3. inq101 and crnlea denote runs of the INQUERY and SMART en­gines, META200 
denotes combination by averaging the posteri­ors obtained using the mixture model, while META900 denotes 
the combination by averaging the posterior probabilities using the Gaussian and exponential .ts assuming 
relevance is given. Thus, any difference between META200 and META900 is caused by the errors in performing 
a mixture .t to the model. LEE denotes Lee s COMBMNZ technique while the manual engine selection tech­nique 
involves selecting the best engine(s) and discarding the worst engine(s) on a per query basis using the 
average precision for that query. Manual engine selection provides an indication of the best combination 
result we can achieve. Note that both META900 and manual engine selection require relevance information 
and are only plotted to provide a baseline for understanding the limits of combi­nation. Figure 9 shows 
recall-precision plots for combining INQUERY and SMART on TREC-3 data. Precision is de.ned as the fraction 
of retrieved documents which are relevant while recall is the frac­tion of relevant documents which have 
been retrieved. The recall­precision graph is usually created by averaging over a certain num­ 12345 
number of combined engines Recall-Precision Plot 1  ber of queries -in this case 50. As the .gure shows 
META200 per­forms considerably better than either INQUERY and SMART -in fact about 6% better than INQUERY 
and 13% better than SMART. LEE is slightly better (about 1%) than META200 although the dif-0.8 ference 
is not signi.cant. META900 has an average precision about 10% better than INQUERY and clearly performs 
better than either META200 or LEE s implying that if the mixture .t could be im­ proved the technique 
would perform even better. Finally, the plot for manual engine selection clearly indicates that both 
META200 and LEE s are close to obtaining the best performance possible from combination. Figure 10 describes 
combination results for the top .ve engines in TREC-3. The x-axis is the number of engines combined while 
the y-axis is the average precision. As the plot clearly shows com­bination clearly improves the results. 
There are four graphs in the .gure. The .rst curve is the average precision of the individual search 
engines. The second plot META200 shows the combina­tion method applied to 1, 2, 3, 4 or 5 engines. As 
can be clearly seen there is a considerable improvement over using even the best search engine and overall 
the improvement seems to increase with the number of search engines combined. With the top 2 engines, 
META200 shows an improvement of 6% over the best single en­gine and using the top 3 engines, META200 
shows an improve­ment of almost 12%. LEE s COMBMNZ technique is also shown in the same graph. It s average 
precision is seen to be slightly worse than META200 but the difference is not really signi.cant. The 
performance obtained using META900 (i.e. combination with the posterior probabilities obtained with relevance 
information) is 15% better than the best single engine. Again this indicates that if the mixture .t were 
improved we could do even better. Figure 11 demonstrates that this approach also works for other languages. 
The .gure shows the combination results for INQUERY and SMART when indexing a Chinese database. The data 
in this case is from TREC-6. As can be clearly seen, combination us­ing both META200 and LEE s COMBMNZ 
show an improvement over either engine. However, in this particular case the improve­ment is much less 
than that for English. Also the difference be­tween META900 and META200 is small indicating that perhaps 
we are close to the limit of what can be achieved. Combination of good search engines usually improves 
the scores. precision % 0.6 0.4 0.2 0 Figure 11: Recall precision graphs for combining inq4ch1 and 
crnlch1 with Chinese queries and Chinese databases. Data from TREC-6 Partly this re.ects the fact that 
the score distribution models .t good search engines better than poor search engines. How­ever, the combination 
of two search engines when the performance of one is substantially worse than the others leads to a result 
which can be worse than that of the best engine. This partly re.ects the well known observation that 
combining a bad classi.er with a good classi.er can lead to a result which may not be better than the 
best individual classi.er. Two search engines INQUERY (inq101) and XEROX (xerox4) were picked. On the 
basis of average precision, inq101 is ranked 4th while xerox4 is ranked 35th among 40 engines in TREC-3. 
The average precision of inq101 is more than twice that of xerox4. Figure 12 clearly shows that INQUERY 
performs much better than the XEROX engine. The combination META200 is much better than XEROX but worse 
than INQUERY. LEE s is slightly better than META200 but still worse than INQUERY. The best option in 
such cases is to avoid combination. Recall-Precision Plot about equally well and and both are better 
than META200 (straight averaging of posterior probabilities). The average precisions of META206 and META207 
are essentially the same as LEE s. We have also carried out other experiments with other engines all 
of which demonstrate that engine selection can be done using the models of score distributions discussed 
here. We note that both META206 and META207 are still worse than using INQUERY alone indicating that 
there is further scope for improving the en­gine selection procedure. Of course, this also implies that 
when one search engine performs much worse than another it may be best not to use the poor search engine. 
Recall-Precision Plot 1 0.8 recall % 0.6 5.1 Automatic Engine Selection The previous example shows 
that if we could have somehow .g­ured out that we need to pick INQUERY as the best possible engine for 
every query then the performance would improve considerably. The ability to model and compute the relevant 
and non-relevant distributions allows us to develop techniques to automatically se­lecting engines on 
a per query basis. Here, we examine two such approaches. The .rst approach essentially tries to ensure 
that the distance be­tween the mean of the normal distribution and the point at which the densities intersect 
is large (all distributions are obtained using the mixture model). The idea is that if this distance 
is large then it will be easier to separate the relevant and non-relevant documents. If the distance 
is less than a threshold, the engine is discarded for that query. The posterior probability of all engines 
selected (i.e. not discarded) for a particular query are averaged to obtain docu­ment probabilities as 
before. If all engines for a particular query are below the threshold, then the one with the highest 
posterior proba­bility is selected. The threshold is selected based on empirical data to be 0.16. The 
results for this technique are labelled as META206 in Figure 13 The second approach uses the posterior 
probabilities obtained using Bayes rule from the mixture components. In some situa­tions the maximum 
of the posterior probability is quite small. A posterior of 0.5 indicates that the relevant and non-relevant 
distri­butions weighted by the priors are of equal magnitude. In other words a posterior of 0.5 indicates 
the point at which the exponen­tial and Gaussian densities intersect after weighting by the prior. It 
is clear that a good engine should preferably have a higher pos­terior. Empirically if the posterior 
for a particular engine and a particular query was less than 0.7 then that engine was regarded as poor 
and discarded for that particular query. If both engines had maximum posteriors greater than 0.7 then 
they were averaged. If neither engine had a maximum posterior greater than 0.7 both were again averaged 
and combined -this is plotted as META207. Figure 13 shows the results of combining two engines whose 
performance is very different. We again use inq101 and xerox4. As is clear from Figure 13, META206 and 
META207 perform precision % 0.4 0.2 0  Figure 13: Recall precision graphs for combining inq101 and xerox4 
by automatically selecting the best engine using proba­bilities and distributions. 5.2 Discussion of 
Combination Results The results above show that the mixture modeling performs as well as the best current 
techniques (Lee s) available for combina­tion. Although there is scope for a slight improvement it is 
also clear that we are approaching the limits of the best performance we can achieve. Lee s COMBMNZ technique 
performs surprisingly well. In the case where INQUERY and SMART are combined we note that for many queries 
INQUERY and SMART have distributions which are very similar. In such a situation, their posterior distributions 
will also look remarkably similar and hence averaging is a good strat­egy. Since COMBSUM involves computing 
a document score by just adding the scores for all engines which .nd that document, it will produce the 
same ranking as averaging and hence it will also be good. COMBMNZ involves multiplying COMBSUM by the 
number of engines which found that document and hence it will also produce good results. In this particular 
situation COMBMNZ involves essentially combining the posterior probabilities without having to do the 
mixture modeling. However, in the more general case, the good performance of COMBMNZ is hard to explain. 
The model for combination proposed here is more intuitively sat­isfying for a number of reasons. First, 
it combines engines in a natural way using probabilities and is therefore easier to explain. Second, 
it indicates where improvements can be made for better performance. Third, the same technique may be 
used for combin­  
			