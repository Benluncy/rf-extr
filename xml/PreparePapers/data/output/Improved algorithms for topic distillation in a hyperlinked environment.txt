
 Improved Algorithms for Topic Distillation in a Hyperlinked Environment Krishna Bharat Digital Equipment 
Corporation Systems Research Center 130 Lytton Avenue Palo Alto, CA 94301 bharat@pa.dec.com Abstract 
This paper addresses the problem of topic distillation on the World Wide Web, namely, given a typ- ical 
user query to find quality documents related to the query topic. Connectivity analysis has been shown 
to be useful in identifying high quality pages within a topic specific graph of hyperlinked documents. 
The essence of our approach is to augment a previous connectivity anal-ysis based algorithm with content 
analysis. We identify three problems with the existing approach and devise al- gorithms to tackle them. 
The results of a user evaluation are reported that show an improvement of precision at 10 documents by 
at least 45% over pure connectivity anal-ysis. 1 Introduction Search services on the World Wide Web 
are the informa- tion retrieval systems that most people are familiar with. As argued by Marchionini 
[23] end users want to achieve their goals with a minimum of cognitive load and a max- imum of enjoyment. 
Correspondingly, in the context of Web searches we observe that users tend to type short queries (one 
to three words) [2, 91, without giving much thought to query formulation. Additionally, it is often the 
case that users themselves are unclear about their information need [12] when framing the query. Since 
de-termining relevance accurately under these circumstances is hard, most search services are content 
to return exact query matches -which may or may not satisfy the user s actual information need. In this 
paper we describe a system that takes a some- what different approach in the same context. Given typ-ical 
user queries on the World Wide Web (i.e., short queries), our system attempts to find quality documents 
related to the topic of the query. Note that this is more general than finding a precise query match 
and not as ambitious as trying to exactly satisfy the user s informa- tion need. The latter is often 
hard to do since most short queries do not express the need unambiguously. In cases where the query is 
ambiguous, i.e. there is more than one possible query topic, our goal is to return relevant Permission 
to make digital/hard copy of all or part of this work for personal or classroom use is granted without 
fee provided that copies are not made or distributed for profit or commercial ad-vantage, the copyright 
notice, the title of the publication and its date appear, and notice is given that copying is by permission 
of ACM. Inc. To CODY otherwise. . to rermblish. to mast on servers or I to redistribute to lists, requirw 
prior specific &#38;?rmission and/or fee. SIGIR 98, Melbourne, Australia @ 1998 ACM 1-58113-015-5 8/98 
$5.00. Monika R. Henzinger Digital Equipment Corporation Systems Research Center 130 Lytton Avenue Palo 
Alto, CA 94301 monika@pa.dec.com documents for (some of) the main query topics. This ex-cludes minor 
interpretations of the query and encourages users to type in queries that are representative of the topic 
they seek to explore. We call the process of finding quality documents on a query topic, topic distillation. 
The situation on the World Wide Web is different from the setting of conventional information retrieval 
sys-tems for several reasons. The main reasons are: . Users tend to use very short queries (1 to 3 words 
per query [2,9]) and are very reluctant to give feed- back. . The collection changes continuously. . 
The quality and usefulness of documents varies widely. Some documents are very focused; oth-ers involve 
a patchwork of subjects. Many are not intended to be sources of information. . Preprocessing all the 
documents in the corpus re-quires a massive effort and is usually not feasible. However, there is an 
additional source of information that an information retrieval system on the World Wide Web can harness: 
namely, the opinions of people who create hyperlinks. A simple approach to finding quality docu-ments 
is to assume that if document A has a hyperlink to document B, then the author of document A thinks that 
document B contains valuable information. Thus, using the in-degree of a document as a measure of its 
quality is a first heuristic. However, transitivity is worth exploiting as well. If A is seen to point 
to a lot of good documents, then A s opinion becomes more valuable, and the fact that A points to B would 
suggest that B is a good doc-ument as well. Using this basic idea, Kleinberg [21] developed a connectivity 
analysis algorithm for hyperlinked environ-ments. Given an initial set of results from a search scr-vice, 
the algorithm extracts a subgraph from the Web containing the result set and its neighboring documents. 
This is used as a basis for an iterative computation that estimates the value of each document as a source 
of rel- evant links and as a source of useful content. While this algorithm works well for some queries, 
it performed poorly in several of our test cases. To better understand its behavior we built a visualization 
tool. This enabled us to discover three problems with connectivity analysis as suggested by Kleinberg, 
i.e. a links-only approach: Mutually Reinforcing Relation-ships Between Hosts (where certain arrangements 
of doc- uments conspire to dominate the computation), Auto-matically Generated Links (where no human 
s opinion is expressed by the link), and Non-relevant Documents (where the graph contains documents 
not relevant to the query topic). In this paper we present several techniques for tackling these three 
scenarios. The last problem is by far the most common, and our general solution is to use content analysis 
to help keep the connectivity-based computation on the topic. We compare the performance of 10 algorithms 
with the basic Kleinberg algorithm on 28 topics that were used previously in [6]. The best approach increases 
the preci- sion over basic Kleinberg by at least 45% and takes less than 3 minutes. This running time 
is dominated by the time to fetch 130 documents from the World Wide Web and can be reduced considerably 
when term vectors for the documents are available. The paper is structured as follows. Section 2 describes 
the connectivity analysis algorithm, its implementation, and the problems we encountered. Section 3 shows 
how be address the first problem, Section 4 gives algorithms addressing the other two problems. In Section 
5 we eval- uate the different algorithms. Section 6 presents consid-erably faster algorithms that additionally 
improve preci- sion. In Section 7 we discuss related work. 2 Connectivity Analysis The goal of connectivity 
analysis is to exploit linkage in- formation between documents, based on the assumption that a link between 
two documents implies that the doc- uments contain related content (Assumption i), and that if the documents 
were authored by different people then the first author found the second document valuable (As-sumption 
ii). In 1997 Kleinberg [21] published an algo-rithm for connectivity analysis on the World Wide Web which 
we describe next. 2.1 Kleinberg s Algorithm The algorithm computes two scores for each document: a hub 
score and an authority score. Documents that have high authority scores are expected to have relevant 
con-tent, whereas documents with high hub scores are ex-pected to contain links to relevant content. 
The intuition is as follows. A document which points to many others is a good hub, and a document that 
many documents point to is a good authority. Transitively, a document that points to many good authorities 
is an even better hub, and similarly a document pointed to by many good hubs is an even better authority. 
In the context of a user query the algorithm first constructs a query specific graph whose nodes are 
doc-uments. Then it iteratively computes the hub and au-thority scores for the nodes. The graph is constructed 
as follows. A start set of documents matching the query is fetched from a search engine (say the top 
200 matches). This set is augmented by its neighborhood, which is the set of documents that either point 
to or are pointed to by documents in the start set. In practice, since the in-degree of nodes can be 
very large, Kleinberg recommends considering at most 50 predecessors of a document. The documents in 
the start set and its neighborhood together form the nodes of the neighborhood graph. Hyperlinks be-tween 
documents not on the same host form the directed edges. Links within the same host* are assumed to be 
by the same author and hence are not indicators of value. We assume throughout the paper that the host 
can be deter- mined from the URL-string. The computation of hub and authority scores is done as follows. 
(1) Let N be the set of nodes in the neighborhood graph. (2) For every node n in N, let H[n] be its 
hub score and A[n] its authority score. (3) Initialize H[n] and A[$ to 1 for all n in N. (4) While 
the vectors N and A have not converged:  For all n in N, A(n] := Ccn,,njEN H[n ] (5) For all n in N, 
H[n] := xCn,n,jEN A[n ] Normalize the H and A vectors. (f-3) (7) Kleinberg [21] proved that the H and 
A vectors will even- tually converge, i.e., that termination is guaranteed. In practice we found the 
vectors to converge in about 10 iterations. The documents are then ranked by hub and authority scores 
respectively. Note that the algorithm does not claim to find all relevant pages, since there may be some 
that have good content but have not been linked to by many authors. In our evaluation of different algorithms 
we use Kleinberg s algorithm [21] as our baseline, which we call base. 2.2 Implementation To determine 
the neighborhood of the start set the al-gorithm needs to follow links that point in and out of these 
documents. Outlinks are easily obtained by fetch- ing the document. One way of obtaining inlinks is to 
use AltaVista queries of the form link : u, which returns a list of documents that point to the URL u. 
This was the implementation used by [21]. In our queries, the neighborhood graph contained on the order 
of 2000 nodes. The running time is completely dominated by the time it takes to fetch the documents. 
With a download rate of 1 document per second queries takes about 30 minutes. To get fast access to linkage 
information within the World Wide Web, we built a Connectivity Server [4] that provides linkage information 
for all pages indexed by the AltaVista search engine. The server provides a special-ized interface to 
compute the neighborhood graph for a set of URLs. This speeds up the graph construction to under half 
a minute and enables us to handle queries in almost real time. We ran the commutation for 150 iterations 
in each case, although the system seemed to converge after 10. 2.3 Problems Encountered We found that 
the algorithm as described above did not work well in all cases. Obviously, if there are very few edges 
in the neighborhood graph not much can be in-ferred from the connectivity. We built a neighborhood visualization 
tool which allowed us to trace the compu-tation and discover three other reasons why the algorithm tends 
to fail: 1. Mutually Reinforcing Relationships Between Hosts: Sometimes a set of documents on one host 
point to a single document on a second host. This drives up the hub scores of the documents on the first 
host and the authority score of the document on the sec-ond host. The reverse case, where there is one 
docu- ment on a first host pointing to multiple documents on a second host, creates the same problem. 
Since we make the (simplifying) assumption that the set of documents on each host was authored by a single 
author or organization, these situations give undue weight to the opinion of one person. 2. Automatically 
Generated Links: Web documents generated by tools (e.g., Web authoring tools, data-base conversion tools) 
often have links that were inserted by the tool. For example, the Hypernews system which turns USENET 
News articles into Web pages, automatically inserts a link to the Hy- pernews Web site. In such cases 
Assumption ii, namely that a human s opinion is represented by the link, does not apply. 3. Non-relevant 
Nodes: We often find that the neigh- borhood graph contains documents not relevant to the query topic. 
If these nodes are well con-nected, the topic drift problem arises: the most-highly ranked authorities 
and hubs tend not to be about the original topic. For example, when running the algorithm on the query 
jaguar and car the computation drifted to the general topic car and returned the home pages of different 
car manufacturers as top authorities, and lists of car manufacturers as the best hubs.    3 Improved 
Connectivity Analysis As discussed in the previous section we identified three problems with Kleinberg 
s algorithm. In this section we address problem 1, mutually reinforcing relationships be-tween hosts. 
The next section addresses problems 2 and 3. Mutually reinforcing relationships between hosts give undue 
weight to the opinion of a single person. Ideally we would like all the documents on a single host to 
have the same influence on the document they are connected to as a single document would. To achieve 
this we give fractional weights to edges in such cases: If there are k edges from documents on a first 
host to a single document on a second host we give each edge an authority weight of l/k. This weight 
is used when computing the authority score of the document on the second host. If there are 1 edges from 
a single document on a first host to a set of documents on a second host, we give each edge a hub weight 
of l/l. Additionally, we discard isolated nodes from the graph. This leads to the following modified 
algorithm: (4) While the vectors H and A have not converged: For all n in N, (5)  H[n ] x auth_wt(n 
,n)  AIn1 := C(n ,n)EN For all n in N, A[n ] x hub_wt(n, n ) (6)  Hbl := C(?%,d)EN Normalize the H 
and A vectors. (7) In the appendix we prove that the H and A vectors con-verge, i.e., that the algorithm 
terminates. This modified algorithm was effective in eliminating the mutually reinforcing relationship 
problem in all the cases where we had encountered it. In our evaluation we call this improved algorithm, 
imp. 4 Combining Connectivity and Content Anal-ysis In this section we combine content analysis using 
tra-ditional Information Retrieval techniques with improved connectivity analysis to tackle topic drift. 
There are two basic approaches both assuming we can determine the relevance of a node to the query topic: 
(i) eliminating non-relevant nodes from the graph, and (ii) regulating the influence of a node based 
on its relevance. We have also experimented with combinations of these techniques. These mostly address 
problem 3 since they discard or pe- nalize nodes that do not belong to the topic. However, in practice 
they also seem to solve problem 2, since au-tomatically generated links often point to pages outside 
the topic. 4.1 Computing Relevance Weights for Nodes The relevance weight of a node equals the similarity 
of its document to the query topic. We describe next how to compute the similarity score of a document 
D. As men-tioned before, the query topic is broader than the query itself. Thus matching the query against 
the document is usually not sufficient. Instead we use the documents in the start set to define a broader 
query and match every document in the graph against this query. Specifically, we consider the concatenation 
of the first 1000 words from each document to be the query, Q and compute similarity(Q, D). In our implementation, 
since queries are long and the document vocabulary tends to be varied we use term frequency weighting. 
We use cosine normalization in weighting both the query and the documents since the deviation in term 
vector lengths is large. See Salton and Buckley [28] for a discussion of weighting options. Specif-ically, 
  Cf&#38;i, x Wj) similarity(Q, Dj) = JCf=,(w# x XI=, ( ij) where wig = freq,, x IDF,, wij = freqij 
x IDF,, freqi, = the frequency of the term i in query Q, f reqil = the frequency of the term i in document 
Dj , IDF, = an estimate of the inverse document frequency of term i on the World Wide Web. 4.2 Pruning 
Nodes from the Neighborhood Graph There are many approaches one can take to use the rel-evance weight 
of a node to decide if it should be elimi-nated from the graph. We investigated approaches based on thresholding 
the relevance weight. All nodes whose weights are below a threshold are pruned. Thresholds are picked 
in one of three ways: Median Weight: The threshold is the median of all the relevance weights. Start 
Set Median Weight: The threshold is the me-dian of the relevance weights of the nodes in the start set. 
Fraction of Maximum Weight: The threshold is a fixed fraction of the maximum weight. WC used max/lO in 
our experiments. On the pruned graph we run the imp algorithm. We call the corresponding algorithms: 
med, startmed, and maxbyl0. 106 4.3 Regulating the Influence of a Node This approach seeks to modulate 
how much a node influ-ences its neighbors based on its relevance weight. If W [n] is the relevance weight 
of a node n and A[n] the author- ity score of the node we use W[n] x A[n] instead of A[n] in computing 
the hub scores of nodes that point to it. Similarly, if H[n] is its hub score we use W[n] x H[n] in-stead 
of H[n] in computing the authority score of nodes it points to. This reduces the influence of less relevant 
nodes on the scores of their neighbors. Combining the previous four approaches with the above strategy 
gives us four more algorithms, which we call: impr, medr, startmedr, and mazbylor.  4.4 Implementation 
Unlike the previous implementation where it sufficed to get the graph from the Connectivity Server, in 
this case we need to fetch all the documents to do content analysis. To build term vectors we eliminate 
stop words and use Porter stemming [27]. For IDF weights, since we know of no source of IDF weights for 
the Web and of no official representative collection, we had to build our own col-lection. Hence we used 
term frequencies measured in a crawl of 400,000 Yahoo! [30] documents in January 1997. 5 Evaluation Traditionally, 
ranking schemes are evaluated by comput- ing precision and recall on a pre-labeled corpus, such as the 
TREC [17] collection. We compare our algorithms based on precision and relative recall at 5 and 10 doc- 
uments. We used relative recall instead of recall since we do not know the number of relevant documents 
for a topic on the Web, or even in the Neighborhood Graph. We used a set of 28 queries previously used 
by [6] in comparing the rankings from their version of Kleinberg s algorithm with category listings on 
the Web. Table 1 gives a listing of the queries ordered by the number of results returned by AltaVista 
in December 1997 for each query, which can be taken as a measure of the topic s popularity on the Web. 
We ran our 8 algorithms and base on each of the queries and considered documents with the top 14 hub 
and authority scores. The set of top authority documents from all the algorithms were pooled together 
randomly and independently rated for relevance by 3 volunteers. The ratings were then combined and the 
final relevance rating for each document was decided by majority vote. A similar rat,ing was done for 
the top hub documents. In each case the subjects were instructed to determine whether the document was 
not relevant to the topic (case i), relevant to the the topic (case ii), or both relevant to the topic 
and a good example of a hub or an authority as the case may be (case iii). Only documents classified 
under case iii by a majority of reviewers (i.e., 2 out of 3) were considered relevant for the purposes 
of computing precision and relative recall. The subjects were encouraged to follow links and browse the 
document s neighborhood before deciding on a rating. Specifically, the subjects were told: You have some 
latitude in deciding what constitutes a good hub or authority. A good hub generally has useful links. 
A good authority is generally a document with use- ful content. If a document with little content has 
links to relevant content-rich documents on the same site (e.g., if it is a Table of Contents page), 
it may still count as an Abb. Query 1 AL ct. 1 t, 1 th fl Y LD lyme dis&#38;se 12123 16 12 BI bicycling 
16956 26 24 FH field hockev I 20410 33 22 Y AP amusement park 25202 19 19 TT table tennis 27409 
12 20 RC rock climbing 31286 27 30 cv computer vision 35762 26 23 SH shakesneare 41885 13 15  Table 
1: Queries used in sorted order of AltaVista re-sult set size in December 1997. The table also lists 
fox each topic the total number of relevant documents that appeared in the top-10 ranking of at least 
one algorithm. For authority rankings this is listed as t, and for hub rankings as th. authoritative 
page. You might instead choose to regard all good hubs as good authorities. Whatever policy you adopt 
please be consistent. Two issues came up: (i) Sometimes queries had more than one interpretation. For 
instance, some reviewers re-stricted architecture to building related topics, whereas others included 
computer architecture as well. (ii) There was disagreement among the reviewers on whether to in- clude 
pages on the topic containing very localized infor-mation, e.g., pages on bicycling trails in New Jersey 
for the query bicycling. No rating was given in cases where documents were not accessible or were in 
a language that the subjects did not understand. To compensate for this we obtained ratings for the top 
14 documents in each ranking, and omitted the unrated documents. This gave us a list of at least 10 documents 
for each algorithm-topic pair with 3 ratings for each. We computed precision and relative recall for 
this list using the combined relevance measure described previously (relevant if placed in class iii 
by a majority of the reviewers). We computed precision at 5 and at 10 documents for each algorithm-topic 
pair, as well as average precision for specific sets of documents and all the documents combined. To 
compute relative recall in the context of a topic, we first determined t, the total number of relevant 
documents for the topic occur-ring in the top-10 ranking of at least one of the algo-rithms. Table 1 
lists values of t for the various topics (t, for authorities and th for hubs). For each algorithm, rela- 
tive recall at 5 (similarly 10) documents was computed as the number of relevant documents in the top 
5 (similarly 10) ranked documents expressed as a fraction oft. Table 2 shows the precision after the 
top 5 and 10 ranked authority documents. We classified the five queries with the smallest AltaVista result 
set size as rare, and the five the with largest result set size as popular. We also give precision values 
for the sets of rare and popular queries. Similarly, Table 3 gives precision values for hub documents. 
First, we discuss the performance in the context of authority ranking. We observe that in all cases imp, 
which eliminates mutually reinforcing relationships be-tween hosts, provides an appreciable improvement 
over base, the algorithm described by Kleinberg. Adding con-tent analysis either by pruning nodes or 
regulating the influence of nodes improves on imp, especially in the case of rare topics. Med, startmed, 
and musbyl0 all perform roughly the same and improve precision by about 10% over i7np. Regulation helps 
imp in all cases, about as much as pruning. For the algorithms that use pruning, adding regulation does 
not seem to affect precision. On both popular and rare topics the algorithms per-formed, in general, 
worse than on all topics. Precision for rare topics is in general lower than for popular topics. We conjecture 
that rare topics do not have enough con-nectivity for the algorithms to exploit, while for popular topics 
that threshold based pruning is too simplistic. In the next section we present algorithms that prune 
more selectively. One of them performs significantly better on popular topics. To summarize authority 
rankings, imp improves pre-cision by at least 26% over base; regulation and pruning each improve precision 
further by about lo%, but com-bining them does not seem to give any additional im-provement. Considering 
precision in the ranking for hubs we find as before that ,imp improves on base (by 23% or more), and 
med improves on imp by a further 10%. Regulation slightly improves imp and mazbyl0 but not the others. 
 Overall hub precisions are better than authority pre-cisions, even for base, but medr still improves 
precision by 45% over base. In general at 10 precision averaged over all topics is higher than on rare 
and popular topics. Due to the distribution of the t, and th (see Table 1) no algorithm can have a better 
relative recall at 10 than 0.65 for authorities and 0.6 for hubs. Base achieved a relative recall at 
10 of 0.27 for authorities and 0.29 for hubs. Our best algorithm for authorities gave a relative recall 
of 0.41; similarly for hubs it was 0.46 (see Table 4), i.e., we achieved roughly half the potential improvement 
by this measure. 6 Partial Content Analysis Although the content analysis based algorithms described 
in the previous section improve precision -they do so at the expense of response time. Query response 
times with imp are about half a minute, whereas content analysis of all nodes in the graph requires downloading 
roughly 2000 documents from the Web which can take about 30 minutes. Ideally, we would like to use the 
advan-tage that content analysis provides -i.e., reduction of the effect of non-relevant nodes, without 
paying the high cost of a full graph download. In this section we de-scribe two algorithms that involve 
content pruning but only analyze a part of the graph (less than 10% of the nodes). This makes them a 
factor of 10 faster than previ- ous content analysis based algorithms, supporting query response times 
of around 3 minutes, which are more tol- erable. Our two algorithms are motivated by the observation 
that not all nodes are equally influential in deciding the outcome of the improved connectivity analysis. 
Some are better connected than others and hence likely to domi- nate the computation. The new algorithms 
attempt to selectively analyze and prune if needed, the nodes that are most influential in the outcome. 
Since the act of pruning itself alters the course of the computation se- lecting the best candidates 
for pruning is problematic. We use two heuristics, degree based pruning and iterative pruning, to select 
the nodes to be analyzed. These are described in the subsections below. In both cases, as before, an 
expanded query, Q, is needed to compute the relevance weights of nodes. Pre-viously the entire start 
set was used to compute Q. With partial content analysis only a subset of the start set (30 documents 
in our implementation) is used for this purpose. These are selected by another heuristic, based solely 
on the information the Connectivity Server can provide -namely the URL and connectivity of each docu- 
ment. With some experimentation we arrived at a heuris- tic that selects nodes based on in-degree, out-degree, 
and match of the URL string with the original query. Specif-ically, we select the 30 start set documents 
that maxi-mize the value of in-degree + 2 x num_query_matches + has_out_links, where num-query-matches 
is the num-ber of unique substrings of the URL that exactly match a term in the user s query, and has-out-links 
is 1 if the node has at least one out-edge and otherwise 0. The documents selected from the start set 
are fetched and their initial 1000 words are concatenated to give Q. Each of them is then scored against 
Q and the 25th per-centile relevance weight is selected as the pruning thresh-old. The pruning threshold 
is used in the next phase (the pruning phase) to eliminate some of the influen-tial but non-relevant 
nodes in the graph. In computing similarity between the query, Q, and a document, D, a slightly modified 
formula is used from before. The weight of terms in the original query is boosted by a factor of three. 
Specifically, wirl is computed as freqi, x IDFi x 3, whenever term i is a (stemmed form) of a term in 
the user s query. This is done in the pruning phase as well. In the pruning phase a hundred nodes are 
selected from the graph by one of two heuristics, which we de-scribe next. They are matched with Q, and 
pruned if their relevance weight is below the pruning threshold. In all at most 130 documents are fetched 
and analyzed. We experimented with two partial pruning approaches: (i) Degree Based Pruing and (ii) Iterative 
Pruning. 6.1 Degree Based Pruning In degree based pruning, the in and out degrees of the nodes are 
used to select nodes that might be influen-tial. Specifically, we use 4 x in-degree + out-degree as a 
measure of influence. The top 100 nodes by this mea-sure are fetched, scored against Q and pruned if 
their score falls below the pruning threshold. After this, con-nectivity analysis as in imp is run for 
10 iterations on the pruned graph. The ranking for hubs and authorities computed by imp is returned as 
the final ranking. This algorithm is called pca0. 108 All Rare Popular At At At At At At 5 10 5 10 5 
10 base 0.52 0.46 0.24 0.18 0.36 0.4 Table All Rare Popular At At At At At At 5 10 5 10 5 10 base 0.6 
0.56 0.44 0.46 0.48 0.42  Without Regulation With Regulation Partial imp med startmed maxby impr medr 
startmedr maxbylOr pa0 peal 0.66 0.73 0.65 0.69 0.67 0.72 0.65 0.7 0.72 0.75 0.58 0.65 0.66 0.62 0.62 
0.65 0.65 0.64 0.64 0.67 0.36 0.64 0.48 0.55 0.6 0.6 0.48 0.6 0.48 0.6 0.24 0.5 0.5 0.43 0.44 0.48 0.54 
0.48 0.44 0.64 0.55 0.6 0.68 0.64 0.55 0.6 0.6 0.6 0.68 0.88 0.54 0.57 0.7 0.6 0.58 0.6 0.62 0.64 0.68 
0.8 2: Average Precision at Top 5 and 10 ranked authority documents Without Regulation imp med startmed 
moxbyl0 0.74 0.87 0.78 0.75 0.73 0.79 0.7 0.73 0.64 0.88 0.72 0.6 0.6 0.76 0.6 0.64 0.8 0.8 0.88 0.8 
0.68 0.74 0.72 0.68 Table 3: Average Precision at Top  6.2 Iterative Pruning For iterative pruning 
we use connectivity analysis itself (specifically the imp algorithm) to select nodes to prune. Pruning 
happens over a sequence of rounds. In each round imp is run for 10 iterations to get a listing of the 
(currently) best hubs and authorities. The top docu-ments by these rankings are examined in decreasing 
order of rank, alternating between the hub and the authority ranking. When examining a document, we fetch 
it and compute its relevance (if it is not already fetched) un-til either 5 documents have been fetched 
in the round or enough top ranked documents have been found to be relevant (15 in our experiments). In 
the latter case the algorithm terminates. In the former case the algo rithm terminates the round and 
starts a new round on the pruned graph, until an allotted quota of documents has been fetched (100 in 
our implementation). The rank-ings computed in the last round are returned as the best hubs and authorities 
overall. The motivation for stopping each round when 5 doc- uments have been fetched is that when combating 
topic drift by pruning, it is usually sufficient if the top ranked documents are pruned, since they tend 
to be high de-gree nodes that support others in the ranking. After this point we think it is more profitable 
to execute another round than to continue with the pruning. This algorithm is called peal. 6.3 Comparison 
with Previous Techniques In Table 2 we show precision for authority ranking by the new algorithms (pca0 
and peal) as well. Even though our main goal was to speed up the computation, pca0 performs comparably 
with the best previous algorithm and pcnl improves precision. We believe that the peal improvement comes 
from the fact that partial content analysis avoids pruning non-influential documents that are below the 
threshold in terms of relevance but are connected to and support good hubs and authorities on the topic. 
Table 3 show precisions for hub ranked documents. For all topics, pca0 and peal perform 10% worse than 
With Regulation Partial impr me&#38; startmedr maxbylOr pea0 peal 0.8 0.87 0.77 0.81 0.8 0.8 0.76 0.81 
0.69 0.76 0.74 0.71 0.8 0.88 0.8 0.8 0.56 0.72 0.76 0.8 0.66 0.76 0.53 0.63 0.8 0.8 0.72 0.8 1.0 0.68 
0.7 0.74 0.6 0.7 0.76 0.54 5 and 10 ranked hub documents medr, the best of the previous algorithms. For 
the top-ics where peal performs poorly we found that it uses up its whole quota of 100 documents, suggesting 
that a larger quota allowing for more pruning would be more successful. For example, in the case of graphic 
design peal used up its quota before it could eliminate a set of irrelevant documents containing automatically 
generated links. These links pointed to a very good authority which placed the irrelevant documents at 
the top of the hub list. In terms of relative recall, compared with the best previous algorithm, selective 
pruning performed compa-rably for authority documents, and about 10% worse for hub documents. 7 Related 
Work The ARC algorithm of Chakrabarti et al [6] also ex-tends Kleinberg s algorithm with textual analysis. 
ARC computes a distance-2 neighborhood graph and weights edges. The weight of each edge is based on the 
match between the query terms and the text surrounding the hyperlink in the source document. Regulation 
is similar to their approach but there are three differences: (i) We use an expanded query instead of 
the original query. (ii) The relevance is computed using the whole document, not just a window surrounding 
the hyperlink. (iii) The weight of an edge is either the relevance of the source document or the target 
document depending on whether authority or hub scores are being computed. Connectivity analysis of Web 
hyperlinks resembles the work on citation and cocitation analysis in the area of bibliometrics. This 
is used to discover influential publi-cations and authors with similar interests within the arti- cles 
of a certain field of study. See [22] for a discussion on applying bibliometrics to the World Wide Web. 
Citation analysis has been criticized (see [S]) as a source of system- atic bias, since members of cliquish 
communities tend to cite each other preferentially, and some authors are cited out of deference rather 
than relevance. On the Web this is less of a problem since the community is diverse and distributed, 
and the right to publish cannot be restricted by cliques. Indeed, the importance of considering refer-ential 
statistics in document selection is increased since Authorities At At 5 10 base 0.15 0.27 Without imp 
med 0.19 0.22 0.35 0.39 Regulation startmed 0.2 0.41 maxby 0.21 0.37 impr 0.2 0.38 With Regulation medr 
startmedr 0.22 0.19 0.39 0.4 maxbylOr 0.21 0.38 Partial pca0 pral 0.22 0.23 0.38 0.41 IIubs At At 5 10 
0.16 0.29 0.21 0.41 0.26 0.46 0.22 0.38 0.21 0.4 0.24 0.43 0.26 0.46 0.22 0.38 0.24 0.43 0.24 0.41 0.23 
0.4 . Table 4: Relative Recall there is no quality control on the Web. Others have used inter-document 
linkage to compute useful data on the Web as well. Pirolli et al [26] run a computation on a inter-document 
matrix, with weights derived from linkage, content similarity and usage data, to identify usable structures. 
PageRank [25] is a ranking algorithm for Web documents that uses connectivity to compute a topic-independent 
score for each document. There has been much work in IR on supporting topic exploration. This is typically 
done by letting users browse topic hierarchies that are either predetermined (e.g., Cat-a-Cone [19]), 
or dynamically constructed by clustering based on user selection (e.g., Scatter/Gather [lo], Pam-phrase 
[3]). Another approach to topic exploration is interactive query expansion where new terms are sug-gested 
to help focus the query (e.g., (24, 151). On the Web there are examples of topic hierarchies (e.g., Ya-hoo! 
[30, 16]), dynamic clustering (AltaVista s Live- Topics [5]) and query expansion (as in Excite [13]). 
The goal of topic exploration is to locate a set of documents dealing with the user s topic of interest, 
whereas topic distillation assumes such a set and finds quality docu-ments within it. Hence, topic exploration 
may be viewed as a powerful preliminary step to topic distillation. This was suggested by Hearst in [18], 
who observed that Klein-berg s algorithm does not bring forth documents that deal with less popular interpretations 
of the query. She suggests first clustering the documents to separate out the subtopics and then analyzing 
the induced subgraphs individually. Another option would be to modify the al-gorithm so that within-cluster 
edges have a higher weight than cross-cluster edges. This would allow nodes belong-ing to smaller, less 
developed topics to be supported by nodes belonging to other related topics. Finally, our approach to 
evaluating precision at a fixed number of result documents based on user rele-vance ratings seems typical 
of ranking evaluations done on the Web (e.g., search service comparisons [7, 111). Conclusions In this 
paper we showed that Kleinberg s connectivity analysis has three problems. We presented various al-gorithms 
to address them. The simple modification sug-gested in algorithm imp achieved a considerable improve-ment 
in precision. Precision was further improved by adding content analysis, with algorithms medr, pca0 and 
peal being the most promising. In our current implemen-tation pca0 and peal compute ranking with a relatively 
fast turnaround (about 3 minutes) when using the Con-nectivity Server to compute the graph. For authorities, 
peal seems to be the best algorithm overall. It provides enough of an improvement over imp to justify 
the overhead of analyzing a small set of docu- ments. For hubs, medr is the best general-purpose algo-rithm, 
but if term vectors are not available for the doc-uments in the collection, we suggest using imp. In 
each case the best algorithm improves precision over baseline Kleinberg by at least 45%. This approach 
is limited to topics that are well repre-sented and well connected on the Web. Additionally, this work 
assumes that the results of a search service query defines a good start set, which is debatable. It would 
be interesting to apply query expansion and clustering to produce a better start set. Hypertext encourages 
documents to be split up into pieces. One could argue that what users are looking for on the Web are 
good sites, containing a set of connected documents on the topic, rather than individual docu-ments. 
Connectivity based ranking schemes might serve this purpose well since they have a tendency to return 
the root document within a site, which is a good start-ing point for exploration. This happens because 
external hyperlinks most often link to the root document, even if it does not have much content. References 
AltaVista, www . altavista. digital. corn/ PI PI Anick, P.G. 1994 Adapting a Full-text Infor-mation Retrieval 
System to Computer the Trou-bleshooting Domain. Proc. of ACM SIGIR 94 pp. 349-358. Anick, P.G. and Vaithyanathan, 
S. 1997. Ex-ploiting Clustering and Phrases for Context-Based Information Retrieval. Proc. of ACM SIGIR 
97 pp. 314-323. PI Bharat, K., Broder, A., Henzinger, M., Kumar, P., and Venkatasubramian, S. 1998. The 
Con-nectivity Server: Fast Access to Linkage Informa-tion on the Web. , Proc. of 7th World Wide Web Conference, 
pp. 469-477, available as www7. conf . au/programme/fullpapers/1938/com1938.htm PI Bourdoncle, F. 1997 
LiveTopics: Recherche Vi- 151 suelle d Information sur 1 Internet. Dossiers de I Audiovisuel, La Documentation 
Francaise No. 74 (July-Aug 1997), pp. 36-38. PI Chakrabarti, S., Dom, B., Gibson, D., Kleinberg, J., 
Raghavan P., and Rajagopalan, S. 1998 Au-tomatic Resource Compilation by Analyzing Hy-perlink Structure 
and Associated Text Proc. of 7th World Wide Web Conference, pp. 65-74. Chu, H. and Rosenthal, M. 1996 
Search En-gines for the World Wide Web: A Compara-tive Study and Evaluation Methodology. Proc. of ASIS 
1996 Annual Conference. I71 Cronin, B. and Snyder, B. 1996 Citation In-dexing s Achilles Heel? Evaluative 
Bibliomet-rics and Non Coverage of the Monographic 181 110 Literature. www.slis.indiana.edu/Research/ 
cronin-achilles.html Croft, W.B., Cook, R., and Wilder, D. 1995. PI Providing Government Information 
on the Inter- net: Experience with THOMAS . U. of Mass. Technical Report 95-45. Cutting, D.R., Karger, 
D.R., Pedersen, J., and WI Tukey, J.W. 1992. Scatter/Gather: A Cluster- B<ased Approach to Browsing Large 
Document Collections. Proc. of ACM SIGIR 92. Ding, W. and Marchionini. G. 1996 Search En- PII gines for 
the World Wide Web: A Compara-tive Study and Evaluation Methodology. Proc. of ASIS 1996 Annual Conference. 
Efthimiadis, E.N. 1993 A User-Centered Evalua-tion of Ranking Algorithms for Interactive Query Expansion 
, Proc. of ACM SIGIR 93, pp. 146- 159. Excite, www. excite. corn/ 1131 Golub, G., Van Loan, C. F., Matrix 
Compu- 1141 tat,ions , Johns Hopkins University Press, Balti- more, 1989. Harman, D.K. 1988 Towards Interactive 
Query 1151 Expansion. Proc. of ACM SIGIR 88 pp. 321- 331. Infoseek, www. inf oseek. corn/ 1161 Harman, 
D.K. 1995 The TREC Conferences R. 1171 Kuhnlen and M. Rittberger (Eds.) Hypertext -Information Retrieval 
-Multimedia: Synergieef-fekte Elektronischer Informationssysteme, Proc. of HIM 95 pp. 9-28. Hearst, M. 
1997 Distinguishing between Web I181 Data Mining and Information Access: Position Statement. KDD 97 Panel 
on Web Data Min-ing. Hearst, M. and Karadi, C. 1997 Cat-a-Cone: An Interactive Interface for Specifying 
Searches and Viewing Retrieval Results using a Large Category Hierarchy. Proc. of ACM SIGIR 97, pp. 246- 
255. Karlin, S., Taylor, H. M., A first course in PO1 stochastic processes , Academic Press, London, 
1975. Kleinberg, J. 1998. Authoritative sources in a hyperlinked environment. Proc. of 9th ACM-SIAM Symposium 
on Discrete Algorithms. Also ;;;;ared as IBM Research Report RJ 10076, May PI Larson, R.R. 1996 Bibliometrics 
of the World Wide Web: An Exploratory Analysis of the Intel- lectual Structure of Cyberspace. Proc. of 
ASIS 96 Annual Conference. 1221 Marchionini, G. 1992. Interfaces for End-User (231 Information Seeking. 
Journal of the American Society for Information Science, 43(2):156-163. Magennis, M. and van Rijsbergen, 
C.J. 1997 The Potential and Actual Effectiveness of Interactive Query Expansion. Proc. of ACM SIGIR 97, 
pp. 324-332. 1241 (251 Page, L. 1997 PageRank: Bringing Order to the Web. Stanford Digital Libraries 
Working Paper, 1997-0072. 1261Pirolli, P., Pitkow, J., and Rao, R. 1996 Silk from a sow s ear: Extracting 
usable structures from the Web. Proc. of ACM SIGCHI 96, pp. 118-125. Porter, M.F. 1980 An Algorithm for 
Suffix Strip- PI ping. Program, 14, 130-137. Salton, G. and Buckley, C. 1988. Tcrm-Weighting Approaches 
in Automatic Text Re-trieval. Information Processing and Manage-ment, 24(5), 513-23. PI Vklex, Weiss 
R., Sheldon M. A., Gifford, D. K. 1997. Fast and Effective Query Refinement. Proc. of ACM SIGIR 97, pp. 
6-15. PI 1301Yahoo!. www . yahoo . corn/ 9 Appendix We prove that the connectivity analysis aglrotihm 
termi-nates. Lemma 1 The improved connectivity analysis algorithm terminates, i.e., the H and A vectors 
eventually converge. Proof. Let IN] be the size of the neighborhood graph. Let B = (bnm) be a matrix 
such that for all 1 5 n 2 IN] and 1 5 m 5 IN], b,, = authority_weight(n,m) and let C = (cnm) be a matrix 
with cnm = hub_weight(m, n) for all 1 5 ~1 5 IN] and 1 5 m < IN]. Then steps (5) and (6) can be rewritten 
as: (5) A := BH _ (6) H := CA  Let D = CB. Every entry of D is non-negative. Since every node n is 
incident to an edge, d,, > 0 for every n. Note that authority_weight(n,m) > 0 if and only if hub_weight(n,m) 
> 0, i.e., b,, > 0 if and only if cmn > 0. Thus, d,, > 0 if and only if d,,, > 0. Consider DIN . There 
exists a permutation of the rows of DIN such that the resulting matrix has the fol- lowing block-diagonal 
shape for some 1 > 0: D 0.. ......... .O El o...rJryjJ 0.. ... .o . . . . . . . . . . .   0 0m Each 
block, i.e., matrix Di with 1 5 i < 1 is square and all its entries are positive. Thus, by the Frobenius 
theory of positive matrices (see e.g. [20]), it follows that the first eigenvalue Xo(Di) > IX(Di)l, where 
X(D,) is any other eigenvalue of Di and there exists an eigenvector Xc for Xo(Di) with positive entries. 
If this condition is fulfilled then 0: . Z converges [14], where Z is a vector with each coordinate equal 
to 1. Since the value of H after the k-th iteration equals Dk . Z, it follows that the H vector and thus 
also the A vector converges. 1 III 
			