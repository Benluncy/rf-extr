
  LH(n) Figure 1: A radiance environment sphere map, Lr;s, is de.ned on the image plane shown on the 
left. Each point (s;t)in Lr;s is associated with a normal, n, and a re.ection vector, r. The H(n) normal 
speci.es the hemisphere, H(n)over which Lis de­ i .ned. tion [14]: r((ii)cos(id!i Lr((r;)fr;i;(r;r)Li((i; 
H Thus, Lris the environment (Li) modulated by the BRDF (fr). His the hemisphere above the surface, 
which varies with surface orientation (see .gure 1). Each texel in a radiance environment map captures 
the pre-integrated value for Lrfor one possible ori­entation of H. Since the radiance environment map 
is indexed by orientation, we can choose any of the storage representations used for traditional environment 
mapping. Figure 1 shows a sphere map representation. Heidrich and Seidel [13] use a similar technique 
of pre-integrating a BRDF and environment. The pre-integrated radiance environment map introduces a couple 
of restrictions for rendering. Since all surface points that have a common normal use the same re.ected 
radiance, only the lighting contribution from a distant environment can be captured, not re.ections of 
local objects or inter-re.ections of a single sur­face. Also, we are restricted to isotropic BRDFs; with 
only a single re.ected radiance stored in the map per surface normal there is no way to record BRDF variation 
with rotation around the normal. We also introduce an approximation to the true lighting equa­tion. A 
radiance environment map is computed with a single view direction, so it is incorrect to use it with 
a perspective view, where the view direction changes from pixel to pixel. While graphics hardware corrects 
for perspective-induced changes in mirror re.ec­tion direction, this correction is not always appropriate 
for the ra­diance environment map. We render perspective views anyway and accept the (usually minor) 
resulting errors as part of the price for interactivity. Obtaining Radiance Environment Maps One method 
to obtain radiance environment maps is to take pho­tographs in the desired environment of a physical 
sphere whose surface BRDF matches that of the target object. The photographs should be taken with a narrow 
.eld of view lens to approximate an orthographic projection and to minimize the re.ection of the cam­era. 
The resulting images are the radiance environment maps, with the integration done by nature. Our algorithm 
requires several ra­diance environment maps, so we require several such images along with the camera 
orientation for each. A second method is to compute the lighting integral numeri­cally given a desired 
BRDF and lighting environment. The lighting environment should be known with high dynamic range for good 
integration results. Such environments can be captured through photographs by the methods of Debevec 
[6], or rendered with a package like RADIANCE [25]. We have used six photographs or images to represent 
Li, arranged as a cube environment map [24]. Since the BRDF and the environment map, Li, are decoupled 
the lighting environment can be reused to compute Lrfor many dif­ferent surface types. Results using 
maps computed in this way are shown in .gure 3. 3 REFLECTION SPACE IBR With conventional IBR, the light 
.eld is sampled by a discrete set of images. For our algorithm, these samples are a set of radiance environment 
maps taken from different viewpoints. These maps must be warped to match a new point of view, then blended 
together. In addition to matching the viewpoint, the warping correlates features on the different maps. 
For traditional IBR, the image cor­relation may require only an af.ne or projective warp [22]. For general 
light .elds it can require gathering light rays in a variety of discontinuous patterns [17]. Since each 
point in a radiance environment map is an integra­tion of the environment and BRDF, the warp that best 
correlates fea­tures in the environment can vary from BRDF to BRDF. By choos­ing a warp that models the 
BRDF well, we can signi.cantly reduce the number of radiance environment maps required for good recon­struction. 
If the BRDF is a perfect mirror and the warp models it as a perfect mirror, we need only sample well 
enough to catch the highest frequencies in the environment. If the warp does not match the BRDF, we must 
sample well enough for the product of the high­est frequencies in the BRDF and environment. This is because 
the lighting integral is essentially a convolution of the BRDF with the environment. For BRDFs that are 
principally re.ective, we use a warp that matches the re.ection directions of the different maps. So 
a point on the source image warps to the point on the destination image that re.ects in the same direction. 
Primarily diffuse BRDFs sug­gest a warp that matches surface normal directions. We can .nd a well-matched 
warp for any BRDF that is radially symmetric about a principal direction and does not change shape across 
the surface. With such a BRDF, the same area of the environment will be inte­grated for corresponding 
points from different views. Lambertian diffuse re.ection and Phong specular re.ection both satisfy this 
restriction, but most more realistic BRDFs do not. Fortunately, since the radiance environment maps sample 
a smooth, continuous function, we can effectively handle a much wider class of BRDFs that are close to 
the symmetric ideal without requiring a large number of sample maps. For example, we have used a nu­merically 
computed BRDF with Fresnel effects and diffuse, specu­lar and backscatter components. For this BRDF, 
we use a warp that matches mirror re.ections. It works because the specular lobe is the only high-frequency 
component of the BRDF and its shape does not vary too much from texel to texel. The Fresnel effects are 
naturally handled by the method and the other components do not require a large number of sample maps 
because they are low frequency. Once the sample maps have been warped to the new viewpoint, they must 
be combined with some reconstruction .lter. Oppen­heim and Schafer [20] describe many sampling and reconstruction 
choices. For simplicity and ef.ciency, we use linear interpolation between neighboring images. The linear 
interpolation uses a spher­ical form of barycentric weights, presented in section 3.4. Thus, for any 
given viewpoint, the three nearby radiance environment maps are warped and blended to create an approximation 
to the new map (see .gure 4). 3.1 Sampling View Directions Certain environment map representations (e.g. 
cube maps) are viewpoint independent, while others (e.g. sphere maps) depend on the viewpoint. In contrast, 
each radiance environment map, whether it is stored in cube map, sphere map or another form, is correct 
for only a single viewpoint. This is because the radiance environment map captures Fresnel re.ectance 
and other view-dependent effects. As alluded to above, the view-dependence does limit the use of each 
map to only one view. This limitation is overcome by pre­computing a set of maps denoted Lr;j;j2f0:::N,1g 
at var­ious viewpoints. The unit view vectors can be thought of as points lying on a sphere. We use re.ection-space 
IBR to reconstruct the map for rendering from the Lr;jmaps, but we still require reason­able coverage 
of the sphere of possible view directions to avoid aliasing artifacts. We have used one Lrfor each viewpoint 
de.ned at the vertices of an icosahedron. This number of samples has been suf.cient for the environments 
and BRDF we have employed and is desirable because its symmetry means that each viewpoint is han­dled 
in an unbiased manner. 3.2 Map Warping Each warp is between a source map, Lr;s(from the precomputed 
set Lr;j) and a destination map, Lr;d(for the current rendering view­point). Points in these maps will 
be called psand pdrespectively. For each map point, p, there is a vector ralong the central re­.ection 
direction of the BRDF. For Phong specular or perfect mirror re.ectors, ris the geometric re.ection vector. 
For diffuse surfaces r is the surface normal. To assist in the warp, we de.ne an invertible function 
g:p!r g(p)depends on both the BRDF and the map representation. It is most easily de.ned in terms of a 
local coordinate system for each map, so we also have a transformation per map to convert the local coordinate 
system to a common global space T:r!r The composition of these functions de.nes the full warp from psto 
pd: ,1,1 pdg0T0Ts0gs(ps) dd This takes a point in Lr;s(de.ned by s and t texture coordinates for a sphere 
map representation). It is converted .rst to a 3D re.ection vector in the local coordinate system associated 
with Lr;s.This 3D vector is transformed to the global space, then to a vector in the local coordinate 
system associated with Lr;d. Finally, the result­ing vector is converted to a point in Lr;d(once again 
given by two texture coordinates if we use the sphere map representation). 3.3 Speci.c Warps We will 
derive two speci.c warp functions. Both use a sphere-map representation for Lr;sand Lr;d. The .rst is 
for BRDFs where the central re.ection direction is the surface normal. The second is for BRDFs where 
the central re.ection direction aligns with the mirror re.ection direction. For both warps, the local 
coordinate system associated with each map is aligned with the camera used to create the map. The x-axis 
points right, the y-axis points up and the z-axis points from the origin toward the camera. Thus transformations 
Tsand Tdare de.ned as 3x3 matrices with columns equal to three axes expressed in global coordinates. 
The surface normal warp uses gnormal: . . 2s,1 gnormal(s;t) . 2t,1 . 1,(2s,1)2+(2 t,1)2 g ,1 normal(x;y;z) 
(x/2 +:5;y / 2 +:5) We base the mirror re.ection warp, gmirroron the x, yand zpro­duced by gnormal: 
.. 2xz gmirror(s;t). 2yz. 2 2z,1 (x;y;z+1) ,1 ,1 g(x;y;z)g mirrornormal 22 x+y+(z+1) Since we have graphics 
hardware to do mirror re.ections with a sphere map, we modify the .nal stage of both warps to use g ,1 
. The following functional composition chains de.ne the mirror two warps: ,1 ,1 pdg0T0Ts0gnormal(ps) 
mirrord ,1 ,1 pdgmirror0Td0Ts0gmirror(ps) Performing three of these warps per texel in the target map 
for every rendered frame is expensive and impractical for an interactive application. A fast, accurate 
approximation is possible by render­ing the destination sphere map as a tessellated disk. Texture coor­dinates 
at each mesh vertex are chosen according to the warping function, and the source image is used as a texture 
while rendering the disk into the frame buffer. To account for the non-linearity of the warp functions, 
the mesh is .ner toward the edges of the disk and coarser near the center. The 3D coordinate system associated 
with the destination map changes as the view moves, but the same disk tessellation can always be used. 
The re.ection vectors, rd,also remain constant for each mesh vertex and can be precomputed. The piecewise 
linear approximation to the warp is accurate for most of the sphere map area. Because we use a sphere 
map repre­sentation, the mirror warp has a singularity at the limit of extreme grazing re.ections around 
the edge of the map the re.ection di­rection exactly opposite the view vector. The warp equation from 
Lr;sto Lr;dfails at this singularity. We can locate pdfor the singularity by warping the problem re.ection 
direction (the negated source map view vector) into the destination map. Near this point in the destination 
map, the source map will become pinched and unreliable instead of warping cleanly. We use a simple distance 
from pdin the destination map to weight our con.dence in the warped source image. This weight is used 
to fade the contribution of each source near its respective singularity. The textured disk method accelerates 
the warping operation in two ways. First, sand tare not explicitly calculated for all the Figure 2: 
Illustrated is the spherical patch de.ned by v0, v1; and v2associated with a particular point of view 
given by vd. By de.nition vdde.nes in the line of sight of the viewer and in general forms three spherical 
triangles within the larger spher­ical triangle patch. Areas a0;a1and a2represent the three weights for 
the sphere maps associated with vertices v0, v1 and v2respectively, where a1;P1;and I1are the dihedral 
an­gles used to compute a1. points on the sphere map, only at the vertices of the mesh. Second, a major 
bottleneck in performing the warp is accessing memory associated with the source and destination maps. 
We leverage the rendering and texturing hardware to solve this memory bottleneck. 3.4 Spherical Barycentric 
Interpolation Once the warps have taken place the warped images must be blended. Our interpolation scheme 
is a spherical variant of clas­sic af.ne barycentric interpolation, as de.ned in Farin [7]. Classic barycentric 
interpolation uses the ratio of the areas of triangles, we instead use the ratio of spherical triangles. 
Any given view vector, vd, will in general lie within a spheri­cal patch as illustrated in .gure 2. Each 
vertex, viof this spherical triangle is the view vector for one of the source images that have been warped 
to match vd. vdis used to form three interior spheri­cal triangles. The weight for the source image at 
vertex iis a ratio of the areas of the spherical triangle opposite viand the overall spherical triangle. 
The area of an interior spherical triangle, ai,on a unit sphere is given by the spherical excess formula 
[2]: aiai+Pi+Ii,7;i20;1;2 The dihedral angles ai, Pi,and Iiare de.ned as: aicos,1((vd®v(i,1)03)'(v(i+1)03®v(i,1)03)) 
Picos,1((v(i+1)03®vd)'(v(i+1)03®v(i,1)03)) Iicos,1((v(i+1)03®vd)'(v(i,1)03®vd)) Where ®is the normalized 
cross product and 0is an index­wrapping operator, de.ned as b,1if a0 a0b0if a;b aotherwise  4 RENDERING 
ALGORITHM This leads to a straightforward interactive rendering algorithm. Pseudo-code for the algorithm 
is given here. It leverages texture mapping graphics hardware in two ways: once to perform the warp­ing 
and blending between the sample images; and again using the generated sphere map in the .nal rendering. 
main() // Set up radiance maps and sphere geometry Gv+LoadGeodesicLocations(); v2f((0;0):::((N,1;N,1)g 
Lr;j+LoadSphereMaps( G;Li;fr); j2f0:::N,1g // Viewpoint tracking loop loop for each frame (xd;yd;vd)+ComputeViewCoordinateSystem( 
) (v0;v1;v2)+FindSubtendedTriangle( G;vd) (a0;a1;a2)+ComputeWeights( (v0;v1;v2);vd) glClearAccum( 0, 
0, 0, 0 ) // Warp and blending loop loop for each of the three vertices, i mesh +ComputeMesh( vi;vd) 
drawMesh(mesh, Lr;i) glAccum(GL ACCUM, ai) end vertex loop glAccum(GL RETURN, 1.0/(a0+a1+a2)) LoadNewSphereMap() 
RenderObject() end frame loop The interactive rendering program outlined above reads in a set of sphere 
maps at a prescribed set of geodesic locations along with the associated triangle faces. This decouples 
the interactive program from any speci.c choice of sphere map sampling view directions. 5 EXAMPLES We 
have validated our technique with several examples. One is shown in .gure 3. This example shows the recently 
introduced Mercedes-Benz S-Class automobile in an outdoor environment. The car shows an isotropic BRDF 
modeling automobile paint, com­puted using techniques similar to that found in Gondek [9]. We modeled 
the clear polyurethane coat over a paint substrate contain­ing paint particles. Using a custom of.ine 
ray tracer we directly solve the lighting integral for each point in twelve pre-computed radiance environment 
maps. Thus the under coat of the paint and the clear coat are both modeled with high .delity. Each sphere 
map takes several minutes to create. Figure 5 shows all of the textures used to render the car example. 
Our software is available online1 . On a Silicon Graphics(R) Onyx2TMIn.niteReality2TM, the interactive 
viewing program runs at a sustained frame rate of 20Hz. 6 CONCLUSION Interactive photo-realistic rendering 
is dif.cult to achieve because solving the lighting integral is computationally expensive. We pre­compute 
this integral into several view-dependent radiance environ­ment maps, each of which would allow realistic 
rendering of arbi­trary geometry from one .xed viewpoint. We dynamically create new maps for new viewpoints 
by combining the precomputed maps using an application of IBR techniques in re.ection space. The re­sulting 
algorithm is readily implementable on most texture mapping 1http://www.sgi.com/software/rsibr/ capable 
graphics hardware. This technique allows rendering qual­ity similar to that presented in Debevec [5], 
but at interactive rates and from arbitrary viewpoints. Some areas of future work to improve this technique 
are ap­parent. We would like to perform the re.ection-space IBR warp on a per pixel basis. We would also 
like to extend the range of BRDFs that can be accurately rendered. For example, we could handle arbitrary 
isotropic BRDFs with multiple high-frequency lobes in multiple passes, though admittedly with a loss 
in interactive perfor­mance. We would decompose the BRDF using a basis of symmetric lobes. Each basis 
function would be independently integrated with the environment and warped in a separate pass. We would 
also like to handle anisotropic BRDFs. A broader area of future research is opened by the idea of re.ection-space 
IBR. Traditional IBR could not have achieved these results; it is limited to rendering geometry contained 
in the source images. Traditional rendering, even using radiance environment maps, could also not have 
achieved these results; it could not pro­vide the viewpoint independence without a fast way to create 
new intermediate maps. By applying IBR to an intermediate image rep­resentation in traditional rendering, 
it becomes possible to produce new algorithms that combine the strengths of both. 7 ACKNOWLEDGMENTS 
The authors would like to thank all the individuals who helped us with the writing and development of 
this work. We are very grateful for the incredible S-Class data set from Andreas Fischer at Diamler-Chrysler 
 he and DiamlerChrysler have been invaluable colleagues and partners in our research into interactive 
surface rendering. Dur­ing the writing and review process the anonymous reviewers pro­vided valuable 
insight and suggestions. Also, Peter Shirley and Gavin Miller helped with guidance and clari.cations 
to re.ne many of the key ideas in the paper. Mark Peercy helped out with technical inspiration, ideas, 
and proofreading throughout the paper writing endeavor. We would also like to thank Gosia Kulczycka at 
General Motors for the numerous paint samples and feedback on the viabil­ity and quality of our technique. 
Greg Larson-Ward was extremely helpful with advice, technical proofreading and energy balanced synthetic 
images from RADIANCE. Thanks to Dany Galgani who provided the excellent .gure 1 illustration on very 
short notice. The cubemaps were stitched together using the Stitcher(R)software pro­vided by REALVIZ. 
And thanks to the crew at Lawrence Livermore National Lab for helping in the printing of the color plates. 
 References [1] BLINN,J. F., AND NEWELL, M. E. Texture and re.ection in computer generated images. Communications 
of the ACM 19 (1976), 542 546. [2] BRONSHTEIN,I., AND SEMENDYAYEV,K. Handbook of Mathematics.Van Nostrand 
Reinhold Company, 1985. [3] CABRAL, B., MAX,N., AND SPRINGMEYER, R. Bidirectional re.ection functions 
from surface bump maps. In Computer Graphics (SIGGRAPH 87 Proceedings) (July 1987), M. C. Stone, Ed., 
vol. 21, pp. 273 281. [4] COOK,R. L., CARPENTER,L., AND CATMULL, E. The Reyes image ren­dering architecture. 
In Computer Graphics (SIGGRAPH 87 Proceedings) (July 1987), M. C. Stone, Ed., pp. 95 102. [5] DEBEVEC, 
P. Rendering synthetic objects into real scenes: Bridging tradi­tional and image-based graphics with 
global illumination and high dynamic range photography. In SIGGRAPH 98 Conference Proceedings (July 1998), 
M. Cohen, Ed., Annual Conference Series, ACM SIGGRAPH, Addison Wes­ley, pp. 189 198. ISBN 0-89791-999-8. 
 [6] DEBEVEC,P. E., AND MALIK, J. Recovering high dynamic range radiance maps from photographs. In SIGGRAPH 
97 Conference Proceedings (Aug. 1997), T. Whitted, Ed., Annual Conference Series, ACM SIGGRAPH, Addi­ 
son Wesley, pp. 369 378. ISBN 0-89791-896-7. [7] FARIN,G. Curves and Surfaces for Computer Aided Geometric 
Design. Aca­demic Press, 1990. [8] GERSHBEIN, R., SCHR ¨ ODER,P., AND HANRAHAN, P. Textures and radios­ity: 
Controlling emission and re.ection with texture maps. In Proceedings of SIGGRAPH 94 (Orlando, Florida, 
July 24 29, 1994) (July 1994), A. Glass­ner, Ed., Computer Graphics Proceedings, Annual Conference Series, 
ACM SIGGRAPH, ACM Press, pp. 51 58. ISBN 0-89791-667-0. [9] GONDEK,J. S., MEYER,G. W., AND NEWMAN, J. 
G. Wavelength de­pendent re.ectance functions. In Proceedings of SIGGRAPH 94 (Orlando, Florida, July 
24 29, 1994) (July 1994), A. Glassner, Ed., Computer Graph­ics Proceedings, Annual Conference Series, 
ACM SIGGRAPH, ACM Press, pp. 213 220. ISBN 0-89791-667-0. [10] GORTLER,S. J., GRZESZCZUK, R., SZELISKI, 
R., AND COHEN,M. F.The lumigraph. In SIGGRAPH 96 Conference Proceedings (Aug. 1996), H. Rush­ meier, 
Ed., Annual Conference Series, ACM SIGGRAPH, Addison Wesley, pp. 43 54. held in New Orleans, Louisiana, 
04-09 August 1996. [11] GREENE, N. Applications of world projections. In Proceedings of Graphics Interface 
86 (May 1986), M. Green, Ed., pp. 108 114. [12] HE,X. D., TORRANCE,K. E., SILLION,F. X., AND GREENBERG,D. 
P. A comprehensive physical model for light re.ection. In Computer Graphics (SIGGRAPH 91 Proceedings) 
(July 1991), T. W. Sederberg, Ed., vol. 25, pp. 175 186. [13] HEIDRICH,W., AND SEIDEL, H.-P. Realistic,hardware-acceleratedshading 
and lighting. In Proceedings of SIGGRAPH 99 (Los Angeles, California, August 8 13, 1999) (Aug. 1999), 
Computer Graphics Proceedings, Annual Conference Series, ACM SIGGRAPH, ACM Press. [14] IMMEL,D. S., COHEN,M. 
F., AND GREENBERG, D. P. A radiosity method for non-diffuse environments. In Computer Graphics (SIGGRAPH 
86 Pro­ceedings) (Aug. 1986), D. C. Evans and R. J. Athay, Eds., vol. 20, pp. 133 142. [15] JENSEN,H. 
W., AND CHRISTENSEN, P. H. Ef.cient simulation of light transport in scenes with participating media 
using photon maps. In SIG-GRAPH 98 Conference Proceedings (July 1998), M. Cohen, Ed., Annual Conference 
Series, ACM SIGGRAPH, Addison Wesley, pp. 311 320. ISBN 0-89791-999-8. [16] KAJIYA, J. T. The rendering 
equation. In Computer Graphics (SIGGRAPH 86 Proceedings) (Aug. 1986), D. C. Evans and R. J. Athay, Eds., 
vol. 20, pp. 143 150. [17] LEVOY,M., AND HANRAHAN, P. Light .eld rendering. In SIGGRAPH 96 Conference 
Proceedings (Aug. 1996), H. Rushmeier, Ed., Annual Conference Series, ACM SIGGRAPH, Addison Wesley, pp. 
31 42. held in New Orleans, Louisiana, 04-09 August 1996. [18] MCMILLAN,L., AND BISHOP, G. Plenoptic 
modeling: An image-based rendering system. In SIGGRAPH 95 Conference Proceedings (Aug. 1995), R. Cook, 
Ed., Annual Conference Series, ACM SIGGRAPH, Addison Wes­ley, pp. 39 46. held in Los Angeles, California, 
06-11 August 1995. [19] MILLER,G. S., AND HOFFMAN, C. R. Illumination and re.ection maps: Simulated 
objects in simulated and real environments. In SIGGRAPH 84 Advanced Computer Graphics Animation seminar 
notes. July 1984. [20] OPPENHEIM,A. V., AND SCHAFER,R. W. Digital Signal Processing. Prentice-Hall, Englewood 
Cliffs, NJ, 1975. [21] POULIN,P., AND FOURNIER, A. A model for anisotropic re.ection. In Computer Graphics 
(SIGGRAPH 90 Proceedings) (Aug. 1990), F. Baskett, Ed., vol. 24, pp. 273 282. [22] SEITZ,S. M., AND DYER, 
C. R. View morphing: Synthesizing 3D meta­morphoses using image transforms. In SIGGRAPH 96 Conference 
Proceed­ings (Aug. 1996), H. Rushmeier, Ed., Annual Conference Series, ACM SIG-GRAPH, Addison Wesley, 
pp. 21 30. held in New Orleans, Louisiana, 04-09 August 1996. [23] VEACH,E., AND GUIBAS, L. J. Metropolis 
light transport. In SIGGRAPH 97 Conference Proceedings (Aug. 1997), T. Whitted, Ed., Annual Conference 
Series, ACM SIGGRAPH, Addison Wesley, pp. 65 76. ISBN 0-89791-896­ 7. [24] VOORHIES,D., AND FORAN, J. 
Re.ection vector shading hardware. In Proceedings of SIGGRAPH 94 (Orlando, Florida, July 24 29, 1994) 
(July 1994), A. Glassner, Ed., Computer Graphics Proceedings, Annual Conference Series, ACM SIGGRAPH, 
ACM Press, pp. 163 166. ISBN 0-89791-667-0. [25] WARD, G. J. The RADIANCE lighting simulation and rendering 
system. In Proceedings of SIGGRAPH 94 (Orlando, Florida, July 24 29, 1994) (July 1994), A. Glassner, 
Ed., Computer Graphics Proceedings, Annual Conference Series, ACM SIGGRAPH, ACM Press, pp. 459 472. ISBN 
0-89791-667-0. Figure 3: Mercedes-Benz S-Class automobile in an outdoor environment.  Figure 4: The 
outer images are source radiance environment maps for a test environment and a mirror BRDF. The next 
layer of images show each map warped to the new viewpoint with appropriate spherical barycentric weighting. 
The center im­age is the .nal generated radiance environment map. Figure 5: All of the textures used 
for .gure 3. Includes the en­vironment, source radiance environment maps for several sur­face types on 
the car, and generated MIP mapped radiance environment maps. 
			