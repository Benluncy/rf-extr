
 Permission to make digital or hard copies of part or all of this work or personal or classroom use is 
granted without fee provided that copies are not made or distributed for profit or commercial advantage 
and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, 
to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; 
1981 ACM 0-89791-029-X $5.00 algorithm is expressed as a program in that lang­uage, when the program 
is compiled, and when it is executed. Most languages have an expliticly stated dependence between consecutive 
statements in the control flow graph (e.g., PASCAL, ALGOL, FORTBAN, SNOBOL, etc.). A few languages [AcDe79], 
[ArGP78] are defined so that some types of dependence are disallowed and others are greatly reduced, 
however, making the programmer responsible for reducing de pendences imposes a difficult task on the 
program mer. High-speed computer systems with multiple functional units or pipelines commonly employ 
a lookahead control unit that breaks dependence at execution time. Studies have shown that the aver­age 
speedup due to lookahead hardware is about a factor of 2 [Kuck78]. We believe that compilers are most 
well suited to solving the problem of breaking program dependence. In fact, much work has been done on 
this prob lem in the past. The renaming of variables and code motion are traditional optimization techniques 
which result in an improved dependence graph [AhU173], [Grie71]. Transformations explicitly aimed at 
lookahead control units are also well known [A1C072]. In this paper, we assume that a dependence graph 
exists and that it is sharp (i.e., arcs are included only when necessary). We will show in Section 3 
that a number of graph transformations exist to remove arcs from a sharp dependence graph. In Section 
4, we discuss node transformations that abstract the graph in useful ways. This results in a directed 
acyclic graph that is amenable to rather straightforward code generation. 2. Source Language and Dependence 
Graphs 2.1 Source Language The transformations described in this paper will operate on programs written 
in a language con­sisting of three types of statements: assignment, &#38;oh loops, and UJ/ L(&#38; loops. 
The last two are well­known compound statements. In this paper, ~Oh loops are restricted in such a way 
that the initial value and the increment of the index variable is always one, and the loop limit is always 
a constant or a variable. These are not serious restrictions, since any more general ~Oh loop can be 
automati­cally translated into this form [BCKT79]. Only two types of variables are allowed, scalars and 
arrays of arbitrary dimension. In this paper, variable element will stand for either a scalar variable 
or an array variable element. Assignment statements are of the form <variable> = <expression>. The expression 
can be any valid Boolean or arith­metic expression, or something of the form id <Boolean-expression> 
J%Zn <expressionl>. &#38;e <expression>. The value of this last construct will be <expres­ sion > if 
<Boolean expression> is true, and 1 <expression > otherwise. 2 It is certainly theoretically possible 
to automatically translate FORTRAN or ALGOL-like programs into equivalent go -to-less programs. This 
has been discussed in [BoJa66], [ AsMa75] . However, the programs resulting from the transformations 
described in these papers could be too complex for practical purposes. A more practical approach to the 
automatic improve­ment of program structures has been taken in [Bake78] . The resulting programs in this 
last case, however, could include go to statements. The assumption we make in this paper is that if the 
source program is written in a FORTRAN-or ALGOL-like language, some sort of preprocessor will attempt 
to translate it into the above lan guage but without going to the extreme of pro­ducing unduly complex 
programs. The translator may then act only on segments of the program pro­duced by the preprocessor, 
leaving the rest of the program as it is. This is similar to the approach taken in the PARAFRASE system. 
Our experience indicates that in most programs a large percentage of the code can be translated into 
well structured, go to-less code.  2.2 Dependence Graphs The main tool we will use in the translation 
process is the dependence graph. This is a di graph whose nodes represent program components, and whose 
arcs are one of five different types. A dependence graph can be built at different levels of abstraction; 
we will discuss this later in the section. For now, we will assume that a graph node represents one of 
the three types of program components: assignment statements, 40Z100P headers, and Wh&#38;2 loop headers. 
We will assign labels to all the program components. These labels will be of the form Ai for assignment 
statements, Fi for ~o)t loop headers, and (Ui for w/ti&#38; loop headers. An example of a program and 
its depend ence graph is given in Fig. 2 (a) . A graph arc represents one of the five pos­sible relations 
between the program components. We now proceed to define these five relations, starting with the loop 
dependence relation. Definition A program component C (either an assignment statement or a loop header) 
is said to be loop dependent on a loop header L(either a ~Oh loop header or a dd?fl loop header) if C 
is embed­ded in the loop statement whose header is L, or if C=L. In the text we will denote this dependence 
as L 6L C, and in the dependence graph it will be represented by arcs of the form shown in Fig. l(a). 
 In Fig. 2, the reader can find examples of the loop dependence relation. We now proceed to dis cuss 
the next three types of relations. Definition Assume a component C and n different loop headers Ll, . 
. . . Ln, such that (1) Li &#38;LC  i=l, . ..jn (2) Li (SL Li+l isl, . . ..n l (3) C is not loop dependent 
on any other loop header.1  An instance C(kl, . . . . kn) of a component C is de­ fined as the component 
C when for 1 ~ i sn the loop whose header is Li is executing its ki-th iteration. Notice that when Li 
is a ~o,h loop header, ki will be the value of its index variable. This is because we have restricted 
the initial value and the increment in ~ok loops to one. When all L Ln are fjofi loop headers, (kl, . 
. . . kn) 1  is called an index set.  Component instances have two sets of variable elements associated 
with them: a set of inputs and a set of outpute. The set of inputs are those vari­able elements fetched 
byjthe component instance, and the set of outputs are those variable elements modified by it. When the 
component is a W~L loop header, the set of outputs is empty, and the set of inputs is given by the Boolean 
expression in the header. When the component is a 40X loop header, say F, the instance F(..., 1) (i.e., 
F at the first iteration of the loop of which it is a header) has the index variable as output, and the 
loop limit as input if it is a variable. The instances F(..., k), k > 1, also have the index variable 
as input. For an assignment statement the set of inputs is deter­mined by the expression, and the set 
of outputs by the variable on the left-hand side. Notice that the set of outputs of a program component 
has always only one element. Definition Consider two, not necessarily dis­tinct components Cr and Cs 
and one instance of each, Cr(z) and c~~), such that cr(~) is executed before Cs(~) in the proper serial 
execution of the program. We say that (a) Cs (~) is output dependent on Cr (~), denoted Cr(~) 6° Cs(~) 
iff they have the same output variable element. (b) Cs(~) is antidependent on Cr(~), denoted Cr(~) 6A 
Cs(~), iff the output variable ele­ ment of Cs(~) is an input variable element of Cr(z) . (c) Cs(~) is 
flow dependent on Cr(;), denoted Cr(~) 6 Cs(~) iff the output variable element of Cr(;) is an input variable 
element of Cs(~), and there is no other instance Ct(~) executed after Cr(~) but before Cs(~) such that 
Cr(~) 6° Ct(~). (Intuitively, the val~e computed by Cr(i) is actually used by Cs(j).) A program component 
Cr is said to be output, anti, lNotice that if C ia a loop header, then C = L n or flow dependent on 
component C iff there exiet s 7I and ~ such that Cr(~) is, respectively, output, anti, or flow dependent 
on Cs(~). The arcs repre­ senting the previous three relations are given in Figs. l(b)-(d). The presence 
of array variables poscsparticu lar problems in the computation of the previous three relations. For 
example, assume two assign ment statements A and A2 such that an array vari­ 1 able V appears on the 
left-hand side of Al and on the right-hand side of A2. To determine whether Al 6 A2, we need to determine 
whether for some instances Al(y) and A2(~), with Al(y) executed before A2(~), the element of V modified 
by A1(~) is the same as the element of V fetched by A2(~). If the subscript of V is a constant in both 
state­ments, this is a trivial task. The other extreme is when it is not possible to make such a determi­nation 
at compile time because the subscript of V is a function of the program input. In this case, we have 
to be conservative and assume that the flow dependence relation holds in order to guar­antee the correctness 
of our transformations. An intermediate case is when the subscript of V in Al(z) is a (possibly multidimensional) 
function ~(~), and A2(~) a function ~(~). U. Banerjee [Bane76], [Bane79]has developed eff~cient algo­rithms 
to determ~ne whether f(~) = g(~) for some ~ and ~ when both f and ~ are polynomials (which is often the 
case). We do not know of any efficient ~lgori_thm to make such a determination when any of f or g is 
a more general nonlinear function. In such cases, we are again conservative and assume that the flow 
dependence relation holds. Another problem is caused by the fact that some instructions may be executed 
conditionally; in this case, as before, we have to be conservative and assume the dependence when in 
doubt. The fifth relation between components is that of input dependence. Definition A component Cl ie 
said to be ~ dependent on another component C2, denoted C2 61 Cl, iff the same variable name appears 
as input to both I Cl and C2. Notice that the 6 relation is sym metric. Input dependence is represented 
by arcs like the one shown in Fig. l(e). For some of the transformations described later, we will need 
more information than that pro­vided by the dependence graph as described. This additional information 
will be conveyed by the internal flow graph which describes the internal etructure of each component. 
The internal flow graph of a Wh&#38;2 header will be the syntax tree of the Boolean expression in the 
header. For an assignment statement, the internal flow graph will be a tree with the left-hand side variable 
as root and the syntax tree of expression as the only sub tree connected to it. For the arcs in the internal 
 209 flow graph, we will use the same type of arc used to represent flow dependence (Fig. l(b)) since 
the concepts represented in both cases are the same. In uJ/L&#38;? headers and assignment statements, 
this arc points towards the root. Example The W&#38;&#38; header uJh.i&#38;? (A(I) ~ C) v (B < 1) has 
the following internal flow graph, The assignment statement ci-~~B~h~fiA+l (?IA(?c has the following 
internal flow graph The internal flow graph of a dote header of the form ~CJk <index-variable> = 1 ZO 
<limiL> is as follows 1 <limit> A <index variable>  22!s where the I operator performs all functions 
of the ~Oh header, like assigning one to the index vari­able the first time it is executed, and adding 
one to the index variable and comparing the result with the limit on subsequent executions. The nodes 
in the internal flow graph repre senting variables or constants are called atoms. Because of this, a 
dependence graph including the internal flow graph is said to be represented at the atomic level. In 
such a graph, the data depen dence arcs (anti, output and flow) will emanate and arrive at the atoms 
that cause these dependence. In Fig. 3, we show part of the dependence graph at the atomic level for 
the program in Fig. 2(a). Later in the paper we are going to treat some compound statements as a single 
unit. For this purpose, we will name a compound statement with the label of the statement header. This 
means that such a label will have two functions; however, in the text the specific meaning of the label 
will al­ways be clear from the context. In the graphic representation, a node representing a whole compound 
statement will be represented by two concentric circles; such nodes will be called compound nodes. The 
concepts of instance, dependence, sets of in­ pute, and seta of outputs can be very easily ex­ tended 
to deal with compound statements. In this paper, however, we will rely on the intuition of the reader 
and will not define such concepts. A final comment. Since dependence graphs may become quite complex 
when all arcs are drawn, we will represent only those arcs of interest in the examples discussed in the 
rest of the paper. 3. Arc Transformations In this section, we present some source pro­gram transformations 
that will modify the depen­dence graph by either removing arcs or breaking cycles. Theee transformations 
are renaming, ex­pansion, node splitting, and forward substitution. 3.1 Renaming Sometimes scalar or 
structured variables are used for different purposee at different points in a program. This is done sometimes 
to increase the readability of the program and often to decrease memory requirements. This approach is 
adequate for sequential programming. However, the use of the same memory location for different purposes 
could impose unnecessary aequentiality constraints on parallel programs. The renaming transformation 
will assign different names to different usea of the same variable, and as a consequence some out­put 
dependence arcs and some antidependence arcs will be removed from the dependence graph of the program. 
Example 3.1 The program shown in Fig. Z(a) uses the variable A in three statements inside the ~o&#38; 
loop; this introduces a large number of arcs in the dependence graph (Fig. 2(b)). The variable A can 
(1) be replaced by two variables, A and A(2), as shown in Fig. 2(b). This eliminates several Out­put 
dependence and antidependence arcs.  We now present an algorithm for renaming scalar variables. A powerful 
algorithm for re­naming structured variables is an open problem. Renaming Algorithm for Scalar Variables 
Assuminx -, a program, P, and a scalar variable. sayA, in P. [1] Build G, the dependence graph of P at 
the atomic level. [2] Consider G , the subgraph of G consisting of the intercomponent flow dependence 
area only (i.e., we drop all other arcs in eluding the flow dependence arcs in the internal flow graph). 
Find the connected components of G where A appears. Assume there are k such components C 1 C2 ... Ck 
. (1) [3] Introduce k different variable names A , A(2) A(k) ,..., , none of them used in P. For ~ ~ 
i ~.k, replace the occurrences of A in CibyA(l) . 210 Example 3.2 Part of the dependence graph G for 
the program in Fig. 2 (a) is shown in Fig. 4. Since there are two connected components involving A, we 
(1) introduce two new variables, A and A(2) to obtain the program in Fig. 2(b). The concept of scalar 
renaming has been known for a number of years [AhU173]. 3.2 Expansion Expansion is a transformation 
that is not as well known as renaming (though it is implemented in the compilers for both the Burroughs 
BSP and the CRAY-1), but is of prime importance in compiling for parallel machines. The object of expansion 
is to take a variable that was used inside a ~OZ loop and to change it into a higher dimensional array 
(or other suitable data aggregate). Like renaming, this process reduces the number of arcs in the de­pendence 
graph. In this case, this is achieved by giving each iteration of the {OE loop its own set of locations. 
Example 3.3 The dependence graph in Fig. 2(b) includes many output dependence and antidependence (1) 
 arcs because of the scalar va~iables A , A(2), and Y. If these variables are expanded into arrays by 
the algorithm described below, we obtain the program in Fig. 2(c) whose dependence graph is much simpler. 
w We now describe an algorithm for the expansion of scalar variables. To this end, we will need three 
definitions. Definition A component or compound statement C is said to be directly @l loop dependent 
on a @fi loop header F, denoted F ;L C, iff (1) F-dL C, and (2) there is no other {OZ loop header F 
such  that F6L F tiL C. Definition A sequence of ~ofi loop headers Fo, F1 , .,,, Finis said to form 
a-iff Fi ;L Fi+l  O<i<m-1.  In other words, a sequence of ~oh loop headers forms a chain when their 
respective loops are nested in the order indicated in the sequence, and there is no other ~ofi loop in 
the nesting. Definition Given a component or compound state­ . ment C, a ~OX loop header F such that 
F 6 L C, and a scalar variable V, we say that C forwards V to the next iteration of F iff (1) V is an 
output variable of C; (2) there is no statement ~, with F 6L D, which appears after C in the text of 
the 100p such that V is an output variable of D; (3) there is no wh,dh? loop header OJsuch that F dL 
W6L C; and (4) in the execution of the body of F, V could be fetched before it is modified.  m Intuitively, 
C forwards V to the next itera tion of a {ofi loop F if the only value given to V by C at any iteration 
of F is still the value of V when the next iteration of F starts, and that value could be used in that 
iteration. Scalar Expansion Algorithm Consider an output variable, V, of a ~ok loop FO with dependence 
graph G. In the algorithm, we will assume that V(I1, 12, ..,, Im) and V(I1, 12, . . . . I ,0, . . . . 
O),m~O, represent the same memorymlocation (if m = O we have V and V(O, . . . . o).) [1] For all components 
C inG, execute step [2]. Then go to step [5] . [21 Let Fo, Fl, . . . . Fm form a chain of {oh loop headers, 
such that Fm;L C. . . . . I be the index variables et 10 11 m of these headers. If V is an input variable 
of C, execute step [3]. If V is an output variable of C, execute step [4]. [3] Let n~mbe the largest 
number such that there exist a component ~ with Ln 6L o, and V is an output variable of ~. Replace all 
occurrences of V on the right­ hand side of C by (1) V(IO-l, . . . . In-l-l, In) if there is a component 
V forwarding V to the next iteration of Ln such that O 6 C. (2) V(IO-l , .,., In_l-l, In-l) otherwise. 
 [4] Replace the occurrence of V on the left-hand side of Cby (1) V(IO-l, . . . . Ire-l-l, Im) if C 
forwards V to the next iteration of Lm. (2) V(IO-l, . . . . Im_l-l, Ire-l) otherwise.  [5] For all 
loops ({ok or titik? loops) Lm inside Lo execute step [6]. Then go to step [7]. [6] Let Fo, Fl, . . . 
. Fm_l ~rma chain of {Oh loop headers with Fm_l 6L L If V is an m output variable of Lm, insert the assignment 
statement V(IO-l, . . . . Im_2-1, a) = V(IO l, . . . . I m_l-l, 6) immediately after the end statement 
of L . m O if Lm is a uJh.&#38; loop Here .8 . max(uLm, O) if Lm is a ~oh loop with [ upper limit ul. 
m and I If Lm forwards V to the next m-1 iteration of Fm_l a I -1 otherwise m-1 [ [71 Immediately after 
the Lnd {Oh of ~o, insert the statement V = V(ULO) where ugO is the upper limit of F m o As was the 
case for renaming, a good expansion algorithm for array variables is an open problem. 3.3 Node Splitting 
The node splitting transformation attempts to break cycles in the dependence graph by reposition­ing 
antidependence arcs. This is achieved through the introduction of new assignment statements. Example 
3.4 The dependence graph in Fig. 2(c) includes a cycle which can be broken if the arc representing .A4 
6A A5 is repositioned. To do this, we split A4 into two assignment statements AL and A; as shown in Fig. 
2(d). Node Splitting Algorithm Consider a dependence graph G at the component level with the loop dependence 
arcs removed, and a cycle C in G. [1] If the cycle C disappears when G is repre­sented at the atomic 
level, then C includes an antidependence arc, say A, and the algo­rithm can proceed to step [2]. If C 
does not disappear, stop. [2] If the arc A emanates from an atom a in component C, then introduce a new 
assignment statement of the form T + a, and replace all occurrences of u in C by T, where T is a variable 
not appearing anywhere in the origi­nal program. [3] Apply the expansion transformation to T. w Example 
3.5 Part of the dependence graph at the atomic level for the program in Fig. 2(c) is shown in Fig. 5. 
Notice that the cycle in the graph of Fig. 2(c) disappears in Fig. 5. To remove the cycle, we introduce 
the statement T = X(1+1) and replace A4 by A 2)(1) = T +X(1-1). After T is ex­ panded, we will obtain 
the program in Fig. 2(d). The expansion and node splitting transformations, as discussed above, change 
the source program by introducing arrays and assignment statements. The goals of these two transformations 
can also be achieved by architectural means. Consider, for ex­ample, the DO ALL instruction of the Burroughs 
FM? multiprocessor [LuBa80]. Variables in the body of DO ALL are defined as local when they belong to 
neither the set of inputs nor the set of outputs of the DO ALL. A copy of all the local variables is 
created in the local memory of each processor be­fore a DO ALL starts execution. This has the same effect 
as expansion. Also, all variables in the set of inputs of the DO ALL are fetched before the DO ALL starta 
execution. This produces the same effect as node splitting.  3.4 Forward Substitution The forward substitution 
transformation elimi­nates flow dependence arcs from component level dependence graphs by substituting 
the right hand side expression of an assignment statement, , into the right-hand sides of other assignment 
statements. The main use of this transformation is that it could be applied before tree-height reduction 
[Kuck78], enhancing the result of this last trans­formation. Example 3.6 Consider the program segment 
in Fig. 6(a). Assume that only the variable F is used outside the segment. After applying forward substitution, 
we obtain the program segment in Fig. 7(a). The atomic dependence graph, if we assume that expressions 
are evaluated from left to right, ia shown in Fig. 7(b). However, if we do not assume any evaluation 
order and apply tree­height reduction, we obtain the dependence graph in Fig. 7(c), which is much better 
than the graph in Fig. 6(b) from the parallel processing point of view. m Assume a set of consecutive 
assignment state ments A A An in a program P. To forward 1 2  substitute a given Ai whose left-hand 
side is a scalar variable, S, we proceed as follows. Scalar Forward Substitution Algorithm [1] Apply 
renaming to all scalars on the right­hand side of Ai. [2] For allAj, j > i, such that (1) Ai 6Aj (2) 
there is noAk, i <k<j such that  Ai 6A Ak (since scalar variables have been renamed, always caused 
replace the hand side of Sin A.. 3 this antidependence by an array variable), expression on the right-Ai 
by all occurrences is of [3] Apply dead code e limination [Grie71]. 4. Dependence Graph Abstraction 
Graph abstraction is a process by which a set of nodes and their internal arcs are merged into a single 
compound node. Any arcs incident to (or from) the set are made incident to (or from) the compound node. 
Graph abstraction has been used in many areas of computer science. In particular, it has been used to 
organize optimization in several ways. For example, an interval is a graph abstraction [Cock70] used 
in data flow analysis. Graph abstraction has also been used to control the scope of optimization as in 
the SIMPL optimizer of [ZeBa74], which optimizes structured blocks from the inside out. We use graph 
abstraction in yet a different way. Graph abstraction can be used to isolate sets of statements that 
can be translated into high quality machine code only when taken as an ensemble. Two examples of this 
type of graph abstraction will be presented. 4.1 LooP Distribution LooP distribution abstracts dependence 
graphs by finding and merging each strongly connected com­ponent in the body of a loop along with the 
loop header node into a compound node. (A strongly connected component (SCC) is a maximal set of nodes 
such that there is a path,between any pair of nodes in the set.) Similarly, each loop body node not in 
any SCC, an independent node (IN), is merged with the loop header node into another compound node. Fig. 
8 shows how the node merging in loop distribu­tion is performed. The following algorithm describes loop 
distri­bution. The most time-consuming step in the algo rithm is step 1, finding the SCCS. However, it 
takes only o(n log n) time on a loop containing n statements if a depth-first algorithm such as Tarjan 
s algorithm is used [AhHU74]. This compares favorably with the fast data flow analysis algo rithms such 
as [GrWe76]. Loop Distribution Algorithm Consider a {oh loop F. whose body consists of the statements 
(simple or compound) S1, . . . . Sn. To distribute Fo, we proceed as follows. [1] Compute the dependence 
graph G for ~. and 1   n [2] Delete F. and create a Jo/c loop header node Foj for each SCC, and each 
IN in the depen­ dence graph. Make each statement in an SCC or IN loop dependent on the loop header as 
sociated with the SCC or IN, and flow depen­dent if the statement refers to the loop in­dex (i.e., a 
{ok loop is created for each SCC and each IN). [3] Build a new dependence graph by creating a compound 
node for each ~ok loop. The loop distribution algorithm can implement several optimization, depending 
on how the depen­dence graph is computed in step 1. We will give two specific examples. 4.1.1 Loop Distribution 
for Vector Processors The first optimization uses loop distribution to generate vector operations from 
multistatement loops . This is achieved by constructing a depen dence graph consisting of flow, anti, 
and output dependence arcs for the multistatement loop and performing the 100P distribution algorithm. 
The dependence graph output from loop distribution in this case is called a p artial order graph, and 
each node in this graph is called a m-block. (The term m-block stems from the fact that loop distribution 
partitions the nodes in the graph into equivalence classes.) Two types of n-blocks are derived. m-blocks 
whose bodies are INs represent vector operations> the goal of the optimization. m-blocks, whose bodies 
are SCCS, are called recurrences. (As a rule of thumb, there is approximately one recur­rence per loop 
in scientific source programs.) Al­though recurrences, nonvector operations, are not the most efficient 
operations on vector and array processors, we have found that relatively few re­currences are intractable. 
Most recurrences are SCCS connected by only flow dependence, primarily because loop distribution is applied 
after several optimization which remove anti and output depen­dence arcs (Section 3). These recurrences 
are most often linear recurrences, such as the row sum of a matrix, which can be speeded up [Kuck78] 
but are still slower than vector operations on vector processors. ([BcKT79] is a recent description of 
results in this area.) Loop distribution applied to a linear recurrence in effect abstracts the SCC to 
a single node representing a call to a linear recurrence solver. Other types of SCC that occur frequently 
are: Boolean recurrences which can be substantially speeded up [BaGK80], and simple non linear recurrences 
[Park77]. The partial order graph constructed in step 3 of the Loop Distribution Algorithm is a directed 
acyclic graph. It can be used to schedule the vector operations and recurrences on a parallel processor. 
The longest chain in the partial order graph defines the minimum execution time for the original source 
program loop. The maximum width or anti-chain in the graph defines an upper bound on the number of processors 
that can be used in parallel computation. Example 4.1 Loop distribution for vector processors produces 
the program in Fig. 2(e) when applied to the program in Fig. 2(d). All state­ ments become vector operations 
except for state­ments A4 and A5 which constitute a linear recur­rence. In the transformed,program, the 
statements are topologically sorted by the partial order graph.  4.1.2 Loop Distribution for Memory 
Management A second application of loop distribution is in memory management; we call this name clustering 
[AbKL79]. Example 4.2 [AbKL79] Consider the program in Fig. 9(a). If each array referenced in this pro­gram 
is on a distinct page, or distinct sets of pages, then the F1 loop requires 9 data pages to execute 
efficiently (with a minimum of page faults). After this type of loop distribution is applied, the transformed 
program (Fig. 9(b)) requires only 5 data pages to execute efficiently. Loop distri­bution has improved 
the program s data locality. The input to loop distribution for memory management is a dependence graph 
constructed for flow, anti, output, and input dependence. Loop  distribution in this case does not use 
SCC but name clusters defined next. Therefore, in the algorithm above SCC should be replaced by name 
cluster. Definition The set of variables referenced in a set of statements is called a name set (NS) 
and is a function of some statement set (SS). We can also compute the set of statements referencing any 
variable in a name set. Let SSO be any statement in a given loop. Call its name set NS and find the 
statement set of NS m o o call it SS Loop distribution iterates this se­ 1 quence until a stable statement 
set is found; this set is called a name cluster.  One might assume that in an average loop, the logical 
flow of the loop would connect all its statements into one name cluster; however, we have found that 
by using loop distribution the data page requirements of programs can be reduced by a factor of 6 [AbKL79]. 
Loop distribution for memory management can be compared with global register assignment (GRA) algorithms. 
Sophisticated GRA algorithms such as [Beat74] generate roughly the SS of each variable referenced in 
the loop. ( ROughly here implies that if at some point in the loop the variable is dead, then a new SS 
is started.) A register is allocated for each SS. Computing SSs requires a connected component computation. 
GRA algorithms in this class can reallocate a register several times within a loop, but clustering does 
not reallocate page frames because of the relatively higher cost of page swapping.  4.2 Loop Fusion 
As a graph abstraction, loop fusion is used selectively to merge two compound ~ofi loop nodes. Thus, 
it is nearly the inverse of loop distribution. But where loop distribution is applied globally to a loop, 
loop fusion is applied selectively. LooP Fusion Algorithm Consider two {ok loops FO, and F1 with the 
following characteristics: (1) Both FO and F1 have the same loop limit. (2) F. and F1 are consecutive 
in the source  program with Fo aPPearing before F1 (If they are not consecutive, then try to make them 
consecutive by moving the statements separating FO and F1 before FO or after F1 whenever possible,) 
[1] Let .s s be the statements in tEe 0.1  O.n body of Fo, and S1 ~, .... S1 m the state­ ments in 
the body of F1. Temporarily create a ~Oh loop containing SO ~, . . . . SO ~, .S1 ~, . . . . s ~ ~ (renaming 
the index variable occur­ rences if necessary), and compute its depen­dence graph G. [2] If G contains 
any arc fromsl i to so j for some 1<i <n, 1<j <m, then fusion is not poss~ble~ Othe wis=, replace the 
loops F. and F1 by a single loop containing the body of both loops In the past, loop fusion has been 
applied glo­ bally to reduce the overhead of loop control [A1C072], [Love77]. We will show that by applying 
loop fusion selectively with different criteria, different optimization can be realized. Loop Fusion 
for Virtual Memory Management Above we saw that loop distribution applied to the proper dependence graph 
can reduce the data memory requirements of a source program. LooP fusion can be subsequently used to 
reduce unneces sary swapping. The criteria used to select pairs of loops for fusion in this case is that 
the NS (Name Set is defined above) of one loop is con­tained in the NS of the other. The following ex­ample 
illustrates loop fusion for virtual memory management. Example 4.3 If the program in Fig. 9(b) is in­put 
to the loop fusion algorithm, the program will be transformed as shown in Fig. 10. In this case, the 
NS of each of the loops before loop fusion is: Name Set (NS) !K?.Q.P {A,B,c,G,H} 1 {D,E,F,x} F2 F3 {D,E,F} 
 The NS for F3 is contained in the NS for F2, all conditions for fusion are satisfied, so ~ and 2 F3 
are fused. Page swapping has been reduced because once a page of D, E, and F are loaded, all operations 
using these pages are performed. H Loop Fusion for Vector Register Processors Processors having vector 
registers such as the CRAY-1 present novel requirements for code genera tors. We will present a sequence 
of transformations which performs very well in this environment. @his sequence is contained in a vectorizer 
for pipe­lined machines described in [KKLW80].) LooP distribution for vector processors first isolates 
the recurrences from the vector operations. Loop fusion is applied to increase the NS of each loop until 
it is as large as the number of vector registers available. o Loop blocking [AbKL79] transforms single 
~Ofi loops into doubly nested loops. The inner loop is set to the size of the vector regis­ters. The 
outer loop increments in steps of the register size through the original loop range. Loop interchanging 
[Kuck80] attempts to avoid 214 recurrences on the inner loop and to reduce memory register traffic by 
interchanging loops when possible. . Register assignment assigns vector registers globally.. It also 
generates loads and stores at the entry and exit of each register alloca­tion block. We present this 
optimization sequence at this junc­ture because of the role played by loop fusion in particular. The 
initial loop distribution generates vector operations that are much larger than the register size. Loop 
blocking remedies that. How­ever, without loop fusion the outer loop overhead would have to be paid for 
each vector operation. At the same time, fusing all of the outer loops to­gether may over-allocate the 
available vec~or regis­ters. Therefore, the loop fusion criterion used in this case is whether the NS 
of the fused loop will be larger than the number of available vector regis­ters. Once fused loops with 
NS approximately as large as the available registers are created, regis­ter assignment need not be as 
complex. (We might label it a global few-to-few assignment strategy, using Day s terminology [DayW70]. 
The only other published method to optimize vector register as­signment is found in [DUKU78], which describes 
a global assignment based on usage counts with no re­allocation of vector registers. 5. Conclusion The 
techniques of this paper have been imple­mented and used in compiling to improve the per­formance of 
ordinary programs for several architec­tures. The transformations of Section 3 remove de­pendence arcs 
and hence increase independence be­tween nodes, while those of Section 4 abstract from a graph parts 
which are convenient for code genera­tion. By removing cycles, they yield a DAG that also is convenient 
for scheduling. It is important to realize that the resulting graph usually consists of many nodes that 
can be compiled directly into the machine languages of cur­rent high-performance machines. Array operations 
arise from independent nodes after loop distribu­tion. The recurrences arising from strongly con­nected 
component cycles are often linear. Many ms­chines have reduction instructions and some (e.g., BSP) have 
a more general linear recurrence solving instructions or functions. The remaining parts of a dependence 
graph must be executed using scalar in structions. Application of these ideaa to machines that can execute 
manv scalar operations at once is discussed in [PaKL80j. References [AbKL79] W. Abu-Sufah, D. Kuck, and 
D, Lawrie, Automatic Program Transformations for Virtual Memory Computers, Proc. of the 1979 Nat l. Computer 
Conf., pp. 969 974, June 1979. [AcDe79] W. B. Ackerman and J. B, Dennis, Val--A Value Oriented Algorithmic 
Language: Preliminary Reference Manual, Lab. for Computer Science (TR-218), MIT, Cam­bridge, MA, June 
1979. AhHU74j A. V. Aho, J. E. Hopcroft, and J. D. Unman, The Design and Analysia of Computer Algorithms, 
Addison-Wesley, MA, 1974. AhU173] A. V. Aho and J. D. Unman, The Theory of Parsin g, Translation, and 
Compiling, vol. 2: Compiling, Prentice-Hall, Englewood Cliffs, NJ, 1973. [AlCo72] F. E. Allen and J. 
Cocke, A Catalogue of Optimizing Transformations, in Design and Optimization of Compilers (R. Rustin, 
Ed.), Prentice-Hall, Inc., NJ, pp. 1-30, 1972. ArGP78] Arvind, K. P. Gostelow, and W. Plouffe, An Asynchronous 
Programming Language and Computing Machine, University of California at Irvine, CA, Dept. of Information 
and Computer Science Rpt. l14a, Dec. 1978. AsMa75] E. Ascroft and Z. Manna, Translating Program Schemes 
to While-Schemas, SIAM J. on Computing, Vol. 4, No. 2, pp. 125-146, June 1975.  [BaGK80] U. Banerjee, 
D. Gajski, and D. J. Kuck, Array Machine Control Units for Loops Containing IFs, Proc. of the 1980 Int 
1. Conf. on Parallel Processing, Harbor Springs, MI, pp. 28-36, Aug. 1980. [Bake77] B. S. Baker, An Algorithm 
for Structur­ing Flow Graphs, J. of the ACM, Vol. 24, No. 1, pp. 98-120, Jan. 1977. [Bane76] U. Banerjee, 
Data Dependence in Ordinary Programs, M.S. thesis, Univ. of Ill. at Urbana-Champaign, Dept. of Comput. 
Sci. Rpt. No. 76-837, Nov. 1976. [Bane79] U. Banerjee, Speedup of Ordinary Pro grams, Ph.D. thesis, Univ. 
of Ill. at Urb. Champ. , Dept. of Comput. Sci. Rpt. No. 79-989, Oct. 1979. [BCKT79] U. Banerjee, S. C. 
Chen, D. J. Kuck, and R. A. Towle, Time and Parallel Proces­sor Bounds for Fortran-Like Loops, IEEE Trans. 
on Computers, Vol. c-28, No. 9, pp. 660-670, Sept. 1979.  [Beat74] J. C. Beatty, Register Assignment 
Algo­rithm for Generation of Highly Opti­mized Object Code, IBM J. of Res. and ~. , Vol. 18, No. 1, 
pp. 20-39, Jan. 1974. [BoJa66] C, Bohm and G. Jacopini, Flow Diagrams, Turing Machines and Languages 
with Only Two Formation Rules, Comm. of the ACM, Vol. 9, No. 5, pp. 366-371, May 1966. [Cock70] J. Cocke, 
Global Subexpression Elimina­tion, SIGPLAN Notices, Vol. 5, No. 7, PP. 20-24, 1970. 215 [DayW70] W. 
H. E. Day, Compiler Assignment of Fig. 1. Five types of dependence graph arcs Data Items to Registers, 
 IBM syst~. J_. , Vol. 9, No. 4, pp. 281 317, 19700 (a) loop dependence .  -3 [DuKu78] D. D. Dunlop 
Allocation and in J. the C. Knight, Register SL/1 Compiler, ~. (b) flow dependence > of the 1978 LASL 
Workshop on Vector &#38; (c) output dependence A > parallel AlamOs, processors, LA-7491-C, NM, pp. 205-211, 
Sept. Los 1978. (d) (e) antidependence input dependence A ,, T > > [Grie71] D. Gries, Compiler Construction 
for Digi tal Computers, Wiley &#38; Sons, NY, 1971. [GrWe76] S. L. Graham and M. Wegman, A Fast and 
Usually Linear Algorithm Flow Analysis, J. of the No. 1, pp. 172-202, 1976. for Global ACM, Vol. 23, 
Fig, 2. Successive to a program application of four transforms [KKLW80] D. J. Kuck, R. H. Kuhn, B. Leasure, 
and F1 : 60LI=l,toN ,, M. Wolfe, Analysis and Transformation Al: A=A+l 3 2 of to Programs appear in 
for Parallel Proc. of the Computation, Fourth Int 1. A2 : Y=A+2 A . .­ F-h / Computer Software Oct. 
1980. &#38; Applications Conf., A3 , Z(I) = Y +V(I) / \ [Kuck78] D. J. Kuck, The Structure of Computers 
 4: A5 : A = X(I) X(1+1) = W(I) + X(1-1) + 1 @ A A . and computations, Sons, Inc. , NY, Vol. 1978. I, 
John Wiley &#38; A6 : W(I+l) = X(I) + 1 Original program cnd JOJL 2 (a) [Kuck80] D. J. Kuck, Class Notes 
for C.S. 433, Univ. of Comput. Ill. Sci. , at Urb.-Champ. 1979. , Dept. of F1 : Al: (joJLI=l,toN A(l) 
= A(2) + ~ [KuMC72] D. J. Kuck, On the Y. Muraoka, and S. Number of Operations C. Chen, Simulta A2 : 
Y=A(1)+2 neously Executable in FORTRAN-Like A3 : Z(I) = Y+V(I) Programs and Their Resulting IEEE Trans. 
on Computers, Vol. Speed Up, C-21, A4 : A(2) = X(1+1) + X(1-1) No. 12, pp. 1293-1310, Dec. 1972. A5 : 
X(I) = W(I) + 1 [Love77] D. B. Loveman, Program Improvement by 6: W(I+l) = X(I) + 1 After renaming 
Source-to-Source Transformation, ~. atd ~Oh 2(b) of the ACM, Vol. 20, No. 1, pp. 121­ 145, Jan. 1977. 
F1 : ~okI=ltoN [LuBa80] S. F. Lundetrom and G. H. Barnes, trollable MIMD Architecture, A Proc. Con­of 
Al: A(l) (l_Q = A 2) (1-1)+1 the 1980 Int 1. Conf. on Parallel A2 : Y(I-1) = A(l) (I-1)+2 processing, 
pp. 19 27, Aug. 1980. A3 : Z(I) = Y(I-1) + V(I) [PaKL80] D. A. Padua, High-Speed pilation D. J. Kuck, 
and D. H. Lawrie, Multiprocessors and Com-Techniques, Special Issue on 4: A5 : A(2) X(I) (I) = = X(1+1)+X(1-1) 
W(I) + 1 Parallel Computers, Processing, Vol. c 29, IEEE No. Trans. 9, pp. on 763­ 6: W(I+l) = X(I) + 
1 After expansion 776, Sept. 1980. [Park77] D. S. Parker, Jr., Nonlinear Recurrences and Parallel Computation, 
 in High speed F1 : fjoxI=l-toN Computer and pp. 317 320, Algorithm Academic Organization, Press, Inc., 
1977. Al: A(l) (I-l) = A 2)(1-1)+1 2: Y(I-1) = A(l) (I-l)t2 [ZeBa74] M. V. Zelkowitz and tion of Structured 
W. G. Bail, Programs, Optimiza-Software A3 : Z(I) = Y(I-1) + V(I) %Ctice and Experience, Vol. 4, No. 
1, A;: T(I-1) = X(1+1) pp. 51-57, 1974. A;: A(2) (I) = T(I-l)+X(I-1) A5 : X(I) = W(I) + 1 ~ 6: W(I+l) 
= X(I) + 1 After ting node split end fjOk 2(d) 216 Fig. 5. Partial dependence graph at the atomicF~1: 
@JLI=l ZON a level for the program in Fig. 2(c)1.1 o A; : T(I-1) = X(1+1) F A5 : X(I) =W(I) + 1 1.2 
0 A6 : W(I+l) =X(I) + 1 end {OZ /=~3: ~oh I=lfON F 1.3 A; : A(2) (I) = T(I-l)+X(I-1) @ end ljofi F14: 
fjoh 1=1 XON ) Al : A(l) (1-1) = A(2) (1-1)+1 Fig. 6. Original program for Example 3.6 1.4 end ~ox 0 
F~5: ~Ofi 1=1 ZON Al: A=B+C A2 : Y(I-1) = A 1) (1~1)+2 B c A2 : D=E+A end 40L 1.5 Al + F16: ~OXI=l ZON 
o A3 : F=G+D A3 : Z(I) = Y(I-1) + V(I) A . 8 (a) ad 60L F 1.6 Q do After distribution EA 2 (e) A2 
+ Fig. 3. Partial dependence graph at the atomic D level for the program in Fig. 2(a) m GD A3 + F 
6i5 (b) Fig. 4. Partial dependence graph at the atomic level for the program in Fig. 2(a) for Example 
3.6 and Fig. 7. Transformed program twO possible atomic dependence graphs A;: F=B+C+E+G B c + (a) E 
+ G + Y (b) F1 : dOXI=l,tON Al: A(I) = B(I) + C(I) Scc IN before abstraction (a) replicated loop header 
(b) resulting abstracted graph (c) Fig. 8. LOOP distribution as a graph abstraction A2 : D(I) = E(I)+F(I)+X(I) 
F1 _ Al A3 : G(I) = B(I) + H(I) QB wd fjOk \ A F2 : @L~=~&#38;oN \3 Ah: \ E(J) = D(J) * F(J) A2 cnd 
~ofi F2 A4o­-~ Original program (a) F~1: fjcJtI=lZON F1 A Al , A(I) =B(I) +C(I) 1 Q \ \ 43 : G(I) 
= B(I) + H(I) . e.vtd fjO)L A3 8 F12: If OX 1=1 XON A2 , D(I)= E(I)+F(I)+X(I) F   1.2 AZ cnd dot 
o F2 : ~O&#38;J==l~ON E(J) = D(J) * F(J) 4 : oF2 _A4 End ~OJL x Transformed program (b) Fig. 9. 
LOOP distribution for memory management ~1: fjO&#38;I=l~CIN F1 AlAl: A(I) = B(I) + C(I) Q \ A3: \ 
G(I) = B(I) + H(I) \ wd fjOk A3 F2 : ~OfiI=lxON 8 A2 : D(I) = E(I) + F(I) + X(I) A4 : E(I) = D(I) * 
F(I) end ~O&#38; Q-Q \  \ 6 A4 Fig. 10. LOOP fusion for virtual memory management 218  
			