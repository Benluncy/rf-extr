
 The Performance of a Service for Network-Aware Applications Katia Obraczka and Grig Gheorghiu USC In 
ormatlon Insbitute f Sciences University of Southern California 4676 Admiralty Way suite 1001 Marina 
de1 Rey, CA 90292 katia, grig@isi.edu (310)822-1511 (voice) (310)823-6714 (fax) Abstract This paper evaluates 
the performance of topology-d, a ser- vice that estimates the state of networked resources by pe-riodically 
computing the end-to-end latency and available bandwidth among them. Using its delay and bandwidth estimates, 
topology-d computes a fault tolerant, minimum-cost spanning tree connecting participating sites. We deployed 
topology-d on 27 Internet sites through-out the world and collected data for a period of two and a half 
months. The results of these wide-area experi-ments show that topology-d s estimates compare quite well 
with latency and bandwidth measurements from existing tools. We observe that the logical topologies computed 
by topology-d are consistent with current latency and band-width estimates. Topologies are also responsive 
to changes in network and server load, as well as in group member-ship. Robustness and fault-tolerance 
distinguish topology-d from other measurement services and prove to be invalu-able in a distributed, 
administratively decentralized en-vironment like the Internet. Participating sit,es maintain a weakly-consistent 
view of the group which provides the basic infrastructure to automatically handle group mem- bership 
dynamics, including failure and recovery. Introduction The growth of the Internet motivated the development 
of a variety of distributed applications, such as information dissemination, multimedia, and metacomputing 
services. These applications need to be network-aware to ensure they get appropriate service from the 
underlying commu-nication and computing infrast,ructure. For example, a client s request to a replicated 
information service should be automatically directed to a close-by, lightly loaded server Being able 
to choose adequate servers requires that ap-plications have access to information about network and Permission 
to make digital or hnrd copies of all or part of this work for personal or cla~.sroom use is granted 
without fee provided that copies are not made or distributed for prolit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to 
post ou scrve:~s or to redistribute to lists, requires prior specilic permission and/or a fee. SPDT 98 
Welches OR USA Copyright ACM 1998 l-581 1%OOl--5/9818...$5.00 server load. We argue that a service that 
provides knowl- edge about the dynamics of the underlying communication and computing infrastructure 
is essential to netzuork-aware applications. This paper evaluates the performance of topology-d, a service 
that estimates the state of networked resources by periodically computing the end-to-end latency and 
avail- able bandwidth among them. Since these are end-to-end measurements, they capture both network 
and server load. Based on these estimates, topology-d computes a low-delay, high-bandwidth spanning tree 
with additional edges for fault-tolerance. Topologies are periodically re-computed to take into account 
network and server load dynamics. Topology-d evolved from the Harvest [l] replication ser-vice [9], whose 
goal was to minimize the network band-width and delay required to replicate broker archives used in Harvest. 
Motivated by the need of other network-aware applications, we have implemented the replicator s network 
estimation and topology computation tool as a stand-alone service we call topology-d. This paper presents 
the results of the experimental performance evaluation study we con-ducted by deploying topology-d on 
27 hosts across the In-ternet. The goal was to validate the tool s estimates and logical topologies. 
These experiments also allowed us to showcase our tool s robustness and fault-tolerance, which are essential 
in a distributed, administratively decentral-ized environment like the Internet. Through topology-d s 
membership protocol, sites can dynamically leave (volun-tarily or due to failures) and join a group without 
disrupt-ing the group s normal operation. Topology-d s estimates can be used in different ways. They 
can be logged for future assessments of the network status. Alternatively, topology-d s data can be either 
di-rectly consumed by applications or it can be periodically published in a directory service for consultation 
by inter-ested third-parties. The latter approach is the one adopted in the Globus metacomputing infrastructure 
[4]. We give more details about topology-d s integratidn into Globus in Section 2. Other potential client,s 
of our tool include replicated databases and distributed information dissemi-nation services, such as 
the World Wide Web and Inter-net archives. We anticipate that other applications can also benefit from 
topology-d s services. Topology-d s log-ical topologies can be tailored to fit the requirements of different 
distributed applications. Currently, topology-d generates fault-tolerant topologies that try to minimize 
both update propagation cost and time in replicated in-formation services. Logical topologies for metacomputing 
systems can simply connect participating sites with a pre- specified number of close-by, well-connected, 
lightly-loaded computation resources. 2 Related Work 2.1 Internet Measurement Internet measurement is 
the subject of the IP Provider Metrics (IPPM) subgroup of the IETF s Bench Marking Working Group (BMWG). 
IPPM s main goal is to provide and standardize metrics and methodologies for Internet performance measurement 
[lo]. The Cooperative Asso-ciation for Internet Data Analysis (CAIDA) s taxonomy surveys measurement 
tools currently available both as free and commercial software. TReno [g] is one tool being considered 
for IPPM-endorsement. It aims at accurately measuring the bulk transfer capacity of network links by 
implementing its own TCP algorithm with Selective Acknowledgements (SACK). TReno measures the throughput 
of a given link independent of the particular TCP implementation on the end hosts. Topology-d and netperf, 
on the other hand, measure the throughput of an end host s particular TCP implementa-tion since this 
is the performance perceived by applications running on that host. Pathchar [6] is a recently released 
tool which estimates bandwidth, delay, average queue and loss rate of every hop between a given source-destination 
pair of Internet hosts. Pathchar uses the ICMP protocol s Time Exceeded response to packets whose TTL 
has expired. bprobe and cprobe [2] probe the network by sending several pairs (bprobe) or a short train 
of packets (cprobe). They estimate network bandwidth by measuring packet inter-arrival time. The goal 
of bprobe is to measure the bottleneck bandwidth along a network path, whereas cprobe attempts to estimate 
the effective bandwidth available to an application in the presence of competing network traffic. One 
advantage of these tools is that they do not load the network with their own traffic (although bprobe 
does have a tendency to send packets of sizes up to 8000 bytes for very short periods of time). Unfortunately, 
at the time we conducted our experiments, the tools were only available for SGI machines and were hard 
to port to other platforms since they depend on SGI s accurate timer mechanisms. While topology-d provides 
its own network bandwidth and latency estimates, it could just as well use the esti- mates from an IPPM-approved 
tool (which for the mo- ment does not exist). We envision our tool as a middle- ware service that provides 
information about the underly- ing communication and computing infrastructure. In fact, topology-d could 
use its group management infrastructure to provide estimates gathered from a variety of network measurement 
tools to applications which depend on this kind of knowledge for adequate performance. Before look- ing 
at some examples of such applications, we discuss two other existing middleware services similar in certain 
as- pects to our tool. The Network Weather Service (NWS) [14] forecasts the end-to-end throughput and 
latency of TCP/IP-based ap-plications. NWS collects data using netperf and then ap-plies a set of forecasting 
methods, including mean-based, median-based, and autoregressive models. NWS then dy-namically selects 
the best forecast according to a spec-ified metric, which can be based on measuring either the mean square 
prediction error or the mean percentage pre-diction error. One drawback of NWS is that it does not adjust 
to failures and only functions well as long as all monitored machines stay up and running. Unlike NWS, 
topology-d was designed to adjust to group membership dynamics. There is an increasing interest in measuring 
performance aspects of the Internet specifically related to Web-browsing. For example, Lachesis [la] 
was designed to assess the per-formance of Internet Service Providers (ISPs). It bench-marks a particular 
ISP by taking a list of so-called land-marks (important sites across the Internet, including DNS root 
servers, well-known FTP servers and popular WWW servers) and measuring the packet loss and network latency 
to each landmark.  2.2 Applications One current client of our tool is the resource broker used in the 
Globus metacomputing infrastructure [4]. Globus aims at enabling the deployment of high-performance, 
computing-intensive distributed applications by taking advantage of resources such as high-speed networks 
and supercomput-ers. The Globus Metacomputing Directory Service (MDS) [3] provides information about 
the current state of available resources. This information is used by the Resource Bro-ker when scheduling 
resources for specific tasks. Topology-d s robustness and distributed group membership protocol made 
it an ideal candidate for the job. Topology-d is now deployed at all the sites participating in the Globus 
project and it provides its network measurements to the MDS. We are currently looking at ways to customize 
the logical topology algorithm to make it more suitable for Globus. Smart clients [15], which is part 
of the Berkeley NOW (Network Of Workstations) project, is another application that can benefit from topology-d 
s services. It was devel-oped as a collection of Java applets and take a client-side approach to providing 
transparent access to multi-server Internet services such as HTTP and FTP. The director applet chooses 
the best machine from a pool of servers providing a particular service requested by the client. The choice 
is transparent to the user and is based on the kind of network state information topology-d provides. 
3 Topology-d For performance and robustness, topology-d was imple-mented as a Unix daemon that does not 
fork, but instead uses non-blocking I/O for all communication. The daemon can be queried in two ways: 
from a Web browser or via a command-line utility called td-client. The HTTP in-terface is better suited 
for interactive sessions: it displays both current estimates and the group s logical topology. The command-line 
interface is useful for batch-mode queries. We wrote a series of Per1 scripts as wrappers around this 
utility and we used these scripts for statistics gathering and parsing. 3.1 Estimate Collection and 
Topology Generation Each machine running topology-d periodically computes available bandwidth and RTT 
between itself and all the other machines in the group. For RTT estimates, a times- tamped UDP packet 
is sent to another member of the group, which simply returns it back to the originator. For bandwidth 
estimates, each machine sends a block of data using TCP (the default for the block size is 32KB). Avail-able 
bandwidth is then computed as: bandwidth = bytes-sent/(tamer,,t_byte -tzmefzrst-by~e ) where the two 
timestamps are taken by the destina-tion machine. When computing the actual estimates to be reported 
to the master, previous history is taken into ac-count in order to avoid transient changes. This damping 
effect is computed using the following exponential average function : new-est = N * old-est + (1 -CY) 
* current-est We currently set (Y to 0.5. It is important to note that topology-d s bandwidth and RTT 
estimates take into ac-count machine load. While a tool like TReno is more ac-curate for transport-layer 
measurements, t,opology-d tries to measure the actual delay and bandwidth seen by an application. A group 
member designated as the master collects the estimates reported by group members into a cost matrix for 
the group which the master uses to compute the group s logical topology. Each entry C,,, in the cost 
matrix cor-resoonds to the communication cost between machines i and j and is given by B,,,/D,v,, where 
B,,, and D,,, are the estimated bandwidth and RTT between nodes i and i. respectively. Topology-d then 
generates the logical topol-ogy by invoking a topology generator program, which uses as input the cost 
matrix and a connectivity requirement k for each node (currently we have L = 2). The topology generator 
first computes a minimum cost spanning tree connecting all the nodes, and then, for each node whose degree 
d is less than the required connectivity k, adds the current cheapest edge until d = Ic. The master periodically 
sends the current logical topology to all the machines in the group. 3.2 Group membership When a site 
joins a group, it sends a join request to the master, which adds it to the list of known sites. However, 
the machine is not officially part of the group until the master distributes a new topology that contains 
the site. There is no protocol for leaving the group. Machines leave a group silently and if the master 
has not heard from a member after a pre-determined period of time, it drops the machine from the group 
membership. The silence pe-riod is configurable and is currently set to one hour. Topology-d was designed 
to be fault-tolerant with re-spect to group membership changes. A group continues to function normally 
as machines leave or join. Even if the master is temporarily disconnected from the rest of the group, 
the members will continue to compute the esti-mates, without receiving topology and group membership 
updates. This weakly consistent membership protocol al- This well-known damping mechanism 1s used in 
TCP s conges-tion control round-trip time computation [5] lows topology-d to be robust to failures of 
group members, including the master. 4 Experiments We installed topology-d at 27 Internet sites including 
17 locations in the US, 6 in Europe, 1 in South America, 3 in Asia and the South Pacific. We configured 
all participat- ing sites as members of a single topology-d group, whose master was excalibur. use . 
edu. Conducting such a large scale, widely distributed exper- iment required considerable effort. Topology-d 
s robust- ness proved to be invaluable in such an autonomously de- centralized environment since it was 
quite common that a machine got rebooted or crashed during the experiments. Other than missing data for 
the unavailable sites, t,he ex- periments proceeded normally. These temporary site fail- ures demonstrated 
topology-d s ability to adjust to mem- bership changes caused by a site voluntarily leaving the group 
and joining in later, and by temporary failures. Table 1 describes the configurable group parameters 
and shows their values for t.he experiments. The master site parameter points to the master of the group. 
The ping periodand bandwidth periodparameters determine how often sites estimate latency and bandwidth. 
The update period and estimates period determine how of-ten the logical topology is updated and how frequest 
sites report their estimates to the group. 4.1 Data Collection We sampled topology-d s bandwidth, latency 
and topol-ogy information every hour over a period of two and a half months, starting beginning of June 
1997 and ending in mid September. To validate topology-d s estimates, we compared them against measurements 
obtained from run-ning ping and netperf on a subset of the machines in the group. In order to restrict 
the NZ network paths we had available, we ran ping and netperf measurements on four sites chosen based 
on their geographic location, how well they were connected, and how stable (in terms of uptime) they 
were. We limited the data collection period to one week; during these seven days, we ran ping and netperf 
every hour while running topology-d. Netperf [7] estimates the available network through-put by sending 
packets of configurable length over a con-figurable period of time (10 seconds by default). This bandwidth 
measuring technique yields more accurate esti-mates than topology-d. On high bandwidth-delay paths, topology-d 
s single 32 Kbyte packet may not be enough to stretch the TCP transmission window beyond slow-start. 
This is likely the main reason why netperf reports consis-tently higher throughput values than topology-d. 
Clearly, there is a tradeoff between estimate accuracy and over-head. One issue for future work is to 
dynamically set the size of the bandwidth probe according to the path being measured. We also ran traceroute 
at three of the data collection sites: redondo. ece .uci. edu, cosmo.mcs.anl.gov, and mirage. irdu.nus. 
sg. Traceroute information is useful in diagnosing transient problems as well as providing insight to 
validate the topologies generated by topology-d. Parameter Value Description master-site ping-period 
bandwidth-period update-period estimates-period excalibur.usc.edu 15min 60min 6Omin 30min points to group 
master latency estimation period bandwidth estimation period topology update period estimate report period 
u Table 1: Group parameters and their values.  4.2 Data Analysis It has been shown [13] that the data 
population obtained by doing network measurements over the Internet does not present a normal or Gaussian 
distribution. Hence, the mean and standard deviation are not meaningful statis-tics for Internet measurements. 
Following the recommen-dations of the IPPM community, we then chose to present the 10th and 90th percentiles 
together with the median and the interquartile range (IQR, i.e. the 75th percentile minus the 25th percentile) 
to summarize our measurements. 5 Results We used ping s and netperf s measurements of latency and bandwidth 
to validate topology-d s RTT and band-width estimates, respectively. Once we validate topology-d s estimates 
of network and server load, we use them to validate the logical topologies generated. 5.1 latency and 
Bandwidth Our results are presented in the form of graphs showing topology-d s latency (in milliseconds) 
and bandwidth (in kilobits/sec) estimate samples taken between a given pair of hosts over time (in hours). 
Latency graphs also show the corresponding ping measurements, while bandwidth graphs present netperf 
data. Figures 1 and 2 show latency and bandwidth measure-ments taken from redondo . ece . uci . edu to 
the three other data collection sites. We use redondo. ece .uci. edu s es-timates as example; measurements 
taken from the other data collection sites present similar behavior. The statis-tics in Figure 3 summarize 
measurements from all data collection machines. Figure l(a) shows that topology-d s and ping s RTT measurements 
to cosmo .mcs . an1 . gov are quite similar. Topology-d s ability to take previous history into account 
dampens the effects of transients, which were captured by ping. For farther hosts connected through bottleneck 
connections, such as transoceanic links, both topology-d and ping report higher variability in RTT. This 
is the casefor itia.math.uch.grandmirage.irdu.nus.sg(Fig-ures l(b) and (c)). We observe higher RTT variability 
in the case of the link to mirage. irdu.nus. sg. To con-firm this variability, we inspected the traceroute 
logs we collected and we found indeed that there were numerous problems with this network path and multiple 
cases of un-reachability. Generally, topology-d s and ping s measure-ments compare quite well. This similarity 
is corroborated by the statistics presented in Figure 3(a). Topology-d s and netperf s bandwidth measurements 
taken from redondo. ece .uci. edu are presented in Fig-ure 2(b) and summarized by the statistics presented 
in Figure 3. They show that although the magnitudes of the measurements taken by each tool differ considerably 
(as explained in Section 4), the shapes of the curves are simi-lar. This means that both topology-d and 
netperf capture similar trends in bandwidth variability over time, although topology-d s use of damping 
prevents it from capturing transients. Again, one can notice the very high variability for the trans-oceanic 
link to mirage. irdu .nus. sg as op-posed to the relative stability of the trans-continental link to 
cosmo.mcs.anl.gov. We also notice from the graphs that there is no clear pattern in network and server 
usage over time, with the possible exception of the bandwidth graph from redondo.ece.uci.edutomirage.irdu.nus.sg. 
Thisgraph presents a certain periodicity, with peaks and slopes roughly every 20 hours2. This pattern 
was consistent across sev-eral sets of bandwidth measurements that we took between these two sites and 
it can also be observed on the reverse path,frommirage.irdu.nus.sgtoredondo.ece.uci.edu. However, the 
pattern is not apparent for the link be-tween cosmo.mcs.anl.govandmirage.irdu.nus.sg,which prompts us 
to speculate that this is an isolated character-istic of the inter-continental link between the US and 
Asia. For all the other graphs, peak and off-peak times are indistinguishable. This is due to the fact 
that our experi-ments included sites that span various time zones and thus the results show the current 
trends in network and server usage which are driven by globally distributed applications such as the 
World Wide Web. One other observation concerns the asymmetry of net- work paths. The fact that network 
links are generally asymmetric has been stated in [ll] and is confirmed by our statistics. Discussion 
Topology-d was not designed to provide accurate network bandwidth and delay measurements, but an estimate 
of the state of the network and its resources. One of the goals of our experiments was to demonstrate 
topology-d s ability to capture network dynamics so that client applications can adjust to changes in 
network and server load. One issue for future work is to investigate other mea-surement tools, such as 
the ones mentioned in Section 2. Experimenting with other tools that use alternate ways of measuring 
network bandwidth and latency will allow us to identify topology-d s strenghts and weaknesses regard-ing 
network state estimation. It will also help us identify tools that could be incorporated into topology-d. 
This 2This periodicity is more noticeable for netperf s measurements than for topology-d s due to the 
latter s use of damping. p ngdau - loplegyd.tia --.- (a) RTT to cosmo.mcs.anl.gov (b) RTT to itia.mcs.anl.gov 
 (c) RTT to mirage.irdu.nus.sg  Figure 1: Latency (RTT in ms) measurements taken from redondo. ece 
.uci. edu over time (in hours). Dotted lines show topology-d estimates, while continuous lines show ping 
measurements. (a)BW to cosmo.mcs.anl.gov (b)BW to itia.mcs.anl.gov (c) BW to mirage.irdu.nus.sg Figure 
2: Bandwidth (BW in kbits/sec) measurements taken from redondo.ece.uci.edu over time (in hours). Dotted 
lines show topology-d estimates, while continuous lines show netperf measurements. Sites COSIIlO.IIlCS.~l.gOV 
cosmo.mcs.anl.gov N A N/A itia.math.uch.gr 0.00/236(87.0)/356 203/216(66.0)/522 mirage.irdu.nus.sg 320/419(101)/678 
340/463(219)/801 redondo.ece.uci.edu 71/78(10)/92 70/74(8)/98 Sites cosmo.mcs.anl.gov cosmo.mcs.anl.gov 
N/A N/A itia.math.uch.gr 0.00/50.0(76.5)/94.0 0.00/63.1(47.2)/81.3 mirage.irdu.nus.sg 12.0/30.0(26.0)/60.0 
6.24/16.4(21.9)/68.6 redondo.ece.uci.edu 117/284(72.5)/321 288/348(20.3)/359 Figure 3: Statistics summarizing 
latency itia.math.uch.gr 0.00/238(78.5)/392 200/221(110)/576 N/A N/A 0.00/652(356)/1,110 488/678(326)/l 
,280 0.00/253(63.2)/410 237/256(145)/921 (a) Latency (in milliseconds) itia.math.uch.gr 0.00/53.0(65.5)/97.0 
0.00/96.1(122)/140 N/A N/A 0.00/31.0(28.5)/65.0 75Oe-3/X.0(34.2)/78.9 0.00/92.0(48.5)/117 9.34/159(123)/186 
 mirage.irdu.nus.sg 333/443(209)/892 337/436(151)/935 0.00/565(198)/862 487/592(213)/1,470 N/A N/A 251/391(249)/708 
289/418(284)/1,250 mirage.irdu.nus.sg 10.0/28.0(24.0)/52.0 14.0/42.0(39.0)/79.0 0.00/24.0(40.0)/52.6 
5.79/67.2(53.6)/86.2 N/A N/A 8.00/36.0(49.0)/103 18.0/80.1(82.4)/142 (b) Bandwidth (in kbits/sec) and 
bandwidth measurements. Rows and redondo.ece.uci.edu 72.0/75.0(6.00)/98.0 70.0/74.0(7.00)/91.4 0.00/248(55.0)/401 
233/250(40.0)/411 251/399(195)/771 275/424(291)/834 N/A N/A redondo.ece.uci.edu 163/260(66.5)/299 224/381(82.9)/407 
0.00/95.0(74.0)/124 40.3/169(49.3)/187 10.6/42.0(49.0)/91.0 4.87/13.3(22.3)/123 N/A N/A columns list 
source and destination machines. Each cell shows the 10th percentile/median(IQR)/SOth percentile for 
our sample population. The numbers on the top line refer to topology-d s estimates, while the numbers 
on the bottom line refer to ping netperf estimates in (b). will provide applications with a range of 
network measure-ment tools available under the same measurement infras-tructure. Topology-d employs an 
active estimation strategy 3 since it generates overhead traffic when performing estimates. One way of 
making active measurement tools less inva-sive is to keep the measurement frequency low. There is a clear 
tradeoff between the overhead generated by active measurement tools, their accuracy and ability to detect 
changes (and avoid transients) in the underlying infras-tructure. Topology-d s estimation frequency is 
a config-urable group parameter that can be set by system adminis-trators to satisfy the needs of specific 
applications, yet keep the overhead traffic to a minimum given the characteristics of the underlying 
administrative domain s resources. As the number of participating sites grow, organizing them into hierarchical 
clusters help limit the amount of es-timation overhead generated. Note that a n-member clus-ter performs 
O(n ) estimates. Therefore, small clusters generate less estimation traffic than larger ones. A des-ignated 
cluster member can be responsible for performing inter-cluster estimation. In [9] we report the overhead 
generated by topology-d for different group sizes. 5.2 Topologies For our experiments, we configured 
topology-d to re-compute the group s topology every hour and sampled the group s topology every hour. 
l-his classification IS according to Vern Paxson s framework [lo] We evaluated the topologies using 
two metrics: (1) correlation RTT estimates and the observed ogy s adaptability to the dynamics work and 
computing infrastructure, bership changes. estimates in (a) and to generated by topology-d between bandwidth 
and topologies, and (2) topol-of the underlying net-including group mem- 5.2.1 Estimates and logical 
Topologies We verify that the logical topologies the master computes for the group reflect indeed the 
estimates collected by the group members. For each host, we calculate the median for the RTT and bandwidth 
estimates taken from that machine to all the other group members. We order the resulting list in decreasing 
order based on the ratio bandwidth/RTT which represents the cost of a given link. Also for each host 
we count how many times any given member of the group was seen among the host s best neigh-bors and we 
order the resulting list in decreasing order. Tables 2 and 3 show the top machines in these two lists 
for redondo. ece .uci. eduover the week we collected topology-d, ping and netperf data. As expected, 
there is a strong correlation between the two lists, with the same machines dominating both lists. This 
close correlation between logi-cal topologies and latency and bandwidth estimates is ob- served for all 
the other machines We notice that the order of cording to redondo s estimates which they occur in the 
list of exception: syc. isi.edu appears neighbors than excalibur.usc.edu. in the group. the five best 
machines ac-is the same as the order in best neighbors, with one more times in the list of It can also 
be ob- niikaan.ld.cc.mn.us    r-dr Figure 4: Adaptability to changes in group membership. Topology 
snapshot taken on Sept. 9 1997 at 5 PM Pacific Time. Topology snapshot taken on Sept. 9 1997 at 6 PM 
Pacific Time Figure 5: Adaptability to changes in group membership. Median RTT 1 Median BW 1 BW/RTT Table 
2: Estimates for redondo. ece .uci. edu [I Site 1 Number of occurences II svc.isi.edu inferno.internet-cafe.com 
excalibur.usc.edu trek.cs.orst.edu ashoka.cs.umn.edu mirage.irdu.nus.sg niikaan.fdl.cc.mn.us pharos.bu.edu 
I 113 107 94 81 72 68 31 1 28 Table 3: Topology neighbors for redondo. ece . uci . edu served that two 
other machines (inferno . internet-caf e . corn and mirage. irdu.nus. sg) appear in the top of the list 
of neighbors, although they are not among the top machines according to the estimates. This is due to 
the fact that the current topology computation algorithm does not account for link asymmetry. So if inferno. 
internet-cafe. corn andmirage.irdu.nus.sgseeredondo.ece.uci.eduamong their best neighbors, then they 
will appear in the list of redondo s neighbors. This sometimes can lead to situa-tions where the topology 
resembles a star having in its center a powerful and/or well connected machine (refer to Section 5.2.2 
for an example). We plan to modify the topology computation algorithm to take into account asy-metric 
links. We will also specify a maximum node degree. 5.2.2 Adaptability Figure 4 shows the star effect 
mentioned in the previous section. Host ashoka. cs . umn. edu is connected to six other hosts. Note that 
the degree parameter the topology com-putation algorithm uses specifies the minimum node de-gree. We 
plan to include a maximum node degree to limit the number of neighbors assigned to a host and therefore 
eliminate star topologies. One aspect of topology adaptability concerns changes in group membership. 
Figures 4 and 5 show two snapshots taken one hour apart during our experiments. We observe a very clear 
transition. When the host ashoka.cs.uron.edu went down, the group reconfigured itself quickly. This topology 
change illustrates topology-d s fault-tolerance to (voluntary or involuntary) membership changes, which 
is extremely valuable in a distributed and unpredictible We also observe that topologies are responsive 
to changes in latency and bandwidth estimates. This statement is supported by the same type of evidence 
we showed for adaptability to group membership changes. Whenever the cost of a link (i.e. the ratio bandwidth/RTT) 
from host A to host B changes significantly enough so that host B is not seen as a neighbor anymore by 
host A, the new recom-puted topology reflects this change. In general, topology snapshots tend to be 
very similar except for a few links. This is a trend we observed for the entire experiment. It is very 
uncommon to find two identical topologies, but changes from one topology to the next are very small and 
generally limited to two or three machines. 6 Conclusions This paper reported the results of a study 
we conducted to evaluate the performance of topology-d, a service that estimates the state of networked 
resources. Based on these estimates, topology-d computes a fault tolerant, minimum-cost spanning tree 
connecting participating sites. We vali-dated topology-d s estimates by comparing them with the ones 
obtained from other well-known tools such as netperf and ping. We showed that topology-d s logical topolo-gies 
were consistent with its estimates of the current net-work state. We also showed that the topologies 
adjust to changes in network and server load, as well as in group membership. The Internet-wide experiments 
proved that topology-d s fault-tolerance and robustness are extremely important in the constant presence 
of reboots and shutdowns; these are essential features in wide-area, administrative decen- tralized environments. 
 References [l] C.M. Bowman, P.B. Danzig, D.R. Hardy, U. Manber, and M.F. Schwartz. Harvest: A scalable, 
customizable discovery and access system. Technical Report CU-CS 732-94, Department of Computer Science, 
University of Colorado-Boulder, July 1994. [2] It. Carter and M. Crovella. Dynamic server selec-tion 
using bandwidth probing in wide-area networks. Boston University Computer Science Dept. TR-96-007, March 
1996. [3] S. Fitzgerald et al. A directory service for configur-ing high-performance distributed computations. 
Pro-ceedings of the 6th IEEE International Symposium on High Performance Distributed Computing, August 
1997. [4] I. Foster and C. Kesselman. Globus: A metacomput-ing infrastructure toolkit. International 
Journal of Supercomputer Applications, Summer 1997. Available at Globus web page http://www.globus.org/. 
[5] V. Jacobson. Congestion avoidance and control. ACM SIGCOMM 88, pages 273-288, 1988. [6] V. Jacobson. 
Pathchar: A tool to infer char-acteristics of internet paths. Available from ftp://ftp.ee.lbl.gov/pathchar/, 
April 1997. [7] R. Jones. netperf. Available from http://www.cup.hp.com/netperf/NetperfPage.html. [8] 
M. Mathis and J. Madhavi. Diagnosting internet con-gestion with a transport layer performance tool. Pro-ceedings 
of the INET 1996, 1996. [9] K. Obraczka, P.B. Danzig, D. DeLucia, and E. Tsai. A tool for massively replicating 
internet archives: De-sign, implementation, and experience. Proceedings of the 16th. IEEE ICDCS, pages 
657-664, May 1996. [lo] V. Paxson. Towards a framework for defining Internet performance metrics. Proceedings 
of the INET 1996, 1996. [ll] V. Paxson. End-to-end routing behavior in the Inter-net. Proceedings of 
the ACM Sigcomm 1996, pages 25-38, August 1996. [la] J. Sedayao and K. Akita. Lachesis: A tool for bench-marking 
internet service providers. Proceedings of the LISA IX Conference, 1995. [13] J.Madhavi V.Paxson, G.Almes 
and M.Mathis. Frame-work for IP Performance Metrics. Internet Draft available at ftp://ftp.ietf.org/internet-drafts/draft-ietf-ippm-framework-02.txt, 
expiration date: July 1998. [14] R. Wolski. Forecasting network performance to sup-port dynamic scheduling 
using the network weather service. Proceedings of the 6th IEEE International Symposium on High Performance 
Distributed Com-puting, August 1997. [15] C. Yoshikawa, B. Chun, P. Eastham, A. Vahdat, T. Anderson, 
and D. Culler. Using smart clients to build scalable services. Proceedings of the USENI,Y 1997 Technical 
Conference, 1997.  
			