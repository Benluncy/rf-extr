
 Formal Verification of Algorithms for Critical Systems* John Rushby Computer Science Laboratory SRI 
International Menlo Park, CA 94025, USA Abstract We describe our experience with formal, machine­checked 
verification of algorithms for critical ap-, placations, concentrating on a Byzantine fault­tolerant 
algorithm for synchronizing the clocks in the replicated computers of a digit al flight control system. 
First, we explain the problems encountered in unsynchronized systems and the necessity, and crit­icality 
y, of fault-tolerant synchronization. We give an overview of one such algorithm, and of the ar­guments 
for its correctness. Next, we describe a verification of the algorithm that we performed using our EH 
DM system for for­mal specification and verification. We indicate the errors we found in the published 
analysis of the al­gorithm, and other benefits that we derived from the verification. Based on our experience, 
we derive some key re­quirements for a formal specification and verifica­tion system adequate to the 
task of verifying algo­rithms of the type considered. Finally, we summarize our conclusions regarding 
the benefits of formal verification in this domain, and the capabilities required of verification systems 
in order to realize those benefits. *This work was performed for the National Aeronautics and Space Administration 
under contract NAS1 17067. t Work performed whale the author was a member of the Computer Science Laboratory, 
SRI International. Permission to copy without fee all or part of this material is granted provided that 
the copiaa ara not made or distributed for direct commercial advantaga, tha ACM copyright notice and 
the title of the publication and its date appear, and notice ia given that copying is by permission of 
the Association for Computing Machinary. To copy otherwise, or to republish, raquiraa a fea and/or apacific 
parmisaion. B 1991 ACM, 0-89791.455.4/91/001 1/()()()1 ...$1.50 Friedrich von Henke Fakultat fur Informatik 
Universitat Ulm Postfach 4066, D-7900 Ulm, Germanyt 1 Introduction Use of formal methods is often advocated, 
and sometimes required, in the construction of soft­ware and digital hardware for critical systems [7, 
30]. Formal methods can range from pencil-and­paper descriptions and analyses in the style of con­ventional 
mathematical discourse, through the use of formalized specification languages, to truly for­mal specifications 
and mechanically-checked verifi­cation. In this paper we describe our experience at one particular point 
in this spectrum of formal meth­ods: we focus on formal verification, rather than specification alone, 
on algorithms rather than more concrete descriptions such as programs and cir­cuits, and on fully mechanized 
methods, rather than those used only by hand. We draw on the experiences of ourselves and col­leagues 
in performing a number of substantial for­mal verifications of algorithms to support fault tol­erance 
in digital flight control systems [23, 24, 27]. In order to keep the description focussed, we con­centrate 
on one particular application that we first undertook some years ago: a mechanically-checked formal verification 
of the Interactive Convergence Algorithm for Byzantine fault-tolerant clock syn­chronization [18, 24]. 
The paper is organized as follows. In the next sect ion, we describe our problem domain in general, and 
fault-tolerant clock synchronization in partic­ular. We describe how attempts to avoid synchro­nization 
have been unsuccessful, and hence the ne­cessity and criticality of fault-tolerant synchroniza­tion algorithms. 
In the third section we outline the Interactive Convergence Algorithm, and the argu­ments for its correctness 
in the manner of a con­ventional mat hematical presentation. Our experience in performing a mechanically­checked 
verification of the algorithm, and the bene­fits we perceive to have derived from that effort, are described 
in the fourth section. In the fifth section we identify the capabilities required in a specifica­tion 
and verification system in order to realize those benefits most effectively. Our conclusions make up 
the final section. Flight Control Systems Increasingly, modern aircraft rely on Digital Flight-Control 
Systems (DFCS): computer systems that interpret the pilot s control inputs and send appro­priate commands 
to the control surfaces and en­gines. The perceived advantages of DFCS over analog or direct mechanical 
and hydraulic control include improved performance, efficiency, and handling, re­duced pilot workload 
and, for military aircraft, im­proved agility, expanded flight envelope (e.g., ex­treme angles of attack), 
and the ability to exploit unstable airframes. Depending on the aircraft de­sign, DFCS may manage all, 
or merely some, of the control surfaces and may or may not have back-up systems comprising either analog 
computers or di­rect mechanical and hydraulic connections between the pilot and control surfaces. The 
Airbus A320 is the only passenger aircraft in current service with a full DFCS that is, one con­trolling 
the primary control surfaces and all three axes [22], but forthcoming aircraft such as the Boe­ing 777 
will also employ comprehensive DFCS.* It is clear that extreme reliability must be re­quired of a flight-critical 
DFCS. A much-quoted figure is a requirement for passenger aircraft that the probability of catastrophic 
failure during a 10 hour flight should be less than 10-9 per hour [9]. Such reliabilities are beyond 
those that can be guaranteed for the computers and other digit al de­vices comprising the DFCS hardware. 
Not only must occasional latent manufacturing defects and 1The Concorde, which received FAA certification 
in 1969, has an analog flight control system with mechanical backup in all three primary axes. the effects 
of aging be considered, but also environ­mental hazards such as power-supply disturbances, lightning 
strikes, and cosmic rays. These fact ors conspire to yield an overall reliability y well below that required. 
It follows that some form of fault tolerance based on replication and redundancy is needed in order to 
achieve an underlying hard­ware platform of the required reliability. In an N-modularly redundant (or 
N-plex) sys­tem, all calculations are performed by N identical computer systems and the results are submitted 
to some form of averaging or voting before being sent to the actuators. Great care is taken to elimi­nate 
single-point failures, so the separate computer systems (or channels, as they are often called in fault-tolerant 
systems) will generally use different power supplies and be otherwise electrically and physically isolated 
as far as possible. It is then reasonable to assume that failures of the separate channels will be statistically 
independent, so that the probability of overall system failure is orders of magnitude better than that 
of the individual chan­ nels. Notice that the purpose of this design is sim­ ply to tolerate random 
hardware faults; there is no protection against design faults. Any such faults in either the hardware 
or the software will be com­mon to all members of the N-plex and all will fail together, In this paper, 
we do not address the issue of design faults in the hardware, nor in the appli­cation soft ware that 
it runs. We are, however, very much concerned wit h the possibility y of design faults in the redundancy-management 
software that har­nesses the failure-prone individual components to­gether as a fault-tolerant N-plex. 
Instead of a sin­gle computer executing the DFCS software, there will be several, which must manage redundant 
sen­sors, coordinate and vote actuator commands, and tolerate faults among their own members. Complexity 
is a source of design faults, and there is a distinct possibility that a large quantity of redundant 
y-management software may lessen, rather than enhance, overzdl reliability. Conse­quently, some designers 
have sought simple so­lutions to the redundancy management problem. A plausible and simple approach uses 
an asyn­chronous design in which the computers run fairly independently of each other: each computer 
sam­pies sensors independently, evaluates the control laws independently, and sends its actuator com­ 
mands to an averaging or selection component that chooses the value to send to the actuator con­ cerned. 
The triplex-redundant DFCS of the ex­periment al AFTI-F16 airplane was built this way, and its flight 
tests reveal some oft he shortcomings of the approach [20]. Because they are unsynchronized, individual 
channels can sample sensors at slightly different times, and thereby obtain readings that differ quite 
appreciably from one another. The gain in the con­trol laws can amplify these input differences to pro­vide 
even larger differences in the results submitted to the output selection algorithm. During ground qualification 
of the AFTI-F16, it was found that these differences sometimes resulted in a channel being declared faded 
when no real failure had oc­curred [19, p. 478]. An even more serious short­coming of asynchronous systems 
arises when the cent rol laws cent ain decision points. Here, sen­sor noise and sampling skew may cause 
indepen­dent channels tot ake different paths at the decision points and to produce widely divergent 
outputs. This occurred on Flight 44 of the AFTI-F16 flight tests [20, p. 44]. Each channel declared the 
others failed; the analog back-up was not selected because the simultaneous failure of two channels had 
not been anticipated and the aircraft was flown home on a single digital channel. Notice that all pro­tective 
redundancy had been lost, and the aircraft was flown home in a mode for which it had not been designed 
yet no hardware jadure had occurred. The AFTI-F16 flight tests revealed numerous other problems of a 
similar nature, to the extent that redundancy management became the primary source of unreliability in 
the DFCS and an impedi­ment to testing [20, pp. 40 41]. These serious diffi­culties in plausibly simple 
designs have stimulated research into principled approaches to redundancy management for DFC S that will 
yield predictable behavior. The favored approach uses synchronized chan­nels, distribution of sensor 
data to all channels, and exact-match voting [5, 8, 13, 15, 16, 31]. Dis­tribution of sensor data ensures 
that each channel performs its computations on the same inputs, and should therefore produce the same 
outputs. Chan­nel failures can therefore be masked by exact-match majority voting of outputs sent to 
the actuators. Synchronization ensures that the clocks of the indi­vidual channels are kept within some 
bounded skew of each other, so that each channel will perform the same computations and will be ready 
to partici­pate in votes and the distribution of sensor data at approximately the same time as all the 
other channels. Of course, it is crucial to this approach that the algorithms and protocols for clock 
synchroniza­tion, sensor distribution, and voting are themselves be fault tolerant. Prior to the investigations 
of the SIFT project [18, 21], the subtlety and deli­cacy of the required algorithms and protocols were 
not properly understood, and the notion of Byzan­tine faults had not been fully articulated.z Con­sequently, 
early synchronization protocols were se­riously flawed: all were vulnerable to Byzantine faults, and 
many were incapable of tolerating less severe classes of faults. For example, the failure of the first 
attempt to launch the Space Shuttle was due to a synchronization problem [11], and some observations 
were lost when the heavy radiation environment at Jupiter caused one of the clocks on the Voyager spacecraft 
to jump several seconds [1]. Byzantine fault-tolerant clock synchronization algorithms are required to 
maintain synchroniza­tion despite the occurrence of any kind of fault whatever, provided there are not 
too many of them. Lamport and Melliar-Smith s paper [18] was a landmark in the field. They not only in­troduced 
three Byzantine fault-tolerant clock syn­chronization algorithms, but they developed for­malization of 
the assumptions and desired prop­erties that made it possible to give a precise state­ment and proof 
for the correctness of such algo­rit hms. In the following section we outline the Interac­tive Convergence 
Algorithm (ICA) one of those introduced by Lamport and Melliar-Smith-and its analysis. The outline includes 
precise statements of all the assumptions, constraints, and key lemmas needed for the analysis. We include 
this level of zA Byzantine fault is one where a faulty component pro­vides conflicting information to 
other components, poten­tially causing non-faulty components to declare each other failed, detail because 
it is necessary to convey the intri­cacy oft he argument, and the challenge it poses to those who require 
complete confidence that all the details and boundary cases are handled correctly. 3 The Interactive 
Convergence Clock-Synchronization Algo­   rit hm and its Analysis The goal of ICA is to maintain the 
clocks of re­dundant channels within some bounded skew 8 of each other. All channels have reasonably 
accu­rate clocks with a maximum rate of drift from real time given by p, and are synchronized within 
some bound 80 initially. The slight differences in their running rates will cause the clocks gradually 
to drift apart, so that they must be desynchronized periodically. Each channel desynchronizes by de­termining 
the differences between its own clock and those of other channels, forming a fault-tolerant average of 
those differences, and adjusting its own clock by that amount. Trivial solutions, such as quantity A 
by zero, and then calculates the arith­metic mean of the resulting set of differences. To gain an idea 
of why this works, consider two nonfaulty channels p and q. For simplicity, assume that these channels 
perform their synchronization calculations simultaneously and instantaneously. If r is also a nonfaulty 
channel, then the estimates that p and q form of r s clock value can differ by at most 26. If r is a 
faulty channel, however, p and q could form estimates of its clock value that dif­fer by as much as 2A 
+ S (since r could indicate a value as large as A different from each of p and q without being disregarded, 
and these channels could themselves have clocks that are 6 apart). As­suming there are n channels, of 
which m are faulty, the egocentric means formed by p and q can there­fore differ from each other by as 
much as (n-m)2c +m(ti +2A) n Thus, provided 2mA 6~2E+ n m those in which all clocks are reset to zero 
at every res ynchronization must be excluded, so there is a small bound Z on the size of the adjustment 
that may be made at each resynchronization. Deter­mining the differences between their clock readings 
may require cooperation among the channels, so they must all engage in the synchronization proto­col 
at approximately the same time. To do this, each channel engages in the synchronization proto­col every 
R seconds, and for a duration of S sec­onds, according to its own clock. Channels may not be able to 
determine the differences between their clocks with absolute accuracy: the quantity ~ is assumed to bound 
the error in A~~, which de­notes the difference between the clocks of channels q and p, as determined 
by p during its ith resyn­chronization. The key to making a clock synchronization al­gorithm resist ant 
to Byzantine failures among its components is the use of a fault-tolerant average in the adjustment step 
[26]. ICA is characterized by use of the egocentric mean as its averaging func­tion. To compute the egocentric 
mean, a chan­ nel replaces all differences A$~ larger than a fixed this procedure will maintain the clocks 
of p and q within $ of each other, as required. The sketch we have just given neglects many im­portant 
details (for example, the channels do not perform the algorithm simultaneously and instan­t aneously, 
and their clocks may drift further apart in the periods between resynchronizations). Since the clock 
synchronization algorithm is a potential source of single point failure in DFC!S, we require an unusually 
high degree of confidence in the anal­ysis of ICA. In the next subsection we outline the fully detailed 
analysis of ICA that we have sub­jected to mechanically-checked verification. 3.1 Formalizing the Arguments 
We need to distinguish two notions of time: clock time is the internal estimate of time that a chan­nel 
obt tins from its clock, while real time is an external, abstract notion of time that provides a common 
frame of reference. Real time need not be directly observable, since clocks are only to be synchronized 
wit h respect to each other, not to an external reference. Clocks are modeled as functions from clock 
to real time: CP(Z )denotes the red time at which channel p s physical clock reads T. Clocks are adjusted 
by subtracting a correction from their readings: suppose p s clock reads T when we would like it to read 
T, then C = T T is the correction that should be subtracted from the physical clock reading to yield 
the value to be reported as the logical clock reading. Since the corrected value T is reported when 
the physical clock reads T (i.e., T + C), we see that the logicaJ clock time T is re­ported at real time 
c(T + C). Corrections are computed once every resynchro­nization period. The correction for channel p 
in period i is denoted C$), and we define C(O(T)= CP(T + C~)) P as the logical clock for p during period 
i. The ith period is denoted l?(i) and runs from clock time T(i) to T(i+l), where T(i) = To + il? (i 
~ O) and To is an arbitrary constant. Synchronization takes place during the last S seconds of each period; 
S(i) denotes the interval [T(i+l) S, T(i+ll]. Now we can formalize the informal description given earlier. 
First we define a good clock to be one that keeps reasonably accurate time. Good clock: A clock c is 
a good clock during the clock time interval [To, TN] if c(T1) C(T2) _ ~ ~~ T1 T2 2 whenever T1 and T2 
(Tl # T2) are clock times / in [To, TN].3 All channels start off with their clocks approxi­ mately synchronized. 
AO: For all channels p and q, Icf)(Z ( )) -c$)(T(0))l <6.. Channels and their clocks can develop faults, 
so we need to say what it means for a channel to be nonfault y. 3This definition implies that good clocks 
are monotonic a fact that is proved in our formal verification. Al: We say that a channel is non faulty 
through period i if its clack is a good clock in the clock time interval [T(o) + C$), T(i~l) + C))]. 
 Now we can state the conditions that ICA is to maintain. The first says that the skew between the clocks 
of nonfaulty channels must be bounded; the second says that corrections must be bounded. Clock Synchronization 
Conditions: For all channels p and q, if all but at most m channels (out of n) are nonfaulty through 
period i, then S1: If p and q are nonfaulty through period i, then for all T in R(i) IC$)(T) -C$)(T)[ 
<4. S2: If channel p is nonfaulty through period i, then [c:+ ) -C))l <x. ICA requires that each nonfaulty 
channel can read the difference between its own clock and that of another nonfaulty channel with a bounded 
error. In order to this, the channels may already need to be synchronized, and so we require S1 and S2 
to hold. A2: If conditions S1 and S2 hold for the i th pe­ riod, and channel p is nonfaulty through period 
i, then for each other channel q, p obtains a value Af~ during the synchronization period S(i). If q 
is also nonfazdty through period i, then \A$j[ s S and \c$)(T + A!]) -c~)(T )] < c for some time T in 
S(i). If p = q, we take A~~ = Oso that A2 holds in this case also. Finally, we can give a formal description 
of ICA. Algorithm ICA: For all channels p: c(~+l) = c$) + A$), P where A(;) = if \A$~[ < A then A$~ else 
O. ~P Note that C:) is constrained by AO. In order to establish that ICA satisfies S1 and S2, the following 
constraints among its parameters must be satisfied. CO:05m<n Cl: R>3S C2:S>E C3:Z>A C4:A~8+~+:S C5: 6>&#38;+pR 
C6: 6>2(c+ps)+=+ ~+= + pA n m n m n m The proof that AO, Al, A2, A3, and CO through C6 are sufficient 
to ensure that ICA achieves S1 and S2 depends on the following 5 lemmas. Lemma 1 If the clock synchronization 
conditions S1 and S2 hold for i, and channels p and q are nonfaulty through period i + 1, then lA$~/ 
<A. Lemma 2 If channel p is nonjaulty through period i + 1, and T and II are such that T + C$) and T+lI+C$) 
are both in the interval [T(o) + c:), T(~+2) + c~+ll], then [C$)(T + H) -[Cg)(z ) + 11]1< ; II-II. Lemma 
3 If the clock synchronization conditions S1 and S2 hold for i, channels p and q are non~auhy through 
period i + 1,and T G S(i), then IC$)(T + L$j) -C$)(T)I < c + PL$. Lemma 4 If the clock synchronization 
conditions S1 and S2 hold for i, channels p, q, and r are non­faulty through period i +1, and T c S(i), 
then lc~)(T) + A~j -[c$)(T) + A$~]l < 2(c + /es) + pA. Lemma 5 If the clock synchronization condition 
S1 holds for i, channels p and q are nonfault y through period i + 1, and T 6 S{i), then We invite the 
reader to contemplate proof of these lemmas and to apply them in a proof of the theorem that S1 and S2 
are satisfied by ICA. In the next section we describe our formzd verification of this analysis, and the 
benefits we derived.  4 Formal Verification Although a broad understanding of why ICA works can be obtained 
fairly readily, detailed proof of its lemmas and of its main theorem requires atten­tion to a mass of 
details and an astonishingly in­tricate argument. The journal proof by Lamport and Melliar-Smith [18] 
is unusually precise and de­tailed, yet it is hard to internalize: it makes use of approximate arithmetic 
and neglect of insignificant terms, and when we examined it we were unable to convince ourselves of all 
the details after several days study. We needed to be convinced of the de­tails, and of all the assumptions 
that underlie the proof, because we had the goal of designing a com­bination of hardware and software 
to implement the algorithm. ICA assumptions such as A2 be­come specifications for the hardware that will 
per­form the exchange of clock values, and constraints such as C6 determine the closeness of the synchro­ 
nization that can be assumed by the rest of the DFCS redundancy management software. In order to resolve 
our doubts concerning the analysis of ICA, we embarked on a formal verifi­cation of ICA in 1988, using 
an early version of our EH DM formal verification system [25]. Our goal was to obtain a complete understanding 
of the ar­gument for the synchronization bound maintained by ICA, and a complete enumeration of the as­sumptions 
on which the argument depends. As we performed the formal specification and verification, we discovered 
that the presentation given by Lamport and Melliar-Smith was flawed in several details. One oft he principal 
sources of error and difficult y was their use of approximations-i .e., approximate equality (%) and 
inequalities (~ and >) in order to simplify the calculations . We eventually found that elimination of 
the approxi­mations not only removed one class of errors, but act ually simplified the analysis and presentation. 
We also found and corrected several other techni­cal flaws in the published proof of Lamport and Melliar-Smith. 
In total, we found that four of the five lemmas in Lamport and Melliar-Smith s proof were false, or flawed 
in some other way, and that the main induction was incorrect. Some of these errors are painfully obvious 
once they have been spotted: for example, the problem in the main induction is that it seeks to establish 
S1, but the inductive step pro­vides only that is strict inequality is assumed, but only an approximation 
is delivered. Other flaws include missing, or insufficiently tight constraints in the statements of some 
lemmas, and typographical er­rors in two of them. Our corrections required slight modifications to the 
assumptions underlying the al­gorithm, and to the constraints on its parameters, and thus changed the 
external specifications of the algorithm. The present ation in the previous sec­tion used our corrected 
statements of assumptions, constraints, and lemmas. A full discussion of the flaws in Lamport and Melliar-Smith 
s original pre­sent ation, and of our revisions, is available in our report on the verification [24, 
chapter 3]. In addition to identifying and correcting flaws in the published analysis of ICA, we were 
able to ex­tract a journal-level description of our revised anal­ysis fairly directly from the text of 
our formal spec­ification and proofs. The EH DM theorem prover is driven from proof descriptions that 
explicitly list the instantiation of all lemmas and axioms to be used in the proof concerned. While tedious 
to con­st ruct, these descriptions provide the key informa­tion needed for a journal-level proof. For 
example, the EH DM proof description for Lemma 2 lists the definition of a good, clock, the definition 
of the log­ical clock values C$)(Z ) and C$)(T + II), and the assumption Al as the key ingredients of 
the proof. It is easy to construct the journal-level proof from these facts. Another benefit that we 
derived from our formal specification and verification of ICA was a com­plete enumeration of all the 
assumptions, defini­tions, and constraints employed. This enumeration is a by-product of the EH DM proof-chain 
checker, which examines the macroscopic structure of a ver­ification in order to ensure that there are 
no undis­charged proof obligations and no circularities in the argument. It is important to those contemplat­ing 
the construction of hardware support for ICA to have this enumeration of assumptions available, since 
it comprises the specification for their part of the overall enterprise. Yet another benefit that we 
derived from our for­mal verification of ICA was the ability to explore the consequences of changed assumptions. 
For ex­ample, the journal proof of Lamport and Melliar-Smith employ~oin assumption that the initial clock 
corrections CP are all zero. We incorporated this assumption into our formal verification. Later, when 
we were contemplating implementation of the algorithm, we recognized that this was a very in­ convenient 
constraint and wondered if it could be eliminated. We explored this possibility by simply eliminating 
the constraint from the formal speci­ fication and re-running all the proofs. We found that the proofs 
of a few internal lemmas needed to be adjusted, but that the rest of the verification was unaffected. 
We found the formal verification of ICA to be quite challenging. The initial effort took a little over 
a man-month, with the properties required of summations and other supporting theories treated as temporary 
axioms (i.e., unproven lem­mas) at that stage. We gradually developed sat­isfactorily primitive axiomatizations-or 
construc­tive definitions-for the supporting theories and developed proofs for all our temporary axioms. 
The current version of the verification uses 19 ax­ioms (most of which are used to specify the ICA and 
its assumptions and constraints), and requires mechanical checking of a shade under 200 proofs. In the 
following section we draw on our experi­ences in verifying ICA and other critical algorithms to identify 
the capabilities required or desirable in a formal verification system to undertake these ver­ifications 
with maximum benefit and ease. Capabilities Required Our verification of ICA helped us identify short­comings 
in the implementation, specification lan­guage, and theorem prover and other tools of EH DM aS well as 
confirming a number of its good features. In the years since we first completed this verification, we 
and our colleagues have worked to improve EH DM, and have tested the benefits by undertaking several 
other verifications of difficult fault-tolerant algorithms [23, 27], as well as a num­ber of moderate 
test-pieces including the Oral Mes­sages algorithm for Byzantine Agreement [21], the finite Ramsey Theorem 
[12], and several examples in hardware verification and software specification. We have revised our specification 
and verification of ICA several times in order to take advantage of new capabilities in EH DM, or new 
insight into how best to formulate certain properties. In this section we draw on the experience gained 
in performing these verifications and identify what we consider to be important issues in the design 
of specification languages and verification systems. We will use ICA to supply concrete illustrations 
of our points, and suggest that the ability to specify and verify ICA in reasonable style could be consid­ered 
a minimum benchmark for mechanized formal verification systems.4 The first point to note is that mechanically­checked 
verification is the very essence of exercises we have performed, and the chief source of the ben­efits 
that accrued. In some domains, formal speci­fications, possibly augmented by pencil and paper analysis, 
can provide worthwhile benefits on their 4Our original verification in EHDM [24, unrevised edition] has 
been duplicated in an extended version of the Boyer-Moore prover by Bill Young [32]. We invite others 
to try it using their own favorite formal specification and verification system. own that is, without 
undertaking mechanically­ checked verifica tion [14].5 That is not the case with most of the exam­ ples 
we have undertaken: we have been concerned mainly with algorithms and the theorems that sus­tain them, 
and these were adequately specified already-albeit in the quasi-formal style of tradi­tional mathematical 
presentations [18, 26]. Merely casting these journal-level descriptions into a for­mal specification 
language is unlikely to reap large dividends. But although we have primarily been concerned with verification, 
this does not mean that we regard formal specification as unimportant. On the con­trary, we consider 
human review essential for cer­tification of truly critical systems, and it is there­fore crucial that 
the formal specifications should be accessible to anyone familiar with traditional mat hematical presentations; 
similarly, the verifica­tion should lend itself to the extraction of a genuine proof that can be followed 
by anyone willing to de­vote modest effort. A formal specification language to be used with mechanically-checked 
verification must strike a very delicate balance between its convenience and expressiveness as a specification 
medium and the automation and effectiveness of the mechanical support that can be provided. Thus, the 
most powerful and automatic theorem provers tend to be associated with the most limited facilities for 
specification (for example, the very powerful Boyer-Moore theorem prover [4] uses a quantifier-free first 
order recursive arithmetic as its specification lan­guage), whereas some more attractive and expres­sive 
specification notations (for example, Z [29]) lack mechanical support for verifications as diffi­cult 
as that of ICA. In the next two subsections we identify some requirements on specification lan­guages 
and theorem provers adequate to the task of verifying ICA, and suggest how the competing 5Mechanical 
checking for syntactic and semantic errors is likely to be useful even when formal verification is not 
contemplated. For example, our colleague Steven Phillips translated the Unix File System example [14, 
pp. 91-140] into EHDM. The typechecking and cross-reference compo­nents of EHDM led him to identify three 
errors (or peculiar­ities) in the original Z specification. requirements for expressiveness and mechanization 
can be reconciled. 5.1 Specification Languages The purpose of specification is communication, and we 
believe that communication with those who will review or implement our specifications is best served 
by a formalism that is as close as possible to that of conventional mathematical presentations in the 
field concerned. For our verification of ICA, we had a readily available benchmark in the form of a conventional 
mathematical presentation of that very algorithm [18]. Our goal was to formally spec­ify and verify ICA 
in a manner that would lend it­self to a journal-level presentation similar t o t hat of the original. 
Examination of the original presentation reveals that it uses, at the very least, first-order logic, 
with arithmetic and full quantification. (Most state­ments are, implicitly, universally quantified in 
their free variables, but assumption A2, for example, re­quires an explicit, nested, existential quantifier.) 
Specification of the ICA algorithm uses a finite summation, which suggests that it will be desirable 
to have available a facility for recursive or iterative definitions. Summation is most naturally regarded 
as a higher-order functional (i.e., it takes the func­tion to be summed as an argument), which sug­gests 
that the specification language should admit at least that fragment of higher-order logic that al­lows 
functions to take functions as arguments and to return functions as values. The journal proof for the 
correctness of ICA is by induction, and it is easy to anticipate that establishing the proper­ties of 
summations will also require proofs by in­duction. Thus, the verification system must either provide 
a repertoire of built-in induction schemes, or the specification language must permit higher­order quantification 
(i.e., quantification over predi­cat es and functions), so that induction schemes can be manipulated 
as ordinary formulas. Next, we can observe that at least two types of numerical quantities are required 
for specification and verification of ICA. Clocktime and realtime are assumed to be dense, whereas synchronizing 
periods are numbered by the naturals. It follows that our specification language should be typed, and 
should provide interpret ations for (at least) the rational (or the real) numbers, and the naturals. 
In general, integers are required as well. Considered as numerical types, clocktime and re­altime share 
the same properties, yet they cannot be combined freely: for example, a formula that adds a term of type 
clocktime to one of type re­altime is likely to be erroneous. Thus there is a distinction between clocktime 
and realtime akin to the notion of dimension, which distinguishes length from velocity even though both 
have the same nu­merical properties. A good specification language should make it possible to catch simple 
errors early, and by simple means; hence, strict typing, possibly augmented by dimensions, is very desirable. 
Although not needed for ICA, type-constructors such as records, enumerations, and tuples are in­valuable 
in most specifications, as is the ability to introduce new, uninterpreted types. Not only is it important 
for a specification lan­ guage to provide a familiar and reasonably rich logic, we consider it important 
that it should, to the extent possible, express that logic in familiar not ation. For example, a rich 
set of propositional connective can enhance the readability of the spec­ification. Thus, specification 
of ICA uses a poly­morphic if-then-else in the definition This is equivalent to the formula and there 
is no reason why the verification system should not silently perform this if-lifting t rans­formation 
and allow the specifier to use the more convenient if-then-else form. Similarly, we should expect to 
be able to write arithmetic operators and relations in their famil­iar infix form, and to be able to 
overload infix and other symbols in a reasonably natural way so that the + in a + g is interpreted appropriately 
accord­ing to the types of its arguments, and the context of its use. Maintaining such a natural notation, 
while providing a rigorous yet straightforward se­mantics and supplying the most effective informa­tion 
to the arithmetic reasoning component of a theorem prover, can involve some sophistication. For example, 
notice that the constraint C6 in­volves several division operations, wit h divisor n m. Clearly, the 
logic must make some pro­vision for partial functions such as division (which is undefined if the divisor 
is zero). There are sev­eral ways to do this. One, which we favor, avoids partial functions (which rapidly 
complicate matters and can require a three-valued logic [6] or a logic of partial terms [10] ) and instead 
defines the signature of division as Q x QZ + Q where Q! denotes the rationals and QZ denotes the nonzero 
rationals a predicatively-defined subtype of the rationals. We allow a term of a supertype to appear 
where one of a subtype is required, provided the term can be proved to satisfy the defining predicate 
of the sub­type concerned: thus theorem proving can be re­quired during t ypechecking. To see how this 
works in EH DM, consider a simplified fragment from C6: ~, where p is a rational, n and m are naturals, 
and n > m. There is no subtraction operation de­fined on the naturals, so n and m are promoted to integers 
(a supert ype of nat urahi), and n m is interpret ed as integer subtraction, yielding an int e­ger result. 
We are now supplying an integer as the second argument to the division operator, which requires a nonzero 
rational in this position. The rationals are a common supertype to both the in­tegers and the nonzero 
rationals, so n m can be promoted to a rational, and then reduced to the nonzero rational required for 
type-correctness if we can prove the theorem n m # O (which in this case follows obviously from the 
constraint n > m). Correctness of ICA is contingent upon a num­ber of assumptions that relate the values 
of several quantities to each other. Some of these can be re­garded as definitions-for example: n A$)= 
~ A(2) ( )Z VP n T=l It is best if the specification language has some definitional principle that admits 
such definitions by conservative extension that is, in a way that is guaranteed not to introduce inconsistencies. 
The definitional principle should be strong enough to admit recursive definitions, such as that underlying 
the recurrence Ensuring the termination of recursive definitions requires theorem proving in general. 
Not all the assumptions underlying ICA are sim­ple definitions, however: constraints CO to C6, and AO 
and A2, are inequalities. Some specification languages are strictly constructive and prohibit the direct 
introduction of axioms. In such cases, one must either provide a construction that satisfies the constraints, 
or make the constraints into ex­plicit hypotheses of the theorems to be proved. The first approach overspecifies 
the problem: the sense surely intended by the conventional mathematical present ation is that any implementation 
that sat­isfies the constraints is considered acceptable. It is not the job of this level of specification 
to describe or overly constrain possible implementations. The second approach makes the specification 
cumbersome without adding any useful security. The concern of those who advocate totally con­structive 
specifications is that unrestricted addi­tion of axioms can introduce inconsistencies and thereby render 
the specification meaningless and any verifications wort bless. But moving the con­straints into the 
hypothesis of the theorem provides no advantage, since altho<ugh FAOA... ACAC63S163S1 is guaranteed to 
be sound, it is useless if the an­tecedent cannot be satisfied. We argue that a useful specification 
language should permit the introduction of axioms, but should also assist (or require) demonstration 
of their consistency (i.e., the existence of a model). This differs from the purely constructive approach 
in that exhibition of a constructively defined model merely serves to demonstrate the consistency of 
the axiomatization, it is not a prescriptive part of the specification. Another glance at the journal 
presentation of ICA shows that the verification uses several proper­ties of the absolute value function 
(1A]) and of finite summations. It will clearly improve the structure and presentation of the overall 
development if spec­ification and proof of these subsidiary properties can be presented separately from 
those of ICA it­self. This argues for some sort of module structure for specifications; reuse of specifications 
argues for richer mechanisms that permit the development of self-cent ained theories, preferably parameterized. 
In the case of ICA, it is attractive if the theories of absolute value and of finite summations can be 
developed and encapsulated for later use. These should cent ain the statement and proof of gener­ ally 
useful properties, such as the triangle inequal­ ity IX + YI < 1x1 + [YI, and identities such as It is 
often necessary to place semantic constraints on the actual parameters that can be supplied to parameterized 
specification modules. For exam­ple, a module specifying the general principle of Noetherian induction 
should only be applied to a well-founded ordering. As we have noted several times before, it can require 
theorem proving to check constraints such as this. The mechanical support required for a specifi­cation 
language obviously includes a parser and typechecker. In addition, since specifications can get quite 
large (1,300 lines of specification and 472 distinct identifiers are used in our verification of ICA), 
browsers, cross-reference generators, and other similar support tools for navigating a large body of 
material are highly desirable. Finally, if formal specifications are to be used for effective communication, 
we consider it highly de­sirable that its mechanical support should include tools that can reproduce 
the typographical conven­tions of normal mathematical discourse. EH DM, for example, provides a LATEX-printer: 
a table-driven tool that converts specification text such as abs(c(p, i, T) -c(q, i., T)) into the notation 
of conventional mathematics used by Lamport and Melliar-Smith: Attractively typeset specifications are 
much easier to compare with arguments presented in normal mat hematical notation, and much easier for 
per­ sons not directly involved in the specification effort to read and study. This latter point is particularly 
important if formal specifications are to be sub­jected to useful peer review and scrutiny. 5.2 Verification 
and Theorem Proving Mechanically-checked verification requires a theo­rem prover or proof checker. (A 
proof checker is simply a theorem prover that requires more human guidance.) It might seem that the more 
powerful and automatic the theorem prover the better. This is a view that needs severe qualification 
however. First, for the purposes for which we are under­taking verification, we want to obtain a genuine 
proof that is a chain of argument that will con­vince a human reviewer rat her a mere grunt of assent 
from a mechanical theorem prover. Many powerful theorem proving techniques (for example, resolution) 
work in ways that do not lend them­selves to the extraction of a proof. Second, many of the theorems 
that we attempt to prove will turn out to be incorrect i.e., they will not be theorems. It is precisely 
because the de­tails of unverified proofs can be missing or flawed that mechanical verification is of 
value: but this means that it is at least as important for the the­orem prover to provide assist ante 
in the discovery of error, as that it should be able to prove true theorems with aplomb. These limitations 
on the utility of powerful au­tomatic theorem proving in the large do not apply in the small. Routine 
manipulations of arithmetic, equalities, disequalities, and inequalities can, and should, be completely 
mechanized. This is so be­cause a human reviewer does not need to exam­ine the argument for elementary 
deductions such asz~y~z~z+l> z,andbecausecom­pzete decision procedures are available (at least for the 
ground case) [28], so that their behavior is pre­dictable. It is hard to overestimate the contribu­tion 
of the decision procedures in EH DM towards the successful completion of our verification of ICA. Arithmetic 
reasoning is fundamental to the analy­sis of ICA and it would be enormously time con­suming to undertake 
its verification with a theorem prover that lacks facility in arithmetic. We consider that most current 
approaches to highly automated theorem proving are unlikely to be completely satisfactory for verifications 
simi­lar to that of ICA: they do not allow a journal­level proof to be extracted, are of little help 
in the discovery of errors in non-theorems, and combine poorly wit h low-level decision procedures.6 
In our opinion, the key to truly effective theo­rem proving will be a productive symbiosis between man 
and machine: the user should guide the over­all strategy and provide the insights, and the ma­chine should 
do the straightforward calculations. Key steps may include the invocation of a lemma or axiom, and the 
generalization or instantiation of variables. The goal should be to maximize the pro­ductivity of the 
human time spent in dialog with the theorem prover. The outcome of a successful proof attempt should 
be two proof descriptions (or possibly one description that serves two purposes): one should be very 
close to a proof that can accompany a journalist yle presentation of the verification; the other should 
be a description that guides the prover to repeat the proof without human guidance. The latter will be 
needed to rerun the proof at later stages in the overall verification, when many sur­ rounding details 
may have changed slightly. Ideally, this second proof description should be robust describing a strategy 
rather than a line by line argument so that unimportant changes to the specification of lemmas will not 
derail it. This rerunning of proofs is essential if we regard the purpose of mechanically-checked verification 
as the acquisition of insight rather than mere certifi­cation. As a verification develops, one discovers 
simplifications, improvements, and generalizations that should be assisted, not discouraged, by invest­ment 
in an existing verification. Even those most interested in certification should recognize that a specification 
is seldom static: a change in external requirements or in the specification of an assumed service will 
require reverification. The dynamic and creative nature of verification development argues strongly for 
the ability to per­form proofs in any order. Some systems require, or strongly encourage, a bottom up 
development in which only previously proven lemmas can be cited in a new proof. This is not the way real 
mathe­matics is performed: one generally prefers to know 6Note that the Boyer-Moore prover, which has 
been used to duplicate our verification of ICA [32], has a powerful linear arithmetic package, and was 
used in a very cent rolled, non­automatic mode to duplicate our proof lemma by lemma. whether a proposed 
lemma is adequate to its in­tended use before attempting to prove it. If proofs can be attempted in any 
order, it be­comes necessary to provide an analysis tool that examines the macroscopic structure of a 
complete verification in order to ensure the absence of circu­larities and unproved lemmas. An additional 
ben­efit of such a tool is a complete enumeration of the assumptions, definitions, and axioms on which 
the verification ultimately depends. 6 Conclusions The virtue of a logical proof is not that it compels 
belief but that it suggests doubts. [17, page 48] The benefits that we obtained from our formal verification 
of ICA can be summarized as follows.  Identification of errors in the published analy­sis and proof. 
 A corrected and simplified journal-level proof for the synchronization bounds maintained by ICA. Note 
that our corrections merely dot the i s and cross some import ant t s in the original; the substance 
of all the arguments is due to Lamport and Melliar-Smith [18].  A formal specification of the algorithm 
and a mechanically-checked verification of its syn­chronization bounds.  A complete enumeration of 
the assumptions and constraints underlying the analysis of ICA.  The ability to rapidly and reliably 
explore the consequences of modifications to certain as­sumptions and constraints.  It is the traditional 
mathematical presentation of our revised proof for the synchronization bounds maintained by ICA that 
we consider one of the main contributions of our work; we hope that any­one contemplating using the algorithm 
will study our presentation and will convince themselves of the correctness of the algorithm and of the 
appro­priateness of the assumptions (and of the ability of their implementation to satisfy those assumptions). 
We and our colleagues have derived similar ben­efits, including the identification of errors in previ­ous 
journal-level proofs [26] in other formal verifi­cations that we have undertaken [23, 27]. We do not 
claim that these benefits can only be obtained through mechanically-checked formal verification. For 
example, the flaws in the pub­lished analysis of IC!A are readily apparent. The fact remains, however, 
that these flaws were not, to our knowledge, identified by the social process of peer review and scrutiny 
to which Lamport and Melliar-Smith s paper has been subjected since its publication, but they were detected 
and we claim were bound to be detect cd by our formal verifi­cation. Contrary to parodies erected by 
some of the detractors to formal verification, a mechani­cal theorem prover does not act as an oracle 
that certifies bewildering arguments for inscrutable rea­sons, but as an implacable skeptic that insists 
on all assumptions being stated and all claims justified. It was the demands of formal verification that 
led us to scrutinize the analysis of ICA with the care required to identify its shortcomings. Similarly, 
it was the depth of understanding of the analysis that we acquired thereby that enabled us to simplify 
and improve the journal-level present at ion of the anal­ysis. Formal specification and verification 
is an unde­niably expensive way of dispelling ignorance and revealing error. The costs can be reduced 
by ap­propriate choice of specification language and the­orem prover, so that effort is brought to bear 
on the substance of the problem, and not on inciden­tal difficulties due to complexity of the notation 
or inadequacy of the theorem prover. We identi­fied some of the important issues in these regards in 
the previous section. The two principal issues are the need to reconcile the desire for expressive­ness 
in the specification language with the ability to provide effective mechanical support, and the requirement 
for the theorem prover to be special­ized towards the requirements of verification: that is, it must 
assist in the rapid identification of the sources of errors in incorrect theorems as well as in the certification 
of true theorems, it must pro­duce a proof suitable for human review, it must have powerful capabilities 
for low-level reasoning in standard theories (e.g., arithmetic), and its high­level capabilities must 
allow productive interaction with a human user. No current system for formal specification and verification 
satisfies all these requirements simult a­ neously, and we are not persuaded that many are even headed 
in the right direction. In our opinion, those that have neglected verification and mechani­ cal support 
to concentrate on specification have not faced up to the hard language design problems that come wit 
h full mechanization, those that have not done large, hard verifications underestimate the theorem proving 
power that is needed, and those that concentrate on theorem proving have inade­quately addressed the 
need to extract a genuine proof from the exercise, the need to interact pro­ductively with a human user, 
and the consequences of the fact that most putative theorems are false. Although no current verification 
system satis­fies our requirements, we believe it is feasible to construct one that will come close to 
doing so. We have found that one of the keys to reconciling the desire for expressiveness with that for 
effective mechanical support is to exploit the latter to as­sist the former: by being willing to use 
theorem proving during typechecking we have found that we can provide an attractive and expressive nota­tion 
without substantial penalty. In addition, our colleague Shankar has developed a very attractive proof 
checker combining decision procedures with effect ive user guidance.7 Another source of expense in formal 
verification is the cost of developing the supporting theories. (In our verification of ICA, for example, 
more than half the proofs are concerned with the properties of the supporting theories of summation, 
abso­lute value, induction, and some fragments of arith­metic. ) As more verifications are performed, 
we TA~ an in&#38;CatiOn of effectiveness, Shanlcw has been able to prove the theorems required to verify 
the Oral Messages algorithm for Byzantine Agreement [21] in under an hour, whereas it took a few days 
to construct these proofs in EHOM (although we were also working out how to do the specifi­cation, and 
how to formulate the theorems during this pe­riod). Bevier and Young, who have also verified this algo­rithm 
(using the Boyer-Moore prover), describe it as <quite difficult [3]. expect a library of reusable theories 
to be devel­ oped. The cost of developing the library will be amortized over many verifications, event 
ually re­ ducing overall costs significantly. Our verification of ICA, and most of the other verifications 
that we have performed, have been concerned wit h algorithms, and other abstractions. We have not proved 
the correctness of specific programs or hardware circuits, although we are currently in the process of 
verifying the design of a hardware circuit to support implementation of ICA. Verification of a physical 
artifact such as a circuit or a program is not absolute in the way that verification of an abstraction 
such as an al­gorithm may be. For example, the digital circuits that implement our ICA circuit may behave 
differ­ently than we assume, or the specification to which we verify our circuit may differ from that 
assumed by the ICA implementation that will use it. These difficulties attend any use of applied mathematics 
in engineering: the underlying model may be in­correct, and the requirements to be satisfied may be misunderstood 
[2]. Formal verification does not make these difficulties any more acute than infor­mal verification, 
nor are they solved by avoiding all intellect ual scrutiny of our designs. Formal verifica­tion does, 
however, make explicit the specification whose satisfaction is verified, and the assumptions on which 
the verification rests. It therefore identi­fies those properties whose satisfaction, or utility, in 
the physical world must be established empiri­cally. Critical systems such as DFCS require the high­est 
degree of human skill and responsibility in their design, analysis and certification. The stochastic 
behavior of ultra-reliable fault-tolerant real-time systems such as DFCS cannot be fully validated by 
purely empirical means; intellectual scrutiny and mat hematical analysis oft he design are required as 
well. We hope to have shown that formal verifica­tion is a tool that can provide practical assistance 
in discharging some of these responsibilities. Acknowledgments EH D M is the collective work of many 
people; over the last few years, much of the effort and creativity have been provided by our colleagues 
Sam Owre and Natarajan Shankar. Ricky Butler of NASA Langley Research Center provided valuable encour­agement 
and guidance in our study of fault-tolerant algorithms. References [1] Anonymous. Reprogramming capability 
proves key to extending Voyager 2 s journey. Aviation Week and Space Technology, page 72, August 7, 1989. 
[2] Jon Barwise. Mathematical proofs of computer sys­tem correctness. Notices of the AlIS, 36:844-851, 
September 1989. [3] W.R. Bevier and W.D. Young. Machine-checked proofs of a Byzantine agreement algorithm. 
Techni­cal Report 55, Computational Logic Incorporated, Austin, TX, June 1990. [4] R.S. Boyer and J S. 
Moore. A Computational Logic Handbook. Academic Press, 1988. [5] Ricky W. Butler, James L. Caldwell, 
and Ben L. Di Vito. Design strategy for a formally verified reliable computing platform. In COMPASS 91 
(Proceedings of the Si;th Annual Conference on Computer Assurance), pages 125 133, Gaithers­burg, MD, 
June 1991. IEEE. [6] J .H. Cheng and C.B. Jones. On the usability of logics which handle partial functions. 
In Carroll Morgan and J.C.P. Woodcock, editors, Proceedings of the Third Refinement Workshop, pages 51 
69. Springer-Verlag Workshops in Computing, 1990. [7] Department of Defense Tkwted Computer System Evaluation 
Criteria. Department of Defense, De­cember 1985. DOD 5200.28-STD (supersedes CSC­STD-001-83). [8] Ben 
L. Di Vito, Ricky W. Butler, and James L. Caldwell. High level design proof of a reliable com­puting 
platform. In 2nd. International Working Conference on Dependable Computing for Critical Applications, 
pages 124 136, Tucson, AZ, February 1991. IFIP WG. 10.4. [9] System Design Analysis. Federal Aviation 
Admin­istration, September 7, 1982. Advisory Circular 25.1309-1. [10] S. Feferman. Polymorphic typed 
lambda-calculi in a type-free axiomatic framework. In W. Sieg, edi­tor, Logic And Computation, pages 
101 136. Amer­ican Mathematical Society, 1990. [11] John R. Garman. The bug heard round the world. ACM 
Software Engineering Notes, 6(5):3 10, October 1981. [12] Ronald L. Graham, Bruce L. Rothschild, and 
Joel H. Spencer. Ramsey Theory. The Wiley Series in Discrete Mathematics and Optimization. John Wiley 
and Sons, second edition, 1990. [13] Richard E. Harper and Jaynarayan H. Lala. Fault­tolerant parallel 
processor. AL4A Journa/ of Guid­ance, Control, and Dynamics, 14(3):554 563, May-June 1991. [14] Ian Hayes, 
editor. Specification Case Stud­ies. Prentice-Hall International (UK) Ltd., Hemel Hempstead, UK, 1987. 
[15] R.M. Kieckhafer, C.J. Walter, A.M. Finn, and P.M. Thambidurai. The MAFT architecture for dis­tributed 
fault tolerance. IEEE Transactions on Computers, 37(4):398-405, April 1988. [16] Hermann Kopetz et al. 
Distributed fault-tolerant real-time systems: The Mars approach. IEEE Mi­cro, 9(1):25 40, February 1989. 
[17] Imre Lakatos. Proofs and Refutations. Cambridge University Press, Cambridge, England, 1976. [18] 
L. Lamport and P.M. Melliar-Smith. Synchroniz­ing clocks in the presence of faults. Journal of the ACM, 
32(1):52-78, January 1985. [19] Dale A. Mackall. AFTI/F-16 digital flight con­trol system experience. 
In Gary P. Beasley, editor, NASA Aircrafl Controls Research 1983, pages 469­ 487. NASA Conference Publication 
2296, 1984. Proceedings of workshop held at NASA Langley Research Center, October 25-27, 1983. [20] 
Dale A. Mackall. Development and flight test ex­periences with a flight-crucial digital control sys­tem. 
NASA Technical Paper 2857, NASA Ames Research Center, Dryden Flight Research Facility, Edwards, CA, 1988. 
[21] M. Pease, R. Shostak, and L. Lamport. Reaching agreement in the presence of faults. Journal of the 
ACM, 27(2):228-234, April 1980. [22] Didier Puyplat. A320: First of the computer­age aircraft. Aerospace 
America, 29(5):28 30, May 1991. [23] John Rushby. Formal specification and verification of a fault-masking 
and transient-recovery model for digital flight-control systems. Technical Report SRI-CSL-91-3, Computer 
Science Laboratory, SRI International, Menlo Park, CA, January 1991. Also available as NASA Contractor 
Report 4384. [24] John Rushby and Friedrich von Henke, Formal ver­ification of the interactive convergence 
clock syn­chronization algorithm using EHDM. Technical Re­port SRI-CSL-89-3R, Computer Science Labora­tory, 
SRI International, Menlo Park, CA, Febru­ary 1989 (Revised August 199 1). Also available as NASA Contractor 
Report 4239. [25] John Rushby, Friedrich von Henke, and Sam Owre. An introduction to formal specification 
and verifi­cation using EHDM. Technical Report SRI-CSL­91-2, Computer Science Laboratory, SRI Interna­tional, 
Menlo Park, CA, February 1991. [26] Fred B. Schneider. Understanding protocols for Byzantine clock synchronization. 
Technical Report 87-859, Department of Computer Science, Cornell University, Ithaca, NY, August 1987. 
[27] Natarajan Shankar. Mechanical verification of a schematic Byzantine fault-tolerant clock synchro­nization 
algorithm. Technical Report SRI-CSL­91-4, Computer Science Laboratory, SRI Interna­tional, Menlo Park, 
CA, January 1991. Also avail­able as NASA Contractor Report 4386. [28] Robert E. Shostak. A practical 
decision procedure for arithmetic with function symbols. Journal of the ACM, 26(2):351-360, April 1979. 
[29] J.M. Spivey, editor. The Z Notation: A Reference Manual. Prentice-Hall International (UK) Ltd., 
Hemel Hempstead, UK, 1989. [30] Interim Defence Standard 00-55, The procurement of safety critical sofiware 
in defence equipment. UK Ministry of Defence, April 1991. Part 1, Issue 1: Requirements; Part 2, Issue 
1: Guidance. [31] John H. Wensley et al. SIFT: design and analysis of a fault-tolerant computer for aircraft 
cent rol. Pro­ceedings of the IEEE, 66(10): 1240 1255, October 1978. [32] William D. Young. Verifying 
the Interactive Con­vergence clock-synchronization algorithm using the Boyer-Moore prover. Internal Note 
199, Compu­tational Logic Incorporated, Austin, TX, January 1991. 
			