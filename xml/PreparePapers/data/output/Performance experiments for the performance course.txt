
 Performance Experiments for the Performance Course Charles M. Shub Associate Professor of Computer 
Science University of Colorado at Colorado Springs 1. ABSTRACT This paper describes a newly instituted 
laboratory in the Computer Science Depart-ment at the University of Colorado at Colorado Springs. The 
reasons for develop-ing the laboratory are delineated. The equipment is then described. This is fol- 
lowed bv a brief descrintion of initial experience with the laboratory. A collec- tion of experiments 
performed within the laboratory is delineated in detail. Finally some brief thoughts on the future use 
of the laboratory and the conclusions drawn at this point in the lab's history are presented. 2. INTRODUCTION 
The Computer Science Department at the University of Colorado at Colorado Springs (UCCS) recently provided 
a laboratory for undergraduates taking the required course in Operating Systems and an elective course 
in Performance Evaluation. The purpose of this paper is to describe the reasons this laboratory was insti- 
tuted, detail the experience with the laboratory, share some example performance experiments developed 
for this lab, and suggest extensions in the use of this laboratory. The paper focuses on the per- formance 
evaluation course aspects of the lab because the prototype experiments are better defined at this stage 
of the laboratory's development. 3. WRY HAVE A LABORATORY We wanted to address national concerns for 
scientific education by not only providing students at UCCS reasonable laboratory facilities to experiment 
with operating Permission to copy without fee all or part of this material is granted provided that 
the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and 
the title of the publication and its date appear, and notice is given that copying is by permission of 
the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/ or specific 
permission. 1989 ACM 0-89791-298-S/89/0002/0222 $1.50 systems, their performance, and their internals, 
but also by serving as a development ground for a collection of widely available exercises that could 
be adapted at other institutions. Our majors take a required course in operating systems in the fall 
of their senior year. Though they have already interacted with operating systems as a user, they have 
not had any opportunities to design or code any internals of an operating system. No laboratory equipmentother 
than interactive access to the cen-tral campus computing utility was avail-able for that course. Thus 
students could not obtain hands on experience with the internals of a com-puter operating system and 
had to depend on either a simulated machine approach or mere observation of the University main frame 
system for experience in operating systems design practice. The simulated machine approach then in use, 
while use-ful, had the disadvantage of not providinghands on laboratory experience for many areas of 
concern. Timing of asynchronousoperations including input/output and interrupt processing is difficult 
in a simulated machine environment. The lack of suitable laboratory equipment for hands on operating 
system experimenta- tion resulted in the institution graduat- ing students with incomplete preparation 
for jobs in local industry. Many graduates routinely design and code device drivers and interrupt handling 
software. Numerous graduates design and code portions of operating systems. Graduates entering a career 
of software development interact with a variety of host operating systems. Exposure to operating system 
internals also enhances the ability of these students to excel at their jobs. Students in the elective 
senior level com-puter system performance course had no means of gathering instrumentation data on system 
performance or attempting to exper-iment with system tuning. The computing center was understandably 
reluctant to allow students the access necessary to experiment with operating system tuningtechniques. 
Also, with the computing center system, the workload could not be controlled. In summary, the lack of 
stand alone com-puter systems for experimentation with operating systems design and developmentseverely 
impacted two courses. Theyinclude the required core course in operating systems and the elective course 
in computer systems performance evalua-tion. 4. EQUIPMENT CONFIGURATION We elected to equip the laboratory 
with workstations instead of Personal Computersfor several reasons. 1. At the time we decided to equip 
the lab, MINIX [TANE87] was not yet avail-able and we wanted at least one rea-sonably robust operating 
system with source available. 2. We wanted equipment comparable with what students might be using in 
indus-try. 3. We wanted networking capabilities. 4. We wanted systems that ran at least two operating 
systems, and the V sys- tem [CHER87] was available at insigni-ficant cost. 5. We wanted processors fast 
enough that rebuilding software would not be a significant time waster. 6. We wanted a multiple processesenvironment 
so we could have multiple users or multiple activities by a sin-gle user. 7. We wanted reasonable development 
tools.  The final choice was to use Sun worksta-tions. The original configuration called for four identical 
systems instead of diskless stations and a server because we wanted the operating system work to be network 
independent. We ended up with a hybrid configuration because of availabil-ity and pricing changes between 
when we selected the equipment and when we were able to place the order. All four systemscurrently run 
either Sun Unix or a limited version of the Stanford V system. Switch-ing between systems is painless 
and onlyrequires minimal time to boot the other operating system. The speed is achieved byeliminating 
file system integrity checks on the "switch OS" boots. 5. LABORATORY EXPERIENCES As always, nothing 
ever goes as nicely as planned. The equipment did not arrive when promised, and there were several glitchesin 
getting the V system up and running.Because of these delays, progress on the operating systems side of 
the laboratoryis not nearly as far as originally pro-jected. The prototypes of the student assignments 
are being developed for the operating systems course, and both operat-ing systems are being used on a 
regularbasis. The development of prototype measurement experiments for the performance course seems to 
have gone well in the initial stages. The variety of performance moni-toring tools is helpful. They include, 
in addition to the normal Unix utilities and profilers, a network monitor that reports network traffic 
and load, and several tools that came with the V system. Local software has included a background load 
generator [BROW881 and a few library-likeinterfaces to make system gathered data available in a cleaner 
format.  6. LABORATORY MEASUREMENT EXPERIMENTS Below, several measurement experiments are described. 
A uniform format has been selected. For each problem, a brief state-ment of the problem is given. This 
is followed by a short discussion of the rationale behind giving the problem. Then a description of student 
approaches are given. We always emphasize the scientific method for approaching the problem. Most of 
the texts or sets of notes that discuss measurement experiments do the same. [BE12781 [BOR079] [FERR78] 
[FERRIl] We cast the problems as performance studies and may give the students the goal of the study. 
In general, the experiments are described reasonably loosely leaving much leeway.The course comes late 
enough in the student's undergraduate program that an unwritten course goal is to foster creativity and 
independent thought as well as an investigative approach. Another benefit of this approach is that there 
never are "cookbook I' answers to the prob- lems. While this can make the gradingharder, it seems (based 
on student feed-back) to be a realistic and valuable approach. For all experiments, class time was devoted 
to discussing the alternatives taken. With the graded reports in hand, student participation was vigorous 
to say the least. As will be obvious, the class- room material was helpful in the experi-mental designs 
chosen by the students. 6.1 Tool Usage This study involves looking at and evaluating the use of the following 
per-formance tools: [The specific list is omitted, but includes the standard utili-ties and any locally 
designed tools.) You may use the on/off line documentation and you may write some small programs that 
USE the tools. Try to limit yourself to at most three hours of machine connect time. Prepare a short 
report describing the available tools and how they might be used. This experiment is the first, and occurs 
early in the course. The students have been given an overview Of the Performance perspective and an overview 
of the metho- dologies that can be used. 6.1.1 What the Students Should Learn There are several important 
concepts here. The first is that there are more tools than the student could possibly hope to master. 
As such, time must be budgeted to get enough depth to make informed choices among the tools for later 
use usuallybased on functionality, and enough depth to understand the differences in resolu-tion and 
invasiveness. This helps later in developing an awareness of the need to focus on what is important and 
not be con-cerned with minutiae. The second important point is one of how students should organize their 
knowledgefor later use. 6.1.2 Typical ApproachesThough most of the students did copiousdocumentation 
perusal, a few tried to write short programs and use all the tools on the programs to see what they did. 
There were two major organizations of reports. The first was a list of tools including, for each, a description 
of what it did and how it could be used. The other organization (which proved to be more utile later) 
was organized by type of measurement one could make with the tools in that class, and how the class members 
differed in detail. 6.1.3 Future Enhancements The only problem experienced with this assignment was the 
child in a toy shopfrustration of there being more than could adequately be covered in the allotted time. 
We are considering a modification where each student is given two or three tools and must write a short 
report on the assigned tools that will be distributed to the rest of the class. This will have the advantage 
of allowing greater focus by the students, while at the same time providingthem with summaries of tools 
they have not studied. A minor disadvantage is the dif-ficulty of grading such an assignment.Another 
concern is whether the student provided documentation will be sufficient for other students to later 
use the tool.  6.2 Window Usage Many systems provide a variety of window- ing front ends to allow applications 
designers to provide elegant user inter- faces designed to increase user produc- tivity. In addition 
to the dollar cost of using such systems, there is some overhead in the systems arising from designing 
the application to run in the windowing environment, the extra space for the extra windowing software, 
and the additional software processing necessary to place output at a desired place on the screen. Prepare 
a short paper detailing the techn- ical tradeoffs involved in using a window- ing environment on the 
Sun. 6.2.1 What the Students Should Learn There are several important concepts here. First, there are 
issues of code size and start up time. Second, there are perfor- mance indices that can not be measured 
 directly. Finally, there are several issues of two different kinds related to background load, the first 
being the over-head within the windowing software, and the second being the effect of backgroundload, 
or other jobs. Finally, students discover anomalies they can't account for until they take the additional 
paging gen-erated by the windowing software into account. Using some of the graphic tools that come with 
the Sun windowing packages,typically VMSTAT and PerfMon, makes this easier, so the students get a lesson 
in how tool selection impacts the ease or difficulty of the study. 6.2.2 Typical ApproachesStudents seemed 
to want to write screen I/O intensive programs and time them in the two environments. When this was not 
enough, other approaches were tried. One student seemed to think that in the win-dowing environment the 
characters were being bitmapped into the frame buffer and there was no physical I/O going on as in the 
plain environment. Particularly interesting was the approach of starting several animation demonstra- 
tion programs in different windows and visually comparing their performance to performance of a single 
animation demo running on a different workstation.  6.3 Network Versus Local Access The Sun systems 
are networked together and provide several vehicles for accessingfiles physically resident on other machines 
and accessing the system from a remote host. The students were asked to design a study to determining 
the impor-tant factors in selecting between a net-work environment and local access to data. 6.3.1 What 
the Students Should Learn The most important issue here is determin-ing what the bottleneck is. Is it 
the net-work, the load on the network, the software processing capacity of the machine, the disk I/O 
time, or some combi-nation. Also, the problem is designedagain to be vaguely specified (real world)and 
open ended. 6.3.2 Typical Approaches There were three approaches that students took. One involved gathering 
timing data accessing both locally and remotely mounted files for comparison purposes. While this approach 
did lead to some differences in file access times, the differences were not significant and it was judged 
by the students that they were unable to saturate anything badly enough to get appreciable differences. 
 The second approach was to time local access with and without competing remote access and to time remote 
access with and without local competing access, and to attempt to draw conclusions from these comparisons. 
The third approach was to use the Remote Procedure Call facilities and compare local versus distributed 
access times. The major failing of this approach was the inability to determine how much overhead was 
in network communication, and how much was in the support software. Moreover, difficulty was experienced 
determiningwhere in the software the delays were occurring. No approach was particularly successful because 
the students were unable to gen- erate enough work to effectively push a potential bottleneck to saturation. 
6.4 Language Efficiency The students were asked to estimate the relative efficiency of various looping 
constructs, storage allocation strategies,and the like to prepare for a projectinvolving some code that 
was not meetingits real time constraints. 6.4.1 What the Students Should Learn This project has red herrings 
in it. The easiest way to compare looping constructs is to examine the code produced by the compiler. 
Also, most, if not all, typicalexperiments are rendered less than accu-rate because the optimizer can 
often change the constructs. Thus the students were rarely, if ever, evaluating preciselywhat they thought 
they were evaluating.The strangest case, for which those stu-dents who merely timed software could not 
explain was the profound difference between "while (--i)" and "while(i--)I' as loop control constructs. 
Thus, a portionof this project is to learn that there are many tools, some of them not as usual as one 
would think, that might be applicable. 6.4.2 Typical ApproachesThere were two major approaches. The most 
common was to write and time software to make comparisons. This invariably led to worse results than 
the alternate approach.The more accurate approach was to have the translator generate code and compare 
the generated code and use instruction timingdata to get the conclusions. These results were more accurate 
and allowed the stu-dents to make quantitative predictionsthat were more useful. Those of our students 
taking the elective performance evaluation course are reacting more favorably to the course (according 
to our standard course evaluation metrics)than they did before the lab came into existence. The free 
form responses show the students like the lab projects as being "frustratingly realistic" and valu- able 
for their future. Similar feedback has come from graduates returning to campus for visits. Perhaps even 
more important than either of these effects is the synergy developedwithin the department by the mere 
presenceof this equipment. Students and facultyalike are using the lab for a variety of projects and 
we now have a vastly wider Variety of creative activity taking place on campus. Thus, we conclude the 
labora- tory is already a success and will con- tinue to have beneficial effects on our program. 7. FUTURE 
USE OF THE LABORATORY The laboratory will continue to be used according to plan. That involves providingthe 
test bed for performance experimentsand the hands on operating systems inter-nals experience. Through 
time, additional performance experiments will be developedand classroom tested. Also, the operating systems 
experiments will be enhanced to the point where they are suitable for stu- dent exercises. Moreover, 
the laboratorywill be able to provide (on a time avail-able basis) facilities to allow both undergraduates 
doing independent study and graduate students doing graduate projectsand theses with a facility for their 
experiments. 8. CONCLUSIONS The project has been successful, even at this early point in the lifetime 
of the laboratory. Through this facility, all our undergraduates are getting hands on experience with 
the Unix Operating System at the User level. This makes them more marketable, and they seem to like that. 
Also, all our students are working with a prototype distributed operating system,another experience that 
enhances not onlytheir employability, but also their base Of knowledge for further education at the graduate 
level. 9. ACKNOWLEDGEMENTS Partial support for this work has been provided by the National Science Foundation's 
College Science Instrumenta- tion Program, Grant # CSI-8650449. Additional support has been provided 
by Sun Microsystems. 10. REFERENCES [BEIZ78] Beizer, Boris, "Micro Analysis of Computer System Performance," 
Van Nostrand Reinhold, 1978. [BOR079] Borovits, Israel and Neumann, Seev, "Computer Systems Perfor-mance 
Evaluation," D. C. Heath, 1979. [CHER87] Cheriton, David R., "The V Dis- tributed System," Stanford University, 
November, 1987. [FERR78] Ferrari, Domenico, I'Computer Systems Performance Evaluation," Prentice Hall, 
1978. [FERR81] Ferrari, Domenico and Spadoni,Massimo, "Experimental Computer Performance Evaluation,l' 
North Holland, 1981. [TANE87] Tanenbaum, Andrew S., "Operating Systems Design and Implementa-tion ,I1 
Prentice Hall, 1987. 
			