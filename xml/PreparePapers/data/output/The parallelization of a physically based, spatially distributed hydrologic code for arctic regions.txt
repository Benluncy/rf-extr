
 THE PARALLELIZATION OF A PHYSICALLY BASED, SPATIALLY DISTRIBUTED HYDROLOGIC CODE FOR ARCTIC REGIONS 
 D. J. Morton * Z. Zhang and L.D. Hinzman The University of Montana - Missoula University of Alaska Fairbanks 
morton @cs. umt. edu zhang@arsc, edu ffldh @aurora. alaska, edu S. O'Connor Department of Mathematical 
Sciences Cameron University, Lawton, OK spoconno @wolverine. cameron, edu Keywords: Parallel processing, 
Hydrologic model. sensitive to potential climate change than temper- ate and tropical regions [3, 6]. 
The present knowl- edge of hydrologic processes in the arctic is defi- ABSTRACT cient, and in order to 
more effectively incorporate these into models it is necessary to construct and A computer model which 
simulates hydrologic pro- verify computer methods for simulating the numer- cesses in arctic regions 
is parallelized for faster ous interactions that take place. execution, primarily on Cray MPP architectures 
Researchers at the University of Alaska have de- but also with portability to other platforms in veloped 
computer models over the years for simu- mind. Because computations are concentrated ir- lating hydrologic 
and thermal processes in arctic re- regularly through the problem domain, a clas-gions, and these are 
currently aimed at the Kuparuk sic domain-decomposition fails to provide perfor- River basin on Alaska's 
North Slope, where .years of mance gains. Additionally, fine-grained computa- hydrological data exist 
as input data and for com- tions impede progress. In this report, we discuss parison against model results 
[5, 4]. Previous to par- the approaches attempted towards parallelization, allelization, the hydrologic 
code was taking approx- from Cray's CRAFT compiler, to MPI and Cray's imately eleven hours on a Cray 
Y-MP to perform a shmem, and the use of a graph partitioning library ll0-day simulation of the seasonal 
hydrological cy- for load-balancing the computations. Performance cle in the Imnaviat Creek watershed, 
a 2.2+ km 2 results for the Cray T3D, T3E, and a cluster of basin (represented as 4200 triangular elements) 
lo- Linux Pentium workstations are provided. cated within the Kuparuk River basin. Since more rigorous 
and extensive simulations were desired, it INTRODUCTION was necessary to improve performance. Therefore, 
the code was to be ported to and parallelized for the The prospect of global climatic change has fueled 
Cray T3D at the University of Alaska's Arctic Re- the need to improve our understanding of the many gion 
Supercomputing Center. There was also inter- interacting atmospheric, aquatic and terrestrial pro- est 
in parallelization for other architectures, includ- cesses. It has become evident that most of these 
ing a cluster of Linux Pentium PC's, so portability processes are interactive and that we need to under- 
issues were addressed. stand the various interdependences. Many of these linkages occur through the process 
of mass and en- INITIAL ANALYSIS ergy transfers in the hydrologic regime. At the same time, it appears 
from simulations of general circula- The first step in parallelizing an unfamiliar com- tion models that 
arctic regions of the world are more puter code is, of course, to analyze it, trying to comprehend the 
general program flow and deter- "This work was funded by the Arctic Region Supercomputing mine what sections, 
or hot spots, might benefit the Center and SGI/Cray Research lnc, through the 1996-97 Uni- versity Research 
~ Development Grant Program and by the Na- most from parallelization. tional Science Foundation through 
grant number ASC-9796204+ Permission to make digital/bard copy of all or pan of this work for personal 
or DESCRIPTION OF MODEL classroom use is granted without fee provided that copies are not made or distributed 
|br profit or commercial advantage, the copyright notice, the title of the Like many computer models 
of physical processes, publication and its date appear, and notice is given (hat copying is by pcmlission 
of this code consists of an initialization section fol- ACM. Inc. To copy other~vise, to republish, 
to post on servers or to redistribute to lowed by an iterative time-stepping section. In- lists, requires 
prior specific permission and/or a t~e. &#38;#169; 1998 ACM 0-89791-969-6/98/0002 3.50 put data includes 
representations of the spatial dis- 684 E ~!:!~!~i:i:i~i~i:i~i:i~i~i:i~i:i:~i~i~.~2~i~!~i:i:!~:i~j~i~i~i~i~i~i~:i:i~i~i~i:i~i~i~!:!:i~i~!~!:~:::::::::::::::::::::::::::::::::::::::::::::~:!~i~:i:i~2~:.~i~ 
:;:!:;:~:~:i:i:i:!:~:i:i:i:i:i~:i:!:!:~:!:~:~:!:i:i:i:!:i:~ ~::'x':~" !i.:'::i:~.:~ii?~:i:!;:':~: ~::':'.':~':i~::. 
":!ii~.:'.'iii!i~:i~:'~~ !!~ii~iiii!iii:':"::::~:i:!:~:~:i............ !;iiiii~i~!!~..;.i!i~i~!~i;i~iiiiiil 
:i:i:~!:~:i:~ :'!~:~:i:i:~:~:~:~:~!: iiiiiiiiiii',', , ,,:: :'' !iiiiiiiii  Figure 1: Triangular element 
partitioning of Im- naviat Creek watershed. Figure 2: Channel network derived for lmnaviat Creek watershed. 
cretization, various physical and control parame- ters, and temporal data such as measured precip- itation 
values. Spatial discretization of the problem domain is accomplished by partitioning the region into 
a mesh of triangular elements (see Figure 1). Each triangle vertex (or node) in the mesh has three-dimensional 
Cartesian coordinates associated with it, and the in- put file representing this discretization is formatted 
as a list of element numbers, along with their nodal coordinates. Thus, after reading this input file, 
the program has a three-dimensional discretized repre- sentation of the region being modeled. The spatial 
representation of the region is next analyzed in order to, among other things, determine the paths that 
water will follow through the region -in other words, the drainage or channel network Figure 3: Predicted 
water storage (in meters) in Im- naviat Creek watershed. Horizontal plane depicts storage on the underlying 
topography. . o,~'/~,wO , ~ut/n~ ::Z:::::: ~,,j~..,...~ ~d~r for Jsl,SO0 "~I ........ ,I ', ~mdtor 
Figure 4: Structure of source code with expansion of primary components. consists of a series of connected 
channel segments, the segments simply being the edges of the triangles along which water will drain. 
The algorithm that constructs the channel network is beyond the scope of this discussion, but the interested 
reader may consult [2] for details. After the drainage network has been established, the iterativ.e time-stepping 
begins, simulating such things as the flow of water through the region, and response of the soil to hydrological 
and thermal pro- cesses. For example, one predicted variable, the amount of water stored throughout the 
region at any given time (as depicted in Figure 3), can in part determine the amount of moisture available 
for evapotranspiration, which in turn can be uti- lized in predicting cloud formation processes for cli- 
mate models. Within each time-step, a number of processes are considered on different scales of tem- 
poral resolution. Referring to Figure 4, we see that in a given time-step, the call to subroutine over-flow() 
(which calculates overland flow across the region) results in a set of loops, one of which calls the 
channels), which in turn has numerous nested loops. The series of nesting that we observe in this section 
results from working with each element and/or channel segment at successively higher tem- poral resolutions. 
An initial performance analysis of this code on a single processor of the Cray T3D showed that out of 
the 30.98 seconds spent in a timestep (note that the timing functions were intrusive and, normally, serial 
execution of a timestep would not have taken so long), 30.92 seconds were spent in the call to overflow 
0 (or in lower level routines called by over. flow()). Further analysis revealed that within one of overflowO's 
loops, routing() was called repeat- edly, and the accumulated time spent in calling and executing routing() 
amounted to 25.92 of the 30.92 seconds spent in overflow(). Thus, it seemed ob- vious that any significant 
performance gains would need to be attained by focusing on the paralleliza- tion of this fine-grained 
region of code. QUICK AND DIRTY PARALLELIZATION WITH CRAFT The initial attempt at parallelization was 
intended to be exploratory in nature, providing a quick, but presumed inefficient, approach towards speed- 
ing up the code. CRAFT (Cray Research Adaptive ForTran) was used as a high-level tool for distribut- 
ing array data and loop iterations within the rout- ing() subroutine. For example, a serial code seg- 
ment such as real a(S12), b(S12) do 100 i=l,S12 a(i) = sqr~ (a(i)**2 + b(i)**2) 100 continue could 
be parallelized easily by using CRAFT com- piler directives to distribute the array elements and loop 
iterations as follows: real a(512)0 b(512) CDIR$ SHARED a(:BLOCK), b(:BLOCK) CDIR$ DO SHARED (i) ON 
a(i) do 100 i=1,512 a(i) = sqrt (a(i)**2 + b(i)**2) 100 continue So, for example, if four processors 
are available, the above code will distribute the arrays in 128- element blocks across the processors, 
then insure that each processor executes only 128 iterations of the loop, operating on its own data. 
Thus, P0 will hold the first t28 elements of arrays a and b, and will execute loops 1 through 128, while 
P1 holds the next 128 elements, and simultaneously executes loops 127 through 256, and so on. If, as 
is often the case, a non-locM array element is referenced, then its value is implicitly read or written 
on the remote processor, the underlying data transfer being hid- den from the programmer. Such a programming 
model can be easy to use, as it hides much of the tedious detail found in message-passing codes, but 
one disadvantage is that the programmer can construct parallel code which may not fully address the importance 
of data lo- cality. In the code described above, although the computationally intensive loops could have 
data and iterations partitioned according to the CRAFT model, where the i th iteration is performed on 
the processor possessing the i th vector elements, there may be some indirect array references of the 
form a(index(i)) = b(i) where index(i) points to an array element not lo- cated in the processor which 
stores element i. The innermost loops of routing() were struc-tured much like the above serial examples 
-there were several vectors indexed by channel segment number, so the channel segments could be parti- 
tioned across the processors (e.g. for N channel seg- ments and P processors, Processor 0 stores entries 
for channel segments 1 to N/P), and, for the most part, processors performed iterations which refer- 
enced locM data. At the same time, each loop con- tained a few indirect references to array elements, 
meaning that occasionally a processor would need an array element that was located on another pro- cessor. 
Despite this potential overhead, the CRAFT model was implemented on the code, with perfor- mance results 
shown in the column labelled CRAFT in Table I. Although the CRAFT implementation provided satisfactory 
performance gains that were relatively easy to achieve, it was not believed to be a good long-term solution. 
First, CRAFT is not a portable approach, although it does have much in common with High Performance Fortran 
(HPF). More im- portantly, it was believed that performance attained via CRAFT compiler directives wouldn't 
compete favorably with that attained through a lower level message passing approach. Just as assembly 
lan- guage programs can often show marked perfor- mance gains over higher-level languages, it was felt 
that a message passing approach could be more 'qine tuned." Table 1: Wall time (in seconds) required 
for one timestep of parallel hydrologic code for various Cray T3D implementations. MPI + No Data' MPI+METIS 
Processors CRAFT MPI METIS Exchange +SHMEM 2 7.44 6.39 6.18 6.26 5.11 4 4.62 5.15 4.80 4.44 3.63 8 3.36 
5.02 4.58 3.52 2.97 16 2.80 6.04 5.62 3.15 3.15 32 3.20 9.34 9.84 2.88 2.73 CONVERSION FROM CRAFT TO 
MPI Because the results of the CRAFT implementa-tion were encouraging, the initial message pass- ing 
approach was to attempt replicating the struc- ture of the CRAFT code. This required that the actions 
performed implicitly through CRAFT di- rectives be explicitly performed through message- passing -channel 
segment data had to be parti- tioned across the processors and remote references had to be facilitated 
through message passing. Be- cause a few of the array references in routing() were actually references 
to non-local entries, it was nec- essary to understand when this happened and in- sure that non-local 
data was available to a process when needed. The memory accesses required by a process were the same 
from timestep to timestep, so it was possible to construct a mapping of these accesses during program 
initialization. Thus, be- fore the timestepping ever started, every process had knowledge of where it 
would need to get non- local data from, where it would need to send its local data, and exactly what 
data needed to be ex- changed. Further, the mappings were constructed so that a process could get all 
required non-local data from another given process in a single stream of data (i.e. sending a vector 
in a single transmission) rather than in individual messages for each piece of data, reducing the cost 
associated with message- passing latencies. At key synchronization points within a timestep, data was 
exchanged by use of Message Passing Interface (MPI) [8] send and re-ceive library calls, insuring that 
data from other processes was available when needed locally. The details of the mappings and the data 
exchanges at each timestep were all hidden inside of new function calls, so that in most cases calling 
new functions was the only modification required in the original serial code. Performance results (displayed 
in Table 1 un- der the MPI heading) were expected to be superior to those of the CRAFT version, but turned 
out to be disappointing.  DEEPER ANALYSIS AND SUBSEQUENT LOAD BALANCING While trying to understand why 
performance was worse using MPI, it was observed that the number of non-local memory references varied 
substantially from process to process. Because the MPI imple- mentation had tried to build on the relative 
success of the CRAFT implementation by utilizing simi- lar data partitioning 'schemes, the channel segment 
data in routing() was partitioned among the pro- cesses in a block fashion based on channel segment numbers. 
For example, for N channel segments and P processes, each process would get a contigu- ous block of NIP 
channel segment data -Process 0 would get channel segments 1 through N/P, Pro-cess 2 would get channel 
segments NIP + 1 through 2N/P, and so on. It was also noted that the indirect references in subroutine 
routing() were to physically adjacent channel segments (which is intuitive, since water flows from one 
channel segment to another). As illustrated in Figure 5, this may lead to excessive interprocessor communications. 
The original serial code, when constructing the network of connected channel segments, was not concerned 
with assigning channel segment numbers in a manner that would physically place channel segment i next 
to chan- nel segment i -F 1 - the channel segment numbering scheme seen in Figure 5 is quite realistic. 
Because the CRAFT and MPI approach towards partition- ing segments among processors had been based on 
channel segment numbers, it was likely that many references to adjacent elements would in fact require 
access to remote data. For example, in the numer-ical partitioning scheme {Figure 5a), computations for 
channel segment 7, located in Processor 0, would require references to segments 11 and 15, both lo- cated 
in Processor 1. Thus, it was obvious that the number of interprocess c.ommunications would be substantially 
reduced if the channel segments could be partitioned on a spatial basis, as is depicted in Figure 5b 
where the only interproeessor communi- cation requirements occur in the computations for 687 3/D 4! ...... 
, Z " /¢ l* ~ll i'~,11, N i2 (') (b) Nwme¢lcal l~rlltionin I SpatiaIPartiUoN~ Figure 5: Numerical versus 
spatial partitioning of channel segments among two processors. Dashed channel segments are in Processor 
0, solid channel segments are in Processor 1. channel segments 8 and 14. To repartition the channel 
segments for reduced inter-processor communication, the software pack- age METIS [7] was utilized. METIS 
is a freely avail- able, powerful graph partitioning system that con- sists of both a command line interface 
for interac- tive use and a programming interface for use within C and/or Fortran programs. The programming 
interface (libmetis.a) was compiled on the T3D, and the data structures for the channel segment network 
were translated to a graph adjacency list (channel segments became graph vertices and con- nections between 
channel segments became graph edges) for input to the appropriate partitioning rou- tine. The partitioning 
routine determined a load- balanced distribution of the channel segments with minimal edge-cut. The channel 
element distribu- tion provided by the METIS routines was quite suc- cessful. In a 2-PE scenario without 
this partitioning (i.e. processors were assigned channel segments in a simple numerical order), each 
process naturally had an equal number of channel segments, but the edge cut was such that each process 
would need to make over fifty non-local memory accesses. The use of METIS reduced this edge cut to three, 
meaning that the number of non-local memory accesses was substantially reduced. The METIS graph partitioning 
algorithm showed remarkable improvements in reducing the number of inter-processor communications, yet 
ac- tual performance gains (given in the MPI+METIS column of Table l) were still worse than that pro- 
vided by the CRAFT implementation which, of course, didn't attempt to reduce the number of in- terprocessor 
communications. Thus, it was hypoth- esized that the MPI Send/Receive functions exhib- ited large iatencies 
relative to the CRAFT implicit shared memory approach. In order to test the im- pact of these latencies, 
the MPI send and receive function calls were commented out, and the pro- gram was executed with no communication 
within the routing 0 subroutine. The timings, given in Ta- ble 1 under the column labelled No Data Exchange, 
confirmed that the MPI communications were in- deed the bottleneck in the computations. One must keep 
in mind that this code exhibits extremely fine- grained behavior. Referring again to Figure 4, recall 
that parallelization is taking place within the loops of routingO, that routing 0 is invoked by overflow() 
sixty times in a given time step, and routing() itself has an outer loop with thirty iterations. Each 
of the outer loop iterations in routing 0 requires data exchanges, implying 1800 data exchanges within 
a time step. Therefore, even if the latency and trans- mission times of a single data exchange are small, 
they will become substantial over a large number of iterations. CRAY SHMEM Since latency of MPI send 
and receive function calls seemed to be the limiting factor in achiev- ing CRAFT-like performance gains, 
CRI's shmem (shared memory) environment was considered for the fine-grained communications portions of 
the code. shraem is a library of functions ajnd subrou- tines developed by Cray Research, Inc. to facil-itate 
structured communications via shared mem-ory on the Cray MPP line of computers [1]. It provides low-level, 
efficient interprocessor commu-nication, much of which can be utilized with the shmem_get 0 function 
for retrieving data from a re- mote processor and shmem.put 0 for placing data on a remote processor. 
Because it is so low-level, the programmer must be aware of and deal with issues of cache coherency. 
Replacement of MPI send and receive function calls with shraem_put 0 and shmem_get 0 calls in the fine-grained 
data ex- changes provided promising results, as seen in the column labelled MPI+METIS-t-SHMEM of Table 
1.  SCALABILITY The test data utilized in this developmental work represented a relatively small region. 
The region was partitioned into 4200 triangular elements, and the number of channel segments (remember, 
these are where the parallelization is focused) was 500. Therefore, it is not surprising that performance 
deteriorated as more processors were added and the computations became increasingly fine-grained within 
overflow 0 and routing(). A larger data set (6448 elements and 730 channel segments) was re- cently made 
available, and, as anticipated, more re- Table 2: Wall time (in seconds) required for one timestep of 
parallel hydrologic code for 6448- element problem. NA entries signify that timing data m not availal 
[e. Processors T3D T3E Linux 2 i5.24 4.64 17.74 4 9.92 3.54 14.80 8 7.21 3.04 19.92 16 5.95 2.86 NA 32 
5.44 NA NA spectable speedups were obtained (see Table 2 - this table illustrates MPI/SHMEM performance 
on the T3D, T3E, and MPI performance on a Linux clus- ter). Much larger data sets, on the order of 51,700 
elements, will be utilized in the future for modeling larger regions, and it is expected that the approach 
developed for this project will provide substantial benefits in obtaining results in a reasonable period 
of time. CONCLUSIONS AND FUTURE DIRECTIONS Surprisingly, the "quick and dirty" CRAFT imple- mentation 
was the most effective in terms of pro- gramming effort versus performance gains. How-ever, due to the 
irregular number of non-local mem- ory accesses in routing(), CRAFT is probably not an efficient approach 
by itself. Different data sets would likely present very complex partitioning of the channel segments, 
which would result in a large edge-cut, leading to numerous fine-grained, implicit interprocessor data 
exchanges. Addition-ally, CRAFT is not portable, although the features utilized in this code may be implemented 
with con- structs available through High Performance Fortran (HPF) implementations. Much of the parallelization 
via MPI was per-formed on a cluster of Linux workstations as a demonstration of the code's potential 
portability and the feasibility of doing such developmental work on a low-cost parallel architecture. 
Due to the fine- grained nature of the message-passing, the Ether- net network was not conducive to significant 
per- formance gains. However, it is expected that even the Linux cluster would produce better speedups 
as the problem size increased, exhibiting coarser gran- ularity.. Work is currently under way to couple 
the hy- drologic model discussed here, with a recently paral- lelized thermal model for arctic regions. 
The hydro- logic model does not emphasize the accurate sim- ulation of heat transfer, while the thermal 
model does, and the thermal model does not emphasize the accurate simulation of moisture transport, while 
the hydrologic model does. The integrated parallel models will attempt to capture the behavior of the 
various feedback loops in which thermal processes influence hydrologic processes, and vice versa. REFERENCES 
[1] Ray Barriuso and Allan Knies. SHMEM User's Guide. Cray Research, Inc., 1994. Revision 2.0. [2] N. 
K. Garg and D. J. Sen. Determination of wa- tershed features for surface runoff models. Jour-nal of Hydraulic 
Engineering, 120(4):427-447, 1994. [3] L. D. Hinzman and D. L. Kane. Potential re- sponse of an arctic 
watershed during a period of global warming. Journal of Geophysical Re- search, 97(D3):2811-2820, 1992. 
[4] D. L. Kane, L. D. Hinzman, C. S. Benson, and K. R. Everett. Hydrology of imnavait creek, an arctic 
watershed. Holarctic Ecology, 12:262-269, 1989. [5] D. L. Kane, L. D. Hinzman, C. S. Benson, and G. E. 
Liston. Snow hydrology of a head- water arctic basin: 1. physical measurements and process studies. Water 
Resources Research, 27(6):1099-1109, 1991. [6] D. L. Kane, L. D. Hinzman, and J. P. Zarling. Thermal 
response of the active layer to climatic warming in a permafrost environment. Cold Re. gions Science 
and Technology, 19:111-122, 1991. [7] George Karypis. METIS: Unstructured Graph Par- titioning and Sparse 
Matrix Ordering System. http://www.cs.umn.edu/karypis/metis/metis.html. [8] Peter S. Pacheco. Parallel 
Programming With MPL Morgan Kaufmann Publishers, 1997. ~" ~ Z~:.~:~ ~- ~-:~...~ .~ .,. ....  689 
........... " ...... :  
			