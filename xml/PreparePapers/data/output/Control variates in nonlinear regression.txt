
 Proceedings of the 1983 Winter Simulation Conference S. Roberts, J. Banks, B. Schmeiser (eds.) CONTROL 
VARIATES IN NONLINEAR REGRESSION James J. Swain School of Industrial and Systems Engineering Georgia 
Institute of Technology Atlanta, Georga 30332 Control variates can be applied to Monte Carlo sampling 
experiments to improve the precision of the results. This method is especially useful in statistical 
problems were low order approximators of a particular variate of interest are available and possibly 
several statistical properties of the variate are to be investigated. In this paper a control variate 
scheme based on the linear approximator ~ of the nonlinear parameter estimator ~ is used to improve the 
precision of the first four moments of ~ and the covariance matrix of the paramter estimates. The control 
variate method is shown to improve the effectiveness of the Monte Carlo results without substantially 
increasing the estimation effort, and it is effective over a wide range of nonlinearities. An approximate 
expression for the effectiveness of the control variate method based on the Beale measure of nonlinearity 
N~ is given. i. INTRODUCTION In the nonlinear parameter estimation or regression problem, the p-vector 
of parameters are estimated from the n responses Yi (i=l, ..., n) which are assumed to consist of the 
true response ~ (~i ; ~) plus an additive error c i , Yl : n(x-i; ~o ) + ~i" (1) The e i are assumed 
to be independently and normally distributed random errors with variance 02 , and ~(x; 8) is a nonlinear 
function of the parameters ~. The unknown parameters ~ are most often estimated by the method of least 
squares. That is, the estimator ~ is chosen to minimize the sum of the squared residuals n n  S(!) 
= % (yi_~(x_i; ~))2 = % e~(! ) (2) i=l i=l  so that s(~) = min S(~). Of course, other estimators for 
8 can be used, but the least squares method is the most common and will be treated here. The nonlinear 
regression problem is similar to that of linear regression, save that the nonlinearity of the response 
function complicates the numerical problem of obtaining the estimates and of resolving the statistical 
properties of the estimators obtained. The solution to the least squares problem is generally not a closed 
form function of the observed responses Yi so that iteratlve procedures are needed to obtain a solution 
(Bard, 1974; Gallant, 1975). The lack of a closed form solution makes the distribution ^ of the estimator 
~ intractible, and the sampling distribution of the estimator ~ is known exactly only asymptotically. 
The asymptotic approximation of the sampling distribution can be very misleading in problems with a small 
sample size. In particular, the finite sample estimator is almost always biased and the shape of the 
confidence region can differ markedly from the elliptical contours of the asymptotic sample distribution. 
 CHI953-9/83/OOUO-0623 $01.00 o 1983 IEEE 624 James J. Swain The statistical properties of the estimator 
 A can be approached using the asymptotic distribution, through a series approximation solution, or 
via Monte Carlo sampling. Under certain circumstances (see Gallant, 1975, for  instance) the distribution 
of ~ is asymptotically normal with mean 60 and variance matrix (FT(Oo)F(00) )-1o2, where F(~) is the 
n by p. Jacobian matrix of first derivatives of n(x;0)  with respect to the parameters 0 and evaluated 
at ~O" Note that the variance matrix is the limiting matrix as the sample size n increases indefinitely. 
To use the asymptotic approximation in practice one uses the finite sample and the two formulas given 
above. This solution is essentially the first order approximation to be described.  Series approximations 
can be obtained by approximating the sum of squares function S(~I) or its derivative ~S(O)/~0 (which 
vanishes at ~) at some point such as ~. The approximator results from minimizing the approximate sum 
of squares function or solving for the point which solves the equation ~S(0)/33 = O. For instance, the 
first order approximator ~ for ~ is given by ~ = 80 + (FT(80) F(O-O)-IFT(80)!" (3) This is similar 
to the asymptotic solution, in that ~ is normally distributed with mean gO and variance matrix (FT(~) 
F(80))-io2 , although in this case the number of rows in F(0) will be finite. The approxlmator ~ corresponds 
to the  Gauss approximation method for finding a solution to the nonlinear least squares problem. Higher 
order approximations can be derived (Box, 1971;  Clarke, 1981), but simplifications are introduced 
in each to make the solution tractable in the multiple parameter case. Note that measures of nonlinearity 
(Beale, 1960; Bates and Watts, 1980) such as Beale's N O have been developed to assess the extent to 
whiGb ~ can be used to approximate A --  the statistical properties of 8. The measures can readily 
be adopted to any parameterization of the model, e.g., ~ = ~(~), and Beale denotes the least possible 
nonlinearity under any parameter- ization as the intrinsic nonlinearity, N$. Monte Carlo methods can 
in principle be used to overcome the limitations of aproximation methods. A direct Monte Carlo algorithm 
consists of N repeated independent samples of the error vector ~(~v' v=l,l,...,N), from which (using 
(i)) the observed vectors y_y v are obtained and used to obtain estimates ~ by repeated solution of 
--V equation (2) for each Zv" The N random sample /% points 8 can be combined to obtain the sample 
 --V A statistics to estimate the properties of 8. For instance, the kthmarginal moments of A A ~' 
~k = E(~ )k (where the exponentiation is component by component) can be estimated using ^ the statistics 
~k , /% = N_I N ~_~ ~k Z ( )k (4) v=l and the variance matrix of the parameter A estimators ~, Var 
(~)_ = Z 8 can he estimated using A the sample statistic ~8' N /%  (~ ^ ^ ^T ES ~ (N-1)-I r. -el)(~-el) 
 (3) v=l The Monte Carlo method is conceptually simple and can be easily extended to nonlinear least 
estimators other than nonlinear least squares or to errors other than normal, but suffers from the drawback 
that N nonlinear estimation prohlems must be solved and that the standard errors of ^ N-I/2 statistics 
such as el decrease only as Therefore, although arbitrary accuracy can be obtained via Monte Carlo sampling, 
arbitrarily many estimation problems may have to he solved to obtain that accuracy. 2. IMPROVED MONTE 
CARLO USING CONTROL VARIATES The efficiency of Monte Carlo sampling can be improved by variance reduction 
techniques (Hammersley and Handscomb, 1964; Law and Kelton, 1982). These methods can take several forms, 
among which the best known are: antithetic variates, stratification, conditional expeetatlons, and control 
variates. Control variates, using the linear approximator (equation (3)) as the control variate is a 
very natural approach. This method is very simple to  Control Variates in Nonlinear Regression 625 implement, 
and it leaves the sampling method unchanged so that the sample estimates ~ are ~v all independent. 
The distribution of ~ is known and ~ is easier to compute than ~ so that very little additional work 
is required to implement the method. In particular, the control ~ can be used as the basis for controls 
for all the marginal moments and covarlances, as well as the quantiles. It is more efficient than direct 
Monte Carlo at all the levels of nonlinearity commonly encountered in practice. The details of the control 
varlate method are given in Swain (1982) and Swain and Schmesier (1983). The control estimators for the 
marginal moments ~_~k, are given by ^ A ^ -E~ k) (6) ~k(Bk ) =-~k -Bk(~k where ^ = N- I N v=l is 
the sample kth moment for ~ and the p by p matrix B k is the control weight matrix. The A method depends 
upon the covarlation between 0 --v and ~v (they both depend upon the same vector of errors, ~v) , so 
that when ~--k is greater than its ^ expectation, it can be assumed that ~ exceeds its expectation as 
well, and a correction proportional to -(~--k -E~--k)r" should be made to ^~-k" #% It can be shown 
that the variance of ~k(~) is minimized for the choice   B k Cov( (~)k (~)k) Vat-l((~)k) and that 
the decrease in the control varlate variance is largest when the correlation between A and 8 is greatest. 
The covariance term in equation (7) for B k is generally unknown, so B k must be specified in some other 
way. Experience with the method has shown that the choice B k = I (I is the p by p identity matrix) is 
nearly optimal and leads to a simple, unbiased estimator for ~k. This is also consistent with the asymptotic 
case, since Bk(Eqn(7)) tends to the identity matrix as the sample size increases indefinitely. A similar 
control variate scheme can also be given for Z 8. Let ~ be a vector of length m=p(p+l)/2 containing the 
lower diagonal elements A A A of % 8 stored by row (e.g., Oli , o12 , ... , ~pp). Let ~ be a similar 
m-vector for ~. Then ~(c) = ~ -c( ~ -z(~) ) (s) is a control variate estimator for Z 8 and again by 
experience the choice of the m by m identity matrix for C is nearly optimal. 3. RESULTS The efficiency 
of two Monte Carlo procedures can be computed as the ratio of their preclsions, with precision measured 
as the inverse of the variance. For the multiple parameter case the scalar measure adopted is the determinant 
of the variance matrices, which is also known as the generalized variance. Then the efficiency of a 
control variate estimator for the kth moment compared to the crude estimator of the kth moment is given 
by A A E k = ( I/ I Var ~_k(1) l)/(I/ I Var ~k(O) I ) = I Var ~_k(0) I/ I Var ~k(1)l since direct 
Monte Carlo corresponds to the use of a control variates with a 0 weighting matrix. Note that efficiencles 
in excess of 1 indicate that the control variance is less than that of the direct Monte Carlo estimator. 
 The sample efficiencies (k=l) are plotted in figure 1 versus the nonllnearlty measure NS, showing that 
the control variate estimator is more efficient than direct Monte Carlo for a wide range of nonlinearities, 
and the efficiency becomes infinite as the nonlinear estimator approaches the behavior of a linear estimator 
(i.e., asymptotically). The range of N 8 depicted is typical of the values encountered in practice. 
 The apparent relation between E 1 and N 8 is not surprising, since the Beale measure N 8 explicitly 
measures the appropriateness of ~ as 626 James J. Swain InE I -L Sample 16 o P=3 --:30 P=2 " --20 
IO  -16 -12 -8 -4 ~I +2 inNe Figure I. Summary of sampling efficlencies for the control estimator ^ 
 ~l(I} for 25 samples taken from Swain (1982). Some points are multiple samples. Sample 16 has one parameter 
which behaves like a linear estimator and therefore has higher than expected efficlencv. an approxlmator 
for ~ and ~ is the control control variable strategy is efficient for all varlate used here. Moreover, 
an approximate but the most extreme nonlinearities (N 8 in relation between the two (for kffil) is given 
 excess of i/Fp,n_p; ~ can be considered extreme, (Swain, 1982) by according to Beale) and because the 
method requires only relatively simple linear E 1 = (2Ne)-P (9) computations, it will always be advantageous 
to use the method. In addition, equation (9) and the degree of this fit is given in figure 2. suggests 
that the efficiency of the method can be The relation between efficiency and Ne, further improved by 
a suitable choice of a equation (9), allows a prediction in advance of parameter transformation. An upper 
bound on the sampling of how efficient the control variate possible efficiency using control variables 
can scheme is likely to be. However, since the be given by substituting the intrinsic Control Variates 
in Nonlinear Regression 627 -! P InE I - 15 Sample 16 0 Efficiency~ Estimator p-IIn (2Ne) i0 0 5 I I 
I I -16 -12 -8 -4 +2 InN e Figure 2. Approximate efficiency formula for the control estimator ~l(I), 
p-lln E 1 = -in 2N 6. The 25 samples are taken from Swain (1982). nonlinearity N$ in place of N o in 
equation (9). For instance, in certain cases a linear approximation based upon the transformed parameters 
~ = (~)k leads to additional efficiency in the control variate estimator for the marginal moments (Swain, 
1982). REFERENCES Bard, Y. (1974), Nonlinear Parameter Estimation, Academic, New York. Bates, D.M., 
Watts, D.G. (1980), Relative Curvature Measures of Nonlinearlty, J. Royal Statistlacal Society, Series 
B, Vol. 42, No. I, pp. 1-25. Beale, E.M.L. (1960), Confidence Regions in Non- linear Estimation, J. 
Royal Statistical Society, Series B, Vol 22, No. i, pp. 41-88. Box, M.J. (1971), Bias in Nonlinear Estimation, 
 J. Royal Statistical Society, Series B, Vol. 33, No. I, pp. 171-201. 628 James J. Swain Clarke, G.P.Y. 
(1980), Moments of the Least Squares Estimators in a Non-linear Model, J. Royal Statistical Society, 
Series B, Vol. 42, No. 2, pp. 227-237. Gallant, A.R. (1975), Nonlinear Regression, The American Statistician, 
Vol. 29, No. 2, pp. 73-81. Nammersley, J.M., Handscomb, D.C. (1964), Monte Carlo Methods, Chapman and 
Hall, LondonÂ° Law, A.M., Keltyon, W.D. (1982), Simulation Modeling and Analysis, McGraw-Hill, New York. 
 Swain, J.J. (1982), Monte Carlo Estimation of the Sampllng Distribution of Nonlinear Parameter Estimators, 
Unpublished Ph.D. Dissertation, Purdue University, West Lafayette, Indiana. Swain, J.J., Schmelser, 
B.W. (1983), Monte Carlo Estimation of the Sampling Distribution of Nonlinear Model Parameter Estimators, 
Technical Report C-83-I, School of Industrial and Systems Engineering, Georgia Institute of Technology, 
Atlanta, Georgia 17pp. (Also Purdue University Industrial Engineering Technical Memorandum 83-2). LEONIDAS 
C. KIOUSSIS is a Research Assistant in the Department of Operations Research at George Washington University. 
He has a Diploma in Mathematics from University of Patras, Greece, and an M.S. in Operations Research 
from George Washington University. He is currently working on his D.Sc. in Operations Research. He is 
a member of ORSA, TIMS and ASA. Leonidas C. Kioussis Department of Operations Research School of Engineering 
and Applied Science George Washington University Washington, D.C. 20052 (202) 676-6084 DOUGLAS R. MILLER 
is an Associate Professor of Operations Research at George Washington University. He received his Ph.D. 
in Operations Research from Cornell, was Assistant and Asso- ciate Professor of Statistics at the University 
of Missouri-Columbia, and worked at NASA Langley Research Center before coming to GWU. He is a member 
of ORSA and ASA. His research interests cover many topics within the realm of applied probability and 
statistics. Professor Douglas R. Miller Department of Operations Research School of Engineering and Applied 
Science George Washington University Washington, D.C. 20052 (202) 676-7528 
			