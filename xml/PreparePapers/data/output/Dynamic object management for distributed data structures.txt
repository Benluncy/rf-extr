
 Dynamic Object Management for Distributed Data Structures Brian K. Totty* Daniel A. Reedt Department 
of Computer Science University of Illinois Urbana, Illinois 61801 Abstract In distributed-memory multiprocessors, 
remote memory accesses incur larger delays than local ac­cesses. Hence, insightful allocation and access 
of dis­tributed data can yield substantial performance gains. This paper argues for the use of dynamic 
data man­agement policies encapsulated within individual dis­tributed data structures. Distributed data 
structures offer performance, f7exibility7 abstraction, and system independence. This approach is supported 
by data from a trace­ driven simulation study of parallel scientific bench­ marks. Experimental data 
on memory locality, mes­sage count, message volume, and communication delay suggest that data structure-specific 
data management is superior to a single, system-imposed policy. Introduction Concurrently executing processors 
access large quantities of data, often sharing the same data with other processors. To increase memory 
bandwidth, and to facilitate the construction of large-scale parallel sys­tems, the system memory is 
typically partitioned into physically disjoint memory modules. Because data communication incurs delays 
in distributed memory systems, it is necessary to carefully control where data resides, and how it is 
accessed by the cooperating pro­cessors, in order to maximize performance. *Supported in part by a DARPA/NASA 
Research Assis­tantship in Parallel Processing administered by the Institute for Advanced Computer Studies, 
University of Maryland, under grant NASA UMCP 26947N, and the National Science Founda­tion under grants 
NSF CCR87-06653 and NSF CDA87.22836. t supported in part by the National Scicncc Foundation un­der grants 
NSF CCR87-06653 and NSF CDA87-22836, by the National Aeronautic. and Space Administration under NASA 
Contract Number NAG-1-613, and by an equipment grant from the Digital Equipment Corporation External 
Research Program. Because access patterns may change in the course of computation, the optimum data management 
pol­icy may change dynamically. Given the communica­tion latency on current, distributed memory parallel 
systems, inappropriate data management may result in catastrophically large data access times, and conse­quent 
poor program performance. However, the pro­grammer must not be excessively burdened with data management 
details. The goal is a compromise be­tween enlightened distributed data management and programming ease. 
Because each application program and its con­stituent algorithms have different memory access pat­terns, 
we believe that a single, system imposed data management policy is unlikely to be effective. In­stead, 
we argue for data structure-specific data man­agement, encapsulated within distributed data struc­tures. 
Like traditional abstract data structures, a distributed data structure contains a set of data values 
and operations to access and manipulate these values. In addition, distributed data structures contain 
fa­cilities to distribute the data values across memory modules, and to access the data values without 
appli­cation knowledge of their physical location. By plac­ing the data management facilities within 
the data structures, static and dynamic information about data structure usage can be exploited to optimize 
the data management. The distributed data structure also hides the tedious data management details, simplify­ing 
the programming process. In short, the distributed data structures approach potentially offers concurrent 
benefits of performance, jleribility, abstraction, and system independence. This paper focuses on a performance 
analysis ofsev­eral dynamic policies for managing distributed data structures. We use symbolic memory 
reference traces and trace-driven simulation to analyze the perfor­mance of severaI variations of each 
mechanism when applied to specific data structures from parallel scien­tific applications. The simulation 
results support the 1063-9535/92 $3.00 (!3 1992 IEEE claim that, data structure-specific data management 
yields significantly improved performance over a sin­ gle, system-imposed policy. 1.1 Related work Many 
hardware and software approaches have been proposed for the organization, partition, placement, and access 
of distributed shared data. Organization has been shown to reduce memory module conflicts by insightfully 
mapping multi-dimensional arrays to a linear memory space [12]. Similarly, careful orga­nization of objects 
within virtual memory pages also reduces delays in hierarchical memory systems [16]. Contemporary software 
systems for distributed mem­ory machines grant the user some control over dis­tributed data organization 
and partition through the use of language annotations [15, 10]. Hardware approaches implement a single, 
system­imposed data access policy across all data structures. Memory controller state machines generate 
the under­lying message traffic that implements the access pro­tocols. The Tera system [1] implements 
uncached re­mote access to distributed memories. The Dash [13] system implements coherent caching protocols 
in the memory controller, and uses distributed directories to maintain the location of copies (5]. Compilation 
schemes such as Fortran D [10] gener­ate explicit data movement to provide the illusion of distributed 
data access at the language level. The IVY system [14] implements access policies in software, ex­ploiting 
virtual memory page table hardware to allow fast local accesses. The Munin system [3] extends vir­tual 
shared memory by providing type-specific access policies to distributed data, in a style similar to our 
distributed data structures. Our approach envisions policy implementation at the user level, rather than 
at the operating system or hardware level. This access permits a programmer to construct arbitrary data 
management policies, and encourages portability across a variety of distributed memory multiprocessors. 
The precise cost of the ap­proach depends on the implementation of local access and the expense of the 
communication interface. 1.2 Overview In 32, we describe the experimental infrastructure used to evaluate 
the performance of data management policies when applied to particular distributed data structures. Memory 
locality statistics from the simu­lations, presented in ~3, indicate that the best static placement of 
data is inferior to even naive dynamic Table 1: Scientific Algorithm Benchmark Set [ Algorithm I Description 
i Assorted--- Matrix Codes bmult Blocked Matrix Multiply fzauss Gaussian Elimination L LU Decomposition 
Linpack sgefa Gaussian Elimination sardc. QR Factorization ssvdc Singular Value Decomposition Eispack 
tredl Symmetric Tridiagonalization elmhes Reduce Matrix To Upper Hessenberu rgeig Find Eigenvectors Of 
General Matri; Sparse Matrix Kernels smvmult Sparse Matrix/Vector Multiply sgentojag Sparse Matrix To 
Jagged Diagonal sfelim Forward Elimination Of Sparse System 1 [miscellaneous Algorithms fft 2-D Fast 
Fourier Transform nws Wavefront Sequence Match schemes, and that the optimum policy varies substan­tially 
with the access pattern. The volume of com­munication traffic is discussed in ~4, along with its relation 
to message latency and message bandwidth. Finally, ~5 summarizes data structure-specific data management 
and discusses our implementation plans for distributed data structures on distributed memory parallel 
architect ures. 2 Methodology To evaluate the variation of policy performance across algorithms, we defined 
a set of data manage­ment mechanisms, simulated several policies for each mechanism, and analyzed the 
efficacy of the policies on several parallel scientific benchmarks, using trace­driven simulations. A 
detailed description of the ex­periments and results can be-found in [17]. 2.1 Scientific parallel benchmark 
set We selected the fourteen scientific algorithms in Ta­ble 1 as a parallel benchmark set. Many of the 
bench­marks are members of the popular LINPACK and EIS-PACK linear algebra libraries. Several sparse 
matrix kernels with strongly data-dependent access patterns that defy static analysis were also included. 
The algo­rithms were expressed in Cedar Fortran using parallel DO loops [9]. We chose the input data 
sizes to gener­ate an average of 500,000 accesses to the major concur­rent data structure of each algorithm, 
corresponding to many millions of memory accesses for the entire program.1 Annotations were automatically 
inserted into the source programs using the ForTrace Fortran preprocessor. z ForTrace converts parallel 
Cedar For­tran source code into instrumented, sequential For­tran. During execution, a trace file is 
generated con­taining symbolic data structure accesses, concurrency structures and synchronization information; 
this al­lows parallel access streams to interleave arbitrarily, up to synchronization points. We emphasize 
that the generated traces are sym­bolic, naming the source-level data structure elements being accessed. 
ForTrace resolves aliases at execu­tion time to generate the symbolic traces. No in­struction stream 
references are recorded, nor are refer­ences to any temporary, compiler generated variables. This provides 
architecture/compiler independence and highlights the underlying algorithmic access patterns, an essential 
attribute for the study of data structure specific data management.  2.2 Data management formulation 
A distributed data structure must provide instances of four mechanisms: organization, partition, inittal 
placement, and access. The organization mechanism is responsible for the layout of the data structure 
in some easily partitionable space. Contemporary compilers use organization functions to map multidimensional 
arrays and composite data structures to linear mem­ ory spaces. The partition mechanism is responsible 
for dividing a reorganized data structure into memory segments called constituent objects (analogous 
to cache blocks). The placement mechanism initially allocates constituent objects to distributed memory 
modules. Finally, the access mechanism implements reads and writes to constituent objects, managing data 
transmis­ sion as well as migration, replication, and consistency strategies.  2.3 Data management policies 
The organization and placement of data dramat­ically affect system performance. Aggregate access LThe 
number of data structure accesses was primarily con­ strained by the simulation time required to cover 
our experi­ mental design space. 2 ForTrace was derived from the MaxPar parser [6] developed at the Center 
for Supercomputing Research and Development (CSRD). patterns are often only approximately sequential, 
and individual data structure access patterns can be strik­ingly irregular. Although a single data management 
model can yield excellent performance for instruction streams and adequate performance for uniprocessor 
data streams, on parallel systems it can perform quite poorly the interactions of concurrent access 
pat­terns are often highly chaotic and data structure de­pendent. This is precisely the motivation for 
exploring a richer set of software managed data structure man­ agement policies. We implemented policies 
for each of the four mech­anisms. Six organization policies (Row, Column, RowSkew, ColSkew, Diag, AntiDiag) 
organize multi­dimensional arrays linearly in row, column, skewed, and diagomd fashions. The StaticSeq 
partition policy fragments a linear data space sequentially into constituent objects of a parametrized 
size. The choice of object sizes affects the relative benefit from software prefetch and the penalty 
due to false sharing.3 The Seq placement policy initially assigns objects to memories in round-robin 
fashion; to provide a point of reference, the Opt placement policy uses a posterior information to optimally 
place the objects. Finally, the Remote, Migrate, Invalidate, and lJp­ date access policies read or write 
data from constituent objects using remote access, object migration, and invalidation and update-based 
coherent replication, respectively.4 Access policies send messages between nodes of a distributed memory 
multiprocessor to implement the data management protocol. The four protocols we evaluated are depicted 
graphically in Figure 1. Re­ mote (Figure la) is the simplest policy, initiating a request and reply 
message between an accessing node and the node where the object data resides. The re­ maining three policies 
allow constituent object data to relocate dynamically. Each relocatable object has a directory entry 
[4] as­ signed to a particular node, that indicates where the object and any copies currently reside. 
In the worst case, an accessing node would first have to commu­ nicate with the directory node to find 
the location of 3 False sharing occurs when logically unshared data items reside in the same physical 
data partition. Disjoint accesses to the partition cause the partition to appear shared, even though 
the individual data elements are not logically shared. False sharing is eliminated when the partition 
size is equal to the logical data size, but increases with the partition size. 4 Invalidation coherence 
allows multiple read-only copies of an object, but any writer will first destroy (invalidate) all copies 
to insure copy consistency. A writer in an update coherence protocol will first update all copies to 
the new value. (a) Remote (b) Migration (c) Invalidation (d) Update Figure 1: Data Management Protocols 
the desired object. In this case, four messages are generated to implement the Migrate policy. In the 
caching protocols, messages also need to be sent to each copy when a write occurs to invalidate or update 
the replicated copies. Both Invalidate and Update have a worst-case message count of 5 + 2N, where N 
copies are purged or updated. 2.4 Simulation infrastructure We developed a simulation system called PoliSim. 
When driven by symbolic data access traces, PoliSim measures the effects of the organization, placement 
, and access policies applied data structures, on a distributed memory tem. Typical experiments simulate 
32 partition, to individual parallel sys­processors. s A policy set was assigned to each data structure 
of the program, and performance data was captured for each data structure to judge the effectiveness 
of the policy set on that data structure. Loop iterations were dynamically assigned to the processors 
in sequential order, as processors became s The large experimental design space prohibited simulation 
of larger numbers of processors. idle. A completely connected network was simulated to ignore interconnection 
Each link of the fully etrized by link latency simulations assumed link has zero latency topology contention 
delays. connected network was param­ and bandwidth. Most of our an ideal network, where each and infinite 
throughput. Syn­ chronization traffic delays were similarly ignored, to focus on the details of the 
data management proto­cols . 3 Memory locality We define memory locality as the fraction of ac­cesses 
that can be satisfied entirely by the local mem­ory, requiring no communication. Dynamic data management 
attempts to increase memory locality by moving, reconfiguring, or replicat­ing data at execution time. 
The dynamic schemes do not come wit bout cost. Dynamic migration and repli­ cation schemes require additional 
messages for proto­col support, including directory maintenance and copy coherence. This extra cost can 
only be justified if sim­pler, static data management schemes are insufficient. \ B Remote _ Remet e 
( ODtimal Placement ) ~ Migrate  = Invalidate = Update ,, :., , ,: ,,, ----1 -c I 11111 r m Benchmark 
Algorithm Figure 2: Memory Locality from Five, Single-Element Data Management Policies To judge the relative 
benefits of simple dynamic mi-different applications, exploiting different parallelism gration and caching 
schemes over static placement, we models, can be reasonably expected to show an even tested twelve of 
our benchmarks with static and dy-wider variance of policy preferences, further arguing namic access 
policies.O The elements of the major data for data structure-specific data management. structure from 
each benchmark were partitioned into For example, Figure 3 shows the memory locality single-element constituent 
objects to eliminate the ef-from two of the benchmarks (Gaussian elimination fects of false sharing. 
The resulting memory locality and singular value decomposition), with the best aver­from the static and 
dynamic policies are shown in Fig-age data organization selected (of the six organization ure 2. policies). 
The horizontal axis of each graph indicates Figure 2 indicates that an optimal static placement the constituent 
object size in bytes, and the vertical of data (the solid black bar) is typically inferior to axis represents 
the resulting memory locality. Each even naive dynamic policies. This optimal placement graph has three 
curves, corresponding to our three was determined from post-execution data and is not dynamic access 
policies. a priori realizable in general. Furthermore, the per-Figure 3a shows memory locality data from 
the formance of the dynamic policies typically increases major array of the Gaussian elimination benchmark, 
substantially (to a point) as object size increases, due when the array was partitioned by columns. The 
local­to prefetch benefits. ity achieved at small partition sizes is low, increases with partition size 
due to prefetch benefits, and de­  3.1 Variation in memory locality clines dramatically at large object 
sizes due to false sharing under the Migrate and Invalidate policies. Mi- Although [17] presents a detailed 
analysis of mem­grate outperforms Invalidate due to a read-modify­ ory locality for each algorithm and 
data management write access pattern, which requires an extra copy in­ policy, space limits preclude 
its inclusion here. Briefly, validation under the Invalidate protocol. The singular we observed that 
optimal policy choice varies substan­value decomposition benchmark (Figure 3b) indicates tially among 
the benchmarks, though all are regular, a preference for read locality, which is provided by the parallelizable, 
scientific algorithms. Algorithms from Update protocol, and to a lesser degree, by the Inval­oWe use 
static and dynamic to refer to policies that recon-idate protocol. Migrate is unable to support active 
figure data at execution time. In some cases, it is possible at sharing of data. compile time to determine 
data reconfiguration points. Though In these two locality vignettes, we are presented the reconfiguration 
heuristic is static, we still classify thb ap­ with cases where different organizations, partition proach 
as dynamic, since reconfiguration occurs at execution time. sizes, and access policies are appropriate 
for different 1 : ;/-4 ! \ / \ \ \ \ \ : +: d o Migrate o Invalidate O Update  1, 1 0 1I!I 24681O 
log~ (partition size) (a) gauss/Column Figure 3: Memory Locality From Selected algorithms. The policy 
memory locality varies sub­stantially across the entire set of benchmarks. Policy choice may also depend 
on data-dependent access patterns, making execution-time data manage­ment essential. The very simple 
sparse matrix-vector multiply benchmark smvmult demonstrates the effect of data on policy performance. 
In Figure 4, two differ­ent input matrices yield different locality results be­cause of the access pattern. 
Most of the algorithms in the benchmark set contained some degree of data­dependent access patterns. 
3.2 Memory locality observations The experimental memory locality data demon­strate a complex interplay 
among algorithm, paral­lelization, and data management. We cannot describe all of the facets of data 
management interaction here, but a few observations warrant special attention: Dynamic data management 
is important. An op­timal static management of data may be inferior to even simple dynamic schemes. 
 It is important to carefully choose the data or­ganization to match the access pattern. Proper choice 
of organization can have significant effect on the overall memory locality.  The choice of partition 
size must balance the ben­efits of prefetch against the penalties of false shar­  _ 7Though the programs 
were all regular, parallelizable, scien­tific Fortran programs, limited data-dependent access patterns 
were fairly frequent, caused by pivoting and other search tech­niques to reduce numerical error. !C 246810 
log~ (partition size) (b) ssvdc-A/Row Benchmarks (Best Organization) ing, as well as considering the 
system overheads in naming large numbers of objects. Data access policies attempt to exploit two types 
of locality. The Migrate and Invalidate policies attempt to exploit write locality. The policies en­hance 
the frequency of writes to local objects by eliminating remote copies of writ able ob.iects. In contrast, 
;he Update ;nd Invalidate poficies at­tempt to optimize read locality. Our memory locality experiments 
showed that the optimal data management choices vary significantly across algorithms. A single, system-imposed 
policy is not likely to yield satisfying performance. Further­more, the performance of different data 
management strategies varies widely for a single algorithm. To mea­sure implementation cost, we need 
to examine the pro­tocol communication characteristics. 4 Protocols and message traffic Whereas memory 
locality measures the important degree of node autonomy, it provides no estimate of the ~ of remote accesses. 
Two major factors con­tributing to the cost of non-local accesses are message latency and message bandwidth. 
Message latency is a delay incurred for every message, regardless of the message size. Larger messages 
incur transmission de­lays that are a function of the message bandwidth. It is useful to separate communication 
traffic into two components: message count and message volume. The message count is the total number 
of messages Not ation Description 1 , , I , 1 100­ 90­ 80­ * Remvte 70­ Invalidate 60­ 0 Update 50­ 40­ 
30­ 20­ 10­ 0 0 ! , 2468 10 12 2 4 6 8 10 12 log~ (partition size) log~ (partition size) Power Bus Matrix 
Steam Injection Matrix (l138_bus) (sherman2) Figure 4: Memory Locality for Sparse Matrix-Vector Multiply 
 M Message Count v Message Volume 1 Memory Locality c Messages Per Miss Average Message Size : Total 
Number Of Accesses T Table 2: Communication Traffic Notation sent by a protocol, independent of message 
size. Mes­sages are sent to move data, update or purge repli­cated copies, and to otherwise implement 
the data ac­cess protocols. Message count is especially important when the communication latency is large. 
The mes­sage volume is the total number of data words sent (the sum of the sizes of all messages sent). 
It is espe­cially important when the bandwidth is low. 4.1 Message count Using the notation in Table 
2, if the number of mes­sages sent per non-local access is a constant c (repre­senting the cost per miss 
in messages), the memory locality is 1, and the number of accesses is A, the mes­sage count M can be 
directly computed as: M= CX(l Z)XA where ( 1 1) is the miss ratios 8The cost of messages per miss, c, 
depends on the ac­ 698 Figure 5 shows the message count from two of the benchmarks. Again, the behavior 
of the protocol de­pends on the algorithmic access policy. In Figure 5a, the high memory locality from 
the Migrate and Inval­idate policies at 256 byte partitions (see Figure 3a) generates very few messages. 
In Figure 5b, though Remote and Migrate have similar locality at large ob­ject sizes, the increased protocol 
message count (c) of Migrate generates increased message traffic. When data organization is chosen carefully, 
the Mi­grate and Invalidate policies send relatively few mes­sages per miss. Typically, the Invalidate 
protocol purges very few copies per miss, keeping the message count low. The Update protocol sends many 
more mes­sages per miss due to the large number of maintained copies. This is an artifact of infinite 
cachingg and false sharing. Some of these updates can be sent concur­rently, if the network is sufficiently 
connected, In the limit, where all purges and updates can occur concur­rently, the effective number of 
messages per miss is two for Remote, three for Migrate, and four for Invalidate and Update. These critical 
messages are highlighted as the black arcs in Figure 1. Extra protocol complexity must be justified by 
commensurate gains in memory locality. An access cess policy protocol, application access pattern, and 
system dynamics. g Realizable implementations would limit the number of copies of an object due to finite 
tag and data store. Re­placement would tend to passively purge old copies of objects. Our simulations 
assumed infinite caches, penalizing the Update p erforrrmnce. I 5 -k Remote 0 Migrate . Invalidate -a­ 
u o 246810 log~ (partition size) (a) gauss/Column Figure 5: Message Count policy p, generating five 
messages per miss, and yield­ing 60 percent locality, would only break even with a zero-locality Remote 
protocol: J!fl.em = Mp 2x(1 O)XA = CX(l .6)XA =+ C=5 With a critical path of three messages, 33 percent 
lo­cality is required for the Migrate policy to be com­petitive with the completely non-local Remote 
policy. Similarly, at least 50 percent locality is required for the Update and Invalidate policies. In 
general, for a policy ~ to generate fewer messages (M) than a policy y (i.e. Ma < My): (:)&#38;j <1 
4.2 Message volume It is also important to reduce the size of messages, and thus the total message volume, 
to diminish mes­sage bandwidth delays. Different protocols have dif­ferent message sizes. The Remote 
and Update pro­tocols tend to have small messages. Migrate and In­validate tend to move entire constituent 
objects move frequently. This causes the average message size for these policies to increase as the partition 
size grows. This behavior is depicted in Figure 6. The average volume VP for a protocol p, depends on 
the average message count, and the average message size (UP): VP= MPXUP ,,,1I 1 35 1-k Remote ,00. 0 
Migrate 1 r o . Invalidate o 30 ,-.4 0 Update x 25 o i 20 0 1 o~ 2 4 6 81012 log~ (partition size) 
(b) smvmult (1138-bus) from Selected Benchmarks  100~ * Remote 90-o o Migrate . IrIvalidat e 80- O 
Update 70­ 60­ / / 50­0/ 40-/ 30- J /20­ 0/ a 10-,, &#38;--&#38; ~ ~ o~ 246810 iog~ (partition size) 
 Figure 6: Average Message Size, u (Mean over All Benchmarks and Organizations) For a policy z to produce 
a smaller message volume than a policy y, then the following must hold:  0+ < 1 V u= Ma =+ <1 UY MY 
  ==% (:)(:)&#38;j < 1 For an access policy p that yields 60 percent locality and 5 messages per miss 
to generate the same message volume as a zero-locality Remote protocol, the average message size must 
be only six bytes: Vrem = Vp (2x(1 O)x A)x6 = (5x(l .6)x A)xaP =+~P =6 Migrate and Invalidate face double 
jeopardy when thrashing. Not only does the locality decrease, but the penalty for each miss increases. 
For this reason, partition size is exceptionally important. Based on our experiments, it appears that 
mem­ory locality near sixty or seventy percent is the min­imum required to justify dynamic memory manage­ment. 
Memory locality less than this amount gen­erates sufficient message traffic to offset the gains in locality. 
4.3 Communication delays Different distributed memory architectures have different communication latencies 
and bandwidths. A taxonomy of several distributed memory systems, or­ganized by processor-normalized10 
communication la­tency and bandwidth was presented in [17]. The taxonomy indicated that processor-normalized 
latency varies by six orders of magnitude, and bandwidth by four. The relationship between protocol traffic 
and com­munication delays indicates that machine architecture also affects protocol choice. This was 
verified by our simulation results. By targeting data management policies of the distributed data structures 
to the host machine s communication behavior, algorithm perfor­mance can be substantially improved. 5 
Summary This paper promoted a schema for distributed data structures, an object-based encapsulation 
of data management for distributed memory architectures. Distributed data structures provide the joint 
benefits of performance, flexibility, abstraction, and system in­ dependence. We analyzed several data 
management policies, im­plementing mechanisms for organization, partition, placement, and access. The 
experiments are similar in flavor to multi-cache coherence studies [2, 4], but are implement ed in software, 
targeted toward large-scale 10The latGnCY ~nd bandwidth values were expressed in terms of native operations, 
t 0 normalize for differences in computation speed. The processor-normalked statistics represent the 
num­ber of operations delay that are incurred during communication. distributed-memory machines, and 
assign individual policies to individual data structures. Our experiments supported the assumption that 
data structure-specific data management can yield sig­nificantly improved performance over a single, 
system­imposed policy. While our experiments show that simple, dynamic policies generate substantially 
higher memory locality than static placement schemes, this increased locality comes at the cost of increased 
pro­tocol message count and volume. The proper choice of data management policy de­pends on architectural 
issues as well as algorithmic access patterns. Communication latency and band­width dictate the expense 
of protocol message count and message size. Early distributed memory systems focused on reducing message 
count because of exorbi­tantly large message latencies. Contemporary systems have much lower latency, 
allowing the use of data nlan­agement policies with larger protocol message counts, and smaller partition 
sizes to support large amounts of parallelism. We are now designing a distributed data manage­ment subsystem 
for several distributed memory multi­processors with different computation and comnmni­cation performance, 
including the Intel Paragon XP/S system [11] and the MIT J-Machine [7]. The data management subsystem 
will facilitate partition, ac­cess, relocation, and replication of distributed data structures. We aim 
to develop efficient software mech­anisms to access the data management facilities. We plan to extend 
the access protocols to incorporate relaxed consistency models, in order to better toler­ate communication 
latencies [8]. The result will be a flexible software system allowing precise data man­agement, while 
maintaining programmability y, and will provide a test-bed for the evaluation of distributed data structure 
management of parallel, real-world pro­grams. References [1] ALVERSON, R., CALLAHAN, D., CUMMINGS, D,, 
KOBLENZ, B., PORTERFIELD, A., AND SMITH, B. The Tera computer system. In Proceedings of the International 
Conference on Sripercomputing (Sep 1990), pp. 1-6. [2] ARCHIBALD, J., AND BAER, J.-L. Cache coher­ence 
protocols: Evaluation using a multiprocessor simulation model. ACM Transactions on Com­puter Systems 
4, 4 (November 1986), 273-298. [3] F3ENNFiTT, J. K., CARTER, J. B., AND ZWAENEPOEL, W, Munin: Distributed 
shared memory based on type-specific memory coher­ence. In Proceedings of the 2md Symposium on Principles 
6 Practice of Parallel Programming (March 1990), pp. 168-175. [4] CENSIER, L. M., AND FEAUTRIER, P. A 
new solution to coherence problems in multicache sys­tems. IEEE Computer C-27, 12 (December 1978), 1112-1118. 
[5] CHAIKEN, D., FIELDS, C., KURIHARA, K., AND AGARWAL, A. Directory-based cache coherence in large-scale 
multiprocessors. IEEE Computer (June 1990). [6] CHEN, D.-K. MaxPar: An execution driven sim­ulator for 
studying parallel systems. CSRD Re­port 917, University of Illinois Center for Super­cornputing Research 
and Development, Ott ober 1989. [7] DALLY, W. J., ET AL. Architecture of a message­driven processor. 
In Proceedings of the 14th Inter­national Symposium on Computer Architecture (June 1987), pp. 189-205. 
[8] GHARACHORLOO, K., GUPTA, A., AND HEN-NESSY, J. Performance evaluation of mem­ory consistency models 
for shared-memory mul­tiprocessors. In Proceedings of the 4th Interna­ tional Conference on Architectural 
Support for Programming Languages and Operating Systems (April 1991), pp. 245-257. [9] GUZZI, M. D. Cedar 
Fortran programmer s handbook. CSRD Report 601, University of Illin­ois Center for Supercomputing Research 
and Development, June 1987. [10] HIRANANDANI, S., KENNEDY, K., AND TSENG, C.-W. Compiler optimization 
for Fortran D on MIMD distributed-memory machines. In Proceed­ings of Supercornputing 1991 (Nov 1991), 
pp. 86 100. [11] INTEL CORPORATION. Paragon XP/S Product Overview, 1991. [12] LAWRIE, D. H. Access and 
alignment of data in an array processor. IEEE Transactions on Com­puters C-24, 12 (Dee 1975), 1145-1155. 
[13] LENOSKI, D., ET AL. The Stanford Dash multi­processor. IEEE Computer (Mar 1992), 63-79. [14] LI, 
K. Shared Virtual Memory on Loosely Cou­p~ed Multiprocessors. PhD thesis, Yale University, Department 
of Computer Science, October 1986. [15] ROSING, M., SCHNABEL, R. B., AND WEAVER, R. P. Expressing complex 
parallel algorithms in DINO. Tech. Rep. CU-CS-430-88, University of Colorado at Boulder, Department of 
Computer Science, March 1989. [16] STAMOS, J. W. Static grouping of small objects to enhance performance 
of a paged virtual nlem­ory. ACM Transactions on Computer Systems (May 1984), 155-179. [17] TOTTY, B. 
K. Experimental analysis of data management for distributed data structures. Master s thesis, University 
of Illinois at Urbana-Champaign, Department of Computer Science, Jan 1992.  
			