
 Avoiding Speaker Variability in Pronunciation Veri.cation of Children Disordered Speech Oscar Saz, 
Eduardo Lleida, W.-Ricardo Rodríguez Communications Technology Group (GTC), Aragón Institute for Engineering 
Research (I3A) Maria de Luna, 1, University of Zaragoza, Zaragoza, Spain {oskarsaz,lleida,wricardo}@unizar.es 
 ABSTRACT This paper deals with the problematic of speaker variabil­ity in a task of pronunciation veri.cation 
for the speech therapy of children and young adults in Computer-Aided Pronunciation Training (CAPT) tools. 
The baseline sys­tem evaluates two di.erent score normalization techniques: Traditional Test normalization 
(T-norm), and a novel N­best based normalization that outperforms the .rst by nor­malizing to the log-likelihood 
score of the .rst alternative phoneme in an unconstrained N-best list. When performing speaker adaptation, 
the use of all the adaptation data from the speaker improves the performance measured in Equal Error 
Rate (EER) of these systems compared to the speaker independent systems; but this can be outperformed 
by more precise models that only adapt to the correctly pronounced phonetic units as labeled by a set 
of human experts. The best EER obtained in all experiments is 15.63% when using both elements: Score 
normalization and speaker adaptation. The possibility of automatizing a more precise adaptation without 
the human intervention is .nally proposed and dis­cussed. Categories and Subject Descriptors H.5 [Information 
Systems]: Information Interfaces and Presentations; J.2 [Computer Applications]: Physical Sciences and 
Engineering General Terms Algorithms, Experimentation  Keywords Pronunciation Evaluation, Children 
Speech, Speech Disor­ders 1. INTRODUCTION Computer-Aided Pronunciation Training (CAPT) tools aim to 
improve the pronunciation skills of the user, who Permission to make digital or hard copies of all or 
part of this work for personal or classroom use is granted without fee provided that copies are not made 
or distributed for pro.t or commercial advantage and that copies bear this notice and the full citation 
on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires 
prior speci.c permission and/or a fee. ICMI-MLMI 09 Workshop on Child, Computer and Interaction November 
5, 2009, Cambridge, MA, USA Copyright 2009 ACM 978-1-60558-690-8/09/11 ...$10.00. might be either a child 
with speech di.culties, or a for­eigner in the process of learning a second language. The technologies 
underneath these tools are based on the abil­ity of the application to detect phonetic mispronunciations 
and provide feedback to the user indicating these mistakes and proposing corrective methods for achieving 
the correct pronunciation [10, 6]. However, the task of pronunciation evaluation for the as­sessment 
and correction of phonological mispronunciations in children and young adults [11] faces a major problem 
in the presence of di.erent sources of variability; being speaker variability one of the more remarkable 
of them, among oth­ers like channel and session variability. This paper aims to evaluate di.erent normalization 
techniques and speaker adaptation frameworks of acoustic models in a pronuncia­tion veri.cation task, 
with the .nal aim of developing CAPT tools, especially oriented to young disabled children. The target 
language is Spanish, but the proposed systems are language independent and, hence, generalizable to any 
other language. This paper is organized as follows: Section 2 will present the pronunciation veri.cation 
problem, how it resembles the speaker veri.cation problem and which methods can be trans­lated from one 
to another. Section 3 will present the cor­pus of disordered speech used for the experiments. Section 
4 will provide the experimental framework with the results using all the techniques proposed in score 
normalization and speaker adaptation. Finally, the discussion and conclusions to this work will be shown 
on Section 5. 2. THE PRONUNCIATION VERIFICATION PROBLEM The di.culties arising in pronunciation veri.cation 
are similar in many facets to the di.culties in traditional speaker veri.cation tasks. In speaker veri.cation, 
the aim is to de­cide whether a segment of speech has been uttered by a given speaker or if, on the contrary, 
an impostor speaker has ut­tered that segment. In pronunciation veri.cation, the aim is to determine 
if a certain part of a speech signal corresponds to a given phoneme, or if it has been substituted by 
the speaker for another phoneme. There are several sources of variability that mask the speaker features 
like channel and session variability in the speaker veri.cation process. In pronunciation veri.cation, 
the phoneme features are masked by sources of variability like speaker and channel variability. Hence, 
similar tech­niques used in speaker veri.cation for avoiding the undesired e.ects of channel variability 
could be used in pronunciation Table 1: Speakers in the disordered speech corpus Speaker Age Gender Correct 
Substituted Deleted Speaker Age Gender Correct Substituted Deleted Spk01 14 Female 98.88% 0.94% 0.17% 
Spk02 11 Male 78.42% 12.41% 9.16 Spk03 21 Male 94.78% 4.54% 0.68% Spk04 21 Female 96.83% 2.05% 1.11% 
Spk05 18 Male 56.51% 26.11% 17.38% Spk06 17 Male 99.32% 0.51% 0.17% Spk07 18 Male 87.07% 7.36% 5.57% 
Spk08 19 Male 69.18% 17.72% 13.10% Spk09 11 Female 91.78% 5.31% 2.91% Spk10 15 Female 78.51% 13.10% 8.39% 
Spk11 20 Female 93.24% 5.15% 2.05% Spk12 18 Male 74.32% 13.96% 11.73% Spk13 13 Female 43.58% 30.48% 25.94% 
Spk14 11 Female 91.01% 5.14% 3.85%  Figure 1: Scoring process veri.cation for avoiding speaker variability. 
 The undesired channel variability is erased in speaker ver­i.cation by a set of proposals like score 
normalization [1], training of channel adapted models [4] or more novel tech­niques like Joint Factor 
Analysis (JFA) [3]. Score normal­ization and re-training of speaker dependent models are two techniques 
that can be straightforward applied to the pro­nunciation veri.cation task, as it was tested during this 
work. 3. CORPUS The corpus used in this work contains speech from 14 young disabled speakers with di.erent 
speech impairments [9]. The distribution of speakers is balanced in terms of age (in the range of 11 
years to 21 years old) and gender (7 boys and 7 girls) as seen on Table 1. Speakers have uttered 4 sessions 
of the words in the Induced Phonological Regis­ter (RFI), a very well known set of 57 words designed 
for speech evaluation in Spanish [5]; giving a total amount of 3,192 isolated word utterances. The sessions 
were recorded in di.erent days to re.ect intra speaker variability. All these 3,192 utterances were labeled 
by a group of ex­perts to detect the substitution and deletion of phonemes in these speakers. Three independent 
labelers evaluated each utterance, and the .nal label (correct, substituted or deleted) was set by consensus. 
The rates of correct, substi­tuted and deleted phonemes by speaker are shown in Table 1. The .nal rate 
of mispronunciations for all the speak­ers was 18% of mispronunciations (substituted plus deleted phonemes), 
what indicated the severe disorders su.ered by some of the speakers. Finally, for the modeling of normal 
healthy speech in the age of the impaired speakers in the corpus, 232 young speak­ers without disabilities 
were recorded uttering one session of the 57 RFI words for a total of 13,224 utterances. The speakers 
were balanced in the range of age of 10 to 18 years old and in gender. 4. EXPERIMENTS AND RESULTS The 
experimental setup was based on the diagram in Fig­ure 1. A Viterbi based forced alignment was made over 
every input utterance, according to the baseform transcription of Figure 2: Histogram of the baseline 
scores the word to be evaluated. The log-likelihood score was ob­tained in Equation 1 as the logarithm 
value of the likelihood probability of the segment of speech s being generated by the model . averaged 
by the total number of frames (Np) assigned to the segment. log(P (s|.)) LL(p) = (1) Np The likelihood 
probability of the speech segment and the model was calculated for the Np frames of the segment as the 
probability of each frame (tn with n =1...Np) in the Gaus­sian Mixture Model (GMM) (g =1...G) that de.ned 
the current state of the phoneme Hidden Markov Model (HMM), as in Equation 2. NpG NN P (s|.)= ((p(g)p(tn|g)) 
(2) n=1 g=1 Posteriorly, a score normalization method was applied, and .nally, the sigmoid function 
in Equation 3 reduced the score interval of the log-likelihood score (LL . [-8, +8]) to LLSigmoid . [-1, 
+1] and a threshold decided whether each phoneme was evaluated as correct or mispronounced. 1 LLSigmoid(p)=2 
* () - 1 (3) 1+ e-LL(p) The acoustic models used were a set of 27 phoneme Hid­den Markov Models (HMM) 
trained from the unimpaired children speech in the corpus, representing 23 phonemes of Spanish, 2 allophones 
(glides [j] and [w]), and two silence models. Each model had 3 states whose probability density Figure 
3: N-best normalization scheme function was a Gaussian Mixture Model (GMM) of 32 Gaus­sians. Every speech 
input frame was transformed into a 39 Mel Frequency Cepstrum Coe.cients (MFCC) containing the .rst 12 
cepstral coe.cients and the log-energy of the frame, plus the .rst and second derivatives of the 13 static 
parameters. The log-likelihood scores for the 14 impaired speakers pre­sented a very low separability 
as it could be appreciated in the histograms of the scores for correct and mispronounced phonemes in 
Figure 2. This histogram presents the scores prior to the sigmoid function, and it could be seen how 
both groups (correct vs. mispronounced phonemes) were severely intermingled. A total of 13,472 correct 
phonemes and 2,880 mispronounced phonemes were evaluated, with their scores calculated, and the Equal 
Error Rate (EER) for these scores was 43%, close to the worse possible scenario of 50% EER, remarking 
the dramatic e.ect of speaker variability. This preliminary study showed, hence, the need for tech­niques 
to avoid the pernicious e.ects of speaker and channel variability in the pronunciation veri.cation task. 
4.1 Score normalization techniques Two score normalization techniques were proposed as a .rst way to 
eliminate speaker variability in the task. The .rst one was the T-norm approach [1] used in speaker ver­i.cation. 
This technique makes a Gaussian normalization of the score achieved by the acoustic model of the target 
speaker; for this task, the normalization is made over the statistics (mean µ and standard deviation 
s) of the scores achieved by the rest of phonemes in the given segment, as in Equation 4 LL(p) - µ LLT 
-norm(p) = (4) s A more novel technique was proposed, following the di­agram in Figure 3. For a given 
speech segment, that the forced alignment had assigned to the baseform phoneme p, out of the list of 
N possible phonemes (1, ..., N), the log­likelihood of phoneme p was calculated as LL(p). The log­likelihoods 
of all the possible N - 1 competing phonemes (1, ..., p - 1,p +1, ..., N) were also obtained for that 
seg­ment (LL(1), ..., LL(p - 1), LL(p + 1), ..., LL(N)) and or­ganized in an N-best list according to 
their values. The log-likelihhod of the .rst phoneme in the N-best list (for example the phoneme l, with 
LL(l)) was .nally subtracted Figure 4: Detection curves with score normalization techniques  Figure 
5: Detection curves with speaker adaptation techniques from the log-likelihood of the baseform phoneme 
(LL(p)) for normalization as in Equation 5. LLN-best(p)= LL(p) - LL(l) (5) This method of normalization, 
in some way resembling cohort normalization [7], was measuring how the baseform phoneme was positioned 
in terms of the N-best list. Pos­itive values (LL(p) > LL(l)) indicated that the baseform phoneme was 
the most likely for the speech segment (higher normalized score meaning higher di.erence with the follow­ing 
phoneme in the list); and negative values (LL(p) < LL(l)) indicated that one of the competitors was more 
likely than the baseform phoneme (lower normalized score mean­ing higher di.erence between the baseform 
phoneme and the .rst phoneme in the N-best list). This score normalization method also obtains automatically 
an alternative transcrip­tion of the prompted word which can be used later for other purposes or for 
feedback in a language learning tool. The detection curves for both normalization techniques are provided 
in Figure 4. The baseline Equal Error Rates (EER) of these proposed systems were 22.80% for T-norm and 
21.60% for the novel N-best normalization, with a 5.26% of improvement over the T-norm, indicating the 
better prop­erties of the proposed score normalization method. Table 2: EER with speaker dependent models 
Unlabeled adapt. Labeled adapt. T-norm N-best T-norm N-best EER 21.29% 19.17% 18.16% 15.63%  Figure 
6: 2-pass adaptation and scoring process  4.2 Speaker adaptation for pronunciation ver­i.cation The 
strategy for speaker adaptation was a set of four leave-one-out experiments where speaker dependent models 
were trained via the Maximum A Posteriori (MAP) algo­rithm [2] and the scores of the evaluated sessions 
were gath­ered to study the detection curves. MAP algorithm provides a fast and reliable convergence 
when su.cient data is avail­able. In this case, there are three sessions for adaptation, so every unit 
appeared a su.cient number of times to pro­vide the good properties of this speaker adaptation. Each 
model was tested over the remaining session not used for the adaptation. Two approaches for speaker adaptation 
were tested. In the .rst one, all the phonemes in the utterance were used for adaptation, without considering 
that some phonemes might be mispronounced or not. In the second approach, only the phonemes which were 
considered as correct by the human la­belers were fed to the speaker adaptation. Detection curves for 
both techniques and both normalizations are plotted on Figure 5 and the EER are in Table 2. EER for them 
were 19.17% for the unlabeled adaptation and 15.63% for the labeled adaptation ( labeled meaning that 
it used infor­mation from the labels given by the experts). Both methods provided relative improvements 
(11.25% and 27.64%) over the speaker independent models but the adaptation that used the experts labels 
gave an extra improvement of more than 15%. Furthermore, in both cases the novel N-best nor­malization 
outperformed the T-norm for an up to 14% of improvement. 4.3 2-pass system Once seen that increasing 
the precision of the acoustic models increased the performance, it was evaluated the pos­sibility of 
obtaining automatically an estimation of the mis­pronounced phonemes, without requiring the manual label­ing, 
with the speaker independent pronunciation veri.cation algorithm. A 2-pass system was designed as seen 
in Figure 6. Speaker adaptation was run over the adaptation data which was previously evaluated by the 
speaker independent pronunciation veri.cation algorithm; hence, the input to the adaptation were the 
speech signals, the baseform transcrip­tions and the evaluation results to discard mispronounced phonemes. 
Those speaker dependent models were used in the pronunciation veri.cation of the test data. The same 
4 leave-on-out experiments were prepared to assure compara­bility with the previous results. This 2-pass 
system was very dependent on the precise op- Figure 7: Detection curves for the 2-pass system Figure 
8: EER zone for the 2-pass system erating point in which the .rst pronunciation veri.cation system could 
be con.gured; for that, three operating points where chosen and studied for this system: The .rst one 
is the EER point of the speaker independent veri.cation system, the second one is the point with only 
10% of false acceptance (with approximately 40% of false rejections) and the third one is the point with 
10% of false rejections (with approxi­mately 40% of false acceptances). Working points situated more 
in the extremes of the detection curve would lead to the situations of no adaptation as all the data 
is rejected or adaptation with all the data as everything is accepted (situation similar to the unlabeled 
adaptation). The three detection curves achieved by the three operating points are plotted in Figure 
7 and zoomed around the EER area in Figure 8. The EER for the three working points were 18.91%, 19.38% 
and 18.82% for the EER, 10% false accep­tance and 10% false rejection operating points respectively. 
It was seen how the best operation points for the .rst pronunciation veri.cation phase in the 2-pass 
system were those who tried to use more data, achieving a 14-15% of im­provement over the speaker independent 
system and a 1.5% of improvement over the unlabeled adaptation that used all the data for re-training. 
This gain had no signi.cance to outperform the system that used all units without con­sidering whether 
they are correct or not, and it was still far from the labeled adaptation that achieved a 15.63% EER 
by retraining only with the phonemes considered correct by the human labelers.  5. DISCUSSION AND CONCLUSIONS 
 Several elements for discussion arose after the experiments run in score normalization techniques and 
speaker adapta­tion for pronunciation veri.cation. Regarding the proposed score normalization techniques, 
the novel N-best normalization outperformed T-norm with both speaker independent and speaker dependent 
models. This gain of performance was due to the properties of the phoneme scores in the pronunciation 
veri.cation task. T­norm hypothesized the Gaussian properties of the di.erent impostor scores and, hence, 
applied a Gaussian mean and standard deviation normalization. In phoneme veri.cation, the Gaussian properties 
of the competing phonemes scores could not be assured. On the contrary, the proposed system aimed to 
hypothesize an alter­native transcription that could be used for providing further information about 
the pronunciation made by the speaker. However, in further work, the N-best based score normaliza­tion 
technique could be improved by using more information of the N-best list of phonemes instead of only 
from the .rst phoneme in the list. In terms of the speaker adaptation frameworks, the gain of performance 
achieved by the models adapted only with the correct phonemes (as labeled by the human experts) was due 
to the better speci.city of the trained models, which only had seen correct data and could separate better 
the phoneme variability in the test utterances. When mispro­nounced data was fed to the adaptation framework, 
models lost part of their ability to discriminate correct and mistaken pronunciations, producing worse 
performance. Further work in this area might include the improvement of the 2-pass system for speaker 
adaptation of the phoneme acoustic models with only correctly pronounced data. The results in this work 
were still far from the better possibility of knowing the mispronounced phonemes by means of the human 
labeling. With all this, these results have proven the feasibility of designing CAPT tools based in the 
proposed systems. An early implementation of them was developed for a Second Language (L2) learning tool, 
VocalizaL2 , which was tested in a multilingual environment with young students at the Vienna International 
School (VIS) [8]. 6. ACKNOWLEDGEMENTS This work was supported by national project TIN2008­06856-C05-04. 
The authors would like to thank Prof. Richard Rose, Yun Tang and Shou-Chun Yin from McGill University 
in Mon­tre´al (Canada) for their fruitful ideas and discussion for this work. 7. REFERENCES [1] R. Auckenthaler, 
M. Carey, and H. Lloyd-Thomas. Score normalization for text-independent speaker veri.cation systems. 
Digital Signal Processing, 10(1 3):42 54, January 2000. [2] J.-L. Gauvain and C.-H. Lee. Maximum A Posteriori 
estimation for multivariate Gaussian mixture observations of Markov chains. IEEE Transactions on Speech 
and Audio Processing, 2(2):291 298, 1994. [3] P. Kenny, G. Boulianne, P. Ouellet, and P. Dumouchel. Improvements 
in factor analysis-based speaker veri.cation. In Proceedings of the 2006 Intl Conf on Acoustics, Speech 
and Signal Processing (ICASSP), pages 113 116, Toulouse, France, May 2006. [4] P. Kenny, M. Mihoubi, 
and P. Dumouchel. New MAP estimators for speaker recognition. In Proceedings of the 8th European Conference 
on Speech Communication and Technology (Eurospeech-Interspeech), pages 2961 2964, Geneva, Switzerland, 
September 2003. [5] M. Monfort and A. Ju´arez-S´anchez. Registro Fonol´ogico Inducido (Tarjetas Gr´a.cas). 
Ed. Cepe, Madrid, Spain, 1989. [6] A. Neri, C. Cucchiarini, and H. Strik. Improving segmental quality 
in l2 dutch by means of computer assisted pronunciation training with automatic speech recognition. In 
Proceedings of CALL 2006, pages 144 151, Antwerp, Belgium, 2006. [7] A. Rosenberg, J. Delong, C. Lee, 
B. Juang, and F. Soong. The use of cohort normalized scores for speaker recognition. In Proceedings of 
the 2nd Intl Conference on Spoken Language Processing (ICSLP -Interspeech), Ban., Canada, 1992. [8] 
O. Saz, V. Rodr´iguez, E. Lleida, W.-R. Rodr´iguez, and C. Vaquero. An experience with a Spanish Second 
Language learning tool in a multilingual environment. In Proceedings of the 2009 Workshop on Speech and 
Language Technologies in Education SLaTE, Wroxall Abbey Estates, United Kingdom, September 2009. [9] 
O. Saz, W.-R. Rodr´iguez, E. Lleida, and C. Vaquero. A novel corpus of children s impaired speech. In 
Proceedings of the Workshop on Children, Computer and Interaction, Chania, Greece, October 2008. [10] 
J. Tepperman, J. Silva, A. Kazemzadeh1, H. You, S. Lee, A. Alwan, and S. Narayanan. Pronunciation veri.cation 
of children s speech for automatic literacy assessment. In Proceedings of the 2006 International Conference 
on Spoken Language Processing (ICSLP -Interspeech), Pittsburgh (PA), USA, September 2006. [11] C. Vaquero, 
O. Saz, E. Lleida, and W.-R. Rodr´iguez. E-inclusion technologies for the speech handicapped. In Proceedings 
of the 2008 International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4509 
4512, Las Vegas (NV), USA, April 2008.  
			