
 = = = = == = These formulae re.ect an online approximation of one E­step in the EM algorithm. We present 
here an intuitive derivation to illustrate why they make sense as such an ap­proximation. We would like 
to recursively estimate P(z|ot) from partial estimates of P(z|ot,i). First, notice that ot,0 is the empty 
word. This immediately gives us the base case. P(z|ot,0)= P(z) We can express P(z|ot,i) in terms of our 
previous informa­tion as follows. P(z|ot,i)= P(w)P(z|w, ot,i-1) w.ot,i We assume that, in a partial observation 
sequence oi, the marginal probability of selecting any word is simply 1/(i + 1). Observe that when w 
= wi, the word is assumed to have been accounted for in P(z|oi-1) and is absorbed in the conditioning. 
When w = wi, we can compute P(z|wi,oi-1) by a simple application of Bayes rule. 1 i P(z|ot,i)= P(z|wi,ot,i-1)+ 
P(z|ot,i-1) i +1 i +1 1 P(wi|z, ot,i-1)P(z|ot,i-1) =+ i +1 P(wi) i P(z|ot,i-1) i +1 1 P(wi|z)P(z|ot,i-1) 
i +1 l P(wi|z:)P(z:|ot,i-1) z i P(z|ot,i-1) i +1 The .nal equation expresses P(z|ot,i) in terms of P(z|ot,i-1). 
As the approximator sees more words in a single observation, it re.nes its posterior distribution of 
the topic. It uses this re.ned posterior to weight the distribution of the next word. 5. EXPERIMENTAL 
RESULTS We applied this segmentation model to two large corpora. First, we examined Speechbot transcripts 
from All Things Considered (ATC), a daily news program on National Pub­lic Radio. Our corpus spans 317 
shows from August 1998 through December 1999. Within these shows there are 4,917 segments with a vocabulary 
of 35,777 unique terms. The shows constitute about 4 million words. We estimated the word error rate 
in this corpora to be in the 20% to 50% range [6]. Note that these are only estimates computed from sam­pling 
the corpora as perfect transcripts are unavailable to us. Additionally, we analyzed a corpus of 3,830 
articles from the New York Times (NYT) to compare the ASR perfor­mance with error-free text. This corpus 
constitutes about 4 million words with a vocabulary of 70,792 unique terms. In all reported experiments, 
we learn an aspect model with 20 hidden factors. 5.1 Aspect model EM training Figure 3 illustrates the 
performance on held out data dur­ing the tempered EM training of the aspect model (see sec­tion 4.1). 
This .gure shows that the NYT corpus converges faster than the ATC corpus, despite the larger vocabulary 
0.9 0.85 0.8 0.75 0.7 0.65 0.6 0.55 0.5 0.45 Iterations of EM  Average COAP of held-out data Figure 
3: Tempered EM convergence in the NYT (upper line) and ATC (lower line) corpora size, since the text 
is error free. Furthermore, the NYT cor­pus converges to a better success rate (see section 5.3 for how 
we measure success).  5.2 Sample results and topic labels In our experiments, we used three variants 
of our two cor­pora. First, we created random sequences of segments from the ATC corpus. Second, we created 
random sequences from the NYT corpus to compare clean versus noisy segmenta­tion. Finally, we used the 
actual aired sequences of ATC segments since this is the domain of the primary problem which we are trying 
to tackle. In the random sequences of segments, we attained almost perfect segmentation on both corpora. 
However, the results are mixed with the original broadcasts of the ATC. Figure 4 shows a segmentation 
from a correctly sequenced transcript of ATC on April 29, 1999. The segmentation is not perfect but hypothesizes 
the detected topic breaks at approximately the correct points in the program. At .rst, there seem to 
be many missed breaks. We argue however that these missed story breaks do not always constitute topic 
breaks and there­fore are not indicative of the performance of our model. To illustrate this, we explore 
a method of topic labeling based on the language model parameters of the aspect model. One way of identifying 
the topics which the segmenter .nds is by the top .fteen words of the P(w|z) parameter for the value 
of z which the Viterbi algorithm assigned to a par­ticular segment. Figure 5 lists these word sets (denoted 
by a letter) as they correspond to the topics in the segmentation (denoted by a number). For example, 
story 14 is about the Israeli/Palestinian con.ict. Its corresponding segment in the hypothesis segmentation 
can be described by the words in topic F which include peace, israeli, and palestinian. Analysis of this 
correspondence often explains missed topic breaks. Articles 11 and 12 are both about the Kosovar refugees. 
Understandably, they are both assigned to topic A and the break between stories goes undetected. Note 
that the segmenter can work even if the top words of P(w|z) fail to give a good topic description. The 
story   
			