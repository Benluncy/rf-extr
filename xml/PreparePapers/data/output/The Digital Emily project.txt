
 Creating a Photoreal Digital Actor: The Digital Emily Project Oleg Alexander Mike Rogers William Lambeth 
Matt Chiang Paul Debevec Image Metrics USC Institute for Creative Technologies Figure 1: The Digital 
Emily Pipeline: Scanning, Scan processing, Rigging &#38; Animation, and Rendering Abstract The Digital 
Emily Project was a 2008 collaboration between facial animation company Image Metrics and the Graphics 
Laboratory at the University of Southern California s Institute for Creative Tech­nologies to achieve 
one of the world s .rst photorealistic digital facial performances. The project leveraged latest-generation 
tech­niques in high-resolution face scanning, character rigging, video­based facial animation, and compositing. 
By building an ani­matable face model whose expressions closely mirror the shapes observed in a rich 
set of facial scans, acquiring realistic skin re­.ectance maps, and faithfully driving the face by video 
of an actual performance, the project rendered a synthetic facial performance which was generally mistaken 
to be a real face. 1 Introduction Creating a photoreal digital actor with computer graphics has been 
a central goal of the .eld for at least thirty years [Parke 1972]. The Digital Emily project undertaken 
by Image Metrics and the Uni­versity of Southern California s Institute for Creative Technologies (USC 
ICT) attempted to achieve an animated, photoreal digital face by bringing together latest-generation 
results in 3D facial capture, modeling, animation, and rendering. The project aimed to cross the "uncanny 
valley" [Mori 1970], producing a computer-generated face which appeared to be a real, relatable, animated 
person. Some of the key technologies employed included a fast high-resolution digital face scanning process 
using the light stage at USC ICT, and the Image Metrics video-based facial animation system. The result 
of the project was by several accounts the .rst public demonstration of a photoreal computer-generated 
face able to convincingly speak and emote in a medium closeup. 1.1 Project Goals The central goal of 
the Digital Emily project was to animate and render a digital face able to address the key requirements 
of a digital actor for feature .lm production: to appear realistic from any view­point, any illumination, 
any expression, and animated to produce any performance. We based the face on a real living person, actress 
Emily O Brien, to allow direct comparison between a real facial performance and a synthesized version 
of it.  2 Previous Efforts at Photoreal Digital Humans A variety of notable efforts have been made 
to create realistic digi­tal actors over the last decade, each leveraging numerous advances in computer 
graphics technology and artistry. In this section, we overview some of these key efforts in order to 
compare and contrast them with the Digital Emily project. The SIGGRAPH 1999 Electronic Theater featured 
"The Jester" [Kaufman 1999], a short animation by Life/FX of a young woman in a dark environment reading 
abstract poetry in jester s cap cover­ing her head and ears. The actress used for the test (and author 
of the poetry) was Jessica Vallot, a dancer of African/Russian decent, whose face was three-dimensionally 
laser scanned and textured us­ing photographic textures stitched together with artistic effort to minimize 
the original shading and specular re.ectance effects, and her performance was recorded with a traditional 
arrangement of motion capture markers. Using an extension of research techniques developed at the University 
of Auckland and MIT, the motion cap­ture dots were used to drive a volumetric .nite element model which 
allowed a high-resolution facial mesh to produce simulated buckling and wrinkling from anisotropic stress 
at a scale signif­icantly more detailed than the original motion capture data was able to record. While 
the skin shading lacked realistic specular re.ectance, and pore detail, the face conveyed realistic motion 
and emotion in signi.cant part due to the skin dynamics model. The lack of noticeable specular re.ectance 
and skin translucency in the shading model was mitigated by the darker color of the actress skin and 
the plausibility that she may have been wearing makeup to cover skin pores. The lack of global illumination 
in the light­ing model was also likely mitigated by the darker skin tone (which would produce less interre.ection 
in facial concavities) and the fact that the performance took place in a dark void under spotlights rather 
than a more natural environment with both direct and ambi­ent illumination. Some bright regions under 
the eyes resulting from specular re.ections in the original face photography resulted in a somewhat odd 
appearance under the eyes, but the emotion content of the performance was clearly communicated and the 
animation, more successfully than not, looked like a real person. The Life/FX process employed in "The 
Jester" was also used in a little-seen test for an as-yet-unmade .lm version of "The Incredible Mr. Limpett", 
at the time to star Jim Carrey as a live-action version of the human-faced .sh originally voiced (and 
resembling) Don Knotts in the 1960 s television series. As for "The Jester", Mr. Car­rey was similarly 
photographed, scanned, modeled and animated, and rendered results of his animated face on a black background 
were notably compelling, especially given the realistic skin wrin­kling around the eyes during extreme 
transitions in expression. The test has not yet been publicly seen. Before the company s demise in the 
early 2000 s, the Life/FX process was used on the SIGGRAPH 2000 Electronic Theater short "Young At Heart", 
featuring Jessica Vallot playing a digitally aged version of the jester character. The Life/FX team consulted 
with cosmetic surgeons to sculpt a clay model of how Ms. Vallot might look in her 70 s, and based the 
elder Jessica s texture maps by editing the texture map from "The Jester" animation. Again, motion capture 
data of Jessica Vallot with a rel­atively dense set of markers was used to drive the elderly version 
of her performance, which was tracked and composited onto plate photography of Jessica shot on an live-action 
set. Despite many successful elements, the test struggled somewhat to meet its more ambitious challenges, 
namely that the face was no longer based on the appearance of a real person, and that the face needed 
to integrate onto a real actor in a real environment providing ample comparisons to reality in frame. 
In addition, the age of the character made the lack of skin pore and .ne crease detail implausible. In 
the end, these issues prevented "Young at Heart" from achieving a fully photore­alistic digital human, 
although the performance certainly conveyed notable emotion and expression. Disney s "Human Face Project" 
was an R&#38;D test for an as-yet­unproduced feature .lm that would show a recognizable older actor encountering 
a younger version of himself -a version already well known to audiences through his feature work [Wolff 
2003; Hyne­man et al. 2005]. Visual effects team member Price Pethel served as the test subject. A facial 
mold of Pethel s face was taken and the resulting cast was scanned using a the Arius 3D high-resolution 
face scanning process, and twenty-two Cyberware scans in various expressions were also acquired. A medium-resolution 
animated fa­cial rig was somewhat laboriously sculpted to mimic the expression scans used as reference. 
A multi-camera facial capture setup was employed to .lm the face under relatively .at cross-polarized 
light­ing. Polarization difference images isolating specular re.ections acquired as in [Debevec et al. 
2000] were used to reveal high­resolution texture detail for the face, and analysis of this texture was 
used to add skin pores and .ne wrinkle detail as displacement maps to the scans. Instead of driving the 
face with facial motion capture dots, a system which performed optical .ow on the performance was used 
to track video of Pethel s performance per pixel and drive the digital version of himself; achieving 
remarkably close matches to the original performance. In addition to making a photoreal ver­sion of Pethel 
at his current age, a younger version of Pethel was artistically constructed based on photographs of 
him in his 20 s. The .nal test was to show the digitally younger Pethel interacting side-by-side with 
real video of Pethel himself. High dynamic range lighting information was captured on set so that the 
digital Pethel could be rendered with image-based lighting [Debevec 1998] to match the on-set illumination 
of the live actor. Final renderings achieved relatively convincing facial motion and lighting in a two­shot 
but less convincing facial re.ectance; a signi.cant problem was that no simulation of the skin s translucency 
[Jensen et al. 2001] had been performed, the effects of which should have been readily apparent in the 
actor s relatively light-colored skin. The Matrix Sequels (2003) used digital actors in several key vi­sual 
effects sequences, most notably for sequences involving Neo (Keanu Reeves) acrobatically .ghting the 
self-replicating Agent Smith (Hugo Weaving), although some scenes were accomplished using prosthetics 
of Weaving s face worn by thinner-faced body doubles. As in Disney s Human Face Project, high-quality 
facial casts of Reeves and Weaving were acquired and scanned with a high-resolution laser scanning system 
(in this case, the XYZRGB system based on technology from the Canadian National Research Council) to 
provide 3D geometry accurate to the level of skin pores and .ne wrinkles. A six-camera high-de.nition 
facial capture rig was used to .lm facial performance clips of the actors under rela­tively .at illumination. 
The six views in the video were used both to animate a facial rig of the actor (in this case, driven 
by markerless optical .ow rather than facial markers) and also, as seen in Mi­crosoft Research s "Making 
Faces" project [Guenter et al. 1998], to provide time-varying texture maps for the dynamic facial ap­pearance. 
Still renderings using image-based lighting [Debevec 1998] and a texture-space approximation to subsurface 
scattering [Borshukov and Lewis 2003] showed notably realistic faces which greatly bene.tted from the 
high-resolution geometric detail tex­ture found in the XYZRGB scans. However, the animated facial performances 
in the .lm were shown in relatively wide shots and exhibited less realistic skin re.ectance, perhaps 
having lost some appearance of geometric detail during normal map .ltering. The animated texture maps, 
however, provided a convincing degree of dynamic shading and albedo changes to the facial performances. 
A tradeoff to the use of video textures is that the facial model could not easily generate novel performances 
unless they too were captured in the complete performance capture setup. Spider Man 2 (2004) built digital 
stunt doubles for villain Doc Ock (Alfred Molina) and hero Spider-Man (Tobey Maguire) using fa­cial re.ectance 
scans in USC ICT s Light Stage 2 device [Sagar et al. 2004]. Each actor was .lmed with four synchronized 
35mm .lm cameras in several facial expressions from 480 lighting di­rections. Colorspace techniques as 
in [Debevec et al. 2000] were used to separate diffuse and specular re.ections. The relightable texture 
information was projected onto a 3D facial rig based on geometry from a traditional laser scan, and illuminated 
variously by HDRI image-based lighting [Debevec 1998] and traditional CG light sources using a custom 
shading algorithm for approximately 40 digital double shots. Using additional cameras, the technique 
was also used to construct digital actors for Superman Returns (2006), Spider Man 3 (2007), and Hancock 
(2008). Due to the extensive re.ectance information collected, the technique yielded realistic facial 
re.ectance for the digital characters, including close­up shots with mild degrees of facial animation, 
especially in Super­man Returns. However, results of the process did not demonstrate emotive facial performances 
in closeup; signi.cant facial animation was shown only in wide shots. Although they were not human characters, 
Gollum and Davy Jones from the Lord of the Rings Sequels (2002, 2003) and Pirates of the Carribbean Sequels 
(2006, 2007) also advanced the level of achievement in digital characters. Key to the success of these 
characters was that real actors (Andy Serkis and Bill Nighy, re­spectively) played the characters in 
the plate photography. This allowed much of the physical, emotional, and photometric effects of the digital 
actor on the .lm s sets and human characters to nat­urally result from real actor s physical presence 
on the set. More importantly, the real actors provided rich sources of reference for the digital actor 
s movements, illumination, and facial performance, all derived from combinations of manual and automated 
techniques. The animation of the .nal characters bene.tted from notably com­plex and expressive facial 
rigs, facial tissue translucency through subsurface scattering [Jensen et al. 2001], and illumination 
using ambient occlusion approximations [Landis 2002] to environmental illumination. Beowulf (2007) used 
a multitude of digital characters for a fully computer-rendered .lm. The performance capture based .lm 
fol­lowing the approach of the 2001 .lm Final Fantasy of construct­ing as detailed characters as possible 
and then driving them with motion capture, keyframe animation, and simulation. Beowulf sub­stantially 
advanced the state of the art in this area by leveraging greater motion capture .delity and performance 
volume, employing more complex lighting simulation, and using better skin shading techniques. While some 
characters were based closely in appear­ance their voice and motion capture actors (e.g. Angelina Jolie 
and Anthony Hopkins), other characters bore little resemblance (e.g. Ray Winstone), requiring additional 
artistic effort to model them. Static renderings of the faces achieved impressive levels of realism, 
although the single-layer subsurface scattering model produced a somewhat "waxy" appearance. Also, the 
artist-driven character creation process did not leverage high-resolution 3D scans of each actor in a 
multitude of facial expressions. According to Variety [Chang 2007], the "digitized .gures in Beowulf 
look eerily close to storefront mannequins ... suspended somewhere between live­action and animation, 
fairy tale and videogame." The Curious Case of Benjamin Button, released Christmas Day 2008, was the 
.rst feature .lm to feature a photoreal human vir­tual character. The .lm s aged version of Brad Pitt 
 seen in the .lm s .rst 52 minutes was created by visual effects studio Digital Domain and "leveraged 
recently developed offerings from Mova, Image Metrics, and the [USC] Institute for Creative Technologies" 
[Robertson 2009]. Silcone maquettes of Brad Pitt as an old man were constructed by Kazuhiro Tsuji at 
Rick Baker s makeup stu­dio and used as the basis for the digital character. Detailed facial re.ectance 
capture was performed for the age 70 maquette in USC ICT s Light Stage 5 device [Wenger et al. 2005] 
from eight angles and 156 lighting directions. Medium-resolution meshes of Brad Pitt s face in a multitude 
of facial expressions were captured us­ing Mova s Contour facial capture system [Perlman 2006], using 
a mottled pattern of glow-in-the-dark makeup to create facial geom­etry from multi-view stereo. Digital 
Domain used special software tools and manual effort to create a rigged digital character whose articulations 
were based closely on the facial shapes captured in the Mova system. Image Metrics video-based facial 
animation sys­tem was used to provide animation curves for the Benjamin facial rigs by analyzing frontal, 
.at-lit video of Brad Pitt performing each scene of the .lm in a studio; these animation curves were 
frequently re.ned by animators at Digital Domain to create the .nal perfor­mances seen in the .lm. Extensive 
HDRI documentation was taken on each .lm set so that advanced Image-Based Lighting techniques based on 
[Debevec 1998] could be used to render the character with matching illumination to the on-set lighting. 
Image-based relight­ing techniques as in [Debevec et al. 2000] were used to simulate the re.ectance of 
the scanned maquette in each illumination envi­ronment as cross-validation of the digital character s 
lighting and skin shaders. The .nal renderings were accurately tracked onto the heads of slight, aged 
actors playing the body of Benjamin in each scene. The quality of the character was universally lauded 
as a breakthrough in computer graphics, winning the .lm an Academy Award for Best Visual Effects. An 
estimate of more than two hun­dred person-years of effort was reportedly spent [Robertson 2009] creating 
the character in this groundbreaking work.  3 Pipeline Overview The major steps executed in the Digital 
Emily project were: 1. Acquiring high-resolution scans of the actor in various facial expressions 2. 
Building a rigged digital character from the scans 3. Video-based facial animation 4. Tracking, Lighting, 
Rendering, and Compositing  The next four sections of this document describe these parts of the project 
in detail.  4 Acquiring high-resolution scans of the actor in various facial expressions Emily O Brien, 
an Emmy-nominated actress from the American daytime drama "The Young and the Restless", was cast for 
the project. After being .lmed seated describing aspects of the Image Metrics facial animation process 
on an informal studio set, Emily came to USC ICT to be scanned in its Light Stage 5 device on the afternoon 
of March 24, 2008. A set of 40 small facial dots were applied to Emily s face with a dark makeup pencil 
to assist with fa­cial modeling. Emily then entered Light Stage 5 for approximately 90 minutes during 
which data for thirty-seven high-resolution facial scans were acquired. Fig. 2 shows Emily in the light 
stage during a scan, with all 156 of its white LED lights turned on.  The light stage scanning process 
used for Digital Emily was de­scribed in [Ma et al. 2007]. In contrast to earlier light stage pro­cesses 
(e.g. [Debevec et al. 2000; Hawkins et al. 2004]), which photograph the face under hundreds of illumination 
directions, this newer capture process requires only .fteen photographs of the face under different lighting 
conditions as seen in Fig. 3 to capture ge­ometry and re.ectance information for a face. The photos are 
taken with a stereo pair of Canon EOS 1D Mark III digital still cameras, and the images are suf.ciently 
few so that they can be captured in the cameras "burst mode" in under three seconds, before any data 
needs to be written to the compact .ash cards. 4.1 Estimating Subsurface and Specular Albedo and Nor­mals 
Most of the images are shot with essentially every light in the light stage turned on, but with different 
gradations of brightness. All of the light stage lights have linear polarizer .lm placed on them, af.xed 
in a special pattern of orientations, which permits the mea­surement of the specular and subsurface re.ectance 
components of the face independently by changing the orientation of a polarizer on the camera. The top 
two rows of Fig. 3 show Emily s face under four spherical gradient illumination conditions and then a 
point-light condition, and all of the images in this row are cross-polarized to eliminate the shine from 
the surface of her skin her specular component. What remains is the skin-colored "subsurface" re.ection, 
often re­ferred to as the "diffuse" component. This is light which scatters within the skin enough to 
become depolarized before re-emerging. Since this light is depolarized, approximately half of this light 
can pass through the horizontal polarizer on the camera. The top right image is lit by a frontal .ash, 
also cross-polarizing out the specular re.ection. The middle row of Fig. 3 shows parallel-polarized images 
of the face, where the polarizer on the camera is rotated vertically so that the specular re.ection returns, 
in double strength compared to the attenuated subsurface re.ection. We can then reveal the specular re.ection 
on its own by subtracting the .rst row of images from the second row, yielding the specular-only images 
shown in Fig. 4. Figure 4: Emily s specular component under the four gradient lighting conditions (all, 
left, top, front) and single frontal .ash con­dition, obtained by subtracting the cross-polarized images 
from the parallel-polarized images. Fig. 5(a) is a closeup of the "diffuse-all" image of Emily. Every 
light in the light stage is turned on to equal intensity, and the po­larizer on the camera is oriented 
to block the specular re.ection from every single one of the polarized LED light sources. Even the highlights 
of the lights in Emily s eyes are eliminated. This is about as .at-lit an image of a person s face as 
can con­ceivably be photographed, and thus it is nearly a perfect image to use as the diffuse texture 
map for the face in building a digital actor. The one issue is that it is affected to some extent by 
self­shadowing and interre.ections, making the concavities around the eyes, under the nose, and between 
the lips appear somewhat darker and more color-saturated than it inherently is. Depending on the (a) 
 (b) (c)   Figure 5: Closeups of Emily s (a) cross-polarized subsurface com­ponent, (b) parallel-polarized 
subsurface plus specular component, and (c) isolated specular component formed by subtracting (a) from 
(b). The black makeup dots on her face are easily removed digitally and help with aligning and corresponding 
her scans. rendering technique chosen, having these effects of occlusion and interre.ection "baked in" 
to the texture map is either a problem or an advantage. For real-time rendering, the effects can add 
real­ism (as if the environmental light were entirely uniform) given that simulating them accurately 
might be computationally prohibitive. If new lighting is being simulated on the face using a more accurate 
global illumination technique, then it is problematic to calculate self-shadowing of a surface whose 
texture map already has self­shadowing present; likewise for interre.ections. In this case, one could 
perform inverse rendering by using the actor s 3D geometry and approximate re.ectance to predict the 
effects of self-shadowing and/or interre.ections, and then divide these effects out of the tex­ture image 
as in [Debevec et al. 2004]. Fig. 5(a) also shows the makeup dots we put on Emily s face which help us 
to align the images in the event there is any drift in her position or expression over the .fteen images; 
they are relatively easy to remove digitally. Emily was extremely good at staying still for the three-second 
scans and many of her datasets required no motion compensation at all. (Faster capture times are already 
pos­sible: 24fps capture of such data using high-speed video cameras is described in [Ma et al. 2008]). 
The shinier image in Fig. 5(b) is also lit by all of the light stage lights, but the orientation of the 
polarizer has been turned 90 de­grees which allows the specular re.ections to return. Her skin exhibits 
a specular sheen, and the re.ections of the lights are now evident in her eyes. In fact, the specular 
re.ection is seen at double the strength of the subsurface (or diffuse) re.ection, since the polar­izer 
on the camera blocks about half of the unpolarized subsurface re.ection. Fig. 5(b) shows the combined 
effect of specular re.ection and subsurface re.ection. For modeling facial re.ectance, we would ideally 
observe the specular re.ection independently. As a useful alternative, we can simply subtract the diffuse-only 
image Fig. 5(a) from this one. Taking the difference between the diffuse-only image and the diffuse-plus-specular 
image yields an image of primarily the specular re.ection of the face as in 5(c). A polarization dif­ference 
process was used previously for facial re.ectance analysis in [Debevec et al. 2000], but only for a single 
point light source and not for the entire sphere of illumination. The image is mostly colorless since 
this light has re.ected specularly off the surface of the skin, rather than entering the skin and having 
its blue and green colors signi.cantly absorbed by skin pigments and blood before re.ecting back out. 
This image provides a useful starting point for building a digital character s specular intensity map, 
or "spec map"; it shows for each pixel the intensity of the specular re.ection at that pixel. However, 
the specular re.ection becomes ampli.ed near grazing angles, such as at the sides of the face according 
to the denominator of Fresnel s equations. We generally model and compensate for this effect using Fresnel 
s equations but also discount regions of the face at extreme grazing angles. The image also includes 
some of the effects of "re.ection occlusion" [Landis 2002]. The sides of the nose and innermost contour 
of the lips appear to have no specular re.ection since self-shadowing prevents the lights from re.ecting 
in these an­gles. This effect can be an asset for real-time rendering, but should be manually painted 
out for of.ine rendering. Recent work [Ghosh et al. 2008] reports that this sort of polariza­tion difference 
image also contains the effects of single scattering, wherein light refracts into the skin but scatters 
exactly once before refracting back toward the camera. Such light can pick up the color of the skin s 
melanocytes, adding some color to the specular image. However, the image is dominated by the specular 
component s .rst­surface re.ection, which allows us to reconstruct high-resolution facial geometry. The 
four difference images of the face s specular re.ection under the gradient illumination patterns (Fig. 
4) let us derive a high­resolution normal map for the face: a map of its local surface orientation vector 
at each pixel. If we examine the intensity of one pixel across this four-image sequence, its brightness 
in the X, Y, and Z images divided by its brightness in the fully-illuminated image uniquely encodes the 
direction of the light stage re.ected in that pixel. From the simple formula in [Ma et al. 2007] involving 
image ratios, we can derive the re.ection vector at each pixel, and from the camera orientations (calibrated 
with the technique of [Zhang 2000]) we also know the view vector. Figure 6: The specular normal map 
for one of Emily s expressions derived from the four specular re.ection images under the gradient illumination 
patterns. Computing the vector halfway between the re.ection vector and the view vector yields a surface 
normal estimate for the face based on the specular re.ection. Fig. 6 shows the face s normal map visualized 
using the common color map where red, green and blue indicate the X, Y, and Z components of the surface 
normal. The normal map contains detail at the level of skin pores and .ne wrin­kles. The point-lit polarization 
difference image (Fig. 4, far right) provides a visual indication of the BRDF s specular lobe shape on 
the nose, forehead, cheeks, and lips. This image provides visual reference for choosing specular roughness 
parameters for the skin shaders. The image can also be used to drive a data-driven specular re.ectance 
model as in [Ghosh et al. 2008].  4.2 Deriving High-Resolution 3D Geometry The last set of images in 
the scanning process (Fig. 3, bottom row) are a set of colored stripe patterns from a video projector 
which allow a stereo correspondence algorithm to robustly compute pixel correspondences between the left 
and right viewpoints of the face. The projector is also cross-polarized so that the stereo pair of images 
consist of only subsurface re.ection and lack specular highlights; such highlights would shift in position 
between the two viewpoints and thus complicate the stereo correspondence process. The patterns form a 
series of color-ramp stripes of different fre­quencies so that a given facial pixel receives a unique 
set of RGB irradiance values over the course of the sequence [Ma 2008]. The .rst projected image, showing 
full-on illumination, is used to di­vide out the cross-polarized facial BRDF from the remaining stripe 
images; this also helps ensure that pixels in one camera have very nearly the same pixel values as in 
the other camera, facilitating correspondence. From these correspondences and the camera cali­bration 
[Zhang 2000], we can triangulate a three-dimensional mesh of the face with vertices at each pixel, applying 
bilateral mesh de­noising [Fleishman et al. 2003] to produce a smooth mesh without adding blur to geometric 
features. However, the surface resolu­tion observable from the diffuse re.ection of skin is limited by 
the scattering of the incident light beneath the skin. As a result, the geometry appears relatively smooth 
and lacks the skin texture detail that we wish to capture in our scans. We add in the skin texture detail 
by embossing the specular normal map onto the 3D mesh. By doing this, a high-resolution version of the 
mesh is created and the vertices of each triangle are allowed to move forward and back until they best 
exhibit the same surface normals as the normal map. The ICT Graphics Lab .rst described this process 
 using diffuse normals estimated in Light Stage 2 in [ICT-Graphics-Laboratory 2001] 2001; more recent 
work in the area includes Nehab et al. [2005].) This creates a notably high­resolution 3D scan, showing 
different skin texture detail clearly observable in different areas of the face (Fig. 7).  4.3 Scanning 
a Multitude of Expressions Emily was captured in thirty-three different facial expressions based loosely 
on Paul Ekman s Facial Action Coding System (FACS) [Ekman and Friesen 1978] as seen in Fig. 8; fourteen 
of these individual scans are shown in Fig. 9. By design, there is a great deal of variety in the shape 
of her skin and the pose of her lips, eyes, and jaw across the scans. Emily was fortunately very good 
at staying still for all of the expressions. Two of the expressions, one with eyes closed and one with 
eyes open, were also scanned from the sides with Emily s face rotated to the left and right as seen in 
the inset .gures of the neutral-mouth-closed and neutral­mouth-open scans. This allowed us to merge together 
a 3D model of the face with geometry stopping just short of her ears to create the complete "master mesh" 
(Fig. 13(a)) and to extrapolate full ear­to-ear geometry for each partial (frontal-only) facial scan 
as in Fig. 16(c).  Figure 8: The thirty-three facial expressions scanned for creating the Digital Emily 
character. We note that building a digital actor from 3D scans of multiple fa­cial expressions is a commonly 
practiced technique; for example, the Animatable Facial Re.ectance Fields project [Hawkins et al. 2004] 
followed this approach in scanning actress Jessica Vallot in approximately forty facial expressions. 
Going further back, visual effects company Industrial Light + Magic acquired several Cyber­ware 3D scans 
of actress Mary Elizabeth Mastrantonio in different expressions to animate the face of the water creature 
in 1989 s The Abyss.  4.4 Phenomena Observed in the Facial Scans The fourteen faces in Fig. 9 show a 
sampling of the high-resolution scans taken of Emily in different facial expressions, and offer an opportunity 
to observe three-dimensional facial dynamics in more detail than has been previously easy to do. A great 
deal of dy­namic behavior can be observed as a face moves, exempli.ed in the detailed images in Fig. 
10. For example, the skin pore detail on the cheek in Fig. 10(a) changes dramatically when Emily pulls 
her mouth the side in Fig. 10(b): the pores signi.cantly elongate and become shallower. When Emily stretches 
her face vertically, small veins pop out from her eyelid Fig. 10(c). (a) (b) (c) (d) (e) (f)   When 
Emily raises her eyebrows, the relatively isotropic skin pores of her forehead transform into rows of 
.ne wrinkles in Fig. 10(d) her skin is too elastic to develop deep furrows. When she scowls in Fig. 
10(e), indentations above her eyebrows appear where her muscles attach beneath the skin. And when she 
winces in Fig. 10(f), the muscles in her forehead bulge out. Examining the progression through Figs. 
10(d,e,f), we see how the bridge of Emily s nose signi.cantly shrinks and expands as the rest of the 
face pushes and pulls tissue into and out of the area. While we may not consciously notice these kinds 
of phenomena when interacting with others, they are present and visible in all faces, and failing to 
reproduce them accurately imperils the realism of a digital character. (a) (b) Figure 7: (a) Facial 
geometry obtained from the diffuse (or subsurface) component of the re.ection. (b) Far more detailed 
facial geometry obtained by embossing the specular normal map onto the diffuse geometry. Figure 9: High-resolution 
3D geometry from fourteen of the thirty-three facial scans. Each mesh is accurate to 0.1mm resolution 
and contains approximately three million polygons. 4.5 Scanning Emily s Teeth Finally, we also scanned 
a plaster cast of Emily s teeth, which re­quired adapting the 3D scanning system to work with greater 
ac­curacy in a smaller scanning volume. Fig. 11(a) shows the cast and Fig 11(b) shows a rendering of 
Emily s digital teeth model; the upper and lower teeth each were the result of merging eight sinusoid-pattern 
structured light scans to form meshes with approx­imately 600,000 polygons.  (a) (b) (c)  5 Building 
the digital character from the scans (a) (b) 5.1 Constructing the Animatable Base Mesh The stitched 
ear-to-ear neutral-expression scan of Emily was remeshed to create a 4,000 polygon animatable mesh as 
seen in Figure 12(a). This drastic polygon reduction from the several mil­lion polygons of the original 
scan was done to make animation of the mesh tractable and to ease corresponding the geometry across scans; 
the geometric skin texture detail would be added back us­ing displacement maps calculated from the high-resolutions 
scans. This was done principally using the commercial product ZBrush to create the facial topology and 
then Autodesk s Maya package to create the interior of the mouth and the eye sockets. Then, UV texture 
coordinates for the neutral animatable mesh were mapped out in Maya, yielding the complete master mesh. 
 8 5.2 Building the Blendshapes Image Metrics originally planned to use the scans captured in the Light 
Stage as artistic reference for building the blendshapes. How­ever, the scans proved to be much more 
useful than just reference. The tiny stabilization dots drawn on Emily s face during the scan­ning session 
were visible in every texture map of every scan. Rather than sculpt the blendshape meshes artistically 
and then project (or "snap") the vertices to the corresponding scans, we used the dots directly to warp 
the neutral animatable mesh into the different ex­pressions. This achieved not only an accurate shape 
for each blend­shape, but also accurate skin movement between blendshapes. The challenge in constructing 
blendshapes from these data was to use the sparse set of stabilization dots to .nd a dense correspon­dence 
between the animatable master mesh and each expression scan. Figure 13 shows an example of three such 
partial frontal scans alongside the animatable master mesh.  (a) (b) (c) (d) Figure 13: (a) Ear-to-ear 
master mesh with (b,c,d) three partial expression scans to be used as blend shapes. Although highly detailed 
and accurate, the expression scans re­quired some pre-processing before appropriate blendshapes could 
be constructed. For example, many meshes have irregular edges with poor triangulation and mesh artifacts 
around the teeth and eye regions. Also, many of the scans contained surface regions not rep­resented 
in the master mesh, such as the surface of the eyes and teeth. These regions should not be corresponded 
with master mesh vertices. Finally, the scans, captured from a frontal stereo camera pair, did not cover 
the full facial region of the animatable master mesh. Figures 14 and 15 show examples of each of these 
issues. These aspects of the data meant that a fully automatic correspon­dence algorithm would be unlikely 
to provide adequate results. The correspondence method we used required some simple man­ual data cleaning 
and annotation to produce data more amenable to automatic processing. For each expression scan, we removed 
regions from both the master mesh and the expression mesh to achieve rough edge consistency. For example, 
we removed the neck region from the expression scans and face sides from the master to produce consistent 
facial coverage. We also removed uncorre­spondable regions from the meshes by deleting vertices; these 
were commonly the teeth and eyeballs regions of the expression scans, as such regions are not present 
in the master mesh. This process also removed most of the mesh artifacts associated with discontinu­ities 
in the expression scans. The 3D locations of the stabilization dots were annotated manually and served 
as a sparse set of known points of correspondence between master mesh and each expression scan. For each 
expression, this sparse correspondence information was used to initialize and stabilize a proprietary 
automatic method of determining a dense correspondence. The method used a 3D spatial location and mesh 
normal agreement measure within a 2D conformal mapping frame (such as in [Wang et al. 2008]) to obtain 
the required dense correspondence. This process resulted in a map­ping of each master mesh vertex to 
a position on the surface of the appropriate expression mesh.  Figure 14: Mesh artifacts and irregular 
edges in the raw Emily scans With the correspondence between our partial, cleaned-up meshes it was possible 
to calculate the shape change required to transform the neutral master mesh to each expression. However, 
a rigid trans­lation/rotation between the master and expression scans was also generally present in the 
data. This rigid transformation must be cal­culated to ensure only motion due to the changing facial 
expression and not residual head motion is represented in each blendshape. To do this, a 3D rigid transformation 
for each expression was cal­culated using a manually selected subset of texture markers. The texture 
markers selected were chosen independently for each ex­pression so as to be the least affected by the 
skin and jaw motion for that expression, and thus provide a good basis for calculating a rigid transformation 
between scans. Once the rigid transformations were calculated and was factored out of each blendshape, 
blendshape deltas for each vertex in the partial master mesh were calculated. This produced partial blendshapes 
as shown in Figure 16(a,b). Any jaw movement had to be subtracted from each blendshape; this step was 
a very important because it eliminated redundancy in the rig. One example was the lipSuck shape where 
both top and bottom lips are sucked inside the mouth. In order to capture the maximum amount of data 
for the lips, this expression was scanned and modeled with the jaw slightly open. After the blendshape 
was (a) (b) (c) Figure 16: (a) A partial master mesh with (b) a partial blendshape expression mesh. 
(c) A complete expression scan, created by .nding correspondences from (b) to (a) to the complete master 
mesh. modeled the jaw movement was subtracted by going negative with the jawOpen shape. A custom algorithm 
was used to map the partial blendshapes onto the full master mesh. The missing parts of the full master 
mesh that were not observed in the expression scan were interpolated using the assumption that the extreme 
edges of the face remained static, which we found to be a reasonable assumption for most expres­sions. 
Where this assumption was not appropriate, the results were artistically corrected to give the desired 
shape. The unknown inter­nal parts of the master mesh, such as the inner mouth and lips, were interpolated 
to move appropriately with the known vertices. Figure 16(c) illustrates the result of this interpolation 
process. The .nal results of the automatic blendshape creation process were then artistically cleaned 
up by a small team of rigging artists to provide a full set of quality-assured expression blend shapes. 
After the blendshape modeling and cleanup process, Image Metrics ended up with approximately 30 "pre-split" 
blendshapes, one blend­shape corresponding to each scan. However, most of the scans were captured with 
"doubled up" facial expressions. For example, the browRaise and chinRaise were captured in the same scan. 
This was done to get through the scanning session quicker and to save processing time. But now the blendshapes 
had to be split up into localized shapes the "post-split" shape set. To do this, we used the Paint Blendshape 
Weights feature in Maya, creating a set of normal­ized "split maps" for each facial region. For example, 
this is how the browRaise blendshape was split up. Three normalized split maps were painted for the browRaise 
blendshape: browRaise_L, browRaise_C, and browRaise_R, roughly following the eyebrow raisers described 
in [Ekman and Friesen 1978]. A custom MEL script was then run which applied each split map to the browRaise 
shape in the Paint Blendshape Weights tool. The left, center, and right browRaise blendshapes were then 
duplicated out. Splitting up the browRaise shapes in this way allowed the animator to con­trol every 
region of the brows independently. However, because the split maps were normalized, turning on all three 
regions together summed to the original pre-split browRaise shape. All the pre­split blendshapes underwent 
this process resulting in a post-split shape set of roughly 75 blendshapes. This shape set gave the Image 
Metrics animators an unprecedented amount of control over each region of the face.  5.3 The Facial Rig 
s User Interface The rig user interface (Fig. 17) was inspired by the facial muscles and their "direction 
of pull". Many of the controls were represented as arrow-shaped NURBS curves, with each arrow representing 
a certain facial muscle. Pulling on the arrows toward its point was equivalent to contracting those muscles. 
Additional animation con­trols were placed in the Maya channel box allowing numerical in­put.   5.4 
Soft Eyes and Sticky Lips The Emily facial rig included two notable special effects: soft eyes and sticky 
lips. Soft eyes is the effect of the rotation of the cornea pushing and tugging the eyelids and skin 
around the eye. The soft eyes setup created for Emily was relatively simple in that each eye had a separate 
blendshape node with four shapes: lookUp, lookDown, lookLeft, and lookRight. The envelope of this blendshape 
node was negated by the blink shape. In other words, whenever the eye blinked, the soft eyes effect was 
turned off. This was done to prevent a con.ict between the blink and lookDown blendshapes. Sticky lips 
is the subtle effect of the lips peeling apart during speech, and has been a part of several high-end 
facial rigs including the Gollum character built at WETA Digital for the Lord of the Rings sequels. The 
Emily rig had a relatively elaborate sticky lips setup involving a series of Maya deformers which provided 
anima­tors a "sticky" control for both corners of the lips.  5.5 Adding Blendshape Displacement Maps 
In order to preserve as much of the scan data as possible, the Emily render had 30 animated displacement 
maps. The displacement maps were extracted using Pixologic s ZBrush software by plac­ing each pre-split 
blendshape on top of its corresponding scan and calculating the difference between the two. Then each 
displace­ment map was cleaned up in Photoshop and divided according to the same normalized split maps 
used to split the blendshapes. This yielded a displacement map for each blendshape in the rig (although 
only a subset of these were used in the .nal renderings). It is im­portant to note that the highest frequency, 
pore-level detail displace­ment map came only from the neutral scan. All the other displace­ment maps 
had a median .lter applied to them to remove any high frequency detail, while still keeping the wrinkles. 
This was done because adding two maps with high frequency detail would result in the pores "doubling 
up" in the render, since the high-resolutions scans had not been aligned to each other at the level of 
skin pores and .ne wrinkles. Therefore, only the neutral displacement map had the pore detail; all the 
other maps had only wrinkles without pores. The displacement map animation was driven directly by the 
corresponding blendshape animation. 10 5.6 Adding the Teeth Plaster casts of Emily s upper and lower 
teeth were made using standard dental casting techniques. The two casts were scanned with a structured 
light scanning system to produce two 600,000 polygon meshes from sixteen merged scans acquired using 
struc­tured lighting patterns based on Gray codes. These teeth scans were manually remeshed to have far 
fewer triangles and smoother topol­ogy. The top and bottom teeth meshes were remeshed to 10,000 polygons 
each as in Fig. 11(c). We then carefully placed the teeth geometry inside the neutral mesh using the 
high-resolution smile scans as reference for the teeth positions. The teeth scanning was done relatively 
late in the facial model con­struction process. Until the teeth were scanned, a generic teeth model was 
used in the animated character. We found this to be signi.cantly less believable than using the model 
of Emily s actual teeth.  6 Video-Based Facial Animation Digital Emily s facial animation was created 
using Image Metrics proprietary video analysis and animation system which allows an­imators to associate 
character poses with a small subset of perfor­mance frames. The analysis of the performance requires 
video from a single standard video camera and is designed to capture all the characteristics of the actor. 
The animation technology then uses the example poses provided by an animator to generate predictions 
of the required character pose for each frame of the performance. The animator can then iteratively re.ne 
this prediction by adding more example poses until the desired animation is achieved. As the process 
is example driven, it presents a natural framework to allow artistic and stylistic interpretation of 
an actor s performance, for example when using a human to drive a cartoon or animal charac­ter. However, 
in this case the technology was used for the purpose of producing a faithful one-to-one reproduction 
of Emily s perfor­mance. The Image Metrics facial animation process has several advantages over traditional 
performance-driven animation techniques which signi.cantly added to the realism of the digital character. 
First, the process is based on video of the actor performing, which provides a great deal of information 
regarding the motion of the actor s eyes and mouth; these are problematic areas to capture faithfully 
with traditional motion capture markers, but are the most important part of the face for communicating 
an actor s performance. Second, the process leverages an appropriate division of labor between the ani­mator 
and the automated algorithms. Because an artist is part of the process, they can ensure that each of 
the key animation poses reads in an emotionally faithful way to the appearance of the actor in the corresponding 
frame of video. This process would be dif.cult to automate, since it involves reading and comparing the 
emotional content of faces with signi.cant detail. Conversely, the trajectories and timing of the facial 
motion between key poses are successfully derived from the automated video analysis; achieving realistic 
tim­ing in photoreal facial animation requires a great deal of skill and time for an animator to achieve 
manually. As a result, the process combines what an animator can do quickly and well with what an automatic 
process can accomplish, yielding facial animation with higher quality and greater ef.ciency than either 
fully manual or fully automatic techniques currently allow.   7 Tracking, Lighting, Rendering, and 
Com­positing Emily s performance was shot from two angles using high­de.nition cameras: a front-on closeup 
for use in the Image Metrics analysis pipeline, and a medium shot in a three-quarter view to provide 
the background plate for the .nal video. The .nal renderings needed to be match-moved to Emily s head 
position in the three-quarter shot. Complicating the process (but representative of many practical production 
scenarios) the cameras were uncalibrated, and there were no markers on Emily s face to assist in tracking. 
Match moving requires sub-pixel accuracy and temporal consistency of pose, otherwise unwanted ".oating" 
effects become easily visible. To achieve this, we developed a manually guided match-moving process. 
We manually set the 3D pose of the animated character on several example frames so that the projected 
location of the model closely matched that of Emily in the video frame. Figure 18 shows the Maya scene 
used to allow 3D pose annotation. Using these frames as templates, we applied a model-based optical .ow 
algorithm [Baker and Matthews 2004] to calculate the required character pose in the intermediate frames. 
To ensure a smooth temporal path within the pose space, we developed a novel weighted combination of 
template frames that smoothly favors the temporally closest set of templates. Any errors were manually 
corrected and added as a new template to derive a new pose-space tracking result until the results no 
longer exhibited artifacts. Since Emily s face would be composited onto live-action back­ground plate 
including real video of her hair, ears, neck, and body, it was imperative for the rendered version of 
Emily s face to look completely convincing. Re.ning the rendering parameters took a rendering artist 
approximately three months to perfect. The render­ing was done in Mental Ray using the Fast Subsurface 
Scattering skin shader. The lighting was based on an high dynamic range light probe image captured on 
the day of the shoot [Debevec 1998] as seen in Figure 19. Many different passes were rendered, includ­ing 
diffuse, specular, matte, and contour passes. In particular, the contour pass (Fig. 20) was very important 
for visually integrating (or "marrying") the different components of facial geometry using different 
amounts of blur in the composite. Otherwise, for example, the line where the eyelid meets the eyeball 
would appear too sharp and thus unrealistic. Compositing was done in the Eyeon Fusion package. Emily 
s .ngers were rotoscoped whenever she moved them in front of her face so they could also obscure her 
digital face. Small paint .xes were done to the .nal renderings around the eye highlights. Two frames 
from a .nal Emily animation are shown in Fig. 21.  8 Reception In conjunction with its premiere at 
the Image Metrics booth in the SIGGRAPH 2008 exhibit hall at the Los Angeles Convention Center, the Digital 
Emily project received widespread attention in the media, most notably by over one million viewers of 
the Digital Emily video on the popular video-sharing web site YouTube. Other notable media coverage included 
The Wall Street Journal, New Scientist, Spiegel, Studio Daily, FX Guide TV, TG Daily, Mahalo Daily, the 
Discovery Channel, Lab TV, and an upcoming Frontline special report by David Kushner. Several reports 
by industry pro­fessionals credited the project with having convincingly crossed the "uncanny valley" 
and being the .rst project to do so: "It is absolutely awesome amazing. I m one of the toughest critics 
of face capture, and even I have to admit, these guys have nailed it. This is the .rst virtual human 
animated sequence that completely bypasses all my subconscious warnings. I get the feeling of Emily as 
a person. All the subtlety is there. This is no hype job, it s the real thing ... I of.cially pronounce 
that Image Metrics has .nally built a bridge across the Uncanny Valley and brought us to the other side." 
 Peter Plantec, " The Digital Eye: Image Metrics Attempts to Leap the Uncanny Valley", VFXWorld, August 
7, 2008 [Plantec 2008] "Emily is a truly monumental achievement, recreating every nu­ance of human facial 
expression, even though what you re actually looking at is the face of a digital actor. Created [by Image 
Metrics] through a partnership with USC s Institute for Creative Technolo­gies (ICT), the team s primary 
objective was to create a completely  convincing, animated computer-generated face, and I think they 
succeeded." Paul Strauss, Technabob.com, August 20, 2008 "When I .rst saw the demo for Project Emily, 
I thought it was just me there on video capture. But then they started removing my skin, it was creepy, 
but I realized that was the Digital Me! I was really amazed." Emily O Brien, Actress 9 Discussion Some 
timeframes and personnel for the Digital Emily Project were: Scanning: one afternoon, 1.5 hours with 
actor in stage, 3 sec­onds per scan, 3 technicians  Scan Processing: 10 days, 37 processed scans, 1 
artist  Rig Construction: 3 months, 75 blendshapes, 1 artist  Animation: 2 weeks, 90 seconds of animation, 
2 animators  Rendering/Compositing: 3 months, 1 artist  Output: 24fps, 1920x1080 pixel resolution 
 A great deal of information was learned and many tools were de­veloped and improved in the context of 
this project, so to apply the process again would likely require signi.cantly fewer resources to achieve 
results of equal or even better quality. Based on the experiences of the Digital Emily project, .ve suf.cient 
(and perhaps necessary) steps for achieving a photoreal digital actor are: 1. Suf.cient facial scanning 
resolution accurate to the level of skin pores and .ne wrinkles, which for Digital Emily was achieved 
with the Light Stage scanning process of [Ma et al. 2007] 2. 3D geometry and appearance data from a 
wide range of facial expressions, also achieved through [Ma et al. 2007] 3. Realistic facial animation, 
such as that which can be captured from a real actor s performance, including detailed motion of the 
eyes and mouth, which was achieved through the semi­automatic Image Metrics video-based facial animation 
process 4. Realistic skin re.ectance including the translucency of the skin, which for digital Emily 
leveraged a subsurface scattering tech­nique based on [Jensen et al. 2001] 5. Accurate lighting integration 
with the actor s environment, which for Digital Emily was done using HDRI capture and image-based lighting 
as described in [Debevec 1998]  9.1 Lessons Learned The experience of creating Digital Emily taught 
us numerous lessons which will be useful in the creation of future digital charac­ters. These lessons 
included: Having consistent surface (u,v) coordinates across the scans which are accurate to skin pore 
level detail would be of sig­ni.cant use in the process; a great deal of the effort involved was forming 
correspondences between the scans.  Although the Image Metrics facial animation system requires no facial 
markers for animation, including at least a few facial markers as the actor performs would make the head 
tracking process much easier.  Giving the digital character their own teeth, accurately scanned and 
placed within the head and jaw, is important for the be­lievability of the character and their resemblance 
to the original person. 10 Real-Time Emily A follow-up effort to the Digital Emily demo at SIGGRAPH 
2008 was "Real-time Emily", an effort to render realistic facial anima­tion and re.ectance in real time 
using a GPU, done in early 2009 for the Consumer Electronics Show (CES) in Las Vegas. The real­time demo 
shows an animation clip of Digital Emily giving a short speech and exhibiting different expressions and 
emotions (Fig. 22). Users can view this animation from any angle and lit by a point light source from 
any direction. The facial deformation model in real-time Emily consists of three parts. The underlying 
geometry is interpolated according to blend­shapes between the various smoothed expression scans represent­ing 
the low frequency characteristics of the facial deformation. Medium-frequency deformation is represented 
by blending dis­placement maps derived from the expression scans. For each frame, an array of blending 
coef.cients is sent to the fragment shader to synthesize a unique displacement map per frame. For the 
top layer we use a single set of hybrid normal maps [Ma et al. 2007] from the neutral scan to give the 
appearance of subsurface scattering, self-occlusion, and bounce light effects within an ef.cient local 
shading model. These various normal maps (red, green, blue, and specular) are stored in tangent space 
and transformed back to world space according to the median frequency displacements and low frequency 
deformed geometry in the fragment shader. The real-time Emily demo is relatively realistic across most 
of the face, and generally reproduces the motion and shading cues seen in the of.ine animation. The demo, 
however, is less realistic around the mouth and eyes. The reason is that detailed dynamic shading ef­fects 
around the eyelids and teeth are not calculated with suf.cient accuracy to appear realistic, and the 
mouth and eyes are the areas of the face which receive the most scrutiny. Creating real-time eyelid, 
eyeball, teeth, and mouth lighting interactions remains an important avenue for future work, as does 
illuminating real-time facial models with environmental (as opposed to point-source) illumination.  
 Acknowledgements The authors wish to thank Steve Caulkin and Kevin Walker for their contributions to 
the Digital Emily project as well as Cyrus Wilson and Graham Fyffe for helping review this report. The 
complete project credits for the Digital Emily project are, for Image Met­rics: Oleg Alexander (Director/Rigger), 
David Barton (Producer), William Lambeth (Render Artist), Cesar Bravo and Matt Onheiber (Facial Animation), 
Emily O Brien (Actress), Christopher Jones (Co-Producer), Peter Busch (Production Manager), Bryan Burger 
(Performance Analysis), Mike Rogers (Computer Vision Engineer), Edwin Gamez (Production Assistant), Steven 
McClellan (Match­mover), Tom Tran (Additional Modeling), Eric Schumacher (Mar­keting), Shannon McPhee 
(Public Relations), Justin Talley (Direc­tor of Photography), Sean Kennedy (Dental Cast), Con.dence Head 
(Music), and for USC Institute for Creative Technologies, Paul Debevec (Senior Supervisor), Matt Chiang 
(Research Program­mer), Alex Ma (Research Programmer), Tim Hawkins (Supervis­ing Researcher), Tom Pereira 
(Producer), Andrew Jones (3D Scan­ning), Abhijeet Ghosh (Re.ectance Modeling), Jay Busch (Techni­cal 
Artist). The authors additionally wish to thank Joe Letteri, Se­bastian Sylwan, Bill Swartout, Cheryl 
Birch, Randy Hill, Randolph Hall, Patrick Davenport, and Mike Starkenburg for their support of this project. 
Portions of this work were sponsored by the Uni­versity of Southern California Of.ce of the Provost and 
the U.S. Army Research, Development, and Engineering Command (RDE-COM). The content of the information 
does not necessarily re.ect the position or the policy of the US Government, and no of.cial endorsement 
should be inferred.  References BAKER, S., AND MATTHEWS, I. 2004. Lucas-kanade 20 years on: A unifying 
framework. Int. J. Comput. Vision 56, 3, 221 255. BORSHUKOV, G., AND LEWIS, J. P. 2003. Realistic human 
face rendering for The Matrix Reloaded . In ACM SIGGRAPH 2003 Sketches &#38; Applications. CHANG. 2007. 
Beowulf. Variety (Nov). DEBEVEC, P., HAWKINS, T., TCHOU, C., DUIKER, H.-P., SAROKIN, W., AND SAGAR, M. 
2000. Acquiring the re­.ectance .eld of a human face. In Proceedings of ACM SIG-GRAPH 2000, Computer 
Graphics Proceedings, Annual Confer­ence Series, 145 156. DEBEVEC, P., TCHOU, C., GARDNER, A., HAWKINS, 
T., POULLIS, C., STUMPFEL, J., JONES, A., YUN, N., EINARS-SON, P., LUNDGREN, T., FAJARDO, M., AND MAR-TINEZ, 
P. 2004. Estimating surface re.ectance proper­ties of a complex scene under captured natural illumination. 
Tech. Rep. ICT-TR-06.2004, USC ICT, Marina del Rey, CA, USA, Jun. http://gl.ict.usc.edu/Research/re.ectance/Parth-ICT­TR-06.2004.pdf. 
DEBEVEC, P. 1998. Rendering synthetic objects into real scenes: Bridging traditional and image-based 
graphics with global il­lumination and high dynamic range photography. In Proceed­ings of SIGGRAPH 98, 
Computer Graphics Proceedings, Annual Conference Series, 189 198. EKMAN, P., AND FRIESEN, W. 1978. Facial 
Action Coding Sys­tem: A Technique for the Measurement of Facial Movement. Consulting Psychologists Press, 
Palo Alto. FLEISHMAN, S., DRORI, I., AND COHEN-OR, D. 2003. Bilateral mesh denoising. ACM Transactions 
on Graphics 22, 3 (July), 950 953. GHOSH, A., HAWKINS, T., PEERS, P., FREDERIKSEN, S., AND DEBEVEC, P. 
2008. Practical modeling and acquisition of lay­ered facial re.ectance. ACM Transactions on Graphics 
27,5 (Dec.), 139:1 139:10. GUENTER, B., GRIMM, C., WOOD, D., MALVAR, H., AND PIGHIN, F. 1998. Making 
faces. In Proceedings of SIGGRAPH 98, Computer Graphics Proceedings, Annual Conference Series, 55 66. 
HAWKINS, T., WENGER, A., TCHOU, C., GARDNER, A., GÖRANSSON, F., AND DEBEVEC, P. 2004. Animatable facial 
re.ectance .elds. In Rendering Techniques 2004: 15th Euro­graphics Workshop on Rendering, 309 320. HYNEMAN, 
W., ITOKAZU, H., WILLIAMS, L., AND ZHAO, X. 2005. Human face project. In ACM SIGGRAPH 2005 Course #9: 
Digital Face Cloning, ACM, New York, NY, USA. ICT-GRAPHICS-LABORATORY, 2001. Realistic hu­ man face 
scanning and rendering. Web site. http://gl.ict.usc.edu/Research/facescan/. JENSEN, H. W., MARSCHNER, 
S. R., LEVOY, M., AND HANRA-HAN, P. 2001. A practical model for subsurface light transport. In Proceedings 
of ACM SIGGRAPH 2001, Computer Graphics Proceedings, Annual Conference Series, 511 518. KAUFMAN, D. 1999. 
Photo genesis. WIRED 7, 7 (July). http://www.wired.com/wired/archive/7.07/jester.html. LANDIS, H. 2002. 
Production-ready global illumination. In Notes for ACM SIGGRAPH 2005 Course #16: RenderMan in Produc­tion, 
ACM, New York, NY, USA. MA, W.-C., HAWKINS, T., PEERS, P., CHABERT, C.-F., WEISS, M., AND DEBEVEC, P. 
2007. Rapid acquisition of specular and diffuse normal maps from polarized spherical gradient illumina­tion. 
In Rendering Techniques, 183 194. MA, W.-C., JONES, A., CHIANG, J.-Y., HAWKINS, T., FRED-ERIKSEN, S., 
PEERS, P., VUKOVIC, M., OUHYOUNG, M., AND DEBEVEC, P. 2008. Facial performance synthesis using deformation-driven 
polynomial displacement maps. ACM Trans­actions on Graphics 27, 5 (Dec.), 121:1 121:10. MA, W.-C. 2008. 
A Framework for Capture and Synthesis of High Resolution Facial Geometry and Performance. PhD thesis, 
Na­tional Taiwan University. MORI, M. 1970. Bukimi no tani (the uncanny valley). Energy 7, 4, 33 35. 
NEHAB, D., RUSINKIEWICZ, S., DAVIS, J., AND RAMAMOOR-THI, R. 2005. Ef.ciently combining positions and 
normals for precise 3d geometry. ACM Transactions on Graphics 24,3 (Aug.), 536 543. PERLMAN, S. 2006. 
Volumetric cinematography: The world no longer .at. Mova White Paper (Oct). PLANTEC, P. 2008. The digital 
eye: Image metrics attempts to leap the uncanny valley. VFXWorld magazine (August). http://www.vfxworld.com/?atype=articles&#38;id=3723. 
ROBERTSON, B. 2009. What s old is new again. Computer Graph­ics World 32, 1 (Jan). 14 SAGAR, M., MONOS, 
J., SCHMIDT, J., ZIEGLER, D., FOO, S.-C., SCOTT, R., STERN, J., WAEGNER, C., NOFZ, P., HAWKINS, T., AND 
DEBEVEC, P. 2004. Re.ectance .eld ren­dering of human faces for ¨. spider-man 2¨In SIGGRAPH 04: ACM SIGGRAPH 
2004 Technical Sketches, ACM, New York, NY, USA. WANG, Y., GUPTA, M., ZHANG, S., WANG, S., GU, X., SAMA-RAS, 
D., AND HUANG, P. 2008. High resolution tracking of non-rigid motion of densely sampled 3d data using 
harmonic maps. Int. J. Comput. Vision 76, 3, 283 300. WENGER, A., GARDNER, A., TCHOU, C., UNGER, J., 
HAWKINS, T., AND DEBEVEC, P. 2005. Performance relighting and re.ectance transformation with time-multiplexed 
illumina­tion. ACM Transactions on Graphics 24, 3 (Aug.), 756 764. WOLFF, E. 2003. Creating virtual performers: 
Disney s human face project. Millimeter magazine (April). ZHANG, Z. 2000. A .exible new technique for 
camera calibration. IEEE Trans. Pattern Anal. Mach. Intell. 22, 11, 1330 1334.  
			