
 The Feature Quantity: An Information Theoretic Perspective of Tfidf-like Measures Akiko AIZAWA National 
Institute of Informatics 2-1-2 Hitotsubashi Chiy0da-ku, Tokyo 101-8430, Japan E-Mail: akiko@nii.ac.jp 
 Abstract The feature quantzty, a quantitative representation of speci- ficity introduced in this paper, 
is based on an informa- tion theoretic perspective of co-occurrence events between terms and documents. 
Mathematically, the feature quan- tity is defined as a product of probabillty and information, and maintains 
a good correspondence with the tfidf-like measures popularly used in today's IR systems. In this paper, 
we present a formal description of the feature quantity, as well as some illustrative examples of applying 
such a quantity to different types of information retrieval tasks: representative term selection and 
text categoriza- tion. Introduction This paper presents the mathematical definition and ap- plications 
of the feature quantity, a measure of specificity of terms or documents in a given document set. To in- 
troduce the basic idea, we first revisit the classical, but nevertheless important question of 'what 
is the mathe-matical implication of tfidf?' in an information theoretic framework. First of all, it is 
assumed that a document is given as an unordered set of terms. Let D = {dl "'dN} be a set of documents 
and W = {wl "- wM} be a set of distinct terms contained in D. The parameters N and M are the total numbers 
of documents and terms, respectively. In our adaptation of a probabilistic view, we also use the notion 
of d~ for an event of selecting a document from D. Similarly, wl is used for an event of selecting a 
term from W. Now, let 79 and 1N be random variables defined over the events { d~ . . . dN } and {Wl . 
. . WM }, respectively. Our objective here is to calculate the expected mutual information between 79 
and W (Figure 1). Assuming all the documents are equally likely at the initial stage, P(dj) = 1/N for 
all d/ E D. Then, the amount of information calculated for each document is identically given by -log(I/N). 
It follows that the self entropy of random variable ~D, which is defined as the expected amount of information, 
is: ~(g~) =-~ P(ds)l°gP(ds) d~ ED Permission to make digital or hard copies of all or part of this work 
for personal or classroom use is granted without fee provided that copies are not made or distributed 
for profit or commercial advan-tage and that copies bear this notice end the full citation on the 'first 
page To copy otherwise, to repubhsh, to post on servers or to redistribute to lists, requires pnor specific 
permission and/or a fee SIGIR 2000 7/00 Athens, Greece &#38;#169; 2000 ACM 1-58113-226-310010007.-$5.00 
./probability / documents"-, probability i ! distribution I"I ~ %"\ distribution I I of query I" that 
represents i terms 'I co-occurrences of \ the system's i submitted to i documents and terms ! knowledge 
on the . the system ( ~ ;} stored documents iI W! ..... WM / "X. terms ./ ~,kdl .... d N I @ @i P(w 
i, dj) P(djl wi)P(wi) = e i I irst. a user I I ben, the I i I submits a query I / I system selects I 
i term to the q a document [ i % I system. I 2. I using the term. I ; k'.. ' "J ' .5" Figure 1: An illustrative 
situation assumed in the calcu- lation of the expected mutual information.  1 1 _tog1. = -N" ~log~ = 
(1) Next, consider a situation where a subset of specified doc- uments that contain wl (E W) are known. 
Let Ni he the number of documents in the subset. Assuming that the N, documents are equally likely, the 
amount of in- formation calculated for each document in the subset is -log(1/Ni). In this case, the self 
entropy of ~D given wl becomes: 7~(vlw,)=-~ P(d~lw~)loaP(d~lw~) d a 6D =-]v,.~Zog--=-log . (2) Since 
documents without wl occur with probability zero, there is no contribution from these documents, i.e., 
the factor (N -Ni) does not appear in the above equation. Now, let us assume that a term wl is randomly 
picked from the whole document set. Denoting the frequency of wl within d~ as f~j, the frequency of w, 
in the whole doc- ument set as f~,,, and the total frequency of all terms ap- pearing in the whole document 
set as F, the probability that w, is selected is ~j ~ = ~. Then, the expect- ed information gain of the 
event, also referred to as the posterior entropy or the expected mutual information, is calculated as: 
Z(79; W) = H(~9) H(~IW) - = ~ P(w,) (H(V) -H(~lw,)) ~,6W wsEW N w.EW N w: GW d z ED Equation (3) equals 
the sum of the product of the term frequency (tf), either in the form of flj or f~,, and the inverse 
document frequency (idf) divided by a constant factor F. Hence, we conclude that from an information 
theoretic point of view, tfldf can be interpreted as the quantity needed for the calculation of the expected 
mu- tual information given by Eq. (3). When tf refers to flj, the tfidf values represent weights of terms 
within each document and are summed up for all the combination of terms and documents. When tf refers 
to f~., the tfidf values represent the significance of corresponding terms in a whole document set and 
are summed up for all the words. We should note here that in the derivation of Eq. (3), 1 1 is implic- 
 the condition P(d~) = ~w(d~) " N-': = itly assumed for consistency, where W(d~) is the set of distinct 
terms contained in dj. In our view, the specific assumption itself represents the heuristic that tfidf 
em-ploys. Then, is there a possibility of extending the defi- nition of tfidf into a more general form 
by applying the same information theoretic view? Bearing this question in mind, the principal idea of 
this paper is that given a component of textual data such as a document or a term, the significance of 
the compo- nent is expressed as a product of the probability that it occurs and the amount of information 
it represents, i.e., (feature) = (probability) × (information). Although conventional information theory 
does not ex-plicitly deal with such a quantity (but uses the one in the calculation of entropy since 
entropy is generally defined as the expected amount of information), we have postulat- ed that what the 
current popularity of the tfidf measure tells us is the usefulness of such a quantity as a measure of 
significance. Another important implication of the above formula- tion is that the two probability distributions, 
P(w~) and P(dj]w~) as shown in Figure 1, can be determined inde- pendently. In the figure, P(wi) represents 
the probability distribution of the query terms submitted to the system, while P(d¢]wl) is the conditional 
probability distribution of documents, given the query term. In other words, P(wi) serves as a model 
of the user, and P(d~lw~) as a model of the retrieved documents. Such a formulation not only is closely 
connected to the previous theoretical de- velopment such as [1] [4] but also allows us to extend the 
classical definition of tfidf in more flexible ways, includ- ing the nonlinear scaling of term frequency, 
which is com- monly practised in today's IR systems. Also, by adopting different term distribution models 
for the same document set, we can successfully connect the vector-space oriented view of the original 
tfidf to the probabilistic ones, as is shown in our text categorization application. This paper reports 
some of the preliminary results of our attempt to expand such ideas. The subsequent sections are organized 
as follows. Section 2 presents the mathematical definition of the feature quantity. Section 3 deals with 
the problem of selecting representative terms where the feature quantity is used as a measure for the 
specificity of a term. Section 4 examines the text cate- gorization problem where the feature quantity 
is used to identify the category best characterized by a given set of terms. Section 5 is the conclusion. 
2 Mathematical Formulation 2.1 Notations and Basic Formulae of Informa- tion Theory As before, let D 
and W be a set of documents and of terms, respectively, and ~ and ~'V be random variables corresponding 
to D and W. Assume a joint probability distribution P(wi,dj) is given for dj E D and wl E W. Here, P(wl, 
dj) provides a fundamental view of the prob- lem. Naturally, a number of strategies are available to 
determine P(wi, dj). For example, P(wi, dj) can be deter- mined directly from occurrences of terms in 
documents. In this case, standard techniques in probabilistic language modelling can be applied, including 
the simplest way of assigning probabilities proportional to the observed oc- currences, or more computationally 
intensive ways, such as frequency discounting of n-gram statistics or the max- imum entropy method. It 
is also possible to choose oth- er distributions for P(w~) while still estimating P(djlw~ ) using the 
standard techniques. In this case, P(w~,dj) is uniquely determined by the general probability formula 
P(~0,, d~) = P(dj Iw,)P(w,). Given P(wl, dj), it immediately follows that P(wi) = ~ dz ED P(w,,dj), 
(4) and P(dj) = Z w,EW P(w,,d~). (5) By general definition of information theory, mutual infor- mation 
between wi and dj is given by , P(w~,dj) ~(w,, dj) = ~og ----. (6) P(w~)P(d~) The expected mutual information 
between D and 1,~ is: z(v;w)= (71 w:EW d3 ED . P(w,,d~) = P(w,,d l logp ). wtGW d a GD Z(79; W) can 
be viewed as the entropy of the co-occurrences of documents and terms. Note that by definition, I(~D; 
W) = 77( ; 9) and Eq. (7) maintain duality regarding docu- ments and terms. The information increase 
of T~ after the event of ob- serving wl can be expressed using Kullback-Leibler infor- mation, which 
is a measure of the difference between two probability distributions. Kullback-Leibler information between 
P(791w,) and P(79) is calculated as: P(dj I~) ]C(P(Dlwl)'P(T~)) = Z P(djlw')l°g ~ " (8) d,ED Similarly, 
the information increase of 'W after the event 3 Feature Quantity in Representative Terms S- of observing 
dj is given by Kullback-Leibler information between P(VVIdj) and P(W): IC(PON[dJ)'P(W)) = E P(wi[dj)l°gP(~w'i[d~)" 
(9) Applying P(wi,dj) = P(djlw,)P(wi ) = P(w, ldj)P(dj) to Eqs. (7), (8) and (9), it is straightforward 
that the following relationships hold between the expected mutual information and Kullback-Leibler information: 
Z(D;W) = E P(w')K:(P(VIwl)' P(D)) drEW" = ~ P(dAPC(P(WldD, P(W)). (10) d a ED 2.2 Quantitative Representation 
of Features Quantitative representation of the feature as formulated in this section is defined as the 
contribution of a specif- ic co-occurrence event to the overall entropy calculation given by Eq.(7). 
The feature quantity of the occurrence of wl and dj is defined as: ~(w,,dj) = P(wi,dj).A4(wi,dj). (11) 
Similarly, the feature quantity of the occurrence of w~ is defined as: ..T(w,;V) = V(w,)~(P(Vlw,), P(V)), 
(12) and the feature quantity of the occurrence of dj as: .~'(d~;W) = P(d~)E(P(WIda),P(W)). (13) In all 
cases, the feature quantity is expressed as a prod- uct of probability and information, the latter being 
either mutual information, in Eq. (11), or Kullback-Leibler in- formation, in Eqs. (12) and (13). Equation 
(12) can further be rewritten as: P(dj[w,) ~'(w,;79) = E P(w')P(djlwO logp~) d~ GD E .T'(wl,d~), (14) 
d~ ED and Eq. (13) as: .T(dj; W) = P(wddj)P(d~) log P(w'ldj) P(w,) w,EW E .T(w,, dj). (15) w,EW Then, 
it follows that the entropy of all co-occurrences is simply expressed as the summation of feature quantity 
values of each case: w,EW d3ED w,EW (16) = E .F(dj;],V)-da ED It is also important to note that the above 
definition is applicable, not only to document-to-term co-occurrences, but also to term-to-term, category-to-term, 
or document- to-descriptor co-occurrences. We will partly see in later sections how the definition is 
applied and extended for these different types of co-occurrence data. election 3.1 Definition of tfkll 
Measure Relevant to the representative terms selection problem are: (i) automatic term extraction in 
computational ter- minology, and (il) feature subset selection in machine learning. Approaches from computational 
terminology mainly concern the problem of determining the specifici- ty of a term within a given document 
set, the result of which is used, for example, for information visualization in IR systems. On the other 
hand, approaches from the machine learning side mainly concern the problem of re- ducing the dimension 
of the features of the documents so that succeeding learning algorithms can effectively be applied, sometimes 
avoiding the over-fitting problem. In both cases, terms are characterized either by documents in which 
they occur, or by terms with which they co-occur. Commonly used statistical measures in term extrac- 
tion include term frequency, ~fidf, document frequency, mutual information, log-likelihood ratio, signal-noise 
ra- tio, and Kullback-Leibler information [3] [14]. In machine learning, such statistical measures as 
mutual information, information gain, odds ratio, and expected cross entropy are used [16] [8]. Among 
these, the expected cross entropy [7] has exactly the same definition as our feature quanti- ty. We would 
like to point out here that although there exist numbers of comparative studies in both fields, the expected 
cross entropy in the machine learning field has never been examined in the term extraction field as far 
as we know. Nor has it ever been pointed out that tfldf and the expected cross entropy follow the same 
mathematical structure; that is, to express the significance of a term by the product of probability 
and information. This moti- vated us to compare these two measures in more detail using an actual data 
set. As before, let flj be the number of occurrences of w, within dj, f~. be the total occurrences of 
wl in all the documents, fd3 be the total occurrences of all terms in dj, and F be the total occurrences 
of all terms in all the documents, i.e., F = E~,ew Ed3eD flj = Ed, EO fa, = ~,eW f~'" The strategy to 
choose representative terms is to select ones with greater feature values. For simplic- ity, we assume 
here that the joint distribution P(wi, d~) is simply determined by the observed occurrences, such that 
P(w,, d~) = /*j (17) T-" From Eqs. (12) and (17), the feature quantity of a term for the whole document 
set is calculated as: f,__A_ = -:---mg . (18) f~. Jd__2_ F Since the selection criterion in the above 
equations is ex- pressed as the product of term frequency and Kullback- Leibler information, we refer 
to such a measure as tfkli in the following. It is also possible to use joint distributions other than 
Eq. (17). For example, taking the influence of unob- served terms into account, the first term of Eqs. 
(19) and (18) can be substituted with (f~. -6)IF, where 6 is the coefficient of absolute discounting 
[11]. When calculating a term's significance with respect to a specific documen- t, the feature quantity 
is expressed as follows from Eqs. (11) and (17): /,j Y(w,; dj) = -- (19) ~log f~" fd~ F 3.2 Experiments 
to Compare tfidf and tfkli Mea-sures Comparing Eqs. (18) and (19) with the traditional defi- nition of 
tfidf(wi) (= (f~./F) × log(N/N,)), it becomes clear that with our information theoretic formulation, 
idf and Kullback-Leibler information play similar roles. Specif- ically, these two quantities match when 
the following con- ditions are satisfied: (C1) fa~/F ~ 1/N (c2) f~j/f~. ~ 1/N~ Here, C1 means all the 
documents have almost equal sizes, while C2 indicates that the occurrence of a term does not differ much 
across the documents. For exam- ple, these conditions naturally hold when the document set under consideration 
is a collection of relatively short articles. Also, C2 is automatically satisfied when f~j is given as 
a Boolean value, i.e., either 1 (occurs) or 0 (does not occur). In the following experiments, we have 
actually cal- culated and compared the values of tfidf and tfkli for each term. The objective is to investigate 
the appropri- ateness of interpreting tfidf as a variation of our feature quantity. Two different types 
of data sets are used in the experiments: (D1) 2,106 abstracts of academic conference papers registered 
by the Japanese Society of Artificial Intelligence, and (D2) 24 groups of abstracts of academic conference 
papers, in total 327,880, each group of which corresponds to a different academic society. Each abstract 
is downloaded from the NACSIS Academ- ic Conference Paper Database, also used in the NTCIR Workshop [10], 
and then is processed by a Japanese mor- phological analyser to extract nouns and also compound nouns. 
Although the language used in the corpus is Japa-nese, the result is language-independent since we only 
use the co-occurrence statistics for the numerical comparison of the two measures. For data set D1, we 
assume that each abstract corre- sponds to a single document. The average size of a docu- ment is 93.4 
words with the standard deviation being 33.0 words, figures which indicate conditions C1 and C2 are satisfied 
in this case. For data set D2, a group of abstracts presented at the same academic society is considered 
to be a single document. In this case, the size variation between documents is extremely large: while 
the largest documen- t contains about 25% of the total 31,450,032 terms, the smallest one contains only 
0.6% of the total. This implies the similarity conditions C1 and C2 no longer hold. Figure 2 shows the 
results where tfidf and tfkli val-ues are calculated for all the different terms using Eq. (18), and 
then summed up for the terms with the same frequency (f~,). The horizontal axis represents the fre- quency 
of the term and the vertical axis represents the averaged (for (a), (b), (d) and (e)) or the totalized 
(for (c) and (f)) feature values. For example, with U2, the tfidf values for all the terms with frequency 
1 is Iog24 and thus the point (1, log24) is plotted on the graph (d). From these results, we can confirm 
our expectation that the values of tfidf and tfkll are almost identical for D1, but differ greatly for 
D2. Although only averaged values are shown in the figure, we have also examined the correlation between 
tfidf and tfkli on an individual ter- m basis. The correlation coefficient for middle frequency terms 
is about 0.9 ,-~ 1.0 for D1, and 0.6 N 0.7 for D2. It is also important to note that on comparing the 
amount of information totalized for each frequency, low frequen- cy terms contain a considerable amount 
of information, while the amount of information contributed by each ter- m is small. Because of the exponential 
nature of term frequency distribution, known as Zipf's Law, this tenden- cy remains unchanged even when 
considering the effect of unobserved terms. 3.3 Expanding the notion of tfidf The theoretical development 
and experimental results show that tfidf and tfkli values axe highly correlated for a relatively homogeneous 
document set, in which case, idf provides a simple but robust estimate of the infor- mation. It has also 
been observed that tfidf and tfkli values differ much for the data set composed of hetero- geneous documents. 
Then, the next question is: which works better under which conditions? We do not examine this issue in 
the present paper, however, for the following reason. Remember that tfidf assumes equal probability for 
aH the documents with w~. In the information theoretic framework, such a strategy maximizes the entropy 
of T~ under the restriction that on- ly the Ni documents with wl have nonzero probabilities. On the other 
hand, tfkli simply uses the observed fre- quency as the estimate of the real probability of w~ with- 
out considering the effect of finite sampling. The optimal allocation, if any, should be somewhere in 
the middle. However, the issue is related to the selection of a proba- bilistic model rather than the 
definition of a significance measure, and is beyond the scope of the paper. Instead of further comparing 
the behaviour of tfldf and tfkli, we show in the following how the notion of tfidf can be expanded using 
the information theoretic framework. In the conventional studies, it is repeatedly pointed out that simple 
term frequency places too much emphasis on high frequency terms, while mutual and other infor- mation 
criteria allocate too much weight to low frequen- cy terms. It is also widely recognized that the classical 
definition of tfidf, with its linear scaling in term frequen- cy, again lays too much stress on high 
frequency terms. The difficulty is in establishing a good balance between frequency and information. 
This problem can easily be reformulated using the probabilistic framework shown in Figure 1. Namely, 
the "term frequency" of tfidf actually refers to the probability of terms submitted to the system, and 
can be determined independently from the frequency of terms in the target documents. Therefore, the tf 
fac- tor may be proportional to the square root of the term frequency, logged, or equally weighted, depending 
on the system's expectation about the query terms. IV~V . . . 5000 4000 3000 200O 1000 0 10 100 1000 
10000 frequency of the word (a) averaged tfidf value for D1 thdf-- 80000 $ 11 | 20OOO ..... I b .... 
10 100 1GO0 10000 100000 10+06 f rsque.'~:y of the word (d) averaged tffidf value for D2 OU~V 7000 6000 
5000 4000 3000 2000 1000 0 /'iiflklJ -- ffclf--180OO0 ffkh .... g ,ooooo 1400OO ~ 1O0OOO ~ 80000 ~ 6o0oo 
40000 20OOO 0 10 100 1000 10OOO 10 100 1000 10000 frequency of the word (b) averaged tfkli value for 
D1 ffkll -- 80000 g ~_ 6o0oo 40000 20000 10 100 ,000 10000 10C~00 1~.06 frequency of 1he word (e) averaged 
tfkli value for D2 frequency of the word (c)cumulated tfidf/tfkli value for D1 8o+06 tf~--7¢+06 ttldl 
--- e~°e I ~ 5o+O6 E 0 ' , , , " ' - 10 100 1000 ,0000 100000 la+08 frequency of the WOrd (f) cumulated 
tfldf/tfkli value for D2 Figure 2: Comparison of tfidf and tfkli values. In particular, the rough but 
simplified assumption of tfldf allows us the following simple extension. Let us consider k independent 
trials, in each of which a s-ingle term is picked randomly from W with probability P(w~) = f~o./F. Let 
"VV~ be an event that w, appears at least once in the selected k terms. It immediately fol- lows that 
the probability of W~ equals the probability that wl is not selected at all subtracted from 1; that is, 
P(~,V/k) = 1 - (1 - Since tfidf employs the rule- of-thumb assumption that all the documents axe equally 
likely at the initial stage and also that all the documents that contain wl are equally likely given 
wl, the informa- tion obtained by ~4~ does not depend on the number of times wl is observed. Then, the 
feature quantity using idf can be simply expressed as If we let k = 1, Eq. (20) equals the definition 
of tfidf. On the other hand, if we let k = co, Eq. (20) equals the definition of idf. However, in general 
situations where conditions C1 and C2 do not hold, the calculation be- comes more complex. Although the 
above extension seems trivial, the im- portant implication is that the specificity of terms can be defined 
so that it changes depending on the hypoth- esized situation, i.e., the distribution of terms submitted 
to the system. Eq. (20) provides a flexible way of trad- ing off frequency and information ranging from 
tfidf to idf. Accordingly, the abstract level of the selected terms is changed from general to concrete. 
Table 1 illustrates how the top 10 ranked terms of Reuters-21578 acq topic category change for values 
k = 1, 103, l0 s, where flj is used instead of f,~, in calculating Eq. (20). The figures on the left 
side indicate the frequencies of the correspond- ing terms in the acq topic category. It can be seen 
that the terms become more specific as the k values increase. Table 1: Example of effect of changing 
k with tfldf k measure with Reuters-21578 acq category. tfidf tfidf louu tfid/lu°°°° 5140 share 696 merger 
221 cyelop 696 merger 313 usalr 215 twa 977 acquir 977 acquit 154 purol 934 sharehold 934 sharehold 102 
chemlawn 313 usair 221 cyclop 77 cyaeq 847 stake 847 stake 60 emeri 221 cyclop 215 twa 54 comdata 215 
twa 268 gencorp 50 pesch 268 gencorp 5140 share 47 norstar 4060 inc 901 acquisit 44 conrac 4 Feature 
Quantity in Text Categorization 4.1 Equations for fq-cr and fq-lr Measures Let C = {cl,..., cjv) be a 
specified set of categories. Al- so, let w* = wll,  ~ win be a sequence of k terms repre- senting a 
document to be categorized. The strategy for text categorization is to identify cj E C that maximizes 
the feature quantity value, given w* (Figure 3). Here, a category is viewed as a single collection of 
terms rather than a collection of independent documents and thus, E- q. (11), used for the calculation 
of the feature quantity across terms and documents, is also applicable for the calculation of the feature 
quantity across terms and cat- egories. For notational simplicity, we denote the set of different terms 
in w* as w +, and the number of times wl (E w +) occurs in w* as h, in the following. Concerning the 
selection of w*, two different formu- lations are considered. (F1) In the first case, it is assumed that 
the k terms axe selected from some unknown distribution. /probability .~.~'-'-'~'~... probability x 
i distribution / categories "%° distribution i from which l k that /" ~ "~ represents i the terms 
of ~ % the system's  the target co-occurrences  I. document / of categorles %. knowldege on ! orlglnate 
. and terms I the known  | ! ....... _~_ i categories ~__~ some unknot,, t e~r~s~ ....... ~ ~ I /~. 
~istribution " ...... {'PA,'_]'u.,*I'~ i ~ PfWi) ~_ (F2)from one k~ '~1' " , \ -/ "~of the known ! ~ 
categories -~ I 1 W*= Wil... Wik ~ I ilAdocument to I . )[Thesyst~ I I 'lbe catego==ed I\ :~t / I selects 
a I ' i] is sulm~itted ~_~.~_..~.~J. [category into ] I il to the systeaa. / l which the I ! ' \ I document 
is I ". C] I classified ~ Figure 3: An illustrative situation assumed in the text categorization task. 
(F2) In the second case, it is assumed that the k terms are selected from one of the categories in C. 
The difference between F1 and F2 roughly corresponds to the difference between the existing vector and 
prob- abilistic IR models. In case F1, the target document is generated independently of the existing 
categories. Then, the objective of the categorization task is to find a cate- gory closest to the target 
document. In case F2, on the other hand, the target document comes from one of the existing categories. 
This time, the objective of the cat- egorization task is to identify the category from which the document 
is most fikely to have come. Of course in actual applications, these two formulations cannot explic- 
itly be distinguished. Nevertheless, they require different mathematical treatments. In case F1, the 
selection of the k terms is independent of the distribution of the terms in the existing N cate-gories. 
Assuming P(w~); the distribution of the observed k terms, and P(cjlwi), the probability of each category 
conditioned by wl, are given; P(cj) is determined by P(cj) = E P(w')P(cjlwl)" (21) w,EW Since terms in 
w* are mutually independent, P(w*,c~) = P(%)1~,,~+ P(w,[%) ~'' Then, from the additivity property of 
the amount of information for independent events, the feature quantity of cj and w* can be expressed 
simply as the summation of the feature quantity of c~ and each w, E w*: y(~*,c~) = ~ hi y(~,,,cj) w , 
Ew'l" = h P'w ~" P(wi, cj) w,Ew+ On the other hand, in case F2, the k terms are se- lected from the same 
existing category cj 6 C. Assuming that P(cj), the probability that c~ is selected as the o-rigin of 
the k terms, and P(w~]c~), the probability of w~ conditioned by %, are given, P(w*, %) is calculated 
as: p(,.,,*,cj)=p(cj) II P(~'Icj)'~'" (23) From P(,,,*) = ]E0~c P(,,,*, cj), P(,,,*) is given by: P(w*) 
= ~_, P(ci) ~I P(w'lcJ)n'" (24) c 3 6C w,Ew+ Then, the feature quantity is calculated as: y(~*, c~) , 
e(w*,c~) = P(,o', cj) ,OgpTgq~T~) H p(w, lcA~" =P(cj) H P(w']ci)~" l°g~°'C~'p(w.) "(25/ wsEw+ Unlike 
the case in Eq. (22), the k terms are not inde- pendent of each other, and Eq. (25) cannot be simplified 
further. Noting the different treatments of the conditional prob abilities in Eqs. (22) and (25), we 
refer to these equations as f q-a and f q-Tr, respectively. 4.2 Incorporating Different Probability Models 
The interpretations of the classification tasks by F1 and F2 are based on different standpoints; that 
is, F1 uses P(wi) and P(ci[wl ) as primary distributions while F2 ~sumes P(cj) and P(wdcj) are given. 
However, if P(w,) and P(cjlwl ) are known, P(cj) and P(wdcj)are uniquely determined by Eq.(21) and from 
Bayes Theorem. The reverse is also true. This enables the comparison of F1 and F2 under the same probabilistic 
assumptions. In our experiments, the following three models are used for comparison. The first model 
is chosen for F1 and referred to as freq. Since F1 assumes that the target document origi- nated from 
some unknown distribution, P(w,) is set equal for all the terms, i.e., P(wi) = 1/M (Vw~ E W), where M 
is the total number of distinct terms. P(cj Iw~) is simply determined in proportion to the observed frequency. 
De- noting the frequency of wl in category cj as f~j, the total frequency of wl for all the categories 
as f~i, and freq is defined as: P(wl) = ~,1 P(cA~,)=-f~j (26) .f~,. The second model is chosen for F2 
and referred to as laplace. With F2, consideration of unobserved events is crucial in determining the 
value of P(w~lcj). The al- location policy of freq does not work at all if w* con-talus only a single 
unknown term because the probabili- ty P(w*, cj) automatically becomes zero for all the cat- egories. 
Laplace, given by the following equations, pro- vides a simple solution to the zero frequency problem: 
1 "4- flj P(cj) = , P(w~lcj) --M + f~, (2~) where ffca is the total frequency of terms in category cj. 
This estimation is known as the Laplace estimator and is often used in the conventional naive Bayes approaches 
in the text categorization field. Now, regardless of the convenience of the Laplace esti- mator, it 
has been widely recognized in probabilistic lan- guage model studies that the estimator does not provide 
a good fit compared with other dedicated discounting meth- ods [6]. Recent studies in the text categorization 
field have also shown that the performance of naive Bayes cat- egorization is sensitive to the estimation 
of P(wilcj) and that there exist cases when other event models outper- form the Laplace estimator [9]. 
Considering these, we in- troduce a new probability model that can deal with both F1 and F2. The third 
model, referred to as miz~ure, is expressed as the mixture distribution of P(cj) in Eq. (27), and P(cjlwi) 
in Eq. (26): P(w,) = -F-' P(cjlw') = (1 - rj-~-" f°J .' r Ef~s (28) where the mixture ratio r is determined 
by analysing ac- tual corpus statistics. Specifically, we have observed that such a model provides a 
good fit with our corpus, but the details are still under investigation. 4.3 Correspondence with Naive 
Text Categoriza- tion Methods Fq-a given by Eq. (22) is closely related to the similarity measure widely 
used in the present IR systems: the nor- malized inner product of term vectors weighted by tfidf. Among 
many of its variations, an example of the conven- tional measure, which is referred to as tfidf-cos in 
this paper, is expressed as: h, × i,, x log~ N t fidf(w*, cj ) (29) 2_, IIw'll IkAI The normalization 
factor is given by ][~o*11 = ~.~÷ h,~ lo N2 and IIcjll = ~,~w(/, j g~:) , with /,j being the fre- quency 
of w~ within category cj. On the other hand, the definition of fq-0" with freq probability allocation 
is rewritten as: 1 A~ x log f~" (30) ~,ew f~o. M Comparing Eq. (29) with Eq. (30), it becomes clear that 
both tfidf-cos and fq-cr entail the same form of the "summation of h, × .flj × log(.)". The difference 
is in their normalization and in their consideration of the amount of information in the log terms. Based 
on this, we can expect that the performance of these two categorization methods match well when the target 
document set conforms with C1 and C2 in the previous section. Next, fq-~r given by Eq. (25) has a clear 
correspon- dence with the naive Bayes method popularly used in conventional text categorization studies. 
The method, referred to as n-hayes in this paper, is based on the max- imum likelihood principle that 
selects the category with the largest probability P(cjlw* ). Thus, the classification strategy can be 
expressed as: nbayes(w*, cj) = P(cj I~,*) = P(c~, w*) P(~*) P(cj) H P(w'lc,)h' = ,~,e~,+ (31) P(w*) where 
the term P(w*) can be omitted since it is common for all the categories. Comparing Eq. (31) with Eq. 
(22), it transpires that the only difference between n-bayes and fq-lr is the con- sideration of the 
amount of information expressed as the log term in Eq. (25). The difference is most clearly demonstrated 
in the extreme case when k = 0 (w* = 0). Without any information available, n-bayes selects the most 
frequent category as the most plausible, while fq-~r judges all the categories to be equally likely. 
However, for larger values of k, the difference between Eq. (25) and Eq. (31) is negligible and does 
not have significant influence on the classification results. 4.4 Text Categorization Experiments In 
the following experiments, three different measures for text categorization are compared: (i) fq-cr, 
(ii) tfidf-cos, and (iii) n-bayes/fq-~r. The data set used is again ab- stracts of academic papers extracted 
from the NACSIS Academic Conference Paper Database. The categoriza- tion task is to identify the society, 
or the class of societies, of an unknown abstract data. Since the size of each ab- stract is sufficiently 
large (the average is about 90 words per abstract), there exists no meaningful difference be- tween the 
n-hayes and yq-zr methods. The training data is the same as data set D2 in section 3.2, which is composed 
of a total of 327,880 abstracts from 24 academic societies. Each category corresponds to either (Pl) 
one of 24 academic societies, or (P2) one of the two society classes; information tech- nology related 
or not related. With P1, the sizes of the 24 categories vary greatly, from a maximum of 7,986,568 terms 
to a minimum of 187,290 terms per category. With P2, the two society classes are of almost equal size, 
with the ratio about 46% to 54%. Based on these figures, we can expect that tfidf-cos works more consistently 
with ffq-o, for Pl than for P2. The categorization task is formulated as a multi-class problem, since 
each abstract belongs to one and the only society/society class. In the evaiuation, a total of 10,000 
abstracts are prepared that are not contained in the train- ing data, but with the same distribution 
across categories. Therefore, if about 25% abstracts of the training data be- long to society A, then 
the test data also contains 25% abstracts from society A. The performance is compared using the ratio 
of the correct judgments, i.e., the number of abstracts classified into the class that they originally 
belong to, divided by 10,000. The size of the training data set is varied as either 1,000, 10,000, or 
327,880 for each of P1 and P2 so that the scalability of different categoriza- tion strategies can be 
compared. For all combinations, three probability modds, freq, laplace, and mixture are tested. The results 
are summarized in Table 2. From these results, we can confirm that tfidf-cos per-forms well with P2. 
In the cases with P1, the perfor- mance of gfidff-cos is considerably degraded as the size of the training 
data becomes large. In contrast, better performance is obtained for larger sizes of the training data 
for fq-a and n-bayes. The best performance values of these two methods are almost equal. Comparing the 
probability models, freq is not applicable for n-hayes due to the zero frequency problem we have already 
mentioned, Table 2: Result of Classification Task (a) Results for P1 with 24 categories the size of 
the training data (b) Results for P2 with two categories I II thesize°fthetrainingdata I  Methods P(djlwi 
) 327,880 !0,000 ] 1,000 tfldf-eos ~eq 0.6835 0.7128 0.6596 laplace 0.6914 0.5486 0.4657 mixture 0.6840 
0.7074 0.6368 fq-a ~eq laplace 0.8097 0.6537 0.7516 0.5463 0.6523 0.5155 mixture 0.8158 0.7558 0.6552 
n-bayes ~eq 0.0000 0.0000 0.0000 (fq-~) laplace mixture 0.7099 0.7929 0.6055 0.7646 0.5626 0.6603 Methods 
P(di[mi ) 327,880 ] 10,000 [ 1,000 tfid]-cos I freq 0.9709 i 0.9483 0.9194 laplace 0.9704 0.9475 0.9185 
mixture 0.9701 0.9437 0.9108 fq-o" freq 0.9595 0.9459 0.9212 laplace 0.9575 0.9405 0.9116 mixture 0.9625 
0.9532 0.9289 n-bayes freq 0.0009 0.0000 0.0000 (Iq-') laplace 0.9658 0.9516 0.9231 mixture 0.9689 0.9579 
0.9329 while with tfidf-cos and fq-a, freq seems to be reason- ably good. Also, laplace does not work 
well with P1 for both f q-a and n-bayes. Mixture performs consistently well in all the cases. Lastly, 
we have so far only referred to the naive tfidf-cos and n-bayes text categorization methods. Other ex-isting 
methods include Rocehio with Relevance feedback [2], Prtfidf based on probabilistic analysis [5], and 
a va- riety of machine learning methods such as LLSF, C4.5, k-NN, and Support Vector Machine [12][15]. 
However, we have observed that with as many as 2,300,000 feature terms, the naive methods sometimes outperform 
other more complicated methods. For example, the standard learning algorithms such as C4.5 or kNN can 
easily be applied if the feature dimension is reduced to 5,000. An- other investigation using the same 
document set showed that the performance is around 0.7 for P1 [13], which is smaller than the values 
0.75 to 0.80 obtained in our ex- periments. Of course, the difference may be overturned by the fine tuning 
of the learning methods. Nevertheless, we think the issue needs further investigation since the data 
used in our experiments is different from Reuters- 21578, the standard test set for text categorization 
tasks today, both in its large scale and also in its specificity to the academic fields. Such phenomena 
may partly be ex- plained by the fact that academic documents are highly domain specific and a considerable 
amount of information is carried by low frequency terms, as has been shown in Figure 2(c)(f). Remarks 
In this paper, we have introduced the feature quantity, a quantitative representation of specificity 
of textual da- ta components. Although our investigation in this paper mainly concerns the consistency 
of the proposed feature quantity with, and not its superiority to, conventional s- tatistical measures, 
we believe such an approach is worth trying since the information theoretic view helps us to apply the 
same mathematical framework to different tar- get problems such as representative term selection, text 
categorization, and also automatic identification of collo- cations and translation pairs. In addition, 
such a view enables us to shed light on commonly practised heuristics such as tfidf or discarding of 
low frequency data at the pre-processing stage. Acknowledgement This work is part of the project "A Study 
on Ubiquitous Information Systems for Utilization of Highly Distributed Information Resources", funded 
by the Japan Society for the Promotion of Science. The authors would specially like to thank Kyo Kageura, 
Atsushi Takasu, and Kenro Aihara at the National Institute of Informatics for sharing experimental data 
and for their helpful discussions. References [1] G. Amati and K. van Rijsbergen. Semantze Infor- mation 
Retrieval, 189--219. Kluwer Academic Pub., 1998. (in "Information Retrieval: Uncertainty and Logics"). 
[2] C. Buckley, G. Salton, and J. Allan. The effect of adding relevance information in a relevance feedback 
environment. In SIGIR'94, 292-300, 1994. [3] S. A. Caraballo and E. Charniak. Determining the specificity 
of nouns from text. In EMNLP'99, 1999. [4] W. R. Greiff. A theory of term weighting based on exploratory 
data analysis. In SIGIR'98, 11-19, 1998. [5] T. Joachims. A probabilistic analysis of the rocchio algorithm 
with tfidf for text categorization. In ICM-L'97, 143-151, 1999. [6] K. Kita. Probabilistic Language Model. 
University of Tokyo Press, Japan, 1999. [7] D. Koller and M. Sahami. Hierarchically classifying documents 
using very few words. In ICML '97, 170-178, 1997. [8] D. Maldenid and M. Grobelnik. Feature selec-tion 
for classification based on text hierarchy. In Working notes of Learning from Text and the Web, CONALD'98, 
1998. [9] A. McCallum and K. Nigam. A comparison of event models for naive bayes text classification. 
In AAAI-98 Workshop on learning for text categorzzation, 42-49, 1998. [10] NACSIS, editor. NTCIR Workshop 
1 -proe. of the First NTCIR Workshop on Research in Japanese Text Retrieval and Term Recognition. National 
Cen- ter for Science Information Systems, 1999. [11] H. Ney, S. Martin, and F. Wessel. Statzstzcal Lan- 
guage Modeling using Leaving-one-out, 174---207. K- luwer Academic Pub., 1997. (in "Corpus-Based Methods 
in Language and Speech Processing"). [12] Y. Singer and D. D. Lewis. Machine learning for information 
retrieval: Advanced techniques. In SI-GIR "99 Tutorial, 1999. [13] A. Takasu and K. Aihara. Variance 
based classifier comparison in text categorization (poster). In SIGIR 2000, 2000. [14] S. Wong and Y. 
YaH. An information theoretic mea- sure of term specificity. Journal of the Amemcan Soczety for Information 
Science, 43(1):54--61, 1992. [15] Y. Yang and X. Liu. A re-examination of text cate- gorization methods. 
In SIGIR '99, 42-49, 1999. [16] Y. Yang and O. Pedersen. A comparative study on feature selection in 
text categorization. In ICML '97, 412--420, 1997.  
			