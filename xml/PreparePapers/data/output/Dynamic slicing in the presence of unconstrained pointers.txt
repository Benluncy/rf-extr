
 Dynamic Slicing in the Presence of Unconstrained Pointers Hiralal Agrawal Bellcore 445 South Street 
 Morristown, NJ 07962-1910  Abstract Program slices are useful in debugging. Most work on program slicing 
to date has concentrated on find­ing slices of programs involving only scalar variables. Pointers and 
composite variables do not lend them­selves well to static analysis, especially when the lan­guage involved 
is not strongly-typed. When debug­ging a program, however, we are interested in analyz­ing the program 
behavior for testcases that reveal a fault. In this paper, we present a uniform approach to handling 
pointers and composite variables such as arrays, records, and unions for the purpose of obtain­ing dynamic 
program slices. The dynamic approach proposed works well even when the language involved allows unconstrained 
pointers and performs no run­time checks, as in C. 1 Introduction The notion of program slicing has been 
discussed ex­tensively in the literature [4, 11, 13, 17, 20]. This dis­cussion, however, has mostly dealt 
with finding slices for programs involving scalar variables (see Section 6, Related Work). Slicing would 
be even more useful in debugging programs that use complex data-structures involving point ers when int 
erst at ement dep en den­cies are hard to visualize by manual examination of the source code. Scalar 
variables are relatively easy to handle because the memory location that corre­sponds to a scalar variable 
is fixed and known at compile time; it does not vary during the course of This research was supported, 
in part, by a grant from the Software Engineering Research Center at Purdue University, a National Science 
Foundation Industry/University Cooperative Research Center (NSF Grant ECD 8913133), and by National Science 
Foundation Grant CCR 89103O6. Permission to copy without fee all or part of this matertial is granted 
provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright 
notice and the title of the publication and its date appear, rmd notice is given that the copying is 
by permission of the Association for Computing Machinery. To copy other­wise, or to republish, requires 
a fee and/or specific permission. 01991 ACM 089791449-X/91/0009/0060 $1.50 Richard A. DeMillo Eugene 
H. Spafford Software Engineering Research Center Purdue University W. Lafayette, IN 47907-1398 program 
execution. Hence, if one statement modifies a scalar variable and another statement references a scalar 
variable, it is easy to determine, at compile time, if the latter statement references the memory location 
modified by the former. The chief difficulty in dealing with an indirect refer­ence through a pointer 
or an array element reference is that the memory location referenced by such an expression cannot, in 
general, be determined at com­piled time. Further, when such a reference occurs inside a loop, the memory 
location referenced may vary from one loop iteration to another. The dif­ficulty is compounded if the 
language used is not strongly-typed and permits integer arithmetic over pointer variables. Techniques 
proposed in [7, 9, 14] may be used to obtain conservative approximations of what a pointer might point 
to, but in the presence of unconstrained pointers, as in C, such analysis has only limit ed usefulness. 
In this case we are forced to make the most conservative assumption: an indirect assignment through a 
pointer can potentially define any variable. The outcome of this assumption is that static slices of 
programs involving pointers tend to be very large; in many instances they include the whole programs 
themselves. Fortunately, while debugging a program we nor­mally have a testcase that reveals the fault 
and we wish to analyze the program behavior for that test­case, not for any testcase. Dynamic slices 
provide precisely this facility. It is possible to perform pre­cise dynamic interstatement dependence 
analysis even when the language is not strongly-typed. In this paper, we present an approach to obtain 
dynamic program slices when the programming lan­guage permits unconstrained pointers. Besides point­ers, 
composite variables such as arrays, records, and unions are also handled uniformly under this ap­proach. 
It also allows precise interprocedural dynamic slicing to be performed. We first present a general framework 
to obtain static slices in the presence of 1Not regarding an array as a single unit.  A,* 7/halu?/dnmn/$asL-dau. 
--a. -----....-------------c - * 45 nain( ) 46 [  Dat.eType dake; :: int nonth-d.sMs-t.able[ 121, day-count-since-st.srnity, 
i; 49 50 51 52 53 54 :: else 57 58 59 60 61 else 62 63 64 E 67 else 68 69 70 ;: 73 74 75 76 /* .wputx 
day-count. since Jan 1, war 1 */ z dav-counb-since-eternity 365 * ( dat.e.year -1); 79 day-count._since-ehernlty 
+= (date. ueiir -1) / 4~ ao dag-counk-since-ekerniky -= (dat.e.yew -1) / 100; 81 da~-COUW-8inC0-EtWnitu 
+= (date.uear -1) / 400s 82 day.count..since.ekimity +. daLe.d~of.thr..yaar; 83 84 Al Cmput.e das-af-ueak 
*/ S5 dat.e.day-ofJAe-week = day-cwnt-since-eternit.g % 7; 86 87 /* prinL day of year *I 88 +9 pritif( 
 day of the war for the date %d/%d/%d is %d.\n , cM.%nmf.h. 69 dake.da~, daLe.war. ~$ 80 91 /* print 
das of wok */ 22 print.-daH-of.tho.ueek (datm.da&#38;of-Lhe.wk)s s  approx. dynamic analysis exaet 
dynsmic enelysis &#38;mllmlm( )~ ~DzEEIE!EEEIEE@=EEIEz-=1~ DcGEiclEEElbEllEiKlm@@ =Hl @ J 1>skakicslice 
 prosrananat lima dake.dw.of.t.htujear 88 E r. .-, T-..* -.--4. 4 vu! , =!... .-=.. - ..-. . A I Figure 
1: Static slice with respect to date. day-of-the-year at line 88. pointers and composite variables, and 
then extend it to the dynamic c-ase. While the static slicing algo­rithm assumes that an indirect assignment 
may po­tentially modify any variable, the dynamic slicing al­gorithm detects exact dependencies. We have 
built a prototype debugging tool, named Spyder, that uses the approach described hereto find both static 
and dynamic program slices for programs written in C [1, 2]. The tool supports a powerful de­bugging 
paradigm involving dynamic slicing [4] and execution backtracking [3] with the help of which pro­gram 
bugs may be localized quickly. To see how dynamic slices differ from static slices, consider the program 
in the main window panel of Figure 12. It reads a date (month, day, year) and 2 Figures 1, 2, 9, 10, 
and 11 are X Window System window dumps of our prototype debugging tool, Spyder-, in operation. finds 
the corresponding day-of-the-year and da~-of­the-week. Consider the case when this program is ex­ecuted 
for the date January 1, 1990, i.e., (month=l, day=l, year= 1990). Statements in reverse video in the 
figure show the static slice with respect to date. day.of.the-year at line 88. Figure 2 shows the corresponding 
dynamic slice. If the day-of-the-year is computed incorrectly for this testcase, the value of date. day 
must be incorrect. Thus the error must be inside the procedure read-date invoked on line 50. Clearly, 
the dynamic slice in Figure 2 will help us lo­ calize the fault much more quickly compared to the static 
slice in Figure 1. In Section 2, we first present a framework for ob­taining static and dynamic program 
slices when the program uses only scalar variables. Then in Sections 3 and 4 we extend our framework 
to handle pointers Figure 2: Dynamic slice with respect to date. day-of.the-year at line 89 for the testcase 
(month= 1, day= 1, year= 1990). /u17/ha/v2/deno/fast--dau -c -­ nain( ) z r Dat,eType date; :: int, nont.h-days-Lable 
[121, day-counL-simc-ebernibu, i; 49 50 $ /* conpute the day-Lable */ 53 for (i=O; i < 7S i++> /* init. 
days for January LO A@ W 54 if <i%2 *a O) 55 non&#38;h-day s-Lable[il . 3i; 56 else nonkh-days-t.able 
[il = 30; 2: for (i=7; i < 12; i++) /* inik days for Iiugust. ta December */ 59 if (i%2 = 0) 60 nonth-days-t,able 
[il = 30; 61 else 62 nont,h.daysl.able[il = 31; 1% /* check if it is a leap-year, and up daLe # of days 
in February */ 65 if <(dake. year % 4 == O W dat.e. yqar Z 100 != O) I I (dat,e. uear % 400 s= O)) 66 
nonth-daysl.able[ll 29; 67 else 68 nonth-days-kabl etll * 28; 69 70 71 72 for (i = O; 73 i < date .nonth 
-1; 74 i++ ) 75 date. day-of-t-he-year += nonLh-daysAable[ il; 76 77 f* .o.put.e day-count since Jan 
1, ysar 1 *I 78 day-count.-since-eternity m 365 * (dake.ysar -1); 73 day-count-since-eternitg +. {date. 
year -1) / 4; BO dag-count-since-eterniky -= {daLe. gear -1) / 100; B1 day-coumt-since-sternity += (date. 
year -1) / 400; B2 day.counL-since-eterniLy +. dahe .day-of-Lhe-ysar; B3 B4 /* conp te day-of-wek */ 
65 date. day-of-the-ueek H day-count-since-eternity % 71 B6 B7 /* print day of war */ B13 +9 printf ( 
 dsy of the ysar for the dat,e Zd/%d/%d is %d.\n , dsts.month, B9 date. day, daLe.ysar, I . , .mmmmmmm; 
I 90 91 /* print day of uesk */ 92 prink-day-of-hhe.ueek(da te.day-of-the-.eek) z 93 static snalysis 
approx. dynamic analysis r %===lP==+==l r==m=-ll+===l~? =EiclEEEiclLiiEIEiidEclEiE!IIEiEcll>dyna.ic progra. 
slice on daLe.day-of-Lhe-year at line Current Testcase #: 1 S3 q .l and composite variables such as 
arrays and records for static and dynamic cases, respectively. Section 5 discusses how our approach may 
be extended to the interprocedural case. Finally, Section 6 outlines re­ lated work. Background In 
the following sections we use a let-in construct adapted from a similar construct in the programming 
language ML [16]. Consider the following generic use of let: let <declarations> in <expression> Here, 
<declarations> consists of a sequence of name bindings that may be used inside <expression>. The scope 
of these bindings is limited to <expression>. The result of evaluating <expression> is returned as the 
value of the let construct. For example, the following expression evaluates to 5. leta=2, b=3ina+b Names 
may also be bound using pattern matching between two sides of the symbol =. For example, if the complex 
number X + Yi is represented by the tuple (X, Y), then the sum of two complex numbers comp!exl and complex2 
may be defined as follows: sum(complexl, compiexz ) = let complex~ = (reall, imaginary), complexz = (realz, 
imaginary) in (reall + realz, imaginary + imaginary) In the above expression, reall, imaginary, reai2, 
andimagi nary2 were alldefined using pattern match­ ing. We also use U notation to denote set unions. 
For example, if S= {xl, X2, . . . . Zn}, then we have: U.ES f(~) = f(~l) u .f(~z) u  u f(~~) U s may 
also be composed. For example, if S1= {xl, Z2} and S2= {yl, Y2}, then we have: UZES1 U,6S2 g(~l Y) = 
u;::; 9(~1 Y) = g(zl, yl) u g(q, yz) u g(zz, yl) u g(zz, y2) 2.1 Simple Static Slicing The Flow-graph 
Flow of a program P is a tuple (V, A) where V is the set of vertices that correspond to simple statements 
and predicate expressions in the program (assignments, reads, writes, etc., and condi­tion expressions 
in if-then-else, while-do, etc.)3, and A is the set of directed edges between vertices in V. If there 
is an arc from node vi to node vj it means control can pass from node vi to node Vj during pro­gram execution. 
Each vertex in the flow-graph has a use and a def set associated with it. The use set of a vertex consists 
of all variables that are referenced during the computation associated with the vertex, and the def set 
consists of the variable computed at the vertex, if any. Consider, for example, the program in Figure 
3. Symbols fl, f2, and fs, in statements S7, S8, and S9, respectively, are used to denote some unspecified 
side­effect-free functions with which we are not presently concerned; only the names of variables used 
in the comput at ion are relevant. Labels S 1, S2, etc., are included only for reference; they are not 
part of the program. Figure 4 shows the flow-graph for this pro­ gram. Node annotations U and D show 
use and def sets, respect ively, for all nodes in the flow-graph. Static Reaching Definitions Given a 
flow-graph F, a node nin F, and a variable var, we define SRD(var, n, Y), the set of all static reaching 
definitions of variable va r at node n in flow­graph F, to be the set of all those nodes in T at which 
var is assigned a value and control can flow from that node to node n without encountering any redefinitions 
of var along the control-flow path. More precisely: 3 In program optimization applications, vertices 
of a flOw­graph correspond to basic-blocks in the program. But for our purposes, it is more convenient 
to associate vertices with simple-statements and predicates. begin Sl: read(N); S2: z:=o; S3: I:= 1; 
S4: while (I <= N) do S5: read(X); S6: if(X<O) then S7: Y := fl(x); else S8: Y := fz(x);  endifi S9: 
z := fs(z, Y); Slo: 1:=1+1; end-while; Sll: write(Z); end. Figure 3: An Example Program SRD(var, n, 
Y) = letX=(V, A) if varcdef(x) then {z} n U(z,n)~A else SRD(var, z, (V, A {(z,n)})) For example, for 
the flow-graph Y in Figure 4, SRD(Z, 11, Y) = {2,9}. Program Dependence Graph The data dependence graph, 
DataDep, of a program P is a pair (V, D), where V is the same set of vertices as in the flow-graph of 
P, and D is the set of edges that reflect data-dependencies between vertices in V. If there is an edge 
from vertex vi to vertex Vj, it means that the computation performed at vertex vi directly depends on 
the value computed at vertex Vj.4 Or, more precisely: DaiaDep(P) = let FIow(P) = (V, A), {(n,x)} varcuse(n) 
xc SRD(var, n, Flow(P)) in (V, D) D = U.CV For example, the solid edges in Figure 5 denote data-dependencies 
among vertices of the flow graph in Figure 4. As SRD(Z, 11, F) = {2, 9}, there are AAt ~th~~ places in 
the literature, particularly that related to vectorizing compilers, e.g., [8], the dkection of edges 
in de­pendence graphs is reversed. For the purposes of program slic­ing, however, our definition is more 
sm.itable, as will become apparent later. 11 D={Y vU=(X1 D={) u=(z) Figure 4: Flow Graph with use(U) 
and data dependence edges from node 11 to nodes 2 and 9 in Figure .5. The control dependence graph, Contro[Dep, 
of a program P is a tuple (V, C), where V is the same set of vertices as in the flow-graph of P, and 
C is the set of edges that reflect control-dependencies bet ween vertices in V. If there is an edge from 
vi to Vj in ControlDep, it means that node vi may or may not be executed depending on the boolean outcome 
of the predicate expression at node Vj.5 For example, the dashed edges in Figure 5 denote the control 
dependencies among its vertices. As state­ments 7 and 8 are immediately nested under the pred­icate at 
statement 6, there are control dependence edges from nodes 7 and 8 to node 6 in Figure 5. ControU%ed(v) 
denotes the predicate statement upon which node v is control dependent. For exam­ple, for the program 
dependence graph in Figure 5, we have L ontrolPred(3) = ~ whereas ControlPred(7) = {6}. The Program dependence 
graph, ProgramDep, of a program P is obtained by merging the data and control dependence graphs of P.6 
Or, 5 This definition of control dependence is for programs with structured control flow. For such programs, 
the control de­pendence subgraph essentially reflects the nesting structure of statements in the p.ogr-, 
and c= b. .=ily b~lt in a syntax­directed manner. In programs with arbitrary control flow, a control 
dependence edge from vertex vi to vertex V3 implies that v, lies along a path from Vj to the nearest 
inverse dOmi­nator of VJ in the control flow graph of the program [8]. This notion of control-dependence 
is equivalent to Podgurski and Clark s strong control dependence [18] (see Section 6, Re­lated Work). 
6 In other applications like vect orizing compilers, the pro­gram dependence graph may include other 
types of depen­dence edges besides data and control dependence, e.g., anti­dependence, output-dependence 
etc., but for the purposes of program slicing, the first two sutlice. de$(D) sets of the program in 
Figure 3. ProgramDep(P) = let DataDep(P) = (V, D), ControUlep(P) = (V, C) in (V, llUC ) For example, 
Figure 5 shows the program depen­dence graph of the program in Figure 3. Static Slice Given a program 
P, a node nin its flow-graph, and a variable var, the static slice of P with respect to var at node n 
is constructed as follows: We first find all reaching definitions of var at node n. Then, from each reaching 
definition obtained, we find all reachable nodes in the program dependence graph of the program. StaticSlice(P, 
var, n) can be precisely defined as follows: StaticSlice(P, var, n) = let F = Flow(P), D = ProgramDep(P) 
~, ReachableNodes(z, D) n Uz6SRD(uar, n, where ReachableNodes(v, ~) is the set of vertices in ~ that 
can be reached from v by following one or more edges in ~. For example, the shaded nodes in Figure 5 
give the static slice with respect to variable Y at statement 9. 2.2 Simple Dynamic Slicing Let F be 
the flow-graph of program P. Let test be a testcase consisting of a specific set of input-values read 
by the program. We denote the execution history of the program P for test by a sequence hist = <VI, V2, 
..., Vn > of vertices in 3 appended in the order in which they are visited during the program execu­tion. 
The execution history at any inst ante denotes the partial program execution until that instance. Figure 
5: Program dependence graph of the program in Figure 3. Solid edges denote data-dependencies and dashed 
edges denote control-dependencies between vertices. Shaded nodes give the static slice with respect to 
variable Y at statement 9. Consider, for example, the program in Figure 3 and Note that both LastOccur 
and DRD result in either the testcase (IV = 2,X = 4, 3). The execution his-the empty set, implying no 
occurrence of the desired tory of the program for this testcase is <1, 2, 3, 41, node, or in a singleton 
consisting of a unique node. 51, 61, 7, 91, 10I, 42, 52, 62, 8, 92, 102, 43, 11>. Note that we use superscripts 
1, 2, etc., to distinguish be­tween multiple occurrences of the same statement in Dynamic Dependence 
Graph the execution history. Last (hist) denotes the last node in hist, and The dynamic dependence graph, 
DynamieDeP, of an Prev(hist) denotes the subsequence with all but the execution history hist is a tuple 
(V, A), where V is the last node in hist. That is, multi-set of flow-graph vertices (i.e., multiple entries 
of the same element are treated as distinct), and A is Last(<vl, .-., Vn l, Wn>) = vn the set of edges 
denoting dynamic data dependencies Prev(<vl, . . . . vn-l, Vn>) = <VI, . . . . %-1> and control dependencies 
between vertices. We use We use the notation <Prev(hist) I Last (hist)> to the symbol u to denote disjunctive 
union of elements. denote the two parts of hist, and <> to denote the DynamicDep is defined as follows: 
empty sequence. Also, Last Occur(v, hist) denotes the last occurrence of the node v in hist. For example, 
DynamicDep(<>) = (d, #) LastOccur(9, <1, 2, 3, 4 1, 51, 61, 7, 91, 101, 42, 52 > DynamicDep(<prevhist 
I next>) = 62, 8, 92, 102, 43, 11>) = {92}. let DynamicDep(prevhist) = (V, A), Dynamic Reaching Definitions 
D = u .ar~u,e(nezt) {(nezt, z)}, xc DRD(var, preuhast) Given a variable, var, and an execution history, 
hist, C = U ZE controipred(nem$) {(nezt, y)} we define dynamic reaching definition of var with re­ !J6~ast0ccur(z, 
prevhist) spect to hist, denoted by DRD(var, hist), to be the in (VU{nezt}, AulluC ) last occurrence 
of the node in hist that assigns a value to var. Or, Consider again the program in Figure 3, and the 
same DRD(var, <>) = ~ testcase (N = 2, X = 4, 3). Figure 6 shows the corre­sponding dynamic dependence 
graph. Notice the two occurrences of node 9. The first occurrence depends DRD(var, <prevhist I lastnode>) 
= if var~ def(iastnode) then { Iastnode } on nodes 2 and 7 for values of variables Z and Y, re­ else 
DRD(var, prevhist) spectively, whereas the second occurrence depends on For example, DRD(Y, <1, 2, 3, 
41, 51, 61, 7, 91, its first occurrence and node 8 for the same values, 101, 42, 52, 62, 8, 92, 102, 
43, 11>) = {8}. respectively. Figure 6: Dynamic dependence graph of the program in Figure 3 for testcase 
(N = 2, X = 4, 3). Solid edges denote data-dependencies and dashed edges denote control-dependencies 
between vertices. Shaded nodes give the dynamic slice with respect to variable Y at the end of program 
execution. Dynamic Slice Given an execution history, hist, of a program, P, on a test-case, test, and 
a variable, uar, the dynamic slice of P with respect to hist and var is the set of all statements in 
hist whose execution had some eflect on the value of var as observed at the end of the ex­ecution. Note 
that unlike stat ic slicing where a slice is defined with respect to a given location in the pro­gram, 
we define dynamic slicing with respect to the end of execution history. If a dynamic slice with re­spect 
to some intermediate point in the execution is desired, then we simply need to consider the partial execution 
history up to that point. Once we have constructed the dynamic dependence graph for the given execution 
history, we can easily obtain the dynamic slice for a variable, var, by first finding the node that corresponds 
to the last defini­tion of uar in the execution history, and then finding all nodes in the graph reachable 
from that node. Dy­namicSlice can be defined precisely as follows: DynamicSlice(hist, var) = ReachableNodes(x, 
DynamicDep(hist))uWmD(var, hist) For example, the shaded nodes in Figure 6 give the dynamic slice for 
variable Y at the end of the execu­tion for the testcase (N = 2, X = 4, 3). 3 Static Slicing with Pointers 
and Composite Variables In Section 2.1, we defined reaching definitions for scalar variables. A definition 
of a variable var at a statement S1 reaches its use at statement Szif there is a path from S1 to S2 in 
the flow-graph of the pro­gram, and no other node along the path defines var. But what happens if S1 
defines an array element A[i], and S2 uses an array element A[j]; if S1 defines a field of a record s.j, 
and S2 uses the whole record s; or if S1 defines a variable X, and S2 uses a pointer dereference expression 
*p? To be able to handle such situations, we introduce below the notion of intersection of two l-valued 
expressions. Intersection of L-valued Expressions An expression is said to be an l-valued expression 
if a memory location can be associated with it. A simple check to find if an expression is an l-valued 
expression is to check if it can appear on the left hand side of an assignment statement. For example, 
expressions var, A[i], s.f, B[i] .T. X, *P, are all l-valued expressions. On the other hand, none of 
the expressions 103, z + g, and a > b, is l-valued. The presence of pointers and composite variables 
such as arrays and records in a programming language requires that both use and def sets of statements 
be defined in terms of l-valued expressions. A use expression el is said to intersect with a def expression 
e2, if the memory location associated with el may overlap with that associated with e2. We iden­tify 
three types of intersections between l-valued ex­pressions: complete intersection, maybe intersection, 
and partial intersection. We informally describe these below: Complete Intersection A use expression 
el completely intersects a def ex­pression ez if the memory location associated with el is totally contained 
in that associated with ez. For example, consider the following code fragment: Sl: x:= ... . S2: .. ...x... 
 Here, use of variable X at S2 completely intersects its definition at S 1. Also, in the following code 
fragment, Sl: s:= ... . S2: .. ... S.f ... use of field s.f at S2 completely intersects the defi­nition 
of record s at S 1. Consider, for example, the program in Figure 1. In this program, use of the vari­able 
day.count.since-eternity on line 85 completely in­tersects with each of its definitions on lines 78 82; 
use of the field date. day-of-the.year on line 89 completely intersects its definitions on lines 71 and 
75; and use of the field date. year on line 65 completely intersects its definition on line 50. Maybe 
Intersection Consider the following situation: Sl: A[i] := ... . S2: .. ...Ah] ... Whether or not the 
use of A[j] at S2 intersects with the definition of A[i] at S1 depends on the actual val­ues of variables 
i and j at statements S 1 and S2, re­spectively. If their values are the same, the two ex­pressions intersect, 
otherwise they do not. We refer to such intersections as maybe intersections. Use of pointer dereferencing 
also causes maybe intersections. In the following code fragment, Sl: *p:= ... S2: := . . .x... use of 
variable X at S2 maybe-intersects with the def­inition at S1 because the pointer variable p may or may 
not be pointing at variable X. Consider again, for example, the program in Figure 1. Use of the ar­ray 
element month-days-table~] on line 75 maybe in­tersects with each of the definitions on lines 55, 57, 
60, 62, 66, and 68. Partial Intersection Consider the following scenario: Sl: S.f:=... . S2: .. ... s 
... The whole record s is used at S2, but only one of its fields is defined at S1. A similar situation 
occurs if an array is used at S2, and only one of its elements is de­fined at S1. We refer to such intersections 
as partial intersections. If a use expression e 1 partially inter­sects with a def expression ez, we 
define PreEXp (el, ez) to be the portion of the memory location associ­ated with el that lies before 
that associated with ez, and ~ost~zp(el, e2) to be the portion that lies after that associated with e2. 
For example, if a statement assigns a value to the element a[4] of an array a de­clared as array[l.. 
10] of integer, and another statement uses the whole array a; then the use of a at the lat­ter statement 
partially intersects the definition at the former. Also, in this case, we have PreExp(a, a[4]) = a[l..3] 
and PostExp(a, a[4]) = a[5..lO]. Static Reaching Definitions Revisited Let CompleteIntersect, May beIntersect, 
and Partial-Intersect be boolean functions that determine if two l-valued expressions have complete, 
maybe, or partial intersections, respectively. We can now extend our definition of SRD, defined in Section 
2.1 for programs involving only scalar variables to that involving point­ers and composite variables. 
Figure 7 shows the new definition. Note that maybe and partial intersections may oc­cur together. For 
example, consider the following sit­uat ion: Sl: A[i].f := . . . S2: .. ... Ah] ... Because we check 
for maybe intersection before par­tial intersection, the former takes precedence over the latter whenever 
they occur together. SRD(var, n, F) = let Y = (V, A) n U(C,?a)~A if clef(z) = ~ then SRD(var, z, (V, 
A {(z,n)})) else /et clef(z) = {var }, A = A {(z,n)} in if CompieteIntersect( var, var ) then {z} else 
if May beIntersect(var, var ) then {z} U SRD(var, z, (V, A )) eke if PartialIntersect (var, var ) then 
{z} U SRD(PreEzp(var, var ), z, (V, A )) U SRD(PosiEzp(var, var ), z, (V, A )) else SRD(var, x, (V, A 
)) Figure 7: The Static Reaching Definition Function SRD. The definitions of data dependence, control 
depen­dence, and a static slice remain the same as given in Section 2.1. Only we now use the new definition 
of SRD, shown in Figure 7, to find the data dependence edges in the program dependence graph. The static 
slice shown in Figure 1 was obtained using the new definition. 4 Dynamic Slicing with Point­ers and Composite 
Variables Dynamic slicing differs from static slicing in that the former has no maybe intersections. 
This implies that for each use of a scalar variable, there is at most one dynamic reaching definition; 
and for each use of a composite variable, there is at most one dynamic reaching definition of each of 
its scalar components. To define dynamic slices in the presence of compos­ite variables and pointers, 
we generalize the notion of an l-valued expression to that of a memory cell. A memory cell is a tuple 
(adr, ien) where adr rep­resents its address in memory, and Zen represents its length in bytes.7 The 
memory-cell corresponding to an l-valued expression exp is given by the tuple (Ad­ dressOf(ezp), Size 
Of(exp)), where Address Of(exp) gives the current address associated with the l-valued expression ezp 
at runtime, and Size Of (ezp) gives the number of bytes required to store the value of exp. We now define 
use and def sets of all simple statements 7Actually, [en is defined in terms of the smallest addressable 
memory unit on the computer a bit, a byte, or a word. For languages where memory allocation for a variable 
is not neces­sarily contiguous, definition of a memory-cell may be appropri­ately changed to include 
the set of all its contiguous sub-cells. and predicates in terms of memory cells instead of l-valued 
expressions. Though the length component of these memory-cells may be determined at compile time, the 
address components may have to be deter­mined at runtime just before the corresponding simple statement 
or predicate is executed. Also, instead of determining intersection of l-valued expressions, we now check 
if two memory cells inter­ sect. Using this formulation, we redefine the DRD function, as shown in Figure 
8. CellIntersect(useCell, defC ell) returns true if there is any overlap between the two cells. Pre 
Cell and PostCell return the non-overlapping portions of the use Cell that lie before and after the overlapping 
por­tion, respectively. It is possible that one or both of these portions may be empty (/en = O). The 
case when both pre-and post-cells are empty is analo­gous to complete intersection in static slicing; 
the case when one or both are non-empty is analogous to par­tial intersection; and, as we mentioned earlier, 
there are no maybe intersections in the dynamic case. The advantage of this formulation is that all the 
usual problems associated with handling pointers in the static case are automatically taken care of in 
the dynamic case because all use and def sets are resolved in terms of memor y cells, and there is no 
ambiguity in determining if two memory cells overlap. As in static slicing, the definitions of data dependence, 
control de­pendence, and dynamic slice remain the same as given in Section 2.2. Only the definition of 
dynamic reach­ing definitions has changed. The dynamic slice shown in Figure 2 was obtained using the 
new definition. For another example, consider the simple program in Figure 9. It initializes an array 
a, reads three val­ues i, j, and k, and increments the ith, jth and kth DRD(cell, <>) = ~ DRD((adr, 
O), hist) = # DRD(ceil, hist) = let hist = <prevhz st I next> in if def(nezd) = ~ then DRD(cell, prevhist) 
 else let rtef(nezt) = {cell } in if CellIntersect(cell, cell ) then {next} U DRD(PreCell(cell, cell 
), prevhist) U DRD(PostCell(cell, cell ), prevhist) else DRD(cell, prevhist) Figure 8: The Dynamic Reaching 
Definition Function DRD. elements of the array, accessing these elements indi-5 Interprocedural Dynamic 
rectly via pointers p, q, and r, respectively. Figure 9 Slicing also shows the dynamic slice with respect 
to afi] on line 29,for the testcase (i= 1,j= 3, k= 3)at The dynamic slicing approach described above 
can the end of the program execution. The corresponding be easily extended to handle programs with proce­ 
static slice, on the other hand, would have included dures and functions. We first consider the case 
when the whole program. parameters are passed by value, as in C. In this case, we simply need to treat 
a procedure invocation, Figure 10 shows a variant of the above program proc(actuall, actualz, . . . . 
actualn), to be a sequence where a loop is used to initialize the array instead of assignments formali 
= actua[i, 1 ~ i ~ n, where of using a separate assignment for each array ele­f Ormdiis the ith formal 
parameter of proc. The use ment. If we execute this program for the same test­set of each of these assignments 
is computed in terms case (i = 1, j = 3, k = 3), we get the following out­ of memory-cells just before 
the procedure is invoked, put: a[l] = 2, a[3] = 4, a[lO] = O. Instead of printing and the def set is 
computed just after the control the value of a[3] it prints that of a [10]. This implies enters the procedure 
because memory cells that cor­ that the value of k somehow got corrupted during the respond to def sets 
belong to the current activation program execution. If we obtain the dynamic slice record of proc on 
the stack. of k on line 27, we would expect only line 8 to be in the slice as that is the only place 
in the program Figure 11 includes a variant of the program in Fig­where k is modified. Instead, we find 
that the loop on ure 2 where segments of code in the main program lines 10 17 is included in the dynamic 
slice, as shown have been moved inside procedures. Figure 11 also in Figure 10. This suggests that the 
variable k was shows the interprocedural dynamic slice with respect clobbered during the execution of 
the loop. Further to date. day-of.the_year on line 91 for the same testcase examination reveals that 
the fault indeed lies with the used to obtain the dynamic slice in Figure 2. loop predicate: it iterates 
ten times when the array Note that unlike the case of interprocedural static is declared to be only eight 
elements long. Figure 12 slicing [11, 12], our approach for dynamic slicing does shows the memory allocation 
made by our compiler not require that we determine which global variables for all variables along with 
their contents at the end of are referenced inside a procedure, or which variables the program execution 
for the above testcases. Note may be aliases to each other, nor do we need to elim­that the memory location 
that corresponds to k in-inate name conflicts among variables in different pro­deed overlaps with that 
of a[9] ! It is situations like cedures. this where precise dynamic analysis in terms of mem­ Call-by-reference 
is even easier to handle: no initial ory cells is invaluable in revealing the fault. assignments to formal 
parameters need to be made. The address of a formal parameter variable is auto­matically resolved to 
that of the corresponding actual parameter. Call-by-result parameter passing is han­dled by making assignments, 
d,udi = formal~, just 8 Memory allocation will vary from compiler to compiler. before control returns 
to the calling procedure. Call­ .---........ ------~-..­ * 1 main( ) 2 3 [ 4 5 imt aIIOl, L J, k, *P, 
*q, *r; 6 7 a[Ol = 0: a[il = 1; : a[21 = 2; 10 11 a[41 = 4; 12 a[51 = 51 13 a[61 = 6; a[71 = 7S ii 8[81 
E a; 16 a[91 9; 17 18 prinkf ( Enter i. 19 20 21 22 :: *Y print, f( a[%dl = %d, a[%dl = Zd, at%dl E %d\n 
, i, a[il. J, m k. aCkl): 30 31 } /-L static analysis )( spprox. dynamic analysis )~ ~~~EEmEIE@EEl~ mmE!ElmEEEICEClbEKICzmCl 
q J > dynanic progran slice on a[j] at line r, .. . ..-. 29 r-,.. --­ *. 1 E Figure 9: Dynamic slice 
with respect to ah] on line 29 for the testcase (i = 1, j = 3, k = 3). by-value-result can be handled 
similarly by making appropriate assignments both at the beginning and the end of the procedure. 6 Related 
Work The concept of static program slicing was first pro­posed by Weiser [19, 20]. Ottenstein and Ottenstein 
later presented an algorithm in terms of graph reach­ability in the program dependence graph, but they 
only considered the intraprocedural case [17]. Hor­witz, Reps, and Binkley extended the program de­pendence 
graph representation to what they call the system dependence graph to find interprocedural static slices 
under the same graph-reachability frame­work [11]. Bergeretti and Carr6 have also defined information-flow 
relations somewhat similar to data­and control dependence relations, that can be used to obtain static 
program slices (referred to as par­tial statements by them) [6]. Podgurski and Clark have extended the 
regular notion of control depen­dence (which they refer to as strong control depen­dence ) to weak control 
dependence that includes interst at ement dependencies involving program non­termination [18]. Uses of 
program slicing have also been suggested in many other applications, e.g., pro­gram verification, testing, 
maintenance, automatic parallelization of program execution, and automatic integration of program versions 
(see, e.g., [20, 6, 10]). Korel and Laski extended Weiser s static slicing al­gorithms for the dynamic 
case [13]. Their definition requires that if any one occurrence of a statement in the execution history 
is included in the slice then all other occurrences of that statement be automatically included in the 
slice. For example, if the program in Figure 3 is executed for the testcase (N = 2, X = 4, 3), and we 
find the dynamic slice for the variable Y at statement S9 during the second iteration, their defini­tion 
will require that statement S7 be also included in the dynamic slice, even though the current value of 
Y is totally unaffected by its execution. Miller and Choi also use a dynamic dependence graph to perform 
flow­back analysis [5] in their Parallel Program Debugger PPD [15]. The last two approaches also treat 
array elements as separate variables. But as they do not resolve use and def sets in terms of memory 
cells, they will fail to detect interstatement dependencies like that illustrated in Figure 10. Also 
our approach provides a uniform way to handling pointers and all types of composite variables. /u17/ha/v2/deim/bugpt.r 
.C m nain( i ; 3t : M. 1, J, k, a[81, 1, *p, *q, *r; prL-tf( Ent.er i, J, k, {O <a i,J,k < 10>: ); 
scanf{ %d %d %d , &#38;i. &#38;j, !lk}; i IHRMm!m 13 :: 17 * 1 :: 2(I 21 a p 8 &#38;a[i]; % 24 25 26 
27 +Q a *p += 1; win~r( a[%dl = %cl, at%cll = %&#38; a[%dl s Xd\n* , i. a[il, j, a[jl, ~ a[kl); H 30 
31 3 \ stat3c enslysis )( sppmx. dynamic snalysis )(~b ~~~~~~ Xm=mmmmml -1 >dynanic program slice on 
 k aL line 27 Current TesLcase #: 1 Figure 10: Dynamic slice with respect to k on line 27 for the testcase 
(i = 1, j = 3, k = 3). 7 Conclusions helping us write IATEX macros for generating them. Static program 
slices tend to be large and imprecise when the program being debugged involves pointers References and 
composite variables such as arrays, records, and unions. They lose their usefulness altogether if the 
[1] Hiralal Agrawal. Towards Automatic Debugging language involved is not strongly-typed and permits 
of Computer Programs. PhD thesis, Department use of unconstrained pointers. While debugging, how-of Computer 
Sciences, Purdue University, West ever, we normally have a concrete testcase that re-Lafayette, IN, 1991. 
veals the fault and we wish to analyze the program be­havior for that particular testcase. Dynamic program 
[2] Hiralal Agrawal, Richard A. DeMillo, and Eu­slices help us find interstatement dependencies for a 
gene H. Spafford. Efficient debugging with slic­given testcase. In this paper we have shown that we ing 
and backtracking. Technical Report SERC­can find accurate dynamic slices even in the presence TR-80-P, 
Software Engineering Research Center, of unconstrained pointers and composite variables. Purdue University, 
West Lafayette, IN, 1990. The approach outlined provides a uniform framework for handling pointers as 
well as various types of com-[3] Hiralal Agrawal, Richard A. DeMillo, and Eu­posite variables. It works 
even when the language is gene H. Spafford. An execution backtracking ap­not strongly-typed or even when 
no runtime checks proach to program debugging. IEEE Sofiware, (out-of-bound array element reference, 
illegal pointer pages 21 26, May 1991. dereference, etc.) are performed. [4] Hiralal Agrawal and Joseph 
R. Horgan. Dy­namic program slicing. In Proceedings of the Acknowledgments SIGPLA N990 Conference on 
Programming Lan­ guage Design and Implementation, White Plains, We would like to thank Michal Young for 
our discus-New York, June 1990. ACM SIGPLAN. SIG­sions with him on notations used in this paper and PLAN 
Notices, 25(6):246 256, June 1990. ----. ... . ---------------=.- I 45 nonbh-days-t.able[il s 31; 46 
47 if ((year % 4 .= O W year % 100 != O) II (yea­ % 400 == O)) 48 nont,h-days-kable [ll = 29; else :: 
nont,h-daysAable [ll = 28; 51 3 52 53 54 void read. datApDat.e) DaLeTgpe *pLlaLe; 55 [ 56 57 58 59 60 
void conpute-day.of-t,he-year {nonth-days-table, pDate) 61 62 inL nonth-days-table[ LlaLeTgpe *pDaLe; 
1; 63 f 64 int, i; 65 66 67 for (i = O; i < pllate->nonth -1; i++) 68 pIlaLe->day-ofAhe-year += nonLh-dags-table[i 
1> 69 z 70 71 void conpute.day.counLsince-eternity(pDate, pCount ) 72 ClaLcType =pllate; 73 int *pCount.; 
74 [ 75 76 *pCount *pCount = 365 * (pLlat,e->year += (ptlate->ysar -1) -1): / 4: 77 *pCount. -= {pOaLe->year 
-1) / 100: 78 79 80 1 *pCount, *pCounb += += (pDaLe->gear pOaLe->day-of -1) / 400; -the-year: 81 82 main{ 
) 83 [ 84 05 86 87 conpuke-day-count-since-et.ernit.y (&#38;date, 8day-count-since-st ernity); 88 dat,e. 
daymfJ.he-ueek $$ dag-count-since-et.ernit.y % 7; 89 90 +9 printf( day of tha yesr for the date %d/%d/%d 
is %d .\n , dab.nonLh, 91 dat.e.dag, date. year, 92 print. -day-of-the-ueek {data. day-of-the-week>; 
93 3 static smdysis )( spprox. dynamic smlysis ) \ ~~~Ez@@lE@El~ =ECIEEIECIIEKIECEEIIZZ=[ quit 1 > dynanic 
progran slice on daLe.dau-ofAhe-year ak line 91 Current Testcase #: 1 Figure 11: Dynamic slice with 
respect to date. day-of-the-year at line 91 for the testcase (month= 1, day= 1, year= 1990).  [5] R. 
M. Balzer. Exdams: Extendible debugging and monitoring system. In AFIPS Proceed­ings, Spring Joint Computer 
Conference, vol­ume 34, pages 567 580, Montvale, New Jersey, 1969. AFIPS Press. [8] [6] Jean-Francois 
Bergeretti and Bernard A. Carr6. Information-flow and data-flow analysis of while programs. ACM Transactions 
on Programming Languages and Systems, 7(1):37 61, January 1985. [9] [7] David R. Chase, Mark Wegman, 
and F. Kenneth Zadeck. Analysis of pointers and structures. In Proceedings of the SIGPLAN 90 Conference 
on Programming Language Design and Implementa­tion, White Plains, New York, June 1990. ACM [10] SIGPLAN. 
June 1990. SIGPLAN Notices, 25(6):296-310, Jeanne Warren. Ferrante, The Karl program J. Ottenstein, dependence 
and graph Joe and D. its uses in optimization. ACM Transactions on Pro­gramming Languages and Systems, 
9(3):319 349, July 1987. Susan Horwitz, Phil Pfeiffer, and Thomas Reps. Dependence analysis for pointer 
variables. In Proceedings of the SIGPLAN 89 Conference on Programming Language Design and Implementa­tion, 
Portland, OR, June 1989. ACM SIGPLAN. SIGPLAN Notices, 24(7):28-40, July 1989. Susan Horwitz, Jan Prins, 
and Thomas Reps. Integrating noninterfering versions of programs. address contents symbolic name 1000 
1068 r 1004 0 1008 1044 q 1012 0 1016 1036 P 1020 0 1024 10 1 1028 0 1032 0 a[O] 1036 2 a[l] 
1040 2 a[2] 1044 4 a[3] 1048 4 a[4] 1052 5 a[5] 1056 6 a[6] 1060 7 a[7] 1064 8 1068 k 9 1072 
 0 1076 3 1080 0 1084 1 Figure 12: Storage layout of theprogram in Figure10 at the end of the program 
execution for the testcase (i=l, j=3, k=3). ACM Transactions on Programming Languages and Systems, 11(3):345 
387, July 1989. [11] Susan Horwitz, Thomas Reps, and David Binke­ley. Interprocedural slicing using 
dependence graphs. A CJ4 Transactions on Programming Languages and Systems, 12(1):26 60, January 1990. 
 [12] J. C. Hwang, M. W. Du, and C. R. Chou. Finding program slices for recursive procedures. In Proceedings 
of COMPSAC, pages 220-227. IEEE, October 1988. [13] Bogdan Korel and Janusz Laski. Dynamic pro­gram slicing. 
Information Processing Letters, 29:155-163, October 1988. [14] J. R. Larus and P. N. Hilfinger. Detecting 
con­flicts between structure accesses. In Proceed­ings of the SIGPLAN 88 Conference on Pro­gramming Language 
Design and Implementation. ACM SIGPLAN, July 1988. SIGPLAN Notices, 23(7):21-34, July 1988. [15] Barton 
P. Miller and Jong-Deok Choi. A mecha­nism for efficient debugging of parallel programs. In Proceedings 
of the SIGPLAN 88 Conference on Programming Language Design and Imple­mentation, Atlanta, GA, June 1988. 
ACM SIG-PLAN. SIGPLAN Notices, 23(7):135-144, July 1988. [16] Robin Milner, Mads Tofte, and Robert Harper. 
The Definition of Standard ML. The MIT Press, Cambridge, MA, 1990. [17] Karl J. Ottenstein and Linda 
M. Ottenstein. The program dependence graph in a soft­ware development environment. In Proceed­ings of 
the ACM SIGSOFT/SIGPLAN Sympo­sium on Practical Sofiware Development Envi­ ronments, Pittsburgh, PA, 
April 1984. ACM SIGSOFT/SIGPLAN. SIGPLAN Notices, 19(5):177-184, May 1984. [18] Andy Podgurski and Lori 
A. Clarke. A formal model of program dependence and its implica­tions for software testing, debugging, 
and main­tenance. IEEE Transactions on Software Engi­neering, 16(9):965 979, September 1990. [19] Mark 
Weiser. Programmers use slices when debugging. Communications of the ACM, 25(7):446-452, July 1982. [20] 
Mark Weiser. Program slicing. lEEE Trans­actions on Software Engineering, SE-10 (4):352 357, July 1984. 
 
			