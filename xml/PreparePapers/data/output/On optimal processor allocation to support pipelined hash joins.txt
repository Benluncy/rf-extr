
 ON OPTIMAL PROCESSOR ALLOCATION TO SUPPORT PIPELINED HASH JOINS Ming-Ling Lo; Ming-Syan Chent, C. V. 
Ravishankar* and Philip S. Yut EECS Department* IBM Thomas J. Watson Research Centert University of Michigan 
at Ann Arbor P. O. Box 704 Ann Arbor, MI 48109 Yorktown Heights, NY 10598 Abstract Among various join 
methods, the hash join has been the focus of much research effort and reported to have In this paper, 
we develop algorithms to achieve optimal pro­ performance superior to that of others, particularly cessor 
allocation for pipelined hash joins in a multiprocessor­because it presents an opportunity for pipelining 
[4] based database system. A pipeline of hash joins is composed of several stages, each of which is associated 
with one join op-[13] [15] [18]. Using hash joins, multiple joins can eration. The whole pipeline is 
executed in two phases: (1) be pipelined so that the early resulting tuples from a the table-building 
phase, and (2) the tuple-probing phase. join, before the whole join is completed, can be sent We focus 
on the problem of allocating processors to the to the next join for processing. A pipeline of hash stages 
of a pipeline to minimize the query execution time. joins is composed of several stages, each of which 
is We formulate the processor allocation problem as a two­ associated with one join operation that can 
be executed, phase mini-max optimization problem, and develop three in parallel, by several processors. 
Though pipelining has optimal allocation schemes under three different constraints. been shown to be 
very effective in reducing the queryThe effectiveness of our problem formulation and solution execution 
time, prior studies on pipelined hash joinsis verified through a detailed tuple-by-tuple simulation of 
have focused mainly on heuristic methods for querypipelined hash joins. Our solution scheme is general 
and plan generation. Most of the prior work on queryapplicable to any optimal resource allocation problem 
for­mulated as a two-phase mini-max problem. plan generation, such as static right-deep scheduling, dynamic 
bottom-up scheduling [16], and segmented right-deep trees [2] 1, resorted to simple heuristics to 1 Introduction 
allocate processors to pipeline stages. Also, these In recent years, multiprocessor-based parallel database 
methods dealt with only memory as a constraint for machines have attracted considerable attention from 
the execution of pipelined hash joins. Little effort both the academic and industrial communities because 
waa made to take processing power into consideration they can efficiently execute complex database opera-and 
optimize processor allocation. Notice that two tions [1] [5] [10] [17]. In relational database systems, 
opportunities for parallelism exist for pipelined hash joins are the most expensive operations to execute, 
espe-joins: Not only can each hash join be implemented cially with the increases in database size and 
query com-by several processors, but also several joins can be plexity [3] [14] [19]. Many applications 
usually need to pipelined. As a result, processor allocation arises as specify the desired results in 
terms of multi-join queries, an important unexplored issue in the performance of some of which may take 
hours or even days to complete. pipelined haah joins. In view of this fact and the As a result, parallelism 
has been recognized as the only increasing demand for better performance of database solution for the 
efficient execution of multi-join queries operations, the objective of this paper is to study and for 
future database management [6] [7] [8] [12]. improve processor allocation for pipelined hash joins. In 
this paper we derive optimal processor allocation *Authors in this work were supported in part by the 
Censor. algorithms that take both memory constraint and tium for International Earth Science Information 
Networking. processing power into account. We assume that a Permission to copy without fee all or part 
of this material is pipeline of hash joins is given a priori, which can be granted provided that the 
copies are not made or distributed for generated based on the approaches in [2] [16]. A pipeline direct 
commercial advantage, the ACM copyright notice and the of hash joins is sequentially executed in two 
phases: title of the publication and its date appear, and notice is given that copying is by permission 
of the Association for Computing Machinsry. To copy othsrwise, or to republish, rsquires a fae 1SeWented 
right-deep trees are bushy trees with right-deep and/or specific permission. subtrees [2]. SIGMOD /5/93/Washington, 
DC, USA e 1993 ACM 0-89791 -592-5 /93/0005 /0069 . ..$1 .5o (1) the table-building phase and (2) the 
tuple-probing phase. In the table-building phase, hash tables are constructed from inner relations using 
hash functions on join attributes. In the tuple-probing phase, tuples of the outer relation are used 
to probe the hash tables for matches. Note that the processing time of each phase is determined by the 
maximal execution time among all stages, and that the same allocation of processors to a stage is retained 
across the table-building and tuple­probing phases. The execution time of a pipeline is thus the sum 
of two correlated msxima. The characteristics of pipelined hash joins allow the processor allocation 
problem to be formulated as a two-phase mini-max optimization problem. Specifically, for a pipeline with 
k stages, the execution time of the pipeline, TS, can be expressed as where WBi and WP; are, respectively, 
the workloads for the table-building and tuple-probing phases in stage i. Consequently, the processor 
allocation problem for pipelined hash joins can be stated as follows: Given WB~ and WP~, O < i < k 1, 
determine the processor allocation (n~) = (no, nl, . . ., nk 1 , so as to minimize ) TS in Eq. ( 1), 
where ni is the number of processors allocated to stage i. For example, consider the workloads shown 
in Table 1 for a pipeline of five stages. First, it is observed that the workloads of stage 2 are less 
than those of stage 3, suggesting that stage 3 should be assigned more processors than stage 2. However, 
stage 3 has a heavier load in the table-building phase than stage 4, while the latter has a heavier load 
in the tuple-probing phase. In such a configuration, there is no obvious way to allocate processors to 
minimize the pipeline execution time specified in Eq. (1). For an illustrative purpose, suppose the total 
number of processors available to execute the pipeline in Table 1 is 20. It can be seen that the allocation 
(ni)= (4, 4, 4, 4, 4) leads p = 1.2, and TS = 2.7, to max~~ *= 1.5, maxvi ~ whereas the one (ni)= (6, 
3, 3, 6, 2), which is based on the workloads of the table-building phase, leads to maxvt *= 1.0, maxvi 
%= 2.5, and TS = 3.5. Clearly, to develop an optimal processor allocation to minimize TS in Eq. ( 1) 
is in general a very difficult and important problem. Since the table-building and tuple-probing phases 
are executed one after the other, we minimize the sum of two correlated maxima in Eq.(1). In view of 
this, the optimal processor allocation problem in Eq. ( 1) is hence termed the two-phase mini­max optimization 
problem. This feature distinguishes our allocation problem from other conventional resource allocation 
problems [9], which may be considered one- Table 1: The workloads in two phases of each stage. phase 
mini-max optimization problems. To develop the optimal processor allocation scheme for the two-phase 
mini-max optimization, we consider the following three constraints: (1) the total number of processors 
available for allocation is fixed, (2) a lower bound is imposed on the number of processors required 
for each stage to meet the corresponding memory requirement, and (3) processors are available only in 
discrete units. We develop solution schemes by incrementally adding constraints to our optimization problem. 
Specifically, three optimal processor allocation schemes are devised, i.e., one under Constraint (l), 
another under Constraints (1) and (2), and the third under all the three constraints. The three allocation 
schemes will be shown to be optimal under their corresponding constraints. Also, it can be verified that, 
for a system with N processors and k pipeline stages, the time complexity of the first two allocation 
schemes is O(k.log k), and that of the third one is O(Nk.log(Nk)). Generally speaking, a complex real 
world problem such as pipelining usually needs to be simplified and ab­stracted before it can be mapped 
into a mathematical model to further analyze and optimize. The contribu­tions of this study are twofold. 
We not only formulate the optimal processor allocation problem for pipelined hash joins as a two-phase 
min-max problem, but also de­rive solutions to this new class of optimizat ion problem. We verify the 
effectiveness of our problem formulation and solution procedure through a detailed simulation of pipelined 
hash joins. Simulation results show that the proposed allocation schemes lead to much shorter query 
execution times than conventional approaches. It is worth mentioning that our solution scheme is gen­eral 
and applicable to any optimal resource allocation problem formulated as a two-phase mini-max problem. 
Although the one-phase mini-max optimization prob­lem has been studied extensively in the literature, 
this study, to the best of our knowledge, provides the first solution to the two-phase mini-max optimization 
prob­lem. This paper is organized as follows. Section 2 provides the background and the problem description. 
Section 3 develops three algorithms for deriving three optimal processor allocations. Proofs of lemmas 
and theorems can be found in [11]. In Section 41 we demonstrate the performance improvement achieved 
by our proposed 70 n3 =4 scheme via simulation. The paper concludes with h1=2 Section 5. processing 
nodes n~2 k=3 o 0 h2=l 2 Preliminaries 2.1 Notation, Assumption and Definition As in most prior work, 
we assume that execution time is the primary cost measure for estimating the efficiency of database 
operations. The architecture assumed is a multiprocessor system with distributed memory and shared disks. 
Each processing node (or processor) in the system has its own memory and address space, and communicates 
with others through message passing. Each node is assumed to have the same amount of memory, and the 
amount of memory available to execute a join is in proportion to the number of processors involved. The 
disks in the system are accessible by all nodes through shared 1/0 buses. A pipeline of hash joins is 
composed of several stages, each of which is associated with one join operation. The relation in a hash 
join that is loaded into memory to build the hash table is called the inne~ relation, while the other 
relation, whose tuples are used to probe the hash table, is called the outer relation. The inner relations 
of a pipeline are the inner relations of its stages. The outer relation of a pipeline is defined to be 
the outer relation of its first stage. In the table­building phase, the hash tables of the inner relations 
are built using hash functions on join attributes. In the tuple-probing phase, tuples of the outer relation 
are first probed, one by one, against the entries in the hash table of the first stage using the corresponding 
hash function. If there are matches, the resulting tuples are generated, and then sent to the next stage 
for similar processing. The table-building and tuple-probing times of a pipeline are the time spans of 
the building and probing phases respectively. The execution of one pipeline is given in Figure 1 for 
illustration. 2.2 Problem Formulation As pointed out earlier, the query processing time can be approximated 
as Tin, + m-a ~ + maxvi ~, where ni is the number of nodes allocated to stage i and Tan8 is the sum of 
those costs independent of processor allocation. It is also derived in [2] that Wl?a is proportional 
to I&#38;1, where [&#38;[ is the size of the inner relation &#38; at stage z of the pipeline2. Since 
Tin8 is constant over all processor allocations, the objective function that we shall minimize is TS 
= maxvl ~ + n-lax., *, the sum of costs dependent on processor allocation. In what follows, we shall 
concentrate on deriving the optimal processor allocation, A = ZTfi~ ~elationstip between WBi and lRi 
I is u.$ef~ in o~ derivation for optimal allocation in Section 3.2 later. N=9 3 n1=3 n packets h3=3 
1 R3\ R. -G s R2 ----, R, I II I@ I ~;; (O../ s ...* .. !00,.<;<, u ... ;ou~i> :O.! \ I .. IL 1\ 
I. ----I stags 2 stage 1 ------  I . !Ocl!..e l l stage 3 Figure 1: Execution of one pipeline. (no, 
nl, . . . . nk 1), to minimize TS, where k is the number of stages in the pipeline. Note that the formula 
for TS does not depend on the order of stages in the pipeline. The workloads of the various stages in 
the pipeline determine the allocation. As mentioned earlier, we approach this problem by expressing it 
as an allocation problem with three constraints. We start with assuming only one constraint holds, and 
incrementally consider more constraints. Constraint I (no idling): The total number of proces­sors assigned 
to all stages is equal to the total num­ber of processors in the system, i.e., ~~~~ ni = N. Constraint 
II (sufficient memory): The amount of memory allocated to each stage must be large enough to accommodate 
the haah table of that stage, i.e., ni z ~, for all i, where M is the memory size of each processor. 
Constraint III (discrete allocation): Processors must be allocated to stages in discrete units. 3 Optimal 
Processor Allocation We shall develop three optimal processor allocation schemes in this section. The 
scheme in Section 3.1 corresponds to Constraint I (denoted by AI), that in Section 3.2 is subject to 
Constraints I and II (denoted by Arr), and the one in Section 3.3 satisfies all the three constraints 
(denoted by AIII ). 3.1 Allocation under Constraint Under Constraint I, ni, the number of processors 
allocated to stage i, could be any positive real number. We obtain the following necessary condition 
for Ax, stating that each stage under JI must be a bottleneck in either the table-building or the tuple-probing 
phase. This is referred to as the two phase allocation condition. Lemma 1: Let TBi and TP~ be, respectively, 
the table-building and tuple-probing times for a stage i under AI. Therl, either TBi = TB, or TP~ = TP, 
where TB and TP are the pipeline building and probing times, respectively. Since this lemma only defines 
the two phase allocation condition as a necessary condition, there may be some non-optimal allocations 
which also satisfy this condition. For every allocation, we identify an ordered pair of sets r = (A, 
B) such that, WB~ A = {stage i I = TB}, (2) n~ p~ = TP}, B = {stage j [ (3) nj ~ WP where TB = maxvi 
and TP = maxvi . Specifically, A (respecti~ely, B) is the set of st;~es that are bottlenecks in the table-building 
(respectively, tuple-probing) phase. Define Pj TP, j @A}. (4) B =B (A rlB)={stagejl TJj It follows from 
Lemma 1 that AUB comprises all stages in the pipeline. The problem of finding the optimal allocation 
can now be reduced to that of finding all (A, B), or (A, B ), satisfying the two phase allocation requirement 
in Eqs. (2) -(4), and then determining the one with the shortest processing time. To avoid the exponential 
complexity of enumerating all possible two phase allocations, we shall first intro­ duce the concept 
of workload ratios (B/P ratios) for the pipeline stages, and then in light of this concept, prove that 
the number of allocations needed to consider is no more than the number of pipeline stages. The B/P ra­ 
tio for stage i is defined as ri = ~. We order the stages in descending order of their B/P ratios, and 
de­ note the sequence as 6 = (so, al, ..., ah-l). For the example profile in Table 1, we obtain Table 
2 where stages are sorted according to their B/P ratios. Let WBr = ~iE1 WBi, WPI = ~iel WPi, and nr = 
~a=f ni, where I denotes A, B or B . An example of B can be found in Figure 2. Then, under the two phase 
allocation condition, we have (5) TP=~=~=wpB ~, Vj E B. (6) ?lj nB ao=3 al=O a2=2 as=l a4=4 WB;6 633 
2 wPi3424 5 r~ 2 1.5 1.5 0.75 0.4 Table 2: The workloads in two phases of each stage after sorting. Tuple 
,......... . .. . ...... . .. . . ....!.... stage 3 stage O stsge 2 descending order of BIP . Figure 
2: Example of a candidate ordered pair r = (A, B) for a pipeline of 5 stages. The pipeline processing 
time can be expressed as: TS. max~+max~. ViEA n~ VaEB n~ Then, we get, where the total number of processors 
~ = nA + nB t. If we select an A, thus fixing W BA and W PB,, TS can be expressed as a function of nA. 
Note that there is an upper bound and a lower bound on the amount of processors that one can assign as 
nA (and nBf) for a given pair of A and B. The reason is that Eq. (7) is valid only when A and B satisfy 
the conditions that they are the bottleneck sets in the table­building and the tuple-probing phases, 
respectively. If for a particular pair of sets A and B (and the associated WBA and WPBJ ), we allocate 
so many processors to stages in set A (i.e., assign so large a number to nA) that the table-building 
times of stages in set A become shorter than those of some stages in set B, A would not be the bottleneck 
set of the table-building phase any more, and Eq. (7) would no longer give the correct pipeline processing 
time TS. Under such an allocation of processors, the processing time TS will be determined Note that 
x-represents x 6 where 6 is an infinitesimal by different sets of Aand B (and hence different WBA quantity. 
Let n~ be the nA that minimizes TS under and WP~l ). Therefore, there is an upper bound on the constraints 
in Eq. (8). the amount of processors one can meaningfully assign to stages in set A, and so is there 
a lower bound on the Lemma 4: nf falls into the set of {inf(nA)$ Sup(nA), nfinc} amount of processor 
assignable to stages in set A since ~ = nA + nBl. This phenomenon is formally stated below. Obviously, 
if n~nc lies between inf (nA ) and 5up(nA), Let a~ be the last stage (the stage with the smallest i.e. 
n~nc satisfies the constraints in Eq. (8), n~ would B/P ratio) in A, i.e., g = IAI 1, and ag+l be the 
first be equal to n~n . Otherwise, if n~nc is smaller than stage in B . inf (nA ), n~ is equal to inf(nA 
). on the other hand, if nfinc is greater than sup(nA ), n: is sup(nA ). The Lemma 2: Under the conditions 
that all stages in procedure PA1 to derive AI can now be summarized as set A (respectively, B) have the 
same table-building follows. time (respectively, tuple-probing time), if the stages in set A and those 
in set B are indeed the bottlenecks Procedure for allocation AI (PA1) in the table-building phase and 
tuple-probin $~h~, K!Z&#38;> respectively, i.e. ~~ ~,Vi~B and~~ 1. For each stage i, O ~ i < k 1, calculate 
its B/P WB ratio Ti= &#38;. ~, Vi E A, then nA ~ust satisfy the following * constraints 2. Sort the 
stages in descending order of ?i to get the sequence &#38; 3. For each j, 1 < j < k, let A = {aO, ....aY_J. 
and  find the nA that minimizes TS = ~+ ~ under and vice versa. the constraint From the above lemma, 
we get the following lemma to determine the optimal allocation for a given set A. Lemma 3: The optimal 
value of nA minimizes (based on Lemma 4). Choose the A and the ~ + ~ under the constraints in Eq. (8). 
associated nA that achieves the shortest processing time. Next we consider the nA that minimizes ~(nA 
) = 4. From this optimal set A and nA, obtain the number K&#38;+!K&#38; ~ _nA without any constraint. 
It can be derived nA of processors ni for each stage by taking * = O. The optimal allocation, n~ c, where 
the superscript onc indicates optimality under no constraints, is (9) This is allocation AI, The corresponding 
optimal total processing time with­out the constraints in Eq. (8) is given by example, the Table we have 
For from profile in 2, the following 4 possible configurations: A= {aO}, TS07ZC = ~ . (WBA + WP~, + 2~WBA 
. WPBJ). (10) {a~, al, az}, {aO, al, az, aq}, or {ao, al, az,a~,aq}. For A= {ao} (and B = {al, az, as, 
aq}), we have WBA=6 We next define inf(nA) and sup(nA ) as follows: and WPB/ = 15 from the rows WBi and 
WPi in Table 1 2, leading to TS= 2.375 by Eq. (7). Similarly, each inf(nA )= N . 7 (11) configuration 
of A determines a value of TS by Eq. 1+~~ (7). It can then be verified that from the 4 possible configurations 
of A, the minimum of TS, denoted by TSal, is 2.362 that is achieved for A= {so, al, az}. From 1 sup(nA) 
= N . (12) Eq. (9), we next have nAn4.51 and nBln 15.49, which ~ WPB, 1 + FP::;l WB~ ) leads to JI= 
(4.51, 3.88,2.25,4.51, 4.85) from Eq. (13). ( In PAI, the combined complexity of Steps 1 and 2 is O(lc 
. log k) because of the need of sorting. The complexity of Step 3 is O(k) since there are only k different 
ways of determining set A, and for each A, there are only 3 potential processor allocations that can 
be optimal according to Lemma 4. The complexity of PA1 is thus O(k . log k). Since PA1 considers all 
the allocations satisfying the necessary conditions stated in Lemmas 1 and 4, the allocation obtained 
AI is the optimal. Theorem 1: PAI determines the optimal processor allocation to the pipeline stages 
under Constraint I. 3.2 Allocation under Constraints I and 11 Next, we take into account Constraint 
II, which imposes a lower bound on the number of processors required for each stage. Let ~ be the number 
of nodes required to hold &#38;, i.e., ~ = ~. As mentioned earlier, WBa = C2 . ll&#38; 1 where C2 is 
a constant. Hence, Constraint 11 w w~f can be expressed as na ~ mi = ~ ~ or every stage i. We will show 
that taking Constraint II into consid­eration for processor allocation amounts to adding the condition 
nA ~ ~ into the inequality Eq. (8) de­rived in Section 3.1. The procedure PAII to derive AII can be summarized 
as follows. Procedure for allocation Arr (PArr) 1. For each stage i, O ~ i < k 1, calculate its B/P 
WB ratio Ti= &#38;. * 2. Sort the stages in descending order of ?i to get the sequence ii. 3. For each 
j, 1 < j < k, let A = {aO,..., aj_l}, and  find the nA that minimizes TS = ~+ ~ under the constraint 
WBA max(~2. M l+~w~.  wf ~)- N < (14) W&#38;9~I ~ 1 + WP., +, WBA Choose the A and the associated nA 
that achieves the shortest processing time. 4. from this optimal set A and nA, calculate the number of 
processors nz for each stage as in Eq. (13). This is allocation ~11. Also Lemma 4 holds if inf(nA) is 
defined as WBA iv inf (nA ) = max( (72-M 1+~~ ). (15) To show that the allocation ~11 satisfies constraint 
II, we need to prove the following two lemmas. Lemma 5: Under allocation ~11, every stage in A satisfies 
Constraint II. The lemma follows directly from Eq. (13) as nA ~ WB >WB w implies n~ nA~_&#38;. CSM Lemma 
6: Under allocation ~111 every stage in B satisfies Constraint II. For example, suppose (~) = (~)= (5.0, 
2.5, 2.5, 5.0, 1.7) for the profile in Table 1, with N = 20. We then have 2 possible configurations of 
A to meet Constraints I and II, i.e., A= {CZo,al, a2}, or {ao, al, a2, as}. From Eq. (7), it can be verified 
that these 2 configurations of A lead to TS=2.40 and TS=2.538, respectively. We hence get AII= (5.00, 
3.33,2.50,5.00, 4.17) and TS~rl= 2.40. Like PA1, PA1l is also of complexity O(k . log k). Following the 
same reasoning as for PAI, we have the theorem below, stating the optimality of PA1l. Theorem 2: PA1l 
determines the optimal processor allocation to the pipeline stages under Constraints I and II.  3.3 
Allocation under three constraints Given allocation AII, one straightforward approach to meet Constraint 
III is to round up the number of processors allocated to some stages, and truncate the number of processors 
for others. However, this simple approach, though applicable to some cases, does not in general yield 
the optimal allocation. It is noted that for certain configurations if the number of processors allocated 
to stage i by ~11 is na, the allocation to stage z by ~rrr can be very different from either ln~] or 
[nil. It is thus necessary to develop the procedure for the optimal allocation ~I1r. From Constraints 
II and III, it follows that stage must be allocated with at least r-l processors, where .,­ m.i = $#. 
If we allocate ~m.il processors to each stage i, there will be L = N ~vj [mjl remaining processors. To 
achieve ~rIr, we must allocate these L processors to appropriate stages. Clearly, the number of additional 
processors allocatable to a given stage, say stage i, is between O and L, and the total number of processors 
allocated to that stage can thus range from [nil to [wI +L. After each stage has got [ml processors 
to satisfy its memory constraint, we consider the issue on how to allocate the remaining L processors, 
The subtlety of the allocation comes from the fact that the optimal allocation of p processors cannot 
be obtained directly from a given optimal allocation of p 1 processors by greedily allocating the additional 
processor to the bottleneck stage. For example, consider a 3 stage pipeline with workload (WBi) = (1,1,2) 
and (WPi) = (10, 10, 2). Assume that each stage needs one processor to hold its hash table. Clearly, 
if there are 4 processors, the optimal allocation is (1, 1, 2), since allocating the extra processor 
to either stage 1 or stage 2 will not change the pipeline building and probing times. However, if there 
are 5 processors, the optimal allocation is (2, 2, 1), meaning that the optimal allocation of 5 processors 
cannot simply be obtained based on the optimal allocation of 4 processors. We therefore need to devise 
an efficient mechanism to determine the optimal allocation. Let TBf and TP~ be, respectively, the table-building 
and tuple-probing times of stage i, where z is the total number of processors allocated to this stage. 
For each allocation of w proces­sors to stage i, we then have a corresponding allocation descriptor, 
(i, z, TB~, TP~ ). We shall maintain certain data structures on the allocation descriptors to facilitate 
the allocation of the L additional processors. First, we order the possible allocation descriptors into 
two allocation queues, QB and QP, each of which consists of all k (L + 1)descriptors. Elements in QB 
and QP are sorted in decreasing order of TB~ and TP~, respectively. Note that each allocation descriptor 
will appear in both QB and QP, but the positions in which it appears in QB and QP may be different. Marking 
an allocation descriptor can be thought of as allocating one additional processor to the stage specified 
in that descriptor. Allocating L additional processors to the pipeline stages then corresponds to ma~king 
L distinct allocation descriptors. Our method of marking allocation descriptors is to follow the descriptor 
orders in either QB or QP. Thus a legitimate marking of L descriptors would be a marking on the first 
j descriptors of QB and the first k descriptors of QP, if there are exactly L distinct descriptors among 
these (j + k) descriptors. (Note one descriptor may fall into both the first j elements in QB and the 
first k elements in QP.) In each stage, the marking can only occur in increasing order of a values for 
that stage. The allocation descriptors for a stage i start f~.1, ~PJ~*l ). When (i, Z, with (i, [ml, 
TBi TB; , TP:) is marked, it means that at least z + 1 [ml descriptors of stage i must have been marked, 
and at least z + 1 processors must have been allocated to stage i. If the last allocation descriptor 
marked for stage i is labeled with z processors, the total number processors assigned to that stage is 
z + 1, and the corresponding stage building and probing times will be TB~+l and TP~+l, respectively. 
Consequently, we have the following lemma. Lemma 7: The table-building time of the first unmarked element 
in QB is the table-building time of the pipeline} and the tuple-probing time of the first unmarked element 
in QP is thetuple-probing time of the pipeline. Denote the i-th element in QB as bi, and the associated 
building time as TB<a,s. pj in QP and TP<P, > are defined similarly. The procedure PA1lI for Arrr can 
be summarized as follows. Procedure for allocation kIr (PAIII) 1.For each stage i, O < i < k 1, first 
allocate (~] processors to that stage. 2. For each stage i, Os i ~ k 1, determine the L + 1 allocation 
descriptors, (i, z, TB~, TP~ ), ~mjl < z < ~mjl + L, for the possible allocations of additional processors, 
where L = N ~vj (mj 1. 3. Construct QB and QP (each with k(L + 1) descrip­tors), where the allocation 
descriptors are sorted in descending order of TB~ and TP~, respectively, 4. For each j, O ~ j ~ L, determine 
k(j)(z (L  j)) such that the number of distinct elements in the set Dj = {b1jb2,..., bj} U {pi, P2,..., 
Ph(j)} k L. (This can be achieved by marking the first j elements in QB first and then the first (L 
j) unmarked elements in QP. Thus Dj is the set of marked allocation descriptors and corresponds to an 
allocation of L processors.) For each j, calculate the time T13<b,+l > + Tp<phtj)+l > ) which is the 
processing time of the pipeline under the corresponding allocation. 5. Choose the j, O s j s L, with 
the shortest processing time. The corresponding allocation is AIII. Consider the example in Table 2. 
Since (w) = (~)= (5.0, 2.5, 2.5, 5.0, 1,7) from AII in Section 3.2, we get ([ml) = (5, 3, 3, 5, 2), meaning 
that there are L = 2 processors left that will be added to the appropriate stages by AIII. QB and QP 
can then be determined, each of which consists of (2 + 1) * 5 = 15 elements in the form of (i, x, TB~, 
TP~ ). We then have, tq = (O, 5,1.2, 0.8), b2 = (3,5, 1.2, 0.6), bs = (4,2, 1.0,2.5), bq = (1,3,1.0, 
1.33), etc., for Q13, and pl = (4,2, 1.0, 2.5), PZ = (4,3, 0.66, 1.66), P3 = (1,3, 1.0, 1.33), IM = 
(4,4,0.5, 1.25), etc., for QP. From Step 4 in PAIII, we obtain for (j, k) = (0,2), TS = TB<5,> + TP<P,> 
= 1.2 + 1.33 = 2.53, for (j, k) = (2,0), TS = TB<5,>+TP<PI> = 1.0+2.5 = 3.5, for (j)k) = (1, l), TS 
= TB<5,> + TP<P,> = 1.2 + 1.66 = 2.86, etc. It follows that the 2 additional processors should be added 
to stage 4 to achieve an optimal allocation AIII = (5, 3,3, 5,4) with TS~l,l = 2.53. It can be verified 
that the complexity of PAIII is O(Nk o log(~k)) because of the sorting in QB and QP. Also, from the way 
QB and Q1 are constructed it follows that the allocation achieved by AIII is the one with shortest processing 
time among all possible allocations. Theorem 3: Considering all three constraints, PAIII gives the optimal 
processor allocation to the stages in a pipeline. 4 Simulation We have formulated the processor allocation 
as a deter­ministic optimization problem and developed optimal solution procedures. In the simulation, 
each individual tuple actually runs through the stages in a pipeline of hash joins, so that the burst 
effects and the actions for pipeline fill-up and depletion are captured. The sim­ulation verifies that 
our formulation of the two-phase mini-max optimization problem well approximates the original pipelined 
hash join problem and provides it with an effective solution procedure. 4.1 The Simulation Model To focus 
on the effect of processor allocation on pipelined hash joins, we simulate pipeline segments of hash 
joins, with and without using the optimal processor allocation scheme. The simulator takes hardware parameters, 
a pipeline segment of hash joins and a processor allocation for the pipeline as inputs. It outputs the 
query processing time of the pipeline. The number of stages in the pipeline is predetermined in the simulation. 
The cardinalities of the relations in pipeline stages are randomly chosen from fixed ranges. In order 
to simulate the behavior of each tuple accurately, we scaled down the average relation size and reduced 
the memory size of each processor accordingly, so that parameters mu-see tTead 14 20 tha.h 12 t Teceiue 
t c Omp 12 thu,ld 8 tpa,t 12 t,.nd 20 2 t znsevt 14 tw,ite  Table 3: Architectural parameters employed 
in simula­tion. the ratio of average relation size to memory size is nonetheless realistic. The hash 
table for each pipeline stage is built in the table-building phase by partitioning its inner relation 
into subtables, one for each processor allocated to the stage. In the probing phase, each incoming tuple 
is routed to one of these subtables at random. A random number generator, coded based on the join selectivities, 
is then used to generate the resulting tuples. The time spent on various actions such as partitioning, 
hashing, matching, or building resulting tuples, are highly dependent on the architecture, OS (or equivalent 
system software) and the actual implementation. However, to better reflect reality, we have determined 
the relative CPU times for the various actions by actually taking measurements for sample code on a SUN 
spare station. These parameters are reported in Table 3. 4.2 Simulation Results Two processor allocation 
schemes are used for each query in the simulation: the optimal processor alloca­tion algorithm PAIII 
and a heuristic HT. Heuristic HT allocates processors to the stages in proportion to their hash table 
sizes. The numbers of processors so allocated are real numbers, which are then converted to integers 
subject to the constraint that the number of processors allocated to each stage is large enough to hold 
its hash table. The pipelines employed in the simulation are chosen to be of five stages (i.e. six relations). 
Results from four experiments are presented here, although more experiments on sensitivity analysis have 
been conducted and indicated similar trends. In each experiment, three hardware configurations, with 
16, 32 and 64 processors, respectively, are used. Sixty pipelines are simulated for each hardware configuration. 
Each combination of pipeline and hardware configuration is simulated three times to capture the randomness 
on the tuple generation 76 250,CO0 [ HT [ Opt Allocaticm 1632 e4 Number of Processors max card Jmn card.4 
60 pplme seqments  Figure 3: Comparison of the optimal scheme and heuristic HT when max card./min card. 
=4. e ~ 250,000 : f+ 200,030 0 ,- E ; 150,CO0 .-E + c i Oo,ouo g a : W,OCO G 0 Number of Processors ma 
card hnln card.6 W flpdme segments  Figure 4: Comparison of the optimal scheme and heuristic HT when 
max card./min card. =6. and matching in a join during the simulation. In the first experiment, the cardinalities 
of all inner relations and intermediate relations are randomly selected from a range between 800 and 
3200, with the ratio of maximal to minimal cardinality equal to 4. The second experiment deals with relations 
with a larger variance, and has relation cardinalities generated from 600 to 3600, with the ratio of 
maximal to minimal cardinality equal to 6. In the third experiment, we randomly set a stage to be the 
bottleneck of the table­building phase by assigning that stage with an inner relation ten times as the 
average size of other inner relations. The ratio of maximal to minimal cardinality of the non-bottleneck 
stages is set to 4. In the fourth experiment, we set one stage to be the bottleneck of the t uple-probing 
phase by adjusting its tuple-probing phase workload to be ten times as the average of other stages. (This 
is achieved by making the intermediate 64 Number of Processors tablelwld,ng bottiened( 633ppsl,ne segments 
 16 32 Figure 5: Comparison of the optimal scheme and heuristic HT for table-building phase bottleneck. 
16 32 64 Number of Processors tuplewcbtng phas+sbottienee% W ppdme segments Figure 6: Comparison of 
the optimal scheme and heuristic HT for tuple-probing phase bottleneck. relation from the previous stage 
ten times as large as the average.) Without loss of generality, we set the bottleneck stage to be the 
last stage in a pipeline. The ratio of maximal to minimal cardinality of the non­bottleneck stages is 
also set to 4. The performance data for the first two experiments are shown in Figures 3 and 4. In can 
be seen that in the first experiment, the optimal processor allocation scheme shows 28~0 to 3 l~o improvement 
over the simple heuristic. When the ratio of the maximal to minimal cardinality increase to 6 in the 
second experiment, the performance improvement of the proposed scheme increases to the range between 
36% and 38%, meaning that when the cardinalities of relations and join attributes become more widely 
varied, PAIIZ offers even better performance improvement. As shown in Figures 5 and 6, it is observed 
that in the case of a tuple-probing phase bottleneck, the optimal processor allocation provides 39 XO 
to 4370 improvement in execution time. In the case of a table-building phase bottleneck, the optimal 
processor allocation shows 45?Z0 to 5 l% improvement, an even better result. It can be seen that the 
more skewed the input is, the more improvement can be achieved by a better processor allocation scheme. 
As a matter of fact, skewed inputs are usually those creating performance bottleneck, and the very ones 
we would like to tackle for better overall performance. In general, it is observed from simulation results 
that the proposed scheme performs very consistently, and can lead to significant performance improvement 
over simple heuristics. Conclusions This paper presents a method for achieving optimal processor allocation 
for pipelined hash joins. We formulated the processor allocation problem as a two­phase mini-max optimization 
problem and developed both exact and approximate solution procedures. We also demonstrated through simulation 
that our problem formulation, though not explicitly incorporating all dynamic effects of pipelining, 
can lead to solutions with substantial performance improvement over previous approaches. The results 
are useful for dealing with both right-deep trees and segmented right-deep trees. References [1] H. Boral, 
W. Alexander, et al. Prototyping Bubba, A Highly Parallel Database System. L?EE Transactions on Knowledge 
and Data Engineering, 2(1):4-24, March 1990. [2] M.-S. Chen, M.-L. Lo, P. S. Yu, and Y. C. Young. Using 
Segmented Right-Deep Trees for the Execution of Pipelined Hash Joins. P? oceedings of the 18th Internatzona~ 
Conference on VeTy LaTge Data Bases, pages 15 26, August 1992. [3] M.-S. Chen, P. S. Yu, and K.-L. Wu. 
Scheduling and Pro.cesqor Allocation for Parallel Execution of Multl-Jom Queries. 1% oceedings of the 
8th International Confe? ence on Data Enginee? ing, pages 58 67, February 1992. [4] D. J. DeWitt and 
R. Gerber. Multiprocessor Hash-Based Join Algorithms. Proceedings of the Ilth International Conference 
on Very Large Data Bases, pages 151-162, August 1985. [5] D. J. DeWitt, S. Ghandeharizadeh, D. A. Schnei­der, 
A. Bricker, H.I. Hsiao, and R. Rasmussen. The Gamma Database Machine Project. IEEE Transactions on Knowledge 
and Data Engineering, 2(1):44-62, March 1990. [6] D. J, DeWitt and J. Gray. Parallel Database Sys­tems: 
The Future of High Performance Database Systems. Cornm. of ACM, 35(6):85-98, June 1992. [7] W. Hong. 
Exploiting Inter-Operator Parallelism in XPRS. P? oceedings of ACM SIGMOD, pages 19­28, June 1992. [8] 
K. Hua, Y.-L, Lo, and H. C. Young. Including the Load Balancing Issue in the Optimization of Multi-Way 
Join Queries for Shared-Nothing Database Computers. Proceedings of the 2nd Conference on Parallel and 
Distributed Info Tmaiion Systemsl pages 74-83, January 1993. [9] T. Ibaraki and N. Katoh. Resource Allocation 
Problems: Algorithmic Approaches. The MIT Press, Cambridge, Massachusetts, 1988. [10] M, Kitsuregawa, 
H. Tanaka, and T. Moto-Oka. Ar­ chitecture and Performance of Relational Algebra Machine GRACE. Proceedings 
of the International Conference on PaTallel Processing, pages 241-250, August 1984. [11] M.-L. Lo, M.-S. 
Chen, C. V. Ravishankar, and P. S. Yu. Optimal Processor Allocation for Pipelined Hash Joins. IBM Research 
RepoI t, RC 18303, September 1992. [12] R, A. Lorie, J.-J. Daudenarde, J. W. Stamos, and H. C. Young. 
Exploiting Database Parallelism In A Message-Passing Multiprocessor. IBM Journal of Research and Development, 
35(5/6):681-695, September/November 1991. [13] H. Lu, K. L. Tan, and M.-C. Shari. Hash-Based Join Algorithms 
for Multiprocessor Com­puters with Shared Memory. Proceedings of the 16th International Conference on 
Very Large Data Bases, pages 198-209, August 1990. [14] P. Mishra and M. H. Eich. Join Processing in 
Relational Databases. ACM Computing Swweys, 24(1):63-113, March 1992. [15] N. Roussopoulos and H. Kang. 
A Pipeline N- Way Join Algorithm Based on the 2-Way Semijoin Program. IEEE Transactions on Knowledge 
and Data Engineering, 3(4):461-473, December 1991. [16] D. Schneider and D. J. DeWitt. Tradeoffs in Processing 
Complex Join Queries via Hashing in Multiprocessor Database Machines. Proceedings of the 16th International 
Conference on VeTy Large Data Bases, pages 469-480, August 1990. [17] M. Stonebraker, R. Katz, D. Patterson, 
and J. Ousterhout. The Design of XPRS. Proceedings of the Ii-&#38;h hiemational Conference on Very Large 
Data Bases, pages 318-330, 1988. [18] A. Wilschut and P. Apers. Dataflow Query Execution in Parallel 
Main-Memory Environment, Proceedings of the 1st Conference on Parallel and Distributed Information S@tems, 
pages 68 77, December 1991. [19] P. S. Yu, M.-S. Chen, H. Heiss, and S. H. Lee. On Workload Characterization 
of Relational Database Environments. IEEE Transactions on Software Engineering, 18(4):347-355, April 
1992.  
			