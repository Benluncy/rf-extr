
 Specifying and Analysing Run-Time Security Policies for Time Dependant Services Boulares Ouchenne University 
of Pau IUT de Mont de Marsan 40000 Mont de Marsan, France boulares.ouchenne@etud.univ-pau.fr ABSTRACT 
We deal with the issue of specifying security policies that can be enforced by monitoring services execution. 
Currently, the vast majority of works focus on access control, are based on logics, and o.er ways to 
express high level properties of real-time systems. However, the expressivenes power of such logics does 
not allow us to express recent usage control re­quirements (like accounting), and the undecidability 
of such logics hardens the task of analysing and querying such se­curity policies. Our work o.ers rather 
an operational ap­proach, by the use of timed automata to specify and analyse security policies that 
can be enforced through mechanisms that work by monitoring the system execution. We show how to specify 
such complex policies as combinations of sim­pler modular policies. Then for a given set of policies, 
we suggest methods to analyse and establish whether this set of policies is consistent or not. Categories 
and Subject Descriptors D.2.4 [Software Engineering ]: Software/Program Veri.­cation formal methods; 
model checking;; D.4.6 [Operating Systems]: Security and Protection;; D.2.1 [Software En­gineering]: 
Requirements/Speci.cations; General Terms Theory, Security, Veri.cation  Keywords Services, Execution 
Monitoring, Security Policy, Timed Model, Usage Control. 1. INTRODUCTION Services become a central component 
in networked system infrastructures, as they will be the main tool for the realiza­tion of many critical 
applications such as remote operations, data collection, industrial automation, sensor networks and Permission 
to make digital or hard copies of all or part of this work for personal or classroom use is granted without 
fee provided that copies are not made or distributed for pro.t or commercial advantage and that copies 
bear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on 
servers or to redistribute to lists, requires prior speci.c permission and/or a fee. SeceS 11, June 9-10, 
2011, Baabda, Lebanon. Copyright 2011 ACM 978-1-4503-0884-7 ...$10.00. Ousmane Koné University of Pau 
Academy of Bordeaux 40000 Mont de Marsan, France ousmane.kone@univ-pau.fr monitoring applications. However, 
these environments need the enforcement of strict security constraints. Some mechanisms need to be estab­lished 
to ensure (a priori) that the access to resources is based on user rights, but one must also check (a 
posteriori) how the service is used, once the access is granted. Runtime monitoring seems the best mechanism 
to deal with these requirements. It allows us to secure systems by means of preventing the occurrence 
of unacceptable behaviors. It corresponds to a set of monitors that supervise how sensitive information 
and other resources are managed, protected and distributed in the system. To specify runtime security 
policies several models and log­ics has been suggested. For instance, models which deal with access control 
[13, 4, 8] needs. However, access control mechanisms can restrict the access to the ressource (i.e., 
it can restrict how one may get access to the service) but can­not control how the ressource is used 
afterwards. Recently, this type of requirements that correspond to obligations have also been suggested 
for example in [12]. Usage control is an extension of access control that controls not only who may access 
which resource, but also how the resource may or may not be used afterwards. For instance, we can specify 
in our security policy that it is mandatory to release the printer if the printer is used for more than 
10 minutes (bounded use policies) or users are not allowed to print more than 10 pages (Accounting). 
State of art and Related Works. In the literature, there has been a great amount of research in security 
policy lan­guages and models speci.cation. They are classi.ed in two main approaches. Logic-based security 
policy speci.cation and Model-based security policy speci.cation. There are many researches on logic-based 
policy speci.­cation [9, 11, 7]. These works o.er .exible and extensible languages to capture expressive 
security policy needs. They also o.er tools which can help security administrators to .nd and manage 
security policies. However, a signi.cant problem is that users have a great di.culty to understand the 
overall e.ects and consequences of their security policies statement. Furthermore, verifying the consistency 
of secu­rity policies and its compliance with a given system speci.­cation have became a hard task for 
security administrators. In Model-based security policy speci.cation, the .rst spec­i.cation model is 
suggested by Schneider [13]. He proposes the Security Automata Formalism to specify security poli­cies 
that can be enforced by monitoring services execution. He shows that enforceable security policies can 
be identi­.ed with safety and liveness properties over sets of traces and proposes security automata 
as mechanisms to enforce security policies. In this work, Schneider o.ers an enforce­ment mechanism that 
works by monitoring execution steps of some executing system, called targets, and terminating the target 
execution if it is deemed illegal by the security automaton. However, important classes of security require­ments 
are very hard to express using traditional safety or liveness properties. Furthermore, continuous time 
is not considered, and the author suggests to express real-time availability in a discrete fashion, by 
counting the number of computation steps. On the other hand, instead of termi­nating the target application 
when illegal sequences execu­tion are recognized, the work in [4] suggests another solution based on 
modifying the target application behaviour. The authors o.er target monitors called edit automata , which 
act as an abstract machine, which examines traces of tar­get application actions and transforms the traces 
when it deviates from a speci.ed policy. Edit automata have a rich set of transformational powers and 
can terminate the ap­plication, suppress undesired or dangerous actions without necessarily terminating 
it, and insert additional actions into observable events. However, edit automata composition is an error-prone 
task and can cause the unsoundness of the security policy, because the events inserted by a policy may 
interfere with those monitored by another policy. To deal with this issue, the programmer has to explicitly 
.x the or­der in which policies are applied. The work of [8] de.nes shallow history automata (SHA), which 
can be considered as a speci.c type of memory-bounded monitor. The access decision is granted by examining 
a .­nite and unordered histor y of previously accepted actions. In [10], authors propose a method to 
automatically enforce the desired security properties. They de.ne controller oper­ators that are able 
to synthesize security automata described in [13, 4] to enforce safety and liveness proprities. The idea 
is to model security automata (including truncation and edit automata) as operators in process algebras 
and exploit sat­is.ability results for temporal logic to automatically synthe­size these controllers 
for a given security policy. The work presented in [2, 3] uses an expressive fragment of metric .rst-order 
temporal logic (MFOTL). They exploit the metric temporal operators to specify monitorable usage control 
policies. However, their logics are not suitable to describe security policies in a behavioral manner 
because they cannot describe the dynamic evolution in the states of monitorable systems, and are not 
expressive enough to spec­ify other security requierements (Accounting). Furthermore, authors do not 
discuss the issue of security policies analysis expressed in their language. The goal of our work is 
to extend previous works into sev­eral directions. First, we de.ne a State/Transition approach to specify 
security policies, which can be considered as an extension of Schneider s Security automata. Our approach 
takes into account four new features which are: (1)Con­tinuous real-time provided by the rigorous model 
of TA [1], allowing us to reason about quantitative time, to control sys­tem behavior continuously and 
describe security policies in a dynamic manner to concisely specify such complex policies for time constrained 
services (2) Obligation with deadline and Accountability, which cannot be expressed neither by safety 
properties, nor by liveness properties (3) For The obli­gation, we deal with two kinds: (a) weak obligation: 
which can be violated, but when the monitor detects the violation (for instance an abusive use), it can 
trigger another obliga­tion (i.e. a compensating action or a sanction like Denial of Service) and(b) 
a strong obligation: or unviolable obligation which cannot be violated in all circumstances and (4) The 
decidability of the emptiness problem for timed automata allows us to analyse and query security policies 
speci.ed in such formalism using so.sticated tools like [5]. The paper is structured as follows: Section 
2 presents a framework to model the security policies against a given de­ployed service. Section 3 presents 
methods to model and analyse security policies expressed with the suggested for­malism. An example used 
as case study is introduced for the illustrations. Section 4 draws the conclusions.  2. MODELING SECURITY 
POLICIES RULES 2.1 Timed Modelling -Syntax &#38; semantics In this paper, we use Timed Automata [1] (TA) 
as a for­mal description technique. A TA is a .nite state machine equipped with a set of synchronized 
clocks, i.e., they advance at the same pace, decorated with .nal states to capture the notions of accepted 
(legal) executions, and there is a di.er­ence in the semantic associated to transition. Indeed, by de.nition, 
a transition in a TA de.nes what is possible to do, but in our work, it de.nes what is permitted (or 
obliged) to do. For our framework, this formalism is suitable as a start­ing point to express the desired 
policies, but also to model the behaviour of the communicating services involved in the monitored systems. 
Moreover, this formalism which is well known and used in practice, allows us to reason with the con­structed 
models, analyse the models and verify some desir­able properties. Each rule is expressed by a Timed Automa­ton, 
that speci.es Acceptable behaviours and system execu­tions. The whole security policy is the set of all 
Acceptable executions, and is the intersection of permitted (by negation non prohibited) and obliged 
executions. Indeed, every obli­gation is also considred as a permission. We consider as time domain T 
. R+ the set of non-negative reals. Let S be a .nite alphabet. A timed word over S is a .nite or in.nite 
sequence of pairs (a0,t0)(a1,t1).....(ai,ti)...., such that for every i . N, ai . S, ti . T, and ti = 
ti+1. Let X be a .nite set of non-negative valued variables, called clocks. A clock valuation over X 
is a mapping v : X -. T that assigns a time value to each clock. Let t . T and x . X, the valuation v 
+ t is de.ned by: v + t(x)= v(x)+ t, for every x . X. For Y . X, we denote by [Y .- 0]v, the valuation 
assign­ing 0 to all x . Y and v(x) to all x . X/Y. The set of guards C(X) is de.ned by the grammar: C(X) 
3 g ::= false | true | x ~ c | c ~ x | x - y ~ c |¬g |g . g. where: c . T, x, y . X, and ~. {<, =}. De.nition 
1. A Timed Automaton (TA) is a tuple A=(S, S, s0, F, X, I, T) where: 1. S a .nite set of states. 2. 
S is the set of actions(services). 3. s0 . S is the initial state. 4. X a .nite set of clocks. 5. 
F . S a set of .nal states. 6. I:S -. C(X) a function that assigns to each state an  invariant. 7. 
T . S × S × C(X) × X × S a .nite set of transitions. Semantics. An element t =(s, a, g, r, s) . T that 
is represented by a,g,r' s ---. smeans that, on the state s we are permitted ' to perform action a and 
the target state s . g/g . C(X) is a clock constraint over X that speci.es when the transition is enabled, 
and the subset of clocks r/r . X gives the clocks to be reset with this transition. Invariant I(s) de.nes 
the temporal condition for remaining in state s. The semantics of a TA A is de.ned by associating a labeled 
transition system LTS(A). A state of LTS(A) is a couple (s, v) such that s is a location and v is a valuation 
over X. We distinguish two types of transitions in LTS(A): 1)Time transitions: for a state (s, v) and 
d . T,(s, v) -d . (s, v + d) if for all: 0 . d' = d, v + d' |= I(s). 2)Discrete transitions: for a state 
(s, v) and a transition a,g,r s ---. s ' : a (s, v) -. (s',v[r :=0]) if (v |= g) . (v[r := 0] |= I(s')). 
We denote by (S × T) * the set of in.nite or .nite sequence over S × T. A Path P in A is a .nite sequence 
of consecutive transitions a0,g0,r0 a1,g1,r1 an-1,gn-1,rn-1 s -----. s1 -----. s2........sn-1 -----------. 
sn. If s = s0 .sn . F then we say that P is an Accepting Path. A Run of the automaton along the path 
P is a sequence of transitions: a1,g1,r1 a2,g2,r2 a3,g3,r3 (s0,v0) -----. (s1,v1) -----. (s2,v2) -----. 
(s3,v3)........, t1 t2 t3 where: s = t1,t2,t3.... is a time sequence in T, and vi(i =1, 2, ..) is a 
clock valuation such that: . . v0(x)=0. .x . X : vi-1 +(ti - ti-1) |= gi. . vi =[ri . 0](vi-1 +(ti - 
ti-1)). A timed trace is a sequence of timed (word) actions . = (s1,t1)(s2,t2)....(si, ti) where: .i 
? 1: ti . ti+1. The timed language L(A) is the set of all timed traces . for which there exists a run 
of A over .. In the following sections, we will introduce an example used as case study.  2.2 Security 
Policy De.nition and Statement The goal of this section is to provide mechanisms to spec­ify security 
requirements also named rules. The rules de.ne obliged and authorized (or by negation prohibited) execu­tion 
sequences. Formally De.ning Security Policy. We specify a security policy (P ) at a high level of TA 
abstraction over a nonempty, .nite set of services S (also referred to as system events). It identi.es 
characteristics, behaviours and usage scenarios that are deemed legal by a given target system. It corre­sponds 
to a set of rules, where each rule speci.es behaviours and usage scenario that meet one speci.c security 
require­ment. If there is only one rule, then the policy equals that single rule. An action or service 
may be applied only when some tem­poral and contextual conditions are satis.ed. Formally, a security 
policy could be de.ned as a set of (.nite or in.nite­length) executions (L(P ) . (S × T) * ) satisfying 
a given set of security properties. For example, a set of executions sat­is.es a termination policy if 
and only if every execution in the set is a .nite sequence of services. In this work, we suppose that 
our security policy is open (i.e. every service not explicitly prohibited is permitted, a service that 
corresponds to no transition in the security pol­icy is simply considered as prohibited). Modelling Security 
Policy Rules. TA is a well suited for­malism to specify security policy needs, in an operational manner. 
A .nite TA incorporates a set of clocks, its transi­tions are labeled with guards (i.e. conditions on 
the values of clocks), actions and updates, which assign new values to clocks. We are namely interested 
in obligations, but in the following section, we will also show how to express both access modes (permissions, 
prohibitions, obligations). Permission: corresponds to executions that are permitted by the security 
policy and represented only through transi­tions in the TA. For instance, the system can perform the 
action if there is such transition labeled by the action name from the current state. Prohibition: corresponds 
to exceptions of permission rules, which are represented simply by the omission of transitions labeled 
by the services name. Thus, actions that correspond to no transition in the TA are simply considered 
prohibited. Obligation: is more concerned with temporal deadline con­dition. Once the obligation condition 
is raised, the user has to perform the concerned service calls according to the pre­scribed deadline 
condition. We represent an obligation through a non accepting state with an invariant ( true) and an 
outgoing transition rep­ = resenting the mandatory action. However, as obligations are stronger than 
permissions, the state invariants which force to leave the concerned state allow us to capture this seman­tic 
and distinguish what is mandatory and what actions are only permitted but not mandatory. In the current 
state, if the authorized transitions are not activated before the deadline, a transition that corresponds 
to the obligation is automatically triggered by the Monitor (i.e. in the case of strong obligation). 
The semantics of obligation is that some action or service must be called, and moreover, that action 
must be executed according to a real-time constraint speci.ed in the invariant. In the following section, 
we will illustrate security rules with examples. The rule R2 below is an example of Obligation speci.ed 
with the model of Fig. 2.  2.3 Introducing the Case Study and Examples The Service Overview. We consider 
a simple networked print service named P rint - Service, which we will detail later. In the general case, 
the right to use the service de­pends on the type of the submitted document, the user cre­dentials, the 
operating environment of the server, etc. The user can be involved in di.erent interactions with the 
net­worked P rint - Service. First, the user can send a request (P -req) to print a .le. The service 
reads the .le page after page (R - p) and detects the type of page -Black&#38;White or Color (B&#38;Wor 
Col) -and then checks the availability of paper (C - p). Finally, the page is printed (P rint) before 
the Return. Note that the Return Service (Re) delivers a report about the execution of the service (terminated, 
pro­hibited, aborted, etc).  Figure 2: Model of R2. Figure 1: Model of R1. More details on this service 
are provided in the following sections. The Security Rules Composing the Policy. In certain corporations, 
the handling of digital documents is subject to certain rules in order to avoid fraudulent uses. The 
re­production rights not only depend on the credit (and creden­tials) of the user, but also on the nature 
of the document. In particular, the reproduction of color documents is prohibited for standard users, 
because on the one hand printing color is expensive, and on the other hand such a document may contain 
seals or stamps which authenticate the original doc­ument. The regulations prescribed by these administrations 
require speci.c procedures to handle this type of document. We take this last case as an example, because 
it is more in­tuitive to be modeled and illustrated by our approach. R1 In English: Prohibit Calling 
the (local) P rint request if a color page is detected on the server side, and in such case only permits 
the Call of the Return service. The case of R1 is expressed by the timed automaton in Fig. 1, where: 
S= {Re, P - req, R - p, C - p, P rint, Col, B&#38;W }. Fig. 1 depicts the TA for the security policy 
that, after per­forming the call of Col service, all services will be prohib­ited expect the Re service. 
It contains two .nal (accepting) states F={a, b} and the initial state is a . In the following, initial 
states are represented by an arrowed circle and the .nal states by a double circle. In Fig. 1 at the 
initial state a , clients can call all services at any time. Permissions (corresponding to a permitted 
calls of a given services by the security policy) are only represented through transitions in the TA. 
For example, the predicate S -{Col} is satis.ed by any in­put symbols (service s call) that are not the 
Col service. Thus, the TA accepts all input sequences that are either .­nite or in.nite many S services 
calls (i.e. represented by a loop transition). After the Col service call, the monitor moves to another 
.­nal (accepting) state b . In this state only the service Re is allowed (because there is only one permited 
transition from b to a labelled with Re ) and prohibited services are ex­plicitly represented by the 
ommission of transitions which have as predicate the name of concerned services. The fol­lowing example 
illustrates how to specify obligations with deadline (strong obligation). R2 In English : Obligation 
to call the Return service Re by 10 time-units after the reception of the printing request P - req . 
The security policy of R2 is expressed by the TA in Fig. 2. In Fig. 2 at the initial state c , clients 
can call all services at any time, but after performing the call of P - req service, Figure 3: Model 
of Obligation with Violation Man­agemant. a new clock named x is rested and will serve as a counter of 
the elapsed time, and the monitor moves to a interme­diate state (non accepting state) d , this second 
has as an invariant (x<= 10) where 10 is the deadline. In this state, only calls satisfying the guard 
(x<= 10) are allowed and all accepted traces must leave that state through a call of the Re service before 
the expiration of deadline to reach the accepting state c . In this case (Strong Obligation), the monitor 
will trigger the call of Re service automatically if the deadline is expired and the obligation is not 
ful.lled. Formally, this TA accepts all input sequences that are .nite or in.nite and each call of P 
- req service is followed by at least one call of the Re service before the expiration of the deadline 
(10 time-units). The Management of Weak Obligation Violation. As it is di.cult to specify some obligation 
requirements without violation, because they require an external interven­tion (Weak Obligation), we 
suggest the use of the strategy proposed in [6]. In our work, obligation is considered as a simple transition 
with a maximum delay assigned to its ful.llment. Thus, the situations of violation of this security requirement 
is simply characterized by the expiration of the delay and the non­ful.llement of the obligation. In 
[6], authors have investigated a sophisticated approach that allows the violation of some security requirements 
and, they consider that this is acceptable if the security policy speci.es additional requirements that 
are applied in this case. In Fig. 3, at the initial state c , clients can call all ser­vices at any time, 
but after performing the call of P - req service, a new clock named x is rested, and the monitor moves 
to a intermediate state (non accepting state) d , this second has no invariant. In this state, calls 
satisfying the guard (x<= 10) are allowed and make the system to be­have correctely and accepted traces 
(without violation) must leave that state through a call of the Re service that satis­  Figure 4: Model 
of R3. Figure 5: Model of R4. .es the guard (x<= 10) to reach the accepting state c . On the contrary, 
when the monitor detects a violation of the obligation (i.e. a call of a service is performed when x> 
10), it triggers a compensating action such as a penalty, and the system behaves like Fig. 3. The monitor 
moves to the sanc­tion state E (it has to remain inactive and denied from any service in at least 10 
min for example). In this case the colck x is reseted and reused, and the obligation to call the Re service 
has to be full.led to reach the accepting state c . The following example illustrates how to specify 
Accounting issues. R3 In English : Prohibits users to call the Print service P rint more than 5 times 
for each printing request P - req . InFig.4atthe initial state i , clients can call all services at any 
time. After the P - req service call, the monitor moves to an­other .nal (accepting) state f and a new 
variable named compt is reseted and will serve as a counter of the number of P rint service call s. In 
this state the service P rint is allowed only if the guard (compt < 5) and at each P rint sevice call, 
the variable compt is incremented to compute the number of calls, and for each call of P - req service 
the variable compt is reseted. Formally, this TA accepts all input sequences that are .nite or in.nite 
and for each call of P - req service is followed by less than 5 times the calls of the P rint service. 
R4 In English : To prevent denial of service, this rule prohibits system users to make two calls simultaneously 
of P -req in 5 time-units. In Fig. 5 at the initial state i , all services are permitted at any time 
(this is represented through a loop of an unguarded transition in the state), but after the call of the 
P - req service, a new clock named x is reseted and will serve as counter of time to measures the time 
elapsed from the last call of P - req service and the state f is reached. In this state, the service 
P - req is permitted only if the guard (x> 5) is satis.ed. In addition, after each call of the service 
P - req , the clock x will be resseted. Figure 6: Restricted Model of P=R1.R2.  3. ANALYSING POLICIES 
The security rules are expressed individually, this allows us to represent complex policies as combinations 
of simpler modular policies. We now present the following method to compose or rather merge such rules. 
As mentioned before, each rule identi.es some speci.c behaviours prescribed by the security policy. An 
action is allowed by the intersec­tion of two rules if and only if it is allowed by each of the two rules. 
This following method calculates policies inter­section, which is useful when two or more parties express 
policy and want to limit the policy behaviour to those that are compatible. De.nition 2. Given two TA 
(policies) Pi =(Si, S,Fi,s0i,Xi,Ii,Ti) for i = 1,2, the Composition of P1,P2 is a TA P1 . P2 =(S, S, 
(sO1,sO2),F1 × F2,X1 . X2,T,I) de.ned with the following minimal conditions: 1. (sO1,sO2) . S. a,g1,r1a,g2,r2 
;; (s1,s2).S.(s1----.s1).T1.(s2----.s2).T2 2. . a,g1.g2,r1.r2 ;;;; (s1,s2)---------.(s1,s2).T .(s1,s2).S 
3. For each (s1,s2) . S : I(s1,s2)= I1(s1) . I2(s2). As an example of intersection, consider the two 
input policies in Fig. 1 and Fig. 2. The result of composition is depicted in Fig. 6. 3.1 Security Policy 
Querying and Analysis When there are several policies, this may cause con.icts due to their complex relationships 
and possibile incompat­ibility with each other. For example P1 may authorize a behaviour while P2 prohibits 
that same behaviour. In our approach, we rather need mechanisms that allow us to check the security policy 
in a behavioural manner. In this area, [14] de.nes an approach for the derivation of speci.cation from 
scenarios of interactions. This approach allows the production of a complete and valid speci.cation from 
a set of such scenarios. It represents the scenarios by timed au­tomata and o.ers algorithms to check 
the consistency of scenarios. It distinguishes two types of inconsistency (time and event inconsistency). 
Event inconsistency is rather the determinism of the automaton, that allows to control the system behaviour. 
Time inconsistency in timed automata is re.ected by the constraints of clocks that can not be met. In 
addition to these conditions, we also need to check that our security policy (i.e. the behaviour) is 
non-blocking and non-empty.  Figure 7: Model of P=R1.R5. Figure 8: Model of R5. De.nition 3. We say 
that a TA (security policy) P = (S, S, F, s0, X, I, T ) is consistent if the following holds: a,g,r 1. 
deterministic: .a . S,s = s ' : ((s ---. s ' ) . T . ;; a,g,r (s ----. s '' ) . T ) . (g . g ' = false). 
a,g,r 2. time-consistency:(. s ---. s ' . T : g = false).(. s . S : I(s)= false). a,g,r 3. non-blocking: 
.s . (S \ F ): . s ---. s ' . T . 4. non-empty:L(P )= Ø.  De.nition 4. We say that, the two consistent 
security poli­cies P1 and P2 are consistent between each other if P = P1 . P2 is consistent. Exemple: 
To illustrate the concept of inconsistency, we consider the example of the rule R1 and a contradictory 
rule, R5. R5 In English : Authorize calling the (P rint) request if a color (Col) page is detected on 
the server side. The case of R3 is expressed by the timed automaton in Fig. 7. The composition of the 
two rules (R1 and R5) is depicted in Fig. 8. We can see that: a,g,r .s = bd . S \ F : . s ---. s ' . 
T . Thus, the two policies are inconsistent because the inter­section of the two behaviours is blocking. 
The following algorithm allows us the construction of a consistent security policy from a given set of 
rules (or security policies). First, one rule is considered as a full security policy and is checked 
for consistency. If the rule is consistent, then it will be com­posed with another rule, and the result 
will also be checked. This process is repeated until the composition of all rules. If an inconsistency 
is detected, then the process will be halted and a message is displayed, indicating which rule has caused 
such a situation. P0 denotes the neutral automaton accepting any sequence, and the function Is-Consistent(P) 
returns true only if P satis.es the conditions in de.nition 3. There are several libraries implementing 
model-analysis techniques for timing constraints. For example the functions developed in tools like UPPAAL 
[5] can be reused to check the consistency of a given security policy. For the sake of space, we only 
present in the next subsection algorithm to check TA emptiness. We note that all conditions (1),(2) Algorithm 
1: Algorithm for Building a Consistent Se­curity Policy. Data: A Set of rules R={r1,r2, .., rn}. Result: 
A Security Policy P. begin P.- P0; while (R= Ø)) do P ick an element ri from R P.- P . ri; if not(Is-Consistent(P)) 
then Stop(Inconsistency Caused by rule ri). and (3) cited above, can be checked in a simpler manner by 
applying the de.nition. 3.2 Checking Timed Automata emptiness We have previously seen that we need the 
vacuum test of language accepted by TA. This problem seems di.cult. In­deed, all the values of clocks 
are unbounded, there is an in.­nite number of valuations of clocks. The number of states of the automaton 
is .nite, but the automaton can be found in countless con.gurations. Indeed each con.guration is char­acterized 
partly by the actual state and valuations of clocks of the automaton. The goal is to construct an abstraction 
that isolates the useful properties and will test the vacuum of the language accepted by the original 
timed automaton. The idea of [1] is that the in.nite state-space admits a .nite abstraction, called the 
region - graph. This abstraction is de.ned by an equivalence relation on clock con.gurations, which induces 
a partition of the state space into a .nite set of regions: two states in a region are equivalent, and 
equiva­lent states have equivalent future behaviors and thus, satisfy the same sets of properties. The 
construction of the region graph is not used in practice because it is too costly in time and memory 
space, other methods are used to decide the problem of emptiness. These are based on on-the-.y con­struction 
techniques to avoid constructing the whole set of con.gurations. In addition, the sets of con.gurations 
will be represented symbolically (Zones). Zone is a set of valuations de.ned by a conjunction of simple 
constraints x ~ c, c ~ x or x - y ~ c where x and y are clocks, sign ~. {<, =}, and c a real value. There 
are mainly two types of algorithms to test on-the­.y the emptiness language of TA: the forward analysis 
and backward analysis. The Principle of a forward analysis is to compute the suc­cessors of the initial 
states, and continue this calculation as long as this new states do not appear in all the .nal states. 
We describe the principle of the forward analysis by the al­gorithm below : The operator which gives 
the precise set of successors of a a,g,r state (s, Z) by transition s ---. s ' is computed by : - . 1. 
Let the time elapse by computing Z = {v + t/v . Z .t . T}. 2. Calculate the intersection with the invariant 
of s. 3. Calculate the intersection with the guard. 4. Reset all clocks in r. 5. Calculate the intersection 
with the invariant of s ' .  Algorithm 2: Algorithm Forward Reachability analysis. Data: A Timed Automata 
named A. Result: Boolean L(A)= Ø? Declarations : Visited :=Ø; /* list contains visited states */ Waiting 
:= (S0,Z0); /* list contains states to be visited, initialized to initial states */ Successors:=Ø;/* 
list contains successors of a states */ begin repeat Pick (s, Z) from Waiting; if (s . F ) then return 
true./* A .nal state is reachable */ else if there is no (s, Z ' ) . Visited where (Z . Z ' ) then Visited 
.- Visited .{(s, Z)}Successors .- {(s ' , P ost(Z, a)) / where a a transition from s to s }Waiting .- 
Waiting . Successors return false./* No .nal state is reachable */ until (Waiting =Ø) Thus, . P ost(Z, 
a) = (( -Z . I(s)) . g)[r =: 0] . I(s ' ). (1) 3.3 Checking the compliance of a system againsta security 
policy When a policy is re.ned from a general model to a spe­ci.c target system, there is an interest 
to check whether any action is deemed acceptable. In our approach, we consider independently an abstraction 
of the system behaviour, a pol­icy, and then check whether the system is compliant with its policy. To 
deal with this problem, we consider the system as a non-empty set of possible behaviours, the policy 
as a set of authorized/correct behaviors, and we then check whether the system can concede with the security 
policy. We can check this by a simple test of, whether the system model is included in the model of the 
security policy (computed by the composition of the security rules). De.nition 5. We say that, the system 
M is compliant with a security policy P if: L(M ) . L(P ).  3.4 Results of The Case Study Our case study 
is about a simple networked print service named P rint - Service. The corresponding model is pre­sented 
in Fig. 9. We will only represent an abstract view of the networked service model, where the di.erent 
labels represent the di.erent interactions that service can perform from a given state. The initial state 
is represented by an arrowed circle and the .nal states by a double circle. The model of Fig. 9 contains 
seven states S={0, 1,2, 3,4, 5, 6}, where the initial state is s0 = 0 and the set of Final states is 
F ={0}, the set of actions S = {Re, P - req, R - p, C - p, P rint, Col, B&#38;W } and fourteen transitions. 
The user can be involved in di.erent conversations with the net­worked P rint - Service service. First, 
the user can send a request (P -req) to print a .le, the service reads the .le page by page (R - p) and 
detects the type of page -black&#38;white Figure 9: Model of the networked service Print-Service M. 
or Color (B&#38;W orCol) -and checks the availability of paper (C - p). Finally, the page is printed 
(P rint). We note that the Return Service (Re) delivers a report about the execu­tion of the service 
(terminated, prohibited, aborted, etc). In the previous sections, we have developed a security pol­icy 
by composing the two rules (R1 and R2). The resulting Policy is denoted by P and the model of the current 
case study is compliant with this policy. In the following, we anlayse it against possible use cases. 
Service Execution -Accepted by the Security Policy.. The following output is an example of normal use 
case, i.e. accepted by the service model (Fig.9) and accepted also by the security policy of Fig.6. A 
normal use case (scenario) is represented by the follow­ing timed trace: .=(P -req, 1)(R-p, 3)(B&#38;W, 
5)(C-p, 7)(P rint, 8)(Re, 9). An accepted run of this scenario over the model of Fig.9 is : P -req,true,Ø 
R-p,true,Ø B&#38;W,true,Ø (0, Ø) ---------. (1, Ø) - ------. (2, Ø) --------. 1 35 C-p,true,Ø P rint,true,Ø 
Re,true,Ø (4, Ø) - ------. (6, Ø) --------. (1, Ø) ------. (0, Ø). 7 89 An accepted run of this scenario 
over P (Fig.6) is : P -req,true,{x} R-p,x<=10,Ø (ac, (x = 0)) ----------. (ad, (x = 1)) ---------. 13 
B&#38;W,x<=10,Ø C-p,x<=10,Ø (ad, (x = 3)) - --------. (ad, (x = 5)) ---------. 57 P rint,x<=10,Ø Re,true,Ø 
(ad, (x = 7)) ----------. (ad, (x = 8)) ------. (ac, (x = 8 9 9)). Service Execution -Rejected by the 
Security Policy.. The following output is an example of use case which is accepted by the service model 
(Fig.9) but rejected by the security policy (Fig.6). A rejected use case is represented by the following 
timed trace: .=(P -req, 1)(R-p, 3)(B&#38;W, 5)(C-p, 7)(P rint, 11)(Re, 13). The run corresponding to 
an accepted scenario over the model of Fig.9 is: P -req,true,Ø R-p,true,Ø B&#38;W,true,Ø (0, Ø) ---------. 
(1, Ø) - ------. (2, Ø) --------. 1 35 C-p,true,Ø P rint,true,Ø Re,true,Ø (4, Ø) - ------. (6, Ø) --------. 
(1, Ø) ------. (0, Ø). 7 11 13 This run is a rejected scenario over P (Fig.6) : P -req,true,{x} R-p,x<=10,Ø 
(ac, (x = 0)) ----------. (ad, (x = 1)) ---------. 13 B&#38;W,x<=10,Ø C-p,x<=10,Ø (ad, (x = 3)) - --------. 
(ad, (x = 5)) ---------. 57 Re,true,Ø (ad, (x = 7)) ------. (ad, (x = 10)). 10 Indeed, the security policy 
is violated in this scenario as the speci.ed obligation was not satis.ed. After the call of P - req , 
it was prescribed to execute anything before 10 time units, and then call the Return service. Therefore 
the call of the P rint request is illegal and the one of the Re request is also illegal, and this sequence 
of execution will be transformed by the Monitor as follows: P -req,true,{x} R-p,x<=10,Ø (ac, (x = 0)) 
----------. (ad, (x = 1)) ---------. 13 B&#38;W,x<=10,Ø C-p,x<=10,Ø (ad, (x = 3)) - --------. (ad, (x 
= 5)) ---------. 57 Re,true,Ø (ad, (x = 7)) ------. (ad, (x = 10)). 10  4. CONCLUDING DISCUSSION In 
practice, enforcement mechanisms are widely used to enforce a uni.ed security policy (e.g., security 
kernels in op­erating systems). Most of the work on modelling run-time security policies [13, 4] focuses 
on access control, preventing us from modeling many common and recent security pol­icy requierements 
(e.g., usage control). In this paper, we have o.ered a formal framework to model security policies with 
an emphasis on obligations in usage control. Then for a given set of policies, we have suggested methods 
to anal­yse and establish whether this set of policies is consistent or not. The bene.t of our proposal 
is also the possibility to exploit and analyze the security properties with tools and a solid literature 
dealing with timed models. For future work, we plan to extend this research by incorporating priorities 
amoung obligations (or permissions) when more obligations are possible, which would enrich the models 
studied. This may somehow complicate the analysis techniques, but the results are likely to provide more 
.exibility and more ex­pressivity to de.ne policies in general. 5. REFERENCES [1] R. Alur and D. L. 
Dill. A theory of timed automata. Theoretical Computer Science, 126:183 235, 1994. [2] D. Basin, F. 
Klaedtke, and S. Muller. Monitoring security policies with metric .rst-order temporal logic. In Proceeding 
of the 15th ACM symposium on Access control models and technologies, 2010. [3] D. Basin, F. Klaedtke, 
S. Muller, and B. P.tzmann. Runtime monitoring of metric .rst-order temporal properties. In Proceedings 
of IARCS Annual Conference on Foundations of Software Technology and Theoretical Computer Science, 2008. 
 [4] L. Bauer, J. Ligatti, and D. Walker. More enforceable security policies. Workshop on Foundations 
of Computer Security, 2002. [5] G. Behrmann, R. David, and K. G. Larsen. A tutorial on uppaal. pages 
200 236. Springer, 2004. [6] J. Brunel, F. Cuppens, N. Cuppens, T. Sans, and J.-P. Bodeveix. Security 
policy compliance with violation management. In Proceedings of the 2007 ACM workshop on Formal methods 
in security engineering, FMSE 07, pages 31 40, New York, NY, USA, 2007. ACM. [7] F. Cuppens, N. Cuppens-Boulahia, 
and T. Sans. Nomad: A security model with non atomic actions and deadlines. In Proceedings of the 18th 
IEEE workshop on Computer Security Foundations, pages 186 196, Washington, DC, USA, 2005. IEEE Computer 
Society. [8] P. W. L. Fong. Access control by tracking shallow execution history. In Proceedings of the 
IEEE Symposium on Security and Privacy, 2004. [9] M. Hilty, A. Pretschner, D. A. Basin, C. Schaefer, 
and T. Walter. A policy language for distributed usage control. In ESORICS, pages 531 546, 2007. [10] 
F. Martinell and I. Matteucci. Through modeling to synthesis of security automata. Electron. Notes Theor. 
Comput. Sci., 179:31 46, July 2007. [11] J. Park and R. Sandhu. The uconabc usage control model. ACM 
Trans. Inf. Syst. Secur., 7:128 174, February 2004. [12] A. Pretschner, M. Hilty, F. Schutz, C. Schaefer, 
and T. Walter. Usage control enforcement: Present and future. IEEE Security and Privacy., 6(4):44 53, 
July/Aug 2008. [13] F. B. Schneider. Enforceable security policies. ACM Transactions on Information and 
System Security, 3(1):30 50, 2000. [14] S. Som´e. D´erivation de sp´eci.cation `a partir de sc´enarios 
d interaction. PhD dissertation .University of Montreal., 1997.   
			