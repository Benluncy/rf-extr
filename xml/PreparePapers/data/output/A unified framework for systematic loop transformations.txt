
 A Unified Fkamework for Systematic Loop Transformations Lee-Chung Lu Department of Computer Science 
Yale University P.O. Box 2158 Yale Station New Haven, CT 06520 lu-lee-chung@yale.edu Abstract This paper 
presents a formal mathematical framework which unifies the existing loop trans­ formations. This framework 
also includes more general classes of loop transformations, which can extract more parallelism from a 
class of pro­ grams than the existing techniques. We classify schedules into three classes: unij orm, 
subdomain­ variant, and statement-variant. Viewing from the degree of parallelism to be gained by loop 
transformation, the schedules can also be classi­ fied as single-sequential level, multiple-sequential 
level, and mited schedules. We also illustrate the usefulness of the more general loop transforma­ tion 
with an example program. Introduction One of the central issues in restructuring com­ piler is to discover 
parallelism automatically and generate correct parallel control structures that can take advantage of 
the large number of pro­ cessors. The advent of massively parallel ma­ chines opens up opportunities 
for programs that have large-scale parallelism to gain tremendous performance over those that do not. 
This paper presents a formal mathematical framework which unifies the existing loop trans­ formations 
such as loop interchanging [1, 2, 17, 19] permutation [3], skewing [17, 19], reversal, the wavefront 
method [7, 9, 10, 11, 13, 14, 15], Permission to copy without fee all or part of this material is granted 
provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright 
notice and the title of the publication and its date appear, and notice is given that copying is by permission 
of tha Association for Computing Machinery. To copy otherwiee, or to republieh, requires a fee and/or 
specific permission. @ 1991 ACM 0-89791-390-6/91 /0004 /0028 . ..!31 .50 and st+tement reordering. This 
framework also includes more general classes of loop transforma­tions which can extract more parallelism 
from a class of programs than the existing techniques. The particular class of programs are those that 
consist of perfectly nested loops possibly with conditional statement where the guards as well as the 
array index expression are affine expres­sions of the loop indices. In the next section, we describe 
the notations and terminologies used in the paper. We then present a formal mathematical framework which 
unifies the exieting loop transformation tech­niques, and sets the stage for discussing the more general 
classes of loop transformers in Section 3. A loop transformer is a function that relates a given loop 
nest with its transformed version, and consists of two parts: a spatial morphism, and a temporal morphism, 
called a schedule. Next, in Section 4, we classify schedules, by the prop­erties of uniformity, into 
three classee: uni­form, subdomain-variant, and statement-variant. Viewing from the degree of parallelism 
to be gained by loop transformation, the schedules can also be classified as single-sequential level, 
multiple-sequential level, and mixed schedules. We also describe the functional forms of the schedules 
for each class. Existing loop transfor­mation techniques are given as examples of these classes of schedules. 
Due to the limited space, please refer to [12] for the algorithms for obtaining the more gen­eral classes 
of schedules. The problem formula­tions for obtaining these schedules are based on dependence index pairs, 
which provide more de­ pendence information than dependence vectors. Since there are many such pairs 
that need to be considered, and they can be infinitely many when the loop bounds are unknown at compile 
time, we need to rely on a technique called poiy­hedra decomposition [8, 15] to manage the com­plexit 
y of the algorithm. In addition, nonlinear programming and bounded enumerative search are required to 
obtain optimal schedules. The complexity of nonlinear programming is reduced by using fast heuristics 
and linear programming as described in [12], which obtain optimal sched­ules for most cases. Finally, 
we illustrate the usefulness of the more general loop transformations with an example program in Section 
5. Versions of the trans­formed program using different schedules are im­plemented on a Connection Machine 
CM/2. The difference in performance, which is essentially due to the available parallelism determined 
by the schedule, can amount to two orders of mag­nitude. 2 Definitions and Terminolo­gies Throughout 
this paper, programming examples are written in a Fortran-like notation although the transformation techniques 
also apply to func­ tional languages. Index Domains Let [a, b] be an interval do­main of integers from 
a to b. We define an index domain D (also called an iteration space in [17]) of a d-level perfectly nested 
loop Loop Nest 1 DO (il = 11, uI){ DO (...){ DO (id = ld,ud) { body } }} to be the Cartesian product 
[11, Ul] x . . . x [1~, u~] of d interval domains [1~, u~] for 1< k < d. For the purpose of formulating 
loop trans­formations, we consider D to be a subset of the d-dimensional vector space over rationals. 
Throughout the paper, we let 1 = (il, . . . . id) and J = (jl, . . . . jd). With the domain and tu­ple 
notations, Loop Nest 1 can be rewritten as follows: DO (l:D) { body } In this paper, we focus on sequential 
loop nests which are perfectly nest ed. We use the following loop nest as a generic example throughout 
the paper, where D is a d dimensional index domain and r [a] is an expression cent aining a: Loop Nest 
L (Generic Loop Nest) DO (l:D) { ... SI : IF(PI) A(X(I)) = . . . ,,. S2 : IF(P2) I?(Z(I)) = TIA(Y(I))] 
... } Data Dependence We now define depen­dence between statements. Let SI and Sz be two statements of 
a program. A flow dependence exists from S1 to S2 if S1 writes data that can subsequently be read by 
S2. An anti-dependence exists from S1 to S2 if S1 reads data that S2 can subsequently overwrite. An output 
dependence exists from S1 to S2 if S1 writes data that S2 can subsequently overwrite. We use the not 
a­tion S1 + S2 to denote a dependence from S1 to S2. Consider Loop Nest L. For statement S z to compute 
the value B(Z(J)) at iteration J, the value A(Y(J)) is needed. If A(Y(J)) is comp­uted from statement 
S1 at iteration 1, i.e. Y(J) = X(I), then we say S2 at iteration J is flow dependent on S l at iteration 
1, denoted by Sl@l +-S2@J.  3 Formalizing Loop Transfor­mat ion We now formalize the notion of loop 
transforma­tion from a source loop nest to a target paral­lel loop nest. A loop transformer is a function 
defined over the Cartesian product of the itera­tion space of the loop nest and the set of state­ments 
in the body of the loop that relates a given loop nest with its transformed version. From the standpoint 
of symbolic transformation of the program text, a loop transformer can be decom­posed into two components: 
the first component, called domain morphism, defines how the itera­tion space should be mapped to a new 
one (with new loop bounds and possibly new predicates guarding the loop body), and the second com­ponent, 
called statement reordering function, de­fines the ordering of the statements in the trans­formed loop 
nest. The process of obtaining a loop transformer, however, suggests another de­composition: a temporal 
morphism and a spatial morphism. 3.1 Loop Transformer and Schedule Kinds of Index Domains For the purpose 
of loop transformation, it is useful to indicate how the index domain shall be interpreted. We do this 
by defining kinds of index domains. The kind of an interval domain D can be either spatial or temporal. 
The kind of a product domain is the product of the kinds of the component domains. For example, 111 x 
D2 is of kind temporal xspatial if D1 is of kind temporal and 112 is of kind spa­tial. A single-level 
loop with a temporal index domain corresponds to a sequential loop (i.e. DO), while a spatial index domain 
corresponds to a parallel loop (i.e. DOALL). Lexicographical Ordering We use the fol­ lowing not ations 
to denote lexicographical order­ ing on elements X and Y of an n-dimensional index domain. We define 
+ to be the lexico­ graphical ordering: we say X + Y if there exists k,l<ksn, such that xl=yl for all 
1,1 <k, and xk<yk. Similarly, wesayX ~YifX +Y orxk=ykforallk, l <k<n. Weusehto denote the zero vector. 
Domain Morphism We define a domain morphism to be a bijective function g from in­dex domain D to index 
domain E, denoted by g:.D + E, such that for all dependence S1O1 + S2@J, condition g(J) g(l) &#38; 6 
holds. In other words, a domain morphism will never reverse the ordering imposed by dependence relations. 
In this paper, we restrict the codomain E of a domain morphism to be a cross product of a temporal index 
domain El and a spatial index domain Ez, i.e. E = El x E2. Under this re­st riction, all parallel loops 
are innermost loops in the transformed loop nest. We define gl and g2 to be two functions: gl:D+E1 (called 
a temporal morphism), and (1) g2:D+E2 (called a spatial morphism). Under domain morphism g, index I 
in the orig­inal loop will be mapped to index J = g(I) in the transformed loop nest. Since g is bijective, 
it has a well-defined inverse, denoted by g-l. Clearly, I = g l(J). The following loop nest Loop Nest 
2 DO ((I: D)) { . . . A(X(I)) . ..} will be transformed into the following new loop nest under domain 
morphism g:~ + El XEz: Loop Nest 3 DO (( J1:E1)) { DOALL (( J2:EZ)) { . . . A(X(g-l(J1 : Jz))) . ..} 
} where (Jl : J2) denotes the concatenation of two vectors J1 and J2. The requirement of g to be subjective 
is in fact not essential. For any injective function g :D + E, we can always derive a corresponding bijective 
function g: D + {g (I) I I ~ D} from D to the image of D under g [6]. Therefore, by zdlowing the codomain 
of a bijective function to be the image of an injective function, we allow a much more general class 
of functions to be used as domain morphism. For comparison, the uni­modular transformations discussed 
in [4, 16] are special classes of bijective functions. The gener­ality does require some nontrivial algebraic 
ma­nipulation to generate correct loop bounds and predicates to guard the conditional statements in the 
transformed loop nest. An automatic trans­formation procedure for doing this based on an equational theory 
is described in [6]. Statement Reordering We now discuss statement reordering. Let S denote the set of 
statements in the loop body. We define a state­ment reordering to be a function h from the set of statements 
to the set of statement labels:   7 :s+[0, s-1], (2) where s = IsI, the number of statements in S. 
 Loop Transformer With g and r defined above, the following function h, called the loop trans~ormer, 
specifies how a loop nest is trans­formed: h:lIx S+ Elx Ezx[O, s-l] (3) h(I, S) = (gl(I), gz(I), T(S)). 
 Schedule Given h defined above, a schedule T is defined to be a function T: DXS+EIXIO, S 1] (4) T(I,AS) 
= (gl(q, ~(s)), such that condition ir(.1, S2) m(l, S1) > b must hold for all dependence S1@.I + S2@J 
in the loop nest. The condition ensures that the or­dering imposed by dependence relations is pre­served. 
Clearly, a schedule determines the se­quential execution of the transformed parallel loop nest. Note 
that by the definition of domain morphism, gl (J) gl (1) can be equal to the zero vector, i.e. S1@U and 
S2@J can be computed at the same iteration in the transformed loop nest. In this case, statement S1 must 
be in front of statement S2 in the loop body, i.e. condition ~(Sl ) < ~(Sz) must hold, to preserve the 
depen­ dence ordering. 3.2 Overall Procedure to Obtain a New Loop Nest Finding a schedule m is to understand 
what is the potential parallelism that can be extracted from the source program. The algorithms for obtaining 
a schedule x is presented in [12]. The so-called strip mining [17] and tding [16, 18] of loops are captured 
by the spatial morphism gz. Given a schedule ~ = (gl, r), the choice of gz, which depends on factors 
such as memory and processor organization and communication cost, should keep a loop transformer h = 
(gl, g2, r) in­jective. A default g2, which is used in the rest of this paper, can be g2(i1, . . . . 
id) = (iPl, . . . . iP~), so as to result in a loop transformer h that is injective, where n is the dimensionality 
of the spatial index domain E2, {pi, . . . . pm} is a subset of interval domain [1, d], and pl < . ..< 
pn. Overall Procedure To summarize, the over­ all procedure to obtain a new loop nest is: 1. First generate 
a schedule T = (gl, r) to max­imize the degree of parallelism by using the algorithms presented in [12]. 
 2. Then determine the spatial morphism g2 of domain morphism based on target machine characteristics 
such as memory and proces­sor organization, communication cost, etc., or use a default function as shown 
above. 3. The loop transformer is simply h = (91>927~). 4. Finally perform symbolic program transfor­mation, 
given the source loop nest and loop transformer h, to obtain the new loop nest. For the formal procedure, 
please refer to [6].  We now discuss different classes of schedules which include the exiting schedules 
in one class. 4 Classes of Affine and Piece-Wise Affine Schedules We call a schedule affine if it is 
an affine function of the loop indices. We call a schedule piece­wise affine if the restriction of the 
function to each subdomain of D and each subset of S is affine. In the loop restructuring literature, 
only affine schedules are considered. In this paper, we consider, in addition, piece-wise afine schedules. 
We now classify schedules according to two properties: (1) the uniformity of the schedule with respect 
to the the set of statements S and the index domain D, and (2) the degree of par­allelism in the transformed 
Loop Nest. 4.1 Properties of Schedules Uniformity Let index domain D be parti­tioned into m disjoint 
subdomains Ilk, 1< k < m; and let the set of statements S be partitioned into n disjoint subsets sk, 
1 < k < n. The gen­eral form of a piece-wise affine schedule ir defined in Equation (4) consists of conditional 
branches, one for each pair of sub domain Di and statement subset Sj, and an affine expression of the 
loop indices is on the right-hand side of each branch. We call a schedule 1. uniform if m = 1 and n= 
1, 2. subdomain-variant if m > 1 and n = 1, (also called a sub domain schedule) 3. statement-variant 
if m = 1 and n> 1,or 4. nonuniform if m > 1and n > 1.  Degree of Generated Parallelism As de­fined 
in Equations (1) and (4), the dimensional­ity of El, the temporal index domain, indicates the number 
of levels of sequential loops in the transformed loop nest. Hence a schedule ~ would generate a target 
loop nest with more levels of parallel loops and thus potentially more paral­lelism if El is of lower 
dimensionality. We call the dimensionality of El the sequential level of m. Schedules can thus be classified 
as: 1. Single-sequential level schedule (SSL) if El is a subset of the set of natural numbers Af. 2. 
Multiple-sequential level schedule (MSL) if El is a subset of Af , where n is a positive integer and 
n < d, the dimensionality of the original loop nest. 3. Mixed schedule (Mixed) if El can be of dif­ 
 ferent dimensions for each pair of subdo­main Di and statement subset Sj. Such a mixed schedule will 
result in transformed programs consisting of imperfectly nested loops, 4.2 Classification and Functional 
Form of Schedules Classification Clearly, the uniformity of z and the dimensionality of r are two orthogonal 
properties, except that a mixed schedule cannot be uniform. Thus there are all together eleven (4* 3 
 1) classes of affine and piece-wise affine schedules. The classes and their acronyms rang­ing from single-sequential 
level uniform sched­ules to mixed nonuniform schedules are in Fig­ure 1. Functional Form We now describe 
the forms of affine and piece-wise affine schedules by us­ing matrix and vector notations. Let T(S) for 
a given S in S be a constant scalar. Let d be the dimensionality of the index domain of the source loop 
nest. Uniform Schedule: 7r(I, s) = (2 1, 7 (s)), (5) IcD, SeS, where T is a constant l-by-d matrix and 
1 is the sequential level of the schedule m. Subdomain Schedule: ( I 6 D, + (T,I, T,(S)) ] 7r(I, s)= 
... (6) I G Dm + (ZJ, Tin(s)) ) [ IeD, SeS,  where T;, 1 < i ~ m, is a constant ii-by-d matrix and li 
is the sequential level of the part of the schedule defined over lli. Statement-Variant Schedule: IED, 
SES, 32 Single-Sequential Multiple-Sequential Mixed Level (SSL) Level (MSL) Uniform (U) SSL-U MSL-U 
Subdomain (SD) SSL-SD MSL-SD Mixed-SD Statement-Variant (SV) SSL-SV MSL-SV Mixed-SV Nonuniform (NU) SSL-NU 
MSL-NU Mixed-NU Figure 1: Classes of schedules where Ti, 1 ~ i ~ n, is a constant ii-by-d matrix and 
Zi is the sequ ential level of the part of the schedule defined over &#38;. Nonuniform Schedule: where 
Tii, l~i~m and l~jsn,isa constant ~ij-by-d matrix and Zij is the sequential level of the part of the 
schedule defined over D~ and Sj. The linear term TI, I c D, determines the form of the sequential loops 
in the transformed loop nest, which includes nesting structures, bounds, and possibly additional predicates 
to guard the loop body. The constant terms T(S) determine the orders of the statements in the transformed 
loop body. 4.3 Examples of Different Classes of Schedules We now give some examples of different classes 
of schedules. We first show that loop interchang­ing, permutation and skewing are special cases of MSL 
uniform schedules. Example 1: Loop Interchanging and Per­ mutation Loop interchanging and loop per­ mutation 
[1, 2, 3, 17, 19] is a process of switching inner and outer loops. Suppose Loop Nest 1 after loop interchanging 
or loop permutation becomes Loop Nest 4 DO (iPl = ZPl,~pl) { DO (...){ DO (ip~ = Pal, ~d) { body } }}> 
where (pl, p2, . . . , p~) is a permutation of (1,2,..., d). Also suppose the m innermost loops are parallelizable. 
The schedule r has the form: 7r(I, s) = (ip172p2> . . . , ipd_m, Ioc(s)), (9)   V(pl) i.e. T= ... , 
and (lo) (1 7 (s)= Ioc(s), (11) where Ioc is a function from S to Af that returns the position of the 
statement Sin the source loop nest, and each V(k) is a vector of length d with k-th element being 1 and 
all other elements being o. Example 2: Loop Skewing This operation transforms Loop Nest 1 as follows: 
shifting index i. with respect to index i~, 1S m < n S d, by a factor of ~, where .f is a positive integer, 
replacing in with the expression (lm + im * f), replacing Un with the expression (u~ + i~ * .f), and 
replacing all occurrences of in in the loop with the expression (in i~ * ~) [17, 19]. The transformed 
loop nest is of the form: 33 Loop Nest 5 DO (il = Jl,ul) { ... DO(in=ln +im*f, un+im*f){ ... DO (id= 
/d, W) { loop body with inbeing replaced by (in -im *f) } } } The schedule for loop skewing is of the 
form: 7r(I, s)=(il, . . ..im . . . . . in+ f*im, ..., i&#38; Ioc(s)), (12) n-th element v(1) . . . i.e. 
T = V(n) + f * V(m) , and (13) ... V(d) \ 7 (s) = Ioc(s), (14) where Ioc(S) and V(k) are the same as 
defined in Example 1. Example 3: SSL Uniform Schedule Loop Nest 6 DO (i=l, n){ DO (j=l, n){ S1:A(i, j) 
= B(i,j l)+i S2 : B(i,j) = A(i-l,j)+j}} An SSL uniform schedule r((i, j, k), S1) = (i, 1), and (15) T((i, 
j,k), sz) = (i, o), will transform Loop Nest 6 into 34 Loop Nest 7 DO (i=l, n){ DOALL (j= l,n) { s~: 
B(i, j) = A(i l,j)+j SI:A(i, j) = B(i,j l)+i}} Example 4: MSL Uniform Schedule Loop Nest 8 DO(i=n l,l, 
1){ DO(j=i+l, n){ DO (k=i, j){ Sl: lF(i+l=k) B(i, j, k) = c(~ + l,j,j) S2: lF(i+l <k) B(i, j,k)=B(i 
+l,j, k) S3: lF(i+j+1<2k) C(i,j,k) = C(i,j,k 1)+ B(i, j,k) } } } A 2-SL uniform schedule 7r((i,j,k), 
s) = (( i, k),loc(s)) (16) will transform Loop Nest 8 into Loop Nest 9. Example 5: Mixed Statement-Variant 
Schedule Consider Loop Nest 8 again. The following schedule transforms Loop Nest 8 to Loop Nest 10, which 
consists of imperfectly nested loops: s=s~--+ ((-i, k), Ioc(s)) T((i,j,k),s)= (17) else + ( (-i,loc(s)) 
Example 6: SSL Subdomain Schedule Another possible transformation of Loop Nest 8 Loop Nest 9 DO (i=l 
n, -1){ DO (k= i, n){ DOALL (j = 1 i,n) { S1: iF(( i+ 1= k) A(k Sj)) B( i, j,k) = C(I i,j, j) S2 : lF(( 
i+ 1< k) A (k S j)) B( i, j,k) = B(1 i,j, k) S3 : lF(( i+j+ 1< 2k)A(k <j)) C( i, j,k)= c( ij, k-1)+~( 
i~,~)} } } Loop Nest 10 DO (i=l n, 1){ DOALL ((j= 1 i,n), (k = i, n)) { SI : lF(( i+ 1 = k) A (k ~ j))~( 
i,j, k) = C(l i,j, j) S2 : lF(( i+ 1< k)A(k <j)) B( i,j, k)= B(l i,j, k)} DO (k= i, n){ DOALL (j = 1 
 i,n){ S3 : lF(( i+j+ 1< 2k)A(k <j)) C( i, j,k) = C( i, j,k l)+ B( i, j,k)} } } Loop Nest 11 DO (t=2,2n 
2){ DOALL (i= n 1,1, 1){ DOALL (j= i + l,n){ Sll: lF((2t+ 3i 3j>O) A(t+i j 1=0)) l?(i, j,t+2i-j)=c(i+l, 
j,j) S12 : lF((2t+ 3i 3jz O) A(t+2i 2j+l=O)) B(i, j, t i+ 2j) = C(i+ l,j,j) S21 : lF((2t + 3i 3j>0)A(t+i 
j 1> 0)) B(i,3 , t+2i-j)= B(i+l, j,t+2i j) S22 : lF((2t + 3i 3j~O)A(t+ 2i 2j+l <O)) B(i, j,-t i+2j) 
= B(i+ l,j, -t-2i+2j) Sal: lF((2t+ 3i 3j>O) A(2t+3i 3j 1> 0)) c(i, j,t+2i j)=c(i, j,t+2i j -l)+ B(i, 
j,t+2i j) S32: iF((2t+ 3i 3j~O) A(2t+3i 3j+l <O)) C(i, j, t i + 2j) = C(ij, t i+2j l)+B(i, j, t, i+2j) 
}}} is the schedule: i+j 2k <o+ (-2i +j + k,lw(q)7r((i, j,k), s)= ,(18) i+j 2k> 0-+ h i + 2j k,loc(S)) 
which transforms Loop Nest 8 into Loop Nest 11. Since there are two affine functions for disjoint subdomains 
of the index domain of the loop nest, each statement in Loop Nest 8 results in two guarded statements 
in the transformed loop nest. In fact Loop Nest 8 is part of the dynamic pro­gramming code presented 
in Section 5. As one can see, an SSL subdomain schedule can result in code of considerable complexity. 
It would be a very tedious and error-prone process for a user to write the code by hand. But a compiler 
can generate the new loop nest, given the schedule, and the original loop nest mechanically. 5 An Application: 
Dynamic Programming To illustrate the usefulness of the more general schedules, we take dynamic programming 
as an example, which has sequential complexity O (n3) for a problem of size n. The source code is given 
in Loop Nest 12. Loop Nest 12 DO(i=l,72-2){ DO(j=i+2, n){ C(i, j) = min;<~<j (h(C(i, k), C(k,j))) } } 
This source program is first transformed in a systematic manner by applying fan-in and fan­out reductions 
[5] to reduce potential concur­ rent accesses of variables. The result is Loop Nest 13. Then the code 
is transformed into three *lisp programs on the Connection Machine CM/2, each with the control structure 
generated by a 2-SL uniform schedule, a mixed statement­variant schedule and an SSL sub domain schedule 
respectively. We also have a sequential Common-Lisp program on the Symbolics to compute the same problem. 
The three schedules are given be­low. For simplicity, we do not give the constant terms T(S) of function 
7r. 2-SL uniform schedule: (19) m(s, (i, j,k)) = (j ~,k ~) mixed stat ement-variant schedule: s=sc~-+ 
T(S, (i, j, k)) = (j-i,k -q (20) else~j i {} SSL subdomain schedule: (i+.i-2k<o+) -2i+j+k (21) 7r(s, 
(i, j,k)) = i+j 2k>O~ i+2j k {1 Experimental Result The experiment is conducted as follows: we run the 
sequential code on the Symbolics and parallel codes on an 8K­processor Connection Machine with Symbolics 
as its host. The results described in Figure 2 and Figure 3 show that the version using an SSL subdomain 
schedule is three orders of magnitude faster than the sequential code, and is two or­ders of magnitude 
faster than the versions using a 2-SL uniform schedule and mixed statement­variant schedule. And the 
program using a mixed statement -variant schedule is about three tp four times faster than the program 
using a 2-SL uniform schedule. 6 Concluding Remarks We present in this paper a formal mathemat­ ical 
framework which unifies the existing loop transformations. We also present more general affine and piece-wise 
affine schedules which can extract more parallelism from a class of pro­ grams than the existing techniques. 
The partic­ular class of programs are those that consist of perfectly nested loops possibly with conditional 
statements where the guards as well as the ar­ray index expression are affine expressions of the loop 
indices. Although the complexity for ob­Loop Nest 13 DO (i=n-l,l, 1){ DO(j=i+l, n){ 7n=(i+j+l)/2 DO 
(~ = i,j){ S.l : lF(k <j) A(i, j,k) = A(i, j l,k) &#38;: IF(i+l =k)B(i, j,k)=C(i+l, j,j) sb~ : lF(i+ 
1< k) ~(i,j, k)= ~(i+ l,j, k) S.l : lF(m = k) C(i, j, k) = hl(A(i, j, k), B(i,j, k), A(i,j, i+j-k),13(i,j, 
i+j-k))  S.2 : lF(nz < k <j) C(i,j, k) = hz(C(i,j, k l), A(i, j,k), B(i, j, k), A(i, j, i+ j k), B(i, 
j,i+j --k)) S=:lF(k =j) C(i, j,k) =C(i, j,k 1) S=2 : IF(II = j) A(i, j,k) = C(i, j,k) }}} n 3-SL sequential 
2-SL uniform mixed stat ement-variant SSL subdomain 32 6.8 10.72 2.47 0.87 64 55.0 42.88 9.73 1.73 128 
440.0 171.50 39.16 3.48 256 3520.0 686.45 235.70 6,96 512 28160.0 2745.80 1159.24 31.70 Figure 2: Running 
time in seconds. e e 1 I 111, 1 18 4@K 32K 25K Virt%!l Pr%!&#38;sors Used Figure 3: Running time vs. 
problem size. taining these more general schedules is high [12], [9] R.M. Karp, R.E. Miller, and S. Winograd. 
we show that the generated code derived from The organization of computations for uni­ a new schedule 
can be two orders of magnitude form recurrence equations. Journal of the faster than the version from 
the existing trans- ACM, 14(3):563-590, July 1967. formations. For programs not in this particu­lar class, 
e.g. programs with pointers, compiler directives can be added into the sequential pro­grams to help the 
compiler to generate efficient parallel codes. [10] S.Y. Kung, K.S. Arun, R.J. D.V.B. Rae. Wavefront 
sor: Language, architecture, tions. IEEE Transactions 31(11):1054 1066, NOV. 1982. Gal-Ezer, and array 
proces­and applica­on Computer, References [11] L. Lamport. The parallel execution of DO loops. Communications 
of the ACM, [1] J.R. Allen. Dependence Analysis for Sub­ 17(2):83 93, Feb. 1974. script gram Variables 
and Transformation. Its Application to PhD thesis, Pro-Rice [12] L.C. Lu mation and M.C. techniques Chen. 
New loop transfor­for massive parallelism. University, Apr. 1983. Technical Report TR-833, Yale University, 
[2] J.R. Allen and K. Kennedy. Automatic Oct. 1990. translation of fortran programs to vector [13] W.L. 
Miranker and A. Winkler. Spacetime form. ACM Transactions on Programming represent ations of computational 
struc- Languages and Systems., 9(4):491 542, Ott. tures. In Computing, volume 32, pages 93 1987. 114, 
1984. [3] U. Banerjee. A theory of loop permutation. [14] D.I. Moldovan. On the design of algorithms 
Technical report, Intel Corporation, 1989. for VLSI systolic arrays. Proceedings of the [4] U. Banerjee. 
Unimodular transformations IEEE, 71(1), 1983. of double loops. In Proc. 3rd Workshop on [15] P. Quinton 
and V.V. Dongen. The mapping Programming Languages and Compilers for of linear recurrence equations on 
regular ar- Parallel Computing. UC. Irvine, 1990. rays. Technical Report 485, IN RIA-Rennes, [5] M.C. 
Chen. A design methodology for syn- July 1989. thesizing parallel algorithms and architec­ [16] M.E. 
Wolf and M.S. Lam. Maximizing par­ tures. Journal of Parallel and Distributed allelism via loop transformations. 
In Proc. Computing, Dec. 1986. 3rd Workshop on Programming Languages [6] M.C. Chen, Y. Choo, and J. Li. 
Com­ and Compilers Irvine, 1990. for Parallel Computing. UC. piling parallel programs by optimizing per­ 
formance. The Journal of Supercomputing, [17] M. Wolfe. Optimizing Supercompilers for 1(2):171 207, July 
1988. Supercomputers. PhD thesis, University of Illinois at Urbana-Champaign, Ott. 1982. [7] J.M. Delosme 
and I.C.F. Ipsen. Systolic array synthesis: Computability and time [18] M. Wolfe. More iteration space 
tiling. In cones. Technical Report RR-474, Yale Uni- Proc. Supercomputing 89, Nov. 1989. versity, 1986. 
[19] M. Wolfe. Optimizing Supercompilers for [8] F. Fernandez and P. Quinton. Extension of Supercomputers. 
The MIT Press, 1989. Chernikova s algorithm for solving general mixed linear programming problems. Tech­ 
nical Report 437, INRIA-Rennes, Ott. 1988.  
			