
 Performance Analysis of Job Scheduling Policies in Parallel Supercomputing Environments Vijay K. Naik 
Sanjeev K. Setia* Mark S. Squillante IBM T. J. Watson Research Center Department of Computer Science 
IBM T. J. Watson Research Center P. O. BOX 218 George Mason Yorktown Heights, NY 10598. Fairfax, VA 
Abstract In this paper we analyze ih? ee geneTa! classes of scheduling policies undef a workload typical 
of h ge­scale scientific computing. These policies diffeT in the manner in which pTocessom are partitioned 
among the jobs as well as the way in which jobs aTe prio%zed for execution on the partitions. OUT Tesults 
indicate that existing static schemes do not perform well un­der vaTying woTkloads. Adaptive policies 
tend to make beiieT scheduling decisions, but iheiT ability to adjust to woTkioad changes is limited. 
Dynamic partitioning policies, on the otheT hand, yield the best peTfoTmance and can be tuned to pI ovide 
desired peTfoTmance dif­ferences among jobs with vaTying Tesource demands. Introduction In recent years, 
distributed-memory parallel pro­cessing systems have matured to deliver excellent per­formance on many 
large-scale scientific applications, with the promise of a continued cost-effective growth in performance. 
These developments in both hard­ware and software technology make it clear that high­performance parallel 
systems will be an important su­percomputing base. To fulfill this role and achieve the best overall 
performance, parallel computing systems, however, cannot be used in a fashion where the entire machine 
is dedicated to a single application, as is of­ten the case. While many applications have exhibited good 
performance on distributed-memory systems, it is less clear how they will perform on a system that is 
shared by more than one job. Users of supercomput­ers expect their individual jobs to perform well. On 
the other hand, in a supercomputing environment, de­mands on the resources vary significantly among jobs 
and system resources must be judiciously distributed *Research was partially supported by the National 
Science Foundation under grant CCR-9002351, while the author was at University of Maryland 824 @ 1993 
ACM 0-8186-4340-4/93/0011 $1.50 University P. O. Box 704 22030. Yorktown Heights, NY 10598. among various 
jobs to achieve the best overall perfor­ mance. This objective raises a number of fundamental resource 
management issues for parallel supercomput­ ing environments which remain open [2]. The alloca­ tion 
of processors among the parallel programs sub­ mitted for execution, i.e., job scheduling, is one such 
issue and is the focus of our study. Since the resource requirements may vary consid­ erably among the 
jobs and since the demand on the resources may be unpredictable, in a supercomputing environment, efficient 
job scheduling that maximizes throughput while maintaining fairness among various jobs has always been 
a critical issue. In the case of su­ percomputers based on distributed parallel processing, this task 
is even more difficult because, in addition to managing the jobs, the scheduler must contend with managing 
a large number of powerful processors. Dis­ tributed parallel systems have traditionally provided only 
rudimentary facilities for allocating processors to arriving jobs. In particular, the policies most often 
used consist of static pam!itioning where the processors are divided into a fixed number of disjoint 
sets that are allocated to individual programs. This approach can lead to relatively low system throughputs 
and resource utilizations under nonuniform workloads [15, 8, 14], which is common at supercomputing centers. 
More recently, system implementations have incorporated scheduling schemes that provide some flexibility 
in re­ grouping processors into different partitions [3, 10]. Adaptive partitioning policies, where the 
number of processors allocated to a program is determined at job arrival and departure instants based 
on the current system state, have also been considered [12, 8, 14]. While these approaches outperform 
their static coun­ terparts by tailoring partition sizes to the current load, the performance benefits 
can be limited due to their inability to adjust scheduling decisions in response to subsequent workload 
changes. A dynamic partitioning policy, where the size of the partition allocated to a program can be 
modified during its execution, allevi­ ates these potential problems but at the expense of Permission 
to copy withcut fee afl or pat of this mstezial is granted, provided !hat the copies we not made or distributed 
for dired commercial advantage, the ACM copyright nciice and the title of the fshkation and its dste 
appear, and ndice is given that ccpying is by permission of !he Association for Computing Mddnev. To 
copy othewise, or to republish, requires a f-antior speMc fxxmiwicm. increased overhead due to data migration 
and recon­ figuration of the application. A better understanding is needed of the tradeoffs involved 
with the above listed general classes of sched­uling policies in parallel supercomputing systems ex­ecuting 
large-scale scientific applications. Our objec­tive in this paper is to determine a set of principles 
for the scheduling of large-scale scientific applications in distributed-memory supercomputing environments. 
We analyze the performance characteristics of the three general classes of scheduling policies described 
above under a job mix representative of large com­putational fluid dynamics applications. The analysis 
is based on steady-state performance measures from both the system and user viewpoints, as well as the 
transient behavior of these policies in response to sud­den workload changes. Our results show that the 
sys­tem scheduling policy must distinguish between jobs with large differences in execution times. We 
also demonstrate the importance of adjusting processor al­location decisions as a function of the system 
load, in­cluding fluctuations in the job arrival process, in order to provide good performance for the 
entire workload. In Section 2, we introduce our modeling analysis and evaluation methodology. Section 
3 describes the scheduling policies considered. In Sections 4 and 5 we present our results and conclusions, 
respectively. 2 Models and Analysis Current and expected near-future trends for super­computing centers 
suggest environments in which jobs with very different resource requirements arrive at var­ious intervals 
to systems consisting of large numbers of processors. Based on these trends, we define system and application 
models, as well as the performance evaluation methodology, used to investigate the issues raised in the 
int roduct ion. 2.1 System and Workload We consider a parallel supercomputing system that consists of 
P = 512 identical processors each having its own local memory module and delivering sustained execution 
rates of 100 MFLOPS in the computation­ally intensive sections of an application. It is assumed that 
the system processors can be partitioned arbitrar­ily and that the presence of other jobs in the system 
has a negligible performance impact on a running job. A general interconnection network supports informa­tion 
sharing among the processors. The network fab­ric is assumed to be such that every processor-pair is 
equidistant in the sense that, at the application level, the same amount of time is spent transferring 
informa­tion between any pair of processors. We also assume that the network bandwidth is sufficient 
to prevent any performance degradation due to the presence of multiple jobs, which is supported by experience 
with recent parallel systems. The latency per message, as experienced at the application level, is assumed 
to be 5 psec and the average achievable message transmission speed is assumed to be 100 MB/see. Arrivals 
to the system are representative of large­scale scientific applications, and are assumed to come from 
a Poisson source with mean rate ~. We con­sider a workload consisting of a mix of jobs with a wide variation 
in resource requirements, as suggested by recent studies of expected supercomputer applica­tions [13], 
Given this large variability in resource de­mands, the system can (coarsely) classify jobs as be­ing 
either small, medium or large. We model this by probabilistically determining the class of a job upon 
its arrival, i.e., an arrival belongs to the small, medium or large job class with probability y p~, 
pm and pZ, p, +pm+ PI= 1,respectively. The resource require­ments of a job are chosen uniformly among 
a set of resource demands associated with the job s class (see Section 2.2). While the primary resource 
requirements of most scientific applications are cpu cycles, memory and 1/0, in this study we only consider 
job classification on the basis of cpu demand as this is a key job characteris­tic for such programs. 
Our methodology, however, is equally applicable when other job characteristics are used. We assume that 
the job stream consists of a large number of small jobs, fewer medium jobs, and even a smaller number 
of large jobs. The probabili­ties p,, pm and pl were chosen to maintain a 320:8:1 ratio of small to medium 
to large jobs, which (approx­imately) represents the ratios of the different per-class mean execution 
times on a single processor. 2.2 Application Profile To study job stream characteristics that are repre­sentative 
of actual supercomputing environments, we conducted a detailed analysis of several large compu­tational 
fluid dynamics (CFD) applications. The par­allel applications used in our study were the NAS par­allel 
benchmarks [1] as well as the parallel versions of ARC3D [11] and INS3D-LU [16] which were orig­inally 
developed at NASA Ames. The computations in these applications are typical of those most used in the 
aeronautics and automobile industry. While they represent only one class of industrial applications, 
the above CFD codes provide sufficient insight into the characteristics of the job stream(s) arriving 
at a typi­ cal supercomputing center. The parallel applications were carefully analyzed for their properties 
on computation, communication, data dependencies, memory usage, data partitioning, load balancing and 
scalability. Algorithms were cho­sen that provide the best possible performance on a given number of 
processors. The resource require­ments of each job represent the actual resource re­quirements of a complete 
parallel application. These were obtained by detailed measurement-based analysis of each application. 
We verified the predicted perfor­mance of our analysis, by implementing the applica­tions on Victor V256 
and on a network of RS/6000 workstations at IBM T. J. Watson Research Center. Not all applications considered 
in our study, could be executed on these systems because of memory and time constraints. The resource 
requirements for these applications were determined by analysis and simula­tion. We refer the interested 
reader to [6, 7, 5] for further details on the relevant analysis. In the present study, a total of 23 
different ap­plications were chosen with grid sizes ranging from 4K through 100M. We assume that each 
parallel ap­plication is capable of executing on several processor configurations, but that there is 
a maximum number of processors, i14azaPPl, on which an application can be executed effectively. For each 
job, we determined MaxaPPl and the execution times for each possible processor configuration on which 
the job could be ex­ecuted. Since each parallel application executes most efficiently on a certain number 
of processors for a given architecture, such an optimum number of processors may not be equal to Maz=PP1. 
The optimum num­ber of processors were determined using the efficiency curves. When a job arrives at 
the system it requests the number of processors on which its performance is optimum, denoted by Req~PPl. 
Table 1 summarizes the characteristics of the 23 applications used in our study. For each job class, 
Class # of PEs Avg. Min Max Jobs sec sec sec Small 15 32 412 4 2532 Medium 4 128 4545 1589 7698 Large 
4 128 33771 18468 48688 Table 1: A summary of job characteristics. we list the number of jobs comprising 
the class and the minimum, maximum and average execution times over all jobs of the class when executed 
on the specified number of processing elements (PEs). More detailed job statistics can be found in [9, 
14]. 2.3 Performance Evaluation Our analysis of the scheduling policies considered is based on two 
metrics that measure performance from different perspectives, namely from the system and user viewpoints. 
The first performance metric, de­noted by ~, is mean job response time. Let Jn denote the nth job to 
leave the system, and define an and C$n to be the arrival and departure times of Jn, respec­tively. We 
then have where Sn = tin CYn. Note that system throughput is directly related to ~ by Little s result 
[4] and, given the above modeling assumptions, is equal to the job arrival rate A. Thus, ~ measures performance 
from the system point of view. Our second performance metric, denoted by ~, is the mean ratio of job 
response time to job service time. More formally, (2) where Un = Sn /zn and Zn denotes the service time 
of Jn on ReqaPpl processors. Note that queuing delays and not being allocated ReqaPP1 processors both 
result in differences between the values of S n and Zn. Thus, ~ reflects the performance perceived by 
the user. In addition to these steady-state measures, tran­sient values of Sn and lJn after different 
bursts of job arrivals are considered. We examine arrival bursts consisting of a single batch of small 
jobs, as well as a sustained increase in the job arrival rate. This al­lows us to compare how well the 
scheduling policies react to sudden changes in the system workload. The above performance measures are 
obtained from a stochastic discrete-event simulation which was writ­ten using CSIM. A large number of 
simulation exper­iments were performed under a variety of workload conditions. In each case, steady-state 
performance measures are obtained using the regenerative method to within 570 of the mean at 95 % confidence 
inter­ vals. Our transient analysis consists of running the simulations long enough for the system to 
reach a steady-state condition under each scheduling policy, introducing an arrival burst, and then recording 
the desired performance metric (Sn or iYn) for subsequent job departures. By steady-state condition we 
mean that the performance estimator remains within 10% of its (previously obtained) steady-state value 
(~ or ~) for at least 100 job departures prior to the arrival burst, and thus the 100 measures immediately 
pre­ceding the burst are (somewhat) representative of the performance measure under steady state. Results 
for several different sample paths are obtained by repeat­ ing these transient simulation runs multiple 
times with different random number generator seeds. 3 Scheduling Policies In this section we define 
the scheduling policies con­sidered in our study, which were first introduced in [8]. We use Avail.Y, 
to denote the number of processors available when a job arrives and &#38;fin~Y ~ to denote the minimum 
number of processors that the system may assign to a job. 3.1 Fixed Partitioning (FP) This is the simplest 
scheduling policy considered, wherein the processors are divided at system configu­ration time into a 
fixed number of disjoint sets, or par­titions, of equal size. We use FP(z x F /z) to denote the policy 
under which there are z partitions each of size P/z. When a job arrives to the system, it is allocated 
a partition if one is available. Otherwise, the job waits in a system queue until a partition becomes 
free. A jht­come jirwt-sewed (FCFS) queuing discipline is used for the system queue.   3.2 Adaptive 
Partitioning (AP) Instead of having fixed partitions, the AP policy ad­justs the size of the partition 
allocated to a job accord­ing to the current system state. When a job arrival finds more than Min ,V, 
idle processors, the system al­locates to it a partition of size min(ReqaPP1, Avail,Y, ). If fewer processors 
are idle, the job is placed in a FCFS system queue. When the execution of a job completes, the released 
processors are divided equally among the waiting jobs under the constraint that no job is as­signed more 
than ReqaPPl or less than Mins y ~ proces­sors. Any jobs remaining in the system queue must wait until 
additional processors become available due to subsequent job completions. The AP policy adapts to changes 
in system load, as reflected by the system queue length. Under light traf­fic intensities, the number 
of processors allocated to a job tends toward that requested by the job, whereas at heavy loads the job 
partition size will be smaller than the job request. 3.3 Fixed Partitioning with Job Priorities (FPJP) 
This policy is similar to the FP policy with the ex­cept ion that the scheduler includes job class informa­tion 
in its allocation decisions. Each partition is des­ignated as belonging to a certain class of jobs, and 
a separate queue is maintained for each job class. Access to a partition is prioritized on the basis 
of the parti­tion s class, such that jobs of the same class have non­preemptive priority over jobs of 
different classes. In addition, larger job classes cannot acquire partitions beIonging to smaHer job 
classes, while small jobs have nonpreemptive priority over medium jobs for large job partitions. Upon 
the arrival of a large job, the system allocates to it a large job partition if one is available. Otherwise, 
the arriving job waits in a FCFS queue associated with the large job partitions. When the execution of 
a large job completes, the available partition is allocated to the job at the head of the large job queue. 
If this queue is empty and the system contains waiting small jobs, the partition is allocated to the 
job at the head of the small job queue. When this queue is also empty, the partition is allocated to 
the job at the head of the medium job queue, if any. The medium job class is handled in a similar manner 
subject to the policy constraints defined above. A small job arrival is allocated any of the available 
partitions in the system, including those belonging to medium and large jobs. If no such partition is 
avail­able, the job waits in a queue associated with the small job partitions. When the execution of 
a small job com­pletes, the available partition is allocated to the job at the head of the small job 
queue. If this queue is empty, the partition remains free until a small job arrives. We use FPJP(z,Y,z) 
to denote the policy under which z, y and z processors are allocated to partitions belonging to small, 
medium and large jobs, respec­tively. Various (fixed) subdivisions of the per-class z, y and z allocations 
are possible under FPJP. 3.4 Dynamic Partitioning (DP) This policy is similar to the AP policy with 
the ex­ception that the partition of processors allocated to a large or medium job can be modified during 
the job s execution. We therefore assume that these ap­ plications are capable of reconfiguring themselves 
in response to requests from the operating system in a cooperative manner. The system scheduler maintains 
 a list of running jobs that can be reconfigured, which we call the interruptible list. After a medium 
or large job reconfigures itself, the job is removed from this list and is not eligible for reconfiguration 
until a certain time interval, denoted by T, has elapsed. If an arriving job requests at most Avail$v, 
proces­sors, then the job is allocated a partition of size Req.PPl. However, if Req.PPl > Availsv. ~ 
Min,Y~ and the interruptible list does not contain a job whose partition size is greater than P/N, where 
N denotes the current number of jobs in the system, then the job arrival is allocated all of the available 
processors. Otherwise, the system scheduler notifies one or more of the jobs on the interruptible list 
with partitions larger than P/N to reduce their partition size to P/N, pro­vided that P/N ~ Min~Y~. To 
minimize the number of interrupted jobs, the system attempts to free up just enough processors so that 
the total number of available processors is close to min(Req.PPl, P/N). Upon the release of processors, 
including that due to a job completion, the available processors are di­vided equally among the waiting 
jobs, if any, under the constraint that no job receives less than Min,Y, or more than Req~PPl processors. 
If the number of available processors exceeds a policy parameter M and the interruptible list contains 
a job whose cur­rent partition size is less than P/N, then the system scheduler not ifies that job to 
increase its allocation to min(P/N, S + kf/2, ReqaPPz), where S denotes the job s current partition size. 
Thus, at most half of the available processors are allocated to one of the large or medium jobs on the 
interruptible list. Note that the policy parameters M and T can be used to control the rate at which 
large and medium jobs are reconfigured. When a job is notified by the scheduler to modify its partition 
size, the application initiates a reconfig­uration at the next programmer-defined reconfigura­tion point, 
e.g., at the end of the current iteration. This reconfiguration is realized in two phases. In the first 
phase, the entire data set representing the cur­rent state is transferred from all processors in the 
old part ition to a subset of Min3Y, processors in the new partition. In the second phase, the data set 
is then redistributed from this subset to all processors in the new partition. This two-phase scheme 
is used for both increasing and decreasing the partition size. If the par­tition size is to be increased, 
the application adds the newly-acquired processors to its partition before com­mencing the first phase 
of reconfiguration, whereas if the partition size is to be reduced, the application re­leases the excess 
processors to the system scheduler at the end of the first phase. 4 Results In this section we consider 
the performance charac­teristics of job scheduling policies within the context of our modeling analysis, 
as described in Sections 3 and 2, respectively. Our main objective is to deter­mine a set of principles 
for the scheduling of large­scale scientific applications in distributed-memory su­percomputing environments. 
The performance mea­sures of interest are mean job response time (i.e., ~), mean ratio of job response 
time to job service time (i.e., ~) and transient values of Sn and Um. Our re­sults, a portion of which 
follow, were obtained as de­scribed in Section 2.3. Additional results can be found in an extended version 
of the paper [9]. Throughout our experiments, we assumed that processors were allocated in units of 4 
and that Min,gs = 32. The FP(z x 512/z) policies consid­ered are those for which x c {1, 2,4, 8, 16}. 
Several FPJP(z,y,z) policies were considered, most notably those with z E {128, 192, 256}, y E {128, 
192} and z E {128, 192} such that z+y+z = 512, We also con­sidered various subdivisions of the per-class 
partitions under FPJP, as well as different values for M and T under DP. The results presented in this 
section are for the parameters that provided the best performance. 4.1 System Performance Our first 
set of results compares system perfor­mance under each of the different scheduling policies as measured 
by ~. We observe that the optimal par­tition size for the FP policies, i.e., that which pro­vides the 
smallest ~, varies with the job arrival rate. When A is small the FP(l x 512) policy yields the best 
performance, but as system load increases, first the FP(2 x 256) policy and then the FP(4 x 128) policy 
pro­vide the smallest ~. We also observe that the AP policy performs as well as the best FP policy for 
a given value of A. These results, which are not shown in the interest of space, demonstrate that the 
system scheduling policy must reduce the number of proces­sors allocated to each job with increasing 
system load. Similar trends have been observed under a different workload mix [8, 14], as well as under 
different system and application assumpt~ns [12, 15]. In Figure 1, we plot S as a function of A for the 
AP, FPJP(128,128,256), FPJP(128,192,192) and DP policies. Similar results for equation (1) taken over 
the individual small, medium and large job classes are plotted in Figures 2, 3 and 4, respectively. As 
noted  ooo~  ~oo, :.001 0.002 0.002 Job Arrival Rate (A)  Figure 1: Mean System Performance as a 
function of A ~nder AP, FPJP and DP. 14G0 1200 1000 800 Im 600 4G0 zoo T Figure 2: Mean System Performance 
of Small Jobs as a function of A under AP, FPJP and DP. above, the AP curves also illustrate the performance 
of the best FP policy for a given arrival rate. More­over, since the FPJ P results are for optimal subpar­tition 
sizes, and since the AP policy performs as well as the best FP policy, the FPJP curves represent the 
performance of a policy under which each of the per­class partitions are subdivided in an adaptive manner 
similar to the AP policy. We observe that the AP policy performs poorly in comparison to the FPJP and 
DP policies, particularly for the small job class. This is because the AP policy does not distinguish 
between the different job classes in its allocation decisions. When J is small, the policy II :,001 
0.002 0.003 0.004 0 00s 0.006 0,007 0,008 0.009 Job Arrival Rate (~) Figure 3: Mean System Performance 
of Medium Jobs as a function of A under AP, FPJ P and DP. 120000, 1 ;.001 0.002 0.003 0.004 0.005 0.006 
0,007 0.008 9 Job Arrival Rate (J) 01 I 0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.oo&#38; 0.009 Job 
Arrival Rate (A) Figure 4: Mean System Performance of Large Jobs as a function of ~ under AP, FPJP and 
DP. tends to allocate a large number of processors to each arriving job because the number of available 
proces­sors is relatively large while the number of waiting jobs is relatively small. Allocating large 
partitions to medium and large jobs decreases their execution time, and hence their ~, but tends to increase 
the time small jobs must wait in the system queue for processors to become available. The performance 
impact of this be­havior increases with A, and even the response times of medium and large jobs suffer 
due to being executed on smaller numbers of processors. These results show that the system scheduling 
policy must distinguish be­tween jobs with large differences in execution times when making allocation 
decisions. The FPJP policies eliminate this behavior by re­ stricting the maximum number of processors 
acquired by a medium and large job. In particular, these poli­ cies perform quite well across most A 
ranges for each of the different job classes. Increasing the number of processors belonging to a specific 
class decreases ~ for that class at the expense of increasing response times for the other classes, as 
expected, Since small jobs can acquire partitions belonging to any class, and have nonpreemptive priority 
over medium j obs for large job partitions, ~ for small jobs is less sensitive to changes in the partition 
sizes for medium and large jobs. We observe that the DP policy also eliminates the problems identified 
above for the AP and FP policies. In particular, the DP policy provides the best perfor­tiance for small 
jobs. This is because the scheduler can interrupt a running medium or large job and al­locate some of 
its processors to a newly arrived small job, thus reducing the time small jobs wait in the sys­tem queue. 
The DP policy also yields the best per­formance for medium and large jobs when A is small because a relatively 
large number of processors are al­located to these job classes under light loads. This policy outperforms 
the FPJP policies for these loads because medium and large jobs can utilize processors that would otherwise 
be idle under the FPJP poli­cies. As ~ increases, however, the best performance for small jobs is obtained 
at the expense of a signifi­cant increase in the mean response time for medium and large jobs. We note 
that the increase in ~ for these larger job classes at heavy system loads is not due to the recon­figuration 
overhead. In [9, 14] we provide reconfigu­ration stat istics for large and medium jobs obtained from 
our simulations of the DP policy. While not in­significant, these overheads represent a relatively small 
fraction of the response times for medium and large jobs. The increase in ~ results from the fact that 
the average partition size for these classes at heavy sys­tem loads is much smaller than their partition 
sizes under the FPJ P policies. When the traffic intensity is high the number of small jobs in the system 
tends to be large. This decreases the number of processors al­located to medium and large jobs under 
DP. We also observe that the average number of reconfiguration that reduce the size of a partition is 
somewhat greater than those which increase the partition. 4.2 User Performance Our next set of results 
compares the performance of each of the different scheduling policies as measured by ~. From the user 
viewpoint, this is the measure of interest as it represents the response time relative to the inherent 
resource demands of the job. In Figure 5 we plot the values of equation (2) taken over the indi­vidual 
small, medium and large job classes under the DP policy as a function of J. Similar results for the 
I I ;.001 0.002 0.002 O.OIM 0005 0.006 0.007 0.008 0.009 Job Arrival Rate (A) Figure 5: Mean User Performance 
for each Job Class as a function of A under DP. 22, I 20 ­18 ­16 small + Medum +. i-age .m­14 ­12 ­10 
8­6­4 2[ ~-.=?:::::.:::::.-.:::~::~.­ ..............9 .......-. ..-. ..-.. ----­f II :.001 0.002 0.003 
0 0C4 0.005 0.006 0.007 0008 0.009 Job Arrival Rate (A) Figure 6: Mean User Performance for each Job 
Clam as a function of A under FPJP(128,128,256). FPJP(128,128,256) policy are provided in Figure 6. 
Note that these curves illustrate a relative factor by which the service received by jobs changes with 
system load. We first observe that the values of ~ across all job classes are comparable when A is small. 
Since con­ tention for the processors is small under both policies at light loads, each job spends relatively 
little time waiting in the system queue for a partition and the number of processors it eventually receives 
is relatively close to ReqaPP1. As expected, ~ increases with sys­tem load. This is due to the fact that 
the number of jobs in the system increases with A, thus increasing the time jobs spend waiting in the 
queue. We note that the DP policy tends to reduce the amount of time a job spends in the system queue, 
at the expense of potentially increasing the job exe­cution time by allocating it a relatively small 
num­ber of processors. Since ikfin~v, is not too small (.ilfin,v, = 32), the system queuing time is the 
domi­nant factor in the response time of small jobs. By re­ducing this factor, DP yields the best ~ for 
the small job class across all system loads. The job execution time, however, is a relatively important 
factor in the response time of medium and large jobs. Since the DP policy decreases the number of processors 
allocated to jobs with increasing A, the job execution time becomes the dominant factor in the response 
time of medium and large jobs at heavier system loads. This explains the sharp increase in ~ for these 
larger job classes with increasing A. We observe that the converse is true under the FPJP policies where 
the ~ value for small jobs in­creases very sharply with system load (note the differ­ence in scale with 
Figure 5) while its value for the large job class remains almost constant (increasing slightly at A = 
0.009). The FPJP policies restrict the mini­mum number of processors allocated to medium and large jobs, 
thus limiting the rise in execution times for these job classes. This is obtained, however, at the expense 
of increasing the amount of time jobs spend wait ing in the system-queue, which has a major impact on 
the response time of small jobs. 4.3 Transient Analysis Our next set of results compares how well the 
sched­uling policies react to sudden changes in the system workload. Bursts of increased arrival activity 
tend to be transitory in most supercomputing environments, To address this situation, as well as compare 
how quickly the effect of an arrival burst dissipates un­der the different policies, we consider the 
case where the job arrival rate remains fixed while the system in­curs a one-time burst consisting of 
a single batch of small jobs. Several different batch sizes were exam­ined. The following results are 
for a batch size of 10, as these highlight the trends exhibited for all of the batch sizes considered. 
In Figure 7 we plot the values of Ut+$ under the DP policy for O ~ s ~ 400 and ~ = 0.007, where the arrival 
burst occurs at time fit+loo and t repre­sents a job departure that satisfies the steady-state condition 
(see Section 2.3). Results for several dif­ 140 ­ 120 ­ ., 100 1 lb 80-. 60 - II  ;~ ..; {..,;.+ 
:. !/: , ~.,,­ : ~ . ,,j, .,,  *k.&#38;@]j.;, o 50 100 150 200 250 300 350 400 Job Departure Index 
(s) Figure 7: Transient User Performance as a function of Job Departure Index under DP with a Single 
Arrival Burst. ferent sample paths were obtained by repeating the transient simulation runs 10 times 
with unique ran­dom number generator seeds. Thus, 10 different Ut+. data points are reported for each 
job departure index s. Each of these sample points are illustrated as dots in the figure. The dashed 
line depicts the average of the 10 data points for a given value of s. Similar re­sults for the FPJP(128,128,256) 
policy are provided in Figure 8. We observe that the FPJP policy yields a significantly higher variance 
in the values of Ut+S than those obtained under DP, even in steady-state (i.e., O ~ s ~ 100). We also 
find that the impact of the single burst under the DP policy has a smaller mag­nitude for both the sample 
points and their averages, and dissipates much more rapidly than under FPJP. Increasing the batch size 
magnifies these trends. To further examine how well the policies react to sudden workload changes, we 
have also briefly consid­ered the case where the burst consists of a sustained increase in the job arrival 
rate. Our preliminary simu­lation results suggest that the DP policy adapts more quickly than FPJP to 
the sudden change in workload. Moreover, the magnitude of performance degradation under the FPJ P policy 
is much greater than that re­alized under DP. Our results demonstrate that the DP policy tends to handle 
periods of low and high traffic intensities 140 i 320 .. .lC$J lb m 60-,, \.i 40-,, . 0 0 50 100 150 
200 250 3@iJ 350 400 Job Departure Index (s) Figure 8: Transient User Performance as a function of 
Job Departure Index under FPJP(128,128,256) with a Single Arrival Burst. better than the other scheduling 
policies considered. This is due to the fact that only DP can take ad­vantage of idle processors during 
such periods of low activity by dynamically increasing the size of the par­titions allocated to long-running 
jobs. Similarly, the DP policy reacts the fastest to any sudden burst in arrivals by quickly adjusting 
its current allocations of processors, whereas the other policies must wait un­til the jobs in execution 
depart before any changes in processor allocations are possible. 5 Conclusions Current and near-future 
trends for supercomputing centers suggest environments in which jobs with very different resource requirements 
arrive at various inter­vals to systems consisting of large numbers of proces­sors. In this paper we 
investigated the impact of these trends on general classes of job scheduling policies. Our main objective 
was to determine a set of principles for the scheduling of large-scale scientific applications in distributed-memory 
environments. We conclude by summarizing these scheduling principles and by briefly discussing how an 
actual implementation can be de­signed to incorporate these principles in its processor allocation decisions. 
Our analysis demonstrates that the system sched­uling policy should reduce the number of processors allocated 
to each job with increasing load, up to a minimum number of processors, in order to provide good performance 
for the entire workload. These processor allocation decisions should distinguish be­tween jobs with large 
differences in execution times, where increasing the performance of a particular job class, from both 
the system and user viewpoints, tends to be obtained at the expense of decreasing the per­formance of 
the other classes. Our analysis further demonstrates the importance of having a scheduling policy that 
adapts appropriately to periods of low and high arrival activity, a common scenario in most su­percomputing 
environments. The DP policy is one approach that is capable of realizing all of these key scheduling 
objectives. Our results show that DP yields the best performance for small jobs while exhibiting poorer 
performance for the medium and large job classes. The converse is true un­der the FPJP policy. It is 
important to note, however, that the benefits of both the DP and FPJP policies can be obtained by appropriately 
choosing the minimum partition sizes on a per-class basis (instead of a single ikfin,Y, value) under 
DP, as well as by adjusting the parameters that control the rate of reconfiguration. In this manner, 
the desired performance differences among the various job classes can be achieved under a judiciously 
parameterized DP policy, without impos­ ing burdensome demands on the users or the system administ rat 
or. Acknowledgements We would like to thank Phil Heidelberger for fruit­ful discussions regarding the 
transient analysis of our study. References [1] D. Bailey, J. Barton, T. Lasinski, and H. Simon. The 
NAS Parallel Benchmarks. NASA Ames Re­search Center Technical Report No. RNR-9 1-002, 1991. [2] R. Carter, 
C. Kuszmaul, and B. Nitzberg. Sys­tem level requirements for multidisciplinary ap­plications on highly 
parallel systems. In Compu­tational Aerosciences conference, 1992. [3] The Connection Machine CM-5 Technical 
Sum­maTy, Thinking Machines Corp., Cambridge, Mass., 1992. [4] J. D.. C. Little. A Proof of the Queuing 
Formula L = A W . Operations ReseaTch, vol. 9, 1961. [5] N. H. Naik, V. K. Naik, and M. Nicoules. Par­allelization 
of a class of implicit finite difference schemes in computational fluid dynamics. Int. J. [12] E. Rosti, 
E. Smirni, G. Serazzi, L. Dowdy, and High-speed Computing, vol. 5, 1993. B. Carlson. Robust partitioning 
policies of mul­tiprocessor systems. Technical report, Dept. of [6] V. K. Naik. Performance Effects of 
Load Imbal-Computer Science, Vanderbilt University, 1992. ance in Parallel CFD Applications. In Proceedings 
of the fifth SIAM Conference on Parallel Process-[13] R. Schreiber and H. D. Simon. Towards the ter­ing, 
1992. aflops capability for CFD. In H. D. Simon, editor, PaTallel CFD -Implementations and Results Us­ 
 [7] V. K. Naik. Scalability issues for a class of CFD ing PaTallel Computers, MIT Press, 1992. applications. 
In Proceedings of the Scalable High PeTfoTmance Computing Conference, 1992. [14] S. K. Setia. Scheduling 
on Multiprogrammed Dis­tributed Memory Parallel Computers. Ph.D. Dis­ [8] V. K. Naik, S. K. Setia, and 
M. S. Squil­sertation, Department of Computer Science, Uni­ lante. Scheduling of Large Scientific Applica­versity 
of Maryland, August 1993. tions on Distributed Memory Multiprocessor Sys­tems. In P? oceedzngs of the 
sixth SIAM Confer-[15] S. K. Setia and S. K. Tripathi. A compara­ence on PaTallel Processing foT Scientific 
Com-tive analysis of static processor partitioning poli­puting, 1993. cies for parallel computers. In 
Proc. of Inii. Workshop on Modeling and Simulation of Com­ [9] V. K. Naik, S. K. Setia, and M. S. Squillante. 
puter and Telecommunication Systems (MAS- Performance Analysis of Job Scheduling Policies COTS 93), 1993. 
 in Parallel Supercomputing Environments. IBM Research Report RC 19138, 1993. [16] S. Yoon, D. Kwak, 
and L. Chang. LU-SGS Im­plicit Algorithm for Three-Dimensional Incom­ [10] PaTagonTM XP/S product Ovemiew, 
Intel pressible Navier-St ekes Equations with Source Corp., Beaverton, OR, 1991. Term. AIAA Paper 89-1964-CP, 
1989. [11] T. H. Pulliam. Efficient Solution Methods for Navier-Stokes Equations. Lecture Notes for 
the von Karman Institute for Fluid Dynamics Lecture Series: Numerical Techniques for Viscous Flow Computation 
in Turbomachinery Bladings, 1986. 
			