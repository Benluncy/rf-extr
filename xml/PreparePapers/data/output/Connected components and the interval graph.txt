
 Connected Components and the Interval Graph Department University E. F. Grove* of Computer of California, 
Science Berkeley Abstract We consider the problem of finding in par­allel the connected components of 
an undi­rected graph when the computation is asyn­chronous. An algorithm is presented that uses a linear 
number of processes and time complex­ity O(log n) in the APRAM [CZ89] model. In addition, the algorithm 
computes a spanning forest for the graph. Previously published al­gorithms, even those running on a concurrent 
CRCW PRAM, are more complicated than the one we present. Moreover, the proof techniques are sufficiently 
robust for application to other models of asynchronous parallel computation. Introduction Cole and Zajicek 
[CZ89] introduced a model of asynchronous computation they call the APRAM model. The Asynchronous Parallel 
Random Access Machine allows processes to read from or write to a global shared ran­dom access memory. 
AH communication be­tween processes is accomplished by writes to *Supported by NSF grant CCR9058440 Permission 
to copy without fee all or part of this material is granted provided that the copies are not made or 
distributed for direct commercial advantage, the ACM copyright notice and the title of the publication 
and its date appear, and notice is given that copying is by permission of the Association for Computing 
Machinery. To copy other­wise, or to republish, requires a fee and/or specific permission. SPAA 92-61921CA 
@992 ACM 0-89791-484-8/92/0006/0382 . . . . . .. $1.50 382 and reads from the global memory. There are 
no concurrent actions in the APRAM model. Each read or write event is given a time, and these times must 
form a total order. A com­putation is correct if for all consistent orders of events it produces the 
correct output. I.e., the APRAM model explicitly incorporates se­rializability. The APRAM model also 
allows multiple reads or multiple writes to be made as single simultaneous operation. Cole and Zaj icek 
pre­sented an algorithm for finding connected com­ponents that uses a linear number of processes with 
time complexity O(log n) in the APRAM model. The assumption that multiple operations can be performed 
as a single operation is re­ally a synchronicity assumption, and should be avoided in asynchronous algorithm 
design. An algorithm that uses this feature is called weak. Cole and Zajicek s algorithm is weak. The 
con­nected components algorithm presented in this paper uses a linear number of processes, time complexity 
O(log n), and is not weak. In ad­dition, the algorithm and proofs presented in this paper are simpler 
than those in [CZ89]. Consider the following situation. Initially, ~ = O and y = (). Then we execute 
two pro­ cesses in parallel. Process 1 Process 2 Z(-I ?J+l IF (y=O) IF (r =O) doA doB In the APRAM model, 
it is not possible to exe­cute both A and B. In practice, Process 1 may be using a two-level memory, 
and Process 2 may have access only to the slower memory. As a result, it may take some time for the write 
to x by Process 1 to propogate to Process 2. As a consequence, in this case it would be pos­sible that 
both A and B would be done. The algorithm we present correctly computes the connected components of an 
undirected graph in this situation. In the conclusion we will dis­cuss the assumptions needed in the 
model in order to obtain logarithmic time bounds using a linear number of processes. 2 Measuring Time 
The definition of the time complexity of an al­gorithm is a central issue to the definition of a model 
of asynchronous computation. In the APRAM model, a separate event is associated with the completion of 
each instruction by any process. A consistent computation is a total or­der {ei} of events with two properties. 
Firstly, a read event on a memory cell x retrieves the last value written, i.e. the value that comes 
from read event er on memory cell z is the value written to z at event eW, where w is de­fined by w = 
rn<~x{i I e~ is a write event to cell z}. Secondly, the events corresponding to the ex­ecution of a 
single process must be consistent with the program being run by the process. For a finite computation, 
we consider a vir­tual clock that ticks before the first event, and then ticks each time all of the processes 
have had an event (executed an instruction) since the previous tick. The sequence of events be­tween 
clicks is called a round. In each round, there is at least one event from every process. The measure 
of time for the computation is the number of rounds. The complexity of an algo­rithm on a particular 
input is the maximum over consistent computations of the number of rounds used, or infinity if there 
is a consistent unbounded computation. 3 The Algorithm The input is an undirected graph G = (V, E) where 
V = {l,...,n} and IEI = m. We are pre­sented with a 2-dimensional array endpt defin­ing the input edges: 
13 = {{endpt(O, i), end@(l, i)} I 1< i< m}. The output is an array dad for which dad(dad(v)) = dad(dad(tu)) 
if and only if v and w are in the same connected component of G. In [SV82], Shiloach and Vishkin gave 
an algorithm to find the connected components of a graph that runs in time O(log n) on a synchronous 
PRAM. Their algorithm makes strong use of the fact that on a synchronous PRAM you can determine the disjunction 
of a set of boolean literals in constant time. In the first step, clear a memory cell. Let there be a 
process for each literal. In the second step, each process writes a 1 to the memory cell if its literal 
is true. Now the cell contains a 1 if and only if the disjunction of the literals is true. In an asynchronous 
model this technique does not work. If the memory cell value is 07 it is never clear that the disjunction 
is false, because it may be that a process assigned to a true literal has paused in executing its instructions. 
The general approach in [SV82] is to main­tain a forest A of connected subsets of the input graph G = 
(V, E). Let there be one process for each edge, and one for each vertex. Irli­tially A contains n singletons. 
In each phase of the algorithm, each edge process e tries to connect the roots of the trees in A that 
contain the endpoints of e. This is done in a manner that guarantees no cycles will be formed. Then the 
vertex processes do two rounds of pointer jumping to reduce the heights of the trees. Our approach is 
similar. We maintain a di­rected graph P, the pointer graph, for which the out-degree of any vertex is 
at most one. The big difference from [SV82] is that we allow cycles in P. The edge processes try to con­nect 
components of P to each other, and ver­tex processes do a variation of pointer jumping. Several arrays 
are used to coordinate these ac­tions: dad, hook, index, and early. The dad and index arrays have size 
n, hook has size n + m, and early has size m. The dad array represents P, in which v has an edge directed 
to dad(v) if v # dad(v). At the end of the algo­rithm, P will be a forest of trees of edge height at 
most 2. Each tree contains exactly the set of vertices from one of the connected components of G . The 
algorithm starts by initializing the data structures. For clarity, we omit the bookkeep­ing necessary 
to check when the initialization is complete. The bookkeeping is easy to imple­ment, and takes O(log 
n) rounds to complete after the initialization is finished. Procedure VertexInit (o): dad(v) i--v index(v) 
~ v hook(v) t O Procedure EdgeInit (e): early(e) t O Initially P contains n singletons, We assign one 
process to each edge and one to each vertex. Edge process e writes its name to hook(index(v)) to suggest 
to vertex process v that the component containing v should be merged with another component of P. If 
the vertex process takes the suggestion, it will up­date dad(v) to point to the endpoint of process e 
s edge determined by 1 early(e). We call this a hook. The 2-dimensional array endpt ini­tially contains 
the edges from the input graph 384 G, and is updated in a way that, in conjunction with dad, preserves 
the connectivity informa­tion G. Call a vertex v a root if v = dad(v). Each vertex process checks to 
see if v is a root. If so, it changes dad(v) if some edge process has left a suggestion in hook(index(v)). 
Once a vertex process has read the name of an edge out of hook(i), it switches to a new index. We use 
restricted pointer jumping to reduce the depth of components of P. If dad(v) < v and dad(v) < dad(dad(v)) 
we do not pointer jump, otherwise we set dad(v) t dad(dad(v)). Pointer jumping on an even length cycle 
dis­connects the cycle. Restricted pointer jump­ing avoids this problem by ensuring that after the pointer 
jumping on a cycle, every vertex can still reach the smallest vertex of the cy­cle. Each vertex process 
for which v is not a root does restricted pointer jumping. Johnson and Metaxas also use this method to 
break cy­cles when finding connected components on a CREW PRAM in 0(log3/2 n) time [JM91]. Procedure 
Vertex(v) : local variables: e, d, g IF (v = dad(v)) e + hook(indez(v)) IF (e # O) dad(v) t endpt(l 
ear~y(e), e) hook(e + n) t O indez(v) - e + n ELSE d t dad(v) g t dad(d) IF ((v < d) OR (d > g)) dad(v) 
e-g The global arrays may change at any time, so a value that will be used more than once in a pass 
through the code is stored in a variable local to the process that is guaranteed not to change during 
the pass. Each edge process can succeed in causing at most one hook, so when e succeeds in a hook, hook( 
e + n) has only re­ceived the write to O during initialization. It is a clean suggestion box for process 
v. Each edge process repeatedly updates endpt(O, e) to dad(endpt(O, e)) and endpt(l, e) to dad(endpt(l, 
e)). The process terminates when it notices that endpt(O, e) and endpt(l, e) are in the same component 
of ~. When both endpt(O, e) and endpt(l, e) are roots, we sug­gest to one of the endpoints to point to 
the other. It is necessary for the running time that the root that was last recognized as being a root 
should be the destination of the suggested hook. We use ear/y(e) to remember which end­point was less 
recently recognized as being a root. The suggest ion to hook is sent by writ­ing e into hook(indez(endpt( 
ear/y(e), e))). Procedure Edge(e) : local variables: root, vO, V1, dO, dl root 4--endpi(early(e), e) 
IF (dad(root) # root) early(e) + 1 earzy(e) VO -endpt(O, e) dO t dad(vO) vl t endpi(l, e) dl t ckzd(vl) 
IF ({vO, dO} n {vi, dl} #0) HALT() IF ((vO = dO) AND (v1 = all)) hook(indez(endpt( early(e), e))) + e 
ELSE endpt(O, e) -dO endpt(l, e) +-dl It remains to detect when the work is fin­ished. One method would 
be to wait until you detect all edge processes have halted, and then halt vertex process v when dad(v) 
is stable. dad(v) will not change again (assuming all edge processes have halted) if either (1) dad(v) 
is a root or (2) dad(dad(v)) is a root, v < dad(v), and dad(v) > dad(dad(v)). These termination conditions 
were not included in the pseudo­code above for the sake of clarity. Cleaning up after the processes that 
have fin­ished their work will just take an additional O(log n) rounds. It suffices to analyze the num­ber 
of rounds required to guarantee that all edge processes have halted and every compo­nent of P is a tree 
of edge height at most 2. 4 Definitions Recall the directed graph P = (V, E(P)) with E(P) = {(v, dad(v)) 
I v # dad(v), v 6 V}. In order to analyze the running time of this algorithm, we will define a potential 
function which measures the sum of the depths of the connected components of P. We will see that the 
potential is initially linear and drops by a constant factor each round. In addition, when the potential 
is small, the output is stable and correct. This gives us the logarithmic time bound we desire. Let REACH(V) 
be the set of vertices reach­able from v in the dad array. REACH(V) sat­isfies REACH(V) = {v} U REACH(dad(v)). 
 For a weakly connected component C of P, let CAND(C) = {v G C I v = min(REAC17(v))}. A vertex v E CAND(C) 
is called a candidate. The largest candidate in a component of P is the leader of the component. For 
a candidate v, define the followers I (v) = {w < V I v = min(REACll(w)). Restricted pointer jumping 
does not change I (v). The candidates are interesting because repeated restricted pointer jumping (when 
there are no hooks) would eventually stabilize with dad(w) = v for each w ~ F(v) \ {v}. T. is the tree 
on ~(v) induced by P. An example may help make things clearer: 1 2 4 65 3 78 9 10 da;(v) 849872196 7 
246 tt 39 t 8 m T2 TAT6 In the previous example, the candidates are {1, 2,4, 6} and 6 is the leader. 
X( TV) indicates whether there is some ver­tex w in T. that could receive a hook that would change CAND(C). 
It is 1 if v is not the leader and for some w G TV there is a com­putation consistent with the current 
configura­tion in which some vertex process z will change dad(z) from x to w before the next time (if 
it exists) that w becomes a leader. Otherwise it is O. 1 1 if3w~TV, x~V: z might hook to w before the 
next time w is leader X(TV) = and v is not a leader { [ O otherwise Consider dad(v) for v < CAND(C). 
Except for the leader, each dad(v) lies in some tree TW, with v< w. TL(C) = {(v, w) [ dad(v) E F(w), 
w G CAJVD(C) \ {V}}. We call the edges in TL(C) tree links. We as­sign weights to the tree links: 3 if 
w = dad(v) wt((v, w)) = 4 otherwise { 386 We need a non-negative height function on trees that reduces 
by a constant factor when each vertex process executes restricted pointer jumping. We measure the height 
of Tv to be 2 less than the maximum number of vertices on a simple path from a leaf to the root: h(TV) 
= max{O, vtx-ht(Tv) 2}.  5 Interval Graph The key to the analysis of the algorithm is to measure the 
potential in terms of an interval graph. Two tree links (V1, WI ) and (vz, Wz) are independent if the 
corresponding open inter­vals of real numbers (VI, W1) and (vZ, WZ) do not intersect. For a connected 
component C of P, the maximum weight of an independent set of tree links is Wt(c) = max Z+) . id set 
Z~T,L(C) ~61 {} In the earlier example, the maximum weight independent set 1 would be {(2, 4), (4,6)} 
with a weight of 3+4=7.  6 Analysis We maintain an invariant. The active edges are ACTIVE = {(endpt(O, 
e), endpt(l, e)) I edge process e has not halted}. Lemma 1: Let 1? = AC T1Vl!7 U E(P). Then the graph 
(V, E ) has the same con­ nected components as the input graph G. Proofi Initially E = E. We consider 
the possible changes to E : Restricted Pointer Jumping: We noted earlier that restricted pointer jumping 
does not change the connected components of P.  endpi!(i, e) t dad(endpi(i, e)): There in no change 
since we already have (endpt(i, e), dad(endpt(i, e))) 6 E . This is guaranteed because no other process 
writes to endpt(i, e). v Hooks To W: If the edge process e that suggested the hook has halted, v and 
w are in the same component of E(P). Otherwise, (v, w) = (endpt(O, e), endpt(l, e)) was al­ready in ACTIVE. 
Edge Process e Halts: The edge process halts only when (encZpt(O, e), endpt(l, e)) links two ver­tices 
in the same component of P. (endpt(O, e), endpt(l, e)) is redundant to the connected components of E 
. The previous proof did not require the to­tal order on events guaranteed by the APRAM model. All that 
was used is that for a single process, the reads and writes to the same mem­ory location succeed in the 
order they are made by the process. The algorithm computes cor­rectly even in the presence of slowly 
propagat­ing writes. By lemma 1, we know that when all edge processes have terminated, the connected 
com­ponents of G are just the connected compo­nents of -P. We need to bound the number of rounds required 
before all edge processes have terminated and all of the components of P are trees of edge height at 
most 2. The obvious approach is to try to bound the sum of the lengths of the longest paths in the components. 
The main complication is that as a result of a hook from the leader of component C to another vertex 
in C , the longest path in C could double in length. To understand how much the path lengths can increase, 
we look at the interval graph of the tree links. As an example, consider an example where the tree links 
form two chains that meet at the leader, al < az < ... <at=land b1<b2<... b~=l. Such chains must be monotone, 
by the defini­tion of tree links. Then we could have a path of length 2t if 1 hooks onto bt. But if the 
two chains came from a concurrent pointer jump on an original chain al< bl<a2<b2... <at= t then we would 
not be able to create a monotone chain of length more than t. The independent sets in the interval graph 
capture exactly what we need. Instead of looking at paths in each compo­nent C, we break the components 
up into the trees Z v (for v c CAND(C)) and focus our at­tention on the sum of the heights of these trees. 
If there were no hooks and lots of pointer jump­ing, we would see the tree heights drop by a constant 
fraction each round. Hooks make the analysis interesting. Consider what happens when the leader 1 of 
C hooks to some z in C. Unless noted other­wise, all notation will refer to the configuration before 
the hook occurs. Let v c CAND(C) be the smallest vertex reachable from Z. After the hook, all of the 
vertices in the component can reach v. CAND(C) changes to no longer in­clude any vertex larger than w, 
and v becomes the new leader of C. This means that all of the trees {TW : v s w, w E CAND(C)} merge to 
form the tree T; of followers of v. We can split the longest path p in T; into several pieces. The pieces 
are paths taken from the TW S and tree links: p= PwllGu, lPw21 . . . I Guk [ Pwk I (Lx) I P%+, In this 
enumeration, wk = ~ and ~k+l = v. The path p follows the path pWl in TW,, then the tree hnk tWz mto !l 
~z, and then the path pW2 in TW,, ... The number of vertices on the path p is ~ vtx-length(p~) S ~(lz(!i! 
~, )+2). We need to use the drop in FVt ( C) to overcome the increase for h(T~). We are ready to define 
the potential of a component C of F . We want the potential to drop when there is pointer jumping, and 
also to drop when the leader hooks to some other vertex in the component. We can guarantee this by defining 
 o(c) = 12+wt(c)+ ~ h(T.)+8*x(Tv). VCCAND(C) The rest of this section will be rather involved and full 
of notation. The ideas are straightfor­ward. Pointer jumping will reduce the poten­tial of each tree 
by a constant fraction. The tree links being counted in Wt(C) either point to trees whose potential is 
dropping or they point near the root of a tree. For the tree links not pointing near the roots of trees, 
we can count part of the drop in potential of the tree to cover a drop in potential for the tree link. 
For the links pointing near the roots of trees, the tree links will pointer jump and reduce the size 
and/or weight of the independent set of tree links, We need the terms x(TU) to guar­antee a drop when 
a leader hooks back into its component, and we need an additional con­stant term to cover the costs of 
merging two components. The rest of this section is devoted to going through the calculations in gory 
detail. Call 1 the independent set of maximum weight in TL(C). We can break 1 into three pieces: 10= 
{(Y, z) E TL(C) Iy < z< v}, II = {(y, z)cT-L(C) Iv <y< Z}, and perhaps a tree link (h, j) with h < v 
< j. After the hook and the associated merging of some trees, the maximum weight set of tree links will 
have weight at most the weight of 388 10 u {(h, w) } (or else we could have chosen 1 differently to 
achieve greater weight). When a hook changes CAND(C), various terms in the potential change. Wt(C) after 
the hook is at most 4 + ~.cIO wt(e). Wt(C) was at least ~eeIOUIl zut(e), so there has been a decrease 
in Wt(C) of magnitude at least 4 + X,cIl ~~(e) Z 31111 4. Lemma 2: JVhen the leader 1 of C hooks to 
some x in C, the potential O(C) drops by at least 1111. Proofi The only term in Q(C) that increases is 
h(TIJ) h(T:) S Zl<i<~+l(h(Tw,) + 2). All of the tree links {tW, ~~ s i < k} are indepen­dent, so k 1 
s 1111.Thus h(T.) ~ (h(Tw, ) +2) s (1111 +2) *2. l<i<k+l Before the hook, X( TV) was 1. After the hook, 
v is the leader of C, so X( TV) changes to 0, and the term 8 * X(TU) no longer adds to the sum. We have 
drops of 8 (for X(TV)) and at least 31111 4 in Wt(C). Putting the terms together we have A@(C) ~ 2(11,1 
+2) 8 (311, ] 4) = llll. B We have just seen that hooking from a leader to a vertex in the leader s component 
reduces the potential. The next step is to show that restricted pointer jumping reduces Q(C) by a constant 
fraction in 0(1) rounds. The trees are easy to analyze. Any path in I v to v that contains at least 3 
vertices will shorten by a constant factor after all of the vertices in the path have executed 1 restricted 
pointer jump. This follows because at most half of the vertices can have v > dad(v) < dad(dad(v)) and 
also any vertex at distance 2 from v will be able to pointer jump. The terms for jy(T..) all drop out 
after a con­ stant number of rounds. If v is not a leader, the only reason that some edge process might 
hook onto w < T. is that the last time it inspected dad(w), w was a root. In a constant number of rounds, 
each edge process will re-examine w and will see that w is not a root. No process will hook to w after 
that unless w becomes a leader. It is slightly more complicated to analyze Wt(C). Let I = {(vi, xi)} 
be a set of inde­pendent tree links after each edge executes re­stricted pointer jumping. ~ = {(~;, yi) 
e TL(C) I (vi, xi) E Y}. II = {(v, y) G J \ (V, ?J)<1 }. lZ = {(v, y) c 10 I Iwt ((v, y)) = 3}. 13 = 
{(~, zJ) c In I I wt ((~, Y)) = 4}. The wt means the weight after pointer jump­ing. For each tree link 
in 11 except a link point­ing to the leader s tree, there must have been at least one associated tree 
link that it jumped over to get into I . Call these tree links J1. Then J1 is independent from 1 and 
has size at least 111I 1. Each tree link in lZ has jumped, so it has had its weight drop from 4 to 3. 
The tree links in 13 have not changed in weight, nor have they jumped over another tree link. As­sociate 
to each (v, y) G 13 the tree y lies in. These trees are disjoint, and each tree had ver­tex height at 
least 3 before the jumping. Define a phase to be the number of rounds required for every processor to 
execute all of the instructions in its procedure at least once. Lemma 3: ~~ @(C) > 16, then after each 
phase O(C) reduces by a constant factor. Proofi Each of the X( TV) S will drop to O after a phase. Some 
tree links from the maximum in­dependent set before the phase will have be­come edges in trees T; if 
the leader has hooked back into C. By lemma 2, their contribution 389 of 2 to the potential after the 
phase is at most 2/3 of the contribution before the phase. Consider an independent set of tree links 
1 after a phase of pointer jumping. We have seen that there was an independent set of tree links II U 
12 U 13 U J1 at the beginning of the phase which were involved in the pointer jumping to produce I . 
We also have ZU~C, AN~(C ) (TV) contributing to the potential. Call wt(l) = Z.CI wt(e). We know wt(I 
\ 13) 4< : *wt(Jl u 12u Jl). Let us assign 1/2 of the decrease in h(TO) to the tree link in 13 associated 
to TV, Then ~ h(T$ + wt(13) VECAND(C) . 0 KCAND(C) The value 16 is chosen for the constant 12 plus as 
much as 4 for the weight the tree link to the leader s tree. The potential that exceeds 16 will drop 
by a constant fraction. m If two components merge together because the leader 1 of one component hooked 
into a different component, the sum of potentials does not increase. We can use the constant 12 in the 
definition of O(C) to pay 8 for the new jy(Tv) (where T. is the new tree for /) and 4 for a possible 
new tree link in the maximum weight independent set. If, after the link, trees need to be merged, the 
analysis is the same as for a leader hooking back into its component. So long as pointer jumping can 
reduce the potential of component C, O(C) will reduce by a constant fraction after each phase. We need 
to take care of the case that pointer jumping cannot reduce the potential. Our method is simply not to 
account for vertices in such com­ponent in the potential function. We need a concept of dead vertices. 
The idea is that a component that has become stable with respect to restricted pointer jumping that is 
waiting to hook onto another component will be marked as dead, and no longer counted to­ward the potential. 
Initially, all vertices are jive. If (1) pointer jumping will not reduce the potential of a component 
C of live vertices, and (2) for every dead vertex v that can reach C in P restricted pointer jumping 
does not change dad(v), and (3) there is no consistent compu­tation in which some vertex will hook into 
C, then we mark all of the vertices in C as dead. The longest path from any dead vertex to the leader 
of C has length at most 3 vertices. Once a vertex is marked dead it remains dead. We define the live 
components LC to be the com­ponents of P \ {v ~ V [ v is dead}, The total potential of the configuration 
is @ = ~ m(c). CELC It remains to check that a live component C for which restricted pointer jumping 
does not decrease @(C) drops out of the sum in a con­stant number of rounds. The only problem is that 
a component of P consisting of dead ver­tices may have hooked onto C. If that com­ponent was live in 
the last two phases, we can use the fact that it dropped out of the sum to amortize for a drop in potential 
for @(C), If no such component exists, all of the dead vertices that can reach C have done enough restricted 
pointer jumping to have stable dad pointers. We have proved: Theorem: The potential @ = ~Cc~C O(C) drops 
by a constant fraction in O(1) rounds. After the potential has dropped to O, in one more phase all of 
the dad pointers will be sta­ble. This proves Theorem: The connected components algorithm termi­nates 
correctly in O(log n) rounds. 390  7 Spanning Forests The connected components algorithm also computes 
spanning forests. At the end, if index(v) # v, then indez(v) n is the edge that last caused vertex v 
to hook. In particu­lar, the edges {indez(v) -n [ v ~ V, v # dad(v)} form a spanning forest for G. 8 
Conclusion There is a simple parallel algorithm to find the connected components of a graph in O(log 
n) rounds on an A PRAM. The proof of running time uses a potential function which is linear at the start 
of the algorithm. In order to argue that the potential drops by a constant frac­tion in each unit of 
time we need two proper­ties. Firstly, the model must allow restricted pointer jumping to reduce the 
length of a path with three or more edges by a constant fraction in constant time. Secondly, the model 
must al­low several processes { ei } and one coordinating process v to choose one of the e~s and commit 
to it in constant time. In any model that allows these two operations, we have an O(log n) time connected 
components algorithm that uses a linear number of processes. Restricted pointer jumping is a powerful 
tool for reducing the lengths of long paths. It is preferable to standard pointer jumping because it 
does not disconnect cycles. The proofs in this paper point out that to analyze an algorithm using restricted 
pointer jumping, a potential function based on the interval graph of tree links is likely to capture 
the time required to reduce paths to short length.  References [B74] R. P. Brent The parallel evaluation 
of general nal of arithmetic the ACM, expressions. Jour­21:201-208, 1974. [CZ89] Richard APRAM: into 
the Cole and Ofer Incorporating PRAM Model. Zajicek. The Asynchrony Proc. of the 1989 rithms June Symposium 
on Parallel Algo­and Architectures, 1:169-178, 1989. [JM91] Donald B. Johnson and Panagiotis Met axas. 
Connected Components in 0(log3f2 IVl) Parallel Time for the CREW PRAM. Proc. of the 32nd Annual Symposium 
on Foundations of Computer Science, pp688-697, 1991. [SV82] Yossi Shiloach and Uzi Vishkin. An O(log 
n) parallel connectivity algo­rithm. Journal of Algorithms, 3:57-67, 11982. 
			