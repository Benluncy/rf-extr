
 Bandwidth-Based Lower Bounds on Slowdown for Efficient Emulations of Fixed-Connection Networks Clyde 
P. Kruskal Kevin J. Rappoport * The University of Maryland Pacific-Sierra Research Abstract This paper 
presents a new method for obtaining lower bounds on the slowdown of efficient emulations between network 
ma­chines based on their communication bandwidth. The proofs measure the communication complexity of 
a message pattern by viewing its graph as a network machine and measuring the communication bandwidth 
/3. This approach yields an intuitive lower bound on the time to route a communication pattern represented 
by Multigraph C on a host machine H (with uniform load) as T > Q (~), and thus a lower bound on the slowdown 
of emul~ting guest machine G on host If as the ratio of their communication bandwidths. 1 Introduction 
In this paper we study the relative communication power of fixed-connection network machines (e.g. Meshes, 
Hyper­ cubes, and Butterflies) through emulations. The notion is that machines of comparable communication 
power should be able to efficiently emulate each other with only a con­ stant increase in execution time, 
while a machine of lesser communication power will be unable to efficiently emulate a more powerful machine 
without a substantial increase in commutation time. tinder the emulation, it is assumed that the host 
haa no knowledge of the actual algorithm that the guest is execut­ ing, and must therefore simulate the 
most generaI guest com­ putation possible. Thus, the emulation proceeds at each step with the host mimicking 
the most general processing and communications actions of the guest. The emulation pro­ cess follows 
the redundant model reported in [7, 11], which is believed to be the most general possible. Machine comparisons 
are quantified in terms of the slow­ down of the best efficient emulation between the guest and host 
machines where the slowdown, S = ~, is the ratio of the emulation time on the host (TH) to the-running 
time on the guest (TG). The goal is for the host to perform the emu­lation of the guest as quickly as 
possible without expending more that a constant factor more operations, or work. The Research performed 
at the IDA Supercomputing Research Center. Permission to copy without fee all or part of this material 
is granted provided that the copies are not made or distributed for direct commercial advantage, the 
ACM copyright notice and the title of the publication and its date appear, and notice is given that copying 
is by permission of the Association of Computing Machinery. To copy otherwise, or to republish, requires 
a fee and/or specific permission. SPAA 94-6/94 Cape May, N.J, USA 0 1994 ACM 0-89791-671 -9/94/0006..$3.50 
inefficiency, I = ~, is the ratio of the work that would . ,, have been performed on the guest to the 
work that waa ac­tually performed on the host under the emulation. In this paper we study only eficient 
emulations where 1 = El(l). The slowdown of an emulation can be induced by two factors: a size difference 
between guest and host, or a dif­ference in communication ability between guest and host. The slowdown 
due to a size difference is at least NG/NH since some host processor must emulate at least Q(.NG/iVH) 
guest processors. The slowdown due to communication dis­parities, S., varies with the particular guest 
and host ma­chines. The best attainable slowdown for an efficient em­ulation occurs when the communication-induced 
slowdown matches the size-induced slowdown to within a constant fac­tor. Thus setting Sc = NG/NH yields 
the maximum sized host that can be used to efficiently emulate a guest of size n. The resulting functional 
relationship yields a measure of the relative computational power of the two machines. For example, we 
show that emulating an n processor de Bruijn graph on an m processor 2-dimensional mesh has a communication-induced 
slowdown of S. ~ Q (n/(@ log n)). An efficient emulation requires that we set e(l) = 1 = -T#&#38; < ~. 
Solving for m as a function of n yields an up­ per-bound on the size of the largest m processor mesh 
(the host) that can efficiently emulate an n processor de Bruijn (the guest) aa m < 0(log2 n). Thus, 
we see that only ex­tremely small meshes can efficiently emulate a de Bruijn graph. In this paper, we 
seek lower bounds on the communication­induced slowdown for emulations between guest and host machines, 
and hence reveal upper bounds on the largest host relative to the guest that can efficiently perform 
any such efficient emulation. 1.1 Overview of Our Results Our main theoretical result is that the communication 
com­plexity of a message pattern can be measured in terms of communication bandwidth (hereafter referred 
to as simply bandwidth). The bandwidth, @(M), of machine M is the ex­pected average message delivery 
rate under the symmetric trafiic distribution in which all source-destination message pairs have equal 
probability [9]. Traditionally, bandwidth is viewed as a measure of the communication power of a network. 
However, bandwidth can be described as a gen­eral graph measure that can be applied to any Multigraph 
 not just one representing a communication network. When applied to a Multigraph representing a communication 
pat­tern it yields a valid measure of communication complexity, and when applied to a Multigraph representing 
a network it yields a measure of communication capability. This observation is used to show that efficiently 
emu­lating message pattern C on host H requires time T ~ $@&#38; We then show that any efficient emulation 
of ) a ;-step comput ation of G must generate a pattern with bandwidth f_2(t,B(G)), and thus emulating 
guest G on host H has slowdown S. > Q ~ . This result is appeal­ () ing because it matches our intu;tion 
for other flow type problems, e.g. the slowdown incurred to simulate a chan­nel of capacity C(G) by a 
channel of capacity C(H) where C(G) z C(H). From the previous example, an n processor de Bruijn graph 
has bandwidth /3(B) = @ (*), and an m node mesh has bandwidth /3(lkf) = ~(~. It will be shown that any 
efficient emulation of the de Bruijn on the Mesh must Since the e mulat;ons a&#38;w redu~dant computations, 
it is conceivable that a strategy could be developed for short emulations in which a host processor generates 
most of the information it needs without communicating with other host processors. In order to disallow 
this strategy we require that the guest computation be sufficiently long approximately the dbrneter 
of the guest network G. This minimal com­put ation time, written as X(G), is related to the bandwidth 
of the guest, and is proportional to diameter for most ma­chines. Additionally, the host is required 
to be bottleneck­free as defined below. We note without proof that most net­work machines studied in 
the literature, including the Tree, X-Tree, Mesh, Butterfly, Shuffle Exchange, de Bruijn graph, are bottleneck-free 
and have A proportional to diameter. Definition Let I M be a machine on n nodes. A traffic distribution 
is called quasi-symmetric if f2(n2 ) of the possi­ble n(n 1) source-destination pairs have equal (nonzero) 
probability and all other pairs are disallowed. Machine H is bottleneck-fee if the average message delivery 
rate under any quasi-symmetric distribution on m < IHI nodes is at most a constant factor higher than 
the message delivery rate under the symmetric distribution (ss measured by ,8(M)). We now have enough 
to present the main result, the Ef­ficient Emulation Theorem. The proof relies on showing that bandwidth 
is preserved under efficient emulations of the guest G. That is, any communication pattern J4 gener­ated 
by an efficient emulation of a t-step guest computation has bandwidth /3(lkf) = c3(t/3(G)) within a 
constant factor of the bandwidth that is generated by a t-step computation on G. In this sense t8(G) 
is an abstract measure on the r., communication work contained in the communication pat­ tern of a t-step 
computation on G, and /3(H) is the amount of thk work that can be generated by H per step. The lower 
bound on slowdown is rmoven usirm a work-bssed amument to yield S >$2 (~(G) /~(H)). Theorem 1 [The Efficient 
Emulation Theorem] Any effi­cient emulation of a fixed degree guest graph G on host H hss slowdown S 
> L? ~) ifi 1) the guest time T satisfh (.. r > (1 + @(l)) ~(G), and 2) H is bottleneck-free. T/G/ ....................................---------\ 
. T i- Figure 1: Communication-induced vs. load-induced slow­down. Host Maximum Host Size Linear Array 
Tree IHI <0 ([G$)Global BUS Weak PPN X-Tree IHI <0 lGl+lglGl Meshk Pyramidk Multigridk IHI <0 (IGI+) 
Mesh of Treesk Table 1: Maximum host sizes for efficient emulation of j­dimensionzd Meshes, Tori, and 
X-Grids. The largest host that can efficiently simulate the guest is obt~ned by Set,ting S= = ~ and solving 
for IH I ss a func­ tion of IGI. The situation is depicted in Figure 1. The two curves show bounds on 
the time required for the simulation as IH I is varied relative to IGl. The linear plot gives the up 
per bound on the time of any efficient emulation based on the relative sizes of the guest and host, while 
the curved plot gives the lower bound on emulation time based on relative bandwidths. The point at which 
the plots intersect gives the smallest possible slowdown, and largest possible host for an efficient 
emulation. These results are summarized by the following theorems. Theorem 2 Eiliciently emulating at 
least T~ ~ Cl (lg IGI) steps of an X-Tree G on host H, where H is one of Linear Arra~ Tree, Global Bus, 
or Weak Parallel Prefix Network, requires H to be of size at most IHI <0 ~~~. Theorem 3 Efficiently emulating 
at least T~ ~ Q ({GI ~) . steps of a j-dimensional Mesh-of-Trees, Multigrid, or Pyr;­ mid G on host H 
requires H to be of size at most lHI < O(~(lGl)) where f(lGl) is given in Table 1. Theorem 4 Efficiently 
emulating at least TG ~ $2(lg IGI) steps of a j-dimensional Mesh-of-Trees, Multigrld, or Pyra­mid G on 
host H requires H to be of size at most lH1 < O(f(lGl)) where f(lGl) is given in Table 2. I Host Mam 
mum Host Size Linear Array Tree Global BUS Weak PPN X-Tree lfflsO(]Gl*lglG1) Mesh Pyramidk Multigridk 
Mesh of Treesk E X-Gridk Table 2: Maximum host sizes for efficient emulation of .$ dimensional Mesh-of-Trees, 
Multigrids, and Pyramids. Host I Mam mum Host Size I Linear Array Pyramidk I Multigridk Mesh of Treesk 
X-Gridk LY I Table 3: Maximum host sizes for efficient emulation of But­ terflies, de Bruijn Graphs, 
Cube-Connected-Cycles, Shuffle- Exchanges, Multibutterflies, Expanders, Weak Hypercubes. Theorem 5 Efficiently 
emulating at least Trs ~ Q (lg IGI) steps of a Butterfly, de Bruijn, Shuflie-Exchange, Cube­Connected-Cycles, 
Multibu t terfly, Expander, or Weak Hy­percube G on host H requires H to be of size at most Iffl < O(j(lGl)) 
where T~ and j(lGl) are given in Table 3. Tables 1, 2, and 3 compare network machines solely on the basis 
of bandwidth. However, there are other commu­nication effects that that also lead to emulation slowdowns, 
including expander and dist ante-based effects. Thus, many of the results listed above are not tight. 
Our work-based approach can bound slowdown based on these other commu­nication effects as well by the 
appropriate choice of abstract work measure (see [15]). 1.2 Previous Work The general idea of mapping 
parallel computations onto par­allel architectures is studied as the mapping problem [1]. Most of the 
early work on emulations involves l-to-l em­bedding in which each host vertex has at most one guest vertex 
mapped to it (see [16] for a survey of general em­ bedding results). Studies of embedding based emulations 
can be found in [1], [4], [12], [14], and [13]. Lower bounds on slowdown can be obtained by exhibiting 
lower bounds on the dilation or congestion of embeddings (not necessarily l-to-l) of the guest into the 
host. For example, embedding a complete ternary tree into a complete binary tree with expansion (of graph 
size) e < 2 requires dilation d > 0 (log log log n) [6], which implies a sim­ilar slowdown on embedding-based 
emulations. Similarly, in [2] it is shown that embedding a non-tree planar graph into a butterfly requires 
dilation Q % where Z(G) is the () size of the smallest 1/3-2/3 vertex separator of G, and O(G) is the 
size of the largest interior face of G. Thus embedding an X-Tree into a Butterfly requires dilation Q(lg 
Ig n), and embedding a mesh requires dilation Q (lg n). A brief survey can be found in [17]. The most 
general efficient emulations, which use redun­dancy are introduced in [7]. The emulations are redundant 
in that a single guest operation can be performed redun­dantly across the host in the hope that some 
long-distance communications may be avoided. This approach turns out to be quite powerful since it allows 
a butterfly to efficiently emulate a mesh of the same size [7] despite the fact that em­bedding the mesh 
into the butterfly requires logarithmic di­lation [2]. Other discussions of efficiency issues can be 
found in [3, 5, 8]. Koch, et al. [7] also introduce new lower bounds on effi­cient emulation using distance-based 
and congestion-based arguments. The dist ante based arguments are used to prove that emulating a tree 
on a k-dimensional mesh has slowdown of S ~ Q ((l G1/lgk lGl)l/(k+lJ). The congestion-based ar­ guments 
show that 1) there are no efficient emulations of k-dimensional meshes or butterflies on expander graphs, 
2) emulating a butterfly on a k-dimensional mesh has slow­ down at least 2°(lHl k) , and 3) emulating 
a k-dimensional mesh on a j-dimensional mesh, j < k, has slowdown at least Cl (lHlck-JJfJ). Of the two, 
the congestion-based arguments are the most complicated and involve tuning complicated graph parameters 
for both the guest and host, and it may be that the difficulty in choosing the graph parameters has limited 
number of results proven by the method. Our results are most similar to the congestion-based lower bounds 
in [7] since we too assume the most general redun­dant emulation strategy, and essentially make use of 
conges­tion arguments. However, comparing the results of our ap­proach to the congestion-based theorem 
of [7] is somewhat difficult. On the one hand, the congestion-based theorem yields slowdown results for 
Expander graph guests, which our bandwidth analysis cannot attain. On the other hand, our approach matches 
their results for non-ezpander guests, and seems to be more intuitive. The notion of slowdown proportional 
to bandwidth ratios is relatively simple com­pared to the more complicated mechanics associated with 
the congestion-based proof. Additionally, our method ad­mits results by merely plugging in well-known 
bounds on bsndwidth and doesn t require the potentially complicated task of tuning guest and host graph 
parameters. Finally, the Efficient Emulation Theorem can also be used to bound the slowdown of efficient 
redundant simulations of algorithms by proving lower bounds on the bandwidth of their communi­cation 
patterns [15]. 2 Proof Outline In this section we outline the proof of the Efficient Em­ulation Theorem 
in more detail. The section begins with a more general definition of the bandwidth of machine G with 
respect to traffic distribution n, written as /3(G, x). A graph-theoretic definition of bandwidth is 
then introduced. The slowdown results are proven using a work-based ar­gument. Using a lower bound on 
the bandwidth of the guest communication pattern C, and an upper bound on the bandwidth of the host we 
show that l-to-l executing C on H requires time T ~ % for any traffic distribu­tion T. Graph Cis any 
communication pattern induced by an efficient emulation of G on a smaller host, and is shown to have 
bandwidth /3(C, ~) > Q(@(G)) for a special traffic distribution ~ with nice properties. An upper bound 
on host bandwidth is shown to be /3(11, ~) ~ O(@(lf)) for the same traffic distribution. The bandwidths 
,B(G) and /?(H) are known and previously compiled in a table. Using these upper and lower bounds we can 
obtain a lower bound on the time to route C on H, and hence the time to emulate G on H. A network machine 
is naturally described by a network mt.dtigraph G where vertices represent processors, and edges represent 
communication links. Similarly, a communication pattern is described by a communication Multigraph where 
vertices represent processors, and edges represent messages sent between processors. A traffic distribution 
ir,l is defined, as in [9], to be the relative frequency of a message originating at processor p, with 
processor p~ as its destination. Bandwidth is functionally defined as follows. Suppose that G contains 
m messages with sources and destinations satisfying T and the machhe tries to deliver all m messages 
in any order. The bandwidth under trafic distribution r, denoted j?(G, m), is the expected value, in 
the limit as m ~ co, of m/r(m) where r(m) is the time required to complete delivery of m messages. In 
other words, the bandwidth is the expected average delivery rate when the message traffic satisfies a 
specific distribution m. In this paper we express bandwidth as a function of machine size. The graph-theoretic 
definition of bandwidth is cast in terms of the congestion of certain types of embedding of multigraphs 
representing traffic distributions into other multi­graphs representing either network machines or communica­tion 
patterns. In what follows E(G) is the number of simple edges (sum of multiplicities over all edges) of 
an arbitrary Multigraph G. Let G and H be mnltigraphs on n vertices. The congestion of an embedding of 
G into H is the maxi­mum number of paths that cross any edge of H under the embedding, while the dilation 
is the maximum length of any path. The following two limits capture concepts analogous to congestion 
and dilation for embedding of G into H in the limit as the edge multiplicities of G grow. In what follows, 
ZG denotes the Multigraph created by multiplying all edge multiplicities of G by scalar x. Definition 
Let G be a graph, and define c(A, B) to be the minimum congestion over all l-to-l embedding of B into 
A, and let 6(A, B) be the minimum dilation for any embedding with congestion c(A, B). The G-congestion 
of H and G-dilation o.f H are respectively, c(H, zG)C(H, G) = lim sup A(H, G) = lim sup 8(H, zG]. x-co 
z-wz The average G-dilation of H, written j(H, G) ~ defined sim­ilarly except that 6(H, zG) is replaced 
with 8(H, zG), the minimum average dilation for any embedding wit h conges­tion c(A, B). A traffic distribution 
is modeled by a Multigraph as fol­lows. If the distribution is real-valued, it is first replaced by an 
approximating rational-valued one. Each processor is represented by a single vertex, and a multi-edge 
is laid for every pair of source-destination processors that have a non-zero probability of message transfer. 
Finally, the edge weights are adjusted so that they are all integral-valued and proportional to the approximating 
rational-valued distribu­tion. The edge weights now reflect the relative frequency of messages passing 
between the source-destination vertex pairs at the endpoints. The resulting Multigraph T= is re­ferred 
to as a trafic mcdtigraph of the traffic distribution T. Let H and T be arbitrary multigraphs. The bandwidth 
of H with respect to T is defined as 9(H, T) = *. This graph-theoretic definition is now shown to be 
equivalent to the operational definition when H is interpreted as a net­work Multigraph and T is interpreted 
as a traffic distribu­tion on H. Bandwidths for several machines are then offered without proof. Theorem 
6 Let G be a network graph of a machine with n processors. The m~ mum expected message delivery rate 
under traffic distribution T is El * . () Proof: Let C be a communications graph on n vertices representing 
the transmission of m messages under a traffic distribution represented by T. In the limit as m ~ co, 
C is well-approximated by the graph *T. Thus, routing the random graph C on G can be well-approximated 
by embedding the graph *T into G in the limit as m + co. In the tilt, the congestion and ddation of such 
an em­bedding into G is given by c = ~C(G, T) and A(G, T) respectively. By the universal routing scheme 
in [10], there exists a routing corresponding to the embeddmg that com­pletes in time O(c + A). The dilation 
is bounded from above by E(G), since otherwise some path would cross an edge more than once and could 
therefore be shortened. Thus in the limit as m ~ co we get O(c+ ~) = O(c+ E(G)) = O(c). A simple flux 
argument gives the lower bound as Q(c) since at most one message crosses an edge per tick. Thus the traffic 
is routed in time  t=&#38;C(G, T) . e(c)=@ ( ) The traffic consists of a total of m messages delivered 
in t = El(c) ticks for an average of @ * messages delivered () per tick. 0 Theorem 7 The values of ~ 
and ~ for several network ma­chines are listed below in Table 4. 1+-oo~: Omitted. 0 Simulating a communication 
pattern C on host H where the vertices of C are l-to-l assigned to the vertices of H requires time T 
z $$&#38;. Note that the bound is valid for any traffic distribution m. The proof uses the concept of 
a routing path induced by an embedding. Consider the graph C induced by a l-to-l embedding of C into 
H. Each edge (v, o) c C is converted into a routing path (u = uO, ul,..., u =v)of length n~lin C . If 
Cisacommu­nication pattern and H is a host machine, then routing C on H amounts to nothing more than 
replacing the edges of C with routing paths to get a new communication pattern c . I Machine----------­ 
1#,­ 1A I -Linear Array Q(1) @(n) Global BUS e(l) e(l) Tree e(l) Q(lg n) Weak Parallel Prefix Net e(l) 
~(lg n) X-Tree @(lg n) @(lg n) k k 1 Meshk @(n+) @(n*) I Mesh of Trees @(lg n) I Multipridk k ,1 , @(lg 
n) Pvramidk I @(n*) I @3&#38;n) Cube-Connected-Cycles , ..*,. , # Multibu t terfl.y I e(~) I @&#38; 
ni Table 4: Table of@ and ~ for network machines. Lemma 8 Let C be a communication graph and let H be 
a host machine where ICI ~ ]Hl. Then, for any traflc dis­tribu tion r, any 1-to-l execution of C on H 
requires time ~ > (?(c , 7r) - m Proof: Any pattern C induced by a l-to-l routing of C on H haa bandwidth 
at least P(C, m). The pattern C is obtained by replacing edges of C with routing paths. Replacing an 
edge with a routing path cannot increase the congestion of any embedding since the new path edges can 
carry the same congestion as the replaced edge. Thus ~(C , ~) ~ /3(C, m). Any t-step computation on H 
generates a communica­tion pattern with bandwidth at most t,f3(H, ~). If H runs for t steps using all 
wires at each steps it generates pattern tH with bandwidth ~(tH, ~) = t/3(H, T). Any t-step com­put ation 
on H generates a communication pattern C ~ tH. Since deleting edges cannot decresse the congestion of 
any embedding, ,B(C, r) s tf?(H, m). The lower bound on running time is obtained using a work-based argument. 
To emulate C the host must generate a pattern with bandwidth at least /3(C, ~), and the host generates 
at most t,B(H, ~) bandwidth in t steps. Thus, H must run for at least T steps where /3(C, r) s T/3(H, 
z). 0 The ratio of the bandwidths of the guest and host is also a lower bound on slowdown of ang sufficiently 
long, efficient redundant emulation. The proof rests on the observation that any communication pattern 
J4 induced by efficiently emulating G will have bandwidth /3(Jf, () z f2(t/3(G)) for some traflic distribution 
&#38; with very nice properties. Before we begin, we formalize the circuits on which our emulations are 
based. Computations on guest G are represented by circuits . A circuit is a directed graph on circuit 
nodes which are de­scribed by 3-tuples (u, t, c) where u is the corresponding ver­tex in G, tis the guest 
time step, and c is the copy number. The set of tuples {(u, i, c) I u G G, 1< c} for fixed i is called 
circuit level i, and fully describes the state of G at time i. For any circuit level the set of 3-tuples 
with the same pro­cessor and time component is called a class, and an element of a class is called a 
representative. The duplicity of a class is the number of elements it contains. The copy number is used 
to introduce redundancy into the circuit by allowing a single guest operation to be per­formed at several 
locations within a level. The set of arcs between levels i and i + 1 represent the data transfers oc­curring 
in G at time i. Arcs run from circuit nodes (u, i, z) to circuit nodes (v, z -I-1, y) if there is an 
arc (u, v) in G. Arcs are laid such that each circuit node (v, i -i-1, y), where i ~ O, has an input 
arc from some circuit node (w, i, z) for each u c G with an arc (u, v) in G. A circuit is eficient if 
it contains at most a constant factor more work than the corresponding guest computation it represents. 
That is, a t-steD circuit is efficient if it cent tins O(lGlt) nodes. ~he remaining proofs proceed as 
f&#38;o w~. First, it is shown for special guests G, any efficient circuit &#38; emulating sufficiently 
many steps of G must have bandwidth /3(@, -y) z O(t/3(G)) for some nice traffic distribution -Y. Furthermore, 
any efficient emulation of @ on a host induces a communi­cation pattern J4 with bandwidth /?(Af, ~) z 
Q(@(G, ~)) for another nice traffic distribution .$. Thus, the bandwidth of the guest computation is 
preserved no matter how clever the emulation. Finally, we show that /3(H, ~) < O(~(H)) for well-behaved 
hosts, and Lemma 8 is applied to show communication-induced slowdown Sc z Q ~). Setting (the communication-induced 
slowdown to match the load­induced slowdown yields the smallest possible slowdown for any efficient emulation 
of the guest on the host. We start bv definine K. . . a sDecial class of almost com­ plete graphs ~rom 
whi;h tr%ic &#38;ultigraphs y and ( will be chosen. We then show that, for certain graphs G, the band­ 
width of any efficient circuit @ is high relative to some nice traffic Multigraph ~ 6 &#38;(.t),l, when 
tis approximately the diameter of G. Definition A graph ~ is in K,,s iff it has ~ vertices and @(r2s) 
edges, and no two vertices have more than s edges between them. Lemma 9 Let G be a graph on n vertices. 
If X(G, K.) ~ O(%) and t = (1 + @(l))~(G, IYn), then for any efficient homogeneous circuit ~t constructed 
from t steps of G there em sts a quasi-symmetric traffic graph y < K&#38; ~t ~,1 where Proof: Let t= 
(1 + a)~(G, K.) where a = 0(1), and let @ be an efficient homogeneous circuit constructed from t steps 
of G. Denote an arbitrary circuit node in level i representing vertex u ~ G by ~i (u). Observe that for 
any circuit node rj (u) there exists a circuit node Tj_, (v) (j > i) that is connected to TJ (u) by a 
path in @ if vertices u and v are connected by a path of length i in G. Otherwise, the emulation is not 
correct. This fact is used to concentrate messages from a large number of nodes to a single node. However, 
to avoid creating too much congestion we follow only those paths that are used in the embedding that 
wit­ nesses ~(G). Such paths through Gt will be referred to as embedding paths, and will be used extensively 
in the proof. We now find the ~(nt) circuit nodes and E3(n2t2) paths that witness the embedding of y. 
The edges in the circuit are of two types. Edges between representatives of different vertices of G on 
adj scent levels are called t-outing edges, while edges between representatives of the same vertex are 
 called identity edges. The paths through the circuit that embed the edges of -y will be called y-edges. 
The nodes which support the embedding are a&#38;o of two types: S-nodes which are contained in the last 
YA levels of @ (largest values of t), and Q-nodes which reside higher up in the circuit. (Recall that 
t= (1 + a)A(G, Kn). ) S-nodes originate large numbers of y-edges that are carried up the circuit by embedding 
paths. Once these 7-edges reach a certain level they are collected together in bundles and pass up the 
identity edges where they are are picked off by the Q-nodes. The S-nodes are collected by circuit level. 
The set of S-nodes for level t (call it the St-set) consists of a represen­tative of each vertex of G 
from the last level (level Lt) of the circuit. The remaining S,-sets are found recursively by following 
identity edges up the circuit. (i.e. S,_l is obtained from S, by following identity edges from each representa­tive 
in level L~ to a representative in level L,-1.) Use thk process to obtain the sets St_% x,. . . . St-l. 
The S-nodes are used to find the Q nodes by following cones up the circuit (see Figure 2). For each S-node 
u follow those embedding paths that start at u and terminate at some destination node within ~ = (1 + 
~) ~ steps. (If more than one such paths exist that follow the same route in G we keep only one. ) The 
set of such paths that t errninat e wit hin the allotted number of steps is called the cone for s, and 
defines a large set of different representatives that can be reached from u without too much congestion. 
Since these cone paths are of length A (not ~), it is con­ceivable that there will be an insufficient 
number of them. This is d~proved as follows. For every y long embedding paths (of length at least ~ + 
1) we ask how many short paths there must be of length 1 ~ 1 s ~ (these are the cone paths). Let the 
number of short paths be z. The smallest possible value of z is obtained when all long paths are of length 
exactly ~ + 1, and all short paths are of length 1. Thus, a lower bound on x must satisfy ~=y(fi+l)+z 
=u((l+;)~+l)+$ y+x y+x Solving for x we get that ~>(a/2)A+ly>~y J 1 2 So, for every y long embedding 
paths that exceed the cut­off there are at least max(fl(y), fl(n2 -y)) short embedding paths (cone paths) 
that fall below the cut-off. Thus, there are f2(n2) cone paths among all S-nodes in each level. The S-nodes 
are used to obtain the Q-sets by following cone paths up the circuit (towards lower numbered levels). 
There is a Q-set for every cone path, and they overlap exten­sively. For every cone path, the Q-set consists 
of the termi­nating node, and all subsequent nodes higher up in the tree that are reachable by traversing 
only identity edges. The Q-sets for a cone originating at a circuit node S,,J c S, are labeled Qi,j,k, 
and are of size (i ~) < lQ,,J,~l ~ O(t). The strategy for embedding 7-edges in paths through @ illustrated 
in Figure 2 is very simple. Each si,~ 6 S, sends a bundle of IQ,,j ,k I y-edges up cone path k towards 
set Q,,j,k. The y-edges then travel up the identity edges and are picked-off one-by-one by the elements 
of Q, ,J,k. The Q-sets, S-sets, and y-edges described above define the quasi-symmetric traffic distribution 
y and its embed­ding. Clearly, any pair of circuit nodes is connected by at Qj-sets ) )Cone on $ij in 
Si Si -set Figure 2: Embedding y-paths in @ most one y-edge. Exactly one vertex of y is mapped to each 
S-node and each Q-node. Thus, there are @(nt) vertices in 7, and @(n2t2) edges, implying that y 6 Kefwt),l. 
The congestion is at most O (max(nt , tC(G, K.))). The cone paths through @ have congestion at most 
C(G, K.) be­cause they follow the routes that witness the embedding of /3(G). Let (c,,$, ci+l,~) be an 
edge in @ crossed by embed­ding paths that cross edge (UJ, uk ) in G. This is shown as follows. Clearly, 
edge (c,,j, cl+l,k ) is crossed by many such embedding paths emanating from S-sets that lie be­low level 
i in the circuit, but the crossing paths from level i correspond to only those embedding paths whose 
(i i)th leg crosses edge (uJ, tik) in G. The congestion of (U3, ~k) is the sum of congestions of paths 
whose mth leg crosses (Uj, Zkk) for all 1 S m s X(G, K~). Thus edge (c,,,, c,+l,k) is crossed by at most 
the number of paths that cross edge (uj, u,) in G. When the cone paths are loaded with bundles of O(t) 
~-edges the congestion grows to O(K!(G, Afn)). The identity edges that distribute 7-edges to the Q-sets 
each carry congestion 0(nt2 ). Thus the congestion is at most O (max(nt2, tC(G, K~))). The embedding 
of ~ is now shown to have the proper bandwidth. Graph 7 has @(nt)vertices and @(n2 t2 ) edges with at 
most one edge between any vertex pair, and is embedded in ~ t with congestion O (max(nt2, tC(G, K.))). 
From the definitions of bandwidth we get Note that ifts O ~ then nt2< O(tC(G, K.)), so () 2 2 Q O (max(nt;,~C(G, 
Kn))) = (tP(G)) () and the Lemma is satisfied. 0 It remains to show that this embedding of f has the 
We now present two simple lemmas that will be combined with Lemma 9 to establish the bandwidth of any 
communi­cation pattern formed by an efficient emulation of a fixed­degree guest G. The first lemma is 
a simple result con­cerning the bandwidth of fixed degree graphs. The second lemma shows how bandwidth 
is conserved when a graph is collapsed into a smaller graph. Lemma 10 If G has fixed degree, then J(G) 
< ~. Proof: A set of m messages will cross a total of m~ edges during routing, This yields a congestion 
of at least E?:) and Kn-congestion at least C(G) ~ ~. Plug in the definition of bandwidth to get X(G) 
~ ~ and the result follows. 0 6(G) The previous lemma removes the inconvenient restri­ction concerning 
~ and ,8 from Lemma 9. The next theorem shows how the bandwidth of circuit @ is affected when it must 
be run on IH I < IG I processors. The process of emu­lating @ on H is treated as a two stage process. 
First, the nodes of @ are collected into [HI sets or super-oertices and edges between circuit nodes collapsed 
into different super­vertices become edges between the super-vertices. (Note: some super-vertices may 
be empty. ) The resulting graph M is then l-to-l embedded into H to complete the emulation. The next 
lemma shows that bandwidth is preserved when C7t is collapsed into M. Lemma 11 Let C be a graph on n 
vertices and let 7 be a quasi-symmetric traffic distribution; ~ 6 K~,o(l ). For any graph M on n/k vertices 
(k = o(n)) induced by collapsing the vertices of C into super-vertices with load O(k), there exists some 
traffic graph &#38; E &#38;1k,@(~2) such that 9(M, t) Z Q(p(c, -/)). Proof: Graph ~ ~ Knl~,e(~2) is embedded 
into M using the embedding that witnesses P(C, -y). Let S be the set of El(n) vertices of C that embed 
7 to witness ~(C, y). Call a path between vertices of S that embeds an edge of ~ a ~-path. Let m = n/k 
= IJ41, and recall that every super­ vertex of M has load O(k) and that k = o(n). Watch the vertices 
of S as they are collapsed into the super-vertices of M. First, count the number of 7-paths that are 
collapsed into self-loops on the super-vertices. The ~-paths that remain run between different super-vertices. 
Each pair of vertices in S has at most 0(1) 7-paths between them. Since ev­ery super-vertex contains 
at most O(k) vertices of S, there are at most 0(k2 ) 7-paths collapsed into a single vertex. Thus there 
are at most 0(mk2) = 0(nk2/k) = O(nk) y­paths collapsed into all super-vertices, and there must re­main 
C2(n2 ) O(nk) = Q(n2) -y-paths embedded between different super-vertices of M (since k = o(n)). Call 
this lsst set of paths P. Recall that the definition of.$ E Knl~,e(~2) requires that 1) ~ have at least 
fl(nz) edges, and 2) ~ have at most k2 paths between any pair of vertices. The ~-paths P and their endpoints 
comprise the embedded graph &#38;. As shown in the previous paragraph, P cent ains f2(n2 ) 7-paths. Every 
super­vertex has at most O(k) elements of S, so there are at most O(k2 ) y-paths between any pair of 
super-vertices. appropriate congestion and hence M has the sought-after bandwidth. The set of paths P 
witness an embedding of C ~ Knj~,e(~2) with $-congestion C(C, f) s O(C(C, 7)) since it consists entirely 
of 7-paths that witness 8(C, 7). Thus, We now have enough to show a lower bound on the band­width of 
any communication pattern induced by an efficient emulation of t = (1 + @(l))~(G, Kn) steps of G. First, 
aPPIY L~mrna ? to obtain the bandwidth of @ relative to traffic dlstrlbutlon y. Next, apply Lemma 11 
to obtain the bandwidth of any communication pattern ill on IH I vertices relative to traffic distribution 
~. The final step before proving the Efficient Emulation Theorem is to show an upper bound on the bandwidth 
of the host relative to the same traffic distribution ~ drawn from KnlC,e~C.~. Care must be taken to 
show that the up­per bound is not violated even if the emulation attempts to cheat by allowing the communication 
pattern C (and hence ~) to be much smaller than H. This last complication is ad­dressed by employing 
bottleneck-freeness to show an upper bound on bandwidth relative to traffic patterns of any size less 
than the host (i.e. 2 s I(1 s IHI). Lemma 12 If H is bottleneck-free, then all traffic multi- Proof: 
Let r = 1.$1. By an averaging argument on the defi­ nition of KT,S, graph [ E Kv,. haa a subgraph T consisting 
of @(If 12) dfierent vertex pairs each with exactly u edges between for some u = (3(s). Trailic Multigraph 
T repre­ sents a quasi-symmetric distribution and thus has band­ width ~ (H, T) = O(9 (H)) by the bottleneck-freeness 
of H. The remaining O(E(T)) edges in f T represent additional messages that, in worst case, increase 
the average delivery rate by at most a constant factor. 0 The bandwidth of the guest communication pattern 
and the host machine has now been established in terms of the same traffic distribution, and the Efficient 
Emulation The­orem can now be proven by applying Lemma 8. The only problem is that pattern M is proven 
only for circuits with a very specific depth; namely depth t= (1+ @(l))~(G, K=). The result is extended 
to circuits of arbitrary depth T (where T z t) by breaking the circuit into subcircuits that must be 
executed sequentially. Proo~:[Of the Efficient Emulation Theorem] Break circuit ~~ into blocks of length 
t= (1 + @(l))~(G, K.), and show the slowdown for each block. Let @ be one such block. By Lemmas 9 and 
10 we get that /3(@, 7) ~ f2(t/3(G)) for some 7 6 Kefnt),l. By Lemma 11, any efficient communication 
Multigraph M induced by an efficient emulation of @ on an lM1-processor host has bandwidth has bandwidth 
$(M, ~) > fl(t/3(G)) for some ~ G K~,e(Cz). Apply Lemma 8 to get a lower bound on the number of steps 
tH that must be run on the host to complete t steps of the guest as tH >-m= tw. BY Lemma 12, ,B(H, &#38;) 
s 0(/3(H)) and we get ~=s> f2(~(G)) = Q(~(G)) /3(G) TH -/3(H, f) 0(/3(H)) = n ~ () 0 3 Conclusion We 
have presented a new method for obtaining lower bounds on the slowdown of efficient emulations between 
network ma­chines. The proofs measure both the communication capa­bilit y of a machine, and the complexity 
of a communication pattern by bandwidth. Lower bounds on the time to effi­ciently execute the communication 
pattern on a host are obtained using a generalization of a work-based argument where the bandwidth is 
viewed as a type of work. The technique is applied to emulations between network machines by showing 
lower bounds on the bandwidth of ef­ficient redundant circuits that emulate the guest machine, and proving 
that this bandwidth is preserved under all ef­ficient simulations of the circuit on a host machine. The 
slowdown results match those obtained in [7] (except for ex­panders) by a congestion-based argument. 
The advant age of our method is that it provides a more elegant and in­tuitively appealing approach, 
and does not require tuning complicated graph parameters. Additionally, our method can be applied to 
redundant emulations of algorithms. Although not treated in this pa­per, the technique is much the same 
[15]. Algorithms are treated as collections of communication patterns that can be efficiently simulated 
by redundant circuits. Lower bounds are obtained on the bandwidth of these circuits, yielding lower bounds 
on the bandwidth of any communication pat­tern induced by any efficient redundant simulation of the algorithm 
on a host. This will be treated in a later paper. Acknowledgements The authors would like to thank Jason 
Meyer for comments on an earlier version of this manuscript. References [1] F. Berman and L. Snyder. 
On mapping parallel zd­gorithms into parallel architectures. In Proceedings IEEE Conference on Pamllel 
Processing, pages 307 309, 1984. [2] Sandeep Bhatt, Fan Chung, Jia-Wei Hong, F. Leighton, and Arnold 
Rosenberg. Optimal simulations by butter­fly networks. In Proceedings of ACM Symposium on Theory of Computing, 
pages 192-204, 1988. [3] H.L. Bodlaender and J. Van Leeuwen. Simulation of large networks on smaller 
networks. h~ormation and Control, 71:143-180, 1986. [4] S. Bokhari. On the mapping problem. IEEE Transac­tions 
on Computers, C-30:207 212, 1981. [5] J. Fishburn and R. Finkel. Quotient networks. IEEE Transactions 
on Computers, C-31:288 295, 1982. [6] Jia-Wei Hong, Kurt Melhorn, and Arnold Rosenberg. Cost trade-offs 
in graph embedding with applications, JA CM, 30(4):709-728, 1983. [7] Richard Koch, Tom Leighton, Bruce 
Maggs, Satish Rae, and Arnold Rosenberg. Work-preserving emula­tions of fixed-connection networks. In 
Proceedings of ACM Symposium on Theory of Computing, pages 227 240, 1989. [8] Clyde Kruskal, L. Rudolph, 
and Marc Snir. A complex­ity theory of efficient parallel algorithms. Theoretical Computer Science, 71:95-131, 
1990. [9] Clyde Kruskal and Marc Snir. Cost-performance trade­offs for interconnection networks. Discrete 
Applied Mathematics, 37/38:359-385, 1992. [10] Tom Leighton, Bruce Maggs, and Satish Rae. Univer­sal 
packet routing algorithms. In Proceedings o.f A CM Symposium on Theory of Computing, pages 422-431, 1988. 
[11] Bruce M. Maggs. Locality in Parallel Computation. PhD thesis, Massachusetts Institute of Technology, 
1989. [12] D.I. Moldovan. On the design of algorithms for VLSI systolic arrays. Proc. IEEE, 71(1):113 
118, 1983. [13] I.V. Ramakrishnan, D.S. Fussel, and A. Silberschatz. On mapping homogeneous graphs on 
a linear array prc­cessor model. Technical report, University of Maryland CfAR, 1983. [14] I.V. Ramakrishnan 
and P.A. Varnman. On mapping cube graphs on VLSI array and tree architectures. Technical report, University 
of Maryland CfAR, 1983. [15] Kevin J. Rappoport. Eficiency in Parallel Computa­tion: Algorithms, Emulations, 
and VLSI Pins. PhD thesis, The University of Maryland, College Park, 1993. [16] Arnold Rosenberg. Issues 
in the study of graph embed­ding. In LNCS 100, pages 150 176. Springer Verlag, 1981. [17] Arnold Rosenberg. 
Graph embedding 1988: Recent breakthroughs, new directions. In LNCS 319, pages 119-123. Springer Verlag, 
1988. 
			