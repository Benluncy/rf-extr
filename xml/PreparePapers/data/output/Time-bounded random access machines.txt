
 TIME-BOUNDED RANDOM ACCESS MACHINES % Stephen A. Cook and Robert A. Reckhow University of Toronto Toronto, 
Canada In this paper we introduce a formal model for random access computers and argue that the model 
is a good one to use in the theory of computational complexity. Results are proved which compare run 
times for recognizing sets using this model (which has a fixed program) with a stored program model 
and with Turing machines. The main result, theorem 3, shows the existence of a time complexity hierarchy 
which is finer than that of any standard abstract computer model. An Algol-like programming language 
is introduced which facilitates proofs of the theorems. I. Random Access Machines A random access 
machine (RAM) con- sists of a finite program operating on an infinite sequence of registers. Each register 
can hold an arbitrary integer (positive, negative, or zero). The con- tents of the registers is denoted 
by the sequence X0,Xi,X2,... Associated with the machine is the function Z(n) which, roughly speaking, 
denotes the time required to store the number n . The most natural values for ~(n) are i) take ~(n) identically 
one, and 2) take £(n) approximately logln I . For the results stated here, if the value of £(n) is not 
given explicitly, we need only assume £(n) is positive, nondecreasing on the positive integers, symmetric 
(~(n) = £(-n)) , and subadditive (£(n+m) ~ £(n) + £(m)) The possible instructions, together with their 
execution time in terms of £(n) , are given in Table i. Here i,j,k, are any non-negative integers. Instructions 
Execution Time Xi÷C,C any integer 1 Xi÷X j +X k ~(Xj)+£ (Xk) Xi÷X j -X k £ (Xj)+£ (Xk) Xi÷Xx. £ (Xx.)+Z 
(Xj) J J XX i ÷X.J Z(Xi)+£(XJ ) TRAm if J X.>O £ (Xj) Read X i Z(input) Print X i £(Xi) Table 1 % The 
results reported here are written up in detail in Reckhow's Master's thesis [i]. Many of them were sketched 
earlier in course notes [2], [3]. The RAM model is described and motivated in [4]. The effect of most 
of the instructions should be evident. For example, X i ÷ C causes X. to assume the value C , 1 while 
X i ÷ Xj + X k causes X i to assume the value Xj + X k . The instruction TRAm if X. > 0 causes control 
to be J transferred to line m of the program if X. > 0 Normally control is transferred J from one 
line to the next. Read X. i causes X. to take on the value of the i next input number, and Print X 
i causes X i to be printed on the output tape. The indirect instruction X i ÷ XX. J causes register 
number X. to be copied J into register number i , provided Xj~ _> 0 . The instruction XX.i ÷ X.j has 
 an analogous meaning. The indirect instructions are necessary in order for a fixed program to access 
an unbounded number of registers as the inputs vary. A R~,I program is started at the first instruction, 
with all registers initially zero, and it halts when a trans- fer is made to a line with no instructions, 
or when a negative indirect address is encountered. We propose the following value for the function 
£(n) : t = [[log21n I] if Inl z 2 £(n) 1 if Inl < 2 (Here Ix] is the least integer z x .) We will call 
this the logarithmic function. Briefly, the reason for taking £(n) logarithmic instead of constant is 
that each register is allowed to hold arbi- trarily large integers. Thus any fixed word length machine 
simulating our RAM would require about log n registers to store the number n held in a single RAM register 
(where the base of the logarithm is the largest number that can be stored in a single register). A more 
extensive argument is given in [4], along with moti- vation for our choice of instructions. We are interested 
in the time re- quired for a RAM to recognize a set A of strings on a finite alphabet = {Ol,...,Op} 
A string w = o. ~.... o. will be presented to 11 12 i n the machine as the string of integers il,i2,...,in,0 
, where 0 indicates the -73- end of the string. The machine must exe- cute a sequence of n + 1 Read 
instruc- tions to acquire the string The machine accepts w by printing a 1 and halting (in a computation 
with input w ) and rejects w by printing a 2 and halting. The computation time is the sum of the execution 
times of all steps in the compu- tation as given by the second column of Table i. We say the machine 
M recognizes a set A of strings within time T(n) iff for every string w on Z ,M halts on input w within 
time T(lwl) and accepts w if w E A and rejects w if w ¢ A . We are interested only in asymp- totic run 
times, which accounts for the fact that the execution times in Table 1 may be off by a constant factor. 
 2. The Programming Language Since the proofs of the results reported here require construction of lengthy 
RAM programs, it is convenient to introduce a programming language called RAM-ALGOL, which is a subset 
of ALGOL 60. The main differences between RAM-ALGOL and ALGOL 60 are: i. Real numbers have been eliminated. 
 2. The only arithmetic operators are + and -  3. Procedures and switches are not allowed to be recursive. 
 4. Arrays are one-dimensional and in-  finite. These differences are described in more detail in 
[I]. In order for RAM-ALGOL to be useful for proving theorems about time bounds, it is necessary to 
implement RAM-ALGOL on a RAM in such a way that the execution time of a RAM "object program" can be deter- 
mined (to within a constant multiple) by inspection of the RAM-ALGOL "source program". This type of implementation 
can be done in a relatively straightforward way, and is described in detail in [i]. The most interesting 
feature of the language to implement is arrays. The declaration integer ark A ; declares A to be an 
infinite array of integers: A[0], A[I], A[2] ..... A RAM-ALGOL program can contain many ar-rays, but 
since there is no recursion, a fixed program P references at most a fixed number of arrays, say k . We 
can then interleave the storage for these k arrays and the scalars and temporaries of P as follows: array 
A 1 is stored in RAM registers Xi'Xk+i+I'X2(k+i)+i'''" array A 2 is stored in RAN registers X2,X(k÷l)÷2,X2(k÷l)÷ 
2 .... array A k is stored in RAM registers Xk,X(k+l)+k,X2(k+l)+k .... scalars of P are stored in RAM 
registers X0,Xk+i,X2(k+i),... So that in general, the value of Ai[J] is found in RAM register Xj(k+l)+i 
The accessing of elements of arrays can now be implemented as follows. Suppose, for example, that we 
want to implement the RAM-ALGOL statement Y := A[Z] , on a RAM. Assume that the integers Y and Z are 
 stored in the RAM registers XY.t~nd X Z respectively, that A is the x of k arrays in program P , and 
that Xtemp is a temporary register. Then we have the following implementation: Program Timing Xtemp+i 
1 [ Xtemp÷Xtemp+Xz ~(i)+~(Z) k÷l ~ : tzmes| l Xtemp÷Xtemp+Xz ~(kZ+i)+~(Z) Xy÷XXtem p ~((k+l)Z+i)+g(A[Z]) 
 Since k and i are fixed, and ~(n) is subadditive, the total execution time of this statement is O(~(Z) 
+ ~(A[Z])) . Storing into an array is implemented in analogous way, so that the execution time of A[Z] 
:= Y is O(£(Z) + £(Y)) Since we are not concerned with constant factors in the timing analysis, the 
other features of ALGOL can generally be implemented in the most obvious way. Since procedures are 
not allowed to be recursive, we can actually replace each procedure statement by its procedure body 
 (as described in the ALGOL 60 Report), and translate the resulting procedureless pro-  gram into RAM 
instructions. The Appendix to this paper contains an example of a RAM-ALGOL program and its timing analysis 
used in the proof of theo- rem i. Complete RAM-ALGOL programs are given for most of the proofs in [i]. 
 Z. Stored Program Machines  The RAM model described in section 1 has a fixed program. Since most existing 
computers are stored program devices, it is worthwhile asking whether the stored pro- gram feature adds 
to the computation speed. The Random Access Stored-Program Machine (RASP) described here is similar to 
the RASP's described by Hartmanis [7]. Our machine has an accumulator (AC), which holds an arbitrary 
integer, an instruction counter (IC), which holds a non-negative integer, and an infinite sequence of 
memory registers X0,Xi,X2,... , each of which can hold an arbitrary integer. The instruc- tions for 
our RASP, along with their timings, are given in Table 2.  -74- An instruction is stored in two Normally 
a RASP program consists of a consecutive memory registers. The first finite number of instructions (the 
first register contains an operation of which is stored in registers X 0 and code (shown in the third 
column of Table X 1 ) and data words, stored in certain 2), and the second contains the parameter of 
the instruction, j , which is either specified memory registers. All other an address or an integer constant. 
Note memory registers and the AC and IC are set that since indirect addressing is not pro- to zero. Execution 
of an instruction con- vided for by the instruction set, RASP sists of retrieving an operation code 
from programs must modify themselves in order register XiC and (provided the operation to access an unbounded 
number of registers. code does not specify a halt operation) RASP Instructions and Execution Times 
 description operation of the execution operation mnemonic code operation time load LOD,j I AC ÷ j; 
~(IC) + £(j) constant IC ÷ IC + 2 add ADD,j 2 AC ÷ AC + Xj; ~(IC) + g(j) + IC ÷ IC + 2 ~(AC) + Z(Xj) 
 subtract SUB,j 3 AC ÷ AC - Xj; ~(IC) + Z(j) + IC ÷ IC + 2. £(AC) + g(Xj) store STO,j 4 x ÷ AC; ~(IC) 
+ Z(j) + ] IC ÷ IC + 2 ~(AC)  branch on BPA,j S if AC>0 then Z(IC) + ~(j) + positive IC+j;otherwise 
£(AC) accumulator IC + IC + 2 read RD,j 6 X.÷next input; Z(IC) + Z(j) + ] IC ÷ IC + 2 g(input) print 
PRI,j 7 output Xj; g(IC) + £(j) + IC + IC + 2 ~(Xj) halt HLT -:o to 0 stop ,.,(It) + and £ (Xlc) 8to~o 
 Note: The machine halts if a. the operation code (Xic) is not between 1 and 7, or b. the parameter 
j (i.e. XiC+i ) is negative for operations  2 through 7. TABLE 2. -75- the parameter j from register 
XiC+i . The appropriate operation is then performed as indicated in the fourth column of Table 2, and 
the cycle is repeated until an illegal operation code or parameter is encountered, at which time the 
machine halts. As with the RAM, RASP execution times are weighted by a cost function ~(n) . The execution 
times of the various RASP instructions are listed in the last column of Table 2. Note that, contrary 
to the RAM instruction timings, the processing of addresses and constants is explicitly charged for on 
a RASP. This is reasonable, since by modifying itself, a RASP program can generate instructions with 
arbitrarily large addresses and constants and then execute them. We also charge for the value of the 
instruction counter at the time of execution of each instruction, since the IC may also grow arbitrarily 
large during execution. Theorem I. For each function T(n) ~ n , a set A is recognizable by a RAM in 
time O(T(n)) if and only if A is recognizable by a RASP in time O(T(n)) The proof that a RAM can simulate 
a RASP in a proportional amount of time consists of the RAM that does the simu- lation. This program 
is given in the Appendix. Simulation of a RAM by a RASP is accomplished by replacing each RAM instruc- 
tion by an equivalent sequence of RASP instructions. Since part of the RASP storage is needed for the 
program, all of the registers used (explicitly or by indirect reference) by the RAM must be offset by 
a fixed constant in the simu- lation. Indirect reference is accomplished in the RASP by instruction modification. 
 4. Turing Machines and RAM's Since the multitape Turing machine is the standard computer model used 
in complexity literature, it is interesting to compare its computing power with that of RAM's. The result 
shows they are not as different as one might expect. Theorem 2. a) If a set A is recognized by a RAM 
P ( P has ~(n) logarithmic) within time T(n) > n , then A is recognized by some multitape Turing machine 
within time (T(n)) 2 b) Conver sely, if'some Turing machine recognizes A within time T(n) ~ n , then 
some RAM (with arbitrary cost function ~(n) ) recognizes A within time O(T(n).~(T(n))) . The proof of 
b) is straight- forward. The simulating RAM simply stores the contents of the Turing machine's k tapes 
in 2k arrays, one tape symbol per array element, with one array for the left half of each tape, and one 
array for the right half. The finite control of the Turing machine is handled by the simu- lating program 
in a fixed amount of time per Turing machine step, except for acces- sing of the tape arrays. Since 
a head of the Turing machine can move at most T(n) squares away from the starting position in T(n) 
steps, the time it takes the RAM to access this farthest tape square is O(~(T(n))) Thus the total time 
for the simulation of T(n) steps is 0 (T (n)'~ (T(n))) The proof of a) is a bit more subtle. The problem 
is that the Turing machine needs an efficient way to simulate the random access storage and the instruc- 
tions of a RAM. Our simulating machine M will use one of its work tapes to hold the contents of P's memory 
in the following format. $*al#cl*a2Oc2...*aj#cj¢  Here the a i (i=1,2, .... j) are the addresses of 
the registers of P that have been stored into thus far in the computation. The a i are represented 
in binary and are arranged in increasing order. The c i (i=l,2,...,j) are the contents of the corresponding 
registers, also stored in binary. LEMMA. This work tape contains at most ~) nonblank squares, where 
k is a constant which depends on P but not on n . (We assume T(n) > 0 .) The proof of this lemma, 
which involves a tedious analysis of several cases, is given in [i]. The key obser- vation in the proof 
is that for ~(n) logarithmic, the number of bits in the binary representation of n is approxi- mately 
equal to ~(n) With this storage configuration, then, it is clear how the Turing machine can simulate 
a single RAM instruction with a small fixed number of passes down its storage tape in time O(T(n)) . 
If P runs for time T(n) , then the Turing 2 machine simulating P halts in O((T(n)) steps. Finally, since 
Turing machines have linear speedup, and since (T(n)) 2 > n there is a Turing machine M' which simulates 
P in time (T(n)) 2 5. The RAM Complexity Classes Definition. A function T(n) on the positive integers 
is time constructible (with respect to a cost function ~(n) ) iff there is some RAM program P (with 
cost function ~(n) ) such that for all n , P reads n , prints T(n) , and halts within time O(T(n)) We 
note that "time constructible" is similar to "real-time countable" for Turing machines, and is the analog 
for time of " O (as defined in [5]) c nstructable" for storage. Theorem 3. If £(n) = I or £(n) is logarithmic, 
and if T2(n ) ~ n£(n) is time constructible with respect to £(n) , then there is a set A ¢ {1,2}* such 
that some RAM program recognizes A within time o(r2(n)) , but for any function Tl(n) satisfying lim 
inf Tl(n) (i) n + ~ ~y : 0 no RAM program recognizes A within time 0 (T 1 (n)) We note that Hennie 
and Stearns [6] prove a similar result for multitape Turing machines, but their result is weaker in that 
the numerator in equation (i) is Tl(n) log Tl(n ) instead of Tl(n) In fact, we know of no argument 
in the liter~ ature proving a result as strong as theorem 3 for a class of abstract computing machines. 
 The proof of theorem 3 is straight- forward for Z(n) = 1 , but is rather subtle for ~(n) logarithmic. 
The trouble in the latter case stems from the neces- sity of estimating the logarithmic function ~(n) 
sufficiently rapidly after each step of computation so that the diagonalizing machine recognizing A can 
shut itself off within time O(T2(n)) RAM instructions can be encoded on the alphabet {1,2} according 
to table 3. Note that numbers are represented in unary notation. RAM Program Codes RAM instruction encoding 
on {1,2} 1. X. ÷ c 121i21~2 1 . + X k 1221i21J21k2 2. X i + X 3 3. X i + Xj X k 1321i21J21k2 4. X i 
÷ XX. 1421i21J2 3  5. XX. ÷ X. 1521i21 j2 z 3  6. TRAm IF X.>0 1621m21J2  ]  7. READ X. 1721i2  
8. PRINT X. 1821i2  1 1 Notes : repeate~mes i. i n denotes the string ii . ii 2.c if c ~_ 0 2. 
c = -2-c-i if c < 0 TABLE 3. A RAM program P can be encoded by con- catenating the encodings of its 
instruc- tions to form the string Wp For w c {1,2}* , we use P to denote the W longest program whose 
encoding is an initial segment of w . Note the identity P = P' , and that for any program R , Wp, 
there are arbitrarily long strings w c {1,2}* such that Pw = R . We can now describe the set A in the 
statement of theorem 3: (i) If Pw with input w halts within time k(Pw).T2(n ) , then w ~ A iff Pw does 
not accept w When Pw with input w does not halt within time k(Pw).T2(n ) , we do not care whether w c 
A or not.  (2) The coefficient k(P w) may depend on Pw ' but not on the part of w that follows Wp  
 W For Z(n) = 1 , k(P w) = i , and A = {wlP w with input w halts within time T2(n) without accepting 
w }, but for ~(n) logarithmic, k(Pw) may be very samll for large P w The proof that no RAM recognizes 
A in time O(Tl(n)) is now easy, for suppose P' recognizes A in time C.Tl(n ) Then choose w E {1,2}* 
so that Pw = P' and long enough (by filling the end with "garbage") so that c'Ti (]wI)<k(P') .T2 (lwI) 
But then w e A => P' accepts w within time   c.T 1 (Iw]) => Pw accepts w within time  C'Tz(]W]) => 
Pw accepts w within time k(P') .r2(Iwl) => Pw accepts w within time k(P w) "T 2 (]w[) => w ~ A, and w 
~ A => P' rejects w within time  C'Tz(|W]) => Pw rejects w within time  k(P w)'T2(lw[) => w g A, 
which is absurd. We conclude that such a P' cannot exist. The proof that some RAM program M recognizes 
A within time O(T2(n)) is constructive. Complete RAM-ALGOL programs for M are given in [I]. M consists 
of two parts: a preprocessor, and a simu- lator. The preprocessor first reads the string w and stores 
it in an array, one symbol per register. It then decodes the program Pw ' and stores it instruction by 
 instruction in a set of arrays. The execution time of the preprocessor is O(n~(n)) , where n = ]w[ 
.  -77- The simulator first sets a timing counter to T2(n) This step takes o(r2(n)) The simulator 
then runs the program Pw ' as stored in the arrays, interpretively with the string w as inputs. It 
uses another array to repre- sent the registers used by Pw If Pw runs for too long, the simulation 
is halted anyway, and w is rejected. If Pw halts on input w within time k(Pw)'r2(n) ,then M accepts 
w iff Pw does not accept w  If £(n) = 1 , it is clear that M can simulate T2(n ) steps of Pw on in- 
 put w , taking no more than some fixed constant amount of time to simulate each step. So that in this 
case, the simu- lation takes time O(T2(n)) , and the total running time for M is O(n) + O(T 2(n)) = 
O(T 2(n)) For £(n) logarithmic, two pro- blems arise. First, the straightforward way of keeping track 
of the simulation time is too slow. The cost of updating the timer in the ordinary way is O(~(T2(n))) 
at each step, so that the total execution time of the simulation would be O(T2(n).~(T2(n)) ) We circum- 
 vent this problem by storing the bits of the binary representation of the time remaining for the simulation 
in an array, and subtracting from this array each time an instruction is simulated. In [i] it is shown 
that, since borrows don't often propagate very far, the time spent keeping track of the simulation time 
in this way is O(T2(n)) The second problem is that we know of no way to compute the logarithmic Z(n) 
in time O(£(n)) We are forced to use an estimate of kin) that is both accu- rate enough and easy to compute. 
For this purpose, we define [0 if Inl < 2 norm(n) =~rlog21og21nl I if In I ~ 2 ' and we use 2 n°rm(n) 
as an estimate of £(n) This estimate is never off by more than a factor of 2 . Considerable pro- gramming 
machinery and saving of auxiliary information (powers of two, norms of all registers) are needed in order 
to insure that norms can be computed quickly enough. Finally, the coefficient k(Pw) must be explained. 
In order to keep the simulation time bounded by a constant times T2(n ) , the simulator must charge 
 not only for the simulation, but it must also charge for the processing of the addresses and constants 
in its stored version of Pw For fixed Pw ' these extra charges are bounded by a constant (l-k(Pw)) 
times T2(n ) , and the amount of execution time of Pw is then at least k{Pw)-r2(n ) But if Pw is large, 
con- taining large addresses or constants, then k(Pw) may be very small (although always strictly > 
0 ). See [I] for more detail about how M recognizes A within time T z (n) . REFERENCES I. Robert A. 
Reckhow. Diagonal theorems for random access machines. Master's thesis, Department of Computer Science, 
University of Toronto, October 1971. 2. Stephen A. Cook. Computational com- plexity using random access 
machines. Diagonalization on ran- dom access machines. CS 230 Notes, Department of Computer Science, 
University of California, Berkeley, January 1970.  3. Stephen A. Cook. Universal random access machines. 
CSC 2428S course notes, Department of Computer Science, University of Toronto, January 1971.  4. Stephen 
A. Cook. Linear time simu- lation of deterministic two-way pushdown automata. Proceedings of IFIP Congress 
71, Foundations of Information Processing.  5. John Hopcroft and Jeffrey Ullman. "Formal Languages and 
their Relation to Automata".  6. F.C. Hennie and R.E. Stearns. Two-tape simulation of multi-tape Turing. 
machines. J.A.C.M. 13, 4 (1966) pp. 533-546.  7. J. Hartmanis. Computational Complexity of Random Access 
Stored Program Machines. Mathematical Systems Theory 5, 3 (1971) pp. 232-245.   -78- APPENDIX RAM-ALGOL 
Program to Simulate RASP Program P RAM-ALGOL Statements begin comment Declarations for program R; integer 
array MEMORY; integer dummy, AC, IC, J;  Boolean simulating; comment The first thing we do is to 
initialize the first p elements of MEblORY, so as to mimic the initial configuration of P; MEMORY[0] 
:= initial value0; MEMORY[0] := initial vaiuel; MEMORY[p] := initial valuep; AC := IC := 0; comment 
The main part of the simulator is the following loop, which is executed once for each instruction simulated. 
If t i is the execution time of a single instruction of P, then the pass through this loop simulating 
the execution of that instruction takes time OCti). Thus if I(n) is the number of instructions executed 
by P on inputs of length n, then T(n)-[~lti z=  and the execution time of R is I~) I (n) O(ti) : 
O(Z t i) : O(T(n)); i=l i=l simulating := true; timing 0(1) 0(1) 0(1) 0(1) 0(I) -79- (continued) 
for dummy := 0 0(i) while simulating do 0(I) begin integer opcode; opcode :: MEMORY[IC]; 0 (9,,(IC)+~ 
(opcode)) if opcode _< 0 v opcode >-8 then 0 (4 (opcode)) simulating := false 0(I) else begin j :: MEMORY[IC+I]; 
O(~(IC)+£(j)) IC := IC + 2; 0(~(IC)) if opcode = 1 then begin 0(1) comment The execution time of LOD,j 
is ZiIC)+£Cj) ; AC := j o(z(j)) end else if opcode = 2 then begin Oil) comment The execution time of 
ADD,j is ~(IC) +£ (j) +~ (AC) +~ (Xj) ; if j < 0 then simulating := false o(z(j)) else AC := AC + MEMORY[j] 
o (~ (j) +~ (AC) ÷~ (X i )) end else if opcode = 3 then begin 0(1) comment The execution time of SUB,j 
is Z (IC)+g (j) +Z (AC)+Z (Xj) ; ifj < 0 then simulating := false Oit,(j)) else AC :: AC -MEMORY[j] 0 
( ~,(j ) +£ (AC) +£ iX i ) ) end else if opcode = 4 then begin 0(i) J comment The execution time o£ STO,j 
is £(IC)+Z (j)+~(AC) ; if j < 0 then simulating := false O(~(j)) e/se MEMORY[j] := AC 0(£(j)+~(AC)) end 
else if opcode = 5 then begin 0(1) comment The execution time of BPA,j is ~(IC)+£ ij)+~ (AC) ; if j < 
0 then simulating := false O(£(j)) else if AC > 0 then IC :: j O(£(j)+~(AC)) end else if opcode = 6 
then begin 0(1) comment The execution time of RD,j is £(IC)+~(j)+~(input); if j < 0 then. simulating 
:= false O(~(j)) else read (MEMORY[j]) 0(Z(j)+£(input)) end else begin comment The execution time of 
 PRI,j is ~ (IC)+~ (j)+~ (X./)~ if j < 0 then simulating . alse O(~(j)) else print(MEMORY[j]) Oigij) 
+g(xj)) end end end of the simulation loop; end of program R; 
			