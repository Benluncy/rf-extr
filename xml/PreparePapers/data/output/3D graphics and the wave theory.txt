
 Ceml}uter Grapltic$ Volume 15, Number 3 August 1981 3D Graphics and the Wave Theory Hans P. Moravec 
Robotics Institute Carnegie-Mellon University Pittsburgh, PA 15213 A bst ract A continuing trend in 
computer representation of three dimensional synthetic scenes is the ever more accurate modelling of 
complex illumination effects. Such effects provide cues necessary for a convincing illusion of reality. 
The best current methods simulate multiple specular reflections and refractions, but handle at most one 
scattering bounce per light ray. They cannot accurately simulate diffuse light sources, nor indirect 
lighting via scattering media, witllout prohibitive increases in the already very large computing costs. 
Conventional methods depend implicitly on a particle model; light propagates in straight and conceptually 
infinitely thin rays. This paper argues that a wave model has important computational advantages for 
the complex situations. In this approach, light is represented by wave fronts which are stored as two 
dimensional arrays of complex numbers. The propagation of such a front can be simulated by a linear transformation. 
Several advantages accrue. Propagations in a direction orth0gonal to the plane of a front are convolutions 
which can be done by FFT in O(n Iogn) time rather than the n 2 time for a similar operation using rays. 
A typical speedup is about 10,000. The wavelength of the illumination sets a resolution limit which prevents 
unnecessary computation of elements smaller than will be visible. The generated wavefronts contain multiplicities 
of views of the scene, which can be individually extracted by passing them through different simulated 
lenses. Lastly the wavefront calculations are ideally suited for implementation on available array processors, 
which provide more cost effective calculation for this task than general purpose computers. The wave 
method eliminates the aliasing problem; the wavefronts are inherently spatially filtered, but substitutes 
diffraction effects and depth of focus limitations in its stead. Permission to copy without fee all 
or part of this material is granted provided that the copies are not made or distributed for direct commercial 
advantage, the ACM copyright notice and the title of the publication and its date appear, and notice 
is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, 
or to republish, requires a fee and/or specific permission. 01981 ACM O-8971-045-1/81-0800-0289 $00.75 
 1. Introduction The realism in computer synthesized images of 3D scenes has gradually increased from 
engineering drawings of simple shapes to representations of complex situations containing textured specular 
reflectors and shadows. Until recently the light models have been to first order; the effect of only 
one bounce of light from the light sources to the eye via a surface was simulated. The stakes have recently 
gone up again with the introduction of ray tracing methods that simulate multipl~ non-dispersive refractions 
and specular reflections [4] [7]. With each increment in the faithfulness of the process new properties 
of real light had to be added. Thus far the evolution has been in the direction of the particle theory 
of light, probably because straight rays preserve most of the properties of 3D computer graphics' engineering 
drawing roots. An obvious and desirable next step in the process is the simulation of scattering, as 
well as specular, reflection of light from surface to surface. That such effects are important is shown 
graphically by the subjective impressions of increasing realism as the models become more complex (see 
references in [7]) and objectively in visual psychology experiments such as those by Gilchrist [2], in 
which subjects were able to intuitively separate the effects of surface albedo and light source brightness 
by noting the relative intensity of scattered illumination. While a specular/refractive bounce can convert 
a ray from a simulated camera or lightsource into two rays, a scattering can make hundreds or thousands, 
heading off in a multitude of directions. Since pictures modelling only specular bounces themselves take 
about an hour of computer time to generate, simulation of multiple scattering (and incidentally, extended 
light sources) by extension of conventional methods is out of reach for the immediate future.  2. A 
Way Out One solution may be a different approach in simulating the physical reality. The ray representation 
implicitly assumes an effectively infinite precision in the position of objects and the heading of rays. 
This leads to some unnecessary calculations, as when several rays bouncing from nearly the same surface 
point in nearly the same direction (perhaps originating from different light sources) are propagated 
by separate calculations. We can evade these and other inefficiencies by using the particle theory's 
major competitor, the wave theory of light. Rather than multitudes of rays bouncing from light sources 
and among objects, the illumination is done by complex wavefronts 289 Computer 6raphics Volume 15, Number 
3 August 1981 described over large areas. Such wavefronts can be represented by arrays of complex numbers, 
giving the amplitude and relative phase of portions of the front, embedded in the simulated space. Lesem.et. 
al. used this approach [5] to make physical holograms (though of disappointingly low quality). Note that 
a plane array of complex wave coefficients is very like a window into a scene. The wavefront, fully described 
by the coefficients, can represent multiple point and extended sources of light at various distances 
behind (or even in front of) the window. Figure 1 is a simple example, a slice through a window generating 
light that seems to come from a point source very far away in the lower left direction. A picture of 
the scene behind the window can be formed by passing the wavefront through a simulated lens onto a simulated 
screen. Moving the lens center changes the apparent point of view. The size of the lens and its distance 
from the scene determine the depth of focus of the image. The maximum number of resolvable pixels in 
the image can be no greater than the number of wavefront coefficients - no more information is present 
and any attempt to get more resolution in the image than in the wavefront will result in blurring. Also, 
no object feature smaller than a wavelength of the simulated light can be represented. Thus the wavelength 
must not be too large. It should also not be smaller than necessary, since the computational cost goes 
up dramatically as the wavelength drops. ;k/2 I Array I Coefficients Figu re 1 : How a wavefront description 
in one  3. Simplifications Here's one way to use the light wave idea. It deviates from a true physical 
simulation in the interests of simplicity and computational efficiency. Goodman [3] presents the fundamental 
mathematics. Feynman [1] is a good source for aspects of real light I've maligned. We will use light 
of a single wavelength, and we assume that all optical effects are linear. The tight is propagated according 
to scalar diffraction theory, which behaves realistically except at very short distances. The wavelength 
must be no larger than the smallest object feature we wish to see, but should not be too small because 
the amount of computation increases dramatically as the wavelength drops. To faithfully represent a wavefront, 
there must be a coefficient every half wavelength across the width of a description, so the number of 
complex numbers needed in a description increases as the square of the inverse wavelength. We also assume 
that only the steady state of the illumination is important. This allows us to play cavalierly with time. 
For instance, a wavefront may be propagated from one plane description to a parallel one some distance 
away in a single step, even though different portions of the effect have different transit times. ave 
 plane can represent a wave travelling in an oblique direction. This is a one-dimensional analog of the 
2D wavefronts discussed in the text. The complex wavefront coefficients are shown here as small arrows. 
The length and orientation of each arrow represents the magnitude and phase of one coefficient. Our waves 
are are linearly superposable, and a complex wavefront can be generated simply by adding the coefficients 
from many simple descriptions. ;k is the wavelength. Computer Graphics Volume 15, Number 3 August 1981 
The scene is represented as a box of half wavelength on a side cells, roughly 1000 cells cubed. The content 
of each cell is characterized by three complex coefficients, one giving the absorption and phase shift 
(simulating refractive index) for transmitted light, the other giving the reflection attenuation and 
phase shift (simulating surface scattering). The third term is unconditionally added during a wavefront 
calculation, which is how we make light sources. This 3D array need not be explicitly stored, but 10002 
cell slices perpendicular to at least one of the major axes should be efficiently extractable at will. 
One suitable storage scheme is the recursively subdivided "oct-tree" representation [6], which permits 
large uniform volumes to be kept as single nodes of a tree. 4. Some Details By contriving scenes like 
Figure 4 for computational cheapness, we can try out some of the methods with modest processing power. 
Such "cheap" scenes consist of a number of very thin objects confined to one of a small number of parallel 
planes. The wavefront calculations are done in a direction parallel to these object planes. The lenses 
and in the scenes are actually thin regions of slowly varying transmissive phase shift. The light sources 
in the the first object plane (plane a) generate a wavefront which we will call WF a. WFa(i,j ) where 
i a.nd j are integers between 0 and 511, represents one complex coefficient in a half wavelength grid 
across the area of plane a. The effect of WF a on b, the second plane, (call it WFab ) is the sum of 
the effects of every WFa(i,j) on every coefficient WFab(k,I). Each such contribution involves noting 
tl~e distance between the WFa(i,j ) and the WFab(k,I) cells and adjusting the phase (because of the time 
delay introduced by travelling over such a distance) and the amplitude (because of the inverse square 
and angle of incidence attenuation or equivalently because of the amount of solid angle cell WFab(k,I) 
occupies when seen from WFa(i,j) ). This operation is equivalent to a multiplication of the complex valued 
wavefront coefficient by a complex valued propagation coefficient to obtain a complex valued contribution 
to the new wavefront. (In my program the propagation coefficients were determined by integrating the 
path from the center of the originating cell over the area of the receiving cell, in a 10 by 10 subdivision 
using Simpson's rule. In fact each coefficient is the sum of four such integrations, representing the 
effect of two repetitions of each source cell in X and Y; the first orders of a periodic repetition of 
the matter planes --this is an implementation detail). Doing this process straightforwardly means multiplying, 
each of our 10242 wavefront coefficients by 10242 propagation coefficients and summinq the products appropriately 
into 10242 resultant waveform coefficients. This is over a trillion complex multiplications and additions, 
and too expensive for present day conventional hardware. Fortunately there is a cheaper way tO achieve 
the same result. Note that for a given k and I the distance from WFa(i,j) to WFab(i+k,j+l), and thus 
the propagation coefficient, is the same for all values of i and j. Call this coefficient Pa,b(k,I). 
We can now express the resultant wavefront by WFab(m,n) = Sumi, j Pab(m-i,n-J) WFa(i,j) This is the convolution 
of WF a with Pab' and will be denoted by WFa*Pab. Such a convolution can be accomplished by taking the 
Fourier transforms, multiplying them term by term then taking the inverse Fourier transform of the result: 
 WFab = WFa ~' Pab = F-I(F(WFa) " F(Pab)) where F denotes Fourier transform and. denotes the termwise 
 product of two arrays of like dimension to obtain a third. Since Pab iS an invariant of the scene, F(Pab 
) can be calculated once and for all when the scene is created. The symmetries Pab(i,j) = Pab(J,i) = 
Pab(-i,j) = Pab(J,-i ) can help cut clown that cost a little. The Fast Fourier Transform (FFT) is a 
method of arranging the calculations so that only about n Iog2n multiplications and a similar number 
of additions are required. A two dimensional transform like we require can be accomplished by taking 
a one dimensional FFT of each row, replacing each row by its transform, then doing one dimensional FFT's 
of the columns of the result. In our case the one dimensional transforms have 1024 points, and require 
about 10,000 complex multiplies each. There are 2x1024 of them to be done per 2D transform, for a total 
of about twenty million multiplies. The inverse transform is computationally equivalent to the forward 
one, so the FFT approach requires 60 million multiplies, compared to the million million of the direct 
method. If the F(P) is assumed to be precomputed the cost drops to 40 million. The FFT approach is thus 
about 20,000 times faster than the direct convolution method. If the naive ray tracing schemes were applied 
to the same scene they would work essentially by the direct method, though probably in a less orderly 
fashion. No organization analogous to the FFT method for wavefronts has been offered for the ray schemes, 
so this 20,000 times speedup is a major advantage for the wave approach. Getting back to our scene, we 
have now calculated WFab, the wavefront impinging on plane b. The transmission coefficients of plane 
b (Tb) which tell of the attenuation and refraction caused by the substances at b are multiplied term 
by term with the incoming wave front (a mere million complex multiplies) to generate a modified front, 
to which is added the effect of any light sources at b (Lb) to give us WF b, the wavefront leaving plane 
b. WF b = WFab.T b + L b = (WFa*Pab).T b + L b The reflection coefficients of b (Rb) are also multiplied 
by WFab to obtain the wave reflected back towards a from b. We call it a waveback, and designate it WB 
a. WB a is stored for future reference. We are now ready to carry the wavefront from WF b to and through 
the next plane c in exactly the same manner in which we moved it from a to b: WF c = (WFb* Pbc)-Tc + 
L c we also save for future reference WB b = (WFb*Pbc).R c (doing the convolution just once for the two 
results, of course). Each such move costs about 50 million complex multiplies (= 200 million real multiplications) 
and can be done in about five compute minutes on a typical medium sized machine" like our VAX 11/780. 
With a 15 megaflop array processor like the CSPI MAP- 300 the time is less than 15 seconds. After a complete 
sweep across the scene (Figure 2) we have all the first order lighting effects if the light sources were 
all in plane a, and some higher order ones, as when light is scattered by b then c to illuminate d. We 
now sweep backwards, from the far end of the scene towards a. Each step is similar to the forward sweep 
except that at each plane the waveback left behind in the first pass is added to the outgoing wavefront, 
like the light sources at that plane, simulating the contribution of reflected light from the last pass. 
For example, the return sweep modifies the wavefront  Computer Graphics leaving plane c into the one 
leaving b by the transformation WF b = (WFc*Pab).Tb + L b + WB a in the same step WB c, the effect of 
light from the return sweep reflected from b to c is stored WB c = (WFc*Pab),R b After several back 
and forth passes, each involving WB's from the previous pass, the wavefront contains the effects of highly 
indirect light The iteration can simply be stopped after a fixed number of sweeps, or the wavefront can 
be checked for significant changes between forward/back cycles. If WF a is nearly the same before and 
after two sweeps (by a sum of squares of coefficient differences, for instance), the process has settled 
down. Ten passes is usually enough, though pathological cases like scenes containing two facing mirrors 
may require more. Images are generated from the ultimate wavefront in a final step by passing the wavefront 
through a new plane which contains a simulated lens that projects the image onto a virtual surface where 
magnitude (absolute value) of the complex wavefront coefficients are extracted, scaled and transcribed 
into a pixel array. Rb I Rc Volume 15, Number 3 August 1981 Color pictures can be made of three such 
images, one each for red, green and blue, with different reflection, transmission and emission properties 
of the things in the object planes. The simulated wavelength is the same in all cases, so the simulated 
color has nothing whatever to do with the physiological color property of real light, which depends on 
wavelength. Figure 4 contains two planes, and was subjected to eight wavefront passes (four each way). 
The first plane contains a coherent light source which projects a beam onto the scattering "lampshade" 
in the second plane. The scattered light reflects back to the first plane and illuminates the "checkerboard" 
there, and so on for eight bounces. A thin lens in the second plane distorts some of the checkerboard 
behind it. The two planes are 300 wavelengths apart, and 512 half-wavelengths on a side. Light from the 
scene was intercepted by a square lens of tl~e same size 10,000 wavelengths away, which formed the displayed 
image yet another 10,000 wavelengths further on. The illumination is monochromatic, but color is used 
in one version of the image to represent the relative phase of the incident light. The red, green and 
blue components of the color are varied SWEEP 1 Rd / ++ I I + --Ld SWEEP 2 WBb WBc WBd Figure 2: The 
order of computation in a wavefront image generation. These are the first two of many passes. Each pass 
leaves behind a trail of reflected waves, which are added into the computation on the next pass. Each 
step within a pass involves multiplying a wavefront description by the transmission coefficients of the 
current matter plane, adding in the light source intensities, and the appropriate reflected wavefront 
from the last pass, and convolving with the propagation coefficients for the gap to the next matter plane, 
to produce the wavefront impinging there. In addition the incoming wavefront is multiplied by the reflection 
coefficients of the current matter plane to generate a new reflected wavefront. Computer Graphics Volume 
15, Number 3 August 1981 with phase like the three components of three phase power; red varies with the 
sine of the phase angle, green with the angle shifted 120 degrees, blue by phase shifted 240 degrees. 
We don't yet have an array processor, so the compute time was about an hour. With an FPS-IO0 it could 
have been generated in 8 minutes. CSPI's MAP-300, with twice as many multipliers, could have done it 
in 4. It should also be noted that the calculations are highly suitable for division among parallel processors 
(most of the cost is in the 1024 point complex FFT's, and 1024 of these, one for each row or column, 
could be carried out in parallel). Since the scenes contain extended light sources and model multiple 
indirect light scattering, all conventional ray methods would have required at least thousands of compute 
hours to produce the same result. There may, of course, be as yet unexplored ray approaches that do scattering 
more efficiently. 5. Defeating Diffraction Since the illumination is by simulated monochromatic light, 
potentially all in phase, there is a tremendous opportunity for probably undesirable effects such as 
interference fringes and "laser speckle" to make themselves manifest. These are made worse by the maximally 
long wavelength of the light chosen to reduce the amount of computation to the bare minimum required 
for the image and object resolution desired. If the scene is lit mainly by light sources extending over 
many half-wavelength cells, most of the problem can be eliminated by randomizing the phase, of the light 
emitted by the individual cells. This effectively simulates a spatially non-coherent source, and blurs 
interference patterns into oblivion. If the light comes mostly from a moderate number of very localized 
regions, the picture can be recomputed a few times with different relative phases between the sources, 
and the results averaged. This simulates phase non-coherence between the different sources. If the scene 
is lit by a very small number of very localized sources simple randomization of light source phases won't 
help. If much of the actual illumination in those scenes arrives indirectly by scattering of the light 
from objects of high albedo, randomizing the reflective and transmissive phase shifts of the scattering 
objects will improve things a little. In general it also helps if objects have "soft" edges which blend 
gradually over the distance of a few cells from the properties of their surrounding to the optical properties 
of their interiors. The above techniques remove low frequency interference fringes but do not significantly 
affect the high frequency phase interferences analogous to the "laser speckle" seen when viewing objects 
in coherent light. This pixel graininess can be smoothed by computing the image to higher resolution 
than needed and averaging, or by recomputing it several times with different random phases in the extended 
light sources or the light scattering surfaces, and averaging the results.  6. Finessing Focus Another 
consequence of the large wavelength is the limited depth of focus of scenes viewed from close up. The 
same limitation is seen in physical light microscope images. Though any plane can brought into focus, 
regions a short distance in frortt of or behind are blurred into oblivion. A simple degree of freedom 
argument makes it clear that a high resolution image with a given number of pixels cannot be generated 
from a wavefront description with fewer coefficients. We cannot get a well defined 1024 by 1024 pixel 
picture from the wavefront description passing through a "pinhole" smaller than 1024 elernents across. 
But since our scenes are only about that many elements in size, it implies the "lens aperture" of our 
simulated camera must subtend a very large angle except when the camera is very far away. There is a 
consequent large parallax shift from one edge of the lens to the other, and thus blurring of all scene 
planes but the one in focus. This problem has been minimized in figure 4 by viewing the scene from a 
great distance. This is not a general solution, since we would like to admire the scenery from other 
points of view. An interesting aspect of imaging at great distances is that the paths from all parts 
of the scene to the image pixels are nearly the same, and thus the computational precision required is 
great, since the image depends on small differences between large numbers. In the physical world this 
translates to light gathering ability, or light source intensity. The precision of the numbers in the 
computation is directly analogous to the number of photons in the real world analog. When a scene is 
viewed from far away with a small aperture, only a tiny fraction of the photons it emits are intercepted 
by the camera. We could solve the problem imperfectly by composing the picture in several steps by combining 
images focussed at different depths. Edges would then be sharp, but object boundaries would exhibit some 
degree of translucency. A hybrid wave/ray approach is possible. The scene could be illuminated in the 
manner we have described, but the image would be generated by subsequently examining the stored reflected 
wavefronts with simulated rays. A ray would be launched from the eye position through each image plane 
pixel and intersected with the first non-empty cell it reaches. Examining the wave coefficient at that 
cell is not enough, unless the matter at that position a perfect diffuse reflector, since the light direction 
is encoded in the phase relationships of nearby elements. A small neighborhood around the ray must be 
examined to determine what fraction of the light is being reflected to the eye position. The more nearly 
the viewed surface approximates a specular reflector, the larger the necessary region, and the less attractive 
this approach. This suggests that a further step in the ray direction, a combination in which waves do 
the diffuse reflection and in which rays like Whitted's [7] do the entire specular part, would be even 
better. This would still leave a region of "slightly diffusing" or "almost specular" surfaces which could 
be handled well neither by the ray nor the wave approach. Another possibility is to image the wavefront 
through a smaller pinhole, but to retain the requisite information by resolving the front at super-resolution, 
with coefficients spaced more closely than a half wavelength. As in the case of viewing the scene from 
a great distance, the precision of the computation (or the "number of photons") must be large. Unfortunately 
I have not figured out how to do the super-resolution computation in any way but by the prohibitively 
expensive direct method. 7. Bigger Games So far we have dealt with very simple scenes in which all 
the matter was confined to a small number of parallel planes. Not co-incidentally these are particularly 
suitable subjects for the wave approach. More general scenes can be co.nstructed by increasing the number 
of matter-containing planes. In the most general case there is such a plane every half wavelength, and 
fully three dimensional objects are described as a stack of parallel slices. 293   Computer Graphics 
Volume 15, Number 3 August 1981 This increases the required computation by a factor of several hundred, 
and also creates a major storage problem. A special purpose Fourier device able to compute a 1024 point 
transform in 100 usec would allow fully general scenes with complete illumination to be calculated in 
about an hour by the straightforward application of the above method. It would be necessary to store 
about one billion complex numbers, representing the approximately 1000 wavebacks generated, temporarily 
during the computation. Storage for a billion complex numbers (the precision need not be great) may not 
be excessive in a machine than can compute at 500 megaflops, but it doesn't make the problem easier. 
It may be possible to avoid explicit storage of the wavebacks by combining them as they are generated, 
dragging them along backwards with the wavefronts in a time-reversed way. The composite waveback would 
be subjected to transformations inverse to the ones it will experience when the sweep reverses. Or maybe 
not. All versions of this idea rve examined have some unavoidable high order light interactions that 
do not have analogs in the physical world. 8. Waves and the Oct-Tree The above schemes, particularly 
the fully general one, have an unpleasant feature. They do as much work on the empty spaces in a matter-containing 
plane as they do on the substance. Propagating a wave through a dense stack of matter planes involves 
hundreds of half-wave baby steps, even if the planes contain only a speck of matter each. We have seen 
that a wavefront can be transmitted across an arbitrarily large gap of empty space by a single FFT convolution. 
In fact, it is just as easy to send it through an arbitrarily large mass of any substance of uniform 
attenuation and phase shift; only the propagation coefficients change. Since typical scenes contain large 
homogeneous volumes, we may be wasting Some effort; computing over volumes when only the surface areas 
require the full power of the incremental method. The fundamental space underlying our 3D models is a 
cubic array of half-wavelength cells. We can organize this into convenient large regions by the following 
process. Suppose our scene is contained in a cube 1024 half wavelengths on a side. If this cube is homogeneous, 
stop; we have our simple description. If it is not, divide it into eight 512 on a side subcubes, and 
represent it by an eight way branching node of a tree pointing to the results of the same procedure applied 
recursively to the subcubes. The division process at every level stops if the subcube it is handed if 
homogeneous, or if it is a primitive half-wavelength cell (in which case it is homogeneous by definition). 
A wavefront can be propagated through such an "oct-tree" structure irregularly, advancing through large 
undivided subcubes quickly, creeping through the hopefully few heavily subdivided regions. There is a 
serious complication. While the wavefronts in the "stack of planes" approach of the last section were 
sent between parallel planes, with light leaving at the sides assumed lost (or wrapped around, depending 
on the details of the convolution), waves leaving at right angles to the principal wavefront direction 
in a subcube of the oct-tree must be accounted for, because it affects adjacent subcubes, Part of the 
problem is a matter of bookkeeping, solved by having two wavefront storage arrays at every boundary surface 
between distinct subcubes, one for each direction of light travel (an alternative way of looking at it 
is that each distinct subcube of the tree has one outgoing wave coefficient array on each of its six 
faces. Incoming waves are stored in the outgoing wave arrays of adjacent subcubes). The other half of 
the problem is calculating the outgoing wavefront on a surface orthogonal to the plane of the incoming 
wavefront description. A wave entering one face of a cube leaves by one parallel face and four at right 
angles, While the parallel face calculation is a simple convolution, the orthogenal ones are not so easy. 
Whereas the propagation coefficients between two co-ordinates in an incoming and outgoing waveform in 
a parallel propagation are a function simply of displacement in X and Y, of the form F(Xout-Xin,Y#ut-Yin 
), the coefficients in an orthogonal transfer.look like F(XoutZ2+ that the rows (say) of the xi n ,Yout_Yin), 
reflecting outgoing array are parallel to the rows the incoming array, but the columns in this 3D situation 
are at right angles. In this example the Y direction can be done by an FFT convolution, but the transformation 
over the X coordinate must be done directly (unless I've missed something). For a face size o{ 1024 by 
1024 we must first do 1024 FFT convolutions for the Y direction (30 million multiplies) giving us 1024 
vectors each with 1024 elements. These vectors must then each be multiplied by 1024 coefficients and 
summed into the output array, for a total of over a thousand million multiplications, a number which 
dominates the FFT step. A cube has four such difficult faces, but the calculations can be shared between 
parallel pairs of outgoing faces and a 1024 on a side cube requires atotal of about two billion multiplications. 
A cube of side n needs about 2n 3 multiplies. Since the amount of work thus goes up approximately linearly 
in the volume, we gain little by subdivision, and the method is of limited interest unless a faster way 
of calculating the sidegoing wavefronts is found.  9. The Grand Synthesis Note that in the stack of 
planes approach, the wavefront propagation consists of a chain of linear transformations; a waveform 
is alternately convolved with a propagation array then multiplied by an array of transmission coefficients. 
Both these operations are special cases of a general linear transform which can be expressed as a huge 
array. If we treat our 1024 by 1024 wavefront arrays as a 10242 (call it a million) element vector (name 
it V), then a general linear transform is a million by million matrix (A) to multiply this vector by, 
plus a million element vector offset (B) to be added (V' = AV + B). A convolution is simply a special 
case of such a matrix A where every row is the same as the previous one except shifted right one place. 
The transmission coefficients can be represented in a matrix zero everywhere except on the main diagonal 
where the coefficients reside. Light sources are simply represented in the constant vector B.  Now a 
chain of linear operations can be condensed into a single one by multiplying the matrices and transforming 
and adding the vectors. This implies that a complex scene consisting of a stack of planes (or otherwise 
described) can in principle be multiplied out into a single matrix and offset. The effect on a wavefront 
of a scene so expressed can be determined by a single matrix multiply. The matrix may contain the effect 
of multiple passes through the scene, and thus provide the effect of high order light indirection in 
a single multiply. Multiplying a complex wavefront (representing external illumination) by such a matrix 
produces an image.bearing front which shows the scene encoded in the matrix freshly illuminated. Although 
manipulation of such matrices, with a trillion elements, is beyond the means of present day machinery, 
they may be practical someday when computers become another factor of one 2~4   
			