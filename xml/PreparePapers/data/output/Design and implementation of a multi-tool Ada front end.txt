
 DESIGN AND IMPLEMENTATION MULTI-TOOL ADA FRONT Rodney M. Bates Viswa Santhanam Donald E. Johnson The 
Boeing Company Defense and Space Group OF A END Military Ai@nes Division, Wichita Branch 3801 S. Ofiver 
Wichi@ 1 Introduction SAGA (Source Analyzer Generator for Ada) is a multi-pur­pose front end for Ada 
source code. We developed it in re­sponse to a regularly recurring need to provide Ada source code analysis 
and/or transformation tools. All such tools re­quire some kind of internal analysis of Ada code. Static 
analysis in Ada is much more diftictdt than inmost other languages, because of the presence of user-defined 
overloading, not only of subprograms, but also of pre­define operations. There are complex interdependencies 
among the various parts of static analysis. Identifier resolu­tion (binding each occurrence of an identifier 
to its declara­tion) depends on overload resolution, which requires complete type analysis, which depends 
in turn on identifier resolution. Many source code tools only need partird analysis. For ex­ample, across 
reference needs only identifier resolution. However, such a tool must nonetheless do a complete static 
analysis, as an intermediate result in computing the neces­sary partial information. For Ada, static 
analysis is a large and complex task. Past efforts to locate and adapt a front end from a compiler had 
keen frustrating. FirsL existing compiler front ends were designed only to support a particular code 
generator and were difficult to adapt to various source code analysis back ends. Second, Ada allows implementation 
dependen­cies in certain areas. The compiler front ends readily avail­able were all designed to make 
a single choice in each such area, whereas we had a clear requirement to support easy adaptation to many 
implementation-dependent dialects. Since we had felt the need for many Ada source code analy­sis and 
transformation tools, we decided to implement a sin­gle static analyzer or front-end which could support 
them au. @1991ACM 0-89791-445-7/91/1000-0016 $1.50 Kansas 67210 2 Description of SAGA SAGA is afront 
end which analyzes Ada somce code and stores the results in internal data structures which are acces­sible 
to various back ends. By itself, it is not a complete tool. It can be combined with one or more back 
ends to yield one or more source code analysis tools and/or transforma­tion tools. The SAGA system, depicted 
in figure 1, consists of a parser and an analyzer. The parser is responsible for converting source forms 
of Ada units into corresponding parsed forms. The analyzer then uses the parsed form files to compute 
stat­ic semantic information. The analyzer initially builds linked data structures in memo­XY,as its 
tuM@ti form of Ada units, These can then be written to external files for subsequent retrieval as often 
as desired. Later, the in-memory data structures can be recon­structed from those files, which avoids 
the need for repeat­ ing the time-consuming task of analyzing a large library of source code. It also 
provides one means by which trans­formed source code can be written out after a code transfor­mation 
tool has been run. Aback end is a set of driver modules the user writes on top of the analyzer modules. 
Its purpose is to receive the seman­tic information extracted by the analyzer and to use it to achieve 
its source code analysis/tmnsformation goal. 2.1 Scanning and Parsing SAGA uses a handwritten lexical 
scanner. We chose this ap­proach because such scanners are quite easy to write, and perform well in a 
place which is a traditional speed bottle­nwk. The scnnner is conventional, returning tokens to the parser 
when requested. The parser for SAGA is produced by a parser generator named Grace. Grace had been developed 
locally several years ago. It can generate either LL or LALR parsers. We used the LALR technique, because 
of its larger set of deter­ministically parsable grammars. The Ada language reference manual (LRM) grammar 
is full of ambiguities. Quite a bit of work is required to transform it into an LALR grammar. After a 
few attempts proved that Report or Transformed Source   I s;om: Ihlxed  I I I Iii User written back 
end PARSER ANALYZER 8 code piggy-backs on to ~ the SAGA Amlyzer Figure 1. SAGA System Overview the task 
would be more time-consuming than we had antici-There am actually two different parsed form formats, 
either pated, we took a public domain YACC grammar as the start-of which the parser can write. One is 
designed for compact­ing poin~ We made minor changes, primarily to provide ness of the file and contains 
minimum information neces­attachment points in the resulting trees for certain items of sary to rebuild 
the tree. The other is designed to be semantic information. In some instances we relaxed the examined 
by humans, and contains enough information to grammar to allow clean parsing even in the presence of 
easily show the structure of the tree to a reader. Both forms , certain errors so that the analyzer may 
report such errors. use only ASCII characters to permit easy porting of parsed Typically, the semantic 
analyzer is poised to issue more form files among dissimilar platforms. Furthermore, both meaningful 
error messagesthan the table-driven parser. formats include sufficient information to be able to recon­ 
struct the source form of the unit. The SAGA parser builds linked derivation trees in memory. We considered 
transforming to some kind of abstract syntax The parser also produces a conventional source listing for 
tree, but decided we could develop SAGA more quickly each unit, with lexical and syntactic error messagesand 
without this step. We have found several places where the numbered lines. shape of trees is inconvenient 
for semantic analysis. How­ 2.2 Semantic Analysisever, these have mostly to do with various pcxk%le semantic 
interpretations of a single syntactic construct. An abstract The analyzer traverses trees for compilation 
units, decorat­syntax tree would not have helped with these problems. ing them with semantic attributes. 
There is an elaborate col­ lection of semantic attribute kinds, and semantic nodes are SAGA uses breadth-first 
parsing. That is, all units in a li­ extensively interconnected with link pointers. We designed brary 
are parsed before any unit is analyzed. Pure parsing of the system from the outset to make the multing 
data stmc­a compilation unit is never dependent on any other unit, ture easily available to many back 
ends. which means that no compilation order is needed for the parsing step. A simple wild card command 
will parse all the The analyzer writes an analyzed form file for each compila­source files. tion unit 
This is analogous to the parsed form file, except it contains an external representation of the semantic 
data The parser writes a parsed form file for each Ada compila­ structure produced by the analyzer and 
the mapping of this tion unit, whose file name is a straightforward transforma­ data structure to tree 
nodes. Like the parsed form files, it is tion of its unit name, together with an indication of its unit 
encoded using ASCII characters, in the interest of easy inter­kind (specification, body, or subunit). 
Once amdysis of any system portability. This file requires what is known in pro­unit begins, the parsed 
form of any other unit it refers to can gramming language implementors circle as a *pickle stor­be found 
by a simple transformation of the unit s name. This age technique , since it represents linked data structure 
technique avoids the need for the traditional Ada library which is an arbitrary graph. system. In the 
following subsections, we address some of the more A parsed form file is an external repnxentation of 
the deri­challenging aspects of developing the analyzer. vation tree. The parser builds this tree as 
linked data struc­2.2.1 Identifier Viiibitity ture in memory and then writes it out. Later, the linked 
form can be rebuilt by reading the file. SAGA uses several kinds of hash tables to locate visible identified, 
along with some other associated analysis-time each with multiple selector meanings (overloaded entries). 
data structures. Identically spelled identifiers am converted to identical pointers during scanning, 
and thii transforma­tion is reconstructed when reading parsed form files. Since we want to preserve the 
identifier case, this transformation is case-sensitive. Each declarative region has an associated symbol 
table, which is a hash table, searched by case-insensitive comprui­sons, containing identifiers deciared, 
implicitly or explicitly, in the region. Character and string literals, which can desig­nate declarations 
just like identifiers also go in this table. Character Mends are treated as case-sensitive. Multiple 
dec­larations with the same simple name are all placed in the symbol table under the name, and lookups 
can return multi­ple meanings. Each declarative region has a profile table as well. The table contains 
entries for overloadable declarations in the region. The simple name and the parameter and result type 
profile are used in the hash code computation and for equaiity test­ing. Thus, overloaded but non-homographic 
declarations will have separate entries in the profile table, while homo­graphs will lead to a single 
entry. The table is used to detect declarations of overloadable homographs in the sad de­clarative region. 
The distinction between implicit and ex­plicit declarations must be taken into account in doing this. 
SAGA maintains two stacks of units during analysis. One is the context stack, which has one entry for 
each unit sur­rounding the current analysis point. l%e other is the USE stack, which has one entry for 
each active USE clause at the cument point of anaiysis. It behaves as a stack for purposes of changing 
its contents, but for purposes of looking up vis­ible identifiers, it behaves as a set, since all USEd 
scopes have equal status in Ada When a directly visible occurrence of a designator is pro­ cessed, SAGA 
goes through all the symbol tables in the context and USE stacks, collecting possibly visible mean­ ings. 
The rule that a non-overloadable meaning is a homo. graph of any meaning with the same designator is 
used to terminate the search through the context stack early, if pos­ sible. Searching through the USE 
stack must complete, since any occurrence of homographs in its diffesent levels hides them all. The meaning 
iist resulting fmm this search is then checked to eliminate overloadable homographs from different scopes. 
If the number of levels is small, a simple O(@) search is done. Otherwise, a form of profile hash table 
is uti for efficiency. This table is like the declarative region profile table, except that it is vertical 
(through the levels of context stack) rather than horizontal (through the dechwa­ tions of one declamtive 
region). For dotted references (expanded names), a single symbol ta­ble can be seamhed. The table to 
be searched is selected by the prefix, and the selector is the search key. In one case, there can be 
multiple prefix meanings (values of task types), 2.2Q Predefine Operators Redefined operators are defined 
for whole classes of param­eter and resuit type profiles. In simpler languages like Pas­cal, the operators 
are usually analyzed in a front end with a single programmed rule which handles all the profile combi­nations 
of the operator. In Ada, a predefine operator can be overridden by a programmer-supplied declaration. 
Howev­er, a single declaration can override only a single profile. The obvious way to analyze the predefincd 
operatms in Ada is to construct a sepamte analysis-time description of every distinct profile of each 
predefine operator, as if the prc­detined operators were programmer-defined. This technique makes selective 
overriding of operator/profile combinations work in a straigh~:tiard manner. We used this technique in 
SAGA. We also extended the technique to apply to certain pre­ destinedoperations which cannot be overridden 
by the pro­ grammer, as a means of simplifying the semantic analyzer itself. We implemented the operations 
AND THEN, and OR ELSE this manner. We also implemented <cl> IN <e>.. ee3> as a ternary operator, using 
this technique. These operations cannot be dcclamd by the programmer, and am recognized syntactically 
in somewhat different ways than ordinary operations. However, analysis-time descrip­tions for them are 
constructed as for all predefine opera­tions, and overload resolution uses these just as it would for 
ordinary optxations. Vkibility of these special operations do not follow the same roles as those applicable 
to ordinary op­erators. l%is fact necessitated a special procedure for find­ing the applicable profiles 
corresponding to each special operation. On the other hand, there remain two exceptions in Ada which 
do not a&#38;pt well to the technique of building explicit descriptions of each profile of an operator. 
These are the multiplying operators * and / , when applied to fixed point operands. For these, every 
combination of fixed point types is aliowable for the left and right operands. Such com­binations amount 
to a cartesian product of profiles which could easily become much too large to construct explicitly. 
Furthermore, the different fixed point types could be de­clared in different scopes, thus complicating 
the manage­ment of visibility of all these profiles. For this csse, we reverted to a technique commonly 
used in other languages. We wrote a specific rule which recognizes any legal combination of fixed point 
operands. If such a combination is found, a unique profile is constructed at the point of the invocation, 
and used to decorate the tree at that point. Additional specird case processing is needed for the case 
where more than one fixed type meaning exists for an operand. 22.3 Overload Resolution For overload resolution, 
we used the classical bottom-up then top-down two pass method. Most of the techniques we found in the 
published literature are variants of this basic idea. On the other hand, we encountered a number of derails 
not addressed in the published articles. The upward pass decorates tree nodes with lists of meanings 
which are consistent with the node itself and with the previ­ously constructed lists of meanings of its 
subtrees. When the top of a subtree for a complete context is reached, externally imposed restrictions 
on its meaning rtre applied to eliminate some of the bottom-up computed meanings, which should yield 
exactly one meaning at the top of the subtree. There can still be multiple interpretations of deeper 
sub­ trees. The meaning of the top node is propagated downward, eliminating meanings inconsistent with 
it. This process should give a single meaning at every node below. At selected points in the syntax, 
SAGA can do early top down processing on a subtree, if the bottom up information alone has been sufficient 
to reduce the number of possible interpretations to one. This feature is complemented by making ordinary 
top-down processing stop when it detects a subtree which has already been fully resolved. Together these 
features can substantially reduce second-pass tree tra­versal. There are several different situations 
in Ada with varying roles which partially restrict the bottom-up or top-down flow of information in overload 
resolution. Additional par­tial resolution information is also sometimes derived from syntax. For example, 
the prefix of an attribute or type con­version must be fully resolvable without regard to context. This 
is easily handled by simply treating such prefixes as topmost subtrees, even though they are inside other 
subtrees which require overload resolution. 2.2.4 Aggregates and Allocators Aggregates and allocators, 
in contras~ must be resolvable without regard to what is inside, although they can take into account 
proptxties of the construct itself. In these two cases, SAGA constructs a kind of temporary semantic 
attribute which denotes the allowable class of types. Ordinary aggre­gates, string literals (which area 
form of aggregate) and rd­locators all have different classes of allowable types. All upward-pass analysis 
routines recognize and handle these attributes, which amount to a kind of type expression, ap­propriately. 
Allocators are especially interesting. The type of an alloca­tor is restricted to the class of access 
types of the type being allocated, which is determined by the subtree of the aUoca­tor. Thus resolution 
of the allocator s type depends on infor­mation both above and below. However, it still requires a type 
expression. Furthermore, the subtree of the allocator does not depend for its resolution on the resolution 
of the al­locator itself. Thus the subtree must be resolvable without regard to context. 2.2S Implicit 
Conversions A particularly interesting case involves implicit type con­versions. The mle in the Ada LRM 
seems reasonable for simple cases,but has very strange consequences in some unusual cases.For each implicitly 
convertible operand, the decision to convert it is made independent of whether other such operands am 
converted. We have found no implemen­tation technique for this in the published literature. We developed 
a scheme which involves maintaining sets of potentird implicit conversions for each possible meaning 
of a whole complete context during the upward pass. At the top of a complete context tree, these sets 
are used to com­pute the minimum set of implicit conversions which should actually occur. This result 
is then used during the downward pass to eliminate interpretations and to trigger construction of actual 
implicit conversion operations. Our technique cor­rectly handles some pathological cases which are not 
han­dled correctly by the compiler we use. 2.2.6 Generics Generic program units are analyzed in essentially 
the same manner as their non-generic counterparts. During an instan­tiation, a map of generic formal 
symbols to the correspond­ing actual symbol is created. With this map in effect, the tree for the generic 
declaration is retraversed, as if the declara­tion occurred at the point of instantiation. Generic bodies 
are not revisited during instantiation. An interesting case arises with generic packages whose visi­ble 
part contains two or more procedwes (or functions) with the same name and parameter count. If their parameter 
pro­files include generic formal types, they could become ho­mographs during an instantiation. Such casescausethe 
nXraversal step to (erroneously) generate an illegal re­declaration message and, therefore, warrant special 
han­dling. 2.2.7 Separate Compilation Analysis of separately written subunits requires nxmuxruc­tion 
of the exact environment in which the corresponding stub declaration occurred. This environment is readily 
avail­able when the parent unit is being analyzed. Thus, one of the methods used for subunit analysis 
is to invoke the analy­sis procedure on the subunit when the corresponding stub is encountered during 
the analysis of the parent unit. However, it maybe desirable to analyze the subunit by it­self. The analyzer 
handles such a request by first requesting the analysis of the parent unit up to the point of the corre­sponding 
stub declaration. In this case, the nested call to analysis for the parent unit exits without desmoying 
the analysis environment so that subunit analysis may build on top of that environment. We prefer this 
technique over sav­ing art environment snapshot at each stub declaration be­cause it minimizes the demand 
on memory. 2.2.8 Pragmas and Attributes We separated analysis of non-built-in pragmas and at­tributes 
into distinct package bodies, because these are two areas in which the LRM leaves choices up to the implemen­tor. 
Our approach makes it relatively easy for a user of SAGA to make modifications to match the behavior 
of a particular compiler. So far, we have not implemented any of these. Our pseudo package STANDARD is 
written in what is al­most Ada . A few SAGA-specific pragmas, allowed only in this pseudo package, change 
the semantics slightly to allow declaration of such things as operations on universal types. Matching 
SAGA to a particular implementation of Ada in­volves modifying this package to reflect implementation 
de­pendencies in STANDARD, e.g. the correct set of predefine integer types, The mechanism for achieving 
this was com­plicated by the fact that there can also be an ordinary pack­age named STANDARD, which is 
not the STANDARD. 3 Experience with SAGA We have had versions of SAGA running for several months and 
have made two internal deliveries within The Boeing Company. Most of our recent effort has been devoted 
to documentation, testing, and adding deferred functions. We reclaim no-longer used heap space, which 
is essential for large programs (over 50,000 lines of source). A large amount of space is used to hold 
possible meanings of trees, most of which are eliminated by the end of analysis of each complete context. 
Furthermore, we did not initially imple­ment the reading and writing of analyzed form files, which meant 
that compacted forms of preanalyzed modules were not available. These limitations all contributed to 
the space­hungry character of the initial version, making storage rec­lamation that much more critical. 
With storage reclamation, we reclaim roughly thee fourths of the space allocated dur­ing analysis. We 
have also implemented two back ends. One is a compi­lation order tool. This relatively simple tool has 
been greatly desired by programmers in the organizations we supporg because many compiler library systems 
do a poor job of supporting the building of a library from a large collection of source code brought 
in from another system. These li­brary systems compute a compilation order only for units in the library, 
which can be put there only in compilation order. Unlike most back ends we envision, the SAGA compilation 
order tool does not require semantic analysis. The other back end is a multi-compilation-unit, semantic 
cross reference. This back end requires identifier resolu­tion, which implies overload resolution and 
full type analy­sis. We contirmed our expectation that SAGA would make the implementation of many back 
end tools quite easy. The user-written portion of the cross reference accounts for less than 4% of the 
source code. Another group within Boeing is also using SAGA in an elaborate code analysis system. This 
system has been work­ing for some time on several other languages. It constructs a large data base of 
facts about a body of code, which is then used to construct any of a wide variety of reports and to an­swer 
questions interactively. They have ported SAGA to an­other Ada compiler and are in the process of using 
SAGA to add A&#38;to its repertoire. This back end task involves tra­versing the trees and attributes 
generated by SAGA and us­ing the information contained therein to generate entries into their existing 
code analysis data base. 4 Validation Vali&#38;ting SAGA has been difiicult. When a tint end is part 
of a compiler, one can run the generated code to prove that it correctly handles a given test case. The 
ACVC (Ada Compilez Validation Capability) tests use this technique ex­tensively. Whh a front end alone, 
a complete check of a test case in­volves manual examination of the generated data structure. Dohtg this 
examination exhaustively is impractical. Never­theless, we have done a significant amount of this kind 
of verification of results. We have data structure dumpers which display the contents of all syntactic 
and semantic data structure in human readable format, so it is possible to check anything manually. The 
code for SAGA is liberally laced with runtime checked, explicitly programmed assertions, which have a 
reasonably good chance of catching bugs. All three of the principal im­plementors had had prior experience 
using this technique, with excellent results. Just verifying that SAGA can analyze a test case without 
any assertion failures and without generating error messages hasbeenhelpful. Inamodestsizedsampleoftestcasesfor 
which we have examined the results careful] y by han~ we have found that bugs in the semantic analyzer 
which did not generate either of these errors are relatively infrequent. They are not however, impossible, 
and we have found some cases which did analyze without these failtuvs, but which nevertheless had incorrectly 
decorated a tree. We have used much of the ACVC validation suite as test cas­es. We have also used SAGA 
s own source code as addition­al test input. Finally, we have a fair sized collection of locally written 
test cases. These are designed on the glass­box principle, i.e. they reflect the SAGA developers knowl­edge 
of its internal structure. We have found these to be fair­ly helpful in locating analysis bugs. They 
are few enough in number that it has been possible to manually examine the output of all of them, at 
least once. We constructed a regression test procedure, which we use with the glass-box tests, It saves 
the results of previous runs and compares them with the results of a newly run analysis, using the human 
readable output forms and a common dif­ference utility. So far, this has not been very useful. We con­ 
sistently find that the continuing development work on SAGA creates legitimate changes in outpu~ which 
are so voluminous that any changes due to real regressions would be lost among them. We hope that the 
technique will be­come more useful after the continuing development work on SAGA slows down. We have 
attempted to validate SAGA through another tech­nique. We have produced a cross reference generator as 
a back end. We attempted to correlate the output of this gener­ator with output from a distantly similar 
analysis tool which is part of an existing Ada compilation system. The output format and organization 
is quite diffenmt for the two tools. We wrote a pair of programs which transform the output of each tool 
into a common form. We then compare the results in this common form. Many analysis errors would show 
up as differences here. Usefulness of this technique has been lhnited by the fact that often there area 
large number of oc­ currences of differences which do not reflect bugs, but just legitimate differences 
in the information the two tools pr­ oduce. We have another validation technique planned. We plan to 
implement aback end which replaces every identifier occur­rence with a fully qualified name, qualified 
all the way back to the libmry unit name. We will compile the Ada code be­fore and after this transformation, 
using any convenient Ada compiler. Except for minor details such as compilation date, source file name, 
etc., both should give the same generated code. We continue to look for new, automatable validation tech­niques 
for SAGA. Of course, everyday use of the software will eventually flush out the majority of significant 
bugs in it. If SAGA is used as we expect, our users will have helped validate the system long before 
the formal techniques lead to validation. 5 Conclusion SAGA is a general-propose Ada tint end, for the 
support of multiple source code analysis and transformation tools, in use at Boeing. The results of its 
semantic analysis are avail­able to many back ends, One major analysis tool, a semantic cross reference, 
is working, and required minimal source code for its implementation. SAGA is a&#38;ptable to match the 
characteristics of different Ada compilers, with various implementationdefined properties. It has been 
successfully ported to one other Ada compiler. Biographies: Rodney M. Bates received a Ph.D. in Electrical 
Engineer­ing fmm Kansas State University in 1971. He has worked both in the software industry and in 
academia, teaching Computer Science. He specializes in compilers and pro­gramming languages and is a 
coauthor of a compiler text­book. He is currently a principal engineer on the technical staff of Boeing 
Military Airplanes, Wichita Branch, where he is presently working as a developer of the SAGA system. 
Viia Santhanam received a Ph.D. in Computer and Infor­mation Science from The Ohio State University in 
1975. Prior to joining Boeing in 1987, he was an Associate Profes­sor of Computer Science at The Wichita 
State University. His interests lie in programming language implementations and artificial intelligence 
for real-time applications. He is presently a senior principal engineer at Boeing, leading the SAGA development 
effort. Donald E. Johnson has a B.S. in Mathematics and an M.S. in Computer Science. He has sixteen years 
of experience h avionics embedded computer systems, four with General Dynamics working the FB-llA Opxational 
Flight Program and twelve with Boeing Military Airplanes working B-52 Offensive Avionics System, B-52 
Integrated Conventional Stores Management System, and the LHX Demonstration/ Validation programs. While 
at BMA, Mr. Johnson has spe­cialized in Operating Systems for these embedded, real-time systems. Mr. 
Johnson was one of the first BMA-Wichita Di­vision employees to receive Ada training and was a contrib 
utor to the Ada Information Management System (ALMS) contract Phase One Interim Technical Report. For 
the past year Mr. Johnson has been working on SAGA. 
			