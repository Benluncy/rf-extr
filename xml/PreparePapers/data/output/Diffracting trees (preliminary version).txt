
 Diffracting Trees (PRELIMINARY VERSION) Nir Shavit Asaph Zemach* Abstract Shared counters are among 
the most basic coordination struc­tures in multiprocessor computation, with applications rang­ing from 
barrier synchronization to dynamic load balanc­ing. Introduced in this paper are diffracting trees, novel 
distributed-parallel data structures for shared counting. Diff­racting trees combine a randomized coordination 
method together with a combinatorial data structure, to yeild a log­arithmic depth counter that improves 
on the log2 depth of counting networks, and overcomes the resiliency draw­backs of combining trees. Empirical 
evidence collected on a simulated distributed shared-memory multiprocessor shows that diffracting trees 
substantially outperform both combin­ing trees and counting networks, currently the most effective known 
methods for shared counting. Not only do diffracting trees have higher throughput and lower latency, 
but unlike any known technique, their latency remains almost constant as the number of processors increases. 
1 Introduction It is hard to imagine a program that doesn t count some­thing, and indeed, on multiprocessor 
machines shared coun­ters are the key to solving a variety of coordination prob­lems such as barrier 
synchronization [20], index distribution, shared program counters [21] and concurrent data structures 
(see also [12, 14, 25]). In its purest form, a counter is an ob­ject that holds an integer value and 
provides a jetch&#38;inc operation, incrementing the count er and returning its previ­ous value. Given 
that the majority of current multiprocessor designs do not provide specialized hardware support for ef­ficient 
counting, there is a growing need to develop effective software based counting techniques. The simplest 
way to implement a counter is to place it in a spin-lock protected critical section, adding an exponential­backoff 
mechanism [1, 4, 15] or a queue lock as devised by Department of Computer Science, School of Mathemat­ 
ics, Tel-Aviv Umversity, Tel-Aviv 69978, Israel. Contact: shanM!2math, tau.ac il This work was supported 
by a Dig]t]al Equipment Corporation ERP Equ]pment Grant Keywords: Shared Counters, Counting Networks, 
Load Balancing, Concurrent Data Structures, Randomization. Permission to copy without fee all or part 
of this material is granted provided that the copies are not made or distributed for direct commercial 
advantage, the ACM copyright notice and thu title of the publication and Its date appear, and notice 
is given that copying is by permission of the Association of Computing Machinery. To copy otherwise, 
or to republish, requires a fee and/or specific permission. S!%A 94-6/94 Cape May, N.J, USA 0 1994 ACM 
0-89791-671 -9/94/0006..$3.50 Mellor-Crummey and Scott [20] or Anderson [4] to reduce contention. Unfortunately, 
such cent ralized methods are in­herently non-parallel and cannot hope to scale well. A recent survey 
of counting techniques by Herlihy, Lim, and the present author [16] suggests that scalable counting can 
only be achieved by methods that are distributed and therefore have low contention on memory and interconnect, 
and are parallel, and thus allow many requests to be dealt with concurrently. The Software Combining 
Trees of Yew, Tzeng, and Lawrie [26] and Goodman, Vernon, and Woest [13], and the Counting Networks of 
Aspnes, Herlihy, and the present author [5], both meet the above criteria, and indeed were found by [16] 
to be the most effective methods for concurrent counting. A combining tree is a distributed data structure 
with a shared count er at its root. Processors combine their incre­ment requests going up the tree from 
the leaves to the root, eliminating the need for all to actually reach the counter. A Bitonic countinrz 
network is a distributed data structure having a layout is~morphic to a Bitonic sorting network [6] with 
a local counter at the end of each output wire. The network has width w << n and depth ~ log2 W.l Combining 
 trees have logarithmic depth and the J esirable property that the unavoidable collisions of processors 
at their nodes are utilized to increase parallelism, but int reduce high depen­dency among processes 
and cannot withstand even a single processor failure. Counting networks on the other hand, support complete 
independence among requesting processes and are highly fault tolerant, but have log2 depth and do not 
make use of the collisions at their nodes. This paper introduces diffracting trees, a new di.stributed­parallel 
technique for shared counting that enjoys the bene­fits of each of the above methods and avoids many 
of their drawbacks. In a manner similar to counting networks, diff­racting trees are constructed from 
simple one-input two­output computing elements called balatacers that are con­nected to one another by 
wires to form a balanced binary tree. Tokens arrive on the balancer s input wire at arbitrary times, 
and are output on its output wires. Intuitively one may think of a balancer as a toggle mechanism, that 
given a stream of input tokens, repeatedly sends one token to the left output wire and one to the right, 
effectively balancing the number of tokens that have been output. To illustrate this property, consider 
an execution in which tokens traverse 1~l german and plaxton [18] have designed elegant combinatOrlal 
constructions of counting networks with depth close to O(log w ), un­fortunately though, at this point 
in time the constants revolved are [exponentially large the tree sequentially, one completely after the 
other. Fig­ure I shows such an execution on a tree of width 4. As can be seen, the tree moves input tokens 
to output wires in increasing order modulo 4. Trees of balancers having this property can easily be adapted 
to count the total number of tokens that have entered the network. Counting is done by adding a local 
counter to each output wire i, so that tokens coming out of that wire are consecutively assigned numbers 
i,i+4, i +(4.2)... A clear advantage of a tree over a network is its depth which is logarithmic in w. 
However, it seems that we are back to square one since the root of the tree will be a hot­spot and a 
sequential bottleneck that is no better than a centralized counter implementation. This would indeed 
be true if one were to use the accepted counting network im­plementation of a balancer a bit toggled 
by each pass­ing token. We are able to overcome the rmoblem based on . the following simple observation: 
an even number of tokens passing through a balancer leave the toggle bit unchanged. This means that if 
one could have independent pairs of to­kens diffracted in a coordinated manner oneto the left and one 
to the right, they could leave the balancer without ever having to toggle the shared bit. The idea behind 
diflrac­tingtrees isto create such a prism mechanism, in front of the toggle bit of every balancer. By 
distributing the prism over many locations , and ensuring that each pair of to­kens uses a different 
location, wewould get a highly parallel balancer with very low contention. The diffraction mecha­nism 
uses randomization to ensure high collision/diffraction rates on the prism, and the tree structure guarantees 
cor­rectness of the output values. Diffracting trees thus combine the the high degree of parallelism 
and fault-tolerance of the counting networks with the logarithmic depth and beneficial utilization of 
collisions of a combinimz tree. We compared the performance of d%racting trees with the above techniques 
on a simulated distributed shared­memory multiprocessor using the well accepted Proteus Par­allel Hardware 
Simulator [8, 7]. We found that diffrac­ting trees substantially outperform both combining trees and 
counting networks, currently the most effective known methods for shared counting. Not only do they have 
higher throughput and lower latency, but in fact, their latency re­mains almost constant as the number 
of processors increases. Diffracting trees can also be used to create a highly par­allel centralized 
job queue implementation or as a general load balancing tool (dropping the counters at the end of the 
output lines). Unlike the distributed j ob queues of Rudolph, Slivkin, and Upfal [24] and the randomized 
log w smooth­ing networks of Aiello, Venkatesan, and Yung [3] which give only a probabilistic correctness 
guarantee that the load will be balanced, diffracting trees always correctly balance the number of tokens 
on their output lines. We believe diffrac­ting trees will prove to be an effective and useful technique 
in other application areas, and are currently testing a mes­sage passing version of the technique that 
can be used both on multiprocessors and computer networks. This preliminary version includes a description 
of count­ing trees, a shared memory implementation of diffracting balancers and an evaluation of their 
performance. The out­lines of the correctness proofs can be found in the appen­dices. yo= 15 Y1 =26 654321 
=X y2= 3 y3=4 Figure 1: A Simple Counting Tree 2 Trees that count We begin by introducing the abstract 
notion of a counting tree, a special form of the counting network data structures introduced in [5]. 
A counting tree balancer is a computing element with one input wire and two output wires. Tokens arrive 
on the balancer s input wire at arbitrary times, and are output on its output wires. Intuitively one 
may think of a balancer as a toggle mechanism, that given a stream of input tokens, repeatedly sends 
one token to the left output wire and one to the right, effectively balancing the number output on each 
wire. We denote by z the number of input tokens ever received on the balancer s input wire, and by vi, 
i c {O, 1} the number of tokens ever output on its ith output wire. Given any finite number of input 
tokens z, it is guaranteed that within a finite amount of time, the balancer will reach a quiescent state, 
that is, one in which the sets of input and output tokens are the same. In any quiescent state, yO = 
[z/21 and VI = [z/2]. We will abuse this notation and use yi both as the name of the ith output wire 
and as the count of the number of tokens output on the wire. A balancing tree of width w is a binary 
tree of balancers, where output wires of one are connected to input wires of another, having one designated 
root input wire and w des­ignated output wires: yo, VI, ... VW I. Formal definitions of the properties 
of balancing networks can be found in [5]. On a shared memory multiprocessor, one can implement a bal­ancing 
tree as a shared data structure, where balancers are records, and wires are pointers from one record 
to another. Each of the machine s asynchronous processors can run a program that repeatedly traverses 
the data structure from the root input pointer to some output pointer, each time shepherding a new token 
through the network. We extend the notion of quiescence to trees in the natural way, and define a counting 
tree of width w as a balancing tree whose outputs y., ... y~ -1 satisfy the following step property In 
any quiescent state, O ~ Vi yj s 1 for any i<j. To illustrate this property, consider an execution 
in which tokens traverse the tree sequentially, one completely after the other. Figure 1 shows such an 
execution on a BINARY[4] counting tree which we define formally below. As can be seen, the net work moves 
input tokens to output wires in increasing order modulo w. Balancing trees having this property are called 
counting trees because they can em+ fly be adapted to count the total number of tokens that have entered 
the network. Counting is done by adding a local counter to each output wire Z, so that tokens com­ ing 
out of that wire are consecutively assigned numbers i,t.+w, . . ..i + (Y, 1) W. Code for implementing 
such a counter can be found in Figure 2. In ciur implementation we will be using a counting tree called 
BIIYARYIUI], which we define below. Let w be a power of two, and let us define the counting tree BINARY[2k] 
induc­ tively. WheR k is equal to 1, the BINARY[2k] network con­ sists of a single balancer with output 
wires gO and yl. For k >1, we const~ct the BINARY[2k] tree from two BrINARY[k] trees and one additional 
balancer. We make the input wire z of the single balancer the root of the tree and connect each of its 
output wires to the input wire of a tree of width k. We then redesignate output wires yO, y], . . . . 
y~-1 of the tree extending from the O output wire as the even output wires yo, Y2, . . . . V2K2 of BINARY12k] 
and the wires YO, Y1, ..., y~ 1 of the tree extending from the balancer s 1 output wire as the odd output 
~~res yl, ys, . . . . yM-I. Theorem A.6 in AP­ pendix A proves that BINARY[2k] is indeed a counting tree. 
type balancer { lock : boolean toggle: boolean next: array [0. . i] of ptir to balancer } constants width: 
global integer root : global ptr to root of Binary [width] tred function typical-balancer(b: ptr to balancer) 
: ptr to balancer lock(b->lock) i := b->toggle b->toggle := not(i) unlock (b->lock) return b->next [i] 
function fetch&#38; incro : integer b := root while not leaf(b) b := balance endrrhile i : = increment-counter-at-leaf 
(b) return i * width + b->number Figure 2: A Shared-Memory tree-based counter implemen­ t ation 3 Diffraction 
Balancing Diffracting trees are counting trees whose balancers are of aL novel type called diffracting 
balancers. In the typical imple­ment ation of balancers (as in Figure 2), each processor shep­herding 
a token through the tree toggles the bit inside the balancer, and accordingly decides on which wire to 
exit. If many tokens attempt to pass through the same balancer con­currently, the toggle bit quickly 
becomes a hot-spot. Even if one applies contention reduction techniques such as expo­nential backoff, 
the toggle bit still forms a sequential bot ­tleneck. One can overcome this sequential bottleneck basecl 
on the following observation: If an even number of tokens pass through a bal­ aracer, they are evenly 
balanced left and right, vet the value of the toggle bit is unchanged. If we could find a method that 
allows pairs of colhding to­kens to pair-off and coordinate among themselves which is diffracted right 
and which diffracted Ieft , they could both leave the balancer without either of them ever havina to 
touch the toggle bit. By performing the colJ.ision/coor­ dination decisions in separate locations instead 
of a global toggle bit, we will hopefully increase parallelism and lower cent ention. However, we must 
guarantee that many such collisions occur, not an obvious task given the aeynchrony in the system. On 
a high level, our implementation of the above is baaed on adding a special prism array in front of the 
toggle blt in every balancer. When a token (processor) P enters the balancer, it first selects a location, 
L, in prism uniformly at random. P tries to collide with the previous processor to select L, or, by waiting 
for a fixed time, with the next processor to do so. If a collision occurs, both processors leave the 
balancer on separate wires without ever attempting to toggle the bit. Figure 3 gives the diffracting 
balancer data structure and contains the code for this type of brdancer. Three synchro­nization operations 
are used in the implementation code: register.tonemory ~wap (addr, val) writes val to ad­dress addr, 
and returns the previous value there,  compare and~wap ( addr, old, new) checks if t he value at address 
addr is equal to old, and if eo, replaces it with new, returning TRUE, otherwise it returns FALSE, and 
 Q test -and~et ( addr ) writes TRUE to address addr and returns the previous value. All three primitives 
can be implemented in a lock-free man­ner using the fashionable load-linkecl/store-conditional op­erations 
available on standard architectures [10, 19]. The code also uses two functions: (a) rsndom(i, j) re­turns 
a random number between i and j; (b) not-empty(i) returns TRUE if i is the PID of some processor and 
FALSE otherwise. The code translates into the following sequence of oper­ations performed by a process 
shepherding a token through a balancer. In Phase 1 of the code the processor announces its arrival at 
the balancerj by writing to the global location array. It then swaps its own PID for the one written 
in a randomly chosen location in the prism array. Assuming it has read the PID of an existing processor, 
it attempts to collide wit h it. The collision itself is accomplished by per­forming two compare-and-swap 
operations. The first erases this processor from the list of processors waiting at this bal­ancer (thus 
assuring no other processor will collide with it), the second erases the other processor, completing 
the diff­raction, and allowing the process to be diffracted to the b-ynext [01 balancer. If the first 
compare-and-swap fails, it means that some other processor hae already managed to collide with it, and 
the processor is diffracted to the b->next [I] balancer. If the first succeeds but the second compare-and-swap 
fails, it means that the processor with whom it waa trying to collide is no longer available, in which 
case it goes on to phase 2. In Phase 2 the processor repeatedly checks to see if it has been diffracted 
by another processor. After spinning spin type. . balancer { size: integer spin: integer prism: array 
[1. . size] of integer lock : boolean toggle: boolean next: array [0. . i] of ptr to balancer } location: 
global array [1. . NJHPROCS] of ptr to balancer function diff-bal(b: ptr to balancer) : ptr to balancer 
/* phase i */ location [mypid] := b place := random(i ,b-%ize) him := register. to-memory -swap (b->prism[place] 
,mypid) if not.empty(him) then if compare-and-swap (location [mypid] ,b ,E!lPTY) then if compare-and-swap 
(locat ion [himl ,b ,EHPTY) then return b->next [01 (a) .= b else location [mypid] else return b->next 
[1] (b) endif /* phase 2 */ forever repeat b->spin times if location [mypid] <> b then return b->next 
[i] (b) endrepeat if test -and-set(b->lock) then if compare. and-swap (location [mypid] ,b ,EHPTY) then 
i := b->toggle b->toggle := not(i) unlock (b->lock) return b->next [i] (c) else unlock (b->lock) return 
b->next [1] (b) end if endif end for Figure 3: Code for traversing a diffracting balancer times, giving 
some other processor a chance to diffract it, the processor attempts to get the toggle bit. If successful, 
it first removes itself from the list of waiting processors and then toggles the bit and exits the balancer. 
If it could not remove itself from the list, it follows that some other proces­sor already collided wit 
h it, and it exits the balancer, being diffracted to b->next [i]. If the toggle bit could not be seized, 
the process resumes spinning. Appendix B containe the formal correctness proof for this algorithm. 3.1 
Some implementation details When a large number of processors concurrently enter the balancer, the chances 
for successful collisions in prism are high, and contention on the toggle bit is unlikely. When there 
are few processors, each will spin a short while, reach for the toggle bit and be off, since all spinning 
is done on a cached copy of the value of 10 c at ion [mypidl it incurs no overhead. The only case where 
a processor ie repeatedly makinrz accesses to memorv. is when no other mocessor can­cels (b; diffractimz) 
it. an: it is const antlv re~chirw for the lock ~~ the toggl~ bit This becomes inc reasingly urdikely 
as more processors enter the balancer. Two parameters are of critical importance to the performance of 
the diffracting balancer: 1 size This value effects the chances of a successful pairing-off. If it is 
too high, then processors will tend to miss each other, failing to pair-off and causing con­tention on 
the toggle bit. If it is too low, contention will occur on the array prism as too many processors will 
be trying to access it at the same time. 1~. spin If this value is too low, processors will not h&#38;e 
a chance to pair-off, and cont&#38;tion will occur on the toggle bit. If it is too high, processors wiU 
tend to wait for a long time, even though the toggle bit may be free, causing a degradation in performance. 
 The choice of these parameters is obviously architecture de­pendent. In our simulations we used size 
= 8,4,2,1,1 for the various levels of a width 32 tree. We also employed a form of exponential backoff 
on the balancer s toggle bit. Each processor kept a local copy of the diffracting balancer s spin variable, 
and doubled it each time it could not seize the lock, thus increasing the amount of time it waited to 
be collided with. The value of the local copy was not re­tained between calls. In order to maximize the 
distribution of the balancer s data structure the prism array was actually an array of pointers to different 
modules of memory. The random number function we used was Proteus fast~andom which is an implementation 
of the ACM Minimal Standard Random Number Generator [22, 9]. 4 Performance We evaluated the performance 
of counting trees relative to other known methods by running a collection of benchmarks on a simulated 
distributed shared-memory multiprocessor similar to the MIT A ~ewife machine [2]. Our simulations were 
performed using Proteusz, a multiprocessor simulator developed by Brewer, Dellarocas, Colbrook and Weihl 
[8]. In this abstract we present the results of running a bench­mark called Index-Dist r~but ion. Index-distribution, 
is a load Version 300, dated February 18, 1993, balancing technique, in which processors dynamically 
choose loop iterations to execute in parallel. As mentioned in [16], a simple example of index distribution 
is the problem of rendering the M andelbrot Set. Each loop iteration covers a rectangle in the screen. 
Because rectangles are independent of one another, they can be rendered in parallel, but because some 
rectangles take unpredictably longer than others, dy­namic load-balancing is important for performance. 
Here is the pseudo-code for this benchmark: Procedure index-dist-bench( uork: integer) loop: i := get-next-indexo 
repeat random(O,work) times /*nothing *I endrepeat goto loop In our benchmark, after each index is delivered 
proces­sors pause for a random amount of time, between O and work. When work is chosen aa O, this benchmark 
actually becomes the well known counting benchmark, where proces­sors attempt to load a shared counter 
to full capacity. We ran the benchmark several times, varying the num­ber of processors participating 
in the simulation, and the amount of work done. Each time we measured: Latency The average amount oftime 
between the moment getnextindex wasca.lled, and the timeit returned with a new index. This was measured 
by taking the time at the beginning ofeach call, the timeat the end, and adding the difference to a global 
counter, which was then divided by the number of increments per­formed. Throughput Theaverage number 
ofindices distributedin a one million cycle period. This cycle count includes the time spent spinning 
in the work loop. It was mea­sured by marking the time after the first 100 incre­ments where performed, 
and then measuring, t, the time it took tomaked more increments. The through­put is: 106d/t. We compared 
a collection of the fastest known software counting techniques with respect to the index-distribution 
benchmark. ExpBackoff A lock using Z est6YTest&#38;Set with exponential backoff [4, 15]. MCS The MCS 
lock of [20]. Each processor locks the shared counter, increments it, and then unlocks it. The code was 
taken directly from the article, and imple­ment ed using atomic Swap and Compare&#38;Swap oper­ations. 
CTree Fetch fJInc using an optimal depth combining tree. We implemented the software combining tree proto­col 
of Goodman et al. [13], modified according to [16], OptimaJ depth means that when p processors partici­pate 
in the simulation, a tree of ,depth [log2 PI is used CNet A BITONIC counting network of width 32 [16]. 
The toggle bit was implemented using a short critical sec.. tion. DTree A Diffracting Tree of width 32. 
Throughput with work=O :­03 70000 ./ 2 ,./ u i ,/. 5 ./ 60000 ./. ./. 50000 4 } ~ 40000 A X­,ih + 30000 
.----A~ ..--­ ,./ /.- 20000 ,* - J ,,. ~. .+ ,* -+ 10000 ./ -+ / **-9.*+-Q-*.-----.-* . . . . . . . 
. . . . . ..g 0 50 100 150 200 250 300 Processors Latencv with work=O 12000 I { x 1 I 1 CNet[321 1 + 
CTree -+-. ExpBackof f .B. - 10000 / MCS 1 ock DTree[321 -W_­~.­ 13 ,.­ .. . [ ... 8000 ~ ,,. ,.. ,.. 
,.. ./ ,./ + ..­ ,.. . ..- ..­,.*.%R. : ---­/ #; 6000 , /+.: /f/+ ;/ 4000 // ~.i ,. ;; ;+ m ,,, ,hA-&#38;<&#38;.A-A.*.-.-._. 
-.-.-.-.-.-. -.-.4 :/:2000 w ,/ d o 50 100 150 200 250 300 Processors Figure 4: Throughput and Latency 
of Major Counting Tech­niques Throughput with work=1000 50000 11111 CNet[32] e CTree -+- P DTree[32] 
-B-. 45000 ,,, / #,,40000 ,.$ ,.. ,,, 35000 ,., ,.$ ,,; 30000 ,,, ,,, ,,, 25000 .,: ,ti 20000 ,d /0 
15000 J?( 10000 5000 o~ o 50 100 150 200 250 300 Processors Latency with work.1000 10000 I 1111 CNet[32] 
4-CTree -+-.-+ #----­ 9000 DTree [321 .i5.. ----­~+.+ -----­ 8000 ;,/ 7000 6000 5000 4000 3000 2000 + 
1000 1I1,[ 0 50 100 150 200 250 300 Processors Figure5: Throughput and Latency with work=1000 The graphs 
in Figure 4 show the latency and through­put of the various counting methods. They confirm previ­ous 
findings and in particular, they agree with the results of [16] on AS.fM, the Aiewi~e machine simulator 
[2]. We have not included results of various optimizations such as build­ing counting networks from balancers 
with four or more in­put /output wires. Elsewhere [11], and in our simulations, these have been shown 
to give performance improvements of at most 25~o, when work=O and almost no improvement as work increases. 
It is clear from these graphs that the MCS lock and the exponential backoff lock do not scale well, la­tency 
grows quickly, and throughput diminishes. This is not surprising, since both are methods for eliminating 
con­tention but do not support parallelism. We therefore concentrate on the latency and throughput results 
of the three distributed-parallel techniques: combin­ing trees, bitonic counting networks and diffracting 
trees. The graphs in Figures 4 and 5 show that diffracting trees give consistently better throughput 
than the other meth­ods and that in terms of latency they scale extremely well, tending to hand out numbers 
in almost constant time (on average). Ratio of Diffraction Per Toggle 25 r , I , # work. O, top balancer 
+ work=O, level 2 balancers -+-. work.looo, top balancer -E!-­ work= 1000, level 2 balancers ..x ..... 
20 P ,D ... / ,., . ,.. ,,. + .. /-..- . ... /­ .$ti; ::.= 5 *:+F  ,,#fm# ,497.x ~ /(,,, ~.x 
0 0 50 100 150 200 250 300 Processors Figure 6: Diffraction Rate The high throughput is explained by 
the parallelism, due to the optimized rate of successful collisions in the prism ar­rays of balancers. 
we claim that while processors that failed to combine in a combining tree must waste cycles waiting for 
earlier processors to ascend the tree, processors in a diffrac­ting tree proceed in an almost uninterrupted 
manner. The justification of this claim is provided by the graph in Fig­ure 6 which shows the diffracting 
rate of tokens in the top two balancers. The diffracting rate is the number of diff­racted processors, 
leaving the balancer without toggling the bit, divided by the number of those that did toggle. For example, 
when the diffraction rate is 20, it means that for ever y token toggling the bit, 20 will be diffracted. 
If we mea­sure a diffraction rate of r at some balancer, we conclude, that if till now ttokens have entered 
it, then the total num­ber of tokens toggling the bit, r, is given by &#38; and the tot al number diffracted 
is ~. The diffraction graph indi­cates a linear a relationship of the form r = cp, where p is the number 
of processors participating in the simulations, and c is some constant. We can now restate the previous 
formula, and say that r x ~. Our measurements indi­cate this predictor to be accurate to within 15% when 
the number of processors is above 64. During sufficiently short time periods, t is bounded by p, since 
only p tokens can exists simultaneously, and we get ~ < ~ < ~. The num­ ber of processors reaching for 
the toggle bit simultaneously (the contention on it) is thus bounded by the constant ~ (We measured this 
constant at about 8.5 for the top-level balancer, when work was O). The diffracting baJancer thus tends 
to keep the contention at the toggle bit constant, re­gardless of the total number of processors in the 
system, an indication of the robustness of the construction. There is an interesting resemblance between 
the shape of the throughput graphs of the combining tree and the diffracting tree. This is probably the 
result of both trees having a similar coordination property: combining collided requests in one and successful 
diffracting of collided tokens in the other. As shown above, as the number of processes in the tree increases, 
the likelihood of such collisions happen­ing grows. Counting networks do not gain a performance advantage 
(and even loose performance) from this phenom­ena, since they are designed only to minimize the number 
of collisions on individual toggle variables, not to take advan­tage of them. When processors fail to 
meet at the nodes, performance of both the tree methods diminishes, but like counting networks, diffracting 
trees still have an advantage given the low contention on the toggle variables. The low latency of diffracting 
trees is due in part to their low depth. While combining trees have a depth of log n where n is the number 
of processes, and counting networks have a fixed depth of 1/2 log2 w where w is the width of the net 
work, diffracting trees have a depth of only log w. For ex­ample, with 256 processors, the combining 
tree rises to depth 8, the width 32 counting networks have depth 10, whereas diffracting trees have depth 
of only 5. The low latency of diffracting trees remains almost constant as the number of processes increases, 
since the high rate of successful colli­sions works in their favor by lowering contention and thus ensuring 
that most processes take order of log w steps. The graphs in Figure 5 show that, as the amount of work 
between accesses to the shared counter increases, the throughput of all methods decreases, as would be 
expected. The latency of the counting network remains unchanged, that of the combining tree initially 
grows rapidly and then seems to begin evening out. The latency of the diffracting trees evens out much 
sooner, and even slightly diminishes as the number of processors increases. We can conclude that diffracting 
trees maintain their superiority even in the face of substantial work loads. When the number of pro­cessors 
is small (less than 64), the latency of all methods, including our own, is greater than that of the exponential 
backoff lock. We are currently studying adaptive versions of the algorithm, (able to shrink or grow in 
order accommodate different numbers of processors) that promise to address this difficulty. In summary, 
diffracting trees enjoy both the parallelism of counting networks and the high coordination of combining 
trees. One must also remember that like counting networks and unlike combining trees, diffracting trees 
can be made lock-free, that is, guarantee progress even if processors fail. 5 Conclusions Current approaches 
to multiprocessor computing are geared towards providing performance speedup for input/output problems 
(as in numericaJ computing) that are solved us­ing parallelized sequential algorithms. This, in our view, 
is the reason why one can program distributed and asyn­chronous multiprocessor machines using tight synchroniza­tion 
paradigms like critical sections and barriers, and still observe good performance. However, future roles 
of multi­processor machines will include application areas in indus­trial production control, aircraft 
flight control, and eventu­ally even robot brains. These will require general purpose multitasking capabilities 
and on-line control of high speed asynchronous data arriving from multiple sources, and are in­herently 
asynchronous and distributed in nature. They will gain more from methods that promote coordination between 
processors without actuaJly synchronizing them. Combining Trees are an example of a data structure that 
achieves parallelism by synchronizing multiple accesses to a single location. Diffracting Trees, on the 
other hand, achieve parallelism by distributing requests to different locations, as­suring correctness 
through coordination. They are an exam­ple of a novel Distributed-Coordinated (DisCo if you will) ap 
preach to concurrent data structure design. We believe this novel approach (which includes lock-free 
and wait-free meth­ods, but does not prohibit use of locks) will better bridge the gap between the applications 
and the machines which run them. References [1] A. Agarwal and M. Cherian. Adaptive Backoff Synchro­nization 
Techniques. In Proceedings of the 16th Inter­national Symposium on Computer Architecture, June 1989. 
[2] A. Agarwal et al. The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. In Proceed­ings 
of Workshop on Scalable Shared Memory Multi­processors. Kluwer Academic Publishers, 1991. An ex­tended 
version of this paper has been submitted for publication, and appears as MIT/LCS Memo TM-454, 1991. [3] 
B. Aiello, R. Venkatesan and M. Yung. OptimaJ Depth Counting Networks. personal communication. [4] T. 
E. Anderson. The Performance of Spin Lock Alterna­tives for Shared-Memory Multiprocessors. IEEE Trans­actions 
on Parallel and Distributed Systems, 1(1):6 16, January 1990. [5] J. Aspnes, M. P. Herlihy, and N. Shavit. 
Counting Net­works and Multi-Processor Coordination. In Proceed­ings of the 23rd Annual Symposium on 
Theory of Com­puting, May 1991. [6] K.E. Batcher. Sorting Networks and their Applications. In Proceedings 
of AFIPS Joint Computer Conference, pages 338 334, 1968. [7] E.A. Brewer, C.N. DelJarocas. PROTEUS User 
Docu­rnentation. MIT, 545 Technology Square, Cambridge, MA 02139, 0.5 edition, December 1992. [8] E. 
A. Brewer, C. N. Dellarocas, A. Colbrook and W.E. Weihl. PROTEUS: A High-Performance Parallel-Architecture 
Simulator. MIT Technical Report /MIT/LCS/TR-561, September 1991. [9] D. G. Carta Two Fast Implementations 
of the Minimal Standard Random Number Generator. CA CM,33(1), January 1990. [10] Digital Equipment Corporation. 
Alpha system refer­ence manual. [11] E. W. Felten, A. LaMarca, R. Ladner Building Counting Networks from 
Larger Balancers University of Wash­ington T.R. #93-04-09 [12] E. Freudenthal and A. Gottlieb. Process 
Coordination with Fetch-and-Increment. In Proceedings of the ~th International Conference on Architecture 
Support for Programming Languages and Operating Systems, April 1991, Santa Clara, California. To appear. 
[13] J.R. Goodman, M.K. Vernon, and P.J. Woest. Effi­cient Synchronization Primitives for Large-Scale 
Cache-Coherent multiprocessors. In Proceedings of the %d AS-PLOS, pages 64-75. ACM, April 1989. [14] 
A. Gottlieb, B.D. Lubachevsky, and L. Rudolph. Basic techniques for the efficient coordination of very 
large numbers of cooperating sequential processors. ACM Transact~ons on Programming Languages and Systems, 
5(2):164 189, April 1983. [15] G. Graunke and S. Thakkar. Synchronization Al­gorithms for Shared-Memory 
Multiprocessors. IEEE Computer, 23(6):60-70, June 1990. [16] M. Herlihy, B.H. Lim and N. Shavit. Low 
Contention Load Balancing on Large Scale Multiprocessors. Pro­ceedings of the %d Annual ASM Symposium 
on Paral­lel Algorithms and Architectures, July 1992, San Diego, CA. Full version available as a DEC 
TR. [17] M.P. Herlihy. Wait-Free Synchronization. ACM Transactions on Programming Languages and Systems, 
13(1):123 149, January 1991. [18] M. Klugerman and C.G. Plaxton. Small-depth Count­ing Networks. 1992 
ACM Symposium on the Theory of Computing. [19] MIPS Computer Company. The MIPS RISC Architec­ture. [20] 
J.M. Mellor-Crummey and M. L., Scott. Algorithms for Scalable Synchronization on Shared-Memory Multipro­cessors. 
Technical Report 342, University of Rochester, Rochester, NY 14627, April 1990. [21] J.M. Mellor-Crummey 
and T.J. LeBlanc. A software instruction counter. In Proceedings of the %d ACM International Conference 
On Architectural Support for Programming Languages and Operating Systems, pages 78-86, April 1989. [22] 
S.K. Park and K.W. Miller. Random number genera­tors: Good ones are hard to find. CA CM, 31(10 ),0cto­ber 
1988. [23] G.H. Pfister and A. Norton. Hot Spot contention and combining in multistage interconnection 
networks. IEEE Transactions on Computers, C-34(11):933-938, November 1985. [24] L. Rudolph, M. Slivkin, 
and E. Upfal. A Simple Load BaJancing Scheme for Task Allocation in Parallel Ma­chines. In Proceedings 
of the 3rd ACM Symposium on Parallel Algor~thms and Architectures, pages 237-245, July 1991. [25] H.S. 
Stone. Database applications of the fetch-and­add instruction. IEEE Transactions on Computers, C­33(7):604-612, 
July 1984. [26] P.C Yew, N.F. Tzeng, and D.H, Lawrie. Distribut­ing Hot-Spot Addressing in Large-Scale 
Multiproces­sors. IEEE Transactions on Computers, pages 388 395, April 1987. A A proof that counting-trees 
count Lemma A.4 letxo, xl, . . ..xn andyo, yl, . . ..yn be two ar-- Following [5], let the state of a 
balancer at a given time be defined as the collection of tokens on its input and output wires. For the 
sake of clarity we will assume that tokens are all distinct. We can now formally state the properties 
of a balancer: safety In any state x ~ y. + YI. (i.e. a balancer never creates output tokens). liveness 
Given any finite number of input tokens m = z to the balancer, it is guaranteed that within a finite 
amount of time, it will reach a quiescent state, that is, one in which the sets of input and output tokens 
are the same. balancing In any quiescent state, yO = (rn/21 and y] = \m/2]. As described earlier, a Counttng 
Tree of width w is a binary tree of balancers, where output wires are connected to input wires, having 
one designated root input wire, z, (which are not connected to output wires of balances), and w designated 
output wires VO)VI).., VW I (similarly uncon­ nected). Let the state of the tree at a given time be defined 
as the union of the states of all its component balancers. The safety and liveness of the tree follow 
naturally from the above tree definition and the properties of balancers, namely, that it is always the 
case that z > ~~=~] y,, and for any fi­nite sequence of m input tokens, within finite time the tree reaches 
a quiescent state, i.e. one in which ~~=~1 y, = m. It is important to note that we make no assumptions 
about the timing of token transitions from balancer to balancer in the tree the tree s behavior is completely 
asynchronous. We will show that if a counting tree reaches a quiescent state, then its outputs, gO, . 
. . . yW_l have the stop property. We present the following useful lemmas due to [5]. Lemma A.1 If yo, 
. . ., YW 1 is a sequence of non-negative integer-s, the following statements are equivalent: l. Foranyi 
<j, O~y, -y, sl. Lemma A.2 Let ZO, . . . , Zk-1 and Ye,..., yk 1 be arbitrary sequences having the step 
property. If k 1 k l X =xy ,=0 ,=0 then x, = y,for all O~ i< k. Lemma A.3 Let XO, . . . . Xk_l andyo, 
. . . . yb_l be arbitrary sequences having the step property. If k 1 k-1 D= D+ 1=0 1=0 bdrary sequences 
having the step propery. Then if nn ,=0 ,=0 then the sequence xo, vo, ~1, Yl, . . ..~n. Yn has the step 
property. Proof outline: There are two cases: 1. EV=O Yt = ~~=o x,, in this case, by Lemma A.2 , both 
sequences are identical, and the proof is trivial. 2. m = z~=o Y, = ~~=o z,+ 1, in this case, Lemma A.3, 
applies. We know from Lemma A.1 that x, = f-l and Y,= ~~1, this means that Vi,x, = y,+,. The joint sequence 
haa the form Z. = yO= xl = yl = ... = Zj-] = yj-1 =$j=yj l=rJ+l l= yj+] l,...,= Zn 1, yn 1. This sequence 
haa the step property. Theorem A.5 The outputs of BINARYIW] have the step prop­erty in any quiescent 
9tate. Proof outline: The proof is by induction. If w = 1 then we are dealing with a BINARY[2] counting 
tree. This tree has two outputs, and is therefore, simply, a balancer. By definition, the outputs of 
a balancer have the step property. Assume the theorem holds for all trees of width w ~ k, and let us 
prove that it holds for w = 2k. According to the construction given in section 2, the big tree of width 
2k, is actually one root balancer whose two outputs are connected to small trees of width k. The even 
leaves of the the big tree are the leaves of the left small tree, and the odd leaves, are the leaves 
of the right small tree. Since the trees are connected by a balancer, we know that the the number of 
inputs to the left and right small trees differ by at most one. By Lemma A.4, the outputs of BINARY[2k] 
have the step property. Building on the work of [5] the following theorem is now immediate. Theorem A.6 
A BINARYIW] tree counts. B A proof of the diffracting balancer implementation This section outlines the 
proof that the shared memory im­plementation of a diffracting balancer is indeed a balancer, i.e., itconforms 
to the formal definition of a balancer given in Appendix A. The proof makes the implicit assumption that 
all threads run to completion. For brevity, we will be using Ct4S instead of compareand~wap. Lemma B.1 
The implementation meets the safety condi­tion. then there exists a uniquej, O ~ j < k, such that Xj 
= yj+l, Proof out line: In our shared memory implementation,andx, =y, fori#j, O<i< k. each token represents 
a thread on some processor. Since the code contains no thread creation commands, no new tokenUsing the 
above we can show that: can be created in the diffractingbalancer. Lemma B.2 The implementation meets 
the liveness condi­ hon. Proof outline: A single processor accessing the balancer will by the code return 
within a finite number of steps. If there are several processes executing the balancer code and no processor 
performs a return then some process must be repeatedly executing in the forever loop. By the code this 
means it must be failing to obtain the lock via the test-and_set operation. This in turn implies that 
the lock must be taken by some other processor, which will within a bounded number of operations return 
a value. For any number m tokens passing through the balancer, it follows that eventually all of them 
must exit the loop. In order to prove the balancing property we will require some definitions. Note that 
Figure 3 marks each return point with a letter. A token exiting the diffracting balancer code via the 
return marked (a) will be called a canceling token. A token that leaves through a return marked (b) will 
be called a cancelled token, and one that leaves through (c) will be called a toggling token. By definition, 
any two tokens passing through the tree concurrently are being shepherded by processes with different 
PID. For a token t,PID(t), will denote its PID. Lemma B.3 At ang time that locati,on[PID(t)] = b there 
M a process PID executing the code of (shepherding a token t through) diffracting balancer b that, has 
not performed its final operation on location [PID(t)]. Proof outline: Initially all location [PID(t)] 
locations are empty, so this property holds. Assume that it holds in some state S and let ns prove that 
it holds in any state S reach­able from S following some operation. The only operation that can set a 
location [PID(t)] to lJ is by process PID and in its next step it is still a executing the code of balancer 
b. All return events are conditional on a final operation that either tests that location [mypid] is 
not b or explicitly sets itto EMPTY. Lemma B.4 The number of canceiled tokens is equal to the number 
of canceling tokens. Proof outline: To show this, we have to show that with each token that exited the 
diffracting balancer code at (a) we can associate a unique token that exited at a point marked (b). We 
first show that for each canceling token, {, there is a unique cancelled token t.If ~ is canceling, then 
it must have succeeded in both of its OtYS operations. This means that it has written EMPTY to both location 
[PID(@ and location [PID(t)], where t is some other token whose PID is held in / s him variable. By lemma 
B. 3, when { succeeded in its C&#38;S(locat ion [P ID(t)] ,b, EMPTY), process PID(t) had yet to perform 
it final access to the location. This final ac­cess can be either: 1. A read, that will find that location 
[PID(t)] <> b; or 2. C&#38; S(location[PID(t)] ,b,EMPTY), that will fail.  In both cases t is a cancelled 
token. Token t is unique to i since it only reads the array prism once. We now show that for each cancelled 
token t there ex­ists a unique canceling token ~. Once again we notice that t,upon entering the diffracting 
balancer code executes the statement location [P ID(t)l : =b at most twice. A successful C&#38;S(locat 
ion [PID(t)] ,b, EMPTY) by titself, must precede the second such write. Thus, since all erasing of b 
from location [PID(t)] is done using a C&#38;S operation only one token ~ can succeed. Successful writing 
into another process location implies by the code that ~ returns as a canceling token. We can now complete 
the proof of Theorem B,5, Theorem B.5 The diffracting blancer code of figure .9 im­plements a balancer. 
Proof outline: Lemmas B. 1 and B.2 prove token safety and liveness. We now assume that the diffracting 
balancer is in a quiescent state and proove that yO = [m/21 and Y1 = [m/2j. Each token exiting the diffracting 
balancer code must be either cancelled, canceling or toggling. Each cancelled token increments VI by 
one, and each canceling token increments y. by one. Lemma B.4 allows us to ignore these tokens, and count 
only the toggling tokens which by the proofs of [5] for a toggle based balancer are properly balanced, 
9 
			