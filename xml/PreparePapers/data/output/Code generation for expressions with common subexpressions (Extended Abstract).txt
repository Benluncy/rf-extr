
 Code Generation for Expressions with. Common Subexpressions EXTENDED ABSTRA CT A. V. Aho and S. C. Johnson 
Bell Laboratories, Murray Hill, New Jersey 07974 J. D. Ullman* Princeton University Princeton, New Jersey 
08540 1. Introduction Easy as the task may seem, many compilers generate rather inefficient code. Some 
of the difficulty of generating good code may arise from the lack of realistic models for programming 
language and machine semantics. In this paper we show that the computational complexity of gen- erating 
efficient code in realistic situations may also be a major cause of difficulty in the design of good 
compilers. We consider the problem of generat- ing optimal code for a set of expressions. If the set 
of expressions has no common sub- expressions, then a number of efficient op- timal code generation algorithms 
are known for wide classes of machines [SU, AJ, BL]. In the presence of common subex-pressions, however, 
Bruno and Sethi have shown that the problem of producing op- timal code for a set of expre~ssions is 
NP- complete, even on a single register machine [BS, S1]. However, Bruno and Sethi's proof of NP-completeness 
uses rather complex expressions, so it leaves some hope of being able to find efficient algorithms for 
generat- ing optimal code for restricted classes of ex- pressions with common subexpressions. Unfortunately, 
we show in this paper that the problem of optimal code generation * Work partially supported by NSF grant 
DCR74-15255. remains NP-complete even for expressions in which no shared term is a subexpression of any 
other shared term. We also show that the optimal code generation problem is NP-complete for these expressions 
on two-address machines, even when the number of registers is unlimited. Faced with these negative results, 
we consider both heuristic and exact solutions for generating code. First, we investigate the worst case 
performance of a collection of fast heuristics for single and multiregister machines. One seemingly reasonable 
heuristic is shown to produce code that is in the worst case three times as long as op-timal; other heuristics 
are given which have a worst case of 3/2 for one-register machines. Then, we present for the single regis- 
ter machine an algorithm which generates optimal code and whose time complexity is linear in the size 
of an expression and ex-ponential only in the amount of sharing. Since the number of common subexpres-sions 
in expressions tends to be limited in practical situations, this approach appears at- tractive. Finally, 
after discussing code gen- eration for commutative machines, we con- clude with a list of open problems. 
2. Background and Definitions 2.1 Dags For the purposes of this paper we can assume we have a compiler 
of the form .~ Front'~ Dag ~ Code End I Ma~ !Generator[ The front end translates a source program into 
a sequence of straight-line intermediate code segments, called basic blocks, along with information that 
identifies the flow of control among basic blocks. Within a basic block the flow of control is sequential. 
Each basic block consists of a se-quence of assignment statements of the form a~-bopc where a, b, and 
c are distinct variables and op is any binary operator for which there exists a corresponding machine 
operation. Since we shall concentrate on generating code for basic blocks in this paper, the flow of 
control information will not be men-tioned further. The dag maker transforms the basic blocks into a 
directed acyclic graph (dag, for short) that represents the computations of the expressions in the block. 
(See [AU] or [CS] for algorithms to construct a dag from a basic block.) We shall not consider the use 
of algebraic identities to transform dags to make them easier to compute; this has been discussed in 
[Brl, [Fa]. Fig. 1 shows a basic block and its corresponding dag. We call node 2 a right child of node 
3, and a left child of node 4. For symmetry, we call node 4 a right parent of node 2, and node 3 a/eft 
parent of node 2. We say node x uses a node y if y is ei- ther a left or right child of x. We say x left 
uses (resp. right uses) y if y is a left (resp. right) child of x. u 1 *'--c --d u2*-b +u 1 u3 "-a * 
u 2 U4 ~-'U2 * U 1 U 5 ~ U 3 + U 4 Basic Block Dag Fig. 1 Basic Block and its Dag A node with no children 
is called a leaf. A node with no parents is called a root. Nodes that are not leaves are often referred 
to as interior nodes. We assume for simplicity that all operations are binary. We ignore con-straints 
that may be introduced into the dag because of side effects. For example, sup- pose two nodes of a dag 
represent the operation of indirect assignment through a pointer. If the two pointers could point to 
the same datum, and the source code specified an order for the two assignments, then an edge in the dag 
connecting the two nodes in the proper order must be intro-duced. 2.2 The Machine Model We assume the 
code generator is to produce code for a multiregister two-address machine. The instructions of the machine 
are of the form (1) r~ .-- r i op ~/ /* op-instruction */ (2) r i "-r i op m /* op-instruction */ (3) 
r~ .-- r/ /* register copy */ (4) r i ~-m /* load */  (5) m~---~ /* store */ Here r i and ยง. are any 
of N >/ 1 registers, and m is any memory location; op stands for any machine operation. When a machine 
has only one register (i.e., N = 1), then there are only type (2), type (4), and type (5) instructions. 
A machine program is a sequence of in- structions. The length of a program is the number of instructions 
it contains. Definition: The optimal code generation prob- lem (OCG) is to produce from a dag a shor- 
test machine program that evaluates and stores all roots of the dag. When discussing single and multireg-ister 
machines, we assume the leaves of the dag are labeled by memory locations and the interior nodes (non-leaves) 
by machine operations. We also assume for conveni-ence that a dag has no root that is a leaf. Example 
1: The dag of Figure 1 can be evaluated on a two-register machine and stored in memory location m by 
the se-quence of instructions: r 1 ~c /*loadc*/ r I ,--- r 1 -d /* evaluate node 1 */ r 2 "-- b /* load 
b */ r 2 ,--- r 2 + r 1 /* evaluate 2 */ t ~----r 1 /* store 1 */ r t ~--- a /* load a */ r I ,-- r 1 
* r 2 /* evaluate 4 */ r 2 ~ r 2 * t /* evaluate 3 */ r I ~--- r 1 + r 2 /* evaluate 5 */ m ~ q /* store 
root */ [] In Section 6, we shall discuss code 3. Why OCG is Hard Bruno and Sethi have shown that OCG 
is NP-complete even for one-register machines. Their proof technique was to po- lynomially transform 
the satisfiability prob- lem with three literals per clause (see [AHU], e.g.) to OCG. Their technique, 
however, resulted in rather complex dags. We begin by showing that OCG is NP-complete for one-register 
machines even on a rather simple class of dags. A node both of whose children are leaves is called a 
level-one node. A shared node in a dag is a node with more than one parent. A level-one dag is a dag 
in which every shared node is a level-one node. A leafdag is a dag in which every shared node is a leaf 
[C]. Several code generation algorithms for one-register machines make use of the no-tion of a left chain, 
that is, a sequence of in- terior nodes n t , n 2 ..... n k such that n i is the left child of hi+ 1 
for 1 ~< i < k. For example, in the dag of Figure 1, 3-5 and 2-4 are the only nontrivial left chains. 
The first (lowest) node on a left chain is called its tail, and the last (highest) node its head. Definition: 
The feedback node set problem (FNS) is: Given a directed graph G, find a smallest set of nodes F (a feedback 
node set) such that removing F from G eliminates all cycles. FNS is a well-known NP-eomplete problem 
(see [AHU], e.g.). Theorem 1: OCG for level-one dags on a one-register machine is NP-complete. Proof." 
We show how to polynornially transform an instance of FNS to OCG. Let generation for commutative machines; 
i.e., machines in which for every type (1) and (2) op-instruction above, thege is also an in- struction 
of the form (1') rj "- ~ op ri (2') r i .-'- m op r i Even for noncommutative machines, OCG is a very 
difficult problem; the next section discusses why. G be the directed graph in the feedback node set problem. 
From G construct a dag D as follows. For each node n in G of out- degree d ~> 0 create a corresponding 
left chain of d+ 1 interior nodes n 0, n I ..... n d in D; no is the tail and n d the head of the chain. 
Make no a level-one node by giving it left and right children labeled by memory locations. The remaining 
edges of D are deter-mined as follows. Suppose the out-edges of n are directed to nodes m I , m 2 ..... 
m d in G. Make the tails of the left chains 21 corresponding to m I , m 2 ..... m d right children of 
tl I , I12 ..... rid, respectively, in D. Fig. 2(a) shows a directed graph G and Fig. 2(b) the dag resulting 
from G using the construction above. (a) Directed Graph G 2, (b) Resulting Dag D Fig. 2 Graph and Corresponding 
Dag We must now show that we can con-struct a minimal feedback node set F for G from an optimal program 
P for D and con-versely. It can be shown that an optimal program for a one-register machine does not 
store and load any uniquely left-used interi- or nodes of a dag (interior nodes with exact- ly one left 
use and no right uses). Thus ex-cept for the loads of leaves, the only loads in P are loads of some level-one 
nodes. It can be seen that these level-one nodes identify a feedback node set F in G. Con-versely, given 
a minimal feedback node set F of G we can construct an optimal program for D by first evaluating and 
storing the tails in D of the left chains corresponding to the nodes in F. For example, {d} is a minimal 
feed-back node set for the directed graph G of Fig. 2(a). The optimal program P corresponding to this 
feedback node set first computes node d o of D in Fig. 2(b). Then P evaluates c 0, c I , b 0, b I , b 
2, a o, a I , d I . To evaluate d 1, P needs to load do; this is the only level-one node loaded by P. 
[] To appreciate the difficulty of generat- ing optimal code for dags, let us assume we are generating 
code for an infinite register machine, a machine in which the number of registers is unbounded. To eliminate 
the problem of deciding what to store or load, let us further assume the leaves of the dag are labeled 
by register names rather than by memory locations; similarly, let us assume we need not store the roots. 
The relevant instructions of the infinite register machine then become ri ,--- ri op ri ri ,--- fi Even 
in this highly simplified environment, the optimal code generation problem is NP-complete. Theorem 2: 
OCG for leaf dags on an infinite register machine is NP-complete. Prool? Similar to that of Theorem 1. 
[] Thus, even if the problems of code selection and storage of intermediate values are made trivial, 
just finding an optimal evaluation order for the nodes of a dag is an NP-complete problem. On the other 
hand, if we perturb the infinite register machine architecture by permitting arbitrary three-address 
instructions of the form r i ~---- rj op r k, then we can generate optimal code for arbitrary dags in 
linear time. We simply evaluate the dag bottom up, level by level, assigning a distinct register to each 
node. The three main problems in code gen- eration are what instructions to use, in what order to do 
the computations, and what values to keep in registers. The results of this section indicate that, for 
two-address machines, just deciding the order in which instructions are to be executed is an NP-complete 
problem. 22  4. Heuristic Techniques Since even simple versions of the op- timal code generation problem 
are NP-complete, it is not surprising that in the past code generation algorithms for dags usually have 
made several restricting assumptions. One approach has been to ignore sharing by representing a set of 
expressions as a forest of trees. For this case a number of optimal code generation algorithms have been 
developed [SU], [A J], [BL], [Was]. Another approach has been to avoid the problem of finding an optimal 
evaluation order by tak- ing some fixed order for the nodes of a dag and then concentrating on optimal 
utiliza- tion of registers ([Bea], [HKMW], [WJWHG], [Fr], e.g.). Optimal code cannot be guaranteed, however, 
without consider- ing sharing and retaining the freedom to reorder code that is inherent in the source 
program. When faced with an NP-complete problem, there are two standard approaches: (1) develop and analyze 
heuristics, and (2) look for useful special cases that  have polynomial time algorithms. We shall 
consider both approaches here. For the analysis of heuristics we use the worst case measure, a time-honored 
way of measuring the goodness of a heuristic which may be applied to various populations of data. For 
our purposes, we define the worst case of an algorithm to be the supremum over all dags of the ratio 
of the length of the code produced by that algorithm to the length of the optimal code for the dag. 4.1 
Two Methods of Accounting for Costs There is a usual way .of charging the interior nodes of a dag for 
the cost of their evaluation. Charge a node one unit of cost for each of: (1) performing its operation, 
(2) storing its value, and (3) loading its left child or copying its left child from another register. 
We call a program semi-intelligent if (1) it performs no useless instructions, i.e., instructions which 
can be deleted without changing the value of the program, (2) it never moves (via loads or register-to-register 
transfers) a value into a register without subsequently left using that value, and (3) it never stores 
the same value more than once.  It is easy to check that a semi-intelligent program will have each of 
its in- structions assigned to some one node by the above scheme. Formally, we may show the following. 
Theorem 3: Let P* be an optimal program for some dag D, and let P be any semi-intelligent program for 
D. Then the ratio of the length of P to that of P* is at most 3. Proof." Using the above costing scheme, 
every interior node of D is assigned at least one instruction, but never more than three instructions 
(a load, an operation, and a store). [] Obviously there are an infinite number of programs to evaluate 
any dag. We shall restrict ourselves to semi-intelligent programs. Doing so serves only to rule out blatantly 
inetiicient programs. There is little loss of generality since we can construct from any program P an 
equivalent semi-intelligent program in time proportion- al to the length of P. There is a second cost 
accounting scheme which we find quite useful. This scheme gives the same overall cost as the scheme above 
but the cost units are appor- tioned differently amongst the nodes. We now charge each node for every 
instruction that affects its value, i. e., for the operation that computes its value, and any loads, 
stores, or register-to-register copies of that value. We call this cost accounting scheme the use-cost. 
If n is a node of a dag, let i(n) and r(n) be the number of times n is used as a left and right child, 
respectively, of some other node. In the dag of Figure 1, for ex- ample, /(1) --0 and r(1) z2. Also, 
/(2) --r(2) = 1. Lemma 1: The following costs are upper and lower bounds on the use-cost of a node n 
with respect to any semi-intelligent program on a one-register machine. 23 case lower upper (1) n is 
a leaf /(n) /(n) n is not a leaf and: (2) /(n) = 0 2 2 (3) /(n) > 1 /(n)+l /(n)+2  (4) /(n) = 1 1 3 
and r(n) = 0 (5) /(17) = 1 2 3 and r(n) > 0 Prool? Let us consider case (3) as an exam-ple. Surely an 
op-instruction evaluating n is necessary. Since n is used more than once and there is only one register, 
one store in-struction is also necessary. Every time n is left used, its value in the register is des- 
troyed, and it. must be reloaded the next time it is left used. Thus, /(n) -1 load in- structions are 
necessary, and I(n) load in-structions are sufficient by condition (2) in the definition of semi-intelligent. 
There-fore, a total of /(n)+ 1 or /(n)+2 instruc-tions is needed. [] Lemma 2: For a multiregister machine, 
Lem- ma 1 holds with the lower bound set to 1 in cases (2) and (5) and to/(n) in case (3). Prooj? Omitted. 
[] We observe from Lemma 1 that it is only interior nodes with one left parent and no right parent that 
could give us a worst case ratio of 3 for the one-register machine. Interestingly, it is quite easy to 
handle such cases. If node n is uniquely left-used, we can alwaysarrange to have n evaluated im-mediately 
before its parent. Thus it is un-necessary to load or store n, and it achieves a use-cost of 1. Therefore, 
we can state the following. Theorem 4: Any. algorithm for a one-register machine which generates semi-intelligent 
programs that avoid storing uniquely left-used nodes has a worst case no greater than 3/2. Note that 
Theorem 4 fails to hold in the multiregister case, since /(n) = 1, r(n) > 0 is another case that can 
yield a worst case ratio of 3, and some other cases yield a ratio of 2. We must also point out that the 
use-cost can, in some cases, underestimate the cost of an optimal program on a one-register machine by 
a factor of 3/2. Fig. 3, for ex-ample, shows a dag whose lower bound use-cost is 6+2p. This lower bound, 
how-ever, is not achievable since any program evaluating this dag must store and subse-quently load each 
of the +-nodes. The cost of an optimal program for the dag is 6 + 3p. 'p nodes Fig. 3 Underestimated 
Dag Thus, if we found an algorithm with a worst case ratio less than 3/2, the proof of that worst case 
bound must use a more so-phisticated cost analysis than the above.  4.2 Heuristics for One Register 
Machines An evaluation order for a dag is any to- pological sort of the interior nodes of the dag. For 
a one-register machine, from an evaluation order we can easily construct a program that is as short as 
any other which computes the dag in that order. Thus an al-gorithm that produces an evaluation order 
for a dag is in effect a code generation algo- rithm for a single register machine. In this section we 
analyze the performance of some 24 simple heuristics for creating evaluation ord- ers for single register 
machines. Definition: The Bottom-up Greedy Algorithm (BUG) creates an evaluation order for a dag by repeatedly 
listing the nodes of a longest left chain that can be currently evaluated. To evaluate a node, both its 
children must have been previously evaluated. Note that it is permissible for the right child of a node 
in a chain to be an unevaluated node which is lower on the same chain. Example 3." Return to the dag 
of Figure 1. The only left chain that carl be listed initial- ly is 1. Then the chain 2-4 can be listed, 
and finally the chain 3-5, giving an evalua-tion order of 12435. In this case the code is: r I *--c r 
1 ~-- r I --d t *"-r 1 r 1 *---b r I *---r 1 + t u *--'r 1 r I *"- r I * t v *---r 1 r I ,---a r I *--- 
r I * u r I ~ r 1 -I- v m ,---r I By Lemma 1, this program is optimal. [] Theorem 5." BUG has a worst 
case ratio of 3. Proof" Figure 4 shows how the worst case of 3 can be approached arbitrarily closely. 
At the right we show p nodes c 1, c 2 ..... Cp, called "controllers," whose initial evaluation is necessary 
for optimal code. If we evalu-ate the controllers first and store them, at a cost of 3p, we can go up 
each of the p left chains with a cost of p + 2 per chain, for a total of p 2 + 5p. However, BUG c~3uld 
select c 1, then the bottom node of each chain, then c 2, then the next node of each chain, then c 3, 
and so on, using three instructions per inte- rior node. The worst case ratio for this ex-ample is3(p 
+ 1)/(p + 5). [] p Fig. 4 BUG Buggerer Definition: The Top-down Greedy Algorithm (TDG) ([AU], p. 866; 
see also [Wai]) works by listing left chains in the reverse order of their evaluation. Repeatedly select 
a node n all of whose parents, if any, have already been listed. Then list n and as many nodes of the 
left chain with head n as may be list- ed. Note that a node may be listed only if all of its parents 
have already been listed, as we are generating the evaluation sequence in reverse. Also, once an unlistable 
node is encountered, we do not proceed further down the chain. After sequencing all interi- or nodes, 
reverse the list to get the evalua- tion order. Example 4: In the dag of Figure 1, we would select the 
root 5 first and find we may list it and its left child 3. Then we could select 4, since its parent, 
5, has already been listed. We could proceed to 2, the left child of 4, since its parents, namely 3 and 
4, have been listed. Finally we list 1. Reversing the list gives 12435 again, so the same code as for 
BUG is produced in this case. [] TDG and BUG, however, are quite different in their worst case performance. 
TDG produces optimal code for Fig. 2 for which BUG produced the worst case code. 25 Theorem 6: TDG has 
a worst case ratio of 3/2. Proof." To see that it is no worse than 3/2, note that if n is uniquely left-used 
by m, then n must be listed immediately after m, and therefore n is evaluated immediately be- fore m. 
Thus case (4) of Lemma 1 is always handled correctly; the lower bound of one instruction charged to n 
is attained. All oth- er cases in Lemma 1 have a ratio of 3/2 or less. For the proof that 3/2 can be 
ap-proached from below, consider the grid of Figure 5. The optimal sequence of evalua- tion goes up (the 
slanted) left chains, start- ing at the bottom right, storing each value with the exception of those 
on the leftmost chain. Roughly two instructions per node are used in this evaluation sequence. On the 
other hand, TDG could list nodes row by row, from the right, taking three instruc- tions per node. [] 
Fig. 5 Grid It is also .worth noting that TDG al-ways handles the case r(n) -~ O, I(n) > 1 correctly, 
since n will be listed immediately after the last of its left parents to be listed. Thus the generated 
code will have only I(n) --1 loads of n. In truth there is no magic about "top down" vs. "bottom up" 
algorithms; by ex- ercising care in the selection of chains it is easy to construct a modification of 
BUG that has the same performance as TDG. Another 3/2 worst case algorithm can be obtained using depth 
first search. Definition: The Depth-first Search Algorithm (DFS) performs a depth-first search (see [AHU], 
e.g.) of the rag, preferring to move to the right child rather than the left when there is a choice. 
Nodes are then evaluated in order of last visit. Theorem 7: The worst case ratio for DFS is 3/2. 4.3 
Heuristics for Multlregister Machines The Top-down Greedy Algorithm can be generalized to the case of 
an N register machine. For a multiregister machine, how- ever, it is no longer sufficient to specify 
only an evaluation order; we must also specify in which register a computation is to be done. The following 
procedure lists the interior nodes of a dag in reverse evaluation order. The register assigned to a node 
is the regis- ter in which that node is to be computed. Stores and loads of registers are performed as 
needed. procedure TDG(n, k);/* n is a node, k is the number of registers available */ if k >/ 1 and n 
is an interior node all of whose parents have been listed then begin list n and assign it register k; 
TDG(right child of n, k-1); TDG(left child of n, k)  end; /* main program */ while not all interior 
nodes have been listed do select an interior node n, all of whose parents have been listed, and perform 
TDG(n, N) Although TDG performs well in many cases, the worst case performance of TDG approaches 3 as 
Ngets large. Theorem 8: The worst case ratio for the TDG Algorithm with N registers is no less than 3N/(N 
+ 1). 26 ProoJ? The grid of Figure 5 provides the essence of the proof. [] Demers [D] has considered 
a generali- zation of DFS to multiregister machines in which a depth-first search is used to obtain an 
ordering of the nodes, and then Beladay's algorithm [Bel] is used to allocate registers. 5. An Optimal 
Algorithm for the One Regis- ter Machine We define s(n), the sharing of a node n in a dag, to be s(n) 
= /(n) + 1 ill(n) > 1 s(n) =2 if/(n) = 1 and r(n) > 0 s(n) = 0 otherwise The sharing, s, for a dag is 
the sum of the s(n) over all interior nodes n in the dag. We shall now present an algorithm for the one 
register case that is optimal and is of time complexity O(p2S), where p is the number of nodes in the 
dag and s is the amount of sharing. To introduce this algo- rithm, we note that any dag can be parti-tioned 
in various ways into left chains. Given a program for a dag, we can create a partition by looking for 
maximal sequences of one or more consecutive operations unin- terrupted by loads. Each such consecutive 
sequence forms a left chain, and the set of left chains so formed partitions the interior nodes of a 
dag. We can obtain a partial converse of the above. We say that a partition of the interior vertices 
of a dag D is legal if the fol- lowing holds: Form a graph G whose nodes correspond to the left chains 
of D and with an edge from c I to c 2 whenever there is a path in D from some node of c l to some node 
of c 2. Then there must be no cycles in G. We may thus state the following. Lemma 3: There is an evaluation 
order pro-ducing a given partition of a dag into left chains if and only if that partition is legal. 
More importantly, we can relate the cost of evaluation sequences to the costs of the heads of the left 
chains. Theorem 9: Let D be a dag. Then there is a constant c o with the following property: Suppose 
P is any semi-intelligent program evaluating D. Let the partition induced by P have k I chains whose 
heads are left used at least once. Let k 2 of these be uniquely left-used. Then the length of P is c 
D + k 1 + k 2. Conversely, if there is a program for D of cost c, then we can find a legal partition 
into left chains with parame-ters k I and k 2 such that c = c D + k 1 + k 2 . ProoJ? c D is the sum over 
all interior nodes of D of the lower bound on cost given in Lemma 1. k 1 accounts for excess loads of 
left-used nodes, that is, the /th load of a node left used / times. Note that a node will be loaded / 
rather than /-1 times if and only if it is the head ofachain, k 2 ac- counts for stores of uniquely left-used 
nodes. [] We intend to reduce the problem of finding an optimal program to that of finding a set of heads 
of chains. Clearly any node with /(n) =0 must be the head of a chain. A node with r(n) = 0 and /(n) = 
1 can always be attached to its left parent in a chain, as claimed in the follow- ing lemma. Lemma 4." 
If zr is a legal partition of dag D, and there is a uniquely left-used node n which heads a chain, then 
the partition formed from zr by removing n from its current chain and attaching it to the chain of its 
parent is also legal. Prooj? If there is a path to n in D, it must go through the unique parent of n. 
[] We thus see that the question of whether or not a node is the head of a chain in an optimal program 
is only un-resolved for those interior nodes with more than one parent, at least one of which is a left 
parent. We may thus try all subsets of these nodes, selecting those which are not the head of a chain. 
For each selected node with more than one left parent, we must also select the left parent to whose chain 
it is attached. The number of these selections can be shown not tO exceed 2 s, where s is the sharing. 
We may thus test each selec-tion, in order of lowest cost, until we find one that is legal. The test 
for legality is seen to be O(p) on a p node dag. Thus: Theorem 10: There is an O(p2 s) algorithm for 
obtaining an optimal program for a dag. 6. Commutative Machines A commutative machine is one in which 
for each instruction of the form r i ~-r i op r/ or r i ~--r iop m there is also an instruction of the 
form r i `-- ri op r i or r i `-- m op r r That is, the register used to hold the result can be the same 
as as either the left or the right register operand. This arrangement al- lows us to think of the order 
of the children of any node as being permutable, as far as code generation is concerned, although the 
operator itself may not be commutative. For one-register commutative machines the analog of a left-chain 
is a worm. A worm is any path in a dag, exclud- ing the leaves. We define a partition of a dag D into 
worms to be legal if the graph whose nodes are the worms of D, with an edge from w I to w 2 if and only 
if some node of worm w 1 has a path in Dtosome node of worm w 2, is acyclic. The following is an analog 
to Theorem 9. Theorem 11: Let D be a dag. Then there is a constant c D with the following property. Suppose 
D has a legal partition into worms such that there are k I worms whose heads are used (either left-or 
right-used), k 2 of which are uniquely used. Then there is a program for D on a commutative machine of 
cost c D -~ k I + k 2. Conversely, if there is a program for D with cost c, then we can find a legal 
worm partition with parameters k I and k 2 as above, such that c = c D + k 1 + k 2 . We can generalize 
the top-down greedy algorithm to commutative machines by listing worms in reverse evaluation order. If 
we always list a uniquely used child im-mediately after its parent, then we can show that 3/2 is a worst 
case ratio for this algo-rithm for the commutative one-register machine. We can also produce an analog 
of the optimal algorithm of Section 5 which is po- lynomial in the size of the dag and ex-ponential only 
in the sharing. The follow-ing lemma is needed; it is not as strong a result as could be proved about 
worm parti-tions. Lemma 5: Let W be a legal worm partition of a dag. Suppose node n I is the head of 
a worm w = n l, n 2 ..... n k in which n 1, n 2 ..... n k are all uniquely used. Then there is another 
worm partition W' of D with a worm of which n 1, n 2 ..... n k is a proper tail (that is, the new worm 
includes at least the parent of n 1) such that the cost of the program induced by W' is no greater than 
the cost of the program induced by W. ProoJ? Fig. 6 shows a fragment of a dag in which node m is on some 
worm; perhaps n, m's other child, is on the same worm. Fig. 6 Worm Construction Remove edge m--. n from 
that worm if it is there, and add the edge m---,n I. This may make node n a head, but n I will no longer 
be a head. It is easy to show that the cost of the partition is not increased, and the fact that n 1 
, n 2 ..... n k are unique- ly used makes a proof of legality for the new partition easy. [] Theorem 
12: There is an O(n6 s) algorithm for finding an optimal program on a com-mutative machine for an n-node 
dag with sharing s. Prooj? The first step is to take the dag D and divide it into trees, an idea discussed 
by [Wai]. For each root or shared node n, we do the following. Find the maximal subtree with n as root 
which includes no other shared nodes, except as leaves. An example 28 is shown in Fig. 7. (a) Dag (b) 
Trees  Fig. 7 Division of a Dag~into Trees Note that each multiply-used node ap- pears as a leaf in 
at most one tree per use. If it is used by two nodes iri the same tree, divide the shared node into as 
many leaves as necessary so that no leaf is used more than once. Note also that no non-shared node appears 
in more than one tree. The algorithm begins by determining which edges of tile various trees belong to 
worms. Later we shall combine the trees to allow worms to cross tree boundaries. Ima-gine worms proceeding 
upward from all leaves in a given subtree. The fact that worms do not actually reach the leaves can be 
accounted for later by removing edges from worms at the bottom. At each interior node n we must decide 
which, if any, of the worms reaching children of n include the node n. If either or both of the children 
of n are part of worms that begin at tree leaves of D or interior nodes of D that are not shared (recall 
some leaves of a tree here may actually be shared nodes of D), then Lemma 5 assures us that we may allow 
any such worm to continue up to n. The only uncertainty occurs when both worms reach-ing the children 
of n originate at shared nodes of D. Then we must try all three possibilities -that either worm or neither 
incorporates n. In the last case, n begins a new worm, a situation which could be necessary to achieve 
a legal worm partition. We see from the above that the only worms where there is uncertainty regarding 
how far up to proceed are those which ori-ginate al~ shared nodes of D. If tree T i has k i leaves which 
are shared nodes of D, then there are at most 3 ki outcomes for the "contests" regarding which worm proceeds 
upwards. Moreover, for each shared node n that is a leaf of T i, n's worm in T/ may or may not connect 
with that worm which in-cludes n in the tree of which n is the root. (Recall the edges from leaves to 
their parents are not really parts of worms.) Thus, we may either include or exclude the edges from node 
n to its parent in T i from the set of edges of D comprising worms. Thus for tree T/ there are at most 
6 ki subsets of edges which could possibly be the edges used in an optimal worm partition of D. Not all 
subsets of edges need be con- sistent. For example, some node could be connected to two or more of its 
parents by selected edges. The number of possible sets of selected edges is no more than 116 k~ which 
is no greater than 6 s, where s is the total sharing of D. Each such'set of selected 29 edges can be 
checked for consistency and legality in O(n) time, where n is the number of nodes of D. If legal, the 
cost of the partition can easily be computed in O(n) time by inspecting the worm heads. We can therefore 
run through all the candi- date worm partitions in O(n6 s) time and select that one with the least cost. 
[] 7. Summary and Open Questions We have analyzed several simple heuristics for generating code for a 
one-register machine, showing them to have a worst case of 3/2 or, in one case, 3. We have also shown 
for the one-register machine, both in the commutative and non- commutative cases, that there are optimal 
code generation algorithms which are linear in the size of the dag and exponential only in the sharing. 
Additionally, we have shown that even some very simple code generation problems are NP-complete. We feel 
that this work only scratches the surface of what can be learned about the important area of code generation 
algo- rithms. We therefore propose the following questions as potentially fruitful areas for fu- ture 
research. 1. Is there an optimal algorithm for multiregister machines which is polynomial in the number 
of nodes and registers, and exponential only in the amount of sharing? 2. How closely can the optimal 
code generation problem be approximated by a polynomial time heuristic on (a) single re-gister, (b) multiregister, 
and (c) infinite re-gister machines? In particular, can we, for all e > 0, develop polynomial time algo-rithms 
with a worst case ratio of 1 + e? What about the same problems for commu- tative machines? 3. On some 
machines certain opera-tions such as multiplication require an even-odd register pair. How do machine 
anomalies such as these affect the computa- tional complexity of code generation? Is optimal code generation 
polynomial, even for trees? 4. How difficult is it to generate code for a tree in which some of the 
leaves are labeled'by registers rather than memory 1o-  cations? The leaves whose values are in re- 
gisters cause a register to be freed when they are used. Sethi [$2] has shown that we can without loss 
of generality evaluate any subtree containing a leaf in a register, pro- vided we can do so with no stores, 
replacing that subtree by a leaf in a register. The problem of what to, do when no such reduc- tions 
are possible appears NP-complete. Is it? Acknowledgements The authors wish to thank Brenda Baker, Brian 
Kernighan, Doug Mcllroy, and Elliot Pinson for their helpful comments on the manuscript. References 
[AHU] A. V. Aho, J. E. Hopcroft and J. D. Ullman, The Design and Analysis of Computer Algorithms, Addison 
Wesley, 1974. [AJ] A. V. Aho and S. C. Johnson, "Optimal Code Generation for Expression Trees," Proc. 
Seventh Annual ACM Symposium on Theory of Computing, May 1975, pp. 207-217. [AU] A. V. Aho and J. D. 
Ullman, The Theory of Parsing, Translation and Compiling, Vol. II, Compiling, Prentice Hall, 1973. [Bea] 
J. C. Beatty, "A Register Assignment Algorithm for Generation of Highly Optim- ised Object Code," IBM 
J. Res. Dev. 18,1 (January 1974), 20-39. [Bel] L. A. Belady, "A Study of Replace-ment Algorithms for 
Virtual Storage Com- puters," IBMSyst. J., 5:2 (1966) 78-101. [Br] M. A. Breuer, "Generation of Optimal 
Code for Expressions via Factorization," Comm. ACM 12,6 (June 1969), 333-340. [BL] J. L. Bruno and T. 
Lassagne, "The Generation of Optimal Code for Stack Machines," J. ACM 22,3 (July 1975), 382- 397. [BS] 
J. L. Bruno and R. Sethi, "Register Al- location for a One-Register Machine," TR-157, Computer Science 
Dept., Penn State Univ., University Park, Pa., Oct., 1974. [C] S. Chen, "On the Sethi-Ullman Algo- 30 
 port #11, Bell Laboratories, Holmdel, N. J., May, 1973. [CS] J. Cocke and J. T. Schwartz, Program-mhlg 
Languages and their Compilers (second edition) Courant Institute, NYU, New York, 1970. [D] A. Demers, 
private communication. [Fa] R. J. Fateman, "Optimal Code for Serial and Parallel Computation," Comm. 
ACM 12,12 (December 1969), 694-695. [Fr] R. A. Freiburghouse, "Register Alloca- tion Via Usage Counts," 
Comm. ACM 17,11 (November 1974), 638-642. [HKMW] L. P. Horowitz, R. M. Karp, R. E. Miller and S. Winograd, 
"Index Register Al- location," J. ACM 13,1 (January 1966), 43- 61. [SI] R. Sethi, "Complete Register 
Allocation Problems," SIAM J. Comput#tg 4,3 (Sep-tember 1975), 226-248. [$2] R. Sethi, private communication. 
[SU] R. Sethi and J. D. Ullman, "The Gen- eration of Optimal Code for Arithmetic Ex- pressions," J. ACM 
17,4 (October 1970), 715-728. [Wai] W. M. Waite, "Optimization," In Com-piler Construction: An Advanced 
Course, F. L. Bauer and J. Eickel, eds., Springer-Verlag, 1974, pp. 549-602. [Was] S. G. Wasilew, "A 
Compiler Writing System with Optimization Capabilities for Complex Order Structures," Ph.D. Thesis, Northwestern 
Univ., Evanston, Ill., 1971. [WJWHG] W. A. Wulf, R. K. Johnsson, C. B. Weinstock, S. O. Hobbs and C. 
M. Geschke, The Design of an Optimizing Com-piler, Elsevier, 1975.  
			