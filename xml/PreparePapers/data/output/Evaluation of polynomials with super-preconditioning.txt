
 EVALUATION OF POLYNOMIALS WITH SUPER-PRECONDITIONING Richard J. Lipto~ Department of Computer Science 
Yale University New Haven, Connecticut 06520 Larry J. Stockmeyer Mathematical Sciences Department IBM 
Thomas J. Watson Research Center Yorktown Heights, New York 10598 i. Introduction In an effort to understand 
the complexity of arithmetic computation, a number of researchers [1,5,7,8,9,11] have studied the following 
question: Given a polynomial f(x), Find a minimal cost straightline program that computes f(x).~ In 
this paper this question is generalized to the following question: Given a polynomial f(x) and an operator 
A that maps polynomials to sets of polynomials, Find a minima] cost straightline program that computes 
some h(x) e A(f(x)). We call this model super-preconditioning. It allows the replacement of f(x) by 
any other h(x) e A(f(x)). The motivation for studying super-preconditioning is threefold. First, in 
a number of interesting applications one does not care whether f(x) is evaluated; one cares only whether 
h(x), which is related to f(x) in a specific way, is evaluated. For example, consider the following operators: 
 SameRoots(f) = {h I f(x) = 0 iff h(x) = 0 for all x~ C}; Interpolatexl, ..,Xn(f) = {h I h(xi) = f(xi) 
for i = 1 ..... n} where Xl,...,x n are fixed points; Near C(f) = {h I If(x) -h(x) I < for all 0 ~x~ 
i} fixed. % This represents work performed while the author was visiting IBM Thomas J. Watson Research 
Center. The work was also supported in part by the National Science Foundation under contract DCR-74-12870. 
 The definition of a straightline program is the standard one and is repeated below. Unless we state 
otherwise, we assume that programs use the arithmetic operations +,-,×,÷ and can fetch arbitrary complex 
scalars. The "cost" of a program is usually taken to be the number of operations of a specified type. 
 (Here, polynomials have, in general, complex coefficients. C denotes the set of complex numbers.) The 
operator SameRoots corresponds to the case where one needs to know only whether f(x) is zero. The operator 
Interpolate corresponds to the case where one cares only about fitting data. The operator Near corresponds 
to the case where one wishes only to approximate f(x) --other variations of the operator Near can of 
course be defined for other norms. The second reason to study super-preconditioning is that it presents 
a natural framework for a large number of open questions about the nature of straightline computation. 
For example, consider the following additional operators: Shift(f) = {h I h(x) = xk. f (x) for some 
ka 0}; Multiples(f) {h I h(x) = g(x) f(x) for some polynomial g(x) not identically zero}; Scalar(f) 
{h I h(x) = 1.f(x) for some ~ C, ~ ~ 0}; Powers(f) {h I h(x) = f(x)) k for some k a i}. Each of these 
super-preconditioning problems corresponds to some open question concerning straightline computations: 
 Shift: In the absence of division can xk.f(x) be easier to evaluate than f(x)? Multiples: Can the product 
of two hard polynomials be easy? Scalar: In the absence of division can l-f(x) be easier to evaluate 
than f(x)? Powers: Can radicals, i.e. kFn "speed up" straightline computation, even if used only at 
the last step? Equivalently, can fk(x) be easier to evaluate than f(x)? Questions of the form "Does 
the operation ... help?" can be naturally posed as super- preconditioning problems. The last reason 
for studying super- preconditfoning is our reaction to these many open questions about polynomial evaluation. 
Can new and more powerful methods be found for proving lower bounds on the complexity of polynomials? 
The answer is partially yes. We have been able to extend two known lower-bound methods. We feel, moreover, 
that the side effect of getting new lower-bound methods is perhaps just as important as our results. 
 The results and proof methods relate mainly to the operator Multiples and to the associated question 
"Can the product of two hard polynomials be easy?" The answer is affirmative, as shown in Section 3. 
There exist polynomials of degree n such that h is easy to compute (specifically, h can be computed in 
O(iog n) operations); and h can be factored as h(x) = f(x). g(x) where f and g are both relatively hard 
to compute (specifically, f and g require at least n r operations for some r > 0). We give two incomparable 
versions of this result with quite different proofs. The first version allows for a larger class of straightline 
programs than the second version, but the second version yields a stronger lower bound (a larger value 
of r). The first version (Theorem 3.1) states that for any ~ > 0 and all sufficiently large n, if a 
polynomial h(x) has n distinct roots, then there is a factor fix) of h(x) such that f(x) requires at 
least 1/4- E n operations for its evaluation by a program using any of the operations +,-,x,÷ and 
any complex scalars. If h(x) is sufficiently easy (say, h(x) = x n- i), it then follows that h(x) can 
be factored as h(x) = f(x) g(x) where both f and g are hard. The key element in the proof of this theorem 
is provided by Strassen [9], who shows that if a polynomial f is computed by a program with sufficiently 
few operations then the coefficients of f satisfy a nontrivial polynomial relation of known degree. 
The second version (Theorem 3.4) states that there is a constant d> 0 such that x n- 1 can be factored 
into monic polynomials f(x) and g(x) where both f and g require at least ( n ) 1/2 d-io~  operations 
for their evaluation by a program without division and with scalars drawn from the ring Z [e 2~i/n] 
.  (This ring is the set of algebraic integers in the splitting field of x n- i.) The proof of Theorem 
3.4 is a generalization of a "counting" method used by Paterson and Stockmeyer [81 to prove the existence 
of hard 0-i polynomials evaluated without division and with scalars from the integers Z only. We do 
not consider it particularlysurprising that an easy polynomial can have hard factors. We do feel, however, 
that the proof methods are novel and might find use in other situations where, given a sufficiently rich 
class of polynomials, one wants a proof that some polynomial in the class is hard to evaluate. (In Theorems 
3.1 and 3.4, the class in question is the set of monic factors of a given polynomial.) By way of perspective, 
we show in Section 2 that a hard polynomial cannot in general have an easy multiple; "in general" here 
means that the coefficients of the polynomial are algebraically independent. Sections 2 and 3 can stand 
on their own. Section 4 contains some concluding remarks and open questions. We close this introductory 
section by defining our notion of algorithm, which is the standard one [1,5,8,g,ii] based on straightline 
programs. Definition. Let S ~ C. An algorithm A over S is a sequence ~(1),e(2),...,e(t) where, for 1 
S Z ~ t, either ~(~) Su {x} or a(i) = (o,i,j) where 1 ~i,j < ~ and o e {+,-,x,÷}.  We say that ~(£) 
defines a nonscalar multiplication if ~(£) = (x,i,j) and neither d(i) S nor ~(j) S; e(i) defines a 
nonscalar division if ~(%) = (÷,i,j) and e(j)~ S. With each step ~(~) we associate, in the obvious way, 
an element IZ C(x). The algorithm A is said to compute the polynomial f C[x] if I£ = f for some 1 ~ 
Z ~ t. The set of scalars S determines what type of preconditioning is allowed. In Sections 2 and 3.1 
we consider the case S = C, while in Section 3.2 we must restrict the set of scalars. We are concerned 
primarily with the number of multiplication/division (we use the notation m/d) operations occurring in 
an algorithm although at times we are also interested in addition/ subtraction (a/s) operations. Z and 
Q stand for the sets of integers and rational numbers respectively. Ix] is the greatest integer ~ x. 
Ix] is the least integer x. If u,v6 R where R is a commutative ring (e.g. R = C[x] or R = Z, depending 
on context), we write ulv iff there exists a w R such that v = uw. We let deg(f) denote the degree of 
the polynomial f. 2. The General Polynomial In this section we consider the so-called general or worst-case 
monic polynomial f(x) = x n+ an_ixn-l+ ... + alx+ a 0  where the coefficient a 0 .... ,an_ 1 are algebraically 
unrelated. For such a polynomial, we find that any polynomial h in Multiples(f) u SameRoots(f) -- and 
therefore any h in Powers(f) -- requires at least as many m/d operations and at least as many a/s operations 
as f. The proof is indirect; we first get lower bounds on the complexity of an arbitrary h Multiples(f) 
and then appeal to known algorithms that yield identical upper bounds on the complexity of f. The proof 
of the lower bounds is based on a transcendence or "degrees of freedom" argument similar to those used 
by Motzkin [7] and Belaga [1] to establish lower bounds on the number of operations required to evaluate 
a general polynomial. The complex numbers ~l,...,~r are algebraically independent iff there is no nontrivial 
H Z[y I .... ,yr ] such that H(el, .... ~r ) = 0. If L is a field, C DL2Q, which is finitely generated 
over Q, then we let trdg(L) denote the transcendence degree of L over Q; see, for example, Stewart [12] 
for definitions. Lower bounds on the complexity of an arbitrary h Multiples(f)U SameRoots(f) are established 
by the following two lemmas. Lemma 2.1. Let f,h Ix] be such that f is monic and h Multiples (f) u 
SameRoots (f) . Let 6[e C n+l denote the coefficients of f and b C m+l denote the coefficients of h 
(where n =deg(f) and m=deg(h)). Then trdg(Q(b)) >_ trdg(Q(a)). This proof and several subsequent ones 
are omitted for the sake of brevity. Complete proofs will appear in the final version of this paper. 
 Lemma 2.2. Let h e C[x] with coefficients b C m+l. Motzkin [7]: If h is computed by an algorithm over 
C using k m/d operations, then trdg(Q(b)) _< 2k. Belaga [i]: If h is computed by an algorithm over C 
using k a/s operations, then trdg(Q(b)) -< k+l. We can now obtain, the main result of this section. 
 Theorem 2.3. Let f,h C[x] be such that f(x) = x n+ an_ixn-l+ ... + alx+ a 0 where a0,... ,an_ 1 are 
algebraically independent and h Multiples(f) u SameRoots(f). i) If h is computed by an algorithm A 
over C, then A contains at least operations and at least n a/s operations. 2) There are algorithms 
A1 and A2 over C that compute f using m/d operations and n a/s operation~ respectively. Remark. We 
have considered only monic polynomials f since, if f is not monic, it is sometimes possible to save one 
m/d operation by choosing a suitable h Multiples(f). Specifically, if n f (x) = E aixl i=0 where a0,...,a 
n are algebraically independent, then it follows from Lemma 2.2 (Motzkin) that f requires in+l_ m/d 
operations, but h = (a~) f can be computed using m/d operations. Remark. The proof of Lemma 2.2 
also implies that if we are given algebraically independent complex numbers rl,... ,r n and we want 
to compute a nontrivial polynomial h such that h(r i) = 0 for 1 <- i -< n, then the polynomial n ]I 
(x- r i) i=l is optimal with respect to, say, the number of m/d operations required. 3. Easy Polynomials 
with Hard Factors We now show that there exist polynomials h(x) of degree n such that h is easy to compute 
-- specifically, h can be computed using a total of O(log n) operations --and h can be factored as h(x) 
= f(x) g(x) where f and g are both relatively hard to compute --specifically, f and g require at least 
n r nonscalar multiplications, where r > 0. As outlined in the Introduction, we give two quite different 
proofs of this result. The first proof is the more general of the two in that it allows algorithms with 
divisions and complex preconditioning.  3.1 Complex Scalars Let f C[x] and define C(f) = min{k I 
there is an algorithm over C that computes f and contains k nonscalar m/d operations}. The principal 
result of §3.1 is the following. Theorem 3.1. For each £ > 0 the following is true for all sufficiently 
large n. Let h C[x] be a polynomial with at least n distinct (complex) roots. i) There is an f C[x] 
such that flh and C(f) z n I/4 -£ 2) If, moreover, (½) nl/4- 6 _ i, C(h) then h can be factored 
as h = f-g with both C(f) ~ n I/4- e and (½) nl/4 -e. C(g) For example, the theorem applies to h(x) 
= x n -i, which can be computed in a total of O(log n) operations. Borodin and Cook [2] point out that 
there exist polynomials of degree n with n distinct real roots that can be computed in O(log n) operations. 
To our knowledge, there are no results on whether there exist such easy polynomials with simple rational 
roots. The first step in the proof of this theorem is  £o get a handle on those polynomials f with 
2(f) -<k. Following Paterson and Stockmeyer [8], we observe that if C(f) -<k then f is computed by the 
following scheme Pk: ~/-3 = i; ~0 = x; Z-i  ~/3£-2 = ~ mz, j~13j ; j=-i £-i ~3£-i ~ m£, j ~3j' " 
 = ' j=-i  ~3£ = ~3£-2 " (m[ ~3£-i + (l-m[) ÷ ~/3Z_i ) for £ = 1,2,3,...,k;  k f(x) = Z mk+l, jU3j 
;  j=-l  where the parameters m£, j ,m[, jm~  range over C. (In fact, the values of the parameters 
m E can be restricted to 0 and i.) The key element in the proof of Theorem 3.1 is provided by Strassen 
[9], who shows that if a polynomial f is computed by an algorithm with sufficiently few operations then 
the coefficients of f satisfy a non-trivial polynomial H with integer coefficients; moreover, he places 
upper bounds on the degree and height of H. (The height of a polynomial is the maximum of the absolute 
values of its coefficients. ) Lipton [5] has observed that one can apply Strassen's analysis to the 
canonical program Pk, thereby producing, for certain n and k, a single polynomial H that is satisfied 
by the coefficients of any nth degree polynomial that is computable by Pk. For a given k, the polynomial 
H of Lemma 3.2 is probably the presently most useful restriction on the polynomials f that enjoy C(f) 
~<k. It is quite possible that Lemma 3.2 will find applications other than the ones described in Lipton 
and Strassen [5,9] and in this paper. Lemma 3.2. Let q->5 and let il,...,i q be pairwise distinct positive 
integers with d = max{il,...,iq}. Let g and k be positive integers such that  gq-m-2 > ds(m+l)qq where 
m = k 2 + 6k+ 1 and s = 2k 2+ 9k+ 2. Then there is a non-trivial polynomial form H Z[y I .... ,yq] of 
degree g and height K 3 such that if f(x) = ~ aixl, aiEC, $_>d, i=0  is computed by an algorithm over 
C using -< k nonscalar m/d operations then H(ail,... ,aiq) = 0.  Note. This lemma is not stated explicitly 
in Strassen [9], although the proof is essentially the same. In brief, we apply the analysis of Strassen's 
Lemmas 2.3 and 2.4 and Theorem 2.5 to the canonical program Pk. In Strassen's notation, m is the number 
of indeterminates that are used in this analysis to parameterize Pk, and s is the total number of arithmetic 
operations (both scalar and nonscalar) in Pk. Define the max-degree (mdeg) of a monomial by mdeg(t~ 
1 ... tep) = max{e I ..... ep}. P  If G C[tl,...,tp], then mdeg(G) is the maximum of mdeg(e) over all 
monomials ~ appearing in G. Lemma 3.3. Let p and n be positive integers, and let G C[tl,...,t p] be 
such that n p~y  and n mdeg(G) s ~.  Let A be a set of n pairwise distinct complex numbers, and 
assume that G(~i,...,~ P) = 0 for all pairwise distinct el .... ,ep A. Then G is identically zero. 
 Proof. The proof is by induction on p and is very similar to Lemma 4 in Lipton [5]. Proof of Theorem 
3.1. Fix an e > 0. Let y and 8 be real numbers such that 1 > y > B > i- 4~. Referring £o Lemma 3.2, let 
 k = [n 1/4 -ej, q = inS], g = [n¥] '  and ij = j for 1 ~ j ~q, so that d = q. Note tha  gq-m-2 > ds(m+l)qq 
 holds for all sufficiently large n because gq-m-2 = nT(nS-o(n g) > n ~(nS+O(nS) = qq+S(m+l)  for 
all sufficiently large n recall that ¥ > 8). By Lemma 3.2, therefore, there exists a non-trivial form 
H Z[yl,...,y q] of degree g such that H is satisfied by the coefficients al,...,a q of any polynomial 
that is computable by k over C, i.e. any polynomial f with C(f) A k. But we want a polynomial relation 
on the roots of f rather than one on the coefficients. The transition from a relation on the coefficients 
to one on the roots is easily made via the elementary symmetric functions. We let s!P ) l  denote 
the ith elementary symmetric function of the p variables t = <tl,...,tp>. The S (p) i  for l~i~p are 
defined by P P . s'P)} Z (x- t i) = x p+ Z (-i) i l (t) x p-i. i=l i=l Now assume n is so large that 
 177 g = In 7] N L~J-i, and therefore q L~J- i. Let P = L~J and define G Z[tl,...,t p] by G(t) = 
(P) ,S (P)+l (t) . ~(P) (t)) H(Sp_q(t) P-q .... ~p-i " It follows that if f C[x] is a polynomial with 
roots pl,...,p p such that deg(f) = p and C(f) ~ k = Lnl/4- ej, then G(Pl , .... pp) = 0. Moreover, 
G is not identically zero because (P) . s (p) s I ,.. , p are algebraically independent [12]. And, 
since deg(H) = g and mdeg(S~ p) ) = i for all i and p, it is easy to see that mdeg(G) ~ g. Recall that 
 n   g<~. Now by invoking Lemma 3.3 with A = {rl,...,r n} where rl,...,r n are distinct roots of h, 
there must exist 1 N £i < £2 < "'" < in N n such that G(ril,r£2,...,rip) ~ 0. Letting P f(x) = H (x- 
rij), j=l we therefore have C(f) > k+l > n I/4- c -w which proves the first part of Theorem 3.1. 
 The second part is immediate from the first since, if h(x) f(x) = g (x) ' then C(f) s C(h) +C(g) 
+ i. 3.2 Restricted Preconditioning Our second proof is a generalization of a "counting" method used 
by Paterson and Stockmeyer [8] to prove the existence of polynomials with 0-i coefficients that require 
roughly ~n multiplications for their evaluation by any algorithm over Z without divisions. It is instructive 
to review this method before generalizing it. First, if f ZEx] can be computed in g k nonscalar multiplications 
by an algorithm over Z without divisions, then f is computed by the scheme Pk' (Pk' is identical to Pk 
except that the parameters m[ for 1-< £-<k are set equal to 1), where the remaining parameters, mg,j 
  and m;,j range over g. Note that the scheme Pk' involves k 2 + 4k + 2 parameters. Moreover, the residue 
classes (mod 2) of the parameters uniquely determine the residue classes (mod 2) of the coefficients 
of f. If ~k' is capable of computing any 0-1 polynomial of degree n (there are 2 n such polynomials), 
we must therefore have 2 k2+4k+2 >_ 2 n,  which gives k >-~-O(i).  To generalize this argument, we 
can consider another commutative ring R in place of Z and another modulus (in general, an ideal) I in 
place of 2. We note two conditions that are essential to the success of the argument. First, the number 
of residue classes modulo I must be "small"; second, we must be interested in computing any member of 
a "large" set S c R[x], the polynomials in S being pairwise incongruent modulo I --in the sense that 
if n aixl  i=0 and n a~x i l i=0  are in the set of interest S then there is some i such that 
 a i ~ a~ (mod I).  In general, if there are p residue classes modulo I and there are b pairwise incongruent 
polynomials in the set S, then we immediately conclude that k2+4k+2 p > b,  and therefore ( ~Oog~p 
) 1/2 k ~ - O(i)  if k nonscalar multiplications are to be sufficient to compute any polynomial in S 
by an algorithm over R without divisions. (All logarithms are to the base 2.) We now apply this method 
to show that h(x) = x n- 1 can be factored into two polynomials, both difficult to compute when compared 
to h, which can be computed using ~ 2- [log n] total operations by an algorithm over Z without divisions. 
In this case, the polynomials of interest are the monic factors of x n- i. As in the basic argument of 
Paterson and Stockmeyer, we must suitably restrict the set of scalars that enter into the algorithm. 
Of course, the scalars must be sufficient to allow the computation of any monic factor of x n- i. In 
this case, we let z[~ n] infinitely many such primes. Recall, however, n-i = {z0 + Zl~n+ z2~2n + ... 
+ Zn_l~ n I z 0 ..... Zn_ 1 Z} be the ring of scalars, where 27Ti/n ~n = e is a primitive nth root 
of unity. (Z[~ n] is the set of algebraic integers in the splitting field Q(~n ) of x n- i.) The main 
result of §3.2 can now be stated. Theorem 3.4. There is a constant d > 0 such that for any n z 2 there 
are monic polynomials f and g such that f(x) g(x) = xn-i and both f and g require at least . n ,1/2 
d (io--~) nonscalar multiplications for their computation by an algorithm over Z[~n] without divisions. 
Moreover, deg(f) = [~] and deg(g) = [~J. The crux of the proof is to choose a modulus I for the counting 
argument so as to satisfy the two essential conditions (small p and large b) described above. A suitable 
modulus is found with the aid of two classical results of number theory (Theorems 3.6 and 3.7 below). 
In order to exploit these results, we choose the modulus to be an ideal of Z[~n]. Ideals and related 
notions are discussed in several texts such as LeVeque and Weiss [3,10]. We outline the necessary concepts 
here. Let R = Z[e] for some algebraic number c~. An ideal A of R is a subset of R such that al+a2eA 
and alreA for all al,a2£A and all rE R. If rl,r 2 R, then r I H r 2 (mod A) iff r I- r 2 A. The concept 
of congruence modulo an ideal satisfies the usual properties of congruences; for example, if r 5 s (mod 
A) and t H u (mod A), then r+ t Z s +u (mod A) and rt _= su (mod A). The number of congruence classes 
of an ideal A (within a ring R 2 A specified by context) is called the norm of A and is denoted NA. The 
following useful lemma is easily deduced from basic facts [3]; we omit the proof. Lemma 3.5. Let R = 
Z[~n] for some n, let A be an ideal of R, and say that z A where z Z. Then there is a positive integer 
m such that NAI z m. The following classical result provides us with ideals of certain norms. Theorem 
3.6. (See also Theorem 7-2-4 of Weiss [i0].) Let p and n be positive integers such that p is prime and 
p Z 1 (mod n). Then there is an ideal of ZIG n] such that NI = p. The problem of finding an ideal is 
thus reduced to finding a ~rime p { 1 (mod n). A classical result of Dirichlet's states that there are 
 that we need an upper bound on the size of p. Inspection of k h (llo°g~) I/2 -O(1) reveals that, since 
 b ~ (number of monic factors of x n -I) = 2 n , p must grow much slower than exponentially in n. 
(In particular we cannot simply choose I to be the principal ideal 2.Z[~n] since the norm of this ideal 
is 2#(n) where ~ is the Euler function.) It would be very nice if p could be bounded by a fixed polynomial 
in n. Fortunately, such a bound is provided by the following deep and remarkable result of Linnik's, 
which is stated here only in the generality we require. Theorem 3.7. There is a constant c > 0 such 
that for any n ~ 2 there exists a prime p such that p ~ 1 (mod n) and p< n c. Now fix an integer n~ 
2, and let R = Z[~] where 2~i/n ~=e Let I denote an ideal, provided by Theorems 3.6 and 3.7, with 
NI = p < n c. Note that pXn because p>n. The final ingredient of the proof of Theorem 3.4 is the fact 
that the monic factors of x n- 1 are pairwise incongruent modulo I. If f e R[x] and deg(f) ! n, write 
 n fix) = ~ ai xi i=0 and let coeffs(f) denote the vector <an,an_l,-..,al,a0 > £ R n+l. The congruence 
relation is extended to vectors in a componentwise manner. Lemma 3.8. Let u and v be monic polynomials 
that divide x n- i. If u ~ v then coeffs(u) 7 coeffs(v) (mod I). Proof. We argue by contradiction. 
If coeffs(u) Z coeffs(v) (mod I), then u(x) { v(x) (mod I) for all x6 R. If u ~ v then there exists 
an integer £ with 0 ~ g ~n-i such that (x- ~£) divides u but does not divide v, so that u(~ Z) H 0 (rood 
I), and therefore u(~ £) £ I. Since (~i _ ~j) = (_l)n-i . n n imj 0Si,j Sn-i [i0], we have u(~i) In 
n, and therefore n n I by the definition of ideal. By Lemma 3.5 we would then have pl (nn) m for some 
positive integer m, which is a contradiction because p is prime and pXn. This proves Lemma 3.8. As in 
the basic argument of Paterson and Stoekmeyer, there are at most  179 k2+4k+2 < n c(k2+4k+2) P  distinct 
choices for the parameters modulo I. Choose the constant d > O such that, for all n Z 2, k < d (loan--n)I/2 
 implies c(k2+4k+2) 1 n n < ~. (in/2]). Assume now that k < d ( n 1/2 log n' "  It follows that 
there is a set E c R n+l with 1 #E < ~-([n~2] )  such that if f is computed by the scheme ~k' for 
some choice of the parameters over R then coeffs(f) ~ e (mod I) for some vector e E. (#s denotes the 
cardinality of the set S.) Let F = {<f,g> I f and g are monic, f(x) g(x) = x n- I, deg(f) = [~], and 
deg(g) = L~]}.  There are, by Lemma 3.8, at most #E pairs <f,g> in F such that coeffs(f) H e (mod I) 
for some e E, and similarly at most #E pairs <f,g> with coeffs(g) H e E. Since #F = ( n in/2]) > 2-#E, 
 there must be some <f,g> F such that neither f nor g is computable by Pk' over R. This completes 
the proof of Theorem 3.4. 4. Conclusion and Open Problems We have shown that there exist polynomials 
f and h with h 6 Multiples(f) such that h is substantially easier to evaluate than f. It is presently 
an open question whether this situation can occur for the related operators Power and Shift (for Shift 
in the absence of division). We have no examples of f and h with h ~ Powers(f) such that h requires 
even one fewer operation than f. It seems reasonable to conjecture that the magnitude of the savings 
(n r versus O(log n)) exhibited in Theorems 3.1 and 3.4 is not possible for Powers. The situation for 
SameRoots appears trivial since x-1 E SameRoots((x-l)n). If, however, we require that f have only simple 
 roots, then again we do not know whether h E SameRoots(f) can be easier than f. A number of more or 
less i~ediate remarks can be made concerning the operator Near E (and its companion operators defined 
in terms of other norms on polynomials). First, there exist hard polynomials that can be approximated 
by easy ones: For any g > 0 and any reasonable notion of "near," there exists a polynomial of degree 
n with algebraically independent coefficients (so that f requires roughly (3/2)n operations) such that 
the identically zero polynomial is g-near to f. On the other hand, there exist hard polynomials that 
cannot be approximated by easy ones: Strassen [9] proves that if the values of a polynomial grow sufficiently 
fast (say f(i) = 221  for i = 1,2,3, .... n where n = deg(f)) then any polynomial sufficiently near 
to f requires n r operations for some fixed r > 0. We are currently investigating questions concerning 
the existence of easy approximations to 0-i polynomials. Acknowledgement. The authors are grateful to 
Edward H. Grossman for a useful conversation concerning algebraic number theory. References I] E. G. 
Belaga. Some problems involved in the computation of polynomials. Dokl. Akad. Nauk SSSR 123:775-777, 
1958. 2] A. Borodin and S. Cook. On the number of additions to compute specific polynomials. Proceedings 
of the 6th Annual ACM Symposium on Theory of Computing, 342-347, 1974.  3] W. LeVeque. Topics in Number 
Theory, Volume II. Addison-Wesley, 1956.  4] Y. V. Linnik. On the least prime number in an arithmetic 
progression I: The basic theorem. Mat. Sbornik 15:139-178, 1944. 5] R. J. Lipton. Polynomials with 0-i 
coefficients that are hard to compute. Proceedings of the 16th Annual Symposium on Foundations of Computer 
Science, 6-10, 1975.  6] A. V. Malyshew. Yu. v. Linnik's works in number theory. Acta Arith. 27:3-10, 
1975. 7] T. S. Motzkin. Evaluation of polynomials and evaluation of rational functions. Bulletin of 
the American Mathematical Society 61:163, 1955. 8] M. S. Paterson and L. J. Stockmeyer. On the number 
of nonscalar multiplications necessary to evaluate polynomials. SIAM Journal of Computing 2:60-66, 1973. 
 9] V. Strassen. Polynomials with rational coefficients which are hard to compute. SIAM Journal of Computing 
3:128-149, 1974. i0] E. Weiss. Algebraic Number Theory. McGraw Hill, 1963. ill S. Winograd. On the 
number of multiplications necessary to compute certain functions. Comm. Pure Appl. Math. 23:165-179, 
1970. 12] I. Stewart. Galois Theory. Halsted Press, New York, 1973. 
			