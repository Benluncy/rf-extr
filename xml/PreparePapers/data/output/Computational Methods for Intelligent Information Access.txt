
 Computational Methods for Intelligent Information Access Michael W. Berry (Presenting Author) Department 
of Computer Science University of Tennessee Knoxville, TN 37996-1301 Of.ce: 615-974-3838, FAX: 615-974-4404 
 berry@cs.utk.edu Susan T. Dumais Information Science Research Group Bellcore, 445 South Street Room 
2L-371, Morristown, NJ 07962-1910 std@bellcore.com Todd A. Letsche Department of Computer Science University 
of Tennessee Knoxville, TN 37996-1301 letsche@cs.utk.edu Permission to make digital or hard copies of 
part or all of this work or personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that copies bear this notice and the full 
citation on the first page. To copy otherwise, to republish, to post on servers, or to redistribute to 
lists, requires prior specific permission and/or a fee. SC '95, San Diego, CA &#38;#169; ACM 1995 0-89791-985-8/97/0011 
$3.50 Abstract Currently, most approaches to retrieving textual materials from scienti.c databases depend 
on a lexical match between words in users requests and those in or assigned to docu­ments in a database. 
Because of the tremendous diversity in the words people use to describe the same document, lexical methods 
are necessarily incomplete and imprecise. Using the singular value decomposition (SVD), one can take 
advantage of the implicit higher-order structure in the association of terms with documents by determining 
the SVD of large sparse term by document matrices. Terms and documents represented by 200-300 of the 
largest singular vectors are then matched against user queries. We call this retrieval method Latent 
Se­mantic Indexing (LSI) because the subspace represents impor­tant associative relationships between 
terms and documents that are not evident in individual documents. LSI is a com­pletely automatic yet 
intelligent indexing method, widely appli­cable, and a promising way to improve users access to many 
kinds of textual materials, or to documents and services for which textual descriptions are available. 
A survey of the com­putational requirements for managing LSI-encoded databases as well as current and 
future applications of LSI is presented. Keywords: indexing, information, latent, matrices, retrieval, 
semantic, singular value decomposition, sparse, updating 1 Introduction Typically, information is retrieved 
by literally matching terms in docu­ments with those of a query. However, lexical matching methods can 
be inaccurate when they are used to match a user s query. Since there are usually many ways to express 
a given concept (synonymy), the literal terms in a user s query may not match those of a rele­vant document. 
In addition, most words have multiple meanings (polysemy), so terms in a user s query will literally 
match terms in irrelevant documents. A better approach would allow users to re­trieve information on 
the basis of a conceptual topic or meaning of a document. Latent Semantic Indexing (LSI) [5] tries to 
overcome the problems of lexical matching by using statistically derived conceptual indices in­stead 
of individual words for retrieval. LSI assumes that there is some underlying or latent structure in word 
usage that is partially obscured by variability in word choice. A truncated singular value decomposi­tion 
(SVD) [15] is used to estimate the structure in word usage across documents. Retrieval is then performed 
using the database of singu­lar values and vectors obtained from the truncated SVD. Performance data 
shows that these statistically derived vectors are more robust in­dicators of meaning than individual 
terms. A number of software tools have been developed to perform operations such as parsing document 
texts, creating a term by document matrix, computing the truncated SVD of this matrix, creating the LSI 
database of singular values and vectors for retrieval, matching user queries to documents, and adding 
new terms or documents to an existing LSI databases [5, 24]. The bulk of LSI processing time is spent 
in computing the truncated SVD of the large sparse term by document matrices. Section 2 is a review of 
basic concepts needed to understand LSI. Section 3 uses a constructive example to illustrate how LSI 
represents terms and documents in the same semantic space, how a query is represented, how additional 
documents are added (or folded-in), and how SVD-updating represents additional documents. In Section 
4, an algorithm for SVD-updating is discussed along with a comparison to the folding-in process with 
regard to robustness of query matching and computational complexity. Section 5 surveys promising applica­tions 
of LSI along with parameter estimation problems that arise with its use. 2 Background The singular value 
decomposition is commonly used in the solution of unconstrained linear least squares problems, matrix 
rank estimation, and canonical correlation analysis [2]. Given an mXnmatrix A, where without loss of 
generality m2nand rank(A)= r, the singular value decomposition of A, denoted by SVD(A), is de.ned as 
T A=U.V (1) where UTU=VTV=Inand .=diag(O1;···;On);Oi 0for 1 .i.r; Oj=0for j2r+1. The .rst rcolumns of 
the orthogonal matrices Uand Vde.ne the orthonormal eigenvectors associated with the r nonzero eigenvalues 
of AATand ATA, respectively. The columns of Uand Vare referred to as the left and right singular vectors, 
respectively, and the singular values of A are de.ned as the diago­nal elements of .which are the nonnegative 
square roots of the n eigenvalues of AAT[15]. The following two theorems illustrate how the SVD can reveal 
important information about the structure of a matrix. Theorem 2.1 Let the SVD of A be given by Equation 
(1) and O1 2O2 ···2OrOr+1=···=On=0 and let R(A) and N(A) denote the range and null space of A, respectively. 
Then, 1. rank property: rank(A)= r,N(A) =spanfvr+1;···;vng,and R(A)=spanfu1;···;urg,where U=[u1u2 ···um 
and V=[v1v2 ···vn: r X T 2. dyadic decomposition: A=ui·Oi·v: ii=1 O22 k2 3. norms: kAkF2 =1 +···+Or,and 
kA2=O1: Proof 2.1 See [15]. Theorem 2.2 [Eckart and Young] Let the SVD of Abe given by Equation (1) with 
r= rank(A) p=min(m;n)and de.ne k AkX uiOiiT = ··v; (2) i=1 then 2 222 min kA-Bk=kA-Akk=O···+O: F Fk+1 
+prank(B)=k Proof 2.2 See [16]. In other words, Ak, which is constructed from the k-largest singular 
triplets of A, is the closest rank-kmatrix to A[15]. In fact, Akis the best approximation to Afor any 
unitarily invariant norm [22]. Hence, min kA-Bk2=kA-Akk2=Ok+1: (3) rank(B)=k 2.1 Latent Semantic Indexing 
In order to implement Latent Semantic Indexing [5, 12] a matrix of terms by documents must be constructed. 
The elements of the term­document matrix are the occurrences of each word in a particular document, i.e., 
A=[aij; (4) where aijdenotes the frequency in which term ioccurs in document j. Since every word does 
not normally appear in each document, the matrix Ais usually sparse. In practice, local and global weightings 
are applied [7] to increase/decrease the importance of terms within or among documents. Speci.cally, 
we can write aij=L(i;j)XG(i); (5) where L(i;j)is the local weighting for term iin document j, and G(i)is 
the global weighting for term i.The matrix Ais factored into the product of 3 matrices (Equation (1)) 
using the singular value de­composition (SVD). The SVD derives the latent semantic structure model from 
the orthogonal matrices Uand Vcontaining left and right singular vectors of A, respectively, and the 
diagonal matrix, .,of sin­gular values of A. These matrices re.ect a breakdown of the original relationships 
into linearly-independent vectors or factor values.The use of kfactors or k-largest singular triplets 
is equivalent to approxi­mating the original (and somewhat unreliable) term-document matrix by Akin Equation 
(2). In some sense, the SVD can be viewed as a technique for deriving a set of uncorrelated indexing 
variables or factors, whereby each term and document is represented by a vector in k-space using elements 
of the left or right singular vectors (see Table 1). Table 1: Interpretation of SVD components within 
LSI. Ak = Best rank-kapproximation to A m = Number of terms U = Term vectors n = Number of documents 
.V = Singular values = Document vectors k r = Number of factors = Rank of A Figure 1 is a mathematical 
representation of the singular value decomposition. Uand Vare considered the term and document vec­tors 
respectively, and .represents the singular values. The shaded regions in Uand Vand the diagonal line 
in .represent Akfrom Equation (2). It is important for the LSI method that the derived Akmatrix not reconstruct 
the original term document matrix Aexactly. The trun­cated SVD, in one sense, captures most of the important 
underlying structure in the association of terms and documents, yet at the same time removes the noise 
or variability in word usage that plagues word­based retrieval methods. Intuitively, since the number 
of dimensions, k, is much smaller than the number of unique terms, m, minor differ­ences in terminology 
will be ignored. Terms which occur in similar documents, for example, will be near each other in the 
k-dimensional factor space even if they never co-occur in the same document. This means that some documents 
which do not share any words with a users query may none the less be near it in k-space. This derived 
representation which captures term-term associations is used for re­trieval. Consider the words car, 
automobile, driver, and elephant.The terms car and automobile are synonyms, driver is a related con­cept 
and elephant is unrelated. In most retrieval systems, the query automobiles is no more likely to retrieve 
documents about cars than documents about elephants, if neither used precisely the term auto­mobile in 
the documents. It would be preferable if a query about au­tomobiles also retrieved articles about cars 
and even articles about drivers to a lesser extent. The derived k-dimensional feature space can represent 
these useful term interrelationships. Roughly speak­ing, the words car and automobile will occur with 
many of the same words (e.g. motor, model, vehicle, chassis, carmakers, sedan, engine, etc.), and they 
will have similar representations in k-space. The contexts for driver will overlap to a lesser extent, 
and those for elephant will be quite dissimilar. The main idea in LSI is to explicitly model the interrelationships 
among terms (using the truncated SVD) and to exploit this to improve retrieval. Term k Vectors Ak = 
 U k k Document Vectors VT k m Xnm Xrr Xrr Xn Figure 1: Mathematical representation of the matrix 
Ak. 2.2 Queries For purposes of information retrieval, a user s query must be repre­sented as a vector 
in k-dimensional space and compared to docu­ments. A query (like a document) is a set of words. For example, 
the user query can be represented by T.1 q^=qUk.; (6) k where qis simply the vector of words in the users 
query, multiplied by the appropriate term weights (see Equation (5)). The sum of these k-dimensional 
terms vectors is re.ected by the qTUkterm in Equation (6), and the right multiplication by . .1 differentially 
weights the sep­ k arate dimensions. Thus, the query vector is located at the weighted sum of its constituent 
term vectors. The query vector can then be compared to all existing document vectors, and the documents 
ranked by their similarity (nearness) to the query. One common measure of similarity is the cosine between 
the query vector and document vec­tor. Typically, the zclosest documents or all documents exceeding some 
cosine threshold are returned to the user [5]. 2.3 Updating Suppose an LSI-generated database already 
exists. That is, a col­lection of text objects has been parsed, a term-document matrix has been generated, 
and the SVD of the term-document matrix has been computed. If more terms and documents must be added, 
two alter­natives for incorporating them currently exist: recomputing the SVD of a new term-document 
matrix or folding-in the new terms and documents. Four terms are de.ned below to avoid confusion when 
discussing updating. Updating refers to the general process of adding new terms and/or documents to an 
existing LSI-generated database. Up­dating can mean either folding-in or SVD-updating. SVD-updating is 
the new method of updating developed in [24]. Folding-in terms or documents is a much simpler alternative 
that uses an existing SVD to represent new information. Recomputing the SVD is not an up­dating method, 
but a way of creating an LSI-generated database with new terms and/or documents from scratch which can 
be compared to either updating method. Recomputing the SVD of a larger term-document matrix requires 
more computation time and, for large problems, may be impossible due to memory constraints. Recomputing 
the SVD allows the new p terms and qdocuments to directly affect the latent semantic structure by creating 
a new (m+p)X(n+q)term-document matrix A,com­puting the SVD of the matrix A, and generating a different 
Akmatrix. In contrast, folding-in is based on the existing latent semantic struc­ture, the current Ak, 
and hence new terms and documents have no effect on the representation of the pre-existing terms and 
documents. Folding-in requires less time and memory but can have deteriorating effects on the representation 
of the new terms and documents. Folding-in documents is essentially the process described in Sec­tion 
2.2 for query representation. Each new document is represented as a weighted sum of its component term 
vectors. Once a new document vector has been computed it is appended to the set of ex­isting document 
vectors or columns of Vk(see Figure 2). Similarly, new terms can be represented as a weighted sum of 
the vectors for documents in which they appear. Once the term vector has been computed it is appended 
to the set of existing term vectors or columns of Uk(see Figure 3). Ak m Xn = Uk m Xk .k k Xk p m 
X(n+p) m Xkk Xkk X(n+p) Figure 2: Mathematical representation of folding-in pdocuments. Ak m Xn = q 
 q .k k Xk VT k k Xn (m+q) Xn (m+q) Xkk Xkk Xn Figure 3: Mathematical representation of folding-in qterms. 
To fold-in a new mX1 document vector, d, into an existing LSI model, a projection, d^,of donto the span 
of the current term vectors (columns of Uk) is computed by d^=dTUk. .1: (7) k Similarly, to fold-in a 
new 1 Xnterm vector, t, intoanexisting LSI model, a projection, ^t,of tonto the span of the current document 
vectors (columns of Vk) is determined by t^=tVk. .1: (8) k  3 Latent Semantic Indexing Example In this 
section, Latent Semantic Indexing (LSI) and the folding-in process discussed in Section 2.3 are applied 
to a small database of medical topics. In Table 2, 18 topics taken from the testbed of 1033 MEDLINE abstracts 
on biomedicine obtained from the National Library of Medicine (see MED collection in [5]). All the underlined 
words in Table 2 denote keywords which are used as referents to the medical topics. The parsing rule 
used for this sample database required that keywords appear in more than one topic. Of course, alternative 
parsing strategies can increase or decrease the number of indexing keywords (or terms). Table 2: Database 
of medical topics from MEDLINE. Underlined keywords appear in more than one topic. Label Medical Topic 
M1 study of depressed patients after discharge with regard to age of onset and culture M2 culture of 
pleuropneumonia like organisms found in vaginal discharge of patients M3 study showed oestrogen production 
is depressed by ovarian irradiation M4 cortisone rapidly depressed the secondary rise in oestrogen output 
of patients M5 boys tend to react to death anxiety by acting out behavior while girls tended to become 
depressed M6 changes in children s behavior following hospitalization studied a week after discharge 
M7 surgical technique to close ventricular septal defects M8 chromosomal abnormalities in blood cultures 
and bone marrow from leukaemic patients M9 study of christmas disease with respect to generation and 
culture M10 insulin not responsible for metabolic abnormalities accompanying a prolonged fast M11 close 
relationship between high blood pressure and vascular disease M12 mouse kidneys show a decline with respect 
to age in the ability to concentrate the urine during a water fast M13 fast cell generation in the eye 
lens epithelium of rats M14 fast rise of cerebral oxygen pressure in rats Table 3: The 18 X14 term-document 
matrix corresponding to the medical topics in Table 2. Terms Documents M M M MMM MMM MM MMM 1 2 3 4 5 
6 7 8 9 10 11 12 13 14 abnormalities 0 0 0 0 0 0 0 1 0 1 0 0 0 0 age 1 0 0 0 0 0 0 0 0 0 0 1 0 0 behavior 
0 0 0 0 1 1 0 0 0 0 0 0 0 0 blood 0 0 0 0 0 0 0 1 0 0 1 0 0 0 close 0 0 0 0 0 0 1 0 0 0 1 0 0 0 culture 
1 1 0 0 0 0 0 1 1 0 0 0 0 0 depressed 1 0 1 1 1 0 0 0 0 0 0 0 0 0 discharge 1 1 0 0 0 1 0 0 0 0 0 0 0 
0 disease 0 0 0 0 0 0 0 0 1 0 1 0 0 0 fast 0 0 0 0 0 0 0 0 0 1 0 1 1 1 generation 0 0 0 0 0 0 0 0 1 0 
0 0 1 0 oestrogen 0 0 1 1 0 0 0 0 0 0 0 0 0 0 patients 1 1 0 1 0 0 0 1 0 0 0 0 0 0 pressure 0 0 0 0 0 
0 0 0 0 0 1 0 0 1 rats 0 0 0 0 0 0 0 0 0 0 0 0 1 1 respect 0 0 0 0 0 0 0 1 0 0 0 1 0 0 rise 0 0 0 1 0 
0 0 0 0 0 0 0 0 1 study 1 0 1 0 0 0 0 0 1 0 0 0 0 0 Corresponding to the text in Table 2 is the 18 X14 
term-document matrix shown in Table 3. The elements of this matrix are the fre­quencies in which a term 
occurs in a document or medical topic (see Section 4). For example, in medical topic M2, the second column 
of the term-document matrix, culture, discharge, and patients all occur once. For simplicity, term weighting 
is not used in this example matrix. Now compute the truncated SVD (with k=2)of the18 X14 matrix in Table 
2 to obtain the rank-2 approximation A2 as de.ned in Figure 1. Using the .rst column of U2 multiplied 
by the .rst singular value, O1, for the x-coordinates and the second column of U2 multiplied by the second 
singular value, O2, for the y-coordinates, the terms can be represented on the Cartesian plane. Similarly, 
the .rst column of V2 scaled by O1 are the x-coordinates and the second column of V2 scaled by O2 arethey-coordinatesforthedocuments(medicaltopics). 
Figure 4 is a two-dimensional plot of the terms and documents for the 18 X14 sample term-document matrix. 
Notice the documents and terms pertaining to patient behavior or hormone production are clustered above 
the x-axis while terms and documents related to blood disease or fasting are clustered near 0.2 0.1 -0.2 
-0.4 -0.6  M1 depressed discharge patients M2 M3 M5   M6 M4 oestrogen behavior  culture 
 study  rats M13 M14 fast Figure 4: Two-dimensional plot of terms and documents for the 18X14 example. 
the lower y-axis. Such groupings suggest that subsets of medical topics such as fM2, M3, M4gand fM10, 
M11, M12geach contain topics similar in meaning. Although topics M1 and M2 share the polysemous terms 
culture and discharge they are not represented by nearly identical vectors by LSI. The meaning of those 
terms in topics M1 and M2 are clearly different and literal-matching indexing schemes have dif.culty 
resolving such context changes. 3.1 Queries Suppose we are interested in the documents that contain information 
related to the age of children with blood abnormalities. Recall T.1 that a query vector (q) can be represented 
as (q^)via q^= qUk. k (see Equation (6)). Since the words of, children, and with are not indexed terms 
(i.e., stop words) in the database, they are omitted from the query leaving age blood abnormalities. 
Mathematically, the Cartesian coordinates of the query are determined by Equation (6) and the sample 
query age blood abnormalities is shown as the vector labeled QUERY in Figure 5. This query vector is 
then com­pared (in the Cartesian plane) to all the documents in the database. All documents whose cosine 
with the query vector is greater than 0:85 is illustrated in the shaded region of Figure 6. 0101 1 T0:1623 
.0:1372 B1 CB0:2068 .0:0488 C BCBC B0 CB0:0597 0:0614 C BCBC B1 CB0:1663 .0:1313 C BCBC B0 CB0:0258 .0:1246 
C BCBC B0 CB0:4534 0:0386 C BCBC B0 CB0:3579 0:1710 C BCBC B0 CB0:2931 0:1426 C BCBC() .1 ()B0 CB0:0690 
.0:1576 C3:5919 0 0:1491 .0:1199 =BCBC B0 CB0:0940 .0:6535 C02:6471 BCBC B0 CB0:0599 .0:2378 C BCBC B0 
CB0:1560 0:0661 C BCBC B0 CB0:4948 0:1091 C BCBC B0 CB0:0460 .0:3393 C BCBC B0 CB0:0369 .0:4196 C BCBC 
B0 CB0:1797 .0:1456 C BCBC @0 A@0:1087 .0:2126 A 00:3814 0:0941 Figure 5: Derived coordinates for the 
query of age blood abnormal­ities. A different cosine threshold, of course, could have been used so that 
a larger or smaller set of documents would be returned. The cosine is merely used to rank-order documents 
and its numerical value is not always an adequate measure of relevance [24, 30]. 3.2 Comparison with 
Lexical Matching In this example, LSI has been applied using two factors (i.e., A2 is used to approximate 
the original 18 X14 term-document matrix). Using a cosine threshold of :85, three medical topics related 
to blood abnormalities and kidney failure were returned: topics M8, M9, and M12. If the cosine threshold 
was reduced to just :75, then titles Figure 6: A Two-dimensional plot of terms and documents along with 
the query age blood abnormalities. M7 and M11 (which are somewhat related) are also returned. With lexical-matching, 
.ve medical topics (M1, M8, M10, M11, M12) would be returned. Clearly, topics M1 and M10 are not relevant 
and topic M9 would be missed. LSI, on the otherhand, is able to retrieve the most relevant topic from 
Table 2 (i.e., M9) to the original query age of children with blood abnormalities since christmas disease 
is the name associated hemophilia in young children. This ability to retrieve relevant information based 
on context or meaning rather than literal term usage is the main motivation for using LSI. Table 4 lists 
the LSI-ranked documents (medical topics) with dif­ferent numbers of factors (k). The documents returned 
in Table 4 satisfy a cosine threshold of :40, i.e., returned documents are within acosine of :40 of the 
pseudo-document used to represent the query. As alluded to earlier, the cosine best serves as a measure 
for rank­orderingonly as Table 4 clearly demonstratesthat its value associated with returned documents 
can signi.cantly vary with changes in the number of factors k. Table 4: Returned documents based on different 
numbers of LSI factors. Number of Factors k=2 k=4 k=8 M9 1.00 M8 0.92 M8 0.67 M12 0.88 M 9 0.89 M12 0.55 
M 8 0.85 M 2 0.64 M10 0.54 M11 0.82 M10 0.48 M10 0.79 M12 0.46 M 7 0.74 M11 0.40 M14 0.72 M13 0.71 M 
4 0.67 M 1 0.56 M 2 0.42 3.3 Folding-In Suppose the .ctitious topics listed in Table 5 are to be added 
to the original set of medical topics in Table 2. These new topics (M15 and M16) essentially use the 
terms as the original topics in Table 2 but in a somewhat different sense or context. Topic M15 relates 
a rise in oestrogen with the behavior of rats rather than patients. Topic M16 uses the term pressure 
in the context of behavior rather than blood. As with Table 2, all underlined words in Table 5 are considered 
signi.cant since they appear in more than one title (across all 16 topics from Tables 2 and 5). Folding-in 
(see Section 2.3) is one approach for updating the original LSI-generated database with the 2 new medical 
topics. Figure 7 demonstrates how these topics are folded-into the database based on k=2 LSI factors 
via Equation (7). The new medical topics are denoted on the graph by their document labels (in a different 
boldface font). Notice that the coordinates of the original topics stay .xed, and hence the new data 
has no effect on the clustering of existing terms or documents. Table 5: Additional medical topics for 
updating. Label Medical Topic M15 behavior of rats after detected rise in oestrogen M16 depressed patients 
who feel the pressure to fast  3.4 Recomputing the SVD Ideally, the most robust way to produce the best 
rank-kapproxima­tion (Ak) to a term-document matrix which has been updated with new terms and documents 
is to simply compute the SVD of a re­constructed term-document matrix, say A~. Updating methods which 
can approximate the SVD of the larger term-document matrix A~be­come attractive in the presence of memory 
or time constraints. As discussed in [24], the accuracy of SVD-updating approaches can be ~ easily compared 
to that obtained when the SVD of Ais explicitly computed. Suppose the topics from Table 5 are combined 
with those of Table 2 in order to create a new 18X16 term-document matrix A~. Following Figure 1, we 
then construct the rank-2 approximation to A~given by T ~~ A2=U2.~2V~2 : (9) Figure 8 is a two-dimensional 
plot of the 18 terms and 16 documents (medical topics) using the elements of U~2 and V~2 for term and 
doc­ument coordinates, respectively. Notice the difference in term and document positions between Figures 
7 and 8. Clearly, the new med­ical topics from Table 5 have helped rede.ne the underlying latent ~ structure 
when the SVD decomposition of Ais computed. That is, one can discuss blood pressure and behavioral pressure 
in different contexts. Note that in Figure 8 (unlike Figure 7) the topics (old and new) related to the 
use of rats form a well-de.ned cluster or subset of documents. Folding-in the two new medical topics 
based on the existing rank-2 approximation to A(de.ned by Table 3) may not accu­rately reproduce the 
true LSI representation of the new (or updated) database. In the case of topic M15, for example, the 
existing LSI model did not re.ect the association of the term behavior with rats, and hence the folding-in 
procedure failed to form the cluster fM13, M14, M15gof related documents shown in Figure 8. 0.2 0.1 -0.2 
-0.4 -0.6  M1 depressed discharge patients M2 M3 M5   M6 M4 oestrogen behavior  culture 
 study  rats M13 M14 fast Figure 7: Two-dimensional plot of folded-in medical topics M15 and M16. 
In practice, the difference between folding-in and SVD-updating is likely to depend on the number of 
new documents and terms relative to the number in the original SVD of A. Thus, we expect SVD­updating 
to be especially valuable for rapidly changing databases.  4 SVD-Updating The process of SVD-updating 
discussed in Section 2.3 can also be illustrated using titles from Tables 2 and 5. The three steps required 
to perform a complete SVD-update involve adding new documents, adding new terms, and correction for changes 
in term weightings. The order of these steps, however, need not follow the ordering presented 0.4 0.2 
0.1 -0.1 -0.3 -0.5 M14 Figure 8: Two-dimensional plot of terms and documents using the SVD decomposition 
of a reconstructed term-document matrix. in this section (see [24]). 4.1 Overview Let Ddenote the pnew 
document vectors to process. Then Dis an mXpsparse matrix since most terms (as was the case with the 
original term-document matrix A) do not occur in each document. D is appended to the columns of the rank-kapproximation 
of the mXn matrix A, i.e., from Equation (2), Akso that the k-largest singular values and corresponding 
singular vectors of B=(AkjD) (10) are computed. This is almost the same process as recomputing the SVD, 
only Ais replaced by Ak. Let Tdenote a collection of qX1 term vectors for SVD-updating. Then T is a qXnsparse 
matrix, since each term rarely occurs in every document. Tis then appended to the rows of Akso that the 
k-largest singular values and corresponding singular vectors of .! Ak C= (11) T are computed. The correction 
step for incorporating changes in term weights (see Equation (5)) is performed after any terms or documents 
have been SVD-updated and the term weightings of the original matrix have changed. For a change of weightings 
in jterms, let Yjbe an mXj matrix comprised of rows of zeros or rows of the j-th order identity matrix, 
Ij, and let Zjbe an nXjmatrix whose columns specify the actual differences between old and new weights 
for each of the j terms (see [24] for examples). Computing the SVD of the following rank-jupdate to Akde.nes 
the correction step. W=Ak+YjZT: (12) j 4.2 SVD-Updating Procedures The mathematical computations required 
in each phase of the SVD­updating process are detailed in this section. SVD-updating incor­porates new 
term or document information into an existing semantic model (Akfrom Equation (2)) using sparse term-document 
matrices (D, T, and YjZT) discussed in Section 4.1. SVD-updating exploits j the previous singular values 
and singular vectors of the original term­documents matrix Aas an alternative to recomputing the SVD 
of A~in Equation (9). In general, the cost of computing the SVD of a sparse matrix [4] can be generally 
expressed as IXcost (GTGx)+trpXcost (Gx); where Iis the number of iterations required by a Lanczos-type 
pro­cedure [2] to approximate the eigensystem of GTGand trp is the number of accepted singular triplets 
(i.e. singular values and corre­sponding left and right singular vectors). The additional multiplication 
by Gis required to extract the left singular vector given approximate singular values and their corresponding 
right singular vector approxi­mations from a Lanczos procedure. A brief summary of the required computations 
for updating an existing rank-kapproximation Akus­ing standard linear algebra is given below. Table 6 
contains a list of symbols, dimensions, and variables used to de.ne the SVD-updating phases. Table 6: 
Symbols used in SVD-updating phases. Symbol Dimensions De.nition AUk .k Vk Zj Yj DT mXnmXkkXknXknXjmXjmXpqXn 
Original term-document matrix Left singular vectors of Ak Singular values of Ak Right singular vectors 
of Ak Adjusted term weights Permutation matrix New document vectors New term vectors Updating Documents 
T Let B=(AkjD)from Equation (10) and de.ne SVD (B)= UB.BV. B Then .! VkO UTB=(.kjUTD); kk OIp T UTT since 
Ak=Uk.kV:If F=(.kjD)and SVD(F) = UF.FV;then it kkF follows that .! VkO UB=UkUF;VB= OIp VF;and .F=.B:(13) 
Hence UBand VBare mXkand (n+p)X(k+p)dense matrices, respectively. Updating Terms .! Let C= Akfrom Equation 
(11) and de.ne SVD (C)= UC.CVT . T C Then .!.! UTO.k kCVk= : OIq TVk .! T If H= .kand SVD(H)= U.HVthen 
it follows that H TVkH .! UkO UC= OIq UH;VC=VkVH;and .H=.C: Hence UCand VCare (m+q)X(k+q)and nXkdense 
matrices, respectively. Term Weight Corrections Let W=Ak+YjZT, where Yjis mXjand Zjis nXjfrom Equation 
jT (12), and de.ne SVD (W)= UW.WV. Then W UTWVk=(.k+UTYjZTVk): kkj If Q=(.k+UTYjZTVk)and SVD(Q)= UQ.QVT, 
then it follows that kjQ UW=UkUQand VW=VkVQ: T Since (UQUk)WVkVQ=.Q=.W. Hence UWand VWare mXkand nXkdense 
matrices, respectively. Table 7 contains the complexities for folding-in terms and docu­ments, recomputing 
the SVD, and the three phases of SVD-updating. Using the complexities in Table 7 the required number 
of .oating-point operations (or .ops) for each method can be compared for varying numbers of added documents 
or terms. As shown in [24] for a con­densed encyclopedia test case, the computational advantages of one 
scheme over another depends the values of the variables listed in Ta­ble 6. For example, if the sparsity 
of the Dmatrix from Equation (10) re.ects that of the original mXnterm-document matrix Awith m»n, then 
folding-in will still require considerably fewer .ops than SVD-updating when adding dnew documents provided 
d«n.The expense in SVD-updating can be attributed to the O(2k2m+2k2n) .ops associated with the dense 
matrix multiplications involving Uk and Vkin Equation (13). 4.3 Orthogonality One important distinction 
between the folding-in (see Section 2.3) and the SVD-updating processes lies in the guarantee of orthogonality 
in the vectors (or axes) used for term and document coordinates. Recall that an orthogonal matrix Qsatis.es 
QTQ=In, where Inis the n-th or­der identity matrix. Let Dpbe the collection of all folded-in documents 
Table 7: Computational complexity of updating methods. Method Complexity SVD-updating documents [I.[4nnz(D)+4mk+k.2m.p+ 
trp.[2nnz(D)+2mk.m +[(2k2 .k)(m+n) SVD-updating terms [I.[4nnz(T)+4kn+k.2n.q+ trp.[2nnz(T)+2kn+k.2n.q 
+[(2k2 .k)(m+n) SVD-updating correction step [I.[4nnz(Zj)+4km+2mj+2kn+3k2j.m+trp.[2nnz(Zj)+2km+2knj.n+[(2k2 
.k)(m+n) .2n. +k. Folding-in documents 2mkp Folding-in terms 2nkq Recomputing the SVD I.[4nnz(A).(m+q).(n+p) 
+ trp.2nnz(A).(m+q) where each column of the pXkmatrix is a document vector of the form ^ dfrom Equation 
(7). Similarly, let Tqbe the collection of all folded-in terms such that each column of the qXkmatrix 
is a term vector of the form ^tfrom Equation (8). Then, all term vectors and document vec­ ()T tors associated 
with folding-in can be represented as U^k=UTjTT kq ()T T and V^k=VjDT, respectively. The folding-in process 
corrupts the kp orthogonality of U^kand V^kby appending non-orthogonal submatrices ^T ^ Tqand Dpto Ukand 
Vk, respectively. Computing U^kTUkand V^kVk,the loss of orthogonality in U^kand V^kcan be measured by 
T kU^TU^k -Ikk2 and kV^V^k -Ikk2: kk Folding-in does not maintain the orthogonality of U^kor V^ksince 
arbi­trary vectors of weighted terms or documents are appended to Ukor Vk, respectively. However, the 
amount by which the folding-in method perturbs the orthogonality of U^kor V^kdoes indicate how much distor­tion 
has occurred due to the addition of new terms or documents. The trade-off in computational complexity 
and loss of orthogonal­ity in the coordinate axes for updating databases using LSI poses interesting 
future research. Though the SVD-updating process is considerably more expensive [24] than folding-in, 
the true lower-rank approximation to the true term-document matrix Ade.ned by Figure 1 is maintained. 
Signi.cant insights in the future could be gained by monitoring the loss of orthogonality associated 
with folding-in and correlating it to the number of relevant documents returned within particular cosine 
thresholds (see Section 3.1). 4.4 SVD-Updating Example To illustrate SVD-updating, suppose the .ctitious 
titles in Table 5 are to be added to the original set of titles in Table 2. In this example, only documents 
are added and weights are not adjusted, hence only the SVD of the matrix Bin Equation (10) is computed. 
Initially, a 18 X2 term-document matrix, D, corresponding to the .ctitious titles in Table 5 is generated 
and then appended to A2 to form a 18 X2matrix Bof the form given by Equation (10). Following Figure 1, 
the best rank-2 approximation (B2)to Bis given by T ^.2 ^ B2=U2 ^V2 ; where the columns of U^2 and V^2 
are the left and right singular vectors, respectively, corresponding to the two largest singular values 
of B. Figure 9 is a two-dimensional plot of the 12 terms and 16 doc­uments (book titles) using the elements 
of U^2 and V^2 for term and document coordinates, respectively. Notice the similar clustering of terms 
and book titles in Figures 9 and 8 (recomputing the SVD) and the difference in document and term clustering 
with Figure 7 (folding­in). 0.4 0.2 0.1 -0.1 -0.3 -0.5 M14 Figure 9: Two-dimensional plot of terms 
and documents using the SVD-updating process. 4.5 SVD-Updating Animation  A three-dimensional animation 
of the terms and documents from the updating example of Section 4.4 can be used to show the placement 
of the original 18 terms and 14 documents, the two doc­ uments folded-in to the rank-3 LSI model, and 
the movement of the terms and docu­ ments when SVD-updating is applied. In the video, the blue axis represents 
the x-axis, the green axis repre­sents the y-axis, and the red axis represents the z-axis. Coordinates 
for these axes, of course, are derived from the 3-largest singular triplets of the appropriate matrix 
Bin Equation (10). In addition, the white spheres represent the terms, the red spheres represent the 
documents (or medical topics), and the green spheres represent the documents (M15 and M16) being added 
to the term-document space. Folding-in and SVD-updating are illustrated by .rst showing the terms and 
documents before topics M15 and M16 are added. Then, the new topics, represented by green spheres labeled 
M15 and M16, are folded-in to the term-document space. After a slight pause, all terms and documents 
are shown moving to the positions they would assume if SVD-updating is used to add the topics M15 and 
M16 to the term-document space. Notice that SVD-updating appropriately moves the medical topic M16 to 
the centroid of the term vectors corresponding to depressed, patients, pressure, and fast.  5 Applications 
of Latent Semantic Indexing In this section, several applications of LSI are discussed ranging from information 
retrieval and .ltering to models of human memory. Some open computational and statistical-based issues 
related to the practical use of LSI for such applications are also mentioned. 5.1 Information Retrieval 
Latent Semantic Indexing was initially developed for information re­trieval applications. In these application, 
a .xed database is indexed and users pose a series of retrieval queries. The effectiveness of re­trieval 
systems is often evaluated using "test collections" developed by the information retrieval community. 
These collections consist of a set of documents, a set of user queries, and relevance judgements (i.e., 
for each query every document in the collection has been judged as relevant or not to the query)1. This 
allows one to evaluate the ef­fectiveness of different systems in retrieving relevant documents and at 
the same time not returning irrelevant documents. Two measures, precision and recall, are used to summarize 
retrieval performance. Recall is the proportion of all relevant documents in the collection that are 
retrieved by the system; and precision is the proportion of relevant documents in the set returned to 
the user. Average preci­sion across several levels of recall can then be used as a summary measure of 
performance. Results were obtained for LSI and compared against published or computed results for other 
retrieval techniques, notably the standard keyword vector method in SMART [25]. For several information 
sci­ence test collections, the average precision using LSI ranged from comparable to 30% better than 
that obtained using standard keyword vector methods. See [5, 13, 7] for details of these evaluations. 
The LSI method performs best relative to standard vector methods when the queries and relevant documents 
do not share many words, and at high levels of recall. Term Weighting One of the common and usually effective 
methods for improving re­trieval performance in vector methods is to transform the raw fre­quency of 
occurrence of a term in a document (i.e. the value of a cell in the term by document matrix) by some 
function (see Equa­tion 5). Such transformations normally have two components. Each term is assigned 
a global weight, indicating its overall importance in the collection as an indexing term. The same global 
weighting is applied to an entire row (term) of the term-document matrix. It is also possible to transform 
the term s frequency in the document; such a transformation is called a local weighting, and is applied 
to each cell in the matrix. 1Exhaustive relevance judgements (when all documents are judged for every 
query) are ideal for system evaluation. In large document collections, however, exhaustive judgements 
become prohibitively costly. For large collections a pooling method is used. Relevance judgements are 
made on the pooled set of the top­ranked documents returned by several different retrieval systems for 
the same set of queries. Most of the top-ranked documents for new systems will hopefully be contained 
in the pool set and thus have relevance judgements associated with them. The performance for several 
weighting schemes have been com­pared in [7]. A transformed matrix is automatically computed, the truncated 
SVD shown in Figure 1 is computed, and performance is evaluated. A log transformationof the local cell 
entries combined with a global entropy weight for terms is the most effective term-weighting scheme. 
Averaged over .ve test collections, log Xentropy weighting was 40% more effective than raw term weighting. 
Relevance Feedback The idea behind relevance feedback is quite simple. Users are very unlikely to be 
able to specify their information needs adequately, es­pecially on the .rst try. In interactive retrieval 
situations, it is possible to take advantage of user feedback about relevant and non-relevant documents 
[26]. Systems can use information about which docu­ments are relevant in many ways. Typically the weight 
given to terms occurring in relevant documents is increased and the weight of terms occurring in non-relevant 
documents is decreased. Most of the tests using LSI have involved a method in which the initial query 
is replaced with the vector sum of the documents the users has selected as rel­evant. The use of negative 
information has not yet been exploited in LSI; for example, by moving the query away from documents which 
the user has indicated are irrelevant. Replacing the users query with the .rst relevant document improves 
performance by an average of 33% and replacing it with the average of the .rst three relevant doc­uments 
improves performance by an average of 67% (see [7] for de­tails). Relevance feedback provides sizeable 
and consistent retrieval advantages. One way of thinking about the success of these meth­ods is that 
many words (those from relevant documents) augment the initial query which is usually quite impoverished. 
LSI does some of this kind of query expansion or enhancement even without relevance information, but 
can be augmented with relevance information. 5.2 Choosing the Number of Factors Choosing the number 
of dimensions (k)for AkshowninFigure1is an interesting problem. While a reduction in kcan remove much 
of the noise, keeping too few dimensions or factors may loose important information. As discussed in 
[5] using a test database of medical abstracts, LSI performance2 can improve considerably after 10 or 
20 2Performance is average precision over recall levels of 0:25, 0:50 and0:75. dimensions, peaks between 
70 and 100 dimensions, and then begins to diminish slowly. This pattern of performance (initial large 
increase and slow decrease to word-based performance) is observed with other datasets as well. Eventually 
performance must approach the level of performance attained by standard vector methods, since with k=nfactors 
Akwill exactly reconstruct the original term by document matrix Ain Equation (4). That LSI works well 
with a relatively small (compared to the number of unique terms) number of dimensions or factors kshows 
that these dimensions are, in fact, capturing a major portion of the meaningful structure. 5.3 Information 
Filtering Information .ltering is a problem that is closely related to information retrieval [1]. In 
information .ltering applications, a user has a rel­atively stable long-term interest or pro.le, and 
new documents are constantly received and matched against this standing interest. Se­lective dissemination 
of information, information routing, and person­alized information delivery are also used to refer to 
the matching of an ongoing stream of new information to relatively stable user interests. Applying LSI 
to information .ltering applications is straightforward. An initial sample of documents is analyzed using 
standard LSI/SVD tools. A users interest is represented as one (or more) vectors in this reduced-dimension 
LSI space. Each new document is matched against the vector and if it is similar enough to the interest 
vector it is recommended to the user. Learning methods like relevance feedback can be used to improve 
the representation of interest vectors over time. Foltz [11] compared LSI and keyword vector methods 
for .ltering Netnews articles, and found 12% 23% advantages for LSI. Dumais and Foltz in [12] compared 
several different methods for representing users interests for .ltering technical memoranda. The most 
effective method used vectors derived from known relevant documents (like relevance feedback) combined 
with LSI matching. TREC Recently, LSI has been used for both information .ltering and infor­mation retrieval 
in TREC (Text REtrieval Conference), a large-scale retrieval conference conference sponsored by NIST 
[8, 9]. The TREC collection contains more than 1;000;000 documents (representing more that 3 gigabytes 
of ASCII text), 200 queries, and relevance judgements pooled from the return sets of more than 30 systems. 
The content of the collections varies widely ranging from news sources (AP News Wire, Wall Street Journal, 
San Jose Mercury News), to journal abstracts (Ziff Davis, DOE abstracts), to the full text of the Federal 
Register and U.S. Patents. The queries are very long and detailed descriptions, averaging more that 50 
words in length. While these queries may be representative of information requests in .l­tering applications, 
they are quite unlike the short requests seen in previous IR collections or in interactive retrieval 
applications (where the average query is only one or two words long). The fact that the TREC queries 
are quite rich means that smaller advantages would be expected for LSI or any other methods that attempt 
to enhance users queries. The big challenge in this collection was to extend the LSI tools to handle 
collections of this size. The results were quite encouraging. At the time of the TREC conferences it 
was not reasonable to compute Akfrom Figure 1 for the complete collection. Instead, a sample3 of about 
70;000 documents and 90;000 terms was used. Such term by document matrices (A) are quite sparse, containing 
only :001 :002% non-zero entries. Computing A200, i.e. the 200-largest singular values and corresponding 
singular vectors, by a single-vector Lanczos algorithm [4] required about 18 hours of CPU time on a SUN 
SPARCstation 10 workstation. Documents not in the original LSI analysis were folded-in as previously 
described in Section 3.3. That is, the vector for a document is located at the weighted vector sum of 
its constituent term vectors. Although it is very dif.cult to compare across systems in any detail because 
of large pre-processing, representation and matching differences, LSI performance was quite good [9]. 
For .ltering tasks, using information about known relevant documents to create a vector for each query 
was bene.cial. The retrieval advantage of 31% was somewhat smaller than that observed for other .ltering 
tests and is attributable to the good initial queries in TREC. For retrieval tasks, LSI showed 16% improvement 
when compared with the keyword vector methods. Again the detailed original queries account for the somewhat 
smaller advantages than previously observed. The computation of Akfor the large sparse TREC matrices 
Awas accomplished without dif.culty (numerical or convergence problems) using sophisticated implementations 
of the Lanczos algorithm from SVDPACKC [4]. However, the computational and memory require­ 3Different 
samples for information retrieval and .ltering and for TREC-1 and TREC-2 see [8, 9] for details. ments 
posed by the TREC collection greatly motivated the develop­ment of the SVD-updating procedures discussed 
in Section 4. 5.4 Novel Applications Because LSI is a completely automatic method, it is widely appli­cable 
to new collections of texts (including to different languages, as described below). The fact that both 
terms and documents are represented in the same reduced-dimension space adds another di­mension of .exibility 
to the LSI retrieval model. Queries can be either terms (as in most information retrieval applications), 
documents or combinations of the two (as in relevance feedback). Queries can even be represented as multiple 
points of interest [18]. Similarly, the objects returned to the user are typically documents, but there 
is no reason that similar terms could not be returned. Returning nearby terms is useful for some applications 
like online thesauri (that are automatically constructed by LSI), or for suggesting index terms for documents 
for publications which require them. Although term-document matrices have been used for simplicity, the 
LSI method can be applied to any descriptor-object matrix. We typically use only single terms to describe 
documents, but phrases or n-grams could also be included as rows in the matrix. Similarly, an entire 
document is usually the text object of interest, but smaller, more topically coherent units of text (e.g., 
paragraphs, sections) could be represented as well. For example, LSI has been incorporated as a fuzzy 
search option in NETLIB [6] for retrieving algorithms, code descriptions, and short articles from the 
NA-Digest electronic newsletter. Regardless of how the original descriptor-object matrix is derived, 
areduced-dimensionapproximationcanbecomputed. Theimportant idea in LSI is to go beyond the original descriptors 
to more reliable statistically derived indexing dimensions. The wide applicability of the LSI analysis 
is further illustrated by describing several applications in more detail. Cross-Language Retrieval It 
is important to note that the LSI analysis makes no use of En­glish syntax or semantics. Words are identi.ed 
by looking for white spaces and punctuation in ASCII text. Further, no stemming is used to collapse words 
with the same morphology. If words with the same stem are used in similar documents they will have similar 
vectors in the truncated SVD de.ned in Figure 1; otherwise, they will not. (For example, in analyzing 
an encyclopedia as done in [3], doctor is quite near doctors but not as similar to doctoral.) This means 
that LSI is applicable to any language. In addition, it can be used for cross­language retrieval documents 
are in several languages and user queries (again in several languages) can match documents in any language. 
What is required for cross-language applications is a com­mon space in which words from many languages 
are represented. Landauer and Littman in [21] described one method for creating such an LSI space. The 
original term-document matrix is formed using a collection of abstracts that have versions in more than 
one language (French and English, in their experiments). Each abstract is treated as the combination 
of its French-English versions. The truncated SVD is computed for this term by combined-abstract ma­trix 
A. The resulting space consists of combined-language abstracts, English words and French words. English 
words and French words which occur in similar combined abstracts will be near each other in the reduced-dimension 
LSI space. After this analysis, monolingual abstracts can be folded-in (see Section 3.3) a French abstract 
will simply be located at the vector sum of its constituent words which are already in the LSI space. 
Queries in either French or English can be matched to French or English abstracts. There is no dif.cult 
translationinvolvedinretrievalfromthemultilingualLSIspace. Exper­iments showed that the completely automatic 
multilingual space was more effective than single-language spaces. The retrieval of French documents 
in response to English queries (and vice versa) was as effective as .rst translating the queries into 
French and searching a French-only database. The method has shown almost as good re­sults for retrieving 
English abstracts and Japanese Kanji ideographs, and for multilingual translations (English and Greek) 
of the Bible [30]. Modeling Human Memory Landauer and Dumais [20] have recently used LSI spaces to model 
some of the associative relationships observed in human memory. They were interested in term-term similarities. 
LSI is often described intuitively as a method for .nding synonyms words which occur in similar patterns 
of documents will be near each other in the LSI space even if they never co-occur in a single document 
(e.g., doctor, physi­cian both occur with many of the same words like nurse, hospital, patient, treatment, 
etc.). Landauer and Dumais tested how well an LSI space would mimic the knowledge needed to pass a synonym 
test. They used the synonym test from ETS s Test Of English as a Foreign Language (TOEFL). The test consists 
of 80 multiple choice test items each with a stem word (e.g., levied) and four alternatives (e.g., imposed, 
believer, requested, correlated), one of which is the synonym. An LSI analysis was performed on an encyclopedia 
represented by a 61;000 word by 30;473 article matrix A.For the synonym test they simply computed the 
similarity of the stem word to each alternative and picked the closest one as the synonym (for the above 
example imposed was chosen :70 imposed, :09 believed, :05 requested, -:03 correlated). Using this method 
LSI scored 64% correct, compared with 33% correct for word-overlap methods, and 64% correct for the average 
student taking the test. This is surpris­ingly good performance given that synonymy relationships are 
no dif­ferent than other associative relationships (e.g., algebra is quite near words like algebraic, 
topology, theorem, Cayley and quadratic, although none are synonyms). Matching People Instead of Documents 
In a couple of applications, LSI has been used to return the best matching people instead of documents. 
In these applications, people were represented by articles they had written. In one application [13], 
known as the Bellcore Advisor, a system was developed to .nd local experts relevant to users queries. 
A query was matched to the near­est documents and project descriptions and the authors organization was 
returned as the most relevant internal group. In another applica­tion [10], LSI was used to automate 
the assignment of reviewers to submitted conference papers. Several hundred reviewers were de­scribed 
by means of texts they had written, and this formed the basis of the LSI analysis. Hundreds of submitted 
papers were represented by their abstracts, and matched to the closest reviewers. These LSI similarities 
along with additional constraints to insure that each paper was reviewed ptimes and that each reviewer 
received no more than rpapers to review were used to assign papers to reviewers for a ma­jor human-computer 
interaction conference. Subsequent analyses suggested that these completely automatic assignments (which 
took less than 1 hour) were as good as those of human experts. Noisy Input Because LSI does not depend 
on literal keyword matching, it is espe­cially useful when the text input is noisy, as in OCR (Optical 
Character Recognition), open input, or spelling errors. If there are scanning er­rors and a word (Dumais) 
is misspelled (as Duniais), many of the other words in the document will be spelled correctly. If these 
correctly spelled context words also occur in documents which contained a cor­rectly spelled version 
of Dumais, then Dumais will probably be near Duniais in the k-dimensional space determined by Ak(see 
Equation 2orFigure1). Nielsen et al. in [23] used LSI to index a small collection of ab­stracts input 
by a commercially available pen machine in its standard recognizer mode. Even though the error rates 
were 8:8% at the word level, information retrieval performance using LSI was not disrupted (compared 
with the same uncorrupted texts). Kukich [19] used LSI for a related problem, spelling correction. In 
this application, the rows were unigrams and bigrams and the columns were correctly spelled words. An 
input word (correctly or incorrectly spelled) was broken down into its bigrams and trigrams, the query 
vector was located at the weighted vector sum of these elements, and the nearest word in LSI space was 
returned as the suggested correct spelling. 5.5 Summary of LSI Applications Word matching results in 
surprisingly poor retrieval. LSI can improve retrieval substantially by replacing individual words with 
a smaller number of more robust statistically derived indexing concepts. LSI is completely automatic 
and widely applicable, including to different languages. Furthermore, since both terms and documents 
are rep­resented in the same space, both queries and returned items can be either words or documents. 
This .exibility has led to a growing number of novel applications. 5.6 Open Computational/Statistical 
Issues There are a number of computational/statistical improvements that would make LSI even more useful, 
especially for large collections: computing the truncated SVD of extremely large sparse matri­ces (i.e., 
much larger than the usual 100,000 by 60,000 term by document matrix processed on RISC workstations with 
under 500 megabytes of RAM), performSVD-updating (see Section 4) in real-time for databases that change 
frequently, and ef.ciently comparing queries to documents (i.e., .nding near neighbors in high-dimension 
spaces). 5.7 Related Work A number of other researchers are using related linear algebra meth­ods for 
information retrieval and classi.cation work. Schutze [27] and Gallant [14] have used SVD and related 
dimension reduction ideas for word sense disambiguation and information retrieval work. Hull [17] and 
Yang and Chute [29] have used LSI/SVD as the .rst step in conjunction with statistical classi.cation 
(e.g. discriminant analysis). Using the LSI-derived dimensions effectively reduces the number of predictor 
variables for classi.cation. Wu et al. in [28] also used LSI/SVD to reduce the training set dimension 
for a neural network protein classi.cation system used in human genome research.  6 Acknowledgements 
The authors would like to thank Gavin O Brien at the National In­stitute of Standards and Technology 
(NIST) for his help with the SVD-updating software and algorithm design. This research was supported 
in part by the National Science Foundation under grant Nos. NSF-CDA-9115428 and NSF-ASC-92-03004. References 
[1] N.J.BELKIN AND W. B. CROFT, Information .ltering and infor­mation retrieval: Two sides of the same 
coin?, Communica­tions of the ACM, 35 (1992), pp. 29 38. [2] M. W. BERRY, Large scale singular value 
computations,In­ternational Journal of Supercomputer Applications, 6 (1992), pp. 13 49. [3] M.W.BERRY,S.T.DUMAIS, 
AND A. T. SHIPPY, A case study of latent semantic indexing, Tech. Rep. CS 92 159, University of Tennessee, 
Knoxville, TN, June 1992. [4] M.W.BERRY ET AL., SVDPACKC: Version 1.0 User s Guide, Tech. Rep. CS 93 
194, University of Tennessee, Knoxville, TN, October 1993. [5] S. DEERWESTER,S. DUMAIS,G. FURNAS,T. LANDAUER, 
AND R. HARSHMAN, Indexing by latent semantic analysis, Jour­nal of the American Society for Information 
Science, 41 (1990), pp. 391 407. [6] J. J. DONGARRA AND E. GROSSE, Distribution of mathematical software 
via electronic mail, Communications of the ACM, 30 (1987), pp. 403 407. [7] S. T. DUMAIS, Improving the 
retrieval of information from ex­ternal sources, Behavior Research Methods, Instruments, &#38; Computers, 
23 (1991), pp. 229 236. [8] , LSI meets TREC: A status report.,in The First Text REtrieval Conference 
(TREC1), D. Harman, ed., March 1993, pp. 137 152. National Institute of Standards and Technology Special 
Publication 500-207. [9] , Latent Semantic Indexing (LSI) and TREC-2.,in The Second Text REtrieval Conference 
(TREC2), D. Harman, ed., March 1994, pp. 105 116. National Institute of Standards and Technology Special 
Publication 500-215. [10] S. T. DUMAIS AND J. NIELSEN, Automating the assignment of submitted manuscripts 
to reviewers, in SIGIR 92: Proceed­ings of the 15th Annual International ACM SIGIR Conference on Research 
and Development in Information Retrieval, N. Belkin, P. Ingwersen, and A. M. Pejtersen, eds., Copenhagen, 
Denmark, June 1992, ACM Press, pp. 233 244. [11] P. W. FOLTZ, Using Latent Semantic Indexing for informa­tion 
.ltering, in Proceedings of the ACM Conference on Of.ce Information Systems (COIS), 1990, pp. 40 47. 
[12] P. W. FOLTZ AND S. T. DUMAIS, Personalized information deliv­ery: An analysis of information .ltering 
methods, Commu­nications of the ACM, 35 (1992), pp. 51 60. [13] G. W. FURNAS,S. DEERWESTER,S. T. DUMAIS,T. 
K.LANDAUER, R. A. HARSHMAN, L.A.STREETER, AND K. E. LOCHBAUM, Informa­tion retrieval using a singular 
value decomposition model of latent semantic structure, in Proceedings of SIGIR, 1988, pp. 465 480. [14] 
S. I. GALLANT, A practical approach for representing contexts and for performing word sense disambiguation 
using neural networks, Neural Computation, 3 (1991), pp. 293 309. [15] G. GOLUB AND C. V. LOAN, Matrix 
Computations, Johns-Hopkins, Baltimore, second ed., 1989. [16] G. GOLUB AND C. REINSCH, Handbook for 
automatic compu­tation II, linear algebra, Springer-Verlag, New York, 1971. [17] D. HULL, Improving text 
retrieval for the routing problem using Latent Semantic Indexing, in Proceedings of the Sev­enteenth 
Annual International ACM-SIGIR Conference, 1994, pp. 282 291. [18] Y. KANE-ESRIG,L. STREETER,S. T. DUMAIS,W. 
KEESE, AND G. CASELLA, The relevance density method for multi-topic queries in information retrieval, 
in Proceedings of the 23rd Symposium on the Interface, E. Keramidas, ed., 1991, pp. 407 410. [19] K. 
KUKICH, A comparison of some novel and traditional lex­ical distance metrics for for spelling correction, 
in Proceed­ings of INNC-90-Paris, 1990, pp. 309 313. [20] T. K. LANDAUER AND S. T. DUMAIS, Latent Semantic 
Analysis and the measurement of knowledge, in Proceedings of the First Educational Testing Service Conference 
on Applications of Natural Language Processing in Assessment and Education, 1994. To appear. [21] T. 
K. LANDAUER AND M. L. LITTMAN, Fully automatic cross­language document retrieval using latent semantic 
index­ing, in Proceedings of the Sixth Annual Conference of the UW Centre for the New Oxford English 
Dictionary and Text Research, UW Centre for the New OED and Text Research, Waterloo On­tario, 1990, pp. 
31 38. [22] L. MIRSKY, Symmetric gage functions and unitarilly invariant norms, Q. J. Math, 11 (1960), 
pp. 50 59. [23] J. NIELSEN,V.L. PHILLIPS, AND S. T. DUMAIS, Retrieving imper­fectly recognized handwritten 
notes, Behaviour and Informa­tion Technology, (1994). Submitted. [24] G. W. O BRIEN, Information Management 
Tools for Updating an SVD-Encoded Indexing Scheme, Master s thesis, The Uni­versity of Knoxville, Tennessee, 
Knoxville, TN, 1994. [25] G. SALTON, Automatic Information Organization and Re­trieval, McGraw Hill, 
New York, 1968. [26] G. SALTON AND C. BUCKLEY, Improving retrieval performance by relevance feedback, 
Journal of the American Society for Information Science, 41 (1990), pp. 288 297. [27] H. SCHUTZE, Dimensions 
of meaning, in Proceedings of Su­percomputing 92, 1992, pp. 787 796. [28] C. WU,M. BERRY,S. SHIVAKUMAR, 
AND J. MCLARTY, Neural networks for full-scale protein sequence classi.cation: Se­quence encoding with 
singular value decomposition,Ma­chine Learning, (1994). To appear. [29] Y. YANG AND C. G. CHUTE, An application 
of least squares .t mapping to text information retrieval, in Proceedings of the Sixteenth Annual International 
ACM-SIGIR Conference, 1993, pp. 281 290. [30] P. G. YOUNG, Cross-Language Information Retrieval Using 
Latent Semantic Indexing, Master s thesis, The University of Knoxville, Tennessee, Knoxville, TN, 1994. 
 Biographies Michael W. Berry received the B.S. degree in Mathematics from the University of Georgia 
at Athens in 1981, the M.S. degree in Applied Mathematics from North Carolina State University at Raleigh 
in 1983, and the Ph.D. degree in Computer Science from the University of Illi­nois at Urbana-Champaign 
in 1990. He was a computer scientist at the Center for Supercomputing Research and Development at the 
University of Illinois at Urbana-Champaign from 1985 to 1990, and an Assistant Professor of Computer 
and Information Sciences at the University of Alabama at Birmingham from 1990 to 1991. He is cur­rently 
an Assistant Professor of Computer Science at the University of Tennessee in Knoxville. His current interests 
include scienti.c computing, computational ecology, parallel numerical linear algebra, parallel and distributed 
computation, information retrieval, and perfor­mance evaluation. His electronic mail address is berry@cs.utk.edu 
and URL is http://www.cs.utk.edu/ berry. Susan T. Dumais is the Director of the Information Sciences 
Re­search Group at Bellcore. She received the B.A. degree in Mathe­matics and Psychology from Bates College 
in 1975, and the Ph.D. degree in cognitive psychology from Indiana University in 1979. She was a member 
of technical staff at AT&#38;T Bell Laboratories until 1984 when she joined Bellcore. Her research is 
concerned with the gen­eral problem of improving human-computer interaction. One area of particular interest 
is how people retrieve information from computer databases. Other research interests include developing 
and evaluat­ing interactive retrieval interfaces, individual differences, methods for combining navigation 
and search, spatial metaphors in information retrieval, and understanding the the impact of new technologies 
on productivity and quality of worklife. Her electronic mail address is std@bellcore.com. Todd A. Letsche 
received the B.A. degree in Computer Science and Mathematics from Wartburg College, Waverly, Iowa in 
1992. He is currently pursuing an M.S. in Computer Science at the University of Tennessee in Knoxville. 
Before attending graduate school, he com­pleted internships with IBM and Cray Research, Inc. His current 
inter­ests include information retrieval, parallel and high-performance com­puting, and performance evaluation. 
His electronic mail address is letsche@cs.utk.edu and URL is http://www.cs.utk.edu/ letsche. Copyright 
&#38;#169; 1995 by the Association for Computing Machinery, Inc. (ACM). Permission to make digital or 
hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that new copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions 
from Publications Dept, ACM Inc., via fax at +1 (212) 869-0481, or via email at permissions@acm.org. 
 
			