
 E.cient reductions among lattice problems* Daniele Micciancio Abstract We give various deterministic 
polynomial time reductions among approximation problems on point lattices. Our reductions are both e.cient 
and robust, in the sense that they preserve the rank of the lattice and approximation factor achieved. 
Our main result shows that for any . = 1, approximating all the successive minima of a lattice (and, 
in particular, approximately solving the Shortest Independent Vectors Problem, SIVP. ) within a factor 
. reduces under deterministic polynomial time rank-preserving reductions to approximating the Closest 
Vector Problem (CVP)within the same factor .. This solves an open problem posed by Bl¨omer in (ICALP 
2000). As an application, we obtain faster algorithms for the exact solution of SIVP that run in time 
n! · s O(1) (where n is the rank of the lattice, and s the size of the input,) improving on the best 
previously known solution of Bl¨omer (ICALP 2000) by a factor 3n . We also show that SIVP, CVP and many 
other lattice problems are equivalent in their exact version under deterministic polynomial time rank-preserving 
reductions. 1 Introduction A lattice L is the set of intersection points of an in.nite n-dimensional 
grid. The successive minima .i(L)(for i=1,...,n) are among the most fundamental parame­ters associated 
to a lattice, and are de.ned as the small­est values .i(L) such that the sphere of radius .i(L) centered 
around the origin contains at least ilinearly in­dependent lattice vectors. Lattice approximation prob­lems 
have been widely investigated since the discovery of the basis reduction algorithm of Lenstra, Lenstra 
and Lov´asz [22], initially for their applications in cryptanaly­sis and combinatorial optimization [11, 
19, 28] and more recently as a potential basis for cryptographic function design [2, 15, 3, 12, 24, 29, 
27, 25]. The most important and widely studied lattice approximation problems (for approximation factor 
. = 1) are: *This research was supported in part by NSF grant CCF­0634909. Any opinions, .ndings, and 
conclusions or recommenda­tions expressed in this material are those of the author and do not necessarily 
re.ect the views of the National Science Foundation. Computer Science and Engineering Department, University 
of California at San Diego, La Jolla, CA 92093, USA. Email: daniele@cs.ucsd.edu. the Shortest Vector 
Problem (SVP): given a lat­tice, .nd an approximately shortest nonzero lattice vector, i.e., a vector 
of length at most .· .1,  the Closest Vector Problem (CVP): given a lattice and a target point, .nd 
a lattice point approxi­mately closest to the target, i.e., a lattice point at a distance from the target 
that is at most . times the distance of the closest lattice point, and  the Shortest Independent Vectors 
Problem (SIVP): given a lattice, .nd a maximal set of approximately shortest linearly independent lattice 
vectors, i.e., n linearly independent vectors (where n is the rank of the lattice) of length at most 
..n.  There is a wide gap between the small (constant or sub­polynomial in the lattice rank) approximation 
factors for which these problems are known to be intractable [1, 23, 21, 10, 14], and the the large (exponential) 
fac­tors for which the problems are known to be solvable in polynomial time [22, 6, 30, 4]. Given our 
limited un­derstanding of the complexity of lattice approximation problems, it is natural to ask how 
these problems re­late to each other for arbitrary approximation factors. Are these problems computationally 
equivalent? Are some harder than others? More speci.cally, we ask if there are polynomial time reductions 
among these prob­lems that work for any approximation factor, and pre­serve both the rank of the lattice 
and the quality of approximation. The importance of preserving the rank of the lattice in reductions 
among lattice problems can­not be overemphasized. For example, since the best currently known solution 
to most lattice problems runs in time exponential in the rank, even doubling the rank would result in 
an exponential slow down in any algo­rithmic application of the reduction. To date, the only known polynomial 
time reduction that preserves both the rank and approximation is the one from SVP to CVP given by Goldreich, 
Micciancio, Safra and Seifert in [16]. The only other similar result we are aware of is a non-deterministic 
polynomial time reduction from SIVP. to CVP. given by Guruswami, Micciancio and Regev [17] for the purpose 
of showing that SIVP. is not NP-hard, under standard complexity assumptions. In this paper we give various 
dimension preserv­ing deterministic polynomial time reductions between lattice approximation problems. 
The main result is a reduction from SIVP to CVP that preserves both the approximation factor and rank 
of the lattice. In fact, we prove something stronger: we give a reduction from the problem of .nding 
linearly independent lattice vectors achieving all successive minima of the lattice to solving CVP. Speci.cally, 
we consider the following problem:  the successive minima problem (SMP): given a lattice, .nd linearly 
independent vectors v1,...,vn (where nis the rank of the lattice) of length at most IviI=..i for all 
i=1,...,n. This is a classic problem in the mathematical study of lattices that subsumes both SVP and 
SIVP as special cases. (Any solution to SMP. is also a solution to SIVP. , and the .rst vector in it 
is a solution to SVP..) So, our result subsumes the reduction from SVP. to CVP. of [16] as a special 
case, and improves on the non­deterministic polynomial time reduction from SIVP to CVP of [17]. Technically, 
we reduce SIVP to CVP by introduc­ing a simple generalization of SVP that bears strong similarities with 
both SVP and CVP. In the short­est vector problem, given a lattice basis B,wewantto minimize the length 
IBxI(where x is an integer vec­tor) subject to the constraint that xi = 0 for some i.We consider a variant 
SVP' of the shortest vector problem where the index i is also given as part of the input, and the goal 
is to minimize IBxIsubject to the con­straint that xi The shortest = 0 for the given index i. vector 
in a lattice B can be easily found by solving all SVP' instances (B,i)for i =1,...,n. One can also think 
of SVP' as a variant of CVP, where the target vector (bi) can be used multiple times: given a lattice 
[b1,...,bi-1,bi+1,...,bn] and target t = bi, .nd the lattice point closest to a nonzero multiple of the 
target. We give simple deterministic polynomial time re­ductions from SMP. to SVP'and from SVP'to . . 
CVP. . Our reductions preserve both the approxima­tion factor and lattice rank. Moreover, the reductions 
work for any approximation factor . and norm. As a re­sult we get the claimed reduction from SIVP. to 
CVP. . An immediate application of our reduction is an algo­rithm to solve SIVP exactly in time n!sO(1) 
(where n is the rank of the lattice and s is the size of the input) by reducing it to CVP and then using 
the CVP algorithm of Bl¨omer [8]. The best previously known algorithm for the exact solution of SIVP 
(also given in [8]) had running time 3n ·n! ·sO(1). Next, we consider two other lattice problems re­cently 
introduced by Bl¨omer and Naewe to design algo­rithms for the deterministic (resp. randomized) exact 
(resp. approximate) solution of SIVP. Speci.cally, we consider the Generalized Closest Vector Problem 
(GCVP), introduced by Bl¨omer in [8] for the design of deterministic algorithms that solve SIVP exactly, 
and  the Subspace Avoiding Problem (SAP), a special case of GCVP recently introduced by Bl¨omer and 
Naewe [9] to design and analyze randomized lattice approximation algorithms for SIVP and CVP.  Using 
the same simple techniques from our main reduc­tion, we are able to show that GCVP. and SAP. are equivalent 
to CVP. and SVP'respectively, under poly­ . nomial time reductions that preserve both the approxi­mation 
factor and rank of the lattice. Our equivalence results hold for any norm and approximation factor, and 
answer in the a.rmative the open questions (posed by Bl¨omer in [8]) about the polynomial time reducibility 
of SIVP and GCVP to CVP. Our results have interesting implications about the exact solvability of lattice 
problems (in the Euclidean norm) under randomized reductions. The fastest known deterministic algorithms 
for the solution of all lattice problems considered in this paper have running time nO(n). However, using 
randomization, Ajtai, Kumar and Sivakumar [4] were able to improve the running time for the exact solution 
of SVP to a simple exponen­tial function 2O(n). Following [4], there have been var­ious attempts to generalize 
the randomized techniques of [4] to the solution of other lattice problems, most notably CVP (as done 
in [5]) and SIVP (as done in [9]). Albeit reducing the running time to 2O(n),none of these attempts have 
led so far to an algorithm that solves CVP or SIVP (or any other lattice problems) ex­actly: [5, 9] only 
give 2O(n)-time randomized algorithms to solve CVP. and SIVP. within a constant factor . =1+ E, for arbitrary 
small E> 0. Our results imply that, in their exact versions (and at least in the Eu­clidean norm,) all 
lattice problems considered in this pa­per SIVP,SMP,CVP,SVP',SAP,GCVP (with the only exception of SVP) 
are equivalent under determin­istic polynomial-time rank-preserving reductions. So, exact solutions can 
be found in exponential time 2O(n) either for all or for none of them. The shortest vector problem SVP 
reduces to any of them, but no reduction is known in the opposite direction. So, this perhaps ex­plains 
why generalizing the randomized algorithm of [4] to other lattice problems has, so far, led only to approx­imation 
algorithms. The equivalence of lattice problems in the exact version is obtained by completing the sequence 
of ap­proximation preserving reductions SIVP. =SMP. =  SVP. SIVP. SVP. SIVP. SMP. Figure 2: Relations 
among lattice problems including the results in this paper. Arrows indicate polynomial time reductions 
that preserve the lattice rank and ap­proximation factor, and boxes enclose classes of equiva­lent problems 
under the same kind of reductions. SVP1  Figure 3: Relations among lattice problems in their ex­act 
version (in the Euclidean norm,) including the re­sults in this paper. Arrows indicate polynomial time 
reductions that preserve the lattice rank and approxi­mation factor, and boxes enclose classes of equivalent 
problems under the same kind of reductions. GCVP. Figure 1: Previously known relations among lattice 
problems. Dotted arrows are trivial relations, where one problem is a special case of the other. Arrows 
indicate polynomial time reductions that preserve the lattice rank and approximation factor. SVP ' = 
SAP. = GCVP. = CVP., with a new re­ . duction from CVP to SIVP for the exact version of those problems. 
For simplicity, the last reduction is presented only for the Euclidean norm, though, with some care, 
the result can be generalized to many other norms. A polynomial time reduction from exact CVP to exact 
SIVP (in the Euclidean norm) had been previ­ously given by Bl¨omer and Seifert [10]. However, their reduction 
increases the rank of the lattice by 1. This has only a minor impact on the complexity of the prob­lems, 
so for all practical purposes, the reduction of [10] can already be taken as a proof that exact CVP is 
not harder than exact SIVP. Still, the question remains if in order to reduce CVP to SIVP, it is necessary 
to in­crease the lattice rank. Here we show that this is not the case, and that the reduction of [10] 
can be modi.ed to make it rank preserving. On the technical side, our modi.cation of the reduction of 
[10] involves a new primal/dual dimension reduction method that might be of independent interest. Giving 
an approximation preserving reduction from CVP. to SIVP. would show that all lattice problems (except 
SVP) are equivalent not only in their exact version, but also in their approximate version, and it is 
left as an open problem. Known relations among lattice problems, prior to this paper, and including the 
new reductions, are summarized in Figures 1, 2 and 3. Outline In § 2 we give some background about lattices. 
The main results of this paper are presented in § 3 where we reduce SIVP and SMP to CVP. In § 4 we present 
additional results establishing the computational equivalence of various lattice problems. §5 concludes 
with a discussion of the problems left open in this paper.  2 Preliminaries In this section we give 
some general background about lattices, and formally de.ne all lattice problems studied in this paper. 
A norm is a function x xI.R+ such .Ithat IxI=0 with equality if and only if x = 0, Ia·xI= |a|·IxI,and 
Ix + yI=IxI+ IyI. Any norm induces a corresponding distance function dist(x,y)= Ix -yI. Lattice problems 
are typically formulated and studied in J the Euclidean norm IxI= i x2, but all de.nitions and most results 
in this paper hold for any norm, subject to some basic computability requirements. Speci.cally, we assume 
that given two vectors x and y, one can e.ciently determine if IxI=IyI  given a vector x and a linear 
subspace S,one can e.ciently determine (a lower bound to) the distance between x and S.  Throughout 
the paper, we refer to norms satisfying these two conditions as e.ciently computable.The distance function 
is extended to sets in the customary way: dist(x,S) = miny.S dist(x,y). The linear space spanned by a 
set of n vectors S is denoted span(S)= {i xisi : xi .R for 1 =i=n}. We now describe some basic de.nitions 
related to lattices. For a more in-depth discussion, see [26]. A lattice is the set of all integer combinations 
{ n } L xibi: xi .Z for 1 =i=n i=1 of n linearly independent vectors b1,...,bn in Rm . The number n is 
called the rank of the lattice, and the set of vectors b1,...,bn is called a basis. A basis can be represented 
by the matrix B =[b1,...,bn] . Rm×n having the basis vectors as columns. The lattice generated by B is 
denoted L(B). Notice that L(B)= {Bx: x . Zn}, where Bx is the usual matrix-vector multiplication. Two 
bases B,B generate the same L( lattice L(B)= B) if and only if B = BU for some unimodular matrix U. (A 
unimodular matrix is a square matrix with integer entries and determinant ±1.) For computational purposes, 
it is usually assumed that all lattice vectors have integers (or more generally rational) entries, so 
that the lattice can be represented by an integer (resp. rational) matrix B .Zm×n . The dual of a lattice 
. is the set .* = {x .span(.): .y ...(x,y).Z} of all vectors that have integer scalar product ((x,y)= 
 i xiyi) with all lattice vectors. The dual of a lattice is a lattice, and if . = L(B) is the lattice 
generated by basis B, then B* = B(BT B)-1 is a basis for the dual lattice, where BT is the transpose 
of B. A sub-lattice of L(B) is a lattice L(S) such that L(S) .L(B). The minimum distance of a lattice 
., denoted .1(.), is the minimum distance between any two dis­tinct lattice points, and equals the length 
of the shortest nonzero lattice vector: .1(.) = min{dist(x,y): x = y ..} = min{IxI: x .. \{0}}. This 
de.nition can be generalized to de.ne the ith successive minimum as the smallest .i such that .iB= {x: 
IxI= .i} contains i linearly independent lattice points: .i(.) = min{r: dim(span(. nrB)) =i}. Another 
important constant associated to a lattice is the covering radius .(.), de.ned as .(.) = max {dist(x,.)}. 
x.span(.) We often abuse notation and write .1(B) instead of .1(L(B)) and similarly for other lattice 
parameters. The following are among the most important and widely studied computational problems on point 
lat­tices. Definition 2.1. (Shortest Vector Problem (SVP)) Given a lattice B . Zm×n , .nd a nonzero lattice 
vector v .L(B) \{0}such that IvI=..1(B). Definition 2.2. (Shortest Independent Vectors Problem (SIVP)) 
Given a lattice B .Zm×n, .nd n linearly independent lattice vectors v1,...,vn .L(B) such that IviI=..n(B) 
for all i=1,...,n. Definition 2.3. (Closest Vector Problem (CVP)) Given a lattice B .Zm×n and a target 
vector t . Rm , .nd a lattice vector v .L(B) such that dist(v,t) =.dist(t,L(B)). SVP and SIVP are both 
special cases of the following classical mathematical problem. Definition 2.4. (Successive Minima Problem 
(SMP)) Given a lattice B . Zm×n , .nd n linearly independent lattice vectors v1,...,vn .L(B) such that 
IviI=..i(B) for all i=1,...,n.  Clearly, SVP. and SIVP. reduce to SMP..We now de.ne three non-standard 
problems on lattices that will be used in this paper. The .rst is a simple variant of SVP. In the standard 
version of SVP, one is asked to minimize the norm IBxI subject to the condition that xi = 0 for some 
i. We consider a variant where the index i is given as part of the input. Definition 2.5. (SVP ' ) Given 
a lattice B . Zm×n and an index i .{1,...,n}, .nd a lattice vector Bx with xi = 0 such that IBxI=.min{IBxI: 
xi =0}. Clearly, SVP reduces to SVP ' in polynomial time: on input a lattice B, one can solve all SVP 
' instances (B,i)for i=1,...,n, and select the best answer. This problem will be instrumental in reducing 
SIVP and SMP to CVP. The next problem was recently introduced in [9] to design randomized sampling algorithms 
for the approx­imate solution of SIVP and other lattice problems. Definition 2.6. (Subspace Avoiding 
Problem (SAP)) Given a lattice B . Zm×n and a linear subspace S, .nd a lattice vector v .L(B) \S such 
that IvI=.dist(0,L(B) \S). Clearly, SVP is a special case of SAP, where S = {0}.Also SVP ' is a special 
case of SAP, where S = {x: xi =0}for some i. In turns, SAP is a special case of the following problem 
from [8]. Definition 2.7. (Generalized Closest Vector Problem (GCVP)) Given a lattice B . Zm×n,a target 
vector t . Rn and an a.ne subspace S, .nd a lattice vector v .L(B) \S such that dist(v,t) = .dist(t,L(B) 
\S). It is easy to see that both CVP and SAP are spe­cial cases of GCVP, where S = Ø and t = 0 respec­tively. 
There are also distance estimation versions of these problems, where, for example, instead of .nd­ing 
a short nonzero vector in a lattice, it is enough to produce the approximate value of .1. These problems 
are usually formulated as decision (or promise) prob­lems. For example, GapSVP. is the problem of dis­tinguishing 
pairs (B,d) where .1(B) = d from pairs where .1(B) >.d. It is easy to see that this promise problem is 
equivalent to computing an approximation .1 .[.1,..1] of the length of the shortest lattice vector. 
Similarly, GapSIVP. and GapCVP. are the promise problems associated to the task of estimating .n(B)and 
dist(t,L(B)) within a factor .. In the exact case (i.e., when . = 1) and at least in the Euclidean norm, 
the dis­tance estimation problems GapSVP1 and GapCVP1 are equivalent (under polynomial time reductions) 
to their corresponding search problems SVP1 and CVP1. (See [20] and [26, Chapter 3].) It follows from 
the re­sults in this paper that the same is true for GapSIVP1 and SIVP1. However, in the approximate 
case .> 1, no such reduction is known. 3 Reducing SIVP and SMP to CVP We show that the problem SVP ' 
is at least as hard as . SIVP. (or, even SMP.), and no harder than CVP.. We point out that the main results 
presented in this section can also be obtained combining the reductions in §4 with trivial reductions 
and previously known results. We present the reductions in this section .rst in order to give a somehow 
simpler and more direct reduction from SMP to CVP. The reader not interested in the complexity of less 
standard problems (e.g., SAP, SVP ' and GCVP) can read this section and then skip most of §4. The reduction 
from SMP. to SVP ' is basedonthe . following simple lemma, which will be useful also later in the paper. 
Lemma 3.1. There is a polynomial time algorithm that on input a lattice basis B . Qm×n and a linear subspace 
S, outputs a new basis B for L(B) such that L(b 1,...,b d)= S nL(B),where d is the dimension of Snspan(B). 
Proof. Assume S is represented by a system of linear equations, i.e., S = {x: Hx = 0}for some given matrix 
H .Qh×m.(If S = span(G) is given as a generating matrix G, then H can be e.ciently computed using linear 
algebra as the orthogonal complement of G.) Consider the h×nmatrix HB, and extend it to a square n×n 
matrix HBC =O by adding n -h identically zero rows. Any square integer matrix C can be put into Smith 
Normal Form S = VCU, where S is a diagonal matrix, with non-negative diagonal entries such that si+1,i+1 
divides si,i for i = 1,...,n-1. In particular, there is a d such that the .rst d diagonal elements of 
S are zero, and the remaining n-d diagonal elements are nonzero.  U and V are unimodular matrices. 
 (See [13] for more information on the Smith Normal Form, as well as algorithms to compute the matrices 
U,V and S in polynomial time.) The output of the algorithm is B = BU. Clearly, B is a basis for L(B) 
because matrix U is unimodular.  Now, consider the intersection SnL(B)= SnL(B )= { Bx = 0}, Bx: x .Zn 
,H and let Q the n ×h matrix consisting of the .rst h columns of V. Since the columns of Q are linearly 
independent, the condition H 0 is equivalent to QH 0.But Bx = Bx = QHB = QHBU = VCU = Bx = Sx = 0 S.So, 
QH if and only if xi = 0 for all i>d, and the set SnL(B) equals L(b 1,...,b d). Finally, since B is a 
basis, the vectors b1,...,bd are linearly independent, and the dimension of SnL(B)is d. D We can now 
give the .rst interesting reduction. Theorem 3.1. For any approximation factor ., there is a polynomial 
time reduction from SMP. to SVP ' . . The reduction preserves both the rank and dimension of the input 
lattice, and works for any e.ciently com­putable norm. Moreover, on input a basis B, the reduc­tion makes 
only oracle calls of type SVP ' (B ,i) where . L(B )= L(B). Proof. Let B be the input lattice. The reduction 
computes linearly independent lattice vectors si .L(B) of length at most IsiI= ..i(B) iteratively, one 
at a time. Assume we have found linearly independent lattice vectors s1,...,sk such that IsiI= ..i for 
i =1,...,k, and let S = span(s1,...,sk). We want to .nd one more lattice vector sk+1 ./S such that Isk+1I=..k+1. 
Using Lemma 3.1 we can .nd a lattice basis B such that L(b 1,...,b d)= S nL(B). Notice that, since s1,...,sk 
.L(B) are linearly independent, dim(L(B) n S)=dim(S)= k. So, it must be d = k,and L(b 1,...,b k)= S nL(B). 
Make n - k calls to SVP '(B ,i)for i = k +1,...,n, and let sk+1 = SVP '(B ,i) be the shortest vector 
returned by the oracle. Certainly, sk+1 ./S because it uses b i a nonzero number of times. We want to 
prove that Isk+1I= ..k+1. By de.nition of .k+1, there exist k + 1 linearly independent vectors v1,...,vk+1 
.L(B)= L(B ) such that IviI=.k+1. Since they are linearly independent, they cannot all belong to S.Let 
vj = Bx ./S. It must be xi = 0 for some i>k, for otherwise we would have Bx .S. Consider the oracle 
call SVP '(B ,i) where i is an index such that xi = 0. Clearly, the optimal solution to the SVP ' instance 
( BxI= B,i) has length at most I .k+1. So, the oracle returns a vector sk+1 = SVP '(B ,i) of length at 
most Isk+1I=.I D BxI=..k+1. Next, we reduce SVP ' to CVP. The idea under­lying the reduction is that 
SVP ' can be regarded as a variant of CVP where the target vector can be used multiple times. Theorem 
3.2. For any approximation factor ., there is a polynomial time reduction from SVP ' to CVP. . . The 
reduction preserves both the rank and dimension of the input lattice, and works for any e.ciently com­putable 
norm. Moreover, on input (B,i), all oracle calls made by the reduction are of type (B ,t) where L(B ) 
.L(B) and t .L(B). Proof. Let (B,i)bean SVP ' input instance, and as­sume without loss of generality 
that i = n equals the rank of B =[b1,...,bn]. Let Ba be an opti­mal solution to the input problem, i.e., 
a lattice vec­tor Ba with an = 0 such that IBaI is minimized. For j =0,...,llog2 AJ(where A is a su.ciently 
large bound to be determined) call the CVP. oracle on input (B(j),t(j)) where B(j) =[b1,...,bn-1,2j+1bn] 
is the matrix ob­tained multiplying the nth column of B by 2j+1 , and t(j) =2j bn. Let B(j)x(j) be the 
solution returned by the CVP oracle on input (B(j),t(j)). The output of the reduction is the shortest 
among all vectors B(j)x(j) -t(j). Notice that any such vector uses bn a nonzero 2j+1 (j)(j) x-2j =2j 
·(2x-1) =0 nn number of times. So, the output is a feasible solution to SVP ' instance (B,n). We need 
to prove that the output solution is within a factor . from the shortest, i.e., IB(j)x(j) -t(j)I=.IBaIfor 
some j. Let j be the highest power of 2 such that 2j divides an. Since an is nonzero, j is well de.ned 
and an = 2j ·(2a -1) for some integer a. Consider the CVP instance (B(j),t(j)). The lattice L(B(j)) contains 
a vector B(j)a ' (where a ' is obtained from a replacing the nth entry an with a) at distance IBaI from 
t(j). So, the CVP oracle must .nd a lattice vector such that IB(j)x(j) -t(j)Iis at most .IBaIas desired. 
It remains to determine a suitable bound A. Since |an| =2j ·|2a -1|= 2j, it is enough to set A to any 
number at least as big as |an|.Let a be (a lower bound to) the distance between vector bn and the linear 
subspace S spanned by b1,...,bn-1.(For the Euclidean norm, this is the length of the orthogonalized vector 
b* , i.e., the component of bn orthogonal to n b1,...,bn-1.) Since (for any a) the vector Ba has norm 
at least a·|an|,and IBaI=IbnIby the optimality of a, it must be |an|=IbnI/a andwecanset A = IbnI/a. 
 Since A can be computed in polynomial time, log2 A is bounded by a polynomial in the input length, and 
the reduction runs in polynomial time. D Combining the two theorems we get the following corollary. Corollary 
3.1. There is a dimension, rank and ap­proximation preserving reduction from SMP. (and therefore SIVP.)to 
CVP. . The reduction works for any computable norm and approximation factor, and it has the additional 
property that on input a lattice B, it makes only oracle calls of type (B ,t) where L(B ) . L(B) and 
t .L(B). 4 Equivalence results Now we turn to the proof that several problems on lattices are equivalent 
from a computational point of view under polynomial time rank-preserving reductions. We start by considering 
the subspace avoiding problem SAP of [9], where, on input a lattice B and linear subspace S, the goal 
is to .nd the shortest lattice vector outside S. Notice that our simple variant SVP ' of the shortest 
vector problem is a special case of SAP, where the subspace to avoid is S = {x: xi =0}. So, there is 
a trivial reduction from SVP ' . to SAP. . The following corollary is an immediate application of Lemma 
3.1, and shows that general SAP. instances are not harder than SVP ' . Corollary 4.1. For any e.ciently 
computable norm and approximation factor ., the problems SAP. and SVP ' are equivalent under deterministic 
polynomial . time rank-preserving reductions. Proof. We already observed that SVP ' is a special case 
of SAP, so there is a trivial reduction from SVP ' to . SAP.. We reduce SAP. to SVP ' . . Let (B,S)be 
an instance of SAP.. We invoke Lemma 3.1 on input (B,S) to get an equivalent SAP. instance ( B ,S) where 
L(B )= L(B)and S = span(b 1,...,b d). Next, we call the SVP ' oracle on input ( B ,i)for i= d+1,...,n,and 
. select the shortest of the vectors returned by the oracle calls. Clearly, all oracle answers belong 
to L(B) \S because they use some b i ./S (for i>d) a nonzero number of times. Moreover, the optimal solution 
to the SAP. input instance must use one of the vectors b i (for i>d) a nonzero number of times, so the 
corresponding call SVP ' (B ,i) returns a vector of length at most . . times longer than this optimal 
solution. D The equivalence between CVP. and its generaliza­tion GCVP. introduced by [8] is less trivial, 
but it can still be easily proved by adapting our reduction from SVP ' to CVP.. . Theorem 4.1. For any 
e.ciently computable norm and approximation factor ., the problems CVP. and GCVP. are equivalent under 
deterministic polynomial time rank-preserving reductions. Proof. Clearly, CVP. is a special case of GCVP. 
(with S = Ø), so there is a trivial reduction from CVP. to GCVP. . We give a reduction in the opposite 
direction. Let (B,t,S)bea GCVP. instance, where B .Zm×n is an m-dimensional lattice, S .Rm is an a.ne 
subspace, and t .Qm is a target point. First of all, we determine if S intersects the lattice L(B), and 
if so, we .nd a lattice point in S. Both tasks can be e.ciently performed using linear algebra. If S 
contains no lattice point, then we can immediately map it to CVP. instance (B,t). So, assume S contains 
a lattice point v .S nL(B). We can also assume, without loss of generality, that v is the origin, for, 
otherwise, we can consider an equivalent GCVP. instance (B,S-v,t-v), where S -v certainly contains the 
origin. To sum up, so far we have reduced general GCVP. instances to instances where S is a linear subspace. 
Next, we use Lemma 3.1 to .nd a basis B for L(B) such that S = span([b 1,...,b k]). Using this basis 
B , we de.ne a collection of CVP. instances as follows. For any i= k+1,...,n,and j =0 ...,llog2 AJ(where 
A is a su.ciently large bound which can be determined as in the proof of Theorem 3.2), de.ne bi-1,2j+1 
 B j =[b 1,..., ·b i,b i+1,...,b n], and  tj  i = t -2j ·bi. i Call the CVP. oracle on all instances 
( B ij ,tj )toget i solutions zj .L(Bj ). The output of the reduction is ii the vector zj +2j ·b i closest 
to the target t. This i completes the description of the reduction from GCVP to CVP. We need to show 
that the reduction is correct. j Notice that the output of the reduction z+2j ·b i i j Bj is a lattice 
point because z.L( ) .L(B)and ii j b i .L(B). Moreover, z+2j ·b i ./S because it i uses the basis vector 
b i ./S a nonzero (in fact, in 2j(2Z + 1)) number of times. So, the output of the reduction is a feasible 
solution to GCVP. instance (B,t,S). All we need to do is to bound the quality of the approximation achieved 
by the reduction, i.e., show that for some i and j, the distance of the vector zj +2j ·b i from the target 
t is within a factor . from i the optimal. Let Ba be the optimal solution to the GCVP. instance (B,t,S) 
expressed in terms of the basis B . We know that [ak+1,...,an] is nonzero because Ba ./S .L([b 1,...,bk]). 
In particular, there is an index i>k such that ai = 0. Let 2j be the highest  power of 2 that divides 
ai, and consider CVP. instance (B ji ,tj ). Let a ' be the vector obtained by replacing the i ' ith coordinate 
of a by a =((ai/2j )-1)/2. The distance i of tj from the lattice L(B j)isatmost i i dist(tji ,L(B j)) 
i =Itj -B ija ' I i L It -2j b hah -2j+1 ' = ·bi -·biaiI h =i L b hahI= It - = It - BaI. h So, the CVP. 
oracle will return a lattice vector zj = i B ji x within distance . ·It - i , BaI from the target tj 
and the corresponding vector output by the reduction satis.es I(z+2j ·b i) -tI= IzBaI j j -tjI=.It - 
i ii as required. D We conclude this section showing that all lattice problems considered in this paper, 
with the only excep­tion of SVP, are equivalent in their exact version un­der deterministic polynomial-time 
rank-preserving re­ductions. The result builds on a simple (rank increas­ing) reduction from CVP to SIVP 
in the Euclidean norm given in [10]. Our result can be generalized to other norms us­ing the fact that 
the (Banach-Mazur) distance of any n-dimensional norm from the Euclidean norm £2 is bounded by a polynomial 
in n. The most delicate part of the adaptation to other norms is the generalization of the reduction 
in [10] to norms other than £2.For simplicity (and in order to use the reduction from [10]) we present 
the result only for the special case of the Euclidean norm. Corollary 4.2. SIVP, SMP, SVP ' and CVP in 
the Euclidean norm are equivalent in their exact version under polynomial time rank-preserving reductions, 
and SVP reduces to any of them. Proof. We have already given rank-preserving reduc­tions SIVP. =SMP. 
=SVP ' . =SAP. =GCVP. = CVP. . In order to prove the corollary we need to give a rank-preserving reduction 
from CVP to SIVP.A re­duction between these two problems is given by Bl¨omer and Seifert in [10]. However, 
their reduction maps rank n instances of CVP to rank (n+ 1) instances of SIVP. Here we use the transference 
theorems of Banaszczyk [7] to modify their reduction into a rank preserving one. This involves a primal/dual 
rank reduction method that might be of independent interest. The idea is to use short vectors in the 
input lattice to .nd short vectors in the dual, and then use the short dual vectors to .nd closest vectors 
in the original lattice. Let (B,t)bearank nCVP instance, and w .L(B) a lattice vector closest to t. We 
want to .nd a lattice point at distance Iw -tIfrom t, making a polynomial number of calls to an oracle 
that solves SIVP problems of rank n. We .rst use the SIVP oracle to .nd a nonzero lattice vector of length 
at most IxI= .n(B). Vector x .L(B) partitions the dual lattice L(B*)= i.Z Si into subsets Si = {y .L(B*): 
(y,x)= i} lying on rank (n-1) hyperplanes at distance 1/IxIfrom each other. By the transference theorems 
we know that .1(B*) =n/.n(B) =n/IxI. So the shortest nonzero vector in the dual lattice belongs to subset 
Si for some i .{-n,...,n}. In fact, since any lattice is symmetric about the origin, it is enough to 
consider i.{0,...,n}. For each iin this range, we can .nd the shortest nonzero vector in Si as follows: 
 For i =0, this is a SVP instance of rank n -1, which can be solved by reducing it to CVP using the rank 
preserving reduction of Goldreich et al. [16], and then reducing the resulting CVP instance (which has 
rank n-1) to SIVP using the reduction of [10].  For i = 0, we .rst .nd an arbitrary lattice point v 
. Si, and the point w in the hyperplane Hi = {y . span(B): (y,x) = i} closest to the origin. Next, we 
.nd the lattice point u . S0 closest to the target w -v.This is a CVP instance of rank (n-1) and it can 
be solved by reduction to SIVP as in the i= 0 case. The point u + v belongs to Si and it is the point 
in Si of smallest norm.  Let d be the smallest of all the dual vectors found for i .{0,...,n}. Since 
the shortest vector in the dual lattice belongs to one of these sets Si,we have IdI=.1(B*). This dual 
vector partitions the original lattice L(B)= Ti into subsets i.Z Ti = {y .L(B): (y,d)= i} lying on rank 
(n-1) hyperplanes at distance 1/IdI= 1/.1(B*) from each other. We observe that the lattice vector w closest 
to t must belong to Ti for an index i.(t,d)±n because |(d,w)-(d,t)| = |(d,w -t)|=IdI·Iw -tI * = . (B).(B) 
1  which, by the transference theorem, is at most n.For each i, we can .nd the lattice vector in Ti 
closest to the target by solving a CVP problem of rank n - 1. Each of these lower dimensional instances 
can be reduced to SIVP (of rank n) using the reduction from [10]. D 5 Open Problems As Figure 2 shows, 
we have now a fairly good picture of the relations among the various lattice problems studied in this 
paper. The .rst question left open in this paper is to determine the relation between SVP. and SIVP.. 
Open Problem 1. Are there deterministic polynomial time reductions between SVP. and SIVP. that preserve 
both the rank of the lattice and quality of approximation? Are the two problems equivalent? Incomparable? 
Or one strictly harder than the other? We remark that for the exact (. = 1) version of the problems in 
the Euclidean norm, there is a reduc­tion from SVP to SIVP. (See Figure 3.) Also, the transference theorems 
of [7] immediately give a polyno­mial time deterministic rank-preserving reduction from GapSVP.n to GapSIVP. 
for any factor .. This sup­ports the conjecture that SVP. is not harder than SIVP.. Proving a reduction 
in the opposite direction (from SIVP to SVP) appears harder (even just for the exact version of the problems) 
as it would imply the NP-hardness of SVP under deterministic polyno­mial time reductions. To date, all 
known NP-hardness proofs for SVP [1, 23, 21, 18] (with the only exception of a deterministic reduction 
of [23] based on a plausi­ble, but unproven, number theoretic conjecture) are ran­domized. Moreover, 
they produce SVP instances with much higher rank than the size of the original problem. So, even .nding 
a randomized rank-preserving reduc­tion from SIVP to SVP would be an interesting result. The other main 
open question is whether the hier­archy of problems shown in Figure 2 is strict, or some of the problems 
can be proved equivalent. In the case of the exact version in the Euclidean norm, Figure 3 shows that 
all problems, with the exception of SVP, collapse to the same class. This was proved giving a rank-preserving 
reduction from CVP to SIVP. Can the same be done for approximate versions of the problems? Open Problem 
2. Is there a deterministic polynomial time reduction from CVP. to SIVP. that preserves the rank of the 
lattice and approximation factor? Answering the above question would show that all problems except SVP. 
are equivalent, and SVP. is not harder than any of them. Even reducing one of the easier problems (SMP. 
or SAP.)to SIVP. would be an interesting result, as it would collapse all the intermediate problems together, 
as well as establish a relation between SVP. and SIVP.. Acknowledgments The author thanks Stephen Checkoway 
for a useful conversation that led to the proof of Corollary 4.2, and the anonymous referees for useful 
comments. References [1] M. Ajtai. The shortest vector problem in l2 is NP­hard for randomized reductions 
(extended abstract). In Proceedings of STOC 98, pages 10 19. ACM, May 1998. [2] M. Ajtai. Generating 
hard instances of lattice problems. Complexity of Computations and Proofs, Quaderni di Matematica, 13:1 
32, 2004. Preliminary version in STOC 1996. [3] M. Ajtai and C. Dwork. A public-key cryptosystem with 
worst-case/average-case equivalence. In Proceed­ings of STOC 97, pages 284 293. ACM, May 1997. [4] M. 
Ajtai, R. Kumar, and D. Sivakumar. A sieve algorithm for the shortest lattice vector problem. In Proceedings 
of STOC 01, pages 266 275. ACM, July 2001. [5] M. Ajtai, R. Kumar, and D. Sivakumar. Sampling short lattice 
vectors and the closest lattice vector problem. In Proceedings of CCC 02, pages 53 57. IEEE, May 2002. 
[6] L. Babai. On Lovasz lattice reduction and the nearest lattice point problem. Combinatorica, 6(1):1 
13, 1986. [7] W. Banaszczyk. New bounds in some transference theorems in the geometry of numbers. Mathematische 
Annalen, 296:625 635, 1993. [8] J. Bl¨omer. Closest vectors, successive minima and dual HKZ-bases of 
lattices. In Proceedings of ICALP 00, volume 1853 of LNCS, pages 248 259. Springer, July 2000. [9] J. 
Bl¨omer and S. Naewe. Sampling methods for shortest vectors, closest vectors and successive minima. In 
Proceedings of ICALP 07, volume 4596 of LNCS, pages 65 77. Springer, July 2007. [10] J. Bl¨omer and J.-P. 
Seifert. On the complexity of computing short linearly independent vectors and short basesinalattice. 
In Proceedings of STOC 99,pages 711 720. ACM, May 1999. [11] E. F. Brickell and A. M. Odlyzko. Cryptanalysis: 
A survey of recent results. In G. J. Simmons, editor, Contemporary Cryptology, chapter 10, pages 501 
540. IEEE Press, 1991. [12] J.-Y. Cai. A new transference theorem in the geometry of numbers and new 
bounds for Ajtai s connection factor. Discrete Applied Mathematics, 126(1):9 31, Mar. 2003. Preliminary 
versions in CCC 1999 and COCOON 1999.  [13] H. Cohen. A course in computational algebraic number theory. 
Springer-Verlag, 1993. [14] I. Dinur, G. Kindler, R. Raz, and S. Safra. Approx­imating CVP to within 
almost-polynomial factors is NP-hard. Combinatorica, 23(2):205 243, 2003. Pre­liminary version in FOCS 
1998. [15] O. Goldreich, S. Goldwasser, and S. Halevi. Public­key cryptosystems from lattice reduction 
problems. In Proceedings of CRYPTO 97, volume 1294 of LNCS, pages 112 131. Springer, Aug. 1997. [16] 
O. Goldreich, D. Micciancio, S. Safra, and J.-P. Seifert. Approximating shortest lattice vectors is not 
harder than approximating closest lattice vectors. Informa­tion Processing Letters, 71(2):55 61, 1999. 
[17] V. Guruswami, D. Micciancio, and O. Regev. The com­plexity of the covering radius problem. Computational 
Complexity, 14(2):90 120, 2005. Preliminary version in CCC 2004. [18] I. Haviv and O. Regev. Tensor-based 
hardness of the shortest vector problem to within almost polynomial factors. In Proceedings of STOC 07, 
pages 469 477. ACM, June 2007. [19] A. Joux and J. Stern. Lattice reduction: A toolbox for the cryptanalyst. 
Journal of Cryptology, 11(3):161 185, 1998. [20] R. Kannan. Minkowski s convex body theorem and integer 
programming. Mathematics of operation re­search, 12(3):415 440, Aug. 1987. [21] S. Khot. Hardness of 
approximating the shortest vector problem in lattices. Journal of the ACM, 52(5):789 808, Sept. 2005. 
Preliminary version in FOCS 2004. [22] A. K. Lenstra, H. W. Lenstra, Jr., and L. Lov´asz. Fac­toring 
polynomials with rational coe.cients. Mathe­matische Annalen, 261(4):513 534, Dec. 1982. [23] D. Micciancio. 
The shortest vector problem is NP­hard to approximate to within some constant. SIAM Journal on Computing, 
30(6):2008 2035, Mar. 2001. Preliminary version in FOCS 1998. [24] D. Micciancio. Almost perfect lattices, 
the covering radius problem, and applications to Ajtai s connection factor. SIAM Journal on Computing, 
34(1):118 169, 2004. Preliminary version in STOC 2002. [25] D. Micciancio. Generalized compact knapsaks, 
cyclic lattices, and e.cient one-way functions. Computa­tional Complexity, 2007. To appear in special 
issue on average-case complexity. Preliminary version in FOCS 2002. [26] D. Micciancio and S. Goldwasser. 
Complexity of Lattice Problems: a cryptographic perspective,volume 671 of The Kluwer International Series 
in Engineering and Computer Science. Kluwer Academic Publishers, Boston, Massachusetts, Mar. 2002. [27] 
D. Micciancio and O. Regev. Worst-case to average­case reductions based on Gaussian measure. SIAM Journal 
on Computing, 37(1):267 302, 2007. Prelim­inary version in FOCS 2004. [28] P. Nguyen and J. Stern. The 
two faces of lattices in cryptology. In Proceedings of CaLC 01, volume 2146 of LNCS, pages 146 180. Springer, 
Mar. 2001. [29] O. Regev. New lattice based cryptographic construc­tions. Journal of the ACM, 51(6):899 
942, 2004. Pre­liminary version in STOC 2003. [30] C.-P. Schnorr. A hierarchy of polynomial time lattice 
basis reduction algorithms. Theoretical Computer Sci­ence, 53(2 3):201 224, 1987.  
			