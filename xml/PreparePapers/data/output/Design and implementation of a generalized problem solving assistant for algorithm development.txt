
 DESIGN AND IMPLEMENTATION OF A GENERALIZED PROBLEM SOLVING ASSISTANT FOR ALGORITHM DEVELOPMENT Barry 
L. Kurtz (kurtz@engr.latech.edu) Unmesh S. Mayekar (usm@engr.latech.edu) Micheal B. O Neal (mike@ engr.latech.edu) 
 Department of Computer Science Louisiana Tech University BOX 10348 Ruston, LA 71272 INTRODUCTION Our 
introduction to computer science course at Louisiana Tech University provides a breadth-first overview 
of the entire discipline, much in the spirit of the Denning report [Denning, 89] and Curricula91 [Tucker, 
91]. We have developed a sequence of eight laboratory modules to be used in closed lab sessions that 
help students develop problem solving skills. The range of topics includes applications (spreadsheets, 
databases), algorithms (data structures, graphics), programming paradigms (imperative, functional), architecture 
(digital logic design), and theory (finite state automata). We have carefully designed the interface 
for all labs to have a uniform look and feel that is mouse, rather than keyboard, oriented. This paper 
reports on techniques we have developed that provide problem solving assistance for a wide range of topics 
as found in our laboratory modules. We have received funding from the National Science Foundation (DUE92543 
17) to develop the laboratory environment for this breadth-fwst overview course. The current student 
profile in the course is quite heterogeneous. About 40-50% of the students are computer science majors, 
while the remainder are drawn from programs throughout the University. Typically, 30-40% of the students 
in CS 100 are women, while approximately 20% of the class is composed of ethnic minorities (predominately 
African American). These studenta represent a wide range of interests and abilities. Most have little 
or no background in college-level mathematics. We are covering a wide range of topics, but materials 
are presented at a very intuitive level so that students will appreciate the breadth of computer science 
as a discipline and gain some valuable knowledge about computer systems. Two other papers provide an 
overview of the entire project [Kurtz, 94; O Neal, 95]. This paper concentrates on the development of 
an intelligent assistant for algorithm development, which we will refer to as the Problem Solving Assistant. 
THE PROBLEM Four of the laboratories (data structures, graphics, imperative programming, and functional 
prograrmning) involve the development of algorithms to solve particukw problems in each domain. We have 
carefully designed our lab environment so that only syntactically correct algorithms can be entered. 
The real problem students encounter is the development of logically correct algorithms. When an algorithm 
is not appropriate to solve a particular problem, what sort of feedback can be providecl? Clearly it 
is inappropriate just to show a correct solution and continue on. We must start with the student s algorithm 
and provide feedback that will enable the student to realiz,e what is wrong with the algorithm and possibly 
give hints as to how the algorithm can be fixed. This problem has been investigated by other researchers 
in Intelligent Tutoring Systems (ITS). Most projects described in the literature deal with one particular 
problem domain. Proust [Johnson, Soloway, appears in Kearsley, 87] is a sophisticated tutoring system 
for Pascal. The system was invoked only after a program compiled successfully. It was designed to identify 
logical flaws in students programs. The system tried to determine the programmer s original intention, 
which may be based on a, misconception about the language, and guide the programmer beyond this misconception 
so that the program could be corrected. Problems were specified to the system using high level program 
plans, such as a sentinel controlled loop for input. The system matched the resulting plan with the student 
program to find discrepancies. The system proved to be reasonably good; it correctly identified over 
70% of the logical errors contained in a moderately complex program. Our approach shares many of the 
same goals as Proust but differs in several ways: our techniques are simpler but apply to a wider domain 
of problems and our approach actively involves the instructor in the development of tutoring strategies. 
Several tutors have been developed to teach Lisp Permission to y without fee all or pan of this material 
is [Wenger, 87]. These tutors tend to be very strict, in the Ygrantad provided t at the copies are not 
made or distributed for direet commerdal advantage, the ACM copyright notice and the title of the publication 
and its date appear, and notice is given that oopying is by permission of the Association of Computing 
sense that they monitored every keystroke entered student. If the student made the slightest deviation 
correct solution, helshe was immediately informed by the from al of this Machinery. To copy otherwise, 
or to republish, requires a fee and told how to correct the error. We much prefer a help and/ors edfic 
permission. SIGCS~ 95 3/95 Nashville, TN USA on demand approach where students develop algorithms O 
1995 ACM 0-S9791-WW9YO003. ...50.5O of their choosing and the system only intervenes when assistance 
is requested. SAMPLE ACTIVITIES Our Problem Solving Assistant has been used in four different labs: data 
structures, graphics, imperative programming, and functional programming. The data structures lab involves 
the graphical manipulation of linear objects, such as stacks and queues. A typical problem is to reverse 
the contents of a queue assuming other data structures are available. A correct approach is to remove 
each item from the queue and push that item onto a stack, until the queue is empty. Then the items should 
be popped off the stack and entered in the queue. All of these operations are carried out by the student 
manipulating graphical representations of the data structures. Whenever the student performs operations, 
code is automatically generated and displayed to represent the sequence of actions. For example, if the 
queue initially contains the items A, B, and C, then transferring these items to the stack would generate 
the code: Remove (A), Push (A), Remove (B), Push (B), Remove (C), Push (C) When these items are transferred 
back to the queue, the generated code is: Pop (C), Enter (C), Pop (B), Enter (B), Pop (A), Enter (A) 
The Problem Solving Assistant uses this generated code to provide feedback to the student. The graphics 
lab contains a drawing tablet and a program window where objects are declared and code is generated. 
Initially students draw objects in the tablet area; the corresponding declarations and code are automatically 
generated. Then the process is reversed and the student is asked to write declarations and code that 
will produce a desired picture. Variable names are generated automatically so that circles are named 
c 1, c2, and so forth. Suppose that the student is asked to draw twenty concentric circles with a center 
at 150, 150 and radius starting at 20 and increasing by 5 for each new circle. A sample solution for 
this problem is: pl := (150, 150); dl := 20; repeat 20 times loop c1 :=(pl, all); draw (c 1); increment 
( dl, 5); end loop; The graphics lab exposes students to several fundamental concepts: object declaration, 
types, assignment, sequencing commands, and repetition. The imperative programming lab follows immediately 
after the graphics lab and develops the imperative style of programming even further. The students can 
declare variables of two types (integer and string), procedures and functions. There are five built-in 
arithmetic operations and three string operations (length, concatenation, and substxing). There are commands 
for input, output, assignment, iteration (While and For), and selection (If). This is the most extensive 
laboratory in the course; typically 2090 of the course time is spent on imperative programming. The first 
half of this time is spent on using the control constructs within the main program. The second half of 
the time stresses subprogram development and includes topics such as parameter passing and subprogram 
invocation. Very simple recursion (e.g., factorial) is introduced to lay the groundwork for the functional 
programming laboratory. The functional programming lab is based on a Lisp-like language. Students can 
define functions, use cond for selection, use five built-in list operations (car, cdr, cons, list, append), 
four built-in integer operations ( +,-,*,/), seven tests (equal, less, atom, null, and, or, not), and 
three operations with literals (t, nil, quote). The new ideas introduced are: the list data structure, 
programming without declarations and assignment, and use of function calls to generate intermediate results 
that are then passed as arguments to other function calls. A sample exercise involves returning the last 
item in a list, with the exceptional case of returning nil if the list is empty. (define last-item (1st 
) (cond ( ( null Ist ) 1st) ((null (cdrlst))(carlst)) (t (last-item (cdr 1st))))) The focus in this lab 
is to expose to the student to the notion that there are different programming language paradigms that 
depend on different approaches (e.g., no reliance on assignment) and use different data structures (the 
list in this case). DESIGN OF THE PROBLEM SOLVING ASSISTANT The Problem Solving Assistant tests algorithms 
at two levels: Black Box testing for input/output behavior and Glass Box testing that looks at the internal 
structure. Black Box testing is most appropriate for the functional programming lab, but is used in some 
of the other labs too. However, it is not appropriate for the graphics lab since the output is a picture 
on the screen. Consider the following example of Black Box testing for the function last-item shown previously: 
Input output Comment (abc) a simple case (a b(cd)) ~cd) a more complex case (a) a a limiting case () 
nil an exceptional case It is impossible to test with every possible set of data, but by carefully selecting 
test cases and making sure that limiting and exceptional cases are included, one could say with increasing 
levels of confidence that if all tests are passed then the function is correct. Black Box testing continues 
until a failure occurs or all tests are passed. When there is a failure, the student is told the inputs 
supplied, the output expected, and the output actually obtained. Glass Box testing looks at the internal 
structure of the student s algorithm. Our first approach in the design of the Problem Solving Assistant 
was based on comparison of the parse tree for the answer entered by the student with parse trees for 
known solutions. The tree comparison rules allowed certain degrees of flexibility, such as the commutativity 
of the addition operation. The goal was to try to find a match of the parse tree for the student s program 
with some perturbation of a model solution. This approach did not prove to be satisfactory. We were spending 
all our time generating rules for equivalent trees and not concentrating on providing meaningful help 
for students. Also this model did not have the generality that we desired. So we abandoned the comparison 
of parse trees and developed a much simpler model based on looking at the algorithm as a sequence of 
tokens. Here is an overview of this strategy: each lab consists of a sequence of exercises that are to 
be completed by all students for each exercise, as appropriate, we have stored a set of inputs and corresponding 
outputs for Black Box testing for each exercise, as appropriate, we have stored a set of templates for 
Glass Box testing  Black Box testing is performed before Glass Box testing; both tests are performed, 
if appropriate  it is permissible for the student s answer to be incomplete at the time assistance 
is requested, however only Glass Box testing is performed  there are two types of Glass Box templates: 
OK templates that match correct answers and error templates that match incorrect answers  templates 
are searched sequentially until the first match; when a match occurs a stored message is shown to the 
student  templates contain special tokens, called clue tokens, that allow for a variety of matching 
techniques  the instructor plays a vital role in this process by supplying the test cases for Black 
Box testing and the templates for Glass Box testing, along with the corresponding messages  the Black 
Box test cases can be enlarged to include critical test values that were overlooked initially  all 
student answers that do not match a Glass Box template are saved for processing by the instructor who 
may discard the answer as not being typical or who may add the answer, possibly modified into a general 
form, and a corresponding message to the set of Glass Box templates  The latter two steps are important 
since they mean the system will get better and better at diagnosing student answers as more student data 
is collected for processing by the instructor. Assistance is provided for most problems in the four labs 
involved, but not all. For example, the initial exercises in the Graphics lab ask students to make free 
form drawing in the drawing tablet. There are no correct answers and no assistance is provided. Black 
Box testing is the primiuy form of feedback for the functional programming exercises. Black Box testing 
cannot be performed for incomplete answers since no output will be produced, but it is possible to perform 
Glass Box testing that may give a hint to the student on how to proceed. There are five clue tokens that 
add robustness to Glass Box testing. (1) nzatch_any : @ The@ in the template is a placeholder that will 
match any corresponding token in the student s answer. Suppose in the Graphics lab the student is asked 
to draw a circle, then a template for a correct answer would be: cl:=(( @,@),@); draw (cl); This template 
will match any values provided for the center of the circle or the radius. (2) ignore_until: $ <argument> 
This allows us to advance through the token stream of an algorithm quickly, until we reach the desired 
location where we will perform some detailed testing. Suppose for an exercise in the functional lab we 
want to find if the first test in the cond is to check for the empty list. A template to perform such 
a test is: ( define $ cond ( ( null@) We first verify we are inside a function definition, then we advance 
until the token cond is encountered. We make sure that the first test in the cond is the null test and 
it is performed on a single argument. (3) next_tokerz_is_not . 1 <argument> This clue token is used to 
check if the student answer is going down a path that will not be productive. In our reverse queue example, 
the student may not have realized that a stack will be needed to solve the problem, The student may have 
developed an incorrect algorithm of the form: remove ( A), insert(A), remove ( B ), insert ( B ), remove(C), 
insert(C) which has the net effect of not changing the queue at all. The template remove ( @) ! push 
would match this answer since the student has not attempted to push the first item on the stack after 
the remove. The message to the student might suggest that he/she think about using a stack to solve the 
problem. (4) next_token_is : A cargument> Suppose wc have asked the student to write a simple recursive 
function in the imperative programming laboratory. After we have verified that the student has declared 
a function inside the program, we may want to check whether an If command is the first command in the 
function. The template to verify this would be: Program $ Function $ Begin A If (5) not_present: # <argument> 
Every function declaration should contain a Return command somewhere, but it is difficult to generalize 
the position of that token. What we really desire to know is that no Return appears anywhere, in which 
case we can provide the student with appropriate assistance. Here is what such a template would look 
like: program $ Function # Return By using combinations of clue tokens we can quickly create templates 
that check very particular conditions in an algorithm yet not get bogged down in exhaustive parse tree 
matching. INSTRUCTOR PARTICIPATION Instructor participation is a key to success using this approach. 
When a new exercise is added to a lab, the instructor should put in test cases for Black Box testing 
and an initial set of templates for Glass Box testing. This testing data should be reviewed after students 
have used the lab. Each time a student requests assistance, there are two possible outcomes for Black 
Box testing (passed or failed) and three outcomes for Glass Box testing (no match found, match an OK 
template, match an error template). The resulting six combinations of outcomes may require some action 
on the instructor s part to change the testing data. Black Template Action Box TYP e passed None Student 
template, possibly with modifications, may be a candidate for an OK template passed OK none passed Error 
A critical Black Box test may be missing or the error template may need revision failed None Student 
template, possibly with modifications, may be a candidate for an error template failed OK There may be 
a faulty Black Box test or the OK I template may need revision failed I Error none We have designed 
a sophisticated user interface so that an instructor can quickly review templates that are candidates 
for new OK templates or new error templates. Below we show a diagram of the most important windows in 
this interface. The main focus is on the instructor looking at a answer, deciding if it is worth saving, 
and, if so, editing the arguments and algorithm to make the template general, and composing a message 
to be shown to the student. Arguments dl cl I Algorithm dl :=20 c1 :=( (150, 150), dl) repeat20 times 
kmp draw (cl) increment(dl, 5) end loop Message You seemto havemissedthe assignmentcommand for cl inside 
the loop. Unlessyou assignthe updated&#38;stancevalueto the cmclevariable Its ~ The exercise for the 
problem given above is to draw 20 concentric circles with a center at (150, 150) and the radii in increments 
of 5 starting at 20. This answer is very close to a correct solution but contains one significant error: 
c 1 is not assigned a new value in the loop. This may not be obvious to the student watching the program 
execute. This program will draw the same circle 20 times, but to the student it may appear to be one 
circle. Without the Problem Solving Assistant, the student may focus on why isn t the loop executing 
instead of the question why isn t the circle changing. What should be done for a student who develops 
a brute force, but correct answer of drawing twenty circles directly without using a loop or any variables? 
Here is the start of such a answer: draw ( ( ( 150, 150),20); draw ( ( ( 150, 150),25); draw ( ( ( 150, 
150),30); etc. Other student may use variables and assignment commands between the draw commands. Even 
though these answers would be technically correct, the instructor should design a template to detect 
such a solution and give a message to the student to try to find a more efficient answer. Notice how 
easy it is to detect such answers using clue tokens. $ draw $ draw $ draw $ draw $ draw This template 
detects multiple draws (5 or more in this case). We have found clue tokens to be a very easy and convenient 
way to allow instructors to focus in directly on particular characteristics of problem solutions. TESTING 
WITH STUDENTS Louisiana Tech operates on the quarter system. Black Box testing was started in the Spring 
quarter 1994 for the functional programming laboratory. Often times students designed functions that 
worked correctly for most data but not for limiting or exceptional cases. Consider the exercise mentioned 
earlier to return the last element of a list. A common answer for someone on the right track might be: 
(define last-item (1st ) (cond ((null (cdrlst))(carlst)) (t (last-item (cdr 1st))))) Thisanswer works 
foralllists except theempty list During Black Box testing the student receives the following assistance. 
Your function did not work for the following case: Input list: ( ) Expected result: nil Your result: 
ERROR, incorrect parameters passed to car, nil This is suftlcient information for a student to add a 
special base case for this particular input value. Glass Box testing was first implemented and tested 
with students in Summer and Fall 1994. Templates were used with selected exercises in the following labs: 
data structures (25 total templates), graphics (33 templates), imperative (59 templates), and functional 
(22 templates). The following data was recorded each time a student requested assistance: the student 
s answer at that time, whether there was a template match or not, and, if so, which template. The percentage 
of template matches varied with the type of lab: data structures (63%), graphics (75%), imperative programming 
(85%), and functional programming (78%). On review of the template structures, we found that we used 
the clue tokens, particularly the tokens ignore_until ($), next_token_is_not ( !), and next_token_is 
(A), much more extensively in the programming language labs and we believe that this accounts for the 
higher percentage of template matches resulting in better assistance for the students. FUTURE DIRECTIONS 
We are learning how to better use the clue tokens to differentiate the errors present in student answers. 
We may develop more clue tokens, as the need arises. We also might make existing tokens more flexible. 
For example, we may want to make sure the same token appears in the student s answer at two or more places. 
We illustrate this with a template for a factorial function: (define @l ( 622 ) $t (@l (-@21 Here we 
have bound @1 to the function name and 622 to the parameter name. We skip ahead to the final alternative 
of the cond, indicated by the test t for true, and verify that the function name on the recursive call 
is correct and the formal parameter is used properly. So far we have not seen enough evidence that students 
use names in the wrong places to warrant this expansion of our match_any token. However, if a need to 
differentiate match_any tokens arises as we expand use of our Problem Solving Assistant, we are prepared 
to make the changes. Our initial approach to building templates involved complete parse trees. The parse 
tree matching became very complex and we decided to use the token stream approach instead. This decision 
did have some tradeoffs and this is most evident in evaluating arithmetic expressicms. There are many 
ways to formulate equivalent arithmetic expressions and with our current system one would have tc~ design 
a separate template for each formulation. The templates would be simpler if we introduce semantic evaluation 
for expressions. So far our exercises have been so simple that the only place we have seen this occur 
is with the commutativity of + and *, which requires only two templates. But we may find as our exercises 
bexome more complex that we have to work on adding more features for evaluating arithmetic expression 
in our templates. Finally, we want to expand our techniques of Black Box and Glass Box testing to other 
laboratories. The finite state automata (FSA) lab requires students to build finite state acceptors for 
a particular string pattern (e.g., the pattern 101) or particular string characteristics (e.g., accept 
any string with an even number of 0s). We currently have an Assist button on the screen that performs 
Black Box testing, but not using the software described in this paper. We hope to bring both types of 
testing into this lab sometime in the future. This also applies to the design of simple combinational 
circuits in the digital logic laboratoqf. We have already introduced Boolean equations in that lab and 
this will give us an abstraction of the circuit being designed that might be used for Glass Box testing. 
In conclusion, we believe we are developing two techniques, Black Box testing and Glass Box testing, 
that provide meaningful assistance to students over a wide variety of topics. The use of token streams 
with clue tokens has proved to be particularly convenient for Glass Box testing. We have made a good 
beginning and plan to more fully develop these approaches. REFERENCES B. Kurtz, M. O Neal, An Interdisciplinary, 
Laboratory­based Course for Computer-based Problem Solving~ Proc. of the National Educational Computer 
Conference, Boston, MA, 1994 M. O Neal, B. Kurtz, Watson: A Modular Software Environment for Introductory 
Computer Science Educationq , SIGCSE Technical Symposium on Computer Science Education, March 1995, Nashville, 
TN P.J. Denning, D.E. Comer, D. Gries, M.C. Melder, A, Tucker, A.J. Turner, P.R. Young, Computing as 
a Discipline , Communications of the ACM, January 1989, Vol. 32, No. 1, pp. 9-23 G. Kearsley (editor), 
Artificial Intelligence and Instruction: Applications and Methods, Addison-Wesley Publishing, 1987 A. 
Tucker, et. al (editors), Computing Curricuia 1991: Report of the ACiWIEEE-CS Joint Curriculum Task Force, 
ACM Press, 1991  E, Wenger, Artificial Intelligence and Tutoring Systems, Morgan Kaufman, 1987
			