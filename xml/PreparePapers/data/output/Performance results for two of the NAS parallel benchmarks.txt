
 Performance Results for Two of the NAS Parallel Benchmarks David H. Bailey NAS Applied Research Branch 
NASA Ames Research Center Moffett Field, CA 94035 Abstract Two problems from the recently published 
NAS Parallel Benchmarks have been implemented on three advanced parallel computer syst ems. These two 
benchmarks are the following: (1) an (embarrassingly parallel Monte Carlo statistical calculation and 
(2) a Poisson partial differential equation solver based on three-dimensional fast Fourier transforms. 
The first requires virtually no interprocessor communication, while the second has a substantial communication 
re­quirement. This paper briefly describes the two problems stud­ied, discusses the implement ation schemes 
employed, and gives performance results on the Cray Y-MP, the Intel iPSC/860 and the Connection Machine-2. 
Bailey is with the Numerical Aerodynamic Simula­tion (NAS) Systems Division at NASA Ames Research Center, 
Moffett Field, CA 94035. Fredrickson is with the Research Institute for Advanced Computer Sci­ence (RIACS) 
at NASA Ames. Frederickson s work was funded by the NAS Systems Division via Cooper­ative Agreement NCC 
2-387 between NASA and the Universities Space Research Association. Paul O. Fredrickson RIACS NASA Ames 
Research Center Moffett Field, CA 94035  1 Introduction The NAS Parallel Benchmarks [3] is a new set 
of benchmarks that have been developed for the per­formance evaluation of highly parallel supercomput­ers. 
These benchmarks consist of five parallel ker­nels , and three simulated computational fluid dy­namics 
(CFD) application benchmarks. Together they mimic the computation and data movement character­istics 
of many large scale aerophysics applications. These benchmarks were developed out of the real­ization 
that none of the conventional approaches to the performance benchmarking of supercomputers is appropriate 
for highly parallel systems. The popular kernel benchmarks that have been used for tradi­tional vector 
supercomputers, such as the Livermore Loops, the LINPACK benchmark and the original NAS Kernels, are 
clearly inappropriate for the perfor­mance evaluation of highly parallel machines. In addi­tion to the 
dubious meaning of the actual performance numbers, the computation and memory requirements of these programs 
do not do justice to the vastly in­creased capabilities of the new parallel machines, par­ticularly those 
systems that will be available by the mid-1990s. On the other hand, a full scale scientific application 
is similarly unsuitable. First of all, porting a large application to a new parallel computer architecture 
requires a major effort, and it is usually hard to justify a major research task simply to obtain a benchmark 
number. For example, there are as yet few results for the PERFECT benchmark suite on highly parallel 
systems. An alternative to conventional benchmark strate­gies is to specify a paper and pencil benchmark, 
i.e. to define a set of problems only algorithmically. Even the input data is specified only on paper. 
Naturally, the problem has to be specified in sufficient detail that a unique solution exists, and the 
required output has to be brief yet detailed enough to certify that the prob­ 166 (D 1991 ACM 0-89791-459-7/91/0166 
$01.50 lem has been solved correctly. The person or persons implementing the benchmarks on a given system 
are expected to solve the various problems in the most ap­propriate way for the specific system. The 
choice of data structures, algorithms, processor allocation and memory usage are all (to the extent allowed 
by the specification) left open to the discretion of the imple­menter. The NAS Parallel Benchmarks were 
constructed on this model. They consist of eight separate benchmark problems: 1. An embarrassingly parallel 
Monte-Carlo statis­tical computation. 2, A simplified multigrid partial differential equa­tion (PDE) 
solver. 3. A conjugate gradient eigenvalue computation that involves unstructured matrices. 4, A Poisson 
PDE solver that employs three­dimensional fast Fourier transforms (FFTs). 5. An integer sort problem, 
which is used in some particle codes. 6< The LU solver of a simulated CFD application. 7. The scalar 
pentadiagonal solver of a simulated CFD application. 8. The block tridiagonal solver of a simulated 
CFD application.  There are several basic requirements on implemen­tations: (1) 64-bit floating point 
arithmetic must be used, (2) programs must be coded in Fortran-77 or C, although a variety of commonly 
used parallel exten­sions are allowed, (3) except for a short list of com­mon library functions and intrinsic, 
assembly lan­guage routines may not be used for any computa­tions. Otherwise programmers are free to 
utilize al­gorithms and language constructs that give the best performance possible on the particular 
system being studied. The detailed definitions of the problems to be solved, as well as the specific 
language rules and timing procedures are given in [3]. This paper describes the implementation of prob­lems 
1 and 4 on the following three systems: (1) a Cray Y-MP with eight processors, (2) an Intel iPSC/860 
system with 128 processors, and (3) a Connection Machine-2 with 32,768 processors. 2 The Embarrassingly 
Parallel Benchmark In this problem, a large number of pairs of Gaussian random deviates are generated 
according to a specific scheme, and the number of pairs lying in successive square annuli are tabulated. 
The only essential re­quirement for communication in a multiprocessor im­plementation is to collect the 
counts at the end. This problem represents the essence of many Monte Carlo physics calculations, and 
it is also typical of such di­verse applications as the analysis of supercomputer memory systems [2]. 
Another reason for its inclusion in the NAS Parallel Benchmarks is to provide a con­trast with other 
problems, such as the 3-D FFT PDE solver, which require substantial interprocessor com­munication. The 
following is a brief statement of this problem. The complete statement, plus references, are given in 
[3]. Set n = 228 and s = 271828183. Generate the pseudorandom floating point values rj in the interval 
(O, 1) for 1 ~ j < 2n using the scheme described below. Then for 1< j < nsetxj = Zrzj-1 1 and yj = 2rzj 
 1. Thus xj and yj are uniformly distributed on the interval ( 1, 1). Next set k = O, and beginning with 
j = 1, test to see if tj = z; + y; < 1. If not, reject this pair and proceed to the next j. If this inequality 
holds, then set k + k + 1, Xk = ~j<( 210gtj)/tj and Yk = Yj J( 2 log tj )/tj, where log denotes the natural 
logarithm. Then xk and Y~ are independent Gaussian deviates with mean zero and variance one. Approxi­mately 
n~/4 pairs will be constructed in this manner. Finally, for O ~ Z < 9 tabulate QI as the count of the 
pairs (Xk, Yk) that lie in the square annulus 1< max(]Xk[, lY~l) < 1 + 1, and output the ten Q( counts. 
Each of the ten Q/ counts must agree exactly with reference values. The 2n uniform pseudorandom numbers 
rj men­ tioned above are to be generated according to the fol­lowing scheme: Set a = 513 and let ZO = 
s be the specified initial seed . Generate the integers z~ for 1 s k s 2n using the linear congruential 
recursion Zk+l = ax, (mod 24 ) and return the numbers rk = 2 4 xk as the results. Observe that O < rk 
< 1 and the r~ are very nearly uniformly distributed on the unit interval. An important feature of this 
pseudorandom num­ber generator is that any particular value ~k of the sequence can be computed directly 
from the initial seed s by using the binary algorithm for exponentia­tion, taking remainders modulo 246 
after each multi­plication. The importance of this property for paral­lel processing is that numerous 
separate segments of a single, reproducible sequence can be generated on separate processors of a multiprocessor 
system. Many other widely used schemes for pseudorandom number generation do not possess this important 
property. 3 The 3-D FFT PDE Benchmark In this problem, a certain Poisson partial differen­tial equation 
(PDE) is solved using three-dimensional fast Fourier transform (FFT) computations. In con­trast to the 
embarrassingly parallel problem, this application requires substantial interprocessor com­munication 
and is thus a good test of network per­formance. The following is a brief description of this bench­mark. 
For full details, see [3]. Consider the PDE where x is a position in 3-dimensional space. When a Fourier 
transform is applied to each side, this equation becomes 8V(.Z, t) = 4c#lzyv(z,t) (%  where v(.z, t) 
is the Fourier transform of u($, t).This has the solution -4a7r /zl tv(z, o) v(z, t) = e Now consider 
the discrete version of the original PDE. Following the above, it can be solved by comput­ing the forward 
3-D discrete Fourier transform (DFT) of the original state array U(Z, 0), multiplying the re­sults by 
certain exponential, and then performing an inverse 3-D DFT. The specific problem to be solved in this 
bench­mark is as follows. Set nl = 256, n2 = 256, and n3 = 128. Generate 2n1n2n3 64-bit pseudorandom 
floating point values using the pseudorandom num­ber generator in the previous section, starting with 
the initial seed 314159265. Then fill the complex ar­ray Ui,j,k, 0Si<nl,05j<nz,0~k< ns, with this data, 
where the first dimension varies most rapidly as in the ordering of a 3-D Fortran array. A single complex 
number entry of U consists of two con­secutive pseudorandomIy generated results. Compute the forward 
3-D DFT of U, using a 3-D fast Fourier transform (FFT) routine, and call the result V. Set c1 = 10 6 
and set t= 1. Then compute where ;isdefined asifor O~i<nl/2 and i nl for nl/2 < i < nl. The indices ~ 
and k are similarly defined with n2 and n3. Then compute an inverse 3-D DFT on W, using a 3-D FFT routine, 
and call the result the array X. Finally, compute the complex checksum ~~~03 Xq,r,, where q = i (mod 
nl ), ? = 3i (mod nz) and s = 5i (mod n3). After the checksum for this thas been output, increment t 
by one. Then repeat the above process, from the computation of W through the incrementing of t, until 
the step t = N has been completed. In this benchmark, Ii = 6. The V array and the array of exponential 
terms for t= 1 need only be computed once. Note that the array of exponential terms for t> 1 can be obtained 
as the t-th power of the array for t= 1. The rules of the NAS Parallel Benchmarks require that all problems 
be implemented in Fortran-77 or C, with some reasonable parallel extensions, and prohibit the usage of 
assembly language comput at ions. How­ever, one exception to this is that vendor-supplied, assembly-coded 
library routines may be employed to compute either individual 1-D FFTs or complete 3-D FFTs. Thus this 
benchmark can be used to contrast the performance one can obtain from a Fortran imple­mentation to an 
implementation that employs certain vendor library routines. 4 System Descriptions The Cray Y-MP used 
in this study is the system in the NAS facility at NASA Ames Research Center. It has eight processors 
and 128 million words of bipo­lar random access memory. It also has 256 million words of solid state 
memory, but this was not utilized here. Its clock period is 6 nanoseconds, so that its the­oretical peak 
performance rate is approximately 2.66 GFLOPS. These codes were compiled and executed under UNICOS 6.0 
system software. The Intel iPSC/860 used in this study, which is also at the NAS facility, employs Intel 
s new RISC processor, the i860. The system has 128 nodes, each with eight megabytes of memory, connected 
with in a hypercube network. The i860 haa a theoretical peak performance of 60 MFLOPS (64 bit), so that 
the the­ oretical peak performance of the system is some 7.68 GFLOPS. In practice it is hard to even 
approach this 168 rate, even from assembly code [5]. The codes were compiled using the Portland Group 
Fortran compiler, version 1.3a. The NAS CM-2 has 32,768 bit-serial processors to­gether with 1,024 Weitek 
64-bit floating point pro­cessors and four gigabytes of memory. Its theoreti­cal peak performance is 
some 14 GFLOPS, although again few codes can approach this rate. The 1.0 sys­tem software, which includes 
the 1.0 slicewise Fortran compiler and CMSSL library, was used in this study. Implementation Techniques 
Curiously, the principal algorithmic challenge in implementing the embarrassingly parallel benchmark 
problem was in devising a scheme suitable for high performance on a Cray supercomputer. In contrast, 
straightforward implementations sufficed on the Intel and CM-2 systems. The principal difficulty on the 
Cray is that the lin­ear congruential scheme presented above for gener­ating uniform pseudorandom numbers 
is not directly vectorizable. One solution, which was employed here, is to generate these numbers in 
batches of 64, which is the hardware vector length of the Cray systems. This may be done by first computing 
the multiplier ii = a64 (mod 246). The first 64 elements of the sequence z~ are computed by the scalar 
algorithm. Thereafter the remainder are computed by the recursion *k+64 = uzk (mod 246) The acceptance-rejection 
operation in the genera­tion of the Gaussian deviates, as well as the accu­mulation of counts, also present 
difficulties for vector computations, but these were resolved by reasonably straightforward means. In 
the 3-D FFT PDE problem, the principal issue is the selection of an efficient means to compute 3-D FFTs. 
On the Cray, complex arrays of size nl x nz x ns, where nl, n2 and nB are powers of two, were declared 
with physical dimensions (nl + 1, nz + 1, ns + 1). Then transforms were performed in all three dimensions, 
with vectorization in the plane not transformed. This is efficient on the Cray since the strides of the 
resulting array accesses are always either one or else one greater than a power of two. Either way, no 
bank conflicts re­sult. The vectorized FFTs were based on a 1-D FFT algorithm described in [1]. Multiprocessing 
on the Cray was achieved by in­serting a few autotasking directives. The FFTs were performed by an all-Fortran 
routine as described above and also by calling Cray)s new library routine CFFT3D. This routine permits 
both single processor and multiprocessor runs to be made. On the Intel, accessing arrays in each of three 
di­mensions is of course not practical because in at least one dimension, the accesses will be across 
processors. Thus the following alternate scheme was employed: 1, n2n3 1-D FFTs of size nl are performed, 
each on data vectors entirely contained in a single pro­cessing node (in fact, within the cache of the 
pro­cessor). 2 n1n3 1-D FFTs of size nz are performed. These data vectors are also contained within individual 
nodes, since n3 never exceeds the number of nodes used. 3 The resulting complex data array is then trans­posed 
to a n3 x nl x n2 array by means of a com­plete exchange operation. 4. n1n2 1-D FFTs of size ns are performed, 
again on local node data. 5. The resulting complex data array is transposed back toanl xn2xn3array. 
 Note that steps 3 and 5 require a complete ex­change) operation, i.e. the data in each processing node 
is partitioned into p sections, where p is the number of processors, and then each processor sends each 
of its p sections to the appropriate target proces­sor. Since this step requires substantial interprocessor 
communication, it is essential that it be as efficient as possible. Recently Bokhari [4] compared a num­ber 
of schemes for this operation. The scheme used here is the one described in that paper as the forced, 
pair-wise, synchronized scheme, which is due to Seidel [6]. Recently Kuck and Associates completed a 
pack­age of assembly-coded mathematical routines for i860­based systems, under contract to Intel. These 
rou­tines included all of the basic linear algebra subrou­tines (BLAS), plus some one-dimensional FFT 
rou­tines. The double precision 1-D complex-to-complex FFT routine from this package was incorporated 
into the benchmark code, so that results have been ob­tained using both this routine and an all-Fortran 
ver­sion. One scheme attempted on the CM-2 is similar to that employed on the Cray, except that there 
is no need for array dimensions to be one greater than pow­ers of two. Another attempted scheme is similar 
to the algorithm employed for the Intel system. Unfor­tunately, for reasons not yet fully understood 
ss of the date of this paper, neither scheme exhibits even remotely respectable performance evidently 
signif­icant parts of the computation are not really being performed in parallel on the CM-2. However, 
TMC has recently provided a rather efficient routine for one-dimensional and multi-dimensional FFTs in 
the CMSSL library. Thus this routine ww employed in the runs cited below. One important element of the 
tuning for the CM-2 implementation was the specification of the layout of the 3-D array to be transformed. 
This was done with the directive cmf$ layout X1( :serial, 1000 :send, i: send) The authors are indebted 
to Robert Krawitz of TMC for this suggestion. All of the implementations for this benchmark em­ploy ordered 
FFTs. Some improvement in perfor­mance may be possible by using unordered (i.e. bit reversed) FFTs. Future 
benchmark efforts will explore this possibility. Performance Results The embarrassingly parallel benchmark 
was run for three problem sizes: n = 224, 22s and 230. All three systems, including the Cray, which is 
known for diffi­culties with numerical inaccuracies, obtained precisely correct counts, apparently confirming 
that billions of floating operations were performed correctly (within acceptable tolerance limits) and 
that the library rou­tines SQRT and LOG returned satisfactorily accurate results. Performance results 
for this benchmark are listed in Table 1. In accordance with the rules for the NAS Parallel Benchmarks, 
run times are the elapsed time of day from start of execution to completion, and MFLOPS figures are for 
64-bit data, with operation counts as given in [3], which in turn are based on some standard figures 
for the SQRT and LOG functions. In cases where two or more runs were possible with the same system on 
the same size problem, but with dif­ferent numbers of processors, speedup figures are shown in the sixth 
column. These numbers are nor­malized based on the run with the smallest number of processors. Not surprisingly, 
these runs exhibit almost perfectly linear speedups. What also scales linearly is the run time versus 
the size of the problem timings for equivalent systems with n = 230 are almost exactly four times the 
timings for n = 228. The results indi­cate that on this type of problem, the full Intel system is roughly 
equivalent to 2.5 Y-MP processors, and that the full CM-2 is roughly equivalent to three Y-MP pro­ cessors. 
It is interesting to note that these results are 41%, 4.7% and 3.1% of the peak performance rates of 
the Y-MP, the Intel and the CM-2, respectively. The low percentages on the Intel and CM-2 are in spite 
of a minimal communication requirement. These figures thus underscore that there is considerable room 
for improvement in the performance of the Intel and CM­2 Fortran compilers and library intrinsic. The 
3-D FFT PDE benchmark was run with the problem sizes 64 x 64 x 64, 128 x 128 x 128 and 256 x 256 x 128. 
All three systems produced check­sums at the end of each iteration that were correct to the required 
12 significant figures. Timing results are shown in Table 2. F or L in the Code column de­notes an all-Fortran 
or a library FFT (a 1-D FFT for the Intel, and a 3-D FFT for the Cray and the CM-2). Timings on the CM-2 
were problematic for this benchmark. The CM timing routines output two run times: CM elapsed time and 
CM busy time . The former is the elapsed time of day between start of ex­ecution and end, and corresponds 
to the definition of run time for the NAS Parallel Benchmarks. The latter excludes time when the CM is 
not actually busy with computation, such as when it is exchanging data with the front end system. In 
the first benchmark, as well as in a majority of applications, these times are not greatly different. 
However, in the 3-D FFT PDE benchmark they are greatly different, especially in the 64x64x 64 problem 
size. This difference is apparently due to time spent in transferring data and program code from the 
front end to the CM-2 as part of initialization for the library FFT routine. Thinking Machines plans 
to greatly re­duce this initialization time in future software releases. For this reason, and the fact 
that the CM busy timings scale more predictably with problem size and number of processors, the CM busy 
timings were also included in the table. The CM elapsed and CM busy timings are distinguished by E or 
B in the Code column. 170 System Y-MP Intel CM-2 Y-MP Intel CM-2 Y-MP Intel CM-2 Table 1: Problem No. 
Size Proc. 2 <1 8 4 8 16 32 64 128 8K 16K 22s 1 8 32 64 128 8K 16K 32K 230 1 8 128 32K 7 Embarrassingly 
Parallel TGiE­ (see.) MFLOPS Speedup 9.41 147.40 1.00 1.25 1109.60 7.53 122.58 11.32 1.00 61.30 22.63 
2.00 31.60 43.89 3.88 15.77 87.95 7.77 7.67 180.83 15.98 3.84 361.20 31.92 13.11 105.80 1.00 6.55 211.76 
2.00 150.10 147.90 1.00 20.10 1104.48 7.47 245.20 90.54 1.00 122.61 181.06 2.00 61.32 362.03 4.00 201.04 
110.44 1.00 100.84 220.15 2.00 50.92 435.98 3.95 603.50 147.14 1.00 80.63 1101.33 7.48 245.21 362.14 
202.94 437.57 Benchmark Performance Results Problem No. Time System Code Size Proc. (sec.) MFLOPS Speedup 
Y-MP F 64x64x64 1 1.21 156.85 L 1 0.82 231.46 Intel F 4 17.08 11.11 1.00 F 8 9.01 21.06 1.90 F 16 4.77 
39.78 3.58 F 32 2.51 75.68 6.81 L 4 11.87 15.99 1.00 L 8 6.39 29.70 1.86 L 16 3.41 55.68 3.48 L 32 1.84 
103.29 6.46 CM-2 LE 8K 10.76 17.64 LB 8K 3.16 60.06 1.00 LE 16K 8.24 23.03 LB 16K 1.82 104.29 1.74 Y-MP 
F 128 X 128 X 128 1 10.27 169.33 L 1 7.14 243.56 1.00 Intel F 32 19.32 90.02 1.00 F 64 9.95 174.82 1.94 
F 128 5.45 319.34 3.54 L 32 13.70 126.89 1.00 L 64 7.14 243.67 1.92 L 128 4.05 429.77 3.39 CM-2 LE 8K 
30.79 56.48 LB 8K 24.67 70.49 1.00 LE 16K 17.11 101.64 LB 16K 12.07 144.08 2.04 LE 32K 13.06 133.15 LB 
32K 6.13 283.69 4.02 Y-MP F 2562 X 128 1 39.23 192.23 1.00 F 8 5.17 1458.61 7.59 L 1 29.31 257.28 1.00 
L 8 6.15 1226.18 4.77 Intel F 128 22.22 339.44 L 128 15.13 498.45 CM-2 LE 16K 51.20 147.29 LB 16K 47.06 
160.24 1.00 LE 32K 28.59 263.76 LB 32K 24.50 307.80 1.92 rable2: 3-D FFT PDE Benchmark Performance Rates 
 The results for this benchmark definitely indicate sub-linear speedup with increasing numbers of pro­cessors 
on the Intel. On the 643 problem, the Intel system (with the library 1-D FFT routine) is only 6.5 times 
faster with 32 processors than with 4. The CM-2 is better in this regard: on the 1283 problem, the CM-2 
is actually 4.02 times faster with 32K pro­cessors than with 8K, although a fall-off in perfor­mance 
is seen with the other two problem sizes. Cu­riously, the all-Fortran Cray program exhibited more nearly 
linear speedups and actually out-performed the library FFT-based program with eight processors on the 
256 x 256 x 128 problem. However, it should be emphasized that speedup figures can be easily misinterpreted. 
For example, one reason that the speedup figures are not worse on the Intel is that the current Intel 
Fortran com­piler is still not achieving nearly the performance that can be obtained through assembly 
coding. The single node performance rate for the all-Fortran code on the Intel system is only about 3 
MFLOPS, whereas the peak 64-bit performance of the i860 is 60 MFLOPS. When a more powerful compiler becomes 
available, then these speedup figures can be expected to fall off more rapidly, because the performance 
will be domi­nated to a greater extent by communication costs. The performance rates on the largest problem 
size indicate that the full Intel system is roughly equiva­lent to 1.9 Y-MP processors (comparing library 
FFT timings), and that the CM-2 is roughly equivalent to 1.2 Y-MP processor (comparing the CM busy timings 
with the Y-MP library rates). The fact that these ra­tios are not as favorable to the highly parallel 
systems on the 3-D FFT PDE benchmark as they were on the embarrassingly parallel benchmark is clearly 
due to the more demanding communication requirement in the 3-D FFT. 7 Conclusions With some algorithmic 
experimentation and im­plementation tuning, all three of the tested systems were able to obtain respectable 
performance rates on these two benchmark problems. However, the In­tel and CM-2 systems, while showing 
promise, are not yet demonstrating on these problems the consistently high level of sustained performance 
that users of Cray systems have come to expect (comparing full systems to fuIl systems). It is true that 
the performance of highly parallel computers on a particular application can in some cases be significantly 
improved by employing alter­nate algorithms that feature less costly communica­tion. However, in many 
cases, including the 3-D FFT PDE benchmark and other problems that require in­tensive computations in 
each dimension of three di­mensional arrays, only modest improvement can be achieved in this manner, 
since a certain amount of communication is unavoidable. Also, many CFD ap­plications and other hyperbolic 
PDE problems must employ implicit algorithms in order to obtain a solu­tion in a reasonable amount of 
time, and these implicit algorithms require substantial long-distance commu­nication. What this means 
is that the usability of a highly parallel system will be quite limited if it cannot perform well on 
communication intensive problems. Thus it is hoped that the next generation of highly parallel supercomputers 
will feature greatly increased communication performance. References [1] Bailey, D. H., A High-Performance 
FFT Algo­rithm for Vector Supercomputers, International Journal of Supercomputer Applications, vol. 2 
(1988), p. 82-87. [2] Bailey, D. H., Vector Computer Memory Bank Contention , IEEE Transactions on Computers, 
vol. C-36, no. 3 (Mar. 1987), p. 293-298. [3] Bailey, D. H., Barton, J. T., Lasinski, T. A, and Si­mon, 
H. D., eds., The NAS Parallel Benchmarks , RNR Technical Report RNR-91-002, NASA Ames Research Center, 
January 1991. [4] Bokhari, S. H., Complete Exchange on the iPSC­860 , ICASE Report No. 91-4, NASA Langley 
Re­search Center, January 1991. [5] Lee, K., On the Floating Point Performance of the i860 Microprocessor 
, RNR Technical Report RNR-90-019, NASA Ames Research Center, Octo­ber 1990. [6] Seidel, S., Lee, M-H., 
and Fotedar, S., Con­current Bidirectional Communication on the Intel iPSC/860 and iPSC/2°, Technical 
Report CS-TR 9006, Dept. of Computer Science, Michigan Tech­nical University, Nov. 1990. 
			