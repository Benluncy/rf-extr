
 Computing with Unreliable Information (Preliminary Version) Uriel Feige " David Peleg t Prabhakar Raghavan 
t Eli Upfal§ 1. Introduction Fault-tolerance is an important consideration in large systems. Broadly, 
there are two approaches to coping with faults. The first is the "reconfigura- tion" approach [3, 9], 
in which faults are identified and isolated in real time. This is done concurrently with computation, 
and is often a significant over- head. A second, different approach is to devise al- gorithms that work 
despite unreliable information, without singling out the faulty information. This latter approach has 
been the focus of much recent work [6, 7, 10, 11, 12, 16]. Here we adopt a new approach to this latter 
paradigm. Model: Our general model will be a (possibly randomized) computation tree, in which each node 
gives the correct answer with some probability > p, where p is a fixed constant in (1/2, 1), bounded 
away from 1/2 and 1. The node faults are indepen- dent. We study the depth of the computation tree in 
terms of a tolerance parameter Q E (0,1/2): on any instance, the computation tree leads to a leaf giving 
the correct answer on that instance with probabil- ity at least 1 - Q. The success probability of the 
algorithm is computed over the combined probabil- ity space of the outcome of individual comparisons 
°The Weizmarm Institute of Science, Rehovot, Israel. Part of the work wad done while this author was 
visiting IBM T.J. Watson and Almaden Fte~arch Centers. tThe Weizmaaom Institute of Science, Rehovot, 
Israel. Supported in part by an Allon Fellowship and by a Bantrell Career Development Fellowahip. IIBM 
T.J. Watson Retvearch Center, Yorktown Heights, NY. A portion of this work was done while the author 
was visiting the Weizmann Institute of Science. §IBM/~]maden Research Center, San Jose, CA, and The Weizmann 
Institute of Science, Rehovot, Israel. Work at the Weizmann Institute supported in part by a Bat-Sheva 
de Rothschild Award and by a Revson Career Development Award. Permission to copy without fee all or 
part of this material is granted pro- vided that the copies are not made or distributed for direct commercial 
advantage, the ACM copyright notice and the title of the publication and its date appear, and notice 
is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, 
or to republish, requires a fee and/or specific permission. and the results of coinflips (in case our 
algorithm is randomized). There are several possible computation trees that could be studied in this 
noisy tree "model; we focus on. two here. The first is the noisy boolean decision tree, in which the 
tree computes a boolean function of N boolean variables zl,...,=~v. Each node in the tree corresponds 
to querying one of the input variables; with some probability, we are given the wrong value of that variable. 
Each leaf is labelled 0 or 1, and corresponds to an evaluation of the func- tion. Secondly, we study 
noisy comparison trees for problems such as sorting, selection and searching. Here the input is a set 
{zl, z2,..., =N} of N num-bers. Each node in the tree specifies two indices i and j. The node responds 
with either "xi > =j" or "zl < zj", and gives the wrong answer with some probability. Each leaf is labelled 
with a permuta- tion representing the sorted order for the input (for selection, the label will consist 
of an index in [1, N]). A simple example is in order here. In the absence of errors, the maximum of N 
numbers can be found by a comparison tree of depth N -1. In the face of a constant probability of error, 
it is possible to re- peat each comparison of the fault-free decision tree O(log(N/Q)) times and obtain 
(by majority vot- ing) a guess for the results of the comparison that is wrong with probability at most 
Q/N. Doing this for every comparison immediately gives us a noisy comparison tree of depth O(N log(N/Q)) 
for find- ing the maximum (we can afford to sum the failure probability of Q/N over the N -1 events). 
In a similar fashion, any decision tree that has depth d in the absence of noise can be turned into a 
noisy one of depth O(dlog(d/Q)). The crux of our work is to show that while this logarithmic blowup is 
unavoidable for certain prob- lems, it is (perhaps surprisingly) unnecessary for certain others. In fact, 
we are able to show such a separation between problems that have the same de- cision tree complexity 
in the absence of errors (The- orems 1,2). A major obstacle to proving the lower bounds is that errors 
cancel-- multiple errors could compound on an input to lead to a leaf giving the correct answer to that 
input, for the "wrong rea- son". Another distinction we make is between the case of a static adversary, 
in which the success proba-bility of individual operations is fixed to p, and the case of a dynamic adversary 
who can set the prob- ability of individual operation dynamically to any value between p and 1. It turns 
out that there is a difference between these two eases. In the dynamic case, the noisy decision tree 
complexity is bounded below by the deterministic (noise free) decision tree complexity, since the adversary 
may always opt for a correct execution (with all individual operations giving the correct value). In 
contrast, in the static case, the noisy decision tree complexity is bounded above by log(n/Q) times the 
randomized (noise free) decision tree complexity. This follows from the fact that the availability of 
basic operations with fixed success probability provides us with a fixed-bias coin, which in turn can 
be used to generate a fair coin. Since there are problems for which the randomized deci- sion tree complexity 
is significantly smaller than the deterministic decision tree complexity (cf. [14]), it follows that 
the presence of fixed probability faults may actually help the algorithm. This points out another difficulty 
in proving lower bounds for the noisy model. Related Previous Work: Pippenger [10] and others have studied 
networks of noisy gates, in which every gate could give the wrong answer with some probability. Kenyon-Mathieu 
and Yao [6] study a boolean decision tree in which an adver-sary is allowed to corrupt at most k nodes 
(read operations) along any root-leaf path. Kleitman et ai. [7] consider the problem of binary search 
on N elements using a comparison tree when an adversary (:an choose k comparisons to be incorrect ("lies") 
on any root-leaf path. This model was further studied by Ravikumar et al. [11, 12]. Yao and Yao [16] 
study sorting networks with at most k faulty comparators. Our work differs from [6, 7, 11, 12] in that 
we allow every node of the decision tree to be indepen- dently faulty with some probability. Thus in 
our model the number of faults is not prescribed in ad- vance --knowledge of this number could well be 
exploited by a "fault-tolerant" algorithm. nPr°b tI'[~ (respectively DN,Q(H))Det Results: Let ~'N,Q ~ 
) denote the minimum depth of any noisy probabilis- tic (respectively deterministic) decision tree for 
in- stances of size N of problem l-I, with tolerance Q. All our lower bounds are for probabilistic trees, 
and all the upper bounds (with the exception of paral- lel sorting) are for deterministic trees. Furthermore, 
all our lower bounds apply against the weaker static adversary (and hence also against a dynamic ad- 
versary), and all the upper bounds apply against a dynamic adversary (and hence also against a static 
adversary). Let THK ~ denote the K-of-N threshold function: given N boolean inputs, the output is 1 
if and only if K + 1 or more inputs are 1. For noisy boolean decision trees we have the following results: 
DProb [ q. ~N ,~ Theorem 1: (1) N,Q \.L "~'~K] ~(N log(m/Q)), where m = min{K, N -K}. (2) D~r°Qb( PARITY) 
"- fl( N log(N/Q)). Theorem 2: (1) r)Det~JN,Q r TH N) O(Nlog(m/Q)). In particular, DDe,~(OR) and DDe,~(AND) 
are both o(g log(l/Q)). (2) DDc,~(PARITY) = O(N log(N/q)). Notice the separation of noisy tree depths 
in these results, whereas decision trees for all these problems have depth N in the absence of noise. 
Problems such as parity have a blowup in tree depth that grows with N, rather than p or Q alone (unlike 
the OR function). In Section 2 we extend these results to all symmetric functions. Let K-SEL be the problem 
of selecting the Kth largest of N elements. In the noisy comparison tree model we have the following 
tight results: Theorem 3: (1) DPN~,°Qb(BINARY SEARCH) -~(]og(N/Q)). ($) DPr,~b(SORTING) = ~(g log(N/Q)). 
(3) DPmQb( MERGING) -~( g log(N/Q)). (4) DP~b(K'SEL) = ~(g log(re~Q)), where m  - rain{K, g -g }. 
Theorem 4: (1) DD~,~(BINARY SEARCH) = O(log(N/Q)). (2) D~v%( SORTING) = O( N log(N/Q)). (8) DD% (MERGING) 
= O(N log(N/Q)).  (4) DD¢,~(K'SEL) = O(Nlog(m/Q)), where m = min{K, N -K}. In particular, the maximum 
can be found by a noisy tree of depth O(N log(l/Q)).  A well-known sports commentator has ob-served 
[5] that the problem of finding the maximum 129 by a noisy comparison tree has a sporting interpre- tation: 
we wish to find the best of N teams by a tournament. In each game, the better team wins with some probability 
>_ p; how many games must be played in order that the best team fail to win with probability at most 
Q? One algorithm we give for finding the maximum by a noisy comparison tree bears a remarkable resemblance 
to the NBA cham-pionship: teams pair up and play a game at the first round, the winners pair up and play 
three games at the next, five in the third round and so on. It can be shown that the best team fails 
to win such a tour- nament with probability < c'(1-p) for some c', and that the total number of games 
is O(N). This fail- ure probability can be reduced to Q by replacing the one game in the first round 
with clog(1/Q) games, the three in the second round by 3clog(1/Q), and SO on. This brings up the following 
natural que.clion: how many days must such a tournament lazt, as-suming a team plays at most one game 
a day? Equivalently, what is the depth of a noisy "EREW" parallel comparison tree with up to N/2 paral-lel 
comparisons at each node? The "NBA" algo-rithm described above requires O(log N log(N/Q)) rounds. In 
Section 4 we show that O(log(g/Q)) rounds suffice for this problem, and also give an optimal randomized 
parallel decision tree for sorting. Theorem 5: There is an N-processor EREW PRAM algorithm that computes 
the maximum of N elements with noisy comparisons, using O(log(N/Q)) rounds and a total of O(N log(l/Q)) 
comparisons, with failure probability < Q. The al- gorithm applies even when each element is allowed 
to participate in at most one comparison per round (i.e., no element duplication is allowed). Theorem 
6: There is a randomized, noisy, par-allel comparison tree (N comparisons per node) of depth O(log N) 
that sorts N numbers with probabil-ity 1 - N -e. 2. Boolean Decision Trees The main result outlined 
in this section will be a lower bound on the depth of any noisy boolean deci- sion tree computing the 
K-of-N threshold function TH N. As a first step, we prove a lower bound for the case K -0, which is the 
OR function. Theorem 7: Prob (~~ DN, Q (OR) = fl \ log ~_, ,/" Proof: Let .~" = (XI, ...,XN) be the 
input vector, let 0 = (0,...,0) and let ij denote an input vec- tor adjacent to 0 on the jth coordinate, 
i.e., with all zeros except Xj = 1. The proof is based on showing that distinguishing between 0 and the 
ad- jacent vectors requires the stated depth. For a leaf £ of a boolean decision tree of depth d and 
an in- put vector )(, let Pr{£1)f } denote the probability of reaching t (in a probabilistic decision 
tree it com- bines the probabilities of the random choices of the algorithm with the probabilities of 
the random an- swers to the queries). For a set L of leaves, define Pr(Llf(} --EteL Pr{t[)(}. Assume 
that Xj appears r(j,g) times on the path from the root to g. Then Pr(gl]j} > ((1 -p)/p)'(J:)Pr{g[O}. 
For any g, ~-~dN=l r(j,Q = N d. Therefore 7:d=x((1-p)/p) ~U't) achieves its minimum (over all choices 
of r(j,g)) as N((1-p)/p)d/N. Thus, letting S denote the set of leaves labeled '0', we get N N j=l j--1 
£ES >-ZZ Pr{t[O} £ES j----1 >_ Pr{SIS)N Clearly Pr{S]O} >_ (1-Q), and for every j, Pr{Sli#} < Q, and 
hence QN > (1 -Q)N The bound on d follows. Note that the proof goes through even against a static adversary. 
A somewhat simpler proof can be given if the adversary is dynamic (Thin. 19). [] Let us now turn to the 
general Threshold function THe, For a vector f( = (X1 .... , XN) of N bits, let w(.~) denote the weight 
of )~, i.e., w()() = ~"~N=I X,. Thus TH g is 1 if w(3f) > K, and 0 otherwise. Theorem 8: For every K 
< N/2, DPr°b(THN)=a ONlogK+ ]og-l__~P } N,Q for some O < O < ½-v'~'q log l/O-p)"  Proof: If K _< max[(1- 
Q)/Q, i00], the ad-versary can announce the values of input vari-ables X1, X2,...XK in advance to be 
1. Comput-ing TH~ is then reduced to the problem of com- puting the OR function of the remaining N -K 
bits. By Theorem 7, this requires a tree of depth n(g log((1 -Q)/Q)/log(p/(1 -p))). Thus for the rest 
of the proof assume that K > max[(1-Q)/Q,100]. Let 0 < # < [1-2Q/(1-Q)]/2, and 0 < 0 _< -#/~31og(1-p)). 
Given a noisy decision tree for TH~ of depth _< 0NlogK we show that its failure probability ex-ceeds 
Q, and this will yield the lower bound of 0N log K on the depth. Since the depth of the tree _< ON log 
K, at most #N input variables are queried more than a = -(log K)/(3 log(1 -p)) times in each computation 
path. Without ]n~ ,,f generality we grant the decision tree some addi-tional information: (1) if a variable 
is queried more than a times, the correct value of the variable is given to the tree, and (2) if a variable 
is queried less than o~ times, the al- gorithm is allowed to query the variable again "for free" until 
it obtains exactly ~ queries of that vari- able.  Thus, it is sufficient to prove the lower bound for 
trees of the following two-phase form, which are more powerful than any decision tree of depth ON log 
K. Phase A: Query every variable exactly a times. Each query returns the correct value with prob- ability 
p. Phase B: Request the values of #N of the in-put variables Xx,...XN; these queries are an- swered correctly. 
 The proof is by means of the probabilistic method: we present the tree with randomly cho- sen inputs 
having K or K + 1 ones, all such inputs being equiprobable. We show that certain leaves of the tree will 
be reached with almost the same prob- ability regardless of whether the input has K or K + 1 ones. We 
first analyze the outcome of Phase A. Let Si denote the set of variables i of whose a queries were answered 
'1' in Phase A. The out- come of Phase A is fully characterized by the vector = ($1, ..., S,~). Let si 
= ISil, and ~ = (s~, ..., s~). Let zi (resp. Yi) denote the number of input vari- ables in Si whose actual 
values are 0 (resp. 1). Let = (z~, ..., za) and 9 = (yx,..., y,~). For a variable X, let Pi ° = Pr{X 
 SiIX = O) = (7)va-i(1_ p)i, and Pi 1 = Pr{X SiIX = 1} = (7)pi(1 -- V) a-i. Let £ be the event (N K 
1)P/° (1 ai) _< zi _< (N K)Pi ° (1 + Ai) for all 1 < i _< a, where ai = 6~/(log N)/(~N -K)P~°). By the 
Chernoff bound [2], (1) Pr{3i, Yi = O) < 1/4 and (2) Pr{~} _< 1/N, where g is the comple- ment of £. 
We will use the following lemma. Lemma 9: 1 Pr{~l(w(X ") = K + 1) A£} 1 Nil s -< Pr{..Sl(w(,~" ) = K) 
^ £} _< 1 + NU-----~" Proof." Let g(t) = (sa,...,st-1,...,s~) denote a distribution of N- 1 variables, 
K ones, and N- K -1 zeros among the sets S1,...,Sa. Similarly, denote by 5(g) = (zl,..., zt- 1,..., z~) 
a distribution of the N -K -1 0-variables among $1, ..., Sa. (N-K-l) ~]-~a epPV,. 1 Pr{z(fi)IC} Zl~...,Ztl--X~...,Z~] 
11i=1 ~, I / Pr{~(&#38;)l£} (N-K-l) ~ N ° {pO~z, . 1 zl,...,zt2-1,...,z,l 11i=1\ ~ i ] zt a Pt °, By 
condition £, ztx Pt°2 1 + 6t, 1 1 1-6tl < <__ < i+NU____~. 1-~7i~ ff <- 1 + 6t'------~-zt2 Pt°~ -1 -6t2 
- By summing over all possible pairs (5(0, 9) such that 5(0 + .Y = s(g) we get Pr{g(gx)IC} Ex(t,)+.~=,(t,)Pr{5(gl)lE}Pr{glC} 
_  Pr{$(t2))lg) --~,(t~)+~=r(t~) Pr{~7(t2)Ig}Pr{91£} " Thus, 1 Pr{$(tl)l£} 1 1 -Nil.------~ < < 1 
+ -Pr{$(t2))l£} -N 1/5" We now add the last variable X, with either one or zero, and get Pr{$l(w(X) 
= K + 1) A £} Pr{~l(w(X) = K) A g} _ ~=1 Pr{$(g)Ig}Pr{X &#38;IX = 1} -~=1 Pr{g(t)[£}Pr{X SelX = 0}" 
 131 Using the fact that Pr{X E St]X = 0} = Pr{Xt E Sa-tlX = 1) we prove 1 Pr{gl(w(X ) = K + 1) ^ £} 
1 1-N i7~ -< Pr{al(w(2) = K)A g'} ~ l'l'N1/'-'-"ff" Since all assignments of K or K + 1 ones to the variables 
have equal probability, by symmetry, all partitions of the variables into sets of sizes sl, ..., sa have 
equal probability and the claim is proven. [] To simplify the analysis of Phase B we assume without loss 
of generality that at the end of Phase A the adversary reveals the locations of K in-put variables with 
value 1. If w(){) is K + 1, and V, iyi > 0, the remaining (unexposed) vari-able is chosen to be from 
set Si with probability zi/(Y~j=l zj) = zi/(N -K -1). Denote by $1, ..., Sa the input to Phase B, where 
Si contains the variables in Si that were not row aled by the adversary. Note that zi/ISil < 1 tbr all 
i (more specifically, ISil = zi for all sets Si except at most one, which might have zi + 1 variables). 
The tree cannot distinguish between variables in Si. Suppose that the tree queries ri variables in Si 
(~-~i ri = tiN). If :~i contains the unexposed 1- variable, the probability that it hits the unexposed 
1-variable is r~/IS~l. The probability that the 1 vari- able is in S~ is proportional to ISi[. Thus the 
prob- ability of hitting the 1-variable in Phase B when V, iyi > 0 is bounded above by ri Zi t~N < < 
2p. ~= N-K-1 ILl-N-K-l- Let S denote a the family of vectors .~ such that if the output of Phase A is 
S E S and Phase B did not find the 1-variable, the algorithm outputs '0'. Lemma 10: 1 1  Pr{Slw(f¢ ) 
= K + 1} _>_ (1 - ~yT-~)(1 - Q) N" Proof: Clearly Pr{Slw(2 ) = K} > 1 -Q, else the tree does not perform 
as claimed. Pr{Slw(X; ) = K} < Pr{g} + Pr{,Sl(w(~:) = K) ^ £}. By Lemma 9, Pr{SIw(X ) = K} 1 < Pr{£} 
+ Pr{Si(w(.Y() = K + 1) A £}(1 + N----F]~) Pr{Slw(2) --K + 1} 1 <_Pr{t}+ P--~r (1 + ~fg). (£}  Thus, 
for an input 3~" with w()~) = K + 1, Pr{S[w(X) = g + 1} I (l 1 ~) (Pr{SI,,,(X) = K} - 1 1 1 > (1- N----i-~) 
(-Q) N" r3 When the output of Phase A is in S, w(X) = K + 1, and Vi, yl > 0, the probability that Phase 
B finds the unexposed 1 is at most 2/a. Otherwise the algorithm outputs 0. Since Q < 1/2, when w()() 
= K+I Pr{3i : Yi = 0IS} < Pr{3i : Yi - 0} 1 -Pr{S} -< for sufficiently large N. Thus, the probability 
that the algorithm outputs 0 when w(.~) --K + 1 is at least Pr{Siw(2) = K+l}(1-Pr{3i : yi = 01S})(1-2u) 
> Q for sufficiently large N. [] A matching upper bound for TH N follows from a variant of the algorithm 
for K-SEL in Section 3. Thus we get Det N Theorem 11: Dlv,O (THT() = O(N log(re~Q)). In particular, 
DDe~(OR) and DDe~(AND) are both O(N log(l/Q)). In fact, we can prove the following more gen-eral result, 
whose proof we omit. A boolean function f on N boolean variables is symmetric if f(X1,...,XN) = f(X,,o),...,X~(N 
)) for ev-ery permutation ~ on {1 .... ,N}. For the func- tion f, let kx be the largest i < N/2 such 
that f()() # f(X') for X,)(' such that w()() = i and w(X ~) = i + 1. Let k2 be the smallest i >_ N/2 
such that f(X) # f()f') for )~,)~ such that w()() = i and w(,~') = i + 1. Let k = max{ka, k~}. Theorem 
12: For any symmetric function f, Prob DN,Q (f) = f~(Nlog(]e/Q)) and r)Vet *-"tc,Q(f) = O(Nlog(k/Q)). 
In   particular, D~,%b( PARITY) = rt( N log( N/Q) ) and D~e,~( PARITY) = O(N log(N/Q)). 3. Comparison 
Trees This section concerns noisy comparison trees. Our first claim is that binary searching and insertion 
in a balanced search tree does not require a blowup in noisy tree depth that grows with N. This result 
can be derived by modifying the algorithms of [7] or [15] and adapting them to our model. The result- 
ing algorithms would in fact be optimal (in terms of the constants involved). We present a simpler and 
somewhat more intuitive algorithm, with the additional advantage that the ideas it is based on can also 
be used for other problems, where the tech- niques of [7, 15] do not seem to apply (see Thin. 20). The 
algorithm is obtained by thinking of a noisy bi- nary search as a random walk on the (exact) binary search 
tree. In discussing upper bounds for searching values V1,V2,...VN in a binary search tree, we will re-fer 
to our noisy comparison tree as an "algorithm" (rather than tree) to avoid confusion with the bi- nary 
search tree. For simplifying the description we shall assume that the searched element is not i i, r 
i,. tree (so its insertion location has to be determined). Each node of the tree represents an interval 
of the input vector. A leaf of the search tree rep-resents an interval between two consecutive input 
values. There are N + 1 leaves, with the ith (1 < i < N+I) representing (V/-t, V/) (assume V0 = -~ and 
VN+I = oo. An internal node u of the tree is labelled by an interval (Vt, Vh), 0 < t < h < N + 1, where 
Vt is the smallest value at a leaf in the sub- tree rooted at u, and Vh the largest. The left child of 
u is labeled (Vt, V,) and the right child is labelled (V~, Vh), where z ----- [L~_~. To search with unreliable 
comparisons we extend the tree in the following way: each leaf Vt is a parent of a chain of length m' 
= O(log(N/Q)). The nodes of the chain are labeled with the same interval as the leaf. (These chains can 
be implemented by counters representing the "depth" from the leaf). Let X be the value searched for in 
the tree. The search starts at the root of the tree, and advances or backtracks according to the results 
of the compar- isons. Whenever reaching a node u, the algorithm first checks that X really belongs to 
the interval as- se.ciated with u, by comparing it to the endpoints of the interval. If this test fails, 
i.e., reveals an in-consistency indicating an earlier mistake, then the computation backtracks to the 
parent of u in the tree. If the test succeeds, the computation proceeds to a child of u. If there are 
two children the algo- rithm compares X to Vz (the center of u's interval) and continues accordingly. 
The search is continued for m = O(log(N/Q)) steps, m < m' (hence it never reaches the endpoint of any 
chain). The outcome of the algorithm is the left endpoint of the interval la- beling the node at which 
the search ends. Lemma 13: For every Q < 1/2, the algorithm computes the correct location of X with 
probability >_I-Q. Proof: We model the search process as a Markov process. Consider a leaf x of the extended 
tree T, and suppose that the searched element belongs to the interval labeling this leaf. Orient all 
the edges of T towards x. Note that for every node v, ex-actly one adjacent edge is directed away from 
v and the other adjacent edges are directed towards v. Without loss of generality we can assume that 
the transition probability along the outgoing edge is at least 2/3, and the probability of transitions 
along all other (incoming) edges is at most 1/3. Other-wise, we can reach that probability by repeating 
each comparison O(1) times and taking the major- ity. Let m I be a random variable counting the num- 
ber of forward transitions (i.e., transitions in the direction of the edges) and let mb denote the num- 
ber of backward transitions (my +mb = rn). We need to show that my -mb > logN with proba- bility 1 - 
Q, implying that the appropriate chain is reached. This follows from Chernoff's bound [2] for m = clog(N/Q), 
for a suitably chosen constant e. O Using N insertions of the above algorithm, each with failure probability 
Q/N, yields a noisy comparison-tree of depth O(Nlog(N/Q)) for sort- ing. Theorem 14: (1) D~%(BINARY SEARCH) 
: O(log(N/Q)). (2) DDe~(SORTING) -O(N log(N/Q)). (3) DD% (MERGING) -O( N log(N/Q)).  We now present 
a noisy comparison tree of depth O(Nlog(m/Q)), m = min{K, N -K} for select- ing the Kth largest of N 
elements (in fact, the tree described can find all K largest elements). By sym- metry, we need only consider 
the case K < N/2. Furthermore, the case v/N < K < N/2 can be han- dled' using our O(N log(N/Q)~_ sorting 
algorithm. Thus we assume that K < ~/N. The idea in find- ing the Kth largest element when K is "small" 
is to use "tree selection" or "heapsort" (see Knuth, pp. 142-145 [8]). In essence, the algorithm op- 
erates as follows. Once a heap is created, the largest element can be extracted from the top of 133 
the heap, and "reheapifying" the rest of the ele-ments requires at most log N exact comparisons. Thus, 
extracting the K largest elements can be done in K log N exact comparisons. For K < V~, KlogNlog(2(K 
logN)/Q) < Nlog(K/Q) noisy comparisons are sufficient in order to extract these elements with failure 
probability smaller than Q/2. The only problem which remains is that of con- structing the initial heap. 
In order to do this, run a "tournament" algorithm similar to the "NBA" algorithm in Section 1 for finding 
the maximum with failure probability Q/2K. The algorithm takes O(N log(K/Q)) steps, and each of the K 
largest ele- ments has probability at most Q/2K of being elim- inated by an element smaller than it. 
Thus, with probability 1 - Q/2, the initial heap is consistent with respect to the K largest elements, 
and this suffices for our purposes. Therefore we have: Theorem 15: DD~,~ (K-SEL) = O(Y log(m/Q)), where 
m = rain{K, N -K}. Note that a comparison tree for K-SEL can be modified to give a comparison tree for 
TH N . Thus the tree described above can be used in the optimal tree for computing any symmetric function, 
proving the upper bounds in Theorems 11 and 12. Let us now turn to lower bounds for the prob- lems discussed 
above. First, it is obvious that our searching and sorting algorithms are asymptotically optimal in the 
comparison model. Theorem 16: (1} DP~,°Qb(BINARY SEARCH) = f~(log(N/Q)). (~) DPN~°Qb(SORTING) = f~(g 
log(N/q)). Next, the fact that a comparison tree for K-SEL implies a comparison tree for TH N enables 
us to derive the following from Theorem 8 Theorem 17: DPIv~,°Qb(K-SEL) = f~(N log(m/Q)), where m = min{K, 
N -K}. Also, a lower bound for MERGING can be de- rived by a reduction from PARITY, giving Theorem 18: 
DPr,~b( MERGING) = f~( g log(N/Q)).  4. Parallel Comparison Trees This section considers two problems 
on N-processor PRAMs in which each comparison operation be-tween two elements independently gives the 
correct result with probability >_ p. Our upper bounds in this model are thus actually algorithms rather 
than merely comparison trees (like our other algorithms before). We first consider the problem of finding 
the largest of N elements, and then turn to sorting. We now give an algorithm for finding the largest 
of N elements. The algorithm can be implemented on an EREW parallel decision tree with at most N/2 comparisons 
per round in O(log(N/Q)) rounds. Furthermore, each input element is involved in at most one comparison 
per round. Because of its sporting interpretation, we will describe the algo- rithm in the tournament 
setting introduced in Sec- tion 1. Let us now describe this setting in more detail. A parallel algorithm 
for computing the maximum is called a tournament if in each parallel step of the algorithm, each input 
element is involved in at most one comparison. A tournament is determin-istic if the comparisons which 
are to be made in each step are uniquely determined by the results of comparisons in previous steps (no 
randomization is allowed). The depth of a tournament is the to-tal number of parallel steps it takes. 
The size of a tournament is the total number of comparisons it involves. A tournament is noisy if each 
comparison might output the wrong answer. We consider noisy tournaments with a dynamic adversary. A noisy 
tournament is Q-tolerant if it outputs the maximal element with probability at least 1 - Q. Theorem 19: 
Any determinis-tic Q-tolerant tournament has depth f~(log(N/Q)) and size a( N log(l/Q)). Proof: Let T 
be any tournament. Let d denote its depth and let s denote its size. Assume that d = o(log(N/Q)). We 
show that the tournament cannot be Q-tolerant. Any Q-tolerant tournament is also a determinis- tic noise-free 
tournament for finding the maximum, hence its depth is at least log N. Thus for any con- stant c > 0, 
if Q > N -c we immediately derive a contradiction. For the case that Q vanishes more rapidly than the 
inverse of any polynomial, it holds that o(log(N/Q)) = o(log(1/Q)). Fix an arbitrary input and let the 
adversary decide not to introduce any error in the comparisons. The tournament must output the correct 
maximal element. Now switch the values of the first and second largest elements in the input, and let 
the adversary try to convince the algorithm that the element which is now second 134 largest is in fact 
the maximum. In order to do this, all the adversary has to do is to cheat in compar- isons between the 
two largest elements, and then the algorithm proceeds exactly as in the case that the inputs are not 
switched. Since there are at most d comparisons between the two largest elements, the adversary's probability 
of success is (1 -p)a > Q, for constant p and sufficiently large N. The bound on the size is proved in 
a similar way. ~3 We remark that a stronger version of the above Theorem, in which the algorithm is probabilistie 
and the adversary is static, can be proved along similar lines as those of Theorem 7, which indeed proves 
this result for the size of tournaments. We state without proof an inequality, due to tto- effding [4], 
which we use in the proof of the next Theorem. Let Xi, for 1 < i < n, be n independent random variables 
with identical probability distri- butions, each ranging over the interval [a, b]. Let )( be a random 
variable denoting the average of the Xi 's. Then Prob(lX - E()?)I > 6) < 2e- ~?-*.~. Theorem 20: For 
every Q < 1/2 there exist a Q-tolerant deterministic tournament for finding the maximum with depth O(log(N/Q)) 
and size O( N log(l/Q)) simultaneously. Proof: The tournament we construct is similar in spirit to the 
noisy binary search procedure of section 3. For simplicity (and without loss of generality) we assume 
that N = 2 m - 1 for some m. Create a bal- anced binary tree of depth m, and arbitrarily place one input 
element in each node (including leaves, root and internal nodes). The algorithm proceeds in rounds. In 
each round, many mini-tournaments are preformed in parallel. Each mini-tournament involves three players, 
and the largest of the three wins with probability at least p, for some constant p to be computed later. 
The mini-tournaments are organized by partitioning the nodes of the tree into triplets in a special way, 
and forming a mini- tournament between the three elements stored in each triplet. The partition into 
triplets depends on the round. In even rounds, each triplet consists of a node at an even level of the 
tree and its two children. Analogously, in odd rounds, each triplet consists of a node at an odd level 
and its two chil- dren. At the end of the round, the winner of each mini-tournament is stored at the 
parent node, and the two other elements are placed as the children. The whole procedure is repeated 
for O(log(N/Q)) rounds. We pause at this stage to give some intuition as to why our construction computes 
the maximum. The tournament is best described as a random walk taken by the maximal element over the 
balanced binary tree. A win at a single mini-tournament may or may not advance the maximum towards the 
root, depending on whether the maximum is already placed at the parent node before the mini- tournament 
begins. But wins in two successive mini-tournaments advance the maximum by at least one step. Likewise, 
if it loses one of two succes-sive mini-tournaments, it may move away from the root by one step, and 
if it loses two successive mini- tournaments, it may move away from the root by two steps. Summing up 
the probabilities of these events, we see that on the average, in two succes- sive rounds, the maximum 
is expected to decrease its distance to the root by at least p2 + 2p- 2 steps. For p > 15/16, this value 
is greater than 3/4, and so in less than 8m/3 steps the maximum is expected to reach the root. (Note 
that any value ofp > 1/2 can be boosted up to 15/16 by repeated comparisons.) Two parts are still missing 
from the construction. One is a method of preventing the maximum from leaving the root once it reaches 
it. The other is a method of decreasing the total number of compar- isons from o(g log(N/Q)) to O(N log(l/Q)). 
This is significant if Q > N -c asymptotically for any constant c. In order to capture the maximal element 
at the root with high probability we adopt the following policy: An element stays at the root as long 
as it has won the majority of mini-tournaments since it last reached the root. We employ a root counter 
which is initialized to 0. In mini-tournaments which involve the root, if the element placed at the root 
wins the mini-tournament, the root counter is incre- mented by 1. If a different element wins, and the 
root counter is at 0, this element exchanges places with the root element. If the root element does not 
win and the root counter has value greater than 0, then the root counter is decremented by 1, and no 
exchange takes place. Lemma 21: The probability that the maximal el- ement a is at the root after d = 
256 log(N/Q) steps is at least 1 - Q/2. Proof: Assume that some other element b is at the root by the 
end of the process. We do not decrease the probability of b ending up at the root if we let 135 it begin 
the tournament placed at the root, and let it win without competition any mini-tournament in which a 
is not involved. This implies that during the whole tournament, only two elements, a and b, could have 
occupied the root. Furthermore, b played exactly d/2 mini-tournaments involving the root, losing at most 
d/4. Now consider a. In d rounds it was expected to advance by 3d/8 steps. Applying the Hoeffding inequality 
with d/2 (for the d selected above), we get that with probability 1 - Q/N, a advanced at least 5d/16 
steps. Exactly log n of these steps can be accounted for as steps taking a from a leaf to just below 
the root. The other 5d/16- log n steps must have been "wasted" on decrementing b's root counter. For 
d as in the Lemma, this value is greater than d/4, contradicting our assumption that b ends the tournament 
at the root. [] Though the depth of the above tournament is O(log(Y/Q)) as desired, its size is O(N log(N/Q)), 
which is too large. In order to diminish the to-tal number of comparisons, we execute a truncation procedure. 
After i. O(log(1/Q)) rounds, we delete the ith lowest level of the competition tree. This has the effect 
of reducing the number of parallel mini- tournaments by a constant factor each O(log(1/Q)) rounds, and 
thus reducing the size of the competi- tion to O(N log(l/Q)). Lemma 22: The probability that the maximal 
element is at a leaf of the truncated tree after 16i(log(1/Q) + 2) rounds is less than Q/2 i+1. Proof: 
The maximal element is expected to ad- vance by at least 6i(log(1/Q) + 2) steps. The prob- ability it 
advanced less than i steps is as specified in the Lemma, by the Itoeffding inequality. [] From the above 
Lemma it follows that the proba- bility that we lose the maximal element in the trun- cation process 
is smaller than Q/2. Thus the total probability that the maximM element does not win the tournament is 
at most Q, completing the proof of the Theorem. 13 We end this section with a very brief outline of a 
randomized parallel sorting algorithm for a CREW PRAM with noisy comparisons. The algorithm has three 
phases. The first phase constructs a search- tree on a random subset of O(N/(log 2 N)) of the el- ements, 
which respects all orderings with high prob- ability. All input elements are involved in this con- struction; 
all but O(N/log 2 N) of them will be dis- carded in the process. In the second phase, we use the search-tree 
and a binary insertion process simi- lar to that in Section 3 to partition the N elements into O(N/(log 
2 N)) sets each of size O(log ~ N) el- ements, such that elements in set i are larger than elements of 
set i- 1 and smaller than elements of set i+ 1. The third phase sorts these O(N/(log 2 N)) "small" sets. 
We describe only the first phase here. For simplicity of exposition we describe the case Q >_ 1IN ~ here. 
The search-tree construction is loosely patterned on Reischuk's [13] parallel sorting algorithm, with 
a number of changes due to complications aris- ing from noisy comparisons. At the first stage, we choose 
O(x/N/logN ) samples and sort them with N processors in O(loglogN) parallel steps by comparing all pairs 
(repeating each compari-son O(logN) times). This gives a sample that is sorted and partitions the input 
into subsets of size O(N h/s) with very high probability. This partition- ing is accomplished by an O(log 
N) binary search in the sorted sample. We now wish to recur on the subsets. At a general step, we choose 
samples of size ISI/log N in each set S and sort them in parallel using ]S] processors for each sample 
of size IS[/logg. We can only afford log IS[ steps for the binary search process of parti- tioning the 
set S; this means many of the elements get inserted into the wrong subset at lower levels. Our fear is 
that these wrongly inserted elements will become part of the search tree we are constructing. We combat 
this possibility by discarding a carefully chosen number of "extreme" elements in each sorted random 
sample. We show that we now go from S to subsets of size < IS/7/~ instead of < [s/s/s; this only increases 
the constant in the running time. This stratagem works provided we have sets S that are larger than c' 
log s N in size; its total time is O((log log N) ~+ log N) parallel steps (details omit- ted). For smaller 
sets S, we switch to a more "brute- force" algorithm. 5. Extensions and Open Problems Using reductions 
from the bounds we have given, we can give tight bounds on the depths of noisy tree for the following 
problems: finding the left-most 1, UNARY-BINARY, COMPARISON, ADDITION and MATCHING (see [1] for definitions). 
We can also extend the results of Section 2 to show that there is a noisy boolean decision tree of depth 
O(Nlog(1/Q)) for any function that can be computed by a constant-depth formula of size N. Another interesting 
direction would be to give a deterministic noisy PRAM algorithm for sorting. We also conjecture that 
there is no noisy sorting network of size O(N log N) that sorts N elements.  Acknowledgments ~¥e thank 
Noga Alon and Yossi Azar for helpful dis- cussions, and for directing us to some of the refer- ences, 
and Oded Goldreich for his illuminating com- ments on a previous draft.  References [1] A.K. Chandra, 
L. Stockmeyer, and U. Vishkin. Constant depth reducibility. SlAM Journal on Computing, 13(2):423-439, 
1984. [2] H. Chernoff. A measure of asymptotic efficiency for tests of a hypothesis based on the sum 
of observa- tions. Annals of Math. Stat., 23:493-509, 1952. [3] J. Hastad, F.T. Leighton, and M. Newman. 
Recon- figuring a hypercube in the presence of faults. In 19th Annual Symposium on Theory of Computing, 
pages 274-284, 1987. [4] W. goeffding. Probability inequalities for sums of bounded random variables. 
J. Amer. Stat. Assoc., 58:13-30, 1963. [5] R.M. Karp. Personal communication. Berkeley, 1989. [~] C. 
Kenyon-Mathieu and A.C. Yao. On evaluating boolean functions with unrealiable tests. Unpub-lished manuscript, 
Princeton Univ., 1989. [7] D.J. Kleitman, A.R. Meyer, R.L. Rivest, J. Spencer, and K. Winldmann. Coping 
with errors in binary search procedures. Journal of Computer and System Sciences, 20:396-404, 1980. [8] 
D. E. Knuth. Sorting and Searching, volume 3 of The Art of Computer Programming. Addison-Wesley, Reading, 
Massachusetts, 1973. [9] M. Pease, R. Shostak, and L. Lamport. Reaching agreement in the presence of 
faults. Journal of the ACM, 27:228-234, 1980. [10] N. Pippenger. On networks of noisy gates. In g6th 
Annual Symposium on Foundations of Computer Science, pages 30-38, 1985. [11] B. Ravikumar, K. Ganesan, 
and K.B. Lakshmanan. On selecting the largest element in spite of erro- neous information. In Automata, 
Languages, and Programming, Lecture Notes in Computer Science, pages 88-99. Springer-Verlag, 1987. [12] 
B. Ravikumar and K.B. Lakshmanan. Coping with known patterns of lies in a search game. Theoretical Computer 
Science, 33:85-94, 1984. [13] R. Reischuk. Probabilistic parallel algorithms for sorting and selection. 
SlAM Journal on Comput-ing, 14(2):396-409, 1985. [14] M. SaYs and A. Wigderson. Probabilistic Boolean 
decision trees and the complexity of evaluating game trees. In ~7th Annual Symposium on Founda- tions 
of Computer Science, pages 29-38, Toronto, Ontario, 1986. [15] J.P.M. Schalkwijk. A class of simple and 
optimal strategies for block coding on the binary symmetric channel with noiseless feedback. 1EEE Trans. 
ln]o. Theory, 17(3):283-283, 1971. [16] A.C. Yao and F.F. Yao. On fault-tolerant networks for sorting. 
SlAM Journal on Computing, 14:120-128, 1985. 137  
			