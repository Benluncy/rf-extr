
 Chapter 32 Learning the Fourier Spectrum of Probabilistic Lists and Trees William Aiello* Milena Mihailt 
 Abstract We observe that the Linial, Mansour, and Nissan method of learning boolean concepts (under 
uni­form sampling distribution) by reconstructing their Fourier represent ation [LMN89] extends when 
the concepts are probabilistic in the sense of Kearns and Shapire [KS90]. We show that probabilistic 
decision lists, and more generally probabilistic decision trees with at most one occurrence of each literal, 
can be approx­imate ed by polynomially small Fourier represent a­tions, and that the non-negligible Fourier 
coeffi­cients can be efficiently identified and estimated. Hence, all such concepts are learnable in 
polynomial time under uniform sampling distribution. This is the first instance where Fourier methods 
result in polynomial learning algorithms: the polynomiality of our results should be contrasted to the 
np lylogn complexities in the analogous cases of [LMN89] and [M90]. The new ingredient of our work that 
allows us to achieve this polynomiality is that via refined Fourier analysis we are able to isolate the 
polynomi­ally small set of non-negligible Fourier coefficients that reside in a super-polynomially large 
area of the spectrum. We further observe that several more gen­eral concept classes have slightly super-polynomial 
(npolyk)gn ) learning algorithms. These classes include all polynomial-size probabilistic decision trees, 
their convex combinations, etc. A concrete special case which results in polynomial learnabil­ity is 
the weighted arithmetization of k-DNF. Bdl ColIl]lltl[\icalioI]s Research, Morristown NJ 07960. aidlo((!fl 
ash .Ixdlcorc.con]. flkll (bmmnnicat.ions Research, hlorrist.own NJ 07!w0, ]I~illail(@)fl&#38;sll .l}cllcorc. 
col]).  1 Introduction There is a famous (and very practical) theorem by Nyquist in signal processing 
which roughly says the following: If a continuous signal has bounded spec­trum, then it can be completely 
reconstructed by discrete sampling . It is every so often the case that uncountable versus count able 
statements for infi­nite domains translate to exponential versus poly­nomial stat ements for finite domains. 
The ap­proach of learning via Fourier transforms that appears here and elsewhere [LMN89] [M90] is, in 
spirit, the finite analogue of N yquist s theorem. In this paper we are concerned with uniform learning 
of probabilistic concepts. Like boolean concepts, probabilistic concepts are functions with domain the 
n-cube. However un­like boolean concepts whose range is the set {O, 1}, the range of probabilistic concepts 
is the interval [0, 1]. The interpretation is as follows: For a prob­abilistic concept p, and for some 
element d of the n-cube, the label of i is 1 with probability P(F) and O with probability y 1 p(i). 
Probabilistic concepts were introduced by Kearns and Shapire [KS90] as a model that captures natural 
inherent uncertainties (the reader is referred to [KS90] for further justi­fication; here we ordy give 
weather-prediction as the simplest of examples: If it was raining on Mon­day, and it is raining on Tuesday, 
then it will rain on Wednesday with probability y O.8. Here @ cor­responds to raining Monday and raining 
Tuesday, while P(3) = 0.8. However, for any specific sequence of rainy Mondays and Tuesdays, the Wednesday 
that follows it either rains or it does not rain ). 291 Uniform learning is a special form of Valiant 
s distribution-free learning [V84] where examples of the concept to be learned are drawn according to 
the uniform (as opposed to arbitrary) distribution. In the uniform learning scenario there is a class 
of probabilistic concepts P, one of which is the target concept p (P is simply a collection of concepts). 
The task of a learning algorithm for the class P is to pro­duce a good approximation ~ of p after a short 
training phase and some fhrther efficient computa­tion. During the training phase the algorithm is presented 
a small number of samples. Each sam­ple is a uniformly generated element of the n-cube 2, together with 
a label 1 or O that is determined by p( ~). Despite their seeming resemblance with boolean concepts, 
the task of learning probabilistic con­cepts presents significantly larger difficulties. This is reflected 
both in general structural theorems (for example the combinatorial dimension that deter­mines distribution-free 
learnability of probabilistic concepts [KS90] is more complex than the Vapnik-Chervonenkis dimension 
[B EHW89]), as well as the fact that distribution-free learnability results are not known to hold for 
several probabilistic con­cepts whose boolean analogues have rather simple distribution-free learning 
algorithms. A prime ex­ample example of this latter situation is decision lists. For the purposes of 
this paper, a probabilistic de­cision list over n variables is a single branch deci­sion tree whose edges 
are labeled by literals or their negations (see Figure 1). The leaves of the tree are further labeled 
by a number in [0, 1]. Each element # of the n-cube naturally follows a path from the root to a unique 
leaf of the list. The value of the decision list on 2 is the label of this unique leaf. Decision lists 
are generally accepted as quite nat­ural concepts and have been studied extensively in the boolean case 
where the pi s are in {O, 1} (see [R87] for a collection of resdts). In partic­ular, it is well known 
that boolean decision lists are distribution-free learnable even in significantly AIELLO AND MIHAIL more 
general cases [R87]. However for probabilistic decision lists the best known distribution-free result 
requires that the list to be learned is monotone, that is, pl>pz >... > p~+l [KS90]. In this paper we 
obtain polynomial-time uniform­learning algorithms for arbitrary probabilistic deci­sion lists Theorem 
3.6. Furthermore in Theorem 3.10 we show that our techniques extend to ob­t ain polynomial-time uniform-learning 
algorithms for probabilistic decision trees with a single occur­rence per literal (see Figure 2). The 
approach that we use for our learning pur­ poses is to consider the Fourier representation of such concepts 
and approximate each Fourier coef­ ficient. In this way, the target concept is approxi­mated in a vector 
sense. This novel method of learn­ing by approximate ing Fourier represent at ions was introduced by 
Linial, Mansour, and Nissan in the context of boolean concepts [LMN89] (more specif­ically, in the context 
of the class ACO of constant depth circuits ); here we make the simple observa­tion that their techniques 
extend to probabilistic concepts: Theorem 2.2. The efficiency of the whole scheme depends upon the number 
of Fourier co­efficients that must be approximated. For proba­bilistic decision lists and single literal 
probabilis­tic decision trees we use detailed Fourier analysis and argue that all but a polynomially 
small set of Fourier coefficients are negligible (Lemmas 3.2 and 3.9). We further give an algorithmic 
scheme that efficiently determines the frequencies of the non­negligible Fourier coefficients (Remark 
3.3, Algo­rithm 1, and discussion before Theorem 3.10). In the terms of [LMN89] and [M90], this polynomially 
small set of non-negligible frequencies resides in the super-polynomially large area of low frequencies 
. Both conceptually and technically, the part of our work that filters the polynomially small set of 
sig­nificant frequencies from a super-polynomi ally large area of low frequencies is entirely new. This 
is also the reason why we obtain polynomial results instead of the np lylog complexities of [MLN89] and 
[M90] in analogous cases. We consider these polynomial results as the main thmst of our work. For arbitrary 
probabilistic decision trees of poly­nomial size (i.e. when the single literal condi­tion is removed) 
we observe that r$ lyl g uniform learning is feasible: Theorem 4.1. This observa­tion fo~ows along the 
line of reasoning in [LMN89], and with the further simplification that the use of Hastad s Switching 
Lemma [H86] is unnecessary ([LMN89] make crucial use of this Lemma in their ACO proof). We also observe 
that all convex com­binations concepts that have negligible high fre­quencies preserve the low-frequency 
property, and are hence also learnable in the uniform model: The­orem 4.2. An interesting consequence 
of this obser­vation is that the weighted arithmetization of k-DNF is learnable in polynomial time in 
the uni­form model, and by the recent results in [M90] also distribution-free! (Theorems 4.3 and 4.4) 
With respect to the potential of all these results to extend to distribution-free learning, we should 
point out that Mansour has developed some fairly elegant techniques to transform Fourier methods for 
uniform learning to distribution-free learning [M90]. Mausour s techniques apply to concepts whose Fourier 
representations are exactly zero on high frequencies (as opposed to approximately zero). Neither ACO 
nor the probabilistic classes that are described here satisfy this requirement. However on the positive 
side, Mansour s work suggests that Fourier methods for uniform learning could be a first approach for 
the general case of distribution­ free learning. Finally, before proceeding to the technical pre­sent 
ation of our work, it is worth mentioning that the all Fourier methods for uniform learning that appear 
here and elsewhere [LMN89] [M90], except for their remarkable comparability to Nyquist s the­orem, posses 
a variety of further desirable algorith­mic features: they are conceptually simple, very easy to implement, 
parallelizable etc. The rest of this extended abstract is organized as follows: In Section 2 we establish 
the context and technical background of our work. In Section 3 we present the polynomial learnability 
results for deci­sion lists and single literal decision trees. In Sec­tion 4 we discuss arbitrary decision 
trees of polyno­mial size and convex combinations of concepts with bounded spectrum. Summary and open 
problems are in Section 5. LISTS AND TREES 293  2 Preliminaries In this section we briefly review the 
Fourier ap­proach to learning. The only new point is equa­tion (2) which suggests Theorem 2.2, namely, 
that the techniques in [LMN89] extend for probabilistic concepts. The set of objects is Qn = {O, I}n. 
Elements of Qn are denoted by n-bit vectors i = Z1 . . . x~.  A n-concept (or simply a concept when 
n is well understood) is a function from the n-cube Q~ to the unit interval, p : Qn + (0, 1].  Pn is 
a set of n-concepts. P = Un~n is a concept class.  A sample of a concept p is a pair (2,13), where 2 
is drawn uniformly from Q., and 1~ is 1 with probability y p( F) and O with probability 1 p(~).  A fmction 
@ is an c-approximation of p if (1/2n)xi lp(~) -F(ql < ~.  Definition 2.1 [Analogous to [K S90]] A concept 
class P is learnable in the uniform model and in time-complexity f(n, c, 6), if there is an algorithm 
that for any n and any p E P. takes as input c, b and m independent samples of p and produces with probability 
1 6 a hypothesis @ which is an c-approximation of p; moreover the running-time is f (n, c, 6). lVe say 
that P is learnable in poly­nomial time if f (n, e, 6) is polynomial in n, c-l, and log 6-1. We say that 
P is learnable in slightly super-polynomial time if f(n, 6, f5) is of the form (nc-l)polylogn.- and polynomial 
in log b l. Concepts can be viewed as elements of the 2n­dimensional vector space of all real valued 
func­tions on the n-cube (one dimension for each domain­point ). In this context, determining concepts 
by their values on each vertex of the n-cube is equiv­alent to using the standard basis. Of course the 
obvious difficulty of the learning task is that all di­rections of the standard basis are equally important, 
and we are required to correctly approximate the projection of p on all but a vanishingly small frac­tion 
of these directions after seeing the behavior of p on a vanishingly small fraction of the directions 
(i.e. the small set of samples). In an effort to over­come this fundamental difficulty, Linial, Mansour, 
and Nissan [LMN89] introduced the idea of switch­ing bases from the standard basis to a Fourier basis, 
so that the projection of p along most directions of the Fourier basis is negligible. Hence the number 
of non-negligible directions is comparable to the small number of samples. The Fourier basis for the 
set of all real valed funct­ions on the n-cube is defined as follows: For each S C [n] consider a parity 
function associated with S in the natural way: XS(Z) = ( -l)wrs(Z) where pars(~) = O if ~ies Zi is even, 
and pars(d)= 1 if Eies xi is odd. It is well known and easy to verify that Us {XS} are indeed an orthonormal 
set with respect to the inner product (p, q) = ~ ~zP(E)q(~)-Hence, any real valued function on the n-cube 
(therefore any concept p) can be written as p(i) = ~(p, xs)xs(q. s The Sth Fourier coefficient of p is 
a(s) = (p, xs) (1) ~ (12(+-q where (?, 1=) and the ( ii, lti, ) s are independent sam­ples of p. The 
learning algorithms that follow are crucially based on the fact that Foum er coefficients are simply 
averages, as suggested by (1), and that these averages can be efficiently estimated, as sug­gested by 
(2). In particular, say that a concept class P has bounded spectrum, if for all concepts p E P all high 
frequency Fourier coefficients (the natural notion of frequency for XS and a(S) is the cardinality of 
S) are negligible in the following sense: h:pt>k a (s) < p01y(n)2-kc ~S:[Sl>k 2(s) s 2/2 where c is 
a constant and k = poly(log n, log ~ 1 ). Then we may approximate the Fourier represen­tation of p along 
the following lines: First use G(S) = O for all S: ]. ill > k. Then use m in­dependent samples (ijl, 
12, ),. . . . (jl~, lC,,, ) and com­ pute ii(S) = (l/m) ~~1 l~,( l)wrs(tii) for all S: IS] < k. If m 
is some appropriate polynomial in n and log ii-l, then straightforward Chernoff k H b &#38;nds guarantee 
that, with probability y at least 1 1 6, la(S) ii(S)/2 < k-l ~ C2/2 for each S. () Hence ~ (a(s) -ii(S))2 
 ,2/2 < S:lSl<k AIELLO AND MIHAIL Now if we use as a hypothesis for p the concept ~ whose Fourier coefficients 
are the ii s, then we have: + X. IP(2) -il~)l = < /+ J(GziG@ G(P(~) -W)))2 &#38; (a(s) -W))z (3) where 
the last two lines follow by Cauchy -Schwart z and Parseval respectively. Now this last quanity is bounded 
by e by the previous remarks, and hence @ is a an e-approximation for p with high probability. (The technicality 
that for some i s we may have ~(~) < 0 or @(F) > 1 can be trivially bypassed by setting these values 
to O or 1 respectively; notice that this only helps the errors in (3)). All the above imply: Theorem 
2.2 [Extension of [LMN90]] lj a proba­ bilistic concept class P has bounded spectrum, then P is learnable 
in the uniform model and in dightly super-polynomial time. In [LMN89] it was shown that the class ACO 
has bounded spectrum and is hence learnable in slightly super-polynomial time. The intuition behind their 
result is as follows: The spectrum of the class ACO concentrates on the low frequencies because it should 
differ significantly from the spectrum of the parity function which is known not to be in ACO [Y85] [H86], 
and whose spectrum consists of a sin­gle frequency: the highest . The results that we observe in Section 
4 with respect to polynomial-size probabilistic decision trees (as well as the stronger polynomial results 
of Section 3) are justified by the same intuition: The spectrum of poly-size decision trees concentrates 
on the low frequencies because it should differ significantly from the spectrum of par­it y which requires 
exponential size decision trees .  3 Probabilistic Decision Lists and Single Literal Decision  Trees 
In the previous section we sketched how bounded spectrum concept classes can be learned in the uniform 
model and in in slightly super-polynomial time. In fact, without using additional structure, this is 
probably the best possible since roughly (lO~n) Fourier coefficients must be approximated. The main contribution 
of this paper is to show that for lists and single literal decision trees there is a polynomial size 
subset of the lowest (l n) fre­quencies in which most of the power is concentrated, and that furthermore, 
this subset can be efficiently identified. Thus we obtain the first nontrivial ex­ample of a concept 
class with bounded spectrum for which it is possible to take advantage of further properties of the spectrum 
and get a polynomial time uniform learning algorithm. The case of lists is treated in detail. The case 
of decision trees follows along the same general lines and is simply sketched. Before presenting the 
learning algorithms let us formalize some useful terminology. As mentioned informally in the introduction, 
a decision list on n variables is a single-branch bi­nary tree with edges labeled by literals xi and 
their negations ~i, so that if the right edge of an internal node is labeled ~i, (resp. ( Zi ) ) then 
the left edge is labeled ~i, (resp. (z;)). There are n + 1 leaves la­beled by pl . . . pn+l. Any Z E 
Q. naturally follows a path in the tree from the root to a unique leaf. The value of a decision tree 
on z E Qn is the label of this unique leaf. To formalize the order in which the literals appear along 
the decision list we introduce a permutation m of [n] . For example, in Figure 1 we have T(1)= 3, T(2)= 
1, 7F(3) =4, 7r(4)= 2. In general the ith level edges of the list are labeled ~=(i) and Z=(i). Con­ versely, 
z~ and ~i are on level r 1(i) of the list. For convenience define the level function /(i) = m-l ( i). 
TO denote whether ~T(i) or ~T(i) label the left edge of the i-th branch of the list, we introduce an 
n-bit O-1 vector ~. For example, in Figure 1 we have ~= 1101 since the left edges are labeled by 23, 
c1, 24, Z2. We further use the notation di to denote the complement of di: ~i = 1 di. o To denote the 
partition of the n-cube which is naturally suggested by the branches of the decision list we introduce: 
Zm(l) = dlj. . .7z=(i_l) = ~ 1> C;= {Z: x~(i)Zdz}, fOr1~i$n G+l = {d,... dn} Clearly, [C i[ = 2n-i for 
1 < i ~ n, and of COUrSe ICn+,l = 1. In the above terms, a decision list is formally defined as follows: 
 Definition 3.1 A concept p over n variables is a decision list if for some permutation T of [n], some 
n-bit O-1 vector d, some pl, . . .. p~+l E [~, 1], and all~= Z1. ..zn E Qn the following holds: ii? E 
Ci ~ p(z) = pi, where the Ci S are as previously defined. LISTS AND TREES For a subset of variables 
{xi : i E S C [n]} itturns out crucial to identify the variable which is farthest down the list. To do 
this we define the maximum level of S or MazL(S) to be j~ S: i(j) 2 l(i) for all i ~ S. Now z~a=~(s) 
is the desired variable. In turn, this suggests a partition of all the subsets of the variables according 
to the maximum level variable in the set: 0, {S : MazL(S) = l},..., {S : MazL(S) = n}. Clearly 1{S : 
MazL(S) = i}l = 2i-1, so that the cardinalit ies of partition classes are polynomial for i=o(]ogn), The 
lines along which we shall efficiently approxi­mate the spectrum of decision lists are, roughly, the 
following: (i) ~s,~~~~(s)>i a2(S) < 2-i, which suggests that for the purpose of approximating the Fourier 
rep­resentation it su#ices to approximate each one of a(S) such that Maz L(S) s i, for i = O(log n). 
Now the set {S : MazL(S) s i} is the powerset of {7r(l) ,. ... m(i)} so there are only polynomially many 
coefficients to approximate, for i = O (log n ). Each one of these coefficients can be approximated efficiently 
and satisfactory in polynomial time by sampling. (ii) The function T on 1,..., i can be approximated 
satisfactorily in polynomial time by further sam­pfing. Point (i) is justified by Lemma 3.2 below which 
cap­tures all the structure of the spectrum. Point (ii) is justified by Remark 3.3 and the learning Algorithm 
1 that follows. Lemma 3.2 For all S G [n], if MazL(S) = i then la(S)l = ~12n-ipi -~ Zn-jpj -ILz+11(4) 
j=i+l [a(S)/ = la({7r(i)})l S 2-i (5) a2(S) < 2 i (6) E S:iVfc3L(S)>i PROOF. To see (4) recall by (1) 
that ZP(Z) (-1)=(=), . *12n-ipi -~ 2 -jf)j -p.+~1 j=i+l where the last two equalities are fairly easy 
to jus­tify as follows: o Clearly S ~ {7r(l), ..., m(i)}. For each j such that ~ < i, we argue that ~3eCj 
( l)mrs(~) = O. This is because all the vectors in Cj have coordinates %(1)> . ..7Z ~(j_l), z=(j) forced 
to dl ,. ... dj_l, dj respectively, while each one of the coordinates is free to vary in {O, 1}. Conse­quently, 
for all such vectors the parity of the sum of their bits that belong to S fl {Zr(l) . . . z=(j)} is fixed. 
And on the other hand, the parity of the sum of their bits that belong to S n {~m(j+l) . . . zT(;)} k 
half the times even and the other half odd. Hence, the quantity ( 1 )Wrs(s) averaged over all vectors 
in C j is zero. %r(j+l) . . . %(i) For j = i, there are 2m i vectors in Ci. Moreover, all vectors in 
Ci have coordinates x=(l), . . .. ~m(i_1)7 Zfi(i) forced to dl, . . .. &#38;_~, di respectively. Hence 
pars~ is fixed. Similarly, for i < j ~ n, there are 2 ~ vectors in Ci. Furthermore all vectors in Cj 
have coordinates x=(l), . . . . Zn(q forced to ~1, . . . . d~ respectively. Hence, pars(i) for E 6 Cj, 
i < j ~ n, is the complement of pa~s(~) for i E Ci. The case when j = n + 1 is easily seen to be pm+] 
with the sign as given in (4). Equation (5) follows from equation (4) by notic­ing that all the pi s 
are in [0, 1]. To verify (6) recall that there are 2~-1 elements in {S : lIfazL(S) = j}, and by (5) each 
of them is at most 2 i. 1 Remark 3.3 AS we discussed before, in principle, for our learning purposes 
we would wish to know the set X(i) = {7r(l),7r(2),. . . . ~(i)} for, say , i = O(log n). If it was the 
case that [a({~(i)})l = 2 i, then X(i) could be identified as follows: (a) For each m(j) . E X(i) we 
would have la({m(i), ~(j)})l = 2-Z which follows horn (5). (b) For each r(jl ) @ X(i) (hence jl > i) 
and for all jz, we would have Ia({m(jl), r(jz)})l s 2 (i+l) by [5) since li4azL({~(jl), 7r(j2)}) > i. 
 Hence if it was the case that Ia({r(i)})l = 2-i, then we would be able to use a small sample, ap­proximate 
with high probability y all a({r(j)} ) s and a( {T(jl ), m(j2 )}) s up to arbitrary accuracy, and by 
 (a) and (b) above this would suflice to isolate X(i).  However, since the condition [a({x(i)})l = 2- 
is in general not true, we need some further technical­ities. In particular, let io be max{j : a ({m(j)}) 
z 2-*}. Clearly, iO < i by (5). Let further Xo(i) = AIELLO AND MIHAIL {7r(l), T(2), . . . . m(io)}. Clearly 
Xo(i) ~ X(i). The algorithm that follows uses the idea of comput­ing a(S) for ISI = 1 and [Sl = 2 that 
was de­scribed in (a) and (b), and isolates some X* : Xo(i) Q X* ~ X(i). We will argue that this sufhces 
for learning pwposes. ALGORITHM 1: Learns Decision Lists BEGIN Stage 1: Approximate Xo(i) and X(i) by 
X*. Set m := 16n2e-4 (log2(n + 1)+ Iogd-l) ; Seti:=log26-2 +logn; Input m samples (rl,ld,),...,(~~,l~m,) 
; X :=g; For jI:=l to n do ii({jl}):=~ ~tltit(-I)par{JI}(~ ) ; For j2 := 1to n, such that ~z# jl do 
 i({~lt~2}) := ~ ~tl~,( l)pariilj~](~ ) ; If ii({j1})z~2-i or for some j2 Zl({jl, j2})>~2-i then X* := 
X* U{~1} ; Stage 2: Approximating the Spectrum. Setm:=n@ (log8n+log8-1 +logC-2) ; Input m samples (~l,lfi),. 
..,(r~,l~n,) ; For each S ~ X*, G(S) := ~ Et lU,( l)~ s(U ) ; END. -, The hypothesis is P j(z) = -&#38;x. 
Zl(s)(-l)p= (=); (If j(~) <-0, then @(i?) := O; If ~(z) >1, then ~(~) := 1;) Claims 3.4 and 3.5 below 
justify the correctness of Algorithm 1. claim 3.4 At the end of Stage 1, XO(i) ~ X* g X(i) with probability 
at least 1 6/2. PROOF (sketch). Follows in the spirit of Remark 3.3 and standard Chernoff bounds. Claim 
3.5 At the end of Stage 2, ~ ~z /p(Z) fi(iE)/ s e with probability at least 1 6/2. PROOF (sketch). 
Assume that at the end of Stage 1 X* is as in Claim 3.4, so that there are at most 2i ii(S) s to be approximate 
ed. Then standard Chernoff bounds suggest that for the particular choice of m the sum of the squares 
of alJ these 6(S) s is bounded by 62/2 with the desired probability y. Hence: Where the last bound holds 
because: ~Sg.1 (io) a2(S) = ~s:iw.zqs)z;, a2(S) 2;=%+, zs:M=.L(s)=j a (s))  {( a2(S)  + ZS:ilf.zL(S)>iO 
~~=i[,+l (XS:Jf.zL(S)=j 2-2;) + 2- < by (5), (6), and the definition of i. 2j2-2i + 2 i < ~2 i {. S ~~=i~+l 
< t2/2, by choice of i in ~lgorithm 1 1 All the above imply: Theorem 3.6 The class of probabilistic 
decision lists is learnable in polynomial time and in the uni­form model via Algorithm 1. In the rest 
of this section we sketch the struc­ture and learning algorithm for decision trees with a single occurrence 
per literal. The idea here is complete] y analogous to the case of decision lists (see Figure 3). % ~iyre 
3 A probabilistic decision tree with single occurrence per literal over n variables can be described 
as fol­lows: Let T be a binary tree with at most n interior nodes. Consider a labeling of the nodes of 
T with the variables Z1, . . ., Zn, so that each variable labels at most one node (see Figure 2). Consider 
a label­ ing of the leaves of T with numbers in [0,1]. Each element 3 of the n-cube follows a path of 
the tree from the root to a unique leaf. The value of the probabilistic decision tree on E is the label 
of this unique leaf. Analogously to the permutation r of decision lists (which can be also viewed as 
a total order on [n]: m(i) < ~(i + l)), a decision tree defines a partial order u on [n] in the natural 
way: If Zi labels a descendent of Zj, then i >. j. Again analogously to decision lists, for some S ~ 
[n], if all elements in S are related in a (hence all elements in S appear on some path from the root 
to a leaf) then define Maz( S) to be the largest element in S. The struc­tural Lemmas 3.7, 3.8, and 3.9 
that follow are the analogues of Lemma 3.2 for decision lists, and can be shown by similar manipulations 
(the proofs are left for the complete paper): Lemma 3.7 For all S ~ [n], if there are at least two elements 
in S that are not related in c , then a(S) = O. Lemma 3.8 For all S C [n] such that all elements in S 
are related in at let i := Maz(S). Then a(S) = a({Maz(S)}) < 2 . Let Yl,.. . , yk be the sets of variables 
that appear along each one of the k paths from the root to each leaf (k < n). Let Xl(i),... ,Xk(i) be 
the subsets of the Yj s that correspond to the i smallest variables of each Yj, that is, the first i 
nodes that are encoun­tered along each path (and if the path is shorter than i then the corresponding 
Xj (i) equals to the corresponding Yj ). So to approximate the spectrum of the tree, it sufiices to approximate 
a(S) for S ~ Xj (i) for all j. There are at most n such Xj(i) s and the powerset of each one of them, 
for say i = O(log n), is polyno­mially small. Hence there is a polynomially small number of coefficients 
to be approximated. Furthermore, the crucial sets Xj(i) can be ap­proximately isolated as in Stage 1 
of Algorithm 1 by estimating a(S) for ISI = 1 and [St = 2, roughly, as follows: For each jl E [n], let 
Xj, (i) = {jl} U {jz : ii({jl, j2}) Z ~2-i};  If [X~l(i)l > clogn then X~l(i) := 0;  The sets Xi(i) 
are approximated by the sets x;,(i); All this discussion can be formalized to Theorem 3.10 which concludes 
the section. Theorem 3.1o The class of probabilistic decision trees with a single occurrence per literal 
is learnable in the uniform model and in polynomial time.  4 Generalizations In this final section we 
argue about general poly­nomial size decision trees, as well as convex con­vex combinations. Proofs here 
are either omit ted or simply sketched. However all details that have been left for the complete paper 
are straightforward to reconstruct. A probabilistic decision tree of polynomial size can be described 
as follows: Let q(n) be a fixed poly­nomial. Let T be a binary tree with at most q(n) interior nodes. 
Consider a labeling of the nodes of T with the variables z 1, . . .. Zn. Consider a label­ing of the 
leaves of T with numbers in [0,1]. Now, as usual, realize that each element i? of the n-cube follows 
a path of the tree from the root to a unique leaf. The value of the probabilistic decision tree on 3 
is the label of this unique leaf. As mentioned in Section 2, and exactly as in [LMN89], polynomial-size 
decision trees are very far from the parity fwction, which results in bounded spectrum and Iearnabilit 
y. Theorem 4.1 [Straightforward Extension of [LMN89]] The class of probabilistic decision trees of polynomial 
size has bounded spectrum. Therefore, the class of probabilistic decision trees of polyno­mial size is 
learnable in the uniform model and in slightly super-polynomial time. PROOF (outline). The proof of Theorem 
4.1 fol­lows identically along the lines of proof of the Main Lemma (Lemma 9) in [LMN89] (with the additional 
ease that the use of Hastad s Switching Lemma is unnecessary). In particular: . First of all, notice 
that if a probabilistic polyno­mial size decision tree is hit with a suitable ran­dom restriction , then 
with high probability the re­stricted concept is a decision tree of small depth. The reason is, roughly, 
that every long branch of the tree will be forced and hence chopped-off in small depth. Second of all, 
notice that like small depth boolean decision trees in [LMN89], small depth probabilistic decision trees 
have absolutely bounded spectrum. AIELLO AND MIHAIL More precisely, it is easy to see that if a probabilistic 
decision tree has depth k then a(S) = O for all S : ISI >k. Now the next steps are fairly technical, 
but identi­cal to the manipulations in Lemmas 5 through 9 in [LMN89]. Very roughly, the idea that the 
random­restriction method suggests is that any poly-size probabilistic decision tree can be written as 
an aver­age of many other probabilistic decision trees most of which are of small depth. This, in turn, 
trans­lates to bounded spectrum.  Once the boundedness of the spectrum has been established, Theorem 
4.1 follows by Theorem 2.2.  1 We finally argue about arbitrary convex combina­tions of bounded spectrum 
concepts. The weighted arithmetization of k-DNF follows as a special case.  Recall that for functions 
gl, g2, . . .. gN, a convez combination of the gi s is a sum of the form xi Aigi where ~i }; = 1 and 
all &#38; s are in [0,1]. Theorem 4.2 If gl, gz, . . . , gN are bounded spec­trum probabilistic concepts 
over n variables (N is arbitrary), and if g is a convex combination of the gi s, then g is a bounded 
spectrum probabilistic con­cept. PROOF. First notice that g is indeed a probabilis­tic concept in the 
sence that the range of g is indeed [0,1] (g is the average of quantities in [0,1]). Then realize that 
for all S S ag(S) = ~i A~ag, (S). Hence: )klSl>k a:(s) = ~S:[Sl>k (~i -Mds))z <(7) ~S:lS[>k xi ~af, (s) 
= xi i ~S:lSl>k al,(s) xi J~pOly(n)2-kC poly(n)2-kC n Now notice that if p is some weiqhted arith­metization 
of k-DNF, that is, p is of the form P(S) = xi ~ici(~), where the ci s are products of k variables or 
their negations, and since acl (S) = O for all ci s and ISI > k, then (7) suggests that For constant 
k, this suggests Xs:lsl>k a;(s) = 0. that there are only 0( nk ) coefficients to be approx­imate ed. 
Therefore: Theorem 4.3 The weighted arithmetization of k- DNF is learnable in the uniform model and 
in poly­nomial time. Furthermore and very interestingly, the strong condition ~S:lSl>k a~(S) = O coupled 
with Man­sour s techniques [M90] suggest: Theorem 4.4 The weighted arithmetization of k-DNF is distribution-free 
learnable in polynomial time. In this sense, it might be interesting to formalize some natural arithmetization 
of ACO and check if the results in [LMN89] extend.  5 Summary and Open Problems Here we used Fourier 
analysis to obtain polynomial­time uniform-learning algorithms for probabilis­tic decision lists and 
single literal decision trees. We further observed that straightforward exten­sion of [LMN89] suggests 
slightly super-polynomial uniform-learning algorithms for arbitrary proba­bilistic decision trees of 
polynomial size. Of course the most challenging question is to ex­tend these results in the distribution-free 
model, or obtain negative evidence for such extensions (in some sense like [KV89] ). It might also turn 
out interesting to pursue a careful study of the wide consequences and uses of Nyqnist s theorem, and 
see how much carries over to the finite case. References [BEHW86] A. Blumer, A. Ehrenfeucht, D. Haus­ 
sler, and M. Warmuth, Learnability and the Vaprnik-Chervonenkis Dimen­sion , Journal of the ACM, 36(4), 
1989, Pp, 929-965. [H86] J. Hast ad, Computational Limitations of Small Depth Circuits , Ph.D. Thesis, 
MIT %ess, 1986. [KS90] M. Kearns and R. E. Shapire, Efficient Distribution-Free Learning of Proba­bilistic 
Concepts, PTOC. of the 31st IEEE Symposium on Foundations of ComputeT Science, 1990, pp 382-391. [Kv89] 
M. Kearns and L. G. Valiant, Cryp­tographic Limitations on Learning Boolean Formulae and Finite Au­tomat 
a, Proc. of the 21st ACM Sym­posium of Theory of Computing, 1989, pp 433-444. [LMN89] N. Linial, Y. 
Mansour, and N. Nis­san, Constant Depth Circuits, Fourier Transforms, and Learnability , Prcx. of LISTS 
AND TREES 299 the 30th IEEE Symposium on Founda­ tions of ComputeT Science, 1989, pp 574-579. [M90] Y. 
Mansour, Learning via Fourier Transforms , Spring 1989, preprint. [R87] R. Rivest, Learning Decision 
Lists , Machine Learning, 2(3), 1987, pp 229­ 246. [V84] L. G. Valiant, A theory of the Learn­ able 
, Communications of the ACM, 27(11), November 1984, pp 1134-1142. [Y85] A. C. Yao, Separating the polynomial- 
Time Hierarchy by Oracles , PTOC. of the 26th IEEE Symposium on Founda­ tions of ComputeT Science, 1985, 
pp 1­ 10. 
			