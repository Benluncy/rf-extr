
 PARALLEL COMPUTING ON PERSONAL COMPUTERS O. Vornberger Dept. of Math. &#38; Computer Science University 
of Paderborn W.-Germany Abstract , ~-~f~Personal Computers is connected to form a ring-structured parallel 
system, Each computer has access to its local memory and can communicate with its two neighbours in 
the ring. This network of asynchronous processors is used to solve in parallel combinatorial optimization 
prob- lems that are too time-and space-consuming to be handled on a single personal computer. Heuristics 
are developed to simulate in distributed memory the typical data structures needed in branch-and-bound- 
algorithms: A single priority queue is maintained and updated in several heaps with very little syn- 
chronization overhead. To show the performance of the ring a parallel version of the Travelling-Salesman-Problem 
is im- plemented. Execution times and speedups for 50 ran- dom graphs solved with up to 16 ring members 
are discussed. I. Introduction It was only within a few decades that computer manufacturers managed 
to reduce the first (in a geometric sense gigantic) electronic computing systems to small handsome machines. 
At the same time they improved the performance by enlarging in- ternal and external memories and by increasing 
the number of instructions per time unit. This minia- turization has been made possible bynew fabrica- 
tion techniques and the idea of very large scale integration. Permission to copy without fee all or part 
of this material is granted provided that the copies are not made or distributed for direct commercial 
advantage, the ACM copyright notice and the title of the publication and its date appear, and notice 
is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, 
or to republish, requires a fee and/or specific permission. X_. &#38;#169; 1986 ACM 0-89791-21 I-.4/86/1200-0115 
75Â¢ The invention of the microprocessor about 15 years ago allowed it to create a new type of computer, 
the so-called "personal computer". Its name stressed the fact that this electronic device was at a single 
person's command, equipped with all the necessary periphery hardware like disc unit, key-board, monitor, 
printer, etc. So, contrary to the philosophy of a mainframe, the user has personal, i.e., exclusive access 
to the electronic resources. Another, oftenused term "home computer" gives a reference to the operation 
location: More and more private households use a computer for business and/ or private purposes. Compared 
to the first exponents in this classl (like the PET 2001 of Commodore) the present day personal computers 
behave like universal mainframes. Especially, the introduction of 16-Bit-processors in 1978 significantly 
improved their capabilities. A large internal random access memory of up to I Million Byte gives room 
for the execution of powerful operating systems, efficient compilers for high level programming languages 
and comfortable application software. So its small measurements and universal capabilities makes the 
personal computer an ideal tool for representatives of many business branches like salesmen, engineers, 
doctors, stu-dents, etc. The tremendous progress in the performance of hard-ware components has -unfortunately 
-no counter- part in the area of software. Since many years pro- "grammers try to develop efficient algorithms 
for various types of problems. However, especially for certain problems in the area of operations research 
and (what today is known as) artificial intelli-gence all solution methods turned out to be very 115 
 time-and/or space-consuming. In the beginning of the seventies Cook [I] and Karp [4] introduced the 
notion of NP-completeness which explained the reason for all these unsuccessful attempts. This term char- 
acterizes a class of combinatorial problems that behave -relative to their inherent solution com-plexity 
-equivalent in the following sense: I. Any algorithm that solves any of the NP-complete problems fast 
(i.e., in polynomial time) can be transformed to fast algorithms for all NP-com-plete problems. 2. The 
proof that a specific NP-complete problem cannot be solved in polynomial time can be ex- tended to the 
whole class. Members of this class are e.g.: Find an independent set of nodes in a graph; find a satisfying 
truth assignment in a Boolean formula; color a map such that adjacent countries get different colors. 
Since no one has ever found a fast algorithm for any of these problems, the term of NP-completeness can, 
despite of the absence of a formal proof be used as a synonym for intractability. So, NP-complete prob- 
lems will -at least in the worst case -consume an intolerable amount of execution time, even for mod-est 
sized problems. (For a comprehensive introduc- tion to the notion of NP-completeness see [2].) At this 
point we begin to see that the personal computer in -say -an engineering office despite its fast processor 
and large memory still seems not powerful enough to solve certain types of opera- tions research problems 
simply because it runs out of time and out of memory. In this paper we show a very simple way to over- 
come this difficulty. We show how one can combine the computing power of several personal computers such 
that they are now able to solve problems that are too time- and space-consuming to be handled on a single 
computer. This is done in two steps: I. Expansion of hardware: By nothing more than a few cables the 
personal computers will be con- nected to form a ring structured parallel system. 2. Modification of 
software: A typical solution method for operations research problems, namely a branch-and-bound algorithm, 
is adapted to this parallel architecture. This requires the design of heuristics that simulate in distri- 
buted memory data structures typically needed by sequential branch-and-bound: A single priority queue 
is maintained and updated with very little synchronization overhead. Both steps, the expansion of the 
hardware and the modification of the software are relatively easy to implement on a set of personal computers 
that ful-fill certain minimal requirements (see below). The main advantage lies in the fact that by this 
approach a true parallel system is organized with- out the need of special designed multiprocessor hardware. 
We show that the personal computer, a mass product available at low rates, is powerful enough to act 
as an integrated part of a loosely Coupled distributed system. The performance of our'parallel architecture 
is demonstrated by means of a parallel solution of the Travelling Salesman Problem. Our approach has 
also been used to implement paral- lel backtracking. The experiences relevant to these topics have been 
reported in [7, 8]. Parallel Branch-and-Bound for the Travelling Salesman Prob- lem has been studied 
by J. Mohan [6]. He used for his experiments the CM*, a multiprocessor system built at Carnegie Mellon 
University, consisting of a hierarchical network of LSI-II computers. The paper is organized as follows. 
Section 2 pre- sents the hardware configuration, Section 3 and 4 describe sequential resp. parallel branch-and-bound. 
In Section 5 the computing times and speedups result- ing from our implementation are discussed. Section 
6 closes with suggestions for further research. 2. The hardware Let us start with a description of the 
typical prop- erties of a modern personal computer like the SIRIUS I, used for programming classes at 
the University of Paderborn: The SIRIUS has - its own microprocessor (Intel 8088) - its own main memory 
for program code and data (256 KB) -two serial ports (RS 232) to the outside world: Each port can send 
one byte tat a transmission rate of - receive one byte ~96oo Bits/sec. - check whether a byte has arrived 
 Programs written in Pascal can be executed by means of a Microsoft Compiler that allows, besides the 
standard language constructs, to access the two serial ports. It seems very obvious how we can use these 
proper- ties to construct from a set of personal computers a multiprocessor network: All computers form 
a ring by connecting the left serial port to the left neighbour and the right serial port to the right 
neighbour. So the basic ingredients of any distrib- uted system, namely the processing units (called 
nodes) and the connections between them (called links), are represented by independent stand alone computers 
and point to point communication lines. It is clear that the ring is the only topology that we are able 
to construct since each node can be connected to two neighbours. Moreover, because of the slow transmission 
rate it takes quite a while to send voluminous data packages. However, compared to networks built around 
a central system bus there are two essential advantages: I. Not only parallel computation is possible 
but also parallel communication. A system bus forms, in contrast to direct point to point connec-tions, 
a bottleneck, since he can manage only one pair of nodes talking to each other at a time. 2. The size 
of the parallel system is (at least theoretically) unbounded. In contrast to a system bus any number 
of nodes can be inserted into the ring and (hopefully) increase the computing power. Because of the very 
restrictive communication possi- bilities it will be the duty of our software to cope with these handicaps 
and we will see later that it is in fact possible (up to a certain degree) to use the ring as a multiprocessor 
system for our pur-poses. 3. Sequential branch-and-bound Branch-and-bound is a popular algorithm design 
technique that has been used successfully for the solution of combinatorial problems in the area of artificial 
intelligence and operations research. Many theoretical and practical problems such as travelling salesman, 
job-shop scheduling, knapsack, vertex cover and graph coloring can be formulated as combinatorial optimization 
problems where we are required to find a solution vector x that minimizes some criterion function f(x) 
subject to a set C of constraints. In a branch-and-bound approach the search through the solution space 
is organized by a partitioning algorithm that repeatedly decomposes a problem into smaller and smaller 
subproblems until the best so-lution is found. Simplified spoken: among all part-ly solved subproblems 
the most promising is pursued next. A survey of branch-and-bound methods in general was presented by 
Lawler and Wood [5]. For our purposes it is more convenient to choose as an example the travelling salesman 
problem and describe a typical branch-and-bound strategy to solve this problem. The travelling salesman 
problem is to find a mini- mum cost roundtrip tour that visits each of N cities once, given the costs 
of travelling from one city to another of those N cities. In other words: We are given an undirected, 
weighted graph G = (V,E,c) with node set V, representing the cities, edge set E, representing the roads 
between the cities and cost function c:E ~ ~U {~}, repre-senting the length of a road (~ denotes the 
non-negative integers; if there is no road between city i and j the cost of edge (i,j) will be ~ = infinity). 
Among all Hamiltonian cycles, i.e., cycles that visit each node exactly once, we have to find the shortest. 
If the graph G forms a clique then there is always such a cycle; if the graph is sparse, i.e., several 
edges are missing, there may not be a Hamiltonian cycle of finite length. A brute force algorithm, namely 
checking all N! city-permutations, leads to an unacceptable amount of computing time, even for modest 
N. On the other hand, the average running time of a branch-and-bound strategy is much smaller, although 
it is still ex- ponential in the worst case. We have to pay for this with an enormous consumption of 
storage, which is used for the implementation of a priority queue. This priority queue, also often called 
heap, con- tains subproblems, i.e., partly solved instances, which are weighed by a bound b. The heap 
supports the insertion of an arbitrary subproblem and the re-moval of the cheapest subproblem. So the 
bound b serves as a heuristic to indicate which subproblem is an ideal candidate to be investigated next 
and to generate in turn new subproblems. The following is a simplified version of a branch- and-bound 
strategy of Held and Karp [3]. Let L = (W,b) be a subtour, i.e., W is a path in G, starting at node I, 
b is a lower bound for W mean- ing, that every Hamiltonian cycle that contains W will cost at ]east b. 
(We will describe later how to compute b.) Let us organize the subtours in a heap H, which allows us 
quick access to the cheapest subtour (i.e. the subtour with the smallest lower bound). Let LÂ° = (Wo,bo) 
be a temporary (not necessarily opti- mum) solution, i.e., Wo represents a HamiltoniaD cycle with cost 
b o. Call a subtour L = (W,b) rea- sonable, if b < b o, i.e., if its bound is smaller than the bound 
of the temporary solution. Then the algorithms reads: Compute an initial temporary solution L o = (Wo,bo), 
e.g. with an approximation algorithm "nearest neighbour". Put L = ({1},0) as first subtour into the heap 
H. While H contains reasonable subtours do begin Remove L = (W,b) the cheapest subtour from the heap., 
Form from L new subtours L 1 ..... L k by expand- ing W at its last vertex x to all neighbours of x that 
do not already occur in W and compute the corresponding lower bounds b I ..... b k. If one of the new 
subtours forms a Hamiltonian cycle that is shorter than the current cycle W o then replace L o by this 
new temporary solution. For i := 1 to k do if L i is reasonable put L i into the heap (i.e., only tours 
that can improve the current solution L are stored for further 0 examination). end; The optimal solution, 
i.e., the shortest Hamiltonian cycle, is W o and has length b o- Notice that towards the end of the computation 
the heap may contain many "unreasonable" subtours that had been inserted at a time where their bound 
was still smaller than the best solution found so far. If this superfluous subtours cause a heap overflow 
we can restore the heap by a garbage collection routine. If, however, there are more reasonable sub- 
tours than possible heap entries the insert proce- dure.has to quit via an error exit and the computa- 
tion aborts. The correctness of the algorithm is obvious, its efficiency depends on the calculation of 
the lower bound b. There are many ways to do this, Held and Karp propose to use a spanning tree. More 
precise: A subtour W divides the nodes of G into those al- ready visisted and those still to be visited. 
Call the still unvisited node set R. Let T be a minimum spanning tree in R, i.e., a tree connecting all 
nodes in R with edges from G with total minimum edge length. Then a lower bound for W is the cost of 
W plus the cost of the cheapest edge from the last node in W to a node in R plus the cost of T plus the 
cost of the cheapest edge from a node in R to the first node in W. The use of this sum as a lower bound 
for W is jus- tified by the fact, that the cheapest Hamiltonian extension that connects the end of W 
to its begin-ning is a spanning tree for R. Spanning trees can be computed fast (Yao [9]), so what one 
expects is an intelligent guidance through the solution space by means of an easy to calculate heuristic 
function. Note that for a clique G a subtour L = (W,b) can always be extended to a Hamiltonian cycle 
since G has a complete set of edges with finite lengths. In sparse graphs, however, a subtour L = (W,b) 
may not be extensible although b < ~, i.e., the con- struction of the spanning tree may yield a finite 
value b as lower bound for W but there is no Hamil- tonian cycle containing W. So W = (W,b) would un-necessarily 
consume storage in the heap and never contribute to a solution. In order to detect such a situation as 
soon as possible we run a test on bi- connectivity on the graph resulting from W and R, i.e., only those 
subtours are stored in the heap that represent biconnected graphs. Since this prop-erty is a necessary 
condition for Hamiltonian cycles we can hopethat by this heuristic the heap size will be reduced. 4. 
Parallel branch-and-bound If we recall our system architecture we realize that it consists of independent 
processors that can run private code and send messages to the neighbours. Especially we notice that our 
ring members do not have access to a global memory that could distribute subproblems to idle processors. 
An obvious answer to this handicap is to distribute somehow one global heap to all network nodes, i.e., 
to implement a local heap for each processor. Then, by exchanging heap parameters all ring members keep 
each other informed. Let us define the following procedure communicate begin if heap contains no reasonable 
subtours then ask the left neighbour for a subtour. if right neighbour asks for a subtour and the heap 
contains a reasonable subtour then send it. if a new (shorter) temporary solution is found, inform the 
left neighbour. if the right neighbour sends a new (shorter) solution then update, if necessary, the 
temporary solution L 0 end; Now if we distribute the input graph to all proces-sors and incorporate the 
procedure communicate into the main while-loop of our sequential branch-and-bound algorithm we are almost 
ready to run it in parallel on the ring. What still has to be changed is the while-condition, because 
we have to go on as long as there are reasonable subtours somewhere in the ring. We have managed this 
by delegating the task of detecting the end of the computation to one special node, called the master. 
At the beginning the master is the only one having the initial sub- tour L = ({1},0) in its heap, all 
other heaps (at the slaves) are empty. A request for a subtour is expressed by the master with a "master 
request". A request for a subtour by a slave is expressed as a "slave request" unless the slave was itself 
asked for a subtour and could not provide one; in this case it formulates its need by a "master request". 
Clearly, when the master receives a "master request" he can conclude that all slaves (and he himself) 
are idle and therefore initiate a stop signal and col- lect the optimum solution. What has been described 
so far had been the first version of our parallel branch-and-bound algorithm. After we had conducted 
several experiments it turned out that some tuning was necessary, mainly to com-pensate the following 
three effects: 1. Initially the work is distributed extremely un- even: The master has the whole input 
problem, all slaves have empty heaps. So they all start at the same time to ask their neighbours for 
work and it takes quite a while until the first subproblem (stored at the master) is splitted over and 
over again and spread to all ring mem-bers. Even then the individual start problems for the slaves are 
not of equal size since slave i constructs his work by restricting a subprob- lem of slave i-I. 2. When 
a processor runs out of work he gets a new subproblem from his neighbour. However, in most cases it takes 
only a few ~iterations to complete-ly solve this subproblem and so the processor is again idle and disturbs 
his neighbour once more. This leads to high interaction and re- sults in loss of performance since each 
time the processes have to synchronize for their commu-nication. 3. As long as a processor has subproblems 
in his heap he sees no reason to demand work from his neighbour. However, it often can be observed that 
the lower bound of the cheapest problem in one of the heaps is much ~igher than this bound in other heaps. 
In fact, it could be higher than  a solution that is still to be found. So it is clear that the processor 
working on these "rela- tive expensive" subproblems does superfluous work. So in order to support a better 
balanced work load our implementation is augmented by three heuristics: I. At the beginning all k processors 
(not only the master) put the initial subtour W = ({1},0) into the heap, then iterate the while loop 
until at least k subtours are in the heap and then processor with number i deletes from its heap all 
subtours but the i-th cheapest. This guaran- tees a fast distribution of disjoint roughly equal sized 
subtours to all processors. 2. Upon a request, not only one problem is sent but several, depending on 
some heuristic arguments such as the total number of subproblems in the heap and the difference between 
the lowest bound in the heap and the cost of the temporary solution. 3. The processor A asks his neighbour 
B for work not only with an "unconditional request" when A has run out of reasonable subtours but also, 
at certain intervalls (depending on an increase of the smallest lower bound in his heap by a cer- tain 
constant) with a "conditional request". Such a request is only granted by B, if B de- tects that his 
own smallest lower bound is smaller than A's smallest lower bound by more than this constant.  At a 
first glance, this "precautious data trans- fer" seems to contradict our aim to minimize communication. 
However, it is not a good idea to have several processors work at subproblems with too high lower bounds. 
Since the lower bound for a subproblem represents the likely- ness that this subproblem will, upon expanding, 
turn into a new and better solution, it makes sense to work on these subproblems first. In a multiprocessor 
system with a single subproblem list the k processors always work on the k smallest subproblems. So the 
additional expense for transmission is justified by a (logically) better balanced work load. Let us summarize 
the main philosophy of our implemen-tation. Parallelism is organized by a ring of inde-pendent stand 
alone computers, each of which can ex- change data with its 2 immediate neighbours at a low transmission 
rate. If it is possible to reduce the need of communication by keeping the ring members busy (i.e., keep 
them working on their local heaps) sending a few slow bytes once in a while will not affect the total 
efficiency too much. 5. Experimental results As noticed in Section 3 depending on the size of the input 
graph heap overflows may occur. In order to be able to compare execution times for different net- work 
configurations the problem class has been chosen such that all graphs could be completely solved also 
on a single processor. This policy, of course, favors the sequential case since it prohibits to use the 
full power of the parallel system which -with 16 heaps -could have solved graphs of sizes far beyond 
the capability of a one-processor-heap. Table 1 and 3 show the execution times in seconds for two sets 
of randomly generated undirected graphs: cliques and sparse graphs. Set A contains 25 graphs, each having 
20 vertices of degree 19, the costs of the edges ranging from 1 to 200. Each graph in set B has 30 vertices 
of degree 4, the edges valued again from 1 to 200. The graphs in set B have 50% more nodes than those 
in set A because due to the addi- tional biconnectivity test the algorithms for set B could run longer 
without producing a heap overflow. The execution time, presented for each graph and configurations of 
I, 2, 4, 8 and 16 processors, covers the total work period, beginning with the distribution of the graph 
and ending with the dis- play of the solution at the master. So included are communication overhead and 
idle times. One of the most important criterions to measure the performance of a multicomputer system 
is the speedup and the efficiency. Let Tk(P ) be the execution time for problem P using k processors. 
Then Sk(P ) := Ti(P)/Tk(P)denotes the speedup for problem P, AS k := Zp Ti(P)/~Tk(P) denotes the average 
speedup and E k := ASk/k is called the efficiency, a mea- sure for the utilization of the k pro- cessors 
Table 2 and 4 show for each graph and configurations of 1,2,4,8,16 processors speedup and efficiency. 
Both sets A and B behave rather well for up to 8 ring members. But especially the cliques of set A produce 
significant loss of performance when solved with 16 processors: the average speedup is only . 9.65. In 
contrary for the sparse graphs of set B an average speedup of 12.13 was achieved with 16 processors. 
The main reason for the poor perfor- mance of the cliques lies in the fact that the "pre- cautious data 
transfer" (described as heuristic 3 in section 4) does work only partly for cliques: There are so many 
subproblems with the same lower bound that during the computation often neighbors have no reason to exchange 
problems with "conditio- nal requests". Then, towards the end of the computation it is detected that 
only a few processors have still subproblems and then it takes time to distribute the work in a 16-member-ring. 
Table 5 and 6 show statistical data for the compu- tation of graphs No. 5 and 7 of set B with 16 pro- 
cessors. The columns have the following meaning: CPU time in seconds spent on working on the graph TRANS 
time in seconds spent on wafting for a pro- blem or transmitting a problem TOTAL total time in seconds, 
i.e. CPU + TRANS % work load in percent, i.e. 100*CPU/TOTAL >0 first moment that the node has run out 
of work >5 first moment that the node has to wait more than 5 seconds to get work from this neigh-bor 
WAIT maximal time period the node has spent for waiting to get work REQ number of requests IN number 
of problems received from left neigh-bor OUT number of problems sent to right neighbor HEAP maximal heap 
size in bytes ITERAT number of iterations of the while-loop Graph No. 5 is an example for a bad performance. 
First of all, the sum of all iterations (a measure for the amount of "real" work) has increased from 
338o (single processor) to 3875 (16 ring members). This means that a certain amount of work has been 
done "twice". In addition the average work load is only 45%, mainly because of the absolute short comp- 
utation time. Both, superfluous iterations and high communication overhead, causes a speedup of 6.55. 
Graph No. 7 is an example for a good performance. The sum of all iterations has decreased from 61437 
(single processor) to 612o9 (16 processors). This is possible because in the 16-member-ring subpro- blems 
are expanded in a different order than in the l-member-ring. So a temporary solution could be bound at 
a time where it can cut off from the heap a lot of (now unreasonable) subproblems. Most pro- cessors 
ran out of work for the first time after more than 7oo seconds and had to wait more than 5 seconds only 
very close to the end. This results in an average work load of 82%. Both, reduced itera-tions and little 
communication overhead, causes a speedup of 13.35. 6. Conclusion We have presented an implementation 
of a parallel branch-and-bound strategy for a set of personal computers. This strategy was tuned to the 
specific requirements of the hardware environment: no global memory, restricted routing (ring), slow 
transmis- sion. Our experimental results show that suitable software can cope with these handicaps and 
produce astonishing high speedup. Although our implementation has concentrated on one specific problem, 
namely the travelling salesman, it seems clear, that also other combinatorial pro- blems could be handled 
in a similar way. In fact, the transformation from the sequential branch-and- bound to a parallel version 
can be done rather auto- matically: It only requires the algorithm to be in an iterative form that specifies 
two basic steps, namely I.) how to split a subproblem into smaller pro- blems 2.) how to compute a lower 
bound. The rest, organization of the heaps and exchanging messages is done by the system. A very crucial 
point for the success of such an asynchronous, distributed solution method is the quality of the balancing 
heuristic. It could be that the lower bound of a subproblem is not precise enough to serve as a measure 
for the amount of work that results from this subprobleml So to improve the balance of the workload it 
will be necessary to de- velop more sophisticated evaluation heuristics that guarantee minimum communication 
and equal sized problem heaps. Number of procesaoro 1 2 4 8 15 ......................................................... 
 Graph NO. 81 Graph No. 82 Graph He. 83 Graph No. 84 13~9 262 3124 SG8 8,98 163 IG13 318 351 83 811 167 
155 ~ ~87 95 133 43 81 Graph No. lie Graph No. 0~ Graph No. e7 Graph NO. 89 Graph No. 89 Graph NO. 18 
Graph NO. 11 Graph NO. 12 Graph NO. 13 Graph No. 14 Graph No. 16 Graph No. IG Graph No. 17 Graph No. 
18 Graph No. 19 Graph No. 20 Graph No. 21 28~5 3~9 262 1823 ~ 3~ 498 868 3458 1686 35 764 388 I~87 1147 
242~ 2593 1435 198 164 852 275 2~78 2r~ 455 1788 826 28 411 211 3498 677 1220 1381 774 1~S 87 k~8 14,8 
1~ 137 246 ~ 438 21 22~ 113 17~8 384 637 689 412 73 SS 241 94 945 98 149 5~6 253 18 120 73 918 198 375 
~ 226 S8 47 155 71 336 98 346 152 28 91 55 738 138 226 223 Graph No. 22 Graph No. 23 Graph No. 24 Graph 
No. 26 1318 1116 2687 176 885 579 1373 98 348 ~ 712 S7 298 172 354 3~ lab 128 298 36 Average: 1627 ~ 
438 245 168 Table 18 Execution times In eacot'~da for lot k ( Cliques, 28 vertices)  NulM:4r of procaalOra 
1 2 4 8 16 Graph No. 81 1714 992 4711 257 155 Graph NO. 02 2878 1US 700 487 248 Graph NO. 83 ~ 2076 14M 
769 461 Graph No. ~ ~ ~ 1534 82.1 518 Graph NO. 05 ~3 348 198 117 92 Graph NO. ~ ~ 1388 7MI 395 232 Graph 
NO. 17 1~ 5509 2788 1Â¢T~ 814 Graph No. 98 6~l 2575 1311 699 391 Graph No. OS 3314 17N 870 48S 291 Grlmh 
NO. 10 6795 ~ 1494 MS 473 Graph NO. 11 ~ 26~1 1878 675 357 Graph No. 12 8587 4287 22113 1144 GS8 Grmh 
No. 13 7557 3851 1947 1016 986 GrapA'~ NO. 14 2820 1455 767 ql4 2M Grnph NO. 16 2810 1438 749 40S 238 
Graph NO. 16 3~t7 1~. 1~ ~,~ 316 Graph NO. 17 2870 1556 77/0 412 242 Graph No. 18 3262 1674 ~ ~ 282 Graph 
No. 19 4679 2393 1212 643 378 Graph No. 20 1671 873 492 268 169 Graph No. 21 4537 2328 1198 62~ 382 Graph 
No. 22 2~ 1399 711 387. 237 GrelPh No. 23 S3~7 2769 13~G 733 414 Graph NO. 24 4340 2219 1129 882 3?2 
Graph No. 35 51~ 2638 1343 784 412 Average: 4349 2223 1140 684 358 Table 3m Execution tines In eeconde 
for set B (degree 4. 38 vertices) Nulgber of procnsllora Number of precentors 1 2 4 8 16 1 2 4 8 1G Graph 
NO. 91 Graph No, 82 Graph No, 83 Graph No. ~ Graph No. gs Graph No. 86 Graph No. 07 Graph NO. 88 Graph 
No. 89 Graph NO. 18 Graph No. 1i Graph No. 12 Graph No. 13 Graph NO. 14 Graph NO. IS Graph NO. 16 Graph 
NO. 17 Graph NO. 18 Graph NO. 19 Graph NO. ~ Graph NO. 21 Graph NO, 22 Graph NO. 23 Graph No. 24 Graph 
NO. 25 1.98 1.88 1.89 1.00 1.98 1.98 1.89 1.98 1.98 1.88 1.98 1.89 1.98 1.98 1,98 1,98 1,98 1.88 1.118 
l.m 1.98 1.m 1.91 1.88 1.98 1.98 1.61 1.94 1.83 1.94 1.86 1.88 1.98 1.91 1.91 1.31 1.91 1,94 1.94 1.25 
1.83 1.84 1.91 1.98 1.99 1.91 1.98 1.33 1.98 1.7'3 3.79 3.16 3.8S 3.62 3o74 3,32 3.01 3.~ 3.$5 3.77 3.$8 
3.~ 3.~ 3.67 1.87 3.34 3.~ 3.72 3.7'7 3.81 3.88 3.81 3.7'3 3.77 3.17 6.98 4.1kS 6.41 6.98 7.113 4.78 
4.76 G.73 6.98 7.25 6.dWt 6.83 6.33 6.35 1.94 6.28 6.32 7,26 6.83 6.48 6.39 6.59 6.48 7.38 4.86 11.29 
G.09 10.34 7.~rJ. 12.81 6.02 6.57 18.47 7.41 11.76 7.18 0.86 9.95 18.57 1.25 8.25 7.9S 9.55 0.82 10.98 
11.63 8.76 6.71 9.98 4.86 Graph No, 81 Graph No, 82 Gr~ No. 83 Grwh No. 84 Graph No. IS Graph No. 08 
Grape~ No. 87 Graph No. m Graph No. 86 Graph No. 10 Graph No. 11 Graph No. 12 Graph No. 13 Graph No. 
14 Graph No. IS Graph No. IG Graph No. 17 Graph No. 18 Graph No. 19 Graph NO. 20 Graph NO. 21 GrIkDh 
No. 22 Graph No. 23 Graph No. 24 Graph No. 25 1.88 1.m 1.88 l.m 1.m l.m 1.m 1.m 1.m 1.98 1.m 1.98 1.80 
1.98 1.88 1.98 1.88 1.98 1.98 1.98 1.08 1.08 1.011 1.98 1.98 1.98 1.91 1.98 1.95 1.77 1.94 1.97 1.97 
1.95 1.95 1.95 1.98 1.98 1.94 1.9S 1,94 1.91 1.95 1.95 1.91 1.95 1.95 1.95 1.95 1.95 3.55 3.83 3,7'3 
3,98 3.17 3.63 3.91 3.86 3.81 3.87 3.78 3.88 3.98 3.88 3.76 3.74 3.74 3.82 3.85 3.78 3.79 3.72 3.87 3.84 
3.84 G.42 7.87 7.38 7.26 5.16 G.81 7.46 7.24 7.13 7.19 7.89 7.44 7.44 6.98 G.~M~ 7.16 G.89 7.83 7.26 
G.43 7.21 6.93 7.38 7.21 7.32 9.52 11.9~ 12.32 11.73 G.S6 11.~ 13.35 12.94 11.3'3 12.23 11.43 12.93 12.98 
11.37 11.81 12.21 11.89 11.57 12.62 9.89 11.98 11.18 13.04 11.67 12.51 .ininums Ilsxl~: Av~aoe: Ef f 
IclencYl 1.00 1.88 1.88 1.98 1.26 1.88 1.32 8.98 1.~7 3.68 3.73 8.~3 1.94 7.38 6.63 8.83 1.26 12.81 9.85 
8.98 Minim.me llnxl mum Average! Eff IclmlcU! 1.98 1.98 1.80 1.88 1.77 1.95 1.95 0.98 3.17 3.91 3.81 
9.95 6.15 7.46 7.19 8.98 G.~5 13.35 12.13 8.78 Table 2: Spaeck,qp-Fsctor for sat A Table 48 Speedup-Factor 
for sat B 122 CPU TRItNS TOTAl. X >S >SMAIT REQ IN ............................................................................... 
OUT HEAP ITERAT 7. AFknowledgements Master Slave 1 59 52 33 38 92 92 84 67 74 84 92 92 18 8 48 49 26 
14 14 48 3274 2548 334 357 B. Monien initiated the project parallel Slave Slave 2 3 88 52 33 48 93 92 
G4 58 83 83 93 92 7 G 53 85 48 91 91 16'9 3782 5748 342 387 algorithms and supported this work by Slave 
Slave 4 S 45 37 45 53 ~ Sil 68 41 81 73 68 ~8 S G 85 57 163 114 114 141 7266 5318 273 229 inspiring discussions. 
R. Funke and M. Slave Slave G 7 44 31 48 59 90 98 48 34 85 38 98 98 7 8 82 62 141 128 128 87 5754 5534 
238 288 Uterm~hle provided the machine level Slave Slave 8 S 31 25 57 63 88 88 35 28 63 63 88 68 S 8 
55 SG 87 76 78 59 4812 4348 188 168 routines. Finally, R. Feldmann and P. Slave 18 Slave 11 24 25 84 
52 88 87 27 28 63 65 71 87 17 22 54 53 59 ,53 SS 52 4212 638G 144 158 Mysliwietz made it all possible 
by very Slave 12 Slave 13 33 34 53 53 SG 87 38 39 64 65 86 87 22 22 69 52 52 39 39 3G 3888 35~ 282 219 
intelligent programming. Slave 14 43 44 87 4S 65 87 22 58 3G 38 3508 249 Slave 1S 4S 41 88 52 85 86 21 
53 38 25 3464 275 Total 6'58 776' 1426 888 1894 18~, 71888 3875 Average 40 48 89 45 87 87 13 59 88 G8 
4492 242 Single 683 261~ 3388 Timle S= Statistical data for computation of graph nO. S, set B, 1G processors 
CPU TRANS TOTAL ~ ~ >SMAIT REO IN OUT HEAP ITERAT ................................... ..._. ....................................... 
Master 871 143 814 82?. 742 814 27 228 SIS G.~ 48578 3787 Slave 1 653 168 813 N 728 813 48 253 ~ 848 
68118 3748 Slave 2 858 163 813 79 718 "787 45 262 ~ 6~1 .47788 3678 Slave 3 G2S 187 812 71; 786 756 58 
269 ~ 525 ~ 2688 Slave 4 638 181 811 77 693 811 17/ 275 625 429 49~42 3647 Slave 5 633 178 811 78 631 
717 78 264 429 268 41398 3621 Slave G 638 172 818 78 632 717 92 285 268 1'73 3~22 :3616 Slave 7 648 163 
818 79 76 ~ 184 223 173 187 2852.8 3585 Slave 8 1~3 185 888 88 714 888 94 214 187 171 28574 3682 Slave 
9 687 121 863 8S 7r3 8M 65 172 171 174 27882 3881 Slave 18 742 66 8418 91 798 11118 S 129 174 129 271;18 
4197 Slave 11 728 68 886 68 796888 S 131 129 215 24988 4163 Slave 12 728 61 887 89 788 887 S 126 218 
383 26858 4127 Slave 13 763 163 MS 87 777 885 6 158 363 811 34226 4813 Sieve 14 715 90 ME 88 ~ ME 7 177 
611 681 46185 3888 Slave 1S 631 113 ~ 86 758 798 8 216 581 SIS 451M 2914 Total 18783 2161 12944 33536487 
6487 687854 612tl8 Average 673 136 888 82 737 ~ 44 263 488 488 37991 3825 Single l~tJ~ 22G97G 61437 
Tlmle 66 Statistical data for computation of oraph no. 7, set 8, 16 processors References: [i] Cook S.A. 
1971. The complexity of Theorem- [7] Monien, B. and O. Vornberger. 1985. The Ring Proving Procedures, 
Proc. 3rd Ann. ACM Sym- Machine, Technical Report No. 27, Dept. of posium on Theory of Computing, New 
York, pp. Computer Science, University of Paderborn, 151-158 West-Germany, submitted for publication 
[2] Garey, M.R. and D.S. Johnson. 1979. Computers [8] Monien, B., E. Speckenmeyer, O. Vornberger. and 
Intractability: A Guide to the Theory of .. 1986. Superlinear Speedup for Parallel Back-NP-Completeness. 
Freeman, San Francisco, Calif. tracking, Technical Report No. 30, Dept. of Computer Science, University 
of Paderborn, [3] Held, M. and R. Karp. 1971. The Travelling West-Germany, submitted for publication 
Salesman Problem and Minimum Spanning Trees: Part II, Math. Prog. i, pp. 6-25 [9] Yao, A. 1975. An O(IEI 
log log IVl) Algorithm for finding Minimum Spanning Trees, Inform. [4] Karp, R. 1972. Reducibility among 
combinato- Process. Lett., 4, pp. 21-23 rial problems, Complexity of Computer Computa- tions, Plenum 
Press, New York, pp. 85-1o3 [5] Law]er, E.L. and D.E. Wood. 1966. Branch-and- Bound Methods: A survey. 
Operations Research 14, pp. 699-719 [6] Mohan, J. 1983. A Study in Parallel Computa-tion: the Travelling 
Salesman Problem. Tech- nical Report CMU-CS-82-136(R), Dept. of Comp- uter Science Carnegie-Mellon University, 
Pitts-burgh 
			