
 A Sorting Algorithm on a PC Cluster Janez Brest, Aleksander Vre2e, Viljem Zumer University of Maribor 
Faculty of Electrical Engineering and Computer Science Smetanova 17, SI-2000 Maribor, Slovenia  janez.brest@uni-mb.si 
Key words: computing system, network, parallel pro- cessing, data sorting, quicksort. Abstract Data sorting 
is the most studied problem in computer science, for both, its theoretical importance and its use in 
so many applications. A parallel sorting algorithm on a heterogeneous sys- tem -PC cluster was implemented 
to minimize the application execution time. In this paper we propose a scheme for communicating data 
among subtasks du- ring application program execution. We present some experimental results of the parallel 
sorting algorithm on a PC cluster. Introduction There is a continual demand for greater computational 
speed from a computer system than currently possible [17]. Areas requiring great computational speed 
include numerical modeling and simulation of scientific and en- gineering problems. Such problems often 
need huge repetitive calculations on large amounts of data to give valid results. In network-based computing 
environment, several computers are linked through a communication network to form a large loosely coupled 
distributed network [6]. One of the major attributes of such a distributed sys- tem is the capability 
that it offers to theuser of a single node to exploit the considerable power of the complete network 
or a subset of it by partitioning and transfer- ring its own processing load to other processors in the 
network. This capability is useful in handling largecom-putational loads. Communication latency is an 
important factor in deciding performance of a parallel or distributed al- gorithms, especially in a low 
speed network or in a communication-intensive task situation [12]. In this paper we are concerned with 
the problem of distributing a single large load that originates at one of the nodes in the network. The 
load is massive compared Permission to make digital or hard copies of all or part of this work for personal 
or classroom use is granted without fee provided that copies are not made or distributed for profit or 
commercial advantage and that copies bear this notice and the full citation on the first page. To copy 
otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission 
and or fee. SAC'00 March 19-21 Como, Italy (c) 2000 ACM l-Sg113-239-5/00/003>_.>$5.00 to the computing 
capability of the node. Hence, the processor partitions the load into many fractions, keeps one of the 
fractions for itself to process, and sends the rest to its neighbours (or other nodes in the network) 
for processing. When the processing at each node is com- pleted, partial solutions are gathered and consolidated 
at the load originating processing to construct the com- plete solution. An important problem here is 
to decide how to achieve balance in the load distribution between processors, so that the computation 
is completed in the shortest possible time [6]. Applications that satisfy di- visibility property include 
processing of massive experi- mental data, image processing applications like feature extraction and 
edge detection, signal processing appli- cations such as computation of Hough transforms. In this paper 
we present a sorting algorithm on a heterogeneous computing system [3, 10, 14, 17]. The heterogeneous 
computing system (PC cluster) is used to minimize the program execution time. It is based on the message 
passing between computers. A realistic bus-oriented network (Ethernet) with multicast type of communication 
regimen is considered. Heterogeneous computing includes both parallel and distributed pro- cessing. The 
purpose and the advantages of using a heterogeneous computing system are presented. Data sorting [2, 
15, 9, 13, 11, 8, 16] is the most stu- died problem in computer science, for both, its theoreti- cal 
importance and its use in so many applications. The parallel sorting method, or sorting strategy, on 
a one- dimensional sub-bus array of processors is presented in [5, 4]. The rest of the paper is organized 
as follows. In Sec- tion 2 an overview of the estimated speedup of parallel sorting algorithms is given. 
In Section 3 a parallel sort- ing algorithm on a PC cluster is described. Experimen- tal results of the 
implemented parallel algorithm on a parallel computing system are presented in Section 4. In Section 
5 the conclusion remarks are given. 2 The Estimated Speedup for Large Data Sorting in a Parallel Computing 
System 2.1 Evaluation of Parallel Programs While writing a parallel program a question about the quality 
of the program arises. There are two ways of evaluating a parallel program. The relation between time 
complexity of a sequential and a parallel algorithm is known a as speedup and it is defined as [17]: 
s(p) = tA  tp' (1) where tl is the execution time on a single processor and tp is the execution time 
on a multiprocessor, and p is the number of processors. In the best case scenario of the speedup would 
be S(p) = p, (2) which is not possible in the real world. There are many reasons: time needed to communicate 
between processes,  speedup is limited by the slowest node, etc.  The next metric of the parallel algorithm 
is program efficiency and it is defined as E(V)= S(p) = t, . (3) P P tr, Algorithm efficiency is the 
relation between time complexity of the sequential program and time complex- ity of the parallel algorithm 
multiplied by the number of processes. 2.2 The Estimated Speedup In this section theoretical aspects 
of the speedup of a parallel sorting algorithm is described. Consider the following parallel quicksort 
algorithm [15]. A number of identical processes, one per proces- sor, execute the parallel algorithm. 
The elements to be sorted are sorted in an array in global memory. A stack in global memory stores the 
indices of sub-arrays that are still unsorted. When a process is without work, it attempts to pop the 
indices for an unsorted sub-array, based on a supposed median element into smaller ar-rays, containing 
elements less than or equal to the sup- posed median value or greater than the supposed me-dian value, 
respectively. After the partitioning step, identical to the partitioning step performed by the se- rial 
quicksort algorithm, the process pushes the indices for one sub-array onto the global stack of unsorted 
sub- arrays and repeats the partitioning process on the other sub-array. What speedup can be expected 
from this parallel quicksort algorithm? Note that it takes k -1 comparisons to partition a sub-array 
containing k ele- ments. The expected speedup is computed by assum-ing that one comparison takes one 
unit of time and finds the ratio of the expected number of comparisons performed by the sequential algorithm 
to be expected time required by the parallel algorithm. To simplify the analysis, assume that n = 2 k 
-1 and p = 2 'n, where m < k. Also assume that the supposed median is al- ways the true media so that 
each partitioning step al- ways divides an unsorted sub-array into two sub-arrays of equal size. With 
these assumptions the number of comparisons made by the sequential algorithm can be determined by solving 
the following recurrence relation: {n-l.+2T(-~ 2) for n = 7,15, 31,. T(n)----2 forn=3, "" (4) The solution 
to this recurrence relation is: T(n)= (n+l)log(n+l)-2n. (5) The parallel algorithm has two phases. First, 
there are more processes to be sorted than arrays. For exam- ple, when the algorithm begins execution, 
there is only one unsorted array. This iteration then requires n -1 time units to perform n -1 comparisons. 
If we assume p > 2 two processes can partition the two resulting sub- arrays in n-I n-3 2 2 time units 
perfbrming n -3 comparisons. Similarly, if p > 4 the third iteration requires time at least n-1 n-7 [---~ 
-1]/2 -I --7 to perform n -7 comparisons. For the first logp it- erations there are at least as many 
processes as parti- tions and the time required by this phase of the parallel quicksort algorithm is 
Tl(n,p) = 2(n+ 1)(1 - l) _ togp. (6) P The number of comparisons perfbrmed is Cl(n,p) = (n+ 1)logp- 
2(p-1). (7) In the second phase of the parallel algorithm there are more sub-arrays to be sorted than 
processes. All the processes are active. If we assume that every pro- cess performs an equal share of 
the comparisons then the time required is simply the number of comparisons performed divided by p. Hence 
c~(n, p) = T(~) -C, (n, p), (S) T~(n,p) = C~(n,p) (9) p The estimated speedup achievable by the parallel 
quicksort algorithm is the sequential time divided by the parallel time: S = T(n)T,(n,p) + T2(n,p)" (10) 
 Table 1 and figure 1 show predicted speedup of par- allel sorting algorithm. For example the best speedup 
we could expect with n = 1,000,000 and p = 16 is ap- proximately 6.53. Why is speedup so low? The problem 
with quicksort is its divide-and-conquer nature. Until the first sub-array is partitioned there are no 
more par- titioning to do. Even after the first partitioning step is complete there are only two sub-arrays 
to work with. Hence many processes are idle at the beginning of the parallel algorithms execution waiting 
for work. 3 Our Method In this section we proposed a scheme of a parallel sort- ing algorithm, which 
is based on the well-known mas-ter/slave paradigm. Master partitions an unsorted data sequence into many 
fractions and sends them to slaves. When the processing at each slave is completed, partial solutions 
are gathered to construct the sorted data sequence. The time complexity is defined as a sum of times: 
Table h Predicted speedup vs. number of processors n p T T1 C1 C2 ' T2 S E 1000000 2 1.79e+07 1.0e+06 
999999 1.69e+07 8.46e+06 1.89 0.94 1000000 4 1.79e+07 1.50e+06 2e+06 1.59e+07 3.98e+06 3.27 0.81 1000000 
8 1.79e+07 1.75e+06 2.99e+06 1.49e+07 1.86e+06 4.96 0.62 1000000 16 1.79e+07 1.88e+06 3.99e+06 1.39e+07 
870726 6.53 0.41 1000000 32 1.79e+07 1.93e+06 4.99e+06 1.29e+07 404114 7.66 0.24 I000000 64 1.79e+07 
1.97e+06 5.99e+06 1.19e+07 186433 8.32 0.13 1000000 128 1.79e+07 1.98e+06 6.99e+06 1.09e+07 85405 8.66 
0.07 1000000 256 1.79e+07 1.99e+06 7.99e+06 9.93e+06 38797 8.83 0.03 5000000 2 1.01e+08 5e+06 5e+06 9.62e+07 
4.81e+07 1.91 0.95 5000000 4 1.01e+08 7.5e+0S le+07 9.12e+07 2.28e+07 3.34 0.83 5000000 8 1.01e+08 8.75e+06 
1.5e+07 8.62e+07 1.07e+07 5.18 0.64 5000000 16 1.01e+08 9.37e+06 2e+07 8.12e+07 5.07eW0S 7.01 0,44 5000000 
32 1.01e+08 9.68e+06 2.49e+07 7.62e+07 2.38e+06 8.39 0.26 5000000 64 1.01e+08 9.84e+06 2.99e+07 7.12e+07 
1.11e+06 9.24 0.14 5000000 128 1.Ole+08 9.92e+06 3.49e+07 6.62e+07 517717 9.70 0.08 5000000 256 1.01e+08 
9.96e+06 3.99e+07 6.12e+07 239328 9.93 0.038 10000000 2 2.12e+08 le+07 le+07 2.02e+08 1.01e+08 1.91 0.95 
10000000 4 2.12e+08 1.5e+07 2e+07 1.92e+08 4.81e+07 3.37 0.84 10000000 8 2.12e+08 1.75e+07 3e+07 1.82e+08 
2.28e+07 5.27 0.65 10000000 16 2.12e+08 1.87e+07 4e+07 1.72e+08 1.07e+07 7.19 0.45 10000000 32 2.12e+08 
1.93e+07 4.99e-F07 1.62e+08 5.07e+06 8.69 0.27 I0000000 64 2.12e+08 1.96e+07 5.99e+07 1.52e+08 2.38e+06 
9.63 0.15 10000000 128 2.12e+08 1.98e+07 6.99e+07 1.42e+08 1.11e+06 10.t4 0.08 10000000 256 2.12e+08 
1.99e+07 7.99e+07 1.32e+08 517717 10.39 0.04 15000000 2 3.27e+08 1.5e+07 1.5e+07 3.12e+08 : 1.56e+08 
1.91 0.96 15000000 4 3.27e+08 2.25e+07 3e+07 2.97e+08 7.44e+07 3.38 0.85 15000000 8 3.27e+08 2.62e+07 
4.5e+07 2.82e+08 3.53e+07 5.32 0.66 15000000 16 3.27e+08 2.81e+07 6e+07 2.67e+08 1.67e+07 7.30 0.46 15000000 
32 3.27e+08 2.90e+07 7.49e+07 2.52e+08 7.89e+06 8.86 0.28 15000000 64 3.27e+08 2.95e+07 8.99e+07 2.37e+08 
3.71e+06 9.85 0.15 15000000 128 3.27e+08 2.97e+07 1.05e+08 2.22e+08 1.7e+06 10.39 0.08 15000000 256 I 
3.27e+08 2.98e+07 1.19e+08 2.07e+08 810849 10.67 0.04 20000000 2 4.45e+08 2e-k07 2e+07 4.25e+08 2.12e+08 
1.91 0.96 20000000 4 4.45e+08 3e+07 4e+07 4.05e+08 1.01e+08 3.39 0.84 20000000 8 4.45e+08 3.5e+07 6e+07 
3.85e+08 4.81e+07 5.35 0.67 20000000 16 4.45e+08 3.75e+07 8e+07 3.65e+08 , 2.28e+07 7.37 0.46 20000000 
32 4.45e+08 3.87e+07 9.99e+07 3.45e+08 I 1.07e+07 8.98 0.28 20000000 64 4.45e+08 3.93e+07 1.2e+08 3.25e+08 
,I 5.07e+06 10.01 0.16 20000000 128 4.45e+08 3.96e+07 1.4e+08 3.05e+08 2.38e+06 10.58 0.08 20000000 256 
4.45e+08 3.98e+07 h59e+08 2.85e+08 l.lle+06 10.87 0.04 Legend: n ... number of elements; p... number 
of processors; T~ ... time of the first phase required by the parallel algorithm; T.2 ... time of the 
second phase; C1 ... number of comparisons in the first phase; C2 ... number of comparisons in the second 
phase; S ... speedup; E ... efficiency. 1. splitting the unsorted data array, consuming merge process 
is avoided. To make the split- ter satisfactory the Median5 function (median value of 2. master-slave 
data sending, five elements) is used. The partitioning process that divides an array into 3. slave sorting 
of the splitted part of the array (using  smaller parts is described as follows. quicksort), First splitter 
value dl satisfies 0 < d~ < n. Equa-4. master-slave data receiving. tion (11) shows the second step of 
an array partitioning. Consider the problem of sorting n elements equally There are two possibilities, 
the value d2 can be greater distributed among p processors. Our idea is to find a set or less than value 
dl. of p -1 splitters to part an array with the n elements. ~ ~zl < d2 < n, ifd1< -~--+A- Each of elements 
in the i-th group is less than or equal 2 (11) to each of elements in the (i+l)-th group. The efficiency 
[0<d2 <'e:~ !, ifdl > e"~"!2 of this algorithm obviously depends on how evenly the To simplify the analysis, 
we sort values dl and d2. input is split, and this term depends on how well the Assume that we already 
have p -1 splitter values. splitters are chosen. The values of splitters dl, d2, ..., dp-1 are between 
0 and Figure 2 shows the idea of our algorithm. First the n. After sorting splitter values, we introduce 
new values unsorted data array is divided into smaller parts by us- rl, r2,..., rp-1 (sizes of small 
parts of an array): ing splitters. Then those smaller parts are sent to the slaves, which sort them using 
the quicksort function. rl = dl -0; By using splitters it is assured that all the elements r2 = d2 -dl; 
left to the splitter element are less or equal to split- ... (12) ter value and all the elements right 
to the splitter are greater than the splitter value. By doing this, the time rv = n -dv-]; ,&#38;.~ 5,000,n00 
.... 10,000,(X)0 ---15,C~9,000 ...... I I I f I I 1~ 1~ 2~ 250 300 3~ Numberol~oe~so~s Figure h Predicted 
speedup vs. number of processors 0 d I d 2 dp-I n r, r 2 r5 rp Figure 2: An example of the splitting 
data array The new value d 7, lies on the interval where the cor- responding value r~ : i 6 1, ...,p- 
1 is maximal. For example, if the value r2 is maximal, then d~, satisfies dl < re < d2. The value dp 
is computed by Me-dian5 function. Three elements are chosen on indexes a = di + 1, b = di+l -1, c = (a 
+ b)/2, if it is assumed that the value ri is maximal. Two elements are chosen randomly between indexes 
a and b. The divide function is similar to the function used by the quicksort algorithm. After calling 
Divide func-tion the greatest sub-array is divided into two halves. Again, the largest part of the array 
is chosen and the new splitter value is computed. The partitioning pro- cess is repeated until there 
is one sub-array for every slave. MASTER. and SLAVE algorithms in pseudo code: Master algorithm: begin 
divide array into smaller parts using Divide function (as many as the number of slaves) accordingly to 
splitters send parts of an array to all slaves receive sorted parts and store them to the same position 
as they were sent from end Slave algorithm: begin accept assigned array sort a part of the array (quicksort) 
send the result to master end  4 Results In this section experimental results of the implemented sorting 
algorithm on a parallel computing system are presented. The parallel algorithm was implemented in the 
C++ programming language. As an interprocess communi- cation the LAM tool [1, 7] was used. The LAM par- 
allel environment features the Message Passing Inter- face (MPI) standard. In the experiment, data were 
randomly generated and 20 independent measurements were pertbrmed and in this section we operate with 
the average values. For sorting sub-arrays, the quicksort function in the C÷+ programming language was 
used. In addition the Median5 function, that is used by the master process to split the array into subtasks, 
was implemented. ?0 65 60 55 ,50 45 40 35 30 25 ,'2 ,4 I, klmber of processo[s Figure 5: Results of sorting 
20 million elements The obtained results using PC cluster (PC machines with 64 or 128 MB of main memory, 
connected m tile local area network using ATM) are shown in Table 2. The first column presents the size 
of the data array. The second column presents the number of PC com-puters (one process per computer). 
The type of" array elements is integer. The time needed for sorting data by one computer is shown in 
the fourth column. In the fifth column there is the execution time needed by the PC cluster (execution 
time of the data splitting only is shown in third column). In the next column speedup factor (Eq. 1) 
is presented. The last column presents the efficiency (Eq. 3). The speedup values are between 1 and 2.6. 
In the last part of the table, speedup values are up to 20. In this case computer with 128 MB of main 
memory began swapping during quicksort execu-tion. The graphs on the figures 3, 4, and 5 show execution 
times using PC cluster to sort 1, 5, 10, 15, 20, 25 mil- lions of data elements, respectively. As it 
is evident, the times almost do not differ when small arrays are used. But when larger arrays are being 
sorted, the differences are more apparent. The experimental results show that the PC cluster approach 
can be profitably used, when the size of the problem is sufficiently large (so that parallel computing 
is required). 713 Table 2: Experimental results  I p I t~[~] tl[~] t~[~] s E 2 0.09 2.76 1.05 0.53 
 4 0.17 1.80 1.62 0.40  6 0.23 1.58 1.84 0.31 1000000 8 0.31 2.91 1.46 1.99 0.25 10 0.29 1.47 1.98 
0.20 12 0.32 1.39 2.09 0.17 14 0.34 1.40 2.07 0.15 2 0.43 15.02 1.11 0.56 4 0.88 9.55 1.74 , 0.44 
  6 1.15 7.80 2.14 0.36 5000000 8 1.34 16.69 7.24 2.31 0.29 10 1.48 6.85 2.44 0.24 12 1.60 6.87 2.43 
0.20 14 1.94 7.19 2.32 0.17 2 0.87 99.72 0.35 0.17 4 1.79 19.51 1.77 0.44  6 2.29 16.15 2.14 0.36 
10000000 8 2.68 34.55 14.81 2.33 0.29 10 3.02 14.00 2.46 0.25 ' 12 3.21 14.20 2.43 0.20 14 3.57 14.22 
2.43 0.17 4 2.68 29.58 1.83 0.45  6 3.43 24.46 2.21 0.37 15000000 8 4.02 54.16 21.97 2.46 0.31 10 
4.44 21.20 2.55 0.26 12 4.83 21.02 2.58 0.21 14 5.19 21.28 2.55 0.18 4 3.54 29.58 I 8.02 2.01  6 
4.59 24.46 17.39 2.90 20000000 8 5.32 574.4 21.97 18.90: 2.36 10 6.16 21.20 20.10 2.01 12 6.43 21.02 
20.40 1.70 14 6.85 21.28 20.11 1.44   Legend: n ... number of elements; p ... number of processors; 
td ... execution tinm of the data array splitting by the parallel algorithm; tl ... execution time of 
sequential program; tp ... execution time of parallel program; S... speedup; E... efficiency. 2,8 5.(~)0,0(10 
s~'n~ls --1.000,000 e f, em ent.~ -- 2.6 2.4 2.2 2 1.6 t.4 J 4 6 B fltnllbQr ~ pr~sa~¢$ I 1 14 4 G 8 
Number ol ploces~o~ lO 112 Figure 3: Results of sorting 1 and 5 million elements a PC cluster and experimental 
study of its performance Conclusion were presented. The obtained results show that a large In this paper 
the theoretical aspects of" the speedup of" data set is sorted faster if there is a PC cluster rather 
a parallel sorting strategy and the sorting algorithm on IOO go 8o 70 6O 5O 40 m~ 2O 30 1 o.coo.o00 
~emenl= 2S 28 27 26 Z5 24 23 2? 21 Numbee ol ~oces~s Figure 4: Results of sorting 10and 15 million elements 
than only one computer used. In the future we want to parallelize the first phase of our sorting algorithm 
and increase the number of machines in the PC cluster.  References [1] Gregory D. Burns, Raja B. Daoud, 
and James R. Vaigl. LAM: An Open Cluster Environment for MPI. In Supercomputing Symposium '9~, Toronto, 
Canada, June 1994. [2] Ralf Diekmann, Joern Gehring, Reinhard Luel-ing, Burkhard Monien, Markus Nuebel, 
and Rolf Wanka. Sorting large data sets on a massively par- allel system. In Proc. 6th IEEE-SPDP, pages 
2-9, 1994. [3] Mary. M. Eshagian, editor. Heterogeneous Com-puting. Artech House, Inc., Norwood, MA 02062, 
ISBN 0-89006-552-7, 1996. [4] Fix and Ladner. Sorting by parallel insertion on a one-dimensional subbus 
array. IEEETC: IEEE Transactions on Computers, 47, 1998. [5] J. D. Fix and R. E. Ladner. Sorting by parallel 
in- sertion on a one-dimensional sub-bus array. Techni- cal Report TR-96-09-02, University of Washington, 
Department of Computer Science and Engineering, September 1996. [6] Debasish Ghose and Hyoung Joong Kim. 
Load partitioning and trade-off study for large matrix- vector computations in multicast bus networks 
with communication delays. Journal of Par-allel and Distributed Computing, 55(1):32-59, 25 November 1998. 
[7] Gregory D. Burns, Vibha S. Dixit, Raja B. Daoud, and Raghu K. Machiraju. All About Trollius. Oc- 
cam users group newletter, August, 1990. [8] Helman and JaJa. Sorting on clusters ofSMPs. IN-FRMTCA: 
Informatica: An International Journal of Computing and Informatics, 23:113-121, 1999. [9] David R. Helman, 
David A. Bader, and Joseph JAJA. A randomized parallel sorting algorithm with an experimental study. 
Journal of Parallel and Dis- tributed Computing, 52(1):1-23, 10 July 1998. [10] Debra Hensgen, editor. 
Proceedings: Sixth Hetero- geneous Computing Workshop (HCW'97). [EEE Computer Society Press, Los Alamitos, 
CA, April 1997. [11] F. T. Leighton. Introduction to Parallel Architec- tures : Arrays, Trees, Hypercubes. 
Morgan Kauf- mann Publishers, 1992. [12] W. M. Lin and W. Xie. Minimizing Communi- cation Conflicts with 
Load-Skewing Task Assign- ment Techniques on Network of Workstations. IN-FRMTCA: Informatica: An International 
Journal of Computing and Informatics, 23:57-66, 1999. [13] Udi Manber. Introduction to Algorithms, a 
Cre-ative Approach. Addison-Wes]ey Publishingg Com- pany Inc., 1989. [14] Min Tan, Howard Jay Siegel, 
John K. Antonio and Yah Alexander Li. Minimizing the Application Ex- ecution time Through Schudeling 
of Subtasks and Communication Traffic in a Heterogeneous Com- puting System. IEEE Transaction on Parallel 
and Distributed Systems, Vol. 8(No. 8):173-186, Au-gust, 1997. [15] M. Quinn. Parallel Computing: Theory 
and Prac- tice. McGraw-Hill, 1994. [16] V. Zumer, M. Ojster~ek, A. Vre~e, and J. Brest. Sorting on Heterogeneous 
Computing System. In MIPRO'99: lO-th International Conference on Computers in Inteligent Systems, volume 
2, pages 1-4, Opatia, Croatia, 1999. [17] Barry Wilkinson and Michael Allen. Parallel Programming: Techniques 
and Applications Using Networked Workstations and Parallel Computers. Prentice-Hall, Englewood Cliffs, 
NJ 07632, USA, 1998. 715   
			