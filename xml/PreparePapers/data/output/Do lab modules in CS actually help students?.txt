
 : D,o ,Lab Modules in CS Actually Help Students? An Empirical Study ( r, / 2, :, , 4 ,I ,,,a Bunny 
J. Tjaden !I I, The George W&#38;in&#38;n University School of Engineering and Applied Sciences , Washington, 
DC 20052 tjadenQseas.gwu.edu / ,,Abshct Three iaboratory modules were develqped to teach C pointers 
to, novice programmbg students, using two different versions of software. The fmt version was an intelligent 
tutoring system; the second was a multimedia tutorial. Six classes of students participated in an empirical 
study involving btith versions of software. What was $sc&#38;ered as a result of this experiment was 
more than jtist the answei to whethei or not a particular piece of software helped the students to learn 
the subject matter. We discovered who used the software, given the freedom of choiiz. We learned about 
the characteristics of software users, which software was more helpful, and what the students think about 
using software to learn. The mbre s&#38;$&#38;ant results of this study are presented in this paper,! 
, The current trend in comptiter science courses is to create laboratory modules for use m either an 
open ed or closed lab. We, as faculty, are putting a great deal of effort into the creatibn of these 
m&#38;h&#38;, but dc. we know for certain that they are effectively helping the students learn, or even 
whether or not the amount of :progress on the part of the student as a result of using the software, 
justifies the enormous amount of time and effort involved in developing such modules. How do the students 
feel about the modules? More importantly, what do the statistics say about how beneficial they really 
are? In an effort to answer these questions and document the process of creating lab modules, an empirical 
study was performed. Two different kinds of software were Permissionto make digit&#38;hard copies of 
all or part of tbis material for personal or classroom use is granted without fee provided that the copies 
arc not made or distributed for profit or commercial advantage, tbe copy- right notice. the title ofthe 
publication and its date appear, and notice is given that copyright is by permission ofthe ACM, Inc. 
To copy otherwise, to republish, to post on servers or to redistribute to lists, requires specific permission 
andlor fee. SIGSCE 98 AtlantaGA USA Copyright 1998 O-89791-994-7/98/ 2..%5.00 det;eloped for laboratory 
modules in an introductory programming course in C. The fust version of the software was an intelligent 
tutoring system (ITS) w$ten in C. The secobd Version of the software was a multimedia tutorial developed 
using a multimedia authoring system. Both versions of the software included the same three lab modules 
in which the same concepts were presented. Both versions of the software presented the concepts in the 
same order. The multimedia software included, sound clips for feedback, consisting of .WAV files from 
the popular television show, The Sinipsons . It also included animations to illustrate some of the concepts 
that were explained using text in the ITS. It was our intention to use the software to determine which, 
if either, version helped the students learn. The topics selected for the modules were the most difficult 
ones &#38;countered in au introductory C programming course: pointers, as they apply to simple variables, 
arrays, and structures. What we learned from our experiment was much more than was expected. We learned 
about how the students accepted the software, how confident they felt that it was helping them learn, 
how much they actually used it, and their attitude toward software for the purpose of learning, in general. 
Many of the-fmdings were surprising. Some were disappointing. The Experiment During the spring of 1996, 
three college level pmgramming cti, consisting of two sections that inet during the day and one section 
that met at night, were given CPR, C Pointer Remediation, an intelligent tutoring system, to help them 
learn the basic concepts about pointers. The software was free and was made readily available. The students 
merely were required to bring a blank, high density diskette to the computer lab office in order to obtain 
a copy. CPR could be run under either DOS or Windows 3.1. It did not have to be iastalled on the hard 
drive of a computer. Input was via the keyboard, because the mouses in the computer labs were in poor 
condition, due to heavy usage. The following s$me$er, three classes, consisting of two day and one night 
section, were supplied with CPRJIM, the multimedia version of the software. The software required 4 diskettes 
for the compressed files. In order for the full capabilities of the software to be accessible, the following 
minimum hardware requirements were necessary: a computer with Windows 3.1, 11 megabytes of available 
memory, a minimum of a 486 microprocessor, a sound card, speakers, and a mouse. The students were again 
given the opportunity to obtain the software free for theii personal use. None of the subjects was coerced, 
nor were they provided with any incentive, either financial or in the form of extra credit, for using 
the software. Because a computer lab could not be reserved for the students as a group, when, how, and 
whether or not they chose to use the software was entirely up to each individual. Each student was given 
an introduction to one version of the software in a computer lab during one 50 minute class period. The 
following task measurements were recorded for each student: midterm grade prior to software use, a quiz 
on pointers to simple variables, a quiz on pointer notation for arrays, a quiz on passing a pointer to 
a structure to a function, and two questions on the fml exam relating to pointers. Because completion 
of all six tasks was required for inclusion in the study, only students who completed the course by taking 
the final exam were considered as subjects in this experiment. All subjects were given the same class 
notes and studied under the same professor. All subjects also filled out a consent form and a questionnaire 
about their background. All subjects filled out an evaluation of the version of software to which they 
were exposed, and responded to a follow-up questionnaire about their software usage on the final day 
of class. The subjects who were expcsed to CPR were classified as Treatment 1 (Tl) and Control 1 (Cl), 
according to whether or not they used the software. Of the 42 students who completed the course during 
the spring semester, 17 chose to use the software and 25 did not. Similarly, the ,CPRJvfM subjects from 
the fall semester were catalogued as Treatment 2 (T2) and Control 2 (C2). Of the 34 subjects who had 
access to the multimedia software, CPRMM, only 13 of them chose to use the software whereas 21 students 
elected not to use it. Source of Sum of Mean Significance Variation Squares DF Square F ofF Between 225.878 
2 112.939 592 ,555 Within 21,183.113 111 190.839 Total 21.408.991 113 Figure 1. ANOVA of Midterm for 
all Users The midterm was admiitered prior to the introduction of the software. No differences between 
Cl, Tl, and T2 were apparent, Figure 1. All students evaluated tbe software one week prior to the tirst 
quiz. They then obtained copies of the software and used it unmonitored. The three quizzes were announced, 
as were their topics. The Results After testing for equality of variance, because of the differences 
in group sizes, Tl and Cl were subjected to a one-way analysis of variance on all six task measurements. 
The results in Figures 2 through 6 indicate tbat the software failed to ma&#38;e a significant difference 
in scores on any of the quizzes or final exam questions. However, it is noted that CPR users scored consistently 
better on the second and third quizzes than did non-users. Nevertheless, by the final exam, theii scores 
were approximately ten percent worse than the non-users, coinciding with the differences in midterm scores 
of the two groups. Source of Sum of Variation Squares DF Between ,.961 Within 327.158 Total 328.119 1 
40 41 Mean Sauare .961 8.179 F -117 S ignificance of F ,733 Figure 2. Quiz 1 ANOVA of CPR Users vs Non-users 
Source of Sum of Mean Significance Variation Squares DF Square F of F Between .74! 1 .741 .lll ,741 Within 
268.235 40 6.706 Total 268.976 41 Figure 3. Quiz 2 ANOVA of CPR Users vs Non-users Source of Sum of Mean 
Significance Variation Squares DF Square F of F Between 4.550 1 4550 481 .492 Within 378.235 40 9.456 
Total 382.785 41 Figure 4; Quiz 3 ANOVA of CPR Users vs Non-users Source of Sum of : Mean Significance 
Variation Squares DF Square F of F Between 9.463 1 9.463 1.365 250 Within 277.322 40 6.933 Total 286.785 
41 Figure 5. Final 1 ANOVA of CPR Users vs Non-users Source of Sum of Mean Significance Variation Squares 
DF Square F of F Between 10552 1 10552 2.501 .122 Within 168.781 40 4.220 Total 179.333 41 Figure 6. 
Final 2 ANOVA of CPR Users vs Non-users For I2 and C2, an analysis of variance was performed across ali 
measures. only the second quiz indicated a significant difference in the scores between users of CPRJIM 
and non-users. It is believed that this result occurred because the software had not yet been obtained 
by Tl and the subjects were quite confused about pointer notation for arrays. Figures 7 through 11 depict 
these findings. Source of Sum of Mean Significance Variation Squares DF Square F of F Between .062 1 
.062 .005 .944 Within 392.879 32 12.277 Total 392.941. 33 I Figure 7. Quiz 1 ANOVA of CPR-MM Users/Non-users 
Source of Sum of 1 Mean Significance Variation Squares DF Square F of F Between 11.661 1 11.661 6.029 
.020* Within 61.897 32 1.934 Total 73.559 33 *Siauificance at the .05 level Figure 8. Quiz 2 ANOVA of 
CPR-MM Users/Non-users Source of Sum of Mean Significance Variation Squares DF Square F of F Between 
6.734 1 6.734 1.114 ,299 Within 193.502 32 6.047 Total 200.236 33 Figure 9. Quiz 3 ANOVA of CPR-MM Users/Non-users 
Source of Sum of Mean Significance Variation Squares DF Square F of F Between 1.188 1 1.188 .188 .671 
Within 207.077 -32 6.471 (. Total 208.265 33 Figure 10. Final 1 ANOVA: CPR-MM Users/Non-users Source 
of Sum of . I Mean Significance Variation Squares DF Square F of F Between .742 1 .742 .149 .703 Within, 
159.876 32 4.996 L Total 160.618 .33 Figure 11. Final 2 ANOVA: CPR-MM Users/Non-users Aii subjects rated 
one version of the software. CPR obtained_ average ratings of 3.9 out of 5 while CPR-MM had ten percent 
higher ratings of 4.4 out of a possible score of 5. Given the higher ratings of the muitimedia software, 
we expected a larger munber of students to use the software. This was not tbe case, however. The percentage 
of subjects using the software during either semester was nearly identical, 40% (17/42) for CPR and 38% 
(13/34) for CPR,MM. It should be noted that some subjects in C2 either did not have a home computer, 
did not have Windows 3.1 as their computer s operating system, did not have necessary sound or graphics 
cards, or could not spare 11 megabytes of memory, and therefore could not install CPRJvIM. Statistics 
for all groups were analyzed to determine who was more likely to use the software. For CPR, the day students 
used the software more frequently than the night students. However, a higher percentage of night students 
used CPR-.MM. In generai, students who attend night classes usually work fuii time and are updating skiIi.s 
or changing careers aud already have a bachelor s degree. One explanation for the higher use by night 
students of CPR-MM is that they had access to higher end computers, either at work or at home, being 
better able to afford more advanced equipment. Table 1 depicts the statistics of day and night students 
software usage. Students Used CPR Used CPRJvIM 11 Students were also categorized by midterm grades into 
users or non-users. In the past, the midterm grade has generally been a fairly accurate predictor of 
what grade the student wiii receive in the course. Additionally, students who do not pass the midterm 
usually withdraw from the course. So, although we began the experiment with approximately 80 subjects 
in each group, about haIf of them failed the midterm and therefore withdrew from the course and the study. 
Table 2 specifies the number of subjects in each group according to their midterm grades aud software 
usage. Tl and!Cl had access to CPR while T2 and C2 had access to CPRJIM. Group A P C D F We expected 
the poorer students to use the software more tban tbe students at the top of the class. Indeed, they 
were more likely to use either version of the software. But we were surprised~ that some of the top students 
chose to use the software. It should also be noted that the CPR users (Tl), with an average age of 33, 
were older by about 3 years than the nonusers (Cl). The multimedia users (T2), with a meau age of 25, 
displayed no difference in age from the non-users (C2). We surmise that the older students were willing 
to &#38;vote more thne to the course and therefore used CPR, despite the fact that it was more difficult 
and less interesting to use. However, younger students were enticed into using the multimedia software 
because it was fun CPR users experienced some disorientation, or loss of seuse of location when using 
the software, as has been reported by Kenny [5J and Allinson [2]. This probably occurred because the 
tutor automatically took over if the same question was answered incorrectly 3 times. The subjects may 
not have been aware that they had changed from the lesson to the tutor mode. The CPR-MM users did not 
report similar disorientation They- were free to attempt a question as often as was necessary, and were 
never forced into the tutor. Rather, exploring the tutor was encouraged by presenting iuteresting animations 
or using other engaging, interactive methods to teach the concept in another manner. Tl reported that 
they used the CPR software for about 4.1 hours. T2 reported that they used CPR_MM for about 4.6 hours. 
We have no way of knowing how accurate these estimates are. It may be that the time spent using CPR was 
over-reported because it was not as interesting to use as the multimedia software. Conversely, CPR-MM 
usage time may have been under- reported because of the entertainiug sounds durhrg the feedback and the 
animations. scores of subjj oil T&#38;Sk Measurements Like Jo&#38;an, et. al. [6J in their experience 
with teachiug Pascal using multimedii we saw no statistically significant differences in scores between 
the software users and those who did not use the software. Both treatment groups had lower average midterm 
scores than their corresponding control groups. Using CPR did not result in higher scores for the subjects 
in Tl. The CPR users finished the course with as large a gap between their scores and those of the non-users 
on the f-1 exam questions covered by CPR. as they initially had on their midterm exams. However, the 
multimedia software users, T2, did halve the differences between their scores and their matching control 
group. The scores of all task measurementsare displayed in Table 3. All scores in Table 3 have been normalized 
in order to more readily compare them. There were 100 total points possible on the midterm. Ten points 
was the maximum score possible on quizzes and questions on the final exam. The titles indicate scores 
on the midterm (Mid), three quizzes (Ql, 42, Q3), and two questions from the final exam (Pl and Fz). 
Ihe differences between the midterm, .9, and the final exam question scores, 1.0, of the CPR users (Tl) 
and non-users (Cl) are about the same. The same differences between the midterm, .6, and the final exam 
questions, .4 and .3, between CPRNM users (T2) and non-users (C2) have been halved. It is clear that 
the subjects who used the multimedia software raised their understanding of pointers, as reflected by 
increased scores on the two final exam questions, to approach the scores of the more capable students 
(at midterm) who did not use the software (Cl). However, the CPR users did slightly worse on their final 
exam questions than their counterparts who did not use the software, increasing the distance between 
the two groups even more than on the midterm. The questions on pointers comprised only 2 of the 13 questions 
on the final exam. One can only wonder how much more, if any, the multhnedia software users might have 
improved their final exam scores across all questions if laboratory modules covering all the topics in 
the course bad been available to them. As de Kemp stated in his keynote address to the Educational Multimedia 
and Hypermedia Ed- Media Conference the use of interactive multimedia systems will improve the quality 
and speed of learning. [4] Group Mid Ql Q2 Q3 Fl F2 condusions One of the greatest surprises and disappointments 
in this study was that more students had not elected to use the software. In order to determine the reason 
for this, a questionnaire was ftied out by T2 and C2 following their final exam. The question asked was 
If you had to choose between learning a concept on the computer or learning the concept with an instructor, 
which would you choose and why? The students overwhelmingly chose the instructor (73%) over the computer 
(lS%), with only 9% opting for a combination of both. It appears, then, that the students are not as 
inclined to use software to promote learning as is commonly believed. We are encouraged that the multimedia 
software did appear to help the subjects learn pointers. As Adams, et. al. s,[l] stated in the working 
group on interactive multimedia pedagogies at the 1996 SIGCSe/sIGCUE Conference, the use of multimedia 
has received criticism for not demonstrating empirically validated improvements hl hxmlhlg. However, 
one of the subjects in T2, and parenthetically, the best student of the fall 1996 semester, shared these 
.observations with us as to why he used the multimedia software: f I I zkidly don t learn very much while 
in the classroom. I listen to what is going on and take notes and tlzen make sense of it when I get home. 
Although in most cases I read the textbook fairly thoroughly, I would rather not if I have an alternative. 
The software is helpjid because it is much more enjoyable than reading a textbook The lSinlpsons . sounds 
definitely added excitement to the program and the animations were also nice. The sojlware also has many 
example problems with answers, which I really like because I think I learn more by seeing and doing examples 
than by just reading about something. The textbook we used in class, as well as any other C books I ve 
seen, does not provide numerous examples that cover the essentials of using pointers and many commonly 
made mistakes. 1 - According to Elliott [3], academia would like to see more computer based instruction, 
but time is a major impediment. We c+n verify this: CPR r&#38;tired 400 hours of coding and debugging 
on the computer. It con&#38;cd of 5,500 lines of code with 193 external ASCII- files. CPRMM required 
200 hours on the computer and consisted of 145 pages. Neither of these times includes the planning, wording 
of lessons, testing, ev+ating, etc. of the software. Both pieces of software are alpha versions. Many 
more hours are necessaq to polish and finalize their form. L So, is the effort involved to develop lab 
modules worth the minimal results in increased student scores? If the software is an intelligent tutoring 
system, our response would be, a resounding no. However, we would be willing to develop further multimedia 
software modules, although others might feel that the results do not justify the time and effort We would 
have liked to see hard evidence, in the form of statistical data, that software contributes to a student 
s learning. But since the poorest students withdrew from the course, and the software has a possibly 
greater potential for helping this type of student, we are satisfied that the slight progress experienced 
by the multimedia software users;is indicative that our efforts were not in vain, with regard to development 
time. However, we are not convinced that all students should be required to complete lab modules. Some 
of them clearly do not require extra practice in order to lcarn a concept, even a complicated OIE. Refetmcts 
1. 2. 3. 4. 5. 6. 7. 8. 9.  Adams, E.S., Carswell, L., Ellis, A., Kumar, A., Mayer, J., Motil, 
J. (1996) Interactive Multimedia Pedagogies. SZGCSE Bulletin, v28 pp. 182191 Allmson, LJ. (1991) Designing 
and Evaluating the Navigational Toolkit. In Engel, EL., Bouwhuis, D.G., Bosser, T., d Ydewalle, G. (Eds) 
Cognitive Modelling and Zn~ractive Environments in Language Learning, Springer- Verlag, Berlin pp. 287-93 
Elliott, GJ., Jones, E., Cooke, A. (1995) Making Sense: A Review of Hypermedia in Higher Education. In 
Maurer, H, (Ed) Proceedingsof Ed-Media 95, pp. 205-210 de Kemp, A. (1995) The New Learning. In Maurer, 
H. (Ed) Proceedings of Ed-Media 95, ,PP. I Kenny, R.F. (1995) The Generative Effects of Instructional 
Organizers with Computer-based Interactive Video. Journal of Educational Computing, v12 n3 pp. 275-296 
Jordaan, D.B., Gilliland, S. (1994) Teaching Pascal Using Multimedia. South African Computer Journal, 
n12 pp. 50-52 Tjaden, BJ., Martin, C.D. (1995) Learning Effects of CA1 on College Students. Computers 
&#38; Education, ~24 n4 pp. 271-277 Tjaden, B.J. (1996) Teaching C Pointers to Novice Programmers using 
an ITS: An Empirical Study. Mark Ireland (Ed) Proceedings of the 1st Psychology of Programming Interest 
Group, Matlock, UK, pp. 130-137 Tjaden, BJ., Martin, C.D. (1997) How Effectively Do College Students 
Learn Programming Concepts using Software? Proceedings of ED-MEDJWED-TELECOM 97, pp. 1936-1937 !  . 
 
			