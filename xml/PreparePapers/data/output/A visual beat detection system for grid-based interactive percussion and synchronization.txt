
 Trishul Mallikarjuna Center for Music Technology Georgia Institute of Technology, Atlanta, GA 30332 
USA A Visual Beat Detection System for Grid-Based Interactive Percussion and Synchronization trishul.mallikarjuna@gatech.edu 
Abstract This document introduces a conceptually novel, simple and ordinarily robust computer-vision-based 
method of extracting musical beats from regular physical gestures of a performer, implemented in VisiBeat: 
a grid-based percussion system on the Max/MSP/Jitter platform for collaborative interactive music. Keywords 
 Music, beat, Horn, Schunk, optical flow, Max, MSP, Jitter, cv.jit, computer vision, grid, physical gestures, 
collaboration, interaction 1. Background By providing newer interfaces, technology is getting more nonÂ­musicians 
involved in music-making, while enabling musicians to have newer and more direct approaches towards the 
same. In this direction, interfaces based on routine physical actions of humans can be considered to 
be some of the more tangible ones, as humans can relate to actions like nodding their head, waving their 
hands or tapping their feet more easily than the skillful playing of an instrument. Many researchers 
have worked on mapping physical gestures to musical events, such as Eric Singer (commercially released 
Cyclops Max object [1]), Todd Winkler [2] &#38; David Rokeby ( Very Nervous System [2]). Many of these 
systems have been asynchronously used in contexts like dance [3] and interactive music [1], and the inverse 
process of generating visual gestures to musical input has also been successfully pursued [4]. However, 
there doesn t seem to have been an attempt to extract beats from regular, periodic gestures of performers 
and generating musical output in synchrony with their movements. The project VisiBeat discussed here 
aims at the same. 2. Processing The system developed is implemented for prototyping on Cycling-74 s 
Max/MSP/Jitter software platform by on an Apple iMac. Video processing is done using Jitter and cv.jit[2] 
objects, including a custom-built Jitter external in C. It takes in live video input through the camera, 
divides it into a grid (3x4), and processes each pane of the grid to extract beat. Steps involved in 
video processing for beat detection are as follows: 1.1. Input RGB is converted to Luminescence matrix 
1.2. Result is subjected to Horn-Schunk algorithm to detect optical flow, which detects motion for each 
pixel in 4 directions left, right, up and down (different colors in top figure). 1.3. Then, the result 
for all pixels of a pane is averaged to give out a single number for each direction separately 1.4. Results 
for opposite directions along vertical and horizontal axes are added with opposite signs to obtain a 
waveform of oscillation for each axis (bottom figure) 1.5. The troughs of this waveform are taken as 
the instantaneous event onset locations corresponding to the downbeat of nodding/tapping 1.6. Inter-onset-intervals 
(IOIs) are determined 1.7. If IOIs are stable within a factor (0.25) for a good number of consecutive 
runs (3), their average value is taken as the new beat period 1.8. Result is passed on to the percussive 
music synthesis system down the line and used appropriately In the prototype implementation, the beat 
periods are fed into a metronome object that triggers percussive sounds through the internal DLS software 
MIDI synthesizer on the iMac. Volume of all sounds are made proportional to the activity (oscillation 
amplitude) in the corresponding pane, and exponentially damped to give echoing effect. Additionally, 
a Jitter external was also developed to obtain activity level and x/y co-ordinates for peak activity, 
which enabled users to use a few panes as virtual x/y tactile pads for controlling melodic instrument 
voices. 3. Performance and Interface The system is robust in that the scene illumination and contrast 
doesn t need to be adjusted too precisely -and the system would work for various ambiences with little 
retuning required. It doesn t depend on a green screen and doesn t use chroma-keying. Oscillation amplitudes 
are also appropriately normalized and scaled, to ensure reasonable output volume for varied activity 
levels resulting, for example, from different distances of the performer from the camera and different 
lighting conditions. Further, the system was found to respond well within the beat period limits set 
in the system: 125 and 1500 ms. The performers would know in which pane(s) activity is being detected 
through a video interface with real-time feedback of their actions (see representative image); the panes 
without activity would go dark exponentially. 4. Further work The system is being optimized for speed 
with GPGPU programming to promote higher resolutions &#38; frame rates for higher accuracy. Further, 
two-dimensional pattern detection is being developed for analyzing the x/y oscillation waveforms so that 
more complex patterns (like figure of 8) may be detected and used for both downbeat and upbeat ; the 
current system detects only downbeats from linear oscillatory motion. 5. Acknowledgements Thanks to 
Dr. Jason Freeman from GT Center for Music Technology for advising the academic course on interactive 
music under which the project was taken up. 6. References [1] Singer E., Cyclops , Web Resource: http://www.ericsinger.com/cyclopsmax.html 
[2] Winkler T., Motion-Sensing Music: Artistic and Technical Challenges in Two Works for Dance , ICMC 
1998 [3] Winkler T., Fusing Movement, Sound, and Video in Falling Up, an Interactive Dance/Theatre Production 
, NIME 2002 [4] Singer E., Castiglia C., Liao S., Perlin K., "Real-time Responsive Synthetic Dancers 
and Musicians"; SIGGRAPH 1996 Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, 
Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			