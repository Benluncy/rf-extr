
 76 Issues in Cache Management Algorithms for Commercial Software Systems PrabuddhaBiswas Oracle Corporation 
One Oracle Drive, Nashua, NH 03062. prbiswas@us.oracle.com Abstract Cuche sizes in software systems 
have increased dramatically in the last few years making it necessary to revisit cache management policies. 
This paper raises some issues that might be useful in improving software cache performance and stresses 
the importance of operational overhead of algorithms. Introduction Caches are used in software systems 
to keep frequently accessed data in memory to avoid disk I/O operations. Most applications would not 
even be practical without these caches. Several studies have been done to understand the behavior of 
cache management algorithms and several innovative algorithms have been proposed and evaluated [l-5]. 
Two primary observations emerge from the different studies: . The focus of most of these studies are 
on reducing the miss rate of the cache. The penalty of a disk access is so heavy that reducing the number 
of disk accesses is almost the only goal of these studies. Some study the impact of the caching policies 
on the overall response time of the system. In distributed systems, the network latencies and the cost 
of processing network requests is factored into the cost of keeping caches on different systems coherent. 
But very little analysis is done on the processing costs and synchronization overheads of the different 
algorithms. Scalability of these algorithms on systems with large caches or with many processing units 
is often ignored. . The least recently used (LRU) algorithm is quite successful in most environments. 
The LRU cache replacement algorithm or some simple variant of it is the overwhelming favorite for most 
commercial systems. Most software system designers almost implicitly II default to the use of LRU without 
considering the Pccess behavior of their application. Often B cache is put in as an after thought to 
improve performance. There are some trends that might warrant revisiting caching algorithms. First, caches 
are becoming huge - Permission to make digital or hard copies of all or part of this work for not made 
or distributed for profit or commercial advantage and that several gigabytes in most cases. Many of the 
caching studies were done in the 70s and 80s when caches were of the order of megabytes. Algorithms that 
are i-easonable for small caches may not be appropriate for the gigabyte caches. Second, it may not be 
possible to achieve high cache hit rates (high ninety percent) with the large caches without the use 
of specialized algorithms based on detailed information about the gccess pattern. It is also extremely 
important to understand the complexity and processing overheads of different competing caching polices. 
Areas of Investigation In this section we will briefly talk about several possible areas tif investigation. 
Some of these ideas have been explored to a certain extent in the past but we need to look at all the 
different aspects of caching policies in light of the new hardware and software trends. Cost of implementing 
caching algorithm An interesting analogy comes from the hardware CPU cache arena. It is generally true 
that a set associative caches provide better hit-rate than a direct-mapped cache of the same size. But 
the CPU clock cycle time inust be stretched 5-10% to accommodate tie selection multiplexer of the set-associative 
cache. As a result, it might be more effective to use a direct mapped cache because the overall cycle-per-instruction 
(CPI) may be higher for the set-associative case in spite of its lower miss-rate [6]. Similarly in a 
software caching environment we heed to focus on the overall processing cost of the cache hit and miss 
paths and introduce an average processing %ost per access metric to evaluate caching algorithms. A few 
changes to reduce the processing cost of the basic LRU cache replacement policy was found to provide 
benefits in large memory, multiprocessing systems. It was observed that the most common access pattern 
is one where a hot object is touched several times in quick succession and then not reaccessed for B 
long period of time. In that environment, it does not make sense to repeatedly move a hot object back 
to the head of the LRU chain. A variation of the basic LRU scheme would be one where an object in the 
hot region of the cache is not moved within the cache and only when it is accessed in the cold pegion 
it is moved back to the head bf the hot personal or classroom use is granted without fee provided that 
copies are copies bear this notice and the full citation on the first page. To copy otherwise, to republish, 
to post on servers or to redistribute to lists, requires prior specific permission and/or a fee . WOSP99, 
Santa Fe, N.M. (c) 1999 ACM l-58113.060-O/99/00... $5.00 end of the LRU chain. A substantial amount of 
processing and latching overhead is avoided by this change without affecting the hit-rate. Another change 
is to break the single LRU cache on large multiprocessing systems into many (in practice, 1 to 2 times 
the number of processors is appropriate) smaller LRU chains. In this case, when one processor has a latch 
on one LRU chain, it will not block other processors from accessing the other LRU chains. This policy 
greatly reduces multiprocessor synchronization wait times. We need to revisit some of the following ideas 
to improve cache performance and understand their processing and latching overheads. Specialized caches 
 The application developer is often aware, ahead of time, of the access pattern of certain objects. In 
a database environment, the administrator might observe that a few large tables dominate the buffer cache 
but the hit rate on those buffers are low. It makes sense to partition the cache and assign tables with 
particular access properties to these partitions. Each cache partition may have their own specialized 
cache replacement policy. Database pages with low hit rate probability could be forced out ahead of pages 
with higher hit rate probabilities. Tables may be statically assigned to specialized caches with an appropriate 
caching policy that match their access characteristics. An obvious extension would be to dynamically 
assign tables to cache partitions. Maintaining statistics on cache objects We can maintain access characteristics 
on objects in the cache and dynamically alter their cache residence properties [8]. If we have specialized 
caches of different sizes, an object may migrate between caches based on its access pattern. With large 
caches, objects spend ample time in the cache to gather access patterns which can then be used to determine 
the cache management policy. We may keep track of objects that are accessed together and use that information 
to improve prefetching algorithms. Using storage architecture information In most practical systems, 
the cache has no information about the storage architecture. We can improve the cache management algorithms 
if we understand the characteristics of the storage subsystem (the disk striping, the hierarchy and type 
of caching used, etc). For example, if the data is on a RAID-5 array with a write-through cache then 
the software cache algorithm to write out modified pages should take into account the stripe width and 
size. Storage subsystems often have large caches with smart prefetch policies. They often contain large 
amounts of bon-volatile memory and implement efficient writing policies. If the software subsystem is 
aware of these Eapabilities, it can Save precious processing time by avoiding things that will be automatically 
aone % the I/O controller ar even better, it might do things that exploit t%e I/O caching algorithms 
more effectively. Optimizer directed caching for databases In the last point we raised the issue of 
how the hardware and software subsystems should cooperate. Continuing on a similar idea, the different 
software subsystems should also cooperate to optimize the overall performance of the system. The basic 
problem with most caching algorithms is that they do not have knowledge of the application s future access 
pattern and therefore cannot make optimal cache replacement decisions. In a database system, the query 
optimizer has valuable information about the access pattern that the cache manager should tixploit [7]. 
In inost commercial database systems there is no interaction between the query optimizer and the database 
cache manager. In fact, the query optimizer should be able to use the effectiveness of the buffer cache 
for different tables to determine the cost of different access options. References 1. A.J. Smith, Disk 
Cache -miss ratio analysis and design considerations , ACM Transactions on Computer Systems, August 1985. 
 2. J.T. Robinson and M.V. Devarakonda, Data Cache Management Using Frequency Based Replacement , Proceedings 
of the ACM Conference on Measurement and Modeling of Computer Systems, Boulder, Colorado, USA, May 1990. 
 3. P. Biswas, K.K. Ramakrishnan, D. Towsley, Trace Driven Analysis of Write Caching Policies for Disks 
, Proceedings of the ACM Conference on Measurement and Modeling of Computer Systems, Santa Clara, California, 
USA, May 1993. 4. V.F. Nicola, A. Dan, D.M. Dias, Analysis of Generalized Clock Buffer Replacement Scheme 
for Database Transaction Processing , 5. R. Jauhari, M.J. Carey, M. Livney, Priority-Hints: An Algorithm 
for Priority-Based Buffer Management , Proceedings of the I 61h VLDB Conference, Brisbane, Australia, 
1990. 6. J.L. Hennessy, D.A. Patterson, Computer Architecture: A Quantitative Approach , Morgan Kaufman 
Publishers Inc., San Mateo, California, USA. 7. H.T. Chou, D. Dewitt, An Evaluation of Buffer Management 
Strategies for Relational Database Systems , Proceedings of the I I lh VLDB Conference, Stockholm, 1985. 
 8. S. Dar, et al., Semantic Data Caching and Replacement , Proceedings of the 221h VLDB Conference, 
Mumbai, India, 1996.  77 
			