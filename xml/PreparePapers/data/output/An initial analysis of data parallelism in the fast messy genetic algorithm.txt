
 AN INITIAL ANALYSIS OF DATA PARALLELISM IN THE FAST MESSY GENETIC ALGORITHM Laurence D. Merkle and 
Gary B. Lamont Air Force Institute of Technology Keywords Data Parallelism, Optimization, Genetic Algorithms 
 Abstract Genetic algorithms (GAs) are highly parallelizable algorithms, inspired by Darwiniaa theories 
of evolution and survival of the fittest, which are used most frequently as function optimizers. The 
messy GA [4, 5, 6] makes use of partially enumerative ini- tialization (PEI) and tournament selection, 
with competitions restricted to similar individuals. Consequently, efficient par-allelization of the 
messy GA demands consideration of data distribution [12]. The fast messy GA replaces PEI with Prob- abilistically 
Complete Initialization (PCI) and building block filtering, and increases the threshold for similarity 
determina- tions [7]. Two algorithmic design approaches to parallelization of the fast messy GA are presented. 
One uses independent sub- populations throughout the execution, while the other uses re- combines subpopulations 
following the primordial phase. Ex-ecution times are examined theoretically and experimentally against 
a fully deceptive function. Experiments are performed on an Intel Paragon parallel supercomputer. Introduction 
The genetic algorithm (GA) has its inspiration in processes of evolution and natural selection [9]. Substantial 
empirical data exists which suggests that the GA is well suited for op- *Sponsored by the Air Force Office 
of Scientific Research and the Air Force Materiel Command's Electronic Systems Center 1994 ACM 089791-647-6/ 
94/ 0003 timization of some classes of functions (e.g. see DeJong [2]). One class for which the GA is 
known to be less well suited is that of deceptive functions [13]. The messy GA (Section) is designed 
specifically to address the limitations of the simple GA associated with this class [4, 5, 6]. The practical 
application of the messy GA is limited by its very large initial population size and corresponding long 
execution times. In order to overcome these limitations, Goldberg, et. al. [7] propose several modifications 
to the original messy GA. He refers to the modified algorithm as the fast messy GA. This paper examines 
two methods for exploiting the data parallelism present in the fast messy GA. The methods are presented 
and examined theoretically, and their performance against a difficult test problem is compared experimentally. 
 Messy Genetic Algorithms The messy GA consists of an initialization, a primordial, and a juxtapositional 
phase. Its improved ability to solve deceptive problems stems from the focus on increasing the proportion 
of highly fit buiding blocks before applying recombinative oper- ators. As originally proposed, this 
is accomplished throught the use of partially enumerative initialization (PEI) and tour- nament selection. 
Original Messy GA PEI results inn population consisting of all possible partial solutions defined over 
k loci, where k is the estimated level of deception present. For an application in which each string 
contains £ genes, and each gene has C possible alleles, the initial population contains k (1) solutions. 
For even modest values of k this is significantly larger than a typical simple GA population size. For 
exam- ple, for a problem using a binary representation with £ = 50 and k = 5, the initial population 
contains 6.78 × 107 individ- uals. Typical simple GA population sizes are in the tens to can be generated 
which consists of strings of length k and thousands. which is dominated by highly fit building blocks. 
Tournament selection is then.used in the juxtapositional phase to reduce the population size by eliminating 
less fit partial so-lutions. Competition is limited to those partial solutions for which the number of 
common definin~ loci is greater than the expected value. A locally optimal solution, called the compet-itive 
template, is used to "fill in the gaps" in partially specified solutions to allow their evaluation and 
subsequent selection. The juxtapqsitional phase is similar to the simple GA in its use of recombinative 
operators. However, in order to recombine variable length strings, standard crossover is replaced by 
cut-and-spllce, which is a one-point crossover operating on variable length strings. Splice and bitwise 
cut probabilities are speci- fied, and are normally chosen to promote rapid string growth from k to g 
[4]. Fast Messy GA In order to reduce the size of the messy GA initial population and the execution time 
of the initialization and primordial phases, Goldberg, et. al. propose three modifications: use of ProbabilisticMly 
Complete Initialization (PCI) in place of PEI, use of building block filtering, and more conservative 
thresholding in tournament selection [7]. The objective of PCI is to ensure that each building block 
is represented in the initial populatioh, and that the expected number of copies for each is sufficient 
to overcome sampling noise. Each individual in the PCI population is defined at g' = -k loci, which are 
selected randomly without replacement (it is assumed that k << g). Goldberg, et. al. argue that the objective 
can be satisfied using a population size of n = 2~(o~)a~(m -1)2 ~ (2) ( where ~ is a parameter specifying 
the probability of selection error between two competing building blocks, P[Z > z(~)] = 1 - c~ where 
Z is a standard normal random variable, B 2 is a parameter specifying the maximum inverse signal-to-noise 
ratio per subfunction to be detected, and m is the number of subfunctions. The fast messy GA uses tournament 
selection and building block filtering (BBF) to enrich the population in the primor- dial phase. The 
effect of several iterations of tournament selec- tion is to substantially increase the number of copies 
'of those " individuals in the population which contain highly fit building blocks. BBF then reduces 
the number of defining loci for each individual by randomly deleting some number of genes. In the process 
it disrupts many but not all of the highly fit building blocks. Some individuals containing highly fit 
building blocks remain to receive additional copies in subsequent iterations of tournament selection. 
Thus, by carefully choosing a primor- dial schedule of tournament selection and BBF, a population For 
any particular problem, some building blocks may con-tribute more to the total fitness of the optimal 
solution than others. However, all of the necessary building blocks must be represented in the population 
if there is to be any chance of combining them. Thus, it is important that the most highly fit building 
blocks do not receive so many copies through tourna- ment selection that the important but less fit building 
blocks are eliminated. This is achieved by allowing competition only between those individuals which 
share at least  o=[~.-..--2.2+ ~2(~')~ l (3) common defining loci, where ),1 and A2 are the number 
of defining loci for each individual, g and z are as defined above, and e2(e -X) (4) is the variance 
of the distribution of a random variable L cor- responding to the number of defining loci shared by two 
ran- domly selected individuals. Thus, a normal distribution is used to approximate the distribution 
of L associated with the initial population. Both Goldberg, et. al. [7] and the present paper ignore 
the dependence of L's distribution on the dynam- ics of the genetic population in subsequent generations. 
Since this threshold is more restrictive, the number of individ- uals tested for compatibility, called 
the shuffle size, is increased accordingly. In practice, it is currently common to use a full shuffle 
and an empirically determined threshold schedule [10]. Exploitation of Data Parallelism A common approach 
to implementation of GAs on coarse grained parallel architectures is the island model [8]. In this approach 
each processor executes a separate GA on a sub-population. Numerous variations exist in which either 
the selection operation executing on a particular processor is af- fected by other processors' subpopulations, 
or processors com- municate some portion of their subpopulations to other pro- cessors. These approaches 
extend with some modifications to the messy GA [12]. This paper examines the solution qual- ity and execution 
time obtained using two designs of the fast messy GA based on the island model. The designs are chosen 
for their simplicity, and do not consider alternatives such as non-homogenous subpopulations [12] and 
migration [11]. The first design, called the fully parallel design, uses indepen- dent subpopulations 
throughout the execution. One proces- sor (the controller), in addition to executing the fast messy GA, 
reads the input parameters and broadcasts them to the remaining processors. From Equation 2, each subpopulation 
contains the formation of an individual containing two building blocks, both building blocks must be 
present in the population. Since (') each subpopulation of the fully parallel design initially con- 
1 ~e' - talns fewer necessary building blocks, it is expected that the n m --2z2(a)fl2(m 1)2 k (5) ~' 
-k individuals, where e is the number of processors. Following completion of the juxtapositional phase, 
a brief.serial phase occurs in which each processor communicates its best indi- vidual to the controller, 
which then reports the overall best individual and its fitness. The second design, called the partially 
paralleldesign, also uses independent subpopulations in the primordial phase. Prior to the juxtapositional 
phase, the controller processor receives each of the other processors' subpopulations and combines them 
in a single population. The controller then carries out the juxtapositional phase serially. After the 
juxtapositional phase, the controller reports its best individual and its fitness. Solution Quality 
The solution quality obtained by the fast messy GA is deter- mined by the proportion of necessary building 
blocks present in the juxtapositional population, and the probability that the building blocks are combined 
in a single individual. We now consider how these quantities vary with P. The proportion of necessary 
building blocks in the overall jux- tapositional population is identical for both designs. A build- ing 
block is present in the juxtapositional population if at least one copy is present in the initial population, 
at least one copy survives each application of BBF, and at least one copy sur- vives each iteration of 
tournament selection. The expected number of copies of a particular building block in the com-bined initial 
populations of all processors is the same as the expected number of copies in the initial population 
of the sin- gle processor design. That is, it is independent of P. Also, since BBF operates on individuals 
indpendently the proba- bility that it will disrupt a building block is independent of P. The probability 
that any particular copy survives tournament selection depends upon both the fitnesses of the individuals 
in which the building block occurs, the fitnesses of the indi- viduals with which it competes, and the 
probability that such a competition wiU take place. The first two factors are in-dependent of P, but 
the latter decreases as P increases since individuals compete only against other individuals of the same 
subpopulation. Thus it is more likely that a building block will survive tournament selection in a small 
population. However, for the same reason, it is less likely that a building block will gain additional 
copies in a small population. The proportion of necessary building blocks in the juxtapositional population 
is expected to increase to a maximumoccurring when P is near the number of necessary building blocks. 
This does not imply that overall solution quality must increase as well. In order for the cut-and-splice 
operator to result in same is true of the juxtapositional subpopulations for this de- sign. Thus, the 
solution quality of the fully parallel design is expected to decrease with an increasing number of pro- 
cessors. In contrast, since the number of necessary building blocks in the combined juxtapositional populations 
is expected to achieve a maximum for some P, the solution quality of the partially ~arallel design is 
expected to achieve a maximum also. Execution Time The execution time of both designs of the fast messy 
GA de- .pends on the execution time of each of the three phases, and the time required for communication. 
The initialization time is determined by the subpopulation size, and is O(P-1). The primordial phase 
execution time is a function of the sub- population size. the shuffle size, and the probability of com- 
patibility for individuals randomly selected from the same sub- population [11]. Since the shuffle size 
is chosen to be equal to the subpopulation size, both are inversely proportional to P. The probability 
of compatiblity of randomly selected solutions does not depend on P in any obvious way. Thus, primordial 
phase execution time is expected to be O(p-2). Similarly, the juxtapositional phase execution time is 
a func- tion of subpopulation size, shuffle size, and probability of com- patibility. For the fully parallel 
design, execution time is again expected to be O(P-1). For the partially parallel design, jux- tapositional 
phase execution time is expected to be indepen- dent of P. Communication time for the fully parallel 
design is expected to be O(P) but negligible for P << n, since only the GA parameters and a single individual 
from each population is communicated. For the partially parallel design, all individ- uals except those 
in controller processor's subpopulation are communicated. Communication time is expected to increase 
asymtotically with increasing P. Thus, for reasonable problem sizes, total execution time for the both 
designs is expected to decrease as O(P -1) for P << n and increase as O(P) otherwise. For P > 1, execution 
time of the fully parallel design is expected to be less than that of the partially parallel design. 
 Experimental Results In order to experimentally determine the effects of each of the designs on solution 
quality and execution time, experi- ments are performed using a substantially updated version of the 
parallel messy GA (PMGA)[3]. The PMGA is part of AFIT's GA Toolkit, which includes several sequential 
and par- allel GAs [3, 11]. Both designs of the parallel fast messy GA are implemented on an Intel Paragon 
parallel supercomputer in C under the Paragon OSF/1 Operating System Release 1.1 operating system. For 
each design, the problem solved is the 50 bit deceptive problem addressed by Goldberg, et. al. [7]. The 
problem con- sists of ten 5-bit s'ubproblems, each of which is an order-5 fully deceptive trap function. 
The total fitness of a solution to the full problem is the sum of the fitnesses of the solutions to the 
subproblems. The encoding scheme for the function is based on a string of fifty genes and a binary genie 
alphabet. The bits corresponding to any given subproblem occur in consecutive genes. The Paragon allocates 
processors in partitions of any size from 1 to a configuration dependent maximum. For these experi- ments, 
partition sizes of 1, 2, 5, 10, 20, 40, 60, and 80 are used. Each implementation is executed 8 times 
for each of the eight partition sizes, using 8 randomly generated seeds. The same seeds are used for 
both designs and all partition sizes. The GA parameters are chosen to match those used by Goldberg, et. 
al. [7, 10]. A total of fourteen applications of BBF are performed, as shown in Table 1. The shuffle 
number is equal Table 1: BBF and Threshold Schedule ~aaar~ai:um ~:laa;1.~ Etw.~+.~gci~ 0 45 39 7 39 35 
11 34 28 15 29 23 19 25 18 23 15 29 19 13 35 16 i0 41 14 9 47 12 7 53 10 6 59 8 5 65 7 4 71 6 3 77 5 
4 to the subpopulation size, the cut probability is 0.02, and the splice probability is 1.0. The primordial 
phase has 81 gener- ations and the juxtapositional has 8. The overflow factor is 1.6, and the total population 
size is 1786 (Kargupta actually reports a population size of 1785). No outer loop is performed, and the 
competitive template is forced to consist of all 0's. The average solution quality obtained using each 
of the designs for each partition size is shown in Figure 1. -Likewise. the mean execution time for each 
of the desings for each partition size is shown in Figure 2. Conclusions The fully parallel design of 
the fast messy genetic algorithm exhibits apparently super-linear speedup up to at least 80 pro- cessors 
for the test problem used in this paper. It does so at the cost of an asymtotic decre~e in solution quality. 
The 10 ...... FulV PI~QII~ DUn 9.S 8.$ , ~ ' $ \,\ l 77.5 '\\ ..... ? I f 65 I0 I ! I I 1 ~ 30 ~0 50 
60 70 80 PT'I:0 IIIICI'I Figure 1: Mean Fitness 1000 ~,. PI1iely Pl~ilel Oemign "'-. Liner ,~,e,~,lp 
...... 100 "..,. 10 "x% 100 Figure 2: Overall Speedup partially parallel design also exhibits apparently 
super-linear speedup, but does so while achieving a maximum in solution quality when the number of processors 
is near the number of building blocks necessary to construct a complete solution.  Recommendations The 
designs examined in this paper do not consider options such as non-homogenous initial subpopulations. 
It has been shown elsewhere [12] that the execution time of the parallel messy GA is strongly affected 
by the distribution of individuals in the initial subpopulations, and in particular bY the proba- bility 
of compatibility between solutions in those populations. Thus, a logical next step is to incorporate 
non-homogenous initialization strategies in the parallel f~t messy GA. Also, migration has been shown 
to have significant effects on both solution quality and execution time in the parallel simple GA. It 
is a natural enhancement of the parallel fast messy GA designs discussed in this paper to incorporate 
such migration methods in both the primordial and juxtapositional phases. Finally, current understanding 
of the fast messy GA is in- sufficient to provide a methodology for selecting a primordial schedule of 
building block filtering and appropriate tourna-ment. eelection thresholds and shuffle sizes. The fast 
messy GA will be substantially more applicable to real problems when a complete theory is available. 
Such a theory must provide a means for dynamically determining thresholds and shuffle sizes based: upon 
the current distribution of the GA population.  Author Contact Information Graduate School of Engineering, 
Department of Electrical and Computer Engineering, ~Vright-Patterson AFB OH 45433-7765 {lmerlde, lamont}~afit.af.mil 
 References [1] Allen, Arnold O. Probability, Statistics, and Queueing Theory: With Computer Science 
Applications. Computer Science and Scientific Computing, San Diego, California: Academic Press, Inc., 
1990. [2] DeJong, Kenneth A. An Analysis of the Behavior of a Class of Genetic Adaptive Systems.. PhD 
dissertation, The University of Michigan, Ann Arbor MI, 1975. [3] Dymek, Capt Andrew. An Examination 
of Hypercube Implementations of Genetic Algorithms. MS thesis, AFIT/GCE/ENG/92-M, Air Force Institute 
of Technol- ogy School of Engineering, Wright-Patterson AFB OH, March 1992 (AD-A248092). [4] Goldberg, 
David E: and others. "Messy Genetic Algo- rithms: Motivation, Analysis, and First Results," Com-plex 
Systems, 3:493-530 (1989). [5] Goldberg, David E. and others. "Messy Genetic Algo- rithms Revisited," 
Complex Systems, ~:415-444 (1990). [6] Goldberg, David E. and others. "Don't Worry, Be Messy." Proceedings 
of the Fourth International Conference on Genetic Algorithms. 24-30. San Mateo CA: Morgan Kaufmann Publishers, 
Inc., 1991. [7] Goldberg, David E. and others. "Rapid, Accurate Op- timization of Difficult Problems 
Using Fast Messy Ge- netic Algorithms." Proceedings of the Fifth International Conference on Genetic 
Algorithms, edited by Stephanie Forrest. 5B-64. San Mateo CA: Morgan Kaufmann Pub- lishers, Inc., 1993. 
¢ [8] Gordon, V. Scott and Darell Whitley. "Serial and Par- allel Genetic Algorithms as Function Optimizers." 
Pro-ceedings of the Fifth International Conference on Genetic Algorithms, edited by Stephanie Forrest. 
177-183. San Mateo CA: Morgan Kaufmann Publishers, Inc., 1993. [9] [10] [11] [12] [13] Holland, John 
H. Adaptation in Natural and Artificial Systems (First mit press Edition). Cambridge, MA: MIT Press, 
1992. Kargupta, Hillol. "Personal communication." Tourna-ment selection parameters in the fast messy 
GA, Decem- ber 1903. Merlde, Laurence D. Generalization and Parallelization of Messy Genetic Algorithms 
and Communication in Par- allel Genetic Algorithms. MS thesis, Air Force Institute of Technology, WPAFB 
OH 45433, December 1992. Merlde, Laurence D. and Gary B. Lamont. "Compar-ison of Parallel Messy Genetic 
Algorithm Data Distri-bution Strategies." Proceedings of the Fi~h International Conference on Genetic 
Algorithms, edited by Stephanie Forrest. 191-198. San Mateo CA: Morgan Kaufmann Publishers, Inc., 1993. 
Whitley, Darrell. "Fundamental Principles of Deception in Genetic Search." Foundations of Genetic Algorithms, 
edited by G. Rawlins. San Mateo, California: Morgan Kaufmann, 1991. 492  
			