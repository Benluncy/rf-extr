
 Concept Based Query Expansion Yonggang Qiu H.P. Frei Department of Computer Science UBILAB Swiss Federal 
Institute of Technology Zurich Union Bank of Switzerland CFI-8t)92 Zurich, Switzerland CH-8021 Zurich, 
Switzerland Abstract Query expansion methods have been studied for a long time -with debatable success 
in many instances. In this paper we present a probabilistic query expansion model based on a similarity 
thesaurus which was constructed automatically. A similarity thesamus reflects domain knowledge about 
the particular collection from which it is constructed. We ad­dress the two important issues with query 
expansion: the selection and the weighting of additional search terms. In contrast to earlier methods, 
our queries are expanded by adding those terms that are most similar to the concept of the query, rather 
than selecting terms that are similar to the query terms. Our experiments show that this kind of query 
expansion results in a notable improvement in the retrieval effectiveness when measured using both recall-precision 
and usefulness. 1. Introduction In weighted Information Retrieval (II?) the number of re­trieved documents 
is related to the number of appropriate search terms. The more search terms, the more documents delivered 
by the IR system. This is why thesaurus browsers are integrated into modern II? systems. They help to 
find additional search terms [Qiu 92]. However, the aim of the retrieval activity is not to retrieve 
a large number of documents. Rather, users are interested in a high usefulness of the retrieved documents. 
The purpose of this paper is to disclose how a higher usefulness can be achieved when a query is expanded 
by choosing carefully additional search terms on the basis of statistical co-occurrence data. Research 
on automatic query expansion (or modifica­tion) was already under way before 1960 when initial re­quests 
were enlarged on the grounds of statistical evidence [Spa 91]. The idea was to obtain additional relevant 
docu­ments through expanded queries based on the co-occurrence Permission to copy without fee all or 
part of this meterial is granted provided that the copies are not made or distributed for direct commercial 
advantage, the ACM copyright notice and the title of the publication and its date appear, and notice 
is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, 
or to republish, requires e fee and/or specific permission. ACM-SIGIR 93-6/93 /Pittsburgkr, PA, USA e 
1993 ACM 0-89791-605-019310006101 60...$1.50 1 of terms. At that time, the co-occnmence of index terms 
was usually the only criterion in the absence of relevance feedback However, this kind of automatic query 
expansion has not been very successful. The retrieval effectiveness of the expanded queries was often 
no greater than, or even less than, the effectiveness of the original queries m 72, Pea 91, Sme 83]. 
We assume that documents and queries are represented by a relatively small number of weighted index and 
search terms. It is to be noted that the probability of a term repre­senting the concept of a document 
is not identical to the probability of the document representing the meaning of the term, Therefore, 
a relationship between terms can be based on the probabilities of the documents representing the terms. 
In this paper, we first give a brief introduction into pre­vious work. We then present a construction 
method that al­lows to get a simiku-ity thesaurus [Sch 92] from a given document collection. In section 
4, we describe a probabilis­tic query expansion and weighting model using the similar­ity thesaurus. 
After describing our test setting, some re­sults of experiments carried out with three standard test 
col­lections are presented in section 5. We then point out two reasons why many of the early automatic 
query expansion methods failed. Finally, we conclude with the main find­iags and point out further research 
and possible applications of the methods presented. 2. Automatic Query Expansion The automatic query 
expansion or modification based on term co-occurrence data has been studied for nearly three decades. 
The various methods proposed in the literature can be classified into the followimg four groups: 1) Simple 
use of co-occurrence data. The similmities between terms are first calculated based on the association 
hypothesis and then used to classify terms by setting a sim­ilarity threshold value Wes 69, Min 72, Spa 
71]. In this way, the set of index terms is subdivided into classes of similar terms. A query is then 
expanded by adding all the terms of the classes that contain query terms. It turns out that the idea 
of classifying terms into classes and treating the members of the same class as equivalent is too naive 
an approach to be useful [Min 72, Pea 91, Spa 91]. 2) Use of document classification. Documents are 
first classified using a document classification algorithm. Infrequent terms found in a document class 
are considered similar and clustered in the same term class (thesaurus class) [Cro 90]. The indexing 
of documents and queries is enhanced either by replacing a term by a thesaurus class or by adding a thesaurus 
class to the index data. However, the retrieval effectiveness depends stxongly on some parameters that 
are hard to determine [Cro 92]. Furthermore, commer­cial databases contain millions of documents and 
are highly dynamic. The number of documents is much larger than the number of terms in the database. 
Consequently, docu­ment classification is much more expensive and has to be done more often than the 
simple term classification men­tioned in 1). 3) Use of syntactic context. The term relations are generated 
on the basis of linguistic knowledge and co-occur­rence statistics [Gre 92, Rug 92]. The method uses 
a grammar and a dictionary to extract for each term t a list of terms. This list consists of all the 
terms that modify t. The similarities between terms are then calculated by using these modifiers from 
the list. Subsequently, a query is ex­panded by adding those terms most similar to any of the query terms. 
This produces only slightly better results than using the original queries [Gre 92]. 4) Use of relevance 
information. Relevance informa­tion is used to construct a global information structure, such as a pseudothesaurus 
[Sal 71, Sal 80] or a minimum spanning tree [Sine 83]. A query is expanded by means of this global information 
structure. The retrieval effective­ness of this method depends heavily on the user relevance information. 
Moreover, the experiments in [Sine 83] did not yield a consistent performance improvement. Gn the other 
hand, the direct use of relevance information, by sim­ply extracting terms from relevant documents, is 
proved to be effective in interactive information retrieval Ear 92, Sal 90]. However, this approach does 
not provide any help for queries without relevance information. In addition to automatic query expansion, 
semi-auto­matic query expansion has also been studied Ekm 92, Han 92, Wad 88]. In contrast to the fully 
automated methods, the user is involved in the selection of additional search terms during the semi-automatic 
expansion process. In other words, a list of candidate terms is computed by means of one of the methods 
mentioned above and presented to the user who makes the final decision. Experiments with semi­automatic 
query expansion, however, do not result in sig­nificant improvement of the retrieval effectiveness &#38;m 
92]. Among the various approaches, automatic query expan­sion by using plain co-occurrence data is the 
simplest method. In contrast to the approaches presented, we use a similarity thesaurus [Sch 92] as the 
basis of our query ex­pansion. First we show how such a similarity thesaurus is constructed and then 
we present a query expansion model in order to overcome the drawbacks of using plain co-occur­nmce data. 
3. Constructing a Similarity Thesaurus A similarity thesaurus [Sch 92] is a matrix that consists of term-term 
similarities. In contrast to a co-occurrence ma­trix, a similarity thesaurus is based on how the terms 
of the collection are indexed by the documents. We show that a similarity thesaurus can be constructed 
automatically by us­ing an arbitrary retrieval method with the roles of docu­ments and terms interchanged. 
In other words, the terms play the role of the retrievable items and the documents constitute the indexing 
features of the terms. With this arrangement: term ti is represented by a vec­tor Ii = (~1, di2, .... 
+n) in the document vector space (INS) defined by all the documents of the collection. The dk s signify 
feature weights of the indexing features (documents) dk with respect to the item (term) ti and n is the 
number of features (documents) in the collection. We adopt the normalized tf bidf weighting scheme [Sal 
88] and define the feature weights ~ by the feature frequency (ff), the inverse item frequency (iiQ, 
and the maximum feature frequency (maxff) as follows. ff(dk,ti) (0.5+ 0.5 ~=ff(ti) ) iif(dk) ~= (1) ) 
 where ff(dk,ti) is the within-item frequency of feature dk in item ti. iif(dk) = log( fi ) is the hVCXSt3 
hem frequency of fea­ ture dk; m is the number of items in the COkCtiOII and Idkl is the number of different 
items indexed by the fea­ture dk. k other words, Idkl is the number of terms appearing in document dk 
maxff(ti) is the maximum within-item frequency of all features in item ~. The feature frequency ff(dk,ti) 
specifies the number of occurrences of the indexing feature dk in item ti. It is anal­ogous to the term 
frequency tf(ti,dk) when the documents are indexed by terms. The definition of the inverse item frequency 
shows that a short document plays a more impor­tant role than a long document. If two terms co-occur 
in a long document, the probability that the two terms are simi­lar is smaller than if they would co-occur 
in a short docu­ment. From formula (1), we can derive that db : I?il= dikz = 1 (2) k= 1 This means 
that Ii is a unit vector representing the term in the document vector space DVS. With these definitions, 
we define the similarity between two terms ti and tj by using a similarity measure such as the simple 
scalar vector product: n SIM(ti,tj) = l?iT 7j = z dk djk (3) k= 1 The similarity thesaurus is constructed 
by determining the similarities of till the term ptks (ti,tj). The result is a symmetric matrix whose 
values are in the following rangcx O S SIM(ti, tj) S 1 (4) Earlier studies ~ 72, Spa71] employed the 
probabil­ities of the terms representing the documents to build a co­occnrrence matrix. In contrast, 
our similarity thesaurus is based on the probabilities of the documents representing the meanings of 
the terms. In other words, we use the weights of the documents in the terms. The construction of such 
a similarity thesaurus for a large database is computationally expensive. However, it is a single expense. 
Adding a few documents to a database with millions of documents hardly changes the relation­ships between 
terms. Furthermore, an update of the simi­larity thesaurus can be achieved by modifying only those entries 
corresponding to terms contained in the new docu­ments. More precisely, we can evaluate the similarities 
be­tween the newly arrived terms and then update correspond­ing entries in the similarity thesaurus. 
How much of this can be done without resealing is an open research issue and is likely to depend on the 
kind of domain knowledge. 4. A Probabilistic Query Expansion Model As already mentioned, most attempts 
at automatically ex­panding queries failed to improve the retrieval effectiveness. The opposite case 
was often true: Expanded queries were less effective than the original queries. Therefore, it was of­ten 
concluded that automatic query expansion based on sta­tistical data was unable to bring a substantial 
improvement in the retrieval effectiveness ~ea 91]. However, our belief is that two of the basic problems 
were not solved when ex­panding queries automatically: 1) the selection of suitable terms; 2) the weighting 
of the selected additional search terms.  We pointed out in section 2 that with most methods, terms 
are selected that are strongly related to one of the query terms. The methods differ in the kind of relationships 
used. The entire query -in other words, the query concept ­is seldom taken into account. This may be 
compared to translating from a natural language text into anothen A dic­tionary look-up for a word does 
not give the final answer in many cases. Rather, the translator who knows the meaning of the text has 
to choose the suitable word from an entire list of possible translations. Likewise, we should consider 
a term that is similar to the query concept rather than one that is only similar to a single term in 
the query. T Fig. 1: Relationships between terms and query in the DVS Let T be a set of indexing terms 
and q be the user query containing two terms, t and t , as shown in Fig. 1. The similarity thesaurus 
of the collection contains the pair-wise similarities of all the terms with respect to this particular 
collection. In Fig. 1 we have represented these pair-wise similarities with fine lines. The closer two 
linked terms are to each other, the more similar they are. t3 is more similar to t than all the others, 
t4 is more similar to t than all the other terms. In addition, we show the virtual term qc that is supposed 
to represent the general concept of the query q. This concept may be obtained by simply calculating the 
centroid of q or by using an appropriate information struc­ture. If the number of terms to be added to 
the query is 2, should we choose t3 and t~ or some other terms? The an­swer to this question is pretty 
obvious when considering Fig. 1: Since t ~ and tz are the terms most similar to the query concept ~, 
they will be chosen as additional search terms instead of t3 and t4. In what follows we explain how such 
terms are determined by using the similarity thesaurus. A query q is represented by a vector ~ = (ql, 
qj, .,., QT in the term vector space (TVS) defined by all the terms of the collection. Here, the qi s 
are the weights of the search terms ti contained in the query q; m is the total number of terms in the 
collection. The probability that a term t is similar to the concept of query q is P(Slq,t). In order 
to estimate the probability, we first apply Bayes theorem -.. . and gee @q t) = (sit) P(qls,t) P(qlt) 
= ~ P(qk) P(qls,t) We assume that the distribution of terms in all the queries to which a term is similar 
is independent: W .fi P(qilS,t) P(sl%o = p(qlt) i_ ~ P&#38;ltJ m P(Slqi,t) = P(qik)P(qlt) 11i=l P(slt) 
 m 1 + ~ p(Slqi,t)-p(qilt) = P(qlt) P(slt)m-1 i=l An additional assumption is that the similarity between 
a term and the concept of a query depends only on the terms contained in the query and not on other terms. 
Hence, 1 P(slq,t) = ~ p(slti,t)p(tilt) (5) P(qlt) P(slt)m-1 ti= q Here, P(Slti,t) is the probability 
that the query term ti is similar to the term t. P(tilt) is the probability that the query term q represents 
the query q. P(qlt) is the probabil­ity that the query q will be submitted to the IR system. P(Slt) is 
the probability that the term t is similar to an arbi­trary query. Formula (5) elucidates that the probability 
of a term to be similar to a query depends on the following factors: -the similarities between the term 
and all the query terms; -the weights of the query terms. As mentioned above, the objective of our query 
expan­sion scheme is to find suitable additional query terms. They should have the property of being 
similar to the entire query rather than to individual query terms. We showed that such terms can only 
be found when an overall similarity scheme is taken into account. Since the similarity the­saums expresses 
the similarity between the terms of the collection in the DVS (defined by the documents of the col­lection), 
we map the vector ~ from the TVS (defined by the terms of the collection) into a vector in space DVS. 
This way, the overall similarity between a term and the query can be estimated. Each query term ~ is 
defined by the unit vec­tor Ii which itself is defined by a number of documents as was pointed out in 
section 3. qi is the weight of term ti b the query. In other words, the concept expressed by the term 
ti in the query has an importance of qi -~i for the query. We assume that the concept expressed by the 
entire query depends only on the terms in the query. Therefore, the vector ~c representing the query 
concept in space DVS is the virtual term vecto~ (6) tiGq  The similarity between a term and the query 
q is denoted by Simqt(q,t). The scalar vector product is used as similari­ty measure: Simqt(q,t) = ilcT 
7 ( z qi ~i)T ~ tie q  = X qi (~~ ~) ti= q Where (~iT ~) is the similarity between two terms defined 
in formula (3): Sirnqt(q,t) = qi SIM(ti,t) (7) z ti= q 163 It is to be noted that the viiues of SIM(ti,t) 
are the en­tries of our similarity thesaurus and therefore are pre-com­puted. All the terms of T can 
now be ranked according to their Simqt value with respect to the query q. The terms t with higher Simqt(q,t) 
are candidates to be considered as ad­ditional search terms. It seems natural to choose the weight weighta(q,t) 
of a selected additional search term t as a function of Simqt(q,t): weighta(q,t)= = (8) tiEq  where 
Os weighta(q,t) S 1 With this choice, additional search terms with similarity 1 to all the terms in the 
query get a weight of 1, additional search terms with similarity O to all the terms in the query get 
a weight of O. After having determined how terms are selected and weighted, we can take into account 
the domain knowledge contained in the similarity thesaurus to find the most likely intended interpretation 
for the user s query. When relevance feedback techniques are used, queries are expanded by adding terms 
from the retrieved relevant documents. The experi­ments in Kar 92] show that adding as few as 20 top 
prop­erly ranked terms, rather than all the terms from the re­trieved relevant documents, can result 
in significant perfor­mance improvement. This is the reason we also add only those terms that are ranked 
in the top positions by the Simqt function, Another reason for only choosing the top ranked terms as 
opposed to setting a weight threshold is for the efficiency sake. The efficiency (response time) of an 
IR system de­pends heavily on the number of terms of the query submit­ted to the system. With a threshold, 
this number cannot be predicted. Therefore, the query q is expanded by adding the follow­ing query qel 
(9) where weighta(q,tj) if tj belongs to the top r ranked terms %j= ~ { otherwise r is the number of 
terms to be added or modified in weight. The resulting expanded query qexpanded is: (lo) After this expansion 
process, new terms may have been added to the original query and the weight of an original query term 
may have been modified had the term belonged to the top ranked terms. The important point of this method 
is that additional search terms are selected dynamically when a query is sub­mitted. More precisely, 
the query and the terms most sirni-Iar to the query concept are classified in the same class. This is 
in contrast to earlier studies when term-classifica­tion was done statically. We believe an important 
weak­ness of the static classification is that it is far too limited to capture both the rich semantics 
of data collections and the information need of users. Let us explain our approach by setting r, the 
number of terms to be added or modified in weight, to m, the total number of terms. In this case, the 
query q is expanded by qe containing all the m~erms. Furthermore, let us consider an arbitrary document 
d = (dl, +, .... d~)T in the TVS where the di s signify term weights f~r this particular doc­ument. Then, 
the similarity between d and ~e is: Z % (ti,tj) ti= q di . qi SIM(ti,ti) (11) 1 where Cq= ~ qi  tic 
q Since the constant Cq depends only on the query, it does not affect the ranking of the documents with 
respect to the query. It is to be noted that formula (11) is analogous to the similarity indicated in 
[Won 87, p. 303] for the Generalized Vector Space Model (GVSM). This means that both the method proposed 
in this paper and the GVSM are going along the same lines. Therefore, the GVSM can also be interpreted 
as a kind of query expansion method. There are, however, two significant differences between the two 
methods. First, the relationship between terms is computed in a different way, although both methods 
use co­occurrence data. We construct a similarity thesaurus as de­scribed in section 3. In the GVSM, 
concepts are derived from terms and used as a basis of the vector space in which similarities are computed. 
Secondly, the GVSM includes all the terms in the expansion and uses qe for ranking documents as shown 
in formula (11). Yet, in our approach, we expand the query only by a few carefully chosen terms ~d USISqexpau~, 
 Similarly, the latent semantic indexing (LSI) approach pee 90] tends to find which terms are used to 
describe a document or a query. In LSI, a set of terms used to index documents is replaced by a relatively 
small number of or­thogonal factors. These factors represent extracted common meaning components of many 
different terms or documents. However, the choice of the number of factors is critical to LSI. If the 
number of factors needs to be changed, the la­tent semantic indexing analysis, a time consuming process, 
has to be reperformed. Although the choice of r in formula (9) is also critical, it can easily be changed 
to satisfy the user information need. 164 5. Experiments and their Results For our experiments, we used 
the three standard test collec­tions shown in Table 1. We compared the retrieval effec­tiveness of our 
automatic query expansion approach with the standard retrieval method using original queries only. For 
the collections CACM and MED, after extracting all the words from the collections and removing stop words, 
we used stemmed terms to index both queries and docu­ments. For NPL, we used the existing indexed form. 
Table 1 indicates the number of documents, the number of queries with relevance information, the number 
of terms, the average number of terms per document and query, the number of terms in queries and the 
average number of rele­vant documents per query. It can be seen that the MED col­lection is rather small 
and the NPL collection is quite siz­ able as a test collection. The CACM collection is of medium size. 
Collection MED CACM NPL documents 1033 3204 11429 queries 30 52 93 terms 8663 7121 7492 avg. doe length 
54.69 24.26 19.96 avg. query length 10.45 11.5 7.15 terms in queries 271 356 337 avg. rele. does 23.2 
L15.31 22.41 Table 1: Collections used for experiments Term weights in both documents and queries are 
deter­mined according to the normalized tf * idf weighting scheme [Sal 88], see also formula (l). For 
the similarity calcula­tions, the scalar vector product was used. In addition, the construction method 
described in section 3 was used to de­termine the similarities between all the terms in the collec­tions, 
i.e., to build up the similarity thesauri. Then, for each query, we rank the terms of the collection 
in decreasing order according to formula (7). Note that this can be achieved very efficiently as the 
pre-computed similar­ities from the similarity thesauri can be used. The top ranked terms are chosen 
to expand or modify the query ac­cording to formulae (8), (9), and (10). The results were evaluated by 
applying the average pre­cision of a set of queries at three representative recall points, namely 0.25, 
0.50, and 0.75. In addition, the use­fulness ljl?re 91] was measured to compare the automatic query expansion 
method with the standard method using original queries. Table 2 shows the retrieval quality difference 
between the original queries and the expanded queries. The tigures indicate that our automatic query 
expansion method yields a considerable improvement in the ret]ieval effectiveness in both automatically 
indexed document collections, i.e., MED and CACM. In addition, there is also an improve­ment with the 
collection indexed by carefully chosen terms, i.e., NPL. go. Yet, when using relevance feedback information, 
the number of additional terms should be as few as 20 &#38;Iar 92]. Why is the number of terms to be 
added smaller when rele­vance feedback is used as opposed to our automatic expan­sion method? With relevance 
feedback, only the terms con­tained in the retrieved documents are considered. The num­ber of these terms 
is much smaller than if one were to con­sider all the terms of the collection. Furthermore, some of them 
may be dissimilar to the query concept. In contrast to relevance feedback, there is a much larger number 
of terms that are considered to be additional search terms. The same evaluation was done by applying 
the useful­ness measure instead of the adjusted precision. The usefid­ness measure @3e 91] is a relative 
measure which compares a retrieval method A and a retrieval method B. It is based on relative relevance 
judgments as opposed to absolute rele­vant and non-relevant assessment. The measure has the added ability 
of determining an error probability that indi­cates how stable the result is. With the measure, the use­fulness 
u(A,B) indicates how often, on the average, method B performs better than method A. The adjusted usefulness 
u*(A,B) indicates how much method B performs better than method A. The error probability Pk expresses 
the reliabil­ity of the usefulness value. The values of u(A,B) and u*(A,B) are between -1.0 and 1.0, 
Pk is between 0.0 and 1.0. The higher u(A,B) and u*(A,B) are, the more effective method B is compared 
to method A. The smaller Pk, the more reliable the usefulness value. In our experiments, the query expansion 
method is B, the method using the original -not expanded -queries is A. We compared the retrieval effectiveness 
of the two methods using the top 20 documents ranked by the two methods for each query. In the evaluation, 
the preferences derived from the top 20 documents ranked by both methods are used. I IIII Collection 
CACM NPL Number of addi­ 80 100 800tional terms u(AJ3) 0.7328 0.5698 0.7478 U*(A B) 0.0597 0.0438 0.0925 
Pk 0.0004 I 0.0006 I 0.0000 Table 3: Usefulness of expanded queries with respect to original queries 
 The usefulness of the automatic query expansion method with respect to the method using original queries 
is shown in Table 3. The results confirm that the query expansion method performs consistently better 
than the method using original queries in the three collections. The error probabil­ity values are quite 
small, 0.0 or almost 0.0. This is an in­dication that the usefulness values here are reliable. Since 
users of an Ill system are normally interested in the top ranked documents, the information needs of 
the users are much better satisfied by using the expanded queries than the original queries. Collection 
CACM I NPL II I avg. precision of 0.5446 0.2718 0.1818 original queries Number of addi­ tional terms 
80 100 800 avg. precision of expanded queries 0.6443 0.3339 0.2349 Improvement + 18.31 ~0 + 22.85 yO 
+ 29.21 ~0 Table 2: Improvement using expanded queries It seems that the improvement increases with 
the size of the collection. In addition, the improvement increases with the number of additional search 
terms that expand the origi­nal query as long as the collection is large enough. Obviously, the large 
collection knowledge than the small ones. quality of the similarity thesaurus collection is better than 
the quality ing to the small collections. This our query expansion method using the similarity thesaurus 
yields higher performance improvement in the large collec­tion than in the two small ones. ..-m CAc?.1 
Ml, 20 . 10 t11c02C031YJ4C0 W60071X3W9CM 10XJ Number of additkmti terms Fig. 2: Improvement using expanded 
queries with various numbers of additional terms In Fig. 2, we show how the number of additional terms 
affects the retrieval effectiveness. It can be seen easily that the improvement by expanded queries increases 
when the number of additional terms increases. When the number of additional terms is between 100 and 
200, the improvement of the retrieval effectiveness remains constant in the small collections MED and 
CACM. Once the number of addi­tional terms gets to be larger than 200, the improvement decreases in the 
small collections, but continues to increase in the relatively large collection NPL. This could be ex­plained 
by the fact that more search terms are needed to dis­tinguish relevant documents from non-relevant documents 
in large collections. The results shown in Fig. 2 indicate that expanding a query by roughly 100 top 
terms seems to be the safe way to contains more domain As a consequence, the created from the large of 
the thesauri belong­seems to be the reason VAB) . *(Q) 0,8 ­ 0,6 0,4 :! =s 0,2 t .-- ~ 0,0 0 Iml 
m 300m Number of additional terms CACM NPL 1 +3) @.m --+ ..(&#38;B) --, --­ .(~~) 0,6 :F 0.4 08 02 
... --------- d@­ 0,0 m 0 103 m 3W NumbeP of additional terms Nnmherof additionalterms Fig. 3: Usefulness 
and the number of additional terms In Fig. 3, we study how the number of expanded terms some experiments 
to see how a threshold affects the re­affects the usefulness of the query expansion method with trieval 
effectiveness when using our query expansion model. respect to the method using original queries. The 
results Only those terms that have a high similarity value to query shown in Fig. 3 are consistent to 
the ones when the recall-terms are considered as candidates of additional search terms, precision measure 
is used. That is, the number of terms to They are ranked according to the following adjusted Sirnqt be 
added should be determined according to the number of function: documents of the collections in order 
to produce a high use­Simqt(q,t) = z qi slM(ti,t) (12) fulness. The number of expanded terms suggested 
in Fig. 3 (~=q) and(SIM(t,,t)>tieshold) is around 50 for the collections MED and CACM, and As is done 
in many traditional query expansion methods,around 350 for the collection NPL. we also added the top 
ranked terms to the original query ac­ cording to formulae (8), (9), and (10). 6. Why did many of the 
early methods fail? Fig. 4 shows the relationships among the retrieval effec-As already mentioned, the 
usual query expansion methods tiveness (improvement of average precision of expanded tend to add a term 
when it is strongly related to one of the queries over original queries), the threshold (0.0 -1.0) of 
query terms. In other words, during the expansion process similarities between terms and the number of 
additional those term-term pairs with a similarity value less than a terms. The results indicate that 
retrieval effectiveness de­threshold value are not taken into account. As we also creases when the threshold 
value increases. When the mentioned, most of these methods failed to improve the re-threshold value is 
greater than 0.6 in MED, 0.8 in CACM trieval effectiveness. With the same idea, we carried out and 0.3 
in NPL, the expanded queries perform even less ef­fective than the original queries. However, with the 
term classification methods mentioned in section 2, high threshold value should be used. Otherwise too 
many terms are classified in the same class and terms are difficult to distinguish from each other. As 
we have seen, however, when we choose a high threshold value, the re­trieval effectiveness decreases. 
This is one reason why many of the early automatic query expansion methods failed. Fig. 5 shows a feature 
of the similarity thesauri of the three collections. One can see that the distribution of the number 
of term-term pairs with a similarity value greater than O is the h distribution. Most term-term pairs 
have a quite small similarity value and few term-term pairs have a high one. When the threshold value 
of similarities gets larger, the number of those term-term pairs with a similar­ity greater than the 
threshold value decreases rapidly. The number of candidates of additional search terms becomes quite 
small. As a result, the top ranked terms maybe dis­similar to query concepts. This is another reason 
why early automatic query expansion methods failed to improve the retrieval effectiveness. 7. Conclusion 
In this paper, we present a query expansion model based on the domain lmowledge contained in an automatically 
con­structed similarity thesaurus. This model is primarily con­cerned with the two important problems 
of query expan­sion, namely with the selection and with the weighting of additional search terms. The 
term selection relies on the overall similarity between the query concept and terms of the collection 
rather than on the similarity between a query term and the terms of the collection. The experiments car­ried 
out on the three test collections show that consistent improvement in the retrieval effectiveness can 
be expected. The main results of the study are summarized as fol­lows: CACM NPL Fig. 4: Effect of the 
similarity threshold. 167 Mm 3 2 1 i ,C 6000fI % Soq 4000I 3000I 2000J 1000 0,0 0,2 0,4 0,6 Similarity 
Fig. 1) The automatic query expansion method based on statisti­cal co-occurrence data can result in significant 
improve­ment in the retrieval effectiveness when measured using both recall-precision and usefulness. 
Consistent perfor­mance improvement was achieved in both automatically indexed test collections and a 
test collection indexed by carefully chosen terms. 2) Since the quality of the similtity thesaurns created 
for a large collection seems to be better than the one for a small collection, the retrieval effectiveness 
seems to in­crease with the size of the collection. Likewise, the number of additional terms per query 
seems to increase with the size of the collection too. 3) The methods that rely on relevance feedback 
information only select among the terms of a few retrieved docu­ments. In contrast, the method described 
selects addi­tional search terms out of the entire term set. Therefore, the number of additional search 
terms is usually larger. 4) We point out two reasons why early attempts in auto­matic query expansion 
failed to improve the retriewd ef­ 1,0 Similarity CACM 0,0 02 0,4 0,6 0,8 1,0 Sidlarh 0,8 1,0 5: Feature 
of similarity thesauri festiveness. The results shown in the paper primarily indicate how dangerous threshold 
values are. A commercial database with millions of documents con­ tains a great deal of terms. The constriction 
of a similarity thesaurus could therefore be computationally expensive. Hence, the construction algorithm 
as well as the storing and the accessing of such a similarity thesaurus has to be stud­ ied carefully. 
With our test collections, all the terms of the collection were taken into account. Since frequent terms 
tend to discrimin ate poorly between relevant and non-rele­ vant documents [Pea 91, Sal 75], they could 
be omitted from the similarity thesaurus of a large collection. However, which terms to ignore has to 
be studied carefully. The retrieval effectiveness could be even improved by omit­ ting some of the poor 
discriminators. The improvement of the retrieval effectiveness by using our approach is around 20-30%. 
This is less than the one reported when using relevance feedback information. However, the relevance 
feedback methods depend heavily on the kind and quality of the user relevance information. In addition, 
this information is hard to get. techniques databases. References [Cro 90] [Cro 92] pee 90] [Ekm 92] 
~re 91] [Gre 92] &#38;Ian 92] (l%r 92] ~S 69] [Min 72] The advantage of our method is that it is fully 
auto­matic. Furthermore, our method can be used in the first run in an IR system when no relevance information 
is yet available. In case relevance information is available, feed­back techniques could be introduced 
to retrieve even more relevant documents. In our future research we will concentrate on a sensible combination 
of our novel query expansion method and rele­vance feedback mechanisms as well as on applying these on 
commercial -and therefore voluminous - Crouch, C. J., An approach to the automatic construction of global 
thesauri, Information Processing &#38; Management, 26(5): 629-40, 1990. Crouch, C.J., Yong, B., Experiments 
in auto­matic statistical thesaurus construction, SIGIR 92, 15th Int. ACMISIGIR Conf. on R&#38;D in Information 
Retrieval, Copenhagen, Denmark, 77-87, June 1992. Deerwester, S., Dumais, S.T., Fumas, Landauer, T.K., 
Harshman R., Indexing tent semantic analysis, Y. of the ASIS, 391407, 1990. Ekmekcioglu, F. C., Robertson, 
A.M., G.W., by la­41(6): Willett, P., Effectiveness of query expansion in ranked­output document retrieval 
systems, .1. of Infor­mation Science, 18(2): 139-47, 1992. Frei, H. P., Sch3uble, P., Determining the 
ef­ fectiveness of retrieval algorithms, Information Processing &#38; Management, 27(2-3): 153-164, 1991. 
 Grefenstette, G., Use of syntactic context to produce term association lists for retrieval, SIGIR 92, 
15th Int. ACMISIGIR Conf. on R&#38;D in Information Retrieval, Copenhagen, Denmark, 89-97, June 1992. 
Hancock-Beaulieu, M., Query expansion: ad­vances in research in on-line catalogues, J. of Information 
Science, 18(2): 99-103, 1992. Harman, D., Relevance feedback revisited, SIGIR 92, 15th Int. ACMISIGIR 
Conf. on R&#38;D in Information Retrieval, Copenhagen, Denmark, 1-10, June 1992. Lesk, M.E., Word-word 
association in document retrieval systems, American Documentation, 20(1): 27-38, 1969. Minker, J., Wilson, 
G. A., Zimmerman, B. H., An evaluation of query expansion by the addi­tion of clustered terms for a document 
retrieval system, Information Storage and Retrieval, 8(6): 329-48, 1972.  Pea 91] [Qiu 92] mug 92] 
[Sal 71] [Sal 75] [Sal 80] [Sal 88] [Sal 90] [Sch 92] [Sine 83] [Spa 71] [Spa 91] wad 88] won 87] Peat, 
H. J., Willett, P., The limitations of term co-occurrence data for query expansion in docu­ment retrieval 
systems, Y. of the ASZS, 42(5): 378-83, 1991. Qiu, Y., ISIR: an integrated system for informa­tion retrieval, 
Proc. 14th IR Colloquium, British Computer Society, Lancaster, 1992. Ruge, G., Experiments on linguistically-based 
term associations, Information Processing &#38; Management, 28(3): 317-32,1992. Salton, G., Experiments 
in automatic thesaurus construction for information retrieval, Informa­tion Processing 71, 1:115-123, 
1971. Salton, G., Yang, C. S., Yu, C.T., A theory of term importance in automatic text analysis, J. of 
the ASZS, 26(l): 33-44, 1975. Salton, G., Automatic term class construction using relevance-a summary 
of work in auto­matic pseudoclassification, Information Process­ing &#38; Management, 16(1): 1-15, 1980. 
Salton, G., Buckly, C., Term weighting ap­proaches in automatic text retrieval, Information Processing 
&#38; Management, 24(5): 513-523, 1988. Salton, G., Buckley, C.: Improving RetrievaJ Performance by Relevance 
Feedback. 1. of the ASIS, 41(4): 288-297, 1990. Schauble, P., Knaus, D., The various roles of information 
structures, 16. Jahrestagung der Gesellschaft fur Klassifikation, Dortmund, 1992. Smeaton, A.F., van 
Rijsbergen, C.J., The re­trieval effects of query expansion on a feedback document retrieval system, 
The Computer Journal, 26(3): 239-46,1983. Sparck-Jones, K., Barber, E. B., What makes an automatic keyword 
classification effective? J. of the ASIS, 18: 166-175, 1971. Sparck-Jones, K., Notes and references on 
early classification work. SZGZR Forum, 25(l): 10­ 17, 1991. Wade, S.J., Willett, P., INSTRUCT: a teach­ 
 ing package for experimental methods in infor­mation retrieval. III. Browsing, clustering and query 
expansion, Program, 22(l): 44-61,1988. Wong, S.K.M., Ziarko, W., Raghavan, V. V., Wong, P. C.N., On modeling 
of information re­trieval concepts in vector spaces, ACM TODS, 12(2): 299-321, 1987.  
			