
 FUZZY CLUSTERING IMPROVES CONVERGENCE OF THE BACKPROPAGATION ALGORITHM Adel M. Abunawass, Ph.D. Opinderj 
it Singh Bhella Min Ding Weiqun Li Computer Science Department Western Illinois University Macomb, Illinois 
61455 E-mail: Adel Abunawass@WIU.Edu Keywords: Backpropagation, Fuz~ Clustering, Fuz~ Logic, Neural 
Networks, Artificial Intelligence. ABSTRACT Fuzzy logic deals with problems which are imprecise and 
uncertain. Fuzzy clustering classifies the class of the elements of the set based on the similarity fuzzy 
concept. The backpropagation algorithm is one of the more popular neural network models. Combining the 
fuzzy clustering with weight adjustments in the backpropagation improves convergence speed and rate of 
convergence. Since units in a network do not contribute equally to the overall error of the network, 
then their connections should be adjusted accordingly. So, it follows that a method that adjusts the 
connections between the units based on their contributions to the overall error of the network should 
yield an improved performance, This paper describes such a method by the use of fuz~ clustering of the 
error signals of the backpropagation algorithm in order to enhance the algorithm's performance. Fuzzy 
logic provides a number of clustering techniques that could be used. The paper describes a technique, 
known as netting, based on similarity relationships where all elements that fall wRhin a predetermined 
neighborhood would make up a cluster. The technique works via a threshold (here known as a clustering 
level or 2-cuO value, often between 0 and 1, that dictates a degree of similarity between error signals 
in order to be in a particular cluster. The connections associated with the error signals in a cluster 
are adjusted accordingly. This paper describes the hczy clustering rmission to make digital/hard copy 
of all or part of this work lbr personal or tssroom use is granted without fee provided that copies are 
not made or tributed for profi~ or commercial advantage, the copyright notice, the title of the ~lication 
and its date appear, and notice is given that copying is by permission of 'M, Inc. To copy otherwise, 
to republish, to post on servers or to redistribute to s, requires prior specific permission and/or a 
lhe. <D 1998 ACM 0-89791-969-6/98/0002 3.50 methodology and its incorporation into the backpropagation 
algorithm as well as results of simulations conducted. I BACKGROUND Fuzzy logic is unlike crisp logic 
which has just two values [0,1 ] (true, false). Fuzzy logic includes not only the values 0 and 1 but 
also the values in between. Using fuzzy logic, more real world problems are able to be modeled with success. 
Lotfi Zadeh introduced the formal idea of fuzzy logic and fuzzy set theory in ! 965 [4,6]. Each element 
of the fuzzy set has a value which is called its degree of membership [1]. Classifying elements of a 
set according to the equivalence relationship is sometimes referred to as a similarity relation. In a 
similarity relation, the members of the set may be viewed as elements in a crisp set whose members are, 
to some de~ee, 'similar' to each other. When the degree is 1, the group is an equivalence class. It can 
be shown that for any similarity relation, R, each 1 -cut on R is a crisp equivalence relation for any 
value of (0, ! ], [2] R= UAJR 2~O,II Each of these relations represents the presence of similarity between 
the elements in that partition or cluster. If we let tc(XR) be a cluster corresponding to the equivalence 
relation aR , then the elements X and Y belong to the same cluster if and only if R(X, Y) >_ I. So each 
similarity relation is associated with a set 1-1(R)= {x(~R)I2 e(O,I]} of clusters of X.[2,5] The procedure 
of fuzzy clustering used here can simply be summarized as follows: (1) establish the similarity relationship; 
 (2) partition the set according to similarity relation at a level ofa A -cut; &#38; (3) based on the 
relation similarity, group the elements into two or more clusters using a variation on the netting method. 
(note: the netting method used here, translated from a Chinese text [10], is not all that different from 
other methods found in the fuzzy logic literature) The l-cut is referred to as the clustering level, 
  clevel. The reader should note that there are several methods to cluster a set of elements.[6] The 
backpropagation algorithm models neural networks processing [7]. The algorithm has been applied to a 
number of domains with great success [ 1,2,3,5,7]. For more details about the backpropagation algorithm 
the reader may refer to the reference section [7]. The algorithm has been shown to be reliable and robust. 
In the backpropagation algorithm, learning is accomplished by adjusting the weights of connections between 
the units (also known as neurons, processing elements, etc.). The adjustment of the weights is based 
on an error signal, 8, generated for each non-input unit. The purpose of the adjustment is to minimize 
the error produce by the units and thus by the network as a whole as follows: for output units j 8j =~j 
-O~V'(Net:) for hidden units i 4 = (Z W~j6j)f'(Netj) J Where,/ is the error signal for unit j; t i 
is the teaching for unit j; O: is the output of unit j; Netjis the input to unit j; f'(Net,) iS the partial 
derivative of sigmoid function f(Net/); ~, iS the weight of the connection between units i &#38; j; aw, 
is the change in the weight between units i &#38; j; r/ is the learning rate (Irate). The weights are 
then adjusted using the calculated error signals as shown in the following two equations:  aW~,,., 
=~8,0, --w,, + .... The weight adjustment does not take into account many important elements that may 
influence the adjustment [9] (i.e. Fan-in of connections to a unit, units that may carry larger error 
than others, etc.) Some researchers have applied different techniques to reduce the error of the network 
in order to improve learning and/or convergence [I,2,6,9]. For example, Samad suggested a heuristically 
based method to reduce the error [9]. In Samad's method the error of a hidden unit j, receiving unit, 
is divided by its degree of saturation, fan-in of unit j, vehen the weights are adjusted. A W!/,,.,,= 
,10,(~) where N, is the fan-in of unit j. In this paper we describe two methods to group different error 
signals in different clusters based on the similarity relation of fuzzy logic. The first method produces 
a cluster of similar high value error signals and the other is of similar low value error signals, we 
call this method the "2-cluster" method [1]. The clustering is based on a clustering level, t-cut. The 
paper also describes an extension of the 2-cluster method to an "n-cluster" method. 2 2-CLUSTER METHOD 
The fuzzy clustering described here is based upon the equivalent similarity relationship on the set to 
be classified. There are several methods to compute the similarity relationship on the set, one such 
method is the maximum-minimum method [8]. In this method, for any two elements of the set, use the ratio 
of the minimum value and the maximum value of the two elements. This is known as the similarity coefficient 
of the two elements. A matrix composed of the similarity coefficients is generated. The matrix is a similarity 
relationship on the elements of the set [8,10]. Next we use the similarity relationship matrix to construct 
a fuzzy equivalence relationship, also known as a clustering graph. The clustering level in the graph 
is determined by a threshold value, A, which determines the degree of fine clustering. Using the A -cut 
and the clustering principle, based here on the netting method [10], the elements in the set are grouped 
into clusters. The clustering principle states that the clustering of any two elements in a set, a graph, 
is the same as clustering the set based on A if and only if in the clustering graph there is a path, 
edge, whose weight is not less than the value A. For example; suppose we have five non-input units with 
the decreasing absolute value of the error signals vector of { ~ = 0.9, 8/= 0.8, dk = 0.5, = 0.2, d~ 
= 0. ! }. We are going to classify the nodes into two clusters so that one has a larger error value than 
the other. First we generate a matrix using the maximum-minimum method as shown in Figure 1 (See Appendix 
A). Second, we apply a user supplied A-cut, also called global clustering level or clevel G, here 0.85 
and a base value of I. All elements that are > 2 are set to 1 278 and the rest are set to 0. Finally, 
the first cluster is identified, as shown below, and the rest of the elements fit into the second cluster 
(See Figure 2). Next, we calculate a multiplier, /3, for all the elements in the high cluster, A (note: 
many variations exist; for example one may allow for a unique ~ for each unitj in cluster A). The multiplier 
is then used to adjust the weights as shown below: (the rest of the weights are adjusted according to 
the original backpropagation algorithm)  r jl jl where units i reside in cluster A and units j are 
the sending units connected to units i. ~Wij , =(l+ P)qS,0, where unitsj reside in the cluster A. The 
clusters and the multiplier need not be calculated every epoch. For example, one may choose to calculate 
them on a regular intervals based on a fixed number of epochs, adaptively based on the change in the 
error of the network and/or other parameters. Another variation on the method multiplies the units in 
the low cluster, B, by (I-fl). 3 N-CLUSTER METHOD We also extended the 2-cluster method to an n-cluster. 
Once we identify the first cluster, then we build the rest of the clusters. This is done by successively 
applying the 2-cluster method to a shrinking matrix of values with an updated A-cut to equal its original 
value divided by the number of the cluster we are building, and with an updated base value to equal the 
error value that did not fit in the current cluster. In the example below the second cluster (B) has 
a A -cut of 0.425 and a base of 0.5 (see Figure 3). Next we calculate the multiplier, fl, for each cluster 
using the formula below. (note: the number of clusters found in this example is 3) fld..,,e,: =(Number_ 
Of_ Clusters_ Found - Cluster Number + I)+ ( clevelc, *(~i~c,,.,,,,,61 /Number Of_Clusters Found)) Once 
the multipliers are calculated the weights are adjust as follows: W<,,,-- ~,, ...... ,16,,,~ ...... O, 
where unitsj reside in Cluster ?. 4 RESULTS Several network architecture and parameters were used in 
the simulation performed (see Table t in Appendix B). For each simulation 1000 runs were performed and 
the results averaged. The average learning cycles, epochs, of successful simulations and the convergence 
rate (percentage of success) are listed in Table 2. As one can see a noticeable improvement in the performance 
of the networks when fuzzy clustering is applied. The use of a random clustering scheme shows the improvement 
is attributed to the fuzzy clustering methods used and not to the clustering phenomenon (see last row 
of Table 2). Additional simulations were run to document the effect of different clustering levels, clevel, 
on the performance of the various networks. The XOR network was ran using the clevel values of 0.5, 0.75 
and 0.85 respectively. The results are reported in Table 3. The results provide another successful approach 
of the application of fuzzy theory to artificial neural networks algorithms to improve learning performance. 
The significant improvement in the learning speed is clearly attributed to the fuzzy clustering methods 
described in this paper and not to random chance. Future work should consider other clustering approaches 
or approximation techniques of the methods described in this paper in order to further reduce the computing 
cost.  5 REFERENCES 1. Abunawass, A., &#38; Ding, M. (1996). An approach to fuzzy clustering of error 
signals in the backpropagation model. Proceedings of the International Association of Management Conference 
Computer Science Track. 49- 52. 2. Abunawass, A. M., &#38; Owen, C. B. (1993). A stochastic approach 
to the backpropagation using simulated annealing. Proceedings of the ICNN '93. IEEE International Conference 
on Neural Networks, 124-128. 3. Indurkya, Gopal, Rajurhar, K.P. &#38; Reddy, Ravi Thouti (1995). Artificial 
Neural Network Approach in Modeling of EDM and Wire-EDM Processes. Design and Implement manufacturing 
System, Prentice Hall: Englewood Cliffs, pp. 161-186. 4. Klir, George J. &#38; Yuan, Bo (1995). Fuzzysets&#38;fuz=y 
logic: theory and applications, Prentice Hall: Boston. 5. Kosk, Bart ( i 992). Neural Networks and Fuzzy 
Systems. Prentice-Hall lnc: Englewood Cliffs. 6. Loos ,James R. (1994) Fuzzy Logic and Neural Networks. 
lntech, pp. 26-29. 7. McClellan&#38; James L.&#38; Rumelhart, David E. (1988). Explorations in Parallel 
Distributed Processing. The MIT Press: Boston.  279 8. Ross, Timothy (1995), Fuzzy Logic with Engineering 
 I 0. Wang, Peizhuang (I 981). Fuz~ Sets Theory and Its Applications. McGraw Hill: New York. Application. 
Scientific Tech Press: Shanghai, 9. Samad, Tareq (1990). Backpropagation improvements P.R.China. based 
on heuristic arguments, International Conference on Neural Networks '90. 6 APPENDIX A (FIGURES) 4 4 
4 4 a~ -0.9 0.8 0.5 0.2 0.1 49 49 49 0.9 0.9 0,8 0.8 0.5 0.2 0.1 'l 0.88 0.55 422 0.11] 49 0.8 0.8 0.8 
0.8 0.88 l 0.62 0.25 0,12[ 0.5 0.5 0.5 0.2 41 / &#38; = 0.55 0.62 I 0.40 0.20[ 0.9 0.8 0.5 0.5 0.5 [0.22 
0.25 0.40 I 0.~0] 0.2 0.2 0.2 0.2 41 LO.Jl 0.12 0.20 0.5 09 0.8 0.5 0.2 0.2 41 41 41 0.1 &#38;l 49 
0.8 &#38;5 &#38;2 &#38;l  Figure 1 b J First cluster {high cluster A) for clevelc of O. 85 J I [ and 
a base of 1. {0. 9, O. 8} I 1 1~0 0 0 o o [ Since this a 2-cluster method, the restfit into the second 
' I Ro~ 0 0il 0 0 I cluster low cluster (t3) = {0.5, 0.2, O. 1} 0 0!0 1 0 / 0 0)0 0 1 t  Figure 2 
First (high) cluster(A) for Another cluster (B) for clevel8 clevel G of O. 85 and base of 1. of O. 425 
and base of O. 5 {0.9, 0.8}, PA:3.49 {0.5}, fiB=2.14 -1 0.88 0.55 0.22 0.11 r o o o 0 0 0  0.88 1 0.62 
0.25 0.12 RoL.~s5 i I R~ 055 0,62 1 0.40 0.20 0 1 0 0 /~085/2)=0 425 i'loo ' ~085/3)=0 283 0,22 0.25 
0.40 ! 0,50 0 0 0 1 0 0 1 I I I I I 0.11 0.12 0.20 0.5 ! 0 0 0 0 I 0 1 1 I 1. IAnother cluster (last 
cluster C) for clevelc of 0.283 and base of .,'2 {0.2, O. 1}, Pc=1.09 I ~ Figure 3 280  7 APPENDIX B 
(TABLES) Network Irate ~ nepochs ecrit clevel Input/Output Patterns XOR 0.5 0.9 500 0.05 0.75 XOR 5-5-5 
0.5 0.9 1000 0.05 0.75 15 I/O patterns generated randomly w/o replacement 10-10-10-0.5 0.9 500 0.05 0.75 
30 I/O patterns generated randomly w/o replacement 3-2-1 0.5 0.9 5000 0.05 0.75 9 even parity = I Table 
I Network Average Learning epochs Convergence Rate Original Bp/XOR 173 84% 2-Cluster/XOR i ! 9 86% N-Cluster/XOR 
103 89% Original Bp/5-5-5 434 99% 2-Cluster/5-5-5 366 99% N-Cluster/5-5-5 168 99% Original Bp/Parity 
3-2-! 2903 67% 2-Cluster/Parity 3-2- I 2401 73% N-Cluster/Parity 3-2-1 1616 80% Original Bp/l 0- I 0-10 
167 100% 2-Cluster/10-10-10 118 100% N-Cluster/10-10-10 68 100% Random/10- ! 0- ! 0 231 79% Table 2 
Network/clevel Average Learning epochs Convergence Rate 2-Cluster XOR/0.5 128 8 !% N-Cluster XOR/0.5 
I 12 83% 2-Cluster XOR/0.75 119 86% N-Cluster XOR/0.75 103 89% 2-Cluster XOR/0.85 125 85% N-Cluster 
XOR/0.85 106 89% Table 3 BIOGRAPHY: Dr. Adel M. Abunawass holds a Ph.D. in Computer Science from North 
Dakota State University earned in 1990. He has published over 40 research papers and has collaborated 
on a number of grants. His research interests include: neural networks; genetic algorithms; artificial 
life; software engineering; and computer science education. Dr. Abunawass is an associate Professor of 
Computer Science at Western Illinois University &#38; the Chair of the Illinois State Academy of Science's 
Computer Science Division. He can be reached at ,4~bl_.4bunawclssa!WlU.Edu. Mr. Opinderjit Bhella is 
an associate at Anderson Consulting, Chicago, IL. His research interests include software engineering 
&#38; fuzzy systems. He can be reached at opinderjit.s.bhella~r..ac.com. Mr. Min Ding is a senior consultant 
at FLEXI Consulting in Hammond, NJ, Mr. Ding's research interests include fuzzy control systems. He can 
be reached at MinDing a FLEXI.COM. Ms. Weiqun Li is a software engineer at NetRight Technologies, Inc. 
in Chicago, IL. Her research interests include OO Software Design. Ms. Li can be reached at wlia NETRIGHT.COM. 
Mr. Bhella, Mr. Ding &#38; Ms. Li have earned MS degrees in Computer Science from Western Illinois University 
and are members of the Computing Honor Society Upsilon Pi Epsilon. 281 ,~-,: 存-.存%存存 :~ ~ ~ ~--" - 
  
			