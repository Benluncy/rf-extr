
 An Optimal Parallel Dictionary * (Preliminary version) Martin Dietzfelbinger Friedhelm Meyer auf der 
Heide ~chbereich 17 Mathematik -Informatik Universit~t-GH Paderborn, D-4790 Paderborn, Fed. Rep. of 
Germany 1. INTRODUCTION A dictionary is one of the most basic dynamic data structures. It supports the 
instructions insert, delete, and lookup. In this paper we describe and analyze a dictionary that can 
be implemented on a parallel computer with p processors. This data structure is optimal, i. e., it is 
capable of processing n instruc-tions in optimal time O(n/p) using space propor- tional to the current 
number of elements in the dic- tionary. Its construction is inspired by the sequential dictionary due 
to Dietzfelbinger et al. from [DKM], which in turn is based on the perfect hashing scheme due to Fredman 
et al. from [FKS]. Supported in part by DFG Grants ME 872/1-1 and WE 1066/1-2 Work done while the authors 
were affiliated with the University of Dortmund, F. R. G. Permission to copy without fee all or part 
of this material is granted provided that the copies are not made or distributed for direct commercial 
advantage, the ACM copyright notice and the title of the publication and its date appear, and notice 
is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, 
or to republish, requires a fee and / or specific permission. &#38;#169; 1989 ACM 0-89791-323-X/89/0006/0360 
$1.50 In more detail, our dictionary has the following prop- erties: 1.) It is implemented on a concurrent 
read/concur- rent write PRAM with priority write conflict resolution, using some number p of processors. 
2.) Each processor is connected to a user that feeds it with an instruction from the above types whenever 
the processor asks for a new instruc- tion. The elements (or keys) that are to be stored in the dictionary 
are drawn from a finite universe U (as usual when hashing is applied). 3.) The PRAM is randomized (i. 
e., it uses a ran- dom number generator), and we apply the uni- form cost criterion. 4.) For each fixed 
e > 0, the dictionary can be im- plemented such that n > pl+~ arbitrarily cho- sen instructions, n/p 
per processor, can be exe- cuted in expected time O(n/p) (which clearly is optimal). (The restriction 
n > pl+~ comes up very naturally from the type of hash functions to be used, compare [DKM].) 5.) If n 
> pl+e elements are in the dictionary then the storage currently used does not exceed O(n) (which is 
also optimal). 6.) The worst case response time for a lookup is O(1) (i. e., optimal). 7.) There is no 
availability delay, i. e., a lookup for an element z yields the correct result even if in- sert or delete 
instructions for x have been read directly before the lookup instruction (perhaps by different processors). 
8.) For each t _> pC, the expected worst case throughput of the processors, worst case taken over all 
processors, is ft(t), i. e., it is expected that each processor has read ft(t) of its instruc- tions 
after t steps. In order to achieve all the above properties we have to handle several additional problems 
in comparison to the sequential case from [DKM]. Some of them are the following. It turns out that we 
cannot simply let every processor execute tim instructions it reads. Thus the distribution of inputs 
has to be or- ganized. - We need stronger probability bounds for the analysis than in the sequential 
case. Typically, we have to bound the expectation of the maxi- mum of the runtimes of the processors. 
For ex- ample, in contrast to the sequential case, linear hash functions do not longer suffice. - We 
need extra considerations in order to avoid availability delays.  Our result provides the first optimal 
parallel dictio- nary. The previously most efficient one was the par- allel implementation of 2-3-trees 
due to Paul et al. from [PVW], which clearly is by a factor O(log(n)) slower than that one described 
in this paper, as it is already in the sequential case. Until now, parallel hashing has been applied 
mainly for emulating a shared memory on a network of pro- cessors, compare [U], [KU], [R], [KRS]. Here 
hashing is used to distribute the shared memory cells among the private memories of the processors in 
the net- work. The first three papers apply polynomials of degree about log p as hash functions. They 
show that p shared memory accesses, one per processor, can be executed in expected time O(log p) (including 
the routing in the network). The last paper applies polynomials of constant degree as hash functions. 
It is shown that pl+C shared memory accesses, p~ per processor, can be executed in optimal expected time 
O(p~). The expected time for a single one of these memory accesses is also O(pe). It is still open whether 
a dynamic structure with sim- ilar features as described in this paper can be de- signed for a network, 
i.e., without using a shared memory. 2. THE 2-LEVEL HASHING SCHEME We start with a brief description 
of the 2-level hash- ing scheme from [FKS] and some of the modifications used in the dynamic structure 
from [DKM]. We consider the following type of hash function (see [FKS] and [DKM]). Let U denote the (finite) 
universe of elements, or keys, and let q > [U I be a prime. Assume w. I. o. g. that U C_ {0, 1 ..... 
q - 1}. Definition. For s, I _> 1 we define 7-/~ := {hala = (ao,al,...,al) e {0, 1,... ,q--I}/+1}, where 
the function ha : U --~ {1 .... , s} is defined by I ha(x):= ( (i=~oaiXi) m°dq) mods. Obviously, an h 
e 7-/t~ can be evaluated in O(l) (thus constant) time. Further, a description of h can be stored in O(l) 
(thus constant) space. Let, for fixed S C U and a randomly chosen h E 7/I the i-th bucket be Bi := {x 
e S[h(z) = i} and its size be bi := IBil, for i = 1,...,s. In what follows we assume that the function 
h is randomly chosen from 7/t, according to the uniform distribution. Probabilities and expected values 
refer to this probability space. We need the following facts, which are easy conse- quences of Thm. 3 
in [DKM]; see also [FKS] for the case l = 1. Let M be such that ISI < M. Fact 1. Ifl > l ~ > 1, then 
8 t' l' i=I r=0 i Fact 2, Ifl=l ands>8M 2,then Prob(h is 1-1 on S) > ½. Definition. h E ~ is called good 
for S if $ aOM. i=l Fact 3. If I > 3 and s > 4M then Prob(h is good for S) >_ ¼. The 2-level hashing 
scheme consists of two hash ta- bles. Let S _C U, ISI < M, be the set of elements to be stored. The primary 
hash function h E ~I4M splits S into buckets Bi of size bi = IBi[, 1 < i < 4M, as defined above. The 
header table HT is an array of length 4M; the component ItT[i] contains infor- mation about the segment 
STi of the secondary hash table in which the elements of Bi are stored. More precisely, the subtable 
$7~ is specified by the descrip- tion of a hash function hi : U --~ {1,..., 8b~} and a pointer to the 
beginning of the segment of length 8b~ in the secondary hash table which contains STi. If we choose a 
good primary hash function for S (see Fact 3) and 1-1 secondary hash functions hi (see Fact 2), we get 
a perfect hashing scheme in which lookups take constant time and which needs O(M) space. (The header 
table takes O(M) space, and since h is good for S, the secondary hash table takes 4M 4M 4 ~i=t 8b~ 
_< 8 ~i=1 b i < 240M space). Furthermore, one can easily construct h and the hi's probabilisti-cMly. 
 3~ ,.ORGANIZATION OF THE SHARED MEMORY For ease of description we assume that each proces- sor t~, 
j = I,...,p, of the PRAM consists of two virtual processors, the lupus processor I/~ and the working 
processor WPj. An input processor reads in-structions from an input queue representing its user. When 
it reads an instruction it does not try to ex- ecute it by itself. Instead it sends the instruction to 
some randomly chosen working processor that is currently idle,i. e., not busy with executing another 
instruction. The data structure built up by the insertions looks as follows. Denote by S the set of elements 
currently stored in the dictionary, ISI _~ M. This set S is stored using a modification of the hashing 
scheme described in Section 2. Here the subtables ST/ are no longer stored in one secondary hash table. 
In- stead, we use p bucket tables BTj, j = 1,...,p, of size CM/p in which STi's can be stored. (The con- 
stoat C will be determined later.) Whenever some working processor WPj wants to insert an element into 
bucket Bi, it builds up a new version of ST/for the extended bucket Bi using a new secondary hash function 
hi. This subtable is stored in Pj's "private" bucket table B~. In order to make it possible to find out 
where an ST/ is stored, a pointer to the beginning of ST/ in the respective B~ is stored in HT[i]. Furthermore, 
HT[i] contains a flag reading "locked" or "unlocked" according as some working processor is currently 
in- serting some element into Bi or not. Clearly, as in Section 2, HT[i] also contains a description 
of hi. An auxiliary hash table necessary to avoid availabil- ity delays will be described in Section 
6. 4. THE BASIC ALGORITHM (a) Input distribution It turns out in the analysis that it is not a good idea 
to have processors execute instructions from their own input queue. Rather~ we distribute tasks ran-domly 
among the working processors. The distribution of tasks works in the following way. We assume that working 
processors that currently have no job display a flag that reads "idle" (otherwise "busy"). Input processor 
I]~, repeatedly executes the following steps: 1. Choose (randomly) j e {l,...,p}.  2. Check whether 
W/~ is idle. olf not, go to 1. 3. Check if any other input processor has picked WP/in this round. If 
so, go to 1. (This check takes constant time on a PRAM with priority rule for write conflicts, if a "com- 
munication cell" is available for each working  processor.) 4. Get the next instruction from the input 
queue of I/~,, and deriver it to WPj. (b) Insertions Initially, immediately after the first-level hash 
func- tion h is chosen, all buckets Bi are empty. In the sequel, when processor WPj wants to insert an 
ele- ment z into the hash table, it performs the following steps. While executing these steps, W]~ is 
"busy". 1. Compute i := h(z). 2. Consult the i-th entry HT[:] of the header ta- ble to determine whether 
Bi is "locked", i. e., whether some other processor is currently in- serting an element into Bi.  3. 
If Bi is "locked", wait until Bi becomes "un- locked" and no processor with smaller number than j wants 
to access Bi. (Since we work with a concurrent-write PRAM, this test takes one cell for each i 6 {1,..., 
s}, and one step.)  4. Change flag in HT[i] to "locked".  5. Locate the current version of ST/via the 
pointer  in HT[i]. 6. Collect all elements stored in STi, store them in a list, and append the new 
element x. The new Bi now consists of the elements in this list. 7. Choose randomly hi 6 7-l~, r = 81Bil 
2, until an hi is found that is 1-1 on Bi. Use this hi to create a new subtable ST/for Bi in BTj.  8. 
Reset poiI~ters in HT[i] so that they point to the new S~l~. Update the description of hi in liT[i]. 
Change flag in HT[i] to "unlocked". (c) Lookups and deletions The description of the instructions "lookup 
x" and "delete x" given here is only preliminary, because we only show how to access elements that are 
al-ready stored in some ST/. In Section 6 below we will describe how these operations can be implemented 
without the adverse effect of availability delays. Lookup x : If W/~ gets this instruction from some 
IP/,, it acts as follows. While executing these steps, WPj is "busy". 1. Evaluate i := h(x). 2. Evaluate 
pos := hi(x). (A description of hi is stored in liT[i].) 3. If STi [pos] contains x, return YES to IPj,, 
oth-erwise NO. (The location of STi within some BTj,, is stored in //T[i].)  Delete x : If WPj gets 
this instruction from some I/~,, it acts as follows (and is "busy" during the time needed). Perform parts 
1 and 2 of the lookup procedure. Then: 3'. If x is found, delete x from ST/ by overwriting it with 0. 
4. Determine whether Bi is "locked". If not, exit. If so, wait until Bi becomes "un- locked". (Some processor 
has inserted some key into Bi, and has constructed a new subtable ST/ in a new location.) 5. Evaluate 
pos := hi(x). 6. If STi[pos] contains x, delete x from ST/ by overwriting it with 0. 7. If x was found 
neither in 3 I. nor in 6, return an error message to I/~,.   (d) Clean-up The purpose of a clean-up 
phase is to clear the queues of working processors waiting for a bucket to become "unlocked" (cf. (b), 
part 3). In a clean-up phase, the input processors do not work; the working processors act as follows, 
independently for each bucket Bi : 1. The processor WPj with the smallest index j among those waiting 
at Bi is determined. (This is easy using the priority write conflict resolution.) 2. WPj collects those 
x E U that are to be in- serted into Bi by the processors waiting at Bi and stores them in a list.  
3. WPj locates ST/ and appends to the list all elements already stored in ST/. The new Bi consists of 
all elements in WPj's list. 4. As 7. and 8. in (b).  (e) Restructuring the dictionary: RehashAll From 
time to time it becomes necessary to choose a new primary hash function h and to set up an en-tirely 
new hash table. This is the case if the random choices made were unsuitable (which happens with a certain 
probability) or when the table capacity M currently in use becomes too small. This restructur- ing is 
managed by the procedure RehashAll, which works as follows. 0. No external input (from the queues at 
the in- put processors) is read during execution of this procedure. 1. Collect all elements currently 
in the system (in the subtables ST~ and those waiting in queues at buckets). Each processor collects 
the ele- ments in its own bucket table B/~; elements waiting at buckets are picked up by the work- ing 
processors associated with them. 2. Store these elements in one contiguous array S of length < M in 
global memory. 3. Split the array S into p segments of length _< M/p each. Assign one of these segments 
to each input processor IPj. 4. Initialize the header table for s = 8M entries. All subtables ST/are 
empty. Choose randomly h E 7/~. 5. (c¢) The input processors perform the cycle de- scribed in (a) for 
up to C. M/p steps. Input processor IP/takes the elements to be inserted from the segment of the array 
S assigned to it in 3. When this seganent is exhausted, IPj stops.  (fl) Concurrently, the working processors 
exe- cute the instruction cycle described in (a), (b), and (c), also for C.M/p steps. While executing 
an instruction, the processor is "busy"; other- wise it is "idle", i. e., ready to accept tasks from 
any input processor. 6. If not all input processors have used up their segment of S, return to 4. 7. 
Perform a clean-up phase for C.M/p steps (see (d)). 8. If not all processors are finished with the clean- 
up, return to 4.  (f) The basic processing phase: Processlnstructions Let some constant c be fixed, 
0 < c < ½. We are now ready to describe the way the system reads T~" M/p instructions from each input 
queue, for M > pl+e. We assume that at the beginning of such a phase at most ~+c " M elements are stored 
in the dictionary. During, the phase the capacity M of the dictionary remains fixed. We try to read and 
execute T~" M/p instructions from each input queue in C. M/p steps. If this does not work, some of the 
choices made prob- abilistically (regarding choices of hash functions or decisions of input processors) 
were bad. In this case, we restructure the whole dictionary, in particular, we choose a new primary hash 
function h, and try again. In detail, ProcessInstructions works as follows. 1. Call RehashAll. 2. (cr) 
The input processors perform the cycle de- scribed in (a) for up to C, M/p steps, each one taking the 
instructions from the external input queue associated with this processor. The in- put processor stops 
when it has read T~ ; M/p instructions from its queue (from the beginning of the current call of ProcessInstructions). 
(fl) Concurrently, the working processors exe- cute the instruction cycle described in (a), (b), and 
(c), also for C. M/p steps. 3. tf not all input processors have read c  rzr' M/p instructions from 
the external input queues, then go to 1. (g) The complete algorithm (simple version) We are now in a 
position to describe the action of the whole system, with insertions, lookups, and dele- tions. In the 
case of lookups and deletions, we use the simple versions described in (c). The algorithm consists of 
a loop whose body essentially is the phase ProcessInstructions described in (f). At the end of each phase 
the elements currently stored in the dic- tionary are counted, and the capacity M of the hash table for 
the following phase is adjusted accordingly. 1. Initially, M is set to pl+e, and the header ta- ble 
is initialized (all subtables ST/are empty). Then ProcessInstructions is called once; in this special 
case, part 1 (RehashAll) is omit- ted and the input processors read M/p = pt instructions from their 
respective input queues. Afterwards, the following loop is performed until the input queues have been 
read completely. 2. The working processors determine the current number no of elements stored in the 
dictionary. (This can be done by direct counting; alterna- tively, the working processors can keep track 
of the number of insertions and deletions each of them has performed during the last phaze.) M := (1 
+ c). max{p TM,n0). . 4. Call Processlnstructions. 5. If the input queues are not yet exhausted, go 
to 2.  5. ANALYSIS In this section, we .determine the time and space re- quirements of the algorithm. 
Theorem. Let n _> pl+C. (i) The algorithm takes O(n/p) steps (expected) to process n instructions, nip 
per processor. (ii) If n elements are stored in the dictionary, O(n) space is used.  (iii) The expected 
number of steps that pass un-til nip instructions from each input queue are processed is O(n/p). In 
Section 6, we will augment the algorithm in such a way that the following additional statement is also 
true. (iv) There are no availability delays. (See the Introduction for an informal description of this 
notion.) Parts (i)~iii) of the theorem are proved in a sequence of lemmas. Recall from Section 2 the 
definition of a hash function h E 7t: that is "good" for some set sou. Lemma 1. Let S be the set of elements 
stored in the dictionary at the beginning of a call to RehashAll. Assume that the primary hash function 
h chosen in part 4 of RehashAll is good for S. Then during one execu- tion of part 5 in the procedure 
R.ehashAll the ex- pected number of steps in which working processors are "busy", summed over all processors, 
is O(M). Proof. We let ~ be the time processors are busy for the sake of Bi. Let bi be the size of Bi. 
Setting up a new subtable STi needs (expected) time O(bi ~) (part 7 in (b)). Since this is done bi times, 
the total time spent for constructing new subtables STi is O(bi~). No processor waits longer at Bi (part 
3 in (b)) than the total time needed for constructing the subtables. Thus the total time processors spend 
for Bi is O(b~) (expected). We sum over i E {1,...,s} to obtain that the total time processors spend 
"busy" is $ E O(b?) = O(M), i=1 by the definition of "good". [] Lemma 2. In the situation of Lemma 1, 
the probability that during one execution of part 5 in RehashAll (Sec- tion 4(e)) all input processors 
are able to completely read in their segment of S is at least ¼, for a suitable choice of the constant 
C. Proof. By Lernma 1, there is a constant C > 0 such that the expected nurnber of steps that working 
processors are busy is at most C- M/8. Thus, with probability ex- ceeding ½ there are at least C.M/2p 
steps during the execution of part 5 of RehashAll in which at least p/2 processors are "idle". During 
such steps each in- put processor has a chance of at least ½. (1 - ~)P > 1/2e that its random choice 
of a working processor is successful (see Section 4(a)). The probability of fewer than [SI/p < M/p successes 
in C . M/2p (in-dependent) trials with probability of a success larger than 1/2e is bounded by M/p 1 
~ 1 ~ CM/~P-" u~=o (CM/2p) " (~e) " (1- ~e ] By Chernoff's bound for the initial segment of the binomial 
distribution (see e.g. [AV], Prop. 2.4(a)), this sum is smaller than exp(-(1 -4elC) 2 . ((l/2e).CMI2P)I2) 
< 2 -'rM/p, for "r := log2 e- (C/8e) (1 - 4e/C) 2. Thus, the prob- ability that one of the input processor 
fails to empty its segment of S is no larger than p 2-'rM/P = 21ogp-3'M/P, which is smaller than ¼ 
for M > (2 + logp)- P/'r. (Since we have assumed M > pl+,, this is true for p sufficiently large.) [] 
For the analysis of the running time of a clean-up phase we will need a bound on the sizes of buckets. 
Definition. (See the definition of "good'! in Sec- tion 2.) h E ~t, is called l-good for S if h is good 
for S and bi_<l+e.M I/t, for 1 <i<s. Fact 4. If s >_ 4M then Prob(h is/-good for S) > ½. Proof of Fact 
4. Let := s l-1 I-1 (Y) i=1 r=0 From Fact 1 (Section 2) it follows by Markov's in- equality that Prob(h 
~ 7-/(I)) < ¼, hence Prob(h E 7-/(I)) > -~. Since also Prob(h is good) > ~, it suf- fices to show that 
all h E 7-/(0 satisfy bi < l+e.M lp, for 1 < i < s. Since s _> 4M and l > 4, we have for all h E 7/(1) 
that I--1 H (bi - r) < M, for 1 <i<s. r=0 If bi < l, we are done. If bi _> l, we have i I I-1 b] < ~. 
. H(bi- r) < et . M. r=O Taking l-th roots yields the claim. [] Remark. Here we may comment on the reason 
for the assumption that n > pl+C made in the Theorem. Note that we cannot expect any parallel algorithm 
for the dictionary problem that uses the 2-level hash- ing scheme to be finished with inserting n elements 
in less time than the size of the largest bucket, as long as we let one processor construct the subtable 
for one bucket. We have seen (Fact 4) that the bound on bucket sizes that results from using polynomials 
of degree 1 as primary hash functions is 1 + e n l/t, for a dictionary that accomodates n elements. 
Thus we must let the algorithm run at least f~(n lp) steps to be sure that the construction of the subtable 
for the largest bucket can be finished. In order to still achieve optimal time O(n/p), we must assume 
that n lIt = O(n/p), i. e., ptl(Z-1) = O(n). Lemma 3. Assume 1 > 3- (1 + e)/e, h is/-good, and C is large 
enough. Then the probability that running a clean- up phase for C- M/p steps (in part 7 of RehashAll) 
does not leave all processors finished with their tasks is at most ½. Proof. (Clean-up phases are described 
in Section 2(d).) Con- ,sider a fixed bucket Bi. Parts I and 2 of the clean-up phase take O(bi) steps; 
part 3 takes O(b~) steps, in the worst case. By Fact 4, parts 1-3 together take O(M 2/I) steps. Since 
pl+t < M and (1+ e) -1 < 365 1-3/1 < 1--2/1 by assumption, we havep _< M 1-2p, i.e., M 2/I _< M/p. Thus 
parts 1-3 take at most CM/2p steps, for C sufficiently large. We now consider part 4, that is, choosing 
a hash function hi E 7-[~, r = 8lBi I ~, and checking if it is 1-1. Clearly, choosing one hash function 
hi and testing if it is 1-1 takes abe steps, for some constant a. Then the probability that C. M/2p steps 
are not sufficient for finding an hi that is 1-1 is bounded by Prob(at least CM/2p steps are needed 
for part 4) Thus, Prob(3Bi: Bi not cleaned-up after CM/2p steps)  -<53 i=1 < 4M 2 -(C]2al2)'Ml-1/(l+e)'2/t 
(using M 1/(1+') _> p and bi 2 < 12. M 2p) _< 4M 2 -(¢12a~2)'Me/z(l+e) (using 2/I < 2,/3(, + 1)). This 
is smaller than ½ for C and p < M 1/(1+~) large enough; thus Lemma 3 is proved. [] Lelnma 4. The expected 
running time of RehashAll is O(M/p). The space needed is at most C. M/p per processor. Proof. (See Section 
4(e) for the description of RehashAll.) Parts 1-4 take O(M/p) steps in the worst case. Here we use that 
(by induction) the bucket table BTj for each working processor WPj has size O(M/p). The probability that 
parts 5-8 are executed without jumping back to 4. is at least Prob(E1 N E2 N E3 fq E4), where - E1 is 
the event that h is/-good; - E2 is the event that the total number of steps working processors are busy 
during part 5 does not exceed ~C M; - Ez is the event that during part 5 all input processors completely 
read their segment of S; - E4 is the event that the clean-up phase (part 7) is successful. By Fact 4, 
Prob(E1) _> ½. By Lemma 1, Prob(E2 [ El) > ½. By Lemma 2, Prob(Ez I E1 N E2) > ½. By 366 Lemma 3, Prob(E4 
I E1 N E2 A E3) > ½. Altogether we obtain Prob(E1 n E~ n E30 E4) > ~. Thus the expected number of times 
parts 5-8 are performed is at most 16. Each execution of these parts takes O(M/p) steps. Thus the total 
expected time for RehashAll is O(M/p). We let the processors use the same segment of the memory each 
time parts 5-8 are executed. Then the space bound is obvious. [] Lemma 5. The expected running time for 
the call to ProcessIn. structions (in Section 4(g), part 4) is O(M/p). The space needed is at most 2C. 
M/p per processor and in addition O(M) for the header table. Proof. (See Section 4(f) for the description 
of ProcessIn- structions.) Let S := $1 U $2, where S1 is the set of elements stored in the dictionary 
at the beginning of the call to ProcessInstruetions and $2 is the set of all elements z so that the instruction 
"insert z" is among the first e "i~ " M/p instructions to be read by one of the input processors. Note 
that S is independent of any random choices made by the algorithm. It is clear that ISll = no (no is 
defined in Section 4(g)) and no < ~-~.M, by part 3in Section 4(g). _ c " by part 1 in Further, [$21 < 
IT~ M ProeessInstructions. Hence IS[ _< M. By exactly the same argument as in the proofs of Lemmas 1, 
2, and 4 we see that the probability that in part 2 of ProcessInstructions each input proces- sor reads 
---£- M instructions from its internal input l+e queue is larger than ~.1 (In the proof of Lemma 1 
a slight variation is necessary due to the fact that some of the instructions read might be deletions 
or lookups. But since lookups take only constant time and deletions take expected time O(bi 2) if Bi 
is the bucket containing the key to be deleted [the processor has to wait until some insertion into Bi 
is finished], the time bound of Lemma 1 persists.) Thus the ex- pected number of times parts 1 and 2 
have to be per- formed is smaller than 8. Each call to RehashAll takes expected time O(M/p), and each 
execution of part 2 takes time O(M/p) by construction. Thus the total expected time for one execution 
of ProcessIn- structions is O(M/p). The space bound is easily established. The proce- dure RehashAll 
and the subsequent execution of part 2 take space at most 2C. M/p (Lemma 4). We "give tap" the old hash 
table at the end of a call to RehashAll and use the same space again when Re- hashAll is called once 
more. Thus the total space needed is twice that necessary for one execution of parts 1 and 2. [] Lemma 
6. Let no denote the number of elements stored in the dictionary at the beginning of part 2 of the complete 
algorithm (Section 4(g)). (i) Parts 2-5 of the algorithm take (expected) time O(max{n0,pt+~}/p) and 
(deterministic) space O(max{n0, pt+~ }/p). (ii) During parts 2-5 of the algorithm e no/p in-structions 
are read by each input processor and subsequently executed by some working proces- sor.  Proof. (i) 
By induction, the size of each of the bucket tables BTj at the beginning of part 2 of the algorithm is 
O(max{n0, pl+~ }/p). Thus part 2 takes (determinis- tic) time O(max{no,pt+~}/p). The time and space bounds 
for part 4 have been proved in Lemma 5. (ii) By construction. []  Parts (i)-(iii) of the Theorem may 
easily be derived from Lemma 6. In the following Section, we will modify the basic algorithm such that 
also (iv) can be guaranteed. 6. ELIMINATING AVAILABILITY DELAYS The basic algorithm just described has 
the follow- ing disadvantage. An instruction "lookup x" read at some time t by some input processor is 
answered correctly only if the following holds: The last instruction "insert x" or "delete x" read before 
time t was read by some input processor at time t', and its execution was finished at some time t" > 
t ~, such that t" < t. The availability delay for the instruction "insert z" is t" -t', i.e., the time 
spent between reading the instruction and having z inserted in (resp. deleted from) STA(r), such that 
the algorithm for "lookup z" finds the correct information about z in STh(~). ; It can be shown using 
methods from [DKM] that" the average availability delay is constant. On thel . other hand, the worst 
case availability delay can only be bounded by O(p~), i.e., it may happen that a "lookup x" yields the 
wrong answer even if x was not inserted or deleted in the last O(p c) steps. In order to avoid such large 
availability delays, we use an auxiliary dictionary AT with the following properties: 1) ATcan hold up 
to p elements. 2) Deletions and lookups take constant time in the worst case; insertions take expected 
con- stant time, even if p processors insert elements simultaneously. 3) For arbitrary ¢ > 0, ATcan be 
implemented in O(pTM ) space. Assume for the time being that such a structure is available. Then the 
following extension of our ba-sic algorithm from Section 4 guarantees availability delay 1, i.e., it 
satisfies part (iv) of the Theorem in Section 5; moreover, none of the other parts of the Theorem is 
violated. The procedures for lookups, insertions, and deletions are modified as follows. Insert x : We 
modify the procedure from 4(b) as fol- lows. Before the algorithm from 4(b) is started, WPj inserts x 
into AT. During the time needed for this no input processor reads a new instruction. After the in- sertion 
procedure from 4(b) is finished, WPj deletes x from AT. During the execution of 4(b), in an in- terleaving 
fashion, WPj continually checks whether x is still in AT. If not (i. e., a "delete x" instruction has 
been executed in between) it stops the insertion. Delete x : We modify the algorithm from 4(c) as follows. 
Before "delete x" from 4(c) is started, WP i checks whether x is in AT. If so, it deletes it from AT. 
If x is not in AT, WPj starts the procedure "delete z" as in 4(c). Lookup x : We modify the procedure 
from 4(c) as follows. Before "lookup x" from 4(c) is executed, WPj checks whether x is in AT. If so, 
it returns YES to IPy and stops the lookup, otherwise it starts "lookup x" from 4(c). Clearly, for the 
basic algorithm with the above mod- ifications, (i), (ii), and (iii) from the Theorem in Sec- tion 5 
remain valid. In addition, there are no avail- ability delays. Thus, in order to establish the Theo- 
rem including (iv), it remains to construct the aux-iliary dictionary AT with properties 1), 2), 3) from 
above. The construction of the auxiliary dictionary will also be based on hashing. We get time bounds'as 
good as demanded in 2) because we allow O(p TM)space for p elements. Namely, in this case we get almost 
perfect hash functions with high probability, as stated in the following easy consequence of Fact 1 (Section 
2). Definition. A function h E 7"/ts is called l-perfect if bi _< l for l < i < s. Fact 5. Let ¢ > 0, 
l > I/¢, s > 4M 1+c, h randomly chosen from 7-/l 8" Then Prob(h is/-perfect) _> ½. In order to deduce 
this from Fact 1, note that bi _< l for l<i<sholdsif 8 I-1 i:l 17i(b,-r) r:O < 1, and apply Markov's 
bound to Fact 1. Now assume ¢ > 0 to be fixed, and l >_ 1/e. ATis a hash table of size 4pl+t; each ATIi 
] can store a set of at most l elements. The hash function hAT used is from ~ with l > 1/¢, s = 4p 1+~. 
The instructions insert, delete, and lookup for AT are executed as follows. We assume as before that 
a working processor W/~ gets the instruction it has to execute from an input processor IPj,. InsertAT 
x : If WPj gets this instruction from IPj,, it does the following. 1. Compute i := hAT(X).  2. If IATIi]l 
< l then add x to AT[i]. (Conflicts are solved using the priority write conflict resolution.) 3. If 
A~i] > 1 then call l:tehashAllAT.  4. Wait, observing x in AT, until x is deleted from AT (maybe by 
WPj itself). While waiting, do not accept instructions from input processors.  LookupAT x : If WPj gets 
this instruction from IPj,, it does the following. 1. Compute i := hAT(X ).  2. Test whether x E AT~i], 
if so, then return YES to IPj,, else return NO to IPj,.  DeleteAT x : If WPj gets this instruction from 
IPj,, it does the following. 1. Compute i := hAT(X).  2. Test whether x E ATIi ], if so, then delete 
x from AT[i], else return ERROR to I/~,.  RehashAllAT : During execution of this procedure, no input 
processor reads any input. Let d := {Jl WPj currently has some element xj stored in AT}. (Note that 
each WPj has at most one element stored in AT.) The WPj 's, j E A, start the following cycle. 1. Call 
deleteAT xj. 2. Let j0 := rain(A). W/~0 randomly chooses an  h e l = r1#1, 3. Call insertAT xj. The 
algorithm just described clearly needs O(p l+e) space, and lookups and deletions take constant time. 
Parts 1 and 2 in InsertAT and parts 1 and 2 in RehashAllAw take constant time. RehashAllAw and InsertAT 
can call each other. But if Rehash. AllAT calls InsertAT, then the hash function hAT chosen in part 2 
of RehashAllAT is not/-perfect for S = {x~ [ j e A}. This only happens with prob- ability ~. Thus, on 
the average, only two calls of RehashAllAT are necessary, and we get expected constant time for insertions. 
7. REFERENCES [AVI [DKM] [FKS] [KU]  [KRS] [PVW] [R] [v] Angluin, D., and Valiant, L. G., Fast prob- 
abilistic algorithms for Hamiltonian circuits and matchings, JCSS 18, 1979, 155-193. Dietzfelbinger, 
M., Karlin, A., Mehlhorn, K., Meyer auf der Heide, F., Rohnert, H., Tarjan, R. E., Dynamic perfect hashing: 
Upper and lower bounds, Proceedings of the 29th IEEE FOCS, 1988, pp. 524-531. Fredman, M. L., Koml6s, 
J., and Szemer~di, E., Storing a sparse table with O(1) worst case access time, J.ACM 31(3), 1984, 538- 
544. Karlin, A., and Upfal, E., Parallel hashing-- an efficient implementation of shared mem- ory, Proceedings 
of the 18th ACM STOC, 1986, pp. 160-168. Kruskal, C. P., Rudolph, L., and Snir, M., A complexity theory 
of efficient parallel algo- rithms, Proc. of 15th ICALP, 1988, pp. 333- 346, Springer LNCS 317. Paul, 
W., Vishkin, U., and Wagener, H., Parallel dictionaries on 2-3 trees, Proc. of 10th ICALP, 1983, pp. 
597-609, Springer LNCS 154. Ranade, A. G., How to emulate shared memory, Proceedings of the 28th IEEE 
FOCS, 1987, pp. 185-194. Upfal, E., Efficient schemes for parallel com- munication, J.ACM 31(3), 1984, 
507-517.  
			