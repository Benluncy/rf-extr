
 Efficient Techniques for Fast Nested Barrier Synchronization* Vara Ramakrishnan Isaac D. Scherson Raghu 
Subramanian {vara, isaac ,ra~hu}~ics .uci. edu Department of Information and Computer Science University 
of California, Irvine Irvine, CA 92717-3425 Abstract Two hardware barrier synchronization schemes are 
pre­sented which can support deep levels of control nesting in data parallel programs. Hardware barriers 
are usually an order of magnitude faster than software implementations. Since large data parallel programs 
often have several levels of nested barriers, these schemes provide significant speedups in the execution 
of such programs on MIMD computers. The first scheme performs code transformations and uses two single-bit-trees 
to implement unlimited levels of nested barriers. However, this scheme increases the code size. The second 
scheme uses a more expensive integer-tree to support an exponential number of nested barriers without 
increasing the code size. Using hardware already available on commer­cial MIMD computers, this scheme 
can support more than four billion levels of nesting. 1 Introduction The data parallel programming model 
allows a natural way of expressing the large degree of parallelism involved in most computationalfy intensive 
and mathematically com­plex problems. In the data parallel model, the programmer can retain a single 
thread of control while performing com­putations on a large set of data. This is perhaps the reason for 
the preponderance of data parallelism in massively par­ allel computations. For example, all of the more 
than 120 parallel algorithms in a survey of three ACM Symposia on Theory of Computing (STOC) were data 
parallel [14]. One way of executing a data parallel program is on an SIMD (Single-Instruction Multiple-Data) 
computer. The semantics of the abstract model have often been confused with its hardware implementation, 
and data parallelism has been equated to SIMD. However, not only is it possible to execute a data parallel 
program on an MIMD (Multiple-Instruction Multiple-Data) computer, there are also several This research 
was supported in part by the Air Force Office of Scientific Research under m-ant number F49620-92-J-0126. 
the NASA under grant number NAG-5-2561, and the NSF under grant number MIP-9205737. Permission to make. 
digital/hard copies of ~11or p~rt of this material with­ out fee is granted provided that the copies 
arc not made or distributed for profit or commercial advantage, the ACM copyright/server notice, the 
title of the publication and its dde appmr. ond notice is given that copyright is by permission of the 
Association for Computing Machinery, Inc. (ACM). To copy otherwise, to republish,to post on servers o 
r to redistribute to lists, requires specific permission wld/or fee. SPAA 95 Santa Barbara CA USA@ 1995 
ACM 0-89791-717-0/95/07.$3.50 advantages of doing so [6]. The chief advantage is that an MIMD computer 
does not force unnecessary synchroniza­tion after every instruction, or unnecessary sequent ializa­tion 
of non-interfering branches, as an SIMD computer does. Moreover, many jobs can be run simultaneously 
on differ­ent partitions of an MIMD computer. These factors, among others, explain the recent market 
trend towards MIMD com­ puters, exemplified by Thinking Machines CM-5 and Cray Research s T3D. Data 
parallel programs involve a form of synchronization called barrier synchronization: A point in the code 
is desig­nated as a barrier and no processor is allowed to cross the barrier until all processors involved 
in the computation have reached it. Since data parallel programs involve frequent barrier synchronizations, 
a computer intended to run data paralfel programs must implement them efficiently. For this purpose, 
current MIMD computers, including the CM-5 and T3D, provide a dedicated barrier tree exclusively for 
barrier synchronizations [2, 13, 16]. Current MIMD computers provide just one barrier tree per user application. 
However, data parallel programs very often require the simultaneous use of more than one barrier synchronization 
tree, due to data dependent conditionals and loops, that have barriers nested in them. Any reason­ably 
large data parallel program has several levels of nested barriers. Since providing as many trees as the 
number of nestings in mozrams is not feasible due to cost constraints. current ma;hines solve this problem 
by also implementing barriers in software. Software barriers are implemented using shared sema­phores 
or complicated message passing protocols. They suffer from either the sequential bottleneck associated 
with shared semaphores or the large communication latencies of message passing protocols. Since dedicated 
hardware bar­rier trees are intrinsically parallel and have very low latency, they are usually an order 
of magnitude faster than software barriers. There exist numerous algorithms and methods in the lit­erature 
for improved software barriers, including [1, 5, 7, 11, 12, 10, 17], but these are improvements on a 
mechanism that is inherently slow. Methods for masking the latency of barriers have also been proposed 
[3, 4]. These methods hide the synchronization overhead as well as the time spent wait­ing for other 
processors to reach the barrier. They depend on being able to schedule other operations on the processors 
while waiting for barriers to complete. Therefore, they per­form welf on some applications and poorly 
on others. The ability to provide hardware support for all nested barriers in a data parallel program 
will result in a significant speedup Reduction Tms ------------Broadcast Trss &#38;&#38; / \(?!$iv ii!l 
/ / \ ,/ / \\ / / \ ! Prieessor LOGALfkg GLOEiL flsg AND&#38;te Duplkiatot Figure 1. A barrier tree is 
used in cuwent MIMD computers to perform barrier synchronizations. 7> n bits / / \\ i , ./ ! ,/ , , 
1 ,/ / ., Pr&#38;assor LOCALflsg GLOSALf~g OP Iogtc Duplicator Figure 2. An OP-tree with n-bit inputs 
and outputs. of all data parallel applications. In this paper, two schemes are presented for support­ing 
nested barriers using only limited hardware. The first scheme uses two single-bit-trees to support any 
number of nested barrierB. The method relies on code transformations, and it increases the code size. 
The second scheme uses an integer-tree, which requires more expensive hardware, to support an exponential 
number of nested barriers without increasing the code size. With hardware currently available on the 
CM-5, this scheme can support more than four bilEon levels of nesting. The barrier synchronization hardware 
available on exist­ing machines is described in Section 2. Section 3 examines the semantics of data parallel 
programs and the problems of implementing them with current barrier synchronization hardware. The first 
scheme forexecuting nested data paral­lel programs, using two single-bit-trees and code transforma­tions 
is described in Section 4.1. The second scheme, using an integer-tree, is presented in Section 4.2. Section 
5 gives a comparison of the two schemes and directions for future work. 2 Barrier Synchronization Trees 
The following functionality is desired of a barrier: A point in the code is designated as a barrier and 
no proces­sor may be allowed to cross the barrier until all processors have reached it. A barrier is 
implemented as follows: Each processor is assumed to have two l-bit flags, LOCAL and GLOBAL. When a processor 
arrives at a barrier instruction in its program, it sets its LOCAL flag and begins testing its GLOBAL 
flag. As soon as the processor finds its GLOBAL flag set, it clears its LOCAL flag and carries on with 
its next instruction. The barrier synchronization tree must ensure that no processor s GLOBAL flag have 
set their LOCAL flags. A barrier tree is implemented trees, called the reduction tree and at their roots 
(see Figure 1). The AND gates, and takes its inputs is set until all processors as two complete binary 
the broadcast tree, joined reduction tree consists of from the LOCAL flags of the processors. The broadcast 
tree consists of duplicators. (A duplicator is a unit that takes in an input and outputs two copies of 
it.) The root of the reduction tree feeds the input of the duplicator at the root of the broadcast tree. 
The outputs of the broadcast tree are delivered to the GLOBAL flags of the processors. parbegin (1) 
computation 1 (2) if (Cl) then (3) read shared data (4) endif (6) write shared data (5) computation 2 
(7) while (C2) do (8) write shared data (9) endwhile parend Figure3. A parallel program containing acondiiional 
and a loop. The barrier tree works as follows: The output of there­duction tree at the root is the AND 
of all the LOCAL flags. The broadcast tree merely copies this output into all the GLOBAL flags. Therefore, 
none of the GLOBAL flags are set to 1 before all the LOCAL flags are set to 1, just as desired. Thetime 
forthe GLOBAL flags to beset after all processors have set their LOCAL flags (measured in gate delays) 
is proportional to the height of the tree, or equiva­lently, El(logp) where p is the number of processors. 
This is much faster than older techniques that used either shared semaphores or complicated message-passing 
protocols. The barrier tree in Figure lisaspecial case of the OP­tree shown in Figure 2. In this case, 
each processor has two n-bit flags, LOCAL and GLOBAL. The processor puts a value on its LOCAL flag, and 
tests for a particular value on the GLOBAL flag, which will be available after a hardware latency of 
@(log p log(n+ 1)). If it finds that particular value on the GLOBAL flag, it proceeds. The tree is constructed 
of n-bit edges. The operation (OP) performed in the nodes of the reduction tree may be any associative 
operation on two n-bit inputs, such as integer maximum, minimum, sum, bitwise AND, OR, etc. For instance, 
if the OP is maximum, the GLOBAL flag returns the maximum of all the LOCAL flag values. The barrier tree 
available in existing MIMD computers is the special case where n=l and the operation is AND. The generic 
OP-t ree will be used later in our solu­tions. 3 Barrier Synchronizations within Parallel Conditionals 
and Loops In the data parallel model of execution, it is assumed that processors run asynchronously between 
barrier synchro­nizations in which all processors participate. The barrier synchronization hardware (an 
AND-tree) in existing MIM D computers is baaed on this model of execution. In real­ity, the local operations 
in a data parallel program may be conditional or loop constructs with control expressions in­volving 
parallel (non-scalar) variables. This leads to barrier synchronizations in which not all processors may 
partici­pate. These constructs drastically affect the hardware bar­rier synchronization requirements 
of data parallel programs. To understand these requirements, the semantics of parallel conditionals and 
loops are examined with the assumption that all barriers must be executed using available hardware. The 
set of processors which must execute the statements inside a parallel conditional or loop construct is 
determined by a parallel control expression. A processor which evalu­ates the expression to false is 
assumed to skip to the end of the construct and wait at a barrier there. Therefore, while executing a 
data parallel program asynchronously, barrier synchronizations need to be placed at the end of loops 
and conditionals. parbegin (1) computation 1 (2) if ( Cl) then (3) barrier (4) read a neighbor s data 
(5) endif (6) barrier (7) urite shared data (9) comput at ion 2 (8) barrier (10) while ( C.2) do (11) 
barrier (12) write shared data (13) end while (14) barrier parend Figure 4. The parallel program of Figure 
3 with the implicit bar­ r-ier-s placed in the code. To ensure a correct implementation of the semantics 
of data parallel programs without a knowledge of the data dependencies in the program, it is also necessary 
to place barriers before and after each remote read/write operation (or computation involving shared 
variables). For the code shown in Figure 3, the implicit barriers are placed and shown in Figure 4. Parallel 
Conditionals In the example in Figure 4, the processors that evalu­ate the control expression Cl (on 
line 2) to false skip to the endif. The semantics of parallel ifs assume an implicit barrier (on line 
6) after the endif. The pro­cessors not participating in the conditional operation fall to this barrier. 
If there were no remote reads or writes in the construct, the processors could continue execution beyond 
the conditional. However, in this example, there is a remote read in the construct, so a barrier is required 
to correctly execute the semantics of the if. Let us assume that an AND-tree is used to execute this 
barrier. The processors which skipped to line 7 set their LOCAL flags on the AND-tree and wait. There 
is another implicit barrier (on line 3) in­side the construct, to ensure that correct data is read in 
line 4. So, a problem is encountered: the AND-tree is already being used, and the barrier at line 3 can­not 
be executed without setting the GLOBAL flags of the processors waiting at line 6, therefore letting them 
cross that barrier. In other words, the existing infor­mation on the AND-tree would be destroyed if it 
were used to execute another barrier. Therefore, an inde­pendent tree is needed to execute the barriers 
inside the if construct. If a parallel conditional has an else, and either the then or else branch contains 
remote read or write op­erations, the following question must be asked: Are there data dependencies between 
the branches? If there are, then one way to ensure correct execution is to sequentialize the execution 
of the branches. This can be done by placing a barrier between the two bran­ches and treating an else 
as a separate conditional: if not (control expression) then. It may be possible to partially overlap 
the execution of the branches by re­ processor 1 processor2 processor3 processor4 processorp OPOP OP 
OP OP OPOP OP OP OP Barrierl f ..OP OP! IBamer2 OPOP OP OP OP OP OP OP OP OP Barriet3 OP OP OP OP OP 
1 Barriw4 . .1 OP OP OP ! Barrmr4 . . OP OP OP I Earrler4 . . 1 , 1 OP OP Barrisr5 , Figure 5. The data 
parallel model of execution due to conditionals and loops. An OP h a local operation OT remote read/wTite 
pevformed a8ynchTonou8[y between bamiers. The * indicates that the pTocessor did not perform that OP 
and skipped to the next barrier. arranging the placement of barriers between them, but this can be done 
only after data dependencies across the branches are known. For programs which produce the same result 
regardless of the order of execution of the branches, a parallel execution of the branches is possible. 
The barrier tree requirements arising from such an execution wilf be discussed in a later section. Parallel 
Loops The semantics of parallel while-do, repeat-until, and for loops are similar. Therefore, without 
loss of gen­ erality, the vhile loop is used to illustrate our schemes in this paper. In the while loop 
of Figure 4, the pro­cessors which evaluate the control expression C.2 to false (on line 10) falf out 
to the end of the loop. All the processors which enter the loop eventually falf out of the loop, perhaps 
after a different number of itera­tions. Just as in the if construct, an implicit barrier (on hne 14) 
is assumed after an endwhile statement, and an independent AND-tree is required to execute the barrier 
(on line 11) inside the loop. Due to parallel loops and conditionals, the data parallel model of execution 
resembles the one shown in Figure 5. The figure represents p processors, PI, P2, . . . . PP executing 
the program shown in Figure 4. Suppose P3, , . . . PP evaluate Cl to false, and falf out to barrier 2, 
PI and Pz encounter barrier 1 inside the conditional before they join the other processors at barrier 
2. Between barrier 2 and barrier 3, all p processors execute all OPS. After crossing barrier 3, they 
encounter the loop. Now suppose PI and P2 evaluate C2 to false, fall out of the 100P immediately, and 
wait at barrier 5. Therefore, only processors Ps, . . . . PP encounter barrier 4. Then they loop back 
to line 10. Let alf of them evaluate C.2 to true and enter the loop again. They encounter barrier 4 once 
more. When they loop back to line 10, let P3 evaluate C2 to false, and fall out to barrier 5. Pe, . . 
. . PP encounter barrier 4 before finally evaluating C2 to false and falling to barrier 5, which is common 
to all processors. The above example shows that even a simple data paraf­lel program may require barrier 
synchronizations which are not common to alf processors executing the program. parbegin e.orae comput 
at ions . . . (1) if ( Cl) then (2) computation 1 (3) while ( C2) do (4) cornput at i on 2 (5) barrier 
 (6) couput at i on 3 (7) endwhile (8) barrier (9) computation 4 (lo) endif (11) barrier  some computations 
. . . parend Figure 6. Bamiem within nested constructs. Note that each oj the three barriers will TequiTe 
a sepavate bamieT tree. A barrier is now defined as a point in the program which cannot be crossed by 
a processor unless every other proces­sor is either at that barrier or at some other barrier which the 
processor will reach later in its execution. Nested Conditionals and Loops In general, a parallel conditional 
or loop construct with­out any nestings can be executed using only hardware barri­ers if two AND-trees 
are available. One AND-tree (TI ) can be used for barriers at the end of constructs, and the other (2 
2) is dedicated for barriers that occur inside such con­structs and in the outer level of code. Processors 
that have fallen out of a construct need to indicate that they wilf not be participating in barriers 
occurring inside the construct, so they must set their LOCAL flags on T 1 as well as T2. At barriers 
within constructs, processors must set their LOCAL flags only on T2. The hardware requirements for supporting 
barrier syn­chronization increase when conditionals and loops are nested. These requirements are exemplified 
by the code in Figure 6. parbegin some computations . . . T1~ CI (1) (2) if (Tl) then (3) computation 
1 (4) endif T2~ C2 (5) (6) while (TI AND T2) do (7) computation 2 (8) barrier (9) computation 
3 (lo) endwhile (11) barrier (12) if (Tl) then (13) computation 4 (14) endif (15) barrier  some 
computations . . . parend Figure 7. A flattening oj the conditional in Figure 6 such that all bamiem 
nested within it aTe moved one level outward. In Figure 6, some processors fall out of the if on line 
1, and wait at the barrier on line Il. Since the code within the if has barriers at two different levels 
(lines 5 and 8), it already requires two AND-trees. Neither of those two trees can be used for the barrier 
on line 11 without losing infor­mation about waiting processors. Hence, a third AND-tree is needed. When 
loops or conditionals are nested in any combina­tion, one additional AND-tree will be required for every 
level of nesting. Providing as many trees as the number of nest­ings in programs is infeasible for deep 
levels of nesting. Due to cost constraints, real machines can only provide a small fixed number of barrier 
trees. The option of using software barriers is available, but since they are an order of magni­tudeslower 
than hardware barriers, it is important to avoid their usage. Therefore, we would like to implement all 
bar­riers using a limited number of hardware trees. 4 Supporting Nested Barriersin Hardware In this section, 
two schemes are proposed to execute nested barriers using only hardware trees and no software barriers. 
The first scheme involves some code transforma­tions leading toincreased code size, and uses just two 
single­bit-trees (one AND-tree and one OR-tree). It accomplishes this by ensuring that all processors 
step through every loop body the same number of times. The second scheme does not alter the code size, 
since it requires no code transforma­tions. However, it uses an integer max-tree, which is more expensive 
than a single-bit-tree. Both schemes execute all barrier synchronizations in hardware, allowing a much 
faster execution of data parallel programs than when software bar­riers are used. 4.1 Using Two Single-Bit-Trees 
Every conditional construct can be transformed so that all barriers within it are moved one level outward. 
By re­peated application of this transformation, all barriers within a nested if (with any number of 
nestings of conditional) can be moved to the outermost level of code. By these trans­formations, a parallel 
program that does not contain any parbegin some computations . . . (1) T3 + (Tl and T2) (2) barrier 
 (3) while (GLOBALOR(T3) = true) do (4) if (T3) then computation 2 endif (5) barrier (6) if (T3) then 
computation 3 endif (7) if (T3) then T3 + (T1 and T2) endif (8) barrier (9) endwhile (lo) barrier 
  some computations . , . parend  Figure 8. Transformed while-loop f70m Figure 7, Tequiring one AND-iree 
and one O&#38;tree foT execution. loops can be executed using just one AND-tree. However, loops cannot 
be flattened in this manner. This is because barriers inside a loop may be executed many times, each 
time on a smaller subset of active processors. Apartial flattening of the code in Figure 6 by transforming 
the if is shown in Figure 7 (based on [8]). We now show how barriers within loops can be executed using 
an AND-tree and one additional single-bit-tree, re­gardless of the number of nestings in the program. 
The tree required is a special case of the generic OP-tree defined in Section 2; here n=l, and the operation 
is OR. Such a tree is commonly known as an OR-tree. Since each processor has LOCAL and GLOBAL flags for 
each tree, the terms LOCALAND and GLOBALAND are used for the AND-tree flags, and the terms LOCALOR and 
GLOBALOR for the OR-tree flags. The while loop shown on lines 6 to 10 of Figure 7 can be transformed 
to the one shown in Figure 8. In the trans­formed version, no processor ever falls out of the while on 
line 3 if there is even one other processor which needs to en­ter the loop. The OR-tree is used to execute 
the transformed code. Each processor evaluates the GLOBALOR of the con­trol expression by placing its 
value in the LOCALOR flag. If at lesst one processor s LOCALOR flag is true, the root of the OR-tree 
outputs a true. Therefore, the GLOBALOR flags of all processors will be set to true (and they all enter 
the loop). This scheme is similar to the implementation used in SIMD machines to execute loops, where 
the front-end uses the OR of the control expressions of all processors to de­cide how many times to broadcast 
the instructions within a loop. In the CM-.5, the programming model allows only scalar variables to be 
used in the control expression of a loop, automatically creating loops which only all or none of the 
processors can enter. All barrier statements are executed using the AND-tree. When a processor reaches 
a barrier statement, it sets its LOCALAND flag to true, and waits for its GLOBALAND flag to be true. 
In the example in Fignre 8, all processors reach line 4 together. Processors whose control expressions 
are false skip computation 2 and reach the barrier on line 5. When all processors whose control expressions 
are true also reach the barrier, the GLOBALAND flags of all processors are true, and they proceed to 
line 6. Note that the barrier is fulfilled even by processors who would not have entered the loop. Line 
6 is executed similarly to line 4. The control expression is revaluated on line 7 only by processors 
whose parbegin some computations . . . (1) if (Cl) then (2) computation 1 (3) while (C2) do (4) computation 
2 (5) barrier (Nesting Leue12) (6) computation 3 (7) endwllile (8) barrier (iVesting Leuell) (9) 
couq.mtation 4 (10) endif (11) barrier (Nestzng Leoel O)  some computations . . . parend Fignre9. 
The program in FiguTe 6, with nested consimcts con­taining bar?iers at three nesting Ievek. control expression 
is currently true. This is because the semantics of the loop are such that a processor whose control 
expression is false must never get a chance to recompute it. The barrier is placed online 8 to ensure 
that all processors check their OR-tree using the same control expression. Thesame AND-tree isused for 
executing the barriersat lines 5 and 10 (which are at different nesting levels). Even if one loop is 
nested inside another, by applying this trans­formation to both loops, the code can be executed using 
just one AND-tree and one OR-tree. For each level of loop nesting, the control expression evaluated and 
placed on the ORtreeis different, but one OR-tree suffices since all pro­cessors are in the same nesting 
level at any given time. This transformation applies to any number of nestings in any combination of 
conditionals and loops. The transformation for conditionals applies to those with an else branch, by 
treating the branch as aseparate condi­tional with a negated control expression. Since all barriers within 
conditionals are moved to an outer level of code, the branches ofeach conditional will be partially or 
fully sequen­tialized. Even if two branches containing barriers are shown to have no data dependencies 
across them, there will be at least a partial sequentifllzation of the branches since only one barrier 
tree is available in this scheme,  4.2 Using an Integer-Tree A scheme for executing nested barrier synchronizations 
in hardware without any code transformations is presented in this section. The scheme uses a mamtree, 
which is a special case of the generic OP-tree shown in Figure 2, with n-bit wide edges and the OP being 
integer maximum. The scheme also requires the use of one n-bit integer counter at each of the processors. 
Consider the example in Figure 9, which has implicit bar­riers at three levels of nesting, labeled 0, 
I, and ,2. If these barriers were implemented in software, Iabelling the synchro­nizing messages or shared 
semaphores with these numbers would suffice to distinguish one barrier from another. In other words, 
as many logical trees as required by the ap­plication can be easily constructed in software. To run this 
example, a naive implementation that uses one hardware tree for every logical tree would require three 
AND-trees (forthree levels of barriers). Byextension, itisnecessaryto provide aamany AND-trees inhardware 
asthere are nested barriers in the application. However, it is only possible to construct a limited number 
of hardware barrier trees. Con­sequently, such an implementation is infeasible. A scheme that implements 
2 1 logical barrier trees using one n-bit max-tree is now presented. Our scheme is baaed on the following 
key observation: barriers at an inner level ofnesting must always complete before a barrier at an outer 
level (.2 before I before 0), If each processor keeps track of its NestingLevel using its integer counter, 
one max-tree is sufficient to execute all nested barriers. The processor increments its counter at every 
nesting (loop or conditional) it encounters, and decre­ments it when it leaves a nesting. When it reaches 
a barrier, it knows its nesting level and only proceeds beyond the bar­rier when the value returned by 
the max-tree is equal to its nesting level. The processor may either poll the tree or be interrupted 
when the tree returns an appropriate value. The barrier synchronization algorithm using amax-tree is 
shown in Figure 10. The working of the algorithm is illus­trated with the following example. Consider 
the case of four processors, PI, P2, P3, P4, executing the example in Figure 9. Initially, all processors 
have their NestingLevel equal to 0, and their LOCAL flags on the max-tree have been initial­ized to cm. 
Levels = (O, 0,0, O), Flags = (CO, co, co, ccI), Max = m At line 1, say processor PI evaluates Cl to 
false and falls to the endif on line 10. It puts its NestingLevel O on the max-tree and waits at the 
barrier on line 11. Processors PZ, Pa, and PA increment their NestingLevels to 1, perform computation 
1 and reach line 3. Levels = (O, 1,1,1), Flags = (O, CCI,CCI,CCI), Max = co Say P2 evaluates C2 to false 
and falls to the endwhile on line 7. It puts a 1 on the max-tree and waits. Processors Ps and P4 increment 
their NestingLevels to .2 and reach line 4. Leve/s = (O, 1,2, 2), Flags = (O, 1, m, m), Max = cm After 
performing computation 2, they reach line 5 (possi­bly at different times). When they have both put a 
.2 on the max-tree. the tree returns a 2. Levels = (O, 1, 2,2), Flags = (O, 1,2, 2), Max = 2 P3 and P4 
cross the barrier and clear the values they had placed on the max-tree (by setting them to M). Levels 
= (O, 1,2, 2), Flags = (O, 1, ccJ,w), Max = m PI and Pz continue waiting, since they are waiting for 
the tree to return a O and a 1 respectively. Then P3 and P4 perform computation 3, loop back to line 
3 and evaluate C.??again. Say P3 evaluates it to false and falls out to the enduhile on line 7. It decrements 
its NestingLevei to 1, reaches the barrier on line 8, and puts a 1 on the max-tree. Levels = (O, 1,1,2), 
Flags = (O, 1,1, ccI), Max = co Pi passes through to line 5 and puts a 2 on the max-tree. The tree returns 
a 2 immediately, which is ignored by pro­cessors PI, P2, and P3. LeveZs = (O, 1,1,2), Flags = (O, 1,1,2), 
Max = 2 paibegin LOCAL -(m NestingLevel t O while (program has not ended) do fetch next instruction 
if (instruction is conditional or loop) then NestingLevel ~ NestingLevel+l erdif if (instruction is end-conditional 
or end-loop) then NestingLevel + NestingLevel l endif if (instruction is barrier) then LOCAL i-NestingLevel 
wait until GLOBAL = LOCAL LOCAL * cm endif endwhile LOCAL t O ~arend Figure 10. Algorithm foT bamier 
synchronization using the max-tree. Pi clears its tree value and proceeds back toline3. Levels = (0,1,1,2), 
Flags = (0,1,1, co), Max= CCI When it finally evaluates C.Z to false, it fails out to line 7anddecrements 
its NestingLevel to 1. 0nline8, it puts thk value on the max-tree, and the tree returns a I. Levels = 
(O, 1,1,1), Flags = (0,1,1,1), Max = 1 Now processors P2, P3 and P4 cross this barrier and do com­putation40n 
line 9. Levels =( 0,1, l, l), Flags= (O, w,co, co), Max=cm They reach the endif on line 10 (possibly 
at different times) and decrement their NestingLevels to 0, Levels =( O, O, O, O), Flags= (O, m,oo, m)) 
Max=ca At the barrier, they each put a Oon themax-tree, and the tree returns a O. Levels = (O, 0,0, 
O), Flags = (O, 0,0, O), Max = O This lets all the processors (including PI) cross the barrier and continue 
execution. Levels = (O, 0,0,0), Flags = (co, co, co, co), Max = co The details of the implementation 
are described in the algorithm in Figure 10. The outermost level of code is as­sumed to be at nesting 
level O. During program execution, the default value placed in the LOCAL flag by each proces­sor is the 
largest n-bit integer, 2n 1. This is done so that the GLOBAL flag values returned by the max-tree are 
larger than the LOCAL flags of any processors that are waiting at barriers, until all processors are 
at barriers. In Figure 10, thesymbol ooisused to represent this number. When a pro­cessor finishes executing 
its program, it sets its LOCAL flag too to avoid interfering with the completion of the program on the 
other rnocessors. This sche~ealso correctly executes aJlconditionrds with else branches. First consider 
else branches which have barriers logically independent from the barriers of the cor­responding then 
branches. In such conditionals, aparallel execution of the two branches should always produce the same 
results. The barriers of the two branches are consid­ered to be at the same nesting level 1. Although 
the barriers are logically independent, they are executed on the same tree. Consider a then branch with 
a barriers and a cor­responding else branch with b barriers. Without loss of generality, let a < b. The 
a barriers of the then are exe­cuted by dlgning one-to-one with the first a barriers of the else, The 
next b a barriers of the else are executed after the then branch processors fall to the end of the conditional, 
and wait at the barrier at nesting level 1 1. Now consider else branches which need to be run after the 
corresponding then branches have been executed (be­cause there are data dependencies across the branches). 
In this case, if the outer level of nesting is 1-1, the nesting level assigned to the then branch is 
1+1 and the nesting level of the else is 1. The first barrier of the else will be completed only after 
all the then barriers have completed and the processors of the then branch have fallen out to the barrier 
at nesting level 1 1. This ensures a correct sequential execution of the two branches. 5 Conclusions 
In this paper we identified an often overlooked problem of efficiently implementing nested barrier synchronizations 
when executing data parallel programs asynchronously on MIMD machines. Two efficient hardware techniques 
forim­plementing nested barrier synchronizations were presented. The first scheme performs code transformations 
and uses two single-bit-trees to implement unlimited levels of nested barriers. Thesecond scheme uses 
aninteger-tree to support an exponential number of nested barriers. One problem with the scheme using 
two single-bit-trees is that thecode size increases significantly dueto the compiler transformations. 
It is common for the transformed code to betwoor three times aslarge asthe original code. This also 163 
 significantly increases the running time of the application. Another disadvantage is that the scheme 
works by mak­ing all processors busy-loop through every iteration of a loop. (A processor fetches the 
instructions inside the loop even if it has already determined that it should not execute the loop.) 
If the scheduling policy on the MIMD computer allows time-sharing of the individual processors, this 
scheme wastes processing power which could be utilized to run other jobs scheduled on the processors. 
The scheme using a max-tree does not rely on any code transformations for execution. Therefore, the code 
size is not increased as in the earlier scheme. This is traded-off with the fact that a max-tree can 
be much more expen­sive to implement in hardware than two single-bit-trees, if n is large. However, since 
an n-bit max-tree can support 2n 1 nested barriers simultaneously, small vaJues of n are :ufficient 
in general. For example, the CM-5, which pro­vides a 32-bit max-tree in hardware as part of its control 
network [9], could support 4 billion levels of nesting (even though the max-tree is not currently used 
in this manner). A shortcoming of both schemes is that they sequentialize (at least partially) the branches 
of conditionals that contain barriers, even if there are no data dependencies between them. Future work 
is required in finding techniques to in­dependently synchronize the branches of a conditional, al­lowing 
a completely parallel execution. This is part of the more general problem of independently synchronizing 
mul­tiple disjoint subsets of processors. Some results in this di­rection are available in [15]. Acknowledgements 
We would like to thank Umesh Krishnaswamy and Luis Miguel Campos for their valuable suggestions and for 
con­vincing us these results were worth writing up. We also thank Bradley C. Kuszmaul for his prompt 
and detailed re­sponses to our questions. References [1] A. Agrawal and M. Cherian. Adaptive backoff 
syn­chronization techniques. In 16th Annual International Symposium on Computer Architecture, pages 396-406, 
1989. [2] Cray Research, Inc., Eagan, MN. Cray T3D System Architecture Overuiew Manual, 1993. [3] R. 
Gupta. The fuzzy barrier: A mechanism for high speed synchronization of processors. In Third Inter­national 
Conference on Architectural Support -for Pro­gramming Languages and Operating Systems, pages 54 63, 1989. 
[4] R. Gupta and M. Epstein. Achieving low cost synchro­nization in a multiprocessor system. Future Generation 
Computer Systems, 6:255-269, December 1990. [5] R. Gupta and C. R. Hill. A scrdable implementation of 
barrier synchronization using an adaptive combining tree. International Journal of Parallel Programming, 
18:161-180, 1989. [6] P.J. Hatcher, A.J. Lapadula, M.J. Quinn, and R.J. An­derson. Compiling data parallel 
programs for MIMD architectures. In W. Joosen and E. Milgrom, editors, Parallel Computing: From Theory 
to Sound Practice, Proceedings of E WPC 92, the European Workshops on Parallel Computing, pages 28 39. 
10S Press, Amster­dam, Netherlands, 1992. [7] L. Kontothanasiss and R. WisniewskL Using schedular information 
to achieve optimal barrier synchronization performance. In Fourth ACM SIGPLAN Symposium on Principles 
and Practice of Parallel Programming, vol­ume 28, pages 64 72, 1993. [8] Bradley C. Kuszmaul. Personal 
communications, 1995. [9] C. Leiserson, et al. The network architecture of the connection machine CM-5. 
In Symposium on Paral­lel Architectures and Algorithms, pages 272-285, June 1992. [10] E. Markatos, M. 
Crovella, and P. Das. The effects of multiprogramming on barrier synchronization. In Pro­ceedings of 
the 3rd IEEE Symposium on Parallel and Distributed Processing, pages 662-669, December 1991. [11] J. 
M. Mellor-Crummey and M. L. Scott. Algorithms for scalable synchronization on shared-memory multi­processors. 
ACM Transactions on Computer S~stems, 9(1):21-65, February 1991. [12] J, M. Mellor-Crummey and M. L. 
Scott, Synchroniza­tion without contention. In Fourth International Con­ference on Architectural Support 
for Programming Lan­guages and Operating Systems, pages 269-278, April 1991. [13] Wilfried Oed. The Cray 
Research Massively Parallel Processor System CRAY T3D. Available by anonymous ftp from ftp.tray.tom, 
November 1993. [14] Gary Sabot. The Paralation Model: Architecture-Independent Parallel Programming. 
MIT Press, Cam­bridge, MA, 1988. [15] R. Subramanian and I. D. Scherson. Networks for mul­tiple disjoint 
barrier synchronizations. Submitted to the 4th IEEE International Symposium on High Per­formance Distributed 
Computing (HPDC 95), 1995. [16] Thinking Machines Corporation, Cambridge, MA. The Connection Machine 
CM-5 Technical Summary, Octo­ber 1991. [17] H. Xu, P. K. McKinley, and L. M. Ni. Efficient im­plementation 
of barrier synchronization in wormhole­routed hypercube multicomputers. The Journal of Par­allel and 
Distributed Computing, 16:172 184, 1992.  
			