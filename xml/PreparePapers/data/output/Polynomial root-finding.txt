
 Polynomial Root-Finding : Analysis and Computational Investigation of a Parallel Algorithm* (Extended 
Abstract) B. Narendran Prasoon Tiwari Computer Sciences Department University of Wisconsin-Madison Madison, 
Wisconsin-53706. Abstract Using the ideas from the NC algorithm of Ben-Or and Tiwari [Journal of Complexity 
6, 417-442, 1990], we de­velop a practical parallel algorithm that approximates the roots of a polynomial 
whose roots are all real. A new elementary proof of correctness is provided and the complexity of the 
algorithm is analyzed. A particular implementation of the algorithm that performs well in practice is 
described and its run-time behaviour is com­ pared with the analytical predictions. Its performance 
is also compared with that of the root-finding algorithm in the PARI package.  Introduction The problem 
of computing the roots of polynomials has always been one of practical and theoretical interest, and 
many methods have been developed to solve this problem. However, the problem is far from being sat­ isfactorily 
solved and no single implementation exists that is known to behave well for all possible inputs. Schonhage 
[Sch82], in his study of the computational complexity of this problem, observed that most avail­ able 
packages for approximating roots of polynomials are neither stable nor robust in the sense that they 
ei­ ther give inaccurate results, or do not terminate when a large degree polynomial is given as the 
input. Even less was known about the parallel complexity until recently. Partially supported by NSF under 
grant CCFL9024516. Pernussiorr to copy without fee all or part of this material is granted provided that 
the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and 
the title of the publication and its date appear, and notice is given that copying is by permission of 
the Association for Computing Machinery. To copy other­wise, or to republish, requires a fee and/or specific 
permission. SPAA 92-61921CA @1992 ACM 0-89791-484-8/92/0006/0178 . . . . . . . !$1.50 178 The first 
theoretical advance in this direction was made by Ben-Or et. al. [B FKT88], who presented an NC algorithm 
that approximated all the roots of a polyno­mial that had only real roots, In addition to showing that 
this special case of the problem was in the class NC, this algorithm did not suffer from the problems 
of stability that characterized most other methods. This algorithm, however, was far from being practical. 
Ben-Or and Tiwari [BT90] refined the ideas in [BFKT88] and developed an NC algorithm for the same problem 
that was considerably simpler. The general problem of find­ing all roots (real and complex) of a polynomial 
is still not satisfactorily solved. Recently, Neff [Nef90] general­ized the methods of [BFKT88] to show 
that the general root-finding problem, without the restriction that all roots are real, is in NC. However, 
like the algorithm in [BFKT88], Neff s algorithm is not practical. In this paper we describe and analyze 
the behaviour of an implementation of a parallel algorithm that ap­proximates the roots of a polynomial 
which has only real rootsl. The polynomial root approximation problem we consider can be defined as follows. 
Given a positive inte­ger p, and a polynomial po(z) of degree n, whose coeffi­cients are m-bit integers 
and whose roots ~1, X2, . . ., Xn are all real, we wish to compute p-approximations 21, %2, . . . . 5n 
respectively to these roots, where the p­ approximation Zj to the root xi is defined to be the rational 
number 2-Y [2P Zi]. The algorithm we describe is based on the ideas used in the NC algorithm of Ben-Or 
and Tiwari [BT90]. We develop a practical version of this algorithm, suitable for implementation on parallel 
machines with a fixed number of processors. The iterative nature of this ver­sion also led us to a new 
proof of correctness that is considerably simpler than the one in [BT90], The new proof is more direct 
and uses only elementary argu­ ] When given a polynomial with complex roots as input, the algorithm reports 
that fact and terminates. ments, avoiding the sophisticated arguments involving Sturm sequences. To 
demonstrate the practicality of the algorithm, we implemented it on a Sequent shared memory multipro­cessor. 
Our experiences in this regard showed hc)w the choices made regarding two issues: the grain of par­allelism 
and the scheduling policy, were crucial in ob­taining a successful implementation. Our decision to parallelize 
the polynomial and matrix arithmetic, but not the integer arithmetic, seems to offer the right level 
of granularity for the available parallelism. Secondly, we explain how the choice of a dynamic scheduling 
pol­icy avoided skew among the processors and resulted in better parallel performance. We also performed 
a detailed theoretical anal,ysis of the algorithm, with a view to predicting its run-time be­haviour. 
While the analysis presented here is specific to the implemented version, it should be pointed oult that 
it enabled us to identify the computationally intensive parts of the algorithm and decide on the apprc)priate 
granularity of parallelism for different phases of the al­gorithm. This in turn influenced our choice 
oft he dy­namic scheduling policy. The analysis also reveals that our algorithm is sufficient 1y comput 
at ion all y intensive, so that an effective division of tasks among processors minimizes the overheads 
of communication and synchro­nization delays and results in good speedups. The anaL ysis is done on two 
levels. The arithmetic complexity of the various phases of the algorithm is analy:ced by assuming that 
the various arithmetic operations on the integers take constant time. Since the integers in our computations 
span several word lengths however, we further refine this analysis by considering the bit com­pleziiy 
of the algorithm, by taking into consideration the sizes of the integers involved. In Section 2 below, 
we outline the theory behind the algorithm and give a proof of correctness. Section 3 discusses the details 
of the parallel implementation. In Section 4 we present the theoretical analysis of the run-time behaviour. 
Section 5 reports on our results, which we briefly summarize below. The primary goals of the implementation 
(the first such for this algorithm) were threefold. First, we wanted to compare our algorithm with other 
existing root­finding routines. A second goal was to compare the ac­tual run-time behaviour of the algorithm 
with our ana­lytical predictions. Lastly, we wished to study its perfor­mance in practice, particularly 
with regard to speedups attained by the parallel implementation. Section 5 de­scribes our results with 
respect to these goals. We be­lieve that our implementation is stable and robust. Ev­idence for this 
claim comes from our comparisons of our implementation with the (sequential) root-finder in the PARI 
package that uses a variant of Newton s method. Figure l(a) is a representative plot of this comparison. 
179 In Section 5.1 we show that our algorithm is able to handle much larger degrees than the PARI routine 
and is also faster (even on a single processor) on all but small degree polynomials. Our analytical predictions 
are shown to accurately predict the number of arith­metic operations performed by the algorithm. See, 
for instance, Figure l(b) which compares the predicted and observed number of multiplications performed 
by the algorithm for a subset of the input instances). We show that in trying to estimate running times, 
we are limited only by the inability to obtain tight upper bounds on the sizes of certain intermediate 
integers that are com­puted by the algorithm. Even with the bounds used in this paper [C0167, C0166], 
the predicted bit complexity is typically only a factor of two more than the observed bit complexity. 
Section 5.1 presents the results of the comparison between predicted and observed behaviour. We demonstrate 
that this version of the algorithm works well in practice and attains good speedups, Figure 1(c) shows 
a representative sample of the speedups attained by our parallel implementation. Section 5.2 presents 
empirical data on speedups for a range of input param­eters. Cosnard and Fraigniaud [CF90] report on 
the im­plementation of a parallel algorithm on the hypercube, where the communication occurs through 
message pass­ing. They use the iterative algorithms of Durand-Kerner and Aberth [CF90, Abe73], and analytically 
compare the computation and communication costs per iteration for different interconnection topologies. 
However, in contrast to the robustness of the algorithm presented here, the performance of these algorithms 
is known to be sensitive to the choice of initial approximations to the roots, and the number of iterations 
needed for con­vergence has been shown to depend greatly and some­what erratically on these choices [Gug86]. 
Cosnard and Fraigniaud present overall running times for only two polynomials of degrees 32 and 128. 
Therefore, we were unable to compare the running times of our implemen­ tation with theirs. 2 The Algorithm 
Due to space limitations, we will assume that the given polynomial po(z) of degree n has n distinct real 
roots, and that n = 2K 1. Multiple roots are easily han­dled by a simple modification of the algorithm 
that is described in [NT91]. Given a degree n polynomial pO (x) that has n distinct real roots Z1 < %Z 
< ... < X., we say that a pair of polynomials (pl (z), pz(z)) is an interleaving pair for Po(*), if de9ree(Pl(x)) 
+ de9? ee(pz(Z)) = n 1 and the n 1roots ~1 < yz ~ .~ yn_l of pi(x) or pz(z) satisfy xl s yl s X2 s YZs 
... 5 y~_l 5 x~. The roots of the interleaving pair thus determine n intervals mu= 32digit9 1600 WOW 
 1 Thisfm&#38;mntatii -Ar@timf FAtimak + 16C4 PARIdgqkhm -+-. Ackd Cmmt -+-SOJml 1400 . ; i ml . 260000 
 / lsm !i // u 1000 ~! ; 2MOO0. 1CW2 600. , \ , i Mm\ !\ ,, / ,,; 6M . 15#Ooo. \, ; 60LM , $$(, i, 
4C4 . ,~ 4000 \, k.. ? -.. 100000 , ~.,, $...:;.*...-_ m . --------------­ 2004 ..-...:...%-.- _______ 
0o I ~o~ ~ 10 E 30 35 35 40 45 50 55 60 65 lo 02466 10121416 &#38; C@&#38;bl Degrm dplymhl NumbmofPmewm 
Figure l(a) Figure l(b) Figure l(c) on the real line such that each interval contains exactly one root 
ofpo(x). Given one such interval, the Interval Problem refers to the problem of finding the root ofpo(z) 
that lies in it. Section 2.1 explains how to compute interleaving polynomials and Section 2.2 discusses 
the Interval Problem. 2.1 Polynomial Remainder Sequences The standard remainder sequence corresponding 
to the polynomial po(z) is the sequence of polynomi­als F O(z), I l(Z), . . . . F n(z) defined as follows2 
[C0167, C0166]. In the following definition, co = 1 and Ci is the leading coefficient of t%e polynomial 
F;(x) for z >0. Fe(z) = PO(Z), I l(z) = pi(z), (1) where deg(I i+l) < deg(Fi), 1 ~ i < n. It is well 
known that each Fi (z) in the sequence is expressible as a linear combination of f o(z) and F1 (z) : 
Define the matrices Si, 1 ~ i < n and Ti,j, 1 s i ~ j < n as follows:   i=(;fi$)l i n-lo and Ti,j 
= c~_l SjSj-l. ..Si, n >~> i> 1. (3) m,~(2,2), l<i~j <n, Define .Pi,j(~) = FL1(z), l~i~j=n, 1, j>i. { 
2 This is similar to the remainder sequence computed by the Euclidean algorithm to compute polynomial 
gcds (see, for in­stance [AHU74]. Theorem 1 The polynomial Pi,j (x) has integer coe&#38;­cients, is of 
degree j i + 1 and has distinct real roots. Inaddition, ifi <j, then, for any1~isk~jsn, the polynomials 
P~,k_l(x) and Pk+l, j(x) are interleaving polynomials for the polynomial Pi,j(x). Proof: See Appendix 
A. B Note that Ti,j = ~Tk+l,jskT~,k_l. Theorem 1 k k l suggests the iterative algorithm of Figure 2 for 
comput­ ing the roots of PO($).  2.2 The Interval Problems We use a hybrid scheme based on Newton s 
method to solve the Interval Problems. Let (a, b) denote the isolat­ing interval containing the single 
root (. The following Lemma due to Renegar [Ren87] identifies a set of good starting points for the Newton 
iterations. Lemma 1 Let p(x) be a polynomial of degree s n, p(~) = 0, and let p be the smallest of the 
distances from &#38; to the other roots of p(z). If o satisfies [~ crl < $, then Newton s iteration, 
starting at cr, converges quadratically from the start. In view of Lemma 1, we follow the strategy outlined 
in [BT90] to quickly determine a subinterval (ti,;) of (a, 6) that contains the desired root ~, and such 
that there is no other root of p(x) within 10(( ii)n2 of ii or within 10(&#38; ;)n2 of;. Then, every 
subsequent Newton s itera­tion (starting from any point in [ii, 8]) improves precision by a factor of 
2, and thus approximates ~ rapidly. A double-exponential sieve [BT90] is used to locate the interval 
(ti, b). Let 10 = (a, b) and let /0 = b u denote the length of the interval 10. By evaluating p(z) at 
a + io/2, we can determine if ~ < a + 1./2. Assume, without loss of generality, that ~ E (a, a+lo/2). 
Evaluate p(z) at points a + 1./22 , i = O, 1,2, ..., to the maximum i. such that ~ 6 (a, a + 1./22 0 
). Let 11 = (a, a + 1./22 0) and let il be the length of Il. If fori E{l,3,5,..., n} Compute root of 
P~,~(z); endfor ; forl=2to IC ~ = p-l;  fori E{l,l+2fi,l +4&#38;...,26 +2}2} Compute P~,j+z8_z(z); Compute 
roots of P~,~+2J_Z(X), using the interleaving roots of P~,i+6_q(X)I and P~+6,~+z6-z(z); endfor ; endfor 
;  Figure 2: Iterative algorithm for computing the roots of po(z) = Pl,n(x). ~ > a + 11/2, then, we 
can isolate the desired interval (ii, ~) by log2(10n2) bisections of the interval II. On the other hand, 
if t < a + 11/2, we repeat the same procedure on 11, to construct another interval 12 and so on. It is 
easy to check that this process will terminate after O(log n) iterations. 3 The Parallel Implementation 
The parallel root approximation algorithm described above was implemented in the C language on a Sequent 
Symmetry shared memory machine with 20 processors [Seq87]. In this section, we present an overview of 
the implementation, The complete details are in [NT91]. The parallel implementation uses an intricate 
dy­namic scheduling paradigm, where the computations to be performed by the algorithm are divided into 
a set of tasks that are maintained in two task queues, according to their pn orit~. Whenever a processor 
becomes free, it picks the first task from these prioritized queues to ex­ecute. Completion of a task 
usually causes other tasks to be added to the queue. The grain of the tasks was chosen small enough so 
as to keep all processors busy for as much of the time as possible, and yet not so small as to make the 
overheads large. The number of prc,cessors used by the algorithm was a parameter that could be varied 
from 1 to the maximum value of 19. The entire algorithm can be divided into two stages. The first stage 
computes the standard remainder and quotient sequences for the given polynomial, using Equations (2). 
The second stage of the algorithm in­volves computing the interleaving polynomials and then computing 
their roots. The interleaving polynomials are computed by executing the loop of Figure ;3. This loop 
is executed in parallel with the loop of Figure 2 for approximating the roots. Note that in the paraJlel 
im­plement ation, independent computations from several 3 Roughly, the tasks were divided into two classes, 
with a higher priority assigned to those tasks whose completion was likely to enable several new tasks 
to begin. iterations, and several interval problems could be exe­cuting simultaneously. See [NT91] for 
details. All the multi-precision computations required by our algorithm were performed using the UNIX 
rep pack­age that handles integer arithmetic for arbitrarily long integers. The rep package uses the 
straightforward algorithms to perform the basic arithmetic. Thus, addi­tion and subtraction of two n 
bit numbers takes linear time, and multiplication and divisions take quadratic time. While we did not 
parallelize any of the multi­precision integer arithmetic, our implementation did perform the polynomial 
and matrix arithmetic in par­allel. 4 Analysis of Running Time In this section we analyze the bit complexity 
of the al­gorithm that was implemented. Our analysis assumes that the running time of the algorithm is 
dominated by the cost of multiplications performed. This is justified by the fact that of the two arithmetic 
operations that take quadratic time in the size of their arguments, the number of multiplications is 
far greater than the number of divisions4. We also assume in the following analysis that the roots of 
the original polynomial are all distinct. For an integer z, let IIzll denote the size of z in bits. For 
a polynomial p(z) with integer coefficients, d(p) is the degree of p(x) and Ilpl I will denote the size 
of the largest coefficient of p(x). Recall that the input is a polynomial Fo(x) of degree n and m-bit 
coefficients and that p is the desired precision in the roots. From Theorem 1, we know that d(Fi) = n 
 i and d(Q~) = 1. For the coefficient sizes, since IIc;II < llF~(z)[l, we will use llF~(z)ll as an estimate 
for llc~ll. We have that: IIFO(Z)II = m, IIFI(c)II = llf ~(x)ll < m+ logn, 4We also have empirical justification 
of this assumption that indicates that 75 to 90 percent of the actual running time is spent in multiplications. 
fori E{l,3,5,..., n} Compute Pi,~(z) and T~,~; endfor ; forl=2t01{ ~ = 21-1; forie{l,l +26,1 +46,...,26+2}2} 
Compute Ti,i+26_2 = Ti+6,i+26-2 * S~+6_1 * T~,~~6-z~ and ~i,i+2,&#38;Z = Ti,i+Z6-Z(21 2); endfor ; endfor 
;  Figure 3: Iterative algorithm to compute the interleaving polynomials. and IIQ1(z)II ~ 2m + logn. 
If we let /3 = 2m + 3 log n + 2, the following bounds are obtained from the work of Collins [CO167, C0166]: 
ll~i(~)ll < i~, llQi(z)ll< 2@. llA~(z)ll ~ (i -l)~+logn, lll?~(z)ll ~ (i -1)/?. (4) [11 i,nll ~ ( i 
1)/3, i >1. (5) llPi,~+~-111 S (2i+ k -2)~. The three computationally intensive phases of the al­gorithm 
are computing the remainder sequence, com­puting the interleaving polynomials and solving the in­terval 
problems. These phases are analyzed below. Ta­ble 1 summarizes the asymptotic arithmetic and bit complexities 
obtained by this analysis. The derivation of the arithmetic complexities is omitted here. We first look 
at the time taken to compute the re­mainder sequence {Fi(z)} and the corresponding quo­tient sequence 
{Qi(~)}. The reader is referred to Section 2.1 for the description of the computations performed. For 
i ~ 2, computing Qi(z) and Fi+l (z) from f i(z) and Fi _ ~(z), using Equations (2) involves 3(n i) multipli­cat 
ions. Using the size estimates derived earlier, the cost of these multiplications is determined to be 
(n -i) [211F,(z)IIIIQ,(z)II + 211F,(z)llllFi_l(z)ll] < (n i) [4i2@ + 2i(i 1)/?2] ~ 6i2~2(n i). Summing 
up for i = 2,..., n 1 gives an overall com­ plexity of 0(n4(m + log n)2) for this phase. We now turn 
to the complexity of computing the in­terleaving polynomials. Using the degrees and sizes of the matrix 
polynomials that were derived earlier, it can be shown that the cost of the matrix multiplication per­formed 
in a specific iteration of the loop in Figure 3 is 0(i263/32). The total work done in computing all the 
in­terleaving polynomials is obtained by summing up this quantity over all executions of the loop and 
results in an asymptotic complexity of 0(n4(m + log rz)2). The predominant computation performed in solving 
the interval problems is the evaluation of various poly­nomials with integer coefficients at rational 
points. For reasons of robustness, we use only integer arithmetic, Let x be a rational point, and p >0 
an integer such that 2~z is an integer. Let p(z) = po+pl Z+p2~2+. . .+p~zd be the polynomial that we 
wish to evaluate at x and let m = Ilp(z)ll. Let X = l12~z11. We perform the desired evaluation by evaluating 
the scaled polynomial P~(~) = 2dPPo + 2(d-l)Pplx + . . . + pdxd. at 2~X. Ob­serve that pP (2P z) = 2df 
p(z). The cost of evaluating the above polynomial using Homer s rule can be shown to be asymptotic to 
E(m, X, d) = mXd + %# (see [NT91]). To compute the roots of the above polynomial, we need to solve d 
interval problems. For each of these interval problems, the hybrid algorithm we use performs 1(X, d) 
evaluations of the polynomial, where I(X, d) = ; log2 X + log(10d2) + O(log X) (6) where the three terms 
in Equation (6) correspond to the number of evaluations performed in each of the three phases of the 
algorithm : double exponential sieve , bisection, and Newton s method respectively. Thus the overall 
complexity of solving all the interval problems for a polynomial of degree d, with m-bit coefficients 
is asymptotically dl(X, d)l?(m, X, d). Using this bound at each iteration of the loop, one can show [NT91] 
that the overall worst case asymptotic complexity of solving the interval problems is 0(n3X(X +/3)( log 
n + logz X)), where ~=2m+2+310g n. However, 1(X, d) in Equation (6) is a worst-case esti­mate on the 
number of iterations performed, and results in a poor estimate in practice. The double-exponential sieve 
typically executes far fewer than ~ logz X itera­tions before it identifies a suitable interval for the 
start of the bisection phase. Under the assumption that the desired root is distributed uniformly in 
the original in­terval, the double-exponential sieve takes only 0(1) it­erations, and we can estimate 
the average number of Computing Interleaving Polynomials Interval Problems Average Case   = Z;; %s:ii:i) 
 Table 1: Asymptotic Complexity of Phases of the Algorithm polynomial evaluations w I. g(x, d) ~ log(10d2) 
+ log [ (dw )(7) where the second term is the number of iterations per­formed by Newton s method given 
the log(10d2) bits of accuracy already attained by the bisection phase. We will use this average case 
estimate rather than the worst­case estimate of Equation (6) in fitting the anallysis to the observed 
data in Section 5. Using this avera,ge case estimate gives an overall average asymptotic complexity of 
O(dx(x + p)(log n + log x)). Note that if all the roots of the original polynomial were in the range 
[ 2 n , 2R], then the value of X in the above analysis can be bounded by R + p. 5. 5 Empirical Results 
In this section, we report the actual running times observed from our implementation of the algorithm. 
We ran the algorithm on polynomials of degrees 10,15,20 ,...,70. For each degree, three different poly­nomials 
were generated. The input polynomials we used were the characteristic equations of randomly generated 
symmetric matrices over the integers. Thus, for each degree n, the size m, of the coefficients of the 
polyno­mial we generate depended both on n and the sizes of the entries of the random matrix chosen to 
generate the polynomial. For the data in this section, the matrices generated were random O 1 matrices. 
The results of this section were obtained by running the algorithm on each input three to five times. 
5.1 Sequential Running Times Table 2 in Appendix B shows the running times for the algorithm on a single 
processor for different values of n and p. In order to validate the analytical expressions for the arithmetic 
complexity of the algorithm, it was run on a single processor and the actual number of rnlultipli­cations 
performed in the different phases were traced. Of course, for the purposes of this section, the ZLnalyti­ 
cal estimates we used were much more precise versions of the asymptotic expressions presented in Section 
4. Furthermore, we considered all phases of the algorithm, 5Recall from Section 2.2 that if I]FO (z)II 
= m, then R < m instead of just the dominant phases as was done in Sec­tion 4. Figure l(b) shows a typical 
plot of the predicted and observed number of multiplications (in this case for p = 32). Note that the 
predicted counts match the observed counts quite well. In predicting the bit-complexity, however, our 
analyt­ical expressions did not provide as good bounds as the ones above. A typical case is illustrated 
by comparing Figures 4(a) and 4(b). Figure 4(a) plots the predicted and observed number of multiplications 
for a particu­lar phase of the algorithm (in this case, the bisection sub-phase of the Interval Problems.) 
The excellent fit exhibited here translates to a rather weak upper bound in Figure 4(b) when we incorporate 
the size bounds on the polynomial coefficients and compare the resultant bit-complexity estimate with 
the actual bit multiplica­tion costs. The reason for this is that the results of Collins [C0167, C0166] 
that were used in this analysis provide worst case upper bounds on the sizes of poly­nomial coefficients. 
Typically, the coefficient sizes are much smaller, and our inability to get better estimates for the 
average case behaviour limits our capability of predicting the run-times. However, these estimates may 
still be used to provide reasonable upper bounds on the run-times. We also compared the one-processor 
run-times of our implementation with the performance of a sequen­tial root-finding algorithm in the PARI 
multi-precision package [BBC091]. Unfortunately, we were unable to run the PARI algorithm on polynomials 
of degree larger than 30. The comparison for degrees less than 30 and p= 30 digitsisshown inFigure 1 
(a). For degrees larger than 15, our implementation takes less time to compute the roots. For smaller 
values of the precision parameter p, while our algorithm s cost decreased significantly, the PARI algorithm 
seemed insensitive to this parameter.  5.2 Speedups Figure l(c) graphs the execution times of the algorithm 
with increasing number of processors for different val­ues of n and p = 32 digits. Table 3 in Appendix 
B presents the same information in the form of speedup figures with respect to the parallel program with 
one processor. Similar speedups were observed for all val­ues of the precision parameter p. We observe 
that the algorithm exhibits good speedups for small numbers of processors. The speedups begin to drop 
at 16 proces­sors, since for the size of inputs we considered, the gran­ularity of the tasks used was 
not fine enough to keep all 16 processors busy at all times. Another somewhat sur­prising situation is 
the fact that in going from one to two processors, the speedups attained are often more than two. We 
believe this phenomenon is due to increased cache size: since each processor has its own cache, the total 
size of the available cache memory increases when more processors are used. The improvement in perfor­mance 
is brought about by the consequent decrease in the number of cache misses [HM89].  6 Conclusions and 
Open Prob­lems The implementation described in this paper has demon­strated that a practical version 
of the NC algorithm of Ben-Or and Tiwari is capable of attaining good perfor­mance and realizes good 
speedups on a shared memory multiprocessor. Furthermore, this version of the algo­rithm seems to be competitive 
with other existing root­finding algorithms and does not suffer from problems of stability and robustness 
that characterize many other implementations[Sch82]. A careful analysis of the algorithm and a compari­ 
son of the analytical estimates with the actual run-time characteristics shows that the arithmetic complexity 
of the algorithm is well understood and predictable with good accuracy. The main bottleneck in attempting 
to predict the actual execution times is the lack of good an­ alytical estimates on the sizes of intermediate 
quantities computed by the algorithm, expressed in terms of the size of the original input. It would 
be interesting to see if improved average case estimates on these quantities can be obtained. The general 
problem of finding all roots of a polyno­ mial, real and complex, is still far from being satisfacto­ 
rily solved. Neff s NC algorithm [Nef90] does not lend itself to a practical implementation. A simpler 
parallel algorithm for the general case would be of theoretical and practical interest.  7 Acknowledgements 
We would like to thank Anne Condon, Alain Kagi, Afroditi Michailidi and T. N. Vijaykumar for their in­terest 
and participation in an early version of the im­plementation, and Henri Cohen for help with the PARI 
package.  References [Abe73] O. Aberth. Iteration Methods for Finding All Zeros of a Polynomial Simultaneously. 
Mathe­ matics of Computation, 27(122):339 344, 1973. [AHU74] A. V. Aho, J. E. Hopcroft, and J. D. Unman. 
The Design and Analysis of Computer Algorithms. Addison-Wesley, Reading, MA., 1974. [BBC091] C. Batut, 
D. Bernardi, H. Cohen, and M. Olivier. User s Guide to PARI-GP, February 1991. [BFKT88] M. Ben-Or, E. 
Feig, D. Kozen, and P. Tiwari. A Fast Parallel Algorithm for Determining All Roots of a Polynomial with 
Real Roots. SIAM Journal of Computing, 17(6), December 1988. [BT90] M. Ben-Or and P. Tiwari. Simple Algorithms 
for Approximating All Roots of a Polynomial with ReaJ Roots. Journal of Complexity, 6:417-442, 1990. 
 [CF90] M. Cosnard and P. Fraigniaud. Finding the Roots of a polynomial on a MIMD multi­computer. Technical 
report, Laboratoire de l Informatique du Paralh%sme, Ecole Normale Sup6rieure de Lyon, February 1990. 
[C0166] G. E. Collins. Polynomial Remainder Sequences and Determinants. American Math Monthly, 7:708 
712, September 1966. [C0167] G. E. Collins. Subresultants and Reduced Re­ mainder Sequences. .lournai 
of the Association for Computing Machinery, 14:128-142, 1967. [Gug86] H. Guggenheimer. Initial Approximations 
in Durand-Kerner s Root Finding Method. BIT, 26:537 539, 1986. [HM89] D. P. Helmbold and C. E. McDowell. 
Modeling Speedup(n) greater than n. In Proc. Interna­tional Conference on Parallel Processing, volume 
III, pages 111-219-111-225, 1989. [Mar66] M. Marden. Geoemetry of Polynomials. Math. Surveys, 3, 1966. 
 [Nef90] C. A. Neff. Specified Precision Polynomial Root Isolation isin NC. In Proc. .91st IEEE Ann. 
Symp. on Foundations of Computer Science, pages 152-162, 1990. [NT91] B. Narendran and P. Tiwari. Polynomial 
Root- Finding: Analysis and Computational Investiga­tion of a Parallel Algorithm. Technical Report 1061, 
Computer Sciences Department, Univer­sit y of Wisconsin-Madison, December 1991. [Pan85] V. Y. Pan. Fast 
and Efficient Algorithms for Sequential and Parallel Evaluation of Polynomial Zeros and of Matrix Polynomials. 
In Proc. 26th IEEE Ann. Syrnp. on Foundations of Computer Sczence, pages 522-531, 1985. [Ren87] J. Renegar. 
On the Worst-Case Arithmetic Com­ plexity of Approximating Zeros of Polynomials. Manuscript, 1987. [Sch82] 
A. Schonhage. The Fundamental Theorem of Algebra in terms of Computational Complexity, 1982. Unpublished 
Manuscript. [Seq87] Sequent Computers. Symmetry Technical Sum­ mary, rev. 1.5 edition, December 1987. 
Architec­ ture Notes. [Tho69] G. B. Thomas Jr. Calculus and Analytic Geome. try. Addison-Wesley Publishing 
Company, 1969. 184  Appendix A : Proof of Correct­ness We present a proof of correctness of the algorithm 
of Section 2. This new proof is considerably simpler than the one in [BT90]. It is more direct and uses 
only el­ementary arguments, avoiding the sophisticated argu­ments involving Sturm sequences. As in Section 
2, let Fo(z) be a polynomial of de­gree n with integer coefficients such that all its roots are distinct 
and real. We first recall the following definitions from Section 2. {Fi(z)} and {Qi(z)} are the remainder 
and quotient sequences respectively, and A~(~) and -Bi(*) are polynomials such that I i(*) = Ai(~)Fo(z) 
+ Bi(x)F1(z). Let co = 1 and for i ~ 1, let Ci be the leading coefficient of Fi (z). The matrices Si 
and Ti,j are defined as follows: ( o1 Si = l~i<n 1. (8)  &#38;% ) Ti,j = c~_lSj Sj_l . . .Si, l<i~j 
<n--l.(9) Observe that ($-%)= isi- s (   N :) () Fe(x) = lIi Fl(z)  and hence, Ai(x) Bi(x) Tl,i = 
Ai+l(~) Bi+l(x) () Finally, recall the definition of the polynomials Ti,j(2,2),  irj(z) = Fi_l(z), 
~1~Siz<Sj3=<n~ { Define the function sgn(z), for real z to be 1, O, or 1 according to whether x < 0, 
x = O or x > 0, respec­tively. We first prove the following lemma: Lemma 1For1< i ~ j < n,Pi,j(x) = Ai_l(z)Bj+l(z) 
 Aj+l(z)B~_l(z), and Pi+l,j-l Pi,j-1 Ti,j = Pi+l,j J i,j () Proof: For1~i~j<n, Ti,j = c?-lT~ ,jT<~-1 
Aj Bj Ai_l Bi_l -1 = c:_ 1 ( Aj+l Bj+l )( A~ Bi ) ( Aj Bi AiBj Ai_l Bj AjBi,.l Aj+lBi AiBj+l Ai_l 
Bj+l .4j+l~i-l ) Hence, Pi)j(x) = Ti,j(2,2) = Ai-l Bj+l Aj+l Bi-l. Since this is true for all i,j, 
we have that ( Pi+l,j-l Pi,j 1 Ti,j = P~+~,j Pi,j ) Theorem 1 Pi,j (x) has integer coefficients, and 
is of degree j i + 1. For any j < n, the leading coeficienis of all the pi,j(~), 1 S i < j, have the 
same sign. Proofi Collins [C0167, C0166] shows that the Fi(r)) Ai(~) and the B~(x) have integer coefficients. 
The in­tegrality of the coefficients of the Pi,j (z) thus follows from the previous lemma. It is easy 
to check that the leading coefficients of all the Fi (z) have the same sign, and that the Qi(~) are linear 
polynomials with positive leading coefficients. From the definition of the Pi,n(~), it is clear that 
Pi,n = Fi_l(~) is of degree n i + 1. From the above matrix equations, we see that Pi,i(~) = Ti,i(2, 
2) = Qi(z) and, forl~i<j<nj P~,.j(z) = Ti,j (2, 2) (lo) = ~ (Ti+~,jSi) (2, 2) (11) i c;_~ . = pi+2,j(z) 
+ Pi+l,j(z) %]2 (12) c; [ z- Given that Qi (z) is a linear polynomial, it is clear from the above that 
Pi,j (x) is a polynomial of degree j i+ 1. Further, since Qi(x) has a positive leading coefficient, 
the leading coefficient of Pi,j (x) has the same sign as that of Pi+l ,j (z), and hence by induction 
the leading coefficients of Pi,j (z), 1 ~ i ~ j, are all positive. m Theorem 2 Pi,j has distinct real 
roots. In addition, if 1< i<j<n,then,for anyk,i<k<j, thepolyno­mials Pi,k_l(x) and Pk+l,j(x) form an 
interleaving pair of polynomials for the polynomial Pi,j(x). Proof: We examine the following cases separately: 
Casel:k=i: For this case, we will actually show a somewhat stronger result, namely that the roots Pi+l,j 
(z) strictly interleave 6 the roots of Pi,j (z). We will consider the cases j = n and j < n separately. 
Subcase l(a): k = i,j = n : Recall that Pi)n (z) = Fi _ 1(z). Thus, we need to show that the polynomial 
Fi (z) interleaves the polynomial Fi_l(~), 1< i s n. We show this by induction on i. By 6 ?41 < Y2 < 
. . . < yk are said to strictly interleave zo < ZI < . . . <Xkif Xo<yl<Zl<y2 <...< !4k<=k. Rolle s Theorem 
[Mar66, Tho69], Fl(z) = F ~(z) strictly interleaves Fo(z). For i > 1, we have ~.(z) = Q~-l(z)F~-l(x) 
 C?_lFi.-2(Z) % (13) c;_ z Consider two consecutive real roots a and b of Fi_ ~(z). By the inductive 
hypothesis, Fi_2(z) strictly interleaves Fi_ 1(x). Therefore, Equation 13 implies that Fi(a)Fi(b) = ~ 
Fi_2(a)Fi_2(b) <0.  () 4 Thus, Fi (x) changes sign when going from a to b and hence has an odd number 
of roots in (a, b). Since the de­gree of Fi (~) is n i, counting the roots carefully shows that Fi (z) 
haa ezactly one root in each interval formed by successive roots of Fi_ ~(z). The strict interleaving 
property just proved also guarantees that the polyno­mial Fi (r) has n i distinct real roots. Subcase 
l(b): k = i,j < n : For j < n, we need to show that F i+l ,j (z) interleaves Pi)j (z), for 1 ~ i < j. 
We show this by downward in­duction on i. For the base case of i = j 1, Lemma 1 gives us the following 
relation between Pj,j (z) = Qj (z) and Pj_l,j(~). P_l ~(z) = Qj(z)Qj-l(~) Cfcf_2 (14) J, C]_l Let a 
s ~ be the two real zeros of the linear polynomials Qj(z) and Qj-l(z). Since both Qj (~) and Qj_l(~) 
have positive leading coefficients and since (cj cj _2)2 > 0, Pj _ 1,j (~) must have two distinct real 
zeros a and b such thata<a~~ <b, For the inductive step, i < j 2, and we have 2 Pi,j(Z) = * pi+2,j (z) 
+ i+l,j (x) $@] 2 (15) %[ z- Since Pi+z,j (z) interleaves Pi+l,j (z) by the induction hypothesis, for 
two successive roots a and b of P;+l,j (z), we have gn(pi,~(a))sgn( pi,j(b)) = sgn(p~+z,j(a))sgn( pi+z,j(b)) 
< 0 and hence Pi,j (z) has at least one root in (a, b). Let a and b be the leftmost and rightmost roots, 
re­spectively, of Pi+l ,j (z), To complete the proof of the interleaving property, we need to show that 
Pi,j (z) haa precisely one root in each of the intervals ( cm, a) and (b, co). From Theorem 1, we know 
that the polynomi­ als Pz,j (z) and Pi+2,j (z) have degrees of the same parity and their leading coefficients 
are of the same sign. Thus, However, from Equation (15), sgn(Pi,j(a))sgn( Pi+2,j(a)) = sgn(pi,j(b))sgn( 
pi+z,i(b)) < 0 Since Pi+2,j (z) has no roots in the two intervals of in­terest, Pi,j (z) must necessarily 
have unique real roots in those intervals. By a careful counting argument, it follows that Pi,j (z) has 
a unique root in each interval formed by the roots of Pi+l,j (z). Case 2: k>i.: We have, Tk+l,j = c~Sj 
Sj_l . . .Sk+l (16) = C~S.j Sj-l . ..Si(S~S~_l . .. Si)-l (17) c~Ti,j Ti-~ . (18) Using Lemma 1, this 
expands to Pk+z,j l Pk+l,j-1 = P~~zyj Pk+l,j () P,,k P~,k_~ Pt+l,]-l Pi,j-1 c: P@l,j P~,j p~+l,k P~+l)k_l 
()( ) From the above matrix equation, we get pk+l,j(~) = C; [P~+l,j(Z)P~,k-l(z) Pi,j($)pi+l,k-l( z)l 
(19) Consider two consecutive roots a and b of Pi,j (x). By case (1), we know that Pi+l ,j (z) strictly 
interleaves P;,j (z) and thus changes sign in the interval [a, b]. If a (b) is a root of Pi,k_l(z), it 
is also a root of Pk+l,j(z), and we can use it as the interleaving root for the in­terval [a, b]. Otherwise, 
at least one of Pt,k_l(z) and Pk+l,j (x) changes sign in [a, b] and we again have an in­terleaving root 
for the interval. Careful counting shows that in this manner, we can assign a distinct root of either 
Pi,k_ 1(z) or Pk+l ,j (z) to each interval formed by the distinct roots of Pi,j(~). S@(P~,j(-CO))Sgn( 
Pi+2,j( CO)) = Sgn(Pj,j(OO))Sgn( P~+2,j(CO)) > 0 Appendix B : Empirical Data T Fi­ . 4 Frecl 8 m ~ paran 
er ~ p 32 iii r 77­ 32 5.7 8.0 118 15 4 5.1 8.0 15.5 26.7 41.0 20 7 12.6 19.3 38.7 66.8 102.6 25 9 31.!5 
45.4 84.2 143.8 217.1 30 12 78. 7 107.2 177.1 288.5 423.8 35 14 174. 7 222.5 342.2 521.2 744.8 40 17 
385.!5 458.5 644.5 911.5 1264.2 45 20 799.<3 919.3 1210.0 1613.6 2120.2 50 23 1517.0 1690.4 2108.0 2692.1 
3412.2 55 26 2860.3 30765 3659.0 4446.3 54552 60 29 4877.4 5228.0 6019.3 7122.2 8476.1 65 32 7785.6 8248.6 
9305.2 10746.5 12506.9 70 36 129305 13557.8 149637 17270.8 19243.2 17able 2: Single processor Running 
Times (in sees.) 140000 5.5e+08 , Analytical Estimate + Analytical Estimate + 5e+os Actual Count -. 
Actual Count ---­120000 4.5e+08 I/ 4e+06 100000 3.5e+os 3e+os 80000 ,.- 2.5e+OS 2e+os 60000 1.5e+06 
le+os 40000 5e+07 ­ 20000 01 3540456055606570 3540455055606570 Degree of polynomial Degree of polynomial 
 Figure 4(a) Multiplication Counts for Bisection phase Figure 4(b) Bit Complexity of Multiplication in 
Bisection phase (p= 32 digits) (p= 32 digits) degree Processors 1248 16 35 1.0 1.96 377 6.58 9.40 40 
1.0 1.99 3,92 7.15 10.43 45 1.0 2.01 3.96 7.37 11.78 50 1.0 199 3.93 7.35 9.13 55 1.0 2.03 3.95 7.64 
11.49 60 1.0 2.03 4.01 7.74 12.09 65 1.0 2.03 4.03 7.85 11.46 70 1.0 2.04 4.05 7.66 11.35  1 Table 
3: Speedups with respect to single processor execution of algorithm (p = 32 digits)  
			