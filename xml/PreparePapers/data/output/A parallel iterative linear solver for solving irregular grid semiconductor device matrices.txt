
 A Parallel Iterative Linear Solver for Solving Irregular Grid Semiconductor Device Matrices E. Tomacruz, 
J.S anghavi, A. Sangiovanni-Vincentelli DepartmentofElectrical Engineering&#38;Computer Sciences, University 
of California, Berkeley 94720 Abstract We present the use of parallel processors for the solu­tion of 
drift-diffusion semiconductor device equations using an irregular grid discretization. Preconditioning, 
partitioning, and communication scheduling algorithms are developed to implement an efficient and robust 
iterative linear solver with preconditioning. The parallel program is executed on a 64 node CM-5 and 
is compared with PILS running on a single processor. We observe an efficiency increase in obtaining parallel 
speed-ups as the problem size increases. We obtain 60~o efficiency for CGS with no preconditioning for 
large problems. Using CGS with pro­cessor ILU and magnitude threshold>ll-in preconditioning for the CM-5 
and CGS with ILUfor PILS, we attain 50% efjciency for the solution of the large matrices. 1: Introduction 
 The simulation of complex three-dimensional semi­conductor devices requires computers with significant 
computational power. [1,2,3,4] have shown how massively parallel computers can be used efficiently for 
drift-diffu­sion device simulation. All these simulators used rectangu­lar grids since they are easy 
to implement, have perfect load balance, and have regular communication patterns. However, irregular 
grids are important in the field of device simulations since they allow the modeling of non­rectangular 
device boundaries and do not require grids for quasi-neutral regions. [5] gives an example of a diagonal 
alpha particle track that would require 2,000,000 rectangu­lar grid points to model accurately whereas 
a general irreg­ular grid would only require 6900 grid points to achieve the same accuracy. Even with 
the reduction of grid points obtained by the use of irreguku grids, semiconductor simu­lation still requires 
significant computational power. A standard latch-up problem, which requires over 50,000 irregular grid 
nodes, may take five hours to simulate on vector machines such as the Cray-2 [6]. Other applications 
such as SOI, parasitic MOSFETS [7], and silicon pixel detectors [8] may require more computational power. 
Although faster vector supercomputers may offer the com­ putational power needed, parallel processors 
provide an attractive alternative. We present a Connection Machine 5 (CM-5) [9] device simulator that 
uses an irregular grid automatically generated by the Omega program [10,11]. For sequential device simulators, 
the nonlinear algebraic system of equa­tions arising from the discretization is efficiently and accu­rately 
solved by a variation of the basic Newton-Raphson algorithm. As usual in this algorithm, most of the 
computa­ tion time is spent on the solution of the linearized system of equations. The focus of our work 
is to speed-up this step. We present heuristics for partitioning, communication scheduling, and preconditioning 
for the efficient imple­ mentation of a parallel iterative linear solver. Parallel results are compared 
with a sequential program called PILS [12]. This paper is organized as follows. An overview of the device 
equations and how they are generally solved is first given. Section 3 describes a parallel linear solver. 
Results are provided and analyzed in Section 4. Finally, conclu­ sion; are drawn in Section 5. 2: Problem 
definition The time-dependent drift-diffusion model of semicon­ ductor devices, based on the Poisson 
equation and the con­tinuity equations for electrons and holes [13], has been used in this papen V* (sV 
V) = q(p-n+N~-NJ V. (pnnV Y-DnV n) -Rn = $ apV* (v/v V+ DPVP)-RP =% where the dependent variables to 
be determined are the electrostatic potential, y, and the electron and hole carrier 1063-9535/94 $4.00 
@ 1994 IEEE concentrations, n and p. Here, W., pP, Dn, DP are respec­tively, the nobilities and diffusion 
coefficients for elec­trons and holes; s, q, Nd, and N= are the permittivit y, electronic charge, impurity 
donor density, and impurity acceptor density respectively; Rn and Rp are recombina­tion-generation rates 
which include the Auger, Shockley -Read-Hall, and impact ionization terms. For spatial discret­ization, 
a grid consisting of a mixture of triangular and rectangular pyramids and prisms as provided by the grid 
generator Omega [10,1 1] is used. Figure 1 illustrates an ECL device with non-rectangular boundaries 
which is described further in [14]. The Scharfetter-Gummel method [15] along with the box method [16] 
is then used to obtain the discrete equations. These nonlinear equations are solved using either a fully-coupled 
or a decoupled Newton method. Trapezoidal/backward difference formula is used for time integration. The 
asymmetric linear system of equations is solved using an iterative linear system solver with preconditioning. 
Figure 1: ECL grid The computational scheme can be represented as fol­lows: problem read-in and setup 
time integration loop Newton-Raphson loop evaluate the equations for the Jacobian and right-hand side 
of the Newton iteration solve the associated linear system post-processing of results Matrix generation 
and linear system solution are the computationally expensive steps of the solution process. [61 states 
that for a typical sequential 3-D simulation with a number of grid points of the order of 100,000, between 
70 and 90% of the total simulation time is used to solve linear systems. Hence, for the current implementation, 
linear device matrices are currently generated with the sequential program called Second [6]. Box discretization 
produces coupling between differ­ent grid points only if the points are neighbors. Hence, the linear 
system resulting from the Newton scheme is very sparse with about 10 nonzeroes per row. Each grid node 
maps into three rows of the matrix. Because of grid irregu­larity, the sparsity structure is complex 
when compared to a simple band structure arising from standard partial differ­ential equations. 3: Algorithms 
3.1: Linear system solution In simulating a semiconductor device, we are con­cerned with solving a sequence 
of structurally identical lin­ear systems with possibly varying degrees of accuracy. Therefore, an iterative 
algorithm with a good starting solu­tion, control over accuracy of the solution, reuseable pre­conditioners, 
almost linear time per iteration, and predictable memory requirements is more appealing than direct methods 
such as gaussian elimination so long as good convergence behavior can be obtained. Modifications of the 
classical conjugate gradient method such as CGS [17] and BiCGSTAB [18] have emerged as preferred solu­tion 
techniques for drift-diffusion device simulation. We have implemented a preconditioned CGS algorithm 
that shows rapid and robust convergence behavior. The precon­ditioned CGS algorithm is as follows: Lec 
rO=r= U-lL-l(b Axe) , fl = 1, p = O, q = O While (ror>c) ( p= l/p; p=r. rO; D=PP; u=~q+r; V=pp+q; p=pv+u; 
VZU lL Ap; ~=vero; CY=p/o; q= av+v; V=u+q; u = ~lL lAv ; r=r ~u; X=x+ci.v } An efficient parallel implementation 
of the CGS algo­rithm requires careful parallelization of each operation. Inner product, matrix times 
vector (Ax), and precondition­ing ( U lL lA ) operations require interprocessor communi­cation. All the 
other operations have operands available locally and hence, can be trivially parallelized. Inner prod­ucts 
can be easily implemented using the reduction opera­tion of the CM-5. Matrix-vector multiplication requires 
the exchange of variables for neighboring grid nodes that lie in different processors. A communication 
scheduling algo­rithm (presented later in Section 3.5) based on a graph algorithm is used for this operation. 
 3.2: Preconditioning Preconditioning is a key operation in CGS since the robustness of this iterative 
linear solver hinges largely on it. Between no preconditioning and full LU, there is a spec­trum of preconditioners 
that offer a wide variety of efti­ciency-robustness trade-offs. The design of the preconditioned addressed 
in this paper requires paralleliza­tion considerations in addition to the robustness issues. Minimizing 
just time per iteration or number of iterations is not sufficient to yield an efficient preconditioned. 
For a parallel preconditioned, we would like to minimize the total CPU time needed by the parallel machine 
to complete the solution of the linear systems. ILU decomposition: Given: A, a nonsingular n x n matrix. 
O. Setk= 1. 1. uk]=akl; j=k, k+l, . . ..n 2. If k = nthen stop. 3. iik=al~lukk; i=k+l, . . ..n 4. 
a~ = a~-1,/#kj; i ,J = k+l, k+2, ....n 5. Increment k and return to step 1.  Forward and backward substitution: 
k 1 k=l,2, . . ..n Yk=bk ~ ~k,y, ; j=l (Yk-,$+,uk,xj) Xk = ; k=n, n-1, ....l kk As illustrated above, 
preconditioning can be consid­ered as two separate operations -preconditioned computa­tion (i.e. computation 
of L and U) and Forward I Backward Substitution (FBS). FBS (i.e. the WIL-lA oper­ation) is carried out 
twice in each linear operation. A full range of options is available for the preconditioned compu­tation 
and FBS -from completely sequential version with full communication to completely parallel version with 
no communication. We investigate the following combina­tions of ILU preconditioning and FBS techniques: 
parallel ILU / parallel FBS, sequential ILU / parallel FBS, sequen­tial ILU / sequential FBS. The parallel 
version ignores communication completely as opposed to its sequential counterpart. It should be pointed 
out that sources of paral­lelism may exist even for sequential ILU and sequential FBS, For all three 
preconditioners, the ordering of the grid nodes within each processor determines the amount of fill­in 
or amount of fill-in ignored, which influences the con­vergence behavior and / or efficiency of the linear 
solver. For a sequential preconditioned such as sequential ILU, the ordering of processors is another 
factor to be considered. The ordering of processors affects the convergence behav­ior and also determines 
the amount of parallelism. The Reverse Cuthill-McKee, minimum degree, and maximum degree algorithms [19] 
are implemented for ordering pro­cessors and grid nodes within each processor. 3.3: Partitioning 3.3.1: 
Complexity of the objective function: The goal is to minimize the total elapsed time needed to obtain 
the solution. The total elapsed time is determined by the pro­cessor that has the longest CPU time requirement 
to carry out its alloted computation and communication task. Hence, the goal of a partitioned is, for 
a given algorithm, to achieve computational balance, to achieve communication balance, and to minimize 
communication requirements. To achieve computational balance in conjugate gradi­ent based iterative solvers, 
there are three issues. First, the linear algebra operations between synchronization points must be computationally 
balanced by balancing the num­ber of grid nodes. These synchronization points which require processor 
to processor communication are the dot product and matrix times vector operations. Second, bal­ancing 
the matrix times vwtor operation is dependent upon the number of grid points and the sum of node degrees. 
Finally, balancing the application of the preconditioned is a function of the sum of node degrees in 
a reachability graph for a given node ordering and level of allowed fill-ins. Two issues need to be addressed 
to achieve communi­cation balance and to minimize the total communication requirement for a given algorithm. 
First, the average and maximum numbers of processor neighbors need to be min­imized. Processor neighbors 
are defined as the number of processors a specific precessor needs to communicate with to accomplish 
its computational task. Minimizing proces­sor neighbors would imply minimizing the latency time required 
for each communication call. Second, the average and maximum numbers of edges cut for each processor 
need to be minimized. There are three operations that may require communi­cation for a CG based solver 
-dot product, matrix times vector, and application of preconditioned. Dot product com­munication can 
be done easily through the use of the CM-5 reduction operation. To minimize the number of edges cut and 
the processor neighbors for a matrix times vector oper­ation, we only have to look at the actual connectivity 
mesh structure. For preconditioned communication, we have to look into the edges of the reachability 
graph that go from one processor to another. This reachability graph is depen­dent upon the given node 
ordering and the criterion used for allowed fill-ins. 3.3.2: Simplified objective function: It is difficult 
to devise an objective function for a partitioned with a CG based solver as its target algorithm when 
dealing with all the issues described in the previous section. We simplify the criterion for the partitioned 
by focusing on the follow­ing parameters with the actual mesh as the basis graph: 1. time to run the 
partitioned 2. difference between the processor with the most nodes and the processor with the least 
nodes (node load balance) 3. average number of processor neighbors 4. maximum number of processor neighbors 
 5. total edges cut 6. difference between the processor with the most edges and the processor with the 
least edges (edge load balance)  These parameters will have different significance for the unpreconditioned 
CG based solver and for the type of preconditioned used. We investigate the geometrical, topo­graphical, 
and spectral partitioning heuristics that recur­sively bipartition [20] the graph representing the input 
mesh structure. Each partitioned has strengths and weak­nesses based on the parameters presented above. 
The sig­nificance of each parameter can be determined by the total elapsed CPU time results for each 
partition. 3.3.3: Geometrical partitioned: The geometrical parti­tioned sorts the grid points according 
to their coordinates along the axis that has the most number of unique coordi­nate points. Several criteria 
can be used to choose the parti­tion point. 1. node with a coordinate different from its neighbor closest 
to the middle 2. node at the middle of the list 3. node that divides the list into two partitions with 
an equal sum of node degrees -  For criterion 2, the grid points at the boundary (grid points with 
the same coordinates as the node at the middle of the list) are further sorted using the other two axes. 
This enables most adjacent boundary grid points to remain in the same partition. The partitioned based 
on the first and second criteria can be enhanced further by swapping nodes between the two partitions 
to obtain sum of node degrees balance. More heuristics may be used to choose the region in which to allow 
the swapping of nodes. The basic idea is to first swap nodes near the boundary and gradually move in 
until the desired sum of node degrees balance is obtained. 3.3.4: Topographical partitioned: The Fiduccia-Mat­theyses 
algorithm [21], which is an improvement of a local search algorithm first presented by [22], is used 
to imple­ment a topographical partitioned. The basic algorithm moves a node from one partition to the 
other partition in an attempt to minimize the cutset while maintaining the load balance between the two 
partitions within a specified toler­ance. A cutset is defined as the number of edges that con­nect nodes 
in different partitions. The algorithm has a cost function that is allowed to increase in order to help 
the par­titioned get out of a Iocat minima. Ten trials are executed for each partitioning result in order 
to desensitize the parti­tioning outcome from the random initial guess. A 5% max­imum load balance deviation 
tolerance is used for each binary partition. 3.3.5: Spectral partitioned: The spectral partitioned [23] 
is based on the computation of the second largest eigen­value and the corresponding eigenvector of the 
Laplacian matrix of the connectivity g;aph. The connectivity graph is the device mesh structure. This 
eigenvector gives distance information about the nodes. Sorting the nodes according to this information 
provides a way of partitioning the mesh. 3.4: Domain to processor mapping No particular order is followed 
in mapping each mesh subdomain to a processor. The partition number generated by the partitioned for 
each subdomain is also used as the processor number. The fat tree network of the CM-5 mini­mizes the 
maximum distance between processors [24]. Hence, we expect minimal communication penalty for using a 
simple domain to processor mapping. 3.5: Communication scheduling Given a partition of grid nodes among 
processors, the abstract model used for scheduling communication is a processor-communication graph. 
Each vertex represents a processor and each edge (i,j) represents the existence of at least one mesh 
edge that has grid nodes in processors i and j. Therefore, an edge in processor-communication graph represents 
communication either in one direction (precon­ditioned computation, forward substitution, and backward 
substitution) or in both directions (matrix-vector multipli­cation). The communication scheduling algorithm 
for matrix­vector multiplication is based on the repeated application of the maximal nonbipartite matching 
heuristic. We decided to use a heuristic algorithm instead of an exact minimizing algorithm since the 
heuristic algorithm is sig­nificantly faster to execute and the heuristic solution is comparable to the 
exact minimized solution. Each pass of the matching heuristic selects the maximal number of com­munication 
edges such that no two have a common vertex. At each step of the maximal matching heuristic, an edge 
that connects a vertex of maximum degree to its neighbor with largest degree is extracted as a matched 
edge and deleted from the graph along with the matched vertices and their incident edges. Therefore, 
this greedy matching heu­ristic tries to minimize the number of passes while attempt­ing to maximize 
the number of matched edge during each pass. The communication scheduling algorithm for the pre­conditioned 
computation that needs to send the rows of a matrix from one processor to another is implemented by a 
simple heuristic that receives data from and sends data to neighboring processors in a specific order. 
The same heu­ristic is used for scheduling communication for the forward and backward substitution. 4: 
Experimental results  4.1: Data structures and code optimization A hash table is used to store each 
row of the matrix and corresponding vectors. A doubly linked list of two­dimensional dense blocks is 
used to represent each row. Arrays are used to transfer data from one processor to another. The performance 
of each CGS iteration is improved by several modifications. First, the removal of subroutine calls avoids 
unnecessary memory loads and stores. Second, CGS loop reorganization, which is done by moving some independent 
CGS steps, improves the cache performance. Third, the combination of several linear algebra operations 
minimizes memory loads and stores. Fourth, address pre­computation speeds-up memory access. Application 
of all these modifications provides a 4X speed-up for CGS with no preconditioning and a 3X speed-up for 
CGS with pre­conditioning. 4.2: Computing environment &#38; test examples All results in Sections 4.4 
and 4.5 are obtained with a 64 node CM-5 with no vector units. 32,64, and 128 node CM-5S with no vector 
units are used to generate the results in Section 4.6. The algorithms are implemented using the C programming 
language with the CMMD 3,0 communica­tions library [25]. Only blocking communication calls which prevents 
the overlap of computation and communi­cation are currently used. Severat device structures described 
in [26] are used to study the partitioning schemes described above. ECL is a trench-isolated bipokm transistor, 
LOCOS is a short chan­nel MOS transistor with surrounding 10COSisolation, and MCT is a MOS-controlled 
Thyristor with integrated MOS controlled n+ emitter shorts and a bipolar gate. It is a device used in 
high power applications such as traction, high-voltage transmission and motor control. Varying bias conditions 
and initial guess values are used to test the solver. 4.3: Partitioning results In terms of single Spare 
partitioning CPU time shown in Table 1, both topographical and spectral partitioned are an order of magnitude 
slower than the geometrical parti­tioned. Criterion 1 in Section 3.3.3 is used for the geometri­cal partitioning 
results in this section. The topographical partitioned can be accelerated by decreasing the number of 
random initial guesses. However, this significantly degrades partitioning results. Table 2 illustrates 
that the spectral partitioned gives the best node load balance (LB .). By using Criterion 2 in Section 
3.3.3, the geometrical parti­tioned will give perfeet node load balance. The topographi­cal partitioned 
can also be modified to give good load balance by tightening the node load balance tolerance in the cost 
function. Table 1: Single processor partitioning time (see) Device Total Nodes Total Edges GEOM CPU Time 
TOPO CPU Time SPEC CPU Time t%l 17678 58794 72,4 1722.7 793.6 10COS 16586 57335 670 1492.0 1250,6 met 
41122 140529 178.0 74864 Failed Table 2: Load balance results GEOM LB. TOPO L B SPEC LB. Devwe mm (max) 
mm (max) min (max) ecl 208 (339) 205 (396) 276 (277) 10COS 210 (308) 252 (267) 259 (260) met 548 (736) 
625 (666) Failed Processor neighbors (P.N.) are defined as the number of processors a processor needs 
to communicate with while doing a matrix times vector operation that is basically rep­resented by the 
mesh connectivity graph. The geometrical partitioned is shown by Table 3 to give the best minimiza­tion 
of average and maximum number of processor neigh­bors. This results degrades when the cost function is 
adjusted to improve node load balance. The topographical partitioned is illustrated by Table 4 to give 
the lowest num­ber of edges cut for the three sample devices. The number of edges cut increases when 
the cost function is adjusted to improve the node load balance. The total number of edges cut for the 
geometrical and the spectral partitions may be improved by applying a variation of the topographical 
algorithm. Grid nodes at partition boundaries may be swapped to decrease the number of edges cut and, 
at same time, still maintain the node load balance. In terms of the difference between processors with 
the most edges and the least edges, the three partitioners gave comparable results. The sum of node degrees 
has a worst case deviation of 30% from the average. Table 3: Processor neighbor results 1 met 7.1,14 
8.0,20 Faded I Table 4: Edges cut results Device GEOM E.c. TOPO E.C SPEC E.C ecl 9112 7749 11641 10COS 
9294 8155 12223 met 15396 13601 Faited We focused on the geometrical partitioned since it can be easily 
modified and executed to produce partitions with varying characteristics. The topographical and spectral 
par­titioners are also examined to see if the best results obtained with the geometrical partitioned 
can be improved. 4.4: CGS with no preconditioning With regards to the simplified cost function, there 
is a trade-off between load balance and communication bal­ance. Comparable results were obtained with 
different par­titioning goals -node load balance, edge load balance, and minimization of processor neighbors. 
The only major per­formance degradation observed was while using the parti­tioning with the perfect node 
and edge balance. This partition resulted in a severe number of processor neigh­bors and of total edges 
cut. Hence, it can be concluded that the performance of CGS with no preconditioning is rela­tively insensitive 
to the type of partitioning. A variation from 10 to 40% of the total CPU time is currently spent on communication. 
The variation is due to the fact that there is a computational imbalance between communication calls, 
imbalance of processor communica­tion requirements, and idle time due to the difference in processor 
neighbors. Minimizing total edges cut by using the topographical partitioned did not improve the perfor­mance. 
A speed-up of more than 40X for 100 CGS iterations with no preconditioning is obtained for large problems 
with the geometrical partitioned. This speed-up corre­sponds to an efficiency of more than 60% in obtaining 
the theoretical maximum speed-up. The results are shown in Figure 2. The same speed-up is obtained for 
a 41,122 node MCT. It should be pointed out that PILS is a highly opti­mized sequential solver and the 
parallel code is also opti­mized as described in the previous section. CPU Time (see) e+:­ 2 le+02 I 
5 LOCOS (CM-5),,.,..,,.,.,,., .............. . . . . 2 ,,,.,,..---­ .......... . . . -----------­ .,,.,,.,,.,..., 
......... --.. -----&#38;j~M-5) le+ol :--....---- [1 I II I II 20.00 25.00 30.00 35.00 40.00 45.00 
 Mesh Size (10A3) Figure 2: CGS with no preconditioning results 4.5: CGS with preconditioning 4.5.1: 
ILU with magnitude threshold fill-ins: In imple­menting the preconditioned Ori the CM-5, we observed 
that any variation of the sequential FBS produced unsatisfac­tory results even with different variations 
of ordering schemes. Since FBS is applied twice in each iteration, its sequentiality left a major portion 
of the processors idle. Hence, we decided to use a parallel FBS which did not require any communication 
and focused on improving the matrix decomposition routine. The parallel LU (PLU) pre­conditioned obtained 
by performing a complete LU decom­position within each processor is found to be robust in practice and 
completely parallel since communication between processors is now eliminated. However, PLU is very computationally 
expensive in the calculation of L and U. This problem is alleviated by introducing an incomplete LU 
routine with some allowed fill-ins. The fill-ins are kept or discarded depending upon their magnitude. 
 Gauss s algorithm [27] described in Section 3.2 is used to generate the incomplete LU decomposition. 
Fill-ins are generated in Step 4 of the algorithm. For a coupled solu­tion, a fill-in unit is a three 
by three matrix and is kept if at least one of the entries is larger than the specified thresh­old. Iterations 
I II1II 400.00 ECL2 300.00 r 1: LOCOS2 . ....................................... 250.00 200.00 150.00 
100.00 50.00 I III IId 1e 36 1e-08 1e+20 le !+48 Threshold Figure 3: Effect of fill-ins on number of 
iterations CPU Time (see) III I I I 250.0( ............-..-----, ECL3 ~ I Locos3    i., . ........ 
: : 150.00 : :~ :: ECL2 1oooo1 + \ 1 i: .... ................................... :: LOCOS2 ;2:: ., 
: 50.00  $i . .....: ;: -......: . ... . 0.00  ,,~: ,. le 36 le 08 1e+20 1e+48 Threshold Figure 4: 
ILU CPU time Figure 3 illustrates the influence of the number of fill­ins on the number of iterations 
needed for convergence. ECL2, ECL3, LOCOS2, and LOCOS3 are 25969, 34877, 27288, and 35875 node devices 
respectively. Figure 4 shows how much faster it is to compute L and U when less fill-ins are retained. 
Figure 5 shows how threshold levels influence the total CPU time which is the composed of ILU computation 
and of the total time needed for CGS to con­verge. Optimal results show a factor of more than 35X speed-up 
compared to PILS. However, differences in the conditioning of the matrix require different threshold 
levels for optimal &#38;formance. CPU Time (see) I 400.00 ........---........-. ECL3 : :  350.00 
: r::: 300.00 1 I :LOCOS3 250.00 ..............................,,,,,,,,,,. :: .-., -. I ....... : : 
 200.00 : ECL2 150.00 ........................................ 100.00 ,. . ...... ............................... 
 ~,... 50.00 ............................. .. IIII II le 36 1e 08 1e+20 le Threshold Figure 5: Total 
CPU time CPU Time (see) 5EIIII II ~  1.----­ 1e+03 ----L-&#38;O;-&#38;LS) 21 1e+02 ECL (CM 5) ................. 
...................................... ........................................ t 5 .....- COCOS (CM.5) 
...... ,,,,,,...... .,,,,,,,,,,,,,,,,,,,,.,.. I ~~ 20.00 25.00 30.00 35.00 40.00 45.00 Mesh Size (10A3) 
Figure 6: Sequential and parallel results 4.5.2: Automatic selection of threshold: Figure 5 shows that 
for large problems, no fill-ins are needed to obtain the optimum performance. However, for smaller problems, 
optimum performance is observed with fill-ins. We imple­ment a routine that automatically searches for 
the threshold that will create a number of fill-ins comparable to the num­ber of entries in the matrix 
being solved. The threshold search is done using a bisection method. Matrix decompo­sition is aborted 
at early stages of the computation if the current fill-in count predicts a substantially lower or higher 
final count compared to the number of blocks in the A matrix. Figure 6 show that results of automatic 
threshold selection algorithm and of the sequential PILS solution. Sequential results are obtained using 
CGS with an ILU pre­conditioning which is commonly used in sequential device simulation. The time to 
do the incomplete factorization for PILS is less than 2 % of the total CPU time to solve the lin­ear 
system of equations. Automatic selection of the thresh­old is also used for larger problems and minimal 
CPU time penalty is observed. Sequential to parallel speed-up increases as the problem size increases. 
Speed-ups of 50% of the theoretical maximum are observed for large prob­lems. Similar speed-up is obtained 
for the MCT device. 4.5.3: Effects of partitioning: For simulations with little or no fill-ins, the best 
results are obtained with partitions having perfect node load balance. Since there is no increase in 
communication compared to CGS with no pre­conditioning, a variation from 2 to 10?io of the total CPU 
time is spent on communication. The same reasons dis­cussed in Section 4.4 explain this variation. For 
simulation with fill-ins, an increase in problem size increases the computational imbalance for FBS. 
Bal­ancing the number of grid points per processor is not suffi­cient for computational balance. For 
example, in the 25969 node ECL device with 400 nodes for each processor, there is a worst-case 27/35 
discrepancy in terms of sum of node degrees. This produced a 49/85 discrepancy in the number of fill-ins 
generated by the LU decomposition within each processor. This imbalance produced a factor of 2 difference 
in CPU time. The edge imbalance is corrected by an enhanced geomewical partitioned described earlier. 
How­ever, this did not improve the results since the number of till-ins generated is not only influenced 
by the original non­zero elements but also by the manner in which the nodes are connected. Again, the 
best results are still obtained with perfect node load balance. 4,5.4: Effects of ordering: As mentioned 
earlier, the Reverse Cuthill-Mckee, minimum degree, and maximum degree algorithms [19] are implemented 
for ordering pro­cessors and grid nodes within each processor. No ordering is needed for the processors 
since the links across proces­sors during preconditioning have been removed for parallel execution. From 
our experiences, the minimum degree grid node ordering gives the best convergence behavior for the ECL 
and LOCOS devices. Reverse Cuthill-Mckee order­ing is observed to give the next best convergence behavior. 
The node with the minimum degree is used as the starting node for the reverse Cuthill-Mckee ordering. 
Both ordering schemes maintain a significant portion of coupling between nodes when fill-ins are allowed. 
Maintaining the coupling of nodes for rectangular grids has been illustrated by [2] to give the best 
preconditioners. This also appears to be a good criteria for irregular grids. 4.6: Scalability Efficiency 
(7.) 70.00 II I I I I 65.00 60.00 55.00 50.00 45.00 40.00 35.00 ~. Mesh Size (1OA3) Figure 7: Scalability 
with problem size Efficiency (%) IIII I 60.00 55.00 50.00 45.00 40.00 35.00 IIII I 40.00 60.00 80.00 
100.00 120.00 Number of Processors Figure 8: Scalability with machine size Figure 7 shows the increase 
in efficiency as the prob­lem size becomes larger. These results are obtained for a 64 node CM-5 using 
the ECL device. Sequential to parallel speed-up increases as the number of processors is increased. However, 
the efficiency of the algorithm is shown in Figure 8 to decrease as the number of processors increases. 
This is due to the fact that, for a static mesh size, the connectivity of the mesh structure is compromised 
fur­ther as the number of processors increases. This degrades the performance of the preconditioned which 
results in an increase of the total number of iterations required for con­vergence. It should be pointed 
out that due to memory limita­tions of the sequential matrix generator, the largest mesh size solved 
is about 50,000 nodes. A major portion of three-dimensional device simulation applications are expected 
to require nodes in the order of a hundred thou­sand. Therefore, with larger parallel computers, we still 
expect a 50% computational efficiency with the simulation of larger device problems. 5: Conclusions &#38; 
future work Parallel computers are shown to be effective in doing irregular grid drift-diffusion device 
simulation. A 50% effi­ciency is obtained for the solution of large device matrices utilizing the iterative 
CGS linear solver with precondition­ing. The best preconditioned observed uses incomplete LU decomposition 
with fill-ins. Preconditioning is parallelized by removing links between processors during ILU and FBS. 
Hence, communication calls are only necessary for dot product and matrix times vector operations. Fill-ins 
are generated using a magnitude threshold criteria that is adjusted to provide fill-ins comparable to 
the number of entries in the matrix being solved. The minimum degree node ordering is observed to give 
the best results. Due to the total parallelism of the preconditioned, no processor ordering is necess~. 
Perfect node load balance is observed to be the most important partitioning parameter. The geometrical 
parti­tioned is the preferred partitioning algorithm since it can obtain perfect node load balance partitions 
an order of magnitude faster than the topographical and spectral parti­tioners. It can obtain this result 
and, at the same time, it tends to minimize average and maximum processor neigh­bors. Minimizing total 
edges cut, as obtained by the topo­graphical partitioned, is not important since the CGS with preconditioning 
algorithm spends less than 10% of the total CPU time doing communication calls. The computational cost 
of the sequential geometrical partitioned is not signifi­cant since it is only done once, while a typical 
device simu­lation requires the solution of numerous time points or voltage points. If a Cray-2 is used 
to run PILS, a 40X performance improvement over Spare workstation performance is obtained [6]. Hence, 
it can be concluded that a 128 node CM-5 with no vector units will exceed Cray-2 performance for large 
irregular grid semiconductor drift-diffusion device simulations. MFLOPS ratings for the solution of device 
matrices are not useful. The best algorithms (algorithm that mini­mizes wall clock time and has good 
convergence proper­ties) are different for sequential and parallel implementations. The parallel algorithm 
will require more total floating point operations to converge since paralleliza­tion degrades the quality 
of the preconditioned. The current implementation uses a sequential matrix generator which takes 10-30% 
of the total sequential CPU time [6]. This should be easy to parallelize since the com­munication requirement 
is the same as the matrix times vector operation. Also, the algorithm speed can be further increased 
with the use of vector units. Groups of nodes within each processor may be done in parallel with the 
vec­tor units while using ILU with minimal fill-ins. 6: Acknowledgments This research is funded by MICRO, 
TMC, and NSF infrastructure grant number CDA-8722788. The authors would like to thank Eric Fraser, Mark 
Kremenetsky, Horst Simon, Stephen Edwards, and Margarita Santos for their help in making this research 
paper possible. References [1] D. Webber, E. Tomacruz, R. Guerrieri, T. Toyabe, and A. Sangiovmi-VincentelIi, 
A Massively Parallel Algorithm for Three-Dimensional Device Simulation, IEEE Trans. cm CAD, Vol. 10, 
pp. 1201-1209, Sept. 1991. [2] E. Tomacruz, J. Sanghavi, and A. Sangiovanni-Vincentelli, Algorithms for 
Drift-Diffusion Device Simulation Using Massively Parallel Processors, IEICE Trans. on Electron­ics, 
Vol. E77-C, pp. 248-254, Feb. 1994. [3] K. Wu, G. Chin, and R. Dutton, A STRIDE Towards Practi­cal 3-D 
Device Simulation -Numericaf and Visualization Considerations, IEEE Trans. on CAD, Vol. 10, pp. 1132­1140, 
Sept. 1991. [4] R. Dutton, Algorithms and TCAD Software using Parallel Computation, VPAD Proceedings, 
pp. 10-12, 1993. [5] W. Coughran, M. Pinto, and R. Smith, Adaptive Grid Gen­eration for VLSI Device Simulation, 
IEEE Trans. on CAD, Vol. 10, Pp. 1259-1275, C)ct. 1991. [6] G. Heiser, C. Pommerell, J. Weis, and W. 
Fichtner, Three-Dimensional Numericat Semiconductor Device Simulation: Algorithms, Architectures, Results, 
IEEE Trans. on CAD, Vol. 10, pp. 1218--1230, oct. 1991. [7] M. Noell, S. Peon, M. Orlowski, and G. Heiser, 
Study of 3-D Effects in Box Isolation Technologies, SISDEP Proceed­ings, pp. 331-340, 1991. [8] L. Bosisio, 
F. Forti, and E. Tomacruz, Measurement and Tridirnensionaf Simulation of Silicon Pixel Detector Capaci­tance, 
IEEE Nuclear Science Symposium, Vol. 1, pp. 338­342, 1993. 32 [9] J. Palmer, and G. Steele Jr., Connection 
Machine Model CM-5 System Overview, IEEE 4th Symp. on the Frontiers of Massively Parallel Computation, 
pp. 474-483, 1992. [10] P. Conti, N. Hitschfel~ and W. Fichtner, Omega -An Octree-Based Mixed Element 
Grid Allocator for the Simula­tion of Complex 3-D Device Structures, IEEE Trans. on CAD, Vol. 10, pp. 
1231-1241, 1991. [11] N. Hitschfeld, P. Conti, and W. Fichtner, Mixed Element Trees: A Generalization 
of Modified Octrees for the Genera­tion of Meshes for the Simulation of Complex 3-D Semicon­ductor Device 
Structures, IEEE Trans. on CAD, Vol. 12, pp. 1714-1725, NOV. 1993. [12] C. Pommerell and W. Fichtner, 
PILS: An Iterative Linear Solver Package for Ill-Conditioned Systems, Supercomput­ing 91, pp. 588-599, 
1991. [13] S. Selberherr, Analysis and Simulation of Semiconductor Devices, Springer-Verlag, Wien, Austria, 
1984. [14] N. Hitschfeld, K. Kens, P. Conti, Omega 3.0 User s Guide, Integrated Systems Laboratory, ETH-Zentrum, 
Zurich, 1991. [15] D. Scharfetter and H. Gummel, Large-Signal Analysis of a Silicon Read Diode Oscillator, 
IEEE Trans. Electron Devices, Vol. 16, pp. 64-77, 1969. [16] R. Varga, Matrix Iterative Analysis, Prentice-Hall, 
Engle­wood Cliffs, NJ, 1962. [17] P. Sonneveld, CGS, A Fast Lanczos-type Solver for Non­symmetric Linear 
Systems, SIAM J. Sci. Stat. Comp., Vol. 10, pp. 36-52, Jan. 1989. [18] H. Van der Vorst, Bi-CGSTAB: a 
fast and smoothly con­verging variant of Bi-CG for the solution of nonsymmetric linear systems, SIAMJ. 
Sci. Stat. Comput., Vol. 13, pp. 631­644, 1992. [19] I. Duff and G. Meurant, The Effect of Ordering on 
Precon­ditioned Conjugate Gradients, BIT, pp. 635-657, 1989. [20] M. Berger and S. Bokhari, A Partitioning 
Strategy for Non­uniform Problems on Multiprocessors, IEEE Trans. on Computers, Vol. C-36, pp. 570-580, 
May 1987. [21] C. Fiduccia and R. Mattheyses, A Linear-Time Heuristic for Improving Network Partitions, 
Design Automation Con­ference Proceedings, Vol. 19, pp. 175-181, 1982. [22] W. Kernighan and S. Lin, 
An Efficient Heuristic Procedure for Partitioning Graphs, Bell System Technical Journal, pp. 291-307, 
Feb. 1970. [23] H. Simon, Partitioning of Unstructured Problems for Paral­lel Processing, Comptiing Systems 
in Engineering, Vol. 2, pp. 135-148, 1991. [24] Z. Bozkus, S. Ranka, and G. Fox, Benchmarking the CM-5 
multicomputer, IEEE 4th Symp. on the Frontiers of h4as­sively Parallel Computation, pp. 100-107, 1992. 
[25] CMMD Reference Manual, Thinking Machines Corporation, Cambridge Massachusetts, 1992. [26] N. Hitschfeld, 
P. Conti, and W. Fichtner, Grid Generation for 3-D Nonplanm Semiconductor Device Structures, SIS-DEP 
Proceedings, Vol. 4, pp. 165-169, 1991. [27] K. Kundert, Sparse Matrix Techniques and Their Applica­tion 
to Circuit Simulation, Circuit Analysis, Simulation, and Design, A. Ruehli cd., North-Holland, New York, 
NY, 1986. 
			