
 Effects of Partitioning and Scheduling Sparse Matrix Factorization on Communication Sesh Venugopal Dept. 
of Computer Science Rutgers University New Brunswick, NJ 08903 Abstract We present a block-based, automatic 
partitioning and scheduling methodology for sparse matm z factor­ization on distributed memory systems. 
U8ing ezpem ­mental results, we analyze this technique for conwnu­nication and load imbalance overhead. 
To study the performance effects, we compare these overheads with those in a straightforward wrap-mapped 
column as­signment scheme. All experimental results were ob­tained using test sparse matrices from the 
Harwell-Boeing data set. The results show that there is a com­munication and load balance trade-off. 
The block-based method results in lower communication cost whereas the wrap-mapped scheme gives better 
load balance. 1 Introduction Partitioning and scheduling the parallel execution of large scientific applications 
on distributed memory systems is a difficult and time consuming task. If the dependencies involved are 
unstructured, as in the case of sparse linear systems, then the taak becomes even more complex. Use of 
naive techniques to extract par­allelism often results in large communication overhead and/or in large 
load imbalance. To reduce communi­cation overhead, locality of data must be exploited and to balance 
the load, the computations must be evenly distributed at all times. When the data depen­dencies are non-uniform 
and unstructured, achieving these two goals simultaneously is difficult. As a result, in such cases, 
the overall performance may turn out to be poor, even if an application has a high degree of ex­tractable 
parallelism. One possible way to minimize the overhead is to make use of the structure of the This research 
waspartially supported by the National Aero­nautics snd Space Administration under NASA contract NASl­18606 
while the firstwos in residence at ICASE, author Mail Stop 132C, NASA Langley Research Center, Hampton, 
VA 23665. and Load Balance* Vijay K. Naik IBM T. J. Watson Research Center Yorktown Heights, NY 10598 
sparse system which can usually be determined prior to performing the numerical computations. When di­rect 
methods are used to solve the sparse systems, this information in the form of the structure of the factored 
matrix is routinely used to reduce computa­tion and/or storage costs. Recently, this information has 
also been applied in extracting parallelism while maintaining low communication and load imbalance costs 
[5], [6], [14]. However, in most cases, parallelism has been extraeted manually, which tends to be ex­tremely 
tedious, error prone, and inflexible. Thus, au­tomation is the key to successful parallelization of such 
applications. To summarize, there are two important issues in the efficient parallelization of sparse 
matrix based computations: Developing technology for the automatic paral­lelization of the computations. 
 Developing a methodology for the extraction of the av~la~le parallelism with minimum commu­nication 
and load imbalance costs.  To address these issues, we have developed an automatic, block-based scheme 
for partitioning and scheduling the computations in factoring a sparse ma­trix. The scheme makes use 
of the structure of the fac­tor and is targeted towards distributed memory sy5 terns. To reduce communication, 
it takes advantage of locality. However, to maintain proper load balance and a high degree of parallelism, 
the scheme makes uae of an adaptive technique in distributing the com­putational work. To demonstrate 
the usefulness of such a partition­ing scheme and to bring out the performance limita­tions that are 
inherent in sparse matrix computations, in this paper, we compare the communication over­head and the 
degree of load balance in the automated block-baaed approach with same of a straightforward and widely 
used column-baaed approach. In the lat­ter scheme, computations associated with an entire 01991 ACM 
0-89791-459-7/91/0866 $01.50 column or row are assigned to a processor and the assignment of these columns 
or rows ia usually done in a wrap-around fashion. We refer to thw scheme as the wrap-mapping or wrap 
scheme. For comparing the performance on practical application, we present results for some of the Harwell-Boeing 
test matrices. In the following discussion, it is assumed that the reader is familiar with the standard 
terminology used in the context of sparse matrix computations. For an explanation of the basic terminology 
and concepts related to sparse matrix computations see [7], [3]. The organization of the rest of the 
paper is as fol­lows. In the next section, the Cholesky factorization is briefly described and some of 
the terminology used in the paper is introduced. The partitioning and sc&#38;dul­ing strategies that 
are used for automation are pre­sented in Section 3. Performance results are described in Section 4 and 
Section 5 concludes the paper. Cholesky factorization The partitioning and scheduling methodology is 
de­scribed in this paper assuming Cholesky factorization as the model numerical method of computation. 
The Cholesky algorithm is simple, well understood, and is widely used. Note, however, that the techniques 
pre­sented here are applicable to other factoring methods as well. In the following, we highlight only 
those as­pects of thhr algorithm that are essential for describhg the partitioned. For details on the 
Cholesky factorirna­tion scheme, see [9]. For the sake of completeness, we first briefly de­scribe the 
four steps involved in the direct solution of Ax = b. (For details see, for example, [8],) It is as­sumed 
that A is symmetric, positive definite and that Cholesky factorization is used in computing the factor 
L, where A = LLT. 1. Ordering: Find a good ordering of the unknowns for elimination. The ordering is 
given by a permu­tation matrix P. Most often, a good ordering implies one which would lead to a sparse 
factor and fewer arithmetic operations in the numerical factorization step. 2. Symbolic Factorization: 
Determine the sparsity structure of the factor L. 3. Numerical Factorization: Compute L. 4. Z%iangu/ar 
Solutions: Using the computed L, solve the triangular systems Lu = Pb, LTV = u and set x = PTV.  row 
..i .... ..............-( row ~i~) i .... .......  Figure 1: Inter-element dependencies in Cholesky 
fac­torization. The basic element-level data dependencies in the factorization process are shown in Figure 
1. In that figure, only the lower triangular part of the matrix to be factored is shown. Li,j denotes 
the element in row i and column j. The direction of arrow indicates the data flow. Thus, elements Lj,k 
and Li,k from column k of the factor L are required in computing element L S,J.. Li,j = Li,j _ L~,k * 
Lj,k is the corresponding operation in the Cholesky factorization. (Initially Li,j is set to ~,j.) We 
refer to this operation as a single update operation. Note that in computing the final value of Li,j, 
it must be updated-by all pairs of non­zero elements Lj,X and Li,~, 1 < k < ~. Finally, after all the 
updates are performed, the element is scaled by the square root of the diagonal element in that column. 
3 Partitioning and scheduling The partitioning scheme presented here is static in the sense that all 
the computations are partitioned before any of the computations are scheduled for exe­cution. For this, 
the partitioned takes as an input the structure of the factor for the sparse matrix. However, the scheme 
is general and does not have knowledge of any matrix structure embedded in it. As stated in the introductory 
section, the aim of the partitioned and the scheduler is to reduce commu­nication and at the same time 
maintain a balanced work load among processors at all times. To achieve this, wherever possible, data 
locality is exploited. This leads to some variation of block-based partitioning; such partitioning approaches 
have been proposed in several linear algebra related problems [2], [12]. With blocking, it is possible 
to achieve a high ratio of com­putation to communication per block. In [11], it is shown that for an 
important class of problems, the block-based partitioning schemes result in an optimal utilization of 
the data accessed (and thus reduce data traffic). Blocking, however, could lead to load im­balance because 
the increase in the size of achedulable units results in a loss of flexibility in distributing work among 
processors. To avoid this, the partitioned de­scribed here partitions the factored matrix into blocks 
of varying sizes that can be assigned in an equitable manner to the processors. It makes use of a heuristic 
where the block sizes are subject to adaptive manipu­lation. In the following we describe the functioning 
of the partitioned in some detail. The partitioning starts with the zero-nonzero struc­ture of the filled 
sparse matrix obtained afier the sym­bolic factorization phase has been completed. Blocks of non-zero 
areas are identified in the filled matrix. We refer to these as dense blocks. On occasions, blocks are 
formed by including small regions that correspond to zeros in the factored matrix in order to obtain 
larger blocks. Inclusion of such areas with zero elements is kept to a minimum. The work in these dense 
blocks is partitioned into sub-blocks which are the basic schedu­lable units. These unit Mocks have a 
regular shape ­each unit block is either a column, a rectangle or a tri­angle. After all the unit blocks 
are identified, the de­pendencies between these blocks are determined. Fi­nally the unit blocks are assigned 
and scheduled on processors. Thus, the steps involved in the automatic partition­ing and scheduling are: 
 Identify dense blocks in the symbolic factor.  Partition each dense block into schedulable unit blocks. 
  Generate and store dependency information for the unit blocks.  Schedule these units on the processors 
of a mes­sage passing system.  Consolidate the non-local memory access infor­mation for each processor 
so as to minimise com­munication overhead.  In the remainder of thb section, we will describe the first 
four steps. S.1 Identification of dense blocks To identify the dense blocks, first clusters of columns 
are determined in the sparse triangular fac­tor. A cluster is a either a column or a strip of con­secutive 
columns. If it is a strip, it contains a dense triangular block at the top and (possibly) a set of off­diagonal 
dense rectangular blocks. This is illustrated using an example shown in Figure 2. In that figure, non-zero 
elements in the filled 41 x 41 matrix are in­dicated by the dark areas. The matrix corresponds to a 5-point 
finite element 5 x 5 grid and is ordered U5 ing Liu s multiple minimum degree algorithm [10]. It was 
generated using the Sparse Matrix Manipulation System developed at the University of Wisconsin [1]. Figure 
2: A 41 x 41 filled matrix. In Figure 2, note the following in the lower triangu­lar part. Cluster 1 
spans columns 1 and 2 and cluster 2 spans columns 3 and 4. Both clusters 1 and 2 have a three-element 
dense triangular block at the diago­nal. Cluster 1 has three dense rectangles below the triangle, each 
of which is 1 x 2, while cluster 2 has two dense rectangles, the upper one being 1 x 2 and the lower 
one being 2 x 2. Clusters 3 through 12 are single columns starting with cluster 3 at column 5. The last 
cluster consists of columns 35 through 41. This clus­ter has one dense triangle and no rectangles below 
it. Note that in this illustration we do not consider col­umn 34 as part of the last cluster because 
of the zero in row 38 of this column. But this can be over-ridden by allowing some seros to be 8 part 
of a triangle. Once the clusters and the triangular and rectangu­lar blocks within each cluster are 
identified, the alg­rithm processes the clusters left to right in the matrix. When a cluster is processed, 
each block in the cluster is partitioned into sub-blocks whkh are schedulable units. Next, for each unit, 
the dependencies are de termined and stored. These steps are explained below. 3.2 Part itioning of a 
block A cluster with a single column is considered to be a schedulable unit and is not subject to further 
par­titioning. In a multi-column cluster, the triangular block is partitioned first. In general, the 
number of partitions of a triangle are determined by (a) the num­ber of processors that are assigned 
to the blocks on which the triangle depends, (b) a certain minimum work requirement per unit subblock. 
The first pa­rameter restricts communication to the group of pr­cessors that work on the triangle and 
its predecessors. The second parameter is used to ensure a satisfac­tory ratio of computation to communication 
for each unit block and is an architecture dependent parame­ter. This parameter may be used to vary block 
sizes from one cluster to the next. For the results presented here we use a fixed size -one for all the 
triangular block and another for the rectangular blocks. This is referred to as the grain Aze and is 
the minimum num­ber of number of matrix elements required in each unit block. The grain size dictates 
a maximum number of partitions, say P&#38; A block is partitioned into at most pd equal sized units; 
(d mod becauae it may not al­ways be possible to break up a block into exactly pd equal sized units. 
Figure 3 illustrates this partitioning. The triangle is partitioned into six parts. One of the rectangles 
is partitioned into four parts and the other is partitioned into three parts. 3.3 Identification of dependencies 
The dependencies in a single update operation at the element level of Cholesky factorization are shown 
in Figure 1. However, for allocation and scheduling of the units, it is necessary to identify the dependencies 
at the block level. In this step, for each unit block, the dependencies are determined and the information 
information on the actual data needed in the update operation is stored. This step also identifies columns 
or block units that are independent, i.e., those that are not updated by any other units. To automate 
this U t2 O d ?11 m r13 r14 123 ra, rn Figure 3: Partitions. process, it is necessary to classify the 
dependencies at the inter-block level. We have classified these depen­dencies into ten categories which 
are enumerated next. Using this classification and the interval tree structure, the partitioned computes 
the dependencies efficiently. The implementation details are given elsewhere. In the following discussion, 
a column is represented by its column number in the matrix, a rectangle is rep resented by its column 
extent (c~, cj ), C~ ~ cj and row extent (?P, rg ), rp S rg, and a triangle is represented by its row 
extent (or column extent, which is the same as the row extent) (ri, rj ), n < rj. 1. A column updates 
a column This forms the base case for the dependencies. A column k updates a column j if Lj, k is non-zero. 
(see Figure 1).  2. A column updates a triangle Let triangle 2 s row extent be (rl, r~). A column k, 
k < rl, updates the triangle if Li,k is non-zero, rl ~ i ~ ra. In Figure 4(a), the non-zero elements 
of column k that are involved in the update are in rows il, i2 and ;3. The points of intersection of 
the dotted lines with each other and of the dotted lines with the diagonal are the points of triangle 
T that are updated by column k.  3. A column updates a rectangle  Let rectangle R s column extent be 
(cl, c2) and row extent be (rl, r~). A column k updates this rectangle if it has pairs of non-zero elements 
~~, k andLj,k, wherec1< i<csandT1< j<ra.In Figure 4(b), the non-zero elements in rows il and iz combine 
with the non-zero elements in rows k rl il ......  T . . . . ...{... \ C4 i3 . .;..l .... ?2 ,, T 
h\ k%! \c1 &#38;? (c)   :;.&#38;j ....k r\ .. k ....... @ R :: :: r3 8.... r4 H h \ C3C4 (d) (e) @(f) 
\ R ?1 c1 d: ..... ?3 rl  ;-g..: R1 : .2 kEd ....... . rz @ :ii &#38; T ::! d ,, ::: r6 &#38;..-:&#38;; 
 \: T\ L C5C6 00 (i) Figure 4: Dependencies 870 i! i -d h to Update a Portion of R ThiM updated portion 
is the set of points given by the intersection of the dotted lines in R s interior. 4. A triangle updates 
a rectangle Let the column extent of rectangle R be (cl, C2) and the column extent of triangle T be (c3, 
c4). The triangle updatea the rectangle if there is an intersection in their column extents. In Fig­ure 
4(c), the shaded portion of T updates the shaded portion of R. 5. A triangle and a rectangle update 
a rectangle Let rectangle RI have column extent (cl, C2) and row extent (rl, rz) and let rectangle R2 
have column extent (c3, c4) and row extent (ra, r4). Let C2 < C3. Let the column extent of trian­gle 
T be (c5, cG). T combines with RI to up date R2 if (cl, CZ) intersects (CS,cc,), (c3, c4) in­tersects 
(c5, cG) and (rl, rz) intersects (rs, r4). In Figure 4(d), the shaded rectangular portion of T combines 
with the entire Ghaded rectangle RI to update the entire Ghaded rectangle Rz. 6. A rectangle updates 
a column Let the row extent of rectangle R be (rl, rz). It update6 a column k if rl ~ k < rz. In Figure 
4(e), the shaded portion of the rectangle between rows k and r2 update the column elements between rows 
k and rz.  7. Two rectangles update a column Let rectangle Rl have column extent (cl, CZ) and row extent 
(rl, rz) and let rectangle Rz have col­umn extent (c3, C4) and row extent (r3, rq). Let rz < rs, Then 
R1 combines with R2 to update a column k if rl s k s rz and (cl, CZ) intersects (c3, c4). In Figure 4(f), 
the elements of R1 which are in the row k between the vertical dotted lines combine with the entire shaded 
rectangle Rz to update the elements between rows r3 and rq in column k.  8. A rectangle updates a triangle 
 Let the row extent of rectangle R1 be (rl, rz) and the row extent of triangle T be (r3, r4). The rectangle 
updates the triangle if (rl, r2) intersects (r~, r4). In Figure 4(g), the shaded portion of R updates 
the shaded portion of T.  9. Two rectangles update a triangle  Let rectangle RI have column extent 
(cl, C2) and row extent (rl, rz ) and let rectangle R2 have col­umn extent (C3, C4) and row extent (r3, 
r4). Let ra < r3. Let the row extent of triangle T be (r~, r~). Then RI combines with Rz to update T 
if (cl, C2) intersects (C3, CJ and (rl, rz) inter­sects (rs, re) and (rat rq) hk3eCtS (rtj, r6). In Figure 
4(h), the Ghaded portion of Rl combines with the entire shaded rectangle Rz to update the 6haded rectanguhu 
portion of T. 10.Two rectandea update a rectamde Let rectangle R1 have column extent (cl, C2) and row 
extent (rl, rz ), rectangle R2 have column ex­tent (C3, c4) and row extent (rs, r4) and rectan­gle R3 
have column extent (es, c!3) and row ex­tent (? s, ? 6). Let ? Z < rs, rz < rs and C4 < ctj. Then R1 
comblnea with Rz to update R3 if (cl, CZ) intersects (c3, c4) and (rs, r4) intersects (? s, rG) and (rl, 
rz) intersect (CS,ce), In Figure 4(i), the Ghaded portion of R1 combines with the shaded portion of Rz 
to update the shaded part of R3. 3.4 Scheduling The scheduling process is Gplit up into two parts: allocating 
unit blocks to processor and ordering the computational work within each processor. In this paper, we 
are concerned with the first part only and the sdent points therein are presented next. First the independent 
columns, ss identified in the previous step, are allocated to processors in a wrap around fashion. The 
remaining clusterG are scanned again from left to right. If a cluster is a dependent column, the entire 
column is allocated to a processor, which is arbitrarily picked from the set of processors which worked 
on the column s predecessors. If the cluster is not a column, the unit blocks in the trian­gular part 
are allocated to processors, followed by the unit blocks in each rectangular block, going top to bot­tom. 
For example, in the cluster shown in Figure 3, the six sub-blocks of the triangle would be allocated 
first, followed by the four sukblocks of the rectangle below it, finishing up with the three sub-blocks 
of the bottom-most rectangle. Allocation within a triangle proceeds by first allo­cating the triangular 
units from top to bottom, fol­lowed by the rectangular units, going top to bottom and left to right. 
In the Figure 3 for instance, the sub­blocks in the triangle would be allocated in the order tl, t3, 
t&#38; t2,t4,t6. A global set of all processors, P9, is maintained, with a marker pointing to the first 
available processor. This marker cycles through the global set in a round-robin fashion and is moved 
up every time a unit block is allocated to the currently available processor. Apart from this, a set 
of proces­sors, P~, which have been already allocatedto some subblock in the triangle is maintained. 
Initially, Pa is empty. The strategy for allocating a processor to a unit rectangle or unit triangle 
is the same. First, the predecessors of the unit block are scanned. For each predecessor, if the processor 
p which worked on it is not in P., the unit block is allocated to p and p is added to Pa. If all of the 
processors whkh worked on all the predecessors of the unit block are already in Pa, the unit block is 
allocated to the currently avail­able processor in P~ and the marker is moved up to the next processor 
in P~. For allocating the units within a rectangle below the triangle, the choice of processors is restricted 
to P~, where P~ is the set of processors to whkh the unit blocks in the triangle are allocated. Since 
there is a large amount of communication between a triangle and the rectangles below it, this strategy 
helps in re­ducing the communication. First, the processors in set Pt are ordered according to increasing 
work. Go­ing in round-robin faahlon through Pt, the processors are assigned to the unit blocks in the 
rectangle, going top to bottom and left to right within the rectangle. For example, let processors pl, 
~ and ~ be assigned to the to the unit blocks on the triangle in Figure 3. Assume that the ordering according 
to work is such that pl < ~ < ~. Then, in the first rectangle below the triangle, rll is allocated to 
pl, r12 is allocated to PZ, rls is allocated to p3, r14 is allocated to P1. The set Pt is sorted again 
and the above strategy is used to allocate r21, r22 and r23, Performance In thh section we present results 
on the perfor­mance of the above described partitioned and sched­uler, in terms of the quality of partitioning 
and allocw tion that it produces. To quantify the results, we mea­sure the communication overhead in 
terms of the total data traflic generated and the load balance in terms of a factor that measures the 
deviation from perfect load balance. We also compare the results with those using the straightforward 
column wrap assignment scheme. For this purpose, we have used some of the representa­tive test matrices 
from the Harwell-Boeing package [4]. These test matrices were partitioned and the work units were scheduled 
as described in the previous sec­tion. Using this output, simulations were carried out to get the performance 
results presented here. For all the results presented in this section, the teat matrices were ordered 
using Liu s modified multiple minimum degree ordering scheme [10]. We used some of the tools Application 
Description BUS1138 Symmetric structure of power system networks CANN1072 Symmetric pattern from Cannes, 
Lucien Marro DWT512 Symmetric submarine frame from Naval Ship R&#38;D Center LAP30 Symmetric matrix representing 
9-point dlscretization of the Laplacian on the unit square with Diricldet boundary conditions LSHP1OO9 
Symmetric matrix from Alan George s LSHAPE problems Table 1: Selected Harwell-Boeing test matrices. from 
SPARSKIT [13] and the Wisconsin Sparse Ma­trix Manipulation System [1] for converting the test matrices 
into various formats, and for ordering and symbolically factoring the matrices. Tables 1 and 2 describe 
the Harwell-Boeing test matrices which were used in our experiments. Appl. Number of Number of Number 
of equations non-zeros non-zeros in factor BUS 1138 2596 3304 CANN 1072 6758 20512 DWT 512 2007 3786 
LAP 900 4322 16697 LSHP 1009 3937 18268 Table 2: Harwell-Boeing matrices sizes. In the following, we 
first quantify the communica­tion and work load distribution aspects of the parti­tioning schemes. Note 
that here we are concerned with the quality of the partitioned/scheduler in distributing the work among 
the processors and hence do not take into account data dependency delays. In practice, the total execution 
may be ailected by the dependency de­lays as well. However, if the number of processors is relatively 
small compared to the number of schedula­ble units, then the allocation scheme described here provides 
enough parallelism to keep the idle time to a minimum. The communication cost is parameterized by the 
to­tal data traffic generated in the system and the mean data traffic per processor. The data traflic 
is defined as a count of all the non-local data accesses. Accessing a single non-local element constitutes 
a unit data traf­fic irrespective of the location from where it is fetched. Once a data element is fetched, 
that element is stored locally and subsequent usage of that element in the local computations does not 
add to the data trafEc. The total data trfic in the system is the sum of the data accesses by all the 
processors in the system. This figure represents the volume of the data that must be transmitted by the 
system during the entire factoriza­tion step. The work load distribution of a partitioning scheme is 
characterized as follows. The computation cost of updating an element of the matrix by a pair of off­diagonal 
elements is assumed to be two units; updat­ing the element by the diagonal element is assumed to cost 
one unit. The computational work assigned to a processor is the sum of the computation costs of all the 
elements assigned to that processor. The quality of the work load distribution for a partitioning scheme 
is measured in terms of the load imbalance resulting from the assignment of the work to the processors. 
The load imbalance factop is defined as, ,= (~~.. ;:e)*N, where WtOt is the total work, N is the number 
of pro­cessors, WaW~ = W:O: /iV is the average work and Wm= is the maximum work assigned to any processor. 
Note that when the load is perfectly distributed, Wma is Wave and J is zero. Table 3 gives the communication 
traflic in the block scheme for two cases respectively. when the grain siEe is 4 and when the grain size 
is 25. Recall that the grain size is the minimum number of elements in any triangular or rectangular 
partition. In both cases, t­tal communication increases with the number of pr­cessors for all the test 
problems. However, when the grain size is increased from 4 to 25, there is a signif­icant reduction in 
communication. For instance, in the LAP30 problem, the g = 4 and g = 25 columns for total communication 
in table 3 show that there is more than 50% reduction in the total communication forp=16andp= 32. Thu 
is due to the fact that as the block size increases, more work ia done in each block with a lot of re-use 
of data. Table 4 describes the work distribution in the block scheme for grain sires 4 and 25. In contrast 
to the re­duction in communication with higher grain size, in most cases, there is an increase in load 
imbalance, as shown in Table 4. Furthermore, the load imbalance factor A increases, in general, with 
the number of pro­cessors, as well. Appl. P Tc al Mean g=4 g=25 4 1335 1194 - BUS 16 1818 1567 114 I 
98 32 1910 1649 60 103 4 47545 40716 11886 10179 CANN 16 138453 80334 -t8653 5021 32 171965 89042 5374 
2783 4 5336 3768 1334 942 DWT 16 10328 5482 + 645 342 32 4 11305 38424 5950 29382 %r-L%- LAP 16 100012 
44738 6251 2796 32 113717 48863 3554 1527 4 42044 29899 10511 7475 LSHP 16 106973 57773 -t­6686 3611 
32 127612 60243 Table 3: Block mapping communication. Overall, the larger the grain size, the smaller 
is the communication, at the cost of larger load imbalance. If the application is run on a system with 
high com­munication cost as compared to computation cost, the block-based partitioning can give good 
performance i.e. the savings in communication will be more than offset the disadvantage of load imbalance. 
Also, the load balance can be improved by using more sophisti­cated strategies to allocate blocks to 
processors. Apart from grain size, another parameter used in the teats was the minimum cluster width. 
For in­stance, if the minimum cluster width is 4, no strip of columns less than four columns wide is 
acceptable as a cluster -it is broken up into individual columns. The larger the minimum width acceptable, 
the fewer num­ber of non-single-column clusters there are. For any problem, if the cluster width is set 
high enough, we end up with all single columns. The results of table 3 and table 4 were obtained using 
a minimum cluster width of four. Table 5 shows the variation of communication and load distribution with 
minimum cluster width for LAP30. The table shows an increase in communi­cation when the width goes from 
2 to 4 and then a decrease when the width goes to 8. Load imbalance shows a complementary behavior. It 
decreases when the width goes from 2 from 4 and then increases when the width goes from 4 to 8. The cluster 
width has to go in step with the grain size. If the cluster width is too small compared to the grain 
size, a large number Appl. Proca. Work Distribution Appl. -F Communication Work str. Total Mean Mean 
A T 0 0 11164 o BUS 4 2485 621 2791 0.02 BUS 16 698 3.59 3.59 16 3705 231 698 0.12 32 349 6.3 6.3 32 
3832 120 349 0.35 4 151460 0.07 0.122 T 0 0 605840 0  CANN 16 37865 0.13 0.62 CANN 4 52363 13090 151460 
0.01 32 18932 0.38 1.26 16 171764 10735 37865 0.05 4 11701 0,17 0.18 32 239646 7489 18932 0.14 DWT 16 
2925 1.14 1.37 T 0 0 46804 0 32 1462 1.48 3.67 DWT 4 7599 1900 11701 0.02 4 108644 0.12 0.16 16 17867 
1117 2925 0.26 LAP 16 27161 0.13 1.13 32 20990 656 1462 0.32 $ * 2.9 T 0 0 4345770 0.24 LAP 4 42663 
10665 108644 0.01 0.74 16 133720 8357 27161 0.06 2.04 32 177625 5551 13580 0.11 -i-0 0 501570 0 LSHP 
4 46347 11586 125392 0.01 Table4: Block mapping work distribution. 16 146322 9145 31348 0.09 32 192977 
6031 15674 0.24 of skinny clusters would be formed towards the left of the matrix. The blocks would 
not have enough matrix Table 6: Wrap mapping. elements to take advantage of reduction in communi­ cation 
offered by the large grain size. communication in going from wrap mapping to the block scheme while 
th~ load imbalance factor goes Width P Communication Work Distr. from 0.14 to 0.38, whereas when the 
grain size is 25, Total Mean Mean I A the savings in communication over wrapmapping is 4 38936 9734 108644 
I 0.03 I 63% while the load imbalance factor goes from 0.14 to 2 16 96235 6015 27161 0.167 1.26. 32 111519 
3485 13580 0.54 4 38424 9606 108644 0.12 5 Conclusions 4 16 100012   II I 32 113717 11 1 In this paper, 
we have described a block based, au­ 4 32569 tomatic partitioning and scheduling scheme for factor­ 
8 16 88408 5526 27161 1.35 ing sparse matrices on message passing systems, The 32 101725 3179 13580 2.3 
primary focus is towards automating the process so that the tedious task of manual parallelization is 
kept Table 5: Variation with width for LAP30, g = 4. to a minimum. The partitioned makes use of data 
lo­cality to reduce communication overhead and at the III IIII Table 6 presents the results for the 
wrap-mapping same time attempts to provide the necessary flexibil­case. The immediately noticeable property 
is the con-ity to the scheduler in manipulating the work alloca­sistently uniform load distribution, 
as seen by the A tion so that the load remains balanced. We have used column. However, a smaller grain 
size in the block the example of Cholesky factorization to describe the scheme gives a tw-fold advantage 
of decrease in com-methodology. However, it can very easily be adopted munication without too much load 
imbalance as com-to other factoring methods used in sparse matrix com­ pared to wrapmapping. For instance, 
consider the putations. In fact, it can be generalized to compu-CANN1072 problem with 32 processors. 
For a grain tations that can be represented as directed acyclic size of four, the block case provides 
a 2870 saving in graphs with sufficient information prior to performing the computations. To analyze 
the effects on the performance of the partitioning and scheduling technique used, we have compared the 
communication overhead in the form of total data traflic with the same in an implemen­tation where a 
straightforward column wrap scheme is used. Five representative test matrices from the Harwell-Boeing 
package were used for this purpose. The comparison shows that the block-based scheme results in a significant 
reduction in the communication overhead as compared to the wrap-mapping scheme. This is in agreement 
with our motivation for blocking. On the other hand, the block method results in more load imbalance. 
Wrap-mappings usually lead to pr­cessors communicating with a large number of other processors leading 
to a large amount of data traflic and possibly to hot-spots. However, in block-based schemes, most of 
the communication among blocks occur within a cluster and hence can mostly be con­fined to small groups 
of processors. Although the in­creased load imbalance is a serious issue, the provision of the parameters 
such as the grain size and the cluster widths allows one to minimize the load imbalance for particular 
applications. Further study of the structure of the sparse matrices is required to optimize these pa­rameters 
for individual applications. Moreover, in real applications factoring is only a part of the overall ­lution 
of the system and other computations such as triangular solves can provide additional flexibility in 
the balancing the load which is not taken into account here. Finally, more sophisticated scheduling strate­gies 
could be used to improve performance. Thus, for systems such as message passing architectures, where 
communication overhead is much more expensive than computation, automated, block-based methods such as 
the one described here may prove to be better al­ternatives. References [1] F. L. Alvarado, The Sparse 
Matrix Manipulation System Users Manual. Technical Report, Univer­sity of Wisconsin, Madison, 1990. [2] 
E. Anderson and Y. Saad, Solving Sparse fii­angular Linear Systems on Parallel Computers. CSRD Report 
No. 794, Center for Supercomput­ing Development, University of Illinois, 1988. [3] I. S. Duff, A. M. 
Erisman, and J. K. Reid, Di­rect Methods for Sparse Matric~. Oxford Science Publications, Clarendon Press, 
1986. [4] I. S. Duff, R. Grimes, and J. Lewis, Sparse Matrix Test Problems. ACM ~ansactions on Mathemat­ical 
Software, Vol. 15, No. 1, pp. 1-14, 1989. [5] G. Fox, M. Johnson, G. Lyzenga, S. Otto, J. Salmon, and 
D. Walker, Solving Problems on Concurrent Processors: Vol. 1 -General Tech­niques and Regular Problems. 
Prentice Hall, 1988. [6] G. A. Geist and E. Ng, Task Scheduling for Par­allel Sparse Cholesky Factorization. 
Int. Journal of Parallel Programming, Vol. 18, pp. 291-314, 1989. [7] A. George and J. W. Liu, Computer 
Solution of Large Sparse Positive Definite Systems. Prentice-Hall, 1981. [8] A. George, M. Heath, J. 
W. Liu, and E. Ng, So­lution of Sparse Positive Definite Systems on a Hypercube. Journal of Computational 
and Ap­plied Math., Vol. 27, pp. 129-156, 1989. [9] G. H. Golub and C. F. Van Loan, Matrix Com­putations. 
The Johns Hopkins University Press, 1983. [10] J. W. H. Liu, Modification of Minimum Degree by Multiple 
Elimination. ACM Transactions on Mathematical Software, Vol. 11, 1985, pp. 141­ 153. [11] V. Naik and 
M. Patrick, Data TrafTic Reduc­tion Schemes for Cholesky Factorization on Asyn­chronous Multiprocessor 
Systems. Proceedings of the 1989 International Conference on Supercom­puting, ACM, Crete, Greece, 1989. 
Also available as IBM Research Report RC 14500, 1989. [12] R. Schreiber and J. J. Dongarra, Automatic 
Blocking of Nested Loops. Technical Report CS­90-108, Computer Science Department, Univer­sit y of Tennessee, 
1990. [13] Y. Saad, SPARSKIT a Basic Tool Kit for Sparse Matrix Computations. Technical Report 90-20, 
RIACS, NASA Ames Research Center, 1990. [14] P. Sadayappan and S. K. Rae, Communication Reduction for 
Distributed Sparse Matrix Factor­ization on a Processor Mesh. Proceedings of Su­percomputing 89, pp. 
371-379, 1989. 
			