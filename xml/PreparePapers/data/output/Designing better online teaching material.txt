
 Designing Better Online Teaching Material Judy Brown and Jiayun Lu School of Mathematical and Computing 
Sciences Victoria University of Wellington Wellington, New Zealand Judy.Brown@mcs.vuw.ac.nz Abstract 
The creation of excellent online teaching material is challenging because it requires that designers 
are able to apply learning theories and usability principles. In this paper we describe a web-based tutorial 
we developed to teach database students about SQL-like operators that can be used to access data in data 
warehouses (very large collections of data used by analysts). This paper describes the processes and 
methods used to develop the tutorial and the techniques we used to test prototypes of our tutorial. We 
show how ideas from user-centered design and learning theory can be usefully combined to create a new 
process for developing online teaching material that meets learning and usability aims. 1 Motivation 
. Computer based instruction is a useful technique to support learning expanding the time, pace and place 
of education. Computer based instruction can come in many forms but today many people are exploring the 
use of the Internet to deploy instructional material. Computer­related subjects are emerging as the the 
first to bring learning online to students and it is important we have methods for developing good teaching 
material. In this paper we describe an online tutorial on data warehousing operators that we developed 
that is rated highly by its users. It is easy to follow (4.5 on a 5 point Likert scale), easy to navigate 
(4), provides useful feedback (4.5) and helps users learn about data warehousing operators in an enjoyable 
and engaging manner. We believe the reason for our success was because of a creative and practical approach 
to using published processes and methods, varied and careful learning and usability tests, and a commitment 
to correcting all learning and usability problems that we identified while testing prototypes of the 
tutorial. The next section of this paper briefly describes the tutorial we developed. Following this 
we describe how we tailored and then combined two processes (one from the learning literature and one 
from the field of human­computer interaction) to create a new process we call the Learner-Centered design 
process. We used this process to guide the development of our tutorial. We show how we fleshed out our 
new process with useful methods. We describe how this new process helped us to meet our usability and 
learning objectives.  2 The Data Warehousing Tutorial The tutorial we developed is being used in an 
advanced database course to supplement a series of lectures on data warehousing operators (data manipulation 
commands). Data warehousing operators help analysts extract the information they need from a data warehouse. 
Our tutorial allows students to review their knowledge of data warehouses, learning about data warehousing 
operators, see examples of the operators in use, and try the operators for themselves The tutorial has 
three main sections. The overall structure of the tutorial is shown in Figure 1 in the frame on the left 
hand side of the screen dump. The first section describes relevant Basic Concepts : We review data warehousing 
fundamentals and describe a sample data warehouse that we use throughout the tutorial. The Data Warehousing 
Operators section, presents five different data warehousing operators that have been described in the 
literature (RollUp, DrillDown, etc.). Each Data Warehousing Operator subsection is further divided into 
four parts. The parts for the Rollup Permission to make digital or hard copies of all or part of this 
work for personal or classroom use is granted without fee provided that copies are not made or distributed 
for profit or commercial advantage and that copies bear this notice and the full citation on the first 
page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior 
specific permission and/or a fee. SIGCSE 2001 2/01 Charlotte, NC USA Copyright 2000 ACM 1-58113-329-4/01/0002 
... $5.00  Figure 1: The "Try it Yourself " subsection of the Rollup Operator operator are shown in 
Figure 1. The first part Introduction describes an operator. The second part Examples shows how the operator 
can be used to extract data from our data warehouse. The third part Short Questions provides the user 
with an opportunity to test their knowledge of an operator and the fourth part Try it Yourself (shown 
in Figure 1) allows users to extract information from our sample data warehouse by constructing queries 
using the data warehousing operators. The Conclusion section relates previous material to each other 
and brings the tutorial to a close. The tutorial is highly interactive. It is available online at: http://www.mcs.vuw.ac.nz/~judyb/cgi-bin/tutorial.pl. 
 3 Processes for Developing Instructional Tools When designing an instructional tool, usability is important 
but achieving good learning outcomes is an even more important goal. Nelson, Whitener and Philcox [5] 
proposed a process for the development of training materials. Steps include the assessment of training 
needs, the analysis of training requirements, and the design, development, implementation and evaluation 
of training materials. This process supports the building of good instructional tools because it encourages 
designers to consider often-neglected issues and organises the designer s work. Nelson et al s process 
has 19 substeps in all. Nelson s design process is iterative. Any problems found at the evaluation step 
will cause revisions at previous steps. Usually after several iterations, the training designers have 
verified they have developed material that produces good training outcomes. This training design process 
was developed for large training projects (online training material that may take years to complete). 
Because the data warehousing tutorial is a relatively small project, only some of these steps were essential. 
These were: . identifying training needs in the assessment step; . describing the trainee population 
and selecting tasks for training in the analysis step; . developing training materials in the development 
step; . and evaluating learner reaction and achievement in the evaluation step. Because usability is 
also an issue in the design of any software product with a substantial user interface we also looked 
at user-centred processes to guide our work. Figure 2: The Learner Centred Design Process Nielsen s 
[6] discount usability process is a scaled down version of the full-blown usability engineering process. 
The discount process is used for simpler applications or for applications where project resources are 
scarce. The steps in the discount process are: Know your Users , Protoype, Design and Modify , Perform 
a Heuristic Evaluation of the Prototype and finally Perform User Testing . To build the data warehousing 
tutorial, the essential steps from the training design process (described previously) were combined with 
the discount usability engineering design process in to create a blended process in Figure 2. The blended 
process allows discount usability engineering to be deployed appropriately slanted towards the development 
of instructional material. Like Lowgren [3] we believe that processes and methods are tools for developing 
the designer s abilities and that the choice for a particular process or tool can never be made in a 
general way; instead, it must always be related to the situation at hand and the people involved . We 
now describes the instructional steps in the blended Learner-Centered design process (Figure 2) in more 
detail and describe how we fleshed out the process using Lowgren s principle and other methods in learning 
and HCI. 4 Know Your Users Before designing an online tutorial, it is necessary to identify training 
needs (i.e. your objectives), identify the users (describe the trainee population) and clarify the training 
tasks (select tasks that will meet your training needs). We found Ostoff and Ford as described in [5] 
helpful here. 5 Analyse Competitive Systems Before we started to design our tutorial, we spent some 
time analyzing other online tutorials as suggested in Neilsen s full-blown usability engineering process. 
Because no other tutorials on data warehousing operators appeared to exist we studied SQL tutorials instead. 
We considered the following issues: interface design virtues and flaws, functionality, organization techniques, 
and style (language and the use of interactivity) The analysis of online SQL tutorials provided us with 
some good examples of effective tutorials and motivation for avoiding problems we observed. 6 Design 
and Develop Teaching Material We developed prototypes of our tutorial and tested our ideas with experts 
or end-users. Our prototypes (there were a series of about six) evolved into our final product. Rather 
than designing prototypes by instinct we followed Carroll and Rosson s [1] six principles for designing 
training materials. The paper now addresses each of Carroll s principles in turn and describes how we 
applied these principles while developing our tutorial. 1. Learn in the Context of Real Work Although 
we didn t develop a task-oriented tutorial and couldn t give users access to a real data warehouse and 
data warehousing engine to process queries, we still tried to adhere to principle 1 as best we could 
by simulating a sales data warehouse and a data warehousing engine. The data warehouse engine lets users 
practice what they have learned about the operators. 2. Provide Additional Educational Artifacts  We 
created a glossary of data warehousing terminology. We added links to the definitions wherever specialised 
terms appeared in the tutorial. 3. Support Users to Apply Prior Knowledge We introduced data warehouses 
by comparing them with traditional database systems that are familiar to our users. When we created example 
queries to demonstrate the new operators, we also showed how the same query could be constructed using 
standard SQL. 4. Exploit Errors as Learning Opportunities  We followed Lewis and Norman's [2] principles 
when designing for errors. We tried to minimise the incidence of error, maximise the discovery of the 
error, and make it easier to recover from an error. We also went beyond this to treat errors as learning 
opportunities for our users. We carefully designed the error messages from our data warehousing simulator 
to help users identify problems in their constructed queries, and to help them construct a correct query. 
For instance, if a user creates a syntactically incorrect query we explain why an error has been made 
in our feedback by providing information about the correct syntax for a query. 5. Support Users to Make 
Progress Quickly We used plain language throughout the tutorial making our tutorial more readable and 
reducing the cognitive load on the user. We also tried to create a tutorial where navigation was easy 
so that users could focus on their learning task. We included basic material on data warehousing for 
users who need to review. 6. Ensure Learning Tasks Require Reasoning and Improvising  We tried to make 
our tutorial engaging by making it interactive as it achieves better learning results (Romiszowski [8]). 
Interactivity, done well, supports the user s learning by providing useful feedback in response to user 
s actions. This allows users to stay focused on their learning (Romiszowski [7]). We also found that 
providing the right sort of interactivity at the right time was important because knowledge is acquired 
in steps (e.g. we acquire the ability to make simple judgements about an area of knowledge before we 
are able to solve problems in that area). For this reason, we designed our tutorial to require users 
to make simple judgements about data warehousing operators before we challenged them to use the operators 
to extract data. 7 Evaluate Teaching Material The evaluation of teaching material is an important step 
in the learner-centered design process. Usability inspections require that experts evaluate the teaching 
material. User testing requires that real end-users try the developed system. With either type of evaluation 
the outcome is analysed and if the results are not satisfactory, revisions are made. 7.1 Evaluating Learning 
The evaluation of learning is very challenging. Inspired by Scriven (as described in [1]) we applied 
some basic techniques to evaluate learning. During user testing we asked users to think aloud. This helped 
us to assess our user s understanding of data warehousing operators. Also during user testing we measured 
the time to complete a task (such as answering a question by constructing a query) and error rates (the 
number of attempts made before a correct query was constructed). Short completion times and low error 
rates suggest that learning has occurred. After user testing we evaluated learning by asking our users 
to complete a short test. Correct answers are indicative that learning has occurred. As well as evaluating 
learning by user testing we also tried to critically evaluate the features we had designed from a learning 
expert s perspective. This required asking a learning expert to evaluate the tutorial and the rationale 
for the features we provided. 7.2 Evaluating Usability The goal of usability evaluation tests is to 
assess if a system is easy to learn, easy to use, contains the right functions and is liked by users 
[6]. Varied testing methods are best because individual methods tend to identify subclasses of usability 
problems. 7.2.1 Usability Inspection We performed usability inspections (evaluation by an expert evaluator) 
after tutorial development but before user testing. After we finished (or substantially redesigned) a 
part of the tutorial an HCI, English language, and learning expert evaluated the web pages. The purpose 
was to find usability problems, grammatical errors and learnability problems (does the tutorial provide 
a less than optimal learning environment?). We used Nielsen s usability heuristics to find usability 
problems. We also checked our tutorial against Nielsen s web design guidelines [10]. Near the end of 
the development of the tutorial we had two data warehousing experts (one from academia, one from industry) 
provide feedback on the content. In all we had four expert evaluators inspecting our prototypes, with 
some evaluators evaluating multiple times. Hundreds of usability problems were identified and removed. 
 7.2.2 User Testing The purpose of user testing is to identify usability problems in the interface design 
by recruiting end-users to test the product. We applied two kinds of usability tests in our tutorial 
design: Rubin s [9] exploratory test and his assessment test. The main difference between the tests is 
the maturity of the prototype used in the test and the type of design flaws that are sought. Exploratory 
tests can result in major changes to a prototype. Assessment tests fine tune a prototype. To slant these 
tests towards standard training evaluation tests, user s reactions, achievements (their performance while 
doing the tutorial) and learning outcomes (as determined by a post tutorial survey) were collected. We 
used the think aloud technique when conducting user tests. Extensive notes were made on usability problems 
encountered by users (navigation difficlties etc). A post­test questionnaire was also used. We tested 
eight users in all, four in each round of testing. Our test participants were diverse but representative 
of our envisioned users. In all, we conducted twelve hours of usability testing which found 30 usability 
problems in round one, and 11 usability problems in round two. In both rounds 70% of problems fell into 
one of three categories: navigational, wording or layout issues. These had straight-forward solutions. 
The other 30% were either organizational problems which required the design of new tutorial parts or 
were difficulties with the design of the input model in the Try it Yourself section. These problems 
required much more effort to solve. In all, solutions were designed for all but two problems. The percentage 
of correct answers was very useful for verifying whether the operator s descriptions were clear. The 
percentage of correct answers in the exploratory test was not satisfactory (50%). This test result forced 
the rewrite of some parts of the tutorial to make them clearer. As a result, the percentage of correct 
answers in the second test (the assessment test) was much better (67%). More importantly, the time spent 
working on questions and query construction dropped from 40.6 minutes to 21.37 minutes on average. These 
results indicate that it may be possible to improve learning by removing usability problems in teaching 
material. Feedback from the questionnaires showed that users enjoyed the tutorial and had understood 
the operators well. Written feedback was very positive. Participant 4 for instance, wrote [the tutorial] 
is easy to use, easy to understand and it supports learning very well. Complete test results can be found 
in Lu s report [4].  8 Summary Online teaching material that is attractive and usable is not sufficient. 
Successful teaching material is primarily determined by its learning outcomes. We have shown that attention 
to both learning and usability appears to improve learning. To produce a usable tutorial with good learning 
outcomes we blended an instructional design process with a usability design process creating a new process 
we called the Learner-Centered design process. To flesh out the process we researched and applied a number 
of methods (e.g. guidelines and testing techniques). As a result both usability issues and learning issues 
were given due consideration in the development of the tutorial. This case study serves as an example 
of how processes and methods can be applied when developing instructional tools. We think we have provided 
a good example of one way to use processes and methods effectively to create online resources for educational 
purposes.  Acknowledgements The authors would like to thank Gill Dobbie for her support and for the 
project idea, Tim Lawson for an expert review of the tutorial, and the eight participants in our usability 
tests. References [1] Carroll, J. and Rosson, M. Managing Evaluation Goals for Training CACM 38(7) (1995), 
40-48. [2] Lewis C. and Norman, D. Designing for Error Readings in Human-Computer Interaction: Toward 
the Year 2000, Morgan Kaufmann Publishers, Inc. (1995), 686­ 697. [3] Lowgren J. and Stolterman, E. Design 
Methodology and Design Practice Interactions, Jan-Feb (1999),13-20. [4] Lu, J. A Web-based Tutorial for 
Learning about Data Warehousing Operators, unpublished master s report, Victoria University of Wellington, 
New Zealand, 2000. [5] Nelson,R., Whitener,E. and Philcox, H. The assessment of End-User Training Needs 
CACM 38(7) (1995), 27­ 39. [6] Nielsen, J. The Usability Engineering Lifecycle Usability Engineering, 
Academic Press professional (1993) [7] Romiszowski, A.J. Producing Instructional Systems, Lesson Planning 
for Individualized and Group Learning Activities, Kogan Page, London, Nichols Publishing (1988). [8] 
Romiszowski, A.J., Developing Auto-Instructional Materials, From Programmed Texts to CAL and Interactive 
Video, Kogan Page, London, Nichols Publishing (1988). [9] Rubin, J. Handbook of Usability Testing: How 
to Plan, Design, and Conduct Effective Tests, John Wiley &#38; Sons (1994). [10] Web page: Web Design 
Guidelines, http://www.sun.com/styleguide  
			