
 Estimating DNA Sequence Entropy J. Kevin Lanctot* Ming Li t En-hui Yang** University of Waterloo University 
of Waterloo University of Waterloo Abstract This paper presents the first entropy estimator for DNA sequences 
that has both proven properties and excel- lent entropy estimates. Additionally our algorithm, the Grammar 
Transform Analysis and Compression (GTAC) entropy estimator, uses a novel data structure to repeatedly 
solve the longest non-overlapping pattern problem in linear time. GTAC beats all known com- petitors 
in running time, in the low values of its entropy estimates, and in the properties that have been proven 
about it. Introduction With the complete DNA sequences of many organisms already known, and the goal 
of completely sequencing the human genome making steady progress, the chal- lenge of using this wealth 
of genetic information bene- ficially presents a new series of problems to be solved. One such problem 
is recognizing and characterizing the different regions of DNA and their different functions. Information 
theoretic entropy is one tool that can be used to help solve these types of problems. DNA stores the 
information for creating and orga- nizing an organism. It can be thought of as a string over the alphabet 
{A, C, G, T }, which represents the four chemical components that make it up. In order to synthesize 
protein, the cell creates a transient copy of a portion of DNA, called mRNA. The sequence of mRNA is 
read, three symbols at a time, and this triplet spec- ifies a single component of the protein called 
an amino acid. The components are strung together in the order their code appears in the mRNA sequence. 
Although made as a linear structure, many types of protein can ~ress: Dept. of Computer Science, University 
of Wa- terloo, Waterloo, Ont. N2L 3G1, Canada. F_,-maih jklanc- tot ~}neumann.uwat erloo.ca. tSupported 
in part by the NSERC Research Grant OGP0046506 and the Steacic Fellowship. Address: Dept. of Com- puter 
Science, University of Waterloo, Waterloo, Ont. N2L 3G1, Canada. F~mail: mli~}math.uwaterloo.ca. $Supported 
by NSERC Research Grant RGPIN203035-98. Address: Dept. of Electrical &#38; Computer Engineering, University 
of Waterloo, Waterloo, Ont. N2L 3G1, Canada. F-~-mail: ehyang@bbcr.uwat erloo.ca fold up on themselves 
and become active structures that control or influence a chemical reaction in the cell. No all of the 
sequence information that gets copied from the DNA ends up specifying a protein. In higher eukaryotes 
(such as plants and animals) much of the mRNA is cut out before the cell translates it into protein. 
The portions that are translated are called exons and the portions that are removed are called introns. 
Random changes in a sequence are thought to be more deleterious if they take place in an exon rather 
than in an intron so these two regions should have different information theoretic entropy. Since the 
alphabet of DNA, and mRNA have four symbols, if these sequences were totally random, it would take two 
bits per symbol to represent the se-quence. However, only a small fraction of DNA se-quences result in 
a viable organism, therefore there are constraints on those sequences which appear in a liv- ing organism. 
Common compression algorithms, such as Huffman, and various Lempel-Ziv based algorithms fail to compress 
DNA sequences at all (for an exam-ple using UNIX compress, see Table 2), and low order arithmetic encoding 
algorithms only compress to around 1.96 bits per base [4]. With only 61 of the 64 possible mRNA triplet 
combinations being used to code for pro- tein, that property alone suggests an encoding of 1.977 bits 
per symbol for exons, so the arithmetic encoding is providing only a slight improvement. 2 Previous Work 
There have been several previous attempts to charac- terize the entropy of DNA. One of the most common 
approaches is to estimate the probability of n-tuples for large n, and use this value to compute the 
block entropy. The problem with this approach is that it converges too slowly, so even though genome 
databases are large and growing larger, the values that are ob- tained systematically underestimate the 
entropy due to the finite sample effect, and must be corrected. Sev-eral researchers address this problem 
and have devel- oped methods to correct it, such as Lib et al. [10], and Schmitt and Herzel [13]. 410 
2.1 Biocompress Another approach is to compress the sequence in order to determine an upper bound on 
the entropy. Such a method has been used by Grumbach and Tahi with their two algorithms Biocompress [3] 
and Biocompress-2 [4]. Biocompress-2 is actually a combination of three approaches: 1) Literal encoding, 
where each symbol is coded as a two bit number; 2) Arithmetic encoding, where a substring of symbols 
is encoded using second order arithmetic encoding; 3) Lempel-Ziv style encoding, where a substring is 
encoded as a pair of integers, one representing the length of the match, and the second representing 
the position of the match somewhere to the left of the current position. Biocompress-2 checks to see 
which method is more efficient to encode a small portion of the input, and then encodes the type of encoding, 
the length, followed by the actual encoding. The entropy estimates provided by Biocompress-2 are presented 
in Table 2. 2.2 Match Length Entropy Estimator Farach et a/. [2] developed a novel algorithm to estimate 
the entropy of DNA sequences called a match length entropy estimator. Letting Li represent the match 
length at the ith character in a sequence, the value of Li is ,the length of the longest substring that 
occurs in two ~places: 1) starting at position i + 1, and 2) a sliding window consisting of the previous 
Nw characters. This algorithm was used to test the differences between the entropy of introns and exons, 
and contrary to what was expected, they found that the average entropy of exons was larger 73 % of the 
time and that the variability of ~2ntrons was larger 80% of the time. Farach et al. also proved that 
their algorithm was universal, that is, that ~he entropy estimate will approach the true entropy as the 
size of the sequence increases, but only under the assumption that the sequence is generated by a Markov 
process. 2.3 CDNA Loewenstem and Yianilos [11, 17] de- veloped CDNA, a program that estimates the entropy 
of DNA sequences. The motivation for CDNA comes from the observation that naturally occurring DNA se- 
quences contain many more near repeats then would be expected by chance. Two parameters that CDNA uses 
to capture the inexact matches are w, which represents the substring size, and h, which represents the 
Ham- ming distance. These parameters are used to create a panel of predictive experts Pw,h, each with 
differing val- ues of w and h. CDNA then learns the weightings of these various experts, using Expectation 
Maximization, so that their predictive ability is maximized when com- bined into a single prediction. 
CDNA has been implemented in two different ways. In the cross validation approach, the sequence is parti- 
tioned into 20 equal segments, the algorithm is trained on 19 of the segments and predicts the remaining 
seg- ment. This approach is repeated 20 times using a differ- ent segment as the test segment each time. 
The value reported is the average of the 20 iterations. While the cross validation estimate is used to 
remove the overhead associated with compression, a simple example will illustrate how this approach can 
severely underestimate the entropy of a genetic se-quence. Let the input be a tandem repeat, say rr, 
where r is a random genetic sequence and hence has an entropy of two bits per symbol. The cross validation 
approach will report an entropy of close to zero, yet the entropy of rr is one bit per base. How likely 
is such an example? Both tandem repeats, such as the one above, and dis- persed repeats are known to 
occur in DNA and comprise a substantial fraction of the human genome [9]. The second method that Loewenstern 
and Yianilos use is called CDNA compress, in which the algorithm uses everything to the left of a nucleotide 
as the learning set to predict its value. The average over all positions is calculated and that is the 
value that is recorded in Table 2. 2.4 Our Results In this paper, we develop a novel entropy estimator 
of DNA sequences, GTAC, which is based on the idea of Kieffer and Yang [7] regarding the design and analysis 
of grammar based codes, and which recognizes the reverse complement property of DNA sequences. Our entropy 
estimator is universal in the sense that it does not assume any source model and works for any individual 
sequence. Moreover, our entropy estimator is well justified from the information theoretic point of view. 
In Table 1, we compare GTAC with the best known entropy estimators using three criteria: Is the code 
universal with respect to any station- ary source? That is, will the entropy estimate con- verge to the 
actual entropy if the sequence is long enough? A limited one, such as the Match Length entropy estimator 
must make the addition assump- tion that the source is a Markov process. Is the run time linear?  How 
good are the algorithm's entropy estimates?  In summary, algorithms such as UNIX compress and Biocompress-2 
were designed as compression algo- rithms, so they tend to overestimate the entropy be- cause they include 
overhead necessary for compression. CDNA was designed as an entropy estimator, but no Table 1: Features 
of Various Entropy Estimators Linear Entropy Algorithm Universal Run Time Estimate UNIx compress yes 
yes worst Match Length limited yes Biocompress-2 yes yes 3rd best CDNA no no 2nd best GTAC yes yes best 
 convergence properties have been proven about it and its entropy estimates are inferior to GTAC's. 3 
Our Approach Before presenting our entropy estimator, we first briefly review grammar based codes, and 
then discuss how well the corresponding entropy estimators are. 3.1 Grammar Based Codes A context-free 
gram- mar (CFG) is a quadruple G --(V, T, P, S) where V is a finite non-empty set of variables, T is 
a finite non-empty set of terminal symbols that is disjoint from V, S is a distinguished element of V 
called the start symbol, and P is a set of production rules which map elements of V onto (V U T)*. Using 
the concept of CFG, Kieffer and Yang [7, 15, 16] recently put forth a new type of lossless source code 
called a grammar based code, and developed a new universal lossless source coding theory. In this theory, 
a grammar based code has the structure shown in Figure 1. The original data sequence x is first L transform 
grammar G x" [ coder I codeword Figure 1: Structure of a grammar-based code. transformed into a context-free 
grammar (or simply a grammar) G~ from which x can be fully recovered, and then compressed indirectly 
by using a zero order arithmetic code to compress G~. To get an appropriate Gx, string matching is often 
used in some manner. Since one has to fully recover x from G~, it is required that G~ satisfy the following 
constraints: The language generated by G~ consists of only z.  G~ is deterministic, that is, any variable 
in V appears only once on the left hand side of the production rules, P.  P does not contain the empty 
string on the right hand side of any rule.  * G~ has no useless symbols. That is, during the process 
of deriving ~ from the production rule cor- responding to the start symbol S, each production rule in 
G~ is used at lease once. Such a grammar is called an admissible grammar. Example 1: Below is an admissible 
grammar G~ with x = aataaatgcaatatatatgc. S -+ BADBCCD A -+ aa B --+ At C --~ at D ~ Cge Suppose that 
the original sequence x is converted into an admissible CFG G~ by a grammar transform. To compress x, 
the corresponding grammar based code then uses an zero order arithmetic code to compress G~ or its equivalent 
form. (The way to convert G~ into its equivalent form and to encode Gx or its equivalent form, is not 
important to us at this point. The reader can see [7, 15, 16] for details. What is important to us here, 
is the characterization of the resulting compression rate, which can lead to efficient entropy estimators.) 
To characterize the resulting compression rate, let w(G~) be the sequence obtained by concatenating the 
right hand side of all production rules of Gx in some order and then deleting the first appearance of 
each variable. Let (3.1) g(az) = ~--~n(s)log Iw(G~)}  $ n(s) where the summation is taken over all 
variables and terminal symbols in Gx, n(s) denotes the number of times the variable(or terminal symbol) 
s appears in w(Gx), and [w(Gz)[ denotes the length of the sequence w(G~). Also, in (3.1), the logarithm 
is relative to base 2, and the convention 0 log c~ = 0 is adopted. In terms of the terminology in [7], 
the quantity H(Gz) is called the unnormalized entropy of the grammar G~. For the CFG G~ shown in Example 
1, w(G~:) = BCDaaAtatCgc and H(G~) = 34.26. The following theorem, proved in [7, 15], characterizes the 
resulting compression rate. THEOREM 3.1. According to arithmetic coding or enu- merative coding, one 
can assign a uniquely decodable binary codeword B(Gx) to each admissible CFG Gx(or its equivalent form) 
such that (3.2) IB(G~)I= f(Gx) --k H(Gx) where IB(G,)[ denotes the length ofthe binary codeword B(Gz), 
and f(Gx) represents the overhead paid to the universality of grammar based codes. In (S.e), f(a,) is 
negligible as compared to H(Gz) and is upper bounded, in the worst case scenario, by f(G,) < 51G~ I + 
 where IG~Idenotes the total enties in the right hand side of all production rules of G,, and a is the 
cardinality of the source alphabet and is 4 in the case of DNA sequences. From Theorem 3.1, it follows 
that in order for a grammar based code to be efficient, the corresponding grammar transform z ~ G, should 
be designed so that the unnormalized entropy H(Gx) of Gx and the overhead f(Gx) are as small as possible. 
An interesting type of grammar transform called irreducible grammar transform has been identified in 
[7]. An admissible grammar Gx is said to be irreducible if the following properties hold: P1 Every variable 
of Gx other than the start symbol S appears at least twice in the right hand side of all production rules 
of Gz. P2 There is no repeated pattern of length > 2 in the right hand side of all production rules of 
G,. P3 Each distinct variable of Gz represents a distinct subtring of z. The admissible grammar G~ shown 
in Example 1 is irre- ducible. A grammar transform is said to be irreducible if it converts every sequence 
z into an irreducible grammar Gz. Several reduction rules have also been proposed in [7] to reduce any 
reducible grammar transforms into irreducible ones. Starting with the grammar consisting of only one 
production rule S -+ z and applying repeat- edly these reduction rules in different orders, one can design, 
in principle, many irreducible grammar trans- forms. Among them are the longest matching substring grammar 
transform [7], which will be analyzed and ex- tended to DNA sequences, and the Yang-Kieffer greedy sequential 
grammar transform [15], which gives rise to universal data compression algorithms significantly out- 
performing the Lempel-Ziv type of algorithms such as the Unix Compress and Gzip algorithms. Interestingly 
enough, no matter how one gets irreducible grammar transforms, they all give rise to efficient universal 
com- pression algorithms, as shown in the following theorem (see [7] and [8] for its proof). THEOREM 
3.2. For any sequence z, let G(z) be the set consisting of all irreducible grammars G representing z. 
Then the following hold. (a) There is a constant c, which depends only the cardinality of the source 
alphabet, such that for any sequence max IGI < clzl sea(x) --log Izl where Ixl denotes the length of 
z. (b) For any stationary, ergodie source {Xi}~°=l with entropy H, the quantity  max{llB(G)In }  HI:G6~(XI""X") 
 goes to 0 with probability one as n -+ oo. REMARK 1. Part (b) of Theorem 3.2 represents the worst case 
scenario. The actual convergence rate at which IB(Gx")I H ?%  where Gx. is an irreducible grammar 
representing X" --XI"'X,, goes to 0 depends on the source OO (X, }i=0 and the irreducible grammar transform 
X n --+ Gx.. Also, from Theorems 3.1 and 3.2, it follows that for any irreducible grammar transform z 
--+ G~, the normalized grammar entropy H(Gx.)/n goes to the actual entropy H with probability one as 
n --~ c~. 3.2 Grammar Based Codes Entropy Estimators As suggested by Theorem 3.2 and Remark 1, we can 
associate an entropy estimator with any grammar based code. Given a grammar based code with grammar transform 
x --+ G~, we define an entropy estimator which simply uses the normalized grammar entropy H(G~)/Iz I 
of G~ as an estimate to the entropy in bits per letter of z. The reason we do not include the normalized 
overhead f(G=)/Iz I in the definition of the associated entropy estimator is obvious--the inclusion generally 
results in an overestimate. When the underlying grammar transform satisfies Property P2 and is asymptotically 
compact in the sense that ]Gzl/Izl goes to 0 as Ixl --+ oo, some ergodic behavior of such entropy estimators 
has been investigated in [8]. In this paper, we focus on a more restricted case in which the underlying 
grammar transform is irreducible. In terms of the terminology in estimation theory, Theorem 3.1, Theorem 
3.2, and Remark 1 imply that whenever the underlying grammar transform is irre- ducible, the corresponding 
associated entropy estimator is consistent and asymptotically unbiased for the class of stationary, ergodic 
sources. Although it would be nice to determine the convergence rate at which the es- timate provided 
by such an entropy estimator goes to the actual entropy H, this problem is in general very difficult 
and thus left open for future research. Further- more, as indicated in Remark 1, the solution to this 
problem depends on the actual source model and the underlying irreducible grammar transform. In our case 
of DNA sequences, we have no idea what kind of source model DNA sequences would obey. To get around these 
difficulties, in the following, we shall take a different approach. Instead, we shall show that no matter 
what the underlying irreducible grammar transform and the source model are, with a high probability these 
entropy estimators will never severely underestimate the actual entropy. THEOREM 3.3. Let {Xi}i~x be 
any data source. Then for a constant d > O, the following holds with probability at least 1 - n-d: ]H(Gx.)[ 
>_ _l logP(Xn ) f(ax.) dlogn n n n n for any grammar transform x --~ Gz, where X n = Xx" "Xn and P(X") 
denotes the probability of X". P~MARK 2. From information theory, one can interpret (-logP(X"))/n as 
the entropy in bits per letter of X". From Theorems 3.1 and 3.2, it follows that for irreducible grammar 
transforms, f(Gx=)/n is quite small and upper bounded, in the worst case scenario, by O(1/logn). Therefore, 
Theorem 3.3 says that with a high probability, the entropy estimators associated with grammar based codes 
with irreducible grammar transforms will never severely underestimate the actual entropy. Proof of Theorem 
3.3: Let Fn denote the set consisting of all sequences z of length n for which there is a grammar transform 
x --+ Gz such that H(Gx) ___-log P(z) -f(G~) -dlogn or equivalently, log P(z) _< -H(Gx) -f(G~) -dlogn 
Then we have Vr{Xn 6 iv,) = ~ P(x) zEF~ ~-- Z 2-H(G')--](G=)--dl°gn x6F~ = n -d ~ 2 -]B(G')[ zeF~ (3.3) 
< n -d where the last inequality is due to the fact that the bi- nary codeword B(Gx) is uniquely decodable, 
and hence the Kraft inequality among all admissible grammars holds: 2 -[B(G)I _< 1 G is admissible From 
(3.3), Theorem 3.3 follows. Theorem 3.3 suggests that among all entropy esti- mators associated with 
grammar based codes with irre- ducible grammar transforms, the best one correspond to the irreducible 
grammar transform which gives the least grammar entropy H(Gx). However, we believe that finding the irreducible 
grammar with the least gram- mar entropy H(Gx) is an NP-hard problem. Instead, in the following section, 
we shall present a linear time algorithm for constructing an irreducible grammar with a good entropy 
estimate. 4 The Algorithm The Grammar Transform Analysis and Compression (or GTAC) is an example of a 
Grammar Based Code. The core of GTAC is to repeatedly solve the longest non-overlapping pattern (LNP) 
problem. The LNP problem is as follows: Given a set of strings, 7 ~, find the longest substring fi such 
that fl occurs in at least two non-overlapping positions somewhere in 7 ~. The LNP problems can appear 
in the context of grammar, G = (V, T, P, S), when we let ~ be the set of all right hand sides of the 
production rules P, and we add the additional constraint that the length offl is at least two. GTAC's 
goal is to repeatedly find the LNP and reduce it, creating a new rule. If an LNP fl appears in the following 
form (both in the same string), A-~ al*fl*~2 *fl*a3 rewrite the previous rule as two rules. A -+ al 
* B * a2 * B * a3  B --->/~ If an LNP/3 appears in different rules, A --~1 */~* a2 B --~ a3 /3, a4 
 then rewrite the previous rules and introduce a new one, as follows. A -+ cq * C * a 2 B ---~ 0~3 * 
C * a4 C~fi GTAC can optionally recognize reverse comple-ments. In DNA sequences, the symbols a and 
t are the complement of each other, and the symbols g and c are the complement each other. A string ~ 
is the reverse complement of/3 if j~r is the reverse of fl with each character complemented. For example, 
the reverse complement of aaacgt is acgttt. As Section 5 makes clear, the ability to detect reverse complements 
is an important feature of a DNA entropy estimator. The GTAC algorithm deals with reverse comple-ments 
by having two sets of non-terminals, regular ones A1,A2,... and reverse complement ones R1, R~, .... 
These non-terminals come into play as follows. Given an input, z, the algorithm first creates the trivial 
gram- mar S -+ x. Next GTAC finds the LNP. If there is two or more occurrences of~, create a rule Ai 
--+ ~ ignoring any occurrences of ~r. If there is only one occurrence of fl and one of if, then create 
a rule using one of the re- verse complement non-terminals, Ri --+ j3, which means interpret the second 
occurrence of the non-terminal in the right hand side of a rule as the reverse complement of the rule. 
For example, given the input aa$actgagtaaa, GTAC first creates the trivial grammar. S -+ aatactgatgaaa 
 Next GTAC finds the largest LNP, which is tact and its reverse complement agta. GTAC reduces this substring 
and creates a new rule S --+ aaRogRoaa Ro --+ tact Next, any remaining LNPs are rewritten, in this case 
there is one, an. S --~ AoRogRoAo Ro -+ tact Ao --~ aa Next, relabel the non-terminals in order of appearance 
with the reverse complement rules starting with symbol R0, and the normal rules with symbol A0. Ao -+ 
A1RogRoA1 Ro -~ tact A1 -+ aa The right hand sides of the rules are concatenated together in the following 
order: the start rule is first, followed by any reverse complement rules, followed by any normal rules 
and the first occurrence of each non- terminal is deleted. g RoA l aatact The entropy is then calculated 
based on the fre- quency of appearance of each symbol using Equa-tion 3.1. Trivial implementations of 
this algorithm requires O(n 3) time, where n is often in the order of a million or more. For this size, 
even an O(n 2) algorithm becomes intolerable. Since a key feature of this approach is to repeatedly look 
for the LNP, the generalized suffix tree (a suffix tree that contains more than one string) is a natural 
data structure to consider because it can find the LNP in time linear in the total length of the right 
hand side of the grammar. However, GTAC continually rewrites the rules, reducing the size of the grammar, 
so a key challenge is keeping the suffix tree up-to-date. Consider the following example where catactag 
is a substring in the input with cat, tact and tag appearing elsewhere in the input. When an LNP is discovered, 
in this case tact, rewriting it not only affects the LNP, it also affects any patterns that overlap with 
the LNP, such as cat and tag. If the algorithm finds an LNP that occurs n times, and is l characters 
long, then rewriting it can affect O(nl) other suffixes. Another complicating factor is that a generalized 
suffix tree directly reports the longest pattern (LP), but our algorithm requires the longest non-overlapping 
pattern, hence we must be able to obtain one from the other. The following lemma gives an algorithm that 
obtains this result, and provides a bound for the size of the LNP given the LP. L~.MMA 4.1. If the length 
of the LP is l, then an LNP can be found with length at least [l/2]. Proof. Let the reported LP start 
at positions k and k+i. Ifi > [I/2] we are done. If not, then the LP is a periodic string, and one can 
find the start of the second string at or beyond k+ [I/21 and it will be at least [l/2] characters long. 
[:3 The preceding lemma characterizes the situation when the LP occurs twice, and the next lemmais needed 
when the LP occurs three or more times. LEMMA 4.2. Given an LP that occurs three times in a string, at 
least two of those occurrences do not overlap with each other. Proof. Proof by contradiction. Assume 
that any two of the three substrings overlap with each other. Let the substrings start at k, k+i, and 
k+j with 0 < i < j < l, where l is the length of the LP. The LP is periodic, and one can use that property 
with the substring starting at k + j to show that the other two substrings match at k A- l and k -4- 
l A- i contradicting the fact that they are LPs. [] With these two lemmas a subroutine for dealing with 
overlapping matches can be outlined. If an LP has just two occurrences, check for overlap, and if necessary 
create non-overlapping substrings. Given three or more matches, that LP will also be an LNP, so keep 
it as is. 4.1 Observations The data structure for GTAC is a suffix tree, along with a copy of the original 
input, and an array of queues. In order to understand how they interrelate, a few observations are necessary 
first. OBSERVATION 1. Since GTAC always considers the longest pattern at each iteration, if it is currently 
looking at an LNP of length l, the longest branch in the tree is at most 21 long, reflecting the fact 
the tree may contain a length 2l LP that corresponds to a length l LNP. Hence the most number of characters 
that the algorithm will have to search down the tree before uniquely identifying a suffix is 21 + 1, 
because at that point it is guaranteed to be in a leaf. So if the LNP starts at position k, only suffixes 
in the range [k-2l, k+l-1] needs to be checked to see if they contains a pattern that overlaps the LNP. 
OBSERVATION 2. No rule will contain a non-terminM from a previous iteration. For example, if a substring 
fllfi2...fit is being replaced by a non-terminal Ai, then there will never be a pattern cAi or Aic, for 
some c, found later on in the grammar, because fllfl2...fit was the longest pattern at that point of 
time. Since an LNP will never straddle over the beginning of non- terminal after it's been introduced, 
the suffix can be considered to end at that point in the suffix tree. For example, if a path from the 
root is PlP2Psfilfl2 .... then that path could be edited as plp2p3Ai or simply as pIp2P3$. For convenience, 
this latter approach will be followed. However the rewrite from fllfl2.., fit to Ai has to be reflected 
somewhere, so the original input string is rewritten, rather than the suffix tree. 4.2 A Linear Time 
Algorithm With the above observations in mind, a more relaxed version of the generalized suffix can be 
used to implement the GTAC algorithm. In all, GTAC uses three data structures: a generalized suffix tree 
T, which provides information about the LPs, an array of queues Q which bucket sorts the LNP information 
by size, and an array, x, holding the original input which provides information about the substrings 
that occur to the left of a given LNP. First T is built from the original input, and then as it is traversed 
information about the LNPs are stored in Q. Then the LNPs are read from Q in descending order of size, 
and Q, z and T are kept up-to-date as the LNPs get removed. The whole algorithm is outlined and explained 
below. GTAC(x) begin 1. create rule: S -4 x; T ---- new suffix_tree(x); md --max_depth(T); O[] = new 
array_of_queues(size = rod); 2. for each interior node n of T do if (n is an LNP) add n to Q[depth(n)]; 
end while;  3. for l = md downTo 2 do while (Q[I] is non-empty) do n = pop(Q[/]); B -= new non_terminal; 
 4. for each fl[ ] = path to node n do p[ ] = 2l chars to left of fl[ ] in y; fori= lto21do if (suffix(p[i]) 
contains fl[1] in T)  5. remove suffix in T after p[2l]; end for; 6. fori= ltoldo if (suffix(fl[i]) 
goes beyond fi[l] in T) remove suffix in T after fl[l]; end for;  7. replace fl with B in rules; end 
for; 8. create rule: B -+ fl; end while;  end for; estimate entropy based on grammar; end alg; 1. 
Initialize Data Structures: Given an input string, z, create the trivial rule S -+ x, a gener-alized 
suffix tree T, and an array of queues Q, with Q[i] representing a list of LNPs of size i. 2. Fill Q: 
Traverse the tree, T, keeping track of the depth. At each interior node n, which represents a pattern, 
check to see if it also represents an LNP and if so create a pointer to that node, and add it to the 
queue that corresponds to it's depth, namely Q[depth(n)]. Also include a back-pointer from the interior 
node to its corresponding entry in the queue. 3. Get pfls: Work through the Q array starting at the 
largest value md. Given an LNP, say fl, from Q[I], for each occurrence of fl, consider the substring 
that extends up to 2l characters on either side of fl, namely PIP2...P21fllfl2...fltsls2...s2t where 
p represents the prefix, and s the suffix of ft. This substring can be determined by consulting T to 
get the list of locations of fl, and then consulting the input string x to get p and s for that occurrence. 
 . Find Suffixes: For each suffix starting at pxP2... and ending at /3tsls~..., perform Steps 4 to 7. 
Descend down the tree on the path piP2.., with two pointers, d-ptr and i-ptr. The d-ptr will point to 
the leaf that corresponds to the unique suffix we are looking for and will eventually delete; the /-ptr 
will point to the node where we insert the end-of-string marker (which is always the node between P21-1 
and /31). Search down this path for the beginning of the LNP,/31. Consistent with Observation 1 above, 
a search will only go as far as the first character in the leaf. . Remove Suffixes Starting in p: If, 
while search- ing down a path,/31 is encountered, then the algo- rithm will begin modifying the tree. 
First the i-ptr stays at the node between P2t-x and/31. A node may have to be created here and a corresponding 
suffix link and entry made in the Q array. The d-ptr continues down the path to the leaf corre- sponding 
to this unique suffix. If this leaf has more than one sibling, then just delete the leaf that d-ptr points 
to. If the parent node only has only two child leaves, then delete the two leaves, and convert the parent 
node to a leaf corresponding to the sibling of the leaf that is being deleted. For example, if the path 
into the parent node is labeled/3i/3i+1 and the two leaves are labeled/3i+2.-, and c... then the parent 
node becomes the leaf corresponding to the suffix/3i/3i+1c .... Wherever a node is deleted, the back-pointer 
from this node are followed, if it exists, and it's corresponding entry in the Q array is removed. As 
well, the end-of-string marker, $, is added to where the i-ptr points to (representing the new place 
where this suffix ends, as explained in Observation 2). When finished with the cur-rent SUIfLX and moving 
to the next one, suffix links are used. A suffix link is a pointer from the inte- rior node that represents 
the suffix starting with ca to the one representing the suffix starting with a, where e is a single character 
and a is a possibly empty string. Both the i-ptr and the d-ptr inde- pendently take the suffix links 
from their current node to move on to the next suffix, or go up a most one node, and use that suffix 
link instead. . Remove Suffixes Starting in /3: A similar process is followed for the suffixes that start 
in /3, except that the entire suffix is eliminated with no need to insert a end-of-string marker anywhere. 
If the path is/3i/3i+x .../3tsls2 ...sj, then the leaf corresponding to that suffix is eliminated from 
the tree entirely, and if necessary the parent node of that leaf becomes a leaf, and the corresponding 
entry in Q eliminated. The final suffix to be considered is/3tsls2 .... . Edit Rules: Finally the rule 
containing /3 is updated by deleting that occurrence of /3 and adding the appropriate non-terminal in 
its place. . Create New Rule: A new rule is created and the right hand side of that rule,/3 is added 
to the suffix tree (actually the last occurrence of/3 in the tree is converted to this new entry). With 
only a few modifications this algorithm also deals with reverse complements. In the first step both the 
string x and ~r are added to the suffix tree, and when removing a suffix both the forward and the reverse 
complement occurrences must be removed. As well, two sets of non-terminals (for normal and reverse complement 
patterns) with the decision about which to use take place between steps 3 and 4. The source code for 
this algorthm will be available at [1]. With a description of the algorithm complete the next step is 
to characterize the running time, which is as follows. THEORBM 4.1. The GTAC entropy estimator runs in 
time linear in the size of its input. Proof. Assume the the input has m characters in it. In step 1, 
each statement can be performed in linear time, such as building the suffix tree [14]. Step 2 is to build 
and maintain Q, the array of queues. There are two aspects, the size of the array, and the number of 
elements in a queue. The size of the array is at most m/2. The number of entries is bounded by the number 
of interior nodes in T, because an interior node could correspond to an LP, but not an LNP. Since each 
interior node in T has between two and five children, and a suffix tree built from a string of m characters 
has m leaves, then picking the worst case, when each node has only two children, T will have at most 
rn nodes. Placing and traversing at most m entries in at most m/2 queues can be done in linear time. 
Steps 3-7 are removing an LNP and taking care of all the patterns that overlap the LNP. For this situation, 
we have the following lemma. LEMMA 4.3. Given an occurrence of an LNP of length l, GTAC removes all possible 
patterns that overlap that occurrence in O(l). Proof. GTAC removes the all the overlapping substrings 
for a given occurrence of an LNP in steps 4 to 7. During these steps, the i-ptr and d-ptr go down a few 
nodes in the tree, possibly a leaf is deleted, an internal node is Sequence name PANMTPACGA MPOMTCG 
CHNTXX CHMPXX SCCHRIII HUMGHCSA HUMHBB HUMHDABCD HUMDYSTROP HUMHPRTB VACCG HEHCMVCG Sequence UNIX Bio- 
CDNA length compress compress-2 compress GTAC 100314 2.12 1.88 1.85 1.74 186609 2.20 1.94 1.87 1.78 155844 
2.19 1.62 1.65 1.53 121124 2.09 1.68 1.58 315339 2.18 1.92 1.94 1.82 66495 2.19 1.31 0.95 I.I0 73308 
2.20 1.88 1.77 1.73 58864 2.21 1.88 1.67 1.70 38770 2.23 1.93 1.93 1.81 56737 2.20 1.91 1.72 1.72 191737 
2.14 1.76 1.81 1.67 229354 2.20 i.85 1.74 Table 2: Comparison of Entropy Values in Bits Per Symbol converted 
to a leaf, and an entry in a queue is deleted. The i and d pointers may go up a node before following 
a suffix link, and then begin dealing with the next suffix. Of these operations, the only one that may 
take more than constant time is when the/-ptr and d-ptr go down the tree a few nodes. We will give an 
argument for the d-ptr, with the argument for the i-ptr being similar. While the d-ptr can travel as 
much as 2l + 1 nodes to get to the leaf representing a single suffix, it's amortized cost for dealing 
with all 31 suffixes is constant. When the d-ptr moves up one node, and over one suffix link, it loses 
height at most two nodes in the suffix tree. This is because the node depth of the suffax ca is at most 
one more that the node-depth of a and a pointer can travel from one node to the next in constant time 
[5]. So in order to look for 31 suffixes, it loses up to 61 nodes due to following suffix links, moves 
forward at most 5/ characters to cover the all the overlapping substrings on 21 characters on either 
side of LNP. Thus GTAC moves forward at most 11/nodes to remove 31 suffixes. [] For a single iteration 
of step 3, say an LNP of length l with n occurrences is found. It takes O(l) time to remove each one 
of them. After n -1 of them are removed, a new rule is created and so the remaining occurrence of the 
LNP is converted to correspond to this rule. So to reduce the size of the original input by O(nl) characters 
takes O(nl), and the amount that gets removed can never exceed to original size of the input m, so this 
phase is O(m) as well. Thus the theorem is proved. D Implementation and Simulation Results Other work 
in the area of estimating the entropy of genetic sequences have used the same benchmark se-quences to 
compare their estimates. These standard se- quences,(available at [12]) come from a variety of sources 
and include the complete genomes of two mitochondria: MPOMTCG, PANMTPACGA (also called MIPACGA); two 
chloroplasts: CHNTXX and CHMPXX (also called MPOCPCG); the complete chromosome III from yeast: SCCHRIII 
(also called YSCCHRIII); five sequences from humans: HUMGHCSA, HUMHBB, HUMHD-ABCD, HUMDYSTROP, HUMHPRTB; 
and finally the complete genome from the two viruses: VACCG and HEHCMVCG (also called HS5HCMVCG). On 
these test sequences, GTAC always beats Biocompress-2. As well, GTAC beats CDNA on eight out of the ten 
se-quence results that are available for CDNA. The en-tropy estimates of all three algorithms are presented 
in Table 2. When GTAC completely ignores reverse comple-ments, the values are only slightly worse (about 
0.01- 0.02 bits/symbol) for eight of the twelve sequences, but dramatically different for four sequences: 
the two chloroplasts, CHNTXX and CHMPXX, and the two viruses, VACCG and HEHCMVCG. The results get worse 
by between 0.09 -0.30 bits per symbol. This is because these sequences are known to contain long reverse 
complements. Two of these sequences are the ones that Biocompress-2 beats CDNA, and values were not available 
from CDNA for the other two. We have performed some preliminary experiments with interesting consequences. 
The first result concerns coding and noncoding regions in g. coll. Around 90% of the genome of higher 
eukaryotes is noncoding whereas about 15% of the genome of E. coli is noncoding. If noncoding regions 
have a definite role, they may be more regular than coding regions which would support the conjecture 
that noncoding regions in prokaryotes are 418 not junk. Our results confirmed this hypothesis. When comparing 
coding and noncoding regions of E. coli we found the following entropy values: . 1.85 bits/symbol for 
coding regions (4,090,525 bases) * 1.80 bits/symbol for noncoding regions (640,039  bases) These results 
are consistent with Farach et a/. [2] who did a similar experiment comparing the entropy of introns and 
exons from human sequences. The second hypothesis we wanted to check was to verify if highly expressed 
essential genes have lower en- tropy than normal genes in E. coli because random mu- tations in normal 
genes are less likely to be deleterious. The results axe as follows: 69 highly expressed essential genes: 
Mean: 1.7521 and sample variance: 0.0043 bits/symboh  244 normal genes: Mean: 1.785 and sample vari- 
ance: 0.0031 bits/symbol.  By statistical tests, with over 99% confidence, our hypothesis is supported. 
6 Conclusion While the idea of using Context Free Grammars in compression algorithms has been around 
for a while, the recent results have shown that if these grammars have the addition property that they 
are asymptotically compact then they are universal. This result has created a whole new family of approaches. 
One such algorithm in this family, namely GTAC, beats all known competitors for estimating the entropy 
of on a set of standard genetic sequences, and has the additional property that it has linear running 
time, and has been proven to be universal without assuming an ergodic source. We are continuing to work 
in this area by modifying GTAC to include some of the approaches that other methods use, such as recognizing 
inexact matches which CDNA does. 7 Acknowledgments We would like to thank Jonathan Badger, Paul Kearney 
and Hualchun Wang for helpful discussion and useful data. References [1] BioInformatics Group Homepage, 
http:// wh.math.uwaterloo, ca. [2] M. Farach, M. Noordewier, S. Savari, L. Shepp, A. Wyner and A. Ziv, 
On the entropy of DNA: Algorithms and measurements based on memory and rapid con-vergence. Proceedings 
o] the Sixth Annual A CM-SIAM Symposium on Discrete Algorithms pp. 48-57, 1994. [3] S. Grumbach and F. 
Tahi, Compression of DNA se- quences. Proceedings of the IEBE Symposium on Data Compression, 340-350, 
1993 [4] S. Grumbach and F. Tahi, A New Challenge for Com- pression Algorithms: Genetic Sequences, Information 
Processing F~ Management 30 (1994), 875-886. [5] D. Gusfield, Algorithms on Strings, Tress, and Se-quences, 
Cambridge University Press, Cambridge, 1997. [6] J. Hoperoft and J. Ullman, Introduction to Automata 
Theory, Languages, and Computation, Addison-Wesley, Reading, 1979. [7] J. Kieifer and E. Yang, Grammar 
Based Codes: A New Class of Universal Lossless Source Codes, submitted for journal publication. [8] J. 
Kieifer and E. Yang, Ergodlc Behavior of Graph Entropy, ERA Amer. Math. Society, VoL 3, no. 1, pp. 11-16, 
1997. [9] B. Lewis, Genes VI, Oxford University Press, Oxford, 1997. [1O] P. Lib, A. Politi, M. Buiatti, 
and S. Ruffo, High Statistics Block Entropy Measures of DNA Sequences, Journal of Theoretical Biology 
180 (1996), 151-160. [11] D. Loewenstern and P. Yianilos, Significantly Lower Entropy Estimates for Natural 
DNA Sequences, ac-cepted for publication in the Journal of Computational Biology. [12] National Center 
for Biotechnology Information, Entrez Nucleotide Query, http://wmLncbi.n]m.nih.gov/ htbin-post/Ent rez/query?db--n_s. 
[13] A. Schrnitt and H. Herzel, Estimating the Entropy of DNA Sequences, Journal of Theoretical Biology 
188 (1997), 369-377. [14] E. Ukkonen, On-Line Construction of SulKx Trees, Algorithmica 14 (1995) 249-260. 
[15] E. Yang and J. Kieffer, Efficient universal lossless com- pression algorithms based on a greedy 
sequential gram- mar transform-Part one: Without context models, to appear in IEBB Trans. In]orm. Theory. 
[16] E. Yang and J. Kieffer, Universal source coding theory based on grammar transforms, Proc. of the 
1999 IBBB Information Theory and Communications Workshop, Kruger National Park, South Africa, June 20-25, 
pp. 75-77.  [17] P. Y~an;]os, CDNA source code, http :// www.neci .nj .nec. com/homepages/pny/software/ 
cdna/ma~.html.  
			