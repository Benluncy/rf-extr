
 A MINI-COURSE ON CONCURRENCY David Jac/&#38;on The Department of Computer Science Universi~ of Liverpool 
P O Box 147 Liverpool L693BX ENGLAND ABSTRACT This paper addresses the problem of teaching the topic 
of concurrency to Computer Science undergraduates. It is sug­gested that a small set of introductory 
lectures on the subject could be of benefit, and the paper outlines some of the con­siderations involved 
in devising such a mini-course . A detailed breakdown of the contents of the lectures is given, and a 
couple of examples of the parallel programming exer­cises are presented. Some comments on the effectiveness 
of the approach conclude the paper. 1. INTRODUCTION Much attention has been focused in recent years 
on the teaching of concurrent to undergraduates [1,2,3,4]. In many Computer Science curricula, the topic 
of concurrency is not confined to a single course unit, but appears in a diversity of units that may 
include computer architecture; distributed sys­tem% operating system% parallel algorithms; and so on. 
Such is the case in our own department, in which it is felt desirable (understandably) that a short introduction 
to concurrency be presented to the students prior to their encountering material under the headings just 
mentioned. One of the questions associated with the satisfactory provision of this desideratum must be 
is it possible to delimit a corpus of material on concurrent which is not only com­pact but also pertinent 
to the later course units? In answer to this, the difficulties involved in producing both hardware and 
software which exploit parallelism revolve chiefly around the handling of problem areas such as process 
creation and termi­nation, synchronization, communication, avoidance of deadlock, etq it is therefore 
reasonable to constrain this introductory set of lectures by placing the emphasis on the nature of these 
concepts, From a pedagogical point of view, this is most easily done in the context of programming, since 
it allows the students to gain a practical feel for ideas which are (to them) quite novel. Permission 
to copy without fee all or part of this material is granted provided that the copies are not made or 
distributed for direct commercial advantage, the ACM copyright notice and the title of the publication 
and its date appear, and notice ie givan that copying is by permission of the Association for Computing 
Machinery. To copy otherwise, or to republish, requiree a fee and/or specific permission. a 1991 ACM 
0-89791-377-9/91 10002-0092,..$1.50 I-qically, the mini-course on concurrency described in this paper 
can be regarded as a separate, self-contained unit; physically it has to be found a niche somewhere within 
the curriculum. Our own undergraduate course structure is such that an understanding of parallelism is 
first required in a second-year unit on systems software the major portion of which is a detailed study 
of operating systems and it is here that the concurrency lectures are placed. Since some room also has 
to be found within the unit for descriptions of assemblers, compilers, etc., the amount of time available 
for the mini-course reduces to just five 45-minute lectures (plus additional practical classes). However, 
it will be seen in Sec­tion 3 how a reasonably adequate grounding in the basic prin­ciples can be covered 
in that period.  2. FURTHER COURSE CONSIDERATIONS Having decided what to teach, the next major consideration 
is how to teach it. More specifkally, a choice of programming language must be made. A whole range of 
parallel program­ming languages have been described in the literature, but few are generally available. 
In our own situation, there were really only two feasible approaches. The first of these possible approaches 
would make use of the C programming language in conjunction with Unix sys­tem calls to handle process 
creation, communication, and so on [5]. This was rejected for two main reasons: firstly, the stu­dents 
on the course are, at this stage of their studies, not fam­iliar enough with either C or Unix to be faced 
with such a degree of intricacy secondly, many of the difficulties that are peculiar to parallel programming 
 which, as has already been decided, form the main emphasis of these lectures are dealt with behind 
the scenes , as it were, by the Unix sys­tem calls themselves, and one is, therefore faced with the problem 
of devising highly artificial situations in which the students have to face these difficulties directly. 
The second approach the one adopted was to use Ada. In our undergraduate degree course, as in others 
[6,7,8], Ada is used as the primary teaching language, and reported experiences of its use in teaching 
parallel programming seem generally favorable [9,10]; thus, there seemed to be no great advantage in 
abandoning Ada for the purposes of this course. Another important consideration is the use of a suit­able 
text-book. A course of this length probably does not in itself justify the purchase of a separate text, 
but the book on operating systems by Deitel [11] contains a good section on 92 coneurreney, and is 
therefore recommended for use on the systems software course as a whole. Those who wish to delve deeper 
into the complexities of concurrent programming are recommended the text by Ben-Ari [12].  3. COURSE 
CONTENT The following is a leeture-by-lecture breakdown of the mini­course in concurrency. 3.1 LECHJRE 
1: PROCESS CONCEPTS Up until this point in their studies, the students have only encountered conventional 
sequential programs with a single flow of control. The main aim of this leeture is to introduce them 
to the idea of programs that perform their computa­tions using several, possibly many, threads of control. 
Before moving on to parallel programming per se in the leetttres which are to follow, the notion of concurrent 
must be presented in a way which fits well into the framework of what the students already know about 
computers and programy to this end, and also to provide a suitable basis for elaboration in the operating 
systems part of the course, the idea of a pro­cess is used as a vehicle for introducing parallelism. 
3.1.1 Definition of a Process A firm grasp of what is meant by the term process is so fun­damental in 
much of the curriculum material which the stu­dents will subsequently encounter to make it worthy of 
extremely careful explanation. Texts such as that by Deitel [11] give several definitions of the term, 
probably one of the most useful being something along the lines OE a program in execution . To be more 
precise, the wording used in this lec­ture i~ A ptwgram is a static representation of an algo­rithm to 
perform some defined task. A process is the activity required to perform such a task, i.e. it is dynamic. 
 Such definitions are, by themselves, not sufficient to convey the full implications of the distinction 
between a program and a process. To a student, a program in exeeution is still just a program. In order 
to show why this distinction is a useful one to make, it is explained to the students that a running 
pro­gram can overlay itself with a completely different program which then gains control, and conversely, 
that when a pro­gram is executed, it may give rise to several processes running in parallel. In other 
words, there may be a one-to-many or a many-to-one correspondence between programs and processes. 3.1.2 
Process Execution At this juncture, some of the brighter students may already be wondering how it is 
possible to run several processes at the same time on the one machine. In addition to making the seemingly 
bland but actually rather important comment that some computer systemsmake use of more than one prOCes­sor, 
this is usually a good time to tell the class what a cotrmxr sm tch is. There is no real need to go into 
all the intricate details of process states, timeslieing and process descriptors, as these will be encountered 
in greater depth later on, but an attempt is made to make the students understand how a rapid switching 
between processes can give the illusion of true con­ currency.  3.13 Examples To make all of the above 
more concrete, the lecture con­cludes with a look at process creation and manipulation in a situation 
with which the students are all very familiar, viz. logging-in under Unix. This is done by first of all 
describing the use of the Unix system calls exec, which allows an execut­ing program to overlay its process 
space with another pro­gram, and jot+c, which creates a separate child process. It is then shown how 
suitable combinations of these calls are used by the Unix system in the management of processes such 
as init, gay,fog nand the ksh command interpreter. The way in which ksh makes use of these system calls 
to execute user commands and shell scripts is also discussed.  3.2 LECTURE 2: CONCURRENCY IN ADA The 
previous leeture was intended to give the students some feel for what is meant by concurrency, albeit 
fairly large­grained. In this leeture and those which follow, the students delve more deeply into the 
constructs provided by a typical parallel programming language. 3.2.1 Fine-Grained Parallelism In order 
to introduce students to the other end of the spec­trum of granularity of parallelism, a simple example 
from Deitel is used. In this, the primitive computational steps required to implement an expression to 
calculate one root of a quadratic equation are considered; it is shown that the number of operations 
executed on a conventional sequential processor can be somewhat reduced on a multi-processor machine 
by scheduling several of these operations to be exe­euted in parallel. It is emphasised that this scheduling 
is not arbitrary, but is based on the data-dependencies inherent in the computation. The above example 
leads nicely into the statement that some programming languages allow the programmer to specify that 
certain operations of his or her choosing are to proceed in parallel, and that in Ada this is done at 
the stafe­ment level using task. 3.2.2 Ada l asks Students on the course will have used procedures, 
and so the syntax of tasks does not, in general, cause any problems for them (although the task declaration 
has to be separate from the task body). What often does cause confusion, however, is the way in which 
these tasks execute if they are not called in the same way as procedures (indeed, I have seen students 
attempt to make such calls of tasks in their programs). Hence, it is worthwhile spending some effort 
on making the point that each task begins its execution as soon as the begin of the enclosing unit is 
reached, and, moreover, the state­ments between the beg tr and end of the enclosing unit exe­eute in 
parallel with those contained in the tasks. It some­times helps in removing the confusion to make the 
parallelkm more explieit by showing how an equivalent program can be obtained by encapsulating the statements 
of the enclosing unh in their own task, and having just a null statement in the enclosing unit.  3.23 
Synchronization This is a novel concept to the students, involving the use of simple examples to explain 
why tasks cannot always be allowed to execute entirely independently of one another, and how synchronization 
is achieved via the Ada rendezvous 93 mechanism. At this stage, the version of the enty statement w 
thout parameters is introduced. One aspect of this statement which sometimes requires claritlcation is 
its one-sidedness , in that it is only the acceptor of the entry call which contains the entry declaration. 
That aside, the semantics of the rendez­vous (that is, the task making the entry Call/accept fiist must 
wait for the other task to catch up before proceeding) are usually straightforward to explain. The extended 
form of the entry accept statement, containing a sequence of statements to be executed before the rendezvous 
completes, is also described here. 3.3 LECTURE 3: PROBLEMS OF CONCURRENCY This lecture introduces the 
students to some of the difficulties that may arise as a consequence of exploiting parallelism (the topic 
of deadlock will be covered in a later lecture). 3.3.1 Indeterminacy To engage the students interest 
and stimulate some discus­sion, I usually begin this lecture by posing the following prob­ lem: Consider 
two tasks, T1 and T2, running in parallel. Each task executes the statement COUNT:= COUNT+ 1; Assuming 
that the value of COUNT was initially zero, what value will it have after completion of the tasks? Apart 
from the occasional self-styled wit who calls out an answer such as forty-seven , the responses predictably 
fall into one of three camps those who think the answer is two , those who think it is one , and those 
who will admit to not knowing the answer. The effect on the audience of being told that it is the don 
t-knows that are correct never fails to surprise. The explanation for this is given in terms of a break­down 
of the actions that a typical processor might perform in executing the above statement: load the value 
of COUNT into a rqkter increment the regkter store the regker contents back into COUNT By writing down 
different interleavings of these actions for the two tasks which may arise as the result of relative 
proces­sor speeds or the choice of scheduling mechanism on a uni­processor, it can be shown how different 
values of COUNT can be obtained. In summary, it is explained that this problem of indeterminacy is a 
consequence of allowing unrestricted access to a shared resource. 3.3.2 Mutual Exclusion This next part 
of the lecture looks at ways of handling the shared resource problem. The terms mutual exclusion and 
cn tical reg on (section) are defined, and example programs that make use of additional tasks to grant 
or deny access to shared resources are examined and discussed. Judging by student programs, it often 
seems to be the case that solutions to parallel programming problems are correct, but not always fair 
. By this, it is meant that although a program works satisfactorily, it may not always make the best 
use of shared resources so as to maximise the degree of parallelism. Hence, it is worthwhile explaining 
that, since only one task may enter the critical region associated with a particular shared resource 
at any one time, critical regions should be kept as small as possibl~ similarly, there should be a separate 
controller task (i.e. the task that grants or denies access to a resource) for each and evey shared resource. 
  3.33 Producer-Consumer Situations Producer-consumer problems provide a good way of exercis­ing the 
parallel facilities available in a programming language, and of testing students in their comprehension 
of those facili­ties. In the remainder of this lecture, the class is given a gen­tle introduction to 
this class of problems through the study of a program involving a single producer and a single consumer, 
communicating via a buffer containing at most one item. A more complicated situation, involving several 
producers and consumers and larger buffers, will be met in the practical work (see Section 4). The example 
program used here is not overly compli­cated but needs detailed discussion of the cases which have to 
be considered, e.g. the producer cannot place an item in the buffer if it is full. The program also makes 
use of the parameterized form of the entry statement, which therefore needs to be described.  3.4 LECI 
URE 4 MORE ADVANCED PROGRAMMING The parallel programming techniques considered in this lec­ture focus 
mainly on the use of the Ada select statement. Having previously shown the students how to avoid indeter­minacy 
through the judicious enforcement of mutual exclu­sion, the examples given here illustrate ways in which 
certain forms of controlled indeterminacy can be deliberately built into parallel software. 3.4.1 Protected 
Variables In the program examples considered prior to this lecture, the entry accept statements have 
been used in such a way that the order in which they take place has been rigidly fixed accord­ing to 
the way the programs have been written. One of the aims of the current example is to show how greater 
flexibility can sometimes be achieved by using the select statement to choose one of a number of entty 
accepts arbitrarily. A second aim is to show how it is possible and better practise to contain all 
accesses to a shared resource within a single task; this is unlike the earlier mutual exclusion example 
where, although access to a shared resource was granted or relin­quished by a controller task, the accesses 
themselves were done by other tasks. The students are asked to consider the problem of writing an Ada 
task that handles all accesses to a single vari­able in such a way that the first access must always 
be a write operation to initialise the variable, but following which any number of read or write operations 
can be dealt with in any order. The employment of the select statement for this pur­ pose is explained, 
particular attention being paid to the semantics of the operation.  3.4.2 The Bounded Buffer This is 
a more sophisticated example than the previous one. It illustrates the way in which a task can manage 
accesses to a queue data-structure, implemented as an array with integer pointers to the front and back 
of the buffer, To be capable of 94 dealing with boundary conditions such as buffer ovefflow, the guaded 
form of the select statement is used, which once again requires careful explanation.  3.5 LECTURE 5: 
TASK TERMINATION AND DEADLQCK 3.5.1 Review of the Select Statement Before moving on to the main subject 
matter of this lecture, the class usually finds it helpful to be given a condensed ver­sion of the main 
points of the previous lecture. This is done by presenting an overview of the general form of the select 
statement, accompanied by a recap of its semantics. This is rounded off with a brief look at the form 
of the select state­ment used for entry calls (as opposed to entry accepts). 3.5.2 Task Termination 
For simplicity, many of the example tasks studied in the preceding lectures have contained infinite loops, 
and are therefore non-terminating. In this section of the lecture, the students are introduced to ways 
of cleanly terminating tasks, in particular those whose main body is a loop for which the exact number 
of iterations is not known in advance of execu­tion. The two methods presented here are the use of an 
addi­tional entry call to request the termination of a task and the more powerful and more generally 
applicable terminate state ment.  3.5.3 Deadlock As a final topic on this mini-course, the concept of 
deadfock is discussed. An interesting observation here is that it is worthwhile drawing attention to 
the use of the alternative term deadly embrace, since this seems to evoke some sense of curiously morbid 
fascination with the phenomenon. The working definition of deadlock given in this lecture is A set of 
processes is deadlocked if each process in the set is waiting for an event that only another process 
in the set can cause. This is elaborated upon initially in terms of the sorts of deadlock situations 
which may arise when processes are requesting and accessing non-shareable and non-preemptible resources 
such as lineprinters, and then in the context of con­current Ada programs. Simple examples of programs 
contain­ing tasks which deadlock are examined. The lecture concludes with a non-detailed look at measures 
for the prevention, detection and cure of deadlock. 4. PRACTICAL WORK In a 5-lecture teaching period 
(i.e. just under two weeks), there is unfortunately very little time that is available for run­ning a 
concomitant programme of practical work. It is rare that I have been able to set more than two or three 
exercises per instance of the course, and it is therefore important that these exercises be chosen judiciously. 
In this section, a couple of the parallel programming problems which have been set in the course in previous 
years are presented. 4.1 AN INTRODUCTORY EXERCISE This first problem is given to the students as soon 
as they have encountered tasks and ~chronization using the simple form of the entry accept statement: 
Write an Ada function which takes an array of integers as its only parameter, and returns the sum of 
all its elements. The function should divide the summation into two parallel tasks one to sum the lower 
half of the array, and one to sum the upper half. Test your function in a program which prompts the user 
for the number of elements in the array, declares the appropriate array, reads in its ele­ments, and 
then passes it to the function. This is not a difficult program to write, but it is surprising how many 
students have difficulty in getting the synchroniza­tion correct.  4.2 A PRODUCER-CONSUMER PROBLEM As 
mentioned in Section 3.3.3, producer-consumer problems provide a good opportunity for applying much of 
what has been taught in lectures. This particular exercise is fairly com­plex and hence cannot reasonably 
be assigned to the class until the end of the mini-course; in particular, knowledge of the select statement 
and deadlock are necessaty if an ade­quate solution is to be produced. This problem, which is a particular 
favourite of mine, has been around our Department in a number of versions for a considerable time, and 
I am indebted to those who have contributed to its evolution. It goes as follows You are required to 
write a program to simu­late the following multiple producer/consumer problem. Craftsman A produces ceramic 
pots at the rate of one evety 5 minutes. Craftsman B produces similar pots at the slightly slower rate 
of one every 6 minutes. All pots are placed on a shelf of limited size (12 pots), from where they are 
taken by a packer and packed into cartons for distribu­tion at the rate of one every 4 minutes. The situation 
is further complicated by a demon wrecker , who appears at a random time once within every hour, takes 
a pot from the shelf and smashes it, thereby rendering it unsuitable for further processing. You can 
assume that the potters are unable to produce any pots when the shelf is full, and that the packer and 
the wrecker are unable to operate when the shelf is empty. (a) Write a program to simulate this situation 
over a period of 4 hours. The output from your program should take the form of a running commentary on 
the actions of each of the workers. Before the program finishes it should give the totals of the numbers 
of pots produced, packed and smashed. You may run the program 95 dave/pots to give you an idea as to 
the form of this output. (b) Investigate what happens if the packer goes to a meeting of UPPA (Union 
of Pot PAckers) for the duration of the third hour. HELPFUL go potty You may HINTS take for programmers 
a copy of the file about to dave/ pots.ada tion which to the contains problem, a skeletal version of 
a solu­ including the defiition of a clock task to drive the simulation, plus some comments to aid in 
the writing of the other tasks. Happy parallel potting! Other programming exercises range in complexity 
between the two just described. For example, one of the problems planned for future use is to write tasks 
to simulate sema­phores, since at the moment very little is said in the course about alternative approaches 
to inter-process communication.  5. CONCLUSIONS In a course which is as tightly time-constrained as 
the one described in this paper, one of the major difficulties lies in deciding what to include and what 
to leave out, and there are undoubtedly those whose ideas on the contents of an intro­ductory course 
on concurrency are somewhat at variance with our own. This is to be expected, but the general principle 
of providing such a mini-course has, in our experience, proved fruitful. The decision to use the facilities 
of Ada was also, in retrospect, a good one, rdthough a fair amount of technical wizardry was necessary 
to circumvent the deficiencies inherent in our particular compiler s task scheduler. In the fiial analysis, 
however, the efficacy of any approach to teaching can only be measured effectively in terms of the benefits 
(or otherwise) conferred upon the stu­dents. The (typically scant) feedback elicited from these stu­dents 
suggests that they fiid the mini-course both interesting and informative, and this is borne out by their 
response to practical work and examination questions. This does not mean that the course will not undergo 
some future revisions in an attempt to improve it; an attractive idea currently under consideration is 
to make use of computer animation tech­niques [13,14] to illustrate the concepts involved. REFERENCES 
1. D Sanders and J Hartman, Getting Started with Paral­lel Programming , Proc. 21st SIGCSE Technical 
Symp., Washington DC, Feb. 1990, pp. 86-88 2. D C Hyde, A Parallel Processing Course for Under­graduates 
, Proc, 20th SIGCSE Tech. Symp., Louisville, Kentucky, Feb. 1989, pp. 170-173  3. R M Butler, R E Eggen 
and S R Wallace, Introducing Parallel Processing at the Undergraduate Level , Proc. 19th SIGCSE Tech. 
Symp., Atlanta, Georgia, Feb. 1988, pp. 63-67 4. C Nevison, An Undergraduate Parallel Processing Laboratory 
, Proc. 19th SIGCSE Tech. Symp., Atlanta, Geor&#38;a, Feb. 1988, pp. 68-72 5. K A Robbins, N R Wagner 
and D J Wenze~ Virtual Rings an Introduction to Concurrency , SIGCSE Bul­letin, Vol. 21, No. 2, June 
1989, pp. 23-28 6. A Radensky, Can Ada be Used as a Primary Program­ming Language? Major Problems and 
their Solutions by Means of Subsets , Proc. 21st SIGCSE Technical Symp., Washington DC, Feb. 1990, pp. 
201-205 7. L E Winslow and J E Lang, Ada in CS1 , Proc. 20th SIGCSE Tech. Symp., Louisville, Kentucky, 
Feb. 1989, pp. 209-212 8. A Radensky et al., Experience with Ada as a First Programming LanWage , SIGCSE 
Bulletin, Vol. 20, No. 4, Dec. 1988, pp. 58-61 9. B L Kurtz and T H Puckett, Implementing a Single Classwide 
Project in Sofhvare Engineering Using Ada Tasking for Synchronization and Communication , Proc. 21st 
SIGCSE Technical Symp., Washington DC, Feb. 1990, pp. 6-11 10. M B Feldman, A V Lopes and M Perez, Small-Ada 
Personal Computer Courseware for Studying Con­current Programming , Proc. 21st SIGCSE Technical Symp., 
Washington DC, Feb. 1990, pp. 206-211 11. H M Deitel, Opemting Systems, 2nd edn., Addison-Wesley, 1990 
 12. M Ben-Ari, Principles of Concwrent Programming, Prentice-Hall, 1982 13. G Olimpo, The Robot Brothers 
an Environment for Learning Parallel Programming Oriented to Computer Education , Comput. Educ., Vol. 
12, No. 1, 1988, pp. 113-118 14. M Zimmermamt, F Perrenoud and A Schiper, Under­standing Concurrent 
Programming through Program Animation , Proc. 19th SIGCSE Tech. Symp., Atlanta, Georgia, Feb. 1988, pp. 
27-31  96   
			