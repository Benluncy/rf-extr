
 Communication-Efficient Parallel Sorting (Preliminary Version) MICHAEL T. GOODRICH* Johns Hopkins Univ. 
goodrich@cs. jhu. edu Abstract We study the problem of sorting n numbers on a p-processor bulk-synchronous 
parallel (BSP) com­ puter, which is a parallel multicomputer that al­ lows for general processor-to-processor 
communica­ tion rounds provided each processor sends and re­ ceives at most h items in any round. We 
provide parallel sorting methods that use internal computa­ tion time that is O(*) and a number of commu­ 
nication rounds that is 0( ~$$~1) ) for h = @(n/p). The internal computation bound is optimal for any 
comparison-based sorting algorithm. Moreover, the number of communication rounds is bounded by a constant 
for the (practical) situations when p < nl l/c for a constant c > 1. In fact, we show that our bound 
on the number of communication rounds is asymptotically optimal for the full range of values for p, for 
we show that just computing the or of n bits distributed evenly to the first O(n/h) of an arbitrary number 
of processors in a BSP computer requires fl(log n/ log(h + 1)) communication rounds. Introduction Most 
of the research on parallel algorithm design in the 1970 s and 1980 s was focused on fine-grain massively-parallel 
models of computation (e. g., see [4, 7, 22, 24, 28, 37]), where the ratio of mem­ ory to processors 
is fairly small (typically O(l)), and this focus was independent of whether the model of computation 
was a parallel random-access machine (PRAM) or a network model, such as a mesh-of-processors. But, as 
more and more par­ allel computer systems are being built, researchers are realizing that processor-to-processor 
communi­cation is a prime bottleneck in parallel comput­ *This research supported by the NSF under Grants 
IRI-9116843 and CCR-9300079, and by ARO under grant DAAH04-96-1-0013. Permission to make digitel/bard 
copies of all or psrt of thk material for personal or classroom use is granted without fee provided that 
the copies are not made or distributed for pmtit or commercial advanhge, the copy­ right notice, the 
title of the publication and its date appear, and notice is given that copyright is by permission of 
the ACM, Inc. To copy otherwise, to republi~, to post on servers or to rediatribuw to lists, requires 
specitio permission andlor fee. STOC 96, Philadelphia PA, USA @ 1996 ACM 0-89791-785-5/96/05. .$3.50 
ing [2, 6, 12, 26, 30, 31, 34, 41, 40]. The real potential of parallel computation, therefore, will most 
likely only be realized for coarse-to-medium-grain paral­lel systems, where the ratio of memory to proces­sors 
is non-constant, for such systems allc,w an al­gorithm designer to balance communication latency wit 
h internal comput at ion time. Indeed, this realiza­tion has given rise to several new computation mod­els 
for parallel algorithm design, which all use what Valiant [40] calls bulk synchronous processing. In 
such a model an input of size n is distributed evenly across a p-processor parallel computer. In a single 
computation round (which Valiant calls a superstep) each processor may send and receive h messages and 
then perform an internal computation on its inter­nal memory cells using the messages it has just re­ceived. 
To avoid any conflicts that might be caused by asynchronies in the network (whose topology is left undefined) 
the messages sent out in a round tby some processor cannot depend upon any messages that processor receives 
in round t (but, clf course, they may depend upon messages received in round t 1). We refer to this 
model of computation as the Bulk-Synchronous Parallel (BSP) model. 1.1 The BSP Model As with the PRAM 
family of computer modelsl, the BSP model is distinguished by the broadcast and combining abilities of 
the network connecting the various processors. In the weakest versicm, which is the only version Valiant 
[40] considers, the net­work may not duplicate nor combine messages, but instead may only realize h-relations 
between the pro­cessors. We call this the ERE W BSP model, noting that it is essentially the same as 
a model Valiant elsewhere [41] calls the XPRAM and one that Gib­bons [19] calls the EREW phase-PRAM. 
It is also the communication structure assumed by the LogP model [12, 25], which is the same as the BSP 
model except that the LogP model does not explicitly re­quire bulk-synchronous processing. But it is 
also natural to allow for a slightly more powerful bulk-synchronous model, which we call the 1 Indeed 
~ pRAM with as many processors aOd memOry cells is a BSP model with h = 1, as is a rnodute parallel corrL­puter 
(MPC) [31], which is also known as is a distributed­memory mczchtne (DMM) [23], for any memory size. 
weak-CREW BSP model. In this model we assume processors are numbered 1, 2, . . .. p, and that mes­sages 
can be duplicated by the network so long as the destinations for any message are a contiguous set of 
processors {2, z + 1,. ... j }. This is essentially the same as a model Dehne et al. [15, 16] refer to 
as the coarse-grain multi-computer. In designing an algorithm for this model one must take care to en­sure 
that, even with message duplication, the num­ber of messages received by a processor in a single round 
is at most h. Nevertheless, as we demonstrate in this paper, this limited broadcast capability can sometimes 
be employed to yield weak-CREW BSP algorithms that are conceptually simpler than their EREW BSP counterparts. 
Finally, one can imagine more powerful instances of the BSP model, such as a CREW BSP model, which would 
allow for arbitrary broadcasts, or even a CRC W BSP model, which would also allow for messages to the 
same location to be combined (using some arbitration rule). (See also [19, 32].) The running time of 
a BSP algorithm is charac­terized by two parameters: TI, the internal compu­t ation time, and Tc, the 
number of communication rounds. A prime goal in designing a BSP algorithm is to minimize both of these 
parameters. Alterna­tively, by introducing additional characterizing pa­rameters of the BSP model, we 
can combine TI and TC into a single running time parameter, which we call the combined running time. 
Specifically, if we let L denote the latency of the network that is, the worst-case time needed to send 
one processor­to-processor message and we let g denote the time gap between consecutive messages received 
by a processor in a communication round, then we can characterize the total running time of a BSP com­putation 
as O(T1 + (L + g c). Incidentally, this is also the running time of implementing a BSP com­putation in 
the analogous LogP model [12, 25]. The goal of this paper is to further the study of bulk-synchronous 
parallel algorithms by addressing the fundamental problem of sorting n elements dis­tributed evenly across 
a p-processor BSP computer.  1.2 Previous work on parallel sorting Let us, then, briefly review a small 
sample of the work previously done for parallel sorting. Batcher [5] in 1968 gave what is considered 
to be the first par­allel sorting scheme, showing that in a fine-grained parallel sorting network one 
can sort in O(log2 n) time using O(n) processors. Since this early work there has been much effort directed 
at fine-grain par­ 2There is also an o parameter in the LogP model, but it would be redundant with L 
and g in this bound. allel sorting algorithms (e.g., see Akl [4], Bitton et al. [7], J6J&#38; [22], Karp 
and Ramachandran [24], and Reif [37]). Nevertheless, it was not until 1983 that it was shown, by Ajtai, 
Kom16s, and Szemer6di [3], that n elements can be sorted in O(log n) time with an O(n log n)-sized network 
(see also Paterson [35]). In 1985 Leighton [27] extended this result to show that one can produce an 
O (n)-node bounded-degree network capable of sorting in O (log n) steps, based upon an algorithm he called 
columnsort . In 1988 Cole [10] gave simple methods for optimal sorting in the CREW and EREW PRAM models 
in O(log n) time using O(n) processors, based upon an elegant (cascade mergesort paradigm using arrays, 
and this result was recently extended to the Parallel Pointer Machine by Goodrich and Kosaraju [20]. 
Thus, one can sort optimally in these fine-grained models. These previous methods are not optimal, how­ever, 
when implemented in bulk-synchronous mod­els. Nevertheless, Leighton s columnsort method [27] can be 
used to design a bulk-synchronous parallel sorting algorithm that uses a constant number of communication 
rounds, provided P3 S n. Indeed, there are a host of published algorithms for achieving such a result 
when the ratio of input size to number of processors is as large as this. For example, a ran­domized 
strategy, called sample sort, achieves this result with high probability [8, 17, 18, 21, 29, 38], as 
do deterministic strategies based upon regular sampling [33, 39]. These methods based upon sam­pling 
do not seem to scale nicely for smaller n/p ratios, however. If columnsort is implemented in a recursive 
fashion, then it yields an EREW BSP al­gorithm that uses TC = O([log n/ log(n/p)]~) com­munication rounds 
and internal computation time that is O(TC (n/p) log(n/p)), where 6 = 2/ (log 3 h)ll), which is approximately 
3.419. Using an algo­rithm they call cubesort, Cypher and Sanz [13] show how to improve the Tc term in 
these bounds * ~ 10g* (n/P)) [log ~/ log(n/P)]2), ad to be 0((25)t10g Plaxton [36] shows how cubesort 
can be modi­fied to achieve Tc = O([log n/ log(n/p)]2). In­deed, Plaxton3 can modify the sharesort method 
of Cypher and Plaxton [14] to achieve TC = O((logn/ log(n/p)) log2(logn/ log(n/p))). Finally, Chv&#38;tal 
[9] describes an approach of Ajtai, Komh%, Paterson, and Szemex+di for adapting the sorting network of 
Ajt ai, Kom16s, and Szemer6di [3] to achieve a depth of O(log n/ log(n/p) ) where the ba­sic unit in 
the network is a black box that can sort (n/pi elements. An effective method for con­structing such a 
network is not included in Chv&#38;tal s report, however, for the method he describes is a non-uniform 
procedure based upon the probabilistic 3perSonaI cOmmunicatiOn method. In addition, the constant factor 
in the run­ning time appears to be fairly large. Incidentally, these latter methods [9, 14, 13, 27, 36] 
are actually defined for more-restrictive BSP models where the data elements cannot be duplicated and 
each inter­nal computation must be a sorting of the internal­memory elements. The only previous sorting 
algorithms we are aware of that were designed with the BSP model in mind are recent methods of Adler, 
B yers, and Karp [1] and Gerbessiotis and Valiant [18]. The method of Adler et al. runs in a combined 
time that is 0( g ~g m + pg + gL), provided p s nl ~ for some constant O < 6 < 1. They do not define 
their algorithm for larger values of p, but they do give a slightly better implementation of their method 
in the LogP model so as to achieve a running time of 0( ,tglogn +pg+~) for p similarly bounded. Gerbessi­otis 
a;d Valiant give several randomized methods, the best4 of which runs with a combined time of O(* + gp 
+ gn/p + L), with high probability, for any constant O < e < 1, provided p s nl-d, where 6 is a small 
constant depending upon e. 1.3 Our results Given a set S of n items distributed evenly across p processors 
in a weak-CREW BSP computer we show how S can be sorted in O(log n/ log(h + 1)) communication rounds 
and O ( (n log n) /p) internal computation time, for h = @(n/p). The method is fairly simple and the 
constant factors in the run­ning time are fairly small. Moreover, we also show how to extend our result 
to the EREW BSP model while achieving the same asymptotic bounds on the number of communication rounds 
and internal com­putation time. Our bounds on internal computation time are optimal for any comparison-based 
paral­lel algorithm. In addition, we achieve a determinis­tic combined running time that is 0(% + (L 
+ gn/p) (log n/ log(n/p) ) ), which is valid for all val­ ues of p and improves the best bounds of Adler 
et al. [1] and Gerbessiotis and Valiant [18] even when p ~ nl ~ for some constant O < 6 < 1, in which 
case our method sorts in a constant number of com­munication rounds. In fact, if p3 ~ n, then our method 
essentially amounts to a sample sort (with regular sampling). If p = @(n), then our method amounts to 
a pipelined parallel mergesort, achiev­ing the same asymptotic performance as the fine­grained algorithms 
of Cole [10] and Goodrich and 4They also give a method with a combmed run­ ning tmne of O([(n/p) 10ga+l 
p + Llog2 p + 9 10g +2 p + g(n/p) 10g p]/ log log p), w]th high probability, prOvided P < n/ 10g +l p. 
Kosaraju [20]. Thus, our method provides a sort­ing method that is fully-scalable over all values of 
p while achieving an optimal internal computation time over this entire range. Indeed, we show that our 
bounds on the num­ber of communication rounds needed to sort n ele­ments on a BSP computer are also worst-case 
opti­mal for this entire range of values of p. We establish this by showing that simply computing the 
or of n bits distributed evenly across @(n/h) processors re­quires fl(log n/ log(h + 1)) number of communication 
rounds, where each processor can send and receive h messages in a CREW BSP computer. This lower bound 
holds even if the number of addition al proces­sors and the number of additional memory cells per processor 
are unbounded. Since this lower bound is independent of the total number of processors and amount of 
memory in the multicomputer, it joins lower bounds of Mansour et al. [30] and Adler et al. [1] in giving 
further evidence that the prime bot­tleneck in parallel computing is communication, and not the number 
of processors nor the memory size. 2 A weak-CREW BSP Algo­ rithm Let S be a set of n items distributed 
evenly in a p-processor weak-CREW BSP computer. We sort the elements of S using a d-way parallel mergesort, 
pipelined in a way analogous to the binary parallel mergesort procedures of Cole [10] and Goc)drich and 
Kosaraju [20]. Specifically, we choose d = max{ [fi;], 2}, and let T be a d-way rooted, complete, balanced 
tree such that each leaf is associated with a subset S, C S of size at most [n/pi. For each node v in 
T de­fine U(v) to be the sorted list of elements stored at descendants of v in T, where we define v to 
be a descendent of itself if it is a leaf. Nc}te that if {WI, W2,..., Wd} denote the children of a node 
v in T, then U(v) = U(WI) U U(wz) .0. U U(w~). Our U goal, then, is to construct i7(root(T)). We may 
as­ sume, without loss of generality, that the elements are distinct, for otherwise we can break ties 
using the original positions of the elements of S. We perform this construction in a bottom-up pipelined 
way. In particular, we perform a series of stages, where in a Stage t we construct a list U~(v) G U(v) 
for each node v that we identify as be­ ing active. A node is full in Stage tif Ut (v) = U(v), and a 
node is active if Ut (v) # 0 and v was not full in Stage t 3. Likewise, we say that a list A stored at 
a node v in T is full if A = U(v). Initially, each leaf of T is full and active, whereas each internal 
node is initially inactive. We say that a list B is a k-sample of a list A if 1? consists of every k-th 
element of A. For each active node v in T we define a sample Lt (v) defined as follows: If v is not 
full, then Li(v) is a dz-sample of Ut(v).  If v first became full in Stage t, then we define Lt(v) to 
be a dz-sample of Vt(v)= U(v); we define L~+I (v) to be a d-sample of Ut(v), and we define Lt+z(v) = 
U(v) (i.e., L~+2(v) is full).  We then define U Ut(v) = Lt l(wl) Lt 1(w2) U . . . ULt_l(wJ, where, 
again, {WI, W2, . . . . Wd} denote the children of node v in T. Note that by our definition of Li (v), 
if a node v becomes full in Stage t,then v s parent be­comes full in Stage t +3. Thus, assuming we can 
im­plement each stage with a constant number of com­munication rounds using the p processors, then we 
will be able to sort the elements of S, by constructing U(rOOt(T)), in just O(k%d ) = 0 ( lo~h~l) com­ 
) munication rounds, for h = @(n/p). Before we give the details for implementing each stage in our algo­rithm, 
however, we establish the folIowing bounds (whose proofs are included in the full version): Lemma 2.1: 
If at most k elements of Ut (v) are in an interval [a, b], then at most dk + 2d2 elements of U~+I (v) 
are in [a, b]. Intuitively, this lemma says that Ut+l (v) will not be wildly different from U~(v). Similarly, 
we have the following corollary that relates Lt+l (v) and Lt (v): Corollary 2.2: If at most k elements 
of Lt (v) are in an interval [a, b], then at most d(k + 1) + 2 elements of Lt+l(v) are in [a, b]. Having 
given this important lemma and its corol­lary, let us now turn to the details of implementing each stage 
in our pipelined procedure using just a constant number of communication rounds. 2.1 Implementing each 
stage using a constant number of communica­tion rounds We say that a list A is ranked [10, 20] into a 
list B if, for each element a A, we know the rank of a s predecessor in B (based upon the ordering of 
elements in A B). If A is ranked in B and B is U ranked in A, then A and B are cross-ranked. The generic 
situation at the end of any Stage tis that we have the following conditions satisfied at each node v 
in T. Induction Invariants: 1. Lt (v) is ranked into Lt-l (v). 2. If v is not full, then Lt_l (w.) is 
ranked in Ut(v), for each child w%of v in T. 3. Lt(v) is ranked into Ut (v).  We maintain copies of 
the lists L,-l (v), L,(v), Ut-l (v), and Ut (v) for each active node v in T, and we do not maintain any 
other lists during the computation. As we shall show, this will allow us to implement the entire computation 
efficiently us­ing just p processors. In order to implement each stage in our computation using just 
O(1) commu­nication rounds we also maintain the following im­portant load-balancing invariant at each 
node v in T. Load-balancing Invariant: If a list A is not full, then A is partitioned into contiguous 
subarrays of size d each, with each subarray stored on a different processor.  If a list A is full, 
then A is partitioned into contiguous subarrays of size d2 each, with each subarray stored on a different 
processor.  We assume that the names of the nodes of v in T and the four lists stored at each node 
v are defined so that given an index, i, into one of these lists, A, one can determine the processor 
holding A[z] as a local computation (not needing a communication step)5. Given that the induction and 
load-balancing invari­ant are satisfied for each node v in T, we can con­struct Ut+l (v) at each active 
node, with the above invariants satisfied for it, as follows. Computation for Stage t + 1: 1. For each 
element a in Lt (wi), let b(a) and c(a) respectively be the predecessor and suc­cessor of a in Lt l (w,). 
We can determine b(a) and c(a) in O(1) communication rounds, for each such a, since Lt (w.) is ranked 
in Lt (wi) by Induction Invariant 1. In fact, if L* (w, ) = U(w,), then this is essentially a local computation. 
Moreover, by our load-balancing invariant and Corollary 2.2, even in the general case, each processor 
(storing a portion of some Li_l (wz) ) will receive (and then send) at most d(d+ 1) + 2 = ~(h) messages 
to implement this step. 5We maintain this assumption inductively, as we show in the full version. 2. 
Determine the location (rank) of b(a) and c(a) in Ut (v). This can also be easily implemented with a 
O(1) communication rounds, as in the previous step. 3. Broadcast a (and its rank in L~ (w,)) to all 
pro­cessors holding elements of Ut (v) between b(a) and c(a). By our load-balancing invariant and Lemma 
2,1 we can guarantee that each proces­sor will receive at most 3d2 + d = @(h) mes­sages to implement 
this step (each processor receives at least one element from each child of v plus as many elements as 
fall in its inter­val of U~(v)); hence, it can be done in O(1) communication rounds. 4. Each processor 
assigned to a contiguous por­tion [e, j) of Ut (v) receives elements sent in the previous round and merges 
them via a simple d-way mergesort procedure to form a sublist of Ut+I (v) of size O(d2) = 0(1-L). It 
is important to observe that the processor for [e, $) receives at least one element from each child of 
w so as to include all all the elements that may in­tersect the interval [e, .f), even if none actually 
fall inside [e, $). This allows us to accurately compute the rank of each element in Ut+l (v) locally; 
hence, it gives us Vt (v) cross-ranked with Ut+l (w). Moreover, this step can be ac­complished in O(1) 
communication rounds and 0(d2 log d) = O((n/p) log(n/p) ) internal com­put ation time. 5. For each element 
a in Ut+l (v) send a message to the processor holding a E Lt (w,) informing that copy of a of its rank 
in Ut+l (v). This step can easily be accomplished in 0(1) communi­cation rounds, and gives us Induction 
Invari­ant 2. 6. Determine the sample Lt+l (w) and rank it into Ui+l (v), giving us Induction Invariant 
3. Also, use the cross-ranking of Ui+l (v) and Ut (v) to rank Lt+l (w) into Lt (v), giving us Induction 
Invariant 1. This step can easily be accom­plished in O(1) communication rounds. 7. Finally, partition 
the four lists stored at each node v so as to satisfy the load-balancing in­variant. Assuming the tot 
al size of all the non­full lists in T is O(n/d), then this can easily be performed in O(1) communication 
rounds us­ing p = @(n/d2) processors.  Therefore, given the above assumption regarding the total size 
of all the lists, in a constant number of communication rounds and an internal computa­tion time that 
is O((n/p) log(n/p) ) we can build the set Ut+l (v) and establish the induction and load­balancing invariants 
so as to repeat this procedure in Stage t+ 2. Let us therefore analyze the total size of all the lists 
stored at nodes in T. Clearly, the size of all the full lists in T is O(n). Moreover, each such list 
contributes at most I/d of its elements to the next higher level in T, and from then on up T each lists 
on a level 1 contribute at most 1/d2 of its elements to lists on the next higher level in T. Thus, the 
total size of all non-full Ut_l (v) or Ut (v) lists forms a geometric series that sums to be O(n/d), 
which is what we require. In addition, any sample Lt (v) or Lt-l (v) that is not full can contain at 
most l/d of the elements of iY(w); hence, the total space needed for all these lists is also O(n/d). 
This establishes the following: Theorem 2.3: Given a set S of n items stored O (n/p) per processor on 
a p-processor weak-CRE W BSP computer, one can sort S in O(log n/ [og(h-t-1)) communication steps and 
O (n log n/p) internal com­putation time, where h = @(n/p). In achieving this result we exploited the 
broad­cast capability of the weak-CREW BSP model (in Step 3). In the next section we show how to match 
the asymptotic performance of Theorem 23 without using such a capability.  3 An EREW BSP Algorithm Suppose 
we are now given a set S of n items, which are distributed evenly across the p processors of an EREW 
BSP computer. Our goal is to sort S in O (log n/ log(h + 1)) communication rcmnds and O(n log n/p) internal 
computation time without us­ing any broadcasts, for h = @(n/p). We achieve this result using a cascading 
method similar to one used by Cole [10]. Let T be a complete rooted d-way tree with each of its leaves 
associated with a sublist St C: S of size at most [n/pi, where d = max{ [(n/p) 1171, 2} (the reason for 
this choice will become apparent in the analysis). Our method proceeds in a series of stages, as in the 
weak-CREW BSP algorithm, with us con­structing the set Ut (w) in each stage, as before: d Ut(v) = u Lt_I(w, 
), 1=1 where each Lt (v) list is defined to be a sample of Ut (v) as in our weak-CREW algorithm. In 
order to perform this construction so as to avoid broadcasts, however, we will accomplish this by constructing 
a larger, augmented list, At(v), such that iYt (v) ~ At(v). We also define a list Dt (v) to be a d2-sample 
of At(v). For each active node v, with parent u and children WI, wz, . . . . wd, we then define d At(v) 
= D~-I(U) U u &#38;-l(?&#38;), z= 1 i.e., At(v) = D~-I (u) U ~~(v). Intuitively, the Dt lists communicate 
information down the tree T in a way that allows us to avoid broadcasts. Indeed, once a copy of an element 
begins to traverse down the tree, then it will never again traverse up (since the D lists are only sent 
to children). Still, even though we are assuming, without loss of generality, that the elements of S 
are distinct, this definition may create duplicate entries of an el­ement in the same list, with some 
traversing down and at most one traversing up. We resolve any am­biguities this may create by breaking 
comparison ties based upon an upward-traversing element al­ways being greater than any downward-traversing 
element, and any comparison between downward­traversing elements being resolved based upon the level 
in T where the elements first began traversing down (where level numbers increase as one traverses down 
T). The goal of each Stage t in the computation, then, is to construct At(v) and Ut (v), together with 
their respective samples Dt (v) and Lt (v). In order to prove that each stage of our algorithm can indeed 
be performed in a constant number of communication rounds on an EREW BSP computer we must estab­lish 
the following bounds (whose proofs are included in the full version): Lemma 3.1: If at most k elements 
of At (v) are in an interval [a, b], then at most (d + l)k + 2(d + 1)2 elements of At+l (v) are in [a, 
b]. This immediately implies the following: Corollary 3.2: If at most k elements of Dt (v) are in an 
interval [a, b], then at most (d+ l)(k + 1) + 3 elements of Dt+l (v) are in [a, b]. In addition, we can 
also show the following: Lemma 3.3: For any two consecutive elements b and c in At(v) let b and c respectively 
be the predecessor of b and the successor of c in At(u), where u is the parent of v in T. There are at 
most (d+ l)(d2 + 1) + 2(d+ 1)2+2 elements of At(u) in the interval [b , c ]. Finally, we have the following: 
Lemma 3.4: For any two consecutive elements b and c in D~_I (u) there are at most (d+ 1)2(cZ4 +5) elements 
of At (v) in the interval [b, c], where u is the parent of v in T. As will become apparent in our algorithm 
de­scription, these bounds are all crucial for establishing that our algorithm runs in the EREW BSP model 
using a constant number of communication rounds per stage. In order to perform the computation for Stage 
t+ 1 using a constant number of communi­cation rounds we assume that we maintain the fol­lowing induction 
invariants for each active node v in T: Induction Invariant: 1. At(v) is ranked into iYt (v). 2. At(v) 
and Dt_l (u) are cross-ranked, where u is the parent of v. 3. A,_ I (v) is ranked into At(v). 4. Dt(v) 
is ranked in Dt_l (v).  We also maintain a load-balancing invariant, sim­ilar to the one we used in 
our weak-CREW BSP al­gorithm, except that we now define a list A stored atanode vtobefull ifA~U(v). Load-balancing 
Invariant: . If a list A is not full, then A is partitioned into contiguous subarrays of size d6 each, 
with each subarray stored on a different processor. If a list A is full, then A is partitioned into contiguous 
subarrays of size d7 each, with each subarray stored on a different processor. Given that the induction 
and load-balancing in­variant hold after the completion of Stage t,our method for performing Stage t+ 
1 is as follows. Computation for Stage t+ 1: For each child w, of v we perform the following com­putation. 
1. For each element a in At (wi) use the ranking of At (w, ) in Ut (w,) to determine if a is also in 
Lt (w,) (together with its rank in Lt (w,) if so). No communication is necessary for this step, given 
Induction Invariant 1. 2. For each such element a in Lt (w,) use the ranking of At (w, ) in Dt l (v) 
to determine the  ranks of the predecessor, b(a), of a andsucces­sor, c(a), ofain Dt l(v). No communication 
is necessary for this step either, given Induction Invariant 2. 3. Foreach ain Lt(w, ),usethe ranks 
of the pro­cessor(s) for b(a) and c(a) in ll~-l(v) to de­termine the respective ranks of b(a) c(a) in 
At_l(v). No communication is necessary for this step. 4. For each a in Lt (w,), request that the pro­cessor(s) 
for b(a) and c(a) in At l (v) send(s) the processor for a the name of predeces­sor, b (a), of b(a) and 
the name of successor, c (a), of b(a) in At(v), using Invariant 3. By Lemma 3.1 and our load-balancing 
invariant, each processor will receive and send at most (d+ l)dG + 2(d + 1)2 = ~(h) messages to im­plement 
this step. 5. Send a (together with its rank in Lt(w,)) to the processor(s) assigned to elements of 
At(v) between b (a) and c (a) to be merged with all other elements of At+l (v) that fall in this range. 
As with the previous step, by Lemma 3.1 and our load-balancing invariant, each processor will receive 
at most (d+ 1) clG+ 2(d + 1)2 = @(h) messages to implement this step. More importantly, by Lemma 3.3, 
each processor will send an element a to at most [((d+ l)(d2+l)+2(d+l) +2)/d ] +1= o(1)  other processors. 
Thus, no broadcasting is needed in order to implement this step. At the parent u of v we assume a similar 
(but sim­pler) computation is being performed. Finally, at node v we perform the following computation: 
1. For each interval [e, j) of elements of At (v) as­signed to a single processor, merge all the ele­ments 
coming from the parent u and children W1, W2, ..., wd to form At+l ( u). Such a proces­sor will receive 
at least one element from each node adj scent to v, plus as many elements of At+l (v) as fall in [e, 
~), for a total of at most d + 1 + (d+ l)d6 + 2(d+ 1)2 = O(h). This mergesort computation amounts to 
a (d+ 1)­way mergesort and can easily be implemented in 0(d7 log(d + 1)) = O((n/p) log(n/p)) inter­nal 
steps. 2. Likewise, for each interval [e, ~) of elements of At(v) assigned to a single processor, merge 
all the elements coming just from v s children W1, W2, ..., wd to form Ut+l (v) (and At+l (v) ranked 
in Ut+l (v), which gives us Induction Invariant 1).  3. Use the rank information derived from the pre­vious 
two steps to rank At+l (v) in Dt (u), giv­ing us half of Induction Invariant 2. Also, rank At(v) in At+l 
(v) giving us Invariant 3 and by an additional calculation a ranking of Dt+l (v) in Dt (v), which is 
Invariant 4. Finally, send a message to each element a in Dt (u) reforming it of its rank in At+l (v) 
so as to complete the other half of Invariant 2. To implement this step requires that each processor 
send at most h messages and each processor receive at most d6(d) = O(h) messages. 4. Finally, repartition 
the lists at each node v so as to satisfy the load-balancing invariant, As­suming that the total size 
of all non-full lists is O(n/d) and the size of all full lists is O(n), then this step can easily be 
implemented in O(1) communication rounds. Let us, therefore, analyze the space requirements of this algorithm. 
The total size of all the U(v) lists on the full level clearly is O(n). Each such list causes at most 
[lU(v) \/dl elements to be sent to v s parent, u. Now the inclusion of these elements in u causes at 
most (d + 1) [lU(v) l/d3] elements to be sent to nodes at distance 1 from u (including v itself). But 
once an element starts traversing down the tree T it never is sent up again. We can repeat this argument 
to est ablish that the existence of U(v) causes at most (d+ 1)2 (\ U(v) l/d51 elements to be sent to 
nodes at distance 2 from u, and so on. Thus, the number of all of these elements that originate from 
u sum to be a geometric series that is O(n/d). Therefore, the total size of all the non-full lists is 
O(n/d). Likewise, the total size of all the lists (and hence the lists on the full level) is O(n). This 
gives us the following theorem: Theorem 3.5: Given a set S of n items stored O(n/p) per processor on 
a p-processor ERE W BSP computer, one can sort S in O(log n/ log(h+ 1) ) com­munication rounds and O 
(n log n/p) internaJ compu­tation time, for h = @(n/p). This immediately implies the following: Corollary 
3.6: Given a set S of n items stored O(n/p) per processor, one can sort S on an ERE W BSP computer with 
a combined running time that is O(W + (L+ gn/p)(log n/ log(n/p))). This bound also applies to the LogP 
model. 4 A Lower Bound for BSP Computations In this section we show that our upper bounds on the number 
of communication rounds needed to sort n numbers on a p-processor BSP computer are opti­mal. Specifically, 
we show that fl(log n/ log(h + 1)) communication steps are needed to compute the or of n bits using an 
arbitrary number of processors in a CREW BSP computer, where h is the number of message that can be sent 
and received by a single processor in a single communication round. Let us begin by formalizing the framework 
for proving our lower bound. Assume we have a set S of n Boolean values Z1, X2, ..., G initially placed 
in memory locations ml, m2, ..., mm with memory cells m(, l)~+l, . . . . m,h stored in the local memory 
of processor pi, for z e {1, 2, . . . . [n/h]}. This, of course, implies that we have at least rn/hl 
proces­sors, but for the sake of the lower bound we allow for an arbitrary number of processors. Moreover, 
we place no upper bound on the amount of addi­tional memory cells that each processor may store internally. 
The goal of the computation is that after some T steps the or of the values in S should be stored in 
memory location ml. Our lower bound proof will be an adaptation of a lower bound proof of Cook, Dwork, 
and Reis­chuk [11] for computing the or of n bits on a CREW PRAM. The main difficulties in adapting this 
proof come from the way the fact that each processor in a BSP computer can send h messages in each communication 
round, rather than just a single value, complicates arguments that bound the amount of information processors 
can communicate by not sending messages. Each processor p, is assumed initially to be in a starting state, 
q;, taken from a possibly-unbounded set of st ates. At the beginning of a round t processor p% is assumed 
to be in some state q;. A round be­gins with each processor sending up to h messages, some of which may 
be (arbitrary) partial broadcasts, and simultaneously receiving up to h messages from other processors. 
Without loss of generality, each message may be assumed to be the contents of one of the memory cells 
associated with the sending pro­cessor, since we place no constraints on the amount of information that 
may be stored in a memory cell nor on the number of memory cells that a processor may contain. A processor 
then enters a new state q~+l that depends upon its previous state q; and the values of the messages it 
has received. A round com­pletes with a processor possibly writing new values to some of its internal 
memory cells based upon its new state qj+l. Before analyzing the most general situation, let us first 
prove a lower bound for the oblivious case, where the determination of whether a processor pa will send 
a message to processor pj in round t de­pends only upon the value of pa and t,and not on the input. Of 
course, the contents of such a mes­sage could depend upon the input. For input string I=(Z1, Z2, ..., 
~n) of Boolean values, let I(k) de­note the input string (zl, zz,. . . . ~k,. . . . %), where ~k denotes 
the complement of Boolean value zk. is a critical input for function ~(1) if ~(l) # ~(l(k)) forallk E{l,2,..., 
n}. (Note that 1 = (O, O,.. ., O) is a critical input for the or function.) Say that in­put index k affects 
[11] processor pi in round twith input I if the state of p, on input I after round t differs from the 
state of processor p, on input I(k) after round t.Likewise, say that input index k aj­fects memory cell 
mi in round twith input I if the contents of m, on input I after round tdiffers from the contents of 
m, on input l(k) after round t. Theorem 4.1: If f : {O, I}n -+ {O, 1} has a criti­cal input, then any 
oblivious CREW BSP computer that computes f requires Q(log n/ Iog(h + 1)) com­munication rounds. Proofi 
Let K(pi, t, 1) (respectively, L(m,, t,1))be the set of input indices that affect processor p, (resp., 
memory cell ml) in round twith input 1. Further, let Kt and Lt satisfy the following recurrence equations: 
K. =O, (1) Lo =1, (2) Kt+l = Kt + hLt, (3) Lt+l = Kt+l + Lt. (4) Note that it suffices to prove that 
IK(P,, t, 1) I < Kt and [L(p,, t, I) I S Lt, for Kt and Lt are both at most [2(h+l)] , and if 1 is a 
critical input for f, then every one of the input indices must affect memory cell ml. That is, if ml 
= ~(1), then lL(ml, T, 1)[ = n, which implies that T is fl(log n/ log(h + 1)). In the full version we 
show how to establish the above bounds on lK(pz, t, 1)1 and lL(p,, t, 1)1 by induction on t.E The main 
difficulty in generalizing this result to non-oblivious computations is that in the non­oblivious case 
a processor pi can receive information from a processor pj by pj not sending a message to pi. Still, 
as we show in the next theorem, this ability cannot alter the asymptotic performance of a CREW BSP computer 
by more than a constant factor for computing the value of a function with a critical input. Theorem 
4.2: If $: {O, I}n + {O, 1} has a critical input, then any CREW BSP computer that com­putes j requires 
Q (log n/ log(h + 1)) communication rounds. Proofi Let K(p,, t,l) and L(rni,t,Q)be asin the proof of 
Theorem 4.1. But now let ~t and Lt be defined by the following recurrence relations: K() =o, (5) Lo = 
1, (6) Kt+l = (2h + l)K, + hLt, (7) Lt+l = Kt+l + Lt. (8) As in the previous proof, it suffices to show 
that ]K(p,, t,l)[ < K, and \L(m,, t,I)l < L,, for K, and Lt are both at most [3(h + l)]~. We establish 
these bounds on lK(pi, t, 1)1 and lL(p,, t, 1)/ by induction on t. First, note that K(p,, t, 1) is empty 
in round t = O, and L(rni, O,1) = {2}if2E{l,2,..., n} and otherwise L(rn,, O, 1) is empty. At the beginning 
of round t a processor p% receives the contents of at most h memory locations, and it also receives information 
by noting that some processors did not send pi a message. Still, after it incorporates this information 
into its new state q~+l it optionally writes to its local memory, as in the pre­vious proof. Thus, if 
we can establish Equation (7), then Equation (8) immediately follows. Say that input index k possibly-causes 
a processor p~ to send a message to processor p, in round twith 1 if p~ sends a message to processor 
p, in round t on input I(k). Using this notion we bound K(IZ, t + 1,1) as a subset of K(p,, t,l) u U 
L(TTJJ, LD u y(fbt>~)> Jcl for some index set Z with Ill < h, where Y(p,, t, 1) denotes the set of all 
indices k that possibly-cause some processor pj to send a message to p, with 1. Thus, we must bound T-= 
lY(p,, t, l)]. So, let Y = Y(pi, t,l) = {kl, kz, . . ..k~} be the set of in­dices IC3that possibly-cause 
a processor P(Ic3) to send a message to p% with I. Note that if r < hKt, then we have established Equation 
(7), so for the remain­der of this proof let us assume that r > hK~ (we will show that if this is the 
case, then r < 2hKt). Say that a subset Y ~ Y is processor-dis~oint if, for any kj and ,@ in Y , p(kj 
)# p(kj ). Claim: If Y = {kJl, kj,,..., k,,,+l} is a processor-disjoint subset of Y of size h+ 1, then 
there is an index kj, in Y such that kj, affects processor p(k~, ) in round twith I(kj, ), where ~jt 
# kj,. Proof (of claim): If this is not the case, then on input l(k~l )(kj, ) . . . (kJ,,+, ) there would 
be h + 1 dif­ ferent messages sent to processor p,, which would violate the correctness of the BSP comput 
iation. (of claim) As done by Cook, Dwork, and Reischuk [11], we employ a combinatorial graph argument 
to de­rive a bound on IY 1. Consider a bipartite graph G whose two node sets are {kl, kz, . . . . !-cr} 
and {pap, . ~., p(k,) }. Let there be an edge be­tween kj and p(kl ) if kj affects p(kl ) in round t 
with I(kl ). Let e denote the number of edges in G. The degree of any node P(Ic, ) is lK(p(kj), L ~(h))l, 
which, by our induction hypothesis, is bounded by Kt. Thus, e < rKt. We can also derive a lower bound 
on e, using our claim above. Let G be a sub­graph of G defined by a processor-disjoint subset of Y ofsize 
h+ 1 together with the processor nodes associated with this subset. Then, by our claim, G must contain 
at least one edge of G. Thus, letting g denote the number of processor-disjoint subsets (like G ) of 
size h+ 1, together with the associated proces­sor nodes, we can write g 5 e. Since placing a node in 
such a G eliminates at most Kt other candidates, r(r Kt)(r 2Kt) ...(r -hKt) g> (h+ 1)! > r-(r hK~)h 
(h+ 1)! Therefore, r(r hK~)h < (h + 1) !rK~, which im­plies that r < hKt + [(h + l)!Kt]lih. By Sterling 
s approximation, then, r < 2hKf, which establishes Equation (7) and completes the proof of the theo­rem. 
Acknowledgements We would like to thank Bob Cypher, Faith Fich, S. Rao Kosaraju, Greg Plaxton, and Les 
Va,liant for several helpful conversations or e-mail exchanges re­garding topics related to this paper. 
 References [1]M. Adler, J. W. Byers, and R. M. Karp. Parallel sorting with limited bandwidth. 7th SPA 
A, 129 136, 1995. [2] A. Aggarwal, A. K. Chandra, and M. Snir. Com­munication complexity of PRAMs. T. 
C.S ., 71:3 28, 1990. [3] M. Ajtai, J. Kom16s, and E. Szemer6di. Sorting in clog n parallel steps. Combinatorics, 
3:1 19, 1983. [4] S. G. Akl. Parallel Sorting Algorithms. Academic Press, 1985. [5] K. E. Batcher. Sorting 
networks and their appli­cations. Proc. 1968 Spring Jotnt Computer Conf., 307-314, Reston, VA, 1968. 
[6] G. Bilardi and F. P. Preparata. Lower bounds to processor-time tradeoffs under bounded-speed mes­sage 
propagation. dth WADS, LNCS 955, 1 12. Springer-Verlag, 1995. [7] D. Bitten, D. J. DeWitt, D. K. Hsiao, 
and J. Menon. A taxonomy of parallel sorting. ACM Comp. Surveys, 16(3):287-318, 1984. [8] G. E. Blelloch, 
C. E. Leiserson, B. M. Maggs, C. G. Plaxton, S. J. Smith, and M. Zagha. A comparison of sorting algorithms 
for the connection machine CM-2. 3rd SPAA, 3-16, 1991. [9] V. Chv&#38;al. Lecture notes on the new AKS 
sort­ing network. Report DCS-TR-294, Rutgers Univ., 1992. [10] R. Cole. Parallel merge sort. SIAM J. 
Comput., 17(4) :770 785, 1988. [11] S. A. Cook, C. Dwork, and R. Reischuk. Upper and lower time bounds 
for parallel random access machines without simultaneous writes. SIAM J. Comput., 15:87 97, 1986. [12] 
D. E. Culler, R. M. Karp, D. A. Patterson, A. Sa­hay, K. E. Schauser, E. Santos, R. Subramonian, and 
T. von Eicken. LogP: Towards a realistic model of parallel computation. dthACM Symp. on Prtnc. and Pratt. 
of Par. Prog., 1 12, 1993. [13] R. Cypher and J. L. C. Sanz. Cubesort: A parallel algorithm for sorting 
N data items with S-sorters. J. of Algorithms, 13:211-234, 1992. [14] R. E. Cypher and C, G. Plaxton. 
Deterministic sorting in nearly logarithmic time on the hypercube and related computers. J. C. S. S., 
47:501 548, 1993. [15] F. Dehne, X. Deng, P. Dymond, A. Fabri, and A. A. Khokhar. A randomized parallel 
3D convex hull algorithm for course grained multi computers. 7th SPAA, 27-33, 1995. [16] F. Dehne, A. 
Fabri, and A. Rau-Chaplin. Scalable parallel geometric algorithms for coarse grained multicomputers. 
9th SCG, 298 307, 1993. [17] W. D. Frazer and A. C. McKellar. Samplesort: A sampling approach to minimal 
storage tree sorting. J. ACM, 17(3):496-507, 1970. [18] A. V. Gerbessiotis and L. G. Valiant. Direct 
bulk­synchronous parallel algorithms. J. of Par. and Dtst. Comp., 22:251 267, 1994, [19] P. B. Gibbons. 
A more practical PRAM model. (1st) SPAA, 158-168, 1989. [20] M. T. Goodrich and S. R. Kosaraju. Sorting 
on a parallel pointer machine with applications to set expression evaluation. J. ACM, to appear. [21] 
W. L. Hightower, J. F. Prins, and J. H. Relf. Imple­mentation of randomized sorting on large parallel 
machines. dth SPAA, 158 167, 1992. [22] J. J4J6. An Introduction to Parallel Algorithms. Addison-Wesley, 
1992. [23] R. Karp, M. Luby, and F. Meyer auf der Heide. Efficient pram simulation on distributed machines. 
2dth STOC, 318-326, 1992. [24] R. M. Karp and V. Ramachandran. Parallel al­gorithms for shared memory 
machines. In J. van Leeuwen, editor, Handbook of Theoretical Computer Sczence, 869 941. Elsevier/MIT 
Press, 1990. [25] R. M. Karp, A. Sahay, E. Santos, and K. E. Schauser. Optimal broadcast and summation 
in the LogP model, 5th SPAA, 142-153, 1993. [26] C. Kruskal, L. Rudolph, and M. Snir. A complex­ity theory 
of efficient parallel algorithms. T. C. S., 71:95 132, 1990. [27] F. T. Leighton. Tight bounds on the 
complexity of parallel sorting. IEEE Trans. on Computers, C­34(4) :344-354, 1985. [28] F. T. Leighton. 
Introduction to Parallel Algorithms and Architectures: Arrays, Trees, Hypercubes. Mor­gan Kaufmann, 1992. 
 [29] H. Li and K. C. Sevcik. Parallel sorting by overpar­titioning. 6th SPAA 46 56, 1994. [30] Y. Mansour, 
N. Nisan, and U. Vishkin. Trade­offs between communication throughput and par­allel time. 26th STOC, 
372 381, 1994. [31] K. Mehlhorn and U. Vishkin. Randomized and de­ terministic simulations of PRAMs by 
parallel ma­chines with restricted granularity of parallel mem­ories. Acts lnformatica, 9(1) :29 59, 
1984, [32] J. M. Nash, P. M. Dew, M. E. Dyer, and J. R. Davy. Parallel algorithm design on the WPRAM 
model. Report 94.24, School of Comp. Sci., Univ. of Leeds, 1994. [33] D. Nassimi and S. Sahni. Parallel 
permutation and sorting algorithms and a new generalized connec­tion network. J. ACM, 29(3):642-667, 
1982. [34] C. Papadimitriou and M. Yannakakis. Towards an architecture-independent analysis of parallel 
algo­rithms. 20th STOC, 510 513, 1988. [35] M. Paterson. Improved sorting networks with o(log n) depth. 
Algorzthmzca, 5(1):75 92, 1990. [36] C. G. Plaxton. Eficzent Computation on Sparse Int.rconnectzon Networks. 
PhD thesis, Dept. of Comp. Sci., Stanford Univ., 1989. [37] J. H. Reif. Synthesis of Parallel Algorithms. 
Morgan Kaufmann, 1993. [38] J. H. Reif and L. G. Valiant. A logarithmic time sort for linear size networks. 
J. A CM, 34(1) :60 76, 1987. [39] H. Shi and J. Schaeffer. Parallel sorting by regular sampling. J. Par. 
and Dist. Comp., 14:362 372, 1992. [40] L. (2. Valiant. A bridging model for parallel com­putation. Comm. 
A CM, 33:103-111, 1990. [41] L. G. Valiant. General purpose parallel architec­tures. In J. van Leeuwen, 
ed., Handbook of The­oretical Computer Science, 943 972. Elsevier/MIT Press, 1990. 
			