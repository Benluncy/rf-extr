
 ATR Media Integration &#38; Communication Research Lab and University of Maryland Real-time 3D computer 
vision gives users control over both the movement and facial expression of a virtual pup­pet and the 
music to which the puppet dances. Multiple cameras observe a person, and human silhouette analysis achieves 
real-time 3D estimation of human postures. Facial expressions are estimated from images acquired by a 
viewing-direction controllable camera, so that the face can be tracked. From the facial images, deformations 
of each facial component are estimated. The estimated body postures and facial expressions are reproduced 
in the pup­pet model by deforming the model according to the estimated data. All the estimation and rendering 
processes run in real time on PC-based systems. Attendees can see themselves dancing in a virtual scene 
as virtual puppets. Kazuyuki Ebihara ATR Media Integration &#38; Communication Research Lab 2-2 Hikaridai 
Seika-cho Soraku-gun Kyoto 631 Japan ebihara@mic.atr.co.jp Collaborators Kazuyuki Ebihara Jun Kurumisawa 
Tatsumi Sakaguchi Jun Ohya ATR Media Integration &#38; Communication Research Lab Larry S. Davis Thanarat 
Horprasert R. Ismail Haritaoglu University of Maryland 124 Enhanced Realities Conference Abstracts and 
Applications
			