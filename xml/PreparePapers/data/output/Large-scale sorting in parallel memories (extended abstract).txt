
 Large-Scale Sorting in Parallel Memories (extended abstract) Mark H. Nodine and Jeflrey Scott Vittert 
Dept. of Computer Science Brown University Providence, R. I. 02912-1910 Abstract. We present several 
algorithms for sort­ing efficiently with parallel two-level and multilevel memories. Our main result 
is an elegant, easy-to­implement, optimal, deterministic algorithm for exter­nal sorting with P disk 
drives. This result answers the open problem posed by Vitter and Shriver. Our measure of performance 
is the number of parallel in­put/output (1/0) operations, in which each of the P disks can simultaneously 
transfer a block of B contigu­ous records. Our optimal algorithm is deterministic, and thus it improves 
upon the optimal randomized al­gorithm of [ViS] as well as the well-known deterministic but nonoptimal 
technique of disk striping. The second part of the paper broadens our cover­age from two-level memories 
to more general multilevel memories. In particular we consider the blocked uni­form memory hierarchy 
(UMH) introduced by Alpern, Carter, and Feig, and its parallelization P-UMH, along with new variants. 
We give optimal and nearly-optimal algorithms for a wide range of bandwidth degrada­tions, including 
a parsimonious algorithm for constant bandwidth. We also develop optimal sorting algo­rithms for all 
bandwidths for other versions of UMH and P-UMH, including natural restrictions we intro­ * Support was 
provided in part by an IBM Graduate Fellow­ship and by a National Science Foundation Presidential Young 
Investigator Award CCR 9047466 with matching funds from IBM Corporation. t Support was provided in part 
by a National Science Foun­dation Presidential Young Investigator Award CCR-9047466 with matching funds 
from IBM Corporation, by National Sci­ence Fosmdation grant CCR-9007851, by the U.S. Army Re­search Office 
under grant DAAL03 91 G-o035, and by the Of­fice of Naval Research and the Defense Advanced Research 
Projects Agency uader contract NOO014-83-K 0146 and ARPA order 6320, amendment 1. Permission to copy 
without fee all or part of this material is granted pro­vided that the copies are not made or distributed 
for direct commercial advantage, the ACM copytight notice and the title of the publication and its date 
appear, and notice is given that copying is by permission of the Association for Computing Machinery. 
To copy otherwise, or to republish, requires a fee andlor specific permission. duce called RUMH and P-RUMH, 
which more closely correspond to current programming languages. 1 Introduction Sorting is reputed to 
consume roughly 20 percent of computing resources in large-scale installations [Knu, LiV]. Of particular 
importance is external sorting, in which the records to be sorted are too numerous to fit in the processor 
s main memory and instead are stored on disk. The bottleneck in external sorting is the time needed 
for the input/output (1/0) operations. Typi­cally data are transferred in large units or blocks; this 
blocking takes advantage of the fact that the seek time is usually much longer than the transmission 
time per record. An increasingly popular (and necessary!) way to avoid the 1/0 bottleneck is to use many 
disk drives working in parallel [GHK, GiS, Jil, Mag, PGK, Uni]. In previous work, Aggarwal and Vitter 
[AgV] pre­sented optimal upper and lower bounds on the num­ber of 1/0s needed for sorting and related 
problems of size N using a two-level memory model in which P physical blocks, each consisting of B contiguous 
records, can be transferred simultaneously in a sin­gle 1/0. This model generalized the initial work 
of Floyd [Flo] and Hong and Kung [HoK]. The lower bounds are based solely on routing concerns and thus 
hold for an arbitrarily powerful adversary, except for case when lkf and B are extremely small, in which 
case the comparison model is used. The model in [AgV] is somewhat unrealistic, however, because in practice 
secondary storage is usually partitioned into separate disk drives, each capable of transferring only 
one block per 1/0. Vitter and Shriver considered the more realistic P-disk model, in which the secondary 
storage is parti­tioned into P physically distinct disk drives [ViS] (see Figure 1). (Note that each 
head of a multihead drive can count as a distinct disk in this definition, as long as each head can operate 
independently of the other heads on the drive. ) In a single (parallel) 1/0 operation, each @ 1991 ACM 
089791-438-4/91/000710029 $1.50 of the P disks can simultaneously transfer one block of Brecords. Thus, 
Pblocks can retransferred per I/0, as in the [AgV] model, but only if no two of the blocks access the 
same disk. This assumption is reasonable in light of the way real systems are constructed. The measure 
of performance is the number of par­ allel 1/0s required; internal computation time is ig­ nored. In 
practice, though, the algorithms dealt with are also very efficient in terms of internal procesing. 
This model also applies to the case in which each of the P disks is controlled by a separate CPU with 
inter­nal memory capable of storing M/P records, and the P CPUS are connected by a network that allows 
some basic operations (like sorting of the M records in the internal memories) to be performed quickly 
in parallel. The bottleneck can be expected to be the 1/0. Vitter and Shriver presented a randomized 
version of distribution sort using two complementary partition­ing techniques. Their algorithm meets 
the 1/0 lower bound given earlier for the more lenient model of [AgV], and thus the algorithm is optimal. 
It can outperform the well-known deterministic technique of disk striping by a logarithmic factor in 
terms of the number of 1/0s. They posed as an open problem the question of whether there is an optimal 
algorithm that is deterministic. In the next section, we answer the open question posed in [ViS] and 
present an optimal deterministic sorting algorithm called Greed Sort. It performs a merge sort in a greedy 
way, using a priority scheme during the first part of each merge process to ap­proximately merge the 
runs together. A second part of the merge process completes the merging. Oddly enough, the intuitions 
of [ViS] suggested that merge sorting with P disks was particularly difficult to do, as opposed to distribution 
sorting. In Section 3, we generalize our two-level perspec­tive of Section 2 and consider multilevel 
hierarchical memories. The levels of memory in many large-scale computer systems progress from very small 
but very fast registers to successively larger but slower compo­nents, such as several layers of cache, 
primary memory, disks, and archival storage. We consider in particu­lar the uniform memory hierarchy 
(UMH) proposed re­cently by Alpern, Carter, and Feig [ACF], as a followup to the elegant HMM and BT models 
of {Aggarwal, Alpern, Chandra, Snir} [AAC, ACSa]. Level ./? con­sists of 0P24 memory locations organized 
as apt blocks of size pf, for constants a and p. Data are transferred between levels 1 and t + 1 in units 
of blocks of size pt, with a communication bandwidth of b(l). A model for parallel hierarchies was introduced 
by Vitter and Shriver, in which P hierarchies are con­nected at their base level via an interconnection 
net­work. Optimal algorithms were presented for the paral­lel versions of HMM and BT (which we denote 
P-HMM and P-BT) [ViS]. The same parallel construction can be applied to UMH; we call the result P-UMH. 
 We present in Section 3 optimal and near-optimal sorting algorithms for UMH and P-UMH for a wide range 
of bandwidth rates b(l), and we present a parsi­ monious schedule for merge sort for the case b(Z) = 
1. We also introduce a natural and easy-to-program re­ striction of UMH, called random-access UMH (or 
RUMH), for which we have optimal upper and lower bounds for all bandwidths and amounts of parallelism. 
We do the same for a sequential model of UMH called SUMH. 2 Greed Sort The parameters for our two-level 
memory model (or disk 1/0 model) in Figure 1 are N = # records in the file; M = # records that can fit 
in primary memory; B = # records per block; P = # disk drives; where M < N, and 1< PB < 1(M M~)/2], 
for ~ < 1. Our measure of performance is the number of parallel 1/0s; during a parallel 1/0, each disk 
can simultaneously transfer one contiguous block of data. For purposes of reference, we number the block 
loca­tions on the P disks a the cyclical way such that Fig­ure 2 represents a sorted list for the case 
B = 1.The B records within each block are numbered contiguously by their relative positions in the block. 
Our Greed Sort algorithm is a type of merge sort. We create initial input runs (sorted lists) of size 
N/M by repeatedly reading in a memoryload, sorting it, and writing it back to the disks. In each subsequent 
pass, we merge together R = -/2 input runs at a time to form larger runs, which are used as input runs 
for the next pass. Each pass will be shown to take O(N/PB) 1/0s, giving us a total 1/0 bound ofl 1+ 
@@@~ N))=o(#?&#38;), o~(( PB which is optimal, by the lower bound of [AgV]. The analysis is presented 
in Section 2.2. Theorem 1 Greed Sort determinisiically sorts N ~ M ? ecoTds with O ( &#38; log ~/ log 
~ ) parallel 1/0s, which is optimal. The basic idea behind Greed Sort is the following: Let us assume 
that each of the R runs to be merged is stored consecutively on disk, beginning on disk 1 and cycling 
through the P disks. In each parallel read oper­ation, the one or two tbest available blocks from each 
1We use the notation log z, where z ~ 1, to denote the quan­tity max{l, logz z}. disk are read into primary 
memory: the block with the smallest minimum key value and the block with the smallest maximum key value. 
Treating each of the P disks independently, we sort its 2B records in primary memory and write the smallest 
B to the output list that we re forming; we put the largest B records at the front of the run from which 
the smallest minimum was taken (note that this run remains sorted after this operation). Figure 3 gives 
an example of this opera­tion assuming that Run 1 contains the block with the smallest minimum and Run 
2 the block with the small­est maximum on disk j. Ties are broken arbitrarily. If the blocks with the 
smallest minimum and the small­est maximum are the same block, we read only the one block and write it 
to the output list. This operation results in an approximately merged list. The crucial observation is 
that the records are within RPB = P~/2 positions of their correct sorted locations. By an appropriate 
use of clus­tering throughout the course of the algorithm, this ap­proximately merged list can be completely 
merged by a single pass consisting of several applications of the columnsort algorithm of Leighton [Lei] 
applied to sub­files of size P-. Then the next merge begins. Columnsort is easiest to visualize as sorting 
into column-major order a matrix with r rows and c columns, with the requirement that c divides T and 
r > 2(C 1)2. It has eight steps, of which the odd­numbered steps are all the same, consisting of sorting 
the records in each column. Steps 2 and 4 are a trans­pose operation, and Steps 6 and 8 amount to a cyclical 
shift by T/2. The pseudocode for the Greed Sort algorithm is given in Figure 4. Since all the records 
within a run are sorted, consecutive blocks on the same disk from a run are in non-decreasing order. 
Thus, the block with the smallest minimum (or maximum) on a given disk must be the next unread block 
on that disk from some run. The collect ion of next unread blocks, one for each (run, disk) pair, is 
called the candidate set of blocks and is stored in a priority queue. There are P disks and R runs, so 
the candidate set has cardinality PR. We assume for convenience that the runs are separated on each disk 
by blocks containing the key value +00, that is larger than any key. The algorithm uses nezt [z, j] to 
keep track of the next block of run i to be read from disk j. The set of all blocks nezt [z, j] com­ 
prises the candidate set. The maximum and minimum key fields of block nezt[i, j] are stored in biggest[i, 
j] and smallest [i, j], respectively. We do our 1/0 opera­ tions into the buffers bl and b2, which each 
consist of P smaller buffers. Buffers bl [j] and b2[j], for 1 s j s P, each hold B records from disk 
j; we denote their max­ imum and minimum keys by max(bl [j]), min(bl [j]), max(b2[j])1 and min(b2[j]). 
In the pseudocode below, when we use the construct do in parallel, we mean that the l/V wlthln the loop 
should be done in paral­lel, not that the actual computation needs to be done in parallel. 2.1 Proof 
of Correctness The correctness of Greed Sort depends on showing that each merge pass correctly merges 
the R = -/2 runs into a single sorted run. Theorem 2 Each sequence of an approximate merge followed by 
columnsorts in the Greed Sort algorithm correctly merges R runs. Theorem 3 below shows that each record 
in the approximately merged output list formed from the R runs is at most L = RPB locations from its 
correct sorted location. That the columnsorts on successive overlapping subfiles of size 2L complete 
the sorting is shown in Theorem 4. Together, Theorems 3 and 4 prove Theorem 2. Theorem 3 In the app70zimately 
merged output list formed from R runs, each record is at most RPB loca­ tions from its correct sorted 
location. This theorem is proved using Lemmas 1-3. Lemma 1 For any two TecoTds x < y written to disk 
j in the appTozimately merged output list, z will be lo­cated at most R 1 blocks later than y on disk 
j. PToof Sketch: Let record y be written to disk j of the output list at step t. (See Figure 3.) (By 
step t, we mean the tth time PB records are written to the output list being formed.) In the example 
given in Figure 3, y ~ {1,2,3, 4}. The crucial observation is that at most one block in every run contains 
values less than y, and that all such values will be written in the next R 1 blocks. 0 We need a definition 
before presenting Lemma 2, which generalizes Lemma 1 by considering all the disks. Definition 1 A sequence 
is called L-regressive if for any two elements z < y, y does not precede z by more than L elements in 
the sequence. Lemma 2 The approximately sorted output list is RPB-regressive. Proof: Let y be output 
on disk j. Then at most one block in any run on disk i < j contains a value less than y as suggested 
by Figure 5. All elements less than y will either be output at the same time or within the next R 1 
time steps, using the same argument as in the proof for Lemma 1. Furthermore, any disk i > j has already 
written all records with keys less than y. Com­bining with Lemma 1, which tells about other records on 
disk j, and noting that each stripe on the collec­tion of disks contains PB records, the maximum re­gression 
is RPi3. 0 Lemma 3 If a list is L-regressive, then every recoTd is at most L locations fTom its comect 
soTted location. Proof Sketch: For simplicity, let all the records have unique keys. Assume that y is 
the jth smallest record and occurs at position i where i < j L. We derive a contradiction by showing 
that there exists an x < y that succeeds y by more than L records and hence the list is not L-regressive. 
There are j 1 records less than y. In order to meet the L-regressive condition, they must all occur 
in the range 1, . . . . i + L. There are i + L < j locations up to the point that is L records beyond 
y, of which y is occupying one. At least one record less than y must be out of the desired range, since 
i + L 1 < j 1. This same argument also shows that the jth record cannot beat any location i > j+ L. 
The case of duplicate keys is more complicated and is covered in the full paper. 1 Lemmas 2 and 3 directly 
prove Theorem 3. Fi­nally, we demonstrate that the final pass of columnsorts finishes the sort. Theorem 
4 If every element in a list is within a dis­tance of L of its sorted location, then a series of sorts 
of size 2L, beginning at every Lth location, will sufice to complete the sort. Proof Sketch: Let there 
be N total records, so that we perform a total of N/L 1 sorts of size 2L. The proof proceeds by induction 
on the number of sorts performed. During step t we sort records tL + 1, tL + 2 ,. ... tL + 2L. We have 
the following invariants at the beginning of step t (see Figure 6) and show that each step maintains 
the invariants: 1. All the records 1, ..., tL are completely sorted. 2. No record in tL + 1,.. ., N 
is more than 2L before  or L after its final position. The last step by definition sorts the last 2L 
records, so that at its conclusion, the whole list is sorted. 0 2.2 Analysis of the Algorithm To prove 
Theorem 1, we first show by a clustering tech­nique that the amount of storage space required for the 
data structures is small enough. Theorem 5 The amount of primary memory space needed for the data structures 
of Greed Sort is O(iW~), forp<l. Proof Sketch: The number of runs that we must merge together in order 
to obtain optimal performance is -/2. AS we pointed out in Section 2, the can­ didate set requires a 
total of P-key fields to be kept in primary memory to decide what blocks to read next. At first glance, 
it seems if P = O(M) and B = 1 that fl(M3/2) storage space will be required in primary memory, which 
is clearly impossible. However, we can use a partial disk striping method throughout the course of the 
algorithm. Assume that P = P(M) grows faster than M , where O < CY < 1/2. We can cluster our P disks 
into clusters of P = Ma clusters of B = BP/P disks synchronized together. Each of the P clusters acts 
like a logical disk with block size B . Thus, the number of primary storage locations we need is at most 
P ~~ ~ Ma~~ = 0( M +1J2). but the number of 1/0s remains the same within a constant factor. We set /3 
= CY+ 1/2. 0 In order to demonstrate that Greed Sort has an optimal 1/0 bound, we need to analyze the 
1/0 effi­cienc y of the columnsort subroutine. Theorem 6 Columnsort sorts N ~ Pm records with O(N/PB) 
parallel I/Os. Proof: First we show that columnsort produces a cor­ rectly sorted sequence when N ~ Pm. 
We define the number of rows in the matrix to be T = M, so that each column can be sorted internally. 
We have making use of our assumption that 2PB ~ M. Thus, the number of columns in our application of 
columnsort is c = N/r ~ ~, and the columnsort requirement that rI ~ 2(c 1)2 is met. Thus, the columnsort 
works correct ly. Steps 1, 3, 5, 6, 7, and 8 can be done easily with O(N/PB) 1/0s. The transpose-like 
operation in Steps 2 and 4 can be done with O(N/PB) 1/0s by a vari­ant of the p x q matrix transpose 
algorithm of [ViS], for p= M and q= N/M s Pm, which we omit for brevity. The resulting number of 1/0s 
is also O(N/PB). 0 The greedy merge reads each record at most three times (once for updating biggest 
and smallest and up to twice for merging) and writes each record at most twice, taking full advantage 
of parallel block transfer. The columnsort routine is called 2N/k 1 times, each time using O(k/PB) 1/0s, 
for a value of k that differs from pass to pass. Thus the total number of parallel 1/0s is O(N/PB): Corollary 
1 Each merging step requires O(N/PB) parallel 1/0s. By the remarks immediately before Theorem 1, this 
concludes the proof of Theorem 1.  3 Uniform Memory Hierarchies Several interesting and elegant hierarchical 
memory models have been proposed recently to model the many levels of memory typically found in large-scale 
com­puter systems. The levels of memory range from very and Snir [ACSa] represents a notion of block 
trans­fer applied to HMM; in the BT model, access to the t+l records at locations z t, z t+l, . . . . 
z takes time f(z) + t.Typical access cost functions are ~(z) = log z and ~(z) = z , for some a >0. A 
model similar to the BT model that allows pipelined access to memory in O(log n) time was developed independently 
by Luccio and Pagli [LuP]. Optimal sorting algorithms for each of these models have been developed [AAC, 
ACSa, LuP]. small but very fast registers to successively larger but slower components such as caches, 
primary memory, disks, and archival storage. The HMM model of Aggarwal, Alpern, Chandra, and Snir [AAC] 
allows access to individual location z in time ~(z). The BT model of Aggarwal, Chandra, For the same 
reason that multiple disks are use­ful in overcoming the 1/0 bottleneck, Vitter and Shriver [ViS] considered 
the HMM and BT models in a parallel framework. In the P-HMM (respectively, P-BT) model [ViS], there are 
P HMM (respectively, BT) hierarchies, which can function independently, as shown in Figure 7. Communication 
between the P hi­erarchies takes place at the base memory level (call it level O), which consists of 
location 1 from each of the P hierarchies. The P base memory level locations are interconnected via a 
network such as the hypercube or cube-connected cycles so that the P records in the base memory level 
can be sorted in O(log F ) time (perhaps via a randomized algorithm [ReV] ). Vitter and Shriver introduced 
optimal randomized sorting algorithms for P-HMM and P-BT [ViS]. The algorithms were based on their randomized 
two-level partitioning technique applied to the optimal single-hierarchy algorithms for HMM and BT developed 
in [AAC, ACSa]. In this section we concentrate on a newer model introduced by Alpern, Carter, and Feig 
[ACF, ACSb], called uniform memory hierarchies (UMH), which offer an alternative model of blocked multilevel 
memories. In the UMHb(4) model (for integer constants a, p ~ 2), the lth memory level (as illustrated 
in Figure 8) con­sists of CYpt blocks, each of size p<; it is connected via buses to levels 4 1 and 
.4+ 1. Each individual block on level 1 can be randomly accessed as a unit and trans­ferred to or from 
level Z+ 1 at a bandwidth of b(.?); that is, each block transfer takes time pt/b(Z). The CPU re­sides 
at level O. The initial input of N elements resides at level [+ logP :1. We can consider parallel UMH 
hierarchies (analo­gous to P-HMM and P-BT), and we call the result­ing model P-UMH. (This is fundamentally 
different from the parallel type of UMH called UPHM mentioned in [ACF].) A UMH or P-UMH program consists 
of a sched­ule of choreographed block transfers and computations. If a RAM program that runs in T(N) 
steps can be scheduled in UMH in ~ T(N) time, the program is said to be parsimonious; note that the constant 
factor must be 1. If the UMH program runs in time O(T(N)), it is said to be eficient. A UMH program whose 
run­ning time is within a constant factor of best possible in the UMH model is said to be optimal. 3.1 
UMH and its Parallelization Optimal sorting in O(N log N) time in UMH is possible only when the bandwidth 
b(l) at level Z is Q( 1/4), or else the time required just to access the N records will be greater than 
O(N log N). Many buses maybe active simultaneously in the UMH model, so conceivably it is possible to 
sort in O(N log N) time even with small bandwidth b(t) = l/(1+ 1). Recently other authors announced an 
efficient UMH sorting algorithm for the case b(l) = l/(1+ 1), based on the optimal two-level distribution 
sort al­gorithm of [AgV], but their UMHll(f+l) algorithm turned out to be inefficient, with a running 
time of Q(N log N), for c >3. Whether or not an O(N log N)­time UMH1i(t+l) algorithm exists is still 
open. In this section we give a near-optimal sorting al­gorithm for the small bandwidth case b(l) = 1/(1+ 
1), and optimal sorting algorithms for several other band­widths. For the special case of constant bandwidth, 
we present a parsimonious algorithm. Since optimal sort­ing seems to require nonoblivious UMH programs, 
the oblivious UMH model of [ACF] must be modified in a reasonable way. In Theorem 7, we assume that the 
-t?th level of the hierarchy can initiate a transfer from the (-t + l)st level when one of its blocks 
becomes empty. In Theorem 8, we assume that the CPU can originate the transfer of a block at level 4 
given the address of the block, with suitable delay. Theorem 7 A variant of merge soTt can be scheduled 
in UiWH1 parsimoniously, assuming a > 2 and ap > 6. The basic idea is to schedule a systolic binary merge 
sort in such a way that the CPU is always kept busy (after a small initial delay and with a small final 
delay for writing the results back). After the initial delay, the CPU (level O) reads one element from 
each of the two lists. At every time step after the initial delay, the CPU will write the smaller element 
to the output list and then read the next element from that list that had the minimum at the previous 
step. We use a double-buffering scheme so that level .4, for t ~ 1, has two blocks from each of the two 
lists being merged. It also has two blocks for the output list. When level 1 1 requests a subblock from 
one of the lists, and this re­quest causes level 1 s buffer to be emptied, then level 1 requests the 
next block from level 1 + 1. In this way, level t always has a sub-block for level -t? 1 available on 
demand. The output blocks fill up at a known rate, so they can be scheduled in advance (again using double­buffering 
to keep an empty subblock available for writ­ing from level 4 1). If one list empties before the other, 
then we immediately begin to send a new list down. The CPU can keep track of how many elements have been 
read from each list, so that it can know when one list is finished and that it just needs to copy the 
rest of the other list to the output. While the CPU is process­ ing the rest of this list, we can be 
filling up our buffers for the next pair of lists to be merged. The number of wasted CPU cycles is only 
O(log N) = o(N log N), so the schedule is parsimonious. Theorem 8 Distribution sort algorithms can be 
sched­uled on P-UMH with the following running time8. The algorithm8 for noncon8tant P for the fir8t 
two band­width cases are randomized. $log N ifb(l) = 1; ) log N O ;log Nlog if b(l) = l/(.t! + 1); 
e 1log P ( )) N l+c/2 @ + #log N if b(~) = p-et, c> O. () F ) The algorithms are optimal, except for 
the mid­dle b(t) = 1/(1 + 1) case, which is off from the best known lower bound of @((N/P) log N) by 
a log((log N)/ log P) factor. The proofs are suppressed for brevity. The algorithm that achieves the 
upper bound for the first case b(l) = 1 is based on a sim­ulation of the P-BT algorithm for f(z) = W 
given in [ViS]. The lower bound follows from the conventional N log N serial bound for sorting. For the 
second case b(t) = l/(1+1), the upper bound and lower bounds are related to the P-HMM approach for f(z) 
= log z [ViS]. The third case b(l) = p-et makes use of the P-HMM lower bound for f(z) = Xciz and an algorithm 
based on deterministic merge sort. We have also scheduled an efficient oblivious sorting algorithm for 
UMH1itf+l] corresponding to a nonoptimal RAM sorting algorithm based on column­sort. By oblivious, we 
mean that that data movement between levels can be prespecified, and thus no ex­tra capabilities of the 
UMH model are required be­yond those of [ACF]. The resulting running time is O(N log N), where c x 3.4. 
 3.2 SUMH and RUMH and their Par­allelizations The UMH model can be difficult to program because many 
buses can be active simultaneously. An earlier version of [ACF] introduced a sequential UMH model, appropriately 
called SUMH, that allowed at most one bus to be active at a time. However, the SUMH re­striction can 
be regarded as too severe. We introduce the following more natural and less severe restriction that corresponds 
more closely to fea­ sible and easy-to-use programming languages: We re­quire that the UMH program correspond 
exactly to a RAM program in which the RAM instruction set is augmented with a block move command that 
can move t contiguous memory elements in time t, for arbi­trary t.Each such block transfer can be implemented 
in UMH by a coordinated series of transfers in which several buses are simultaneously active but cooperat­ing 
on that single transfer. We call this natural vari­ant of UMH the random-acces8 UMH model, or simply 
RUMH. For example, a block of fi elements can be moved from the top memory level all the way down to 
the CPU (or anywhere in between) in -m time in UMH1 and RUMH1, but it requires @(@log N) time in SUMH1. 
The parallel versions of RUMH and SUMH are called P-RUMH and P-SUMH, respectively. Theorems 9 and 10 
give matching upper and lower bounds for sorting in the RUMH and SUMH models and their par­allelizat 
ions. Theorem 9 The running time8 mentioned in Theo­rem 8 are matching upper and lower bounds for sorting 
in P-R UMH. The upper bounds are achieved by the al­gorithms mentioned in Theorem 8. The algorithms for 
nonconstant P for the jir8t two bandwidth cases are randomized. Theorem 10 The following bounds are matching 
up­per and lower bounds for sorting in P-SUMH. The al­gorithms for nonconstant P fop the jirst two bandwidth 
case8 are randomized. log N @ $log Nlog if b(.t) = 1; log P  ( )) @ :log Nlog: if b(l) = 1/(1+ 1); 
[) N l+c/2 e +#log N if b(t?) = p cl, C> 0, F (() ) The structures of the formulas in Theorems 9 and 
10 suggest several different relationships between the RUMH and SUMH models on the one hand and the HMM, 
BT, and two-level models on the other hand (cf. Theorems 5 and 6 in [ViS]); accordingly the upper and 
lower bounds combine in an interesting way several techniques from [AAC, ACSa, AgV, ViS]. The proofs 
are suppressed for brevity. The distri­bution sort algorithms of Theorem 8 are RUMH algo­rithms. Both 
sets of algorithms use a distribution sort framework for the higher bandwidths and merge sort for the 
lower bandwidths. The upper bound for the first two cases b(l) = 1 and b(t) = 1/(4 + 1) are achieved 
by an algorithm based on the optimal P-HMM algo­rithm of [ViS], for access functions f(z) = log z and 
j(z) = logz z, respectively. The upper bound for the b(l) = p- 4 case is achieved by deterministic merge 
sort. The lower bounds are proved using an approach similar to that of [AAC]. We superimpose on the P-SUMH 
model a sequence of two-level memories. For i=l,2,3 ,. ..> the ith two-level memory has P disks, internal 
memory size 34 = PCY(p2(i+1J 1)/(p2 1), and block size B = pi. The cost of an 1/0 in each superpo­sition 
is B (for b(l) = 1), Blog B (for b(f!) = 1/(.2+ l)), and B1+C (for b(t) = p- 4 ). The lower bounds follow 
by applying the lower bounds from [AgV] for the two­level case. The third case also requires the use 
of the the conventional N log N serial bound for sorting. Conclusions We have presented the first optimal, 
deterministic ex­ternal sorting algorithm for multiple disks, improving significantly the randomized 
algorithm of [ViS]. The greed sort algorithm is easy to implement and is effi­cient in terms of internal 
computations. We have also given optimal or near-optimal sort­ing algorithms for UMH and its parallelization 
P-UMH, and we have derived tight matching bounds for sorting in the restricted models RUMH and SUMH and 
their parallelizations. Some parallel versions are random­ized. The RUMH model is particularly useful 
because it is easy to visualize and to extend current program­ming languages and compilers. An interesting 
open problem is whether a distribution-type sort can be implemented determinis­tically and optimally 
in terms of the number of parallel 1/0s in the two-level model. This could have applica­tions to optimal 
deterministic sorting in P-HMM, P-BT, P-UMH, P-RUMH, and P-SUMH, for which the current optimal algorithms 
in several cases are random­ized. Another interesting open question is whether it is possible to sort 
in O(N log N) time UMH1iif+l). References [AAC] A. Aggarwal, B. Alpern, A. K. Chandra and M. Snir, A 
Model for Hierarchical Memory, Pro­ceedings of 19th Annual ACM Symposium on The­ory of Computing (May 
1987), 305 314. [ACSa] A. Aggarwal, A. Chandra and M. Snir, Hierarchi­cal Memory with Block Transfer, 
Proceedings of 28th Annual IEEE Symposium on Foundations of Computer Science (October 1987), 204-216. 
[AgV] A. Aggarwal and J. S. Vitter, The Input/Output Complexity of Sorting and Related Problems, Communications 
of the ACM (September 1988), 1116 1 127, also appears in Proceedings of 14th An­nual International Colloquium 
on Automata, Lan­guages, and Programming (lCALP), LNCS 267, Springer-Verlag, Berlin, 1987. [ACF] B. Alpern, 
L. Carter and E. Feig, Uniform Mem­ory Hierarchies, Proceedings of the 3Lst Annual IEEE Symposium on 
Foundations of Computer Science (October 1990), 600-608. [ACSb] B. Alpern, L. Carter and T. Selker, Visualizing 
Computer Memory Architectures, Proceedings of the 1990 IEEE Visuahation Conference Founda­tions of Computer 
Science (October 1990). [Flo] R. W. Floyd, Permuting Information in Ideal­ized Two-Level Storage, in 
Complexity of Com­puter Calculations, R. Miller and J. Thatcher, cd., Plenum, 1972, 105-109. [GHK] G. 
Gibson, L. Hellerstein, R. M. Karp, R. H. Katz and D. A. Patterson, Coding Techniques for Han­dling Failures 
in Large Disk Arrays, U. C. Berke­ley, UCB/CSD 88/477, December 1988. [GiS] D. Gifford and A. Spector, 
The TWA Reservation System, Communications of the ACM 27 (July 1984), 650 665. [HoK] J. W. Hong and H. 
T. Kung, 1/0 Complexity: The Red-Blue Pebble Game, Proc. of the 13th Annual ACM Symposium on the Theory 
of Com­puting (May 1981), 326 333. [Jil] W. Jilke, Disk Array Mass Storage Systems: The New Opportunity, 
Amperif Corporation, Septem­ber 1986. [Knu] D. Knuth, in The Art of Computer Programming, Volume 3: Sorting 
and Searching, Addison-Wesley, Reading, MA, 1973. [Lei] T. Leighton, Tight Bounds on the Complexity 
of Parallel Sorting, IEEE Transactions on Com­puters C-34 (April 1985), 344 354, also appears in Proceedings 
of the 16th Annual ACM Symposium on Theory of Computing, (April 1983), 71-80. [LiV] E. E. Lindstrom and 
J. S. Vitter, The Design and Analysis of BucketSort for Bubble Memory Sec­ondary Storage, IEEE Transactions 
on Comput­ers C-34 (March 1985), 218-233. [LuP] F. Luccio and L. Pagli, A Model of Sequen­tial Computation 
Based on a Pipelined Access to Memory, Proceedings of the 27th AnnuaJ AJler­ton Conference on Communication, 
Controlj and Computing (September 1989). [Mag] N. B. Maginnis, Store More, Spend Less: Mid-Range Options 
Around, Compu terworld (Novem­ber 16, 1986), 71. [PGK] D. A. Patterson, G. Gibson and R. H. Katz, A Case 
for Redundant Arrays of Inexpensive Disks (RAID), Proceedings ACM SIGMOD Conference (June 1988), 109-116. 
[ReV] J. H. Reif and L. G. Valiant, A Logarithmic Time Sort on Linear Size Networks, .lournsl of the 
ACM 34 (January 1987), 60 76, also appears in Proceedings of the 15th Annual ACM Symposium on Theory 
of Computing (April 1983), 10 16. [Uni] University of California at Berkeley, Massive In­formation Storage, 
Management, and Use (NSF Institutional Infrastructure Proposal), Technical Report No. UCB/CSD 89/493, 
January 1989. [ViS] J. S. Vitter and E. A. M. Shriver, Optimal Disk 1/0 with Parallel Block Transfer, 
Proceedings of the 22nd Annual ACM Symposium on Theory of Computing (May 1990), 159 169. External memory 
(disks) t tII Figure 1: Two-1evel memory model. Figure 2: Disk 1 Disk 2 Disk 3 Disk 4 EIla1 26 42 71 
83 94 4 29 47 77 85 S)6 7 31 60 79 89 98 14 32 68 80 91 !29 An example of a sorted list with F = 4, 13 
= 1. Disk j n &#38; 3 5 8 Run 28 1 ... 2 4 6 ~ Run >7 2 ... . . . 21 . >7 Run i ... . . . . Disk j Run 
1 I Run 2 Run i . . . . k1 2 3 4 guaranteed <7 Figure 3: Assume that Run 1 contains the block with 
the smallest min~mum and Run 2 that with the smallest maximum on some disk ~ . Then this figure shows 
what the situation will be after the blocks have been processed. algorithm Greed Sort { Create the initial 
runs} repeat N/hf times read the next ill records into primary memory, PB at a time sort the it4 records 
internally write the M records back onto disk, PB at a time end repeat repeat until only 1 run is left 
{ Merge together R = W/2 input runs at a time } R:= -/2 { The output runs of the previous stage become 
the input runs for this stage } repeat until all input runs have been processed pick the next R runs 
to process for i:= 1 to R do { Initialize }  read in parallel the first P blocks of run i into buffer 
bl forj:=l toPdo nezt[i, j] := 1 biggest[i, j] := max( bl [j]) .wnallesi[i, j] := min(bl [j]) end for 
end for  { Do an approximate merge of the R runs } repeat until all records of the R runs have been 
processed for j := 1 to P do in parallel { Find the best one or two blocks to read from each disk } 
bestrunl [j] := i such that biggest[i, j] is a minimum bestrun2 [j] := i such that smallest[i, j] is 
a minimum read block nezt[bestrunl [j], j] of run bestrunl [j] from disk j into buffer bl [j] if bestrunl 
[j] # bestrun2 [j] then read block nezt [bestrun2[j], j] of run bestrun2 [j] from disk j into buffer 
b2 [j] merge bl [j] and b2 [j] in place internally write b2 [j] to block nezt [bestrun2 [j], j] of run 
bes-trun2 [j] on disk j endif write bl [j] to disk j in the output list read block nezt [bestrunl [j], 
j] + 1 of run bestrunl [j] from disk j into buffer bl [j] { Update data structures } nezi[bestrunl ~], 
j] := nezt [bestrunl [j], j] + 1 biggest[bestrunl ~], j] := max(bl [j]) { May be +co } sndlesi[bestrunl 
[j], j] := min(bl [j]) { May be +co } if bestrunl [j] # bestrun2 [j] then smallest [bestrun2 [j], j] 
:= min( b2 [j] ) { The biggest hasn t changed } end if end for end repeat { Do a restorative pass to 
turn the approximate merge into a complete merge} L := RPB T := total # of records in the R runs forn:=Oto 
T/L 2do use columnsort to sort records nL + 1,nL + 2, ....nL + 2L of the output list end for end repeat 
end repeat Figure 4: The Greed Sort algorithm. >8I... ... Runl I Run2 Run I m . >7 >7 ... ... ... Disk 
1 I Diskj ... nI  >7 ... Disk P Figure 5: All the records output on disk j at this step are guaranteed 
to have keys ~ 7 (in fact the largest key will be 4). Any disk prior to disk j has at most one block 
per run containing elements less than any key output on disk j, all of which will be written no more 
than R 1 blocks after this one. Any disk after disk j has already output all keys less than those written 
on disk j. Thus, a smaller key can follow a larger one by at most RPB locations in the approximately 
merged output list. I : Wmsoriijd.. ,,,, , ,...;.:: { 1 tL tL+L tL+2L Figure 6: A series of sorts of 
size 2L suffices to fix up an L-regressive list. *  CPU CPU CPU ii 44 / / i I Figure 7: A parallel 
hierarchical memory. The P individual memory hierarchies are all of the same type, such as HMM, BT, or 
UMH. The P CPUS can communicate among one another via the interconnection network (which can be a hypercube 
or cube-connected cycles, for example). Level 1 Figure 8: The uniform memory hierarchy (UMH), pictured 
here for the case a = 3, p = 2. The Ah memory level contains apt blocks, each of size p z. It is connected 
via buses to levels t 1 and t + 1. Each level I block can be randomly accessed and transferred to level 
1 + 1 at a bandwidth of b(l) (that is, in p~ /b(.?) time).  
			