
 On the Complexity of Bilinear Forms with Commutativity Joseph Ja'Ja w Department of Computer Science 
The Pennsylvania State University University Park, Pennsylvania 16802 We consider the problem of computing 
a set of bilinear forms in the case when the indeterminates commute. We develop lower bound techniques 
which seem to be more powerful than those already known in the literature for the commutative case. An 
un- expected result is the fact that Duality theory does not hold in the commutative case; we prove that 
the multiplication of 2 x n by n x 2 27n- matrices requires at least [-~--] multiplications while it 
is possible to multiply 2 x 2 by 2 x n matrices using only 3n + 2 multiplications. We also settle the 
question of whether commutativity can reduce the number of multiplications by 1/2 by showing that this 
can never happen. On the other hand, we show that, over alge- braically closed fields, the complexity 
of computing a pair of bilinear forms is the same whether or not commutativity is allowed. We feel that, 
in general, commutativity will have little effect whenever the constant set is an algebrai- cally closed 
field. i. Introduction The problem of computing a set of bilinear forms has recently received considerable 
atten- tion in the literature (e.g., [BDi], [F], [HM], [H], [Jl], [J2], [L], [LB]). This class includes 
many important problems such as the multiplication problems of matrices and polynomials. In this paper, 
we are interested in investigating the multiplicative complexity of a set of bilinear forms with conmnuting 
indeterminates and its rela- tionship to that of the case with noncommuting indeterminates. We will 
establish the fact that Duality theory does not hold in the colmnutative case by showing that the multiplication 
of 2 x n 27n by n x 2 matrices requires at least [--~--] multiplications over the integers, while it 
is possible to multiply 2 x 2 by 2 x n matrices (or n x 2 by 2 x 2) using only 3n + 2 multi- plications 
with integer coefficients ([Will, [Wi2]). On the other hand, we show that, over algebraically closed 
fields, the complexity of computing a pair of bilinear forms is the same whether or not com~utativity 
is allowed. We feel that, in general, commutativity will have little effects whenever the constants 
are drawn from an algebraically closed field. We also develop lower bound techniques which seem to be 
more powerful than those already known for the commutative case ([LB]). Moreover, we settle the question 
of whether eommutativity can reduce the number of multiplications required to compute a set of bi- linear 
forms by 1/2 by showing that this can never happen. We will review quickly some of the problem formulations 
known in the literature and which will be used freely in the following sections. The general problem 
of computing a set of bilinear forms can be defined as follows. Let K he a com- mutative ring and let 
x = (Xl, x2, ..., Xp) T and Y = (Yi' Y2' "''' yq)T he two column vectors of indeterminates, we have 
to compute m > 1 bilinear fo rT~S : P q B i = Z Z rijkXjy k= xTGi y , i=l, 2 m, j=ik=l rijk~K where 
G i is p x q matrix with elements in K. Our model consists of the class of bilinear pro- grams, where 
a bilinear program consists of a sequence of instructions of the form fj ÷ a=objj , 197 &#38;#169; 1979 
ACM 0-89791-003-6/79/0400-197 $00.75 See page ii i < j < i, oe{+, -, x} and each f. is a new 3 variable, 
each aj or b.3 is either (i) an ft' t < i, or (2) an indeterminate or (3) a constant from K. All the 
active multiplications are be- tween linear forms of x and y with coefficients from K. It is well known 
that it is no loss of generality to consider this class of algorithms rather than the more general class 
of straight-line programs. It is clear from above that the minimal number of active multiplications required 
to m compute a set of bilinear forms {Bi}i= 1 is the smallest integer D such that Bi= E= c~iZ fz(x, 
y) f[(x, y) , i < i < m, hi where f£(x, y) and f[(x, y) are linear forms in x and y over K. If we assume 
that indeterminates do not commute, then we can restrict the active multi- plications to be of the type 
f(x) f' (y) , where f(x) and f'(y) are linear forms in x and y respectively. In this case, the noncommutative 
complexity (or simply, complexity) is the smallest integer ~ such that 6 (*) Bi= j iE= Bij fj(x) f'.(y)3 
' 1 < i < m. From now on, we will use the notation 6{B i}  or 6{G i} to mean the noncommutative complexity 
m of {Bi}i= 1 and ~/{B i} or ~/{G i} to mean the complexity of computing {Bi}~= in the case of I_ 
commuting in~eterminates. Since the case of non- commuting indeterminates has been studied considerably 
more than the commutative case, we will try to exploit the results already known in the noncommutative 
case to get more insight on the commutative case. We now recall these re- sults which will be needed 
later. Let fj(x) = <bj, x) and f~(y) = <cj, y) in (*); we have 6 Bi=xTGiY=j~=i ~ij <bj, x) <cj, y)=xT( 
j=l ~ij T bj cj)y, i = i, 2, ..., m.  Since the above equality must hold for all values of the indeterminates 
x and y over K, we conclude that G i = ~ Bij bj c~, i = I, 2 ..... m. jl Therefore, ~ is equal to the 
smallest num- ber of rank one matrices necessary to include the Gi's in their span ([BDI], IF], [L]). 
 Another interesting formulation given in ([BDi], [L]) is to introduce a set of indetermin- m ates {si}i= 
1 and to consider the trilinear form m m p q h(s,x, y)= ~ siBi= ~ ~ ~ rijkSiXjy k i=l i=l j=l k=l It 
is easy to see that ~ is the smallest number such that 6 h(s, x, y)= E <Bi, s)~i, x)<c i, y), £=i 
and we now have a completely symmetric problem with respect to s , x and y ; for example, the above 
problem is equivalent to that of computing p bilinear forms associated with the m × q matrices Gj = 
(rijk)ik , j = i, 2, ..., p. This property, referred to as Duality in the literature, was also discovered 
independently by Hopcroft and Musinski [HM]. As an immediate cor- ollary, the complexity of multiplying 
an m × n matrix by an n x p matrix is the same as the of multiplying an m × p matrix by a p x n matrix 
for example, and we talk about the (m, n, p) matrix multiplication problem. As we will see later, this 
property does not hold in the commu- tative case. Another definition, which we will find quite useful, 
is that of characteristic matrix m G(s) = ~ siG i associated with the set i=l xTGi y , .-., m B i 
= i = i, 2, m, where the {si}i=l, as before, are a set of indeterminates. The cor- responding noncommutative 
complexity will be denoted by 6{G(s)} and that of the commutative case by ~{G(s)}. Note that ~ is the 
smallest number such that G(s) = Z <~j, s>bj c~ 8jEKTM b.aK b and c.EK q. j=l ' ' 3 J An interesting 
observation made in ([BDi], [DG]) is that 6 is invariant under the action of the group $= G£(K, m) 
x G%(K, p) x G£(K, q) in the following sense: for any (P, Q, R)e~, the trilinear form h(Ps, Qx, Ry) 
has the same length as that of h(s, x, y). Of particular interest is a subgroup I of $, called the 
 isotropy group, and defined as those elements of which satisfy h(Ps, Qx, Ry) =h(s, x, y) ,  could 
be used to generate many optimal algorithms out of a single optimal algorithm. We will see how to use 
this fact to establish good lower bounds (see also [HE], [HL]). 2. Lower Bound Techniques Howell and 
Lafon [HL] have shown how linear independence arguments can be used to obtain a lower bound for the 
quaternion product in the corm~utative case. Van Leeuwen and van Emde Boas [LB] have generalized this 
argument and stated it in a general form relative to any set of bilinear forms; they applied this argument 
to several specific problems and obtained (relatively) good lower bounds. In this section, we combine 
the linear independence argument with other tech- niques, used in determining lower bounds for the rank 
of a tensor [BDi], to obtain more powerful lower bound techniques. Let B i = xTGi y, 1 < i < m, be a 
set of bilinear forms over a commutative ring K. The m commutative complexity of computing {Bi}i= 1 
is the smallest integer ~ such that Bi= ~ f£(x, y) fi(x, y) i= i, 2,  % i ~i£ , ..., m, where aiEeK 
, f£(x, y) and f~(x, y) are two linear forms in x and y , say f%(x, y) = <a£, x)+ ~£, y) and f[(x, 
y)= <aA' x)+ <~i, y}. Hence, Bi= %~i eiz{<a£' x)+ <b%, y)}{<~£, x# + <b£, y~}, i =I, 2 ..... m. Expanding 
and substituting xTGi y for B i , we get aT ~)y + xTGiY = %=E 1 eiA{x T a A a£x + xT(a£ ..  bz)y 
+ yT(b£ T i°l, 2, . , m , i.e., I£ ~T ei £ az az = ~ T =i Z~l ei£ b£ bz=0 (+) ~Gi= Z~ I~ ~ii{ai~ + 
~a£bz  }T , i= i, 2, ..., m. We will use the equations (+) to establish several of the results in this 
section. We can also formulate the above problem in a matrix times a vector form [Wi2] as follows: BIll] 
xTGi xTG 2 B 2 = : y = ~(x)y.  xT~n Since the indetermlnates commute, we also have P q P q B = Z 
Z -=E ~ rlj k Yk)Xj j=l k=l rijk Yk xj-j l(k 1  and the above multiplication problem can be also be 
viewed as a product of the form ~(y)x. We now recall an important notion to get lower bounds [Wi2]. 
Let H(x) be a p x q matrix, whose entries are functions of the vector x over a field ~. The rows rl(x 
) ..... rp (x) are said P  to be linearly independent if whenever Z %. i  i=l ri(x)e~q, lieS, then 
%i = 0 Vi. Otherwise, the rows are called linearly dependent. Define the row rank of H(x) to be the maximum 
number of linearly independent rows of H(x). We can similarly define the column rank of H(x). The following 
result is well known ( [Wi2], [Fill). Lemma 2.1: Let m {Bi}i= 1 be a set of billnear forms over a field 
~, and let ~(x) be as de- fined above. Then the commutative complexity of computing {Bi}~.=l satisfies 
 v{B i} > max(column rank (~(x)), row rank (~(x)).  The same is true for ~(y). Before establishing 
the next lower bound result, we give the following characterization of ~{G(s)} which has been shown in 
([HI, [LH]). Theorem 2.2 ([HI, [LH]): Let G(s) be the characteristic matrix associated with a given 
set of p × q billnear forms {B i T m = x GiY}i= 1 over  a ring K (of characteristic # 2). Let N(s) 
be a (p + q) x (p + q) characteristic matrix such that N(s) + N(s) T =  T(s)  where 6{N(s)} is minimal 
among all (p + q) x (p + q) characteristic matrices N(s) which satisfy the above equation. Then @{N(s)} 
= p{G(s) }. Proof: Consider equations (+) again, ~T T Gi= Z c~i~{a ~ b~ + ~ b~}, i=l, 2, ..., m. Note 
that a£ bi+a% b = ~i% ~T ~Z=l [b i a~ +b Z a~ 0  a~ JaR Z (~i£ + Z aii £=i £=i i = i, 2, ..., 
m, m Now, if we take H(s) = Z siH i , where i=l [a£ ~T gel (*) H i = %E=l ~i£ , i=l, 2, ..., m, The~ 
 H(s) + H(s) T = s) (*) implies that 6{H(s)} < ~{G(s)} and  ~{N(s)} < ~{S(s)}. We now prove the 
reverse inequality. Let N(s) be any (p + q) x (p + q) characteristic matrix of minimal degree and which 
satisfies N(s) + N(s) T = (s)T It follows that [X T yT]{N(s) + N(s)T}II = [X T [x T yT] N(s) rx] 
 i.e. lJ = xTG(s)Y ' if characteristic (R) # 2. Consider any minimal algorithm for N(x) , say" II T 
6 i [bZl bi2]  N(x) = %=I La 2j where dim aZl = dim bzl = p and dim a%2 = dim b%2 = q. Thus, 6 T 
T xTG(s)y = Z <~%, s) (xTa%l+yTa£2)(b%ix+b%2 y) %=1 and hence p{G(s)} < 6{N(s)}. B We use the above 
characterization to estab- lish the second lower bound result. The main idea is the same as that of substitution 
argu- ments (see e.g. IBM]) or partitioning techniques [BDi]. Suppose a set of bilinear forms is par- 
titioned into two subsets, the first indexed by m r {si}i= 1 , the second by {ti}i= I. Let the cor- 
 responding characteristic matrix be given by G(s) + H(t). Without loss of generality, we assume the 
Hi's to be linearly independent. We are ready for the next theorem. Theorem 2.3: Let G(s) + H(t) be 
as given above. Then, over any ring K , we have ~{G(s) +H(t)}~dim t+min ~{S(s)+H(Rs)} , R where R 
is a (dim t) x (dim s) matrix over K. Proof: From Theorem 2.2, we have u[G(s)+H(t)} = ~{Nl(S)+N2(t )} 
, where  {Ni(s) +N2(t)}+ {Ni(s) +N2(t)} T = (G G(s) +H(t)- (s)T+ H(t) T 0  and 6{Ni(s) + N2(t)} is 
minimal. Using the Par- titioning technique (Theorem i0 of [BDi]), we ob t ain 6{Nl(S) +N2(t)}~dim t+min 
~{Nl(S) +N2(Rs)} , R  and thus p{G(s)+H(t)}>dim t+min p{G(s)+H(Rs)}. R  We can obtain the main lower 
bound result stated in [LB] as an immediate corollary of Theorem 2.3 and lemma 2.1. We now establish 
a lower bound theorem which, for a small number of hilinear forms, gives stronger lower bounds than those 
of the previous techniques. Theorem 2.4: Let G(s) be the characteristic matrix of a set of bilinear 
forms {Bi}~= 1 over a ring K. Then ~{G(s)} > ~ 8 G(s)  Proof: We again consider equations (+): '/ 
~T ~ b~ = 0 [ ~ C~i% a£ a%= b% £=1 £__E1 ~i% II b%+~% , i=l, m. Gi= i~l ~i%{a% ~T bE } 2 .....  
Notice that i 0 ~ ~ b~+ ~% b% 0 G = %~i ~i% a% + b% a P a% [~ a~] a% [b E a%] P ai% + E ai% %=1 
   IJ II £=1 i= i, 2, ..., m It follows that 2p rank one matrices include the in their span. Therefore, 
 61o G(s) 22v. I] Corollary: Suppose we partition a set of billnear forms into two subsets indexed 
respectively by s and t. Then 1 p{G(s) +H(t)}~dim t+~ rain R 6FG(s) + H(Rs) 0 L0 G(s) r + H(Rs) I 
 It is well kno~ that, for any characteristic ~trix G(s) , we have ~{G(s)} ! 2p{G(s)}. open question 
is whether there exist bilinear fo~s for which co~utativlty will reduce the i  number of multiplications 
by ~. Using the above theorem, we will prove that this can never happen. Theorem 2.5: Let G(s) be any 
characteristic ~trix whose row and column ranks are r and e respectively. Then, we have 1 ~{G(s)}~½{~{G(s)}+max(r, 
c)} >~ ~{G(s)}.  Proof: We know that ~6{G(s)}+column rank {GT(s)}= GT(s) ~{G(s)}+r.  In [Jl], we 
proved that  cT(s) and hence 6{GT(s)}+column rank {G(s)}> GT~(s)  ~{G(s)} + c.  Using now Theorem 
2.4, we get i ~{G(s)}Z½ {6{G(s)}+max(r, c)} >y 6{G(s)}. D Corollarx: There exist no sets of bilinear 
forms for which commutativity reduces the optimal number 1 of multiplications by ~. Note that Theorem 
2.5 gives -(s) l 1 {~{M(S)}+n 2} ~IMnnnJ~2 nnn ' where M (s) is the characteristic matrix of the nnn 
(n, n, n) matrix multiplication problem. Using Theorem 2.4 directly, it is easy to get ~rM(S) } > 2n 
2 I nnn ---n , which is comparable to the best known lower bound of 2n 2 - i in the noncommutative 
case ([BDi]). We will now give an example to show that our lower bound techniques will produce stronger 
lower bounds than any of the previously known techniques. Let n = 2k , keZ +. Consider the computation 
of the following pair of bilinear forms, B 1 =xlY 1 + x2Y 2 + ... +XnY n , B 2 = XlY 2 + x3Y 4 + ... 
+Xn_lY n. This can be viewed as the following multi- plication problem I l x2 x3 "'" Xn-i x I 0 x 3 
0 ... 0  p'nJ Applying the techniques of [LB], it is not possible to obtain anything better than n 
+ 1 as a lower bound. However, using our techniques, we will see how to obtain ~ as a lower bound. We 
will illustrate it for n = 4 and the same proof carries for any n. Note that the characteristic matrix 
is given by Sl G(s) : li s 2 s I 0 0 0 s I ii s 2 0 0 s From Theorem 2.4, we have 0 1 ~{G(s)} Z y 
l [o (s) = ~ H(S). cT(s) Rearranging columns, we get %2 0 0 0 s I 0 0 0 s I 0 0 0 0 0 0 0 0 s 2 0 0 
0 s I 0 0 0 s I 0 0 0 0 0 0 H(s) 0 0 s I 0 0 0 0 0 0 0 s 2 0 0 0 s I 0 0 0 0 s I 0 0 0 0 0 0 0 s 2 0 
0 0 s] Applying Theorem i0 of [BDi] to the last four columns, we get s2 + ~iSl e2Sl e3Sl e4Sl s 1 
0 0 0 BlS 1 s 2+82s I 83s I 84s I ~{H(s)}~ 4 + ~ 0 s I 0 0 0 0 s I 0 rls I r2s I s 2+r3s I r4s I 0 
0 0 s 1 TlS 1 T2s I T3s I s 2 +T4s [ where ~i' 8i' r i , TicK. Using proper row operations, we obtain 
 0 0 %2 0 s I 0 0 0 0 s 2 0 0 0 s I 0 0 ~{H(s)} > 4 + 6 0 0 s I 0 = 12. 0 0 s 2 0 0 0 0 s. 0 0 
 0 s 2 Therefore, ~{G(s)} ~ ~i ~{H(s)} > 6. It follows that B{G(s)} = 6.  Theorem 2.6: The complexity 
of computing the two Let X and Y be the following two matrices. bilinear forms B 1 = xlY 1 + x2Y 2 + 
... +xnY n B 2 = xlY 2 + x3Y 4 + ... +Xn_lY n is precisely [~J , even if we use commutativity. 3. 
Complexity ' of the n x 2 x 2 Matrix Multipli- cation Problems  As we have mentioned in the introduction, 
each set of bilinear forms is equivalent to five other sets of bilinear forms obtained by permuting the 
indices of the corresponding trilinear form. This property, which has been referred to as Duality in 
the literature, has been used to obtain general algorithms ([HM] e.g.) and establish lower bounds. Brockett 
and Dobkin [BDi] have exploited this property to get relatively good lower bounds for matrix multiplication 
in spite of the fact that their techniques are essentially based on linear independence and substitution 
arguments. In this section, we will establish the fact that Duality theory does not hold if the indeterminates 
commute. We consider, for this purpose, the n x 2 x 2 matrix multiplication problems, i.e., the problem8 
of multiplying n × 2 by 2 x 2 , 2 x 2 by 2 x n and 2 x n by n × 2 matrices. We know that, in the noncommutative 
case, all of these problems are equivalent and each of the optimal 7n algorithms requires precisely 
[~-] multipli- cations [HK]. It is also well known [Wi] that  it is possible to multiply n x 2 by 2 
x 2 matrices (or 2 × 2 by 2 × n) with 3n + 2 multiplications in the commutative case with only integer 
constants. Waksman [Wa] reduced the number of multiplications by i, but his algorithm 1 uses the constant 
5" Surprising enough, none of these algorithms generates a fast commutative algorithm for the multiplication 
of n × 2 by 2 x n matrices. We will prove that this problem 27n  requires at least [-~--] multiplications 
whenever we use integer constants; this shows that, this problem is harder than any of its duals. In 
the rest of this section, we describe Winograd's al-  gorithm and the modification introduced by Waksman, 
and we later establish the lower bound for the n × 2 by 2 x n matrix multiplication problem.  x F x 
1 f I.  L~2i x22] Ly2i Y22 Y2nJ Winograd's algorithm to compute XY goes as fol- lows. Compute ~l=XllXl2' 
~2 =x21x22 n i = YliY2i , (Xll + Y2i)(Xl2 + Yli ) (x21+Y2i)(x22+yli ) i = I, 2, .... n. Then we have 
 XllYli + xl2Y2i = (Xll + Y2i ) (x12 + Yli ) -~i -Hi (++)  x21Yli + x22Y2i = (X2l + Y2i ) (x22 + Yli 
) -~2 -Hi' l<i<n. It is obvious that the above algorithm requires 3n + 2 multiplications. We can reduce 
the number of multiplications by one as follows. Compute (Xll + Y2i)(Xl2 + Yli ) (x21 + Y2i)(x22 + 
Yli ) (Xll-Y2i)(Xl2-Yli) 1 < i < n (x21- Y21)(x22 -Yll ) Note the following i E 1 + D i = ~ { (Xll 
+Y2i ) (x12 + Yli ) (x12 -Y2i ) (x12 - Yli ) } i < i < n 1 ~2 + nl = 2 { (x21 + Y21 ) (x22 + Yll ) 
+ (x21 -Y21 ) (x22 -  Yli)} Now we can compute the remaining ~2 + Hi by the equation ~2+ni = (~2+nl)+ 
(~l+ni)- (~l+nl) i = 2, ..., n. Using the same equations (++) as before, the above algorithm requires 
3n + i multiplications. We prove that the above algorithms are es- sentially optimal up to 2 or 1 multiplication. 
 Theorem 3.1: The problem of multiplying 2 x 2 by 2 x n matrices requires at least 3n  multiplications 
in the commutative use, even if the constants come from a field. Proof: It is easy to check that (one 
form of) the corresponding characteristic matrix is given by I ils2 "'" Sn i] G(s) = n+l "'" S2n SlS 
2 ... s n Sn+ I s 2  Applying Theorem 2.3 with t = (Sn+l, ..., S2n), we get ~{G(s)} > n + SlS 2 . 
   I[ nl}  where a ~ means a linear combination of s l, s 2, ..., s n. Since column rank SlS2 ... 
= 2n, we obtain l/{G(s)} > 3n. D Note that it is not possible to obtain this lower bound by using the 
techniques of [LB]. Note also that the same lower and upper bounds hold for the multiplication problem 
of n x 2 by 2 x 2 matrices. We now consider the problem of multiplying 2 x n by n x 2 matrices. Before 
establishing a lower bound we need a couple of results Let G(s) be the characteristic matrix of the cor- 
responding bilinear problem Then, it is straightforward to check that (one form of) G(s) is given by 
 ~i s2 s 3 s 4 s I s 2 G(s) -- s 3 s 4 , n blocks. s I s2 s 3 s 4  Recall that (A, B, C)eG~(2n) x G%(4) 
× G%(2n) is an element of the isotropy group I G of G(s) if AG(Bs)C = G(s). We need the following theorem 
from [BDi]. Theorem 3.2 [BDi]: (A, B, C)EI G if, and only if, there exist nonsingular matrices P , Q 
and R of dimensions n , 2 and 2 respectively, such that A = P ~ Q, C = P ~ R and B = (Q ~ RT~ Theorem 
3.3: Let (A, B, C)el G and let H(s) be the characteristic matrix Let A' , B' and C' be given by: A' 
= , B' = B "and C' = . 0 C T 0 A T Then (A', B', C')gl H. Proof: We have AG(Bs)C = G(s). Hence, cTGT(Bs)A 
T = GT(s). On the other hand, 0 cTGT(Bs)A T  Therefore, (A', B', C')CI H. 0 We need also to establish 
the following two facts whose proofs are immediate from [Jl]. Lemma 3.4: Over any field, we have the 
following: s 2 0 . 3k, where k is the s 1 number of blocks. s 2  %1 o s 2 s I 0 s 1 s 2 s I 
 6 = 3k, s 1 s 2 0 s I s I s~ 0 s I  where again k is the number of blocks. We are ready for the 
following theorem Theorem 3.5: Any commutative bilinear algorithm which computes the matrix product 
of 2 x n by 27n n x 2 requires at least [--~-] multiplications over Z 2 or Z. Proof: The techniques 
we will use are similar to those introduced in [J2]. We prove that FG(s) 0 1 27n {H(s)} = 6 z L0 > --~-, 
 6Z2 2 GT(s) and, using theorem 2.4, the result would follow. Since dim s = 4, the only possible linear 
 forms over Z 2 are s I, s 2 , s 3 , s 4 , s I + s 2 , s I + s 3, s I + s 4, s 2 + s 3, s 2 + s 4, s 
3 + s 4, s I + s 2 + s 3, s I + s 2 + s 4, s I + s 3 + s 4 , s 2 + s 3 + s 4 and s I + s 2 + s 3 + s 
4. Thus, any dyadic decomposition of H(s) , T H(s) = Z <~£, s) DA, D~ rank one matrices, A=i can be 
partitioned into different subsums, each subsum corresponds to one linear form of s over Z 2. Let the 
number of rank one matrices (between parentheses) associated with each linear form be given as follows. 
 Sl(C~ I) , s2(C~ 2) , s3(~ 3) , s4(~ 4) s I + s2(812) , s I + s3(813) , s I + s4(814) s 2 + s3(823) 
, s 2 + s4(824) , s 3 + s4(834) s I + s 2 + s3(¥123) , s I + s 2 + s4(Y124) , s I + s 3 + s4(Y134) , 
s 2 + s 3 + s4(Y234) , S 1 + s 2 + s 3 + s4(~)__. Note that T = ~I + ~2 + a3 + e4 + 812 + 813 + 814 
+ 823 + 824 + 834 + Y123 + Y124 + Y134 + Y234 + Set s 4 = 0 and s 2 = s 3. Then ~4 + 823 + Y234 terms 
vanish. Using lemma 3.4, the resulting problem requires 6n multiplications; thus, T ~ 6n + ~4 + 823 
+ Y234" (i) Similarly, setting s 2 = 0 and s I = s 4 , we get T ~ 6n + e2 + 814 + Y124" (2) On the 
other hand, if we set s I = 0 and then s 3 = 0, we get the following T~ 6n + ~i (3) T ~ 6n + ~3" (4) 
 From theoresm 3.2 and 3.3, it follows that there exists (P, B, Q)el H such that B = (i1 ~1~ 1 1 (0 
1 ) , i.e., 1 1 0 0 0 1 0 0  B = 1 1 1 1 0 1 0 1  Hence s I + s 2  s 2 Bs = s I + s 2 + s 3 
+ s 4 ~s~ s 2 + s 4 I This transformation generates a new algorithm given by T N(s) = PH(Bs)Q = 
Z <~, s') PDAQ. %=i Note that if r(s) is a linear form in the original algorithm with ~ rank one matrices, 
r(Bs) has also ~ (different) rank one matrices in the new algorithm. Using the above transfor- mation, 
the number of rank one matrices associated with each linear form in the transformed algorithm is given 
by s I + s2(~l) , s2(~ 2) , s I + s 2 + s 3 + s4(~3) , s 2 + s4(e 4) si(812) , s 3 + s4(B13) , s I 
+ s4(814) , s I + s 2 + s4(823), s4(824) , s I + s3(~34) s I + s 3 + s4(X123) , s I + s 2 + s4(Y124) 
, s 2 + s3(~134) , s I + s 2 + s3(Y234) , s3(9). in the new algorithm, we get Setting s 2 = s 3 and 
s 4 = 0 (5)  T > 6n + 824 + Y134 + Y123" Setting s 3 = 0 and then s 1 = 0 , we get T > 6n+~ (6) (7) 
 T_> 6n + 812" Similarly, we have (8) T > 6n + 813 (9) T > 6n + 834. Add up (I) -(9) to get 9T > 
54n + T, i.e., 8T > 54n. 27n Thus ~  Z F-V] 1 ~{H(s)} 27n  Therefore ~{G(s)} > ~ >--~-. 4. Complexity 
of Pairs of Bilinear Forms The complexity of computing a pair of bilinear forms has been determined 
in [Jl], in the case where the indeterminates don't commute. It is in- teresting to know whether commutativity 
can reduce the number of multiplications in this case. We prove that the complexity of computing a pair 
of bilinear forms is the same whether or not commuta- tivity is assumed. In order to establish this fact, 
we will recall some basic facts shown in [al]. Let G(s) be the characteristic matrix cor- responding 
to a pair of bilinear forms over a fieldS. Then G(s) is equivalent to a canonical characteristic matrix 
of the following form diag{Lgl(S) "'" Ler(S) L~l(S) ... L T~k Cl(S) "'" Ok(S) }  where 0 < E 1 < e 
2 < _ _ ... _< E r , 0 < nl _< N2 _< .. < nk are called the minimal indices (r, k > 0) , s 2 s 1  
 12 sl I}  LE(s ) = ".. ".. e s 2 s  and s I s 2 s I s 2  Ci(s) = s I s 2 ~i0s2"''~ini -2s2 Sl 
+ Sini-1 's"  1 < i < t, such that the polynomials ni-i ni-i n i Pi (x) = ~i0- ~il x+ "'" + (-i) ~ni_l 
x + x are the nontrivial invariant polynomials of the corresponding pairs of matrices [Jl]. Theorem 
4.1 [Jl]: Let {ei}i=l ' =i ' X }t Pq( ) q=l be as defined above and which corres- pond to a pair of bilinear 
forms {Bi, B 2} and let 5 be a field which contains the roots of Pl(X) and such that Card 5~ max {e 
i, Nj, deg pq(X) }. Then i,j,r  r k ~5{Bi, B2}= E ei+ Z ni+r+k+£, i=l j =i where £ is the number of 
Pr(X) 's which don't factor into distinct linear factors over 5. The following theorem is the main result 
of this section. Theorem 4.2: Let G(s) be the characteristic matrix corresponding to a pair of bilinear 
forms over a field 5 which contains the roots of a Pl(X) (as defined above) and such that Card is 
large enough. Then ~{G(s)} = ~{G(s)}. Proof: Let H(s) be defined by  If (s) 0 r] H(s) = G(S)  Note 
that H(s) corresponds to a pair of bilinear forms such that its minimal indices are the union of the 
minimal indices of G(s) and those of G(s) T. Note also that the invariant polynomials of G(s) are the 
same as those of GT(s). There- fore the invariant polynomials of H(s) are the same as those of G(s) 
repeated twice. Theorem E+l 4.1 implies r k ~{H(s)}=2 Z ei+2 Z nj+2r+2k+2£, i=l j=1 i.e. ~{a(s)}=2~{G(s)}. 
 Theorem 2.4 implies that p{G(s)} = 6{G(s)}. The lower bounds for the case when the field has a small 
cardlnality which have been stated in [Jl] and [J2] hold also for the commutative case. We consider 
one example Let ~ = Z 2. Consider the following charac- teristic matrix, which corresponds to the problem 
of multiplyinga linear polynomial by an (n - i)- degree polynomial, 1 s2 s I s 2  Ln(S ) = ".. ".. 
n E s I s 2 I n+l  In [J2], we have proved that -n +  6z2{Ln(S)}  We now prove a similar result 
for ]Jz2{Ln(S)}. From Theorem 2.4, we know that IIz2{Ln(S)} > g 6Z 2 LnT(S ) We use an argument similar 
to that used in the proof of theorem 3.5. Consider any algorithm for LnT(s)  Hn(S) in(S) 0 ] T = 
= E <el, s) D%.  L~(s) £=i  Since the only linear forms over Z 2 are s I , s 2 and s I + s 2 , the 
above decomposition can be partitioned as follows ~i e2 a3 Hn(S) = Z _(1) + Z S 2 D~ 2) + Z (Sl+S2> 
D~ 3) S ID% £=i %=1 %=1  Hence 9 T = ~I + ~2 + ~3" Set s I = 0 ; ~I terms disappear. However,  the 
resulting problem requires at least 2n multi- plications. Thus T _> 2n + el (i). Similarly, T ~ 2n 
+ ~2 (2) (s 2 = 0) i 2n + 8 (3) (s I = s2).  Summing up (i), (2) and (3), we get 3T > 6n + T , i.e., 
T > 3n. 1 3n It follows that ~{Ln(S)} --> 2 6z2{Hn(S)} ~T and  therefore ~z2{Ln(S)} = n + [~] = ~z2{Ln(S)}. 
5. References [BM] A. Borodin and I. Munro, "The Computational Complexity of Algebraic and Numeric Prob- 
lems," American Elsevier, New York, 1975. [BDi] R. W. Brockett and D. Dobkln, "On the Optimal Evaluation 
of a Set of Bilinear Forms," Linear Algebra and Its Applications 19, 207-235 (1978). [BD2] R. W. Brockett 
and D. Dobkln, "On the Number of Multiplications Required for Matrix Multiplication," SIAM J. COMPUT, 
Vol. 5, No. 4, December 1976. [DG] H.F. deGroote, "On Varieties of Optimal Algorithms for the Computation 
of Bilinear Mappings," Mathematisches Instltut der Universitat Tubingen technical report, 1978. Also 
to appear in Theoretical Computer Science. [Fi] C.M. Fiduccia, "Fast Matrix Multiplica- tion," Proceedings 
of the 3rd Annual Symposium on Theory of Computing, pp. 45-49. [F2] C.M. Fiducc&#38;a, "On Obtaining 
Upper Bounds on the Complexity of Matrix Multiplication," in Complexity of Computer Computations, (R. 
Miller and J. Thatcher, editors), Plenum Press, 1972.  [HK] J.E. Hopcroft and L. Kerr, "On Minimizing 
the Number of Multiplications Necessary for Matrix Multiplication," SIAM J. Appl. Math. 20 (1971), 30-36. 
 [HM] J.E. Hopcroft and J. Musinski, "Duality Applied to the Complexity of Matrix Multi- plication and 
Other Bilinear Forms," SIAM J. COMPUT., Vol. 2, No. 3, Sept. 1973.  [HI T.D. Howell, "Tensor Rank and 
the Complex- ity of ~ilinear Forms," Ph.D. Thesis, Cornell University, Sept. 1976. 207  [HL] T.D. Howell 
and J. C. Lafon, "The Complex- ity of the Quaternion Product," TR 75-245, Department of Computer Science, 
Cornell University, June 1975. [HI L. Hyafil, "The Power of Commutativity," Proceedings of the 18th 
Annual Symposium on Foundations of Computer Science, Providence, Rhode Island, 1977. [Jl] J. Ja'Ja', 
"Optimal Evaluation of Pairs of Bilinear Forms," Proceedings of the Tenth Annual Symposium on Theory 
of Com- puting, San Diego, California, 1978. [J2] J. Ja'Ja', "Computation of Bilinear Forms over Finite 
Fields," Technical Report CS-78-03, Department of Computer Science, The Pennsylvania State University, 
January 1978. [L] J. C. Lafon, "Optimum Computation of p Bilinear Forms," Linear Alsebra and Its Applications 
i0, 225-260, 1975. [LB] J. van Leeuwen and P. van Emde Boas, "Some Elementary Proofs of Lower Bounds 
in Com- plexity Theory," Linear Algebra and Its. Applications 19, 63-80 (1978). [S] V. Strassen, "Gaussian 
Elimination is not Optimal," Numerische Mathematik 13 (1969), pp. 354-356. [Wa] A. Waksman, "On Winograd's 
Algorithm for Inner Products," IEEE Trans. Compute!s, 19 (1970), 360-361. [Will S. Winograd, "A New 
Algorithm for Inner Product," IEEE Trans. Computers, 17 (1968), 693-694. [Wi2] S. Winograd, "On the 
Number of Multiplica- tions Necessary to Compute Certain Functions," ~ormnun. Pure App !. Math., Vol. 
XXIII, 165-179 (1970). [Wi3] S. Winograd, "On the Multiplication of 2 by 2 Matrices," Linear Algebra 
Appl. 4 (1971), 381-388.   
			