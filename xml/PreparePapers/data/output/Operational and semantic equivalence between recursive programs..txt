
 OPERATIONAL AND SE~ANTIC EQUIVALENCE BETWEEN RECURSIVE FROGRAMS. Jean-Claude RAOULT Jean VUILLEMIN 
 Universit4 de Paris-Sud Informatique - B~t. 490 91405 ORSAY (France) ABSTRACT : In this paper, we show 
that two widely different notions of program equivalence coincide for the language of recursive definitions 
with simplification rules. The first one is the now clas- sical equivalence for fixed-point semantics. 
The other one is purely operational in nature and is much closer to a programmer's intuition of program 
equivalence. RESUME : Nous montrons ici que deux notions en apparence tr&#38;s diff4rentes d'4quivalence 
entre programmes r4cursifs coYncident. La premi&#38;re est l'4quivalence induite par les s4mantiques 
de treillis et points fixes. L'autre est de nature purement op4rationnelle etest beaucoup plus proche 
de l'in- tuition qu'un programmeur peut avoir de l'4quivalence entre programmes. NB : This is an extended 
abstract : the last and most technical section has been bypassed, for lack of space. A complete version 
is being published in the preprints of ORSAY University, and one can get it by writing to either of the 
authors. -75 - I. INTRODUCTION 2. II~FORMAL PRESENTATION OF THE THEOREM The main object of this paper 
is to show that two apparently very different notions of program equivalence are in fact identical. The 
first one stems from the now classical fixed-point semantics originated by Scott. The other one is purely 
operational and cons- titutes what we think a programmer without any knowledge of lattices or fixed-point 
would use to formalize his intuitions about program equivalence. This result provides strong evidence 
as to what constitutes the right semantics for program- ming languages. It also establishes a nice duality 
between program manipulating systems : on one side, we have a wide class of (partial) interpretors, each 
of which being usable to show non-equivalence between some programs ; on the other side, we have just 
as wide a class of (partial) program proving systems, each of which being usable to show equi- valence 
between particular programs. We believe that this provides an attractive theoretical framework in which 
to consider modern systems for constructing programs : partial and symbolic interpretation, program optimization 
and debugging. For specificity, our results are stated and proved for the programming language of recursive 
definitions and simplification rules. We feel how- ever that they have a much more general signifi- cance, 
and similar results should hold of any pro- gramming language. Evidence in this respect is provided by 
work on the k-calculus (cf. Hyland, Levy, Wadsworth) and on LCF (cf. Milner, Plotkin). In order to help 
the reader through this pa- per, we have attempted to present our result at three different levels : 
in section 2, we try and give an informal presentation of the main theorem, illustrated by one simple 
example. In section 3, we show how to derive the theorem from two key lemmas. Each of these lemmas is 
then pro- ved in section 4. All authors mentionned in the text are refe- renced in the bibliography. 
 The language -or rather the class of langua- ges- we are using is derived from Kleene's and Mc Carthy's 
recursive definitions. We distinguish between data, allowing many kind of data-types- integers, lists, 
trees, reals, etc...-, and the control mechanism, namely recursion, which is the means by which we can 
perform computations over data. The data-type is determined by a set of sim- plification rules. To fix 
ideas, consider the un- realistic but simple example of natural numbers, represented in unary by 0, 
sO, ss0, .... Here, sx is intended to denote the successor x÷1 of x. The inverse operation, decrement 
kx.x-1 is repre- sented by the symbol d whose "meaning" is for- ced by the rules dsx -~ x dO ~ 0. 
The other basic operation kxyz. if x = 0 then y e_~lse z, denoted by c, combines test for zero and McCarthy's 
conditional operator. The corres- ponding simplification rules are : c0yz ~ y csxyz -* z. From these 
primitives s, d, c and O, we can de- fine any computable function through the use of rec~'sion. For instance, 
the addition axy of (the unary representation of) two natural numbers can be defined as : axy ~ cxyadxsy 
which, giving up prefix notation, reads : x+y -* if x=0 then y else (x-m)+(y+1). Similarly, the product 
pxy can be computed by pxy -~ cx0apdxyy that is p(x,y) -~if x=0 then 0 else (p(x-l,y)+y). We now possess 
a program P, which is formed by a pair (S,R) of simplification rules S = Idsx ~ x, dO ~ O, c0yz-~ y, 
csxyz ~ z 1 and (recursive) rewriting rules R = laxy ~ cxyadxsy, pxy -~ cx0apdxyyl. Each expression 
(term) can then be computed accor- ding to P. For example : as0ssO ~ cs0ss0ads0sss0 ~ adsOsss0~ aOsss0 
cOsss0ad0ssssO R sss0 S which means I+2 ~ if I =0 then 2 else (1-1)+(2+1) (I-I)+(2+I ) ~ 0+3 if 0=0 
then 3 else (0-I)+(3+I ) 3. The previous computation is summarized by the notation . asOss0 ~ sssO. 
. Similarly, ass0s0 ~ sssO and we can thus infer that the two terms as0ssO and ass0sO are equi- valent 
in the context of program P, since they compute the same value, namely sss0. This is an operational notion 
of equivalence which we denote by P ~ ass0s0 = asOss0, op a very fancy way to express 2+I = I+2 ! So 
far, we are thus quite willing to call equivalent any two terms which evaluate to the same (finite) expression, 
representing an element of our data-type. Even this may not be so simple : are the terms t I = sssssssssO 
and t 2 = ssssssssssO equivalent ? Well, after labo- riously counting up the symbols we realize that 
t 1% t 2 ; in order to do this, we have to go through an algorithm which should not be hidden in our 
definition. It is thus natural to consider the term t 3 = dddddddddx and compute t3[tl/x] ~ 0 versus 
t3[t2/x ] ~ sO. From this fact we conclude that P~ op tl % t2 since one can find a context, namely t3, 
in which t3Etl/x] evaluates to zero while t3[t2/x] does not. Our emphasis has thus shifted : we say that 
two terms are different if we can find a "context" in which one term evaluates to zero and the other 
one does not. Two terms will be equi- valent if one cannot show them to be different. This being agreed 
upon, all that remains to do is to define what exactly a context is. Plugging both terms t I and t 2 
into the same term t 3 is certainly a legitimate operation: this is similar to using the code of t I 
and the code of t 2 in the same place within the code of t 3 ; if t I and t 2 are equivalent to start 
with, we get equivalent programs after this transforma- tion. Note that interesting observations can 
al- ready be made : consider the recursive definitions ux ~ usx and ix ~ six. At first glance, both ex- 
pressions u0 and i0 seem to represent equally hopeless non-terminating programs. Looking closer, we realize 
that uO is indeed void of information while i0 represents the interesting number defined by ~ = ~+ I. 
To be convinced that indeed i0 ~ u0, consider t = cxO0, the conditional c being defined by the rules 
above ; we have t[i0/x] = ci000 ~ csi000 ~ 0 while t[u0/x] = cuO00 ~ cusO00 ~ cuss000 ~ o.o which never 
simplifies to 0 ; it follows that iO %u0. In fact, i0 has all kind of properties, among which being 
greater than any number, if we define > by the rule x A Y~ if y=0 then x else (x-l) ~ (y-l). Another 
natural construction of a context is to pass parameters to both terms. In this way, we show thai psxy 
and pxsy are different, i.e. p(x+1,y) ~ p(x,y+1), by setting, for instance, x=0 and y=s0, in which case 
ps0s0 ~ sO while P p0ss0 O. On the other hand, no matter which pa- rameter is passed to pssOx and axx, 
these will not become different, a hopeful sign that the iden- tity p(2,x)= 2 xx = x+x is indeed valid 
here. Parameters can also be other subroutines : the terms pkxy and pxky are shown different by adding 
(for example) the rewriting rule kx--sx to our set Re If P' =P+{kx~ sx}, we compute pk0sO ~, 0 and pOksO 
~, 0 establishing PI= pkxy ~ pxky. The intuitive meaning of this op context operation is : as long as 
the subroutine k is not specified, two programs using k should be regarded different if there is a way 
to define k which makes them different. Again, they are equivalent if they cannot be shown different, 
no matter how is specified the "dangling" subroutine k. For example, we shall soon be able to show that 
P~op kpssOx = kaxx, which means kp(2,x) =k(x+x) no matter which program k happens to be. One important 
way of forming a context is -77 - ioeo adding simplification rules. In the defini- tion of all programming 
languages, there are si- tuations in which the semantics of the basic ope- rations is not fully specified 
: input-output, zero divide, overflow, to list a few. Each imple- mentation of the language has to take 
decisions with respect to those peculiarities ; these must be consistant with the rest of the language 
but each may be quite different from the others. Two programs which are equivalent in one implementa- 
 tion might thus differ in another° We are lead to define as equivalent programs which are undistin- 
 guishable in any legitimate implementation of the language. To be concrete, consider again P =R+S with 
 S = {dsx ~ 0, dO ~ O,c0yz~y, csxyz~z} , and R = {ux ~ uxl, this last u being the unde- fined program. 
Should the terms u0 and cu000 be considered equivalent or can they somehow yield dif- ferent informations 
? To answer this non-obvious question, consider what happens if we add a new ze- roing function z to 
our natural numbers, speci- fied by the rules : S' ={zsx ~ 0, z0 ~ 0, zcx00 ~ 0}. Although these rules 
look funny, there is nothing w~ong with them, i,e. they constitute a legitimate extension to our previous 
set of rules. Let p, = S+S'+R ; compute zu0 ~,0 while zcu000~,0, thus showing u0 and cu000 to be dif- 
ferento To summarize this discussion, we call equiva- lent expressions which cannot be shown different. 
In order to show that two expressions t I and t 2 are different, given a program P =R+S, one has to find 
a context consisting of I) expressions t~ and t~ obtained from t I and t 2 by replacing variables by 
other terms; 2) a term t in which t~ and t~ are passed as parameters, i.s° replace a variable in t ; 
3) recursive definitions R' ~R ascribing mea- ning to whatever "loose" functions t I and t 2 may contain, 
or @ven redefining recursi- vely a primitive function ; 4) simplification rules S' ~ S which add new 
 primitive functions to the data-type, provi- ded that it is consistant (in a technical sense which 
will be defined later) with the set S. Once this context is found, t I and t 2 will be different, denoted 
P ~op tl ~ t2' if a computa- tion using, say, t I terminates with result zero, whereas it does not terminate 
with result zero using t2, i.e. t[t'/x], ~ 0 and not tit'/x] * 0 noted t[t;/x] ~ * ....o. If such a context 
cannot be found,~we say that t I and t 2 are operationally equivalent (undistin-guishable would be more 
precise) and we w-rite op tl = t2" Another completely different and fruitful ap- proach to defining program 
equivalence is through interpretations. An interpretation I consists of a domain D I with added structure 
: to each symbol1 a of integer arity ~(a) = p is associa- ted a p-ary function a I : D I P D I. Symbols 
of arity zero are interpreted as elements of the do- main. For the program P = R+S above R = {axy -* 
cxyadxsy}, S = {dsx ~ x, dO -~ 0, c0yz -~ y, csxyz -~ z l, a possible inter)retation can be constructed 
as follows : -the domain D I is made of two copies IN and ~__ of the natural numbers, denoted respectively 
0, I, 2 ..... n.... and 0, 1, 2 ..... ~ .... together with an extra element oo . This domain is ordered 
by : n ~ m n c m for n < m, and n ~ oo The relation ~ is pictured below 0 I 2 . . . n n ~ L " 0 -the 
symbols, s, d, o ~d a are interpreted as: I) si(n ) : n+1, si~ ) : n+1, si(O°) : ~, di~)=O , 2) di(n+1):n 
, di(O):O , dz(n+1)=n , dz(aO):o % 3) ci(O,Y,z):y, ci(n+1,Y,Z):Z, Cl~,y,z):O oi(n+1,y,z ) : z, ci(°°,y,z 
) : z. ~) ai(~,m):n+m, ai~,Y):O, ai(n,a)=n+m, ai(O%y ) = O, al(n,-- ) : oo. Given such an interpretation 
I, a mathemati- cal object t I is associated with a term t {T(A)  in the obvious way : to the term sad0s0 
is asso- ciated si(ai(di(0), si(O))) = 2 ; to asOx is as- sociated the function ~(n) defined by ~(n) 
= n+1, ~) = n+1, ~(~) = ~ etc. Of course, our interpretations must have spe- cific properties (the domain 
is an algebraic par- tial order, base functions are monotone and conti- nuous, recursively defined functions 
are least fixed points, simplification rules must be valid in the interpretation) which are defined precisely 
in the next section and whose intuitive meaning is discussed elsewhere (cf. Scott). Equality under the 
interpretation I is deno- ted by PI= ~ t = t' and, in our example P~I a~xy=axsy while P~I asxy % saxy. 
Semantic equivalence is then defined as meaning equivalence under all interpretations and we note : P~ 
t=t' iff PI=I t=t' se  for all interpretations I. In our example it is true that P~ asxy = axsyo The 
object of this se  paper is to prove the following : Theorem. Operational and semantic equivalences 
coincide, i.e. t=t' iff P~ t=t'. P~ op se  3. FORMAL DEFINITIONS AND PROOF OF THE MAIN RESULT 3.1. 
Symbols. The set from which are drawn the symbols of our language is a denumerable graded alphabet A 
: every element a in A has an arity ~(a) which is the number of arguments of the function repre- sented 
by the symbol a. The elements having arity n form a subset An, and A is the disjoint union of all the 
A : n A = A 0 + A I +...+ A n + ... A = {a EA I ~(a) = n}- n  Alphabet A is partitioned into A=B+X 
where X = [x0,...,x n .... } __c A 0 is a set of varia- bles, each of which being of arity zero ; B is 
a set of symbols for functions. We assu- me that the set B 0 of constant is denumera- ble and contains 
the distinguished element 0: 0 EB O.  3.2. Terms. With every graded alphabet A is associated a set 
of terms T(A) which is the language defi- ned by the grammar S ~ Z aS o.. S aEA ~(a) times Thus T(A) 
contains A 0 ; if t I,..., tn are terms, and aEA is a symbol of arity n, then n is a term. at I ... 
t n To make examples more readable, we feel free to use a(tl,...,tn) rather than atl...tn, or even tlat 
2 for binary operators, and if x = 0 then y else z instead of cxyz.  3.3. Substitutions. Let A=B+X 
; a subst~ution is a mapping o : X ~ T(A) which is the identity for all but a finite number of variables. 
We write  o = [to/X 0 ..... tn/Xn] , meaning o(xi) = t i  for 0 < i < n and o(xi) = x i for i > n. 
The substitution is extended into a mapping : T(A) ~ T(A) by t ~ t~=t[to/X 0 ..... tn/Xn], which denotes 
the term t in which the variables x0,..., Xn have been replaced simultaneously by t0,...,t n. We use 
greek letters to denote substi- tutions, and it is convenient to use the postfix notation to rather 
than the prefix o(t).  3.4. Programs. A program P, which will fix the rules accor- ding to which we 
rewrite terms, is a binary rela- tion over T(A). Of course, we impose many restric- tions on the kind 
of relation which forms syntac- tically legitimate programs° First of all, we want this relation to 
be effective and a drastic way to achieve this is to decrete that P must be finite (strictly speaking, 
this is not usually the case in real life programming languages, since the simplifica- tion mechanism, 
i.e. the hardware of the machine, is not conceptually bounded). Then, we partition program P into a 
dis- joint union of two finite relations R and S where -R is a finite set of (possibly recursive) rewriting 
rules of the type fxl...x ~ r, with n r E T(A)o To discard possible ambiguities when deriving fx 1...xn, 
the term r must not contain  variables other than x I ,... ,x n and each f must appear at most once 
in the left-hand side of the rules in R. We let U(P) denote the set of symbols f £B which appear in 
the left-hand side of the recursi- ve rules R of a program P =R+S. Such symbols must not appear in the 
simplification rules S of the same program. -the simplification rules in S are of the type g~ d, with 
gET(A) and dgX+Ibxl...Xnlb£Bl+B ~ To deserve their name, they must reduce the size of the terms to which 
they apply, that is Ig~I ~ Id~l for any substitution ~ and g-~d in S  (here, I tl is the number of symbols 
in term t). By repeatedly applying simplifications t ~ tl-~ ... ~ tk = ~ , we can associate with any 
term t a term t to which no rule of S applies. Such a term ~ is called irreducible for S. Legitimate 
simplifica- tions must have the strong property that each term t £ T(A) can be simplified into a unique 
irredu- cible ~ . In this case, we say that S has the Church-Rosser property. Note that O, and in fact 
 any element of B 0 is irreducible. Another important restriction on simplifica- tions is that no variable 
x £X is allowed to ap- pear twice in the left-hand side g (and thus in the right-hand side d, because 
of the length-re- ducing property) of a rule g ~ d in S.  REMARK. This excludes simplifications like 
the pa- rallel conditionel cxyy ~y or the parallel minus mxx ~ O, msxx -* sO. There are many reasons 
for re- jecting such rules ; to discuss some of them, con- sider P=R+S with R= If ~ sf, g-~ sg, u ~ 
ul and S =Idsx~x, dO-~O, cOyz~y, csxyz-*z, cxyy -* y, mxx-~O, msxx-~sO I. Here, mxy could be thought 
of as x-y. It can be argued (Vuillemin) that no efficient computation rule exists for such programs. 
More unpleasant still is the fact that, even though f -~ sf and g ~ sg are the same program up to renaming, 
they have very different behaviours : consider the ternB h=ccufxsO0 and h[g/x] versus h[f/x] ; we have 
h[f/x] p 0 while h[g/x] z ~ O, on the contrary.  Finally, what is the value of ruff ? Well, mff~O on 
one hand, but, since f -~ sf, we also have ruff ~ msff ~ sO ! Even though R and S both have the Church-Rosser 
property, it has somehow been lost in their union P = R+S. To summarize, a simplification set S is a 
find te collection of rules g -~ d where g E T(A) and dEB0+X+IbXl...XnlbEB I such that n   I) Idl 
 2) each variable x £ X which appears in d also appears in g ; 3) each variable x £ X appears at most 
once in g and at most once in d ; 4) the set S has the Church-Rosser proper- ty (which can be tested 
by an algorithm due to Knuth and Bendix). These conditions are equivalent to those given at the beginning 
of the paragraph. 3.5o Computations.  Given a program P, we construct a relation over terms as follows 
: for t, t' £ T(A) de- fine t ~ t' whenever there exists a term s £ T(A), a variable x E X, a substitution 
G : T(A) T(A) and a rule (u -~v) EP such that t = s[u~/x] and t' = siva/x]. Relation ~ is then the reflexive 
and transitive closure of ~. In section 4, we prove that ~ has the Church-Rosser property, i.e. for any 
 t, tl, t2£T(A ) such that ~¢p tl t t 2 there exists t 3 E T(A) such that tl . t 3 t 2 This property 
and the irreducibility of O en- sures that (t ~ 0 and t p tl) implies t I p O. 3.6. Operational equivalence. 
 If P and P' are both programs, we say . that P ~ P', when t I p t 2 implies t I ~,t2, which is the 
case for example if all rules in P are also in P'.  DEFINITION. Two terms r and s are operatior~ll 
2 equivalent with respect to program P, denoted P ~ r =s op if for all term t E T(A), all variable x 
E X, all substitution o and all program P' containing P : t[ro/x] ~ 0 iff t[so/x] ~ 0. 3o7° Semantic 
equivalence. Let us recall some definitions and results from Scott (or Milner) : DEFINITION. A complete 
partial order (c.p.o.) is a set D together with a partial order c such that I°) D has a minimal element 
I D ; 2°) every chain x I c ... c x c ... has a least upper bound (1.u.b.) denoted sup x or U x .  
i I i l A subset A of D is directed when, for every pair of elements x,y £ A, there exists an element 
z £A with x ~ z and y ~ z. The conjunction of conditions I ° ) and 2 ° ) is then equivalent to 3 °) every 
denumerable directed subset admits a lub. DEFINITION. A mapping f : D ~ D' between two cpo's is I) 
monotone when x~y implies f(x)~f(y), 2) continuous when it is monotone and for all denumerable directed 
sets A,  f(sup A) = sup f(A). The second condition is equivalent to the conjunc- tiQn of the following 
two : 2') f(ID) = ID, 2") for all chains ) u X I C...c x c. f(Ux i  ----n--"'' ± : i f(xi)" For polyadie 
functions, monotonicity and continuity are understood componentwise, according to the fol- lowing proposition. 
PROPOSITION I.- Any (possibility infinite) product of cpo's is a cpo for the product of the orders. 
-The set [D ~ D'] of continuous functions between two cpo's is a cpo for the point- wise ordering. PROPOSITION 
2. Every continuous function f : D~D has a least fixed point Yf in D, and yf = nU> 0 fn(±). We are 
now ready to define the semantics of our programming language. DEFINITION. An interpretation I of T(A) 
con- sists of I ° ) a cpo D, called the domain of I ; 2°) for each constant b EB 0 an associated bIED 
; 30 ) for each symbol b EB (n>0) an associa- n ted continuous function b I £ [D n -~ D]o By extension, 
any term t E T(B) is interpreted as an element t I of D according to the rules : -if t=c EB 0 then t 
I=c I -else t=btlo.ot with b £B and n n t I = bi(tli .... ,tni)O Next, if we assign to each variable 
x £ X an element d in D, a term t of T(B+X) will X be interpreted as an element ti(dx) in D ac- cording 
to the rules above. Thus t I should be re- garded as a continuous function D X ~ D. In fact, almost all 
the assignments x ~ d are irrelevant, x since only the variables occuring in t matter ; if X(t) denotes 
the finite set of variables oc- curing in ~ then t I is a continuous function D x(t)- D. So far, such 
interpretations have nothing to do with the program P, and we are only interested in the ones which "validate" 
P. DEFINITION. An interpretation I of T(A) valida- tes a relation -~when t -~ t' implies t I = t~. Of 
course, an interpretation which validates P validates S a fortiori. Conversely, an interpre- tation I 
of T(A-U(P))which validates S admits several extensions (on the same domain D) into an interpretation 
of T(A) validating Pc In the sequel, we use the following order bet- ween the interpretations of T(A) 
on the domain D : l k I<I'_ <=> V tET(A) V dxEDXQt), tI(dx)--~tI'(dx)" PROPOSITION 3. Let I be an 
interpretation of T(A-U(P)) which validates S. There exists a least extension of I into an interpretation 
of T(A) which validates P. Such an extension is called a P-model. Proof : calling D the domain of I, 
the set E of all extensions J of I on the domain D is itself  every unknown function is interpreted 
as the cons- tant function of value I ; and every chain of ex- tensions J0 <--'''~ Jk ~''" admits a 
lub J defined by Y t ET(A) tj (dx) =~ tjk(dx)o Furthermore, J~ validates S since the functions interpreting 
the left and the right-hand side of S are continuous. Now the set R of (recursive) rewriting rules 
 can be interpreted as a function, denoted also R : E~E. Given an extension J , in which the right- 
hand membeTs rl,...,r of the rules in R are ir~ q terpreted as functions rij,.o.,rqj , these func- 
tions are chosen to interpret fl .... ' f in R(J). q The extension J validates P if and only if it is 
a fixed point of R. But R is known to be continu- ous (Cfo Scott or Nivat) hence admits a least fi- xed 
point in the cpo E. [] We can thus define : DEFINITION. Two terms t and t' are semantical- ly equivalent 
with respect to program P, denoted P~ t = t' se  when t I = t~ for all P-models I. 3.8. The theorem 
of coincidence. THEOREM I. The operational and semantic equivalen- ces coincide : V P, V t, V t' P~ 
t=t' <=> P~ t=t' op se Proof : let us first prove that two terms t and t' semantically equivalent are 
also operational- ly equivalent. Therefore, take a program P' con- taining P, a term c in T(A), a variable 
x and a substitution ~. If two terms t and t' are se- mantically equivalent with respect to program P, 
are they equivalent with respect to program P' ? The answer is given by the following proposition. PROPOSITION 
4. Suppose that for two programs P =R+S and p1 =R,+S, the following two rela- tions hold : I °) R C 
R' 2 o ) t ~ * t' ~> t~* t'.  Then a P'-model is also a P-model. Proof : let I' be a P'-model. Call 
J the res- triction of I' to the terms of T(A-U(P)), J' the I the P-model which is the least extension 
of J validating Pc Since I' is an extension of J va- lidating P, one has immediatly I < I' In the other 
direction, notice first that, if I 0 and I 0 ' denote the least extensions of J and J' into interpretations 
of T(A), then I; ! I 0 since the interpretations of all functions coin-cide except for those in U(P')-U(P) 
which are ! interpreted as i in I 0. Secondly, suppose that we have the following diagram, in which 
the vertical and slanted lines indicate the order I !  l hK K'f I I,So I o Then, if (f ~ r) is 
in R, we have rK, ~ rK, and if (f ~ r) is in R'-R, we have rK'~rI' =fI' =fK" Thus, by definition of R'(K') 
and R(K), the following diagram is also true I' ~'(K') t I, I 0 10J This proves by induction on that 
R'n(I'0-- n )< Rn(I0) and when n goes to infinity, that I' < I, concluding the proof. Since P' contains 
P, the two assumptions in the proposition are true ; any P'-model is thus a P-model, and the terms t 
and ~ are equivalent with respect to P' : for all P'-models I : t I = t~. ~ence (to) I = (t'o) I and 
o[to/x] I = c[t'o/x] I . Assu~e that c[t~/x] -,* O, then in all P'-models I, p, c[t~/x] I and 01 will 
be equal, hence c[t'G~x] I = 0i, that is P~se c[t'a/x] = 0 We can then apply the termination lemma 
: TERMINATION LEMMA. If a term t is semantically equivalent to zero, then it computes to zero : (P'~se 
t=0)~--~tp,* 0.  Conversely, suppose that two terms t and t' are not equivalent. Then the separation 
lemma pro- ved in section 4 provides an effective evidence that they are not operationally equivalent 
: SEPARATION LEMMA. Let t and t' be two terms in T(A) such that PI= se t % t'. There exists a monadic 
function k, a substitution ~ and a simplification set S' ~ S such that the two assertions kt~ p~, 0 and 
kt'~ ~, 0 with P' =R+S' are equivalent. Thus ,the termioation and separation lemma, put together, provide 
the proof of Theorem I. [] -83 - In view of our main result, it is tempting to try and classify various 
program-proving, dispro- ving, constructing and debugging systems. It is always possible to look at such 
a system as defi- ning a congruence over terms, assuming a certain program P =R+S is given, which fixes 
the con- text° The only congruences of interest fall either ~i in the A or in the B part of figure I. 
Part A consists of the congruen- ces which are contained in the semantic equivalen- ce : for any such 
congru- ence =A' equality r =A s figure I implies P~ r = s. Such  se systems are therefore fit to 
prove program equiva- lence, i.e. correctness. The crudest example, at the Very top of the figure is 
the identity r=r, which is of little practical interest. More inte- resting is the relation r ~ S which 
expresses a very strong form a program equivalence. In an Al- gol context, this congruence would establish 
rela- tions such as a loop unwinding : while p do B~ if p then B ; while p do B fi. More interesting 
still are the congruences induced by program transforming systems such as the traditional code optimizers 
or program "mas- saging" systems (cf. Burstall). Of course, as such systems become more powerful, (i.e. 
the in- duced congruence comes closer to semantic equi- valence ) they automatically become less efficient, 
and there is nothing to do about this since se- mantic equivalence is not recursive (it is H 2- complete). 
One program-proving system is known to be more powerfull in this sense than all other existing ones, 
and it is Milner and Scott's LCF (cf. Milner, Vuillemin). Part B of the above diagram consists of the 
congruences which contain the semantic equivalence, i.e. PI= r = s implies r = B s or rather r % B s 
implies P~ r % s. Such systems are thus used for proving that prbgrams are different, which usually reflects 
some error which has to be corrected. At the bottom of part B in the diagram lies the uni- versaLl 
relation V r,s, r = s. More interesting is the congruence induced by the traditional debug- ging method 
: run both programs on a finite set of (carefully chosen) sample data and see wether they agree. Partial 
interpretors provide an attractive form of debugging : suppose that we are dealing with two programs 
manipulating lists. As a first step towards proving (or disproving) their equi-  valence, it might 
be easier to consider these pro- grams as working on integers, viz° the length of the lists being processed, 
and prove that the pro- gram thus induced are equivalent, vizo that they compute lists of the same 
length. Similar systems attempt to do type checking, verify that arrays referencing variables never 
run out of bounds...etc. Acknowledgements : Early considerations of David Park strongly motivated us 
to try and formalize the idea of different ~ather than equivalent pro- grams. Many enjoyable and stimulating 
discussions  with G@rard Berry and Jean-Jacques L@vy have great- ly contributed to this work. BIBLIOGRAPHIC 
REFERENCES G. BERRY and J.J. LEVY : "Minimal and optimal com- ~utations of recursive programs". To appear 
in JACM (1978). C. BOEHM : "Alcune proprieta delle forme 13-il- normali nel k-K calcolo". In Consiglio 
nazionale delle ricerche,n°696, Roma° R. BU~3TALL and J. DARLINGTON : "A transformation system for developping 
recursive programs". In JACM, vol. 24, p. 44-67, (1977). J. GOGUEN, J. THATCHER, E. WAGNER and J. WRIGHT 
: "Initial algebra semantics and continuous a:lgebras". Th~se, Paris VII.  G. HUET : "Confluent reductions, 
abstract proper- ties and application to term rewriting system~' In Proco of the 18 th Symposium on FOCS, 
pc 30-45, (1977). J.M. HYLAND : "A syntactical characterization of the equality in some models of the 
lambda calculus ". Ir J. of T_~s, (1975).  S. KLEENE : "Introduction to metamathematics". Van Nostrand 
C O (1977).  D. KNUTH and P. BENDIX : "Simple words problems in universal algebras". In computational 
problems in abstract algebras, Leech J. Ed., Pergamon Press, Braunschweig  (19 o). J.J. LEVY : "R@ductions 
correctes et optimales dans ls k-calcul". Th&#38;se d'Etat, Paris (1978).  Jo Mc CARTHY : "Towards a 
mathematical science of computation". In Proc. IFIP Munich, Conf., p. 21-28, North Holland (1962). "Recursive 
functions of symbolic expres- sions and their computation by machine". Part I, CACM vol. 3, P. 184 sq 
(1960). R. MILNER : "Models of LCF". In Stanford Artificial Intelligence labo- ratory, MEMO AIM-186, 
STAN-CS-73-332. M. NIVAT : "On the interpretation of recursive program schemes ". In Rapport de recherche 
n o 84, IRIA- Laboria. G. PLOTKIN : "LCF considered as a programming language". Memoransum SAI-RM-S, 
University of Edimburgh. J~A. ROBINSON : "A machine oriented logic based on the resolution principle". 
In J. ACM, vol. 12, p. 23-41 (1965). D. SCOTT : "Continuous lattices". Oxford mono PRG-7, Oxford University 
 (1972). J. VUILLEMIN : "Syntaxe, sdmantique et axio- matique d'un language de programmation simple". 
Thbse, Paris VI, (1974). C. WADSWORTH : "Semantics and pragmatics of the lambda-calculus"° Thesis, Oxford 
(Sept. 1971). -85 -   
			