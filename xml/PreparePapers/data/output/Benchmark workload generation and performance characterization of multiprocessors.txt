
 Benchmark Workload Generation and Performance Characterization of Multiprocessors Arun Nanda and Lionel 
M.Ni Department of Computer Science Michigan State University East Lansing, MI 48824-1027 Abstract In 
this paper, a comprehensive benchmark work­ load generation and performance characterization I J I methodology 
is described for multiprocessors support­ ing the shared-variable computational paradigm. The method 
can be tailored to meet the selective assess­ % inter­ ment needs of each individual situation. Our ap­proach 
is based on characterizing a unit grain of com­putation to generate a desired benchmark workload, and 
using a family of workload emulation kernels to systematically investigate the effect of each parameter 
in the workload on the multiprocessor performance. I % . . . connection Network III. $$j# .,,,.,,,,,,,:,,,,,,,,.,,,,,,,,.,,,: 
The resultant characterization is independent of any particular application or algorithm. Figure 1. Shared 
memory organization  Introduction We take a logical view of multiprocessors with Effective performance 
evaluation of highly-parallel shared-address space as shown in Figure 1. The mem­systems is essential 
because these systems must func-ory modules &#38;fl, . . . . MN represent distributed shared­tion at 
the limits of their computing potential in order memory modules whereas G1, . . . . GM are global meet 
the overwhelming demands of large scientific ap-shared-memory modules. Either of these components plications. 
Selective characterization of performance may be absent from the memory hierarchy. Simi­along controlled 
performance dimensions is integral to larly, caches Cl, . . . . CN may or may not be present. the design 
and implementation of better algorithms. No assumptions are made regarding the topology of Furthermore, 
data obtained from controlled perfor-the interconnection network or the distribution of mance characterization 
can also aid designers of fu-the shared-address space across the memory hierar­ture architectures and 
systems by pointing out flaws chy. Hence, the performance evaluation technique and assist in site-specific 
procurement needs with re-is equally applicable not only to the shared-memory spect to a target class 
of applications. architectures available today but also to any highly- Essentially, what is needed is 
a flexible benchmark scalable designs that may evolve over the years. It is workload generator and a 
performance characteriza-assumed that the underlying programming paradigm tion methodology that can be 
tailored to meet the uses shared variables for sharing data and performing selective assessment needs 
of each individual situa-synchronization operations. tion. More importantly, the characterization should 
The benchmark workload generator uses a hier­be independent of any particular application or al-archical 
approach to construct a variety of artificial gorithm. In this study, we have presented such a workloads 
of interest using the parameters that most methodology to characterize the performance of mul-influence 
the behavior of concurrent program execu­ tiprocessors supporting a shared-address space, and tion. At 
the lowest level, it uses a single grain of com­ illustrated its use on two commercial shared-memory 
putation, called a unit grain, as the unit of parallel multiprocessors. workload specification. The unit 
grains are assembled 1063-9535/92 $3.00 @ 1992 lEEE into a parallel computation structure at a higher 
level thus incorporating an algorithmic component into the workload. We have selected a class of iterative 
algo­rithms using a phase and transition structure as our parallel computation model. The performance 
of a parallel program workload using shared-variables and exhibiting only implicit synchronizations (caused 
by contention for shared re­sources such aa shared memory, critical sections) is in­fluenced strongly 
along three major dimensions: the distribution of shared-data over the memory hierar­chy and the concurrent 
memory reference patterns to access them, the nature of synchronization oper­ations between concurrent 
processes to preserve the consistency of shared-data, and the presence of global synchronization barriers. 
Along each performance di­ mension, the behavior of a given program is a complex function of a number 
of architectural as well as ap­ plication parameters. The performance characterize relies on the creation 
of a number of selective work­ load patterns using the benchmark workload genera­tor suitable for measuring 
the sensitivity of system performance along each of the above three perfor­mance dimensions. A family 
of workload emulation kernels memory access degradation (MAD), syn­chronization access degradation (SAD) 
and barrier ac­cess degradation (BAD) kernels have been devel­oped and used to observe and quantify 
the perfor­mance behavior for each of the dimensions. The rest of this paper is organized as follows. 
In Section 2, the individual components of the pro­posed performance characterization methodology are 
described. This method is used to investigate the memory access and synchronization performance of two 
commercial multiprocessors. The details of the benchmark workload generation and the experimen­tal results 
for the two cases are presented in sections 3 and 4, respectively. Section 5 summarizes the main results 
along with comments on the planned future work related to this study. Evaluation Methodology In this 
section, we describe a systematic technique that can be used to devise synthetic workloads that measure 
basic individual components that affect the performance of shared-memory multiprocessors, and then use 
these select workloads to evaluate their per­formance. Thus, the synthetic workloads are used as benchmarks 
for the performance characterization process. This implies that the former activity, bench­mark workload 
generation, and the latter, perfor­mance characterization, must be seen as two distinct activities. The 
results obtained from the system char­acterization task represent different aspects of inter­action between 
the machine architecture and the ap­plication structure. 2.1 Parallel Workload Definition We model the 
computation in a single process (or thread of activity), which is part of the parallel work­load, ss 
a sequence of loop iterations that may be random or deterministic. Each such loop iteration represents 
a single grain of computation, called a unit grain and denoted as G. The sequence of iterations, therefore, 
represents a string of grains constituting the execution profile of a single processor in a parallel 
program. The unit grain is the fundamental level at which all performance measurements are taken. .......1.... 
 # B  , / :[: ~, ./ LCCK ,x es G .:; ;:g..,, ,, ...~  F? . o,..~ , ,,, ,,,,,,, LNLOCK8 :,, ~:, ;. 
4 , 3 ..: ........... ............­ ........... ... GS Cnbcal Secbon .......L .... Figure 2. Structure 
of a unit grain Each unit grain G is further assumed to be com­posed of exactly three granules: shared-memory 
ac­cess, local computation and synchronization (Fig­ure 2). A shared-memory granule, denoted as g~, is 
concerned with accessing globally shared data needed for the computation. Most often, access to glob­ally 
shared data within this granule would be in concurrent read mode, since writes to shared data must be 
properly guarded within critical sections in order to preserve memory access consistency. In situ­ations 
where concurrent writes are legitimate and con­sistency preserving, however, gm could include writes 
to shared data. A local computation granule, denoted as ge, represents the portion of the execution grain 
that performs CPU bound computation using only process private data. We assume that any shared data needed 
for the computation is first retrieved into a process private area (possibly internal registers or processor 
cache) before being used. A synchroniza­tion granuie, denoted as g., represents inter-processor synchronization 
operations such as mutual exclusion (for implicit synchronization), event post/wait (for explicit synchronization) 
etc. This granule imposes an ordering restraint on the otherwise concurrent ex­ecution of a multiprocessor 
application. Using this decomposition, the unit grain G is defined to be a 3-tuple of granules. G=(gm, 
gc, gs) A special characterization called null characteriza­tion, and denoted by gi = ~, is reserved 
to indicate that granule gi is absent from the unit grain. Any component granule in the definition of 
G can be null, reflected by the alternate bypass paths shown around each granule in Figure 2. The unit 
grain G is char­acterized by choosing an appropriate characterization for each of its component granules. 
The choice of at­tributes needed to characterize each granule depends upon what aspect of the multiprocessor 
system per­formance is under study. 2.2 A Parallel Computation Model To be universally applicable, the 
system characteriza­tion measurements must be based on a uniform model of execution for parallel computations 
so that the re­sults of an experiment can be related to previous and future experiments. A popular model 
for program ex­ecution is the phase and transition model. A parallel computation can be viewed as a sequence 
of compu­tational phases separated by global synchronization points (or barriers) (as shown in Figure 
3). Many sci­entific and engineering problems adhere to this model in practice. Application examples 
represented by this computational structure include partial differen­ tial equations (PDE), parallel 
FFT, molecular-motion computations, weather prediction models, etc. Q START Phrse 1 : :,,:::,,,.,.:* 
RICE.:. ..., . . ... .:: Pk.e 2 : .i3ARslw:.. . : :::: ~~~ Phasev c) Figure 3. Structure of parallel 
program execution Each phase is comprised of a string of loop itera­tions. The computation and communication 
patterns are well defined during each iteration. Frequently in these applications, the computation can 
be uni­formly distributed among processors thus assigning equal amount of work with identical characteristics 
to each processor. Therefore, we can assume that each process performs a series of identical iterations 
within a phase. The performance of a parallel system in the short term during one iteration is used 
to model its long term performance [1]. in Figure 3, assume that there are v computational phases. Assume 
that phase k is comprised of rk iden­tical iterations on each processor and the number of processors 
employed (degree of parallelism) is Nk. If tki is the time it takes to complete one iteration on processor 
i during phase k, then the total time to complete rk iterations on processor i is given by rktk~ since 
all iterations are identical. Hence, the time Tk required to complete phase k of computation is given 
by where tk is the effective iteration time for phase k. At the end of each phase, all processes must 
wait for the slowest process among them before they can continue. The time spent in waiting for the last 
process to ar­rive is already accounted for in the phase execution time Tk (~k ). However, the additional 
time penalty needed to broadcast the event of the arrival of the last process at the barrier contributes 
to the total ex­ecution time. This time, TB (Nk ), depends not only on the number Nk of processors involved 
in the bar­rier, but also on the implementation and the method used to busy-wait for the arrival of the 
last process. If all the sequential components of the parallel program execution, such as creation of 
parallel processes etc., can be represented by the single time component TS, then the total execution 
time T of the computation is given by T = TS + ~(Tk(Nk) + TB(N~)) k=l TS + $(r~tk(Nk) + TB(Nk)) k=l 
Using this model, if the per-iteration execution time fk and the barrier performance TB can be accurately 
characterized for a given workload for varying degrees of parallelism Nk, then the overall performance 
of the computational workload can be estimated. 2.3 Structure of Experiments We have, so far, defined 
the unit grain on a single pro­ cessor. We now incorporate e the element of execution parallelism and 
the impact of implicit synchroniza­ tions into our framework. For this purpose, we have established an 
experimental structure for our mea­surements (see Figure 4) that consists of one test processor (called 
Po), a variable number, N, of competitor processors (called P,, P2, . . . . P~), and  a number, ill, 
of data elements that are shared by the test and competitor processors.  -----------------------------i? 
I I , I t ~ .......... I I , i , 8 , , I t , 0 , ,,;=r<&#38;*~;, 1 $-------------------------­ w 
- Figure 4. Competitor workload framework The unit grain whose performance is under study is executed 
on the test processor P.. This grain is called the test grain and denoted by Gt = (g~, g~, g; ). A null 
characterization of the test grain (i. e., Gt = (~, q5, ~)) is, of course, meaningless. Each competitor 
processor executes a unit grain called the competitor grain and denoted by G. = (gfi, g:, g~). A null 
characterization maybe used for GC. Every competitor processor, PI, .... PN, executes an identical copy 
of the competitor grain GC concur­rently. The number of competitor processors, N, can be varied to control 
the degree of parallelism and, hence, the extent of implicit synchronization among the concurrent grains. 
The value of h4 determines the size of the shared-data space within which all the grains operate. A hot-spot 
scenario results from set­ting M =1. The set of input parameters to the experiment, I, can now be consolidated 
and written as I = {N, itJ, Gt, GC} Note that by setting (7$ = Gc, we can create a ho­mogeneous workload; 
or by using different characteri­zations for Gt and GC (Gt # GC), we can create a het­erogeraous workload. 
By varying N, the performance degradation of Gt under varying degrees of external interference can be 
measured. We will adopt the con­vention Gi [z] to refer to attribute z of grain Gi and Z to represent 
a varying attribute. For a given set I = {N, Al, Gt, G.} of input parameters, the execu­tion time of 
the test grain Gt when competing with N concurrent instances of G. is denoted by TN (Gt \ G,), and read 
as the time to execute Gi given that N concurrent instances of (GCexist. 2.4 Workload Emulation Kernels 
Once a characterization for the execution grain has been selected, we need an emulation program that 
mimics the specified workload pattern. The Mem­ory Access Degradation (MAD), Synchronization Ac­cess 
Degradation (SAD), and Barrier Access Degra­dation (BAD) kernels are a family of such emulation programs. 
No real computation is performed by these kernels. Their only purpose is to mimic the usage of shared 
resources of the specified workload and gener­ate synthetic loads that are designed to stress a par­ticular 
aspect of the target system. Each kernel is written to use a set, I, of input parameters specifica­tion 
and generate a set of performance measures, @(I), of interest by executing the the emulated workload 
in a controlled experiment. Each experiment represents a point in the performance space of the system. 
As seen from Figure 4, every participating pro­cessor executes a unit grain (test or competitor), specified 
by the input parameters I, repetitively and records the execution time per unit test grain. Tjo TO @(I) 
= TN(Gt I G.) = Nitr The choice of suitable values for Nitr and NTePeat is crucial to the minimization 
of experimental error and the confidence interval of the measured quanti­ties. For a detailed discussion 
on this subject, see [2]. We define two metrics of performance based on the experimental execution times. 
The first metric, called unit grain eficiency and denoted by ~, is a measure of the relative performance 
of the unit grain in the presence of contention with respect to its uncontested execution time. In general, 
for a given combination of Gt and G=, it is a function of the number N of competitor processors and is 
given by the following ratio. To(Gt I Gc) 4(N) = TN(G, [ Gc) The value of ((N), O < <(N) <= 1, for a 
given point in the performance space, expresses the level to which the execution performance of the test 
grain has dete­ riorated as a result of conflicts. A value of <(N) = 1 indicates no degradation at all 
representing the ideal situation. The next metric, called interference and denoted by @(N), offers a 
measure of how much longer is the expected execution time of a test grain in a conflicting situation 
compared to its expected conflict-free execu­tion time. This results in the following definition of the 
interference measure. TN(Gt I G.) 7 o(Gt I G.) = 1 C(N) O(N) = TiO(Gt I G,) <(N) 3 Memory Access Performance 
In this section, we describe the unit grain character­ization chosen for the MAD kernels to measure and 
quantify the losses resulting from resource contention due to shared-data access during the parallel 
workload execution. Only a few excerpts from the experiments performed are presented to illustrate the 
usefulness of our approach. A more comprehensive study of the memory access performance using this approach 
is re­ported in [3]. 3.1 Workload Characterization The major consideration in memory system design for 
multiprocessors is that the memory bandwidth must match the memory demand of the processors. The ef­fectiveness 
of the memory design in meeting this goal depends not only on the organization of the memory hierarchy, 
but also on the distribution of the shared data in the hierarchy, the memory reference pattern of the 
program, and the locality of memory references. In addition to temporal locality and spatial locality 
of references, parallel computing also makes a new type of locality, called processor locality, desirable. 
To keep high processor locality, unnecessary interleaving of references by more than one processor to 
the same memory data should be avoided. We need a set of attributes for characterizing the unit grain 
that can systematically probe the memory system by creating diverse sets of memory address streams to 
determine its sensitivity to different performance factors. Characterization of gm: The shared-memory 
access granule gm is character­ized by a &#38;tuple of attributes: gm = (t, p,s, m). The first attribute, 
t,simply indicates the type of shared­memory access, read (t = O) or write (t = 1), being performed. 
The next attribute, p, controls the start­ing address for shared-data access by the concurrent processes 
relative to each other. It denotes the dis­tance between the starting points of memory access streams 
of the parallel processes expressed as number of shared data elements. In other words, if there are &#38;f 
shared data elements in all, then the processor Pi begins its string of memory accesses with element 
ip (modulo M). Thus, if the shared data elements are accessed with a regular stride, then the attribute 
p can be used to stagger the starting addresses of mul­tiple processors in any desired fashion. For instance, 
a value ofp = O will cause all the participating proces­sors to begin their shared data access with the 
element numbered O. The attribute s represents the stride of shared data access from one memory access 
to the next, thus defin­ing the spatial distribution of the memory request streams. By manipulating the 
access stride, the ef­fect on performance of the mapping strategies used to assign elements of an array 
to the memory banks at a given hierarchy can be evaluated. Depending on how the shared data elements 
are distributed over the memory hierarchy, using different access strides will cause the memory request 
transactions to tra­verse over different components of the processor to memory interconnection. Finally, 
the attribute m denotes the number of memory accesses to be performed within a single memory-access granule. 
The value of m determines the granularity of shared data access within a grain. The main purpose of changing 
this attribute is to con­trol the density of memory requests, thus highlighting the interaction between 
request bursts and idle peri­ods. Characterization of gc: Since all the computation within granule gc 
oper­ates purely on processor private data out of a pri­vate memory space (assumed to be available locally), 
by our definition, it does not alter the memory in­terference bahavior of the shared data access stream 
which is external to the processor. Its only influence is setting the memory access rate. Hence, we have 
characterized the computation granule gc by simply a l tuple consisting of a delay count: gc = (c). The 
attribute c represents the number of computational steps performed within a unit grain, and is expressed 
in terms of a standard delay unit (a step of compu­tation). Charact erizat ion of gs: As only the shared 
memory access performance is of interest here, the null characterization was chosen for the synchronization 
granule, i.e., g, = ~. Using the individual granule characterizations, the definition for the unit grain 
G can be written as the 3-tuple of tuples. G = ((t, p,s, m), (c), ~) Assigning constant values to each 
attribute creates a deterministic instance of the unit grain G, whereas assigning probabilistic values 
creates a stochastic in­stance. 3.2 Summary of Experiments The two multiprocessor systems used in our 
study are the Sequent Symmetry S81 and the BBN TC2000. The Sequent Symmetry S81 [4] is a bus-based shared­memory 
multiprocessor that supports up to 30 pro­cessor nodes. Each processor node consists of an Intel 80386 
processor equipped with a 64 KB two-way set­associative cache. All caches in the system are kept coherent 
by snooping on the bus. Cache coherence is enforced by using a write-invalidate copy-back caching policy. 
The BBN TC2000 Butterfly parallel proces­sor [5] is a distributed shared-memory multiprocessor. Each 
processor node contains a Motorola 88100 RISC processor, 4 or 16 Mbytes of memory, 16 Kbytes each of 
instruction and data cache and a switch interface. All shared-data, by default, are not cached. Each 
pro­cessor can access its own memory directly, and can access the memory of any node through a multistage 
switching network. All the experiments were carried out using a single word (32 bits) as the size of 
each shared data element. Only single memory-access streams were used in each grain. The shared data, 
with A4 elements, were al­located using the shmalloc ( ) call on each machine. On the Symmetry, the data 
elements are interleaved across the memory modules with a interleaving gran­ularity of 32-bytes. On the 
TC2000, the shared-data uses shared, uncached memory and is scattered across the allocated cluster. We 
only present some excerpts here from a detailed study using the MAD kernels in [3]. 3.2.1 Homogeneous 
workloads In these experiments, the attributes for the test and competitor grains were set to be identical, 
i.e., Gt = G.. Thus, the resultant performance degrada­tion when concurrent grains with identical execution 
behavior compete was measured. Spatial distribution. By manipulating the stride s of shared-data access, 
and by choosing a value of M large enough to cause a complete sweep of all the memory modules, the ef­fectiveness 
of the interleaving of the main memory system is probed. Changing the value of s, in effect, creates 
different spatial distributions of the memory access stream generated by each process. Temporal distribution. 
The variation of the density of memory requests of each processor is accomplished by altering the number 
Sequent Symmetry ~~ 0.4 -N= lo+-N= 20-o­N=10 -e-­N=20-o- I1 0 02468 10 No. of computation steps (c) 
fi, M=128K, Gt=G&#38;=2, s=16,rn= l,c=O] Figure 5. Effect of temporal distribution of accesses of computation 
steps performed within the computa­tion granule g.. This corresponds to a shared memory access followed 
by a subsequent interval of c units of delay with no memory ~access. Figure 5 shows the improvement in 
unit grain efficiency that is achieved as a consequence of increasing the length of gc on the Symmetry. 
The effect is particularly striking for write operations, since the intervening computational delay without 
any bus accesses provides sufficient time for the cache-invalidation traffic on the bus to reach qui­escence. 
Memory hot spot. The interference profiles generated by setting M = 1 is indicative of the performance 
of the execution grain under severe contention (hot spot) conditions. In these experiments, the processors 
not only contend for the global interconnection network, but also for a single shared-data item. Size 
of shared data. By manipulating the size M of shared data, all memory references on the Symmetry can 
be kept in the cache, or made to flush cache on each pass through the shared-data. The TC2000, on the 
other hand, does not cache shared data. However, varying the shared-data size on the TC2000 revealed 
some inter­esting facts. The efficiency ( was observed to behave identically for values of M from 1 through 
4. Pro­gressive improvement in ~ was observed for each in­crement of 4 in the value of M (Figure 6). 
This would imply that the scattering ofshared-data by the system across cluster memory modules was done 
in chunks of 4 elements (i. e., 16 bytes). Thus, going from M = 4 to M = 16 (and so forth) increases 
the number of memory modules, for which the processors contend, from 1 to 4 (and so forth) leading to 
a decrease in BBN TC2000 I 1 I I I I I I 1 I 1! h4 = 256 ----i o~ 048121620242832 No. of competitors 
(N) f 7,ii, Gt=Gc[t =0, p=l, s=l, T7z=l, c= o] Figure 6. Effect of shared-data size on read contention. 
3.2,2 Heterogeneous workloads Using a heterogeneous workload, we have investi­gated the interactions 
that occur between concurrent read and write memory access streams. In particular, we observed the conflicts 
between concurrent read and write memory access streams using the following two scenarios: (a) Case J: 
the test grain performs read (write) ac­cesses to shared data with uniform stride, while the competitor 
grain performs write (read) ac­cesses with random stride, (b) Case 2: the test grain performs read (write) 
ac­cesses to shared data with uniform stride, while the competitor grain performs write (read) ac­cesses 
to a single shared (hot spot) location.  The measured efficiencies and interference values are documented 
in [3]. 3.2.3 Random memory access Most multiprocessor memory organizations are de­signed to use special 
techniques (such as memory in­terleaving, skewing) to maximize the performance of uniform memory-access 
patterns. However, in or­der to highlight shared memory performance for non­uniform accesses, we measured 
memory bandwidth under random access conditions, expressed as Words Accessed Randomly Per Second (WARPS), 
to quantify this performance. This is done using a homogeneous workload consisting of only memory-access 
granules gm and varying its stride attribute s randomly. The results of these tests are presented in 
Figure 7. 4 Synchronization Performance The unit grain characterization technique was also used to construct 
a group of synchronization access workloads to study the behavior of concurrently exe­cuting grains when 
mutual exclusion is enforced. Mu­tually exclusive access to read-write shared data is performed within 
a critical section (CS) guarded by a pair of lock/unlock operations. In this section, we briefly present 
some of the results obtained from the synchronization performance measurements [6]. 4.] Workload Characterization 
Besides the implementation technique used for the lock and unlock operations for the critical section, 
the other significant factors that affect the execution be­havior of a unit grain subject to mutual exclusion 
are the frequency of synchronization and the length of the critical section. The frequency determines 
how often does a grain in execution encounter a serialization ef­fect, whereas the duration of the CS 
governs the time for which the lock is held thus controlling the number of (idle) processors waiting 
to acquire the lock (queue length). The shared-memory access granule g~ is comprised of a I tuple of 
the single attribute m that denotes the number of memory accesses performed outside the critical section. 
These memory references are to shared-data that can be accessed concurrently. The main purpose of changing 
this attribute is to control the memory access density external to the CS thus observing the impact of 
the CS implementation on the external memory accesses and vice versa. The computation granule gc is represented 
simply by a l iupie consisting of a delay count c. The attribute c represents the number of standard 
delay units . Since the durations of the granules g~ and g. in­directly determine the frequency of occurrence 
of the synchronization granule, we characterize gm with a 3 tuple of additional attributes necessary 
to control the duration of the CS and the shared-data access pattern within it. g. = (C,, m,, f,) The 
value of the attribute c. indicates the number of computational steps performed within the CS, using 
processor private data, expressed in exactly the same delay unit as in g.. This time interval is marked 
by the fact that there is no access to the shared-memory, and thus no contribution to the global interconnec­ 
tion network traffic, by the processor executing the granule. The attributes rrz~ and ~, together define 
the nature of memory accesses performed from within the critical sect ion. The tot al number of shared 
memory references within the CS is given by m$, while f~ in­dicates the fraction of these references 
to shared data PTE%a%l that are write operations. All the shared data ac­cesses within this granule 
are assumed to go out over the global interconnect thus contributing to network traffic. Using the individual 
granule characterizations, the definition for the unit grain G can be written as the 3-tup/e of tuples. 
4.2 Summary of Experiments We chose three spin-lock implementations for com­parison on each of the target 
systems studied. The first one was the native lock/unlock operations pro­vided on each system (referred 
to as the NAT lock) to support parallel programming. This support is in the form of function calls in 
a parallel program­ming library. The other two implementations selected somewhat represent two extremes 
of busy-waiting ef­ficiency. The test-and-test-and-set lock (referred to ss the TAS lock) spins by reading 
the shared lock variable until it becomes free, and then attempts a test-and­set operation to acquire 
the lock, The last spin-lock implementation chosen was a list-based queuing lock devised by Mellor-Crummey 
and Scott [7] (referred to as the MCS lock) with the characteristics that it spins on locally-accessible 
flag variables only and re­quires a constant number of network transactions per lock acquisition on machines 
with and without coher­ent caches. As in the case of memory access perfor­mance, both homogeneous and 
heterogeneous workload settings were used in the experiments. The heteroge­neous workload families were 
particularly useful in re­vealing the interactions between code executed within and outside the CS. An 
important fundamental criterion for any lock implementation is its latency the time it takes to acquire 
and release it in the absence of competition. Table 1 shows this measure for the locks used in our study. 
For the TC2000, since a dichotomy in the memory hierarchy exists, the latency when the lock variable 
is in the processor-local memory or a remote memory are shown separately. 4.2.1 Homogeneous workloads 
Frequency of synchronization A critical section (CS) synchronization enforces a sequential ordering on 
the participating processors. The probability that a processor arriving at the CS finds it busy, thus 
incurring a queueing delay, is pro­portional to the frequency with which the CS is ac- NAT 7.4 ps ]4.3 
ps ] 12.1 ps ] Table 1. Latency of locks used in the experiments cessed [8]. By increasing the granularity 
of compu­tation between synchronization points, it is possible to alleviate the loss in parallelism. 
To determine the granularity at which the loss in efficiency due to syn­chronization is minimized for 
a given number of pro­cessors, we measured the unit grain efficiency by vary­ing the length of the computation 
granule gc for each value of N [6], Computation to communication ratio An important factor governing 
the performance of an application on a multiprocessor is its computation to communication ratio. Communication 
on multi­processors is achieved through shared data structures that are modified in critical sections. 
The length of the CS is representative of the amount of time re­quired for inter-processc,r communication. 
In Fig­ure 8, the variation in grain eficiency is shown as a function of the length of communication 
in CS (c, ) for a fixed length of computation (c). A similar effect is also accomplished by varying the 
amount of shared­data access within the (1S (m. ) for a fixed amount of shared-data access outside the 
CS (m). However, now the shared-data accesses within the CS may also encounter additional memory-access 
conflicts result­ing in longer CS duration. The grain efficiency on the Symmetry decreases with an increase 
in the CS length. For N = 10, the MCS lock is a little less effi­cient for low CS lengths. This can be 
attributed to its higher latency compared to the other two spin-locks. However, for higher values of 
N, lock contention be­comes the dominant factor and MCS outperforms the others. On the TC2000, Mcs exhibits 
decreasing effi­ciency with increasing CS length, but constantly out­performs the other two implementations. 
4.2.2 Heterogeneous workloads Using a heterogeneous workload, we have investi­gated the interactions 
that occur between concurrent execution of code within and outside the critical sec­tion. In particular, 
we explored the following two sce­narios: (a) Impact of memory accesses done outside the CS on shared-data 
access within the CS, and (b) Impact of the CS execution on memory accesses  Sequent Symmetry BBN TC2000 
MWARPS MWARPS ,, ~.... ....... ...-.. ...... ........................... . ....................... ...........7 
 12 . ................................................................................................. 
 8­ 6­ 4­ 2­ 9 pwoc Iapmlc 17 voc 21 pmc 1 pmc 7pm 13 Pr0c 49 proc 2s proc 31 pmG = Re-m Wri2e = Raad 
~ Wrte 1P 5PC Figure 7. Random access performance expressed in MegaWARPS Sequent Symmetry (g. = 120) 
BBN TC2000 (gC = 300) If 0.8 -0.8 -=10 ( ((N) 0.6 - 0.6 - I ! 6) < 0.2- 0.2­ . (j~ 1IIII 0-O2468101214 
0 5 10 15 20 25 30 Length of computation in CS (c.) Length of computation in CS (c. ) fi, itf = 128K, 
Gt = G.[gm = (0), 9C,9S = (Z,QO)l Figure 8. Effect of non-CS to CS computation ratio on performance Sequent 
Symmetry BBN TC2000 1.2 MCS -1.2- I I I I I 1IIIi1I NAT * 1( TAS-1( 0.8-0.8-C(N) 0.6 -((N) 0.6 ­ 0.4- 
0.4­ 0.2 -0.2-I I 1 1 I ItIIIII 0o O 4 8 12 16 20 04 8 121620242832 No. of competitors (N) No. of competitors 
(N) fi, iw=1281f, G,[g~ = (O, l,4,20), gc = d,gs = @], G.[grn = @,gc = @,gs = (0,0,0)1 Figure 9. Impact 
of CS spin-lock on non-CS memory accesses 28 done outside the CS, An important measure of synchronization 
perfor­mance is the additional amount of interconnection network traffic caused by multiple processors 
attempt­ing to synchronize, and the impact of this traffic on the execution of the other components of 
a unit grain. This measure was obtained by recording the performance of a test grain composed of only 
shared­memory memory accesses (granule gm ) when compet­ing with grains comprised of only critical section 
ac­cesses (granule g$ ). The results are shown in Figure 9. As expected, the MCS spin-lock outperforms 
the other two by a significant margin on the Symmetry for val­ues of N > 4 due to its constant number 
of network accesses per lock acquisition, thus contributing mini­mal additional bus traffic. On the TC2000, 
the NAT and TAS spin lock traffic interferes with other shared­data access causing a decrease in the 
grain efficiency. The extent of degradation is not as prominent as for the Symmetry due to the higher 
bandwidth of its in­terconnect. 5 Conclusions and Future Work We have presented a synthetic approach 
to perfor­mance evaluation of multiprocessors supporting a shared-address space that can be tailored 
to indi­vidual performance measurement needs. Its primary strength lies in the fact that the characteristics 
of the benchmark workload used to measure a machine per­formance can be customized and, hence, used to 
assess a multiprocessor along selective performance dimen­sions. The foundation for the benchmark workload 
generator is based on the definition of a und grain of computation and selection of an appropriate set 
of attributes to characterize the unit grain. The per­formance characterize is comprised of a family 
of workload emulation kernels. The proposed method­ology has been used to characterize and compare the 
memory access behavior [3] (using the MAD kernels) and the synchronization performance [6] (using the 
SAD kernels) of two commercial multiprocessors. A slight variant of the MAD kernels has also been used 
at Oak Ridge National Laboratory for a preliminary evaluation of the newly announced KSR1 system from 
Kendall Square Research [9]. In order to complete the characterization of a multiprocessor performance 
along the three primary dimensions discussed in Sec­tion 1, we are working on constructing and validating 
a set of BAD kernels to evaluate the impact of barriers on a parallel workload performance [10]. 1. D. 
F. Vrsalovic, D. P. Siewiorek, Z. Z. Segall, and E. F. Gehringer, 1 erformance prediction and calibration 
for a class of multiprocessors, IEEE Transactions on Computers, vol. 37, pp. 1353­1364, NOV. 1988. 2. 
R. Saavedra-Barrera, Machine characterization and benchmark performance prediction, Tech. Rep. UCB/CSD 
88/437, University of California, Berkeley, June 1989. 3. A. Nanda and L. M. Ni, MAD kernels: An exper­imental 
testbed to study multiprocessor memory system behavior, Tech. Rep. MSU-CPS-ACS-48, Michigan State University, 
Department of Com­puter Science, 1992. 4. Sequent Computer Systems Inc., Symmetry Tech­nical Summary, 
1987. 5. BBN Advanced Cclmputers Inc., Cambridge, Massachusetts, TC2000 Technical Product Sum­mary, 
Nov. 1989. 6. A. Nanda and L. M. Ni, SAD kernels: An evalua­tion methodology for synchronization behavior 
of multiprocessors, Tech. Rep. MSU-CPS-ACS-55, Michigan State University, Department of Com­puter Science, 
1992. 7. J. M. Mellor-Crumrney and M. L. Scott, Al­gorithms for scalable synchronization on shared­memory 
multiprocessors, ACM Transactions on Computer Systems, vol. 9, pp. 21-65, Feb. 1991. 8. A. Nanda, H. 
Shing, T.-H. Tzen, and L. M. Ni, Resource contention in shared-memory multi­processors: A parameterized 
performance degra­dation model, Journal of Parallel and Dis­tributed Computing, vol. 12, pp. 313-328, 
July 1991. 9. T. H. Dunigan, Kendall Square multiprocessor: Early experiences and performance, Tech. 
Rep. ORNL/TM-12065, C~ak R~dge National Labora­tory, Oak Ridge, Mar. 1992. 10. A. Nanda and L. M. Ni, 
BAD kernels: A study of barrier performance in shared memory multi­processors, Tech. Rep. (in preparation), 
Michi­gan State University, Department of Computer Science, 1992.   
			