
 Towards a Hardware Fault-Injection Testbed to Support Reproducible Resiliency Experiments Ron Sass 
Recon.gurable Computing Systems Lab Univ. of N. Carolina at Charlotte 9201 University City Blvd. Charlotte, 
NC 28223 rsass@uncc.edu  Rahul R. Sharma Recon.gurable Computing Systems Lab Univ. of N. Carolina at 
Charlotte 9201 University City Blvd. Charlotte, NC 28223 rahulrsh@gmail.com Nathan DeBardeleben Los 
Alamos National Laboratory P.O.Box 1663,MS B272 Los Alamos, NM 87544 ndebard@lanl.gov As the largest 
computers in world continue to scale up to very large number of nodes, signi.cant new problems emerge. 
Due to a number of di.erent sources, hard and soft failures cause correctness issues and performance 
degrada­tion in high component count machines. These issues simply do not occur at an appreciable rate 
in smaller scale machines. This presents a serious problem to researchers interested in studying and 
countering the issues because e.ects are not reproducible smaller scale testbeds. The Recon.gurable Computing 
Cluster (RCC) project is investigating the feasibility of using just FPGAs acceler­ators to build a high 
end computing computing system. However, since the hardware is very con.gurable, it is pos­sible to also 
use this infrastructure to introduce special­purpose hardware cores that are designed to perturb or cor­rupt 
an existing system in a controlled way. This architec­ture o.ers an ideal platform for performing fault 
injections and Heisenberg-free monitoring of parallel programs. This work-in-progress describes several 
potential fault injectors and system monitors. A bus cycle stealer is demonstrated. This small hardware 
core can be programmed to wake up pe­riodically and execute a variable number bus transactions. The e.ect 
on performance (single node and 16 node systems) are reported. Categories and Subject Descriptors C.4 
[Performance of Systems]: fault tolerance reliabil­ity; C.3 [Computer Systems Organization]: Special-Purpose 
and Application-Based Systems Keywords FPGAs, Parallel Computing, Fault-Injection  GeneralTerms Experimentation, 
Measurement, Performance Copyright 2009 Association for Computing Machinery. ACM acknowl­edges that this 
contribution was authored or co-authored by an employee, contractor or af.liate of the U.S. Government. 
As such, the Government re­tains a nonexclusive, royalty-free right to publish or reproduce this article, 
or to allow others to do so, for Government purposes only. Resilience 09, June 9, 2009, Munich, Germany. 
Copyright 2009ACM 978-1-60558-593-2/09/06 ...$5.00. 1. INTRODUCTION As High Performance Computers reach 
petascale and be­yond, many problems are presenting themselves which had not been previously as prominent. 
Power consumption is increasingly becoming prohibitive and one approach to mit­igate it is through low-power 
accelerator processors like Los Alamos National Laboratory s Cell/Opteron hybrid, Road­runner. Roadrunner 
is currently both the fastest[3] and seventh greenest (energy-e.cient)[2] supercomputer. Hy­brid computing, 
however, presents many challenges in pro­grammability. The DARPA Exascale Report[4] identi.ed the four 
main barriers to reaching exascale as power, pro­grammability (through concurrency and locality), memory/ 
storage, and resilience. FPGAs present another accelerator­based hybrid architecture which also bene.ts 
from low power and will likely be a viable option for many application do­mains at petascale and beyond 
due to the tradeo.s between cost, power, customization, and performance. As researchers study fault tolerance 
and resilience-based software mechanisms to keep applications running in the face of failure, they are 
often presented with the challenge of how to test this software. Checkpointing techniques are already 
beginning to consume so much valuable compute time that simply preparing to recover from a failure is 
too costly[7]. At the same time, the reliability research community lacks a testbed for performing fault 
injection to simply test the methodologies that are developed. While high-end super­computers are failing 
often, smaller-scale testbeds are not due to the large component-count di.erences. The Recon.gurable 
Computing Cluster project is primar­ily interested in the feasibility of scaling a network of Plat­form 
FPGAs to a PetaFLOP. In this architecture, the FP-GAs are the central computing device of the node (not 
an accelerator to a microprocessor). Each FPGA node has a processor, bus, memory controller, network 
adapter, etc. con.gured into its programmable logic and runs a patched mainline Linux kernel. The cluster 
runs Open MPI and es­sentially any MPI application can be compiled and executed on this platform. Incidentally, 
the proposed architecture is also an ideal platform for resiliency issues. Since the hardware is very 
con.gurable, it is possible to introduce special-purpose hard- LA-UR 09-02916. This project was supported 
in part by the National Science Foundation under NSF Grant CNS 06-52468 (CRI). The opinions expressed 
are those of the authors and not necessarily those of the Foundation. ware cores that are designed to 
perturb or corrupt an exist­ing system in a controlled way. Similarly, it allows for the introduction 
of novel hardware monitors. This is important because, going forward, it is not clear what events or 
signals will indicate degraded performance or pending catastrophic failure. Speci.cally, we propose the 
use of FPGAs to develop a fault injection hardware testbed where various dials and probes can be con.gured 
to not only create failure in the hardware (dials) but also experiment with hardware-based failure identi.cation 
mechanisms (probes). It is still an open research question as to what probes and counters might be useful 
in hardware to a software layer to detect potential fail­ure. Through a recon.gurable approach, we can 
experiment with real applications running on real hardware and observe injected faults in an e.ort to 
identify the appropriate set of probes. Most importantly, experiments are reproducible: one does not 
have to rely on natural phenomenon and use very large systems to test new mitigation techniques. While 
much of the resilience research focuses on increas­ing the utilization of expensive compute resources, 
a growing subset of resilience deals with the notion of getting the cor­rect answer. As component counts 
continue to soar, more and more is data being corrupted. Partially this is due to cosmic radiation[9] 
which, in particular, increases with altitude, but also we are seeing common bugs as the cul­prit. A 
recent S3 outage at Amazon which brought the system down for eight hours was tracked back to single­bit 
corruption which occurred after MD5 checksums had been performed[1]. Data corruption like this is particu­larly 
disastrous when it comes in the form of silent data corruption[6]. That is, when data gets corrupted 
in a way that does not halt operation but causes the application to produce incorrect results; perhaps 
even only slightly erro­neous. NASA and other mission critical organizations have used triple-modular-redundancy 
(TMR) for years to miti­gate this. However, building three petascale supercomput­ers to use a voting 
mechanism to ensure accurate answers does not seem .nancially practical. The proposed testbed is a network 
of Platform FPGAs. That is, the basic compute node in the parallel computer is a highly con.gurable FPGA 
device. A typical con.guration might include processors, buses, and all the typical periph­erals found 
on a commodity main board, including the net­work interface. The basic FPGA node is advanced enough to 
support the stock Linux kernel (with some FPGA patches) running OpenMPI, so virtually any MPI application 
can be compiled for this system. Since most of the node s func­tionality is implemented in programmable 
logic, it is possi­ble to introduce arbitrary hardware components that arti­.cially perturb any component 
in the system in ways that range from subtle (introducing delays in a data path) to catastrophic (data 
corruption en masse). We call these di­als because their impact can be varied to approximate dif­ferent 
environments (high altitude, space, etc.) Similarly, it is possible to monitor the system without disturbing 
it and without manufacturing specially instrumented hard­ware. This avoids the Heisen-bug problem where 
software­based monitoring has the unintended consequence of chang­ing the behavior of the system. This 
is possible because the FPGA can be con.gured to have hardware core that oper­ates in parallel with the 
original system: sensing, but not using, any of the computing resources. The rest of this paper is organized 
as follows. In section 2, we describe the testbed hardware in more detail. Next we describe several prospective 
probes and dials in section 4. One probe has been implemented and some experimental results running an 
MPI program one and sixteen nodes is presented section 5. section 6 brie.y summarizes our imme­ diate 
future goals. 2. BACKGROUND 2.1 Recon.gurable Computing Cluster The Recon.gurable Computing Cluster 
(RCC) Project is investigating the idea of using a network of Platform FPGAs as a high-end, parallel 
computing system. FPGAs are highly con.gurable Integrated Circuit (IC) devices. Their predom­inant use 
is in embedded computing systems where they replace a large numbers of Large Scale Integrated (LSI) cir­cuits; 
however, they are increasingly appearing as compute accelerators to assist commodity microprocessors 
in high performance systems. The RCC Project also targets high performance computing but it uses a Platform 
FPGA is the primary compute device. (The adjective Platform denotes its role as a central compute device 
rather than a peripheral. Nothing physically distinguishes it from an ordinary FPGA device.) Non-critical, 
sequential code is still executed on an embedded processor core (integrated into the FPGA) but there 
are no separate microprocessors in the system. Spirit is a small-scale model of the RCC concept that 
has been constructed in the Recon.gurable Computing Sys­tems lab on the campus of The University of North 
Car­olina at Charlotte. Each node consists of a Xilinx Virtex-4 (V4FX60) device and 512 MB of DDR2 RAM. 
The V4FX60 has 56,880 programmable logic cells, 48 DSP blocks, on-chip RAM, 16 Multi-Gigabit Transceiver 
(MGT) blocks. Using the integrated transceivers, a custom low-latency, 64 Gbps direct connect network 
has been developed in house that has eight independent channels. These channels can be be bonded at the 
link layer which allows us to wire the nodes into a variety of topologies from a 64 node ring to a 4-ary 
3-cube. (A Gigabit Ethernet network also connects every node over two 48-port switches and carries administrative 
tra.c to support services such as ssh and and NFS.) The programming model is essentially a hybrid message­passing 
cluster. That is, the embedded processors run a lo­cally developed GNU/Linux distribution based on the 
stock Linux kernel and OpenMPI middleware. Some latency-sensi­tive MPI functions, such as MPI_Barrier 
have been directly implemented in hardware and others will migrate to hard­ware in the future. The system 
supports the conventional co-processor model with the processor delegating work to specialized units 
and then waiting for operation to com­plete. It also supports the computing-in-the-network and computing-in-the-.lesystem 
models where a packet of data is operated on in-.ight (either to the network or disk). It also supports 
the compute model where a processor s instruction set is extended to include application-speci.c instructions. 
The hypothesis of the RCC project is that a parallel collec­tion of Platform FPGAs will be more cost-e.ective 
 when scaled to hundreds-of-thousands of cores than commod­ity o.-the-shelf/open source computing solutions. 
This idea is not without controversy. FPGAs typically are an order­of-magnitude slower in clock frequency. 
The same arith­metic circuit typically require 4× more transistors than the equivalent direct CMOS implementation. 
Certainly, most programmers are not familiar with designing for FPGAs. Nonetheless, FPGAs have a number 
of advantages as well. The potential degree-of-parallelism is far greater than mass produced, general-purpose 
.xed-function CMOS designs, the potential for late specialization, and its slower clock fre­quency results 
in signi.cant power savings. Also, since the entire node is hosted on one integrated circuit thus there 
is a single, shared memory hierarchy which distinguishes it from other compute-accelerator approaches. 
Ultimately results from on-going experiments will help determine whether the advantages of the RCC approach 
will overcome the de.cien­cies and whether the net gain is su.cient to dramatically improve the scalability 
of the system. 2.2 Resiliancy While studying the scalability of the proposed network-of-FPGAs architecture 
was the genesis of the RCC project, the investigators realized that the infrastructure was also an excellent 
resource for studying the e.ects of jitter (or operating system noise) in large scale systems and natural 
phenomenon such as faults due to cosmic radiation. Because the system has the capability of introducing 
probes and dials in the programmable logic of the FPGA, this infrastructure can inexpensively introduce 
specialized units to measure and manipulate performance of or inject faults into individual nodes to 
learn about system performance. Moreover, the current infrastructure re.ects an inversion of performance: 
fast networks, ever-larger memory capacities, .at memory latencies, and (relatively) slow processor clock 
frequencies. Consequently, the small scale 64-node Spirit foreshadows the very large scale parallel machines 
of the future. Performance Degradation. Jitter is the accumulation of multiple periodic and aperi­odic 
interruptions to the application program. The interrup­tions are of varying duration. The periodic interruptions 
are due to routine run-time system operations. The aperiodic interrupts are due to a number of environmental 
entropy cir­cumstances (such as random packets which may appear on the network interface some may be 
destined for the local node but most are not). These slight (or sometimes lengthy) delays to a single 
process, are believed to cause enormous systemic problems as the number of processors and tasks increase 
to the scale of tens-of-or hundreds-of-thousands of processors [8]. Fault Injection. As mentioned in 
the introduction, it is expected that the push towards exascale computing will necessarily lead to less 
robust hardware. To compensate, it is important to develop software mechanisms that manage faults. However, 
to ex­plore novel techniques, researchers need platforms to test their ideas. The idea of using the largest 
systems available today and waiting for natural phenomenon to occur is unac­ceptable. It wastes expensive 
resources and it is not repro­ducible. While it is feasible to create faults in software, it requires 
an application to be executing in parallel that may have subtle e.ects on the original application s 
behavior.  3. POTENTIALPROBES&#38;DIALS To provide some context of what is possible with a Re­con.gurable 
Computing Cluster, several potential hardware cores that could be integrated with an MPI system are de­scribed 
here. The results included in this work-in-progress report are based on cores which have been implemented; 
they are described in the next section. 3.1 Probes Modern processors are heavily instrumented to help 
ap­plication developers code for performance and APIs exist to gather run-time information [5]. Application 
pro.ling also provides information about the performance of an applica­tion. Finally, a typical operating 
system, such as Linux, will gather statistics about certain events (network pack­ets received, for example). 
However, all of these techniques inadequate if one wants to predict future performance prob­lems or faults. 
Either the data collected is too general (all network packets are aggregated) or the data are intended 
to be analyzed post-mortem after the application ends. The probes described here have two components. 
The front-end is a hardware core that is collecting targeted data from the system. The back-end is a 
collection of hardware cores designed to analyze the data at run-time. We do not know a priori which 
of these, if any, will be useful. However, with platform described here, we can introduce the cores into 
running systems and judge their e.ectiveness. Below we consider several front-ends and then potential 
back-end hardware cores. TracePort. The PowerPC 405 (which is the embedded processor fre­quently used 
in Spirit) has a trace port. This trace port is intended to be hooked up to a high-speed logic analyzer 
to help developers debugging real-time applications. The trace port s encoded output stream provides 
cycle-accurate infor­mation about events inside the processor (branches taken, exceptions, cache misses, 
and so on). It is an obvious .rst choice for a probe since a hardware core could decode the stream and 
provide a back-end core with speci.c event in­formation. A simple one-bit signal that pulses every time 
a branch is taken. Bus Sniffer. Unless one purchased specialized equipment (and installed in on every 
node of a cluster), it would be very di.cult to monitor transactions on the system and peripheral buses 
of a modern processor. On a Platform FPGA, these buses are implemented in the programmable logic. Thus 
is it is straight forward to add a hardware core that sits on a bus and gathers data. This might be a 
stream of memory ad­dresses, the occurrence of burst transaction, or a reference to I/O space. Network 
Analyzer. The equivalent of the network interface card is also im­plemented as a hardware core on the 
Platform FPGA. As packets travel to and from the network, a probe front-end could be used to gather more 
targeted information. For ex­ample, rather than simply count the number of packets, a probe could track 
packets by destination. Memory Controller. Using ECC memory will automatically detect and correct a bit 
error when the data is read from o.-chip SDRAM. The memory controller (which is implemented on the FPGA) 
performs this task before moving the data onto the bus. A front-end probe could inserted to collect the 
occurrences of these corrections. (A back-end probe could then be used to track the rate; a high rate 
might suggest a sudden environ­mental change and a potential hard fault.) DiskPerformance. Disk drives 
already failure prediction via S.M.A.R.T. in­terface. However the focus is on Reliability-Availability-Ser­viceability 
(RAS) rather than performance. If a modern disk drive detects a bad sector, it silently substitutes another 
sec­tor from a pool of extra blocks and continues. This adds an­other degree of variability to transfer 
rate which may prove useful to system-wide monitoring. Once data is being collected, a back-end of a 
probe is required to continuously analyze the data at run-time. It is envisioned that one or more back-ends 
might be connected to a single front-end to capture di.erent summary data. Counts and Rates. The simplest 
back-end would be to simply count the oc­currence of an event. Combined with a software reset and threshold 
register, it could be used to trigger a defensive technique or encourage a checkpoint. By introducing 
time, the counts can be converted to a rate which may be more valuable than an absolute count. Examples 
include inter­rupts per second, nearest neighbor messages per second, and branches per second. SlidingWindow. 
Another dimension can be added by managing a window of periodic rates. Using this data, well-known algorithms 
can used to automatically detect a sudden shift. If multi­ple windows of di.erent sizes are used to log 
data say data over the last 60s, 3600s, and 86,400s then gradual deviations from average behavior can 
be detected. This is information might indicate to the user that the time-to­result will be longer than 
expected which might cause a user to stop a job and ask a di.erent question. Entropy. Several probe front-ends 
might be connected to a back­end core that computes entropy. Gathering data from well­functioning systems, 
it may be possible to determine the average amount of randomness one might expect to see. If a system 
deviates from this, it might be an indication that system is failing or about to fail. Arti.cial Neural 
Networks. A .nal probe back-end that might prove useful would be to create an arti.cial neural network 
hardware core. The neural network could be trained on well-functioning systems and may be able to detect 
atypical behavior that indicates a failing or about to fail system.  3.2 Performance Dials Most commercial 
interest is in RAS. However for a rela­tively small, but important, groups RAS is undesirable. For example, 
the ability to extrapolate performance is crucial to computational scientists because they use execution 
time to size their experiments. In other circumstances, the value of the result depends on the time it 
takes to deliver. One might choose to abort a run and ask a simpler question if the value of the results 
diminishes as the wall clock time increases. Unfortunately, RAS technology, which corrects faults by 
extending time-to-solution, run counter to these users goals. In this section we introduce a number of 
hardware cores that e.ect hardware performance with and without corrupt­ing the ultimate solution. These 
are called dials because they allow the degree of interference to be varied. This gives resiliency researchers 
investigating new techniques the abil­ity to ask what-if questions and stage di.erent scenarios for debugging. 
For example, a technique may work well un­der current conditions and assumptions about future fault rates 
but what if the occurrence of single-event upsets is twice as common as predicted for the next generation 
of sil­icon? Having a hardware dial to arti.cially inject faults in a controlled and reproducible rate 
is essential to answer these questions. Below are few examples of dials that could be introduced. Cycle 
Stealer. The system bus is a complex, asynchronous, shared com­munication media. The processor uses the 
bus to move cache lines to and from o.-chip (main) memory and its cache. This is relatively rare compared 
to reading and writing data to its cache. Moreover, modern bus protocols are designed to pre­vent one 
bus master from hogging the bus. So unless a bus master designed to interfere with the system takes a 
large percentage of the bus bandwidth, it won t make a noticeable impact on the processor-memory bandwidth. 
However and this is extremely important for distributed systems a cycle stealer will e.ect the latency 
of individual operations. Thus, have a hardware core that periodically (based on some preset frequency) 
wakes up and continuously negotiates for the bus has enormous potential. These disruptions might not 
e.ect the individual node in terms of time-to-solution but the systemic e.ect (from asynchronicity due 
to the la­tency incurred) might be signi.cant. Bit-Flipper. Cosmic radiation and other natural phenomenon 
cause errors in various subsystems. To simulate this behavior a bit .ipper hardware core could be attached 
at di.erent places to inject faults. If designed to randomly change bits in main memory, then it would 
simulate multiple bit errors (that ECC memory cannot correct). If it was designed to change bits in the 
memory controller, then it would simulate single bit errors. It would also be possible to insert the 
bit .ipper at other location in the system (disk, network, etc.) Many other dials are possible. Also 
as faults in real sys­tems are statistically characterized, it is possible to incor­porate more sophisticated 
dials that statistically mimic real systems.   4. PROBES&#38;DIALSIN DEVELOPMENT We have begun to implement 
several of the probes and dials mentioned in the previous section. The hardware that has been designed 
thus far is described below. 4.1 PowerPCTraceProbe Under normal circumstances, PowerPC, the Hard Intellec­tual 
property processor generates the read and write com­mands on the bus. In such a scenario, all other cores 
in the system (e.g.: UART, BRAM, specialized compute kernels) act as slaves on the bus, incapable of 
initiating bus transfers. The capability to initiate bus transfers is central to our idea of using Soft 
Intellectual Property FPGA resources, since it can be used to simulate out of control situations in the 
system or in speci.c target locations without intervention by the CPU. The only intervention that may 
be required is during initialization where a software enable activates the disturbance units. Even this 
can be done away with, by us­ing an external pin on the device. We have made extensive use of counters 
and Linear Feedback Shift Registers in an at­tempt to randomize the disturbance patterns. Although this 
will provide only a pseudo-random control, the complexity of the randomizer units can be increased if 
required. 4.2 Cycle Stealer To quantify the e.ect of devices demanding bus cycles, a cycle stealer was 
designed as a hardware core for the pro­grammable logic resources of the FPGA. The cycle stealer dial 
was built to control the frequency (the time between two consecutive disturbance causing events) and 
duration of each disturbance (in the form of bus trans­actions). It mainly consists of a free running 
LFSR (Linear Feedback Shift Register) and a mean delay setting. LFSR is a simple Pseudo-Randomn Number 
Generator. The mean delay input controls the time between a completed bus trans­action and start of the 
next bus transaction. The LFSR gives an element of randomness which can .ne tuned using another control 
that masks bits generated by the LFSR. The mean delay input is compared to the sampled and masked LFSR 
output. When the comparator .nds a match, it trig­gers a state machine which demands control of the system 
bus. This state machine is capable of reading from or writ­ing to any memory mapped on the system. Thus 
this bus cycle stealer dial can be controlled to cause a wide range of long and short duration interruptions 
on the system bus. A linux device driver helps control parameters such as fre­quency of interruptions 
and number of bytes to be read or written. However due to the nature of bus reads, bus writes and ar­bitration 
statregies followed by the processor, aspects such Figure 2: Block Diagram of the Bit Flipper unit as 
bus transaction setup time, bus arbitration time, busy time and memory access times are beyond the scope 
of con­trol. Also to maintain the genuineness of the test, the bus is only stressed by the bus stealer, 
not causing intentional defects in the system. For this reason, the cycle stealer does not issue another 
request on the bus until the last request was achnowledged and ful.lled. The bus cycle stealer provides 
us with a unique opportu­nity to quantify disturbances that cause an increase in the execution times 
of programs. This dial can be implemented into single node systems or into one node in multiple-node 
systems or into multiple nodes in multiple-node systems as recon.gurable FPGA core. 4.3 SDRAM Bit-Flipper 
In an attempt to simulate a single bit corruption in a sys­tem causing a crash of an operating system 
and/or future resiliency tests, we have concieved the concept of a bit .ip­per , which when enabled will 
generate a random address within a speci.c range of addresses and then corrupt one random bit at the 
targetted memory location. The design involves a counter which counts upto a preset value, and then samples 
an output from a Linear Feedback Shift Register (LFSR). The output from the LFSR can be used to target 
a speci.c location in a range of addresses, i.e. by masking some of the generated bits and o.setting 
it with base address of the target memory location. For e.g.: If we wish to target any location in a 
512MB DDR2 memory with a base address of 0x00000000, we know that the high address is 0x1FFFFFFF, we 
propose to mask the LFSR output with the high address to generate some ran­dom memory location, which 
we can add with the base lo­cation address and generate a victim address location. A read-corrupt-write 
operation by a bus master module will be able to convincingly simulate a bit .ipping simulation.  5. 
EXPERIMENTAL RESULTS Using the existing infrastructure and the cycle stealer dial that has been developed, 
we wanted to perform a pre­liminary investigation. The objective of this investigation is two-fold. First, 
we wanted to determine how disturbing the performance of one node would e.ect the system over all. And, 
second, whether it is possible to use information from the system to detect the failing node sooner than 
the failing nodes recognizes it. At this stage of the project, the broad aim is to under­stand what hardware 
perturbations have the most impact and what hardware probes are best able to detect interfer­ence. Since 
we have not developed all of the proposed probes and there is relatively little previous work, these 
experiments are exploratory in nature. 5.1 Set-Up All of the experiments were carried out on Spirit a 
network of 64 Platform FPGAs fabricated and housed in the Recon.gurable Computing Systems lab in The 
University of North Carolina at Charlotte. A sub-cluster wired as 2­ary 4-cube (a 4 × 4 torus) with single 
4-Gbps channels was used for the parallel runs, illustrated in Figure 3a. Each node consists of a Xilinx 
ML-410 developer board (Xilinx Virtex-4 FX 60 FPGA and 512 MB of DDR2 RAM) with a disk drive and the 
custom network (Figure 3b). A typical node con.guration for an MPI-based system is shown in Figure 3(c). 
 However, for these experiments a simpli.ed test harness was created. This is illustrated in Figure 4. 
The speci.c hardware con.gurations are described below. PowerPC 405 It operates at 300 MHz and runs the 
Linux compilation and the test program. The test program used for the cycle stealer experiment was one 
of the NAS Parallel Benchmarks (Integer Sort). This was chosen for its convenient execution time with 
single node operation. BRAM is RAM used by the system. DDR2 stores the Linux code. CF or Compact Flash 
stores the con.guration information used for FPGA recon.guration. DDR is used as extra memory space. 
In the cycle stealer experiment, it has been used to cause non-fatal inter­ruptions to the system. UART 
is a serial port connected to the console. IIC is used to determine network IP address. MGT protectors 
are used as wrappers around Multi-Giga­bit Transceivers (MGT). MGTs have been implemented in a high speed 
network and have been explained in [10],[11]. Future experiments propose to use this net­work. Disturbance 
Unit is placed on the PLB, and is set at the start. It can simulate any random interruption or dis­aster 
in the system. Each node of the cluster was con.gured have a simple processor/system bus/memory (PowerPC 
405/64-bit PLB-DDR2 controller) structure with the network interface on the system bus. The operating 
system used was a custom, GNU/Linux distribution, Ezra. This includes the main line Linux kernel (2.6.24) 
patched for the Virtex-4 FX and the GNU C library (glibc-2.9). The benchmarks were compiled with the 
GNU Compiler Collection (gcc 4.3.3) and OpenMPI (1.2.5) was used for the message-passing experiments. 
 5.2 Bus Threshold Experiment The cycle stealer bus module described in the previous section was implemented 
for the Processor Local Bus (PLB). It includes to system-programmable parameters, µfreq and µdur. The 
former is the average number of clock cycles before a cycle stealer event occurs and the latter is the 
av­erage duration of an event. To simplify the implementa­tion, both averages are computed in hardware 
by adding the programmed value and a uniformly distributed pseudo­random number over the range of -2n 
to 2n - 1. For these experiments, n was arbitrarily set so that 2n was approxi­mately half of µ. A 32-bit 
Linear Feedback Shift Register (LFSR) was used to generate pseudo-random numbers to vary the frequency. 
The issue we wanted to quantify was how much bus inter­ference on one node was needed to have a signi.cant 
impact on the cluster as a whole. Intuitively, one can imagine that a single node running a particular 
benchmark will have cer­tain amount of excess bus bandwidth and could absorb the impact of a cycle stealer 
up to the point. So .rst we char­acterize this by running the Integer Sort application on a single node. 
We .xed the µdur and varied µfreq. Two ex­periments were conducted. The .rst simply measured the slow 
of one node due to the disturbance. The second exper­iment disturbed one node but ran the test on 16 
nodes. The execution times are shown in Figure 5. To observe the e.ects of duration versus frequency, 
a sim­ilar experiment was run where µfreq was .xed and the dura­tion, µdur, was varied. These results 
are shown in Figure 6. Although not as dramatic as we hoped, there is one strong conclusion in the data. 
Disturbing one node will result is a slow down for that node and that slow down is re.ected in the whole 
system. Also, it appears that duration, rather than frequency, has a larger impact on performance. This 
may be due to the sophisticated bus protocol which is de­signed to prevent one bus master from interfering 
with the performance of other masters.  6. CONCLUSION AND FUTUREWORK As high-end computer systems research 
pushes towards exascale computing, resilience will become ever more criti­cal. There are many open questions 
that range from fault detection and fault prediction to recovery mechanisms. Crit­ical to all of these 
questions is a testbed that provides con­trolled fault injection. In this paper, we have described a 
set of probes and dials that can be integrated into an FPGA­based message-passing parallel computer. 
The dials can be used to perturb performance or inject faults. The probes are hardware cores that instrument 
the system with aim of de­tecting or predicting system-wide conditions such as a faults or performance 
degradation. Although this is a work-in-progress report, we do have some preliminary results to present. 
First, by perturbing the shared system bus by periodically performing small to large transactions does 
impact performance. One would ex­pect this for a shared resource, such as a bus. However, we were surprised 
by how well the bus was able to tolerate fre­quent short interruptions and infrequent long interruptions. 
For the NAS Parallel Benchmark IS (Integer Sort), it was observed that interfering with one node s bus 
did degrade  (a) (b) (c) Figure 3: (a) 16-node network (b) top-level block diagram of one Platform FPGA 
node (c) processor/bus con.guration of one Platform FPGA (a) (b) Figure 5: percent increase in execution 
time of NAS Parallel Benchmark IS as µfreq increases; µdur is .xed at 0.8µs, 328µs, and 84ms for (a) 
one node and (b) 16 nodes (a) (b) Figure 6: percent increase in execution time of NAS Parallel Benchmark 
IS as µdur increases; (a) one node and (b) 16 nodes a sixteen processor system; although the e.ect was 
not as pronounced as one might have expected. This may be do to the nature of the application. In the 
near future, we expect to have a probe ready. The PowerPC core in our node has a Trace Port which provides 
a great deal of information about branches and other events inside of the processor. Likewise, we can 
observe bus trans­actions and network activity. The goal would be to try to relate these data to eminent 
failure or detect faults. The next dial we intend to develop will inject hard faults into the system 
(.ip bits in main memory) to allow others to test rollback algorithms.  7. REFERENCES [1] Amazon s3 
outage. [2] The green 500 list 2008. [3] Top 500 supercomputing sites 2008. [4] ExaScale comoputing study: 
Technology challenges in achieving exascale systems. Technical report, Sept. 2008. [5] S. Browne, J. 
Dongarra, N. Garner, G. Ho, and P. Mucci. A portable programming interface for performance evaluation 
on modern processors. The International Journal of High Performance Computing Applications, 14:189 204, 
2000. [6] C. Constantinescu, I. Parulkar, R. Harper, and A.. S. Michalak. Silent data corruption a T 
myth or reality? Dependable Systems and Networks With FTCS and DCC, 2008. DSN 2008. IEEE International 
Conference on, pages 108 109, June 2008. [7] J. T. Daly. Evaluating the performance of a checkpointing 
application given the number and types of interrupts. In Proceedings of the First Workshop on High Performance 
Computing Reliability Issues, San Francisco, Feb. [8] K. B. Ferreira, P. Bridges, and R. Brightwell. 
Characterizing application sensitivity to os interference using kernel-level noise injection. In SC 08: 
Proceedings of the 2008 ACM/IEEE conference on Supercomputing, pages 1 12, Piscataway, NJ, USA, 2008. 
IEEE Press. [9] S. Michalak, K. Harris, N. Hengartner, B. Takala, and S. Wender. Predicting the number 
of fatal soft errors in los alamos national laboratory s asc q supercomputer. Device and Materials Reliability, 
IEEE Transactions on, 5(3):329 335, Sept. 2005. [10] R. Sass, et al. Recon.gurable computing cluster 
RCC project: Investigating the feasibility of FPGA-based petascale computing. In FCCM 07: Proceedings 
of the 13th Annual IEEE Symposium on Field-Programmable Custom Computing Machines (FCCM 07), pages 127 
138, Washington, DC, USA, 2007. IEEE Computer Society. [11] A. G. Schmidt, W. V. Kritikos, R. R. Sharma, 
and R. Sass. Airen: A novel integration of on-chip and o.-chip FPGA networks. In To Appear in: FCCM 09: 
IEEE Symposium on Field-Programmable Custom Computing Machines, 2009.  
			