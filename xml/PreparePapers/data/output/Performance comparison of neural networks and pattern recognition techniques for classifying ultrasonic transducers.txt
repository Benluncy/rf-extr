
 PerformanceComparisonofNeural Networksand Pattern RecognitionTechniquesfor Classifying Ultrasonic Transducers 
 M.S. Obaidatand D.S. Abu-Saymeh Computer Engineering Research Laboratory Department of Electrical &#38; 
Computer Engineering University of Missouri-Columbia  Abstract Neural networksandpatternrecognition 
techniques have been effectively used in characterizingand classifying patterns.Inthispaper,we focusontheperformanceofboth 
techniques in classifying ultrasonic transducers.The work was aimed at comparing the performanceof both 
techniques for : (1) preclustering of transducers into classes; (2)~ognizing transducers. l hecharacterizationalgorithms 
used are based on neural network and pattern recognition techniques. The competitive learning technique 
provides poor results as compared to the K-Means for prtxlustering. While for recognition, it is found 
that artificial neural network techniques provide tkr better classifkation nsults as compared to the 
pattern remgnition techniques. A multilayerbackpmpagationneuralnetworkis developed for characterizing 
the transducers which provides a misclassifwation rate of 6%. Two other multilayer neural networks, sum-of-products 
and a newly devised neural network called hybrid sum-of-products have a misclassificationrateof 10%and7%,respectively.Thebest 
patternrecognition technique for this application is found to be the perception which provides a misclassification 
rate of 23%. The worst patternrecognition technique is found to be the Bayes theorem method which provides 
a misclassification rate of 54%. Index Terms Performance Evaluation, Neural Networks, Pattern Recognition, 
Ultrasonic Transducers, Classification, CharacterizationTechniques. Permission to copy without fee all 
or part of this material is granted provided that tha oopies are not made or distributed for direct commercial 
advantage, the ACM copyright notica and the title of the publication and ite date appear, and notice 
is givan that cop~ngiabypermissionoftheAssociationfor Computing Machinery.Tc copyotherwise,cr to republish,requiresa 
fee and/orspecificpermieeion. 01992 AcM o.aglgl.sOz-x/gz~ z/lzsd...$ 1.So  I. Introduction Themodern 
technical advances in robotics ad artificial intelligence systemsinchI&#38; the capability of these systems 
to recognize and classify objects. In some cases, the recognition process could be easily accomplished 
by means of tmditional pattern recognition techniques, but in other cases, the task nx@res mom complex 
mcognizm. The process of Ultrasonic Transducerrecognition was not easily accomplished by traditional 
pattern recognition techniques Furthermore, it is important to be able to compare the various recognition 
techniques. Ultrasonic transduce am being used in a wide spectrumof applications including acoustic imaging, 
animal science, acoustic microscopy, nondestructiveevaluation and medical imaging. Withtheincreaseintheiiusage,a 
ned arisesforprecisemethodsto temevaluateandcharwterize thesetransducers.The momwellknownthe opemting 
characteristicswill aW themoreaccuratethesignalanalysisbe[1].Themanufacturersforeachtransducer chamcteristicsmust 
be verified before installing it into the system. It is not surprisingto find transduuxs thatdo notmeet 
expectations. The transducersmust be tested periodically to check for any performance degradation, Furthermore, 
the measured petiorrnance parameterscan be used to predicl the lifetime of the transducers. Various methods 
for measuring the charWeristkS of ultrasonic transducersare available, such as the methods presented 
by Obaidat et al [1], Hotchkiss et al [2], Stump [3], etc.. Obaidat et al [1] presented an automated 
system for characterizing ultrasonic transduce using traditional Pattern Recognition techniques. Moreover, 
they compamd the performanceof traditionalpatternrecognition techniqws for the characterizah of transduce. 
llE system, shown in Figure 1, consists mainly of a workstdon computer, a receivedpdser with gated peak 
detector, various monitoring devices that include a spectrum analyzer and a digital 3-D Positioning Mechanism 
lB- I  ~Transducer J I  Microprocessor-treed Osci..Uoscope spectrum Analyzer Controller * *  3 Host 
Computer F~re 1: A block diagram of the system hadware. oscikscope, a microcomputer-based 3D-positioning 
wntroller, and an A/D converter. The system was used in our work to measure parameters such as the focal 
length, dCpthOf field near tield Sh3f)t!,peak ~lWXXy response, center 6 equemey, bandwidth,peak power, 
inseriionloss, impulselengthandactiveelementdiameter. Hotchkisset al [2] introducd variousmethodsto evaluate 
the performancem-of transducersand cumpsre them to the manufacturers specifkations. Fdmnore, Sproatet 
al [4] introduceda device for chmwterizing ultrasonictransducersin the f~ld. LaCasseet al [5]used@ternclassificationto 
inspectcastingsforwalls thatarekm thin. Patternrecognition techniques are well known for their ~plic+ltion 
in c~lfication and CkUWkTiZitiOfl [6]. Among these techniques are the K-Means [7], Minimum Distance [8], 
the Cosine measure [9], perception [10], and Bay# rule [11]. Ilese methods am based either on minimizing 
individual misclassifkation error, total cluster misclassiftiion am, or global mean squared error, Among 
the successful applications of neural networks are recognition, cktssifiion and identifiion applications 
[12-20]. Obaidat el al [12] presented a methd to identify computer users by presenting the time intervals 
between suwssive keystrokes to a neml netwti. lhe keystrokes are created by users while typing a known 
~UCXKY3 Of characters. h tiir W~ tky diSCUSSedthl% types of networkrca mukilayer feedfcmvardnetworktrained 
using the backpqagation algorithm, a sunmf-pmducts network trained with a modifkxtion of backpropa@on 
algorithm, and a new hybrid amhitecture that combinca the two. Fukushima [13] proposal a Neural Network 
for visual patterwrecognition. rile network consisted of neuronÂ­like cells of the analog type with a 
hiemrchical multilayered Wucture consisting of a cascade of many layers of d.is. The network has forward 
connections which manage the function of pattern recognition and backward cmwdims which manage the function 
of selective attention, pattean segmentation, and associative recall. Kohonem [14] described a Neural 
Phonetic Typewriter. lle Typewriteris based on a neuml nehvork processor for the recognition of phonetic 
units of speech which transcribesdictation using an unlimited vocabulary. Inthispaperwe presentneuralnetworkandpattern 
recognition methodologies for the characterization of ultrasonic transducers.Various techniquesare investigated, 
evaluated,comparedand analyzed.SectionII describesthe NeuralNetworkand pattern Recognitionalgorithmsused 
to classify the transducers. Section III presents the results of the classification and discusses, anatyzes, 
and compares the various algorithms. Finally, SectionlV containsconcluding remarks. 11. Feature Extraction 
and Classification Techniques - / Theprocessofrecognitionof ultrasonictransducers aftercollecting thecharacterizingparameterscanbedivided 
into two steps. Fmg the extraction of features. SeCon&#38;the classification of transducersusing these 
features. Feature extraction can be viewed as defining a group of transducers (which may contain only 
one transducer)by uniqueparametersor featuresthatdistinguish them from the rest of the groups. A clustering 
algorithm such as the traditional K-Means algorithm or a neural network with competitive learning algorithm 
wouid divide the transducersinto clusters. Moreover, transducerswithin a cluster would be similar in 
their characteristics and different from the characteristics of transducers in other clusters.Theeffectiveness 
offhealgorithmcanbemeasured by evaluating the mean distance between the means of different clusters and 
the mean deviation from the means within the same cluster. An effective algorithm will produce clusters 
with large mean distance and small mean deviation. The mean deviation is computed as : (1) where ~c is 
the mean of the cluster to which Ai is classifkd and N is the number of transduce. Both the K-Means and 
the competitive learning algorithms are investigated in this work. The K-Means algorithm was fmt proposed 
by MacQueen [7]. The algorithm is based on minimizing the sum of the squared distances for all the points 
in a class. The algorithm starts by selecting K-transducers as the centers of rxwh class. Then, the rest 
of the transducers are divided into these classes by plwing each transducer in the class with minimum 
distance. The mean for each class is recomputed and the transducers are redistributed into the chases 
accdlng to the new means. The algorithm is repeated until no change is found in the clarification of 
all transducers. A neural network in general consists of several layers of neuron-like computing units. 
Figure 2 shows the organization of the backpropagation and competitive learning multilayer neural networks 
used in this work. Each neuron computes the weighted sum of its inputs and accordingly generates an output. 
The weights on the inputs are modified in the karning process to adapt to the required mapping between 
the input and the output of the network. The training can be either supervised or unsupervised. In supervised 
training the netwok is presented with a pair of inputandoutputpatternsand the Ieamingalgorithmwould direct 
the change of weights on the inputs of each unit to minimize the global mapping error. Unlike the supervised 
training, the unsuptmised training requires no output pattern. Instead, the training is based on competiticm 
between the output units. Only the winner of the competition generates an output and the change of weights 
is directed toward minimizing the errorin the output of the winner. Competitive learning algorithm is 
considered as an unsupervised training algorithm in which the units in a given layer am grouped into 
clusters.Units within a cluster have inhibitive connections and the unit with the largest weighted sum 
achieves its maximum value while the nxt of the units within a cluster are pushed toward their minimum 
value. Only the units that win the competition undergo the process of learning by modifying the weights 
on their inputs. For a stimulus k, the change in the weight of the link between units i and j is given 
by: where uctive~ is equal to 1 if unit j in the lower layer is active in response to stimulus k, and 
nactive~is the number of active units in the pattern of stimulus k, ande isthe learning rate. Other algorithms 
are investigated in this paper that require supervised training such as the backpropagation, sum-of-products(SOP) 
and hybridsum-of-products (HSOP) neural network and traditional minimum distance algorithms. The means 
of the clusters genexated by the K-Means algorithm are used as basis for training these algorithms. Many 
pattern recognition teehniquea require trainingdata such as the minimum distance, cosine measure, pmeptron 
and bayes rule. l%eminimum distance classifii a transducers~. as being of class Cj if: qecj e pi-tjl 
s pi-q for all k (3) The cosine measure is similar to the minimum distance measure. Instead of choosing 
the class with minimum distancethecosinemeawuechoosestheclasses with minimum angle which can be computed 
by using : OutputPatterns Internaf Representation Units InputPatterns Fipre 2 Topologyof the multilayer 
backpropagation neuml network paradigm used in this work. (4)   -%&#38;ii Thepercqtm isconsidesedas 
the basisfrom whichneml neworksevolved. In ti pe.m@mn atgorithm a weight matrix is developed 71w weight 
matrix is continuously modiilcd such that if it is muttiplkdby a set Ofparametersfmacertain tranaduccritwillyiddavector 
in which the largest eksnent is the demerit coneaponding totheclass towhkhthetransducer belongs. Abac@rqagationnetworkistrainedbyjm5senting 
eachpattexnwiththeoutputasso@ed withthatin@.After cyctinganumb oftimesthroughthepatteans,thenetwork Woutdbe 
abkbgencmte theaame output when it ia PmJ@xt withttEinput. l%e trainingof backpropagationcan be divided 
into twosteps, the fawardppagation and the ~*. -g thefofwaIdpmP@m, theoutput of eachunitiscalculatedbyevduadngtheweightedsumof 
its inputa and passing the result through a sigmoidat fimction: where~ is the sigmoidal fu&#38;ion which 
can be defii w (6) Ikbackward propa@ionis@yuad stage duringthetrainingprocesa. fh eoutput-is~ andtheaTor 
is bUickpfopagatedhghklaYadti networkTheoutputemoris evatuated88: ~, (p,} qpvl) ( n where 1, isthetarget 
Vatuc for tit j, 0/is theoutputof unit j, O,is thCOUtpUtOf tit i feedingh Unitj,SndOl OU@Ut~ is the wcjght 
of the link between units i andj. For intend unita the 8 is back propagated using : (8) Then the weights 
on k inputi of =h unit is modifkd accordingto the formula: C)wput Layer Sum4M40ducIs Layer Ihddcn Layw 
Back PropagationLayer LnpulLayu work. F~e 3 Topology of the hybrid summf-products neural network paradigm 
used in this (9) (12) Amv = e~p, Whexecia theleamingrate. where 8k fortheoutputunitsis definedas Thesumd-products(SOP)networkis 
veryclosely (13) 8#*-oJl@tJ  relatedtotheback pmpagadonnetwofk.In facttheoldy differenceis theintroductionof 
ndincan tyteamsin the andtheenur can be backpfop@edmhg: neufontransfexfunctkm.m~ty isneceswyin (14) 
8,= ~at~ffj 4-) manyeasesfortheneuralnetworktobeableto implement ( * Y ndinear mappings. In a sum+xoducts 
network the output of a The hybrid sum+woducts (HSOP)isa twolayes neuIuniadefiIwdas: newalnetwork. l%efifstlaycris 
identicaltoa q ~(q (lo) ~xkyeLdti-titi@mamÂ­Of-productsstlWtufe Ofthe HSOP layer.F@ure3 slmws h when netwti. 
Thestructum ofthisnetwti is bsaedm minimizingthe calculationsby havinga kkpropagatien (11) &#38; ~ @~J 
 layer which mqdres fewer calculationsthsn a sum*fÂ­pfoducts.Mesnwhile, to pmerve the nonEnwity* a - Tlms, 
whenmodiijingtheweights,bothOi andOj should sum+moducta layer is used. Ilms this slructum is a be consideredandthechsngein 
theweightsis givenas compmmise betwea speed and nonlinearity.  Mean Distance Between Classes LII + K-MuM 
 Figure 4: The mean distance between the classes versus the K-Meansand theCompetitiveLearningalgorithms. 
III. Results The described system is used to characterize and classify 83 different transduce fmm variousmanufacturers. 
AH parametersare normalized to the range [0-1] to assure equal initial weighing of all parameters.First 
the K-Means and the Competitive Learning are used to divide the transducersinto classes. The performanceof 
the algorithms are studied for various number of classes. Since Competitive Learning axepts only digital 
inputs, the input data is digitized using l-bit, 2-bit and 4-bit digitization schemes.Them wasn t a significant 
improvement between the various digitization schemes. The competitive learning results were unstable, 
and varied from one experiment to another depending on the initial random weights and the order of training. 
However, the K-Means showed a steady output during alt trials. Figure 4 shows the mean distance between 
the classes versus the number of classes for both the K-Means and the Competitive Learning (CL) with 
4-bit digitized input. Clearly, the K-Means is much better than the competitive learning due to the higher 
mean distance between classes (clusters). Figure 5, also supports the K-Means algorithm showing a lower 
mean deviation within classes for the K-Means algorithm. To validate that the mean distance and the mean 
deviation can distinguish between these algorithms the means of the classea obtained ~ 4-bit CL the number 
of classes for the classification generated by from both algorithms are used to train a backpropa@on 
neuralnetwork. hen all transducersareclassified using the trained backpropagation network F@re 6 shows 
that the misclassification percentagethatre.mdtsffom thisprocessis lowerfortheb@cpropagationneuralnetworkthatis 
trained using the K-Means Classes as compared to that trained using the 4-bit Competitive Learning (CL) 
technique. A class size of 13 is chosen to train the rest of the pattern recognition and neuml network 
algorithms. This choice is based on selecting a number of classes that truly identify the physical characteristics 
of the transducers in each class, and assure that no classes contain a single transducerunless it is 
completely unique. Furthermore,the misclassification rate for that number of classes has to be low. After 
nmning the rest of the algorithms using this class size, the misclassifkation percentage for each algorithm 
is computed and is shown in Figure 7. The backpropagation paradigm with a misclassification rate of 6%is 
found to be the best classification algorithm followed by the hybrid sum-of-prcxhda and the original 
sum+f-pmducts algorithms with 7% and 10% misclassifbtion rates, respectively. The pexceptronalgorithmwithamisclassification 
rateof23%is considcmd a compromise between pattern recognition and neural network techniques. This shows 
a great success for neuralne4workalgorithmsforclassifying thetmnsducers.In  Mean Deviation within Classes 
 9 60 1 1 , TOOUM 1 1 I Uumbu 1 1 Uuwlswlr Of Cbus 1 1 1 1 1 u + K-Maaw * 4-bit u F&#38;we 5 ?he mean 
devistion within the CISSSCSversusthe number of classesfor the classiicstion generatedby the K-Means 
snd the Competitive Learning algorithms,  Misclassification Percentage -~ m 60?OD n nna?m ; b~ Of =: 
A K-N@as_ 4-bit CL Figure 6: Misclsssificstionpementageversusthenumberof clssses usingthebiwkprupagstionneuralnetwork 
trainedby the dsta obtained from the K-Mesns and Competitive Learning algorithms. Misclassification 
Percentage M ~wn so? . m- - Jim? -as Classification A~orithm Figure 7: he missclas.sifkation percentage 
for the Bdcpropagation, Pereeptron, Minimum Distanct Cosine Measure and the Bayes classifiers. trainingthe 
neuralnetworksthe initial learningmte and momentumParatneten were set to 0.5 and decreased during training. 
When the total sum of the squared error of the output starts to fluctuate the learning rate is decreased 
by 10% until their values reached 0.09. It was noted that the Back propagation was the least sensitive 
to the values of momentum and learning rate. Furthermore, the sum-ofÂ­prmhwtsrequired longer training 
time. In general, once the neural network was trained for a certain mapping it was hardertotrainitto 
implementanothexmapping. IV. Conclusions The paper presents a unique and powerful system for characterizing 
Ultrasonic transducers.Furthermore,the papercomparestheperformanceof variousneuralnetwork and patternrecognitiontechniques 
for chameterizing these transducers.Neural network techniques produced excellent results in the recognition 
process. The backpropagation algorithm was the best with a rixognition rate of 94%, followed closely 
by the newly-devised hybrid sum-ofÂ­productsalgorithmwith 93%recognition rote.The sum-ofÂ­productsalgorithmproduceda 
90%recognition rate,which was the worst neural network algorithm, while the best trdt.ional patternrecognition 
technique was the perceptmn with a 77% recognition rate. Finally, the worst algorithm was the Bayes technique 
with a 46% nxognition rate. The resultsclearlyshowthehighperformanceof neuralnetwork techniques. To furtherdistinguish 
among theneuralnetwork algorithms, other training properties were studied. The backpropagationwas the 
least sensitive to the initial values of learning rate and momentum, and it required fewer training iterations. 
The comparison of neuml network and pattern recognition techniques revealed the inefficiency of the competitive 
learning algorithm to precluster the transducers, especially when compared to the K-Means algorithm. 
 References : [1] M.S. Obaidat and J.W. Ekis, An Automated System for Characterizing Ultrasonic Transducers 
using Pattern Recognition, IEEE Trans. on Instrumentation and Measurement, pp. 847-850, Vol. 40, No. 
5, October 1991. [2] F. Hotchkiss, D. Patch and M. Stanton, Exizn@es of Ways to Evaluate Tolerances on 
Transducer Manufacturing Specifications, Materials Evaluation,pp. 1195-1202,Vol. 47, No. 10, 1987. [31 
R. Shape, ResearchTechniques inNondestructive Testing, pp. 175-215, Vol. 3, New York Academic Press, 
1977. [4] W. SproaL W. Lewis, and L. Walthour, Ultrasonic Transducer Characterization for Field Use, 
The World Conference on Nondestructive Testing, pp. 1938-1941, Vol. 3, 1985. [5] V. LaCasse, J.R. Hay, 
and D.R. Hay, Pattern Recognition of Ultrasonic Signals for Detection of Wall Thinning, Proceedings of 
the 1987 NATO Advanced ResearchWo&#38;hop, pp. 189-198,1988. [6] S. Bleha and M.S. Obaidat, Dimensionality 
Reduction and Feature Extraction Applications in Identlfiing Computer Users, lEEE Transactions on Systems, 
Man, and Cybernetics, pp. 452=456, vol. 21, No. 2, March/April 1991. [71 J. MacQwxm, SomeMethodsfor Classification 
of Multivariate Data, ~ngs of the 5* Berkeley Symposium on Mathematics, Probability and Statistics, pp. 
281-297, Vol. 1, 1%7. [8] J. TOU, Engineering Principles of Pattern Recognition, Advances in Information 
Systems Science, Vol. 1, Plenum Pnxss, New YorlL 1%9. [9] J. Tou and R. Gonzalez, Pattern Recognition 
Principles, , Addison-Wesley, 1974. [10] F. Rosenblatt, The Perception: A Perceiving and Recognizing 
Automation, Cornell Aeronautical LaboratoryReport, Report #85460-1, 1957. [11] D. BlackWell and M. Girshick, 
Theory of Games and Statistical Decision, Wiley, New York, 1954. [12] M.S. Obaidat, D. Macchairolo and 
S. Bleh&#38; An Intelligent Neural Network System for Identi~ng Computer Users, in Intelligent Engineering 
Systems Through Artificial Neural Networks, pp. 953-959, 1991. [13] K. Fukushi~ A Neural Network for 
Visual Pattern Recognition, IEEE Computer, pp. 65-74, March 1988. [14] T. Kohonen, The Neural Phonetic 
Typewriter, IEEE Computer, pp. 11-20, March 1988. [15] B. Widrow and M. Lehr, 30 Years of Adaptive Neural 
Networks: Perception, Madaline, and Backpropagation, Proceedings of the IEEE, pp. 1415-1442, Vol. 78, 
No. 9, September 1990. [16] R. Hecht-Nielsen, Neurocomputing, Addison-Wesley Publishing Company, Reading, 
MA, 1990. [17] J. Hampshire and A. Waibel, ANovel Objective Functionfor Improved Phoneme Recognition 
Using Time-Delay Neural Networks, IEEE Transactions on Neural Networks, pp. 216-228, Vol. 1, No. 2, June 
1990. [18] W. Lin, F. Liao, C. Tsm, and T. Lingutla, A Hierarchical Multiple-View Approach to Three-Dimensional 
Object Recognition, IEEE Transactions on Neural Networks, pp. 84-92, VO1.2,No.1, January 1991. [19] S. 
Kong and B. Kosko, Differential Competitive Learning for Centrm dEstimation and Phoneme Recogm tion, 
IEEE Transactions on Neural Networks, pp. 118-124, Vol. 2, No. 1, January 1991. [20] K. Fukushima and 
N. Wake, Handwritten Alphanumeric Character Recognition by the Neocogm tron, IEEE Transactions on Neural 
Networks, pp. 355-365, Vol. 2, No. 3, May 1991.  
			