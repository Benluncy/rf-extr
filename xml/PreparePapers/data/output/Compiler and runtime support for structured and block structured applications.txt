
 Compiler and Runtime Support for Structured and Block Structured Applications Gagan Agrawal Alan Sussman 
Joel Saltz Dept. of Computer Science University of Maryland College Park, MD 20742 {gagan, als, saltz}@cs.umd.edu 
Abstract Scientific and engineering applications often involve struc­tured meshes. These meshes may be 
nested (for multigrid or adaptive codes) and/or irregularly coupled (called Irreg­ularly Coupled Regular 
Meshes). We have designed and implemented a runtime library for pamllelizing this gen­eml class of applications 
on distributed memory parallel machines in an eficient and machine independent man­ner. In this paper 
we present how this runtime library can be integmted with compilers for High Performance Fortran (HPF) 
style pamllel progmmming languages. We discuss how we have integrated this runtime library with the For­tran 
90D compiler being developed at Symcuse University and provide experimental data on a block structured 
Navier-Stokes solver template and a small multigrid example par­allelized using this compiler and run 
on an Intel iPSC/860. We show that the compiler pamllelized code performs within .20% of the code pamtlelized 
by inserting calls to the runtime libmry manually. Introduction In recent years, dktributed memory parallel 
machines have been widely recognized as the most likely means of achiev­ing scalable high performance 
computing. However, there are two major reasons for their lack of popularit y among the developers of 
scientific and engineering applications. First, it is very difficult to parallelize application programs 
on these machines. Second, it is not easy to get good speed­ups and efficiency on communication intensive 
applications. Current distributed memory machines have good commu­nication bandwidths, but they also 
have high startup la­tencies that often result in high communication overheads. Recently there have been 
major efforts in developing pro­gramming language and compiler support for distributed memory machines. 
Based on the initial work of projects like Fortran D [9, 13] and Vienna Fortran [5], the High Performance 
Fortran Forum has recently proposed the first version of High Performance Fortran (HPF) [8]. HPF al­lows 
programmer to specify the layout of distributed data and specify parallelkrn through operations on array 
sec­ tions and through parallel loops. Proposed HPF compil­ers are being designed to produce Single Program 
Multiple Data (SPMD) Fortran 77 (F77) code with message pass­ing and/or runtime communication primitives. 
HPF offers the promise of significantly easing the task of programming 1This work was supported by ARPA 
under contract No. NAG-1-1485, by NSF under grant No. ASC 9213821 and by ONR under contract No. SC 292-1-22913. 
The authors assume all responsibility for the contents of the paper. @ 1993 ACM 0-8186-4340-4/93/0011 
$1.50 dktributed memory machines and making programs indc+ pendent of a single-machine architecture. 
--- It is recognized that reducing communication costs is crucial in achieving good performance on applications 
[12]. While current systems like the Fortran D project [13] and the Vienna Fortran Compilation system 
[5] have imple­mented a number of optimizations for reducing communica­tion costs (like message blocking, 
collective communication, message coalescing and aggregation), these optimizations have been developed 
only in the context of regular prob­lems (i.e. in code having only regular data access patterns). Special 
effort is required in developing compiler and run­time support for applications that do not necessarily 
have regular data access patterns. Our group has already devel­oped compiler embedded runtime support 
for completely irregular computations [10]. One CISSS of scientific and engineering applications in­volves 
structured meshes. These meshes may be nested (as in multigrid or adaptive codes) or may be irregularly 
coupled (called Irregularly Coupled Regular Meshes) [6, 7]. Multigrid codes typically have a number of 
meshes at differ­ent levels of resolution, so may require array section moves with non-unit strides. 
Multigrid codes are often used in computational fluid dynamics applications. Examples of Ir­regularly 
Coupled Regular Mesh (ICRM) problems include multiblock Navier-Stokes solvers and structured adaptive 
multigrid problems. These problems have the following characteristics: The data is divided into several 
interacting regions (called blocks or subdomains).  There exists a computational phase during which 
com­putation in each block can be carried out indepen­dently.  Data access patterns within each block 
are regular.  c Communication between subdomains is restricted to regular array sections (possibly including 
non-unit strides). Block structured grids are frequently used in computa­tional fluid dynamics to model 
geometrically complex ob­jects that cannot be easily modeled using a single regu­lar mesh. In Figure 
1, we show how the area around a wing has been modeled with a block structured grid. Block structured 
grids are used in applications such as turbu­lence, combustion, global climate modeling and device sim­ulation 
[2, 3, 14, 16, 17]. We have developed a set of runtime primitives for parrd­ldlzing these applications 
on distributed memory machines Permkon to copy without fee all or pan of this material is gmnted, pm~ided 
that the copies am not made or distrilwted for direct ccinmercial advantage, the ACM copyright notice 
and the title of the fmblication and ils date appear, and nohce is gm en that capying is by pxmission 
cl the 578 Association for Comfnmng MachmeIY. To copy dbemvise, or to repubfkh, requires a fee ador 
s@Ic permission. WinS Regi~n (suMomain 1) Control Surface (subdymsim 2) Subdoxnain 1 Figure 1: Block 
structured grid around in an efficient, convenient and machine independent man­ner. These routines are 
collectively called the multiblock Parti library [6]. These primitives allow a user to Lay out distributed 
data in a flexible way, to enable good load balancing and minimize interprocessor com­munication, s Give 
high level directions for performing data move­ment, and Distribute the computation across the processors 
us ing the owner computes rule. The primitives have been designed so that communi­cation overheads are 
significantly reduced (by using mes­sage aggregation). These primitives provide a machine­independent 
interface to the compiler writer and applica­tions programmer. We view these primitives as forming a 
portion of a portable, compiler independent, runtime sup­port library. In thk paper we address the problem 
of integrating this runtime support into compilers for HPF style parallel pr~ gramming languages. We 
discuss the additional function­ality required in the current version of HPF to facilitate proper data 
layout for these applications. We discuss how the compiler can identify the data access patterns that 
can be efficiently handled using the runtime support for block structured problems. We also discuss the 
compiler trans­formations required for automatically generating the calls to these primitives. We have 
implemented these compiler transformations in the Fortran 90D (F90D) compiler being developed at Syracuse 
University [4]. We have parallelized a representa­tive part, which we call a template, of a three dimensional 
Navier-Stokes solver using this compiler. We present ex­perimental data for this compiler parallelized 
application run on an Intel iPSC/860 and compare it with that of a hand pars.llelized (i.e. parsll~lzed 
by inserting calls to the runtime library by hand) version. We have also ex­perimented with a multignd 
template that involves regu­lar section moves with strides. We find that the compiler a wing, showing 
an interface between blocks parallelized code always performs within about 20% of the hand parallelized 
code. We discuss the optimization that we have used to achieve this performance. We also dk­cuss other 
possible optimizations and their impact on the performance of the compiler parallelized code. Several 
other researchers have also developed runtime li­braries or programming environments for block structured 
applications. Bad en [14] hss developed a Lattice Program­ming Model (L PAR). This system, however, achieves 
only coarse grained parallelism since a single block can only be assigned to one processor. Quinlan [15] 
has developed P++, a set of C++ libraries for grid applications. While these provide a convenient interface, 
the libraries do not optimize communication overheads. The rest of this paper is organized as follows. 
In Sec­tion 2, we introduce the runtime library that we have de­veloped. In Section 3, we discuss how 
the compiler recog­nizes the data access patterns that can be handled using the runtime primitives that 
we have developed. In this section we also discuss the compiler transformations for automati­cally inserting 
the calls to the runtime library routines. In Section 4, we present experimental results on the perfor­mance 
of a compiler parallelized multiblock Navier-Stokes solver template and a multigrid template run on an 
Intel iPSC/860. We conclude in Section 5. 2 Multiblock Parti Primitives In this section we describe the 
details of the runtime sup­port library that we have designed for parallelizing block structured codes 
on distributed memory machines. We first dkcuss the nature of computation and communication in block 
structured applications and then describe the primi­tives to facilitate parallelization of these applications. 
In typical block structured problems there are at most a few dozen blocks of varying sizes. We therefore 
assume that we will have to assign at least some of the blocks to mul­tiple processors, so that there 
are two levels of parallelism available. Coarse-grain parallelkrn is available for proces­ing the blocks 
concurrently. Associated with each block is a self-contained computation that can, except for boundary 
conditions, be operated upon independently of the other blocks. In addition, the computation for individual 
blocks has fine-grain parallelism available. Applying coarse-grain parallelism will help to keep communication 
overhead to a manageable fraction of the computation time. For a typical block structured application, 
the main body of the program consists of an outer sequential (time step) loop, and an inner parallel 
loop. The inner loop iter­ ates over the blocks of the problem, after applying bound­ ary conditions 
to all the blocks (including updating in­terfaces between blocks). Applying the boundary con­ditions 
involves interaction (communication) between the blocks. In the inner loop over the blocks, the computation 
in each block involves only intra-block regular communi­cation. Partitioning of the parallel loop is 
the source of the coarse-grained parallelism for the application. Further­more, within each iteration 
of the inner loop there is fine­grained parallelism available in the form of (large) parallel loops. 
Several forms of run-time support are required for block structured applications. First, there must be 
a means for expressing data layout and organization on the processors of the dktributed memory parallel 
machine. Second, there must be methods for specifying the movement of data re­quired both because of 
partitioning of individual meshes (intra-block parallelism) and because of interactions be­tween different 
meshes (inter-block rmrallelism). Third. \/ there must be some way of transforhng dktributed ar­ray indexes 
specified in global coordinates (ss in sequential code) into local indexes on a given processor in the 
dis­tributed memory parallel machine. The bhdng of blocks to processors has important per­formance implications, 
both in terms of reducing commu­nication overheads and achieving load balance. In irregu­larly coupled 
regular mesh applications, there are compu­tational phases that involve interactions only within each 
block. Therefore, communication overheads are reduced if each block is not divided across a large number 
of proce~ sors. Since the amount of computation associated with each block is directly proportional to 
the number of elements in the block, good load balancing is achieved by binding processors to blocks 
in a ratio proportional to their sizes. Therefore, the blocks are dktributed onto dkjoint subsets of 
processor space. The relative sizes of these processor spaces are determined by the relative number of 
grid points in each block. Since, in a typical block structured application, the num­ber of blocks and 
their respective sizes is not known until runtime, the distribution of blocks onto processors is done 
at runtime. The distributed array descriptors (DAD) [4] for the arrays representing these blocks are, 
therefore, gen­erated at the runtime. Distributed array descriptors con­tain information about the portions 
of the arrays residing on each processor, and are used at runtime for performing communication and distributing 
loops iterations. We will not discuss the details of the primitives that allow the user to specify data 
distribution. For more details, see [18]. Two types of communication are required in block struc­tured 
applications: intra-block communication because a single block may be partitioned across the processors 
of the dktributed memory parallel machine, and inter-block com­munication because of boundary conditions 
between blocks, caused by the szsignment of blocks to different processors to obtain coarse-grained parallelism. 
The data access pat­tern in the computation withb a block is regular. This im­ plies that the interaction 
between grid points is restricted to nearby neighbors. Such communication is handled by allocation of 
extra space at the beginning and end of each array dimension on each processor. These extra elements 
are called overlap, or ghost, cells [5]. Depending upon the data access pattern in a loop, the necessary 
data is copied from other processors and is stored in the overlap cells. The communication is performed 
in two phases. First, a subroutine is called to build a communication schedule that describes the required 
data motion, and then another subroutine is called to perform the data motion (sends and receives on 
a distributed memory parallel machine) using a previously built schedule. Such an arrangement allows 
a schedule to be be used multiple times in an iterative al­gorithm. This amortizes the cost of buildlng 
schedules, so that the preprocessing time should not be a significant part of the execution time of this 
type of program. The communication primitives include a procedure Ouer­lap. CelLFilLSched, which computes 
a schedule that is used to dhect the filling of overlap cells along a given dimension of a distributed 
array. Overlap. Cell_ FilJ.Sched executes on each processor that contains a part of the distributed array, 
and, for a given processor i, determines both which other processors require data that is stored on processor 
i and which other processors store data that processor i requires. The primitive Regular.Section-Copy-Sched 
carries out the preprocessing required to copy the contents of a reg­ular section [11], source, in one 
block into a regular sec­tion, destination, in another (or the same) block. The interactions between 
blocks for block structured applica­tions are limited to the exchange of regular sections. The ReguJar-Section-Copy-Sched 
primitive supports data moves between arbitrary rectangular sections of two blocks, and can transpose 
the data along any dimension. For a given processor, Regular-Section.Copy@ched produces a sched­ule that 
sDecifies a a set of send and receive calls for in­ . terprocessor communication along with a pattern 
of intra­processor data transfers (for the parts of the source and destination subsections that reside 
on the same processor). The schedules produced by Ouerlap-Cell_FilLSched and Regu2ar-Section-Copy $ched 
are employed by a primitive called Dat~Moue that carries out both interprocessor com­ munication (sends 
and receives) and intra-processor data copying. The final form of support provided by the multiblock 
Parti library is to transform all indexes into distributed arrays from the global value (an index into 
the whole dis­tributed array) to a local index on the processor execut­ing a distributed array reference. 
Two primitives, Lo­caLLower-Bnd and Local-Upper-Bound, are provided for transforming loop bounds (returning, 
respectively, the lower and upper indexes of a given dimension of the ref­erenced distributed array). 
In general, however, each di~ tributed array reference (read or write) must have the array index transformed 
from a global to a local reference for cor­rect parallel execution. 3 Compiler Support In this section 
we first discuss the additional functionality required in the current version of HPF to support block 
structured applications. We describe how a compiler can analyze the data access patterns associated with 
a loop, to recognize communication patterns that can be handled us­ing the runtime primitives for block 
structured problems. We then describe the compiler transformations for gener­ating the calls to these 
runtime primitives. We also briefly dkcuss how loop iterations are dktributed to achieve par­allelism. 
We have implemented the transformations in the For­tran90D compiler being developed at Syracuse University. 
In the current version of this compiler, parallelism can be expressed through single statement forall 
loops and For­tran90 array expressions. The current version of HPF also allows multiple statement fomll 
loops and do loops with no loop carried dependencies (via the independent state­ment) to express parallelism. 
Although we are restricted to single statement ~oraUloops in the Fortran source code, there is still 
sufficient expressiveness to exploit parallelism in application codes. 3.1 Language Support The current 
version of HPF does not support idl the func­tionality required for block structured applications. In 
block structured problems, the problem geometry is divided into a number of blocks of different sizes. 
As we have dk­cussed in the previous section, each of these blocks needs to be dktributed onto a portion 
of the processor space. The current version of the HPF does not provide any con­venient mechanism for 
distributing arrays (or templates) onto a part of the processor space. We allow the program­mers to create 
subspaces of the processor space. Arrays or templates can then be mapped onto any processor sub­space. 
We will not discuss the details of the syntax that our experimental compiler uses for declaring processor 
sub­spaces. With the block dktributions supported in the current version of HPF, the entire array gets 
distributed uniformly across the processors of the dktributed memory parallel machine. This may not be 
ideal for load balancing for many applications. While the programmer may declare a large array, not all 
the elements of the array may be actual mesh points participating in computation. Some of the ar­ray 
elements at both ends of each dimension may be used for participating in exchanges between blocks. We 
refer to such array elements as external ghost cells. For exam­ple, the actual declared arrays for a 
given block may be 52 x 12 x 12, with two external ghost cells at the beginning and end of each dimension. 
This means that the actual mesh representing the computation is of size 48 x 8 x 8. It is these mesh 
points that must be dktributed evenly across the processors onto which the block is distributed, so that 
the computation load will be evenly balanced. The external ghost cells at both ends of each dimension 
are then stored at the first and last processor along that dimension in the processor space. For example, 
if an array with 8 elements, plus two external ghost cells on each end (for a total of 12 elements), 
is distributed on 4 processors, we would like to store 2 mesh points on each processor along that di­mension. 
The first and last processors can then store the external ghost cells at the beginning and end, respectively. 
This results in a much better load balance than simply dis­tributing 3 array elements onto each processor 
(which will result in the first and last processors having only 1 real mesh point and the intermediate 
processors having 3 real mesh points each). The current version of HPF does not provide any mecha­ nism 
for specifying external ghost cells. We need additional functionality in the align statement to express 
them. We do this by explicitly specifying the number of external ghost cells at the beginning and end 
of each dimension: !HPF$ DIMENSION A(105,105) !HPF$ ALIGN A(i,j) WITH T(i:2:3,j:2:3) This example says 
that an array of size 105x105 is aligned with a template of size 100x100, with 2 external ghost cells 
at the beginning of each dimension and 3 external ghost cells at the end of each dimension. If the template 
T is dis­tributed by blocks onto a two dimensional processor space, A(3:102,3: 102) also gets distributed 
in the same fashion. Our purpose here is not to introduce new syntax but to achieve the additional functionality 
that we need. We be­lieve that this functionality will be added, in some form, in a future version of 
HPF. 3.2 Identifying Communication Patterns In this subsection we discuss how the compiler identifies 
the communication patterns that can be handled using the runtime support for block structured problems. 
While we have designed the runtime support with block structured applications in mind, the runtime primitives 
can also be used to efficiently handle communication for many other types of applications. Our purpose 
is not to provide a gen­eral framework for compiling jorall statements; we are only interested in recognizing 
the patterns that can be handled efficiently using the primitives we have developed. We do not consider 
applications in which indirection ar­rays may need to be analyzed to identify communication patterns, 
The irregular communication arising from use of indirection arrays can be handled using the Parti primitives 
for irregular problems, which have also been integrated with compilers for HPF-style languages (includhg 
the Rice Uni­versity Fortran 77D compiler [10] and the Syracuse Univer­sit y Fortran 90D compiler [4]). 
F90D and HPF also provide a number of intrinsic functions (such as reduction, spread, etc.). We assume 
that if a computation can be done using these intrinsic, it is either written this way by the pro­grammer 
or is recognized by the compiler in an early phase of the compilation. We restrict our discussion to 
the problem of analyzing a single statement forall loop for communication patterns. F90D also provides 
array expressions for expressing pad­lelism, but these array expressions can always be translated into 
equivalent }orali loops. We classify the data access patterns associated with a forall loop as being 
one of three kinds: Completely regular ones (not involving any communi­cation).  Ones that can be handled 
by filling in overlap cells.  Ones that require regular section moves.  Consider any /orall statement 
with array expressions in­volving an array A on the left hand side and an array B aa one of the arrays 
on the right hand side. The form of the forall statement is assumed to be as follows: forall (il = /01 
: hil : Stl, . . . ,im = 20771: him : .%?m) A(fI,..., fJ) = . . . B(gl,..., gn) . . . The ik, (k = 1..rn) 
are the loop variables associated with the forali statement. 10k, hik and Si?k are respectively the lower 
bound, upper bound and the stride for each loop variable. For the left hand side array A, ~1, f2, . . 
. fj are the subscripts. Similarly, for the right hand side array B, 91, 92, .. . g~ are the subscripts. 
The array subscripts ~ and g are either loop invariant scalars or a a linear function of some loop variable. 
The form of the subscript is: gk = dkizk +-d2k Here, ilk and i2k are loop variables. If a subscript is 
a loop invariant scalar, then we say that the loop variable ilk (or, i2k ), is 4 and clk (or, c2k) is 
o. dk, C2k, dlk and d2k may be expressions, but they must not involve any loop variable. Our primitives 
are not applicable for cases in which multiple loop variables are associated with a particular array 
subscript or when the same loop variable appears in more than one subscript for a particular arraY or 
when a subscript is a higher order function of a loop vari­able. Such cases are handled by the current 
Fortran 90D compiler using the Parti primitives for irregular problems. Also, the HPF specification allows 
the lower bound, upper bound and stride expressions for each loop variable to be evaluated in any order. 
Consequently, the lower bound, up­per bound and stride for any loop variable are not allowed to be a 
function of any other loop variable. It is possible, in general, that a loop variable may appear only 
on the right hand side array or only in the left hand side array. If a par­ticular loop variable appears 
only in the right hand side, this represents successive overwrites on the same memory location of the 
left hand side array. Such code is not likely to appear in practice and therefore, we do not consider 
this case. If a particular loop variable appears only in the left hand side array, this represents a 
spread operation. We az­sume that it is written using the intrinsic spread operation, and is not a part 
of the fomll. Depending upon how the arrays A and B are distributed and aligned with respect to each 
other, we consider three different cases. These are: Case I: Arrays A and B are aligned to different 
tem­plates. Case II: Arrays A and B are identically Jlgned to the same template. This case also requires 
that A and B are arrays of identical shape and size, i.e. having the same number of dimensions and the 
same size in each dimension. Case III: Arrays A and B are Jlgned to the same template, but with a different 
stride, offset and/or in­dex permutation. This means that the the arrays A and B are mapped to the same 
processor subspace, but each in a different manner. In thk paper, we restrict to discussing the details 
of cases I and II only. Csse III is a generalization of case II; details of the analysis for this case 
are presented in [1]. 3.2.1 Case I Since the arrays are aligned to distinct templates, the com­ munication 
is always handled using the regular section move primitive from the runtime library. We expect that if 
a user hss declared distinct templates then they are either distributed over different processor subspaces, 
or have a dif­ferent number of dk.tributed dimensions. Therefore, there is no regularity in the communication 
associated with a forall statement containing references to such arrays. It is possible that a programmer 
may create more than one template with the same number of distributed dimen­sions, distributed over the 
same processor subspace. We can extend our analysis to consider the processor subspace over which distinct 
templates are dktributed in determin­ing any regularity in the communication required. How­ever, we do 
not discuss this possibility here. 3.2.2 Case II The data access patterns associated with this case may 
be completely regular, or may require overlap cells to filled in, or may require a regular section move. 
Let DD(A) denote the set of dimensions of the array A that are distributed. Under the assumptions for 
Case II, DD(B) = DD(A). In terms of the form of the forall state­ment and the array subscripts that presented 
in Section 3.2, the condition for the associated communication to require a regular section move is : 
3 j c DD(A) s.t. 1. ilj # i2, ,or, 2. CIJ # c2j ,or, 3. dlj # d2j -d either dlj or d2j is not a compile 
time constant, or, 4. ilj = ~,i2j = ~=ddlj # d2j.  The first condition states that there is index permuta­tion. 
In that case, a regular section move will be required. The second condition states that, rdong the jth 
dimension, the elements of the arrays A and B are being accessed with different strides. Again, thk case 
will require a regular section move. The third condition corresponds to the fact that there are non-constant 
offsets. If there are constant offsets, then only the overlap cells need to be filled in. For overlap 
cells, space needs to be allocated at compile time, so the number of overlap cells must be known at compile 
time. If the offsets are not compile time constants, then we use a regular section move to handle communication. 
This situation can also be handled by shifts into a temporary ar­ray [4]. The fourth condition says that 
along dimension j a loop variable does not appear in either the left hand side or the right hand side 
index and the loop invariant scalars are different. This represents a copy from one location to another, 
but because of the loop variables associated with other dimensions, will typically require a regular 
section of data to be moved. Since the distributed array descriptor is not available at compile-time, 
it cannot be determined at compile time whether this data move will require any interprocessor communication. 
So we handle this kind of data move using the regular section move primitives we have already discussed. 
The data access pattern requires filling in overlap cells, if the following condition holds: 1. A regular 
section move is not required and 2. 3j c DD(A) St. dlj # d2j.  Ar ~sA and Bar aligned idem Cally L.H.S. 
R.H.S. Reg. Sec. OverlaD Expr. Move Req. Fill Re~. m YES NO A(i,j) B(2*iJ YES NO A(iJ B(i+nl,j+2) YES 
NO A(i,j) B(i+l,j+2) NO YES A(i,j) B(i,j) NO NO Figure 2: Analyzing communication for Case II The first 
condition just states that we have not already determined that a regular section move is required. Th~ 
second condition states that there is a difference in the offsets along some (distributed) dimension. 
Overlap cells must be tilled along each dimension in which there is a difference in the offsets. In Figure 
2 we show examples for the different possibilities within case II, for identically aligned two dimensional 
arrays A and B. 3.3 Generating calls to the runtime library Once the nature of the communication required 
has been identified, the compiler must insert the appropriate calls to the runtime primitives. We identify 
each distributed di­mension j of the array B for which dlj # d2j. One call to the schedule building primitive 
Overlap-Cell_FilLSched and one to the data moving primitive Data-Move is in­serted for each such dimension. 
Since all computations are distributed using the owner computes rule, overlap cells are filled in for 
the right hand side array B. If there is more than one array on the right hand side, then the analysis 
described in Section 3.2 is done for each such array, For each of the right hand side arrays that requires 
a regular section move, a temporary array is de­clared and a regular section move is done from the right 
hand side array into the temporary array, If there is only one array on the right side (i.e. the ~omll 
loop represents only a copy and does not have any computation), then the regular section move is performed 
directly from the right hand side array to the left hand side array. 3.4 Distributing loop iterations 
Once the calls have been inserted for communicating the required array elements, the loop iterations 
must be dis­tributed among the processors (i.e. the loop bounds need to adjusted to compute on data local 
to a processor). As we stated earlier, this is done by using the owner computes rule. Since the distributed 
array descriptors are built at runtime, it is not possible to compute the 10CSJloop bounds on each processor 
at compile time. We partition the loop based upon the portion of the dis­tributed arrays that are owned 
by a given processor. This is done by inserting runtime calls to the the library primitives Local_Lower.Bound 
and Local-Upper-Bound. For arrays that are not in canonical form, we can still partition the loop based 
upon the owners compute rule. Consequently, we never need to scatter any data after the loop has been 
executed. c ORIGINAL F90D CODE c Arrays A, B and C are distributed identically FORALL (i= l:100,j = 1:100) 
. A(i,j) = B(i+l,j) + C(iJ) c TRANSFORMED CODE Dim= 1 No-Of-Cells = 1 sched = Overlap.Cell-Fill_Sched(DAD, 
 Dim, No-OLCells) c DAD is dist. array desc. for A, B and C c i is dimension 1, j is dimension 2 Call 
Data_Move(B,sched,B) L1 = Locrd-Lower-Bound( DAD, 1) L2 = Local-Lower-Bound( DAD,2) HI = Local-Upper-Bound( 
DAD ,1) H2 = Local-Upper-Bound( DAD,2) do 10i = L1,H1 do 10j = L2, H2 10 A(i,j) = B(i+l,j) + C(i,j) 
Figure 3: Overlap cell fill and loop bounds adjustment In Figure 3 we show an example of how the calls 
to prim­itives for filling in overlap cells are inserted by the compiler. In Figure 4, we show how the 
compiler inserts calls to the primitives for moving regular sections. In both examples, the transformed 
code containing the calls to the runtime library will run as SPMD code on each processor of the distributed 
memory parallel machine, In the compiler gen­erated code schedule building primitives will be called 
every time any forcdl loop requiring communication is executed. The hand coded version can build a schedule 
once and reuse it in subsequent iterations. Similarly, in the compiler gen­erated code, runtime calls 
to the loop bound adjustment primitives will be made each time a loop is executed. The hand coded version 
can reuse the adjusted bounds over the multiple time steps, and also for multiple loops that have the 
same loop bounds. These two factors may cause the compiler generated code to perform worse than the hand 
parallelized code. 4 Experimental Results In this section we present the experimental results we have 
obtained on the performance of a compiler parallelized an irregularly coupled regular mesh template and 
a multigrid template. We have parallelized a template from a multiblock com­putation fluid dynamics application 
that solves the thin­layer Navier-Stokes equations over a 3D surface (multi­block TLNS3D), using the 
prototype Fortran 90D com­piler described in Section 3. The version of the multiblock TLNS3D code we 
are working with was developed by Vatsa et al. [19] at NASA Langley Research Center, and consists of 
nearly 18,000 lines of Fort ran 77 code. The template, which was designed to include portions of the 
entire code that are representative of the major computation and com­munication patterns of the original 
code, consists of nearly 2,000 lines of F77 code. We have hand parallelized thk template by manually 
inserting calls to the multiblock Parti routines. C C ORIGINAL F90D CODE Arrays A, B are distributed 
forall (i= I: IOO:2J = c TRANSFORMED NumSrcDim = CODE 2 SrcDim(l) SrcDim(2) SrcLos(l) SrcLos(2) SrcHis(l) 
SrcHis(2) SrcStr(l) SrcStr(2) Sched =2 =1 =2 =1 = 100 = 100 =2 =2 identically 1:50) A(i,j) = B(2*j,i) 
NumDestDim = 2 DestDim(l) = 1 DestDim(2) = 2 DestLos(l) = 1 DestLos(2) = 1 DestHis(l) = 100 DestHis(2) 
= 50 DestStr(l) = 2 DestStr(2) = 1 = Regular.Section-Move.Sched (DAD, DAD,NumSrcDlm, NumDestDim, SrcDim, 
SrcLos, SrcHis, SrcStr, DestDim, DestLos, DestHis, DestStr) Call Data-Move(B,Sched,A) Figure 4: Regular 
section move TWO BLOCKS: 49 X 17 X 9 Mesh (5o Iterations) No. of Compiler Hand Hand Proc F90 F77 8 7.49 
6.69 16 4.64 4.07 4.03 32 2.88 2.32 2.30  6.17 L Figure blocks lelized worse in the 6: Performance 
comparison for larger mesh, two (sec.) F77 code. The hand parallelized F90 code performs than the hand 
parallelized F77 code. This is because, F90 version, all computation is done through single statement 
forall loops that result in the creation of (large) temporary arrays. Such use of temporary storage, 
and the fact that no loop fusion between parallel loops is done by the compiler, increases the number 
of cache misses on each processor. However, the difference in performance between the F90 and F77 hand 
parallelized versions decreases as the number of processors increases. This is because as the number 
of processor increases, less memory is required on each processor, so the effect on cache utilization 
is less sig­nificant. The difference in performance of the hand paral­lelized F90 and the compiler parallelized 
code comes from two major factors. First, in the compiler generated version,ONE BLOCK: 49 X 9 X 9 Mesh 
50 Iterations the runtime calls for computing new loop bounds are made in each loop iteration, as compared 
to only once for the hand parrdlelized version. Second, as the template is run over a large number of 
time steps, the compiler generated  =T=T=P= :8 4.17 4.06 4.00 16 2.47 2.35 2.28 32 1.55 1.45 1.41 
 Figure 5: Performance comparison for small mesh, one block (sec.) We converted the F77 code to F90D 
manually, by rewrit­ing the the major computational parts of the code usiug sin­gle statement fomll loops 
and F90 array expressions, also adding the required data distribution directives. We then parallelized 
the code by running it through the F90D com­piler. We also created a hand parallelized F90 version of 
the template in which all computations are done with single statement fomll loops, but the calls to the 
runtime primi­tives are inserted manually. We now compare the relative performances of compiler parallelized 
F90 code, hand parallelized F90 code and hand parallelized F77 code, varying the mesh size aud number 
of blocks for the application, and also varying the num­ber of processors used on an Intel iPSC/860. 
In Figure 5, we present the performance results on a 49 x 9 x 9 mesh (with one block), comparing the 
performance of the three versions from 4 to 32 processors. In Figure 6, we present the performance results 
on a 49 x 17 x 9 mesh (split into two blocks), comparing the performance of the three ver­sions from 
8 to 32 processors. The compiler parallelized F90 code performs within around 20% of the hand paral­version 
makes repeated calls to the runtime library to build communication schedules, whereas in the hand parallelized 
version the calls are lifted out of the time step loop. To reduce the additional cost due to this second 
factor, our runtime library saves schedules. When a call is made for generating the schedule, the library 
searches a to check if any schedule with exactly the same is present. If so, the saved schedule is returned. 
nique still has the overhead of searching through table, as compared to a hand para.llelized version. 
hash table parameters This tech­the hash To study the exact costs of each of these factors, we present 
a more detailed experiment. In Figure 7, we study the effect of the optimization in the library of saving 
schedules. Version I is a compiler parrdlelized version in which the library does not save any schedules. 
This version performs badly because of the high cost of rebuilding the schedules for every iteration. 
Version II is the compiler parallelized version in which the library saves schedules. This results in 
a major gain in perfor­ mance. Version III represents the case where the compiler performs sophkticated 
interprocedural analysis to reuse the schedules during successive time steps. While our current compiler 
technology does not perform such optimization, we can study the effect of this optimization by generating 
the code to reuse the schedules by hand. Version III per­forms better than version II, in which the schedules 
are reused within the library, but the difference is not large. In the compiler parallelized version, 
runtime calls are made to the functions for adjusting loop bounds for each forall loop on each time step. 
The hand parallelized version can store the loop bounds computed during the first time TWO BLOCKS :49 
X 9 X 9 Mesh (SO Itera ons No. of Compiler Compiler Compiler Compiler Compiler Hand Proc. Version I Version 
II Version III Version IV Version V F90 4 13.45 7.63 7.41 7.39 7.33 6.79 7­ 4.54 4.19 2.62 2.39 1.66 
1.47 ~I Version I : Runtime Library does not save schedules Version II : Runtime Library saves schedules 
Version III : Schedule reuse implemented by hand Version IV : Multiple Statement forall loops Version 
V : Loop bounds reused within a procedure Figure 7: Effects of various optimizations (in sec.) Forall(i=l:N,j=l:N) 
FINE(2*i)2*j) = COARSE(i,j)  Forall(i=l:N.i= l:2N) FINE(2*i:lJ = (FINE(2*i,j) + FINE(2*i-l,j+l) + FINE(2*i-2,j) 
+ FINE(2*i-l,j-1))/4 Forall(i=l:2N,j= l:N) FINE(i,2*j-1) = (FINE(i,2*j) + FINE(i+l,2*j-1) + FINE(i,2*j-2) 
+ FINE(i-l,2*j-1))/4 Figure 8: Multigrid mesh refinement code step, for subsequent reuse. Additionally, 
a procedure may moves with strides. The code, part of which is shown in contain several loops involving 
the same array on the left Figure 8, involves copying from a coarse grid into a finer hand side that 
have the same loop bounds. Our compiler grid, followed by interpolation on the finer grid. Again we generates 
separate runtime calls for adjusting loop bounds compare the performance of the compiler generated code 
for each such loop. Such optimizations will be implemented and the hand parallelized F90 code. The results 
are shown in a future version of the compiler. in Figure 9. For the first iteration, the time taken by 
the In Figure 7, the difference between version III and the compiler generated version and the hand parallelized 
ver­hand parallelized F90 version shows the extra cost of gen-sion are identical. For subsequent iterations, 
the time taken erating loop bounds at runtime for each forall loop dur-by the compiler generated version 
is greater, but it is within ing each time step. The results show that generating loop 10% of the hand 
parallelized version. The compiler paral­bounds at runtime is the major factor in the performance lelized 
code performs worse because of the additional costs difference between the compiler parallelized version 
and the of adjusting loop bounds at runtime and the lookups into hand parallelized versions. hash tables 
required to reuse schedules. If the source language allows multiple statement fomlls, 5 Conclusions then 
the number of runtime calls to the loop bound adjust­ment routines will be substantially reduced. Version 
IV, To reliably and portably program distributed memory par­created by hand, has the loop bound adjustment 
calls in-allel machines, it is important to have both a machine inde­serted as if the language supports 
multiple statement forall pendent language and runtime support for optimizing com­loops. This version 
performs slightly better than version munication. High Performance Fortran and its variants III. In version 
V, we show the results of an unimplemented have emerged as the most likely candidates for machhe optimization 
in which the compiler is able to identify the independent parallel programming on distributed memory 
loops with the same left hand side array and same loop machines. bounds within a subroutine. Then the 
compiler needs to Scientific and engineering applications frequently involve generate calls to the loop 
bound adjustment functions only structured grids or meshes. These meshes may be nested once for each 
such set of loops. This optimization also pro-(as in adaptive or multigrid solvers) or irregularly coupled 
vides an improvement over version IV. (ICRMS). Block structured grid applications are a signif- We have 
also used the compiler to parrdlelize a multigrid icant part of scientific and engineering applications. 
We style mesh refinement in which there are regular section have designed and implemented a set of (compiler 
embed­No. of Compiler --Compiler: By hand: By hand: Proc. I t iteration Per Subsequent I t iteration 
Per Subsequent Iteration 4 .36 .226 8 .33 .125 16 .22 .098 32 .17 .060  Figure 9: Multigrid mesh 
 dable) runtime primitives for parallelizing these applica­tions in an efficient, convenient and machine 
independent manner. In this paper we have addressed the problem of integrat­ing the runtime support for 
block structured applications with a compiler for an HPF-like parallel programming lan­guage. We have 
presented the method the compiler uses to analyze the data access patterns associated with parallel loops. 
These methods are used to identify communication patterns that can be efficiently handled using the commu­nication 
primitives that the multiblock Parti library sup­ports. We have also presented a set of simple loop trans­formations 
that the compiler performs for automatically generating calls to the runtime primitives. We have implemented 
the compiler analysis method in the Fortran 90D compiler being developed at Syracuse Uni­versity. We 
consider this work to be a part of an integrated effort toward developing a powerful runtime support 
system for HPF compilers. To measure the performance overheads of the compiler parallelized code, we 
performed experiments with a template from a 3D multiblock Navier-Stokes solver and a multigrid template. 
We compared the performance of compiler parallelized code with the performance of hand paridlelized F90 
and F77 codes, and have shown that the compiler parallelized code performs within 20% of hand parallelized 
F77 code. The optimization of having the run­time library save and reuse communication schedules allows 
the compiler parsllelized code to perform almost as well as hand parallelized code. We have also experimented 
with other optimization. The optimization of reusing computed loop bounds within a subroutine improves 
the performance of the compiler parallelized code and brings of the hand parallelized version. We plan 
to integrate our runtime support compilers, including the Fortran 77D compiler oped at Rice University 
[13] and the Vienna pilation System [5]. We also plan to work on our runtime support and compiler techniques 
structured applications. Acknowledgements We gratefully acknowledge our collaborators, it within 10% 
with other being devel-Fort ran Com­applying both to other block Geoffrey Fox, Alok Choudhary, Sanjay 
Ranka, Tomasz Haupt, and Zeki Bozkus for many enlightening discussions and for sJlowing us to integrate 
our runtime support into their emerging For­tran 90D compiler. The detailed discussions we had with Sanjay 
Ranka, Alok Choudhary and Zeki Bozkus during their visits to Maryland were extremely productive. We are 
also grateful to V. Vatsa and M. Senetrik at NASA Langley Iteration .36 .207 .33 .119 .22 .095 .17 .056 
 refinement performance (sec.) for giving us access to the multiblock TLNS3D application code. References 
[1] Gagan Agrawal, Alan Sussman, and Joel Saltz. Com­piler and runtime support for structured and block 
structured applications. Technical Report CS-TR­3080 and UMIACS-TR-93-45, University of Maryland, Department 
of Computer Science and UMIACS, April 1993. [2] M.J. Berger and P. Colella. Local adaptive mesh re­finement 
for shock hydrodynamics. Journal of Com­putational Physics, 82:67-84, 1989. [3] M.J. Berger and J. Oliger. 
Adaptive mesh refinement for hyperbolic partial differential equations. Journal of Computational Physics, 
53:484 512, 1984. [4] Zeki Bozkus, Alok Choudhary, Geoffrey Fox, Tomasz Haupt, Sanjay Ranka, and Min-You 
Wu. Compil­ing Fortran 90 D/HPF for distributed memory MIMD computers. Submitted to the Journal of Parallel 
and Distributed Computing, March 1993. [5] B. Chapman, P. Mehrotra, and H. Zima. Program­ming in Vienna 
Fortran. Scientific Programming, 1(1):31-50, Fall 1992. [6] C. Chase, K. allelization of Proceedings 
of Supercomputing. [7] A. Choudhary, Crowley, irregularly the Sixth ACM G. Fox, C. Koelbel, S. Ranka, 
J. Saltz, and A. Reeves. Par­ coupled regular meshes. In International Conference on Press, June 1992. 
 S. Hiranandani, K. Kennedy, and J. Saltz. Software sup­ port for irregular and loosely synchronous prob­lems. 
Computing Systems in Engineering, 3(1-4):43­52, 1992. Papers presented at the Symposium on High- Performance 
Computing for 1992. [8] D. Loveman (Ed.). Draft tran language specification, port CRPC-TR92225, Center 
Computation, Rice University, Flight Vehicles, December High Performance For­version 1.0. Technical Re­for 
Research on Parallel January 1993. [9] Geoffrey Fox, Seems Hiranandani, Ken Kennedy, Charles Koelbel, 
Uli Kremer, Chau-Wen Tseng, and Min-You Wu. Fortran D language specification. Tech­nical Report CRPC-TR90079, 
Center for Research on Parallel Computation, Rice University, December 1990. [10] R. v. Hanxleden, K. 
Kennedy, C. Koelbel, R. Das, and J. Saltz. Compiler analysis for irregular prob­lems in Fortran D. In 
Proceedings of the 5th Workshop on Languages and Compilers for Parallel Computing, New Haven, CT, August 
1992. [11] P. Havlak and K. Kennedy. An implementation of interprocedursl bounded regular section analysis. 
IEEE Tmnsactions on Pamllel and Distributed Sys­tems, 2(3):350-360, July 1991. [12] S. Hiranandani, K. 
Kennedy, and C. Tseng. Eval­uation of compiler optimizations for Fort ran D on MIMD distributed-memory 
machines. In Proceedings of the Sixth International Conference on Supercomput­ing. ACM Press, July 1992. 
[13] Seems Hiranandani, Ken Kennedy, and Chau-Wen Tseng. Compiling Fortran D for MIMD distributed­memory 
machines. Communications of the ACM, 35(8):66-80, August 1992. [14] Scott R. Kohn and Scott B, Baden. 
An implemen­ tation of the LPAR parallel programming model for scientific computations. In Proceedings 
of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, pages 759-766. SIAM, March 
1993. [15] Max Lemke and Daniel Quirdan. P++, a C++ vir­tual shared grids based programming environment 
for architecture-independent development of structured grid applications. Technical Report 611, GMD, 
Febru­ay 1992. [16] S. McCormick. Multilevel Adaptive Methods for Par­tial Differential Equations. SIAM, 
1989. [17] J.J. Quirk. An Adaptive Grid Algorithm for Compu­tational Shock Hydrodynamics. PhD thesis, 
Cranfield Institute of Technology, January 1991. [18] Alan Sussman and Joel Saltz. A manual for the multiblock 
PARTI runtime primitives. Technical Re­port CS-TR-3070 and UMIACS-TR-93-36, University of Maryland, Department 
of Computer Science and UMIACS, May 1993. [19] V.N. Vatsa, M.D. Sanetrik, and E.B. Parlette. De­velopment 
of a flexible and efficient multigrid-based multiblock flow solver; AIAA-93-0677. In Proceedings of the 
91st Aerospace Sciences Meeting and Exhibit, January 1993. 
			