
 1979 Conference on Simulation, Measurement and Modeling of Computer Systems A SIMULATION ENVIRONMENT 
FOR PERFORMING DATAFLOW RESEARCH Steve P. Landry Bruce D. Shriver Computer Science Department University 
of Southwest Louisiana Lafayette, Louisiana 70504 Abstract  Dataflow languages and processors are currently 
being extensively studied because of their respective ability to specify and execute programs which exhibit 
a high degree of parallel and/or asynchronous activity [12, 7J. This paper describes a comprehensive 
simulation environment that allows for the execution and monitoring of dataflow programs. One overall 
objective of this facility was to meet the needs of researchers in SUCh diverse areas as computer architecture, 
algorithm analysis, and language design and impleJnentation. Another objective was to accommodate the 
semantics of several o£ tne conten~ing abstract aataflow models [2, 4]. A~aitionally, it was ~esireG 
to enhance the abstract dataflow models which the simulator would support. These oD]ectives, combined 
with the desired debugging and metering requirements, directed the design of the overall system. A brief 
introduction to ~ata~low and its related terminology is given to assist the reader. A companion paper 
~6] describes an augmentation to the oasic simulation facility presented here that allows for the execution 
of dataflow programs on processors having £inite resources. Introouction  Algorithms expressed and 
executed in aataflow are controlled by the arrival of ~ata at transformational actors. This can be contrasted 
to control flow environments where the locus of execution is based on an instruction pointer which identifies 
the operation to be performed at any point in time. Dataflow algorithms can be expressed as directed 
graphs where the arcs represent data paths and the nodes represent operations to be performed on the 
data tokens arriving on the incoming arcs. The graph shown in Figure 1 is a oataflow program. ~ m I 
oP1 1 Figure 1 Sample Dataflow Program and Graph The names within the nodes of the graph indicate the 
operation to be performed. The availability of the input tokens and the ability of the output arc(s) 
to receive data are the only conditions which must be satisfied for any operation to take place. The 
act of performing the operation is called firin~ the node and results in the consumption of the input 
tokens and production of output tokens. The node labeled "OPI" has two input arcs associated with it, 
nodes "OP2" and "OP4" have one input arc, node "OP3" has three input arcs. If tokens arrive on both of 
OPl's input arcs and its output arc is empty, then the input tokens will be consumed, the transformation 
OPI will be performed on the data and an output token will be produced. If a token had arrived on the 
input arc to OP2, coincident with the arrival of input tokens to OPI, and OP2's output arcs were empty, 
then both nodes OPI and OP2 could fire simultaneously.  131 Node OP4 must only wait u~ztil OP2has finished 
executing when its output arc is empty in order to fire, whereas node OP3 must wait for both OPl and 
OP2 to complete. Thus, the synchronization of asynchronous processes is accommodated very naturally in 
this particular abstract dataflow model. The simulator facility must, of course, give the user the illusion 
that the parallel, asynchronous activity just described is actually happening. The graph of Figure I 
is surrounded by a dashed line (called the ~rocedure boundary} and given a name, proca, so that it may 
be uses in other dataflow programs. Since only the availability of data on input arcs and the ability 
of the output arcs to take more data is required for the node to fire, parallelism in algorithms is exhibited 
naturally. Substantial interest and research has recently been directed toward the utilization of dataflow 
principles as a basis £or designing computing systems [I, 2, 8, 12] and languages [I, 2, 4], anu as 
a modeling and program ~evelopment tool rIM|. The dataflow simulator system described here, DFSS, consi&#38;ts 
of an extensive set of tools for defining, executing, and metering programs written as aataflow graphs. 
 Simulation Environment Overview  The oD3ective of obtaining a highly generalizes tool which is designed 
to support several of the areas of research mandates that the simulation be performed at a level which 
is based on a comprehensive yet general abstract model of dataflow computation. The identification of 
a simulation nucleus, coupled with the ability to specify architecture, language, and algorithm characteristics 
is the basis of the DFSS simulation environment. DFSS executes the dataflow model of computation at the 
graph level. It should be noted that there are several candidates for abstract dataflow graph models 
[see, for example, 2 and 4]. These abstract models differ on such issues as the characteristics of the 
arcs (i.e. can they be queued or not), the arc/node protocol (e.g. are feedback signals sent along the 
arcs), and the like. A major ob3ective in identifying the simulator nucleus was to be able to accommodate 
a wide variety of the essential characteristics of the various abstract models and thus not be bound 
to any of them. Specific constraints and policies for a particular application of the simulator are introduced 
to the simulation process through descriptor tiles Or special purpose executable procedures. The desire 
to provide for semantic realization of all nodes without binding such operations to the nucleus along 
with the high degree of parallelism and asynchronous activity inherent in the model provide significant 
challenges in the design and implementation of such a simulation facility. Simulation Facility APplicabilit 
Y DFSS is well suited for use in dataflow language research, since it is relatively easy to both realize 
alternative sets of primitive nodes which would define a dataflow language and to define alternative 
synchronization strategies. The researcher using DFSS for these purposes is interested in such items 
as the conciseness and compactness of the algorithms written in the language being evaluated, the degree 
of parallelism offered by a selected set of primitives, the frequency of use of various primitives, etc.. 
Such investigations are assisted by making use of the DFSS node meters, token meters, arc meters, and 
parallelism meters that are to be discussed later. The user of DFSS has the ability to define virtual 
timings for node firing and arc transport as well as the maintenance of a system wide virtual clock to 
control and monitor simulated activities. This feature provides some of the basic requirements needed 
to support investigations examining alternative architectures for executing dataflow programs. In addition 
to this capability, investigators may utilize a nodal unit manager to specify the number and characteristics 
of selected nodes to be used within a simulation run. Researchers evaluating alternative organizations 
are often interested in obtaining measurements on such items as total algorithm realization times, waiting 
times, queue lengths, degree of parallelism, ratio of arc and node timing, counts for used nodes and 
arcs, how the number and availability of nodal units affect throughput, etc. Thus, almost all DFSS meters 
are of potential value but among the most important are the virtual time meters and the parallelism meters. 
 One of the most natural and easily realized objectives of users of DFSS is that of examining the run-time 
characteristics of dataflow programs. For categorization purposes, we call this "algorithm behavior research" 
or "the study of parallelism in dataflow". In either case, the researcher is often interested in characterizing 
the algorithm using measurements such as parallelism obtained, node firings summarized by node classifications, 
and histograms of firing counts by node type. 132 To provide a maximum amount of flexibility fDr the 
simulator user experimenting with various collections of primitive nodes, no nodes have been bound to 
the design and implement&#38;tion of the simulator. Instead, a well defined protocol between the simulator 
and the nodes which it invokes is set forth. In addition, a set of system supplied primitive nodes are 
available for the user who does not have need to experiment with primitive nodes and whose problem solutions 
can be realized with those nodes provided. As a result of the ability to introduce nodes of arbitrary 
semantics into the system as primitives, it is possible to gain substantial efficiencies by replacing 
complex or time consuming dataflow procedures with equivalent primitives. This approach allows one to 
perform top oown refinement and bottom up replacement naturally within DFSS. Writing anq Translating 
Dataflow Procedures for Execution on DFSS ~e prlmary interface to the DFSS nucleus is the representation 
of uataflow programs in a textual equivalence of a dataflow graph. High level languages which produce 
object cove for execution on DFSS produce generateu code in the form of a textual realization of a dataflow 
graph [see, for example, ii|. For those users working oirectly in the graphical programming language 
the first step to running a aataflow procedure on DFSK is transforming the dataflow graph into a textual 
format by labeling the dataflow graph to contain the node identifiers, line ioentifiers, and data typing 
information requires to completely specify the aataflow graph to DFSS. The general procedure is as follows: 
 i) Ensure that the dataflow procedure has been assigned a procedure name. 2) Each input and output 
token line of the procedure must be iuentified with a variable name which is unique within that procedure. 
Also a data type must be specifie~ for each such line. 3) Each node within the procedure must be assigned 
an identifier name which is unique within the procedure. The node realization module or uataflow procedure 
which will accomplish the semantics of a node must be specified for each node. The designated node realization 
module need not exist at translation time but will be dynamically called at runtime by the name specified. 
 After transcribing the dataflow graph into a textual dataflow programming language, the user employs 
a standard system editor to create a source program segment. The source program is then sL, hmitted to 
a dataflow translator which produces the intermediate form which will be executed by DFSS. DFSS has 
been designed to execute in interactive and batch processing environments, and debugging capabilities 
to satisfy both execution modes have been implemented. The goals of good debugging tools require that 
one provide compile-time and run-time diagnostics, and effective run-time fault-finding aids. To facilitate 
batch usage of the debugging facility, the option of specifying a file (or files) as the source from 
which debugging commands are to be obtained is implemented. The objective of providing effective run-time 
fault-finding aids is not as easily achieved. However, a major objectives that has guided the design 
of DFSS has been to allow the debugger to explore as narrow an execution path as possible of the total 
history of the simulation with the intent of locating those operations which transform reasonable tokens 
into invalid results. A simulation run is begun by executing the dfp_run command: "dfp_run <pathname> 
-<arguments>" where <pathname> is the file system pathname of the internal segment produced by the dataflow 
programming language translator. The -<arguments> may be selected from the following list: i) m-node_trace", 
"-nt" = produces a message on user output every time a node firing ~s attempted. The node name and virtual 
time of the firing is provided. 2) "-input_token_trace", "-itt" = produces messages on user_output for 
each input token being input to the nodes being fired. The input line number and token value are provided. 
 3) "-output token trace", "-ott" = produces messages on user_output for each output token being Output 
from the nodes being fired. The output line number and token value are provided. 4) "-all meters", "-am" 
= indicates that all metering accumulation is to be performed during simulation. Meters may be dynamically 
monitored or modified by using df_eval. If any metering indicators are on at simulation termination, 
or have been on during the simulation run, a query is made to the user to  token is created once the 
read from th~ stream is complete. The output facilities of DFSS consist of an automatic mechanism and 
an output node which can be used like other nodes. The automatic output mechanism produmes a token value 
on the user output st.ream when a token reaches the. outer boundary of the outer-most procedure being 
simulated. The output consists of the identifier assigned to the token in the source and the value of 
the token when it reaches the boundary. The other mechanism is the node "stream_output" which is designed 
to accept three input tokens ano to output information to a specified stream. Although thi&#38; node 
does not generate any output tokens, another node "stream_outputwith_sync" is identical to this node 
but produces a boolean output token which may be used for synchronization. Debuggin~ anU Metering Capabilities 
 We use the term "evaluation" to include both debugging anu measurement. The seemingly separate capabilities 
of oeougging ano measurement have been considereO jointly for several reasons. (i) Both objectives require 
the tracking o£ stmulator activities at the elementary operations level, i.e., the ability to recognize 
node firings, token generation and parallelism. For example, the oebugging interface must be able to 
isolate the firing of a particular node so that it may be tested for appropriate output tokens, while 
the metering interface must be able to aetect the same activity because i~ is requirea to count such 
events. Although the reason for detecting and tracking elementary operations may be somewhat different, 
the requirement to oo so exists for both.  (2) The user interface required for both debugging and measurement 
share many of the same types of requests. For example, the ability to stop a simulation run at key points 
and observe the current environment is a characteristic requirement for both interfaces.  (3) Many times 
information obtained while debugging a procedure can indicate what activities should be measured and 
conversely, often time&#38; measurements may obviate areas which should be considered highly suspicious 
of being faulty. For example, if a programmer realizes from the measurement facility that a certain set 
of nodes fired many more times than expected, an undesired looping condition or invalidly  generated 
tokens in that area of the dataflow program may be suspected. Because of the congruenciescited above 
and the implementation efficiencies which have been gained, both the measurement and debugging interface 
have teen realized in a single command interface, "dataflow evaluate" (df_eval). A/though df_eval provides 
a single, common interface for both measttrement and debugging, it is still possible for a user to perform 
the conventional debugging or measurement activities without being concerned, or even aware of the other 
features. Debugging Consideration § Run-time debugging aids have been partitioned into two classes: 
static and dynamic. Static debugging aids have been designed to allow examination of a dataflow procedure 
after it has completed execution and/or while it is running but without being able to alter any action 
of the simulation. Examples of this type of debugging aid includes tracing and post-termination dumping. 
The dynamic debugging aids not only allow displaying, stopping, and conditional stopping, but also provide 
for simulation progress to be altered by allowing the debugger to modify the value of tokens. Examples 
of specific dynamic debugging capabilities include the ability to establish breakpoints at which token 
values and trace control options may be changed. The static aids currently available include the ability 
to trace node firing, procedure calls, and token creations. All trace data may be either displayed or 
directed to a file manipulated by analytic programs. The ability to select and specify more than one 
trace destination provides a mechanism whereby certain subsections of a simulation may be isolated for 
independent analysis. Also included in the category of static run-time aids is the ability to set up 
breakpoints which stop simulation at particular nodes, thus isolating particular areas for investigation. 
When a simulation has stopped at a breakpoint it is possible to display token values, display the source 
and original comments associated with a node and token, and create formatted dumps of the token pool 
or active and candidate node lists. These dumps may be either in display or machine readable format for 
post-simulation analysis. Dynamic debugging aids which provide control over simulation progress include 
the ability to: a) interrupt a simulation and gain  control,  135 b) print values of selected tokens, 
 c) change values of selected tokens, d) set and reset breakpoints, e) set and reset trace options, 
 f) Step through a virtual time cycle of simulation and stop again, g) debug node realization modules, 
 and  h) resume simulation. The df eval command provides symbolic, interactive analysis facilities for 
dataflow procedures. Its features permit a user to interrupt an executing simulation at particular node 
firings, display and modify token values and types, and examine additional internal simulator items. 
 The df eval command is used to examine an active simulation process at points where interpretation has 
been suspended Dy one of the following: I. Breakpoint: the simulation is temporarily halted at a point 
selected by the user and df eval entered automatically. Simulation can De resumed at the point it was 
halted. 2. Error: an error, such as invalid token type or improperly terminated procedure, can interrupt 
simulation. After an appropriate error message is printed, df eval is automatically invoked.  3. ~uit 
signal: a simulation may be stoppeo oy using the break key. A new system command level is established, 
and the user can call df eval to analyze the simulation status.  A procedure to be analyzed with df 
eval must oe in the standard internal--form aetined for the dataflow simulator. Information regarding 
virtual times associated with token creations and node firings is maintained Dy the simulator on a continuing 
basis. When any information is displayed for nodes or tokens, the virtual times associated with these 
items are also displayed. Using df eval requests, a user can set a breakpoint before or after the firing 
of any node. The implementation of a breakpoint consists of setting flags in the simulator's internal 
active node list entry associated with a given node, and on each call to the node (or aataflow procedure), 
the flags are interpreted and appropriate actions are performed. The df eval command may be used either 
before-- the execution of a dataflow simulation or during a simulation run. When invoked before a simulation 
run, it is primarily used to establish and reset breakpoints. The meaningful commands which can be 
used when df_eval is invoked prior to simulation are: after, before, breaks, description, execute, get_command_Beg, 
node, quit, reset, status. A complete list and description of all df eval commands can be found in [5]. 
some of the more important capabilities are described below: meters-display <meter_class> [-proc] [-stream 
<stream_name>] meters -sample <meter class> [-proc] [-stream <stream, name>] meters -start <meter class.> 
[-proc] meters -stop <meter ~lass> [-proe] meters -reset <mete?_class> [-proc] The meters request can 
be used either in a breakpoint command list or at the df eval command level to manipulate metering data. 
When used at the df eval command level while at a simulation breakpoint, the action requested is performed 
immediately. On the other hand, if a meter request is issued as part of a breakpoint command list the 
action will be performed when the breakpoint is taken. By using the meters request with a conditional 
breakpoint it is possible to selectively display, sample, start, stop, or reset the metering data based 
on si~ulato£ conditions. In general, meters axe accumulated in both simulator wide data bases and per 
procedure data bases whenever meters are on. The ahility to manipulate either set of data bases is provided 
by the optional -proc argument as described below. The metering action performed by each type of meters 
command is as follows: -display request displays on the io stream specified by the -stream argument 
an ASCII dump o£ current meters, for the specified meter class. If no meter class is specified all meters 
are displayed. If the stream has not been opened then the file name defaults to be the same as the switch 
name. If the -proc argument is present then only the meters for the current procedure are displayed. 
If no -proc argument is specified the simulator wide meters are displayed. -sample request dumps an 
internally formatted data structure to the io switch specified by the -stream~ argument. The contents 
 136 of the data s.tr~ctuza represents a current snapshot of the selected meter class. If no meter 
class is specified all meters are displayed. If the -proc argument is present then only the meters for 
the current procedure are sampled. If no -proc argument is specified the simulator wide meters are sampled. 
If no -stream argument is specified then "sample_output" is assumed. -start request sets the internal 
switches of the simulator so that metering data is collected on the selected meter class. Ii no meter 
class i~ specified than all n~ters are started. If the -proc argument is present then only the meters 
for the current procedure are started. If no -proc argument is specified the simulator wide meters are 
started. -stop request sets the internal switches of the simulator so that metering data is not collected 
for the selected meter class. If no meter class is specified then all meter s are stopped. If the -proc 
argument is present then only the meters for the current procedure are stopped. If no -proc argument 
is specified the simulator wide meters are stopped. -reset request reinitializes to 0 all meters in 
the specified meter class Dut leaves the internal metering switches unchanged. That is, metering is not 
turned on or off by this request. If no meter class is specified then all meters are reinitialized. If 
the -proc argument is present then only the meters for the current procedure are reset. If no -proc argument 
is specified the simulator wide meters are reset. node configuration {-alter group timing I -~gt} --<group 
number> <new time> node_configuration {-alter op_timing -rot} <operation name> <group number> <new time> 
 node configuration {-alter units I -au} {add I delete} <number of units> <group number> node configuration 
{-alter vtime I -ac} <new vt[me wanted> The use of node configuration provides the_ user wi~h a copy 
of the c~rrent node configuration or allowa for a dynamic alteration. node configuration alone will 
request a copy of the current configuration. -alter_group_timing request will take the integer specified 
by <group number> as the number of the group whose firing time is being altered and <new time> as the 
time to be inserted as the new firing time for that group. -alter_op_timing request will take the character 
string specified by <operation name> as the operation whose fire time is to be changed, and <group number> 
as the group in which that fire time is to change and <new time> as the operation fire time to insert. 
 -alter units request will take the command specified by <function> ("add" or "delete"] and will perform 
that function on <number of units> within the group specified by <group number>. -alter vtime request 
will take the integer specified by <new vtime> (a 0, 1 or 2] as the vtime to be used to determine the 
source of virtual firing times.  node_resource control -on [name of specification file} node resource 
control -off node--resource-control {-operations ---ops} {operation name]  node resource control {-groups 
I -grp} --[group n~mber] node resource control {-re initialize ---reinit}-- The node resource control 
request deals with changing the status of the resource manager use or with the statistics which are kept 
when it is in use. -on request may be used with or without an argument depending on the situation. If 
the node resource manager was previously in use then the -on request may be used without an argument 
to restart use of the node resource manager using the same architecture. If the node resource manager 
is to be initiated or restarted with a new architecture then the argument must be given and it will be 
used as the name of the  137 8. James E. Rumbaugh, "A Parallel Asynchronous Computer Architecture for 
Data Flow Programs", Doctoral Thesis, Massachusetts Institute of Technology, May 1875.  9. Mohammad 
R. Shekarchi, "The Realization of a Simple Operating System in a Data Flow Progr@mming Language", Master's 
Thesis, University of Southwestern Louisiana, August, 1978.  10. Bruce D. Shriver and Steve P. Landry, 
"An Overview of Dataflow Related Research at The University of Southwestern Louisiana", Langages a Machines 
Cadencees par Lea [Muznes, Eds. J,. C. Syre and D. Compte, ONKRA  CERT, Toulouse, France February, 
II~71~. ii. Solecki, M.L., "REDBALL: a Relational Algebra Compiler to Generate Dataflow Programs," Computer 
Science Department, MSc Project, University of Southwestern Louisiana, December, 1978. 12. Eds. J.C. 
Syre and D. Compte, Langages a Machines Cadencees par les Donnes, ONERA CERT, Toulouse, France, February, 
1979.  13. King-Song Weng, "Stream Oriented Computation in Recursive Data Flow S~emas", Masters Thesis, 
Laboratory for Computer Science, Massachusetts Institute of Technology, 1975.   13S  
			