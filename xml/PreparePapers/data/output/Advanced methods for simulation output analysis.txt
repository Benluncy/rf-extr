
 ADVANCED METHODS FOR SIMULATION OUTPUT ANALYSIS Christos Alexopoulos School of Industrial and Systems 
Engineering Georgia Institute of Technology Atlanta, Georgia 30332-0205, U.S.A. ABSTRACT This paper reviews 
statistical methods for analyz­ing output data from computer simulations of single systems. In particular, 
it focuses on the problems of choosing initial conditions and estimating steady­state system parameters. 
The estimation techniques include the replication/deletion approach, the re­generative method, the batch 
means method, the standardized time series method, the autoregressive method, and the spectral estimation 
method. INTRODUCTION The primary purpose of most simulation studies is the approximation of certain system 
parameters with the objective of identifying parameter values that op­timize some system performance 
measures. If some of the input processes driving a simulation are random, then the output data are also 
random and runs of the simulation program only give estimates of system per­formance characteristics. 
Unfortunately, a simulation run does not usually produce independent, identically distributed (i.i.d.) 
observations; therefore classical statistical techniques are not directly applicable to the analysis 
of simulation output. A simulation study consists of several steps such as data collection, coding and 
verification, model valida­tion, experimental design, output data analysis, and implementation. This 
paper focuses on the use of output data for estimating system performance mea­ sures. There are two types 
of simulations with regard to output analysis: 1. Terminating simulations. The termination of a transient 
simulation is caused by the occurrence of an event E. An example is the simulation of a computer network 
until n jobs are completed. 2. Steady-state simulations. The purpose of a steady-state simulation is 
the study of the long-run behavior of the system of interest. A performance measure of a system is called 
a steady-state param­eter if it is a characteristic of the equilibrium dis­tribution of an output stochastic 
process (Law and Kelton 1991). An example is the simulation of a con­tinuously operating communication 
system where the objective is the computation of the mean delay of a data packet. Section 2 discusses 
methods for analyzing output from terminating simulations. Section 3 reviews ap­proaches for removing 
bias due to initial conditions in steady-state simulations. Section 4 presents tech­niques for point 
and interval estimation of steady­state parameters. Finally, section 5 contains conclu­sions and recommendations 
for additional studies by the interested reader. 2 TERMINATING SIMULATIONS We start with the output analysis 
methodology for terminating simulations. Suppose that we simulate a system until n output data X1, X2, 
. . . . Xn are col­lected with the objective of estimating p = E(~~ ), where x. = ~ ~~=1 Xi is the sample 
mean of the data. For example, Xi may be the transit time of unit i through a network of queues or the 
total time station i is busy during the ith hour. Clearly, ~~ is an unbiased estimator for p. Unfortunately, 
the X; s are generally dependent random variables mak­ing the estimation of the variance Var(~n ) a non­trivial 
problem. In many queueing systems the Xi s are positively correlated making the familiar estima-­tor 
S2(n)/n = ~~=l(Xi ~n)2/[n(n 1)] a highly biased estimator of Var(~~). To overcome this problem, one 
can run k in­dependent replications of the system simulation. Assume that run i produces the output data 
 101 X~l)Xi2, ..., Xi.. Then the sample means are i.i. d. random variables, is also an unbiased estimator 
of p, and is an unbiased estimator of Var(~n ). If in addition n and k are sufficiently large, an approximate 
100(1 a) percent confidence interval for p is where td,~ represents the y-quantile of the tdistribu­ 
tion with d degrees of freedom. Law and Kelton (1991) review sequential proce­dures for determining the 
number of replications re­quired to estimate p with a fixed error or precision. Their procedure for obtaining 
an estimate with a rel­ative error l~n pl/lpl < -y and a 100(1 a) percent confidence interval has performed 
well for initial sam­ple size no > 10 and y <0.15. A well-known sequen­tial procedure for constructing 
a 100(1 CY) percent confidence interval for p with a small absolute error IX. -p! < /3 is due to Chow 
and Robbins (1965) (see also Nadas 1969). Law (1980) observed that the procedure is very sensitive to 
the value of P. The method of replications can also be used for estimating performance measures other 
than means. For example, suppose that we want to estimate the p-quantile, say yP, of the maximum queue 
size in a single-server queueing system during a fixed time win­dow. We run k independent replications, 
denote by ~ the maximum observed queue length during repli­cation i, and let Y(l), Y(2), . . . . Y(k) 
be the order statis­tics corresponding to the Yi s. Then a point estimate for yP is if kp is an integer 
 YP= Cw) otherwise { Y( l~P+lJ ) and a confidence interval for yp is described in Welch (1983, pp. 287-288). 
INITIALIZATION PROBLEMS FOR STEADY-STATE SIMULATIONS One of the hardest problems in steady-state simula­tions 
is the removal of the initialization bias. Suppose that {Xi : i ~ 1} is a discrete-time output stochas­tic 
process from a single run of a steady-state sim­ulation with initial conditions 1 and assume that, asn 
+ co, P(Xn < xII) + P(X < z), where is the corresponding steady-state random variable. We consider the 
estimation of the steady-state mean p = liu+~ E(X. [1). The problem with the use of the estimator T. 
for a finite n is that E(X. II) + p. The most commonly used method for eliminating the bias of T. identifies 
a index 1 <1 ~ n 1 and truncates the observations Xl, . . . . Xl. Then the es­ timator X.,1 = ~kxi 
n 1. $=1+1 is generally less biased than ~n because the initial conditions primarily affect data at 
the beginning of a run. Several procedures have been proposed for the detection of a cutoff index 1 (see 
Fishman 1972; Ga­farian, Ancker, and Morisaku 1978; Kelton and Law 1983; Schruben 1982; Schruben, Singh, 
and Tierney 1983; Wilson and Pritsker 1978a, b). The procedure of Kelton (1989) uses a pilot run to estimate 
the steady­state distribution and starts a production run by sam­pling from the estimated distribution. 
More sophis­ticated truncation rules and initialization bias tests have recently been proposed by Chance 
and Schruben (1992), Goldsman, Schruben, and Swain (1993), and Ockerman (1995). We now briefly discuss 
the graphical procedure of Welch (1981, 1983) which is popular due to its sim­plicity and generality. 
Another graphical method has been proposed by Fishman (1978a,b) in con­junction with the batch means 
method (see section 4.3). Welch s method uses k independent replica­tions with the ith replication producing 
observations Xii, X~z, ..., Xin and computes the averages k Fj=~~X~j, j=l, . . ..n. Z=l Then for a given 
time window w, the procedure plots the moving averages against j. If the plot is reasonably smooth, then 
1 is chosen to be the value of j beyond which the sequence of moving averages converges. Otherwise, a 
different time window is chosen and a new plot is drawn. The choice of w is similar to the choice of 
an interval width for a histogram. STEADY-STATE ANALYSIS Several methods have been developed for the 
estima­tion of steady-state system parameters. Below we briefly review these methods and provide the 
inter­ested reader wit h an extensive list of references. We focus on the estimation of the steady-state 
mean p of a discrete-time output process {Xi : i > 1}. Anal­ogous methods for analyzing continuous-time 
output data are described in a variety of texts (Bratley, Fox, and Schrage 1987; Fishman 1978b; Law and 
Kelton 1991). The process {Xi } is called strictly stationary if the joint distribution of Xi+jl, Xi+jz, 
. . . . Xi~j~ is in­dependent of i for all indices jl, jz, . . . . jk. If E(Xi) = p, Var(Xi) < co for 
all i, and the Cov(Xi, Xi+j) = Cj is independent of i, then {X;} is called covariance­stationary. 4.1 
The Replication/Deletion Approach This approach runs k independent replications, each of length n observations, 
and uses the method of Welch (198 1, 1983) to discard the first I observations from each run. One then 
uses the i.i.d. sample means to compute point and interval estimators for the steady-state mean p (see 
section 2). The method is characterized by its simplicity and generality and works well if n and k are 
sufficiently large. 4.2 The Regenerative Method This method assumes the identification of time in­dices 
at which the process {Xi} probabilistically starts over and uses these regeneration epochs for ob­taining 
i.i.d. random variables which can be used for computing point and interval estimates for the mean p. 
The method was proposed by Crane and Igle­hart (1974a, 1974b, 1975) and Fishman (1973, 1974). A complete 
treatment of the regenerative method is given in Crane and Lemoine (1977). More precisely, assume that 
there are (random) time indices 1 s T1 < Tz<... such that the portion {X~, + j, j > O} has the same distribution 
for each i and is independent of the portion prior to time Ti. The portion of the process between two 
successive regeneration epochs is called a cycle. Let Yi = ~~~~, .Xj and Zi = Ti+l Ti fori=l,2, . . 
. and assume that E(Zi) < co. Then the mean p is given by E(YI) ~ ~ Analysis Now suppose that one simulates 
the process {Xi } over n cycles and collects the observations YI, . . . . Ym and Z1, . . ..Zn. Then 77 
 is a strongly consistent, although typically biased for finite n, estimator of p. Furthermore, confidence 
in­ tervals for p can be constructed by using the random variables Yi pZi, i = 1, . . . . n and the 
central limit theorem (see Iglehart 1975). For small sample sizes and bounded Yi and Zi, one can compute 
the confi­ dence interval in Alexopoulos (1993) which provides superior coverage over confidence intervals 
based on the central limit theorem at the expense of increased width. The regenerative method is difficult 
to apply in practice because the majority of simulations have ei­ther no regenerative points or very 
long cycle lengths. A class of systems the regenerative method has suc­cessfully been applied to is inventory 
systems. 4.3 The Batch Means Method The method of batch means is frequently used to es­ timate the steady-state 
mean p or the Var(~n ) and owns its popularity to its simplicity and effective­ ness. Basic references 
on the method are Conway (1963), Fishman (1978a,b), Law and Carson (1979), and Schmeiser (1982). The 
classical approach divides the output xl, ..., X. of a long simulation run into a number of contiguous 
batches and uses the sample means of these batches (or batch means) to produce point and interval estimators. 
To motivate the method, assume temporarily that the process {Xi } is covariance-stationary and split 
the data into k batches, each consisting of b observa­tions (assume n = kb). The ith batch consists of 
the observations X(i l)b+ll x(i l)b+2> ...> xi~ fori=l,2,..., k and the ith batch mean is given by 
b K(b) = ; ~x(i-l)b+j. j=l Since the process is covariance-stationary, it can be shown that Cl(b) ~ 
cov[yi(~), i+i(b)l. is independent of i. Therefore, if the autocovariance function {Cj } is such that 
Cl(b) ~ O aa b increases, we can identify a batch size b for which the batch means Yi(b), i = 1, ..., 
k are approximately i.i.d. nor­ mally distributed. Then we form the grand batch mean estimate the Var[Yi(b)] 
by VB= + &#38;(b) -Yk) , Z=l and compute the approximate 100(1 a) percent con­fidence interval for p 
The main problem with the application of the batch means method in practice is the choice of the batch 
size b. If b is small, the means Y~(b) can be highly cor­related and the resulting confidence interval 
will fre­quently have coverage below the user-specified nom­inal coverage. Alternatively, a large batch 
size will likely result in very few observations and potential problems with the applicability of the 
central limit theorem. An extensive study of batch size effects was conducted by Schmeiser (1982). We 
now focus on consistent estimation batch means methods. These methods assume the existence of a parameter 
az, often referred to aa the variance of the process {Xi }, such that a central limit theorem holds and 
aim at constructing an asymptotically consistent estimator for u2. In the following, N(O, 1) denotes 
the standard normal distribution, < ~ denotes con­vergence in distribution, and a~ denotes conver­gence 
with probability y 1 (see Billingsley 1968). Also, {W(t) : t ~ 0} is the standard Brownian motion pro­cess. 
The limiting result (3) is implied under the fol­lowing assumption. Assumption of Strong Approximation 
(AS A). There exist finite constants p, a >0 and A E (O, 1/2] such that n(xn /.4) = Uw(?z) + o(rw-~) 
W.p. 1. This assumption states that for almost all sample paths, the process {n(~n p)/a} is close to 
a stan­dard Brownian motion. The ASA is not restrictive as it holds under rela­tively weak assumptions 
for stationary ~-mixing pro­cesses, Markov chains and regenerative processes. See Damerdji (1994a) for 
details. Here we should point out that the constant A is closer to 1/2 for processes having little autocorrelation 
while it closer to zero for processes with high autocorrelation, The ASA also implies ~n 4 p as n + cm. 
Now consider sequences of batch sizes {bn : n > 1} and numbers of batches {k. : n ~ 1} such that knbn 
~ n and knbn/n + 1 aa n a co. Assume that the following assumptions are satisfied. (Al) b.+ co and k. 
~ co monotonically as n ~ co (A.2) b~ nl- ~lnn ~ O as n ~ co (A.3) There exists a finite positive integer 
a such that jjbJn)a < co. n=l Damerdji (1994a) shows that, as n e m, and ~n /l d k = -+ (0 1) 5) The 
last display implies that is an asymptotically valid 100(1 a) percent confi­dence interval for p. One 
can easily verify that the batching sequence bn = [no] satisfies assumptions (A.1)-(A.3) if O c (1 2~, 
1). Damerdji (1994b) dis­plays additional limiting properties of the variance estimator VB (n). We now 
elaborate on the following two batching rules. The Fixed Number of Batches (FNB) Rule. Fix the number 
of batches at k. For sample size n, use batch size bn = [n/k]. The Square Root (SQRT) Rule. Choose batch 
sizes b. and number of batches k. so that lim n-+~bn/&#38; = 1 and lim.-wk.lfi = 1. The FNB rule produces 
asymptotically valid con­fidence intervals for p but does not yield an asymp­totically consistent estimator 
for a . The confidence interval has the form (2) with b in the expression for VB replaced by b~. Since 
consistent estimation meth­ods produce confidence intervals whose half-widths have minimal expectation 
and variance (Glynn and Iglehart 1990), the FNB rule will induce confidence intervals that tend to be 
wider than confidence inter­vals resulting from consistent estimation methods. The SQRT rule results 
from a theorem in Chien (1989). Under some additional stationarity as­sumptions on {Xi}, the theorem 
implies that Z~n 1 IV(O, 1) and the convergence will be fastest if bn and Icn grow proportionally to 
@. Unfortu­nately, in practice the SQRT rule tends to seriously underestimate the Var(~n) for fixed n. 
Fishman and Yarberry (1993) studied the estima­tion of the steady-state mean customer waiting time in 
an M/M/1 queue. The coverage rates of the con­fidence intervals produced by the FNB rule converge much 
faster to the nominal coverage than the cover­age rates of the confidence intervals resulting from the 
SQRT rule. Furthermore, this disparity grows with increasing traffic intensity. With the contrasts between 
the FNB and SQRT rules in mind, Fishman and Yarberry (1993) proposed two strategies that dynamically 
shift between the two rules. Both strategies compute confidence intervals at times njSn02~, j=O,l,. .. 
J.J. The LBATCH Strategy. At time nj, if a hypoth­esis test detects autocorrelation between the batch 
means, the bat thing for the next review is determined by the FNB rule. If the test fails to detect correla­tion, 
all future reviews omit the test and employ the SQRT rule. The ABATCH Strategy. If at time nj the b pothesis 
test detects correlation between the batch means, the next review employs the FNB rule. If the test fails 
to detect correlation, the next review em­ploys the SQRT rule. Both strategies LBATCH and ABATCH yield 
ran­ dom sequences of batch sizes. These sequences imply convergence results analogous to (4) and (5) 
(Fish­ man and Yarberry 1993). Preliminary experiments indicate that the more conservative ABATCH strat­ 
egy comes closer to the FNB rule s superior coverage rates with shorter confidence intervals. These fea­ 
tures make it an attractive compromise between the extreme FNB and SQRT rules. We should point out that 
a plot of the batch means is a very useful tool for checking the effects of initial conditions, non-normality 
of batch means, and exis­ tence of correlation between batch means. An interesting variation of the traditional 
batch means method is the method of overlapping batch means proposed by Meketon and Schmeiser (1984). 
For given batch size b, this method uses all n b + 1 overlapping batches to estimate p and Var(Xn). 
The first batch consists of observations Xl, . . . . Xb, the second batch consists of X2, . . . . Xb+l, 
etc. Welch (1987) noted that both traditional batch means and overlapping batch means are special cases 
of spectral estimation (see section 4.6) at frequency O and, more importantly, suggested that overlapping 
batch means yield optimal variance reduction when one forms sub­batches within each batch and applies 
the method to the sub-batches. For example, a batch of size 64 is split into 4 sub-batches and the first 
(overlapping) batch consists of observations Xl, . . . . Xfjq, the sec­ ond consists of observations 
X17, . . . . XSO, etc. Another article in this volume (Sherman 1995) re­views a variety of batching rules 
and tests their effi­ciency. 4.4 The Standardized Time Series Method This method was proposed by Schruben 
(1983). The standardized time series is defined by and, under some mild assumptions (e.g., strict sta­ 
tionarity and ~-mixing), where {1?(t) : t ~ O} is the standard Brownian bridge process (see Billingsley 
1968). Informally, {Xi} is @­mixing if Xi and Xi+j are approximately independent for large j. IfA= J; 
al?(t)dt is the area under B, then the identity E(A2) = a2/12 implies that az can be estimated by multiplying 
an estimator of E(A2) by 12. Suppose that the data xl,. ... Xm are divided into k (contiguous) batches, 
each of size b. Then for sufficiently large n the random variables b A; = ~[(n+ 1)/2 ~]x(~-ly+j, i = 
~,.. .,k j=l become approximately i.i.d. normal and an estimator of E(A2) is Hence an (approximate) 100(1 
 a) percent confidence 4.6 The Spectral Estimation Method interval for p is This method also assumes 
that the process {Xi } is covariance-stationary. Under this assumption, the ~ ~ ~k *tk,l_./2 G % n, variance 
of Xn is given by where VT = 12 E(A2). The standardized time series method is easy to implement and 
has asymptotic advantages over the batch means method (see Goldsman and Schruben 1984). However, in practice 
it can require pro­hibitively long runs as noted by Sargent, Kang, and Goldsman (1992). The theoretical 
foundations of the method are given in Glynn and Iglehart (1990). Ad­ditional developments on the method, 
as well as other standardized time series estimators, are cent ained in Goldsman, Meketon, and Schruben 
(1990) and Golds­ man and Schruben (1984, 1990). Finally, Damerdji (1994a) shows that under the assumption 
of strong approximation in section 4.3, bat thing sequences sat­isfying assumptions (A. 1)-(A.3) yield 
asymptotically consistent estimators for the process variance U2.  4.5 The Autoregressive Method This 
method was developed by Fishman (1978b) and assumes that the output process {Xi } is covariance­stationary 
with mean p and ~JR=_m ICj I < co, and can be represented by the autoregressive model of or­der p P ~bj(xi-j 
P)= ~i, j=tl where b. = 1 and {q} is a sequence of uncorrelated random variables with mean O and variance 
a?. The procedure in Fishman (1978b) determines an order p and computes estimates bj and @ of bj and 
a: respectively. Then for large n an approximate 100(1 a) percent confidence interval for p is where 
and the degrees of freedom are computed from The major difficulty with the use of the autore­gressive 
method is the validity of the autoregressive model. A generalization of the method was proposed by Schriber 
and Andrews (1984). ( n-1 Var(Tn) = ~ CO+ 2~(1 ~/ 7Z)Cj . j=l ) The name of the method is due to the 
fact that if ~~_m lCj I < co, then nVar(~n) ~ 2ng(()) as n a co, where g(~) is the spectrum of the process 
at frequency J and is defined by where i = m. Therefore, for large n the estimation of Var(~n) can be 
viewed as that of estimating g(0). Estimators of this variance have the form S=+o+%w c where p and the 
weights are chosen for improving the properties of the variance estimator Vs. The selection of these 
parameters is discussed in Fishman (1978b) and Law and Kelton (1984). Further discussions of spectral 
methods are given in Heidelberger and Welch (1981a,b, 1983). 4.7 Quantile Est imat ion A variety of 
methods have been proposed for estimat­ing quantiles of steady-state data (see Iglehart 1976; Moore 1980; 
Seila 1982a,b; Heidelberger and Lewis 1984). The methods differ in the way the variance of the sample 
quantile is estimated. It should be men­tioned that quantile estimation is a harder problem than the 
estimation of steady-st ate means. 4.8 Mult ivariate Est imat ion Frequently, the output from a single 
simulation run is used for estimating several system parameters. The estimators of these parameters are 
typically corre­lated. As an example, consider the average customer delays at two stations on a path 
of a queueing net­work. In general, Bonferroni s inequality can be used for computing a conservative 
confidence coefficient for a set of confidence intervals. Indeed, suppose that Di is a 100(1 a) percent 
confidence interval for the pa­rameter pi, i = 1,. ... k. Then k i=l This result can have serious implications 
as for k = 10 and ai = 0.10 the r.h.s. of the above inequal­it y is equal to O. If the overall confidence 
level must be at least 100(1 a) percent, then the ~i s can be chosen so that ~~=1 cq = CY. The existing 
multivari­ate estimation methods include Charnes (1989, 1990, 1991) and Chen and Seila (1987). 5 CONCLUSIONS 
The purpose of this paper is to alert the user on a variety of issues and methodologies related to the 
analysis of output data from a simulation of a single system. Several aspects of output analysis were 
left out such as comparison of systems, design of simu­lation experiments, and variance reduction methods. 
These subjects are treated in a variety of articles in this volume and in several texts including Bratley, 
Fox, and Schrage (1987), Fishman (1978 b), Kleijnen (1974, 1975), and Law and Kelton (1991). ACKNOWLEDGMENTS 
Christos Alexopoulos work was supported by Air Force Office of Scientific Research Grant No. F49620­ 
93-1-43. The author would like to thank George Fish­ man, Dave Goldsman, and Andy Seila for their many 
fruitful discussions. REFERENCES Alexopoulos, C. 1993. Distribution-free confidence intervals for conditional 
probabilities and ratios of expectations. Management Science 40(12):1748­ 1763. Billingsley, P. 1968. 
Convergence of probability mea­sures, Wiley, New York. Bratley, P., B. L. Fox, and L. E. Schrage. 1987. 
A guide to simulation, 2d Ed. Springer-Verlag, New York, New York. Chance, F., and L. W. Schruben. 1992. 
Establish­ing a truncation point in simulation output. Tech­nical Report, School of Operations Research 
and Industrial Engineering, Cornell University, Ithaca, New York. Charnes, J. M. 1989. Statistical analysis 
of multivari­ate discrete-event simulation output. Ph.D. Thesis, Department of Operations and Management 
Sci­ence, University of Minnesota, Minneapolis, Min­nesot a. Charnes, J. M. 1990. Power comparisons for 
the mul­tivariate batch-means method. In Proceedings of the 1990 Winter Simulation Conference, 281 287. 
IEEE, Piscataway, New Jersey. Analysis 107 Charnes, J. M. 1991. Multivariate simulation output analysis. 
In Proceedings of the 1991 Winter Sim­ulation Conference, 187 193. IEEE, Piscataway, New Jersey. Chen, 
R. D., and A. F. Seila. 1987. Multivariate in­ference in stationary simulation using batch means. In 
Proceedings of the 1987 Winter Simulation Con­ference, 302-304. IEEE, Piscataway, New Jersey. Chien, 
C.-H. 1989. Small sample theory for steady state confidence intervals. Technical Report No. 37, Department 
of Operations Research, Stanford University, Palo Alto, California. Chow, Y. S., and H. Robbins. 1965. 
On the asymp­totic theory of fixed-width sequential confidence intervals for the mean. Annals of Mathematical 
Statistics 36:457-462. Conway, R. W. 1963. Some tactical problems in dig­it al simulation. Management 
Science 10 :47 61. Crane, M. A., and D. L. Iglehart. 1974a. Simulating stable stochastic systems, I: 
General multiserver queues. Journal of the ACM 21:103-113. Crane, M. A., and D. L. Iglehart. 1974b. Simulating 
stable stochastic systems, II: Markov chains. t70ur­nal of the ACM 21:114 123. Crane, M. A., and D. L. 
Iglehart. 1975. Simulat­ing stable stochastic systems III: Regenerative pro­cesses and discrete-event 
simulations. Operations Research 23:33-45. Crane, M. A., and A. J. Lemoine. 1977. An intro­duction to 
the regenerative method for simulation anaJysis, Springer-Verlag, New York, New York. Damerdji, H. 1991. 
Strong consistency and other properties of the spectral variance estimator. Man­agement Science 37: 1424 
1440. Damerdji, H. 1994a. Strong consistency of the vari­ance estimator in steady-state simulation output 
analysis. Mathematics of Operations Research 19:494-512. Damerdji, H. 1994b. On the batch means and area 
variance estimators. In Proceedings of the 1994 Winter Simulation Conference, 340-344. IEEE, Piscataway, 
New Jersey. Fishman, G. S. 1972. Bias considerations in simu­lation experiments. Operations Research 
20:785­ 790. Fishman, G. S. 1973. Statistical analysis for queueing simulations. Management Science 20 
:363 369. Fishman, G. S. 1974. Estimation of multiserver queueing simulations. Operations Research 22:72­ 
78. Fishman, G. S. 1978a. Grouping observations in dig­ital simulation. Management Science 24:510 521. 
Fishman, G. S. 1978b. Principles of discrete event simulation, John Wiley, New York, New York. Fishman, 
G. S., and L. S. Yarberry. 1993. An im­plementation of the batch means method. Tech­nical Report UNC/OR/TR/93 
l, Department of Operations Research, University of North Carolina, Chapel Hill, North Carolina. Gafarian, 
A. V., C. J. Ancker, and F. Morisaku. 1978. Evaluation of commonly used rules for detecting steady-state 
in computer simulation. Naval Re­search Logistics Quarterly 25:5 11 529. Glynn, P. W., and D. L. Iglehart. 
1990. Simulation analysis using standardized time series. Mathemat­ics of Operations Research 15: 1 16. 
Goldsman, D., M. Meketon, and L. W. Schruben. 1990. Properties of standardized time series weighted area 
variance estimators. Management Science 36:602-612. Goldsman, D., and L. W. Schruben. 1984. Asymp­totic 
properties of some confidence interval estima­tors for simulation output. Management Science 30:1217-1225. 
Goldsman, D., and L. W. Schruben. 1990. New con­fidence interval estimators using standardized time series. 
Management Science 36:393 397. Goldsman, D., L. W. Schruben, and J. J. Swain. 1993. Tests for transient 
means in simulated time series. Technical Report, School of Industrial and Systems Engineering, Georgia 
Institute of Technol­ ogy, Atlanta, Georgia. Heidelberger, P., and P. A. W. Lewis. 1984. Quan­tile estimation 
in dependent sequences. Operations Research 32:185-209. Heidelberger, P., and P. D. Welch. 1981a. A spectral 
method for confidence interval generation and run length control in simulations. Communications of the 
ACM 24:233-245. Heidelberger, P., and P. D. Welch. 1981b. Adap­tive spectral methods for simulation output 
anal­ysis. IBM Journal of Research and Development 25:860-876. Heidelberger, P., and P. D. Welch. 1983. 
Simula­tion run length control in the presence of an initial transient. Operations Research 31: 1109 
1 144. Iglehart, D. L. 1975. Simulating stable stochastic sys­tems, V: Comparison of ratio estimators. 
Naval Research Logistics Quarterly 22:553-565. Iglehart, D. L. 1976. Simulating stable stochastic sys­tems, 
VI: Quantile estimation. Journal of the ACM 23:347-360. Iglehart, D. L. 1978. The regenerative method 
for simulation analysis. In current Trends in Programming Methodology, Vol. 111, eds. K. M. Chandy, and 
K. M. Yeh, 52 71. Prentice-Hall, En­glewood Cliffs, New Jersey. Kelton, W. D. 1989. Random initialization 
methods in simulation. IIE Transactions 21: 355 367. Kelton, W. D., and A. M. Law. 1983. A new ap­proach 
for dealing with the startup problem in dis­crete event simulation. Naval Research Logistics Quarterly 
30:641-658. Kleijnen, J. P. C. 1974. Statistical techniques in simu­lation, part 1, Marcel Dekker, New 
York, New York. Kleijnen, J. P. C. 1975. Statistical techniques in sim­ulation, part 11, Marcel Dekker, 
New York, New York. Law, A. M. 1980. Statistical analysis of the output data from terminating simulations. 
Naval Research Logistics Quarterly 27:131-143. Law, A. M., and J. S. Carson. 1979. A sequential pro­cedure 
for determining the length of a steady-state simulation. Operations Research 27:10 11 1025. Law, A. M., 
and W. D. Kelton. 1984. Confidence intervals for steady-state simulations, I: A survey of fixed sample 
size procedures. Operations Research 32:1221-1239. Law, A. M., and W. D. Kelton. 1991. Simulation modeJing 
and analysis, 2d Ed. McGraw-Hill, New York. Meketon, M. S., and B. W. Schmeiser. 1984. Over­lapping batch 
means: Something for nothing? In Proceedings of the 1984 Winter Simulation Confer­ence, 227 230. IEEE, 
Piscataway, New Jersey. Moore, L. W, 1980. Quantile estimation in regenera­tive processes. Ph.D. Thesis, 
Curriculum in Oper­ations Research and Systems Analysis, University of North Carolina, Chapel Hill, North 
Carolina. Nadas, A. 1969. An extension of the theorem of Chow and Robbins on sequential confidence inter­vals 
for the mean. Annals of Mathematical Statis­tics 40:667 671. Ockerman, D. H. 1995. Initialization bias 
tests for stationary stochastic processes based upon stan­dardized time series techniques. Ph.D. Thesis, 
School of Industrial and Systems Engineering, Georgia Institute of Technology, Atlanta, Georgia. Sargent, 
R. G., K. Kang, and D. Goldsman. 1992. An investigation of finite-sample behavior of con­fidence interval 
estimators. Operations Research 40:898-913. Schmeiser, B. W. 1982. Batch size effects in the analysis 
of simulation output. Operations Research 30:556-568. Schmeiser, B. W., and W.-M. T. Song. 1993. Opti­mal 
mean-squared-error batch sizes. To appear in Management Science. Schriber, T. J., and R. W. Andrews. 
1984. An ARMA-based confidence interval for the analysis of simulation output. American J. Math. Manage­ 
ment Sci. 4:345 373. the TIMS College of Simulation. He is an Associate Schruben, L. W. 1981. Control 
of initialization bias Editor for IIE Transa ctions and Co-Editor for this in multivariate simulation 
response. Communica­ volume. tions of the ACM 24:246 252. Schruben, L. W. 1982. Detecting initialization 
bias in simulation output. Operations Research 30:569 590. Schruben, L. W. 1983. Confidence interval 
estima­ tion using standardized time series. Operations Re­ search 31:1090 1108. Schruben, L. W., H. 
Singh, and L. Tierney. 1983. Optimal tests for initialization bias in simulation output. Operations Research 
31: 1167 1178. Seila, A. F. 1982a. A batching approach to quantile estimation in regenerative simulations. 
Manage­ ment Science 28:573 581. Seila, A. F. 1982b. Percentile estimation in discrete event simulation. 
Simulation 39: 193 200. Sherman, M. 1995. On batch means in the simulation and statistical communities. 
In Proceedings of the 1995 Winter Simulation Conference, this volume. IEEE, Piscataway, New Jersey. Welch, 
P. D. 1981. On the problem of the initial tran­ sient in steady state simulations. Technical Report, 
IBM Watson Research Center, Yorktown Heights, New York. Welch, P. D. 1983. The statistical analysis of 
simu­ lation results. In The computer performance mod­ eling handbook, ed. S. Lavenberg, 268 328, Aca­ 
demic Press, New York. Welch, P. D. 1987. On the relationship between batch means, overlapping batch 
means and spec­ tral estimation, In Proceedings of the 1987 Win­ ter Simulation Conference, 320 323. 
IEEE, Piscat­ away, New Jersey. Wilson, J. R., and A. A. B. Pritsker. 1978a. A sur­ vey of research on 
the simulation startup problem. Simulation 31:55-58. Wilson, J. R., and A. A. B. Pritsker. 1978b. Evalua­ 
tion of startup policies in simulation experiments. Simulation 31:79-89. AUTHOR BIOGRAPHY CHRISTOS ALEXOPOULOS 
is an Associate Professor in the School of Industrial and Systems En­gineering at the Georgia Institute 
of Technology. He received his Ph.D. in Operations Research from the University of North Carolina at 
Chapel Hill. His re­search interests are in the areas of applied probabil­ityy, statistics, and optimization 
of stochastic systems. His recent work involves problems related to the opti­mal design of telecommunications 
and transportation systems. Dr. Alexopoulos is a member of ORSA and 
			