
 Personalization in Multimodal Interfaces Martin Kurze Deutsche Telekom Laboratories Ernst-Reuter-Platz 
7 10587 Berlin, Germany +49.30.34 97-25 96 martin.kurze@telekom.de ABSTRACT This paper describes a novel 
view to the area of multimodal interfaces from the perspective of personalization. As a position paper 
it analyses properties of multimodality in relation to features of personalization. As a short research 
paper, it describes the context of a newly developed personalization framework and its implementation 
with multimodality in the focus. As an industry paper it presents insights on current research and considerations 
within the telco industry. The main purpose of this paper is to initiate a discussion on the problem 
field.  Categories and Subject Descriptors H.5.1 [Information Interfaces and Presentation (e.g., HCI)]. 
Multimedia Information Systems -Evaluation/methodology. H.5.2 User Interfaces -User-centered design -User 
interface management systems (UIMS).  General Terms Design, Experimentation, Human Factors, Standardization. 
 Keywords Personalization, Multmodality, Adaptation. 1. INTRODUCTION There are an ever growing number 
of applications, interfaces and interface metaphors in the world of today where ubiquitous computing 
is becoming more and more part of reality. Unfortunately this growing number of opportunities for all 
increases the complexity of use to a degree which already begins to exclude more and more people from 
taking advantage of this evolution. In both, research and industry, we observe that people hesitate to 
adopt new interfaces or new devices and modalities because they feel overwhelmed by the sheer number 
of choices and new paradigms offered through new (multimodal) interfaces. To cope with this problem, 
at lest two solutions are discussed today: Permission to make digital or hard copies of all or part of 
this work for personal or classroom use is granted without fee provided that copies are not made or distributed 
for profit or commercial advantage and that copies bear this notice and the full citation on the first 
page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior 
specific permission and/or a fee. ICMI 07 Workshop on Tagging, Mining and Retrieval of Human-Related 
Activity Information, November 15, 2007, Nagoya, Japan Copyright 2007 ACM 978-1-59593-870-1/07/11 $5.00. 
 Standardization might sooner or later lead to a small number of interaction paradigms on how interfaces 
are used under certain modalities.  Adaptation is expected to let the application and/or its user interface 
change according to users needs and expectations.  Both approaches fulfill certain requirements and 
might sooner or later succeed even combined. However, we believe that standardization will not solve 
the problem of new interaction paradigms with new modalities or new combinations of modalities: standards 
need some time to evolve or be defined. On the other hand, adaptation could use knowledge about the individual 
user and change/adapt the user interface according to the actual user s needs. This requires an in depth 
knowledge of the user, his habits and the tasks he performs. In addition adaptation is an action, not 
just a feature. Therefore, the application being adapted must be capable to change the interface according 
to "external" factors. This paper focuses on an application overarching approach to build a user model 
and at the same time proposes an architecture for personalized (intelligent) multimodal interfaces. 
 2. WHAT IS PERSONALIZATION There are a number of different definitions of personalization in research 
and industry (see e.g. [2] and [3] for some examples). In the User-Interface and Usability Area, we usually 
define Personalization as the process of adapting an entity or a system according to some properties 
of a single person. Several of the terms used here demand for further explanations. In the given Context, 
we focus on our working definition and the way we understand and use them in our research work and implementation. 
Personalization is a process. This means that personalization is not just a profile or a set of rules 
but rather a dynamic procedure which constantly derives actions/settings for the target entity/system. 
This process can run automatically on a machine (computer) and thus consumes calculation resources and 
needs data for producing output that makes sense and serves its purpose. Since automated personalization 
influences (adapts) a piece of software that a user works with, we have to explore this piece of software 
in more detail: Personalization can adapt the user interface (e.g. font-size, background colours, number 
of buttons or menu-items) and/or the content (e.g. the place of choice for a weather forecast or the 
favourite music). Of course, there is a wide range of cases/subjects which can be adapted and might belong 
to the user interface as well as to the content, e.g. the format in which a personalized newspaper is 
brought to the user. The actual adaptation is then performed in a way taking into account some knowledge 
about the user, his personal properties, also called the user model or simply the profile. This profile 
might contain general data such as age, gender and visual capabilities/limitations. The profile might 
on the other hand contain information derived from tracked usage behaviour or content preferences or 
even self-defined needs. These properties are then used to generate recommendations for the interactive 
application (or for the user himself). Finally, personalization in its literal meaning refers to a single 
person, thus there will not be two identical recommendations for any two persons in the user group. This 
is the theory. In practice, users often can be clustered. Sometimes a stereotype, i.e. a hypothetical 
prototypical user will serve the purpose of personalization very well. In these cases adaptation is what 
we call good enough . We will see below how this influences the art of personalization in our context 
of multimodal applications. There are a few general questions, an application designer should ask himself 
(and decide upon later on) when introducing a personalization component into the overall design of a 
given application or system. The first (and most important) question would be: Who should adapt, the 
system or the user? (or both). As described above, we propose to allow both types of personalisation. 
We call them explicit personalization (when the user actively adapts certain settings) or implicit personalization 
(when the system derives recommended adaptations). As we will see below, it is crucial to keep the user 
in control and let the system only act as a kind of recommender/assistant who can alter certain settings 
but always can be overwritten by the user. This way at least the subjective feeling of the user will 
remain positive. (see also [3] for details) The perception of control is under any circumstances essential 
for the user acceptance. However, user acceptance is not he only reason to introduce personalization. 
E.g. in shopping systems, shopkeepers try to offer (and sell!) goods that meet the users/customers needs 
but at the same time guarantee some profit for the shopping service. In fact profit remains the main 
reason for introducing any kind of functionality including personalization. Often profit and user benefit 
are contradictions and thus cannot be combined. This paper does not examine this difficult question but 
rather concentrates on technical approaches to raise user satisfaction and/or task performance through 
personalization. The present section described the general approach and constraints of personalization 
in interactive applications. Since multimodal applications are by definition interactive , the findings 
above should also apply to this class of applications. As we will describe in the following section, 
multimodality contains both, challenges and opportunities for personalization. In fact there has been 
a certain hype when multimodality become available: People believed that now every one can use everey 
application with his or her preferred modality and thus raise performance and user satisfaction at the 
same time. It was widely expected that personalization systems would (explicitly or implicitly) enable 
this type of automatic adaptation to user needs . In reality, we have seen a lot of different approaches 
with limited success and different results. This paper offers a new approach: offering an overarching 
personalization tool that acts independently from individual applications. The following sections will 
go into details here.  3. WHAT MAKES PERSONALIZATION SO TRICKY IN MULTIMODAL INTERFACES? As described 
above, personalization relies on knowledge about the user and his habits, a user model built from this 
knowledge and the ability to alter certain properties of the interactive application (the interface and/or 
the content). Multimodality adds complexity to all these factors. This has been examined to a certain 
degree in a special context (applications in museums [1]), but a more general approach toward this field 
of application is still missing. We try to address the issue by conceptual and practical way. Input through 
new modalities/devices must be interpreted to perform the intended task (the main objective for the performed 
action). Today it still causes problems to get and interpret all this input. For example, personalization 
may require additional effort if the circumstances and additional features of the interaction must be 
tracked. In addition, the multi in multi-modal makes an interpretation for personalization purposes difficult: 
The personalization system must detect which modality was used (together with which others) and deduct 
the reason why the user acted in the described way. On the other hand, multimodality offers the user 
a richer choice to select the tools, modalities and methods to perform his task. Thus we can assume that 
the number of different interaction-and content-preference-profiles will be much higher in multimodal 
systems. Also the distances of the individual profiles in a multi dimensional (multimodal) interaction-paradigm 
space are probably much higher than in regular application scenarios. The same observation can be made 
in the user model context: a multimodal user model is more complex but also richer and more powerful 
than a conventional one. In particular an application overarching personalization engine faces enormous 
challenges: The engine must decide whether a particular action in a given modality was performed because 
the user preferred to use this modality or because he had no choice (since he was hindered by the context 
or the application). Therefore building a user model becomes more complex. Also using/evaluating the 
user model and the available adaptation mechanisms is different: In general, a personalization engine 
creates recommendations which then are interpreted by the user or (more often) by the application being 
personalized. For example, a personalization engine for multimodal applications/interfaces has to guess 
why a user never used a certain modality in a given context: Either user really does not want to use 
it here or the user does not know that the other modality would help him. So the personalization engine 
has to make a decision on an incomplete data basis.  Figure 1: Architecture of the Personalization Framework 
with affected internal and external components The situation gets even more complicated in the case of 
application overarching personalization: Since until today, there is no generally accepted standard for 
multimodal interaction paradigms; thus, every application defines them at will. What should a personalization 
engine recommend in such cases? Always use the preferred modality for a similar task (e.g. undo is always 
linked to a gesture, but not to the same)? Or recommend the best way to do certain things depending on 
the application?  4. T-LABS PERSONALIZATION FRAMEWORK APPROACH 4.1 General Approach and Overview Building 
on the basis of the general characteristics of personalization, T-Labs implemented an application overarching 
personalization framework which aims to offer the maximum flexibility combined with best possible personalization 
effects. Highly modular architecture enables several types of personalization and can cope with virtually 
all types of content. Figure 1 (???) gives an overview on the architecture of our framework and it s 
relation to the applications. The most prominent key modules of the framework are the sensors which collect 
information on the user s interactions, the user model (profile database) which is implemented as centralized 
homogeneous storage and management unit and the classification and recommendation engine. Our personalization 
framework supports explicit personalization via a dedicated user interface and implicit personalization 
via the sensors mentioned above. Even though it was originally designed for GUI-oriented applications, 
we found that the general approach and systems architecture make it suitable for non GUIÂ­applications 
as well. Speech as well as multimodal interfaces can be enhanced with the recommendations of our PF and 
in many cases can contribute the user model. Let s look at an example: A general property of a user is 
his age, let s assume he is 65 years old. This information was derived from the explicit personalization 
step and might lead to the assumption that his visual capabilities are reduced. However, examining the 
font-preferences in his usage of applications as spreadsheets, word processors etc. (he prefers small 
fonts and zoom-factors for the view), the system derived that he has no visual limitations. On the other 
hand, he usually pumps up the volume of his IP-phone application, thus we can assume that he has certain 
hearing conditions. As a result, when opening a movie-player application, the personalization framework 
might raise the volume level a bit but would keep the window size untouched, even it is quite small. 
However, if the PF has reasons to believe that the user does not know that windows can be resized, it 
might offer a respective hint (to the player application or even to the user). The example above shows 
how explicit and implicit personalization work together and allow a cross-application personalization 
of a (quite simple) multimodal scenario. We are currently developing some more scenarios and use cases 
focussing on multimodality (see below, section Conclusion and Next Steps). Currently we approach the 
ambitious multimodality level of personalization by testing and enhancing our framework with various 
applications using a single modality (but not the same modality).  4.2 First results and observations 
Our personalization framework is implemented as such in a prototypical state. Sample applications for 
personalization have been chosen and to a large degree implemented: A widget-based Whiteboard allows 
explicit personalization of numerous properties concerning the interface and the content of the whiteboard 
application. This is an advanced GUI, using the visual modality.  An application for personalized newspaper 
production and presentation was generated, supporting implicit personalization: the newspaper is composed 
of articles that were selected depending on earlier reading habits. In addition we intend to support 
and audio interface to this application: The news are read to the user and he can (optionally) control 
the application via voice (use case: a commuter who wants to read the newspaper driving to work in his 
car)  Looking at another modality (audio/video and a little haptics), we are currently implementing 
a multimodal electronic program guide for an advanced home scenario. The guide s user interface (supporting 
the use of advanced remote control units with haptic (tilt etc) control is currently redesigned to make 
use of the profile knowledge we collected in the personalization framework.  Initial observations show 
that the framework produces valid and valuable results for the scenarios described above and even across 
different scenarios. Still, there is no proof that multimodal interfaces can benefit from our PF. This 
formal and/or practical proof will have to be done in the next steps, sketched in the subsequent chapter. 
 5. CONCLUSION AND POSSIBLE NEXT STEPS: 5.1 Exploiting the Presentation Framework to Advance Multimodal 
Interfaces Multimodality and Personalization are the two factors that will probably boost the development 
of human computer interfaces in the coming years. Even though they appear to address totally different 
aspects of the user interface and interaction paradigms, a closer examination shows that both have relevant 
interfering properties. Multimodality adds complexity to personalization but also opens up new opportunities: 
By collecting more information about user habits, the user model gets much richer and more powerful. 
Thus the personalization engine can recommend more precisely. Since these recommendations are targeted 
to applications in most cases (and not to the user directly which is of course also possible), multimodal 
applications have more possibilities to adapt their interfaces and/or the presented content. Our conceptual 
research and first experiments show that personalization is doable in multimodal applications and benefits 
very much from an application overarching approach. The experience also shows that the objective and 
approach would benefit very much from certain standards in terms of multimodality and also in terms of 
personalization. Currently we have to touch most applications directly to personalize them (implanting 
sensors and actuators). Future applications might offer an open interface to allow personalization engines 
to read interaction logs/profiles and write recommendations. In particular multimodal applications and 
their interfaces will benefit from a more advanced personalization paradigm and framework since the human 
user is a multimodal entity and expects to be treated holistically as such and still feel most comfortable 
with his or her needs and preferences. This also includes the general objective of interactive systems: 
Provide the user with the most pleasant experience of use. 5.2 Next Steps We will follow this general 
approach and elaborate it further. To prove the statements, assumptions and concepts described above, 
we will continue implementing our personalization framework and applying it to various applications, 
emphasizing multimodal applications. In parallel, we seek for comparable approaches and other research 
activities in the field to compare approaches and possibly collaborate on future multimodal personalization 
engines.  6. ACKNOWLEDGMENTS This work was inspired and to a large degree funded and supported by Deutsche 
Telekom Laboratories. The implementation was done by Alexander Korth, Leonard Hennig and Till Plumbaum 
from the Technical University of Berlin. Pointers to special aspects of multimodal interfaces have been 
contributed by the UTI and MultiMod projects of T-Labs.  7. REFERENCES [1] Bowen, J.P. and Filippini-Fantoni, 
S., Personalization and the Web from a Museum Perspective. In David Bearman and Jennifer Trant (eds.), 
Museums and the Web 2004: Selected Papers, Arlington, Virginia, USA, 31 March 3 April 2004. Archives 
&#38; Museum Informatics, pages 63 78, 2004. [2] Peter Brusilovsky, Alfred Kobsa, Wolfgang Nejdl (eds.) 
The Adaptive Web: Methods and Strategies of Web Personalization; Springer, Berlin Heidelberg New York, 
2007. [3] Privacy Enhanced Personalization (proceedings of the PEP workshop, Montreal, Canada, April 
22nd, 2006  
			