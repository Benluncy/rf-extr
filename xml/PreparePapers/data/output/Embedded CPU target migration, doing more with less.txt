
 Embedded CPU Target Migration, Doing More With Less Robert Greene George Lownes Martin Marietta Communications 
Systems Camden, lJJ 08102 ABSTRACT Today s technology is changing and improving at such a rapid rate 
that even the most state of the art systems seem to be obsolete before completion. Sometimes, changing 
customer requirements dictate that a smaller, faster, and more powerful system be produced to perform 
the same tasks. Target migration, the transfer of a software product from one target system to another 
more powerful target system, is one cost effective way to upgrade system performance. Target migration 
requires careful analysis to determine the impact of the new target environment on system performance. 
In many cases, the goal of target migration is to improve processor loading and throughpu~ while reducing 
memory requirements. A specific methodology needs to be established that will enable the design team 
to perform the required analysis and then be able to identify and measure the improvements on the system 
performance and memory usage. This methodology must be established early by the design team and then 
followed through all phases of the target migration effort. This paper describes the methodology used 
by a Martin Marietta (formerly GE Aerospace) design team that was tasked to determine the impact of a 
target migration to the MIL-STD-1750A Architecture. The methodology is divided into three phases. The 
steps to be performed in each phase are deseribed and examples are provided from an actual project. The 
lessons learned from our experiences with this new methodology will benefit other daign teams tasked 
with migrating existing software products to smaller but more powerful target systems. Permission to 
copy without fee all or pmt of this material is granted pmwded that the copws are nc4 made or d&#38;nbuted 
for direct commercial advantage, the ACM copyright notice and the title of the publication and Its date 
appear, and notice is gwen that copying IS by permission of the Association for Computing Machinery. 
To copy otherwise or republish, requires a f= and/or spedc pemumon. @1994 ACM 0-89791-6662/94/0011-0429 
3.50 1.0 INTRODUCTION 1.1 Project Background Martin Marietta (MM) in Camden, NJ successfully designed, 
developed and formally tested a prototype Ada­based real-time embedded communications system under a 
contract to Boeing Aerospace Corp.. After evaluation of the prototype, a follow-on contract was awarded 
which required the MM design team to develop a new design to substantially reduce the processing time 
on several critical paths and to reduce processor loading. Because the customer s primary goal was to 
improve system performance, additional processing power was required. This could be accomplished by either 
adding additional processors to the existing system to distribute the processing load or by nxkxsigning 
the hardware and software environment to support a more powerful CPU, while possibly reducing the number 
of processors needed in the system. The second of these two approaches was chosen, identifying the MIL-STD-1750A 
Architecture as the new target environment. A MM engineering goal was to reduce processor loading to 
below 50% during average and peak processing conditions. The resultant design replaced six INTEL 80C86 
microprocessors with two Rockwell RI 1750AB microprocessors. The processing time for all the critical 
paths was reduced as required and the processing load was reduced to less than 50%. 1.2 Need for a Methodology 
Repackaging an existing software product to meet improved performance requirements in a new target environment 
mx#res a non-traditional design methodology. Even though the existing software product was not broken, 
we needed to develop a methodology that would enable us to fix it by making it perform the same functions 
faster. l%e new methodology enabled the design team to be able to determine how much performance gain 
was needed, which MIL-STD-1750A implementation to choose, the number of processors needed in the system, 
the allocation of software requirements to the processors, and how to measure the expected system performance 
gain and memory requirements for each processor.  2.0 TARGET MIGRATION METHODOLOGY OVERVIEW 2.1 Goal 
of the Target Migration Methodology The goal of establishing and following a methodology was to ensure 
that all salient issues involved in a target migration would be analyzed, that the appropriate functions 
and resources would be properly allocated, and that the resultant hardware and software design would 
satisfy the system requirements. The target migration methodology is designed to generate much faster 
development and higher quality results than the traditional development methodologies. It is based on 
the use of integrated tools, small teams of highly trained and motivated personnel, and iterative analysis/design 
processes that are designed to identify and remove obstacles to porting software to a new target environment. 
 2.2 Selection of a Design Team The new design was achieved utilizing a self-directed multi-disciplined 
design team. The design team was responsible for establishing the methodology used to guide the target 
migration design effort and for ensuring that the design goals were achieved. The design team was selected 
based on their project knowledge, complementary skills (hardware, software, systems) and compatible personalities. 
The team was empowered to actively participate in the decision making process and to be prepared to accept 
responsibility for the results achieved.  2.3 Major Phases in the Target Migration To insure the thoroughnws 
of the methodology, the self-directed design team fiit considered the general phases which had to be 
accomplished to complete the target migration. This led to the identification of three main phases which 
encompassed the various steps in the methodology, as shown by Figure 1.  In the first phase, Identification 
of System Deficiencies (Identify Needs), the goal is to determine where the existing system lacks performance 
and what alternatives are available. l%is phase contains two distinct steps. The first involves the collection 
of detailed performance data on the existing system. l%e second involves the identification of an alternative 
target envirorunent to enhance system performance. Upon completion of the f~st phase, the design team 
progresses to the second phase of the methodology, the Design Process. This phase consists of four steps. 
The first is to identify the major software requirements and all processing functions needed to accomplish 
each requirement. Once this is completed, the next two steps involve comprising relative timelines of 
the processing functions for each major software requirement, and compressing these timelines for the 
purpose of grouping similar functions and minimizing concurrent processing. The last step is to determine 
the number of processors needed in the new target environment, using the timelines as a guide for allocation 
of the processing functions to processors. Doing More With Less The last phase of the methodology is 
to perform Design Verification. This phase is the most important of the three phases, since the evaluation 
of the new system determines if the new target environment meets the requirements. This phase consists 
of the last three steps in the target migration methodology. The f~st is to perform benchmarks in the 
new target environment. Once benchmark data is collected, the next step is to perform loading and throughput 
analysis on the new design. The final step is to make an evaluation of the new design to determine if 
the system deficiencies identified in the first phasehave beenaddressed. Figure 1 shows the target migration 
methodology as a waterfall chart. The steps are shown as being grouped by methodology phase. The diagram 
illustrates how each step of the methodology flows into the next. The target migration methodology is 
an iterative process. If the Design Veritlcation phase shows that the identifkd system deficiencies have 
not been addressed, then it may be necessary to proceed back to the Design Process phase, to perform 
a reallocation of processing functions to processors. In some cases the new target environment may not 
be adequate to meet requirements, and it may be necessary to proceed back to the first phase of the methodology, 
to identify an alternative target environment  2.4 Steps in the Target Migration Methodology The steps 
of each phase of the target migration methodology are briefly described below. .. m D-cles P~ w Collect 
Performance Data on the Existing System -this step provides a performance evaluation of the existing 
system and defines the basis for which any new design is analyzed. Identifi New Target Environment -this 
step involves the determination of the new target environment, based on expected performance, customer 
requirements, and availability of software tools. Deskzn hCt%SS Ph&#38;  Zdentifi Major Software Requirements 
-this step involves the identification of major software requirements, along with the breakdown of each 
into processing functions within the requirement.  Comprise Relative Timeline of Processing Functions 
­this step involves graphically depicting the relative sequence of the processing functions in relation 
to eaeh other.  9 Compress Relative Timelines -this step involves compressing the relative timelinea 
into compressed timelines which group similar functions and minimize concurrent processing. Allocate 
Processors -this step involves making a determination of the optimum number of processors needed to meet 
all major requirements, using the timelines as a guide. 430 Figure 1. Target Migration Methodology Waterfall 
Chart Perform Benchmarks -this step involves the perfcmnance of benchmark testing to ver@ the actual 
processing improvement of the new target CWhOMWIIL Pe~orm Loading and Throughput Analysis -this step 
involves the caicuktion of processor bading and throughput analysis fa each pweasor. Petiorm Evaluation 
dNew Deshzn -this ateu involves peribrming an evaluatkn ofthe &#38;w target en ~nt based onthe pmxasor 
loading and throughput analysis and to determine if the system deficiencies have been Thetiugetn@ation 
mcdmdobgywas &#38;vebpedas aresult of theneed toanalyzc atarget migration on aa actual projca oncethetargetmigratioa 
mcthodobgyhd been developed, the next task was to apply the methodology to the existing systcm.  3.0 
DESIGN METHODOLOGY IMPLEMENTATION 3.1 Identify System Deficiencies The first phase of the target migration 
methodology, Identifkation of System Deficiencies, is necessary to Wermine Where theexisting syatemdocs 
not meet performance requirements and rhen to determine design altcznativea to enhanw performance. Thisphase 
consists of the fnt two stepa of the design methodology, Cok.tion ofperformance syatcmand dataontheexisdng 
idcat&#38;ation ofanewtargc4envhmnenL 3.1.1 Collect Performance Data on the Existing System Collection 
of performance data of the existing system providca the metrics by which to evaIuate system F-ormance. 
Thestepisneumuy to identify where a system lacks performance and which areas of the S)%tCm need improvement. 
Another goal is to establish bmharks by which to evaluate any new dwign effot his StCJl in the mdhOdOlOgy 
is XW@iShd by f-identifying the criteria for system performance memmmem theapafollning thoee masumnats. 
Two main criteria for measuring the performance of software systcxnsarepromsor badingandpoccssa throughput 
Promsor loading is calculated by identif@g a givcllamount oftime, CXwindow, thatrhesystemhasto -pbte 
a function, then IMsuringthe actual amountof fimethatthe proccam was executing the sotlware which performs 
the function. The processor loading is the Percc4Moftime thepmesaor waaexccuting theaoftware function 
out of the entire time window. In reality, proceWU loading measmwnenrs arc 0ftc41 more complex thanthia. 
All backgrolmd taaksand interrupt proc=dng being perfonncd during the rime window must be taken into 
account. Avczage bading ovcz large windows and peak loading ovcf smaller, more critical windows must 
be Compild fer au pmmaora. If ovaall average loading ia high, then that ia an indication that the existing 
target is not *lcient to meet the rcquiremcmts. If peak loading ia high in only some windows, then the 
software performing the processing during these windows may need to be redesigned to make the existing 
target acceptable. A good engineering goal for processor loading is 50%. This leaves reserve processing 
power in a system to accommodate changes in requirements without changing the target environment. In 
cases where average or peak loading exceeds 100%, or the time that a software function takes to complete 
its processing exceeds the window for that function, the second criteria for measuring performance, processing 
throughput, becomes a standard by which to evaluate the system. In some cases, a processing window is 
defined to be as short as necessary to complete a function. This means that processor loading will always 
be 100%, in order to perform a task as quickly as possible. The method to determine if the throughput 
requirements have been met is to measure the time to perform the function and compare that time against 
the throughput requirement for that function. Once a complete system evaluation has been performed, it 
can be determined if the existing target environment is acceptable or must be improved. Assuming that 
the software metrics show overall processor loading to be too high, or that throughput requirements cannot 
k met with software redesign, then a new target environment must be considered. In the case of the MM­developed 
prototype communications equipmenb analysis of the system showed that both average and peak processor 
loading exceeded the desired engineering goal of 50%. In addition, several throughput requirements needtxi 
to be cut by as much as a factor of three. This analysis led to the decision for a target migration. 
The next step of the methodology involves the choice of the new target environment 3.1.2 Identify New 
Target Environment Identification of a new target environment depends on many factors. The foremost 
reason for selecting a new target is that the existing target can not meet performance requirements. 
The most direct way to solve this problem is to select a more powerful CPU. There are, however, other 
important factors which affect target selection. (e.g. power dissipation, hardening requirements, I/O 
bandwidth, etc.) The purpose of this step of the methodology is to consider all significant issues of 
identifying a new target. Thexe are no hard and fast rules in choosing a new target other than that of 
increased processing power. Other factors depend on program-specific needs, such as customer hardware 
requirements and availability of software development tools for that target environment. In the case 
of the MM project in which this methodology was applied, four main factors were involved in selecting 
the Rockwell RI 1750AB CPU. These were the expected performance improvement customer radiation hardness 
requirements, preferred vendor requirements, and 432 availability of software tools. The analysis of 
the MM prototype had shown that a new target would have to provide a CPU throughput improvement which 
was three times that of the existing system. The Rockwell 1750 target environment exceeded this requiremen~ 
providing an instruction per second rate of about five times that of the existing Intel 8086 target environment. 
Another key factor in choosing the RI 1750 was that it met radiation hardness requirements im:posed by 
the customer. In addition, Rockwell had been selected by the customer as the preferred vendor for the 
entire program, of which the MM-developed communications equipment was only a part. Even though these 
were the main factors in selecting the RI 1750 target, the availability of software tools for the 1750 
was an issue also considered. Before the decision to go with the 1750 had become final, the issues of 
an Ada cross compiler for the 1750 and In-Circuit-Emulation (ICE) capability had to be researched. It 
was determined that three Ada compiler vendors could meet program requirements, and that the ICE capability 
for the Rockwell 1750 was under development. The RI 1750 target environment met the key requirements 
for a new targe~ and the next phase of the methodology, the Design Process phase, was initiated. 3.2 
Design Process The Design Process phase consists of the next four steps of the design methodology. These 
steps involve (1) identification of major software requirements, breaking down each into major processing 
functions; (2) the ordering of processing functions within a requirement on relative timelines; (3) the 
consolidation of the relative timelines into compressed timelines, grouping related processing functions; 
and (4) the determination of the number of processors needed to provide the perfomuuxx to satisfy all 
requirements. To&#38;her, these four steps provide the system architecture under the new target environment. 
3.2.1 Identify Major Software Requirements The identification of the major software requirements is intended 
to begin the target migration redesign effort from the requirements analysis phase. This step is naessary 
to provide the opportunity to consider alternative software requirements allocation and hardware/sofhvare 
design from that of the existing sYs@m* This step is performed just as software requirements analysis 
should normally be perfomwd, but the analysis is done from the standpoint of a different target environment. 
With a new target environment, designers may want to break down major software requirements differently 
than in the existing system. Each major requirement should be broken down into processing functions performed 
to satisfy the requirement. Processing functions should be broad enough to provide a manageable number 
of functions, but not so broad that the functions encompass different kinds of processing. The reason 
for this is to simplify the later methodology steps of comprising dative and compressed timelines. Embedded 
CPU Target Migration, Doing More With Less An example from the MM-developed communications equipment 
demonstrates how this step is performed. The prototype communications equipment was designed to communicate 
on either one of three UHF networks or one of two VHF networks. The equipment was required to handle 
all protocol control and network timing functions for all the networks, perform data encoding and decoding, 
as well as perform all modulation and demodulation of messages on the proper radio frequencies. Therefore, 
major requirements for the system involve message reception and transmission on all UHF and VHF networks. 
A generic example of a major requirement could be given as Receive Message and Transmit Response. This 
major requirement is then broken down into processing functions, as follows: (1) Setup radio for message 
reception, (2) Demodulate message, (3) Decode message, (4) Analyze message and determine response, (5) 
Setup radio for response transmission, (6) Encode response, and (7) Modulate response. The functions 
listed are broad, but encompass similar kinds of processing. This process of breaking out the major requirements 
must be completed for all major software requirements. When all requirements have been identified and 
broken down into processing functions, the next step in the methodology, comprise relative timelincs, 
is completed for all the major software requirements. 3.2.2 Comprise Relative Timeline of Processing 
Functions The relative timelines provide a graphic means by which to view all processing functions within 
a major software requirement and to show how these functions are performed in dation to each other. This 
step is needed to show how the functions are arranged to satisfy the requhement and to show concurrency 
of fimctions. This step is performed by determining the order in which the processing functions occur, 
then drawing the functions out on a relative timeline of events. In cases where one function causes another, 
these two functions would be laid out on the timeline one after the another. In the case where a requirement 
has critical timing, some fimctions maybe shown as occurring in parallel. Extending the example given 
in the previous step, the processing functions in the Rtxeive Message and Transmit Response major requirement 
are laid out in Figure 2. This figme depicts that each processing function causes the next. However, 
in the case of functions 3 and 7, these functions are caused by the previous function, but also occur 
concurrently with the previous function. This is due to the fact that the requirement has critieat timing. 
The net result is a relative timeline which shows all functions laid out in a manner which satisfies 
the requirements as quickly as possible. The relative timelines must be completed for all major software 
requirements, The next step involves using the relative timelines to produce compressed timelines.  
  3.2.3 Compress Relative Timelines The compressed timelines provide a gmphic means to show a logical 
grouping of processing functions within a major software requirement on their own timelines. This step 
is needed to show which functions cart be grouped together, with the intent of grouping similar functions, 
while minimizing processing concurrency on the compressed timelines. This step is performed by placing 
the overlapping processing functions from the relative timeline on separate compressed timelincs. The 
example illustrating this step shows that there are overlapping functions, and assumes that the processing 
load of performing those functions does not allow the functions to be performed in parallel on the same 
CPU, even in a new target environment providing additional processing power. Extending the Receive Message 
and Transmit Response example through this step, there are two basic types of processing functions; control 
functions and signal processing functions, as seen in Figure 3. The figure shows the con~ol functions 
on the first compressed timeline and the signal processing functions on the second compressed timeline. 
Neither compressed timeline shows overlapping functions. The compressed timelines must also be completed 
for all major software requirements. The next step involves using the compressed timelines to determine 
the number of processors needed under the new target environment. 3.2.4 Allocate Processors The allocation 
of processors involves choosing the optimum number of processors needed to meet the major software requirements, 
then allocating the processing functions to processors. This step is necessary to determine the hardware 
environment needed to support the new target and to perform an evaluation of that new target environment 
The step is performed by using the compressed timelines in determining the number of processors needed 
to adequately perform all processing functions. In general, the processing fimctions grouped together 
on a compressed timeline require a dedicated processor. A compressed timeline contains similar processing 
functions, maximizing processing capability, but, minimizing processing concurrency. In cases where processing 
load is relatively low on two separate compressed timelines, then the processing functions on those compressed 
timelines may be both allocated to a single processor. Concurrent 1.Setup Receive t I 2. Demodulate Message 
I 3. Decode Message tI 4. Analyze Message 5. Setup Transmit 6. Encode Response 7. Modulate Response 
  I II Figure 2. Receive Message and Transmit Response Relative Timeline Control ~ **WV .. V/w. .+wJ.v.%..V.uv.v..+.6./.. 
V,W,v ..VJ.+.. W Functions h 3 Signal Processing Functions Figure 3. Receive Message and Transmit Response 
Compressed Timeline processing in a processor is not a problem if overall processor loading is determined 
to be low. If the determined numl!er of processors is more than one, then the interfaces between the 
processors may have an impact on the allocation of processing functions to processors, especially if 
that allocation requires large data transfers between processors. The type of interfaces should be identified. 
The bandwidth of interfaces should complement the increased processing power of the new target environment. 
If interfaces are relatively slow, then increased processor power is defeated. Interface handling should 
also be considered. Incmsed software handling of an interface will increase processor loading over that 
of the processing functions already being performed by the processor. In general, the fewer number of 
processors needed to handle all processing functions reduces the interpmcessor interface requirements. 
The previous example in Figure 3 shows two compressed timelines, with control processing functions on 
the fwst and signal processing functions on the second. It was known that the control processing functions 
were relatively normal to low processing loading and that the signal processing was very high in processing 
loading. The control functions and the signal processing functions could not be handled in parallel on 
the same CPU, even in the new targel environment. Ibis resulted in the need for a multiprocessor system, 
using two RI 1750 microprocessors as the baseline. When making these allocations of processing functions 
to processors, the compressed timelines for all major software requirements must be considered. These 
allocations and the number of processors must be consistent for all major requirements. Once a baseline 
design is established under the new target environmen~ the last phase of the methodology, Design Verification, 
is performed. 3.3 Design Verification The Design Verification phase consists of the last three steps 
in the methodology. These steps determine if the new target environment meets the system performance 
requirements. The three steps in evaluating the new target involve (1) benchmark testing of the new target 
environment (2) a loading and throughput analysis of each processor in the new environment; and (3) the 
determination whether the new design meets the requirements. Together, these last three steps determine 
the merit of migrating to the new target environment. 3.3.1 Perform Benchmarks Benchmark testing involves 
establishing the performance improvement of the new target environment. This step is needed to verify 
the expected performance gains of the new target using application specific software. This is best accomplished 
by obtaining access to a test bed under the new target, then measuring software execution on the new 
target for code which has been measured on the existing target system. This provides means of determining 
how much faster the new target executes the same code. Several code samples should be run to get performance 
figures for all types of processing in the system. A ratio of speed improvement under all processing 
conditions can now be determined. This ratio is applied to known processing functions to determine how 
 434 Embedded CPU Target Migration, Doing More With Less much faster these functions are executed under 
the new target environment. In the case of the MM prototype, the target vendor was able to supply a test 
bed in which to run benchmark code, and a 1750 Ada compiler with which to build the executable objects 
to be tested on the 1750 environment. This enabled the MM design team to use existing Ada code from the 
original prototype for execution under the new target environment. The testing of several executable 
objects had shown a CPU performance improvement for the new target environment of 10 times that of the 
original target system. Further analysis showed that the perforrnaiice improvement was actually only 
by a factor of 5 times that of the original system, due to the fact that the memory access times un&#38;r 
the new target environment slowed the actual target CPU execution. A speed improvement ratio of 5:1 for 
new target to existing target was assumed Once a performance improvement ratio for the new environment 
is established, processor loading and throughput analysis can be performed under the new target environment. 
3.3.2 Perform Loading and Throughput Analysis The loading and throughput analysis of the new target 
is completed using data from two previous steps in the methodology, collection of performance data on 
the existing system, and the performance of benchmarks against the new target. The step is needed to 
provide the data used as input to the final step, the evaluation of the new design. The loading and throughput 
analysis must be done in a manner similar to that done in step 2 of the design methodology, the collection 
of performance data on the existing system. The same types of processing windows should be used for measuring 
processing loading, and the same throughput requirements should be checked. The method to determine the 
actual loading and throughput calculations for the new target is to use the measured values for processing 
functions in the existing system, and reducethesebytheratiosproduced asaresultofbenchmark testing. In 
the case of the receive message and transmit response example shown in Figures 2 and 3, all the processing 
functions listed for that major requirement had been measured as a result of collection of performance 
data on the existing system. The timing measurements for these processing functions wem then reduced 
by the ratio of 5:1 to estimate the timing measurements for those functions under the new target environment. 
Once the new timing measurements were obtained using the ratio, processing loading for all processors 
under average and peak conditions could be calculated, and processing throughput could be verified. Once 
all necessary calculations for performance under the new target have been made, the evaluation of the 
new target environment can be completed 3.3.3 Perform Evaluation of the New Design The evaluation of 
the new design under the new target environment is the most important of all the steps in the methodology, 
however, such an evaluation could not occur without the data produced from all the preceding steps. This 
step is not only needed to dt%ermine if the new target environment can meet all system performance requirements, 
but also to determine how well those requirements were met and how much margin is available. Processor 
loading and throughput analysis determines if the the number of processors chosen is sufficient to handle 
desired processing load and throughput requirements. A gord for processor loading is 50% loading or less, 
over all average and peak processing windows. In some cases, peak loading of over 50% maybe acceptable, 
if a throughput requirement takes precedence over processing loading. Obviously, the smaller the processing 
window, the greater the chance that the loading will approach 100%. The intent of processing loading 
is to measure the actual time the CPU is performing processing functions over a time window which involves 
several functions. Processor loading requirements are usually needed to insure additional processing 
power for future system enhancements without compromising existing performance. Processing throughput 
requhements are more straightforward requirements to measure. The intent is to determine if certain processing 
functions can be completed within a certain time. For the MM target migration to the 1750, the desired 
goals of reducing processing load and the requirements of reducing processing throughput on critical 
paths was realized. The example of the receive message and transmit response scenario was one of the 
critical paths which had to be reduced considerably. Under the RI 1750 target environment and new design 
approach, the processing time on this critical path was reduced by a factor of 3, meeting customer requirements. 
In addition to reducing processing time on this critical path, processing loading was reduced, as desired, 
while still being able to meet the throughput requirement. Average and peak loading for the Control Functions 
processor was kept under 50%. Peak loading for the Signal Processing Functions processor was calculated 
to be 70%, but average loading was under 50%. The 70% peak loading was determined to be acceptable considering 
the nature of the intensive algorithms for the signal processing functions. While making evaluations 
of a new targe~ there are alternatives if expected loading or throughput requirements are not achieved. 
If processing load is greater than the desired goal, then additional processors may be considered. If 
processing throughput is greater than the desired goal, the R-allocation of processing functions to processom 
may be considered. The process of selecting the wrect number of processors in a multi-processor system 
is iterative until the ideal number of processcm and desired processing load on each processor is determined. 
Several steps in the target migration methodology may be repeated until the desired result is achieved 
for the new target environment.  4.0 LESSONS LEARNED The application of the target migration methodology 
yielded a thorough analysis of the MM target migration effort. The use of this methodology resulted in 
a high degree of confidence in the new target environment and design approach. Several other important 
lessons were learned as a result of the application of this methodology. It is important to carefully 
select design teams comprised of people with complementary and compatible personalities. Cohesive teams 
triumph through teamwork dissident teams flounder because of confrontations and confusion. Having a compatible 
migration path so that software can be easily ported to new hardware saves valuable time and resources. 
It is a difficult and time consuming process to select a new compiler/tool vendor each time migration 
to new target environment is required. Balancing tasks between hardware and software helps improve system 
performance. An early prototype of the system/software design helps determine the complexity or simplicity 
needed in the hanlware design. When doing hardware/software tradeoffs, systems, software rm~ hardware 
engineers should work hgether on requirements analysis and designs. 5.0 SUMMARY Increasingly, defense 
and commercial sector users are directing their contractors to port their current real-time software 
applications to the much faster and/or more radiation hardened IvIIL-STD-1750A Architecture. Many companies 
will be trying to develop methodologies that will enable them to migrate the software faster than their 
competitors. The target migration methodology encompasses a set of techniques and processes that can 
be used to build applications, including complex, mission critical applications, in months, rather than 
years. Its processes are highly flexible and can be adapted to the nature of the systems and tools being 
used. The idea is to do things right the first time, using a multidisciplined feedback loop that shortens 
development cycles, cuts product costs, and increases first-pass product reliability and performance. 
ABOUT THE AUTHORS Robert B. Greene was the Software Engineering Mrmaga for the development and testing 
of the Terminal Controller Ada applications software and the Modem Controller target migration effort 
for the Small ICBM Program. He supervised the self-directed design team which developed and applied the 
target migration methodology described herein. Mr. Greene is currently the Software Engineering Manager 
for the developmen~ documentation and testing of the SHF and UHF DAMA protocol real-time embedded Ada 
applications software for the MicroSATCOM program for Rome laboratories. Mr. Greene received his Engineering 
degree from Spring Garden College in Philadelphia% PA. Mr. Greene can be reached electronically at gmene@carib.ge.tom, 
George B. Lownes is a Senior Member of the Engineering Staff at MM Communications Systems. He was a Lead 
Software Design Engineer for the development of the Ada applications software for the GE-developed prototype 
communications equipment and for the ensuing target migration effort for the Small ICBM Program. In addition, 
he was a participant in the self-directed design team which developed and applied the target migration 
methodology described herein. Mr. Lownes received his B.S. degree in Computer Science from West Chester 
University in West Chester, PA. Mr. Lownes can be reached electronically at glownes@czunden.ge.com. 
436 
			