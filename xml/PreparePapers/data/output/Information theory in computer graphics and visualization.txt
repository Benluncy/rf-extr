
 Information Theory in Computer Graphics and Visualization Mateu Sbert* Miquel Feixas Ivan Viola Jaume 
Rigau§ University of Girona University of Girona University of Bergen University of Girona Miguel Chover¶ 
Jaume I University Abstract We present various applications of information theory for computer graphics, 
based on the use of information measures such as entropy and mutual information. Some application areas 
are hierarchical ra­diosity, pixel supersampling, best view selection, scene exploration, ambient occlusion, 
mesh saliency, mesh simpli.cation, and scien­ti.c visualization. 1 Course Description We present a half-day 
course to review several applications of in­formation theory for computer graphics and visualization. 
Infor­mation theory tools, widely used in scienti.c .elds such as en­gineering, physics, genetics, neuroscience, 
are also emerging as useful transversal tools in computer graphics and related .elds. We introduce the 
basic concepts of information theory and how they map into the application areas. Application areas in 
computer graphics are viewpoint selection, mesh saliency, scene exploration, ambient occlusion, geometry 
simplication, radiosity, adaptive ray­tracing and shape descriptors. Applications areas in visualization 
are view selection for volume data, .ow visualization, ambient oc­clusion, time-varying volume visualization, 
transfer function de.­nition, time-varying volume visualization, multimodal fusion, iso­surface similarity 
maps and quality metrics. The applications fall broadly into two categories: the mapping of the problem 
to an in­formation channel, as in viewpoint applications, and the direct use of measures as entropy, 
Kullback-Leibler distance, Jensen-Shannon divergence, and f-divergences, to evaluate for instance the 
homo­geneity of a set of samples or being used as metrics. We also dis­cuss the potential applications 
of information bottleneck method that allows us to progressively extract or merge information in a hierarchical 
structure. 2 Course Organizer Mateu Sbert. *e-mail: mateu@ima.udg.edu e-mail:feixas@ima.udg.edu e-mail:ivan.viola@uib.no 
§e-mail:rigau@ima.udg.edu ¶e-mail:chover@uji.es  3 Proposed Length Half-day, Beginner level. 4 Intended 
Audience and Prerequisites We target the course primarily at computer graphics and visualiza­tion researchers 
and practitioners. In addition, information theory practitioners will learn about the presented applications. 
We will stress the common aspects of the applications to clearly see the kind of problems information 
theory tools can help solving. The reader is expected to have a basic background in com­puter graphics. 
Information theory basics are presented and self­contained in this course. 5 Syllabus 1. Introduction 
to Information Theory (60 minutes) Presenter: Miquel Feixas Information theory deals with the transmission, 
storage, and processing of information, and is applied to .elds such as physics, statistics, biology, 
neurology, and learning. It has been successfully applied to areas closely related to computer graphics, 
such as medical imaging and computer vision. We present the concept of information channel and the most 
basic information-theoretic measures: Shannon entropy (in­formation content or uncertainty of a random 
variable), con­ditional entropy (uncertainty in a communication channel), Kullback-Leibler distance between 
two probability distribu­tions, mutual information (shared information in a communi­cation channel) and 
its different alternative decompositions, and entropy rate (average information content per symbol in 
a stochastic process). Some important inequalities, such as Jensen-Shannon inequality, log-sum inequality, 
and data processing inequality, together with information bottleneck method, are also reviewed. Finally, 
other divergence mea­sures and generalized entropies are brie.y introduced. To facilitate the understanding 
and applicability of the pre­vious information measures and methods, several simple ex­amples and algorithms, 
such as the entropy of a sequence of characters and the information channel between stimuli and responses, 
will be introduced. 2. Applications in Computer Graphics Presenter: Mateu Sbert a. Uni.ed Viewpoint 
Framework for Polygonal Models (30 minutes) Viewpoint selection is an emerging area in computer graphics 
with applications in .elds such as scene exploration, image­based modeling, and volume visualization. 
Best view selec­tion algorithms are used to obtain the minimum number of views to understand or model 
an object or scene. This ap­plication has a high pedagogical value as helps us to under­stand and reinforce 
all the information-theoretic concepts in­troduced in the .rst part of the course. We present a uni.ed 
framework for viewpoint selection and mesh saliency based on the de.nition of an information chan­nel 
between a set of viewpoints and the set of polygons of an object. Both conditional entropy and mutual 
information are shown to be powerful tools to deal with viewpoint selection, object exploration, viewpoint 
similarity and stability, view­based ambient occlusion, and view-based saliency. View­point mutual information 
can be extended by incorporating importance factors such as saliency and stability. Applica­tions to 
non-photorealistic rendering, molecular visualization, and mesh simpli.cation are also reviewed. b. Applications 
to Global Illumination, Shape Recognition and Image Processing (45 minutes) We introduce scene complexity 
measures and their applica­tion to radiosity. Radiosity is a viewpoint independent global illumination 
technique that discretizes the scene into small polygons or patches to solve a transport system of equations. 
The way the scene is discretized is critical for the efciency of the result. First, we dene a scene information 
channel, which allows us to study the interchange of information between the patches. From the study 
of this channel, several rene­ment oracles, i.e., criteria for subdividing the geometry, are obtained, 
aimed at maximizing the transport of information. We also present adaptive ray-tracing. This technique 
is aimed at tracing more rays only where they are needed. Information­theoretic measures, such as Shannon 
entropy, Tsallis entropy, and f-divergences, will be used to dene adaptive renement cri­teria. Another 
application of information-theoretic measures is to obtain different shape descriptors based on the complexity 
of the object. Shape descriptors are important when classifying and retrieving objects from databases. 
Inner and outer com­plexity, obtained from mutual information calculation with uniformly distributed 
lines, can be used to classify different families of 2D and 3D objects. A short overview of basic image 
processing techniques will also be given. Algorithms of image processing, such as split­and-merge segmentation 
and image registration, will be pre­sented as paradigmatic examples for the basic concepts of en­tropy, 
mutual information, data processing inequality, and in­formation bottleneck method. Break (15 minutes) 
 3. Applications in Visualization Presenter: Ivan Viola a. Visualization and Information Theory (20 minutes) 
Visualization and interaction can be seen as an information communication platform between a human and 
digital data capturing certain phenomenon, its structure or process. Infor­mation theory at the same 
time provides tools to quantify the ef.ciency of transmitted information in a channel. From the theoretical 
perspective information theory can be used to as­sess the visualization ef.ciency or evaluate the visualization 
parameters under which the information transfer is most ef.­cient. b. Information Theory in Scienti.c 
Visualization (55 min­utes) We discuss speci.c applications of information theory in vol­ume visualization. 
Viewpoint entropy and viewpoint mu­tual information, as measures for viewpoint quality, can be adapted 
for volume data by evaluating the volume elements visibility instead of polygonal visibility discussed 
earlier. We can consider the information contained in a volume in various ways: as a set of voxels, iso-surfaces, 
or volumetric objects. Visibility of these elements yields characteristic viewpoints. Application in 
speci.c medical diagnostic scenarios will un­derline utility of automatic view selection for user guidance. 
Time varying data imposes an additional challenge on view selection. We will discuss view selection for 
entire sequence and camera path generation to allow for expressive viewpoints during playback. Besides 
the view selection, information the­ory tools can serve as steering mechanism for de.ning other visualization 
parameters. Instead of a set of viewpoints, one can de.ne a set of representative iso-surfaces in a dataset. 
Il­lustrative exploded views concept can also accommodate in­formation theory tools for de.ning an axis 
of explosion, as well as the explosion partitioning based on similarity. In multimodal volume visualization, 
the data fusion can be con­trolled with information-theory measures. The transfer func­tion speci.cation 
is a challenging task. With measures derived from Kullback-Leibler distance this time-consuming process 
can be ef.ciently assisted.  6 Course Presenter Information Mateu Sbert is a full professor in Computer 
Science at the Uni­versity of Girona, Spain. He received a M.Sc. in Theoretical Physics (1977) at the 
University of Valencia, a M.Sc. in Mathemat­ics (1983) at UNED University (Madrid) and a Ph.D. in Computer 
Science at the Universitat Politcnica de Catalunya. His research in­terests include the application of 
Monte Carlo, Integral Geometry and Information Theory techniques to Computer Graphics and Vi­sualization. 
He has authored or co-authored more than 150 papers, participated in four Eurographics tutorials, and 
served as a member of program committee in international conferences. Miquel Feixas is an associate professor 
in Computer Science at the University of Girona, Spain. He received a M.Sc. in The­oretical Physics at 
the Universitat Autnoma de Barcelona (1979) and a Ph.D. in Computer Science at the Universitat Politcnica 
de Catalunya (2002). His research is focused on the application of Information Theory techniques to Computer 
Graphics and Visual­ization. He has co-authored more than 50 papers, served as a mem­ber of program committee 
in international conferences, and par­ticipated in a Eurographics tutorial on Applications of Information 
Theory to Computer Graphics. Ivan Viola is an associate professor at University of Bergen, and scienti.c 
adviser at Christian Michelsen Research (CMR), Bergen, Norway. He received M.Sc. in 2002 and Ph.D. in 
2005 from Vi­enna University of Technology, Austria. His research is focused on illustrative visualization 
for communication of complex scien­ti.c data. Viola co-authored several scienti.c works published in 
international journals and conferences such as IEEE TVCG, IEEE Visualization, and EuroVis and acted as 
a reviewer and IPC member for conferences in the .eld of computer graphics and visualization. He is member 
of Eurographics, NorSIGD, IEEE Computer Society, VGTC, and ACM SIGGRAPH. 7 Description of the Course 
Notes The course notes include a copy of each presenter slides, a com­plete bibliography for each of 
the topic areas, and a document on the basics of information theory. This document is excerpted from 
the book Information Theory Tools for Computer Graph­ics , M. Sbert, M. Feixas, J. Rigau, M. Chover, 
and I. Viola, Syn­thesis Lectures on Computer Graphics and Animation, Morgan &#38; Claypool Publishers, 
2009 (http://dx.doi.org/10.2200/ S00208ED1V01Y200909CGR012).  Acknowledgements This work was supported 
in part by Grant Numbers TIN2010­21089-C03-01 from the Spanish Government and 2009-SGR-643 from the Catalan 
Government, by the VERDIKT program (# 193170) of the Norwegian Research Council, and by the strategic 
funding for the MedViz research network (# 911597 P11) obtained from Helse Vest. CHAPTER 1  Information 
Theory Basics In 1948, Claude Shannon published a paper entitled A mathematical theory of commu­nication 
[22] which marks the beginning of information theory. In this paper, Shannon de.ned measures such as 
entropy and mutual information1, and introduced the fun­damental laws of data compression and transmission. 
Information theory deals with the transmission, storage, and processing of information and is used in 
.elds such as physics, computer science, mathematics, statistics, economics, biology, linguistics, neu­rology, 
learning, image processing, and computer graphics. In information theory, information is simply the outcome 
of a selection among a .nite number of possibilities and an information source is modeled as a random 
variable or a random process. The classical measure of information, Shannon entropy, expresses the information 
content or the uncertainty of a single random variable. It is also a measure of the dispersion or diversity 
of a probability distribution of observed events. For two random variables, their mutual information 
is a measure of the depen­dence between them. Mutual information plays an important role in the study 
of a communication channel, a system in which the output depends probabilistically on its input [4, 33, 
34]. This chapter presents Shannon s information measures (entropy, conditional en­tropy, and mutual 
information) and their most basic properties. The information bot­tleneck method, f-divergences, and 
generalized entropies are also introduced. Good ref­erences of information theory are the books by Cover 
and Thomas [4], and Yeung [34]. Note: This chapter is excerpted from the book Information Theory Tools 
for Computer Graphics , M. Sbert, M. Feixas, J. Rigau, M. Chover, and I. Viola, Synthesis Lectures on 
Computer Graphics and Animation, Morgan &#38; Claypool Publishers, 2009 (http://dx.doi.org/10.2200/S00208ED1V01Y200909CGR012). 
1 In Shannon s paper, the mutual information is called rate of transmission.  1.1 Entropy After representing 
a discrete information source as a random process, Shannon asks him­self: Can we de.ne a quantity which 
will measure, in some sense, how much information is produced by such a process, or better, at what rate 
information is produced? [22]. In his answer, Shannon supposes that we have a set of possible events 
whose probabilities of occurrence are p1, p2, ..., pn and asks for the possibility of .nding a measure, 
denoted by H(p1,p2,...,pn), of how much choice is involved in the selection of the event or of how uncertain 
we are of the outcome. If this uncertainty measure exists, Shannon considers reasonable to require of 
it the following properties: 1. H would be continuous in the pi. 2. If all the pi are equal (i.e., pi 
=1/n), then H should be a monotonic increasing function of n. With equally likely events there is more 
choice, or uncertainty, when there are more possible events. 3. If a choice is broken down into two 
successive choices, the original H should be the weighted sum of the individual values of H. The meaning 
of this property, called grouping property, is illustrated in Fig. 1.12 .  FIGURE 1.1: Grouping property 
of the entropy. On the left, we have three possibilities with probabilities p1 =1/2, p2 =1/3, p3 =1/6. 
On the right, we .rst choose between two possibilities each with probability 1/2, and if the second occurs, 
we make another choice with probabilities 2/3, 1/3. The .nal results have the same probabilities as before. 
In this example, it is required that H(1/2,1/3,1/6) = H(1/2,1/2)+ (1/2)H(2/3,1/3). The coe.cient 1/2 
is because the sec­ond choice occurs with this probability. After these requirements, Shannon proved 
the following theorem: 2 This example has been used by Shannon in [22]. Another example showing the recursive 
character of this property is in Fig. ??.  Theorem 1.1. The only measure H satisfying the three above 
assumptions is of the form H = -K n. pi log pi, (1.1 ) i=1 where K is a positive constant3 . To prove 
this theorem, Shannon assumed H(1/n, 1/n,... , 1/n)= f (n) and de­ m composed a choice from sequally 
likely possibilities into a series of m choices from s equally likely possibilities. Thus, from the previous 
required property (3), f (sm)= mf (s). In essence, this expression contains the intuition that the uncertainty 
of m choices should be m times the uncertainty of only one choice. One function that ful.lls this requirement 
is the logarithm function (see the complete proof in [22]). There are other axiomatic formulations which 
involve the same de.nition of un­certainty [4, 16]. Shannon called this quantity entropy4, as it can 
be identi.ed with the entropy used in thermodynamics and statistical mechanics. Let X be a discrete random 
variable5 with alphabet X and probability distribution {p(x)}, where p(x) = Pr{X = x} and x .X . In this 
book, {p(x)} will be also denoted by p(X) or simply p. This notation will be extended to two or more 
random variables. As an example, a discrete random variable can be used to describe the toss of a fair 
coin, with alphabet X = {head, tail} and probability distribution p(X)= {1/2, 1/2}. De.nition 1.1. The 
entropy H(X) of a discrete random variable X is de.ned by H(X)= - . p(x) log p(x), (1.2) x.X where the 
summation is over the corresponding alphabet and the convention 0log0 = 0 is taken. In this book, logarithms 
are taken in base 2 and, as a consequence, entropy is expressed in bits. The convention 0 log 0 = 0 is 
justi.ed6 by continuity since x log x . 0 3 This constant will be taken equal to 1 in the de.nition of 
entropy (Equ. 1.2). 4 In the Eighteenth Century, R. Clausius introduced the term entropy in thermodynamics 
and L. Boltz­mann gave its probabilistic interpretation in the context of statistical mechanics. The 
relationship between the Boltzmann entropy and Shannon entropy was developed in a series of papers by 
E. Jaynes [13]. The link between the second law of thermodynamics ( The entropy of an isolated system 
is non-decreasing ) and the Shannon entropy is analyzed in [4]. 5 We assume that all random variables 
used are discrete unless otherwise speci.ed. 6 See in Yeung s book [34] the discussion on probability 
distributions which are not strictly positive.  as x . 0. The term - log p(x) represents the information 
content (or uncertainty) as­sociated with the result x. Thus, the entropy gives us the average amount 
of informa­tion (or uncertainty) of a random variable. Information and uncertainty are opposite. Uncertainty 
is considered before the event, information after. So, information reduces uncertainty. Note that the 
entropy depends only on the probabilities. We will use in­terchangeably the notation H(X) or H(p) for 
the entropy, where p stands for the probability distribution p(X). For example, the entropy of a fair 
coin toss is H(X)= -(1/2) log(1/2) - (1/2) log(1/2) = log 2 = 1 bit. For the toss of a fair die with 
alphabet X = {1, 2, 3, 4, 5, 6} and probability distribution p(X)= {1/6, 1/6, 1/6, 1/6, 1/6, 1/6}, the 
entropy is H(X) = log6 = 2.58 bits. Some relevant properties of the entropy [22] are 0 = H(X) = log 
|X |  H(X) = 0 when all the probabilities are zero except one with unit value.  H(X) = log |X | when 
all the probabilities are equal.   If the probabilities are equalized, entropy increases.  The binary 
entropy (Fig. 1.2) of a random variable X with alphabet {x1,x2} and probability distribution {p, 1 - 
p} is given by H(X)= -p log p - (1 - p) log(1 - p). (1.3) Note that the maximum entropy is H(X) = 1 bit 
when p =1/2.  De.nition 1.2. The joint entropy H(X, Y )of a pair of discrete random variables X and 
Y with a joint probability distribution p(X, Y )={p(x, y)} is de.ned by H(X, Y )=- LL p(x, y)logp(x, 
y), (1.4) x .X y .Y where p(x, y)=Pr[X =x, Y =y]is the joint probability of x and y. The conditional 
entropy of a random variable given another is de.ned as the expectedvalueoftheentropiesoftheconditionaldistributions. 
De.nition 1.3. The conditional entropy H(Y |X) of a random variable Y given a random variable X is de.ned 
by ( - L p(y|x)logp(y|x)) H(Y |X)=L .X =- L x L)==x x .X p(x, y)logp(y|x), (1.5) p(x)H(Y |X p(x) .Y 
y L .X y .Y x 7 where p(y|x)=Pr[Y =y|X =x]is the conditional probability of y given x. The conditional 
entropy can bethought of in terms of a communication or in­formation channel X . Y whoseoutputY dependsprobabilistically 
onitsinputX. Thisinformationchannelischaracterizedbyatransitionprobabilitymatrixwhichde­terminestheconditionaldistributionoftheoutputgiventheinput[4].Hence,H(Y 
|X) correspondstotheuncertaintyinthechanneloutputfromthesender spointofview, andvicesaforH(X|Y ).NoetatingeneralH(Y 
|X) nthisbok,the evrth=H(X|Y ).Ioconditional probability distribution of Y given x will be denoted by 
p(Y |x) and the transitionprobabilitymatrix(i.e.,thematrixwhoserowsaregivenbyp(Y |x))willbe denotedbyp(Y 
|X). Thefollowingpropertieshold: H(X, Y )=H(X)+H(Y |X)=H(Y )+H(X|Y )  H(X, Y )= H(X)+H(Y )  H(X)= 
H(X|Y )= 0  7 The Bayes theorem relates marginal probabilities p(x) and p(y), conditional probabilities 
p(y|x) and p(x|y), and joint probabilities p(x, y): p(x, y)= p(x)p(y|x)= p(y)p(x|y). (1.6 ) If X and 
Y are independent, then p(x, y)= p(x)p(y). Marginal probabilities can be obtained from p(x, y) by summation: 
p(x)= Lp(x, y) and p(y)= Lp(x, y). y.Y x.X  If X and Y are independent, then H(Y |X)= H(Y ) since p(y|x)= 
p(y) and, conse­quently, H(X, Y )= H(X)+ H(Y ) (i.e., entropy is additive for independent random variables). 
As an example, we consider the joint distribution p(X, Y ) repre­sented in Fig. 1.3.left. The marginal 
probability distributions of X and Y p(X, Y ) Y y1 y2 p(X) x1 X x2 x3 0.125 0.125 0.25 0 0 0.5 0.25 0.25 
0.5 p(Y ) 0.375 0.625 H(X, Y ) = 1.75 FIGURE 1.3: Example of joint, marginal, and conditional probability 
distributions of random variables X and Y . On the left, joint distribution p(X, Y ), marginal distributions 
p(X) and p(Y ), and joint entropy H(X, Y ). On the right, transition probability matrix p(Y |X) and conditional 
entropy H(Y |X).  p(Y |X) Y y1 y2 H(Y |x . X ) x1 X x2 x3 0.5 0.5 1 0 0 1 H(Y |x1) = 1 H(Y |x2) = 0 
H(Y |x3) = 0  H(Y |X)=0.25 are given by p(X)= {0.25, 0.25, 0.5} and p(Y )= {0.375, 0.625}, respectively. 
Thus, H(X)= -0.25log 0.25 - 0.25log 0.25 - 0.5 log 0.5=1.5 bits, H(Y )= -0.375 log 0.375 - 0.625 log 
0.625 = 0.954 bits, and H(X, Y )= -0.125 log 0.125 - 0.125 log 0.125 - 0.25log 0.25 - 0 log 0 - 0 log 
0 - 0.5 log 0.5=1.75 bits. From the transition probability matrix p(Y |X) represented in Fig. 1.3.right, 
we can compute H(Y |X) as follows: 3 H(Y |X)= L p(xi)H(Y |X = xi) i=1 =0.25 H(Y |X = x1)+0.25 H(Y |X 
= x2)+0.5 H(Y |X = x3) =0.25 × 1+0.25 × 0+0.5 × 0=0.25 bits 1.2 Relative Entropy and Mutual Information 
We now introduce two new measures, relative entropy and mutual information, which quantify the distance 
between two probability distributions and the shared information between two random variables, respectively. 
 De.nition 1.4. The relative entropy or Kullback-Leibler distance DKL(p,q)between two probability distributions 
p and q, that are de.ned over the alphabet X , is de.ned by DKL(p,q)=L p(x)log pq((xx)) . (1.7) x.X Theconventionsthat0log(0/0)=0andalog(a/0)=8 
ifa> 0areadopted.The relativeentropysatis.esthedivergenceorinformationinequality DKL(p,q)= 0, (1.8) withequalityifandonlyifp 
=q.Therelativeentropyisalsocalledinformationdiver­ 8 gence[6]orinformationaldivergence[34],anditisnotstrictlyametricsinceitisnot 
symmetricanddoesnotsatisfythetriangleinequality. De.nition 1.5. The mutual information I(X;Y) between 
two random variables X and Y is de.ned by I(X;Y)=H(X)- H(X|Y)=H(Y)- H(Y|X) LL p(x,y) L p(y|x) =p(x,y)log 
=p(x)L p(y|x)log . (1.9) p(x)p(y) p(y) x.X y.Y x.X y.Y Mutualinformationrepresentstheamountofinformationthatonerandomvari­able,theinputofthechannel,containsaboutasecondrandomvariable,theoutputof 
thechannel,andviceversa.Thatis,mutualinformationexpresseshowmuchtheknowl­edgeofY decreasestheuncertaintyofX,andviceversa.I(X;Y)isameasureofthe 
sharedinformationordependencebetweenX andY.Thus,ifX andY areindependent, thenI(X;Y)=0. Notethat themutual 
informationcan beexpressed as therelative entropybetweenthejointdistributionandtheproductofmarginaldistributions: 
I(X;Y)=DKL(p(X,Y),p(X)p(Y)). (1.10) MutualinformationI(X;Y)ful.llsthefollowingproperties: I(X;Y)= 0withequalityifandonlyifX 
andY areindependent  I(X;Y)=I(Y;X)  I(X;Y)=H(X)+H(Y)- H(X,Y)  I(X;Y)= min{H(X),H(Y)}  8 A metric 
between x and y is de.ned as a function d(x, y) that ful.lls the following properties: (1) non­ negativity: 
d(x, y) = 0, (2) identity: d(x, y)=0 if andonlyif x = y, (3) symmetry: d(x, y)= d(y, x), and (4) triangle 
inequality: d(x, y)+ d(y, z) = d(x, z).  I(X; X)= H(X) The relationship between Shannon s information 
measures can be expressed by a Venn diagram, as shown in Fig. 1.49. The correspondence between Shannon 
s informa­tion measures and set theory is discussed in [34]. H(X) H(Y) FIGURE 1.4: The information diagram 
represents the relationship between Shannon s in­formation measures. Observe that I(X; Y ) and H(X, Y 
) are represented, respectively, by the intersection and the union of the information in X (represented 
by H(X)) with the information in Y (represented by H(Y )). H(X|Y ) is represented by the di.erence between 
the information in X and the information in Y , and vice versa for H(Y |X). For the example presented 
in Fig. 1.3, the mutual information can be easily com­puted: I(X; Y )= H(Y ) - H(Y |X)=0.954 - 0.25 = 
0.704 bits. 1.3 Inequalities In this section, we introduce a group of inequalities that are essential 
in the study of information theory and for the development of the concepts presented in this book [4, 
34], and, in particular, to derive most of the re.nement criteria. 1.3.1 Jensen s Inequality In this 
section, we introduce the concepts of convexity and concavity. Many important inequalities and results 
in information theory are obtained from the concavity of the logarithmic function. 9 The information 
diagram does not include the universal set as in a usual Venn diagram.  De.nition 1.6. A function f 
(x) is convex over an interval [a, b] (the graph of the function lies below any chord) if for every x1,x2 
. [a, b] and 0 = . = 1, f (.x1 + (1 - .)x2) = .f (x1) + (1 - .)f (x2). (1.11) A function is strictly 
convex if equality holds only if . =0 or . =1. De.nition 1.7. A function f (x) is concave (the graph 
of the function lies above any chord) if -f (x) is convex. 2 For instance, xand x log x (for x> 0) are 
strictly convex functions, and log x (for x> 0) is a strictly concave function. Fig. 1.5 plots x log 
x and log x. 4 2 0 -2 -4 FIGURE 1.5: Plots of the strictly convex function x log x (red) and the strictly 
concave function log x (blue) for x . (0, 3]. Jensen s inequality can be expressed as follows. If f is 
a convex function on the interval [a, b], then n. n. LL .if (xi) - f.ixi = 0, (1.12) i=1 i=1 where 0 
= . = 1, Ln .i = 1, and xi . [a, b]. If f is a concave function, the inequality i=1 is reversed. A special 
case of this inequality is when .i =1/n because then nn . 1 . 1 LL xi = 0, (1.13) f (xi) - f nn i=1 i=1 
that is, the value of the function at the mean of the xi is less or equal than the mean of the values 
of the function at each xi.   Jensen s inequality can also be expressed in the following way: if f 
is convex on the range of a random variable X, then f(E[X]) = E[f(X)], (1.14) where E denotes expectation 
(i.e., E[f(X)] = L x.X p(x)f(x)). Observe that if f(x)= x2 (convex function), then E[X2] - (E[X])2 = 
0. Thus, the variance is always positive. One of the most important consequences of Jensen s inequality 
is the divergence inequality DKL(p,q) = 0 (Equ. 1.8). Some properties of Shannon s information mea­sures 
presented in Sec. 1.1 and Sec. 1.2 can be derived from this inequality. 1.3.2 Log-sum Inequality The 
log-sum inequality can be obtained from Jensen s inequality (Equ. 1.12). For non­negative numbers a1,a2,...,an 
and b1,b2,...,bn, the log-sum inequality is expressed as n n Ln ai LL i=1 ai ai log - ai log = 0, (1.15) 
Ln bi i=1 bi i=1 i=1 with equality if and only if ai/bi is constant for all i. The conventions that 0 
log 0 = 0, 0 log(0/0)=0, and alog(a/0) = 8 if a> 0 are again adopted. From this inequality, the following 
properties can be proved [4]: DKL(p,q) is convex in the pair (p,q).  H(X) is a concave function of 
p.  If X and Y have the joint distribution p(x,y)= p(x)p(y|x), then I(X; Y) is a concave function of 
p(x) for .xed p(y|x) and a convex function of p(y|x) for .xed p(x).  1.3.3 Jensen-Shannon Inequality 
The Jensen-Shannon divergence, derived from the concavity of entropy, is used to mea­sure the dissimilarity 
between two probability distributions and has the important fea­ture that a di.erent weight can be assigned 
to each probability distribution. De.nition 1.8. The Jensen-Shannon (JS) divergence is de.ned by nn LL 
JS(p1,p2,...,p n; p1,p2,...,pn)= Hpipi - piH(pi), (1.16) i=1 i=1 where p1,p2,...,pn are a set of probability 
distributions de.ned over the same alphabet with prior probabilities or weights p1,p2,...,p n, ful.lling 
Ln pi =1, and Ln pipi i=1 i=1  is the probability distribution obtained from the weighted sum of the 
probability distri­butions p1,p2,...,pn. From the concavity of entropy (Sec. 1.3.2), the Jensen-Shannon 
inequality [2] is obtained: JS(p1,p2,...,p n; p1,p2,...,pn) = 0. (1.17) The JS-divergence measures how 
far the probabilities pi are from their mixing distribution Ln pipi, and equals zero if and only if all 
the pi are equal. It is important i=1 to note that the JS-divergence is identical to the mutual information 
I(X; Y) when pi = p(xi) (i.e., {pi} corresponds to the marginal distribution p(X)), pi = p(Y|xi) for 
all xi .X (i.e., pi corresponds to the conditional distribution of Y given xi), and n= |X | [2, 26]. 
 1.3.4 Data Processing Inequality The data processing inequality is expressed as follows. If X . Y . 
Z is a Markov chain10, then I(X; Y) = I(X; Z). (1.18) This result proves that no processing of Y, deterministic 
or random, can increase the information that Y contains about X. In particular, if Z = f(Y), then X . 
Y . f(Y) and, consequently, I(X; Y) = I(X; f(Y)) [4].  1.4 Entropy Rate Using the property H(X1,X2)= 
H(X1)+ H(X2|X1) (Sec. 1.1) and the induction on n [34], it can be proved that the joint entropy of a 
collection of n random variables X1,...,Xn is given by n H(X1,...,Xn)= L H(Xi|X1,...,Xi-1). (1.19) i=1 
We now introduce the entropy rate that quanti.es how the entropy of a sequence of n random variable increases 
with n. 10 For random variables X, Y , and Z, X . Y . Z forms a Markov chain if p(x,y, z)= p(x)p(y|x)p(z|y). 
That is, the probability of the future state depends on the current state only and is independent of 
what happened before the current state. See a more general de.nition of Markov chain in Sec. 1.4.  De.nition 
1.9. The entropy rate or entropy density HX of a stochastic process11 {Xi}is de.ned by 1 HX = lim H(X1,X2,...,Xn) 
(1.20) n.8 n when the limit exists. The entropy rate represents the average information content per symbol 
in a stochastic process. For a stationary stochastic process12, the entropy rate exists and is equal 
to HX = lim HX (n), (1.21) n.8 where HX (n)= H(X1,...,Xn) - H(X1,...,Xn-1)= H(Xn|Xn-1,...,X1). Entropy 
rate can be seen as the uncertainty associated with a given symbol if all the pre­ceding symbols are 
known. It can be also interpreted as the irreducible randomness in sequences produced by an information 
source [9]. If {Xi} is a Markov chain13, then Xn is called the state at time n. A stationary Markov chain 
is characterized by its initial state and a transition probability matrix P = {Pij }, where Pij = Pr{Xn+1 
= j|Xn = i} is called transition probability. A distribution on the states such that the distribution 
w = {wi} at time n+ 1 is the same as the distribution at time n is called a stationary distribution. 
A Markov chain is called irreducible if it is possible to go from every state to every state in a .nite 
number of steps, that is, there is always a path between any two states. A Markov chain is said to be 
aperiodic if it has no periodic state. A periodic state is a state that can be visited back by a path 
starting from it only at multiples of a given period [3]. An irreducible and aperiodic Markov chain is 
called ergodic. For an ergodic Markov chain, the stationary distribution w exists and is unique, and 
wj = limn.8(Pn)ij . The stationary distribution satis.es the left eigenvector equation wP = w. Thus, 
we can also 11 A stochastic process or a discrete-time information source {Xi} is an indexed sequence 
of random variables characterized by the joint probability distribution p(x1,x2,...,xn) = Pr{(X1 ,X2 
,...,Xn)= (x1,x2,...,xn)} with (x1,x2,...,xn) .X n for n = 1 [4, 34]. 12 A stochastic process {Xi} is 
stationary if two subsets of the sequence, {X1 ,X2 ,...,Xn} and {X1+l,X2+l,...,Xn+l}, have the same joint 
probability distribution for any n,l = 1: Pr{(X1 ,...,Xn)=(x1,x2,...,xn)} = Pr{(X1+l,X2+l,...,Xn+l)=(x1,x2,...,xn)}. 
That is, the sta­ tistical properties of the process are invariant to a shift in time. At least, HX exists 
for all stationary stochastic processes. 13 A stochastic process {Xi} is a Markov chain if Pr(Xn+1 = 
xn+1|Xn = xn,...,X1 = x1) = Pr(Xn+1 = xn+1|Xn = xn), for n =1,2,... and for all xi .X .  think of the 
stationary distribution as a left eigenvector of the transition probability matrixP. The entropy rate 
of a stationary Markov chain, with stationary distribution w andtransition probabilitymatrixP,isgivenby 
1 HX = lim H(Xn|Xn - 1,...,X1)= lim H(Xn|Xn-1) n.8 n n.8 nn =H(X2|X1)=- L wi L Pij logPij . (1.22) i=1 
j=1 1.5 Entropy and Coding Inthissection, wepresentdi.erentinterpretationsoftheShannonentropy: As we 
have seen in Sec. 1.1, - logp(x) represents the information associated with the resultx. The value- logp(x) 
can also beinterpreted as the surpriseassociated with the outcome x. If p(x) is small, the surprise is 
large; if p(x) is large, the sur­prise is small. Thus, entropy (Equ. 1.2) can be seen as the expectation 
value of the surprise[8].  A fundamental result of information theory is the Shannon source coding theorem, 
which deals with the encoding of information in order to store or transmit it e.­ciently. Thistheoremcanbeformulatedin 
thefollowingways[4,8]:  Given arandomvariableX,H(X) ful.lls H(X)= f<H(X)+1, (1.23)  where. istheexpectedlengthofanoptimalbinarycodeforX.Anexampleofan 
14 optimalbinarycodeistheHu.maninstantaneouscoding. IfweoptimallyencodenidenticallydistributedrandomvariablesXwithabinary 
code,theShannonsourcecodingtheoremcanbeenunciatedinthefollowingway: 1 H(X)= fn <H(X)+ , (1.24) n where 
fn is the expected codeword length per unit symbol. Thus, by using large blocklengths,wecanachieveanexpectedcodelengthpersymbolarbitrarilyclose 
totheentropy[4]. 14 A code is called a pre.x or instantaneous code if no codeword is a pre.x of any other 
codeword. Hu.man coding uses a speci.c algorithm to obtain the representation for each symbol. The main 
characteristic of this code is that the most common symbols use shorter strings of bits than the ones 
used by the less common symbols.  For a stationary stochastic process, we have H(X1,X2,...,Xn) H(X1,X2,...,Xn) 
= f n < +1 (1.25) nn and, from the de.nition of entropy rate HX (Equ. 1.20), lim f n . HX . (1.26) n.8 
Thus, the entropy rate is the expected number of bits per symbol required to describe the stochastic 
process. From the previous Shannon theorem, it can be proved that entropy is related to the di.culty 
in guessing the outcome of a random variable [4, 8] since H(X) = q<H (X)+1, (1.27) where q is the average 
minimum number of binary questions to determine X. This idea agrees with the interpretation of entropy 
as a measure of uncertainty. 1.6 Continuous Channel In this section, entropy and mutual information 
are de.ned for continuous random variables. Let X be a continuous random variable with continuous cumulative 
distri­bution function F (x) = Pr{X = x}. When the derivative F '(x)= f(x) is de.ned and J8 f(x)dx = 
1, then f(x) is called the probability density function (pdf) of X. The -8 support of X is given by SX 
= {x : f(x) > 0}, that is, the set of points where the function is non-zero. The statement if it exists 
should be included in the following de.nitions involving integrals and probability density functions. 
See a more detailed presentation in Cover and Thomas [4], and Yeung [34]. The di.erential entropy of 
a continuous random variable X is de.ned similarly to the entropy of a discrete random variable (see 
Equ. 1.2). De.nition 1.10. The continuous or di.erential entropy h(X) of a continuous random variable 
X with a pdf f(x) is de.ned by h(X)= -f(x) log f(x)dx. (1.28) SX De.nition 1.11. For two continuous 
random variables X and Y with joint pdf f(x, y), the continuous conditional entropy h(Y |X) is de.ned 
as h(Y |X)= -f(x, y) log f(y|x)dxdy, (1.29) SXSY (x)  where f (y|x) is the conditional pdf and SY (x)= 
{y : f (y|x) > 0}. De.nition 1.12. For two continuous random variables X and Y with joint pdf f (x, y), 
the continuous mutual information Ic(X; Y ) is de.ned as f (x, y) Ic(X; Y )= h(X) - h(X|Y )= f (x, y) 
log dxdy. (1.30) f (x)f (y) SX SY (x) Following the exposition in [4], we divide the range of the continuous 
random variable X into discrete bins of length . (Fig. 1.6.a). Then, assuming the continuity of f (x) 
within the bins and using the mean value theorem, for each bin there exists a value xi such that (i+1). 
f (xi).= f (x)dx. (1.31) i. The discretised version of X is de.ned by X. = xi, if i. = X< (i + 1). (1.32) 
with probability distribution p(xi) = Pr{X. = xi} = f (xi).. Thus, the entropy of X. is given by H(X.)= 
- L p(xi) log p(xi)= - L f (xi). log(f (xi).) ii = - L f (xi). log f (xi) - log. . (1.33) i If f (x) 
log f (x) is Riemann integrable, we obtain that lim (H(X.) - log .) = h(X), (1.34) ..0 since h(X) = lim..0(- 
Lf (xi). log f (xi)). Thus, in general, the entropy of a con­ i tinuous random variable does not equal 
the entropy of the discretized random vari­able in the limit of a .ner discretisation. We can also see 
that, due to the fact that - lim..0 log .= 8, the entropy H(X.) goes to in.nity when the bin size goes 
to zero: lim H(X.)= 8. (1.35) ..0 For instance, if f (x)=1/k in the interval (0,k) (Fig. 1.6.b), then 
h(X)= - J k(1/k) log(1/k)dx = log k. Observe that the di.erential entropy is negative when k< 1.  (a)(b) 
In contrast with the behavior of the di.erential entropy, the mutual information between two continuous 
random variables X and Y is the limit of the mutual informa­tion between their discretised versions. 
Thus, in the limit of a .ner discretisation we get Ic(X; Y ) = lim ..0 I(X.; Y.). (1.36 ) Kolmogorov 
[14] and Pinsker [19] de.ned mutual information as Ic(X; Y )= supP,QI([X]P ;[Y ]Q), where the supremum 
(sup) is over all .nite partitions P and Q of X and Y , respectively. From this de.nition and Equ. 1.36, 
two important properties can be derived: the continuous mutual information is the least upper bound for 
the discrete mutual information and re.nement can never decrease the discrete mutual in­formation. This 
last property can also be deduced from the data processing inequality (Equ. 1.18) [10]. 1.7 Information 
Bottleneck Method The information bottleneck method, introduced by Tishby et al. [29], is a technique 
that extracts a compact representation of the variable X, denoted by X., with minimal loss of mutual 
information with respect to another variable Y (i.e., X preserves as much information as possible about 
the control variable Y ). Thus, given an information channel between X and Y , the information bottleneck 
method tries to .nd the optimal tradeo. between accuracy and compression of X when the bins of this variable 
are clustered.  Soft [29] and hard [25] partitions of X can be adopted. In the .rst case, every x .X 
can be assigned to a cluster x .X with some conditional probability p( x|x) (soft clustering). In the 
second case, every x .X is assigned to only one cluster x .X (hard clustering). In this book, we consider 
hard partitions and we focus our attention on the agglomerative information bottleneck method [25]. Given 
a cluster x de.ned by x = {x1,...,xl}, where xk .X for all k .{1,...,l}, and the probabilities p( x) 
and p(y|x ) de.ned by l L p( x)= p(xk), (1.37) k=1 l 1 p(y|x )= L p(xk,y) .y .Y, (1.38) p( x) k=1 the 
following properties are ful.lled: The decrease in the mutual information I(X;Y ) due to the merge of 
x1,...,xl is given by dIx = p( x)JS(p1,...,p l;p1,...,pl) = 0, (1.39) where the weights and probability 
distributions of the JS-divergence are given by pk = p(xk)/p( x) and pk = p(Y |xk) for all k .{1,...,l}, 
respectively. An optimal clustering algorithm should minimize dIx . An optimal merge of l components 
can be obtained by l - 1 consecutive optimal merges of pairs of components. 1.8 f-Divergences Many di.erent 
measures quantifying the divergence between two probability distribu­tions have been studied in the past. 
They are frequently called distances , although some of them are not strictly metrics. Some particular 
examples of divergences play an important role in di.erent .elds such as statistics and information theory 
[17]. Next, we present a measure of divergence between two probability distributions called f-divergence. 
This measure was independently introduced by Csisz´ar [5] and Ali and Silvey [1]. The following de.nition 
is taken from Csisz´ar and Shields [6]. De.nition 1.13. Let f (t) be a convex function de.ned for t> 
0, with f (1) = 0. The f-divergence of a distribution p from q is de.ned by (p(x)) Df(p, q)= L q(x)fq(x) 
, (1.40) x.X where the conventions 0f (0/0) = 0, f (0) = limt.0 f (t), 0f (a/0) = limt.0 tf (a/t)= a 
limu.8(f (u)/u) are adopted. For the purposes of this book, we present three of the most important f­divergences: 
Kullback-Leibler, Chi-square, and Hellinger distances. These can be ob­tained from di.erent convex functions 
f (see Fig. 1.7): 1.5 1 0.5 0 -0.5 FIGURE 1.7: Plots for three strictly convex functions: t log t 
(blue), (t - 1)2 (red), and v 1/2(1 - t)2 (green). From these functions, the Kullback-Leibler, Chi-square, 
and Hellinger distances are obtained, respectively. Kullback-Leibler distance or information divergence 
[15]: If f (t)= t log t, the Kullback-Leibler distance is given by p(x) DKL(p, q)= L p(x) log q(x) . 
(1.41) x.X Chi-square distance [18]: If f (t)=(t - 1)2, the Chi-square distance is given by (p(x) - q(x))2 
D.2 (p, q)= L q(x) . (1.42) x.X  Hellinger distance [12]: v If f (t)=1/2(1 - t)2, the Hellinger distance 
is given by 1 Dh2 (p, q)= L(.p(x) - .q(x))2 . (1.43) 2 x.X Note that none of the above distances ful.lls 
all the properties of a metric. However, the square root of the Hellinger distance is a true metric [7]. 
According to Csisz´ar and Shields [6], f-divergences generalize the Kullback-Leibler distance. Using 
the analogue of the log-sum inequality (Sec. 1.3.2), given by nn Ln ai LL i=1 ai bif - bi f = 0, (1.44) 
Ln bii=1 bi i=1 i=1 many of the properties of the information divergence extend to general f-divergences. 
If f is strictly convex the equality in Equ. 1.44 holds if and only if ai/bi is constant for all i. 
1.9 Generalized Entropies R´enyi [20] proposed a generalized entropy which recovers the Shannon entropy 
as a special case, and, Harvda and Charv´at [11] introduced a new generalized de.nition of entropy which 
also includes the Shannon entropy as a particular case. Sharma and Mittal [23], and Sharma and Taneja 
[24] introduced two-parameter entropies where R´enyi and Harvda-Charv´at entropies are particular cases. 
Tsallis [30] used the Harvda­Charv´at entropy in order to generalize the Boltzmann entropy in statistical 
mechanics. The introduction of this entropy responds to the objective of generalizing the statistical 
mechanics to non-extensive systems15 . For the objectives of this book we review the so-called Harvda-Charv´at-Tsallis 
entropy. De.nition 1.14. The Harvda-Charv´at-Tsallis entropy HaT (X) of a discrete random variable X 
is de.ned by 1 - L x.X p(x)a HaT (X)= k, (1.45) a - 1 15 An extensive system ful.lls that quantities 
like energy and entropy are proportional to the system size. Similarly to Shannon entropy, a fundamental 
property of the Boltzmann entropy is its additivity. That is, if we consider a system composed by two 
probabilistically independent subsystems X and Y (i.e., p(x, y)= p(x)p(y)), then H(X,Y) = H(X) + H(Y). 
This property ensures the extensivity of the entropy but strongly correlated systems present non-extensive 
properties that require another type of entropy ful.lling non-additivity. Tsallis proposed the Harvda-Charv´at 
entropy in order to deal with these pathological systems.  
			