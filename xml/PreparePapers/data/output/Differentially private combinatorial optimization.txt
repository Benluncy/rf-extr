
 Di.erentially Private Combinatorial Optimization Anupam Gupta Katrina Ligett Frank McSherry Aaron Roth 
Kunal Talwar October 11, 2009 Abstract Consider the following problem: given a metric space, some of 
whose points are clients, select a set of at most k facility locations to minimize the average distance 
from the clients to their nearest facility. This is just the well-studied k-median problem, for which 
many approximation algorithms and hardness results are known. Note that the objective function encourages 
opening facilities in areas where there are many clients, and given a solution, it is often possible 
to get a good idea of where the clients are located. This raises the following quandary: what if the 
locations of the clients are sensitive information that we would like to keep private? Is it even possible 
to design good algorithms for this problem that preserve the privacy of the clients? In this paper, we 
initiate a systematic study of algo­rithms for discrete optimization problems in the frame­work of di.erential 
privacy (which formalizes the idea of protecting the privacy of individual input elements). We show that 
many such problems indeed have good approximation algorithms that preserve di.erential pri­vacy; this 
is even in cases where it is impossible to pre­serve cryptographic de.nitions of privacy while comput­ing 
any non-trivial approximation to even the value of an optimal solution, let alone the entire solution. 
Apart from the k-median problem, we consider the problems of vertex and set cover, min-cut, k-median, 
facility location, and Steiner tree, and give approxi­mation algorithms and lower bounds for these prob­lems. 
We also consider the recently introduced sub­modular maximization problem, Combinatorial Public Projects 
(CPP), shown by Papadimitriou et al. [28] to be inapproximable to subpolynomial multiplicative factors 
by any e.cient and truthful algorithm. We give a di.erentially private (and hence approximately truth­ful) 
algorithm that achieves a logarithmic additive ap­proximation. 1 Introduction Consider the following 
problems: Assign people using a social network to one of two servers so that most pairs of friends are 
assigned to the same server. Open some number of HIV treatment centers so that the average commute time 
for patients is small.  Open a small number of drop-o. centers for un­dercover agents so that each agent 
is able to visit some site convenient to her (each providing a list of acceptable sites).  The above 
problems can be modeled as instances of well-known combinatorial optimization problems: re­spectively 
the minimum cut problem, the k-median problem, and the set cover problem. Good heuristics have been designed 
for these problems, and hence they may be considered well-studied and solved. However, in the above scenarios 
and in many others, the input data (friendship relations, medical history, agents lo­cations) represent 
sensitive information about individu­als. Data privacy is a crucial design goal, and it may be vastly 
preferable to use a private algorithm that gives somewhat suboptimal solutions to a non-private opti­mal 
algorithm. This leads us to the following central questions: Given that the most benign of actions pos­sibly 
leaks sensitive information, how should we design algorithms for the above problems? What are the funda­mental 
trade-o.s between the utility of these algorithms and the privacy guarantees they give us? The notion 
of privacy we consider in this paper is that of di.erential privacy. Informally, di.erential privacy 
guarantees that the distribution of outcomes of the computation does not change signi.cantly when one 
individual changes her input data. This is a very strong privacy guarantee: anything signi.cant about 
any individual that an adversary could learn from the algorithm s output, he could also learn were the 
individual not participating in the database at all and this holds true no matter what auxiliary information 
the adversary may have. This de.nition guarantees privacy of an individual s sensitive data, while allowing 
the computation to respond when a large number of individuals change their data, as any useful computation 
must do. 1.1 Our Results In this paper we initiate a system­atic study of designing algorithms for combinatorial 
op­timization problems under the constraint of di.erential privacy. Here is a short summary of some of 
the main contributions of our work.  While the exponential mechanism of [25] is an easy way to obtain 
computationally ine.cient private approximation algorithms for some problems, the approximation guarantees 
given by a direct appli­cation of this can be far from optimal (e.g., see our results on min-cut and 
weighted set cover). In these cases, we have to use di.erent techniques often more sophisticated applications 
of the expo­nential mechanism to get good (albeit computa­tionally expensive) solutions.  However, we 
want our algorithms to be computa­tionally e.cient and private at the same time: here we cannot use the 
exponential mechanism directly, and hence we develop new algorithmic ideas. We give private algorithms 
for a wide variety of search problems, where we must not only approximate the value of the solution, 
but also produce a solution that optimizes this value. See Table 1 for our re­sults.  For some problems, 
unfortunately, just outputting an explicit solution might leak private information. For example, if we 
output a vertex cover of some graph explicitly, any pair of vertices not output reveals that they do 
not share an edge so any private explicit vertex cover algorithm must output n - 1 vertices. To overcome 
this hurdle, we instead privately output an implicit representation of a small vertex cover we view vertex 
cover as a location problem, and output an orientation of the edges. Each edge can cover itself using 
the end point that it points to. The orientation is output privately, and the resulting vertex cover 
approximates the optimal vertex cover well. We deal with similar representational issues for other problems 
like set cover as well.  We also show lower bounds on the approximation guarantees regardless of computational 
considera­tions. For example, for vertex cover, we show that any E-di.erentially private algorithm must 
have an approximation guarantee of O(1/E). We show that each of our lower bounds are tight: we give (com­putationally 
ine.cient) algorithms with matching approximation guarantees.  Our results have implications beyond 
privacy as well: Papadimitriou et al. [28] introduce the Com­binatorial Public Project problem, a special 
case of submodular maximization, and show that the prob­lem can be well approximated by either a truth­ful 
mechanism or an e.cient algorithm, but not by both simultaneously. In contrast to this nega­tive result, 
we show that under di.erential privacy (which can be interpreted as an approximate but  robust alternative 
to truthfulness) we can achieve the same approximation factor as the best non­truthful algorithm, plus 
an additive logarithmic loss. Finally, we develop a private ampli.cation lemma: we show how to take private 
algorithms that gives bounds in expectation and e.ciently convert them (privately) into bounds with high 
probability. This answers an open question in the paper of Feldman et al. [15].  Table 1 summarizes 
the bounds we prove in this paper. For each problem, it reports (in the .rst column) the best known non-private 
approximation guarantees, (in the second column) our best e.cient E-di.erentially private algorithms, 
and in each (in the third column) case matching upper and lower bounds for ine.cient E-di.erentially 
private algorithms. For a few of the e.cient algorithms (marked with a ) the guarantees are only for 
an approximate form of di.erential privacy, incorporating a failure probability d, and scaling the e.ective 
value of E up by ln(1/d). 1.2 Related Work Di.erential privacy is a rela­tively recent privacy de.nition 
(e.g., see [11, 8, 27, 6, 23, 15, 12], and see [9] for an excellent survey), that tries to capture the 
intuition of individual privacy. Many algo­rithms in this framework have focused on measurement, statistics, 
and learning tasks applied to statistical data sets, rather than on processing and producing combina­torial 
objects. One exception to this is the Exponential Mechanism of [25] which allows the selection from a 
set of discrete alternatives. Independently, Feldman et al. [15] also consider the problem of privately 
approximating k-medians for points in 1d . Their model di.ers slightly from ours, which makes the results 
largely incomparable: while our results for general metrics translated to 1d give smaller additive errors 
than theirs, we only output a k­median approximation whereas they output coresets for the problem. Their 
lower bound argument for private coresets is similar to ours. Prior work on Secure Function Evaluation 
(SFE) tells us that in fact the minimum cut in a graph can be computed in a distributed fashion in such 
a way that computations reveals nothing that cannot be learnt from the output of the computation. While 
this is a strong form of a privacy guarantee, it may be unsatisfying to an individual whose private data 
can be inferred from the privately computed output. Indeed, it is not hard to come up with instances 
where an attacker with some limited auxiliary information can infer the presence or absence of speci.c 
edges from local information about the minimum cut in the graph. By relaxing the whole input privacy 
requirement of SFE, di.erential privacy Non-private E.cient Algorithms Information Theoretic Vertex 
Cover 2 × OPT [29] (2 + 16/E) × OPT T(1/E) × OPT Wtd. Vertex Cover 2 × OPT [18] (16 + 16/E) × OPT T(1/E) 
× OPT Set Cover ln n × OPT [21] O(ln n + ln m/E) × OPT  T(ln m/E) × OPT Wtd. Set Cover ln n × OPT [7] 
O(ln n(ln m + ln ln n)/E) × OPT  T(ln m/E) × OPT Min Cut OPT [16] OPT + O(ln n/E)  OPT + T(ln n/E) 
CPPP (1 - 1/e) × OPT [26] (1 - 1/e) × OPT - O(k ln m/E)  OPT - T(k ln(m/k)/E) k-Median (3 + .) × OPT 
[1] 6 × OPT + O(k2 ln2 n/E) OPT + T(k ln(n/k)/E)a  Table 1: Summary of Results. Results in the second 
and third columns are from this paper. a[15] independently prove a similar lower bound. is able to provide 
unconditional per element privacy, which SFE need not provide if the output itself discloses properties 
of input. Feigenbaum et al. [14] extend the notion of SFE to NP hard problems for which e.cient algorithms 
must output an approximation to the optimum, unless P=NP. They de.ned as functional privacy the constraint 
that two inputs with the same output value (e.g. the size of an optimal vertex cover) must produce the 
same value under the approximation algorithm. Under this constraint, Halevi et al. [17] show that approximating 
the value of vertex cover to within n1-. is as hard as computing the value itself, for any constant .. 
These hardness results were extended to search problems by Beimel et al. [2], where the constraint is 
relaxed to only equate those inputs whose sets of optimal solutions are identical. These results were 
extended and strengthened by Beimel et al. [3, 4]. Nonetheless, Feigenbaum et al. [14] and others show 
a number of positive approximation results under versions of the functional privacy model. Halevi et 
al. [17] provide positive results in the function privacy setting when the algorithm is permitted to 
leak few bits (each equivalence class of input need not produce identical output, but must be one of 
at most 2b possible outcomes). Indyk and Woodru. also give some positive results for the approximation 
of £2 distance and a nearest neighbor problem [20]. However, as functional privacy extends SFE, it does 
not protect sensitive data that can be inferred from the output. Nevertheless, SFE provides an implementation 
of any function in a distributed setting such that nothing other than the output of the function is revealed. 
One can therefore run a di.erentially private algorithm is a distributed manner using SFE (see e.g. [10, 
5]), in the absence of a trusted curator. 2 De.nitions Di.erential privacy is a privacy de.nition for 
compu­tations run against sensitive input data sets. Its re­quirement, informally, is that the computation 
behaves nearly identically on two input data sets that are nearly identical; the probability of any outcome 
must not in­crease by more than a small constant factor when the input set is altered by a single element. 
Formally, Definition 2.1. ([11]) We say a randomized compu­tation M has E-di.erential privacy if for 
any two input sets A and B with symmetric di.erence one, and for any set of outcomes S . Range(M), (2.1)Pr[M(A) 
. S] = exp(E) × Pr[M(B) . S] . The de.nition has several appealing properties from a privacy perspective. 
One that is most important for us is that arbitrary sequences of di.erentially private computations are 
also di.erentially private, with an E parameter equal to the sum of those comprising the sequence. This 
is true even when subsequent computations can depend on and incorporate the results of prior di.erentially 
private computations [10], allowing repetition of di.erentially private steps to improve solutions. 2.1 
Approximate Di.erential Privacy One re­laxation of di.erential privacy [10] allows a small ad­ ditive 
term in the bound: Definition 2.2. We say a randomized computation M has d-approximate E-di.erential 
privacy if for any two input sets A and B with symmetric di.erence one, and for any set of outcomes S 
. Range(M), (2.2) Pr[M(A) . S] = exp(E) × Pr[M(B) . S]+ d. The .avor of guarantee is that although not 
all events have their probabilities preserved, the alteration is only for very low probability events, 
and is very unlikely to happen. The d is best thought of as 1/poly(n) for a data set containing some 
subset of n candidate records. We note that there are stronger notions of approximate di.erential privacy 
(c.f. [24]), but in our settings, they are equivalent upto poly(n) changes in d. We therefore restrict 
ourselves to this de.nition here. 2.2 The Exponential Mechanism One particu­larly general tool that 
we will often use is the expo­nential mechanism of [25]. This construction allows dif­ ferentially private 
computation over arbitrary domains and ranges, parametrized by a query function q(A, r) mapping a pair 
of input data set A (a multiset over some domain) and candidate result r to a real valued score . With 
q and a target privacy value E, the mech­anism selects an output with exponential bias in favor of high 
scoring outputs: (2.3) Pr[E.(A)= r] . exp(Eq(A, r)) . q If the query function q has the property that 
any two adjacent data sets have score within . of each other, for all possible outputs r, the mechanism 
provides 2E.-di.erential privacy. Typically, we would normalize q so that . = 1. We will be using this 
mechanism almost exclusively over discrete ranges, where we can derive the following simple analogue 
of a theorem of [25], that the probability of a highly suboptimal output is exponentially low: Theorem 
2.1. The exponential mechanism, when used to select an output r . R gives 2E.-di.erential privacy, letting 
ROPT be the subset of R achieving q(A, r)= maxr q(A, r), ensures that Pr[q(A, E.(A)) < max q(A, r) - 
ln(|R|/|ROPT|)/E - t/E] q r (2.4) = exp(-t) . The proof of the theorem is almost immediate: any outcome 
with score less than maxr q(A, r) - ln(|R|/|ROPT|)/E - t/E will have normalized probabil­ity at most 
exp(-t)/|R|; each has weight at most exp(OPT - t)|ROPT|/|R|, but is normalized by at least |ROPT| exp(OPT) 
from the optimal outputs. As there are at most |R| such outputs their cumulative probabil­ity is at most 
exp(-t). 3 Private Min-Cut Given a graph G =(V, E) the minimum cut problem is to .nd a cut (S, Sc) so 
as to minimize E(S, Sc). In absence of privacy constraints, this problem is e.ciently solvable exactly. 
However, outputting an exact solution violates privacy, as we show in Section 3.1. Thus, we give an algorithm 
to output a cut within additive O(log n/E) edges of optimal. The algorithm has two stages: First, given 
a graph G, we add edges to the graph to raise the cost of the min cut to at least 4 ln n/E, in a di.erentially 
private manner. Second, we deploy the exponential mechanism over all cuts in the graph, using a theorem 
of Karger to show that for graphs with min cut at least 4 ln n/E the number of cuts within additive t 
of OPT increases no faster than exponentially with t. Although the exponential mechanism takes time exponential 
in n, we can construct a polynomial time version by considering only the polynomially many cuts within 
O(ln n/E) of OPT. Below, let Cost(H, (S, Sc)) denote the size EH (S, Sc) of the cut (S, Sc) in a graph 
H. Algorithm 1 The Min-Cut Algorithm 1: Input: G =(V, E),E. H(n 2: Let H0 . H1,..., . 2) be arbitrary 
strictly increasing sets of edges on V . so n 3: Choose index i . [0, ] with probability propor­ 2 tional 
to exp(-E|OPT(G . Hi) - 8 ln n/E|). 4: Choose a subset S . 2V \ {Ø,V } with probability proportional 
to exp(-ECost(G . Hi, (S, Sc))). 5: Output the cut C =(S, Sc). Our result relies on a result of Karger 
about the number of near-minimum cuts in a graph [22] Lemma 3.1. ([22]) For any graph G with min cut 
C, there are at most n2a cuts of size at most aC. By enlarging the size of the min cut in G . Hi to at 
least 4 ln n/E, we ensure that the number of cuts of value OPT(G . Hi)+ t is bounded by n2 exp(Et/2). 
The downweighting of the exponential mechanism will be able to counteract this growth in number and ensure 
that we select a good cut. Theorem 3.1. For any graph G, the expected cost of ALG is at most OPT + O(ln 
n/E). Proof. First, we argue that the selected index i satis.es 4 ln n/E < OPT(G . Hi) < OPT(G) + 12ln 
n/E with probability at least 1 - 1/n2 . For OPT > 8 ln n/E, Equation 2.4 ensures that the probability 
of exceeding the optimal choice (H0) by 4ln n/E is at most 1 - 1/n2 . Likewise, for OPT < 8 ln n/E, there 
is some optimal Hi achieving min cut size 8 ln n/E, and the probability we end up farther away than 4 
ln n/E is at most 1 - 1/n2 . Assuming now that OPT(G . Hi) > 4 ln n/E, Karger s lemma argues that the 
number ct of cuts in G . Hi of cost at most OPT(G . Hi)+ t is at most n2 exp(Et/2). As we are assured 
a cut of size OPT(G . Hi) exists, each cut of size OPT(G . Hi)+ t will receive probability at most exp(-Et). 
Put together, the probability of a cut exceeding OPT(G . Hi)+ b is at most Pr[Cost(G . Hi,C) > OPT(G 
. Hi)+ b] =exp(-Et)(ct - ct-1) t>b = (exp(E) - 1)exp(-Et)ct t>b 2 = (exp(E) - 1)exp(-Et/2)n t>b  
The sum telescopes to exp(-Eb/2)n2/(exp(E/2) - 1), and the denominator is within a constant factor of 
the leading factor of (exp(E)-1), for E< 1. For b = 8ln n/E, this probability becomes at most 1/n2 . 
Theorem 3.2. The algorithm above preserves 2E­di.erential privacy. Note that the .rst instance of the 
exponential mechanism in our algorithm runs e.ciently (since it so n is selecting from only objects), 
but the second 2 instance does not. We now describe how to achieve (e, d)-di.erential privacy e.ciently. 
First recall that using Karger s algorithm we can e.ciently (with high probability) generate all cuts 
of size at most kOPT for any constant k. Indeed it is shown in [22] that in a single run of his algorithm, 
any such cut -2k 2k+1 is output with probability at least nso that nruns of the algorithm will output 
all such cuts except with an exponentially small probability. Our e.cient algorithm works as follows: 
in step 4 of Algorithm 1, instead of sampling amongst all possible cuts, we restrict attention to the 
set of cuts generated in n7 runs of Karger s algorithm. We claim that the output distribution of this 
algorithm has statistical distance O(1/n2) from that of Algorithm 1, which would imply that we get (e, 
O( 1 2 ))-di.erential privacy. n Consider a hypothetical algorithm that generates the cut (S, Sc) as 
in Algorithm 1 but then outputs FAIL whenever this cut is not in the set of cuts generated by n7 runs 
of Karger s. We .rst show that the probability that this algorithm outputs FAIL is O( 1 2 ). As shown 
n above, OPT(G . Hi) is at least 4 ln n/e except with 1 probability Conditioned on this, the cut chosen 
2 . n in Step 4 has cost at most 3OPT(G . Hi) except with 1 probability Since each such cut is in the 
sample 2 . n except with exponentially small probability, the claim follows. Finally, note that this 
hypothetical algorithm can be naturally coupled with both the algorithms so that the outputs agree whenever 
the former doesn t output FAIL. This implies the claimed bound on the statistical distance. We remark 
that we have not attempted to optimize the running time here; both the running time and the value of 
d can be improved by choosing a larger constant (instead of 8) in Step 3, at the cost of increasing the 
additive error by an additional constant. , 3.1 Lower Bounds We next show that this addi­tive error is 
unavoidable for any di.erentially private algorithm. The lower bound is information-theoretic and thus 
applies also to computationally ine.cient al­gorithms. Theorem 3.3. Any E-di.erentially private algorithm 
for min-cut must incur an expected additive O(ln n/E) 1 cost over OPT, for any E . (3 ln n/n, ). 12 
Proof. Consider a ln n/3E-regular graph G =(V, E) on n vertices such that the minimum cuts are exactly 
those that isolate a single vertex, and any other cut has size at least (ln n/2E) (a simple probabilistic 
argument establishes the existence of such a G; in fact a randomly chosen ln n/3E-regular graph has this 
property with high probability). Let M be an E-di.erentially private algorithm for the min-cut. Given 
the graph G, M outputs a partition of V . Since there are n = |V | singleton cuts, there exists a vertex 
v such that the mechanism M run on G outputs the cut ({v},V \{v}) with probability at most 1/n, i.e. 
1 Pr[M(V, E)=({v},V \{v}) = . n Now consider the graph G' =(V, E'), with the edges incident on v removed 
from G, i.e. E' = E \{e : v . e}. Since M satis.es E-di.erential privacy and E and E' di.er in at most 
ln n/3E edges, Pr[M(V, E')=({v},V \{v})] = 1/n1/3 . 1 Thus with probability (1 - ), M(G') outputs a 1 
n 3 cut other than the minimum cut ({v},V \{v}). But all other cuts, even with these edges removed, 
cost at least (ln n/6E). Since OPTis zero for G', the claim follows. 4 Private k-Median We next consider 
a private version of the metric k­median problem: There is a pre-speci.ed set of points V and a metric 
on them, d : V × V . R. There is a (private) set of demand points D . V . We wish to select a set of 
medians F . V with |F | = k to  minimize the quantity cost(F )=d(v, F ) where v.D d(v, F ) = minf.F 
d(v, f). Let.=maxu,v.V d(u, v) be the diameter of the space. As we show in Section 4.1, any privacy-preserving 
algorithm for k-median must incur an additive loss of O(. · k ln(n/k)/E), regardless of computational 
con­straints. We observe that running the exponential so n mechanism to choose one of the k subsets 
of medians gives an (computationally ine.cient) additive guaran­tee. Theorem 4.1. Using the exponential 
mechanism to so n pick a set of k facilities gives an O( poly(n))-time k E-di.erentially private algorithm 
that outputs a solution with expected cost OPT + O(k. log n/E). We next give a polynomial-time algorithm 
that gives a slightly worse approximation guarantee. Our algorithm is based on the local search algorithm 
of Arya et al. [1]. We start with an arbitrary set of k medians, and use the exponential mechanism to 
look for a (usually) improving swap. After running this local search for a suitable number of steps, 
we select a good solution from amongst the ones seen during the local search. The following result shows 
that if the current solution is far from optimal, then one can .nd improving swaps. Theorem 4.2. (Arya 
et al. [1]) For any set F . V with |F | = k, there exists a set of k swaps k (x1,y1),..., (xk,yk) such 
that (cost(F ) - cost(F - i=1 {xi} + {yi})) = cost(F ) - 5OPT. Corollary 4.1. For any set F . V with 
|F | = k, there exists some swap (x, y) such that cost(F ) - 5OPT cost(F ) - cost(F -{xi} + {yi}) = . 
k Algorithm 2 The k-Median Algorithm 1: Input: V , Demand points D . V , k,E. 2: let F1 . V arbitrarily 
with |F1| = k, E ' . E/(2.(T + 1)). 3: for i =1 to T do 4: Select (x, y) . Fi × (V \ Fi) with probability 
proportional to exp(-E ' × cost(Fi -{x} + {y})). 5: let Fi+1 . Fi -{x} + {y}. 6: end for 7: Select j 
from {1, 2,...,T } with probability propor­tional to exp(-E ' × cost(Fj )). 8: output Fj. Theorem 4.3. 
Setting T =6k ln n and E ' = E/(2.(T + 1)), the k-median algorithm provides E-di.erential pri­vacy and 
except with probability O(1/poly(n)) outputs a solution of cost at most 6OPT + O(.k2 log2 n/E). Proof. 
We .rst prove the privacy. Since the cost function has sensitivity ., Step 4 of the algorithm preserves 
2E '. di.erential privacy. Since Step 4 is run at most T times and privacy composes additively, outputting 
all of the T candidate solutions would give us (2E '.T ) di.erential privacy. Picking out a good solution 
from the T candidates costs us another 2E '., leading to the stated privacy guarantee. We next show the 
approximation guarantee. By Corollary 4.1, so long as cost(Fi) = 6OPT, there exists a swap (x, y) that 
reduces the cost by at least cost(Fi)/6k. 2 As there are only npossible swaps, the exponential mechanism 
ensures through (2.4) that we are within additive 4 ln n/E ' with probability at least 1 - 1/n2 . When 
cost(Fi) = 6OPT + 24k ln n/E ', with probability 1 - 1/n2 we have cost(Fi+1) = (1 - 1/6k) × cost(Fi). 
This multiplicative decrease by (1 - 1/6k) applies for as long as cost(Fi) = 6OPT + 24k ln n/E ' . Since 
cost(F0) = n., and n.(1 - 1/6k)T = . = 24k ln n/E ' , there must exist an i<T such that cost(Fi) = 6OPT 
+ 24k ln n/E ', with probability at least (1 - T/n2). Finally, by applying the exponential mechanism 
again in the .nal stage, we select from the Fi scor­ing within an additive 4 ln n/E ' of the optimal 
visited Fi with probability at least 1 - 1/n2, again by (2.4). Plugging in the value of E ', we get the 
desired result. Increasing the constants in the additive term can drive the probability of failure to 
an arbitrarily small polyno­mial. 4.1 k-Median Lower Bound Theorem 4.4. Any E-di.erentially private algorithm 
for the k-median problem must incur cost OPT + O(. · k ln(n/k)/E) on some inputs. Proof. Consider a point 
set V =[n] × [L] of nL points, with L = ln(n/k)/10E, and a distance func­tion d((i, j), (i ' ,j ')) = 
. whenever i = i ' and d((i, j), (i, j '))=0. Let M be a di.erentially private algorithm that takes a 
subset D . V and outputs a set of k locations, for some k< n . Given the nature of the 4 metric space, 
we assume that M outputs a k-subset of [n]. Foraset A . [n], let DA = A × [L]. Let A be a size-k subset 
of V chosen at random. k We claim that that EA,M [|M(DA) n A|] = for 2 any E-di.erentially private algorithm 
M. Before we prove this claim, note that it implies the expected cost k of M(DA) is × .L, which proves 
the claim since 2 OPT = 0. 1 Now to prove the claim: de.ne f := EA,M [|A n k M(DA)|]. We can rewrite 
k · f = EA,M [|A n M(DA)|] = k · Ei.[n]EA\{i},M [1i.M(DA)] Now changing A to A ' := A\{i}+{i ' } for 
some random i ' requires altering at most 2L elements in DA' , which by the di.erential privacy guarantee 
should change the 2 .L probability of the output by at most e=(n/k)1/5 . Hence Ei.[n]EA',M [1i.M(DA' 
)] = f · (k/n)1/5 . But the expression on the left is just k/n, since there at at most k medians. Hence 
f = (k/n)4/5 = 1/2, which proves the claim. Corollary 4.2. Any 1-di.erentially private algorithm for 
uniform facility location that outputs the setv of chosen facilities must have approximation ratio O(n). 
Proof. We consider instances de.ned on the uniform metric on n points, with d(u, v) = 1 forall u, v, 
and 1 facility opening cost f = v . Consider a 1-di.erentially n private mechanism M when run on a 
randomly chosen v subset A of size k = n. Since OPT is kf = 1 for v these instances, any o(n)-approximation 
must select k at least locations from A in expectation. By an 2 argument analogous to the above theorem, 
it follows that any di.erentially private M must output n/20 of the locations in expectation. This leads 
to a facility v opening cost of O(n). 4.2 Euclidean Setting Feldman et al. [15] study private coresets 
for the k-median problem when the in­put points are in 1d . For P points in the unit ball in 1d , they 
give coresets with (1 + e) multiplicative error, and additive errors about O(k2d2 log2 P ) and O(16kd)2dd3/2 
log P log dk) respectively for their ine.­cient and e.cient algorithms. Since Euclidean k-median has 
a PTAS, this leads to k-median approximations with the same guarantees. We can translate our results 
to their setting by looking at a (1/P )-net of the unit ball as the candidate set of n-points, of which 
some may ap­pear. This would lead to an ine.cient algorithm with additive error O(kd log P ), and an 
e.cient algorithm with additive error O(k2d2 log2 P ). The latter has a multiplicative error of 6 and 
hence our e.cient algo­rithms are incomparable. Note that coresets are more general objects than just 
the k-median solution. 5 Vertex Cover We now turn to the problem of (unweighted) vertex cover, where 
we want to pick a set S of vertices of minimal size so that every edge in the graph is incident to at 
least one vertex in S. In the privacy-preserving version of the problem, the private information we wish 
to conceal is the presence of absence of each edge. Approximating the Vertex Cover Size. As mentioned 
earlier, even approximating the vertex cover size was shown to be polynomially inapproximable under the 
constraint of functional privacy [17, 2]. On the other hand, it is easy to approximate the size of the 
optimal vertex cover under di.erential privacy: twice the size of a maximum matching is a 2-approximation 
to the optimal vertex cover, and this value only changes by at most two with the presence or absence 
of a single edge. Hence, this value plus Laplace(2/E) noise provides E-di.erential privacy [11]. (Here 
it is important that we use maximum rather than just maximal matchings, since the size of the latter 
is not uniquely determined by the graph, and the presence or absence of an edge may dramatically alter 
the size of the solution.) Interestingly enough, for weighted vertex cover with maximum weight wmax (which 
we study in Section 5.2), we have to add in Lap(wmax/E) noise to privately estimate the weight of the 
optimal solution, which can be much larger than OPT itself. The mechanism in Section 5.2 avoids this 
barrier by outputting an implicit representation of the vertex cover, and hence gives us a O(1/E) multiplicative 
approximation with E-di.erential privacy. The Vertex Cover Search Problem. If we want to .nd a vertex 
cover (and not just estimate its size), how can we do this privately? In covering problems, the (private) 
data imposes hard constraints on the a solution, making them quite di.erent from, say, min­cut. Indeed, 
while the private data only in.uences the objective function in the min-cut problem, the data determines 
the constraints de.ning feasible solutions in the case of the vertex cover problem. This hard covering 
constraint make it impossible to actually output a small vertex cover privately: as noted in the introduction, 
any di.erentially private algorithm for vertex cover that outputs an explicit vertex cover (a subset 
of the n vertices) must output a cover of size at least n - 1 with probability 1 on any input, an essentially 
useless result. In order to address this challenge, we require our al­gorithms to output an implicit 
representation of a cover: we privately output an orientation of the edges. Now for each edge, if we 
pick the endpoint that it points to, we clearly get a vertex cover. Our analysis ensures that this vertex 
cover has size not much larger than the size of the optimal vertex cover for the instance. Hence, such 
an orientation may be viewed as a privacy-preserving set of instructions that allows for the construction 
of a good vertex cover in a distributed manner: in the case of the undercover agents mentioned in the 
introduction, the complete set of active dropo. sites (nodes) is not revealed to the agents, but an orientation 
on the edges tells each agent which dropo. site to use, if she is indeed an active agent. Our algorithms 
in fact output a per­mutation of all the vertices of the graph. Each edge can be considered oriented 
towards the endpoint appearing earlier in the permutation. Our lower bounds apply to the more general 
setting where we are allowed to output any orientation (and hence are stronger). 5.1 The Algorithm for 
Unweighted Vertex Cover Our (randomized) algorithm will output a per­mutation, and the vertex cover will 
be de.ned by pick­ing, for each edge, whichever of its endpoints appears .rst in the permutation. We 
show that this vertex cover will be (2+O(1/E))-approximate and E-di.erentially pri­vate. Our algorithm 
is based on a simple (non-private) 2-approximation to vertex cover [29] that repeatedly se­ lects an 
uncovered edge uniformly at random, and in­cludes a random endpoint of the edge. We can view the process, 
equivalently, as selecting a vertex at random with probability proportional to its uncovered degree. 
We will take this formulation and mix in a uniform dis­tribution over the vertices, using a weight that 
will grow as the number of remaining vertices decreases. Let us start from G1 = G, and let Gi be the 
graph with n - i + 1 vertices remaining. We will write dv(G) for the degree of vertex v in graph G. The 
algorithm ALG in step i chooses from the n - i + 1 vertices of Gi with probability proportional to dv(Gi)+ 
wi, for an appropriate sequence (wi). Taking wi = (4/E)×(n/(n- i +1))1/2 provides E-di.erential privacy 
and a (2+16/E) approximation factor, the proof of which will follow from the forthcoming Theorem 5.1 
and Theorem 5.2.  As stated the algorithm outputs a sequence of vertices, one per iteration. As remarked 
above, this permutation de.nes a vertex cover by picking the earlier occurring end point of each edge. 
Algorithm 3 Unweighted Vertex Cover 1: let n .|V |, V1 . V, E1 . E. 2: for i =1, 2,...,n do o 3: let 
wi . (4/E) × n/(n - i + 1). 4: pick a vertex v . Vi with probability proportional to dEi (v)+ wi. 5: 
output v. let Vi+1 . Vi \{v}, Ei+1 . Ei \ ({v}× Vi). 6: end for Theorem 5.1. (Privacy) ALG satis.es E-di.erential 
privacy for the settings of wi above. Proof. For any two sets of edges A and B, and any permutation p, 
let di be the degree of the ith vertex in the permutation p and let mi be the remaining edges, both ignoring 
edges incident to the .rst i - 1 vertices in p. Pr[ALG(A)= p] Pr[ALG(B)= p] n (wi + di(A))/((n - i + 
1)wi +2mi(A))=(wi + di(B))/((n - i + 1)wi +2mi(B)) . i=1 When A and B di.er in exactly one edge, di(A)= 
di(B) for all i except the .rst endpoint incident to the edge in the di.erence. Until this term mi(A) 
and mi(B) di.er by exactly one, and after this term mi(A)= mi(B). The number of nodes is always equal, 
of course. Letting j be the index in p of the .rst endpoint of the edge in di.erence, we can cancel all 
terms after j and rewrite Pr[ALG(A)= p] Pr[ALG(B)= p] wj + dj(A)(n - i + 1)wi +2mi(B) = × . wj + dj 
(B) i=j (n - i + 1)wi +2mi(A) An edge may have arrived in A, in which case mi(A)= mi(B)+1 forall i = 
j, and each term in the product is at most one; moreover, dj (A)= dj(B) + 1, and hence the leading term 
is at most 1+1/wj < exp(1/w1), which is bounded by exp(E/2). Alternately, an edge may have departed from 
A, in which case the lead term is no more than one, but each term in the product exceeds one and their 
product must now be bounded. Note that mi(A)+1 = mi(B) for all relevant i, and that by ignoring all other 
edges we only make the product larger. Simplifying, and using 1+ x = exp(x), we see  (n - i + 1)wi +2mi(B) 
i=j (n - i + 1)wi +2mi(A) (n - i + 1)wi +2 = (n - i + 1)wi +0 i=j 2 =1+ i=j(n - i + 1)wi . . 2 = exp 
. .. i=j (n - i + 1)wi The wi are chosen so that 2/(n - i + 1)wi = i vv (e/ n)1/2 i is at most e. i 
 Theorem 5.2. (Accuracy) For all G, E[ALG(G)] = (2 + 2 avgi=n wi) ×|OP T (G)|= (2 + 16/E)|OPT(G)|. Proof. 
Let OP T (G) denote an arbitrary optimal solu­tion to the vertex cover problem on G. The proof is inductive, 
on the size n of G. For G with |OP T (G)| > n/2, the theorem holds. For G with |OP T (G)|= n/2, the expected 
cost of the algorithm is the probability that the chosen vertex v is incident to an edge, plus the expected 
cost of ALG(G \ v). E[ALG(G)] = Pr[v incident on edge] +Ev[E[ALG(G \ v)]] . We will bound the second 
term using the inductive hypothesis. To bound the .rst term, the probabil­ity that v is chosen incident 
to an edge is at most (2mwn +2m)/(nwn +2m), as there are at most 2m vertices incident to edges. On the 
other hand, the prob­ability that we pick a vertex in OP T (G) is at least (|OP T (G)|wn + m)/(nwn +2m). 
Since |OP T (G)| is non-negative, we conclude that Pr[v incident on edge] = (2 + 2wn)(m/(nwn +2m)) = 
(2 + 2wn)Pr[v . OP T (G)] Since 1[v . OP T (G)] =|OP T (G)|-|OP T (G \ v)|, and using the inductive hypothesis, 
we get E[ALG(G)] = (2 + 2wn) × (|OP T (G)|- Ev[|OP T (G \ v)|]) +(2 + 2avg wi) × Ev[|OP T (G \ v)|] i<n 
= (2+2wn) ×|OP T (G)|+(2 avg wi - 2wn) × Ev[|OP T (G \ v)|] i<n  The probability that v is from an optimal 
vertex cover is at least (|OP T (G)|wi +m)/(nwi +2m), as mentioned above, and (using (a + b)/(c + d) 
= min{a/c, b/d}) is at least min{|OP T (G)|/n, 1/2} = |OP T (G)|/n, since |OP T (G)| < n/2 by assumption. 
Thus E[|OP T (G \ v)|] is bounded above by (1 - 1/n) ×|OP T (G)|, giving E[ALG(G)] = (2 + 2wn) ×|OP T 
(G)|+(2 avg wi - 2wn) × (1 - 1/n) ×|OP T (G)| . i<n Simpli.cation yields the claimed results, and instanti­ating 
wi completes the proof. Hallucinated Edges. Here is a slightly di.erent way to implement the intuition 
behind the above algo­rithm: imagine adding O(1/E) hallucinated edges to each vertex (the other endpoints 
of these hallucinated edges being fresh hallucinated vertices), and then sampling vertices without replacement 
proportional to these altered degrees. However, once (say) n/2 vertices have been sampled, output the 
remaining vertices in random order. This view will be useful to keep in mind for the weighted vertex 
cover proof. (A formal analysis of this algorithm appears in the full version.) 5.2 Weighted Vertex Cover 
In the weighted ver­tex cover problem, each vertex V is assigned a weight w(v), and the cost of any vertex 
cover is the sum of the weights of the participating vertices. One can extend the unweighted 2-approximation 
that draws vertices at random with probability proportional to their uncov­ered degree to a weighted 
2-approximation by drawing vertices with probability proportional to their uncovered degree divided by 
their weight. The di.erentially pri­vate analog of this algorithm essentially draws vertices with probability 
proportional to 1/E plus their degree, all divided by the weight of the vertex; the algorithm we present 
here is based on this idea. De.ne the score of a vertex to be s(v)=1/w(v). Our algorithm involves hallucinating 
edges: to each vertex, we add in 1/E hallucinated edges, the other endpoints of which are imaginary vertices, 
whose weight is considered to be 8 (and hence has zero score). The score of an edge e =(u, v) is de.ned 
to be s(e)= s(u)+s(v); hence the score of a fake edge f incident on u is s(f)= s(u), since its other 
(imaginary) endpoint has in.nite weight and zero score. We will draw edges with probability proportional 
to their score, and then select an endpoint to output with probability proportional to its score. In 
addition, once a substantial number of vertices of at least a particular weight have been output, we 
will output the rest of those vertices. Assume the minimum vertex weight is 1 and the maximum is 2J . 
For simplicity, we round the weight of each vertex up to a power of 2, at a potential loss of a factor 
of two in the approximation. De.ne the jth weight class Vj to be the set of vertices of weight 2j. In 
addition, we will assume that |Vj | = |Vj+1| for all weight classes. In order to achieve this, we hallucinate 
additional fake vertices. We will never actually output a hallucinated vertex. Let Nj denote |Vj|. Algorithm 
4 Weighted Vertex Cover 1: while not all vertices have been output do 2: pick an uncovered (real or hallucinated) 
edge e =(u, v) with probability proportional to s(e). 3: output endpoint u . e with probability propor­tional 
to s(u). 4: while there exists some weight class Vj such that the number of nodes of class j or higher 
that we ve output is at least Nj /2= |Vj |/2 do 5: pick the smallest such value of j 6: output ( dump 
) all remaining vertices in Vj in random order. 7: end while 8: end while We imagine the ith iteration 
of the outer loop of the algorithm as happening at time i; note that one vertex is output in Step 3, 
whereas multiple vertices might be output in Step 6. Let nmi be the sum of the scores of all real vertices 
not output before time i, and m i be the sum of the scores of all real edges not covered before time 
i. 5.2.1 Privacy Analysis Theorem 5.3. The weighted vertex cover algorithm preserves O(E) di.erential 
privacy. Proof. Consider some potential output p of the private vertex cover algorithm, and two weighted 
vertex cover instances A and B that are identical except for one edge e =(p, q). Let p appear before 
q in the permutation p; since the vertex sets are the same, if the outputs of both A and B are p, then 
p will be output at the same time t in both executions. Let vt be the vertex output in Step 3 at time 
t in such an execution; note that either p = vt, or p is output in Step 6 after vt is output. The probability 
that (conditioned on the history) a surviving vertex v is output in Step 3 of the algorithm at time i 
is: Pr[pick e] · Pr[output v | pick e] edges e s(e) s(v)(d(v)+1/E) · s(v) = · = . m i + nmi/E s(e) m 
i + nmi/E e:v Since we compare the runs of the algorithm on A and B which di.er only in edge e, these 
will be identical after time t when e is covered, and hence B e+e Pr[M(A)=p](dA(vt)+1/. )s(vt) mni/. 
= i . i=tmA Pr[M(B)=p](dB(vt)+1/. )s(vt)e+nei/. i Note that if the extra edge e . A \ B then BA dA(vt) 
= dB(vt)+1 and m = m i , so the ratio of i the probabilities is at most 1 + E< exp(E). Otherwise, BA 
the leading term is less than 1 and m = m + s(e), ii and we get  Pr[M(A)=p] s(e)1 =1+ = exps(e) · E 
· Pr[M(B)=p] i=tnei/.i=tnei Let Tj be the time steps i = t where vertices in Vj are output in p. Letting 
2j * be the weight of the lighter 1 endpoint of edge e, we can break the sum into i=tnei two pieces and 
analyze each separately: 1 11 =+ , i=tnei j=j* i.Tj nei j>j* i.Tj nei For the .rst partial sum, for some 
j = j* , let 111 1 = ++ ... + such that i0 >i1 > i.Tj nei nei0 nei1 nei. = 2-j * ... > i.. We claim 
that nmi0 Nj* /2. Indeed, since e has not yet been covered, we must have output fewer than Nj* /2 vertices 
from levels j* or higher, and hence at least Nj* /2 remaining vertices from Vj* contribute to nmi0 . 
In each time step in Tj , at least one vertex of score = 2-j * 2-j 2-j is output, so we have that nmi£ 
Nj* /2+£·. Hence 11 1 = ++ ... i.Tj nei 2-j* 2-j* Nj* /2 Nj* /2+2-j 1 + . 2-j* Nj* /2+ Nj 2-j 2-j * +j 
De.ning . = · Nj* /2, the expression above simpli.es to . + Nj 2j11 1 ++ ... + = 2j ln ..+1 .+Nj . Nj 
=2j ln 1+ . . Now using the assumption on the size of the weight classes, we have Nj = Nj* =. Nj/. = 
2j * -j+1, and 1 hence = (j* -j +2)2j, for any j = j*. Finally, i.Tj nei 1 = j=j* (j* - j + 2)2j = O(2j 
* ). j=j* i.Tj nei 1 We now consider the other partial sum . j>j* i.Tj nei For any such value of i, we 
know that nmi = 2-j * Nj* /2. Moreover, there are at most Nj* /2 times when we output a vertex from some 
weight class j = j* before we output all of Vj* ; hence there are at most Nj* /2 terms 1 in the sum, 
each of which is at most 2-j* , giving Nj* /2 a bound of 2j * on the second partial sum. Putting the 
two together, we get that Pr[M(A)= p] = exp(s(e) · E · O(2j * )) = exp(O(E)),Pr[M(B)= p] using the fact 
that s(e) = 2 · 2-j * , since the lighter endpoint of e had weight 2j * . . 5.2.2 Utility Analysis Call 
a vertex v interesting if it is incident on a real uncovered edge when it is picked. Consider the weight 
class Vj : let Ij 1 . Vj be the set of interesting vertices output due to Steps 3, and Ij 2 . Vj be the 
set of interesting vertices of class j output due to Step 6. The cost incurred by the algorithm is 2j 
(|Ij 1| + |Ij 2|). j j |] = 4(1+e) Lemma 5.1. E[2j |I1 OPT je Proof. Every interesting vertex that 
our algorithm picks in Steps 3 has at least one real edge incident on it, and at most 1 hallucinated 
edges. Conditioned on selecting e an interesting vertex v, the selection is due to a real edge with 
probability at least 1/(1 + 1 ). One can show e that the (non-private) algorithm A that selects only 
real edges is a 2-approximation [29]. On the other hand each vertex in Ij 1 can be coupled to a step 
of A with probability e/(1 + e). Since we rounded up the costs by at most a factor of two, the claim 
follows. Lemma 5.2. E[|Ij 2|] = 6 E[ |I1 |] j '=jj ' Proof. Let tj denote the time that class j is dumped. 
Recall that by (5.2.1), we pick a surviving vertex v with probability . (d(v)+ 1 ) · s(v) at each step. 
This e expression summed over all uninteresting vertices is 2-j ' .j'=jVj' is at most (1/e) j '=j Nj' 
= 2-j+1Nj/e. On the other hand, at each step before time tj , all the interesting vertices in Ij 2 are 
available and the same expression summed over them is at least 2-j |Ij 2|/E. Thus for any t = tj , conditioned 
on outputting a vertex vt ..j'=jVj' in Step 3, the probability that it is |I2|2-j /e |I2| interesting 
is at least (|I2|2-jj +21-j Nj )/e = 3Njj (using j |I2|= Nj ). Now since we output Nj/2 vertices from 
j .j'=jVj' in Step 3 before time tj, we conclude that |I2||I2| Njj j E|I1 | |I2|= × = . Taking j '=jj 
' j 23Nj 6 expectations completes the proof. We can now compute the total cost of all the interesting 
vertices dumped in Steps 6 of the algorithm. E[cost( I2)]= 2j E[|I2|] jj jj 2j = 6 E[|Ij 1 ' |] j j'=j 
+1 = 6 E[|Ij 1 ' |]2j ' j'  = 12 · E[cost( Ij 1)]. j Finally, combining this calculation with Lemma 
5.1, we conclude that our algorithm gives an O( 1 ) approxima­ e tion to the weighted vertex cover problem. 
 5.3 Vertex Cover Lower Bounds Theorem 5.4. Any algorithm for the vertex cover problem that prescribes 
edge-orientations with E­di.erential privacy must have an O(1/E) approximation guarantee, for any E . 
( 1 , 1]. n Proof. Let V = {1, 2,..., i 1 l}, and let M be an E­ 2 di.erentially private algorithm that 
takes as input a private set E of edges, and outputs an orientation ME : V × V . V , with ME(u, v) .{u, 
v} indicating to the edge which endpoint to use. Picking two distinct vertices u = v uniformly at random 
(and equating (u, v) with (v, u)), we have by symmetry: Pru,v[MØ((u, v)) = u]= 1 . 2 Let *u =(V, {u}× 
(V \{u})) be the star graph rooted 11 at u. Since *u and Ø di.er in at most - 1 < edges 2 and M satis.es 
E-di.erential privacy, we conclude that 1 Pru,v[M u ((u, v)) = u] = . 2e Thus the expected cost of M 
when input a uniformly 1 random *u is at least ×i 1 l, while OPT(*u) is 1. We 2e 2 can repeat this pattern 
arbitrarily, picking a random star from each group of 1/E vertices; this results in graphs with arbitrarily 
large vertex covers where M incurs cost 1/E times the cost. 6 Set Cover We now turn our attention to 
private approximations for the Set Cover Problem; here the set system (U, S) is public, but the actual 
set of elements to be covered R . U is the private information. As for vertex cover, we cannot explicitly 
output a set cover that is good and private at the same time. Hence, we again output a permutation over 
all the sets in the set system; this implicitly de.nes a set cover for R by picking, for each element 
R, the .rst set in this permutation that contains it. Our algorithms for set cover give the slightly 
weaker (E, d)-privacy guarantees. 6.1 Unweighted Set Cover We are given a set system (U, S) and must 
cover a private subset R . U. Let the cardinality of the set system be |S| = m, and let |U| = n. We .rst 
observe a computationally ine.cient algorithm. Theorem 6.1. The exponential mechanism, when used to pick 
a permutation of sets, runs in time O(m!poly(n)) and gives an O(log(em/OPT)/E)-approximation. Proof. 
A random permutation, with probability at least s o-1 m has all the sets in OPT before any set in OPTc 
. OPT s o m Thus the additive error is O(log /E). OPT The rest of the section gives a computationally 
e.cient algorithm with slightly worse guarantees: this is a modi.ed version of the greedy algorithm, 
using the exponential mechanism to bias towards picking large sets. Algorithm 5 Unweighted Set Cover 
1: Input: Set system (U, S), private R . U of elements to cover, E,d. 2: let i . 1, Ri = R, Si .S. E 
' . E/2 ln( e ). d 3: for i =1, 2,...,m do 4: pick a set S from Si with probability proportional to exp(E 
' |S n Ri|). 5: output set S. 6: Ri+1 . Ri \ S, Si+1 .Si -{S}. 7: end for 6.1.1 Utility Analysis At 
the beginning of itera­tion i, say there are mi = m - i + 1 remaining sets and ni = |Ri| remaining elements, 
and de.ne Li = maxS.S |S n Ri|, the largest number of uncovered ele­ments covered by any set in S. By 
a standard argument, any algorithm that always picks sets of size Li/2 is an O(ln n) approximation algorithm. 
Theorem 6.2. The above algorithm achieves an ex­pected approximation ratio of O(ln n + ln ' m )= O(ln 
n + ln m ln(e/d) ). Proof. As there is at least one set containing Li ele­ments, our use of the exponential 
mechanism to select sets combined with Equation 2.4 ensures that the prob­ability we select a set covering 
fewer than Li - 3 ln m/E elements is at most 1/m2 . While Li > 6 ln m/E, with probability at least (1 
- 1/m) we always select sets that cover at least Li/2 elements, and can therefore use no more than O(OPT 
ln n) sets. Once Li drops below this bound, we observe that the number of remaining ele­ments |Ri| is 
at most OPT · Li. Any permutation there­fore costs at most an additional O(OPT ln m/E '). 6.1.2 Privacy 
Continuing the analysis from above, Theorem 6.3. The unweighted set cover algorithm preserves (E, d) 
di.erential privacy for any E . (0, 1), and d< 1/e. Proof. Let A and B be two set cover instances that 
di.er in some element I. Say that SI is the collection of sets containing I. Fix an output permutation 
p, and write si,j (A) to denote the size of set Sj after the .rst i - 1 sets in p have been added to 
the cover. Pr[M(A)= p] Pr[M(B)= p] n exp(E ' · si,pi (A))/( j exp(E ' · si,j (A))) = exp(E ' · si,pi 
(B))/( j exp(E ' · si,j (B))) i=1 t exp(E ' · st,pt (A)) j exp(E ' · si,j (B)) = · exp(E ' · st,pt (B)) 
j exp(E ' · si,j (A)) i=1 where t is such that Spt is the .rst set containing I to fall in the permutation 
p. After t, the remaining elements in A and B are identical, and all subsequent terms cancel. Moreover, 
except for the tth term, the numerators of both the top and bottom expression cancel, since all the relevant 
set sizes are equal. If A contains I and B does not the .rst term is exp(E ') and the each term in the 
product is at most 1. Now suppose that B contains I and A does not . In this case, the .rst term is exp(-E 
') < 1. Moreover, in instance B, every set in SI is larger by 1 than in A, and all others remain the 
same size. Therefore, we have: Pr[M(A)=p] Pr[M(B)=p] P P '' ' t (exp( )-1)· j.SI exp( ·si,j(A))+ exp( 
·si,j (A)) j P = i=1 j exp( ' ·si,j (A)) t = (1 + (exp(E ') - 1) · pi(A)) i=1 where pi(A) is the probability 
that a set containing I is chosen at step i of the algorithm running on instance A, conditioned on picking 
the sets Sp1 ,...,Spi-1 in the previous steps. For an instance A and an element I . A, we say that an 
output s is q-bad if i pi(A)1(I uncovered at step i) (strictly) exceeds q, where pi(A) is as de.ned above. 
We call a permutation q-good otherwise. We .rst consider the case when the output p is (ln d-1)-good. 
By the de.nition of t, we have t-1 pi(A) = ln d-1 . i=1 Pr[M(A)= p] Pr[M(B)= p] t t = exp((exp(E ') 
- 1)pi(A)) = exp(2E ' pi(A)) i=1 i=1 1 1 = exp(2E '(ln( )+ pt(A))) = exp(2E '(ln( )+1)). d d Thus, for 
any (ln d-1)-good output p, we have Pr[M(A)=p] = exp(E). We can then invoke the following Pr[M(B)=p] 
 lemma, proved in appendix A Lemma 6.1. For any set system (U, S), any instance A and any I . A, the 
probability that the output p of the algorithm above is q-bad is bounded by exp(-q). Thus for any set 
P of outcomes, we have Pr[M(A) .P] = Pr[M(A)= p] p.P = Pr[M(A)= p] p.P:p is (ln d-1)-good + Pr[M(A)= 
p] p.P:p is (ln d-1)-bad = exp(E)Pr[M(B)= p]+ d p.P:p is (ln d-1)-good = exp(E)Pr[M(B) .P]+ d. Corollary 
6.1. For E< 1 and d =1/poly(n), there is an O( ln n ln m )-approximation algorithm for the un­weighted 
set cover problem preserving (E, d)-di.erential privacy. 6.2 Weighted Set Cover We are given a set system 
(U, S) and a cost function C : S. R. We must cover a private subset R . U. W.l.o.g., let minS.S C(S) 
= 1, and denote maxS.S C(S)= W . Let the cardinality of the set system be |S| = m, and let |U| = n. 
Algorithm 6 Weighted Set Cover 1: let i . 1, Ri = R, Si .S, ri . n, E ' =, 2 ln(e/d) so log m+log log(nW 
) T =T ' 2: while ri = 1/W do 3: pick a set S from Si with probability proportional ss oo to exp E ' 
|S n Ri|- ri · C(S) or halve with probability proportional to exp(-E ' T ) 4: if halve then 5: let ri+1 
. ri/2, Ri+1 . Ri, Si+1 .Si, i . i +1 6: else 7: output set S 8: let Ri+1 . Ri \S, Si+1 .Si -{S}, ri+1 
. ri, i . i +1 9: end if 10: end while 11: output all remaining sets in Si in random order Let us .rst 
analyze the utility of the algorithm. If R = Ø, the algorithm has cost zero and there is nothing to prove. 
So we can assume that OPT = 1. We .rst show that (whp) ri . Ri/OPT. Lemma 6.2. Except with probability 
1/ poly(m), we |Ri| have ri = for all iterations i. 2OPT Proof. Clearly r1 = n =|R1|/2OPT. For ri to 
fall below |Ri|/2, it must be in ( |Ri| , |Ri|] and be halved 2OPT OPT in Step 6 of some iteration i. 
We ll show that this |Ri||Ri| is unlikely: if at some iteration i, = ri = , 2OPT OPT then we argue that 
with high probability, the algorithm will not output halve and thus not halve ri. Since all remaining 
elements Ri can be covered at cost at most |Ri| OPT, there must exist a set S such that |SnRi| = , C(S) 
OPT and hence |S n Ri|= C(S) · |Ri|= C(S) · ri. OPT Hence ui(S) := |S n Ri|- ri · C(S) = 0 in this case, 
and the algorithm will output S with prob­ability at least proportional to 1, whereas it out­puts halve 
with probability proportional to exp(-E ' T ). Thus, Pr[ algorithm returns halve ] < exp(-E ' T )= 1/ 
poly(m log nW ). Since there are m sets in total, and r ranges from n to 1/W , there are at most m + 
O(log nW ) iterations, and the proof follows by a union bound. Let us de.ne a score function ui(S) := 
|S n Ri|- ri · C(S), and ui(halve) := -T : note that in Step 4 of our algorithm, we output either halve 
or a set S, with probabilities proportional to exp(E ' ui(·)). The following lemma states that with high 
probability, none of the sets output by our algorithm have very low scores (since we are much more likely 
to output halve than a low-scoring set). Lemma 6.3. Except with probability at most 1/ poly(m), Step 
4 only returns sets S with ui(S) =-2T . Proof. There are at most |Si|= m sets S with score ui(S) =-2T 
, and so one is output with probability at most proportional to m exp(-2TE). We will denote this bad 
event by B. On the other hand, halve is output with probability proportional to exp(-TE). Hence, Pr[halve]/Pr[B] 
= exp(TE)/m, and so Pr[B] = m/ exp(TE) = 1/ poly(m log nW ). Again there are at most m + O(log nW ) iterations, 
and the lemma follows by a trivial union bound. We now analyze the cost incurred by the algorithm in 
each stage. Let us divide the algorithm s execution into stages: stage j consists of all iterations i 
where n |Ri|. ( 2n j , 2j-1 ]. Call a set S interesting if it is incident on an uncovered element when 
it is picked. Let Ij be the set of interesting sets selected in stage j, and C(Ij) be the total cost 
incurred on these sets. Lemma 6.4. Consider stages 1,...,j of the algorithm. Except with probability 
1/ poly(m), we can bound the cost of the interesting sets in stage 1,...,j by: C(Ij ' ) = 4jOPT · (1 
+ 2T ). j'=j Proof. By Lemma 6.3 all the output sets have ui(Si) = -2T whp. Rewriting, each Si selected 
in a round j ' = j satis.es |Si n Ri| +2T 2j ' +1 OPT C(Si) == (|Si n Ri| +2T ), ri n where the second 
inequality is whp, and uses Lemma 6.2. Now summing over all rounds j ' = j, we get (6.5) C(Ij ' ) j '=j 
2j ' +1 OPT ss oo (6.6) =|Si n Ri| +2T. n j'=ji s.t. Si.Ij ' Consider the inner sum for any particular 
value of j ': let the .rst iteration in stage j ' be iteration i0 naturally Ri . Ri0 for any iteration 
i in this stage. Now, since Si n Ri . Ri0 and Si n Ri is disjoint from Si' n Ri' , the sum over |Si n 
Ri| is at most |Ri0 |, which is at most n 2j '-1 by de.nition of stage j ' . Moreover, since we are 
only concerned with bounding the cost of interesting sets, each |Si n Ri|= 1, and so |Si n Ri| +2T = 
|Si n Ri|(1 + 2T ). Putting this together, (6.5) implies 2j ' +1 OPT n C(Ij ' ) = × (1 + 2T ) 2j '-1 
n j '=jj '=j =4j OPT (1 + 2T ),  which proves the lemma. Theorem 6.4. (Utility) The weighted set cover 
algo­rithm incurs a cost of O(T log n OPT) except with prob­ability 1/ poly(m). Proof. Since the number 
of uncovered elements halves in each stage by de.nition, there are at most 1 + log n stages, which by 
Lemma 6.4 incur a total cost of at most O(log n OPT · (1 + 2T )). The sets that remain and are output 
at the very end of the algorithm incur cost at most W for each remaining uncovered element; since ri 
< 1/W at the end, Lemma 6.2 implies that |Ri| < 2OPT/W (whp), giving an additional cost of at most 2 
OPT. We can adapt the above argument to bound the expected cost by O(T log n OPT). (Proof in the full 
version.) Theorem 6.5. (Privacy) For any d> 0, the weighted set cover algorithm preserves (E, d) di.erential 
privacy. Proof. We imagine that the algorithm outputs a set named HALVE when Step 4 of the algorithm 
returns halve, and show that even this output is privacy preserving. Let A and B be two set cover instances 
that di.er in some element I. Say that SI is the collection of sets containing I. Fix an output p, and 
write ui,j(A) to denote the score of pj (recall this may be halve) after the .rst i - 1 sets in p have 
been selected. Pr[M(A)= p] Pr[M (B)= p] n exp(E ' · ui,pi (A))/( j exp(E ' · ui,j(A))) = exp(E ' · ui,pi 
(B))/( j exp(E ' · ui,j(B))) i=1 t exp(E ' · ut,pt (A)) j exp(E ' · ui,j(B)) = · exp(E ' · ut,pt (B)) 
j exp(E ' · ui,j (A)) i=1 where t is such that Spt is the .rst set containing I to fall in the permutation 
p. After t, the remaining elements in A and B are identical, and all subsequent terms cancel. Moreover, 
except for the tth term, the numerators of both the top and bottom expression cancel, since all the relevant 
set sizes are equal. If A contains I and B does not the .rst term is exp(E ') and the each term in the 
product is at most 1. Since E ' = E, we conclude that in this case, for any set P of outputs, Pr[M(A) 
.P] = exp(E)Pr[M(B) .P]. Now suppose that B contains I and A does not . In this case, the .rst term is 
exp(-E ') < 1. Moreover, in instance B, every set in SI is larger by 1 than in A, and all others remain 
the same size. Therefore, we have: Pr[M(A)=p] Pr[M(B)=p] PP '' ' t (exp( )-1)· j.SIPexp( ·ui,j(A))+ 
j exp( ·ui,j(A)) = ' i=1 j exp( ·ui,j (A)) t = 1+(e ' - 1) · pi(A) i=1 where pi(A) is the probability 
that a set containing I is chosen at step i of the algorithm running on instance A, conditioned on picking 
the sets Sp1 ,...,Spi-1 in the previous steps. For an instance A and an element I . A, we say that an 
output s is q-bad if i pi(A)1(I uncovered at step i) (strictly) exceeds q, where pi(A) is as de.ned above. 
We call a permutation q-good otherwise. We .rst consider the case when the output p is (ln d-1)-good. 
By the de.nition of t, we have t-1 pi(A) = ln d-1 . i=1 Continuing the analysis from above, Pr[M(A)= 
p] Pr[M(B)= p] t = exp((exp(E ') - 1)pi(A)) i=1 t = exp 2E ' pi(A) i=1 ss oo = exp 2E ' ln d-1 + pt(A) 
ss oo = exp 2E ' ln d-1 +1 . Thus, for any (ln d-1)-good output p, we have Pr[M(A)=p] = exp(E). Pr[M(B)=p] 
 Finally, as in the proof of Theorem 6.3, we can use lemma 6.1 to complete the proof. 6.3 Removing the 
Dependence on W We can remove the dependence of the algorithm on W with a simple idea. For an instance 
I =(U, S), let Sj = {S . S| C(S) . (nj,nj+1] }. Let Uj be the set of elements such that the cheapest 
set containing them is in Sj. Suppose that for each j and each S .Sj , we remove all elements that can 
be covered by a set of cost at most nj-1, and hence de.ne S ' to be S n (Uj . Uj-1). This would change 
the cost of the optimal solution only by a factor of 2, since if we were earlier using S in the optimal 
solution, we can pick S ' and at most n sets of cost at most nj-1 to cover the elements covered by S 
\ S ' . Call ' this instance I =(U, S '). Now we partition this instance into two instances I1 and I2, 
where I1 =(.j evenUj , S '), and where I2 =(.j oddUj , S '). Since we have just partitioned the universe, 
the optimal solution on both these instances costs at most 2 OPT(I). But both these instances I1, I2 
are themselves collections of disjoint instances, with each of these instances having wmax/wmin = n2; 
this immediately allows us to remove the dependence on W . Note that this transformation is based only 
on the set system (U, S), and not on the private subset R. Theorem 6.6. For any E . (0, 1), d =1/ poly(n), 
there is an O(logn(log m + log log n)/E)-approximation for the weighted set cover problem that preserves 
(E, d)­di.erential privacy.   6.4 Lower bounds Theorem 6.7. Any E-di.erentially private algorithm 
that maps elements to sets must have approximation fac­tor O(log m/E), for a set cover instance with 
m sets and 1 ((log m)/E)O(1) elements, for any E . (2 log m/m 20 , 1). Proof. We consider a set system 
with |U| = N and S a uniformly random selection of m size-k subsets of U. We will consider problem instances 
Si consisting of one of these m subsets, so OPT(Si) = 1. Let M be an E-di.erentially private algorithm 
that on input T . U, outputs an assignment f mapping each element in U to some set in S that covers it. 
The number of N possible assignments is at most m. The cost on input T under an assignment f is the cardinality 
of the set f(T )= .e.T f(e). We say assignment f is good for a subset T . U if k its cost |f(T )| is 
at most l = . We .rst show that any 2 .xed assignment f : U . [m], such that |f-1(j)|= k for all j, is 
unlikely to be good for a randomly picked size-k subset T of U. The number of ways to choose l sets from 
among those with non-empty f-1(·) is at most so N . Thus the probability that f is good for a random 
l sos ok N lk N1/10 size-k subset is at most . Setting k =, lN k and l = , this is at most 2 lk k/2 Ne 
lk ek3 = 2-k log N/4 = . lN 2N Let m =22 .k . The probability that f is good for at least t of our m 
randomly picked sets is bounded by t m 2-k log N/4 = 22 .kt2-tk log N/4 = 2-tk log k/8 . t Thus, with 
probability at most 2-Nk log k/8 , a .xed assignment is good for more than N of m randomly N chosen size-k 
sets. Taking a union bound over m= 22 .kN possible assignments, the probability that any feasible assignment 
f is good for more than N sets is at most 2-Nk log k/16 . Thus there exists a selection of size-k sets 
S1,...,Sm such that no feasible assignment f is good for more than N of the Si s. Let pM(Ø)(Si) be the 
probability that an assignment drawn from the distribution de.ned by running M on the the empty set as 
input is good for Si. Since any .xed assignment is good for at most N of the m sets, the average value 
of pM(Ø) is at most N/m. Thus there exists a set, say S1 such that pM(Ø)(S1) = N/m. Since |Si| = k and 
M is E-di.erentially private, pM(S1)(S1) = 1 exp(Ek)pM(Ø)(S1) < . Thus with probability at least 2 
half, the assignment M picks on S1 is not good for S1. Since OPT(S1) = 1, the expected approximation 
ratio of M is at least l/2= log m . 4 Additionally, one can take s distinct instances of the above problem, 
leading to a new instance on s · N elements and s · m sets. OPT is now s, while it is easy to check that 
any private algorithm must cost O(s · l) in expectation. Thus the lower bound in fact rules out additive 
approximations. It is natural to ask if this lower bound is tight for the weighted case. Unlike the unweighted 
case, a di­rect application of the exponential mechanism does not lead to a good approximation guarantee. 
In the full ver­sion of the paper, we give an (ine.cient) di.erentially private algorithm for weighted 
set cover matching the lower bound above. 7 Facility Location Consider the metric facility location problem: 
we are given a metric space (V, d), a facility cost f and a (pri­vate) set of demand points D . V . We 
want to select a set of facilities F . V to minimize d(v, F )+f ·|F |. v.D (Note that we assume uniform 
facility costs here in­stead of di.erent costs fi for di.erent i . V .) Assume that distances are at 
least 1, and let . = maxu,v d(u, v) denote the diameter of the space. We use the result of Fakcharoenphol 
et al. [13] that any metric space on n points can be approximated by a distribution over dominating trees 
with expected stretch O(log n); moreover all the trees in the support of the distribution are rooted 
2-HSTs they have L = O(log .) levels, with the leaves (at level 0) being exactly = V , the internal nodes 
being all Steiner nodes, the root having level L, and all edges between levels (i +1) and i having length 
2i . Given such a tree T and node v at level i, let Tv denote the (vertices in) the subtree rooted at 
v. By Corollary 4.2, it is clear that we cannot output the actual set of facilities, so we will instead 
output instructions in the form of an HST T =(VT ,ET ) and a set of facilities F . VT : each demand x 
. D then gets assigned to its ancestor facility at the lowest level in the tree. (We guarantee that the 
root is always in F , hence this is well-de.ned.) Now we are charged for the connection costs, and for 
the facilities that have at least one demand assigned to them. Algorithm 7 The Facility Location Algorithm 
1: Input: Metric (V, d), facility cost f, demands D . V ,E. 2: Pick a random distance-preserving FRT 
tree T ; recall this is a 2-HST with L = O(log .) levels. 3: let F . root r. 4: for i =1 to L do 5: for 
all vertices v at level i do 6: let Nv | and N= Nv + Lap(L/E). = |D n TvNv 7: if N· 2i >f then F . F 
. v. Nv 8: end for 9: end for 10: output (T,F ): each demand x . D is assigned to the ancestor facility 
at lowest level in T . Theorem 7.1. The above algorithm preserves E­di.erential privacy and outputs a 
solution of cost OPT · log . log n log2 . O(log n log .) · . For the privacy analysis, instead of outputting 
the set F we could imagine outputting the tree T and all the counts N; this information clearly determines 
Nv F . Note that the tree is completely oblivious of the demand set. Since adding or removing any particular 
demand vertex can only change L counts, and the noise added in Step 6 gives us e/L-di.erential privacy, 
the fact that di.erential privacy composes linearly gives us the privacy claim. For the utility analysis, 
consider the noiseless version of the algorithm which opens a facility at v when Nv · 2i = f. It can 
be shown that this ideal algorithm incurs cost at most f +O(log n log .)·OPT (see, e.g., [19, Theorem 
3]). We now have two additional sources of error due to the noise: Consider the case when Nv · 2i = f> 
N2i, which Nv · increases the connection cost of some demands in D. However, the noise is symmetric, 
and so we overshoot the mark with probability at most 1/2 and when this happens the 2-HST property ensures 
that the connection cost for any demand x increases by at most a factor of 2. Since there are at most 
L = O(log .) levels, the expected connection cost increases by at most a factor of L. Consider the other 
case when Nv · 2i <f = N2i Nv · , which increases the facility cost. Note that if Nv · 2i = f/2, then 
opening a facility at v can be charged again in the same way as for the noiseless algorithm (up to a 
factor of 2). Hence suppose 1 that N= (f/2i), and hence we need Nv - Nv 2 to consider the probability 
pi of the event that 1 f Lap(L/E) > (f/2i), which is just L exp(-2i+1 L ). 2 n Note that if for some 
value of i, f = L 2i+1 log L2 , e the above probability pi is at most 1/Ln, and hence the expected cost 
of opening up spurious facilities at nodes with such values of i is at most (1/Ln) · Ln · f = f. (There 
are L levels, and at most n nodes at each level.) For the values of i which are higher; i.e., for which 
L 2i+1 2 n f < log L, we pay for this facility only if e there is a demand x . D in the subtree below 
v that actually uses this facility. Hence this demand x must have used a facility above v in the noiseless 
solution, and we can charge the cost f of opening this facility to length of the edge 2i+1 above v. Thus 
the total cost of spurious facilities we pay for is the cost of the noiseless solution times a factor 
n L log L2 . e Thus the expected cost of the solution is at most log . n log2 . (7.7) OPT · O(log n 
log .) · log . EE 8 Combinatorial Public Projects (Submodular Maximization) Recently Papadimitriou et 
al.[28] introduced the Com­ binatorial Public Projects Problem (CPP Problem) and showed that there is 
a succinctly representable version of the problem for which, although there exists a constant factor 
approximation algorithm, no e.cient truthful al­gorithm can guarantee an approximation ratio better than 
m 21 - , unless NP . BP P . Here we adapt our set cover algorithm to give a privacy preserving approxima­tion 
to the CPP problem within logarithmic (additive) factors. In the CPP problem, we have n agents and m 
resources publicly known. Each agent submits a private non-decreasing and submodular valuation function 
fi over subsets of resources, and our goal is to select a n size-k subset S of the resources to maximize 
fi(S). i=1 We assume that we have oracle access to the functions fi. Note that since each fi is submodular, 
so is n fi(S), and our goal is to produce a algorithm for i=1 submodular maximization that preserves 
the privacy of the individual agent valuation functions. Without loss of generality, we will scale the 
valuation functions such that they take maximum value 1: maxi,S fi(S) = 1. Once again, we have an easy 
computationally inef­.cient algorithm. Theorem 8.1. The exponential mechanism when used s o m to choose 
k sets runs in time O( poly(n)) and has k s o m expected quality at least (1 - 1/e)OP T - O(log /E). 
k  We next give a computationally e.cient algorithm with slightly worse guarantees. We adapt our un­weighted 
set cover algorithm, simply selecting k items greedily: Algorithm 8 CPP Problem 1: Input: A set of M 
of m resources, private functions f1,...,fn, a number of resources k, E, d. m 2: let M1 . M, F (x) := 
fi(x), S1 .Ø, i=1 E ' . . e ln(e/d) 3: for i =1 to k do 4: pick a resource r from Mi with probability 
pro­portional to exp(E '(F (Si + {r}) - F (Si))). 5: let Mi+1 . Mi -{r}, Si+1 . Si + {r}. 6: end for 
7: Output Sk+1. 8.1 Utility Analysis Theorem 8.2. Except with probability O(1/poly(n)), the algorithm 
for the CPP problem returns a solution with quality at least (1 - 1/e)OPT - O(k log m/E '). Proof. Since 
F is submodular and there exists a set S* with |S| = k and F (S)= OPT, there always exists a resource 
r such that F (Si + {r}) - F (Si) = (OPT - F (Si))/k. If we always selected the optimizing resource, 
the distance to OPT would decrease by a factor of 1 - 1/k each round, and we would achieve an approximation 
factor of 1 - 1/e. Instead, we use the exponential mechanism which, by (2.4), selects a resource within 
4 ln m/E ' of the optimizing resource with probability at least 1 - 1/m3 . With probability at least 
1-k/m3 each of the k selections decreases OPT-F (Si) by a factor of (1-1/k), while increasing it by at 
most an additive 4 ln m/E ', giving (1 - 1/e)OPT + O(k ln m/E ').  8.2 Privacy Analysis Theorem 8.3. 
For any d = 1/2, the CPP problem al­gorithm preserves (E '(e - 1) ln(e/d),d)-di.erential pri­vacy. Proof. 
Let A and B be two CPP instances that di.er in a single agent I with utility function fI . We show that 
the output set of resources, even revealing the order in which the resources were chosen, is privacy 
preserving. Fix some ordered set of k resources, p1,...,pk write Si = i-1 {p(j)} to denote the .rst i 
- 1 elements, j=1 and write si,j(A)= FA(Si + {j}) - FA(Si) to denote the marginal utility of item j at 
time i in instance A. De.ne si,j(B) similarly for instance B. We consider the relative probability of 
our mechanism outputting ordering p when given inputs A and B: Pr[M(A)= p] Pr[M(B)= p] k exp(E ' · 
si,pi (A))/( j exp(E ' · si,j(A))) = exp(E ' · si,pi (B))/( j exp(E ' · si,j(B))) , i=1 where the sum 
over j is over all remaining unselected resources. We can separate this into two products k k exp(E ' 
(A)) exp(E ' · si,j (B)) · si,pi j · . exp(E ' · si,pi (B)) j exp(E ' · si,j(A)) i=1 i=1 If A contains 
agent I but B does not, the sec­ond product is at most 1, and the .rst is at most k exp(E ' (FI (Si) 
- FI (Si-1))) = exp(E '). If B con­ i=1 tains agent I, and A does not, the .rst product is at most 1, 
and in the remainder of the proof, we focus on this case. We will write ßi,j = si,j (B) - si,j(A) to 
be the additional marginal utility of item j at time i in instance B over instance A, due to agent I. 
Thus k Pr[M(A)= p] j exp(E ' · si,j (B)) = Pr[M(B)= p] j exp(E ' · si,j(A)) i=1 k j exp(E ' ßi,j) · 
exp(E ' · si,j(A)) = j exp(E ' · si,j (A)) i=1 k = Ei[exp(E ' ßi)], i=1 where ßi is the marginal utility 
actually achieved at time i by agent I, and the expectation is taken over the probability distribution 
over resources selected at time i in instance A. For all x = 1, ex = 1+(e - 1) · x. Therefore, for all 
E ' = 1, we have: k k Ei[exp(E ' ßi)] = Ei[1 + (e - 1)E ' ßi] i=1 i=1 k = exp((e - 1)E ' Ei[ßi]). i=1 
 As in the set-cover proof, we split the set of possible outputs into two sets. We call an output sequence 
q­ k good for an agent I in instance A if this sum Ei[ßi] i=1 is bounded above by q, and call it q-bad 
otherwise. For a (ln(ed-1))-good output p, we can then write Pr[M(A)= p] = exp((e - 1)E ' · ln(ed-1). 
Pr[M(B)= p] Moreover, note that since the total realized utility of any agent is at most 1, if agent 
I has realized utility ui-1 before the ith set is chosen, then ßi is distributed in [0, 1 - ui-1]. Moreover, 
ui = ui-1 + ßi. Lemma A.2 then implies that the probability that the algorithms outputs a (ln(ed-1))-bad 
permutation is at most d. The theorem follows. Remark 1. By choosing E ' = E/k, we immediately get E-di.erential 
privacy and expected utility at least (1 - 1/e)OPT - O(k2 ln m/E). This may give better guarantees for 
some values of k and d. We remark that the k-coverage problem is a special case of the CPP problem. Therefore: 
Corollary 8.1. The CPP algorithm (with sets as re­sources) is an (E, d)-di.erential privacy preserving 
algo­rithm for the k-coverage problem achieving approxima­tion factor at least (1-1/e)OPT-O(k log m log(2/d)/E). 
8.3 Truthfulness The CPP problem can be viewed as a mechanism design problem when each agent i has a 
choice of whether to submit his actual valuation function fi, or to lie and submit a di.erent valuation 
' function f if such a misrepresentation yields a better i outcome for agent i. A mechanism is truthful 
if for every valuation function of agents j = i, and every valuation ' function fi of agent i, there 
is never a function f = fi i such that agent i can bene.t by misrepresenting his ' valuation function 
as fi . Intuitively, a mechanism is approximately truthful if no agent can make more than a slight gain 
by not truthfully reporting. Definition 8.1. A mechanism for the CPP problem is .-truthful if for every 
agent i, for every set of player valuations fj for j = i, and for every valuation function ' fi = fi: 
E[fi(M(f1,...,fi,...,fn))] ' = E[fi(M(f1,...,f i ,...,fn))] - . Note that 0-truthfulness corresponds 
to the usual notion of (exact) truthfulness. (E, d)-di.erential privacy in our setting immediately implies 
(2E + d)-approximate truthfulness.We note that Papadimitriou et al. [28] showed that the CPP problem 
is inapproximable to an m 21 - multiplicative factor by any polynomial time 0-truthful mechanism. Our 
result shows that relaxing that to .-truthfulness allows us to give a constant approximation to the utility 
whenever OPT = 2k log m log(1/.)/. for any ..  8.4 Lower Bounds Theorem 8.4. No E-di.erentially private 
algorithm for the maximum coverage problem can guarantee pro.t larger than OPT - (k log(m/k)/20E). The 
proof is almost identical to that of the lower bound Theorem 4.4 for k-median, and hence is omitted. 
 9 Steiner Forest Consider the Steiner network problem, where we are given a metric space M =(V, d) on 
n points, and a (private) subset R . V × V of source-sink (terminal) pairs. The goal is to buy a minimum-cost 
set of edges so V E(R) . such that these edges connect up each 2 terminal pair in R. As in previous 
cases, we give instructions in the form of a tree T =(V, ET ); each terminal pair (u, v) . R takes the 
unique path PT (u, v) in this tree T between themselves, and the (implicit)  solution is the set of 
edges E(R)= PT (u, v). (u,v).R The tree T is given by the randomized construction of Fakcharoenphol 
et al. [13], which guarantees that E[cost(E(R))] = O(log n) · OPT; moreover, since the construction is 
oblivious to the set R, it preserves the privacy of the terminal pairs perfectly (i.e., E = 0). The same 
idea can be used for a variety of network design problem (such as the buy-at-bulk problem) which can 
be solved by reducing it to a tree instance. 10 Private Ampli.cation Theorem In this section, we show 
that di.erentially private mechanisms that give good guarantees in expectation can be repeated privately 
to amplify the probability of a good outcome. First note that if we simply repeat a private algorithm 
T times, and select the best outcome, we can get the following result: Theorem 10.1. Let M : D . R be 
an E-di.erentially private mechanism such that for a query function q, and 1 a parameter Q, Pr[q(A, 
M(A)) = Q] = . Then for 2 1 ' any d> 0, E ' . (0, ), there is a mechanism M which 2 satis.es the following 
properties: Utility: Pr[q(A, M(A)) = Q] = (1 - 2-T ). ' E.ciency: M makes T calls to M. ' Privacy: M 
satis.es (ET )-di.erential privacy. Note that the privacy parameter degrades linearly with T . Thus 
to bring down the failure probability to inverse polynomial, one will have to make T logarithmic. To 
get E '-di.erential privacy, one would then take E to be E ' /T . If Q was inversely proportional to 
E, as is the case in many of our algorithms, this leads to an additional logarithmic loss. The next theorem 
shows a more sophisticated ampli.cation technique that does better. Theorem 10.2. (Private Amplification 
Theorem) Let M : D . R be an E-di.erentially private mecha­nism such that for a query function q with 
sensitivity 1, and a parameter Q, Pr[q(A, M(A)) = Q] = p for E ' 1 some p . (0, 1). Then for any d> 
0, . (0, ), 2 ' there is a mechanism M which satis.es the following properties: 1 Pr[q(A, M(A)) = Q 
- 4 ' log( 'dp )] = (1 - d). ' 11 M makes O(( )2 log( )) calls to M. ' dp ' dp  ' M satis.es (E +8E 
')-di.erential privacy. The proof of the result appears in the full version of the paper. References 
[1] V. Arya, N. Garg, R. Khandekar, A. Meyerson, K. Mu­nagala, and V. Pandit. Local search heuristics 
for k­median and facility location problems. SIAM J. Com­put., 33(3):544 562, 2004. [2] A. Beimel, P. 
Carmi, K. Nissim, and E. Weinreb. Pri­vate approximation of search problems. In Proceedings of the thirty-eighth 
annual ACM symposium on The­ory of computing, pages 119 128. ACM New York, NY, USA, 2006. [3] A. Beimel, 
R. Hallak, and K. Nissim. Private Ap­proximation of Clustering and Vertex Cover. Theory of Cryptography 
Conference, 4392:383, 2007. [4] A. Beimel, T. Malkin, K. Nissim, and E. Weinreb. How Should We Solve 
Search Problems Privately? In CRYPTO, volume 4622, page 31. Springer, 2007. [5] A. Beimel, K. Nissim, 
and E. Omri. Distributed private data analysis: Simultaneously solving how and what. In D. Wagner, editor, 
CRYPTO, volume 5157 of Lecture Notes in Computer Science, pages 451 468. Springer, 2008. [6] A. Blum, 
K. Ligett, and A. Roth. A learning theory approach to non-interactive database privacy. In Pro­ceedings 
of the fourtieth annual ACM symposium on Theory of computing, pages 609 618. ACM New York, NY, USA, 2008. 
[7] V. Chvatal. A greedy heuristic for the set-covering problem. Mathematics of operations research, 
pages 233 235, 1979. [8] C. Dwork. Di.erential privacy. In Automata, languages and programming. Part 
II, volume 4052 of Lecture Notes in Comput. Sci., pages 1 12. Springer, Berlin, 2006. [9] C. Dwork. Di.erential 
Privacy: A Survey of Results. In Theory and Applications of Models of Computation TAMC 2008, volume 4978 
of Lecture Notes In Com­puter Science, pages 1 19, 2008. [10] C. Dwork, K. Kenthapadi, F. McSherry, I. 
Mironov, and M. Naor. Our data, ourselves: privacy via dis­tributed noise generation. In Advances in 
cryptology EUROCRYPT 2006, volume 4004 of Lecture Notes in Comput. Sci., pages 486 503. Springer, Berlin, 
2006. [11] C. Dwork, F. McSherry, K. Nissim, and A. Smith. Cal­ibrating noise to sensitivity in private 
data analysis. Proceedings of the 3rd Theory of Cryptography Confer­ence, pages 265 284, 2006. [12] C. 
Dwork, M. Naor, O. Reingold, G. Rothblum, and S. Vadhan. On the complexity of di.erentially private 
data release: e.cient algorithms and hardness results. In Proceedings of the 41st annual ACM symposium 
on Symposium on theory of computing, pages 381 390. ACM New York, NY, USA, 2009.  [13] J. Fakcharoenphol, 
S. Rao, and K. Talwar. A tight bound on approximating arbitrary metrics by tree metrics. J. Comput. System 
Sci., 69(3):485 497, 2004. [14] J. Feigenbaum, Y. Ishai, T. Malkin, K. Nissim, M. J. Strauss, and R. 
N. Wright. Secure multiparty com­putation of approximations. ACM Trans. Algorithms, 2(3):435 472, 2006. 
[15] D. Feldman, A. Fiat, H. Kaplan, and K. Nissim. Private coresets. In Proceedings of the 41st annual 
ACM symposium on Symposium on theory of comput­ing, pages 361 370. ACM New York, NY, USA, 2009. [16] 
L. Ford and D. Fulkerson. Maximal .ow through a network. Canadian Journal of Mathematics, 8(3):399 404, 
1956. [17] S. Halevi, R. Krauthgamer, E. Kushilevitz, and K. Nis­sim. Private approximation of NP-hard 
functions. In Proceedings of the thirty-third annual ACM symposium on Theory of computing, pages 550 
559. ACM New York, NY, USA, 2001. [18] D. Hochbaum. Approximation algorithms for the set covering and 
vertex cover problems. SIAM Journal on Computing, 11:555, 1982. [19] P. Indyk. Algorithms for dynamic 
geometric problems over data streams. In Proceedings of the thirty-sixth annual ACM Symposium on Theory 
of Computing, pages 373 380. ACM New York, NY, USA, 2004. [20] P. Indyk and D. Woodru.. Polylogarithmic 
Private Approximations and E.cient Matching. Theory of Cryptography, 3876:245, 2006. [21] D. Johnson. 
Approximation algorithms for combinato­rial problems. J. Comput. Syst. Sci., 9:256 278, 1974. [22] D. 
R. Karger. Global min-cuts in RNC, and other ram­i.cations of a simple min-cut algorithm. In Proceedings 
of the Fourth Annual ACM-SIAM Symposium on Dis­crete Algorithms (Austin, TX, 1993), pages 21 30, New 
York, 1993. ACM. [23] S. Kasiviswanathan, H. K. Lee, K. Nissim, S. Raskhod­nikova, and A. Smith. What 
can we learn privately? In Proceedings of the 49th annual Symposium on Founda­tions of Computer Science, 
2008. [24] A. Machanavajjhala, D. Kifer, J. M. Abowd, J. Gehrke, and L. Vilhuber. Privacy: Theory meets 
practice on the map. In ICDE, pages 277 286. IEEE, 2008. [25] F. McSherry and K. Talwar. Mechanism Design 
via Di.erential Privacy. In Foundations of Computer Sci­ence, 2007. FOCS 07. 48th Annual IEEE Symposium 
on, pages 94 103, 2007. [26] G. Nemhauser, L. Wolsey, and M. Fisher. An anal­ysis of approximations for 
maximizing submodular set functionsI. Mathematical Programming, 14(1):265 294, 1978. [27] K. Nissim, 
S. Raskhodnikova, and A. Smith. Smooth sensitivity and sampling in private data analysis. In STOC 07 
Proceedings of the 39th Annual ACM Sym­posium on Theory of Computing, pages 75 84. ACM, New York, 2007. 
[28] C. Papadimitriou, M. Schapira, and Y. Singer. On the Hardness of Being Truthful. In Foundations 
of Computer Science, 2008. FOCS 08. 49th Annual IEEE Symposium on, 2008.  [29] L. Pitt. A simple probabilistic 
approximation algo­rithm for vertex cover. Technical report, Yale Univer­sity, 1985. A Missing Proofs 
In this section, we prove Lemma 6.1. The lemma is a consequence of the following more general inequality. 
Consider the following n round probabilistic pro­cess. In each round, an adversary chooses a pi . [0, 
1] possibly based on the .rst (i - 1) rounds and a coin is tossed with heads probability pi. Let Zi be 
the in­dicator for the the event that no coin comes up heads in the .rst i steps. Let Yj denote the random 
variable n i=j piZi and let Y = Y1. Lemma A.1. Let Y be de.ned as above. Then for any q, Pr[Y >q] = exp(-q). 
Proof. We claim that for any j and any q, Pr[Yj > q] = exp(-q), which implies the lemma. The proof is 
by reverse induction on j. For j = n, Yn is 0 if the nth coin or any coin before it comes up heads and 
pn otherwise. Thus for q = pn, the left hand side is zero. For q . [0,pn), the left hand side is at most 
(1 - pn) = exp(-pn) = exp(-q). Finally, for q< 0 the right hand side exceeds 1. Now suppose that for 
any adversary s strategy and for all q, Pr[Yj+1 >q] = exp(-q). We will show the claim for Yj . Once again, 
for q = 0, the claim is trivial. In round j, if the adversary chooses pj, there is a probability pj that 
the coin comes up heads so that Yj = 0. Thus for any q = 0, Pr[Yj >q]= Pr[pjZj + Yj+1 >q] = (1 - pj )Pr[Yj+1 
>q - pj]. Using the inequality (1 - x) = exp(-x) and the inductive hypothesis, the claim follows for 
Yj. To map the randomized algorithm to the setting of lemma A.1, we consider running the randomized weighted 
set cover algorithm as follows. When choosing a set S in step i, the algorithm .rst tosses a coin whose 
heads probability is pi(A) to decide whether to pick a set covering I or not. Then it uses a second source 
of randomness to determine the set S itself, sampling from {S : I . S} or {S : I . S}with the appropriate 
conditional probabilities based on the outcome of the coin. Clearly this is a valid implementation of 
the weighted set cover algorithm. Note that the probabilities pi(A) may depend on the actual sets chosen 
in the .rst (i - 1) steps if none of the .rst (i - 1) coins come up heads. Since lemma A.1 applies even 
when pi(A) s are chosen adversarially, lemma 6.1 follows. We also prove a more general version of Lemma 
A.1 that applies to non-Bernoulli distributions. This lemma will be needed to prove the privacy of our 
algorithm for submodular minimization in Section 8. We now consider a di.erent n round probabilistic 
process. In each round, an adversary chooses a distribution Di over [0, 1], possibly based on the .rst 
(i - 1) rounds and a sample Ri is drawn from the distribution Di. Let Z0 =1 and let Zi+1 = Zi - RiZi. 
Let Yj denote the random n variable j=1 ZiE[Ri] and let Y denote Y1. Lemma A.2. Let Y be de.ned as 
above. Then for any q, Pr[Y >q] = e exp(-q). Proof. We prove a stronger claim. We show that for Pr[Yj 
= qZj] = e exp(-q). The proof is by reverse induction on j. For j = n, Yn = E[Rn]Zn = Zn since Dn is 
supported on [0, 1] and hence has expectation at most 1. Thus the claim is trivial for any q = 1. For 
q = 1, the right hand side is at least 1 and there is nothing to prove. Supppose that for any q and any 
strategy of the adversary, Pr[Yj+1 = qZj+1] = e exp(-q). We show the claim for Yj . Once again the case 
q = 1 is trivial, so we assume q = 1. Let µj denote E[Rj ]. Note that Yj = Zjµj + Yj+1. Moreover, Zj+1 
= (1 - Rj)Zj. Thus, Pr[Yj = qZj]= ERj.Dj [Pr[Yj+1 = qZj - µj Zj]] q - µj = ERj.Dj [Pr[Yj+1 = Zj+1]] 1 
- Rj q - µj = ERj.Dj [e exp(- )]. 1 - Rj We show that for any distribution D, the last term is bounded 
by e exp(-q), which will complete the proof. Re-arranging, it su.ces to show that for any distribu­tion 
D on [0, 1], µ - qR ER.D[exp( )] = 1. 1 - R Since µ-qR is positive when R = µ/q and negative 1-R otherwise, 
one can verify that for any R, exp( µ-qR ) = 1-R exp( µ-qR ). Moreover, since exp(·) is convex, the 1- 
µ q function lies below the chord and we can conclude that µ µ exp( µ-qR ) = exp( )+ R(exp( µ-q ) - 
exp( )). 1- µ 1- µ 1- µ 1- µ qq qq Thus it su.ces to prove that µµ - qµ exp( )+ µ(exp( ) - exp( )) = 
1, 1 - µ 1 - µ 1 - µ q qq or equivalently -q -µ 1+ µ(exp( ) - 1 = exp( ). 1 - µ 1 - µ q q This rearranges 
to 1 - exp(- µ ) = µ(1 - exp(- q )). 1 - µ 1 - µ q q x Consider the function f(x)=1 - exp(-1- µ ). f 
q is convex with f(0) = 0and f(1) = f(q) = (1 - q exp(-1- µ )). Thus f(µ) = µf(1) = µf(q), for q = 
1. q The claim follows.  
			