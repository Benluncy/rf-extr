
 On Extracting Randomness From Weak Random Sources * Extended Abstract Amnon Ta-Shmat Abstract a defective 
source is, the most general one [ZUC90] assumes nothing about the nature of the source, ex­cept that 
no string has too high a probability in the We deal with the problem of extracting as much given distribution: 
randomness as possible from a defective random source. We devise a new tool, a merger , which DEFINITION 
1.1 The rein-entropy of a distribution is a function that accepts d strings, one of which is D is Hm(D) 
= rnznc(-log(D(z)). uniformly distributed, and outputs a single string that is guaranteed to be uniformly 
distributed. We show how to build good explicit mergers, and how An extractor [NZ93] is a function E(z, 
y) s.t. mergers can be used to build better extractors. the distribution of E(z, y) is very close to 
uniform, for any source X with large rein-entropy, when z is Previous work has succeeded in extracting 
taken from X and g is a short truly random string. some of the randomness from sources with large I.e., 
the extractor uses the short random string y torein-entropy. We improve on this in two respects. extract 
the randomness present in X. First, we build extractors for any source, whatever its rein-entropy is, 
and second, we extract all the randomness in the given source. DEFINITION 1.2 ( A variation on [NZ93] 
) E : {O,l}n x{O, 1} H {O,l}m is rn,c)­ ran (n,t,rn , Efficient extractors have many applications, and 
eztmctor, if for ang distribution X on {O, l}n whose we show that using our extractor we get better re­ 
rnin-entropy is at least m, the distribution of E(x, y) sults in many of these applications, e.g., we 
achieve when choosing x according to the distribution X and the first explicit IV-superconcentrators 
of linear size y uniformly in {O, l}t, is within statistical distanceand polyloglog(N) depth. of E from 
uniform. 1 Introduction 1.2 Previous Work and Our New Extractor 1.1 The Problem Extractors have been 
studied extensively in many papers IZUC90, ZUC91, NZ93, SZ94, ZUC93, Zuc]. We deal with the problem of 
extracting as much Much re;earch has also been done on a related struc­ randomness as possible from a 
defectivez source of ture, called disperser, in which the output bits are randomness. There are various 
ways to define what not necessarily close to uniform, but have a weaker random property that suffices 
,e.g., for RP simula­ This work was supported by BSF grant 92-00043 and by a Wolfeson award administered 
by the Israeli Academy of tions [WZ93, SSZ95]. In the following table we list Sciences. some of the currently 
known explicitly constructible *Inst~tute of Computer Science, Hebrew University of extractors: Jerusalem. 
email: am@cs.huji.ac.il I m min-ent I trandom bits Pmmieeion to make digital/haN copies of elt os part 
of this material for pommel or classroom uac is granted withoot fee provided that tbe copies are not 
made or distributed for profit or commercial advantage, the c~y­ri~ht notice, the title of the publication 
and ita &#38;te appear, nnd notice III gwen that copyright ia by permieaion of the ACM, Inc. To copy 
otherwise, to mpubiish, to poet on ecrvere or to radiatribute to tiata, requires specific permiaaion 
ardor fee. m Er STOC 96, Philadelphia PA,USA e 19% ACM 0-89791-785-5/96/05. .$3.50 1see remark 3,1. We 
improve upon previous results in two ways. we can build an extractor by trying to extract ran-First, 
our extractor works for any min-entropy, smail domness from (X I Y = y) for all possible values y or 
large. Second, we extract all the randomness of Y, and then merging all these possibilities into a present 
in the source: single quasi-random string using a merger. Theorem 1 For every c = c(n) and m = 1.4 Applications 
m(n) S n, there 2s an es@icit family of (72, m, t = poly(?og(n), tog(~)), m = m, c) edractors En, that 
Our new construction improves many of the known can be constructed in polynomial time in n. applications 
of extractors. Most of the applications we list here, can be obtained by plugging our new In addition 
we use our new techniaue. to con-extract or into previous constructions. struct a different extractor: 
Corollary 1.1 (following [WZ93]) For any N and Theorem z For every constant k and -y > 0 there 1 < a 
< N there is an explicitly constructible a­ is some constant 6 > 0 and an exparwiing graph with N uertices, 
and maximum de­ (n, n7, O(iog(n)log(k)n), Cl(n ), *) extractor, where gree 0(~2p0ty1QgL0g(~)) 2 . log(k)n 
= loglog . log n. Corollary 1.1 has applications on sorting and se­k lecting in k rounds. See [WZ93, 
Pip87, AKSS89] for more details. Non-constructive extrac­tors require only O(log(n) + log( ~)) truly 
random Corollary 1.2 (following [WZ93]) For every N bits, matching the current lower bound of [NZ93], 
there is an efficiently constructible depth 2 su­ while our construction requires poly(log(n), log( ~)) 
perconcentrator over N vertices with size O(N . truly random bits. Thus, we can add to the above ~potyloglog(iv) 
) table: m t random bits ml ref [WZ93] prove that a direct corollary of cor 1.3 is: any m poly(log(n) 
+ log(~)) m Thm 1 C?(n ) O(log(n) log(~)n) fl(nd) Thm 2 Corollary 1.3 ([WZ93]) For any N there is an 
ex­any m O(log(n) + log(+)) m Lower plicitly constructible superconcentrator over N ver-I bound 1 tices, 
with linear size and polyloglog(N) depth 3. Reducing tto the optimal remains the major open Corollary 
1.4 For any 6>0 and k >0, BPP can problem. o(tOg@)n) u be simulated m time n sing a weak random source 
X with minentropy at least n6 4 .  1.3 Our Technique Our results can also prove a deterministic version 
The main new tool we use is the notion of a of the hardness of approximating the iterated log of merger 
. This is a function that accepts d strinm,-, MaxClique. See [ZUC93] for more details. one of which is 
uniformly distributed, and outputs a single string which is guaranteed to be uniformly dis­tributed. 
The difficulty of designing such a merger 1.5 Paper Organization is that we do not know beforehand which 
of the In section 2 we describe our notation, and in sec­ strings is the uniformly distributed one, and 
that tion 3 we give some preliminary definitions and re­ the other strings may be correlated with it 
in an sults. In section 4 we prove Theorem 1, assuming arbitrary manner. In fact, we use the merger under 
two theorems on mergers and composition of ex­ more general conditions: the index of the uniformly tractors 
which we prove in section 5 and section 6 distributed string may be a random variable itself, respectively. 
and may be correlated with the input strings. Due to lack of space, we omit many of the tech­nical proofs. 
Also, the proof of Theorem 2 will, only Mergers, as extractors, extract randomness from a defective 
source of randomness. However, mergers deal with random sources having simple structure, appear in the 
final version of the paper. which simplifies the task of constructing good merg­ 2 The obvious lower 
bound 1s ~ The previous upper ers. bound [WZ93, SZ94] was O(&#38; 210gt NJ1 2+0(1)) We briefly illustrate 
how mergers can be used to 3This Improves the current upper bound of build extractors. Suppose for any 
random source X o(rog(N)l/2+ (1) ) there is some hint Y = Y(z) s.t. it is easy to extract 4 T h,s ,mproves 
the [SZ94] n ~t g( )) bound. For RF , randomness from (X I Y), and suppose we do not [SS295] showed this 
can be done in polynomial time. The corollary Immediate] y follows Theorem 2. A proof of Theo­ know what 
this value Y = Y(X) is. Using mergers, rem 2 will appear m the final version of the paper. 277 2 Notation 
 We use standard notation for random variables and distributions. We distinguish between random vari­ables 
and distributions: the value of a random vari­able is the result of some trial done in a probabil­ity 
space, and thus random variables may be cor­related, as opposed to distributions that are merely functions 
assigning probabilities t~ events. Given a random variable A we denote by A the distribution A induces. 
We use capital letters to denote random variables and distributions, and small let ters to denote ele­ments. 
If ~ is a distribution, z E ~ denotes pick­ing x according to the distribution ~. If A and B are (possibly 
correlated) random variables then (A / B = b) is the conditional distribution of A given that B = b. 
We denote by U~ the uniform distribution over {O, 1} . For a random variable X = Xl 0 ...0 Xn over {o, 
I}n we write X[,,,l as an abbreviation to the ran­dom variable X, o X[,+ll . ..0 X3, where o stands for 
concatenation. The same applies for instances Z[,,jl. We define the v~riation distance between two dis­tributions 
~ ~rd ~ defined o~r the s~me space, as d(X, Y) = ~lX Yll = ~Za[X(a) Y(a)]. We say . ~ is c-close to 
~ if d(X, Y) < e. We say two random variables A and B are e close if d(~, ~) < e. We say ~ is c quasi-random 
if it is e-close to uniform. 3 Preliminaries 3.1 Extractors QEFINITIO~ 3.1 The min-enJropy of a distribution 
D is HW(D) = rnint(-log(D(z)). DEFINITION 3.2 E : {O, I}n x {O, 1} H {O, I}m is an (n, m, t,m ,c)-extractor, 
if for any distribution ~ on {O, I}n with HH(~) ~ m, the dw$ribution of E(x, y) when choosing x s ~ and 
y E Ut, is e close to Um!. REMARK 3.1 This definition is slightly diflerent from the one zn [i?Z93, 
SZ9.4], where E is called an extractor if E(z, y) o y is close to uruform, while we only demand that 
E(z, y) is close to uniform. 3.2 Explicit Extractors DEFINITION 3.3 We say E = {En} is an ezphcit (n, 
m, t,m , c) -extractor, if there is a Turvng mac­ hine that given x E {O, I}n and y G {O, I}t, outputs 
E~(x, y) in polynomial time in n + t. As mentioned in the introduction, various ex­plicit extractors 
have been built so far. Of those we use the following two extractors: Lemma 3.1 /SZ94] There is some 
constant c > 1 s.t.for any m = Q(iog(n)) there is an exphcit (n, 2m, m, cm, 2 ml )-extractor Am. We denote 
this constant c by c... Lemma 3.2 [SZ94] 5 Let m(n) ~ n112+7 for some constant ~ > 0, then for any E 
there is an explicit (n, m(n), O(iog n log(~)), +, c)-extractor. The first extractor is actually a restatement 
of the existence of explicit tiny families of hash func­tions [SZ94, GW94], and can easily be achieved 
us­ing small c biased sample spaces [SZ94]. We also need the following simple lemma from [NZ93]: Lemma 
3.3 [NZ93] Let X and Y be two correlated random vartables. Let ~ be a distribution, and call anx bad 
if(Y IX =x) isnot eclose to~. If P+ob&#38;~Y(x is bad) < q then X o Y is c+ q close to ~xB.  3.3 More 
of The Same Suppose we have an extractor E that extracts ran­domness from any source having at least 
m min­entropy, how much randomness can we extract from sources having M rein-entropy when M >> m ? The 
following algorithm is implicit in [WZ93]: use the same extractor E many times over the same string z, 
each time with a fresh truly random string r,, until YOU get M m output bits. The idea is that as long 
as the length of E(x, rl ) 0 ...0 E(z, r~) is less then M m, then with high probability (X [ E(z, rl 
) 0 ...0 E(x, r~)) still contains m min­entropy, and therefore we can use the extractor E to further 
extract randomness from it. Simple cal­culations give the following two lemmas: Lemma 3,4 Suppose that 
for some m = m(n) there is an explicit (n, m, t, m!, c) extractor E~. Then, for any Al = M(n) z m, and 
any safety parameter s > 0, there w an explicit (n, Ikf, kt, min{km , M m s}, k(c + 2- )) extractor. 
Lemma 3.5 Suppose that for any m z % there zs an ezP1icit (n, m, t(n), ~, e(n)) eztractor Em. Then, for 
any m, there is an explicit (n, m, O(j(n)iog(n)t(n)), m-m, log(n) (c+2- (n)))­ extractor. 4 Main Theorem 
In this section we prove Theorem 1, using our new tool of mergers. In subsection 4.1 we define what 5 
The parameters here are simplified. The real parameters appearing m [S294] are somewhat better a merger 
is, and in subsection 4.2 we explicitly build mergers . In subsection 4.3 we use mergers to efficiently 
compose extractors, and in subsection 4.4 we show how good somewhere random mergers imply good extractors. 
Finally, in subsection 4.5 we use this to prove our main theorem. 4.1 Somewhere Random Mergers DEFINITION 
4.1 X = Xl 0 ...0 xd ts a d-block (m, .s,q) somewhere random source, if each X, is a random vartable 
over {O, l}m, and there is a rars­dorn variable Y over [0..d] s.t. for any i c [1..d]: d((XJY = i), U~) 
< c and F rolI(Y = 0) < q. We also say that Y a s an (m, e, V) selector for X. A somewhere random merger 
is a function ex­tracting randomness from a somewhere random source: DEFINITION 4.2 M : {O, I}dm x {O, 
I}t * {O, I}m is a (d, m, t, m , e) somewhere random merger, if for any d block (m, O, O) somewhere random 
source X, the distribution of E(z, y) when choosing x 6 ~ and y e Ut, 2s c close to UnI. 4.2 Explicit 
Somewhere Random Mergers DEFINITION 4.3 We say M = {M. } is an ezpiicit (d, m, t, m , e) -somewhere random 
merger, if there is a Turing machzne that given x ~ {O, I}dm and y ~ {O, I}t outputs Mn(x, g) in polynomial 
t2me m dm+t. It turns out that any (2rn, m, t, m , c) extractor is a (2, m, t, m , c) somewhere random 
merger. Hav­ing a 2 block somewhere random merger, we build a d block merger by merging the d blocks 
in pairs in a tree wise fashion. Thus, we have the following theorem, which we prove in detail in section 
5: Theorem 3 Assume for every m there is an ex­plicit (2Tn, rn, t(m), ra k(m), t(m)) edractor Em, for 
some monotone functions t and k. Then there 2s an explictt (21, m, 1. t(m), m 1. k(m), 1 ~c(m)) somewhere 
random merger. The [SZ94] extractor of lemma 3.2 works for any source with H~ (~) z nl /2+T. Thus, using 
lemma 3.4, by repeatedly using the [SZ94] extractor, we can extract from a source having H~ (~) ~ ~ at 
least %_n 1/2+v quasi-random bits. Thus, we have a 2 &#38;erger that does not lose much randomness in 
the merging process. Applying Theorem 3 we get a good n merger. Thus we have the following lemma, that 
is proven in detail in Appendix A : Lemma 4.1 Let b > 1 be any constant and suppose j = f(m) = .f(m(n)) 
is a function s.t. f(m) < 36 and for every m ~ mO(n) : ~(m) > b ~ log(n). Then for every m > m. there 
is an ex­ pltcit (n, m, log(n) polylog(m) . ~2(m) . log(~), m ~, log(n) . poly(m) c) somewhere random 
merger. Corollary 4.2 For every m ~ 2-, there is an (n, m,polglog(n) log(~), Q(m), poiy(n), . c)­somewhere 
random merger M = Mm. ProoE Take f(m) = logdm for some constant d > 2. For any constant b, m > 2= and 
n large enough, logdm ~ b ~ log(n), and the corollary follows lemma 4.1. 1 Notice that Theorem 3 and 
corollary 4.2 take ad­vantage of the simple structure of somewhere ran­dom sources, giving us an explicit 
somewhere ran­dom merger that works even for sources with very small min-entropy that can not use the 
[SZ94] ex­tractor of lemma 3.2. 4.3 Composing Extractors [CG88] showed how to extract randomness from 
sources X that can be broken into blocks Xl o X2, s.t. Xl and (X2 ] Xl = xl) contain a lot of random­ness. 
Furthermore, for any source A-, for most ~ E X there is a good splitting point i (this is highly infor­mal, 
and will be stated precisely soon). This leads to the following algorithm: Given an input string x split 
it into two ccmsec­utive strings xl o X2, use El to extract randomness from X2, and use Ez with the extracted 
randomness to further extract randomness from xl. Obviously, given a string z we do not know what is 
the right splitting point, so we try all Iz I = n possible splitting points. This gives us a somewhere 
random source with n blocks, that can be merged into a single cluasi­random string by a good somewhere 
random merger. ALGORITHM 4.1 Suppose El is an (n, ml, tl, tz, (l)­ extractor, E2 is an (n, mz, tz,ts,~z)-extractor, 
and M w an (n, tz, P1, Olj ~z) somewhere random merger. Define the function Ez ~ El as foilows: Given 
a E {O,l}n, toss rl G {O, 1} , and T2 E {o,l}~ . n Let q, = E1(a[L, nl, rl) and z~ = Ez(all,~-ll, QL 
, fort =l,.,., rs. ) M n Let Ez6El = Zlo. ..ozn, and Ez @ El = M(E2 eEl, rz). Theorem 4 Suppose El 
is an (n, ml, tl, tz, ~1)­ extractor, E2 is an (n, mz, tz,ts,&#38;)-extractor, and M is an (n, t3, PI, 
01, <z) somewhere random merger. Then for every safety parameter s > 0, El ~ Ez is an(n, ml+mz+s, tl+pl, 
ol, (l+ (z +(3 + 8n2- /3eztractoror. Now we define composure of many extractors by: DEFINITION 4.4 Suppose 
E; is cm (n, m,, t,, t,+ I -t si+l, <,) extractor, .for i = 1, ..., k, s, > 0 and S2 = 0. Suppose ~t 
~$ an (n, L+2 + ~t+z, P,, t~+2j L) somewhere random merger, for any i = 1 . . . k Mk.l 1. We define by 
induction the function E~ 0 Mk-z Mb-l E~.l @ ...Ez % El to equai Eh @ (Ek-., 8 . . .Ez ~ E,). Theorem 
5 Suppose E,, M, are as aboue, then for Mk I any safety parameter s > 0, E = Eh 0 Mk ~ E~.] @ . ..Ez 
?$ El is an (n, Z$=lm, + (k -1)s, tl + ~f~;~~, tk+l, @=I~t + ~;::it + (~ ­l)n2-S/3+3 ) -extractor. If 
E,, M, are expiictt, then so isE. REMARK 4.1 It may appear that left associativity is more economic 
in terms of number of truly ran­ dom bzts used. However, we know how to implement rzght associativity 
composure in polynomial time (us­ ing a dynamzc programming procedure) and we do rtot know of such an 
algorithm for-left associativity composure-s. 4.4 Good Somewhere Random Mergers Imply good Extractors 
 Assume for every m ~ m we have a good somewhere w ... random merger M. Then we can let E = Am @ . . 
. Ab,m ~ At,m ~ A-, where A, is the extractor of lemma 3.1 and b is some constant, 1 < b < c~z, to get 
an extractor that extracts Q(m) bits from sources having m rein-entropy. Using lemma 3.5, we get an extractor 
that extracts m bits. Thus, we see that good somewhere random mergers imply good extractors. We prove 
the following lemma in detail in Appendix A : Lemma 4.3 Suppose m = m(n) is a function s.t. for every 
m z rli(n) there is an explicit (n, m, t, m ­~, c) somewhere random merger, where c is the con­ stant 
~, and c,, as the constant in lemma ~.1. Then for any m there M an explicit (n, m, O(rn . log(n). iog(~) 
+log2(n) t), m,poly(n). c)-eztractor Unfortunately, corollary 4.2 asserts the existence of good mergers 
only form z 2-, and therefore plugging this into lemma 4.3 we get: Corollary 4.4 For every m the~e as 
an (n, m, 0(2 H.polylog(n) log(+)), m,poly(n).6)­ extractor B~.  4.5 Putting It Together The extractor 
1? in corollary 4.4 uses 0(2-. polylog(n) . log(~)) truly random bits to extract all of the rando-mness 
in the given source. Al­ though O(2@0@l . poiy~og(n) . Jog(i)) is quite a large amount of truly random 
bits, we can use the [S294] extractor, to extract nl/3 bits from n2/3 min­~13 >> O(2-. entropy, and then 
use these n poiylog(n) log( ~)) bits, to further extract all of the remaining rein-entropy. More precisely, 
if B is the extract or in corollary 4.4, E., the extractor from lemma 3.2 and I@ the merger from corollary 
4.2, then E = l? ~ E,z extracts L?(m) bits from sources having m z n2f3 rein-entropy, using only polylog(n) 
truly random bits! That is, we get the following lemma (whose proof appears in Appendix A) : Lemma 4.5 
Let c z 2- 7 for some constant -y < 1. There is some constant /? < 1 s.t. for ev­ery m > np there is 
an explicit (n, m,polylog(n) . /og(*), R(m), poty(n) 6) eztractor. Now that we know how to extract all 
the random­ ness from sources having Q(np) min-entropy with only polylog(n) truly random bits, by lemmas 
3.5 and Theorem 3 we have good somewhere random mergers, for every m. Thus by lemma 4.3 we have good 
extractors for every m. We prove in detail in Appendix A that: Theorem: For every constant T < 1 , c 
z z n~ , and every m = m(n) there is an explicit (n, m,polylog(n) . log(~), m, e)-estractor. 5 Explicit 
Somewhere Random Mergers In this section we build explicit somewhere random mergers. We observe that 
a 2 block merger can be obtained from the previously designed extractors of [NZ93, SZ94]. Once such a 
merger is obtained, any number of blocks can be merged in a binary-tree fashion. 5.1 A 2-block somewhere 
random merger A d block (m, c, ~) somewhere random source X, can be viewed intuitively as a source composed 
of d strings of length m, with a selector that for all but an q fraction of the inputs, can find a block 
that is c ~asi-random. Thus, it is natural to suspect that X is E + q close to a distribution with m 
min­entropy. The following lemma (proved in appendix B. 1) states this precisely: Lemma 5.1 (1) Any (m, 
~, q) somewhere random source X is c + q close to an (m, O, O) somewhere random source X . (.2) For any 
(m, O, O) somewhere random source X, Hw(~) ~ m. Since a (2m, m, t, m , c) extractor E extracts randomness 
from any source X with H~ (~) z m, in particular it extracts randomness from any (m, O, O) somewhere 
random source, and therefore by definition E is a (2, m, t,m , c) somewhere ran­dom merger. Thus: Corollary 
5.2 Any (2m, m, t, m , e)-esiractor ts a (2, m, t, m , c)-somewhere random merger. 5.2 A d block somewhere 
random merger Given a d block somewhere random source, we merge the blocks in pairs in a tree like fashion, 
re­sulting in a single block. We show that after each level of merges we still have a somewhere random 
source, and thus the resulting single block is neces­sarily quasi-random. ALGORITHM 5.1 Assume we can 
build a (2, m, t(m), m k(m), e(m)) somewhere random merger E, for some monotone functions tandk. We 
build M~ : {O,l} m X {O, 1}1 (m) M {O, I}m-i k(m), by induction on 1: Input: xl== x~o. . .z~l, where 
each z: E {O, I}m. Output : Toss t = tl 0 . . . t~, where tj c {O, l} (m). If 1 = O output X1, otherunse: 
1 1 n Let z, = E(z~,_l O&#38; , t[) ,fOr i = 1,...,21-1. n Let the output be M-1($1 0.. .$!l , tl 0.. 
.tt-1). Theorem: [Same as Theorem 3] Ml w a (21, m, 1 t(m), m l. k(m), l.c(m)) somewhere random merger. 
Proofi For j = 1, ...,0 denote by Z~ the random variable whose value is d = z; o . . . z~, , where the 
input z is chosen according to ~, and tis uniform. Notice that ~ is the distribution ~, and ~ is the 
distribution of the output. The theorem follows immediately from the fol­lowing claim: Claim 5.1 Denote 
mj = m (1 j)k(m). 1$X is an (ml, O, O) somewhere random source, then for anyl~z~2J, d( (Z: I Y c [21-J(2 
 1) + 1,2 - 2]) , Um, ) ~ (1 -j) ~c(m) o The proof of the claim is by downward induction on j. The basis 
j = 1 simply says that for any i , d((X, IY = i) , U~ ) = O,which isexactly the hypothesis. Suppose it 
is true for j. Informally, the induction hypothesis says that if Y points to z~,-.l then Z~,_l is uniform, 
and if Y points to Zj, then Zj, is uniform. Thus, it is natural to suspect that if Y points to Z;, or 
to Z;,, then Z~,_l o Z;, is a 2­block somewhere random source. Indeed, we show that Z~,_l o Z~, with 
the right conditioning on Y is (1 j) ~ e(m) close to some ~ with lYM(~) ~~ mj. Since Z: 1 = E(z#, l 
oZ;, , tj) and E is an extractor the claim follows. A full proof appears in Appendix B. REMARK 5.1 Notice 
that we use the same random string t] for all merges occuring in the j th layer, and that this is possible 
because in a somewhere ran­dom source we do not care about dependencies be­tween diflerent blocks. Also 
notice that the error is additive in the depth of the tree of merges (i.e. tn 1), rather than in the 
size of the tree (21). 6 Composing Extractors In subsection 6.1 we show how to efficiently compose two 
extractors, and in subsection 6.2 we compose many extractors. 6.1 Composing Two Extractors The algorithm 
computing the composed extractor was presented before as algorithm 4.1, along, with Theorem 4, that we 
prove here. Proofi [Of Theorem 4] Obviously, it is enough to show that El@ E:~ is an (t,,+ s /3 ) somewhere 
random source. TO &#38; <2,%2 prove this, assume HW(~) z ml + mz +s. Denote by Q, and Z, the random variables 
with values q, and z, respectively. Also, let Q = 2 S13, c, := 2c3, and c1 = 2EZ. We define a selector 
for Z = ZI o . . . 0 Zn = El e Ez in two phases: first we define a function ~ which is almost the selector 
but has few bad values, then we correct ~ to obtain the selector Y. DEFINITION 6.1 Define f(w) to be 
the last i s.t Prob(X[,,~l = w[,,~l I X[l,, _ll = wII,, -1]) < (E2 e3) . 2-ml DEFINITION 6.2 Define 
w to be bad if f{w) = t and: 1. Probzex(f(x) = i) ~ cl, or, 2. Probzex(f(z) = i I fl[l),-ll = w[l,, 
11) < c2, or,  9. ProbZGx(X, = w, I ZI1,,.=]1 = VJ[l,,-1]) :: E3 We denote by B the set of all bad w. 
We denote by B, (i = 1, 2, 3) the set of all w satisfying condition (i). DEFINITION 6.3 Let Y be the 
random variable, ob­ tained by taking the input a and letting Y = Y(a) where Y(w) = O if w is bad, and 
f(w) otherwise. It holds that Prob(w is bad) < n(ei + C2 + c3) < 8n. 2-s/3 ( the proof is easy, see appendix 
C.2). We complete the proof by showing that (Z, I Y = i) is (I + (2-close to uniform. Claim 6.1 If Prob(Y 
= i I X[l,,-ll = wII,,-1]) >0 then HM (X[,,nl I Y = i and Xii,,-11 = wp,t-11) 2 ml Therefore, for any 
such wII,,.-I], (Q, I Y = z and X[l,, ll = w[l,,_l ) is (1-close to random (since El is an extractor 
. Hence by lemma 3.3, the distribution (Xp,,--ll I Y = i) x (Q, \ y = i and X(l ,,_ll = u+ ,,_ll ) is 
(l close to the distri­ \ bution (X[l,,_ll I Y = i) x U. But, Therefore, using the extractor Ez we get 
that (Z, [ Y = 2) is &#38; + <z-close to uniform. o The proofs of claims 6.1 and 6.2 appear in ap­pendix 
C.  6.2 Composing Many Extractors In this subsection we prove Theorem 5, which claims that E~ &#38; 
. . . ~ EI can be efficiently calculated: Proofi [of Theorem 5] Correctness : By induction on k. For 
k = 2 this follows from theorem 4. For larger k s this is a straight for­ward combination of the induction 
hypothesis and Theorem 4. Running time : Mk_l Mk-z We compute Ek @ E~ 1 @ ...E2 5 E] using a dynamic 
programming procedure: n Given zE~, wetossvG{0,I}tl andYjG {O,l} $ forj=l,..., k. n Next, we compute 
the matrix J4 where .-. . EI)(qt,nl, g/ O;I 0 . . . o~~), forl<i<n and l<j~k. The entries of the first 
row of M, lf[l, i] can be filled by calculating El (Z[t,nl, y). Suppose we know how to fill the J th 
row of M. We show how to fill the j + 1 th row. Denote gl = M[j,l] for / = a, . . . ,n, and let 21 = 
l?~+l(z[,,~-llj 91).  Let M[j+l, i]= Mj(z, o.. .zn, yJ).  By the composition definition M[j, i] has 
the correct value, and clearly, the computation takes polynomial time in n. o 7 Acknowledgments I would 
like to thank my advisor Noam Nisan for his collaboration on this research. Thanks to David Zuckerman 
for his hospitality when I visited him last year. I would like to thank David Zuckerman, Avi Wigderson, 
Michael Saks, Arvind Srinivasan and Shiyu Zhou for interesting discussions. Many special thanks go to 
Roy Armoni for our many interesting talks on-the subject. References [AKSS89] M. Ajtai, J. Komlos, W. 
Steiger, and E. Szemeredi. Almost sorting in one round. In Advances in Computer Re­search, volume 5, 
pages 117 125, 1989. [CG88] B. Chor and O. Goldreich. Unbiased bits from sources of weak randomness and 
probabilistic communication complexity. SIAM Journal on Computing, 17(2):230­261, 1988. [GW94] 0. Goldreich 
and A. Wigderson. Tiny families of functions with random proper­ties: A quality-size trade-off for hashing. 
In Proceedings of the 26th Annual A CM Symposium on the Theory of Computing, ACM, pages 574-583, 1994. 
[NZ93] N. Nisan and D, Zuckerman. More deter­ministic simulation in logspace. In Pro­ceedings of the 
25th Annual ACM Sympo­sium on the Theory of Computing, ACM, pages 235-244, 1993. [Pip87] N. Pippenger. 
Sorting and selecting in rounds. SIAM Journal on Computing, 16:1032 1038, 1987, [SSZ95] M. Saks, A. Srinivasan, 
and S. Zhou. Explicit dispersers with polylog degree. In Proceedings of the 26th Annual ACM Symposmm 
on the Theory of Computing, ACM, 1995. [SZ94] A. Srinivasan and D. Zuckerman. Com­puting with very weak 
random sources. In Proceedings of the 35th Annual IEEE Symposium on the Foundations of Com­puter Sc8ence, 
1994. [WZ93] A. Wigderson and D. Zuckerman. Ex­panders that beat the eigenvalue bound: Explicit construction 
and applications. In Proceedings of the 25th Annual ACM Symposium on the Theory of Computing, ACM, pages 
245-251, 1993. [Zuc] D. Zuckerman. Randomness-optimal sampling, extractors, and constructive leader election. 
Private Communication. [ZW90] D. Zuckerman. General weak random sources. In Proceedings of the 31st Annual 
IEEE Sympostum on the Foundations of Computer Sc2ence, pages 534-543, 1990. [ZUC91] D. Zuckerman. Simulating 
BPP using a general weak random source. In Pro­ceedings of the 32nd Annual IEEE Sym­posium on the Foundations 
of Computer Science, pages 79-89, 1991. [ZUC93] D. Zuckerman. NP-complete problems have a version that 
s hard to approxi­mate. In proceedings of the 8th Struc­tures m Complexity Theory, IEEE, pages 305-312, 
1993. A Main Theorem   A.1 Proof of lemma 4.1 n By lemma 3.2 there is an explicit (m, W, 0(10g27i2 
~109(~))> *, c) extractor. n By lemma 3.4 there is an ex­plicit (2rn, m, 0(~2(m) logzm log(~)), m *, 
poly(m) . c) extractor. n By Theorem there explicit (n, m, O(log(;) . polyiog(m) ~f2(m~ log(~), m log(n) 
. *, log(n) . poly(m) ~E)­ .. . somewhere random merger. For any m > mo A.2 Proof of lemma 4.3 Proofi 
The lemma follows easily from lemma A. 1 using lemma 3.5. o Lemma A,l Suppose for any m < m < m there 
is an explicit (n, m, t, m ~, c) somewhere random merger, where E is the constant in lemma 4.3. Thenj 
for any m s m s m there zs an explicit (n, m, O(rn. log(~) + log(n) t),Q(m), poiy(n) . c), extractor 
E. Proofi Let b = c.Z ~(1 ~). Clearly b is a constant, and 1< b< c~z. Define m, = b . m .log(~), and 
let I be the first integer s.t. .Z~=12m, < ~. Ml l Define E = El @ J3PI . . a El, where: 2 -mzj2 _ n 
E, is the (n, 2m,, m,, c,, . m,, ) extractor A~C from lemma 3.1 n M, is the (n, c,z. m,+l, t, b.m, +l, 
c) sornewhere random merger promised in the hypothesis of the lemma. Now we use theorem 5 with t,= m, 
ancls,= (c,. b)m,_l, and we also take s = ;. By Theorem 5, E is an (n, )!l~=12m, + (1 1)9, tl+ z;~; 
t,tl+l, x:=12 -m /2 + x::~c + (1 l)n2-$/3+ ) extractor. Since t = O(log(n)) and c z 2 /:) (oth­erwise 
the result is trivial), E is an extractor as re­quired. Since A,, M, are explicit, so is E. o A.3 Proof 
of lemma 4.5 Proofi Choose 6 = &#38; and ,8= 1 ~. Let the extractor E be E = B~ ~ E,, where n E., is 
the (n, np, 0(log2n . log(~)), n2P--l, c) ­extractor of lemma 3.2. n B~ is the extractor from corollary 
4.4. n M is the merger from corollary 4.2. . nv = f2(2m . 109(*)), Since nzp- = n E = Bm ~ E,z is well-defined. 
By theorem. 4, for every m, E is an explicit (n, m+np+n~, poiylog(n). log(~), ~(m), poly(n) . c) extractor. 
In particular if 17M(X) = O(n@) we extract C2(HW(X)) as required. n u A.4 Proof of theorem 4.5 n By 
lemma 3.5, lemma 4.5 implies an explicit (2n, n,polyiog(n) log(~), n -nc, poly(n) ~ e) extractor. n There 
is some constant d (that depends only on -f) s.t. for every logdn ~ m ~ n, log(n) . mp ~ &#38;. Therefore 
by theorem 3, for every m there is an explicit (n, m,polglog(n) . log( ~), m ~, poiy(n) . c) somewhere 
random merger. n By lemma 4.3, this implies an ex­piicit (n, m,polylog(n) . log(~), m,poly(n) ~e)­extractor, 
for any m. Plugging c = -, gives the theorem. B Somewhere Random Mergers B.1 A Somewhere Random source 
has large min­entropy Lemma B.1 If A = Xl 0 ...0 .Yd w an (m, c,q) somewhere random source, then X M 
q close to an (m, c, O) somewhere random source X . 283 Proofi [of lemma B.1] Let Y be an (m, c, q) selector 
for X. Denote p = Prob(Y = O) ~ q. Define the distribution ~ by: o Ifi=O D(i, $) = Prob Y,x)=(t,m) 1 
otherwise { (l-p It is easy to see that ~ is a distribution. Define the random varia~le Y o X as the~esult 
of the pick­ing (i, z) C D, i.e. Y oX = D. It is clear that Cl(Y, m) < (@z-7?, m) = p < ~. Now we want 
to show that Y is an (m, c, O) se­lector for X . It is clear that Prob(Y = O) = 0. Itis not hard to see 
that for any i > 0 we have: Prob(.Y = z IY = i) = Pr-ob(X = x [Y = i). Therefore, since we know that 
(X, IY = i) is e close to Um, we also know that (X~/ Y = i) is c close to Um, thus completing the proof. 
o Lemma B.2 Let X = Xl o.. .oXd be an (m, c, O)­somewhere random source, then X is c close to an (m, 
O, O)-somewhere random source Z. Proof: [of lemma B.z] Let Y be an (m, c, O) selector for X. Fix some 
i E [1.. d]. We know that d((X, I Y = t), U~) ~ c. Define a distribution Z( ) by: Z( )(z) = ~. Prob(X 
=zIX, =x,and Y=i) - if F rob(X, = z, and Y = 1) > 0, Z( )(z) = *­ if Prob(X, = z, and Y = i) = O, and 
-Z( )(Z) = O otherwise. It is easy to check that Z( ) is indeed a distri­ bution, and that Z: ) = U~. 
Define Y o Z to be the random variable obtained by choosing i accord­ing to Y, then choosing z according 
to Z( ), i.e., for all t > 0, (Z /Y = i) = Z(i). Also, denote X( ) = (X IY = t). Then: Prob(Z, = z, I 
Y = i) = Z~ )(z,) = 2-m We soon prove that: Claim B.1 d(X( ), Z( )) < c, Thus: d(~, ~) <d(=, m) = 2,>0 
Pr(Y =i) ~d((X IY =i),(Z IY =i)) = Z,>Q Pr(Y = i) . d(X( ), Z(;)) ~ c Hence Z satisfies the requirements 
of the lemma. o Proofi [of claim B.1] We need to show that for any A ~ Ax, lX( J(A) Z(;)(A) / ~ c. It 
is sufficient to show this for the set A containing all z E AX s.t. X( )(z) > Z( )(z). This can be easily 
seen, using the fact that for any x c A: Pr(Z = x I Z; = u, and Y = i) = ~~~~~,111~=~~1 = Pr(X=z[ X,=a, 
and Y= i). - o Lemma B.3 Let X = Xl 0 ...0 X~ be an (m, O,O) somewhere random source, then H~(~) ~ m. 
Proofi Suppose Y is an (m, O, O) selector for X. Prob(X = z) = Z,e[l ~1P~ob(Y = i) . Prob(X, = .2, / 
Y = 1) ~ Z;e[l..dl Prob(y = i) . 2-m = 2-m o Combining lemmas B.I, B.2 and lemma B.3 we get lemma 5.1. 
 B.2 Proof of claim 5.1 Proofi The proof is by downward induction on j. The basis j = 1 simply says 
that for any i , d((X, I Y = i) , U.~ ) = O, which is exactly the hypothesis. SUp­pose It is true for 
j, we prove it for ~ 1. By the induction hypothesis: n d( (Z~,_l I Y C [2~-~(2i 2) + 1,21 -~(2i 1)1) 
, urn, ) S (i ~)~(m) n d( (Z;* \ [2 - (21)+1,1, 2 - 22]) , Umy ~~ (J-j) .c(m; Soon we prove the following 
lemma: Lemma B.4 Let A, B and Y be any random vari­ables. Suppose that d((A I Y ~ S1 ), U~) ~ c and 
d((ll I Y E S2), fJ~) < ~ for some disjoint sets S1 and S2. Then (A oB IY E S1US2) is ~-close to some 
~ with H~(~) ~ m. Therefore: c [Z~,_, oZ~, [ Y 6 [21-~ + (i-l) +1,2~-~+ i]) is (1 -j) ~c(m) close to 
some ~ with HN(~) z ml. n Since Z~-l = (ZL-I o Z;, , ~j), it follows that (Z~- I Y E [2 -J+1(i l)+l,2 
- +1i]) is (t -J). e(m) close to E~, (x,t~) where z 6 ~ and Hm (~) ~ mj. Therefore, it is (~ j) c(m) 
+ c(m) close to random, as required. o B.2.1 Proof of lemma B.4 Prob(x[l,t_l] = W[l,, 1]) = Proofi 
We define random variables Y , A o B as follows: ~~o~(ql,n] = ql,n]) f ~o~(-~[t,n] = qt,n] I X[l,t 1] 
= W[l,, -1]) n Pick Y =icS1uS2with: Pr(Y = i) = PT(Y=21YCSIUS2). ~~o~(x[l,n] = ql,n]) nChoose a ob e(Ao 
BIY= i). l%ob(xt = wtlx[l,i-l])~rob( x[t+l,n] = W[,+l,n]lx[l,t]) However,it is easy to prove that: Claim: 
Pr(A = a IY = t) = Pr(A = PTob(x[t+l,n] = W[t+l,n]lx[l,t]) > (Q t3)2 ~ a l Y=i)and Pr(l? =b l Y =i)=Pr(l?= 
 I%ob(xt = w, I X[l,,-1] = ql,t-1]) 263 b l Y= i). ~~ob(x[l,n] = W[l,nl) < 2-[7W+m2+s) Define The first 
line is true since ~(w) = i, the second1 If Y CSl z = since w @ &#38;, the third since HH(X) ~ ml +m:l 
+.s. 2 Otherwise, i.e. Y E S2 { Thus, It is not hard to see that: ~ m~ s Prob(x[l,t_l] = W[l,,-1]) 
< Claim B.2 (A IZ = 1)= (A IY ES1) and (3 .(Q-eJ 1) (B IZ =2)=(BIYCS2). Therefore, Hence, Z is an (m, 
e, O) selector for A o B . PTob(x[,,,-q = q,,,-,]Il (z)= q < Therefore by lemma 5.1, A o B is c close 
to Prob( X[l,, _l]=UJ[l,, -l]) some X with H~ (~) z m. However, it is not hard < Prob(Y(x)=t) to see 
that: Claim: ~= (Ao BIYESIUSZ). Thus, (Ao B IY GS1 uS2) = A oB isc close to some X with H~ (%) z m, completing 
the proof. o The first line is true since Prob(A I 1?) s ~&#38;. The second by Eq. (1). The third by 
Claim (C.2). C Composing Extractors 1  C.1 Composing Two Extractors C.2 Technical Lemmas Proof: [of 
claim 6.1] For any w s.t. Y(w) = Z: In this subsection we state some easy technical lem­mas used in subsection 
6.1. l+ob(x[t,n] = W[t,n]Ix[l,t-l] = W[l,z-1], y(~) = O < Pmb( .K[,,m]=w[.,~] I X[l,, _L]=WII,, _I] ) 
< PrOb(Y($)=~ I X[l, z_l]=w[l, t...l] ) .2 ,3 . 2-ml Claim C.2 For any i, if PrObZGX(Y(Z) = i) >0, Prob(Y($)=t 
I XII, t-I]= WII,, l] ) then ProbzeX(Y(z) = t) ~ c1 C2 c3. c~ e~) ~ ml z z m, C2 E3 Claim C.3 1. 
PTob(x c B,) s nc,, fori = 1,2,3. The first line is true since PToZI(A I B) < %, the second line since 
f(w) = i, and the third follows 2. For any i: Prob(f(x) = i and x E B2) s 62 Claim Cl. o 3. For any t 
and WH,,-11: Prob(f(x) = t and x E B3 I ZII,, 1] = W[l,, 1]) <63 Proofi [of claim 6.2] Take any OJ[l,,_ll 
that can be extended to some ~. For any i: Prob(f(x) = i and x c B3) < C3 w with Y(w) = Z. 285 
			