
 STATISTICS IN SI~I%rLATION: HOW TO DESI~ FOR SELECTING THE BEST ALTERNATIVE ABSTRACT In many simulation 
studies the experimenter (the person running the simulation) has under con- sideration several (two or 
more) proposed proce- dures (e.g., for running a real-world system), and is simulating in order to determine 
which is the best procedure (with regard to certain specified criteria of '~goodness"). Such an experimenter 
does not wish basically to test hypotheses, or construct confidence intervals, or perform regression 
analy- ses (though these may be appropriate minor parts of his analysis); he does wish basically to select 
the best of several procedures, and the major part of his analysis should therefore be directed towards 
this goal. It is precisely for this problem that ranking- and-selection procedures were developed. These 
procedures set sample size (in simulation this means run-length) explicitly so as to g~larantee that 
the probability that "the procedure actually selected by the experimenter is the best procedure" is suitably 
large. In this paper we first review the background ideas of ranking-and-selection a,~ contrast them 
to other approaches to multi-population problems (which, while sometimes appropriate in such areas as 
social science experimentation, are almost wholly inappropriate for use in statistical design and analysis 
of simulation experiments). Recom- mended procedures for several common situations are then outlined 
in detail. References where further theoretical details may be obtained are provided, along with information 
on current developments in the area. It is intended that the motivation and technical detail given be 
sufficient for intelli- gent application in many common situations (though other situations will still 
require supplementary consultation). I. BACKGROUND OF MULTI-POPULATION PROBLF~MS Statistics for manyyears 
concerned itself to a large extent with problems in which the basic observations came from one source 
or population (one-population problems). Two-population prob- lems were well-known (if unsolved, for 
example the Behrens-Fisher problem), but for the most part it was a one-populationworld until some time 
in the 1990's when R. E. Bechhofer, by pioneering work Edward J. Dudewicz Department of Statistics The 
Ohio State University (see reference (I) for the first published account of this work, a major event 
in statistical thought) in ranking-and-selection, brought the subject to full light of day wlth a context 
other than the type described by saying (as in classical ANOVA) "We have k populations, but would like 
to test the hypothesis that we really only have one." The relevance of the pioneering ranking-and-selection 
work to statistical design and analysis of simula- tion experiments was soon recognized by workers in 
the field. For example, on p. 93 of (3), Conway stated in 1963 that "...the analysis of variance seems 
a completely inappropriate approach to these problems. It is centered upon the test of the hypothesis 
that all of the alternatives are equi- valent. Yet the alternatives are actually dif- ferent and it is 
reasonable to expect some dif- ference in performance, however slight. Thus, the failure to reject the 
null hypothesis only indi- cates that the test was not sufficiently powerful to detect the difference 
- e.g., a longer run would have to be employed. Moreover, even when the investigator rejects the hypothesis, 
it is highly likely that he is more interested in iden- tifying the best alternative than in simply con- 
cluding that the alternatives are not equivalent. Recently proposed ranking[-and-selection] proce- dures...seem 
more appropriate to the problem than the conventional analysis of variance tech- niques ...." This recognition 
has continued to the present day, as is exemplified by the fact that in Kleijnen's (9) treatise on statistical 
aspects of simulation 77 pages (out of 390 which are non- introductory) are devoted to ranking-and-selection 
procedures (which are also often called "multiple ranking procedures ~' by Kleijnen and others). Nevertheless 
it was true, as pointed out by Conway (3) in 1963, that "...the investigator is still going to have 
difficulty satisfying the assumptions (normality, common variance, independence) that the statistician 
~lll require. '~ However in recent years this difficulty has also largely been re- moved. '~ile in Section 
iI below we will introduce the ranking-and-selection area with an example using the traditional assumptions 
(normality, comz mon variance, and independence of observations) for simplicity, in Section III work 
of recent years al- lows us to recommend procedures given recently for much more general situations. 
 Winter Simulation Conference II. RANKING-AND-SELECTION In order to introduce the area of ranking-and- 
selection, let us talk in terms of a simple explic- it problem, that of choosing (i.e., selecting) the 
job shop precedence rule which yieldsthe highest output on the average. (This particular example is chosen 
only for ease of reference, and we could as easily taik of selecting the queue discipline which yields 
the highest output on the average, or the investment strategy whichylelds the highest return on the average. 
The reader is encouraged is think of an example pertinent to his field and rephrase the considerations 
given below in terms of that example.) To be specific, suppose that it is desired to select that one 
of I0 job shop precedence rules which has the highest average output per period. If we run the shop with 
rule one for one period, we will observe some output, say Xll. However, in a second period, st~]1 using 
rule one, we will ob- serve a different output, say Xl2. S~m~larlywe obtain output Xl3 in a third period,...,output 
 ~N in an Nth period. Thus, in each period the output using rule one differs. However, it is reasonable 
to assume that it varies about some value, say~ l, in the sense that if we average the output per period 
using rule one over manyperiods the ntm~er so obtained will be close to ~l" To take into account the 
variability in output, ass~e that Xll obeys a normal probabi- lity distribution, has mean value ~l' 
and has vari- 2 ance ~ . Similarly for XI2,...,XIN. Then, if one period's output doesn't affect another's, 
 Xll +X12+".. +XIN will obey a normal probability distribution~rith mean value ~I and variance ~2/N, 
i.e. its variabi- lity from~ I is decreased and (if N is large) we ex- expect Xl ~ ~i" Now, considerations 
like the above hold for each of the i0 rules. Thus, we may observe the outputs over N periods of each 
of the_ lO rules and obtain average outputs X l, X 2, ..., Xlo. Since we expect these to be close to 
the mean values ~1'~2 ''''' ~lO of the lO rules, we select the rule yielding the largest average as 
having the highest output (see Table 1). TABLE 1 Rule I Rule 2 ... Rule i0 Period i Xll Period I X21 
Period i XlO,l Period 2 X12 Period 2 X22 Period 2 Xlo,2 Period XIN Period N X2N Period N XIO2N il ~2 
~10 = output in period Xij J using rule i. (1) Select rule yielding max(Xi,X2,...,Xlo). However, it 
will be hard to distinguish the best rule (i.e., the one with the highest mean output) when the mean 
outputs of the other rules are very close to the largest one, since although X1, X2, ..., Xl0Will be 
close to ~l' ~2' "'" ~lO the values ~l' ~2' "''' ~lO are also close to each other. (E.g., although we 
may be 9% sure that  I~i-~i I ~ o. 5, I~2-~21 ~ o. 5 .... , t~o-~IoI ~ o. 5,  if in reality ~l=~2=...~9--a33.hO 
and ~i0--a33.41 we may fall to pick the best rul_e, that with mean output ~i0' since ~, X2, ..., X 9 
as well as XIO will be close to ~I0" This can be remedied by raisingthe sam.~le size N so that we.are, 
e.g., 9% sure that IXI-W11 < 0.O1, IX2-w21 < O.O1 ..... I%0-~10 ~ Thus, if N isn't large enough, ]_ 
0.01.) the probability of selecting the best rule may be unacceptably small; here "large enough" depends 
on the closeness of ~l' ~2' "''' ~lO (see Illustration l, where ~[1S ~ "'" ~ ~[lOS denote the mean outputs 
in n~erical order from smallest to largest). ILLUSTRATION 1 I I t ~1[11  ~t[g] ~l[ll,)  L'-~ -'l 
If it is the case, however, that ~[9] is very close to ~[10] then we may not care whether we select the 
rule with mean output W[lO] or the rule with mean output ~[9] (which is almost as good). In some cases 
we may only care about our chances of selecting the best rule when ~[lO] -W[9] ~ O.1, in which case we 
wish to be 90% sure that we make a "Correct Selection" (abbreviated "CS'); i.e. we desire December 6 
- 8 1976 In general, this desired statement is of the form Prob{CS} ~F* if 6 ~ ~[lO] - ~[9] ~ 6- (2) 
 where 6* and P* are pre-set by the experimenter (e.g., 6- = 0.i and P* = 0.90). Note that 6- must be 
positive (a requirement with 6- < 0 is meaning- less since ~[I0] - ~[9~ ~ 0 always,-by the defini- tion 
of ~[lO] as the largest of ~l' ~2' "''' ~lO ) and that O.lO <F* < 1 (P* < 1 since we can never be absolutely 
certain that the best rule yielded the largest of Xl X2, ..., Xlo --there is always a chance, however 
small, that another rule did; P* > O.lO, since we can be assured of a 10% chance of correct rule selection 
by picking one of the rules at random). It is clear that the Prob[CS} is minimized when rules other than 
the best have mean outputs as large as possible. When we ~'care" (i.e., when ~[i0] -- ~[9] ~ 6-) this 
means the Prob[CS~ is a minimum when ~[i] ..... ~[9] : ~[i0] -6*, which is therefore called the least- 
 favorable configuration (LFC). Since available tables (1) allow us to choose N so that Prob[CS] is at 
least P* when the LFC ~[i] ..... ~[9] = ~[i0] - 6. is the case, we can choose N so as to achieve (2). 
 This example suggests certain conclusions about selection procedures. First they are pre- cise; that 
is, the selection approach can give us a rational basis for choosing N (the nt~nber of periods to be 
observed) and tell us (e.g.) how the Prob[CS} varies as we change N, and how large the Prob[CS} is if 
in fact ~[1] ..... ~[9] = ~[lO] - 6 for some value 6 other than 6-. C~n- trast this to a typical old-style 
approach: test- ing the hypothesis that the lO mean outputs are equal (perhaps by running an Analysis 
of Variance on an elaborately-designed experiment) and then selecting the rule yielding the largest sample 
mean as having the largest mean output if the test accepts the hypothesis that the're un- equal (while 
saying "there's no difference" if the test accepts the hypothesis they're equal). Not only does such 
an approach make little sense be- cause we know they're not equal and should thus always select, but 
it offers no rational (with re- gard to the problem for which it is being used), precise grounds for 
choice of N. Note that this does not mean that one should neglect to use care- ful design choice if the 
selection approach is ap- propriate to his problem (see p. 29 of (1) for further details). Second, selection 
procedures are practical in two ways. First, they are applicable to problems often arising in practice, 
and second, they are feasible because quantities such as necessary sam- ple sizes N have been tabled 
or can be computed. This may be contrasted to the situation in other branches of statistics where some 
quantities are almost impossible to compute.  The essential problem formulation of 1994 is thus that 
we have: populations (sources of observations) UI' .... ~k (k ~ 2) ~th respective unknown means Ul,...~k 
for their observations, and whose observations obey a normal probability distribution with a common 
known variance 2 about their respective means; a goal of selecting the population associated with = 
max(~l,...,~k); a probability require- ~[k] ment that Prob[CS} > P* (1/k <P* < I) if ~[k] - ~[k-l] ~ 
6" ~5" > 0); and a p roc?d_ure of selecting the population yielding XMAX =   maX(Xl' x2' .... ~1" 
 While the above example assumed normality of observations, common variance of output per period for 
each rule, and independence of observations across periods (all somewhat difficult to justify), the procedures 
given in Section I!I weaken these assumptions to an extent sufficient to make this approach feasible 
in a significant number of simu- lation studies. As a n~nerical example, if we have k = I0 rules with 
g = 5 units per period and wish to have probability of correct selection at least P* = 0.99 whenever 
the average output of the best rule is at least 8* = 2 units larger than the other outputs, then we will 
need a sample size of at least  h~(~)2 (3.4182)2(9)2 N -(S*) 2 (2)2 = 73.03 (3) periods, and so would 
take N = 74 periods in the simulation. The factor ~(P~) needed is obtained from tables originally given 
by BecILhofer (1) which are reprinted on p. 3&#38;7 of (9). Note that itls known (4) that a good approximation 
to the required as long as we have a common variance ~ if we have N is given by ~l ~ (8.)2 , (4) which 
in our e xample is N 1 = 74 .89. Also n2 ote that correlations within periods (i.e., XII , ~I' "''' 
 XlO,I are correlated) but not across periods (i.e. Xil and Xi2 are uncorrelated), and if all correla- 
 tions are positive or zero, then the N of eqtlation  (3) is conservative. This was shown in (4) and 
partially ~ustifies the traditional wisdom that ~'positive correlation within a block is helpful in simulation". 
 III. RECOMMENDED SELECTION PROCEDURES If we are faced ~ith k ~ 2 normal populations with unknown means 
~l,...,~ k and a common known 2 variance ~ , then the sample size N calculated from equation (3) will 
suffice when the observations are independent (and, in fact, is sufficient and con- Winter Simulation 
Conference servative even if one has positive correlations within periods). More often in simulation 
the rules to be eval- uated will have ~nequal variances ~12, .., ~ which are also unknown. This problem 
was solved recent- ly by Dudewlcz and Dalal (6), and the solution has been applied in simulations for 
selecting water resource system alternatives by Vicgns and Schaake (ii) and in accounting system simulations 
by Lin (i0). This solution is very appropriate if no correlations are present. Since correlations within 
a population are often present (e.g., due to lack of frequent regeneration points), a heu- ristic procedure 
recently developed and studied by Dudewicz and Zaino (8) will be given for this more general problem. 
 The rec(~mended Procedure A(~i,si 2)_ is as fol- lows. Take an initial sample of N O = 30 observa- 
 tions using each rule. Calculate 2 2 s. h (which is the number of observations which would be needed 
if we had all correlations Pi = O (zero correlation), where h depends on k and P* and is given in Table 
2 below, extracted from (7)). Cal- culate N zO(x. --X )(X~. ,-X~) (6) ~i = N z Â° (x~-~i)2 n=l 
and form the iO0(i-~)% confidence interval for Pi from NO--I (i-~)t2No_3(l-~2) (7)   (pi--~i)2 ~ 
with ~ : .O5. (Here tr(q) is the lOOq percent  point of Student's-t distributionwith r degrees of freedom.) 
If this 9~ confidence interval con- tains 0 i = O, judge the sample size N O as being adequate for population 
i. Otherwise celculate F /l+~ih9  N21 : LMit -- J] (8) and continue the run until we have N2i observa- 
 tions from Wi" ~-~inally calculate Xl'"''~ based on all available observations and select (as being 
best) that_populationwhich produced the largest of XI,..., ~. 2  While Procedure A(~ i, si) is heuristic 
(unlike the procedure of Dudewicz and Dalal for Pi = O,  which is entirely rigorously derived), studies 
show it should be sufficient to preclude gross er- rors due to significant correlations. (Work in progress 
presently studies properties of Procedure ^ 2  A(0i,si) in o further detail by Monte Carlo, com- pares 
A(~i,s~) with procedures based on the regen- erative methods of Iglehart, and develops a cor- responding 
fully rigorous mathematical procedure by utilizing the Heteroscedastic Method recently developed by Dudewicz 
and Bishop (see (2)).) TABLE 2 C~antity h Needed in Equation (5)  P* = .99 P* =.99 k= 2 2.41 3.49 
k= 3 2.81 3.81 k= 4 3.03 4.01 k= 5 3.18 4.14 k: 6 3.30 4.25 k= 7 3.39 4.33 k= 8 3.46 4.40 k= 9 3.53 
 4.46 k = i0 3.58 4.51 k = 19 3.79 4.71 k=20 3.92 4.84 k = 25 4.03 4.94 BIBLIOGRAPHY  i. Bechhofer, 
R. E. "A Single-Sample Multiple Decision Procedure for Ranking Means of Normal Pop- ulations with Known 
Variances," in Annals of Mathematical Statistics, Vol. 25 (195-~-~-,pP7 16- 39. 2. Bishop, Thomas A. 
Heteroscedastic ANOVA, MANOVA~ and Multiple-Comparisons. Unpublished Ph.D. Dissertation, Department of 
Statistics, The Ohio State University. Columbus, Ohio, 1976.  3. Conway, R. W. "Some Tactical Problems 
in Digital Simulation," in Management Science, Vol. IO, No. i (October 1963), pp. 47-61.  4. Dudewicz, 
Edward J. "An Approz~mation to the Sample Size in Selection Problems,' in Annals of Mathematical Statistics, 
Vol. 40, No. 2-T~)V- pp. 492- 497.  5. Dudewicz, Edward J. Introduction to Statis-  tics and ProbabilitM. 
Holt, Rinehart and Winston, New York, 1976. 6. Dudewicz, Edward J. and Dalai, Siddharths R. "Allocation 
of Observations in Ranking and Selec- tionwithUnequal Variances," in Sankh~N, Series B, Vol. 37, Part 
i (1975), PP. 28-78. 7-Dudewicz, E. J., Ramberg, J. S., and Chen, H. J. "New Tables for Multiple Comparisons 
With a Control (Unknown Variances)," in Biometrische December 6 - 8 1976  8. Dudewicz, Edward J. and 
Zaino, Nicholas A., Jr. "Allowance for Correlation in Setting Simulation Run-length via Ranking-and-Selection 
Procedures," Technical Report, Department of Statistics, Stanford University, Stanford, California~ 1976. 
To appear in Management Science.  9. KleiJnen, Jack P. C. Statistical Techniques in Simulation.~ .Part 
II. Marcel Dekker, Inc., New York, 1979.  i0. Lin, W. T. ~Multiple Objective Budgeting Models: A Simulation," 
USC Working Paper ~12-01-79, Department of Accounting, Graduate School of Business Administration, University 
of Southern California, Los Angeles, California. ll. Vicgns, Guillermo J. and Schaake, John C.~ Jr. 
"Simulation Criteria for Selecting Water Resource System Alternatives," Re~ort No. 1~4, Ralph M. Parsons 
Laboratory for Water Resources and Hydro- dynamics, Department of Civil Engineering, Massachusetts Institute 
of Technology, Cambridge, Massachusetts, September 1972. Winter Simulation Conference   
			