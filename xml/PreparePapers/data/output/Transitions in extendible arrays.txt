
 Permission to make digital or hard copies of part or all of this work or personal or classroom use is 
granted without fee provided that copies are not made or distributed for profit or commercial advantage 
and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, 
to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee.&#38;#169; 
1973 ACM 0-12345-678-9 $5.00 TRANSITIONS IN EXTENDIBLE ARRAYS* Arnold L. Rosenberg Mathematical Sciences 
Department IBM Watson Research Center Yorktown Heighta, New York ARSTRACT Arrays are among the best understood 
and most widely used data structures. Yet even now, there are no satisfactory techniques for handling 
algor ithme involving extendible arrays (where, e.g. , rows and/or columns can be added dynamically). 
In this paper, the problem of allocating storage for extendible arrays is examined in the light of our 
earlier work on data graphs and addressing schemes. A formal analog of the assertion that simplicity 
of array extension precludes simplicity of trans­ition (marching along rows/columns) is proved. 1. INTRODUCTION 
Few classes of data structures are as well understood or as widely used as are arrays. The most primitive 
of high level languages offer some array processing facilities; certain languages such as AFL have been 
designed with arrays as the basic data structure. One striking deficiency in current techniques for handling 
arrays is the poor handling of extendible arrays --arrays whose sizes in vari­ous dimensions may change 
in the course of execut­ing an algorithm. Consider, for instance, a situ­ation in which a two-dimensional 
array is stored by rows, Should one wish to add a new row to this array, one could do so with no material 
change to the functions which access elements of the array or which are used to traverse rows or columns 
of the array. In contrast, adding a column to this array would create two undesirable options: One could 
store the column willy-nilly and thereby destroy the simplicity of the element-access and transition 
functions. Or, one could opt to pre­serve the simplicity of these functions at the coat of reallocating 
storage for the whole array; only elements of the first row of the old array would escape being moved. 
(Note that we are ignoring all problems that accrue because of other material stored near our array.) 
Is such asyrmnetry in the handling of rows and columns inevitable? Do there exist schemes for allocating 
storage for arrays which are at once easily extendible and easily accessible and traversable? Of course 
a formal framework is required before one can even state these questions with precision. In Section 2, 
we discuss the problem of find­ing a model for arrays which is suitable for study­ing problems of implementation 
and extendibility. We settle on two models, each of which is approp­ riate in certain contexts: The _ 
array, which is oriented towards languages such as FORTRAN and PL/1, models a situation where no attempt 
to add both rows and columns dynamically to an array is 1 the ~rth~t array, which is compatible envisaged; 
with the APL view of arrays, models situations where no constraints on dynamic extensions to an array 
are assumed. Section 3 is devoted to study­ ing array realizations in which row and/or column transitions 
are very efficient, being realized by additive displacements. Using the auxiliary notion of a shift chain, 
sharp distinctions between the . prism and orthant models are demonstrated. Specifi­cally we prove in 
a sense made precise in the text that the goals of easy extension of arrays (adding rows or columns at 
will) and of efficient transition along both rows and columns are incompatible. How­ever, we demonstrate 
also that these goals can almost be attained simultaneously at the cost of inefficient storage utilization. 
The material in this paper is developed more fully in the first half of [3]. 2. BASIC NOTIONS A. Arrays 
and Their Realizations A definition of array suitable for studying array implementation should (a) expose 
the structure in array coordinates which facilitates implementation and (b) specify those moves in the 
array which are to be implemented efficiently. Let N denote the positive integers, and let Nn= {I,.. 
,n} for ncN. (2.1) A k-dimensional ~ (scheme) of size <nl, . . ..~> (k,nl, . . ..nkcN) is an ordered 
pair A = (P,M) where (d ~ = NnlX N x---xN is the aet of 2 k positions of the array; (b) M is a finite 
set of -, each move ~ being a partial transformation lJ:p + P of the set of positions. 1 In this and 
subsequent discussions we concentrate on two dimensional arrays for ease of exposition. The formal sections 
of the paper do not share this restriction. * This research was supported in part by ONR Contract NQO014-69-C-0023. 
218 Xi+l if i=j A = (P,M) ia a standard array i.f former being defined the latter only on only multiples 
on multiples of 3. of Figure 2 and 2 illus- M = {uj, njljcNk} where for arbitrary trates schematically 
the layout of storage under this realization. x = xl .­ .,x >6P,k Our notion of realization suggests 
immediately if xjcNn _l (Xlsj)i = if i+j I j i I ~ undefined otherwise. [x. -l if i=j) 1 J. if x d j 
nj-{l}i+j } (Xmj)i = i f ~ un~fined otherwise, a is the j _axis successor, j th the j sxie predecessor. 
In this paper we study only standard arrays; however, our proof technique are sufficiently general to 
overcome this restriction. Figure 1 portrays a standard array of size <3,4>, The symbology of (2.1) should 
be materially clarified by thie picture. Our notion of realization or implementation of an array is 
purposely as general as possible. Our notion is geared to establishing a correspon­dence between posit 
ione/moves in th~ array be~ng realized and positions/moves in an idealized infinite linear space of addresses. 
See [1] for a discussion (in a more general setting) of the rationale behind our definition. (2.2) A 
realization of the array A = (P, M) ia a pair of total, one-to-one functions R = <~,p> where (a) &#38;:P 
+ N is the storage ~; (b) P:M+ {partial transformations of N} is the access ~; subject to the conditions 
 (c) peP ia in the domain of ueM precisely when (P@(!.lP)d f; (d) if pc domain(v), then (Pu)~= (P@(!-lP). 
These conditions assert that transitions in the array can be effected within the realization and that 
any transition in the realization reflects one in the array. Definition (2.2) is easily illustrated by 
the following realization of the array of Figure 1. i-13j-l(2.3) (a) For <i,j>cP, <i,,>; = 2 (b) Using 
Church s familiar lambda notation for functions, u 1P = An[2n], mlp = An[n+2], U2Q = An[3n], n2p = An[n+3]. 
Note that ITlp and r2p are both nontotal, the three criteria for assessing the quality of a real­ .. 
ization, and the precealng example suggests a fourth: (a) Complexity of element access: the compu­tational 
complexity of the storage map ~. (b) Complexity of transition: the computa­tional complexity of the 
transition functions MP = {!.IPIucM}. (c) Complexity of extension: the complexity of obtaining, from 
a realization of an array A of size <nl, . . ..nk>. a real­  ization of a superarray of A of size <n 
,...where n~c{ni, ni+l} for 1 i each i (i.e., the extent to which ~ and p must be changed). (d) Inefficiency 
of Storage Utilization: the extent to which a storage map leaves gaps (i.e., is not onto a prefix Np 
of N). The main thrust of the investigation of which this paper is the first output is to determine tradeoffs 
between criterion (c) and the other three criteria. Here we concentrate on criterion (b) vs. criterion 
(c). In [3,4] we study all four crite~a as well as other concomitants of easy extension. We do not, at 
this point, wish to pin our­selves down to precise interpretations of (a) -(d). [This is at least partly 
because we feel that these notions do not admit absolute definitions; the pro­ priety of a given notion 
of efficiency may depend on the intended computing environment.] Even at an intuitive level, however, 
we can remark that realization (2.3) has the following characteristics: (a) Element access is not simple, 
since ~ is exponential in i and j. (c) Extension is very simple, singe F and P needn t be changed at 
all.z v (d) Storage utilization is not efficient since only twelve of the 108 affected locations are 
used; i.e. , only 11% of N108 is used for storing the array.  (b) Transition is rather simple, since 
each step involves just one multiplication/ division. We contrast realization (2.3) with the follow­ing 
store-by-row scheme. 2 Strictly speaking, the domain of ~ would have to be enlarged, but we ignore this 
formal point since the required extension of ~ is straight­forward and requires no change to ~ on P. 
(2.5) (a) For <i,j>cP, <i,j>~ = 4(i-l)+j, (b) IJlp = An[n+4]; TIO = An[n-4]; n(02p) = n+l if n ZO (mod 
4) { undefined otherwise; n(n2p) = n-1 if n ~1 (mod 4) { undefined otherwise. Figure 3 illustrates schematically 
the layout of storage under realization (2.5). As before, even an intuitive notion of simplicity suffices 
to evaluate this realization relative to the criteria of (2.4): (a) Element access is simple, since 
~ is linear in i and j. (b) Transition is simple, since each step involves a single addition/subtraction 
(plus, possibly, a test for a residue class modulo 4). (c) Ease of extension is more complicated to 
evaluate. Adding a row is very simple,  equiring 0 change n k r p dd­ing a column is rather difficult 
since II ~~ ~~ ~~anged (in fact, o*ly remain fixed) and so,, must p. (d) Storage utilization is very 
efficient, the 12 cells of the array being assigned to locations 1-12 for 100% utilization. IS it inevitable 
that realization (2.3), which is easily extended, admit more complicated trans­itions than the not easily 
extended realization (2.5)? We pave the way for answering this question. B. Two Models for Arrays We 
need a model for arrays which allows us to discuss extendibility, Now, easy extension is epitomized by 
a situation where the storage map for the extended array is just a (functional) extension of the storage 
map of the original array. Therefore, easy extension along a certain direction is assured if one constructs 
a paper realization not of the array in question but rather of a superarray which is infinite along the 
desired direction. One then realizes the array in hand by a (functional) restric­tion of the storage 
map of the infinite array. Unbridled extendibility is thus embodied in an array scheme which is infinite 
in every direction, one whose positions are the lattice points of the positive orthant of k-space. These 
infinite array schemes afford us the desired models of arrays? models which ensure simplicity of extension 
of an allocation scheme as we study simplicity of trans­ition. Realizations of infinite arrays are not 
unfam­iliar to anyone who haa considered realizations of finite arrays, One would be hard put to specify 
a realization of an array which waa not really the restriction of a realization of an infinite array 
scheme which had the given array as a prefix. For instance, realization (.2.3) of our aemple array of 
size <3,4> is a restriction of a realization of the 2-dimensional orthant ~. (2.6) The k-dimensional 
orthant ~ (kcN) is the pair = (Pk,~) where ik (a) Pk = Nk = NxNx.--xN (k times); (b) ~= {uj,mjljENk}; 
for x= <x ,.. .,x>ep and jcNk,  1 kk (Xuj)i = Xi+l if i=j if i+j{ i (xlTj)i = p xi-l xi if if i=j i#j 
if xjcN-{l] undefined otherwise. ia the axis j successor and T. is j 3 the axis j predecessor. Similarly, 
realization (2.5) is a restriction of a realization of the following (2-dimensional 4-column) prism array, 
as can be seen in our comment about the ease of adding rows in that realization. (2.7) The 2-dimensional 
c-column ~rism array is the pair IIc = (P,M) where (a) P= NxN . c (b) M = {IJ1,1J2,~l,~2};  for ci,j>cP, 
<i.,j>u = <i+l,j>;1 <i, j>n = ~ J<i-l,j> if i>l ~ undefined otherwise; <i, j>u ~= \<i,j+l> if j.sNc_l 
\ undefined otherwise; <i,j>n ~= j<i,j-l> if jeNc-{l} ~undefined otherwise. IIc is obviously a prefix 
of which is bounded 2 in the second coordinate. By considering generally prefixes of which are bounded 
in k-1 coordin­ ik ates, one obtains the family of prism arrays. (Hybrid arrays obtained by bounding 
fewer than k-1 coord­inates can, of course, be defined; the reader can easily extend our results to such 
hybrids.) By viewing array realizations, henceforth, as realizations of infinite array schemes, we shall 
be better able to recognize the type of extension which they admit gracefully. Conversely, by con­ 220 
strutting realizations for an infinite array scheme which models the kind of array extensions one wishes 
to facilitate, one can attempt to optimize our other quality criteria, all the while being assured of 
ease of extension. 3. ALLOCATION SCHEMES WITH ADDITIVE TRANSITIONS Many computational procedures call 
for repea­ted traversal of the axes (e.g., rows or columns) of arrays. We now consider how compatible 
criter­ion (b), ease of transition, is with our main topic of study, criterion (c) , simplicity of extension. 
We have, to this point, purposely avoided pin­ning down any notion of simplicity of transition. At this 
point, we can no longer afford such aloof­ ness. However, we shall restrict our attention in the main 
to two types of access maps which are so obviously efficient that we have no reluctance in labelling 
them so, namely, access maps which assoc­ iate with an axis successor a (possibly piecewise) 3 Relative 
to this rather additive displacement. strong notion of efficient transition, we uee the prism and orthant 
models to establish formal analogs of the following results. (I) No array realization which enjoye simple 
transitions along both rows and columns admits simple ad junction of both rows and columns. (II) No 
easily extendible array realization affords simple transition along both rows and columns.  The negative 
impact of these results is mitigated by exhibiting a readily extendible realization with relatively eimple 
transitions (but, unfortunat­ ely, with inefficient storage utilization). This last example will serve 
also to hint at the diffi­culty of establishing formal analogs of (I, 11) which are materially stronger 
than ours. A. Shift Chains4 The development in this section is facilitated by a slight digreaaion into 
a more abstract frsme­work. Specifically, we introduce the notion of a shift of a set and the related 
notions of shift relationa and shift chains. . Let A be a transformation of a set S, i.e. , a (possibly 
nontotal) function from s into s. ~k For any nonnegative integer k, define by: 1° = 1s (the identity 
function on S); and gener­~i+l ~i ally, = lli (the composition of with X), For arbitrary a,teS, we say 
that a A-precedes k t -- written s<At --if sa = t for some kcN. 3 That is, successive steps along, 
say, a row are effected by successively adding a given constant which, in the piecewise case, depends 
on the row. 4 The material in this section presupposes some elementary concepts of modern algebra. The 
reader can consult a beginning text like 15] for back­ground. Clearly i is a transitive relation. (3.1) 
The transformation A is a shift (of S) if the following three conditions hold: (a) k is injective (= 
one-to-one); (b) ~ is cycle free --for no kcN doee ~k  have a fixed point; (c) each scS~ (the range 
of 1) has a A-predecessor which is not in SA. For any shift A, the relation <A (which we now refer to 
generically as a shift relation) is a strong partial order; that is, it is asymmetric, and irreflexive 
as well as transitive. Even stron­ger, the order ie the union of disjoint well­ i orders. The relation 
can, thus, be viewed as a collection of parallel shift chains, each chain . with a minimum element meS 
which is connected to 2 mX, which in turn is connected to ml = (mk)A, and so on, possibly forever. Our 
interest in shift chains and relations stems from the fact that the axis successors of array schemes 
are shifts, as indeed, are all func­tions obtained as finite compositions of the suc ceseors (e. g., 
diagonal successors like U102). No axis predecessor is a shift in orthant arrays since the predecessors 
relations lack minimal elements. Each predecessor save one is a shift in a prism array. In finite arraye 
the notione of predecessor and successor are interchangeable at the whim of the beholder, so both types 
of transformations are shifts. Thus, information about realizations of shift relations can be translated 
into information about array realizations with emphasis on the access maps. We consider first t-he problem 
of realizing a single shift relation. (3.2) Let S be a set, let A be a shift of S, and let MA s S be 
the set of minimal elements of the shift relation Let A ~:S + N be a total one-one function. we say 
that T has the (a) additive property for k if, for each m~MA there is a constant kmeN such that, for 
all ncNu{O}, if mc domain(in) , then (mln). = (m.) + n-km; uniform additive property for i if there is 
a constant k6N such that, (b) for all SCS, if SE domain(l), then (SI)T = (ST) + k. The term additive 
derives from the fact that any l-chain can be traversed by successive additions of a constant which depends 
only on the chain. The term uniformly additive indicates that all chains are eerved by the same constant. 
When can we find a T with one of these pro­perties? Some terminology will help us expose the answer. 
The shift relation is finitary if A each shift chain is of finite length. The relation is thin if the 
set of minimal elements is A finite, so that there are but a finite number of shift chains. Finally, 
<A is limited if all but finitely many of its shift chains are of finite length. Theorem 1. Let S be 
a countable set with a shift A. (a) There is a total one-one map Ta:S + N with the additive property 
for 1. (b) When is limited5 --and only A when is limited --there is a total one­ A one map TU:S + N with 
the uniform additive property for l., In general we are interested in realizing more than one shift in 
an efficient way. For example, one would usually wish both row and column successors to be easy transitions 
in a matrix. It appears that ~ositive results about two or more shifts can be sought fruitfully only 
in a concrete environment, where information about the relation­ships among the shifts can be brought 
to bear. Hence, we defer consideration of injections with the (uniform) additive property for several 
shifts to the next subsection where we consider only arrays. However, an important negative result with 
implica­ tions for orthant arrays can be formulated in the present framework. (3.3) Let 1 and p be shifts 
of the set S. We say that i and p are independent if there do not exist Ses and L,meN for which S1l = 
Spin. [Of course, we assume both are defined.] 5 In particular, <A is limited if it is either thin or 
finitary. 6 In the context of arrays, independence is essen tially linear independence of vector displacements. 
When i and u are total, independence is precisely linear independence. As an example, any two axis successors 
in any of our array models are independent. Theorem 2. Let S be an infinite set, and let 1 and u be independent 
total shifts of S. There is no total one-one map 7:S + N which has the additive property for both A 
and U. Theorem 2 can be generalized along the following lines. Say that the (not necessarily total) independent 
shifts A and v are both to be realized with the additive property. Then, intuitively, if the i-chain 
emanating from eeS is long, then the P-chain em~ating from s must be short, and vice versa. B. Applications 
to Arrays Rather than continue in the abstract vein of Section A, we return to our study of allocation 
schemes for arrays, but we do so in the light of Theorems 1 and 2. Transitions in Prism. In any prism 
array, one axis successor is a thin and each of the others is a finitary shift of the set of cells, 7 
Hence, by Theorem 1, any single successor can be realized with the uniform additive property. In fact, 
the orthogonality of the axis successors can be exploi­ ted to realize all successors with the uniform 
additive proper-Such totally additive schemes can be easily derived from the familiar schemes used in 
compilers for languages like FORTRAN and PL/l; a derivation of such schemes appears in [6, p.296]. Note, 
however, that these familiar schemes must be modified somewhat to comply with the restriction, (c) P6P 
is in the domain PCM precisely when (P@(PP)cp~ in the definition (2.2) of realization. To be more precise, 
the schemes alluded to are perfectly good realizations of prism arrays, but they are not totally additive. 
Note, for example, our ~lization (2.5) which implements the 2-dimensional 4-column prism array using 
a scheme like those in [6] (cf. Figure 3.). Note in (2.5) that u2p was defined as follows: for all neN, 
n+l if n #O(mod 4) n(02p) = { undefined otherwise. We could not opt here to let CJZP be the successor 
function (kn[n+l]) or else we would have (<i,4>@(u2P) = <i+l,l>~, while <i,4>u2 # <i+l,l>. This problem 
is overcome by adding gaps to the scheme of realization (2.5); for instance, 7 strictly sPeaking the 
terms thin and finitarY have been defined only for shift relations; how ever, the reader should have 
little problem in deciphering our loose usage, 222 (3.4) (a) For <i,j~cP, <i,j>~ = 5(i-1) + j. (b) Olo 
= ~n[n+5] ; Tlp = kn[n-5]; J20 = ~n[n+l] ; n2p = An[n-l]. The modified storage layout is depicted in 
Figure 4. For the loss of less than 20% storage utilization (in the limit),8 one thus obtains a totally 
addi­tive realization of this prism array. Using (3.4) as a model, the reader can most assuredly derive 
the appropriate modifications of the general schemes in [6, p.296]. Thus, prism arrays admit transition 
which are very efficient, albeit at the cost of ease of extension. What happens to ease of transition 
when ease of extension is emphasized? Transitions in Orthant Arrays. In an orthant array, all axis successors 
are total, and none is limited. Any two distinct successors are independent shifts of the set of cells. 
From these facts one can deduce Fact 1. (Theorem 1) Any single axis succes­sor can be realized with the 
additive property, but none can be realized with the uniform additive property. Fact 2. (Theorem 2) No 
realization can have the additive property for more than one axis successor. Fact 2 is the formal version 
of our assertion that, in some sense, ease of extension (which is embodied in an orthant realization) 
is. incompatible with ease of transition (which we are equating with the additive property). Together 
with our earlier discussion, Fact 2 points out why the prism model is to be preferred when extension 
is of little concern. HOW well can one do with the additivity guar­ anteed by Fact 1? At the cost of 
quite inefficient storage utilization, one can do rather well - at least in two dimensions. Consider 
the following orthant realization which is depicted in Figure 5. (3.5) (a) For ci,j>cP , <i,j>~ = 2 -1(2j-1). 
2 (b) alp = ln[2n] ; TIP = kn[n+2]; for <i,j>6P , 2 (<i,j>~) (02p) = (<i,j>~) +2i ; (zi,j>;)(n2p) = (zi,j>,@ 
-2i if j>l, { undefined otherwise. In this realization, the row euccessor U2 is additive. While the 
column successor al is not additive, it is just multiplication by 2, an easily implemented operation 
on most computers. Thus , this realization affords quite efficient transitions ae well as easy extension. 
However, l_t is a rather poor realization in its utilization of storage; and it quickly deteriorates 
in this respect 8 Observe that 4k elementS of N5k_1 are used for a k row prefix. as the prefix implemented 
grows by adding rows: An m row, n column prefix uses only mn of the 2mT1(2n-1) locations affected by 
the allocation, so that, in the extreme, a column vector use exponentially fewer of the affected locations 
as It grows. Such sparseness is an -inevitable concomitant of any realization with a multiplicative transition. 
Realization (3.4) is valuable, not necessarily as a technique for allocating storage for arrays, but 
as an illustration of the complicated inter­play among the quality criteria of (2.4). It indi­cates also 
that a materially stronger statement than Fact 2 of the incompatibility of ease of extension and ease 
of transition will require delicate formulation. We have thus far left unanswered the question of whether 
or not there is a good realization of the orthant arrays. In [3,4] we demonstrate that, at least in special 
situations, such good real­izations do exist. Much study is needed before we really under­stand these 
best understood of all data-structures. ACKNOWLEDGMENT It is a pleasure to acknowledge the encourage­ment 
and assistance of several of mv colleawes. notably John S. Lew and Shmuel Winograd. -- REFERENCES 1. 
A. L. Rosenberg, Data grapha and addressing schemes. J.CSS 5 (1971) 193-238. .  2. A. L. Rosenberg, 
Exploiting address ability in data graphs. In Computational Complexity: Courant Computer Science Symposium 
7. (R, Rustin, cd.), Algorithmic Press, New York, 1973. 3. A. L. Rosenberg, Allocating storage for extendible 
arrays. IBM Report RC-4306, 1973. Also submitted for publication. 4. A. L. Rosenberg, Inherent limitations 
of  extendible array realizations. In pre­paration. BACKGROUND 5. G. Birkhoff and S. Mac Lane, A Survey 
of Modern Algebra, Macmillan, New York, 1961. 6. D. E. Knuth, The Art of Computer Program­ming I: Fundamental 
Algorithms, Addison-Wesley, Reading, Mass. 1968.  Figure 1. The Array Scheme A of Size <3,4>. Figure 
2. Realization (2.3): A Sparse Realization of A. Figure 3. Realization (2.5): The Array A Stored by 
Rows. 224 Figure 4. Realization (3.4): A Totally Additive Realization of the 4-Column, 2-Dimensional 
Prism Array. Figure 5. Realization (3.5): A Realization of with Simple Transitions. 2 
			