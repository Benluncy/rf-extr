
 A Taxonomy for Identifying a Software Component from Uncertain and Partial Specifications Giancarlo 
Succi, Francesco Baruchelli, Marco Ronchetti LII/DISA, Universit5 di Trento Via Zeni 8, 1-38068 Rovereto, 
Italia e-mail: {First.Last} @lii.unitn.it KEYWORDS: Uncertain Specifications, Reuse, Classifi- cation, 
Retrieval. ABSTRACT Software maintenance and reuse depends on a system for inserting and subsequently 
searching for software objects in a repository. A number of classical methodologies from library and 
information science exist and seem appropriate to this purpose. All of these methodologies bring to a 
high degree of uncertainty both in the specification of the searched objects and in the way an object 
is classified. To give a basis for a better understanding of these approxi- mations, this paper presents 
a formal view of the basic li.brary science methodologies for indexing and classifica- tion. The formalization 
permits the computational com-plexity of the different methodologies to be compared. In considering the 
special needs for extensions to the method- ologies to support software reuse, a strategy based upon 
a hybridization of the simplest three methodologies is pro- posed rather than using or developing more 
complex meth- odologies. The authors conclude that some extensions oriented towards the inevitable informal 
human side of the process may be necessary based on a domain analysis of the envi- ronment in which such 
methodologies must operate, but these extensions should be accounted for by engineering the proper elements 
into a formal framework which will "Permission to make digital/hard copy of all or part of this material 
without fee is granted provided that copies are not made or distributed for profit or commercial advantage, 
the ACM copyright/server notice, the title of the publication and its date appear, and notice is given 
that copying is by permission of the Association for Computing Machinery, Inc.(ACM). To copy otherwise, 
to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or 
a fee." &#38;#169; 1996 ACM0-89791-820-7 96 0002 3.50 support software reuse and reusability. INTRODUCTION 
Software reuse can help to increase the quality and to decrease the cost of a program, but it comes along 
with a fundamental problem: the location and the retrieval of soft- ware objects from a large collection 
called a repository. To reuse a software object, you first have to find it! The essence of the problem 
is to deposit software objects into the repository according to descriptions of the objects expressed 
in a formal language concise enough to serve as a subsequent indexing scheme. Then at some future time, 
a potential reuser of the object may describe his concept of a desired object in the same indexing scheme 
language. The translation of an informal description into a limited formal language inevitably bring 
to a loss of information and to a certain degree of uncertainty in the characteriza- tion of the object. 
The indexing scheme should be composed of two proc- esses: the process in which a description is associated 
to the object which is inserted (classification), and the search query process (retrieval). A high degree 
of uncertainty can be found in the retrieval process, too. In fact, the specifications of the objects 
to look for are very often uncertain and only partial: only very rarely one knows exactly what he is 
looking for, and even in this case the informal description which characterize the searched object has 
to be translated into the limited formal language, with a resulting increase in the degree of uncer- 
tainty of the query. In order to better understand how this uncertainty affects the classification and 
the retrieval of software assets, it is important to deeply understand the commonly used classifi- 570 
cation methodologies. For this reason in the following a tbrmal representation of the methodologies will 
be given, along with an evaluation of their computational complexity. A survey of methods for representing 
software components for reuse was made by Frakes and Gandel in [8]. This sur- vey includes traditional 
library science methods, knowl- edge based methods and hypertextual systems, while completely formal 
approaches (such as predicate calculus, Horn clauses, or a declarative machine language) to repre- sent 
search queries or classifications of inserted objects as tbrmal specifications [2], [3], [5], [12], [13]), 
and automatic programming using generative methods [4], [9], [ 10], [ 11 ], [i]) have been excluded since 
they completely side step the classification/retrieval language compatibility issues and put other problems 
in their place. Artificial Intelligence and HyperText systems were included in this survey, but these 
systems work on a fine-grained level of detail requiring massive investments of effort to establish the 
complete con- nectivity of the elements in the cases of complex software objects/systems. It is commonly 
accepted as an alternative to formal approaches with a large-grained level of descrip- tive representation 
for software objects to use the widely known classification and indexing schemes originally developed 
in the context of library and information sci- ences. These classification and indexing methodologies, 
which will be intuitively summarized in the next section (A Tax- onomy of the Common Classification Methodologies), 
have aspects that are easily formalized, as will be described in Section 3. Then the formal classification 
and indexing methodologies will be subjected to a subsequent complex- ity analysis, presented in Section 
4, where we examine advantages in combining the various simple schemes over more complex variants. In 
Section 5, some possible exten- sions to make the simple schemes more operational in prac- tical contexts 
will be covered, and in Section6, we will summarize and draw conclusions. +A TAXONOMY OF THE COMMON CLASSIFICA-TION 
METHODOLOGIES We present here a summary of the survey of Frakes and Gandel [8]to give an overview of 
the standard classification and indexing methodologies used by library and informa- tion science; further 
work by Frakes and other researchers in the field can be found in [7]. Traditional library and information 
science indicate with indexing, or classification, the process of creating a repre-sentation (essentially 
a description) of an object. Obviously there is a strong connection between classifica- tion and retrieval. 
A general retrieval process can be depicted as follows: the user of the information database created 
by a sequence of classifications and insertions decides what kind of component he wants, starting from 
his own informal description of the potentially interesting objects desired. From such an informal mental/verbal 
description, a formalized query is formulated which can be matched against the classification of existing 
components. The process presumes to have a repository filled with objects, with each object having been 
associated with a short abstract description serving to index it. This descrip- tion has to be formalized 
into a classification scheme (anal- ogously as is done for the query) so that queries can be successfully 
matched against it. Many indexing languages have been developed for library and information retrieval 
applications; they define an item's physical location in a library and summarize what the item is about 
by describing its subject or content. What an infor- mation item considered to be 'about' may vary with 
the search situation which is part of the need for carefully engi- neering the classification and retrieval 
language so that it will indeed be useful. Information may be attached to an object manually, from user 
input, or automatically, i.e., extracted from informa- tion such as text found in the object itself (presuming 
the object contains text itself) or documentation accompanying the object (such as a microfilm or an 
executable binary pro- gram). ClassificationlVocabularies I Uncontrolled I I [ Term~ms Classed Unelassed 
extracted extracted from text fromtext I I I I Subject With Without Enumerated Faceted Descriptors headings 
syntax Syntax I I I Thesaurus  Figure 1: Taxonomy of Standard Vocabularies for Classification One can 
characterize such languages as either "languages with controlled vocabulary" or "languages with uncon-trolled 
vocabulary" as shown in the above Figure 1. It must be noted that in the controlled indexing vocabularies, 
it is possible to limit some lexieai aspect of the language (i.e., the terms or words in the language). 
In either controlled or uncontrolled, the rules for combining terms and the logical relationships are 
defined separately from the lexical primi- tives. For controlled vocabularies, the essential idea is 
that by restricting indexers and searchers to a common set of terms, one a-priori encourages searches 
to match appropriate objects. Thus, the "languages with controlled vocabulary" were developed to help 
ensure that terms used by indexers and searchers would be the same. They can be separated into classed 
systems and keyword systems. 571 Classed Systems In classed systems, the terms of the language are controlled 
and structured based on certain classes. All terms in the same class exhibit a similar set of properties 
that semanti- cally justify their being assigned membership in that class. There are two types of classed 
systems, enumerated and faceted.  Enumerated Classification In enumerated classification, a single subject 
matter class label must be assigned to an object. The system assumes that classes must be mutually exclusive, 
and classes cannot be combined to form new classes ex post facto (after the system has been devised) 
though classes may be later sub- divided. This means that classes must have at least a partial hierarchical 
taxonomy. A simple example is the Dewey decimal system which has named classes assigned to deci- mal 
number where the highest level on classification is associated with the integer part and progressively 
smaller decimal fractions correspond to subdivisions of subject matter topics into subtopics. The hierarchical 
structure makes it easy for searchers and indexers to interpret semantic relationships between subject 
matter terms. Users can easily modify their searches to be more specific or more general by moving up 
or down the classification hierarchy. A disadvantage of enumerated methodology is that all pos- sible 
class labels describing a domain are listed in an initial "classification scheme". Hence, the changes 
in such a scheme cannot be made without totally restructuring the system. In software reuse, i.e. applications 
where domains and terminology are constantly changing, this is a serious limitation. Faceted Classification 
Infaceted classification, one or more descriptive properties or attributes of an object may be assigned 
a value from a pre-agreed structured set of basic terms associated with each such attribute. These attributes 
are limited in number and are called facets. Each facet conveys information about a particular descriptive 
aspect or property of a class or set of objects. The facets collectively may be thought of as spanning 
an attribute vector space which will be useful in describing any particular object or in describing groups 
(i.e., classes or sets) of objects according to logical condi- tions expressed over the facets: e.g., 
[Function=PopQueue] A[Arg ument Count> 3 ]. The structure of terms allowed within each facet reflects 
semantic relations between the terms -for instance, one term being more general than another or more 
specific than another or being a synonym or a mutually exclusive alter- native for another term. The 
use of relations results from the need to reference objects which are entered and sought from varying 
views of specificity of the objects "aboutness" (what the object is about). Since facets are similar 
to coordinates or bases for locating and describing objects, they are usually well defined (i.e., non-arbitrary 
and non-ambiguous) and as orthogonal to one another as possible. When searching for a particular object, 
one must specify terms for one or more relevant facets according to an idea of the class of objects that 
one may like to use. Both searchers and indexers can benefit from the ability to describe sets or classes 
of objects using combinations of facet value expressions. This is particularly useful for indexers to 
easily assign composite classes during the indexing process, synthesizing new classes from the pre- existing 
classes that correspond to value assignments of single facets. Because of this flexibility, faceted system 
are also easier to update and modify as individual facets can be changed independently of other facets. 
 Unclassed or Keyword Based Systems Unclassed systems are more amorphous, less structured, than classed 
systems. Usually, they are open to the intro- duction of new concepts as new concepts evolve, whereas 
classed systems must rely on a fixed set of terms pre-defined by the class structure. Unclassed systems 
rely sim- ply on making an assignment of symbols to objects from a fixed set of symbols known as keywords, 
and consequently they are commonly referred to as keyword based systems. Keyword based approaches focus 
on attaching one or more keywords (acceptable natural-language words and phrases) to each object in order 
to describe its properties. Terms are typically arranged alphabetically, However, keyword systems provide 
less information about the relationships between terms. Thus, there can be a mis- matching of the objects 
being sought due to the lack of a common understanding between indexers and searchers regarding the semantics 
between different keywords. So, in order to provide both indexers and searchers information about the 
semantic relationships among terms as an alpha- betically arranged keyword list, a thesaurus system is 
com- monly included. Much like classed systems, two possible approaches exist, corresponding to whether 
the keywords regard subject mat- ter or an open variety of properties.  Subject-headings Indexing languages 
as subject-heading systems does not allow the synthesis of basic terms to express composite concepts 
because all composite terms are created before the system is used by indexers. However, it does not preclude 
multiple keywords in a search that describes different views on the subject matter. Some freedom is 
thus allowed to the searcher, and more descriptive power is given to the indexer. Since there is no assumed 
structure among the keywords, new terms can be added at a later time - for instance when a completely 
new technology is invented. Descriptors Descriptor systems use keywords designed to allow search- ers 
to synthesize terms and composite terms using Boolean operations. In this way, some of the syntactic 
finesse of fac- eted approaches is given to a searcher though not to an indexer. Keywords systems are 
easier to create and to modify than classed systems. In fact they allow the addition of terms to cover 
new concepts without affecting existing terms. Free Text Retrieval For uncontrolled vocabularies, the 
essential idea is that by permitting flexibility both on the classification and retrieval side, the language 
is a-posteriori adaptable to the needs. The lack of restrictions on terminology allows indexers to enter 
a more complete and characteristic description of an object in the library/repository, and searchers 
can have access to a wider set on concepts to match against -espe-cially if the sought concept can usefully 
be conceptualized in more than one way. Thus, the "languages with uncon- trolled vocabulary" were developed 
to help ensure that nat- ural language and "free text" could be used to describe objects with less cognitive 
overhead in classifying and retrieving. In an uncontrolled vocabulary, no restriction is placed on which 
terms can be used to describe an item. Some subcate- gories exist in the methodology, but all searches 
involve the search for textual substrings within a textual description. The source of the text may be 
extracted from a separate description - e.g., an abstract or brief textual characteriza- tion associated 
~ith the item to be retrieved, or the text may be derived directly from the object in the repository/library 
itself. In the later case, the exact syntax or some reflection of it (e.g., maintaining word orders of 
the main terms in a phrase -also somewhat erroneously called "keyword ~in context"). In addition, terms 
may have associated weights to show rel- ative importance -either in queries or indexing entries. These 
weights are usually derived from the frequency dis- tribution of the terms within the objects. A free 
text based solution can easily also allow Boolean combinations of substrings in the textual description 
sought to be matched. One significant advantage of free text is that since terms are unrestricted and 
indexing terms can be made as specific as it is possible to provide maximal descriptive power from a 
human's point of view. On the other hand since such textual descriptions are overly informal and tend 
toward natural language, they are unfortunately ambiguous and prone to mismatching or failure to match 
queries. FORMALIZING THE METHODOLOGIES This section is devoted to formally defining the different classification 
mechanisms, the way the queries should be posed for them and the connected answers. For each mechanism, 
first the classification template is pre- sented, then the query is formalized and the definition of 
the result is stated; finally some examples are presented. Keywords The conceptually simplest mechanisms 
for classifying and retrieving objects is by means of a sequence of keywords. Keywords represent a controlled 
vocabulary, and hence constitute an explicitly defined keyword set: K-{k,.k2, k3 ..... k.). where k 
I ..... k, are individual keywords. For the purpose of classifying objects (denoted variously as O, O 
j, O x ..... herein), a set of one or more keywords is associated to each object so that a descriptor 
of an object can be formally viewed as a non-empty subset of keywords: Dk(O)=ldk~}cK.  Moreover, the 
objects that we are interested in classifying and retrieving based on keywords are collected into a set 
called the Repository: Repository-'- { Oi }. For retrieval, a simple query is defined again (like a descriptor) 
as a non-empty set of keywords: Qk={qkilcK.  Usually, the keywords in a descriptor are considered as 
constituting elements of a disjunctive assertion (connected with an or operator), while the keywords 
in a query are considered as forming a conjunctive condition (a test con- structed with and operators). 
I.e., ok(o)-~vdki . and Qk(O)-=^qki. Subsequently, the result of a query against the Repository is 
Rk(Qk)={O: O Repository, V qk i G Qk, qk i E Dk(o)}. This means that the result is the set of all objects 
with descriptors containing all the keywords of the query. For example, if we were dealing with microcomputers, 
we might have keywords associated with various aspects of the CPU, ROM memory, RAM memory, disk, type 
of display and number of serial I/O ports. A descriptions of typical system might look like: { 1Mb_Ram, 
Color_Display, SCSI_disk, 68000_CPU}. Note that not all relevant keywords have necessarily been specified. 
This is not a requirement of the methodology. -j . A query might look like { 1Mb_Ram, Color Display }. 
 This would certainly include the above descriptor as a result if it were recorded in the repository, 
but it could also produce { 1Mb_Ram, Color Display, IDE_disk, 8080_CPU }. as an additional result. It 
is also possible to have more complex queries that con- tain or operators: QOk= Qk I v Qk2v "" v Qk m 
. where each subquery component Qk i is precisely as betbre, a list of keywords: Qki={qki,l, qki,2 ..... 
qki.ni } The result of such an or-query is the union of all the results obtained from each group of keywords 
in the or-query. For- mally, according to the previously defined conventions, an or-query is a sequence 
of expressions composed of key- words partly connected with and and partly with ors: Q°k=(qkl, I ^ qkl, 
2 ^ ... qkl,nl ) v (qk2, I ^ qkz, 2 ^ ... qk2,n2 ) v ... v (qkm, I A qkm, 2 ^ "- qkm,n m). and the result 
of the query can be written as: R(Q°k)= R(Qkl) U R(Qk2) t.) ... R(Qkm). Returning to the earlier microcomputer 
repository, we see that an or-query might be expressed as { 1Mb_Ram, Color_Display} OR {SCSI_disk, 68000 
CPU }. \which could return as a result { 1Mb_Ram, Color_Display, SCSI disk, 68000_CPU}, {2Mb Ram, B&#38;W_Display, 
SCSI..disk, 68000 CPU}, { 1Mb_Ram, Color_Display, 8000_CPU, Dinosaur }, { IMb_Ram, Color_Display, IDE_disk, 
8008_CPU}. Weighted Keywords Sometimes there are not any objects whose keywords match the query. In such 
cases, it can be interesting to determine the set of objects constituting "good" approxima- tions of 
the desired object. To do so, besides the simple keywords methodology, it is possible to assign weights 
to keywords to determine their relevance to an object descrip- tion or the query, i.e., the level of 
specificity the keywords have in describing an object. For instance, a set of key- words describing the 
pen I have on my desk is {black ink, Parker, thin}; JP is looking for a black ink Pelikan, and we do 
not have one; it is likely that the keyword Parker is less significant than the one black ink, therefore 
my Parker is likely to satisfy JP. Weights can be considered as either properties of keywords or relevance 
to the larger context of the description or query, depending on the object described; we assume the latter 
case, since it is more general. Although weighted keywords per se are not one of the basic methodologies, 
they correspond to one form of the thesaurus which is a standard fixture of the keyword methodology. 
Moreover, weights can also be assigned to each keyword in a query or a description of an object. The 
descriptor of an object can be synthesized as follows: DWk(o)= { (dki, dwti) }. that is, the descriptor 
is a set of couples formed by a key- word and a descriptor weight of such keyword. The query is again 
a non empty set of keywords, now a query weight is associated with each keyword: QWk={(qkj,qwti)}. The 
result of a query is a function of the way a weight con- tributes towards scoring the match of an individual 
query keyword against a descriptor keyword in an object (a matching function ~wk) and of how such results 
are com- bined to determine the overall result for all the keywords in a query considered against all 
the keywords in a particular object (a cumulator function Fwk); formally: RWk(QWk,Fwk,~wk)={O:O~ Repository: 
vWk(OX)>vWk(o)) } where the function v wk expresses distance or similarity between query and repository 
objects and is defined as: vWk(O) _-Fwk({ ~wk(qwti, matchwt(qk i,Dwk(O))) }) where matchwt(qki,DWk(O))= 
dwt i if qk i E DWk(o); NULL otherwise. Nearly always this equation is presented in its simplified form, 
where F wk is Z (summation) and ~wk is rI (multipli- cation of weights); in such cases, the null value 
is 0. Coming back to the earlier example, we can assume that a red Pelikan pen, and two black pens, a 
Parker and a Pilot, are available. We also assume that all the weights in the object descriptors are 
1 and in the query the color weight is 1, while the weight for the brand is 0.5. Using the simpli- fied 
view in executing JP's query for a black Pelikan, the values associated to the three pen objects will 
be 0.5 for the red Pelikan (1"0 for the color + 1"0.5 for the brand), 1 for the black Parker (1"1 for 
the color + 1"0 for the brand) and 1 for the black Pilot (same as the Parker). The result is therefore 
the set formed by the black Parker and the black Pilot since both of these object have the maximum value. 
Using this approach, the answer may not be satisfied by the solution since it is an approximation. Furthermore, 
it is a delicate task to assign weights to keywords, both in the query side and in the description side. 
Note also that this kind of search does not require any semantic inference on the keywords, it is just 
an optimiza- tion problem.  Free Text Using a free text classification mechanism, the descriptor of 
an object is a sequence of natural language sentences infor- mally describing the object. Informally, 
the descriptor of an object is represented as a sequence of words, quite like the descriptor used in 
the keyword based classification. How- 574 ever, different from the keyword case, the whole informal 
description is processed to determine the most relevant words of it, and these can be taken intact as 
subsequences with order dependent structure. In other words, the descrip- tor is not a set of terms, 
(in a set, the order of elements is undefined and no duplicates are allowed) but a list of descriptors, 
which can be further processed to determine the important words and the associated relevance. This can 
be formalized in the following way: Dftinf(O)=[w l,w2,w3,w 4 ..... Wn]. and after the analysis, there 
exists a set of couples (describ-ing word, relevance) as: Dft(o)= { (dwi,rdwi) }. A possible simple strategy 
to assign relevance values is to delete the conjunctions, prepositions, articles, pronouns, and other 
low importance words, and then tabulate the rela- tive occurrence frequency of the remaining words. For 
instance, taking the IEEE definition of a process: a sequence of steps performed for a given purpose, 
initially the descriptor is: Dftinf(Process)=[a, sequence, of, steps, performed, for, a, given, purpose]. 
then the relevance is determined by eliminating the low importance words a, of, for and a, and then weighting 
each remaining word in terms of its relative number of occur- rences. Here no object appears twice, so 
all are equally weighted and the final result is: Dft(Process)={(sequence, 0.2), (steps, 0.2), (performed, 
0.2), (given, 0.2), (result, 0.2)}. More analysis can be applied to the words in order to elimi- nate 
further irrelevant terms (such as given) and possible synonyms. However such details are not analysed 
here since the focus is on the general taxonomy rather than on details. A query is a set of words, quite 
like the case of keywords: Qft={qwj}. The result is the set of objects which maximizes a correla- tion 
value expressed in terms of relevances of the descrip- tors and a function, F ft, composing the relevances 
of each term in the query relative to the corresponding terms in the descriptor of an object from the 
Repository: Rft(Qft,I"ft)={O:O \in Repository, not (~exists O x \in Repos- itory: vft(oX)>vft(o)) } where 
the function v ft is defined as: vft(O)=Fft( { matchwt(q wj,Dft(o)) }), where matchwt(qwj,Dft(o))=rdwi 
if \exists dwi, qwj= dw i \wedge (dwi, rdwi) \in Dft(o), 0 otherwise. For instance, again the function 
F ft can be ~ and other interpretations of the matching function may be possible. This method can be 
further refined adding weights to each word in the query: In this case, the query would be: QWtt={ (qwi, 
rqwi)}. Therefore the result of such a query can be modelled in terms of an integrating function F wft 
and a combining func- tion (I)W ft: Rwft(Q wft, F wft, ~wft)={O: O ~ Repository, not (3 O x Repository: 
vWft(oX)>vWft(o)) } where the function v wft is defined as: vWft(O)-FWft({~wft(rqwi, r(qw i, Dft(O)))}). 
However the focus of this paper is more on the general tax- onomy rather than on the details of how the 
different meth- ods can be combined in all the possible ways; so we shall not dwell on such details much 
further. Simple Faceted With a simple faceted approach, the descriptor of an object is an attribute 
vector. The value of each component attribute of the vector has to be one from a set of its prede- fined 
values which are organized as a forest (a partial order- ing corresponding to a group of trees). The 
forest defines a general to specific ordering of the values. Each arc in the tree is associated with 
a value ranking from 0 to I and defining the proximity of the derivation of the general to the specific 
relation between values: a value of 1 means equivalent whereas 0 means no arc (the values are not related 
at all nor are they mutually exclusive). A query is a template or partially instantiated vector of values. 
In the best case, an exact match exists; then the set of the exact matches is the result of the query. 
Otherwise, the result is the set of the best matches, defined in terms of the above- mentioned values 
of the arcs. More formally, the attribute space is depicted as: AS={vkl} X {vk2} X -.. X {vknf}, where 
 { vki } =domain(Faceti), and so the descriptor of an object can be written as a vector: Dsf(o)=(dvJ 
t, dvJ 2 ..... dvJnf) E AS, such that dvJj ~ {vki}, and the query correspondingly is a vector as well: 
QSf=(qv 1, qv 2 ..... qVnf) E AS, such that qvj G {vki}. The result of the query can be formally expressed 
as the set of the objects that are closest to the desired object, closest in terms of the values of the 
faces of their descriptors and in terms of two functions. One function, cb sf, determines the proximity 
of the value of each single face of all the objects in the repository to the corresponding facet value 
of the desired query object. The other function, ~f, integrates the proximities of the different faces 
to determine the global proximity of each object in the repository to the desired object. RSf(Q sf, F 
sf, ~sf)={O:O ~ Repository, not (B O x ~ Reposi-  tory: pSf(Ox, QSf)>pSf(O,QSf))}. 575 where: QWSf=((qvi, 
rqv,), (qvJ2, rq~2) ..... (qVnf, rqvnf)). pSf(Ox' Qsf)=I"sf({ ~Sf(qvi, dvOi)}). For computing the result, 
the weights must be taken into In the forest of values for each facet, a value or possibly two values 
can be associated to each arc specifying the proximity of its two nodes. Allowing two values permits 
modelling the unusual (but sometimes useful) case in which the upward value (i.e., from child to parent) 
is not equal to the downward value (i.e. from parent to child). Such values are used by ¢b sf to determine 
the proximity of the nodes corresponding to its two arguments -the facet value terms from a query and 
a repository object respec- tively. If they are neighbours, then the solution is straight- forward since 
~sf takes the value of the connecting arc; otherwise, ¢b sf searches for the shortest path between the 
two nodes, taking into account the general-to-specific tax- onomy, i.e., computing the distance between 
the query- term node and the first ancestor common to both the query- term node and the repository-object-term 
node. The function F sf is used to integrate all the values coming from the different faces to determine 
the global proximity of two objects -the hypothetical query object and any repository object. Linear 
Simple Faceted The usual choice is to linearize the functions used to deter- mine the proximity of two 
objects: to compute ~sf, all the values of the traversed links are multiplied. The values associated 
with the links span from 0 (no link) to 1 (com- plete equivalence). To compute F sf, the usual ~. (summa- 
tion) over the values coming from the ~Sfs is taken. As a side reference note that this can be formalized 
further using the common algebra vector notation: the face values can b'e viewed as a vector dv_.., j 
as well as the desired object ~y.. Then pSf(O, Qsf) in the linear case can be represented as the usual 
scalar product: ~J .lsf dv. where the component by component multiplication is substituted by the ~sf. 
In this case the result of a query can be expressed elegantly as: RlSf(d._x)={O:O ~ Repository, not (30 
x ~ Reposi- tory: {fix Ox. dv>gy. O. dv) 1. Adding Weights to Faces Additionally, weights can be added 
to each face, both in the descriptor and in the query, to represent respectively the relevance of such 
face for the object and the interest of the user in such aspect of the object. In such a case, the descrip- 
tor of an object is represented as: Dwsf(o)=((dVJl, rJdvl), (dv 2, rJdv 2) ..... (dvnf, rJdvnf)). and 
the query as: account. The rJdv i in the function ¢b wsf, and the rJqvi in the F wsf, lead to the following 
result function: Rwsf(Q wsf, F wsf, ¢bwsf)={O:O E Repository, not (30 x Repository: pWSf(Ox, QWSf)>pWsf(o, 
Qwsf))} where: pWSf(O, QWSf)=Fwsf({~wsf(qvi, dv°i, rJdvi, rqvi)}). And this can be linearized as: RWSf(Qwsf)={O:O 
E Repository, not (30 x e Repository: pWSf(Ox' Qwsf)>pWSf(O' Qwsf)) }. where: pWSf(O, QWSf)=Z(~wsf(qvi, 
dr°i) x r°dvi) x r°qvi . and ¢b wsf is the usual maximum proximity function, the same as ¢b sf.  Faceted 
with Semantic Connections In the case of a faceted description with semantic connec- tions, common subtrees 
between elements of the same tree or of different trees can be present. Therefore, the domain of each 
face is organized as a directed graph. The formulae are formally still the same as in the case of the 
simple fac- eted classification mechanism (see Section Simple Fac- eted). Here below the corresponding 
forms are presented for the descriptor of an object, for the query and for the result: Dfsc(o)=( dvj 
1, dvJ2 ..... dvJnf) Qfse=(qv 1, qv 2 ..... qVnf) RfSC(Q st, F fsc, ¢bfse)={O:O ¢ Repository, not (30 
x Repository: pfSC(OX, Qsf)>pfsc(o, Qfs¢))} pfSC(O ' Qfsc)=l" fse(~fSC(qvi ' dvOi)) The difference is 
in the complexity analysis as will be shown in Section ComPlexity Analysis of the Methodolo- gies. Linear 
and Weighted Faceted with Semantic Connec- tions Furthermore, there can be a linearization of the classifica- 
tion and of the search leading to the following definitions: Rifsc(dv)={O:O ~ Repository, not (30 x ¢ 
Repository: ~y.Ox .Ifs¢ dv>qv O .Ifsc dY.)} The difference between the Jfsc function we have here and 
the previous .Isf is only in the computational overhead required by such a function. Section Complexity 
Analysis of the Methodologies is devoted to analysing such proper- ties. Obviously, weights can be added 
to each face and to each component of the query. In such a case we have the follow- 576 ing representation, 
which resembles very much'that of the simple faceted representation (see Adding Weights to Faces); the 
only difference is in the complexity: Dwfsc(o)= ((dVJl, rJdvl), (dvJ 2, rJdv 2) ..... (dvJnf, rJdvnf)) 
Qwfsc=((qv I, rqvl), (qv2, rqv2) ..... (qVnf, rqvnf)) RWfSC(Q wfsc, F wfsc, ~wfsc)={O:O ~ Repository, 
not (30 x Repository: pWfSCOX, Qwfsc)>pWfSC(O, Qwfsc))} pWfSC(O,QWfSC)=l"wfsc( { ~wfsc(qv i, dr°i, rqv 
i, rOdvi) }). And this can be linearized as: RlWfSC(QlWSf)={O:O e Repository, not (30 x ~ Repository: 
plwfsc(ox' Qwsf)>plwfsc(o' Qwfsc)) } plwfsc(o' QlWfsc)=E(~lwfsC(qvi, dv°i) × rOdvi) × rqvi). where ~lwfsc 
is the usual maximum proximity function, the same as ~wsf. As for the other cases of weighted represen- 
iations, the details of these methodologies are not explained further in this paper, since they are not 
regarded as relevant for the present purposes. COMPLEXITY ANALYSIS OF THE METHODOLO- GIES Here we make 
a summary of the computational complexity of the methodologies discussed in the previous sections. It 
is assumed that in general the repository is unsorted and that it has no more structure than a set. This 
reduces every case of a search to a sequential search through the reposi- tory. For the trees and graphs, 
we similarly assume that the structure is minimal - i.e., that only a set of nodes and a set of connective 
arcs are known, so to locate an item requires a sequential search. The parameters considered are the 
following: n o, the number objects in the repository nq, the maximum cardinality of a query n d, the 
maximum cardinality of a descriptor nfr the maximum of the depths of the forests of the val- ues of the 
faces ng, the maximum number of nodes in the graphs of the values of the faces nf, the number of faces 
Table 1 at the end of this section gives the worst case com- putational overhead for each method previously 
discussed, In the following, the entries summarized in the table are to be verified. In all cases, we 
assume an unordered reposi- tory, so as to fairly compare the results of analysing the dif- ferent methodologies. 
Keywords. In the worst case, each keyword in the query has to be checked against each keyword of the 
object descriptor, leading to a complexity equal to 0(n o x nq x na). Weighted keywords. In this case, 
the Situation is the same as in the case of plain keywords, apart from the fact that the functions FWk(nq) 
and ~wk have to be taken into account. FWk(nq) is computed at most n o times and its complexity depends 
upon the maximum cardinality of the descriptors, nd' ~wk is computed at most n o X nq times. The final 
for- mula is therefore max(0(n o × nq x rid), nq X 0(~wk), 0(FWk(nq))). Linear weighted keywords. When 
Fwk(nq) is Z, its associ- ated complexity becomes 0(n o x nq X nd), and when cb wk is the product, then 
its complexity is 0(n o x nq). Substituting the Fwk(nq) and owk complexities into the above derived formula 
for weighted facets leads to a global complexity equal to 0(n o x nq × no). Note that this result is 
the same as the one obtained in the plain keywords mechanism; therefore the difference between the two 
is only in a constant factor. This is fairly obvious since the core of both methods requires matching 
elements from the domains of the keywords, the ones from the domains of objects found in the repositories 
against those from the query. The constant factor takes into account that the linear weighted case is 
more onerous to handle. Free text and linear free text. These methods are analo- gous to keywords apart 
from having an infinite vocabulary. In particular free text resembles the weighted keywords, where the 
relevance of each word of the description is the weight of the keyword. Here there is no need for a 
func-tion since combined effect of multiple query word matches is handled by F ft. The complexity is 
therefore max(0(n o x n r X nd), 0(Fft(nq))). Since F ft is ]~ for the linear case, 0(Fft(nq))<<0(no 
x n r X n d) and linear free text has conse- quently a complexity equal to 0(n o × n r x rid). Simple 
faceted. In the simple faceted mechanism, for each object in the repository the proximity of its facet 
values to the ones of the desired component has to be computed, which amounts to considering each face 
in turn. This opera- tion depends on finding the distance between two nodes in the termspace of a facet. 
Such distance is upper-bounded by the maximum depth of the corresponding tree of values which is upper-bounded 
by the maximum of such depths among all trees; therefore across all the faces, this collec- tively leads 
to a complexity of O(nf x O(q~Sf(nfr))). Finally, all the values coming from each face have to be combined 
by F sf, leading to a net result equal to O(n o × max (0(l'~f(nf)), 0(nf X 0(~)Sf(nfr)))) ). Linear 
simple faceted. In this case, the complexity of ~lsf is 0(nfr) since, intuitively, at most each of the 
two elements to be matched has to go to the root of the tree, and the com- plexity of F Isf is straight-forwardly 
nf, since F Isf is the sum- mation. Thus, substituting the ¢1) Isf and l r'lsf complexities into the 
formula derived for simple facets, the result is 0(n o × nf X nfr ). Faceted with Semantic Connections. 
The case of facets with semantic cohnections is entirely analogous to the sim- ple faceted case, except 
for the fact that instead of nfr, the global cardinality of the graph, ng, has to be considered. Consequently, 
the complexity formula is 0(n o x max(0(FfSC(nf)), 0(nf × 0(~fSC(ng))))). Linear Faceted with Semantic 
Connections. This case is again analogous to the linear simple faceted case, except for the fact that 
the minimum distance between two nodes of a graph (The above assumes the hypothesis of non sparse graphs; 
otherwise if n a is the number of arcs in the graph, the complexity is 0(na2).) is 0(ng2). The resulting 
complex- ity is 0(n o x nf x ng2). Table 1" Summary of the Computational Complexity of Different Classification 
Methodologies name complexity keywords 0(n o x nq x nd) weighted max(0(noXnqXnd),nqXO(q~wk),o(FWk(nq))) 
keywords linear weighted O(n o x nq x n d) keywords free text max(0(n o x n r X ha), 0(Fft(nq))) linear 
free text 0(n o x n r x n d) simple faceted 0(n o x max (0(l"Sf(nf)), 0(nfx0(~Sf(nft))))) linear simple 
0(n o x nf x nfr ) faceted faceted with 0(n o x max(0(FfSC(nf)), 0(nx0(~fSC(ng))))) semantic con- nections 
linear faceted O(n o x nf x ng 2) with semantic connections  MINIMIZING THE SEARCH OVERHEAD There are 
definite savings to be gained in implementation which exploit additional structure imposed on the reposi- 
tory. For instance, ~lfs¢ may be implemented as a large lookup table of node to node distances in which 
case its complexity is easily bounded by nfr 3. Similarly, minimally perfect hash functions or other 
rapid access techniques could be employed, but due to cost and other practical con- siderations associated 
with building the table, it can only be constructed periodically. This means that some degree of incremental 
sequential search is always necessary. The goal of these methodologies is to minimize search time and 
maximize the ease of use as well as the likelihood of identifying the right component. The most common 
approaches are in favour of weighted keyword based sys- tems which are simple enough to be understood 
by novices, result in reasonably fast search and are quite effective in locating the desired artifacts 
within only a few interactions with the user [7], [6]. Another approach may be to first apply a simplified 
method retrieving a more general match than required. This would permit more involved retrieval to be 
applied selectively on another wave of processing that would restrict the consider- ation only to the 
objects retrieved on the first pass. In case of remote repositories, the situation is radically dif- 
ferent since the network traffic has to be taken into account. Any network traffic overload is likely 
to largely over-shadow the search overhead. This implies that a slower search mechanism is acceptable 
provided that it leads closer to the solution limiting as much as possible the inter- actions with the 
systems on the WAN. Problems with distributed databases on wide area networks mean that it is important 
to work first in terms of duplicated distributed indexes rather than trying localize the complete repository. 
However, considerations of such concerns are beyond the scope of the present paper. Thus, the strategy 
proposed from this consideration is that in the interest of keeping down the computational complex- ity 
one should allow a disjunctive hybridization of the sim- ple approaches rather than extending these basic 
approaches to allow more sophistication (e.g., semantic connections on facets instead of remaining restricted 
to trees as the structure for facet values). CONCLUSIONS The paper began by considering the needs of 
a software classification (indexing) and retrieval (search) process with software reuse in mind. There 
is the informal aspect of the need for a language and understanding of semantics com- mon to both sides 
of the process, based on the context in which the objects in a repository will be reused. To some extent, 
this can not be solved solely with a formalism. There needs to be agreement on the part of indexers and 
searchers about the "architectural style" used by a particular community of software producers. Employing 
indexers unfamiliar with the style of structuring programs used in that community will virtually guarantee 
lack of reusability based on the classification process. For example, consider the two cases depicted 
in Figure 2 below: in one case the assumption is that a piece will be monolithic and in the other, the 
searcher structures the search with the hope of finding subcomponents satisfying separately formulated 
requirements. In the first case, there may be parts of the total requirements satisfying a series of 
partial require- ments and their union would satisfy the total requirements collectively. Thus, the individual 
searcher must have a strategy that was foreseen by the indexer (or alternatively, the searcher must know 
the style used by the indexer) - oth- erwise, a query stands little chance of being stated in a way that 
is compatible with what has been inserted in the repository. \ / t   It@l f Requirements are viewed 
Requirements are organized holisticly as an indivisible according to the partial unit - no subsystems 
were requirements of subsystems. defined at the time of the System requirements are classification of 
the original the collection of subsystem piece of software into the requirements. system. Figure 2: Different 
Architectural Styles for Organizing the Same Functionality in a Software Product This paper presented 
the basic library science approaches to indexing and classification and their implications for searchers 
of the repository. A formal view of the methodol- ogies was put forward with the intent of comparing 
compu- tational complexity of different methodologies. In considering the special needs for extensions 
to the meth- odologies to support software reuse, a strategy based upon a hybridization of the simplest 
methodologies was pro- posed rather than using or developing more complex meth- odologies. The authors 
conclude that some extensions oriented towards the inevitable informal human side of the process may 
be necessary. This is the reason a domain analysis of the environment in which such methodologies must 
operate should be tightly coupled to the design of the termspace for keywords and facets. Any extensions 
should be accounted for by engineering the proper elements into a formal frame- work which will support 
software reuse and reusability, with the goal of avoiding undue computational complexity and recognition 
of the fact that ambiguity and mismatch of semantics prevent even the most sophisticated formalism from 
being effectively used. This is another argument in favour of simplicity rather than sophistication in 
classifica- tion and indexing schemes. If the scheme is well under- stood' and computationally efficient, 
it will be successful; otherwise it will not. REFERENCES [1] D. Barstow, An Experiment in Knowledge-based 
Auto- matic Programming, Artificial Intelligence, Vol. 12, pp. 73-119, North-Holland, Amsterdam, 1979. 
[2] B. Bech, Declarative Programming for Component Interconnection, Proceedings o/the WISR '92 5th Annual 
Workshop on Software Reuse, Latour, L., Philbrick, S. and Stevens, M., eds., Palo Alto, CA, October 20-29, 
1992. [3] B. Cheng and J. Jeng, Formal Methods Applied to Reuse,Proceedings o/the WISR '92 5th Annual 
Work- shop on Software Reuse, Latour, L., Philbrick, S. and Stevens, M., eds., Palo Alto, CA, October 
20-29, 1992. [4] N. Dershowitz, Program Abstraction and Instantiation, A CM Transactions on Programming 
Languages and Systems, Vol. 7, No. 3, July 1985. [5] A.J. Dix, Formal Methods for Interactive Systems, 
Aca-demic Press, 1992. [6] W.B, Frakes and R. Baeza-Yates, Eds,, Information Retrieval: Data Structures 
an Algorithms. Prentice Hail, Englewood Cliffs, N.L, 1992. [7] W.B. Frakes and T.P. Pole, An Empirical 
Study of Rep- resentation Methods for Reusable Software Compo- nents, IEEE Transaction on SoftwareEngineering, 
Vol. 20, No. 8, pp. 617-630, August 1994. [8] W.B. Frakes and P.B. Gandel, Representing Reusable Software, 
Information and Software Technology, Vol. 32, No. 10, pages 653-664, December 1990. [9] S. Komiya, Automatic 
Programming by Composing Program Components and Its Realization Method, Future Generation Computer Systems, 
Vol, 5, North- Holland, 1989. [ 10]C.W. Krueger, Software Reuse, ACM Surveys, June 1992. [11 ]J..Rockmore, 
Knowledge-based Software Turns Speci- fications into Efficient Programs, Electronic Design, July, 1985. 
[ 12JR. Steigerwald, Reusable Component Retrieval with Formal Specifications, Proceedings o/the WISR 
'92 5th Annual Workshop on Software Reuse, Latour, L., Philbrick, S. and Stevens, M., eds., Palo Alto, 
CA, October 20-29, 1992, [13]G. Yu, Automatic Retrieval of Formally Specified Real-Time Software Components, 
Proceedings of the WISR '92 5th Annual Workshop on Software Reuse, Latour, L., Phiibrick, S. and Stevens, 
M., eds., Palo Alto, CA, October 20-29, 1992. 
			