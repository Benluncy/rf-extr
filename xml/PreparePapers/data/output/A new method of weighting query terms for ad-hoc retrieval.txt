
 A New Method of Weighting Query Terms for Ad-Hoc Retrieval K.L. Kwok email: kklqc@cunyvm.ctmy .edu Computer 
Science Dept., Queens College, City University of New York, Flushing, NY 11367, USA. Abstract Ad-hoc 
retrieval relies on the evidence from a user s query to provide a sufficient variety of terms as well 
as different term frequencies for differentiating term importance. Short queries lack both typea of information. 
A new method of automatically weighting query terms for ad-hoc retrieval is introduced that works for 
short queries. It is based on the term usage statistics in a collection and no training is required. 
Expximents with both the TREC2 and TREC4 ad-hoc queries show that this weighting scheme can provide significantly 
better results at the initial retrievat stage. At the expanded query stage, results vary from equal to 
significantly better than those relying on the originst query weights. In particular, this automatic 
method provides similar improvements to extra short queries of two to four content terms only. 1. Introduction 
In ad-hoc retrieval, an IR system normally relies on a user s query as a clue to select documents and 
rank them for output to satisfy the user s needs. A verbose, descriptive query can provide two useful 
functions for retrieval: a variety of terms that me related to the topic of need, and an indication of 
term importanm when some terms are used repetitively. The former helps in recall while the latter helps 
in precision. Short, terse queries provide neither. Statistical IR systems convert a need description 
automatically into a query consisting of a list of content terms and weight them based on term usage, 
and therefore the longer the need description, the better should be the retrieval effectiveness in general. 
This can be seen in the latest two TREC (Text REtrieval Conference) experiments, where the ad-hoc retrieval 
environment (collections, queries, retrieval techniques) may be regarded as approximately similar except 
for query sizes. A tabulation of the distribution of Permission to make dlgitalJhard copy of all part 
of this work for per­sonal or classroom use is granted without fee provided that copies are not made 
or distributed for profit or commercial advantage, the copy­right notice, the title of the publication 
and its date appear, and notice is given that copying is by permission of ACM, Inc. To copy otherwi­se, 
to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or 
fee. SIGIR 96, Zurich> Switzerkurd@1996 ACM 0-89791-792­8/96/08.$3.50 queries according to the number 
of unique terms (after stopword removst) in the TREC3 (query #151-200) and TREC4 (query #202-250) sets 
is shown in Fig.1. In TREC3 [Harrn95] where the 50 queries were two paragraphs long with average size 
of about 19 unique terms, the best automatic retrievals submitted by participating sites as measured 
by the non-interpolated precision averaged over all queries were in the range of 0.3-0.4. In TREC4 [Harm96], 
the 49 queries were deliberately shortened to a few words to about a couple of sentences with average 
size of about 6 terms, and the corresponding best average precision achieved were about 0.25-0.3, a drop 
of over 25Y0. Thus, in statistical IR systems it is advisable for users to pose longer, more verbose 
descriptions of their needs. In practice, however, users tend to submit queries of a few words only [LuKe95]. 
For example, one has this experience when issuing searches on the intemet. One wants to minimize the 
chore of text entry as well as the mental work of identifying more related terms, and hopes that the 
retieval system would be sufficiently intelligent to know what one wants. Statistical IR systems therefore 
can benefit if automatic methods can be devised to help lengthen a short query with topically-related 
terms or to effectively weight the terms for importance. One single word in generat is ambiguous in meaning 
and should not be used as a query (except in classical Boolean systems with manual, controlled keyword 
indexing). Two words like operating room , operating system or operating characteristic as queries may 
have clear meaning and intention to a human being, it is however difficult for statistical IR systems, 
unless the system happens to have the two words accounted as an ordered phrase. This is because IR systems 
have to work with large (multi-gigabyte collection size) and diverse (no domain constraint for queries 
and documents) environments, and it is quite difficult to build intelligence to deduce a user s intention 
from ANY two words. Without involving too much discussion of what constitutes short queries, we will 
assume that the TREC4 queries are representative of them. In this paper, we discuss and propose methods 
on how one can handle these short queries in an automatic fashion for our IR system called PIRCS. We 
essentially repeat the TREC4 ad-hoc experiments using automatic query weighting and compare with previous 
results. The environment of the TREC4 ad-hoc experiment consists of employing 49 given TREC topics to 
form queries and do retrieval against the Tipster Disk 2 &#38; 3 text data. These disks contain articles 
from Wall Street Journal (1990-2), Associated Press (1998,90) and San Jose Mercury News (1991 ), Ziff-Davis 
Computer Magazine, Federal Register and U.S. Paterm totslling 2 GB in over 1/2 million documents. In 
our system we split long documents into units of about 550 words in length ending on a paragraph tmundary, 
thus resulting in 832,780 subdoctsments. From these, a master lexicon with usage frequency was derived 
numbering 643,755 terms of which 55,599 are two-word adjacency phrases. No. of TREC3 TREC4 Short.TREC2 
Unique No. of No. of No. of Terms Queries Queries Queries 2 21 3 52 4 93 5 86 6 94 7 27 8 115 9 365 
10 137 115 4 12 323 13 22 14 3 15 1 161 1 17 4 18 4 19 4 20-39 20 Avg. Size 19.08 6.22 8.00 Fig.1: Distribution 
of Query Sizes for TREC3, TREC4 and Short.TREC2 (Description Section only) Topics 2. Ad-hoc Retrieval 
in TREC4 2.1 The PIRCS Retrieval System PIRCS, acronym for Probabilistic Indexing and Retrieval -Components 
-System, is our text processing and retrieval system that supports gigabyte scale ad-hoc retrieval and 
routing of full-text documents using free text descriptions of users needs. It is based on probabilistic 
models of indexing and retrieval, and combmed with network learning and activation spreading. Past TREC 
experiments have shown that this system can give exemplary and highly competitive results. Factors contributing 
to PIRCS effectiveness include: 1) improvemen~ in item representation use of two-word phrases in addition 
to single stemmed words, and segmentation of long documents into subdocument units help diminish ambiguity 
for retrieval and learning; 2) sophistication in term weighting: many variables such as withii-itcm term 
frequencies, collection and inverse collection term frequencies, item lengths, etc. are used to provide 
dkcriminating clues for retrieval; 3) adaptation with learning network: historic user relevance judgment 
information in the case of routing, and best irthially retrieved documents in the case of ad-hoc retrieval, 
can be used as tratilng samples in our network to provide better term weighting and query representation 
via query expansion; and 4) combination of multiple retrieval lists: evidence of relevance from different 
retrieval strategies can be combined to provide more effective final ranked retrieval results. PIRCS 
has been described in our previous TREC papers [KwPK93, KwGr94, KwGL95] as well as in [Kwok95,90]. Basically, 
given a query K and documents d,, we use two retrieval algorithms that give the following document-focused 
and query-focused rankiig RSVS (retrieval status value) respectively RSVti~ = z, W,k * Wb RSVu~=z~ W&#38; 
* Wh. Wh is the weight of term ~ used in ~ and is content oriented, and w,~ is the weight assigned to 
k based on its collection properties as well as its usage in d,, and is more discrimination oriented. 
(Simh for Wh and w~). They are implemented as weighted edges connecting documents and queries to terms 
in a bi-duectional network. The sum is over all terms common to d, and ~. Results of these two algorithms 
are then combined to return a final ranking RSV for document di as follows: RSV, = a* RSVtid + (l-a)* 
RSVu~ where tkac 1 is an adjustable parameter usually set to between 0.5 and 0.9. By viewing a document 
as not monolithic but composed of conceptual components each representable as a term and hence removing 
the binary assumption of the normal probabilistic model, Wk is given by S(TermFreqJLength,), and correspondingly 
for Wti. S(.) is a signal function to suppress outlying values and Length, is the length of ~ including 
repeating te~s [KWOkW$W. ik (~d ~m=pondiwly for WJ consists of a trainable factor and an Inverse Collection 
Term Frequency factor ICTF = log [(TotalTcrms-ColFrcqJ/ColFreqJ. ICTF is like the Inverse Document Frequency 
except that it also accounts for term frequencies, not just bbwy . The ~ainable factor at the initial 
stage when queries and documents perform hard self-learning would be given by log [TermFreqJ(bmgtQ-TermFreqJ]. 
Self-learning is the idea that every document or query is regarded as relevant to itself, and rdlows 
our PIRCS system to perform learning even at the initial stage when no judged relevant information is 
available for training. In our system we perform soft self-learning and W&#38; has value lying between 
I~F and ICTF + the hard self­learning vahse. When more documents known relevant (or assumed relevant) 
to ~ are available, this trainable factor can attain more accurate value and provide better retrieval 
results. Moreover, based on the training documents, edges connecting ~ to new terms may be added resulting 
in an expanded query. This enables retrieval of documents that do not share common terms with the originrd 
query. These processes of training, query expansion and retrieval are performed by sophisticated algorithms 
and are implemented as activation spreading in the network. 2.2 The Two-Stage Ad-Hoc Retrieval Process 
In order to obtain good ad-hoc results on average, it is now standard practice to do two retrievals in 
succession [B SAS95, RWJHG95, KwGL95] for each query. The fust stage is the initial retrieval where the 
ad-hoc query is used dkectly. The n best-ranked documents from this retrieval are then regarded as relevant 
without user judgment, and employed for feedback to train the initial query term weight and to do query 
expansion. This expanded query retrieval then provides the final results. Thk second stage retrieval 
in general gives substantially better results than the initial if the initial retrieval is reasonable 
and has some relevant documents withkt the best n, but can also give degraded results if the initial 
retrieval is bad. This strategy works not only because the initial query terms got trained to more effective 
weights for dte second stage, but also because the expansion process can bring in terms that iwe highly 
associated with the original terms. Thus, if dogs and cats are the terms in the ititial query, the system 
may expand it with terms like pets , leash , feline , etc. and the process works like a dynamic thesaurus 
[EvLe94]. This s~ategy is important for short queries and helps to provide more clues when the user does 
not. In order for this two-stage process to work properly, the quality of the irthial retrieval is crucial. 
TMs initial quality can be improved by having more variety of terms and better indication of term importance 
in the original query. This paper deals with the second aspect. 2.3 Weighting of Query Terms for Initial 
Retrieval It is well-known that weighting of terms provides improved retrieval because it differentiates 
useful terms from less useful ones. The most popular and effective term weighting is the tPidf scheme 
[SSBU88], where tf denotes the term frequency property that is local and content-oriented to an item 
(query or document) [Luhn58], while idf denotes the factor that is global and discrimination-oriented 
for the collection [Spar72]. While the global factor is easy to estimate in large collections, the lecal 
property depends on the size of an item. When a short query is submitted, practically all terms are used 
once only and it offers no help for differentiating which term(s) is more important in representing the 
query s content. In TREC4 the Cornell group [BSMS96] approach the problem by introducing a document 
length normalization weighting, and obtained the best automatic ad-hoc average non-interpolated precision 
value of 0.294. They study the behavior of relevant documents in general and find that long documents 
have a significantly higher probability of being relevant to queries than short ones. Based on this, 
they use a pivot weighting formula that adjusts for this according to document as well as query lengths. 
They also dkpense of their usual cosine formula. The result speaks for itself, but it should be stressed 
that a collection need not behave this way, since relevancy is a property that should be based on content 
--a document can be relevant to a need with just one sentence containing the needed information irrespective 
of document length. The length normalizing process also involves an extra step of haining from old results 
and it assumes that the new collection has characteristics sidar to the trainiig data. One of our approaches 
to this same problem [KwGr96] is by manually repeating some terms that we think are important for a query 
s content. For example, query #203: What is the economic impact of recycling tires? is reformulated as 
What is the economic impact of recycling tires? recycling. tires. We in effect double weight the judged 
useful terms while makiig sure that no new terms or phrases are ad&#38;d. We also did not double weight 
phrases. Because the queries are shor~ the choice of terms involves very little intellectual effort. 
We also make minimal adjustments ordy and our policy is to double-weight three or four terms at most 
per query, quite often less. It takes less than % minute per query and is not burdensome to the user. 
The effects of this adjustment sxe improvements of about 10% in initird retrieval and 9V0 in expanded 
query retrieval compared to using the original queries without changes. This can be seen in Fig.4 where 
the evaluation results are tabulated. Under the label Original are two columns showing the initial stage 
and expanded-query stage retrievals using the raw query terms directly, and under the Man.Wt label are 
the corresponding results employing thk manual weighting. Thus, we have demonstrated that weighting of 
terms is useful for short queries. The question is: can we devise methods to weight these terms with 
the right proportions automatically so that retrieval improvements can be attained? Since the TREC4 conference, 
we have made further progress in this problem and have discovered just such a method for these short 
queries and is presented in the next section. This weighting is done only on the query side of our network, 
while the document side of the network remains unchanged. 3. New Automatic Weighting Scheme for Short 
Queries I/log max(cutoff,DocFreJ: Like other weighting methods, our goal is to deduce the importance 
of terms based on their frequency of usage. If it succeeds, it would work on average only; specific instances 
may get degraded results. Many parameters need also to be set. What follows are the arguments for developing 
this new weighting scheme a) Average Within-Document Term Frequency --avti In most statistical IR systems 
including ours, queries are represented as content term lists like documents and treated similarly. Since 
a short query is normally flat without frequency information to help differentiate term inprtance, this 
leads us to look at the whole collection and observe how terms behave within documents. The quantity 
of interest is the average k-th term frequency in a document defined ~ avtf~ = ColFreJDocFr~. Here, 
ColFr~ is the collection frequency counting repeat usage, and DocFr~ is the document frequency of term 
k counting only presence. This average term frequency says that whenever term k is used, it will be employed 
avtf~ times m a document on average for this collection. In the absence of information from the user, 
this seems a reasonable value to employ for differentiating term importance in short queries. It means 
that if the user were more diligent in spelling out hk/her needs in a verbose fashion, the terms used 
for describing this need might probably be weighted in the same way as these average frequencies that 
are observed in a general collection. In Fig.2 we have tabulated the highest artd lowest avtf~ values 
of some TREC4 query terms together with their document frequencies. As expected, 2-word phrases and some 
rare words are seldom used repetitively and they have avtf close to 1. Yet they could be important for 
content description. Examples are terms like automobilindustr, statefinanci, nuclearprolifer, terrestri, 
etc. On the other hand, no high frequency terms (say those with DocFr~ >= 35000) have avtf vahtes below 
1.2, implying that words that are used frequently across documents tend also to be used repetitively 
within documents. Qualitatively, about 20% of the query terms with avtf between 1.2 to 1.5, 25% of those 
between 1.5 to 2.0, and about 15% with avtf > 2.0 are high frequency. Examples of the last category are 
words like commiss, fund regul, materi, etc.; they may not be so important for content description. Using 
the avtf values alone therefore may not constitute effective weighting. What one would like is to attach 
high weights to terms that have high avtf values and also occur moderately seross documents. Thus, we 
need to make adjustments to this basic avtf vahte. b) Inverse log DocFr~ function with cut-off --The 
goal stated in the last paragraph could be achieved by employing the usual idf factor (log(N/DocFreJ) 
to modulate the avtf value so that high frequency terms will get de-emphasized and vice versa. However, 
idf has the adverse effect of over­emphasizing the low frequency terms. This leads us to try the inverse 
log DocFr~ function with cut-off that has been used successfully in our TREC3-4 routing experiments to 
remedy the situation. The usual idf factor varies according to a difference (log N -log DocFreJ with 
respect to log DocFr~, while our function behaves inversely (l/log DocFreJ with a cut-off. On the other 
hand, we also do not want to over-emphasize low DocFr~ terms. Finally, allowing the contention between 
the avtf~ and the inverse log msx factors to be varied, we arrive at the following weight, function that 
is attached to each query tcrnx w~ = (avtfJ /log max~cutoff,DocFreJ/norm where norm is a normalizing 
value making z~ w~ = 1. a takes a value between 1 and 2: when a is increased, the influence of avtf (i.e. 
repetitive usage within a document) will be larger. Cutoff is chosen as 2000 through experimentation. 
We will call this an average term frequency weight modulated by an inverse document frequency factor 
and denote it as avtPidf. A table of some TREC4 query terms with the highest and lowest values of this 
new weight at a=l.5 is given in Fig.3 to give readers a feel of its quality. Compared with Fig.2, the 
effect is that low frequency terms are promoted on the lis~ while high frequency terms are demoted, trying 
to approximate the goal that most content terms have high avt.Pidf vtdues and vice versa. c. Peaking 
Adjustments: Our manual method discussed in Section 2.3 double-weights certain judged useful terms only, 
with no intermediate weight variation. Yet it performs quite well and it indicates that this emphasizing 
or peaking operation is effective. We attempt to do so automatically as a second order adjustment to 
our avdidf weight. This requires criteria to order and select which terms and how many to peak, and by 
how much. Our policy is to choose only a few terms to emphasize &#38;cause the chance of makiig a sequence 
of right decisions drops rapidly. In the absence of manual judgment of which terms to selec~ we use the 
avtPidf weight values directly for term selection and peak at most the highest three terms of a query 
according to its size. The details are described in the next paragraph. In making selections, we divide 
terms arbitrarily into three frequency ranges as in [SaYY75]: low (DocFr~ < 2000), high (DocFr~ >= 35000), 
and medium (those in between). We also count the size of a query in two ways: L , which includes all 
unique content terms in a query (after stopwrod removal and stemming), and L which excludes high frequency 
ones. Since Unnormalized Term DocFre avtf Term DocFre avt~idf fuelcell 113 4.83 fuelcell 113 1.397 cell 
10559 3.81 fema 406 0.905 fema 406 3.62 cell 10559 0.803 dna 1222 3.29 dna 1222 0.787 us~oduct 563 2.90 
usproduct 563 0.648 drug 28533 2.87 lotteri 1847 0.638 lotteri 1847 2.87 recycl 3392 0.490 oil 32569 
2.85 drug 28533 0.473 tax 33373 2.85 Wast 12684 0.468 Wast 12684 2.69 oil 32569 0.463 cmnrniss 38528 
2.62 tax 33373 0.462 recycl 3392 2.51 friendlifwe 95 0.457 nuclear 14417 2.47 copyright 3329 0.434 insur 
26030 2.44 quebec 1556 0.415 patient 12338 2.43 nuclear 14417 0.405 church 13636 2.39 patient 12338 0.403 
fund 48242 2.36 commiss 38528 0.402 microsoft 25629 2.36 forest 6972 0.401 plant 30405 2.35 church 13636 
0.388 forest 6972 2.33 insur 26030 0.375 copyright 3329 2.31 patent 9719 0.373 friendliiire 95 2.29 cancer 
8041 0.373 regul 46208 2.29 tunnel 2003 0.362 materi 43668 2.28 micro soft 25629 0.357 patent 9719 2.27 
fuel 14973 0.351 cancer 8041 2.23 plant 30405 0.350 parti 57428 2.14 hypnosi 68 0.344 .. .. seem 17099 
1.09 halt 8532 0.136 currentlaw 662 1.09 absolv 221 0.136 broke 12325 1.09 lstestdevelop 389 0.136 viabl 
1019 1,09 extent 12052 0.135 malfess 111 1.08 caus 37889 0.135 upkeep 218 1.08 instant 17596 0.135 d~agr 
2393 1.08 harsh 2399 0.135 honest 2522 1.08 trend 14965 0.134 nuclearprolifer 83 1.08 great 40042 0.134 
ongo 4298 1.07 idea 28141 0.134 industrination 909 1.07 proper 10564 0.133 sober 877 1.07 advantag 34078 
0.132 providmcreas 344 1.06 controlagreem 48 0.132 helppeopl 873 1.06 ongo 4298 0.132 statefinanci 132 
1.06 maintain 43280 0.130 gobroke i 142 1.06 success 37603 0.130 conditexist 250 1.05 occur 35080 0.130 
camag 293 1.05 prospect 14899 0.130 protectu. 213 1.05 fail 38136 0.129 widespread 7634 1.05 ensur 21267 
0.129 technologadvanc 623 1.04 widespread 7634 0.121 automobilindustr 280 1.04 broke 12325 0.121 successstori 
591 1.04 rest 24752 0.121 harsh 2399 1.03 suffer 24925 0.120 latestdevelop 389 1.02 possibl 15805 0.120 
absolv 221 1.02 seem 15732 0.117 controlagreem 48 1.00 latest 28172 0.116 Fig.2 Some TREC4 Ad-Hoc Query 
Terms with Highest and Fig.3 Some TREC4 Ad-Hoc Query Terms with Highest and Lowest avtf Values &#38; 
their Dcwument Frequencies Lowest Unnormalized av~idf Values (rx=l .5) &#38; their Document Frequencies 
 the avtf+idf weighting already produces good improvements to our av@ idf scheme without peaking performs 
uniformly better retrieval results (Section 3.2), we do not want to make too big in all measures by between 
7.9~0 to 10.2% in initial retrieval, an adjustment and decide to use a percentage of the minimum and 
by between 7.9 70 to 12.7% in expan&#38;d query retrieval. weight (minwt) within a query s term set for 
enhancement. The This shows that for these short queries with little or no term algorithm for making 
the peaking decision follows: frequency informatio~ the use of avtf from the collection is reasonable, 
and our simple avtPidf weighting scheme works. if (L <= 4 or L = O) no_peaking When compared with the 
Man.Wt cdn-nns, the avtPidf else if (L-1 or k2) peal.one.term weighting scheme without term peaking is 
within 3% of the else if (L=3 or L=4) peak_two_terms manual results at the initial retrieval stage. At 
the fiial else peak_three_terms. retrieval stage with expanded query, it is 290 to 370 better in precision 
measurements, and about 1.3% worse in relevants When a query is too short (L <=4) peaking a wrong term 
may retrieved. have a large adverse effect, and it is better not to take a chance. When the query is 
longer, we peak accordingly, but the term After adding term peaking adjustments to avt~idf initial we 
peak must not be high frequency, i.e. DoeFre e 35000. The retrieval remains simihw to no peaking, but 
further small gains value for peaking also has a variation if the term has a term are achieved for expanded 
query retrieval giving a 5?k0 frequency >1 in the original query (meaning that the user has improvement 
in AvP (average non-interpolated precision .298 repetitively used this low or medium frequency term) 
vs .283) and 1.6% in P@30 (average precision at 30 documents 0.75%ninwt is added to its weight. Otherwise, 
for a low retrieved .437 vs .430) when compared with Man.Wt . But RR frequency term (DocFre c 2000) we 
add only 0.25*minwt (relevants retrieved 4229 vs 4258) remains slightly worse by because these terms 
include most of the 2-word phrases and W%. The final retrieval results are about the same as those in 
have already been given higher weights due to other idf factor. [BSMS96]. Other terms are given a boost 
of 0.5*minwt. Thus, avtPidf weighting can give results rivaling this manual Naturally, thk algorithm 
and the parameters could change as method. It should be stressed that in the manual experiment, more 
experimentation is done. the weights of phrases are left unrnodlfie~ in thk new automatic scheme, phrase 
weights do get modified and 3.2 Results and Discussion sometimes peaked automatically.  3.2.1 Ad-Hoc 
Retrieval To study further about this new weighting scheme, we have chosen another set of topics (#101-150) 
for experimentation We tabulate in Flg.4 the results of our automatic query term with the same text data. 
This set of 50 queries were originally weighting scheme for the TREC4 ad-hoc experiment (49 used in TREC2 
ad-hoc (against the Disk 1&#38;2 text data), and queries) under the avtf*idf and avtPidf+Peak columns 
with have quite elaborate writeups. For this investigation, we retain a set to 1, 1.5 and 2 respectively. 
The best results are only the Description section of each topic and the query sizes achieved using a 
= 1.5. When compared with the Original after stemming and stopword removal is also shown in Fig. 1 columns 
(where the raw queries are used without modification), under Short.TREC2. They average to about 8 terms 
per query. avtt%df <.----.-----.-avtP idf + peaking ............> Original Man.Wt a.1.S tx=l a=l.5 a=2 
............ .......... .........-.......... .......... .-.-------Init ExpQ Init ExpQ Init ExpQ Init 
ExpQ Init ExpQ Init ExpQ RR 3327 3896 3648 4258 3667 4202 3698 4210 3694 4229 3678 4205 AvP .202 .260 
.226 .283 .221 .293 .220 .291 .222 .298 .223 .296 P@30 .318 .405 .353 .430 .343 .438 .343 .435 .344 .437 
.343 .440 RR = Total Relevants at 1000 Retrieved Documents, maximum achievable = 6501. AvP = Average 
Non-Interpolated Precision. P(FJ3O = Average Precision at 30 Documents Retrieved. Fig.4 Two-Stage Ad-Hoc 
Retrieval Results Averaged over 49 TREC4 Topics against Tipster Disk 2 &#38; 3 via Various Weighting 
Methods avtpidf <............. avtPidf + peaking ............> Orfginal Man.Wt a= 1.5 a=l a. 1.5 a.2 
.......... .......... .......... .......... .......... .----..--- Init ExpQ Init ExpQ Init ExpQ Init 
ExpQ Init ExpQ Init ExpQ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ...................... RR 5834 
7208 6233 7146 6309 7311 6387 7256 6385 7045 AvP .163 .250 .176 .249 .179 .2S6 .181 .259 .176 .240 P@30 
.369 .451 .381 .440 .393 .460 .394 .456 .387 .446 RR =Totd Relevmw atl~Retieved Documen~, mmimwachievable 
=l4383. AvP =Average Non-Interpolated Precision. P@30 = Average Precision at 30 Documents Retrieved. 
 Fig.5: Two-Stage Ad-Hoc Retrieval Results Averaged over 50 Short TREC2 Topics against Tipster Dkk 2 
&#38; 3 via Various Weighting Methods The retrieval results are shown in Fig.5. Here, we did not do popular 
with casual users. manuaf weighting; only experiments with the original queries are available for comparison. 
Peaking again gives small avt~idf increases of 1.590 to 4~o comprwed to no peakiig. when Orfginal Man.Wt 
a. 1.5 compared with the original queries at irtitiaf retrieval, avdidf .......... .........-........... 
 at CS=l.5 with peaking is better in all measures and produces Init ExpQ Init ExpQ Init ExpQ improvements 
of between 6.8% to 11 Yo. Expanded query . . ----------------------------------------------------------------------­ 
retrievaf results however do not improve much against the expanded query results of the Original column, 
better by only RR 848 1053 908 1084 906 1058 about 1 to 3~0. It should be noticed that with the Original 
queries, the expanded query retrieval produces a very AvP .223 .255 .226 .282 .240 .281 substantial increase 
of 53% over initial. It appears that the set P@30 .238 .340 .254 .358 .263 .356 of best n initially retrieved 
documents in thk case are well­-suited for the feedback process, even though the precision measures are 
low. Thus, even though our av~idf initial RR = Total Relevants Retrieved at 1000 Documents, retrievaf 
is better, the best n documents do not retain this maximum achievable = 1954. advantage for feedback. 
It seems that the quality of the n best AvP = Average Non-Interpolated Precision. feedback documents 
need not have correlation with the average P@30 = Average Precision at 30 Documents Rerneved. precision 
measures. Fig.6: Two-Stage Ad-Hoc Retrieval Results Averaged over 16 3.2.2 Retrieval using Extra Short 
Queries Extra Short TREC4 Topics against Tipster Dkk 2 &#38; 3 via Various Weighting Methods A look at 
Fig.1 shows that for the TREC4 ad-hoc queries, there are 16 extra short ones with sizes less than or 
equaf to 4 content terms, and for them we do not perform peaking . Most casual 3.2.2 Term Selection users, 
including our experience with intemet searching, would be of this type, and it is interesting to see 
whether Our algorithm for peaking terms represents an attempt to select improvements in retrieval are 
made using avWidf. Fig.6 shows important terms out of a small se~ and could have usefulness the same 
evaluation results for these 16 alone, and they are in other contexts. Since we already have a set of 
manually sitnh in behavior to the full set. Thus, compared to selected terms in TREC4, it is interesting 
to see how successfid tigind , the automatic method is 6.8% to lo.s?t. better in our term picking ability 
is compared to manual selection. Fig.7 initial retrieval, and IMO to 10.2% better in expanded query tabulates 
selective queries that have terms emphasized to retrieval for all measures. The result also rivals those 
of illustrate the quality of our automatic term selection algorithm. manual weighting. Thus, the automatic 
avflidf weighting is effective and useful for these extra short queries that are There are 321 content 
terms in the 50 TREC4 queries (including query #201 ), out of which 115 were selected manually for emphasis 
plus 4 words that were repetitively used in the original topics. However, 16 queries have 4 or less content 
terms and they are not subject to peaking. The end result is that only 91 manually selected terms (in 
34 queries) are useful for comparison purposes. Moreover, they are all single terms because we did not 
include phrases for emphasis in the manual process. Within the 34 queries, our automatic algoridun peaks 
16 phrases and 85 single terms. Of these 85, 51 agree with the manual selections. This gives a precision 
of 60$Z0(51/85) and a recall of 56% (51/91), assuming that the manually selected terms are the standard. 
However, only 4 of the 16 phrases do not share a term with those that were manually emphasized (#203: 
economimpac~ #208: usproduc~ #225 managagenc, #239: conditexist). The other 12 are narrower concepts 
of a chosen term. If they are regarded as also correct, the precision Manual Automatic #203 recycl economimpact 
tire recycl tire #206 parti thirdparti prognosi viabll #216 osteoporosis patient disease osteoporosis 
#219 auto export japanes mexico #222 capit capitpunish punish crime crime deterr #227 militari friendlifire 
fire accid death train friendli #237 altern oil energi gasolin automobil pollut #239 region conditexist 
concentr concentr cancer cancer #248 electron electrontechnolo blind blind #250 firearm firearm crime 
crime arnmunit correl Fig.7 Comparison of Term Selection by Manual and Automatic Methods for Selective 
Queries ratio becomes 6290 (63/101 ). Queries #219 and #237 are dte only two that have no common single 
word selections nor phrases containing narrower concepts between the automatic and manual methods. However, 
in both cases automatic weighting gives better performance. It needs further study to understand what 
and how emphasizirtg certain terms can lead to improved retrieval. 4. Conclusion A good query for ad-hoc 
retrieval should provide sufficient variety of terms as well as evidence to differentiate terms such 
as different term frequencies. Short queries lack both types of information. We have introduced a new 
automatic weighting scheme for query terms that can give significant improvements at the initial retrieval 
stage compared with using the original queries under the TREC4 short query environment. The fiial expanded 
query retrieval results rival those based on weighting with manual judgment. Lesser improvements we also 
observed with the short TREC2 ad-hoc queries. This weighting scheme is based on observed collection statistics 
and no trairdng from previous retrieval characteristics is required. Future investigation would be to 
device methods to automaticrdly add topically-related terms to a short query so that it can have sufficient 
variety of terms at the initial retrieval stage. This way, a short query may have the capabilities of 
a good query. Acknowledgments The author would like to acknowledge beneficial comments by the anonymous 
referees. This work is partially supported by a grant from PSC-CUNY. References [BSAS95] Buckley, C., 
Salton, G., Allan, J. &#38; Singhal A (1995). Automatic query expansion using SMART TREC3. Irx Overview 
of the Third Text REtrieval Conference (T REC-3). Hsrman, D.K. (Ed.). NIST Special Publication SP 500-225, 
pp.69-80. [BSMS96] Buckley, C., Singhal, A., Mitra, M. &#38; Salto~ G. New retrieval approaches using 
SMART: TREC 4. to be published. [EvLe94] Evans, D. &#38; Lefferts, R (1994). Design and evaluation of 
the CLARIT-TREC-2 system. Im The Second Text REtrieval Conference ~REC-2). Harman, D.K. (Ed.). NIST Special 
Publication SP 500-215, pp.137-150. [Harm96] Harman, D.K. The Fourth REtrieval Conference (TREC-4). to 
be published. [Harm95] Harm-D.K. Overview of the Third REtrieval Conference (TREC-3). In Overview of 
The Third Text REtrieval Conference (TREC-3). Hannan, D.K. (Ed.). NIST Special publication 500-225, 1995, 
pp.1-19. [KwGL95] Kwok, K.L., Grtmfeld, L. &#38; Lewis, D.D. (1995). TREC-3 ad-hoc, routing retrieval 
and thresholdmg experiments using PIRCS. In: Overview of the Third Text REtrieval Conference (TREC-3). 
Harrnan, D.K. (Ed.). IWST Special Publication SP 500-225, pp.247-255. [KwG196] KWOIG K.L. &#38; Grunfeld, 
L. TREC-4 Ad-Hoc, Routing Retrieval and Ffltering Experiments using PIRCS. to be published. [KwGt94] 
Kwo~ K.L. &#38; Grunfeld, L. (1994). TREC-2 Document retrieval experiments using PIRCS. In The Second 
Text REtrieval Conference (TREC-2). Harrnan, D.K. (Ed.). NIST Special Publication 500-215, pp. 233-242. 
[KwPK93] Kwok, K.L., Papadopolous, L. &#38; Kwan, Y.Y. (1993). Retrieval experiments with a large collection 
using PIRCS. Im The First Text REtrieval Conference f.TREC-1). Harrnan, D.K. (Ed.). NEST Spcial Publication 
500-207, pp. 153-172. [KwoW5] Kwok, K.L (1995). A network approach to probabilistic information retrieval. 
ACM Transactions on Office Information Systems, 13:325-353. [Kwok90] Kwok, K.L (1990). Experiments with 
a component theo~ of probabilistic information retrieval based on single terms as document components. 
ACM Transactions on Office Information Systems, 8:363-386. [Luhn58] Luhrt, H.P (1958). The automatic 
creation of literature abstracts. IBM J. of Research &#38; Development. 2159­ 165. [LuKe95] Lu, X.A 
&#38; Keefer, R.B. Query expansiordreduction and its impact on retrieval effectiveness. h Overview of 
the Thiid Text REtrieval Conference (TREC-3). Hat-man, D.K. (Ed.). NIST Special Publication 500-225, 
1995, pp.231-239. [RWJHG95] Robertso~ S.E., Walker, S., Jones, S., M.M. Hancock-Beaulieu &#38; Gatford, 
M (1995). Okapi at TREC-3. In: Overview of The Third Text REtrieval Conference (TREC-3). Harmart, D.K. 
(Ed.). NIST Special Publication 500-225, 1995, pp.109-126. [SaBu88] Salton, G. &#38; Buckley, C (1988). 
Term-weighting approaches in automatic text retrieval. Information processing &#38; Management, 24:513-523. 
 [SaYY75] Salton, G., Yang, C.S. &#38; Yu, C (1975). A theory of term importance in automatic text analysis. 
J. of American Society of Information Science. 26:33-44. [Spar72] Sparck Jones, K (1972). A statistical 
interpretation of term specificity and its application in retrieval. Journal of Docurnentatioz 8:11-21. 
 
			