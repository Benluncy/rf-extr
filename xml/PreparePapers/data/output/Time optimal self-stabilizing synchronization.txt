
 Time Optimal Self-Stabilizing Synchronization EXTENDED ABSTRACT Baruch Awerbuch Shay Kuttent Yishay 
Mansour$ Boaz Patt-Shamir~ George Varghesef Abstract In the network synchronization model, each node 
maintains a local pulse counter such that the advance of the pulse numbers simulates the advance of a 
clock in a synchronous network. In this paper we present a tame optimai sel&#38;stabilizing scheme for 
network syn­ chronization. Our construction has two parts. First, we give a simple rule by which each 
node can com­ pute its pulse number as a function of its neighbors pulse numbers. This rule stabilizes 
in time bounded by t?te diameter of the network, it does not revoke global operations, and does not require 
any additional memory space. However, this rule works correctly only if the pulse numbers may grow unfoundedly. 
The second part of the construction (whzch is of indepen­ dent interest in its own right) takes care 
of this prob­ lem. Specifically, we present the jirst self-stabilizing reset procedure that stabilizes 
in tzme proportional to the diameter of the network. This procedure can be combined with unbounded-register 
protocols to yield bounded-register algorithms. Lab. for Computer Science, MIT. Supported by Air Force 
Contract TNDGAFOSR-86-0078, ARO contract DAAL03-86­K-01 71, NSF contract CCR861 1442, DARPA contract 
NOOO14­89-J-1988, and a special grant from IBM. t IBM T.J. Watson Research Center. $Tel-Aviv University 
and IBM T.J. Watson Research Center. $Lab. for Computer Science, MIT. Research partly done while visiting 
IBM T.J. Watson Research Center. Supported in part by DARPA contracts NOOO1 4-92-J-4o33 and NOOO14­92-J-1799, 
ONR contract NOOO14-91-J-1O46, and NSF contract 8915206-CCR. !IDEG, 55o King Street, Llttleton, MA 01460. 
 Permission to copy without fee all or part of this material is granted provided that the copies are 
not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the 
publication and its date appear, and notice ia given that copying is by permission of the Association 
for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. 
25th ACM STOC 93-51931CA,USA @ J993 AG~ Q-89~9J-59 J-7/93 /QQQ51Q652,.. $J.~Q 1 Introduction The quinteaaential 
problem of virtually every dis­ tributed computation is how to remove uncertainty that might effect the 
task at hand. The nature of clis­ tributed systems is such that the tasks may be influ­enced by many 
factors; to mention just a few, we need to confront difficulties such as physically dispersed inputs, 
asynchrony of computation and communi­cation, dynamically changing networks, and many more. It is highly 
desirable to have some kind of an automatic transformer that allows a protocol designer not to lose generality 
while assuming a (friendlier model than the given real-world environment. De­spite its desired generality, 
such a transformer must be cheap, in the sense that its overhead should be kept small so that it is still 
practical. In this paper we present a transformer algorithm for reactive protocols that eliminates two 
of the prob­lematic uncertainties. Specifically, we implement a distributed pulse counter at the nodes 
that simulates (in some precise sense defined below) the advance of a clock in a true synchronous system. 
Our algorithm has the important feature that it starts operating cor­rectly regardless of its initial 
state (algorithms of this kind are called self-stabilizing ). In practice, this means that the algorithm 
adjusts itself automatically to any change in the network or any unpredictable fault of its components, 
so long as the faults stop for some sufficiently long period. Our algorithm is sim­ple and easy to implement. 
The algorithm stabilizes in time proportional to the diameter of the network, and hence it is optimal 
in that respect. Problem Stat ement. We are given an asyn­chronous message passing network, and our objec­tive 
is to implement a dwtributed pulse service at the nodes. The service must provide the node at all time 
with a pulse number, subject to the following condi­tions. Synchronization: Any message sent at local 
pulse i is received at the other endpoint before local pulse i+ 1. Progress: There exist parameters A 
and p (that may depend on the network topology), such that for any time interval of length t> A, the 
number of consecu­tive pulses generated at every node is at least p(f A). The parameter A is called network 
slack and p is the progress rate. A self stabilizing protocol is required to satisfy these conditions 
only after a certain stabilization time has elapsed. More intuition and elaborate description of the 
desired properties from such a service are given in Section 2 below. Previous Work. Synchronizers were 
the target of considerable research, following the work of Awer­buch [Awe85]. For example, see [AS88, 
PU89, AP90, APPS92]. The concept of self-stabilization was in­troduced by Dijkstra [Dij74]. A few general 
stabi­lizer schemes that upgrade non-stabilizing protocols to be self-stabilizing have been proposed 
since. These are typically based on a reset procedure that can im­pose an acceptable state on the system. 
Such schemes can be combined with the aforementioned synchroniz­ers to obtain a self-stabilizing synchronizer. 
We refer below to the known stabilizers and their complexity bounds. In [AG90], a pre-specified bound 
on the di­ameter of the network is required, and the protocol stabilizes in time quadratic in that bound. 
We re­mark that in a dynamic system, such a bound is typ­ically significantly larger than the actual 
diameter. In [AKY90], a self-stabilizing spanning tree construction is given, that can be used as the 
basis to a stabilizing reset procedure. The stabilization time of that proto­col is O(n2), where n is 
the actual number of nodes in the system. In [DIM9 1], a randomized spanning tree protocol is implicitly 
given. That protocol stabilizes in expected time O(d log n), where d is the actual di­ameter of the network. 
In [APV91, Var92] a general self-stabilizing reset protocol is presented, whose sta­bilization time is 
O(n). In [AV9 1], a self-stabilizing synchronizer is presented. This protocol stabilizes in time linear 
in a pre-specified bound on the diameter of the network. It is worth mentioning the important work of 
Spinelli and Gallager [SG89] on topological update in dynamic networks. This algorithm has many at­tractive 
features (in addition to its simplicity). The main property of the algorithm is that it stabilizes in 
time proportional to the diameter. Although not stated as such, it is easy to verify that the algorithm 
is self-stabilizing. The drawbacks of the algorithm are its large space requirement (which is inherent 
for the topology update problem), its message complex­ity (that might be exponential), and the assumption 
that the node identifiers are in the range 1 . . . n. Our Results. The contribution of this paper is 
twofold. First, we present a simple new rule for self­stabilizing synchronization of networks, with network 
slack d (the diameter of the network), and progress rate 1. Our rule does not invoke global operations, 
stabilizes in time linear in the actual diameter of the network without any prior knowledge, and does 
not require any additional memory space (other than for the pulse counter). In the course of development 
of this rule, we obtain some interesting results regarding common synchronization rules, with applications 
to clock synchronization schemes. We believe that the analysis of the new rule captures some of the inherent 
properties of synchronization. However, this rule suf­fers from a serious disadvantage, namely it requires 
unbounded pulse numbers to ensure correctness. We fix this flaw in our second major result. Specifically, 
we give the first time-optimal self-stabilizing reset procedure, i.e., a reset protocol that stabilizes 
in time proportional to the actual diameter of the network. This result is of independent interest in 
its own right, being the first time-optimal stabilizer. The heart of the reset procedure is a novel self-stabilizing 
spanning tree algorithm that in diameter time produces a tree with diameter height, The bounded protocol 
needs unique identifiers of the nodes, and a pre-specified bound on the diameter of the network; the 
complexity of the space requirement and messages size depends logarithmically on the bound. Notations 
and Model of Computation. We model the processor network as a fixed undirected graph G = (V, E). For 
u,v E V we denote by dist(u, v) the length of the shortest path be­tween u and v. We follow the notational 
conven­tion that n = [Vi, and that d = diameter(G) = mah,uev {dist(u, v)}. For each node v c V, we de­note 
Af(v) = {u : did(u, v) = 1}. In this abstract we assume the model of unit capacity data links, in which 
there is at most one outstanding message in transit on every channel at any given time. (This model was 
defined and justified [APV91, Var92] as a realis­tic model for any message passing system with some bound 
on the capacity of the channels.) The message delivery time can be arbitrary, but for the purpose of 
time analysis we assume that each message is de­livered in one time unit. We use the method of !o­ cal 
detection [AKY90, APV91, Var92], in which each node constantly sends its state to all its neighbors. 
This enables us to present our protocol in a compact formulation of local rules. These rules are functions 
that take the state of the neighborhood (made avail­able by the underlying local detection mechanism), 
and output a new state for the node. Paper Organization. This paper is organized as follows. In Section 
2 we develop the requirements from a desirable synchronization scheme, by exploring the disadvantages 
of some of popular schemes. In Section 3 we specify the new synchronization rule and analyze its complexity. 
In Section 4 we develop an optimal self-stabilizing reset procedure and explain how to apply it to the 
synchronization rule to obtain a protocol that works with bounded registers. 2 Requirements and Examples 
In this section we consider a few preliminary ideas for synchronization rules. By studying their proper­ties, 
we develop a set of requirements from a desirable scheme. Synchronization. The synchronization requirement 
can be defined as follows (cf. problem statement in Section 1). Definition 1 Let G = (V, E) be a graph, 
and let P : V -+ N be a pulse assignment. We say that the con­figuration (G, P) is legal for node v, 
denoted legal(v), if for al/u e JV(v) we have [P(u) P(v)l < 1. We say that the configuration (G, P) is 
legal if for all v ~ V, legal(v) holds. The idea behind Definition 1 is as follows. In the synchronous 
setting, all messages sent at pulse i are received by pulse i + 1. When we simulate execu­tions of synchronous 
protocols on an asynchronous network, we do not have a global pulse-producing clock. Rather, we want 
to maintain the validity of the messages sent. This can be done by ensuring the a node sends pulse i 
+ 1 messages only after it has received all the pulse i messages from its neighbors. Since message delivery 
is not simultaneous, there can be a skew of the pulse counters at neighbors, but this skew is allowed 
to be at most one: if the pulse num­bers at two adj scent nodes differ by more than 1, then necessarily 
the node with the higher pulse number has advanced without receiving all messages of prior pulses. This 
notion of legal configuration gives rise to the following simple synchronization rule, which is implicit 
in the a synchronizer of [Awe85]. Rule 1 ( Min Plus One) P(v) + rnrnvl {P(u) +1} The idea is that whenever 
the pulse number is changed, the node sends out all the messages of pre­vious rounds which haven t been 
sent yet. Stabilization. As is well known, Rule 1 is stable, i.e., if the configuration is legal (as 
in Definition 1), then applying the rule arbitrarily can yield only legal configuration. Notice however, 
that if the state is not legal, then applying Rule 1 may cause pulse numbers to drop. This is something 
to worry about, since the regular course of the algorithm requires pulse num­bers only to grow. Thus 
it is conceivable that actions taken in legal neighborhoods interfere with the ac­tions taken in illegal 
neighborhoods. This intuition is captured by the following theorem. Theorem 1 Rule 1 M not self-stabilizing. 
Proofi By a counter example. Consider the pulse configuration of a 10-processor ring depicted in Fig­ure 
1 (a). Clearly, the vertical edges indicate illegal state. Consider now the execution described in Fig­ure 
1 (a-i), obtained by repeated application of Rule 1. It is readily seen that the last configuration (i) 
is basically identical to configuration (a), with all the pulse numbers incremented by one, and rotated 
one step counter-clockwise. Repeating this schedule re­sults in an infinite execution in which each processor 
takes infinitely many steps, but none of the configu­rations is legal. I Let us make a make a short digression 
here. The­orem 1 has an interesting corollary for clock synchro­nization: one of the popular schemes 
for clock syn­chronization [LL84] is repeated averaging . Roughly speaking, in the repeated averaging 
rule each node sets its value to be the average value of its neigh­bors, while advancing the clock if 
this average is close enough to its own value. Corollary 1 Repeated averaging does not stabilize. Proof 
Sketch: The scenario in the proof of Theo­rem 1 shows that averaging with rounding down does not work. 
A similar scenario can be constructed for averaging with rounding up. B Tzme Complexity. One idea that 
can pop into mind to try to repair the above flaw is to never let pulse numbers go down. Formally, the 
rule is the following. Rule 2 (Monotone Mm Plus One) Rule 2 can be proven to be self-stabilizing. However, 
it suffers from a serious drawback regarding its st abi­lization time. Consider the configuration depicted 
in Figure 2. Q 3 24 12 (d) 33 4 ,+ 3 a 3 0 3 44 44 323 2 (h) (i) 23 3 ~ 44 44 55 Figure 1: An execution 
using Rule 1. The A quick thought should suffice to convince the reader that the stabilization time for 
this configura­tion using Rule 2 is in the order of 1000000 time units, which seems to be unsatisfactory 
for such a small net­work. This example demonstrates an important prop­erty that we shall require from 
any self-stabilizing protocol: the stabdization time must not depend on the initial state; rather, it 
should be bounded by a function of the network topology. A clear lower bound on the stabilization time 
is the diameter of the net­work (e.g., if n 1 nodes must change their pulse number). In the example 
above, and using Rule 2, the stabilization time depends linearly on the value of the pulses, thus implying 
that the stabilization time can be arbitrarily large. Truly Distributed Model. The next idea is to have 
a combination of rules: certainly, if the neighborhood is legal, then the problem specification requires 
that Rule 1 is applied. But if the neighborhood is not legal, another rule can be used. The first idea 
we consider is the following. Rule 3 (Maximum) mi~~~t.) {P(u)+l} , if legai(v) P(v) ­ ma~c~t.) {P(u), 
P(v)} , otherwise { It is straightforward to show that if an atomic ac­tion consists of reading one s 
neighbors and setting its own value (in particular, no neighbor changes its value in the meanwhile), 
then Rule 3 above indeed Figure 2: A pulse assignment for Rule 2. node that rnovedin each step in marked. 
converges to a legal configuration. Unfortunately, this m;del, traditi&#38;ally c;lled central demon 
model [Dij74, BP89], is not adequate for a truly distributed system, which is based on loosely coordinated 
asyn­chronous processes. And as one might suspect, Rule 3 does not work in a truly distributed system, 
as we demonstrate in the following theorem. Theorem 2 Rule 3 is not self-stabilizing in an asyn­ chronous 
system. Proofi By a counter example. Consider the execu­tion of a line of 3 processor depicted in Figure 
3. (a) (b) (c) @@+@+-@@@+ (d) (e) @@@@@-@l Figure 3: An execution using Ruie 3 in a truly dis­tributed 
system. The nodes that send or receive are marked. Since the estimate of the middle processor of the 
value of the left processor in configuration (c) is 2, it applies the correcting part of Rule 3, resulting 
in yet another illegal configuration (d). Since configuration (e) is equivalent to configuration (a) 
with pulse num­bers incremented by 2, we conclude that repeating this schedule results in an execution 
with infinitely many illegal states. ~ Locality and Simplicity. Finally, we would like to address two 
properties which are somewhat harder to capture formally. It seems, however, that these prop­erties are 
of the highest importance in practice. The first such property is locahly. By locality we mean that it 
is much preferable that a processor will be able operate while introducing only the minimal pos­sible 
interference with other nodes in the network. In other words, invoking a global operation is consid­ered 
costly, and we would like to avoid it as much as possible. One way to capture this intuition ap­proximately 
in our case is to require that the only state information at the nodes is the pulse number, and that 
protocols should operate by applying local rules as above. The problem with the conventional theoretical 
approach here is that it neglects to con­sider the frequent cases, by focusing on the worst case scenario. 
As an illustrative exercise, contrast this ap­proach with the following solution: whenever an ille­gal 
state is detected, reset the whole system. This solution, although it may be unavoidable in some rare 
cases in practice, does not seem particularly ap­pealing as a routinely activated procedure. Consider, 
for example, the common situation in which a new node joins the system (perhaps it was down for some 
while). We would like the protocols to feature grace­ful joining in this case, i.e., that other nodes 
would be effected only if necessary (e.g., the neighbors). The last property we require from distributed 
pro­tocols is even harder to define precisely. Essentially, we would like to have the protocols conceptually 
sim­ple. This will make the protocols easy to understand, and therefore, easy to maintain. This requirement 
is one of the main obstacles for many sophisticated protocols that are not implemented in practice. 
3 Optimal Self-Stabilizing Rule In this section we give a simple, self-stabilizing} opti­mal rule of 
synchronization, and analyze its st abiliza­tion time. Specifically, our synchronization scheme is based 
on the following rule. Rule 4 (Max Minus One) mi~~~(VJ {P(u)+l} , if legal(v) (v) + ma~c~(v) {P(u) 1, 
P(v)}, otherwise { In words, Rule 4 says to apply a minimum plus one rule (Rule 1) when the neighborhood 
seems to be in a legal configuration, and if the neighborhood seems to be illegal, to apply a maximum 
minus one rule (but never decrease the pulse number). The sim­ilarit y to the maximum rule (Rule 3) is 
obvious. The intuition behind the modification is that if nodes change their pulse numbers to be the 
mazimum of their neighbors, then race condition might evolve, where nodes with high pulses can run away 
from nodes with low pulses. If the correction action takes the pulse number to be one less than the maximum, 
then the high nodes are locked , in the sense that they cannot increment their pulse counters until all 
their neighborhood have reached their pulse number. This locking spreads automatically in all the in­fected 
 area of the network. Formally, the way Rule 4 corrects any initial state is analyzed in detail in the 
proof of Theorem 3 below. We remark that the anal­ysis presented here is simplified (the case analysis 
of the interleaving due to asynchrony is omitted). This is done to improve exposition of the central 
ideas. Theorem 3 Let G = (V, E) be a graph with diameter d, and let P : V ~ N be a pulse assignment. 
Then applying Rule ~ above results in a legal configuration in d time units. In order to prove Theorem 
3, we shall need some tools to analyze the behavior of the synchronization scheme. The basic concept 
that we use is a certain po­tential value we associate with every node, described in the following definition. 
Definition 2 Let v be a node in the graph. The po­tential of v is denoted by #(v) and is dejined by +(v) 
= y~;{P(u) -P(v) -dist(u, v)} . Intuitively, ~(v) is a measure of how much is v not synchronized, or 
alternatively the size of the largest distance-adjusted skew in the synchronization of v. Pictorially, 
one can think that every node u is a point on a plane where the ~-coordinate represents the dis­tance 
of u from v, and the y coordinate represents the pulse numbers (see Figure 4 for an example). In this 
representation, v is the only node on the y-axis, and #(v) is the maximal vertical distance of any point 
(i.e., node) above the 45-degree line going through (O, P(V)). Let us start with some properties of+, 
whose (triv­ial) proofs are omitted. Lemma 2 For all nodei v E V, ~(v) >0. Lemma 3 A configuration of 
the system is legal if and only if for all vc V, #(v) = O. We now show the key property of Rule 4, namely 
that the potential of the nodes never increases when Rule 4 is applied. Lemma 4 Let P be any pu[se assignment, 
and sup­pose that some node u changes its pulse number by applying Ru~e 4-Denote the new Wse number of 
u by Pt(u), and the potential of the nodes in the new configuration by qi). Then for all nodes v E V, 
d (v) s 4(V). pulse numbers +$. ,. .&#38;.. ,,, ~>,. + +8 Q.. ,.. 6 @(C)\ Qf!f e .,.. a b ,., ,,. 
,,# d ,.~ 5 ~-z. ...  ,!!... t ..... 3 .; 1 d   f-+=cef ---~Lcefromb(i) Figure 4: On the lefl is 
an example of a graph with pulse assignment (i). Geometrical representations of this configuration are 
shown in (at) and (iii). The plane corresponding to node c is in the middle (ii), and the plane corresponding 
to node b is on the right (iii). As can be readily seen, ~(c) = 1,and ~(b) = 4. Also, l?(c) = 1, and 
O(b) = 1 (see Definition 3). Proof Sketch: The first easy case to consider is the dist (w, v) s dist(u, 
v) + 1. This implies that potential of u itself. Since P (u) > P(u), we have P (u) P(v) dist(u, v) 
~ (u) = ~~.{P(wI) -P (u) dist(w, u)} ~ (P(w) 1) P(v) (dist(w, v) 1) = P(w) P(w) dist(w, v)~ rr.a, 
{P(w) P(u) dist(w., u)} (1) = ~(u) . and we are done. I As noted above, the inequality in (1) is strict 
if (Note, for later reference, that the inequality in (1) ~(u) >0. In other words, each time a node with 
pos­is strict if ~(u) > O.) Now consider w # u. The only itive potential changes its pulse number, its 
potentialvalue that was changed in the set decreases. This fact, when combined with Lemmas 2 and 3, immediately 
implies eventual stabilization. {P(vJ) -P(v) -dist(w, v) I W G V} However, this argument leads to a proof 
that the sta­bilization time is bounded by the total potential of is P(u) P(v) dist(u, v). There are 
two cases to the configuration, which in turn depends on the ini­ examine. If u changed its pulse by 
applying the rein tial pulse assignment. We need a stronger argument plus one part of the rule, then 
there must be a node in order to prove a bound on the stabilization time w which is a neighbor of u, 
and is closer to v, i.e., that depends only on the topology, as asserted in the dist (u, TJ) = dist (w, 
v) + 1.Also, since rein PIUS one statement of Theorem 3. Toward this end, we define was applied, we 
have P (u) < P(w) + 1. Now, . the notion of wavefront . P (u) P(v) dist(u, v) Definition 3 Let v be 
any node, The wavefront ofv, < (P(w) +1) -P(TJ) -(dist(w, v) + 1) denoted O(v), is defined by = P(w) 
 P(v) dist(w, v) O(v) = ::; {dist(u, v) I P(u) P(v) dist(u, v)=~(v)} . and hence the q5(v) does not 
increase in this case. The second case to consider is when u has changed In the graphical representation, 
the wavefront of a its value by applying the max minus one part of node is simply the distance to the 
closest node of on the rule. The reasoning is dual to the first case: let the potential line (see Figure 
4 for an example). w be a neighbor of u with P(w) = P (u) + 1. clearly, Intuitively, one can think of 
O(V) as the distance to the closest largest trouble of v. The importance of the wavefront becomes apparent 
in Lemma 6 below, but let us first state an immediate property it has. Lemma 5 Let v E V. Then ~(v) = 
O if and only if #(v) = o. Lemma 6 Let v be any node with O(v) >0, and let W(v) be the wavefront ofv 
after one time unit. Then o (v) < @(v) 1. Proof: Suppose @(v) = f >0 at some state. Let u be any node 
such that P(u) P(v) dist(u, v) = ~(v), and dist(u, v) = f. Consider a neighbor w of u which is closer 
to v, i.e., dist(w, v) = f 1 (it may be the case the w = v). From the definition of Q(v), it follows 
that P(w) < P(u) 1. Now consider the next time in which w applies Rule 4. If at that time Q(v) < ~, 
we are done. Otherwise, w must assign P(w) -P(u) 1. No greater value can be assigned, or otherwise Lemma 
4 would be violated. At this time, P(w) P(v) dist(w, v) = q5(v) also, and hence @( V)< f l. H The next 
corollary follows from Lemmas 5 and 6. Corollary 7 Let v be any node. Then after O(v) time units, ~(v) 
= O. We can now prove Theorem .3. We first re-state it. Theorem 3 Let G = (V, E) be a graph with diameter 
d, and let P : V ~ N be a pu!se assignment. Apply­ing Rule ~ above results in a legal configuration in 
d time units. Proof: By Lemma 3, it suffices to show that after d time units, q$(v) = O for all v G V. 
From Corollary 7 above, we actually know that a slightly stronger fact holds: for all node v c V, after 
Q(v) time units, 4(v) = O. The theorem follows from the facts that for all v G V, @(v) s d, and by the 
fact that ~(v) never increases, by Lemma 4. 1 4 Stabilization with Bounded Registers In this section 
we propose a general scheme for sta­bilizing unbounded-values algorithm that are imple­mented with realistic 
(and therefore, bounded-size) registers. Our scheme is based on the following idea, which is described 
in detail in [APV92]. First, we let the registers be of size large enough so as to accom­modate normal 
operation when initialized at some default value. The crucial part of the scheme is that whenever some 
processor hits the bound of values which the register is capable of storing, it invokes a reset protocol. 
Roughly speaking, the effect of this procedure is to eventually supply all the nodes in the system with 
a (signal , such that these signals consti­ tute a consistent reference point in time, in which the nodes 
can reset their local state and start the compu­ tation anew. The justification for the usage of such 
a costly global operation is that it is invoked rarely. The best implementation of a self-stabilizing 
reset protocol to date is given in [APV91]. The stabiliza­ tion time of this procedure can be bounded 
only by the length of the longest simple path in the network, i.e., O(n) for general networks. In this 
section we give the first implementation of a reset procedure that works in diameter time. The idea is 
to construct a subgraph whose longest simple path has length O(d). Then, when the reset procedure of 
[APV91] is applied only on the links of that sqbgraph, we get a reset protocol that stabilizes in O(d) 
time. Thus, the problem is reduced to the construct ion of such a sub graph. In the remainder of this 
section we develop a simple algorithm that produces a shortest paths tree rooted at some node in the 
network. Since the longest simple pat h in such a tree consists of no more than 2d links, in this we 
will complete our construct ion. 4.1 Basic Protocols It is fairly safe to say that one of the first distributed 
self-stabilizing algorithms (although it was never in­troduced as such) is the Bellman-Ford algorithm 
for shortest paths [Be158]. In the shortest paths problem, there is a designated source node s, and each 
node v has a variable d., called the distance estimate of v. The goal of the algorithm is that dv will 
hold the ac­tual distance of v froms. We shall consider here only the simple case of unweighed edges. 
The algorithm can be expressed in our formulation by the following rule. Rule 5 (Be[!man-Ford) ifv=s 
d.~ 0 1+ min{du : u ~ ~(v)} , otherwise { Rule 5 can be extended easily to produce, at each node, a pointer 
to the neighbor on the shortest path to s, thus constructing a shortest paths spanning tree. (The extension 
only involves introducing at each node a consistent tie breaker among its incident edges.) It is straightforward 
to verify, by induction on dist ante, that if all nodes start with distance estimate which is not smaller 
than their true dist ante from v, then their estimate stabilizes on the true value in time pro­portional 
the diameter of the graph. It is slightly less obvious, but nevertheless true, that this holds also for 
initial estimates which are smaller than the true distance. Informally, the reason for this is that the 
nodes keep verifying that their distance estimate has some neighbor wit h an estimate that supports it, 
and therefore if a node v # s has dv < d. for all u G ~(v), its distance estimate will increase, until 
it reaches the true estimate which is supported by a solid {flow of values from s. It is important to 
notice that assuming the exis­tence of a designated source in a distributed system is painful. More formally, 
this means that we assume that leader election can be done; unfortunately, leader election in the context 
of dynamically changing net­works is an equivalently difficult task. The common solution for this problem 
is to assume that each node has a unique ID (this is a more reasonable assump­tion to accomplish in practice, 
and it is actually the industry standard). The idea now is to construct the shortest paths tree rooted 
at the node with the mint­mal ID in the network. This is done by labeling each distance estimate with 
its alleged source, and letting smaller IDs always take precedence over larger ID, Technically, the implementation 
is as follows. Each node v maintains a pair (T-V,dO), where rv denotes the minimal ID seen so far (the 
alleged root ), and d. is the distance estimate to rv. We assume that there is a distinct ID hardwired)) 
in every node v. The nodes repeatedly apply the following rule. Rule 6 (Bellman-Ford with IDs) First, 
a node v computes rv e min {IDV, r.}, where u E N(V). Then v computes d. by setting d. +­1 + min{d. : 
u E ~(v) and rv = ru} ifrv # IDV, or d. ~0 ifrv =IDV. Let us remark first that Rule 6 above is the first 
rule throughout this discussion that violates the local­ity condition in that it has an extra state component, 
namely r.. But Rule 6 has more serious problems. The fact that a small ID overrides all other IDs makes 
this rule vulnerable to bad initial assignments. Con­sider, for example, the case in which the system 
is initialized in a way such that at some node v there is an alleged root rw = r*, where r* is not the 
ID of any node. Assume further that r is smaller than all the actual IDs in the system. In this case, 
apply­ing Rule 6 must eventually results in a state in which r. = r for all v G V. Notice, however, that 
there­after the distance estimates at the nodes will grow unfoundedly, since unlike the case of Rule 
5, there is no true source to halt the increase of the du variables. (Another way to see this is by noticing 
that the true distance from any node to r* is infinity.) We call such a bad case ghost root. Let us remark 
here that the ghost root phenomenon is fairly frequent: it happens whenever the node with the minimal 
ID crashes. A common fix to this widely used rule [MRR80, AG90], is to parametrize the protocol with 
some hardwired bound on the maximal distance estimate. The nodes simply do not consider any estimate 
that may cause them to break the bound. More precisely, denote the pre-specified bound by D. D is called 
the bound parameter of the protocol. The modified rule is as follows. Rule 7 (Bellman-Ford with IDs and 
Bound Param­eter D) First, a node v computes r. ~ min {IDU, r~} where u E ~(v) and d. < D. Then v computes 
dv by setting dti + 1 + min {du : u ~N(v) and ru = ru} if ru #IDu, ordv hO ifru =IDU. It is not difficult 
to show that the effect of ghost roots cannot last more then D time units, and there­fore Rule 7 stabilizes 
in O(D) time (see Lemma 8 below). Therefore, if the bound parameter D is close to the actual diameter 
d, Rule 7 is close to optimal. But unfortunately, having such a estimate of the di­ameter is an unrealistic 
assumption in a world of dy­namically changing networks, which is our ultimate goal. Typically, D is 
at least n, the number of nodes in the network, and it might be significantly larger. Notice that we 
must have some bound on the num­ber of nodes: otherwise, unique IDs would have been impossible to get. 
Also, coming up with some bound is pretty easy: 264 is a good bound on the diameter of all networks in 
the foreseeable future. However, the maximal dependence on such huge bounds we are willing to tolerate 
is polylogarithmic (e.g., the size of the registers allocated to hold node IDs). 4.2 The New Protocol 
We are now ready to present the new protocol for self-stabilizing spanning tree. The stabilization time 
of the tree is O(d) time units, provided that a bound D on the diameter is known in advance. We re­mark 
that the size of the messages in this algorithm is O(k log D), where k is the size of the IDs. The idea 
is as follows. Assume that we are given some bound D on the diameter of the network whose true diameter 
is d, which maybe significantly smaller. As argued above, such a bound is easy to get. To stabilize in 
time proportional to d, we run log D + 1 independent (versions) of Rule 7 in parallel, where in version 
i, O ~ i ~ log D, we set the bound parameter to be 2i. Before we proceed to explain how we use the results 
of these independent versions, let us consider theexecutions of each version separately. First, consider 
the versions whose bound parameter is larger than the true diameter d. For these versions, as mentioned 
above, we have the following property. Lemma 8 If 2i ~ d, then version i stabilizes in O(2i) time units. 
Now consider versions number i such that their bound parameter 2i is smaller than d. Perhaps sur­prisingly, 
these versions do not necessarily stabilize in time proportional to their bound parameter, and not even 
in time proportional to the true diameter. Their stabilization time can be as high as E)(n). To see this, 
consider the graph with initial parent assign­ment depicted in Figure 5 (a). (a) (b) Figure 5: An initial 
state for version i = O (a). The numbers in the nodes indicate their ID, and the ar­rows indicate their 
parent pointer. The stable jinai configuration is depicted in (b). Notice that with the given IDs assignment, 
the sta­ble configuration is the one depicted in Figure 5 (b). But since the center of this graph is 
occupied by a small ID, the information must propagate on the perimeter. It is straightforward to generalize 
this ex­ample to arbitrarily large n while keeping the diam­eter 4, and thus obtain a lower bound of 
Q(n) time on the stabilization time. (The O(n) upper bound follows from the fact that the sum of the 
heights of trees in any forest subgraph is O(n).) The crucial insight needed here is that actually we 
do not need lower versions to stabiiize. All we really need is that lower versions will know that they 
do not yield a tree that covers all the network. And this can be done fairly easily, using the following 
obser­vation: if a version number i satisfies 2i < d, then for any tree of version i, at all times, there 
is at least one node which is in the fringe of the tree, i.e., has a neighbor with a different alleged 
root. These fringe nodes can therefore detect that their tree does not cover the whole network. Hence, 
the only thing we need now is to let all other nodes of this tree know that version i is actually void. 
This can be done using the standard technique of broadcast on trees. More specifically, each node v maintains 
two bits, which we call d.coverv and u-coverV. Informally, the u-cover bit is used to propagate information 
up the tree, by taking repeated logical and of the u.cover bits of the children of the node, and the 
d-cover bit is used to propagate information down the tree, by copying the d-cover bit of the parent. 
Below, we give the formal rules for the d-cover and u-cover bits. We denote the the parent of v by parent., 
and the set of children of v by childu. Rule 8 1 if childv = 0 and 1, b u EN(v), ru = rV if child. # 
O andu.coverv e u.coveru A Vu EN(v), r. = r. uc chtldV 1 ( o, otherwise d-cove rpar.ntw, ifrw #v d-.coveru 
* { u-coverv 1 ifrv =V For Rule 8 we have the following lemmas. Lemma 9 If 2i < d, then afier O(2i) 
time units, at all nodes v, d-coverv = O for version i. Lemma 10 If 2i ~ d, then afier 0(2i) time units, 
at all nodes v, d-coverw = 1 for version i. We can now specify the complete protocol. We run log D + 
1 versions in parallel. Version i, O < i ~ logD, executes Rule 7 with bound parameter 27, and executes 
also Rule 8. Every version thus maintains its d-cover bit. A node v selects its output by finding the 
minimal i such that d-cover-v = 1 for version i. The tree edges of that version are the output of the 
combined protocol. The combination of Lemmas 8, 9, and 10 gives the following theorem. Theorem 4 In O(d) 
time units, the algorithm pro­duces a shortest paths tree rooted at the min~mal ID node in the network. 
As a final remark, let us point out again an interest­ing property of the algorithm. Although the output 
of the algorithm stabilizes after O(d) time units, the state of the algorithm does not. In particular, 
for the low versions (with 2 < d), full stabilization is guar­anteed to occur only after O(n) time; and 
for the high versions, the stabilization may take as long as O(D) time. But almost magically, the reievant 
portion of the system stabilizes in O(d) time. [AG90] [AKY90] [AP90] [APPS92] [APV91] [APV92] [AS88] 
[AV91] [Awe85] Anish Arora and Mohamed G. Gouda. Distributed reset. In Proc. 10th Conf. on Foundations 
of Sofiware Technology and Theoretical Computer Science, pages 316­ 331. Spinger-Verlag (LNCS 472), 1990. 
Yehuda Afek, Shay Kutten, and Moti Yung. Memory-efficient self-stabilization on general networks. In 
Proc. dth Work­shop on Distributed Algorithms, pages 15 28, Italy, September 1990. Springer-Verlag (LNCS 
486). Baruch Awerbuch and David Peleg. Net­work synchronization with polylogarith­mic overhead. In 31st 
Annual Sympo­sium on Foundations of Computer Sci­ence, 1990. Baruch Awerbuch, Boaz Patt-Shamir, David 
Peleg, and Mike Saks. Adapting to asynchronous dynamic networks. In Pro­ceedings of the 2dth Annual ACM 
Sympo­sium on Theory of Computing, pages 557 570, May 1992. Baruch Awerbuch, Boaz Patt-Shamir, and George 
Varghese. Self-stabilization by lo­cal checking and correction. In 32nd An­nual Symposium on Foundations 
of Com­puter Science, pages 268 277, October 1991. Baruch Awerbuch, Boaz Patt-Shamir, and George Varghese. 
Self-stabilizing network protocols. Unpublished manuscript, 1992. Baruch Awerbuch and Michael Sipser. 
Dy­namic networks are as fast as static net­works. In 29th Annual Symposium on Foundations of Computer 
Science, pages 206-220, October 1988. Baruch Awerbuch and George Vargh­ese. Distributed program checking: 
a paradigm for building self-stabilizing dis­tributed protocols. In 32nd Annual Sym­posium on Foundations 
of Computer Sci­ence, pages 258 267, October 1991. Baruch Awerbuch. Complexity of network synchronization. 
J. ACM, 32(4):804-823, October 1985. [Be158] [BP89] [Dij74] [DIM91] [LL84] [MRR80] [PU89] [SG89] [Var92] 
Richard Bellman. On a routing prob­lem. Quarterly of Applied Mathematics, 16(1):87-90, 1958. J.E. Burns 
and J. Pachl. Uniform self­stabilizing rings. ACM Transactions on Programming Languages and Systems, 
11(2):330-344, 1989. Edsger W. Dijkstra. Self stabilization in spite of distributed control. Comm. ACM, 
17:643-644, 1974. Shlomo Dolev, Amos Israeli, and Shlomo Moran. Uniform self-stabilizing leader election. 
In Proc. 5th Workshop on Dis­tributed Algorithms, pages 167 180, 1991. J. Lundelius and N. Lynch. An 
upper and lower bound for clock synchronization. Information and Control, 62(2-3):190-204, 1984. John 
McQuillan, Ira Richer, and Eric Rosen. The new routing algorithm for the ARPANET. IEEE Trans. Comm., 
28(5):711-719, May 1980. David Peleg and Jeffrey D, Unman. An optimal synchronizer for the hypercube. 
SIAM J. Comput., 18(2):740-747, 1989. John M. Spinelli and Robert G. Gal­lager. Broadcasting topology 
informa­tion in computer networks. IEEE Trans. Comm., May 1989. George Varghese. Self-Stabilization by 
Lo­cal Checking and Correction. PhD thesis, MIT Lab. for Computer Science, 1992.  
			