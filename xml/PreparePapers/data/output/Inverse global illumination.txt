
 Supplemental materials for this paper can be found in this directory. recovery process. The second 
problem we face is that in a real scene, surfaces will exhibit mutual illumination. Thus, the light that 
any particular sur­face receives will arrive not just from the light sources, but also from the rest 
of the environment through indirect illumination. As a result, the incident radiance of an observed surface 
is a complex function of the light sources, the geometry of the scene, and the as-yet-undetermined re.ectance 
properties of all of the scene s sur­faces. In this work, we use radiance data from photographs and image-based 
rendering to estimate the incident radiances of sur­faces in the scene. This allows us to estimate the 
re.ectance prop­erties of the surfaces in the scene via an iterative optimization pro­cedure, which allows 
us to re-estimate the incident radiances. We refer to this procedure as inverse global illumination. 
Addressing these two problems makes it possible to robustly re­cover re.ectance parameters from the limited 
radiance information present in a sparse set of photographs, and the accommodations made are appropriate 
for a wide variety of real scenes. Even when they are not met, the algorithm will compute the re.ectance 
prop­erty parameters that best .t the observed image data, which in many cases can still yield a visually 
acceptable result. The input to our algorithm is a geometric model of the scene, a set of radiance maps 
taken under known direct illumination, and a partitioning of the scene into areas of similar non-diffuse 
re­.ectance properties. The algorithm outputs a set of high-resolution albedo maps for the surfaces in 
the scene along with their specular re.ectance properties, yielding a traditional material-based model. 
This output is readily used as input to traditional rendering algo­rithms to realistically render the 
scene under arbitrary lighting con­ditions. Moreover, modi.cations to the scene s lighting and geom­etry 
and the addition of synthetic objects is easily accomplished using conventional modeling methods. Geometry 
Reflectance Properties  Radiance Maps Lighting Geometry Reflectance Properties  Radiance Maps Lighting 
Figure 1: Overview of the Method This .gure shows the relation­ship between global illumination and inverse 
global illumination. Global illumination uses geometry, lighting, and re.ectance prop­erties to compute 
radiance maps (i.e. rendered images), and inverse global illumination uses geometry, lighting, and radiance 
maps to determine re.ectance properties. 1.1 Overview The rest of this paper is organized as follows. 
In the next section we discuss work related to this paper. Section 3 describes inverse radiosity, a stepping 
stone to the full algorithm which considers diffuse scenes. Section 4 presents a technique for recovering 
spec­ular re.ectance properties for homogeneous surfaces considering direct illumination only. Section 
5 describes how these two tech­niques are combined to produce our inverse global illumination al­gorithm. 
Section 6 completes the technical discussion by describ­ing how high-resolution albedo maps are derived 
for the surfaces in the scene. Section 7 presents re.ectance recovery results from both real and synthetic 
data, a description of our data acquisition, and synthetic renderings which are compared to real photographs. 
Section 8 presents some conclusions and avenues for future work. 2 Background and Related Work The work 
we present in this paper has been made possible by previ­ous work in BRDF modeling, measurement and recovery, 
geometry acquisition, image-based rendering, and global illumination. In graphics, there is a long history 
of modeling surface re­.ectance properties using a small number of parameters. Recent ef­forts in this 
direction include models introduced in [14, 32, 25, 17]. These models have been shown to yield reasonable 
approximations to the re.ectance properties of many real materials, and they have been used to produce 
realistic renderings. On the other hand, considerable recent work has presented meth­ods for measuring 
and recovering the re.ectance properties of materials using imaging devices. [32] and [16] presented 
tech­niques and apparatus for measuring re.ectance properties, includ­ing anisotropic re.ection. [5] 
measured directional re.ectance properties of textured objects. [27] and [21] showed that diffuse and 
specular re.ectance properties could be recovered from multi­ple photographs of an object under direct 
illumination. [36] recov­ered re.ectance properties of isolated buildings under daylight and was able 
to re-render them at novel times of day. [7] estimated ma­terial properties of parts of a scene so that 
they could receive shad­ows and re.ections from synthetic objects. [10, 20] used a model of the scene 
and forward radiosity to estimate diffuse albedos to in­teractively modify the scene and its lighting. 
Although mutual illu­mination has been considered in the problem of shape from shading [23], it has not 
yet been fully considered for recovering non-diffuse re.ectance properties in real environments. A survey 
of some of the methods is in Marschner [21]. Certain work has shown that changing the lighting in a scene 
does not necessarily require knowledge of the surface re.ectance properties taking linear combinations 
of a large set of basis im­ages [24, 35] can yield images with novel lighting conditions. Recent work 
in laser range scanning and image-based model­ing has made it possible to recover accurate geometry of 
real-world scenes. A number of robust techniques for merging multiple range images into complex models 
are now available [34, 30, 4, 27]. For architectural scenes involving regular geometry, robust pho­togrammetric 
techniques requiring only photographs can also be employed. The model used in this research was constructed 
using such a technique from [9]; however, our basic technique can be used regardless of how the geometry 
is acquired. Work in global illumination (e.g. [11, 15, 31, 37]) has produced algorithms and software 
to realistically simulate light transport in synthetic scenes. In this work we leverage the hierarchical 
subdi­vision technique [13, 1] to ef.ciently compute surface irradiance. The renderings in this paper 
were produced using Gregory Ward Larson s RADIANCE system [33]. Photographs taken by a camera involve 
nonlinearities from the imaging process, and do not have the full dynamic range of real world radiance 
distributions. In this work we use the high dynamic range technique in [8] to solve these problems. 
3 Inverse Radiosity Most real surfaces exhibit specular as well as diffuse re.ection. Re­covering both 
diffuse and specular re.ectance models simultane­ously in a mutual illumination environment is complicated. 
In this section, we consider a simpli.ed situation where all surfaces in an environment are pure diffuse 
(Lambertian). In this case, the global illumination problem simpli.es considerably and can be treated 
in (a) (b) (c) Figure 2: (a) The lighting and viewing directions at different points on a surface are 
different with respect to a .xed light source and a .xed viewpoint. This fact can be used to recover 
a low-parameter BRDF model for the surface from a single image. n s and Hi s are the normals . and halfway 
vectors between lighting and viewing directions at different locations on the surface. We can infer that 
surface point P. with normal n.is close to the center of the highlight, and point P. with normal n.is 
relatively far away from the center. (b) An example of an isotropic specular highlight, (c) An example 
of an anisotropic specular highlight. the radiosity framework [28]. We de.ne inverse radiosity as recov­ering 
the diffuse albedo at each surface patch in the environment, provided that the geometry, the lighting 
conditions and the radiance distribution in the scene are known. In the next section we will discuss 
another simple case recovering more general re.ectance models with specularity considering only direct 
illumination and we address the full problem in Section 5. In the radiosity framework [28], the surfaces 
in the environment are broken into a .nite number of patches. The partitioning is as­sumed to be .ne 
enough that the radiosity and diffuse albedo of each patch can be treated as constant. For each such 
patch, . Bi Ei PiBjFij (1) j where Bi, Ei,and Piare the radiosity, emission, and diffuse albedo, respectively, 
of patch i,and Fijis the form-factor between patches iand j. The form-factor Fijis the proportion of 
the total power leaving patch ithat is received by patch j. It can be shown that this is a purely geometric 
quantity which can be computed from the known geometry of the environment [28]. We take photographs of 
the surfaces, including the light sources, and use a high dynamic range image technique [8] to capture 
the radiance distribution. Since Lambertian surfaces have uniform di­rectional radiance distributions, 
one camera position is suf.cient for each surface. Then Biand Eiin Eqn. (1) become known. Form­factors 
Fijcan be derived from the known geometry. Once these . are done, Pi(Bi-Ei)I(jBjFij). The solution to 
inverse radiosity is so simple because the photographs capture the .nal so­lution of the underlying light 
transport among surfaces. 4 Recovering Parameterized BRDFs from Direct Illumination Before tackling 
the general case of re.ectance recovery from pho­tographs of mutually illuminated surfaces with diffuse 
and specular components, we study another special case. Consider a single sur­face of uniform BRDF which 
is illuminated by a point light source in known position and photographed by a camera, also in a known 
geometric position with respect to the surface(Fig. 2). Every pixel in the radiance image provides a 
measurement of radiance Liof the corresponding surface point Piin the direction of the camera, and the 
known light source position lets us calculate the irradiance Ii incident on that point. Our objective 
is to use these data (Li,Ii)to estimate the BRDF of the surface. Since the BRDF is a function of four 
variables (az­imuth and elevation of incident and viewing directions) it is obvi­ous that the 2-dimensional 
set of measurements for a single cam­era/light source pairing is inadequate to do this in general. How­ever 
for many materials it is possible to approximate the BRDF adequately by a parameterized BRDF model with 
a small number of parameters (e.g. Ward [32], Lafortune [17], He [14] etc). We use Ward s parameterization 
in which the BRDF is modeled as the sum of a diffuse term e:and a specular term PsK(c,0).Here Pdand Psare 
the diffuse and specular re.ectance of the surface, re­spectively, and K(c,0)is a function of vector 
0, the azimuth and elevation of the incident and viewing directions, and parameterized by c, the surface 
roughness vector. For anisotropic surfaces chas 3 components; for isotropic surfaces chas only one component 
and reduces to a scalar. The precise functional form of K(c,0)in the two cases may be found in Appendix 
1. This leads us to the following equation for each surface point Pi, Pd Li( PsK(c,0i))Ii (2) where 
Li, Iiand 0iare known, and the parameters Pd,Ps, care unknowns to be estimated. Depending on whether 
we are using an isotropic or anisotropic model for the specular term we have a total of 3 or 5 unknown 
parameters, while there are as many constrain­ing equations as the number of pixels in the radiance image 
of the surface patch. By solving a nonlinear optimization problem (see Appendix 1 for details), we can 
.nd the best estimate of Pd,Ps, c. There are two important subtleties in the treatment of this op­timization 
problem. One is that we need to solve a weighted least squares problem, otherwise the larger values from 
the high­light (with correspondingly larger noise in radiance measurements) cause a bias in parameter 
estimation. The second is the use of color information which needs to be done differently for dielectrics 
and metals. Both of these issues are discussed in Appendix 1. To obtain an obvious global minimum for 
this optimization prob­lem and achieve robust parameter recovery, the radiance image should cover the 
area that has a specular highlight as well as some area with very low specular component. If the highlight 
is missing, we do not have enough information for recovering specular param­eters, and can only consider 
the surface to be diffuse. 5 Recovering Parameterized BRDFs in a Mutual Illumination Environment We 
are now ready to study the general case when the environment consists of a number of surfaces and light 
sources with the surface re.ectances allowed to have both diffuse and specular components. Consider a 
point Pion a surface patch seen by camera Cv(Fig. 3). The radiance from Piin the direction of the camera 
is the re­ A j j  Figure 3: Patch Ajis in the radiance image captured by camera Ck. The specular 
component at Ajin the direction of sample point Pi is different from that in the direction of camera 
Ck. The difference is denoted by /S. .ection of the incident light contributed by all the light sources 
as well as all the surrounding surfaces. Eqn. (2) generalizes to . PdLp;Aj LCvp;ECvp;jFp;Aj (3) . Ps 
, jLp;AjKCvp;Aj where LCvp;is the radiance value in the direction of camera Cv at some sample point 
Pion the surface, ECvp;is the emission in the direction of camera Cv, Lp;Ajis the radiance value along 
the direction from patch Ajto point Pion the surface, Fp;Ajis the analytical point-to-patch form-factor 
[2] between sample point Pi and patch Aj,and PsKCvp;Ajis the specular term evaluated at Pi for a viewpoint 
at camera Cvand a light source position at patch Aj. The arguments, cand 0,of Khave been dropped to simplify 
notation. As before, our objective is to estimate Pd, Ps, and specular roughness parameters c. Of the 
other variables in Eqn. (3), ECvp; for nonsources, and LCvp;can be measured directly from the radiance 
image at camera Cv. In general, the radiances Lp;Ajcannot be measured directly but have to be estimated 
iter­atively. Suppose patch Ajin the environment appears in another radiance image taken by camera Ck(Fig. 
3). Only if we assume Aj is Lambertian, does Lp;Ajin Eqn. (3) equal LCkAj, the radiance from Ajto camera 
Ck. Otherwise, the diffuse components will be equal, but the specular components will differ. Lp;AjLCkAj/SCkp;Aj 
(4) Here /SCkp;AjSp;Aj -SCkAjis the difference between the specular components Sp;Ajand SCkAjof the 
radiances in the two directions. To compute the specular differences /SCkp;Aj,we need the BRDF of Aj, 
which is initially unknown. The estima­tion of /S(Section 5.1) therefore has to be part of an iterative 
framework. Assuming that the dominant component of re.ectance is diffuse, we can initialize the iterative 
process with /S (this sets Lp;AjLCkAj). To recover BRDF parameters for all the surfaces, we need radi­ance 
images covering the whole scene. Each surface patch needs to be assigned a camera from which its radiance 
image is selected. At least one specular highlight on each surface needs to be visible in the set of 
images, or we will not be able to recover its specular re.ectance and roughness parameters. Each sample 
point gives an For each camera position C For each polygon T For each light source O Obtain the intersection 
P between plane of T and line CO (O and O are symmetric about T); Check if P falls inside polygon T; 
Check if there is any occlusion between P and O; Check if there is any occlusion between C and any point 
 in a local neighborhood of P; /* A highlight area is detected if P passed all the above tests.*/ End 
Figure 4: The specular highlight detection algorithm. equation similar to Eqn. (3). From these equations, 
we can set up a weighted least-squares problem for each surface as in Appendix 1. During optimization, 
we need to gather irradiance at each sample point from the surface patches in the environment. One ef.cient 
way of doing this is to subdivide each surface into a hierarchy of patches [13, 1] and link different 
sample points to patches at differ­ent levels in the hierarchy. The solid angles subtended by the linked 
patches at the sample points should always be less than a prescribed threshold. There is a radiance value 
from the patch to the sample point and a /Sassociated with each hierarchical link. For each sample point, 
we build hierarchical links to a large number of patches, and gather irradiance from these links. The 
amount of memory and computation involved in this process limits the number of samples for each highlight 
area. To make a rea­sonable tradeoff, we note that irradiance from indirect illumination caused by surrounding 
surfaces generally has little high-frequency spatial variation. Because of this, it makes sense to draw 
two sets of samples, one sparse set, and one dense set 2. For the samples in the sparse set, we build 
hierarchical links and gather irradiance from the environment as usual. For the samples in the dense 
set, only their irradiance from light sources is computed explicitly, their irradiance from indirect 
illumination is computed by interpolation. We are now ready to state the complete inverse global illumi­nation 
algorithm. First detect all specular highlight blobs falling inside the radiance images using knowledge 
of the positions of the light sources, the camera poses, and the geometry (Fig. 4). Set the initial /Sassociated 
with each hierarchical link to zero. We can then recover an initial estimate of the BRDF parameters for 
each surface independently by solving a series of nonlinear optimization problems. The estimated specular 
parameters are used to update all /S s and Lp;Aj s associated with the hierarchical links. With the updated 
incident radiances, we can go back and re-estimate the BRDF parameters again. This optimization and update 
process is iterated several times to obtain the .nal solution of the BRDFs for all surfaces. The overall 
algorithm is shown in Fig. 5. 5.1 Estimation of .5 Suppose there is a hierarchical link lp;Ajbetween 
a sample point Piand a patch Ajwhich is visible to a camera Ck(Fig. 6). The /S for lp;Ajis de.ned to 
be the difference of the specular component in directions ACjPiand AjCCk. To estimate this difference, 
we need to obtain the specular component along these two directions given the BRDF parameters of patch 
Aj. A one-bounce approximation of /Sfor link lp;Ajcan be obtained by using Monte Carlo ray­tracing [32]. 
Because of off-specular components, multiple rays 2We choose the two sets of samples as follows. We .rst 
.nd the center of the highlight area in the image plane and rotate a straight line around this center 
to a number of different positions. The dense set of samples is the set of points on the surface corresponding 
to all the pixels on these lines. We choose the sparse set of samples on each line by separating two 
consecutive samples by some .xed distance in the object space. Detect specular highlight blobs on the 
surfaces. Choose a set of sample points inside and around each highlight area. Build hierarchical links 
between sample points and patches in the environment and use ray tracing to detect occlusion. Assign 
to each patch one radiance image and one average radiance value captured at the camera position. Assign 
zero to /Sat each hierarchical link. For iter=1toN For each hierarchical link, use its /Sto update its 
associated radiance value; For each surface, optimize its BRDF parameters using the data from its sample 
points; For each hierarchical link, estimate its /Swith the new BRDF parameters. End Figure 5: The Inverse 
Global Illumination algorithm.  Figure 6: Random rays are traced around the two cones to obtain a one-bounce 
approximation of /S. should be traced and the direction of the rays is randomized around the mirror directions 
of ACjPiand AjCCk, respectively. For each possible ray direction, the probability density of shooting 
a ray in that direction is proportional to K(cj,0)where 0encodes the incident and outgoing directions. 
Intuitively, most of the rays fall inside the two cones Qp;Ajand QCkAjcentered at the two mir­ror directions. 
The width of each cone depends on the specular roughness parameters cjof patch Aj. The radiance along 
each ray is obtained from the patch hit by the ray. Suppose LQP;and Aj LQCkAjare the average radiance 
values of the rays around the two cones, respectively, and PsAjis the specular re.ectance of patch Aj. 
Because the average value of Monte Carlo sampling approxi­mates the total irradiance modulated by K(cj,0), 
/Scan simply be estimated as PsAj(LQP;-LQCk). This calculation could Aj Aj be extended to have multiple 
bounces by using path tracing [15]; we found that the one-bounce approximation was adequate for our purposes. 
 5.2 Practical Issues We do not have a formal characterization of the conditions under which the inverse 
global illumination algorithm converges, or of error bounds on the recovered BRDF parameter values. In 
practice, we found it worked well (Section 7). Here we give some heuristic advice on how to acquire images 
to obtain good performance. Use multiple light sources. A specular highlight directly caused by one of 
the light sources should be captured on each surface. Having multiple light sources increases the probabil­ity 
that this can be achieved, and lets the whole scene receive more uniform illumination. This also increases 
the relative contribution of the diffuse component at any particular sam­ple point Pi, and supports the 
/Sinitialization, since highlights from different sources will usually occur at differ­ent locations 
on the surface. Use concentrated light sources. If the incoming radiance dis­tribution is not very directional, 
the specular highlights will be quite extended and it will be dif.cult to distinguish the spec­ular component 
from the diffuse one.  6 Recovering Diffuse Albedo Maps In the previous sections, we modeled the re.ectance 
properties as being uniform for each surface. In this section, we continue to do so for specular parameters 
because a small number of views of each surface does not provide enough information to reliably estimate 
specular parameters for each point individually. However, we relax this constraint on diffuse albedo 
and model it as a spatially varying function, an albedo map, on each surface. The diffuse albedo for 
any point xon a surface is computed as: Pd(x)D(x)II(x) (5) where Pd(x)is the diffuse albedo map, D(x)is 
the diffuse radiance map, and I(x)is the irradiance map. Suppose there is an image covering the considered 
surface which gives a radiance map L(x)D(x)S(x)where S(x)is the spec­ular radiance map seen from the 
image s camera position. Then the diffuse radiance map in Eqn. (5) can be obtained by subtracting the 
specular component from each pixel of the radiance map L(x) using the specular re.ectance parameters 
already recovered. We estimate the radiance due to specular re.ection as the sum of spec­ular re.ection 
due to direct and indirect illumination. The specular re.ection due to direct illumination is computed 
from the knowl­edge of the direct lighting and the estimated re.ectance properties, and we estimate the 
indirect specular re.ectance by tracing a per­turbed re.ected ray into the environment in a manner similar 
to that in Section 5.1. The irradiance I(x)can be computed at any point on the surface from the direct 
illumination and by using analytical point-to-patch form-factors [2] as in previous sections of this 
paper. For ef.ciency, we compute the irradiance due to the indirect illumination only at certain sample 
points on the surfaces, and interpolate these indirect irradiance estimates to generate estimates for 
all surface points x. Of course, care must be taken to suf.ciently sample the irradiance in regions of 
rapidly changing visibility to the rest of the scene. Something that complicates estimating diffuse albedos 
in this manner is that in highlight regions the specular component of the re.ectance S(x)will be much 
larger than the diffuse component D(x). As a result, relatively small errors in the estimated S(x)will 
cause large relative errors in D(x)and thus Pd(x).However, just as a person might shift her view to avoid 
glare while reading a movie poster, we make use of multiple views of the surface to solve this problem. 
Suppose at a point xon a surface, we have multiple radiance val­ues {Lk(x)}.from different images. The 
highest value in this k.. set will exhibit the strongest specular component, so we simply re­move this 
value from consideration. For the remaining values, we subtract the corresponding specular estimates 
Sk(x)from the ra­diance values Lk(x), to obtain a set of diffuse radiance estimates Dk(x). We compute 
a .nal diffuse radiance component D(x)as a weighted average of the Dk(x), with weights inversely proportional 
to the magnitude of the estimated specular components Sk(x)to minimize the relative error in D(x). We 
also weight the Dk(x) values proportionally to the cosine of the viewing angle of the cam­era in order 
to reduce the in.uence of images at grazing angles; such oblique images typically have poor texture resolution 
and ex­hibit particularly strong specular re.ection. Since we are combin­ing information taken from different 
images, we smooth transitions at image boundaries using the image blending technique in [9]. Once diffuse 
albedo maps are recovered, they could be used to separate the diffuse and specular components in the 
specular high­light areas. This would allow recovering more accurate specular pa­rameters in the BRDF 
model. In practice, however, we have found good estimates to be obtained without further re.nements. 
 7 Results 7.1 Results for a Simulated Scene We .rst tested our algorithm on a simple simulated cubical 
room with mutual illumination. This allowed us to verify the accuracy of the algorithm and compare its 
results to ground truth. All the six surfaces of the room have monochromatic diffuse and specular components, 
but each one has a distinct set of parameters. Each of the surfaces has spatially uniform specularity. 
We assigned two sur­faces to be anisotropically specular and added 10-20% zero mean white noise to the 
uniform diffuse albedo of two surfaces to sim­ulate spatial variations. We used the RADIANCE rendering 
sys­tem [33] to produce synthetic photographs of this scene. Six of the synthetic photographs were taken 
from the center of the cube with each one covering one of the six surfaces. Another set of six zoomed-in 
photographs were taken to capture the highlight areas. The scene was illuminated by six point light sources 
so that specu­lar highlights could be observed on each surface. These twelve im­ages along with the light 
source intensity and positions were used to solve the BRDF parameters. The images of the specular high­lights 
are shown in Fig. 7. Some of the highlights are visually very weak, but corresponding parameters can 
still be recovered numer­ically. The original and recovered BRDF parameters are given in Table 1. For 
the last two surfaces with noisy diffuse albedo, the recovered albedo values are compared to the true 
average values. The total running time for BRDF recovery is about half an hour on aSGI O. 180MHz workstation. 
The numerical errors shown in Table 1 are obtained by com­paring the recovered parameters with the original 
ones. There are three sources of error: BRDF modeling error, rendering error, and BRDF recovery error. 
BRDF modeling error comes from the in­ability of a given BRDF model to capture the behavior of a real 
material. By using the same model for recovery that RADIANCE uses for rendering, BRDF modeling error 
was eliminated for this test. However, because RADIANCE computes light transport only approximately, 
rendering error is present. We thus cannot deter­mine the exact accuracy of our BRDF recovery. However, 
the test demonstrates that the algorithm works well in practice.  7.2 Results for a Real Scene In this 
section we demonstrate the results of running our algorithm on a real scene. The scene we chose is a 
small meeting room with some furniture and two whiteboards; we also decorated the room with colored cards, 
posters, and three colored metallic spheres3. Once the BRDFs of the materials were recovered, we were 
able to re-render the scene under novel lighting conditions and with added virtual objects. 3The spheres 
were obtained from Baker s Lawn Ornaments, 570 Berlin Plank Road, Somerset PA 15501, (814) 445-7028. 
True Recovered P: 0.3 0.318296 Pa 0.08 0.081871 ax(a)0.6 0.595764 au " 0.03 0 0.030520 -0.004161 Error(%) 
6.10 2.34 0.71 1.73 True 0.1 0.1 0.3 Recovered 0.107364 0.103015 0.300194 Error(%) 7.36 3.02 0.06 
True 0.1 0.01 0.1 Recovered 0.100875 0.010477 0.101363 Error(%) 0.88 4.77 1.36 True 0.3 0.02 0.15 
 Recovered 0.301775 0.021799 0.152331 Error(%) 0.59 8.90 1.55 True 0.2 0.05 0.05 Recovered 0.206312 
0.050547 0.050291     Error(%) 3.16 1.09 0.58    True 0.2 0.1 0.05 0.3 45 Recovered 0.209345 
0.103083 0.050867 0.305740 44.997876 Error(%) 4.67 3.08 1.73 1.91 Table 1: Comparison between true and 
recovered BRDF parame­ters for the six surfaces of a unit cube. The .rst and last surfaces have anisotropic 
specular re.ection. They have two more parame­ters: second roughness parameter Cyand the orientation 
Iof the principal axes in a local coordinate system. The errors shown are the combined errors from both 
rendering and recovering stages. 7.2.1 Data Acquisition We illuminated the scene with three heavily frosted 
3-inch diam­eter tungsten light bulbs. Using high dynamic range photography, we veri.ed that the lights 
produced even illumination in all direc­tions. A DC power source was used to eliminate 60Hz intensity 
.uctuations from the alternating current power cycle. We used a Kodak DCS520 color digital camera for 
image acqui­sition. The radiance response curve of the camera was recovered using the technique in [8]. 
We used a wide-angle lens with a 75 degree .eld of view so that we could photograph all the surfaces 
in the scene from a few angles with a relatively small number of shots. Forty high dynamic range radiance 
images, shown in Fig. 8, were acquired from approximately 150 exposures. Twelve of the images were taken 
speci.cally to capture specular highlights on surfaces. The radiance images were processed to correct 
for radial light falloff and radial image distortion. Each of these corrections was modeled by .tting 
a polynomial of the form ar.br.to cali­bration data captured with the same lens settings used for the 
scene images. To reduce glare and lens .are, we shaded the lens from directly viewing the light sources 
in several of the images. Re­gions in the images corresponding to the light stands (which we did not 
model) or where excessive remaining glare was apparent were masked out of the images, and ignored by 
the algorithm. The thin cylindrical light stands which appear in the synthetic render­ings have been 
added to the recovered model explicitly. The radiance images were used to recover the scene geometry 
and the camera positions (Fig. 9) using the Fac¸ade [9] modeling system. Segmentation into areas of uniform 
specular re.ectance was obtained by having each polygon of each block in the model (e.g. the front of 
each poster, the surface of each whiteboard, the top of each table) have its own uniform specular re.ectance 
parameters. The positions and intensities of the three light sources were re­covered from the .nal three 
radiance images. During BRDF re­covery, the area illumination from these spherical light sources was 
computed by stochastically casting several rays to each source.  7.2.2 BRDF Recovery Given the necessary 
input data, our program recovered the surface BRDFs in two stages. In the .rst stage, it detected all 
the high­light regions and recovered parametrized BRDFs for the surfaces. In this stage, even if a surface 
had rich texture, only an average dif­  Figure 7: Synthetic grey-scale images of the interior of a unit 
cube in the presence of mutual illumination. These are used for recovering the BRDF model of each surface. 
The top row shows the six images taken at the center of the cube with each one covering one of the six 
surfaces. The bottom row shows the six zoomed-in images taken to capture one specular highlight area 
on each surface. The .rst and last surfaces have anisotropic specular re.ection. The last two surfaces 
have 20 and 10 percent zero mean white noise added to their diffuse albedo, respectively. Pd(red) Pd(green) 
Pd(blue) Ps(red) Ps(green) Ps(blue) C whiteboard 0.5794 0.5948 0.6121 0.0619 0.0619 0.0619 0.0137 roundtable 
top 0.7536 0.7178 0.7255 0.0366 0.0366 0.0366 0.0976 door 0.6353 0.5933 0.5958 0.0326 0.0326 0.0326 0.1271 
wall 0.8543 0.8565 0.8036 0.0243 0.0243 0.0243 0.1456 poster 0.1426 0.1430 0.1790 0.0261 0.0261 0.0261 
0.0818 red card 0.7507 0.2404 0.3977 0.0228 0.0228 0.0228 0.0714 yellow card 0.8187 0.7708 0.5552 0.0312 
0.0312 0.0312 0.1515 teal card 0.4573 0.5951 0.5369 0.0320 0.0320 0.0320 0.1214 lavender card 0.3393 
0.3722 0.4437 0.0077 0.0077 0.0077 0.1144 red ball 0 0 0 0.5913 0.1862 0.3112 0 green ball 0 0 0 0.2283 
0.3694 0.3092 0 blue ball 0 0 0 0.2570 0.3417 0.4505 0 Table 2: BRDF parameters recovered for the materials 
in the test room. All of them are isotropic, and most of them are plastic. The balls are metallic. fuse 
albedo was recovered. Surfaces for which no highlights were visible the algorithm considered diffuse. 
The second stage used the recovered specular re.ection models to generate diffuse albedo maps for each 
surface by removing the specular components. The running time for each of the two stages was about 3 
hours on a Pentium II 300MHz PC. The results show our algorithm can recover accurate specular models 
and high-quality diffuse albedo maps. Fig. 10 shows how specular highlights on the white board were removed 
by combining the data from multiple images. Fig. 11 shows the albedo maps obtained for three identical 
posters placed at different places in the room. Although the posters were originally seen in different 
illumination, the algorithm successfully recovers very similar albedo maps for them. Fig. 12 shows that 
the algorithm can remove color bleeding effects: colors re.ected onto a white wall from the cards on 
the table do not appear in the wall s diffuse albedo map. Table 2 shows the recovered specular parameters 
and average diffuse albedo for a variety of the surfaces in the scene. We indicated to the program that 
all the materials are isotropic, and that the metallic spheres only have ideal specular components4. 
4For surfaces that have only ideal specular re.ection, such as mirrors, there is no diffuse component 
and the roughness parameter is zero. We can still recover their specular re.ectance .sfrom a single image 
by noting that the specular re.ectance can be computed as the simple ratio between two radiance values. 
One is the radiance value in the image corresponding to the intersection between the surface and a ray 
shot from the camera position; the other is the radiance value of the environment along the re.ected 
ray. In practice, we shoot a collection of rays from the camera position to obtain the average re.ectance. 
 7.2.3 Re-rendering Results We directly compared synthetic images rendered with our recov­ered BRDF 
models to real images. In Fig. 13, we show the com­parison under the original lighting conditions in 
which we took the images for BRDF recovery. In Fig. 14, we show the comparison under a novel lighting 
condition obtained by removing two of the lights and moving the third to a new location, and adding a 
new object. There are a few differences between the real and synthetic images. Some lens .are appears 
in the real images of both .gures, which we did not attempt to simulate in our renderings. We did not 
model the marker trays under the whiteboards, so their shad­ows do not appear in the synthetic images. 
In Fig. 14, a synthetic secondary highlight caused by specular re.ection from the adjacent whiteboard 
appears darker than the one in the real image, which is likely due to RADIANCE s approximations for rendering 
sec­ondary specularities. However, in both .gures, real and synthetic images appear quite similar. Fig 
15 shows four panoramic views of the rendered scene. (a) shows the hierarchical mesh with the initial 
estimates of radiance obtained from the images. (b) shows the entire room rendered in the original illumination. 
(c) shows the entire scene rendered with novel lighting. The original lights were removed and three track 
lights were virtually installed on the ceiling to illuminate the posters. Also, a strange chandelier 
was placed above the spheres on the table. The new lights re.ect specularly off of the posters and the 
table. Since the chandelier contains a point light source, it casts a hard shadow around the midsection 
of the room. The in­terior of the chandelier shade is turquoise colored which results in turquoise shadows 
under the spheres. A small amount of synthetic glare was added to this image. (d) shows the result of 
adding syn­thetic objects to various locations in the room, including two chairs, a crystal ball, two 
metal boxes, and a .oating diamond. In addition, a very large orange sculpture, was placed at the back 
of the room. All of the objects exhibit proper shadows, re.ections, and caustics. The sculpture is large 
enough to turn the ceiling noticeably orange due to diffuse interre.ection. The video for this paper 
shows a .y­through of each of these scenes.  8 Conclusions and Future Work In this paper we have presented 
a new technique for determining re.ectance properties of entire scenes taking into account mutual illumination. 
The properties recovered include diffuse re.ectance that varies arbitrarily across surfaces, and specular 
re.ectance pa­rameters that are constant across regions. The technique takes as input a sparse set of 
geometrically and photometrically calibrated photographs taken under calibrated lighting conditions, 
as well as a geometric model of the scene. The algorithm iteratively estimates irradiances, radiances, 
and re.ectance parameters. The result is a characterization of surface re.ectance properties that is 
highly con­sistent with the observed radiances in the scene. We hope this work will be a useful step 
towards bringing visual spaces from the real world into the virtual domain, where they can be visualized 
from any angle, with any lighting, and with additions, deletions, and modi.cations according to our needs 
and imaginations. There are a few directions for future research. We wish to apply our technique to more 
general geometrical and photometric data, such as multispectral radiance images and geometry accquired 
from laser scanners. It would be of signi.cant practical value to be able to calibrate and use existing 
or natural illumination in recovering re.ectance properties. The algorithm should be more robust to er­rors 
in the geometric model, misregistration of the photographs, and errors in the light source measurements. 
It would also be of theoretical value to obtain conditions under which the algorithm converges. Acknowledgments 
The authors wish to thank David Culler and the Berkeley NOW (Network of Worksta­tions, http://now.cs.berkeley.edu/) 
project, and Tal Gar.nkel for his help in using the NOW to render the video sequences. Thanks to Gregory 
Ward Larson for advice in us­ing RADIANCE and estimating re.ectance, Carlo S´equin for providing the 
sculpture model, and the reviewers for their valuable comments. This research was supported by a Multidisciplinary 
University Research Initiative on three dimensional direct visu­alization from ONR and BMDO, grant FDN00014-96-1-1200, 
the California MICRO program, Phillips Corporation, Interval Research Corporation, Pixar Animation Stu­dios 
and Microsoft Graduate Fellowship.  Appendix 1. BRDF Model and Parameter Recovery In this appendix we 
present more details on the BRDF model, introduced in Section 4, and how its parameters are recovered. 
We use Ward s [32] model for the specular term in the BRDF, which could be modeled as either isotropic 
or anisotropic. In the isotropic case, exp[-tan2Æa2]K(a,0) (6) 2 r=se;=ser4-a where ais a scalar surface 
roughness parameter, e;is the incident angle, eris the viewing angle, and Æis the angle between the surface 
normal and the halfway vector Hbetween the lighting and viewing directions. e;, erare two components 
(along with <;, <r) of the vector 0which represents the incidence and viewing directions. In the anisotropic 
case, we need two distinct roughness parameters ax, aufor two principal axes on the surface and an azimuth 
angle "to de.ne the orientation of these principal axes on the surface relative to a canonical coordinate 
system. Then, the parameter vector .actually has three components (ax,au,")and we have: 22 exp[-tan2Æ(=s2< 
ax s n2< au)]K(.,0)r =se;=ser 4-axau (7) where Æis the same as in the isotropic case, and <is the azimuth 
angle of the halfway vector Hprojected into the local 2D coordinate system on the surface patch de.ned 
by the two principal axes. To compute <, ", which relates this coordinate system to the canonical coordinate 
system, is necessary. Now to parameter recovery. We wish to .nd P:, Paand .that minimize the squared 
error between the measured and predicted radiance, .P: 2e(P:,Pa,.)(L;-I;-PaK(.,0;)I;)(8) - ;=l where 
L;is the measured radiance and I;is the irradiance (computable from the known light source position) 
at sample point P;on the surface, and mis the number of sample points. Note that given a guess of ., 
K(.,0;)becomes a known quantity, and mini­mizing the error ereduces to a standard linear least-squares 
problem for estimating P: and Pa. Plugging in these values in the right hand side of Eqn. (8) lets us 
compute eas a function of .. The optimization problem thus simpli.es to a search for the optimum value 
of .to minimize e(.). This is either a one-dimensional or three-dimensional search depending on whether 
an isotropic or anisotropic model of the specular term is being used. We use golden section search [26] 
for the isotropic case, and the down­hill simplex method [26] in the anisotropic case. It is convenient 
that neither method requires evaluating the derivative e.(.), and both methods are fairly robust. To 
deal with colored materials, we estimate both diffuse and specular re.ectance in each of the red, green, 
blue color channels. The specular roughness parameters . are the same for all color channels. The nonlinear 
optimization is still over 1 or 3 parameters, since given ., P:and Paestimation for each channel remains 
a linear least squares problem. To make the parameter estimation additionally robust, we make two simple 
exten­sions to the basic strategy derived above. The .rst is to solve a weighted least squares problem 
instead of the vanilla version in Eqn. (8). Radiance measurements from the highlight area have much larger 
magnitude than those from the non-highlight area. Correspondingly the error in those measurements is 
higher both because of noise in imaging as well as error in the BRDF model. Giving all the terms in (8) 
equal weight causes biased .tting and gives poor estimation of the diffuse re.ectance. From a sta­tistical 
point of view, the correct thing to do is to weight each term by the reciprocal of the variance of expected 
error in that measurement. Not having a good model for the error term, we chose a heuristic strategy 
in which the weight W;for the i-th term l in the summation in Eqn. (8) is set to where .is some ad hoc 
or iter­ J(..9;)atively improved roughness vector. Since the roughness of most isotropic materials is 
less than 0.2, we used an initial value between 0.1 and 0.2 for scalar a. The second re.nement to improve 
parameter recovery is to use specular color in­formation. For instance, specular highlights on dielectric 
and plastic materials have the same color as the light source, while the color of specular highlights 
on metals is the same as their diffuse components, which is the color of the light modulated by the dif­fuse 
albedo. For plastic objects, there would be one distinct variable P:for each color channel, but the same 
variable Pafor all color channels. For metallic objects, there would be one variable P:for each channel 
and a common ratio between the specular and diffuse re.ectance in all channels. Thus, we can reduce the 
degree of freedom from 2Nto N+1 where Nis the number of color channels. For plastic, we can still obtain 
both analytic and numerical linear least-squares solutions for the N+1 vari­ables provided the other 
parameters are .xed. The program performs a heuristic test to determine whether a material should be 
estimated with the metal or plastic specular re.ectance model. Our program .rst solves for the specular 
re.ectance of each color channel separately and then checks to see if they are larger than the estimated 
diffuse components. If they are larger, then the material is considered metallic. Otherwise, the plastic 
model is used. Then the smaller number of parameters corresponding to these material types are solved. 
 References [1] AUPPERLE, L., AND HANRAHAN, P. A hierarchical illumination algorithm for surfaces with 
glossy re.ection. In SIGGRAPH 93 (August 1993), pp. 155 164. [2] BAUM,D. R., RUSHMEIER, H. E., AND WINGET, 
J. M. Improving radiosity solutions through the use of analytically determined form factors. In SIGGRAPH 
89 (1989), pp. 325 334. [3] CHEN, E. QuickTime VR -an image-based approach to virtual environment navigation. 
In SIGGRAPH 95 (1995). [4] CURLESS,B., AND LEVOY, M. A volumetric method for building complex models 
from range images. In SIGGRAPH 96 (1996), pp. 303 312. [5] DANA,K. J., GINNEKEN,B., NAYAR,S. K., AND 
KOENDERINK,J. J. Re­.ectance and texture of real-world surfaces. In Proc. IEEE Conf. on Comp. Vision 
and Patt. Recog. (1997), pp. 151 157. [6] DEBEVEC,P.,YU,Y., ANDBORSHUKOV,G.Ef.cientView-DependentImage-Based 
Rendering with Projective Texture-Mapping. In 9th Eurographics Work­shop on Rendering, (1998), pp. 105-116. 
[7] DEBEVEC, P. Rendering synthetic objects into real scenes: Bridging traditional and image-based graphics 
with global illumination and high dynamic range pho­tography. In SIGGRAPH 98 (July 1998). [8] DEBEVEC, 
P. E., AND MALIK, J. Recovering high dynamic range radiance maps from photographs. In SIGGRAPH 97 (August 
1997), pp. 369 378. [9] DEBEVEC, P. E., TAYLOR,C. J., AND MALIK, J. Modeling and rendering architecture 
from photographs: A hybrid geometry-and image-based approach. In SIGGRAPH 96 (August 1996), pp. 11 20. 
[10] DRETTAKIS,G., ROBERT, L., AND BOUGNOUX, S. Interactive common il­lumination for computer augmented 
reality. In 8th Eurographics workshop on Rendering, St. Etienne, France (May 1997), J. Dorsey and P. 
Slusallek, Eds., pp. 45 57. [11] GORAL,C. M., TORRANCE, K. E., GREENBERG,D. P., AND BATTAILE,B. Modeling 
the interaction of light between diffuse surfaces. In SIGGRAPH 84 (1984), pp. 213 222. [12] GORTLER,S. 
J., GRZESZCZUK,R., SZELISKI,R., AND COHEN,M. F. The Lumigraph. In SIGGRAPH 96 (1996), pp. 43 54. [13] 
HANRAHAN,P., SALZMAN,P., AND AUPPERLE, L. A rapid hierarchical ra­diosity algorithm. In SIGGRAPH 91 (1991), 
pp. 197 206. [14] HE,X. D., TORRANCE, K. E., SILLION,F., AND GREENBERG,D. P. A comprehensive physical 
model for light re.ection. In SIGGRAPH 91, (August 1991). [15] KAJIYA, J. The rendering equation. In 
SIGGRAPH 86 (1986), pp. 143 150. [16] KARNER,K. F., MAYER,H., AND GERVAUTZ, M. An image based measure­ment 
system for anisotropic re.ection. In EUROGRAPHICS Annual Conference Proceedings (1996). [17] LAFORTUNE, 
E.P.F., FOO,S., TORRANCE,K.E., AND GREENBERG,D.P. Non-Linear Approximation of Re.ectance Functions. In 
SIGGRAPH 97, (1997), pp.117-126. [18] LAVEAU,S., AND FAUGERAS, O. 3-D scene representation as a collection 
of images. In Proceedings of 12th International Conference on Pattern Recognition (1994), vol. 1, pp. 
689 691. [19] LEVOY,M., AND HANRAHAN, P. Light .eld rendering. In SIGGRAPH 96 (1996), pp. 31 42. [20] 
LOSCOS,C., FRASSON,M.-C., DRETTAKIS,G., WALTER,B., GRANIER,X., AND POULIN, P. Interactive Virtual Relighting 
and Remodeling of Real Scenes. Technical Report, iMAGIS-GRAVIR/IMAG-INRIA, (May 1999), http://www­imagis.imag.fr/Membres/Celine.Loscos/relight.html. 
[21] MARSHNER,S. Inverse Rendering for Computer Graphics. PhD thesis, Cornell University, August 1998. 
[22] MCMILLAN, L., AND BISHOP, G. Plenoptic Modeling: An image-based ren­dering system. In SIGGRAPH 95 
(1995). [23] NAYAR,S. K., IKEUCHI,K., AND KANADE, T. Shape from interre.ections. International Journal 
of Computer Vision 6, 3 (1991), 173 195. [24] NIMEROFF,J., SIMONCELLI, E., AND DORSEY, J. Ef.cient re-rendering 
of naturally illuminated environments. In 5th Eurographics Workshop on Rendering (1994). [25] Oren, M., 
and Nayar,S.K., Generalization of Lambert s Re.ectance Model , Computer Graphics Proceedings, Annual 
Conference Series, pp.239-246 (1994). [26] PRESS,W., FLANNERY,B., TEUKOLSKY,S., AND VETTERLING,W. Numer­ical 
Recipes in C. Cambridge Univ. Press, New York, 1988. [27] SATO,Y., WHEELER,M. D., AND IKEUCHI, K. Object 
shape and re.ectance modeling from observation. In SIGGRAPH 97 (1997), pp. 379 387. [28] SILLION,F. X., 
AND PUECH,C. Radiosity and Global Illumination. Morgan Kaufmann Publishers, San Francisco, 1994. [29] 
SZELISKI,R., AND SHUM, H.-Y. Creating full view panoramic image mosaics and environment maps. In SIGGRAPH 
97 (1997), pp. 251 258. [30] TURK,G., AND LEVOY, M. Zippered polygon meshes from range images. In SIGGRAPH 
94 (1994), pp. 311 318. [31] VEACH, E., AND GUIBAS, L. J. Metropolis light transport. In SIGGRAPH 97 
(August 1997), pp. 65 76. [32] WARD, G. J. Measuring and modeling anisotropic re.ection. In SIGGRAPH 
92 (July 1992), pp. 265 272. [33] WARD, G. J. The RADIANCE lighting simulation and rendering system. 
In SIGGRAPH 94 (July 1994), pp. 459 472. [34] Y.CHEN, AND MEDIONI, G. Object modeling from multiple range 
images. Image and Vision Computing 10, 3 (April 1992), pp.145 155. [35] WONG T.-T., HENG P.-A., OR S.-H. 
AND NG W.-Y. Image-based Rendering with Controllable Illumination. In 8th Eurographics Workshop on Rendering, 
(June 1997), pp.13 22. [36] YU,Y., AND MALIK, J. Recovering photometric properties of architectural scenes 
from photographs. In SIGGRAPH 98 (July 1998), pp. 207 217. [37] YU,Y., AND WU, H. A Rendering Equation 
for Specular Transfers and its Integration into Global Illumination. Eurographics 97, In J. Computer 
Graphics Forum, 16(3), (1997), pp. 283-292. Figure 8: The complete set of forty radiance images of the 
room used to recover re.ectance properties. Except for a few small areas, every surface in the room was 
seen in at least one radiance image. Each radiance image was constructed from between one and ten digital 
pictures depending on the dynamic range of the particular view. Black areas indicate regions which were 
saturated in all input images, and are not used by the recovery algorithm. The last three radiance images, 
reproduced ten stops darker than the rest, intentionally image the light bulbs. They were used to recover 
the positions and intensities of the sources. Figure 9: The model of the room, photogrammetrically recovered 
from the photographs in Fig 8. The recovered camera positions of the forty photographs are indicated. 
   Figure 14: A comparison between real and virtual, this time with novel lighting. Two of the lights 
were switched off and the third was moved to a new location. In addition, a real mirrored ball was placed 
on the red card. The scene was photographed from two locations and these real views are shown in the 
top row. To render the bottom row, we recovered the camera positions and light source position in the 
top views, estimated the material properties and position of the ball, and added a virtual ball to the 
model. The main noticeable difference is camera glare; however, some inaccuracies in the model (e.g. 
the whiteboard marker tray was not modeled) are also apparent. Otherwise, the illumination of the scene 
and appearance and shadows of the synthetic object are largely consistent. (a) Initial hierarchical 
polygon mesh, with radiances assigned from images. (b) Synthetic rendering of recovered properties under 
original illumination. (c) Synthetic rendering of room under novel illumination. (d) Synthetic rendering 
of room with seven virtual objects added. Figure 15: Panoramic renderings of the room, with various changes 
to lighting and geometry.   
			