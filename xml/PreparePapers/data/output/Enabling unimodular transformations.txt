
 Enabling Unimodular Transformations* Ron Sass Matt Mutka sass @cps.msu.edu mutka@cps.msu .edu Department 
of Computer Science Michigan State University East Lansing, MI 48824 1027 Abstract The development of 
a unimodular transfomuztion theory and associated algorithms has renewed interest in FOR-TRAN DO loops 
that are not perfectly (or tightly) nested. In this paper we summarize a number of techniques that con­vert 
imperfectly nested loops into perjectly nested loops. We examined over 25,000 lines of scientljic FORTRAN 
ker­nels and benchmarks. Statistics are reported on how of­ten impe~ect loops occur and how effective 
two transfor­mations (scalar forward substitution and loop distribution} are at converting impe~ectly 
nested loops into perfectly nested loops. Furthec we describe a compiler that inte­grates scalar forward 
substitution, loop distribution, and unimodular transfomnations while maintaining the basic philosophy 
of unimodular transformation theory. While our data indicate that imperfectly nested loops still present 
a problem, the compiler we describe is no more limited by perjectly nested loops than other restructuring 
compilers available today. Introduction The automatic restructuring of FORTRAN code for par­allel execution 
has focused mainly on the exploitation of DO loops. A majority of the performance-increasing tech­niques 
developed over the last twenty years assume that loops are perfectly nested. Researchers have begun to 
ques­tion this assumption [11] as new developments, such as uni­modular transformation (UT) theory, continue 
to be limited to perfectly nested loops. This assumption was not a con­cern with the older techniques 
because restructuring com­pilers performed a large number of transformations. A few of the transformations 
convert some portion of the imper­fect loop nests into perfect loop nests. If restructuring com­ pilers 
are based on the application of a single unimodular *This research was supported in, part by NSF under 
grant number CDA-9222901 transformation, then imperfect loop nests are not converted. In [6], Banerjee 
discusses perfectly nested loops and sug­gests that general loops will be considered in a future vol­ume 
of [6]. Nevertheless, general loops remain a problem [5]. This paper analyzes how often perfect loop 
nests occur in scientific FORTRAN codes, analyzes the effects of known transformations on the codes, 
and describes a compiler that is fundamentally based on unimodular transformations but is not as restrictive 
with respect to perfectly nested loops. We began with a small, preliminary study that suggested two well-known 
transformations can be used effectively to make loops perfectly nested. Here, we follow up on this study 
by coding these two transformations in a FORTRAN compiler and measuring their effectiveness on a large 
num­ber of scientific FORTRAN programs. The results are pre­sented in \3. We measure effectiveness by 
counting the number of perfectly nested loops before and after the appli­cation of the transformations. 
Our concern here is with the ability to enable other performance-increasing transforma­tions, such as 
the unimodular transformations, and not with the direct benefits (increased performance) of these trans­formations. 
Our paper has three significant results. First, analysis of our data has led us to formulate three classes 
of loop nests (perfect, apperfect, and general) whereas others have only considered two: perfect and 
imperfect. We discuss existing imperfect-to-perfect loop transformations and related work in $2. Although 
using three classes instead of two is a sim­ple change, it is important because it allows us to identify 
the relative significance of the loop nests that current re­structuring compilers handle well and the 
loop nests that they do not. A second result is that almost all reasonable apperfect loop nests can be 
transformed into perfect loop nests by two known existing transformations. The third re­sult is that 
loops in our general class, which includes loops for which no automatic general-to-perfect transformation 
is known, occur frequently in scientific codes. Therefore, we have identified a class of loops that deserves 
more attention from researchers and suggests that less work on apperfect­ 733 1063-9535/94 $4.0001994 
IEEE to-perfect transformations is needed. In \4, our paper discusses the high-level design of a re­structuring 
compiler that is not restricted to perfect loop nests. We discuss how to integrate the apperfect-to-perfect 
transformations and unimodular transformations without violating the basic philosophy behind unimodular 
transfor­mation theory. Related Work Little has been written about imperfect loop nests. Of­ten imperfect 
loop nests are given a modest treatment when loop transformations are described. Because the assump­tion 
of perfectly nested loops is so common, many re­searchers do not explicitly state the assumption. In 
this sec­tion we discuss the advantages and the disadvantages of a number of existjng transformations 
that are useful for con­verting loops. Loop Distribution [14] (LD) is an old and popular tech­nique based 
on data dependence. This method makes loops suitable for automatic conversion to vector or paral­lel 
form. Nevertheless, a major problem with loop distri­bution is that it cannot break a strongly connected 
compo­nent in the graph of data dependence. Although LD works well in general, most researchers are concentrating 
on ad­dressing the problem of breaking strongly connected com­ponents. Another minor problem is that 
loop distribution can make a single loop nest into multiple nests. If each nest needs a barrier synchronization, 
then loop distribution can be costly for some architectures. Compilers frequently use loop fusion to 
counteract this [15], but loop fusion is diffi­cult if a unimodular transformation is applied to one 
of the nests because the compiler may not be able to match the loop bounds. There is a very simple method 
for converting imper­fectly nested loops into perfectly nested loops by introduc­ ing conditional statements. 
The example below demon­ strates the technique. \ DO I= LB1, UB1 DO I= LB1, UB1 DO J=LB2 , UB2 P2 (I) 
DO K=LB3 , UB3 DO J=LB2 , UB2 IF K=LB3 THEN ~s(I, J) IF J=LB2 THEN P,(I) DO K=LB3 , UB3 )+ F 3(I, J) 
S(l,J,K) END IF END DO S(I,J,KJ END DO END Do END DO END DO / END DO Most researchers cite Abu-Sufah 
s non-basic-to-basic loop transformation[ 1] when referring to this technique, but the technique is 
probably much older. (Lamport [ 12] men­tions the technique in 1974.) M.E. Wolf [20] points out an important 
condition for the legality of this transformation; one that has frequently been omitted. Informally, 
the condi­tion ensures that we do not move a statement into the body of a loop if there exists the possibility 
that the loop body will not be executed. This technique has the advantage that it is easy to perform 
and is legal for a broad range of imper­fect loop nests. A serious disadvantage is that in every iter­ation 
of the loop body the conditional needs to be evaluated in order to determine if the statements should 
be executed or not. For some architectures (e.g., superscalar) these con­ditionals can have a large effect 
on the execution speed. M.J. Wolfe briefly discusses imperfect loop nests in [22]. He begins with the 
non-basic-to-basic approach but points out that the same dependence that prevent loop distribu­tion will 
frequently prevent loop interchange. Regardless, he recognizes that there is a class of loops for which 
loop interchange is still legal and presents the (complex) condi­tions needed to interchange these imperfect 
loop nests. The legality conditions make this method difficult to perform and, because it was developed 
before UT theory became well understood, it is not clear how the conditions can be merged with the theory. 
In his Ph.D. thesis, M.E, Wolf [20] discusses imper­fect loop nests. Similar to M.J. Wolfe, he starts 
with Abu­Sufah s non-basic-to-basic method but he states that under certain conditions, after the unimoduhw 
transformations, some of the IF statements can be moved out of the inner­most loop (and part of the condition 
removed). See Wolf [20] for more details. Wolf claims ([20], p. 51) that if the unimodular transformation 
is a skew then the non-perfectly [sic] nested portions would be just moved in and moved out again [to 
their original positions] . This makes performing the transformation on imperfect loop nests for loop 
skew­ing easy. Nevertheless, it remains complex for general uni­modular transformations and it is not 
clear how often it may improve upon the non-basic-to-basic transformation. The phase method, described 
in [17], is an attempt to extend UT theory to include loops that are not perfectly nested. It has the 
advantage that it is only limited by the structure of the loop and not by the desired transformation 
or the presence of strongly connected components, The phase method in [17] only applies to doubly nested 
loops, which is a serious practical limitation. Another minor prob­lem is that a single loop nest may 
introduce one or two extra barrier synchronizations. A technique that we found useful in our research 
is Scalar Forward Substitution (SFS). This transformation technique has also been called Expression Folding 
and Scalar Propagation. For specific cases, this technique is useful for making loop nests perfect. SFS 
will propagate an expression forward within a certain scope (in our case, a DO loop) when there is exactly 
one clef (that does not involve the previous value of the variable) and one or more subsequent uses. 
The meaning of clef and use are given in [2]. The SFS method is demonstrated in the example be­low: DO 
I=LB1 , UB1 DO I= LB1, UB1 TEMP=C*Y(I)-2 DO J=LB2 , UB2 DO J=LB2 , UB2 1. =(~*y(~)_z)*x(~,J) + =TEMP*X(I, 
J) }: END DO END DO END DO END DO 1: SFS is useful in parallel environments because it re­moves data 
dependence. Frequently, it will enable a loop to be executed in parallel. SFS actually increases the 
amount of computation, but the negative effect of the ex­tra computation is much smaller than the positive 
effect of parallel execution. (In fact, SFS is the inverse of common subexpression elimination [2], which 
is used in many opti­mizing compilers for scalar machines to remove the extra computation.) To summarize, 
each of the above transformations han­dles some imperfectly nested loops and each transforma­tion has 
its own restrictions on when it may be used. No single transformation is clearly superior. Furthermore, 
it is not obvious how to combine these these transformations to make them more useful. It is these concerns 
we address in the next section. 3 Data Analysis In this section we present the results of a static analysis 
of loop nests. First we state the definitions used to classify loop nests. Next our methodology is described. 
Finally, we present the data gathered and discuss its implications. Our preliminary investigation indicated 
SFS and LD were very promising as techniques to convert imperfect loop nests into perfect loop nests. 
We hypothesized that combining these two would be sufficient. We did not in­clude the other transformations 
because of the previously mentioned disadvantages, the complexity of the transfor­mation, or the inherent 
limited potential. Our goal is to identify which loops are handled effectively by SFS and LD and determine 
the significance of the loops that are not han­dled. Definitions. We define a pe~ect loop nest to be 
a loop whose body meets one of two conditions: either (I) the body consists of a sequence of non-loop 
statements with a single-entry point and single-exit point (SESE), or (II) the body consists of exactly 
one perfect loop nest. This defini­tion agrees with Abu-Sufah s definition of basic loops [1] and with 
the concept of perfectly nested used by Banerjee [4], Wolfe [22], and others. Tightly-nested has also 
been used to describe perfect loop nests [12]. The techniques used to handle loop nests that are not 
perfect are limited to certain loop structures. We establish a class based on this characteristic, that 
is a superset of the perfect loop nests but still does not encompass all loop nests. We call this class 
of near-perfect loops apper$ect.l (While not all techniques handle all of the loops in this class, we 
are not aware of any automatic techniques that handle loops outside of this class.) Our definition of 
an apperfect loop nest is one whose body consists of any combination of SESE sequences of non-loop statements 
and apperfect loops. The third class is general and includes all legal FORTRAN loops. The ex­amples in 
Figure 1 are general loop nests but not apperfect. The class of loops impetj$ect includes all general 
nests that are not perfect. This definition of imperfect matches the common usage of the term. Figure 
2 shows the relation­ships between these classes. In addition to the formal classes described above, 
we give special consideration to certain loops. The first of these special classes is loop nests that 
have sequential I/O statements. The execution order of the iteration space for these loops is critical 
because transforming these loop nests requires a number of assumptions that may be unreason­able. (We 
assume that the input records are ordered; we also assume that the order of the output records is signifi­cant.) 
Because we are concerned primarily with unimodu­lar transformations and because singly-nested loops cannot 
be transformed with unimodular transformations, we also separate loops nests of depth one (singly-nested 
loops). Methodology. We examined 25,000 lines of scientific FORTRAN code and classified all of the loop 
nests. In or­der to process the sample data, we developed a program to parse FORTRAN, perform the necessary 
transformations, and classify the loop nests. We used the parser and database library from the Sigma 
Toolbox 0.2a (which, in turn, was based on the SIGMACS project) [7, 10, 18]. Although the library included 
subroutine calls to do loop distribution and scalar forward substitution, we rewrote these calls to make 
them more comprehensive and robust. We relied on the toolkit s data dependence tests, clef/use calculations, 
and 1We have created this word using the prefix up-to suggest that these loops are almost perfect, especially 
with the number of transformations available (32) to make them perfect. 755 C This example shows a loop 
C nest within the structure of C a conditional. DO I=l,N IF ( T(I) .EQ.O ) THEN DO J=l,N s END DO END 
IF END DO (a) C This example shows a loop body C that is not SESE. (There is C i.s branch out of the 
loop body.) DO I=l,N DO J=l,N s IF ( ALPHA.LT.ERR ) THEN GOTO 10 END IF END DO END DO (b) C This example 
shows a loop body that is not C SESE because the RETURN is effectively a C branch out of the loop body. 
DO I=l,N DO J=l,N s IF ( ALPHA.LT.ERR ) THEN RETURN END IF END DO END DO (c) Figure l: Thethree examplesinthisfigure 
aregeneral loopneststhatare notapperfect.  ~ imperfect Figure 2: This diagram shows the relationships 
be­ tweentheterms perfect, apperfect, general, andimper­ feet. unparse routines to complete our source-to-source 
com­piler. When classifying the loops, our program first checks each loop to see if the loop contains 
any I/O statements. If so, the loop is classified Order-Critical, otherwise it checks the depth of the 
loop and singly-nested loops are marked as such. Finally, it tests for perfect, apperfect, and general, 
in that order. The summarized results are listed package-by-package inTablel. D~cussion The packages 
we have selected are well­knownFoRTRAN codes thatareavailable tothepublicon the Internet. (We retrieved 
these via Netlib [8].) Linpack is acollection oflinear algebra subroutines. TheMisc pack­age includes 
subroutines that were collected from various sources e,g, ,samplesused inresearchpapers todemon­strate 
the techniques described in 52. The Nascodes is a collection of five NAS benchmarks. The Svdpack con­tains 
a number of singular value computation subroutines. Toeplitz is a package that performs Toeplitz matrix 
compu­tations (Toeplitz matrices are special cases of persymmet­ric matrices). In all cases, if multiple 
versions of therou­tines existed (for example, single precision version, double precision version, complex 
version, etc.), we chose only to include one version, usually the double precision version. Ourintent 
istookeep theoriginal relative proportions. In Table 1: Loop statistics with various transformations 
applied. No Transformations #Order­ #Singly- Application #Nests #Perfect #Apperfect #General Critical 
nested linpack.f 146 2 8 3 0 133 mist.f 153 13 24 2 11 103 nascodes.f 82 14 15 2 3 48 svdpack.f 151 18 
17 21 8 87 toeplitz.f 49 7 5 4 0 33 TOTAL 581 54 69 32 22 404 Loop Distribution #Order­ #Singly- Application 
#Nests #Perfect #Apperfect #General Critical nested linpack.f 150 4 6 3 0 137 mist.f 179 23 14 2 11 129 
nascodes.f 91 24 13 2 3 49 svdpack.f 154 19 16 21 8 90 toeplitz.f 60 11 3 4 0 42 TOTAL 634 81 52 32 22 
447 Scalar Forward Substitution #Order­ #Singly- Application #Nesh #Perfect #Apperfect #General Critical 
nested linpack.f 146 4 6 3 0 133 mist.f 153 19 18 2 11 103 nascodes.f 82 22 7 2 3 48 svdpack.f 151 18 
17 21 8 87 toeplitz.f 49 7 5 4 0 33 TOTAL 581 70 53 32 22 404 Scalar Forward Substitution followed by 
Loop Distribution #Order­ #Singly- Application #Nests #Perfect #Apperfect #General Critical nested linpack.f 
152 7 3 3 0 139 mist.f 183 31 6 2 11 133 nascodes.f 95 34 3 2 3 53 svdpack.f 155 19 16 21 8 91 toeplitz.f 
56 11 34 0 38 TOTAL 641 102 31 32 22 454 757 addition, if a package uses the same subroutines for differ­ent 
algorithms, we only include the subroutine once. The programs TSVD1 and TSVD2 in Svdpack both call DGEMM. 
Since TSVD1 and TSVD2 come in separate, stand-alone FORTRAN files, DGEMM appears twice. To avoid skew­ing 
our results, we applied the following rule: if two sub­routines have the same name and are textually 
(line-by­line) identical, we removed one of the routines from our sample. If the two routines performed 
the same function but were slightly different (perhaps written by different au­thors), then we left both 
subroutines intact. All of the sub­routines included in this study are listed in the Appendix. There 
is a practical consideration to take into account re­garding these results. First, we are using Sigma 
s data de­pendence tests. These routines are fast but not exact. We have observed that the Parafrase-2 
compiler distributes a loop nest when ours does not because Parafrase-2 s exten­sive data dependence 
tests indicate that LD is legal. Since we cannot safely distribute the loop nest, based on Sigma s data 
dependence test, we have to leave the loop nest apper­feet. This means our analysis may err by being 
overly con­servative. It is important to note that the general class does not change under any of these 
transformations. This is to be expected since these loops are distinguished by the fact that we do not 
have an automatic loop transformation to handle them. An encouraging result is that the two transformations 
(SFS and LD) are fairly effective. More encouraging is that together they are significantly better than 
either one alone. The data does not validate our hypothesis at the beginning of this section SFS and 
LD are not sufficient but it does suggest we have made progress. In fact, we can see that a large number 
of the troublesome apperfect loops (16 of 31) are from a single package, Svdpack. This suggests that 
many users will find SFS and LD useful but a few users who use a package such as Svdpack, may observe 
poor per­formance. If the Svdpack package is ignored, we see that of the 109 relevant loops (perfect, 
apperfect, and general) shown in Table 1, three-quarters are perfect after both trans­formations are 
applied. When Svdpack is included 31 loop nests remain apperfect. This surprising result led us to in­ 
vestigate further. We extended our compiler to write the apperfect loops into a separate file after both 
transformations were applied. Then we inspected the loops manually. We noticed a num­ber of similarities 
and decided to further classify these loops. We searched for characteristics that would cause both transformations 
to fail. We identified four character­istics of loop bodies: (I) presence of a recurrence state­ment; 
(II) presence of an induction variable; (III) multiple DO loops; and (Iv) the presence of very complicated 
data Table 2: This table shows the loop characteristics that cause SFS and LD to fail and how often they 
occur in our sample. Recurrence: 24 dependence. We have included samples of the four cases in Figure 
3. The number of times each of these cases oc­curred is shown in Table 2. Note that the total exceeds 
31 because some nests exhibited more than one of the charac­teristics. These characteristics are not 
new discoveries. Recur­rence statements are discussed in [3] and [9]. These state­ments tend to be difficult 
to parallelize. Usually, the most effective technique is to replace the statement with a machine-optimized 
library call, essentially, change the algorithm. In [9] the authors reference a study [13] that showed 
this library-call solution yielded a 50% increase in performance for a Conjugate Gradient algorithm on 
the Cedar architecture. In [3], Allen and Kennedy include an example that has an induction variable and 
mention a trans­formation induction variable substitution (described in [23]). This transformation removes 
the induction variable by computing its value from the normal loop index and loop constants. (Induction 
variables are actually a special case of recurrence statements.) When multiple DO loops occur, as in 
Figure 3(c), it is either because the loops are in the same strongly connected component or the data 
de­pendence tests could not establish that the loops belong in separate strongly connected components. 
Either a change in the algorithm is required or a more accurate dependence test is needed. Likewise, 
cases with the characteristic of very complicated data dependence may only benefit from an algorithmic 
change. In general, these problems are be­ yond any of the loop transformations commonly available. 
An interactive tool may be important for these situations: analysis and transformations could be handled 
in bulk, au­tomatically, but the difficult cases ones that require a dif­ferent algorithm could be 
highlighted by the tool. We have found that, of the remaining31 apperfect loop nests, most require changes 
to the algorithm. Additional apperfect-to-perfect transformations have little potential for improvement 
over SFS and LD. Thus, we conclude that most of the loops in the apperfect class no longer represents 
a challenge to researchers. doj=l, ip acc = O.OdO doi=1, ip if (iflag .eq. O) then acc = acc+a(j, i)*b(ibstar+i) 
else acc = acc+a(i, -j) *b(ibstar+i) end if end do c(icstar+j) = acc end do (a) doj =1, n temp = zero 
doi=1, m temp = temp+a(i, j)*x(ix) ix = ix+incx end do Y(jy) = Y(jy)+alpha*ternP jy = jy+incy end do 
(b) dokb=l,nml do i = n-kb+l, n work(i) = a(i, n-kb) a(i, n-kb) = O.OdO end do do i = n-kb+l, n t = 
work(j) call daxpy(n, t,a(l,j), l,a (l,n kb),l) end do end do (c) doi=k+l, n a(i, k) = a(i 1, k+l)*a(k+l, 
k+l) doj=k+l, n a(i, j) = a(i, j)+a(i, k)*a(k, j) end do end do (d) Figure 3: These loops come from 
our sample of FOR-TRAN codes. The nest (a) has a recurrence statement, thenest(b)hasan inductionvariable 
(andarecurrence statement), the nest (c) has multiple DO s, andthe de­pendences in the nest (d) made 
it impossible to apply anyofourtransformations. 4 Compiler Design Thedatain$3isimportant becauseitallowsus 
todesign arestructuring compilerthatis basedonunimodular trans­formations(UTs)but isnotlimitedtoperfect 
loopnests. In this section we discuss the differences between restructur­ing compilers that are based 
onUTs and others basedon generate-and-test. We show how to integrate scalar forward substitution and 
loop distribution withUTs and argue that the resulting compiler is still essentially a UT-based com­piler. 
Generate-and-Test versus Unimodular Transformation Theory. Before the emergence of unimodular transfer­mation 
theory, restructuring compilers either allowed the user to select the order in which to apply the loop 
transfor­mations orthecompiler hadtogenerate its own order. As researchers advanced the state of restructuring 
compilers, it was realized that a single, static ordering decided apri­ori will not be good in all cases 
 certainly not optimal. Whitfield and Soffa [19] discuss this in terms of transforma­tions enabling (or 
disabling) other transformations. In prac­tice, over a large collection of loops, one might observe that 
transformation X will frequently enable transformation Y and at the same time observe that transformation 
Y will en­able transformation X. Thus, we should not give a fixed or­der to these two transformations. 
M.J. Wolfe [21] gives ex­amples involving scalar and parallel transformations where a particular ordering 
is good in one case and bad for an­other. Likewise Wolfe gives two examples where the same is true for 
the reverse ordering. Thus a restructuring com­piler that desires optimal or near optimal transformations 
needs to follow a brute force algorithm: generate an or­der, test for legality and optimality, generate 
another order­ing, test, and so on. This approach is necessary because of the large number of transformations 
and because the trans­formations ud hoc and special nature make the interactions too complex to manage. 
In contrast, compilers with algorithms based on unimod­ular transformation theory avoid the brute force 
approach. By limiting the transformations to just three elementary transformations, the theory disencumbers 
the complex in­teractions. Thus, a complex unimodular transformation composed of many elementary transformations 
becomes manageable. Specifically, we can determine the legality of the transformation, the new loop bounds, 
and the new loop body with ease. Thus, one can design algorithms to find the desired unimodulartransformation 
that maximizes apartic­ular goal in polynomial time. This is a substantial develop­ment over compilers 
based on generate-and-test. Each approach has its advantages and disadvantages. There are a large number 
of transformations available to 759 a generate-and-test compiler. Some of these are crucial for converting 
apperfect loop nests to perfect loop nests. But, with the generate-and-test compilers, we only have the 
brute force approach to ordering the transformations. Com­ pilers based on unimodular transformations 
have polyno­ mial time algorithms that guarantee optimality for particu­ lar goals, but unimodular transformations 
can only be ap­ plied to perfect loop nests. Thus, if we can join the apperfect-to-perfect loop trans­ 
 formations discussed in the previous section with unimod­ ular transformations then we have removed 
the disadvan­ tage to compilers based on unimodular transformation the­ ory. We indicate how this may 
be done in the next section. Integrating SFS/LD with Unimodular lkans­formations. There are two options 
to consider when join­ing new transformations to the unimodular transformation theory. The first option 
is to make these transformations part of the elementary transformations and re-establish the proofs and 
algorithms that are principal to the theory. The second option is to determine if there is a fixed order 
that should be applied to the transformations to assure optimal­ity. Although it may be possible to incorporate 
these trans­formations into the theory, it would significantly modify the framework. Also, the changes 
to the proofs would not be trivial. Instead, we show that SFS followed by LD is never worse than any 
other ordering sequence. To establish this ordering, we consider the four possibil­ities: no transformations, 
SFS only, LD only, and LD fol­lowed by SFS. We will show that SFS followed by LD is always as successful 
as any other ordering. We define suc­cessful to mean that the transformed loop nests: (I) are per­fect, 
if possible; (II) result in the fewest number of nests (i.e., fewest barrier synchronizations); and (III) 
contain the minimum extra computation. We assume that (I) is the most important because it means that 
unimodular transfor­mations will succeed at parallelizing the loop and (III) is the least important. 
No Transformation. SFS is always as good as or bet­ter than no transformations because we are able to 
remove the bad effects of SFS (extra computation) if needed. If we find that after applying SFS that 
the unimodular trans­formation is unable to parallelize the loop, then common subexpression elimination 
and strength reduction (both are well-known transformations [2]) will undo SFS. So either SFS contributes 
to the parallelization or it is equivalent to not applying any transformations. Similarly, LD is always 
as good as or better than no transformations. If LD has been applied and does not con­tribute to the 
parallel execution of the DO loop, it is easy to undo LD with loop fusion. (If the loop is parallelized 
by a unimodular transformation, then it is unlikely loop fusion will succeed because of the new loop 
bounds.) This, too, is common practice [15]. Therefore, if SFS followed LD is applied to a loop nest, 
the composition of these two will either contribute to the parallel execution of the loop or we can undo 
the effects. Thus, it is never worse than no transformations. SFS Alone. As in the previous argument, 
the effects of LD can be removed by loop fusion if LD does not con­tribute to the parallel execution 
of the loop nest. Thus, SFS followed by LD is for all cases no worse than SFS alone. LD Alone. Loop distribution 
uses data dependence between statements to build a dependence graph. The strongly connected components 
of this graph become sep­arate loop nests. If we are relying on loop distribution to make loop nests 
perfect, then we desire the graph to be as weakly connected as possible. If SFS is legal, then it will 
always remove at least one dependence (between the clef statement and one or more subsequent use statements). 
Removing a dependence will never increase the strength of a dependence graph. Thus, performing SFS followed 
by LD is no worse than performing LD by itself. LD followed by SFS. Suppose that there exists an ex­ample 
E, where LD followed by SFS is better than SFS fol­lowed by LD. Then both transformations must contribute 
because we have shown that one or the other alone could not have out-performed SFS followed by LD. This 
in con­junction with the fact that SFS cannot disable LD (the pre­vious argument), implies that LD must 
have enabled SFS. Since LD neither creates nor destroys statements, it must have separated some statements 
into different loops such that it is now legal to apply SFS. Such separation is impos­sible because all 
the occurrences of the scalar being substi­tuted that would make SFS illegal must appear in the same 
strongly connected component. Therefore, LD could not have separated them. Thus, example E cannot exist. 
Since our intent is use SFS and LD to make loop nests suitable unimodular transformations, clearly these 
two come before the unimodular transformation is applied. Thus, we have argued that SFS followed by LD 
is an op­timal ordering. We include loop fusion, common subex­pression elimination (CSE), and strength 
reduction to en­sure that our transformations are successful for all cases. This order is shown in Figure 
4. 5 Conclusion Presented with these results, we draw three conclusions. First, while unimodular transformations 
assume that loop nests are perfect, this is not a practical limitation for a com­piler based on unimodular 
transformation theory. With the use of scalar forward substitution and loop distribution, we 760 have 
shown that compilers mations are no more limited work. based on unimodular transfor­than any other proposed 
frame­ 1-lScalar Forward Substitution -1. uLoop Distribution Second, we found that new apperfect-to-perfect 
loop transformations have very little potential unless they specifically address the problems in Table 
2. After apply­ing scalar forward substitution and loop distribution, most apperfect loops become perfect 
loops. Since the remaining loops require an algorithmic change we feel that it is un­likely that these 
loops will be handled by a loop transfor­mation. They are significant, though, and feel some trans­formation 
is needed. By distinguishing the apperfect loops from the general loops, we established our third result. 
There are a signif­icant number of loops outside of the apperfect class for which there are no loop transformations 
available. We sus­pect that some of these loops have potential and thus de­serve more attention from 
the research community. References L1Unimodular Transformations -1 uLoop Fussion [1] [2] [3] W. Abu-Sufah. 
Improving the Pe~ormance of Virtual Memory Computers. PhD thesis, University of Illi­nois at Urbana-Champaign, 
November 1978. Alfred V. Aho, Ravi Sethi, and Jeffrey D. Unm­an. Compilers: Principles, Techniques, and 
Tools. Addison-Wesley Publishing Company, Read­ing, Massachusetts, 1986. Randy Allen and Ken Kennedy. 
Automatic trans­lation of fortran programs to vector form. ACM Transactions on Programming Languages 
and Sys­tems, 9(4), October 1987. CSE &#38; Strength Reduction [4] Utpal Banerjee. Unimodular transformations 
of dou­ble loops. In Alexandru Nicolau, David Gelernter, Thomas Gross, and David Padua, editors, Advances 
in Languages and Compilers for Parallel Processing, pages 192 219. Pitman Publishing, 1991. Figure 4: 
ommended The optimal ordering of transformations in this paper is depicted here. rec­ [5] [6] Utpal Banerjee, 
July 1992. Personal communica­tion to R. Sass during a short course on Restructuring Compilers in Trento, 
Italy. Utpal Banerjee. Loop Transfomnations for Restruc­turing Compilers: The Foundations. Kluwer Aca­demic 
Publishers, Norwell, Massachusetts, 1993, [7] F. Bodin, S. Lelait, and D. Windheiser. Sigma box. Technical 
Report Sigma Toolbox Manual, sion 0.2alpha, Irisa and Indiana University, July tool­ver­1993. 761 [8] 
[9] [10] [11] [12] [13] [14] [15] [16] [17] [18] J.J. Dongarra and E. Grosse. Distribution of math­ematical 
software via electronic mail. Communica­tions of the ACM, 30, 1987. R. Eigenmann, J. Hoeflinger, G. 
Jaxon, and D. Padua. Cedar fortran and its restructuring compiler. In Alexandru Nicolau, David Gelemter, 
Thomas Gross, and David Padua, editors, Advances in Languages and Compilers for Parallel Processing, 
pages 1 23. Pitman Publishing, 1991.  Dennis Gannon, Jenq Kuen Lee, Bruce Shei, Sekhar Samkaiand Srivinas 
Narayana, Neelakantan Sundare­san, Daya Atapattu, and Francois Bodin. Sigma II: A toolkit for building 
parallelizing compilers and per­formance analysis systems. In Proceedings of the Programming Environments 
for Parallel Comput­ing, Edinburgh, Scotland, April 1992. Wayne Kelly and William Pugh. A framework for 
unifying reordering transformations. Technical Re­port CS-TR-2995. 1, University of Maryland, College 
Park, April 1993. Leslie Lamport. The parallel execution of do loops. Communications of the ACM, 17(2), 
February 1974. U. Meier and R. Eigenmann. Parallelization and performance of conjugate gradient algorithm 
on the cedar hierarchical-memory multiprocessor. Techni­cal Report 1035, University of Illinois at Urbana-Champaign, 
Center for Supercomputing R&#38;D, 1990. David Padua, David Kuck, and Duncan Lawrie. High-speed multiprocessors 
and compilation tech­niques. IEEE Transactions on Computers, C-29(9), September 1980. V. Sarkar. Automatic 
partitioning of a program depen­dence graph into parallel tasks. IBM Journal of Re­search and Development, 
35(5/6), September 1991. Ron Sass and Matt Mutka. Enabling unimodular transformations. Technical Report 
CPS-94-20, De­partment of Computer Science, Michigan State Uni­versity, April 1994. Ron Sass and Matt 
Mutka. Transformations on dou­bly nested loops. In Proceedings of the International Conference on Parallel 
Architectures and Compila­tion Techniques, August 1994. Bruce Shei and Dennis Gannon. Sigmacs: a pro­grammable 
programming environment. In Alexandru Nicolau, David Gelernter, Thomas Gross, and David Padua, editors, 
Advances in Languages and Compil­ers for Parallel Processing, pages 88 108. Pitman Publishing, 1991. 
 [19] Debbie Whitfield and Mary Lou Soffa. An approach to ordering optimizing transformations. In Proceed­ings 
of the Second ACM SIGPLAN Symposium on Principles and Practices of Parallel Programming, pages 137 146, 
March 1990. [20] Michael E. Wolf. Improving bcali~ and Parallelism in Nested Loops. PhD thesis, Stanford 
University, Au­gust 1992. [21] Michael Wolfe. Scalar vs. parallel optimizations. Technical Report CS/E 
90-010, Oregon Graduate In­stitute of Science and Technology, 19600 NW von Neumann Drive, Beaverton, 
OR 97006, 1990. [22] Michael J. Wolfe. Optimizing Supercompilers for Su­percomputers. Pitman Publishing, 
128 Long Acre, London WC2E 9AN, 1989. [23] M.J. Wolfe. Techniques for improving the inherent parallelism 
in programs. Technical Report 78-929, Department of Computer Science, University of Illi­nois at Urbana-Champaign, 
July 1990. Appendix: Composition of Sample Below are the names of the subroutines we used from each 
package in Table 1. Further information can be found in the appendix of [16]. The technical report includes 
a brief description of ezch of these subroutines. Linpack: dchdc. f dchdd. f dchex. f dchud. f dgbco. 
f dgbdi. f dgbfa. f dgbsl . f dgeco. f dgedi . f dgefa. f dgesl . f dgtsl . f dpbco. f dpbdi . f dpbfa. 
f dpbsl . f dpoco . f dpodi . f dpofa. f dposl. f dppco. f dppdi. f dppfa. f dppsl. f dptsl. f dqrdc. 
f dqrsl. f dsico. f dsidi. f dsifa. f dsisl. f dspco. f dspdi. f dspfa. f dspsl. f dsvdc. f dtrco. f 
dtrdi . f dtrsl. f Mist: 1000d. f ddasrt. f nal. f na2. f ode. f wolfe. f Nascodes.fi buk. f cgm. f embar. 
f fftpde. f mgrid. f Svdpack.E las2. f blsl. f bls2. f sisl. f sis2. f tmsl. f tms2 .f bias. f Toeplitz.f: 
tsld. f tsldl. f tslz. f tslzl. f cslz. f cqrd. f cqrz. f tgsld. f tgsldl. f tgslz. f tgslzl. f ctslz. 
f ccslz. f cgslz. f salwz. f ctgslz. f Cctslz. f Cccslz. f Ccgslz. f 
			