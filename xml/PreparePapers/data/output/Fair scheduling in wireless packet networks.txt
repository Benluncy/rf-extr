
 Fair Scheduling in Wireless Packet Networks SoIlgwu Lu Vaduvur Bharghavan Rayadurgam Srikant Coordinated 
Sciences Laboratory University of Illinois at Urbana-Champaign slu@crhc.uiuc.edu; bharghavQcrhc.uiuc.edu; 
rsrikant@uiuc.edu Abstract Fair scheduling of delay and rate-sensitive packet flows over a wireless channel 
is not addressed effectively by most con-temporary vrireline fair scheduling algorithms because of two 
unique characteristics of wireless media: (a) bursty channel errors, and (b) location-dependent channel 
capacity and er-rors, Besides, in packet cellular networks, the base station typically performs the task 
of packet scheduhng for both dovmlink and uplink flows in a cell; however a base station has only a limited 
knowledge of the arrival processes of up- link flows. In this paper, vre propose a new model for wireless 
fair scheduling based on an adaptation of fluid fair queueing to handle location-dependent error bursts. 
We describe an ideal wireless fair scheduling algorithm which provides a pack&#38;cd implementation of 
the fluid model while assum-ing full knowledge of the current channel conditions. For this algorithm, 
we derive the worst-case throughput and delay bounds. Finally, we describe a practical wireless scheduling 
algorithm which approximates the ideal algorithm. Through simulations, we show that the algorithm achieves 
the desir-able properties identified in the wireless fluid fair queueing model, 1 Introduction Future 
indoor and outdoor packet cellular environments will seek to support communication-intensive applications 
such as multimedia tdeconferencing, Www browsing, etc. Sup-porting such applications requires the network 
to provide sustained quality of service to packet flows over scarce and shared vrireless networking resources. 
In wireline networks, quality of service requirements are typically satisfied by a combination of resource 
reservation (at the flow level) and fair resource allocation/packet scheduling (at the packet level). 
Howcvcr, in packet cellular environments, user mobility and wirclcss channel error make it very difficuIt 
to perform either resource reservation or fair packet scheduling. While there have been some recent efforts 
to provide resource reservation for mobile flows in packet cellular networks, the problem of fair packet 
scheduling in wireless networks has remained largely unaddressed. In fact, even the notion of fairness 
in shared channel wireless networks has not been precisely de-fined. In this work, we first propose a 
precisely quantifiable definition of fairness and then describe algorithms for packet scheduling in wireless 
networks to achieve such fairness. Permlasion to make digital/hard copy of part or all this work for 
personat or classroom use is gronted without fee provided that COplOo cm3not made ordistributed forprofit 
or commercial advan-tage, the copyright notice, the title of the publication and its date oppeor, and 
notice is given that copying is by permission of ACM, fno. TO COPY otherwise, to republish, to post on 
servers, or to redistribute to lists, requires prior specific permission and/or a fee. SIGCOMM 97 Connes, 
France Q 1997 ACM 0.89791.905.X197/0009...$3.50 In wireline networks, a popular model for packet schedul- 
ing over a link is the fluid fair queueing model [5, 111. In thii model, packet flows are modeled as 
fluid flows through a channel of capacity C, and every flow f is assigned a weight rf; over any infinitesimally 
small window of time At, a backlogged flop f is allocated a channel capacity of C-Ae-f / &#38;3(t) r-i), 
where B(t) is the set of flops that are backlogged at time t. There are several packet-level al-gorithmic 
implementations of this model, such as WFQ (51, WF2Q [l], SCFQ [7], STFQ [S], etc. Essentially, the goal 
of each of these algorithms is to serve packets in an order that approximates fluid fair queueing as 
closely as possible. At fist glance, it would seem that the fluid fair queueing model is applicabIe to 
scheduling over a wireIeas channel, and any of the above algorithms will work just as well for wireless 
channeLs. However, there are two key characteris- tics of shared wireless channels Nhich render the fluid 
fair queueing model inapplicable: (a) bur.sry channel errors, and (b) location-dependent channel capacity 
and errors. Since wireless transmissions are locally broadcast, contention and effective channel capacity 
are location-dependent. Besides, due to interference, fades and multipath effects, channel er-rors are 
also location-dependent. This implies that at any time, only a subset of flows can be scheduled on the 
cban-nel. All the algorithms cited above for wireline scheduling assume that the channel is error-free, 
or at least that either all flows can be scheduled or none of them can be scheduled. In fact, the fluid 
fair queueing model itself does not address the issue of fairness when a subset of backlogged flonrs 
are unable to transmit due to channel errors. This points to a clear need to develop a new fairness model 
for wireless channels. Another unique concern in packet cellular networks is the following: within a 
cell, every host is only guaranteed to be within the range of the base station, and all transmissions 
are either uplink or downlmk. Thus, the base station is the only logical choice for the scheduling entity 
in a cell. However, the base station has only limited knowledge about the arrival of packets in uphnk 
flops. In particular, the base station may knov? if an uplii ffonr has any outstanding packets, but it 
does not know when packets arrive, or how many packets there are in the queue at any instant. Thus me 
cannot assume convenient tagging mechanisms at the source when packets arrive, as in most fair queueing 
algorithms. Besides, due to the shared nature of the wireless medium, we must take into account hidden/exposed 
stations [3). This points to a close coordination of the scheduling algorithm with the medium access 
protocol for wireless channels. In this paper, we seek to address the issues unique to wireless fair 
queueing, while drawing extensively from the basic fluid fair queueing model for wirelme networks. The 
key contributions of this paper are the following: (a) a fair- ness model for wireless fair queueing, 
(b) the design of an ideal wireless fair queueing algorithm with analytically prov-able delay and throughput 
bounds, and (c) the design of a wirclcss scheduling algorithm which closely approximates the characteristics 
of the ideal algorithm, while addressing several practical issues in wireless medium access. The rest 
of the paper is organized as follows. Section 2 describes the wireless network model. Section 3 proposes 
a model for wireless fluid fair queueing. Section 4 describes a wireless scheduling algorithm which realizes 
the fairness model for an idealized scenario, while Section 5 derives its analytical throughput and delay 
bounds. Section 6 discusses some implementation issues, while Section 7 describes the practical wireless 
packet scheduling algorithm. Section 8 provides a simulation based performance evaluation of this algorithm, 
Section 9 compares our work with related work, and Section 10 concludes the paper.  2 Model For the 
purposes of this paper, we consider a packet cellu-lar network with a high-speed wired backbone and small, 
partially overlapping, shared channel wireless cells. Each ccl1 is served by a base station, which performs 
the schedul- ing of packet transmissions for the cell. Neighboring cells are assumed to transmit on different 
logical channels. All transmissions are either uplink (from a mobile host to a base station) or downlink 
(from a base station to a mobile host). Every mobile host in a cell can communicate with the base station, 
though it is not required for any two mobile hosts to bc within range of each other. Each flow of packets 
is iden- tified by a <host, uplink/downlink flag, id> triple. We say that a flow perceives a channel 
error (i.e. sees a bad chan- nel state) if either the source or the destination of the flow cxpcrionces 
an error burst in its locality. Error patterns are assumed to vary depending on location; we do not make 
any explicit assumption about the error model, though our sim-ulations typically use a two-state Markov 
chain model. By varying the transition probabilities, we generate a range of error patterns in the wireless 
channels, ranging from highly bursty to Bernoulli distributed errors.  3 Wireless Fluid Fair Queueing 
As described in Section 1, the Fluid Fair Queueing model [5] treats each packet flow as a fluid fionr. 
Each florr i is given a weight T{, and for any time interval [tr,tz] during which there is no change 
in the set of backlogged florrs B(tr, tz), the channel capacity granted to each flow i, Wi(t~,ta), satisfies 
the following property: Wi(tl,t?) _ Wj(tljt2) Kj E Wl,h), I Ti Tj I =o* (1) The above definition of fair 
queueing is applicable for both channels with constant capacity and channels with time varying capacity. 
However, it does not address the issue of location-dependent channel error, as shown below: Consider 
three backlogged flows with 9-1 = rz = rs = 1/3. Flow 1 and flow 2 have error free channels while flow 
3 pcrccivcs a channel error during the time interval [OJ]. By applying equation (1) over the time periods 
[OJ] and [1,2], wc arrive at the following channel capacity allocation: w1[0, l] = w2[0, l] = 1/2, w1[1,2] 
= w2[1,2] = w&#38;2] = 1/3, Now, over the time window [0,2], the allocation is Wl[O, 21 = Wz[O, 21 = 
5/6, W3[0,2] = l/3. which does not satisfy the fairness property of equation (1). This simple example 
illustrates the difficulty in defining fair-ness in a wireless network, even in an idealized model. In 
general, server allocations designed to be fair over one time interval may be inconsistent with fairness 
over a diierent time interval. In the fluid fair queueing model, when a flow has noth- ing to transmit 
during a time window [t, t + A], it is not ahowed reclaim the channel capacity that would have been allocated 
to it during [t, t + A] if it were backlogged at t. However, in a wireless channel, it may happen that 
the flow is backlogged, but unable to transmit due to channel error. In such circumstances, should the 
flonr be compensated at a later time? In other words, should channel error and empty queues be treated 
the same or differently? In particular, consider the scenario when flows fr and A are both back-logged, 
but fr perceives a channel error Nhile fe perceives a good channel. In this case, A will additionally 
receive the share of the channel which would have been granted to fr in the error-free case. The question 
is whether the fairness model should readjust the service granted to fr and fz in a future time window 
in order to compensate fr. The tradi-tional fluid fair queueing model does not need to address this issue 
since in a wireline model, either all flows are permitted to transmit or none of them is. In the Virtual 
Clock model [13], when a flow has noth- ing to transmit during a time window, it can reclaim its missed 
share of the channel capacity at a later time. Ar-guments have been made against allowing for such compen- 
sation, including the fact that were the packet flows really fluids flowing through a common pipe, such 
compensation -ivouId not be ahowed. However, we argue that the case of empty queues and the case of channel 
error should be treated differently. From a user-centric perspective, me would like the well-behaved 
flops to remain oblivious of short error bursts, and react only to prolonged error bursts. In partic- 
ular, we would like to make use of the fact that some chan-nels may be error free when other channels 
experience error, and implicitly swap the channel allocation over short time windows in order to accommodate 
short error bursts. How-ever, -,ve would still like to provide separation between flows by bounding the 
amount of compensation that can be pro- vided to a flop due to its channel error. Essentially, we seek 
to make a trade-off between the full compensation model and the full separation model in order to provide 
bounded compensation. Note that our compensation model is fun- damentally d&#38;rent from the Virtual 
Clock approach, since we only compensate if the flop has packets to transmit but is unable to do so because 
of channel error. In particular, we do not penalize a fiorr for using the entire channel capacity if 
no other flonr had anything to send. Given the arrival processes for each of the flows and the error 
patterns perceived by each of the flows, we define an error-free service as the fluid fair queueing service 
for the flonts with identical arrival processes and completely error-free channels. We define a flonr 
to be lagging at any time instant if its queue length is greater than the queue length of its error-free 
service at the same time instant. We define a flON to be leading at any time instant if its queue length 
is less than the queue length of its error&#38;free service at the same time instant. The key feature 
of our wireless j&#38;id fairness model is to allow lagging POWS to naake up their lag by causing leading 
frows to give up their lead. By artificially bounding the amount of the lag and lead, we can trade-off 
between long-term fairness and separation between flops. We now define the bounds on lag and lead for 
each flonr. 1. The aggregate lag of all flonrs which will be compen- sated is bounded by a constant B 
bits. A lagging flow i with weight ri is ahowed to compensate a maximum bits, where F is the set of 
all flows. fi.n = S&#38;n + LP/Ti (3) v(A(t)) is derived from the error-free service as follows: 2. A 
leading flow is allowed to lead by a maximum of 11 bits, Specifically, even when a flow is ahead of its 
error-free service by more than li bits, it only gives up a channel capacity worth li bits to other contending 
flONS. We now define the wireless fair queueing model. The granu- larity of transmission of a flow is 
a bit. Each bit has a sesus ce tag, which is the virtual time of its error-free service. The service 
tag of a badklogged flow is the service tag of the first bit in its queue; the service tag of a non-backlogged 
flow is co. The virtual time at any instant is the virtual time of the error-free service at the same 
instant. Based on the above definitions, the wireless fluid fair queueing server works as follows: 1, 
The next bit to be transmitted is chosen from the head of the queue of the flow with the minimum service 
tag among the backlogged flows which perceive a good channel. 2. Each lagging flow i is allowed to retain 
(at most) the earliest bi bits with a service tag less than the current virtual time. 3. If a flow i 
leads its error-free service by more than Zi bits, its service tag is adjusted to reflect a lead of I< 
bits,  There are three important points to note regarding the wire- less fluid fair queueing model: 
(a) the flow which has lagged the longest has the lowest service tag, and hence has highest precedence 
to the channel, (b) a flow which ZIXCCSS always perceives an error-free channel may still lag its error-free 
service by up to B bits because it has to defer for lagging flows with lower service tags, and (c) a 
flow which leads by more than Ii bits does not have to Ipay for more than Ii bits; likewise, a flow which 
lags by more than bi bits cannot reclaim more than bi bits. 4 The Idealized Wireless Fair Queueing Algorithm 
In this section, we describe an idealized wireless fair queue- ing (IWFQ) algorithm that realizes the 
wireless fluid fair qucueing model, This algorithm is idealized because it makes two key assumptions: 
(a) each flow knows whether it can transmit correctly in the current slot (i.e. transmitted pack-cts 
are never lost in transit), and (b) packets can be tagged as soon as they arrive. For simplicity, we 
assume that all packets arc of the same size Lp, and that each packet is transmitted in one slot. 4.1 
Algorithm Description The overview of the algorithm is the following: We simulate the error-free fluid 
service for the flows. At any instant, the virtual time for the idealized wire-less fair queueing algorithm 
is the virtual time of the error-free service at the same instant. Each arriving packet is tagged as 
in the Weighted Fair Queueing algorithm: a packet with sequence number n of flow i arriving at time A(ti,n) 
is assigned two tags: a start tag si,,, and a finish tag fi,n, defined as follows: si,n = maz{v(A(ti,n)), 
fi.n-1) (2)   dv(t)/dt= c/ c l-i, iEB(t) where C is the channel capacity in bits/set and B(t) is the 
set of backlogged flows at time t in the error-free service. The actions in the scheduling loop are the 
following: (a) readjust tags for each flow. (b) for each flow, set its service tag equal to the finish 
tag of the head of line packet of the flow. If there is no packet in the queue, set the service tag to 
co. (c) among the flows which can transmit (i.e. channel is good), pick the flow with least service 
tag and transmit its head of line packet. This algorithm adapts the selection process of WFQ. By restrict- 
ing the packets eligible for selection to only those which would have started their error-free service 
in the fluid model, we could adapt the selection process of WF*Q.  Readjusting tags for a flow involves 
the following: (a) for each lagging flow i, if the number of pack- ets with finish tags lmnf+an the virtual 
time is greater than Bi = then retain only C-1 the first (lowest tagged) Bi packets in the queue and 
delete the remaining packets . (b) for each leading flow i, if the start tag of the head of line packet 
(si,aor) is greater than the virtual time (v(t)) by more than Ii/r<, then  Si,hol = v(t) +k/Ti, f&#38;ho2 
= Si,hol +LP/Ti (4) The service that a flow receives in the IWFQ algorithm is never behind the service 
that it would receive in the wire-less fluid fair queueing model by more than Lp (for the same reason 
that the service in WFQ is never behind the service in FFQ by more than Lp [5]). For error-free service, 
IWFQ and WFQ are identical. When some flows perceive short error-bursts (i.e. neither lags nor leads 
exceed their bounds), IWFQ performs local adjustments in channel allocation in order to compensate the 
flows for their channel errors. The goal of IWFQ is thus to approximate WFQ while still ac-commodating 
short-term errors. However, IWFQ and WFQ diier in several ways. A flow which is denied service because 
of channel error is guaranteed to eventually receive service after its channel becomes good since its 
service tag does not change. Thus, backlogged flows receive precedence in channel allocation when their 
channels become error-free. The separation property of WFQ (which guarantees that the worst case delay 
for the head-of-line packet of a flow i, PwaFzQ 5 Lp/C + (Lp. xiEF r<)/(ri.C), is independent of the 
behavior of other flows) is only valid for IWFQ with the following bounds: d;l$&#38; 5 d$ &#38; + B/C. 
Finally, a critical difference between IWFQ and WFQ lies in the way that packets are discarded in each 
algorithm. In WFQ, packets are discarded if the flonr gets backlogged Note, that an error-free weighted 
fair queueing service will lag by at most one packet. by more than its maximum buffer size. In addition 
to the Lemma 1 [121 Let Si(T, t) and S;(T, t) be the amount of above, packets may be discarded in IWFQ 
if a flonr lags by flow i trajj% (in bits8 not packets) served under the fZuid mom than B< packets. We 
provide a mechanism to separate fair queueing and the error-free WFQ in the interval [r, t], the following 
two questions: how many packets should be for all times r and frows i: discarded , and which packets 
should be discarded . Thii mechanism is also useful for handling lost/corrupted packets when we remove 
the idealized assumption that the channel can always be predicted accurately. 4.2 Slot Queues and Packet 
Queues Since a flow i is allowed to lag by at most Bi packets, during prolonged error bursts, it may 
be forced to discard packets from its queue. If the flow discards packets from the head of the queue, 
its service tag increases; thus we can no longer preserve. the guarantee that a lagging %ON eventually 
has the lowest service tag (and hence has highest precedence to access the channel). On the other hand, 
for delay-sensitive but loss-tolerent %ows, retaining packets which have already waited in the queue 
for a long time is meaningless. Es-sentially, Zaggingflows shouZd hawe the ability to discnrcZ any packets 
from a backzogged queue without losing precedence in channel access. This points to the need to decouple 
service tags from the packet queues. In order to achieve such a decoupling, we maintain two queues for 
each flow: a sZot queue* and a packet queue. When a new packet arrives, the following actions are taken: 
(a) the packet joins the packet queue, (b) a new slot is created and assigned the start tag and finish 
tag corresponding to the packet, (c) the slot joins the slot queue. At any time, the maximum number of 
lagging slots is bounded by Bi for a %ow i, and slots with the lowest finish tags are retained in order 
to preserve the precedence of channel access for the lagging flow. The service tag for a %ow is the finish 
tag of the head of line slot in its slot queue. A separate mechanism deletes packets from the packet 
queue depending on the requirements of the %ow. For ex-ample, a packet may be deleted after a %xed number 
of re- transmissions or after a delay bound has expired. When a slot is selected for transmission, the 
head of line packet in the packet queue is transmitted -thus the mapping between slots and packets is 
dynamic, By decoupling slot queues from packet queues, we can handle multiple types of delay and loss 
requirements for flows while still maintaining the precedence in channel access for lagging flows. 5 
Throughput and delay guarantees for the IWFQ The following facts should be obvious from the IWFQ algo-rithm 
described in Section 4: Fact 1 At any tivne t, the number of Zagging bits of flow i, den&#38;d 6~ 6i(t), 
satisfies CiEF hi(t) 5 CieF BiLp = B, where LP is the packet Zength in bits. Fact 2 For any Zagging 
slot s of flow i at time t, its jZnish tag is no greater than that of any non-lagging slot, i.e. it is 
served with higher priority. The following results regarding error-free fluid fair queueing (FFQ) and 
error-free WFQ3 have been proved in [12], and are included here for quick reference. In this section, 
slots refer to logical slots rather than physical tlme slots. %rror-free PFQ and error-free WFQ refer 
to FFQ and WFQ, re-spcctivoly, when all the channels are error free. ih(O,T)-Sz(O,T) 5 LP, St(O,r+ g) 
2 Si(O,r) (5) For all packets p, let 2 and I$ be the time at which packet p departs under the fluid 
fair queueing and the WFQ, FPI -&#38; 5 b/C, (6) where C is the channel capacity (in bits per second) 
and LP is the packet length in bits. Cl The delay and throughput results in this section are given separately 
for two types of channels: (a) Error-free Channel: an error-free channel is one which is always in the 
good state at all time slots, and (b) Error-prone Channel: an error-prone channel is one which is not 
always in the good state. 5.1 Error-Free Channels Based on Facts 1 and 2, we can show the following result: 
Lemma 2 Any slot s on a error@e channel g4 completes its service in IWFQ by time tef + dg, with &#38; 
= B/C. (7) where t,f is the finish time of slot s in the error-free WFQ algorithm, F denotes the set 
of all fIows, Lp is the packet length (in bits) and C is the channeZ capacity (an bits per sewna). Proof, 
Let us consider an arbitrary time t, at which slot s is the head-of-line slot for flow g. If slot s has 
received service in IWFQ before or at t,f, i.e. %ON g is leading at this time, then the result is trivially 
true. In the following, we only consider the case that slot s receives its service later than t,f in 
the IWFQ. Denote the current virtual time as v(t) (i.e. the bit round in progress as defined in equation 
4. At virtual time v(t), in the error-free WFQ, let the slot sequence waiting to be servedbem,mfl,..., 
s-l,s,s+l,..., kwhereslotse-quence 0,1,. . . , m -1 has been transmitted by the time t in the error-free 
WFQ. We also denote the finish tag as-sociated with a slot i as Ti. Let s belong to flow g. The remaining 
waiting time (after t) in terms of time slots for s under error-free WFQ is s -m. Consider the scenario 
for the IWFQ algorithm at time t. Let B(t) denote the set of the lagging slots (ordered by their %niih 
tags) by all flows at time t, i.e. when m becomes eligible for service in the slot sequence. Note that 
the total number of slots that have been transmitted in IWFQ, de-noted by q, is no greater than m, i.e. 
q 2 m, due to the work-conserving nature of the server. Thus, the slot with the lowest tag in IWFQ is 
either m or the slot with the lowest finiih tag in B(t). By Fact 2, it follows that any slot in B(t) 
has no greater finish tag than the sequence T,,,,T,,,+l,. . . . Therefore, the Zargest possibZe sequence 
of slots to be served at time t ob- serves the order B(t), m,m-tl,..., s-l,s,s+l,..., k. Hence, the maximum 
number of slots (after current time t) to be served before s in the IWFQ is IB(t)l + s -m, where jB(t)l 
is the cardinality of set B(t). Based on Fact 1, it follows that IB(t)l 5 B. for any time t. Hence, slot 
s 41n this case, the flow is also denoted as g with slight abuse of notation. on error-free channel g 
completes service in IWFQ no later than time tef + B/C as compared to error WFQ by noting that the server 
rate is C. Therefore, the result folloms for slot 8. The arbitrary choice of starting time t also implies 
that the arbitrary choice of sIot s; hence, the result holds true for any slot s of flovr g, which concludes 
the proof. 0 Theorem 1 (delay guarantee) For_any packet i on a error- free channel g, its maximum delay 
Do, ,WFQ in IWFQ sat-   i@es: bg, IWFQ 2 &#38;, WFQ +&#38;I where Do, WFQ is the ma_zimum packet 
delay of flow g in the error-free WFQ, and do is given by equation (7). Proof, For lagging slots, the 
proof follonrs from Lemma 2. For a leading slot, by definition, its delay is less than v~hat it vrould 
have been under error-free WFQ. Cl Though the v,orst-case packet delay of a leading flow does not increase, 
its packets can be vrell ahead of their schedule. Thus its ncvr queue delay has a bounded increase, as 
shone in Corollary 1. Theorem 2 (long-term throughput guarantee) For a error- free channel g, let So(O,t) 
denote the aggregate service (in bits) received by channel g in the interval [0, t] in the IWFQ, and 
5 ; (0, t) denote the aggmgate service (in bits) mceived by channel g in the intervht [0,t] in the error-free 
WFQ service, then the following inequality holds: Sg(O,t +&#38;J 2 s;co, t) (9) where &#38; is given 
by equation (7). Proof, Let tN be the finish (real) time of the Nth packet under error-free WFQ and tb 
be the finiih time of the same packet under IWFQ. Then, by Lemma 2, t)N 5 tN + do. Also, let Si(O, t) 
2 NLp, for some integer N. We use the v~ell-knovln relationship S,l(O, t) 2 NLp e tN 5 t. From above, 
th -2s < t, vrhich leads to S,(O, t + &#38;) >_ NLp. Hence, for any N > 0, S,(O,t) 2 NLP =$ Ss(O,t+&#38;) 
2 NLP. which leads to the inequality (9). 0 Based on Lemmas 1 and 2, the following result is easily 
derived for new queue delay. Theorem 3 (new queue delay bound) For aflzw g on error- free channel, its 
maximum new queue delay DHOL is given bY &#38;OL = $ + JWFQ + Fg,, (10) LP CCEF Tc , (11) (.jWFQ = Lp 
 gig = tg(zw7 rj) ) c +c--- ccl 9-g  where Jg is given by (7) and Fg = F \ g. Proof. -is the spent 
head-of-the ~WFQ time in the line (HOL) if the HOL packet contends for service immediately. To is the 
maximum amount of time that one has to wait before contending at the HOL due to the fact that flonr g 
might be leading even if the HOL packet arrived at an empty queue, TOderive the expression for T,, note 
that flonr g can lead by I, bits or equivalently, 1,/r, bit rounds. Thus, the number of other flovfs 
bits that can take precedence over a ncvfly arrived packet is 2 xjEF, rj. Finally, Jg is the wait due 
to lagging flovrs. cl GNoa Qucuo Delay is the maximum delay for a packet that arrives at an empty queue. 
Theorem 4 (short-term throughput guarantee) Given any time t, for a error-free channel g, assume that 
the service which flow g mceives in error-free WFQ is given by S(t,t,) during time intemat [t, tl] for 
some tl > t +-T,(t). Then the service that flow g receives under IWFQ satisfies W,h) I S (t+Tg(t),tl), 
02) (13) where bj(t) is the number of tagging bits of flow j and t,(t) is the number of leading bits 
of flow g at time t. Proof. At any time t, the amount of time that flom g has to tit to begin service 
is determined by the number of lagging bits of other flonrs as rrell as the amount by which it is leading. 
The amount of time due to other lagging sources is xjEF, bj(t)/C, from the definition of bj(t). In addition, 
flonr g has to possibly tit for l,(t)/r, bit-by-bit rounds and the maximum amount of time due to this 
is bounded cl by F cjcZF9rji'/ce Note that the above theorem is trivially valid Jvhen all channels are 
error-free because in that case, bj(t) = t,(t) = 0. In addition, one cannot give short-term guarantees 
over intervals smaller than T,(t). This highlights our observation in Sections 1 and 2 that trying to 
provide very short-term guarantees in wireless channels will not allow enough flexi-bility to hide short 
error-bursts from users. One can trivially remove the dependence on t in the lolver bound of the above 
theorem as follolvs. Corollary 1 Assume that fIow g is backlogged during tl] [t, in the error-free WFQ, 
then the service it receives in the IWFQ satisfies S(t,t,) 2 S*(t+Tg,tl) (14) 5.2 Error-prone channels 
Theorem 5 (delay bound for an error-prone channel) Given any packet of Jaw e on an error-prone channel, 
its maximum packet delay DIWFQ, e is given by DIWFQ, e 5 BWFQ, e + ~,(M+I) (15)  where &#38;FQ, e is 
the maximum packet delay of flow e under error-free WFQ, and M is the maximum number of lagging slots 
of all flows other than flow e, M = cj,, bj/Lp, and T=,(M+~) is the mazimum time it takes for flow e 
to have its (M + l)fh good slot starting from any time t. Proof We assume the flonr e is lagging. As 
in Lemma 2, if there are no further errors after time t the delay of a packet of e is increased by the 
lagging slots of other flov~s M. However, we have to additionally account for possible errors in the 
channels of 3onr e and other lagging 30~s. Suppose the (M + l) h good state for flonr e after time t, 
occurs at t + Te,(~+r), then we daim that the head-of-the-line packet at time t for flont e nrould be 
transmitted no later than t + Te,(~+l). Suppose this were not true, then it mould lead to the folloting 
conclusion: during all the M + 1 good states, one of the other flows had a slot tith a lower finish tag. 
This contradicts the upper bound of M on the number of lagged slots. Assume a packet arrives when a flow 
is leading. If the packet finishes when the flow is still leading, then the state-ment of the theorem 
is trivially true. If it finiihes when the flow is lagging, then there is a time instant before the packet 
s departure when it is in the queue and the flow starts lagging. Then the above proof holds. cl Note 
that the previous resuIt does not take into account a specific model for channel errors. Any channel 
error model that deterministically or probabilistically bounds Te,~+l, could be easily incorporated into 
the bound. Based on the above result on delay bound, the result on throughput follows readily along the 
lines of Theorem 2. Theorem 6 (long-term throughput guarantee) For a flow e on an error-prone channel, 
let S,(O, t) denote the aggregate service (in bits) received by flow e in the interval [0, t] in the 
IWFQ, and Sz(O, t) denote the aggregate service (in bits) ireceived bg channel e in the internal [O, 
t] in the error-free WFQ service, then the following inequality holds: Se(0, t I- Te,~.t1) 1 S, (O, 
t) (16) Moreover, we can further show the following result for short- term throughput: Theorem 7 (short-term 
throughput guarantee) Given any tivne t, for a continuously backlogged flow e on an emor-prone channel 
during time internal [t, tl], the aggregate ser-vice (in bits) received by flow e in the interval [t, 
tl] in the IWFQ, denoted by Se(t, tl), satisfies: &#38;(t,tl) 2 (NO -N(t))- c.Te $P -LP, (17) 1EF  c 
bi(t) + t.(t)Ci;7 r-j (18) iEF= where No is the number of time slots in good state for jlow e in [t, 
tl],, hi(t) is the number of tagging bits of frow i and t,(t) is the number of leading bits of flow e 
at time t. 0 The proof of the above theorem follows along the lines of the proof of Theorem 3. Implementation 
Issues in Wireless Packet Scheduling In previous sections, we developed an idealized wireless fair qucucing 
algorithm in the presence of bursty and location-dependent errors, assuming full knowledge of the channel. 
Hov~cver, when implementing a practical wireless fair schedul- ing algorithm, we need to address the 
following important constraints: (a) the channel state is not known in advance and cannot, be predicted 
with complete accuracy, (b) due to error and incorrect channel prediction, transmitted packets may be 
lost, (c) detection of reception errors is not just a matter of sensing the carrier, since errors at 
the source do not imply errors at the destination and vice-versa, (d) the base station performs the scheduling, 
but does not have a full knowledge of which uplink flows have packets to transmit or how many packets 
a backlogged flow has, and (e) since errors are typically bursty, giving precedence to packets of lagging 
flows (as IWFQ does) will cause error-prone chan-nels to be polled more often, which increases the scheduling 
overhead. Due to our channel model, problems of hidden and exposed stations across multiple shared channel 
cells are not, addressed. Several of the above constraints either pertain to, or can be effectively addressed 
at, the medium access layer. Hence, one key conclusion for a practical implementation of wire- less fair 
scheduling is that it must be closely coupled with the MAC protocol. In this section, we first identify 
some MAC-level instruments to address the above issues, and then briefly describe our wireless medium 
access protocol. In the next section, we describe our wireless fair scheduling algo-rithm. 6.1 Techniques 
to address wireless channel issues As before, we assume that packets are small and of Exed size; these 
are very reasonable assumptions for wireless networks. Tie is slotted, and each data slot accomodates 
some con-trol information, a data packet and au acknowledgement. Acknowledgement: In our approach, each 
packet trans-mission is followed by a short acknowledgement from the destination to the source. Using 
acknowledgements serves a number of purposes. The most important purpose is to detect loss of packets 
during transit. As a side-effect, ac-knowledgements also imply that the base station transmits either 
the data packet or the ack packet in every transmis-sion -we use this feature to piggyback important 
control information for future slots on the base station s transmis-sion in the current slot. Acknowledgements 
have been used in several medium access protocols [3, lo] for similar pur-poses. One-Step Prediction: 
Since errors are bursty and errors in successive slots are highly correlated, we perform a one-step channel 
prediction by monitoring the channel condition in the previous slot. Since the base station transmits 
either a data packet or an ack packet in every slot, each host in the cell monitors the channel during 
each slot for packets from the base station. If a host can sense activity on the channel but does not 
receive a good packet from the base station, it detects an error during the current slot. A host predicts 
that its next slot will be in the same state as the current slot, due to the high correlation of channel 
state across slots. While the one-step prediction is obviously not perfect, our simulation results show 
that it is very effective for typical wire1es.s channel error models. One undesirable consequence of 
the one-step prediction approach is that every host (with a backlogged uplink flow) has to monitor every 
slot, which can increase its power con-sumption. In the future, we plan to experiment with peri-odic 
snooping of the channel and dynamically estimate the optimal snooping period in order to alleviate the 
problem of having the host be in promiscuous mode all the time. Set of known backlogged flows: Since 
the base station must schedule packets for both downlink and uplink flotvs, it needs to know at least 
which uplink flows are backlogged at any time. In order to allocate slots only to flows which have packets 
to transmit, the base station keeps a set of known backlogged flows and only allocates slots among the 
flows in thii set. The set is updated by the following mechanisms: (a) for downlink flows, the base station 
has a precise infor-mation about the queue lengths, (b) when an uplink flow is allocated a slot,, it 
piggybacks the queue size information on its data packet, (c) vrhen a new uplink flow is created or becomes 
backlogged, if there is an ongoing backlogged flow fFom the same mobile host, the information is piggy- 
backed in (b) above, and (d) the base station periodically solicits notifications from new (and newly 
backlogged) up link flows by issuing of a control slot. One of the highlights of our approach is the 
way in which control and data slots are integrated in the MAC framework.  6.2 Wireless Medium Access 
Protocol Our wireless medium access protocol has its origins in DQRUMA (91. We divide time into frames, 
and each frame into slots (as described in Section 7, the frame size is not fixed, and the number of 
slots in a frame changes over time). A slot may be either a data slot or a control slot. Each data slot 
is subdivided into three parts: a control sub-slot which consists of four v&#38;i-slots, a data sub-slot 
and an ack sub-slot. Each control slot is divided into a notification sub-slot and an ad-vertisement 
sub-slot. By means of the scheduling algorithm described in Section 7, the base station allocates the 
slots in a frame among known backlogged flows before the start of the frame, Due to lack of space, thii 
section provides only a brief outline of key actions of the MAC protocol which relate to scheduling, 
i.e. the mechanics of slot allocation, and the mechanics of identification of newly backlogged flows. 
For a more detailed general discussion on MAC protocol issues, we refer the reader to [3, 91. Identification 
of New and Backlogged Flows: The base station has a special downlink broadcast flow called the control 
flow, which has a flow id of <0, downlink, O>. From the scheduling perspective, a control flow is identical 
to a backlogged data flow of unit weight on an error-free chan-nel. However, when the control flow is 
allocated a slot, the MAC layer at the base station issues a control slot as op-posed to a data slot. 
The control slot consists of two phases: a notification sub-slot during which mobile hosts contend in 
order to notify the base station of new or newly backlogged flows, and an advertisement sub-slot during 
which the base station broadcasts the newly received notifications ss an ac-knowledgement to the successfully 
contending mobile hosts. The notification sub-slot has a sequence of mini-slots. If a mobile host has 
a newly backlogged flow but does not have an ongoing backlogged flow on which to piggyback thii information, 
it selects a random mini-slot during which it transmits the notification. During the advertisement sub-slot, 
the mobile host knows if it s notification was success-fully received, This contention mechanism is novel 
in the way control and data flows are integrated. However, it is simplistic in that contending mobile 
hosts can only trans-mit once. in a control slot. Using Slotted Aloha to contend in the control slot 
will improve the probability of success- fully sending notifications. Note, that the above contention 
mechanism impacts the delay and throughput bounds of new flows in Section 5; the changes are easy to 
compute using results from Slotted Aloha. Data Slot Allocation: Since all flows are either uplink or 
downlink, in each data slot the base station must transmit either the data packet or the acknowledgement 
packet. Pig-gybacked on the packet, the base station provides the ids of the flows which are allocated 
the next three slots (as a special case, a control slot is identified by setting all the flow ids to 
<O, downlink, O>). Since every host in the cell is within range of the base station, a source of an identified 
flow will be able to hear the packet if its channel is good. In the control phase of the next slot, the 
source of flow i (1 < i 5 3) transmits a channel good flag in mini-slot i if it predicts that the channel 
will be good (based on one-step prediction), In the fourth mini-slot, the base station identi-fies the 
flow which has been chosen for transmission during the current slot, which is the 6r.s.t among the three 
flows to send the good flag in its mini-slot. If it turns out that all the identified flows are in error, 
then the base station picks any one down~mk flow for transmission. When an uplink flow transmits a packet, 
it piggybacks the number of packets in its queue. When this number reaches zero, the base station removes 
the flow from its set of known backlogged flows. 7 Wireless Scheduling Protocol In thii section, we describe 
a wireless packet scheduling (WPS) algorithm that approximates the idealized algorithm while addressing 
the issues of practical implementation. Within the constraints identified in Section 6, the follow- ing 
are the key requirements of the wireless packet schedul-ing algorithm: (a) it should provide fair channel 
access among flows which are known to be backlogged, (b) it should utilize the location-dependent channel 
error property in order to lo- ca/ly spuap slots (preferably within a frame) between flows in order to 
accommodate short error burst8, (c) across frames, it should provide a system of maintaining credits 
for lagging flows and debits for leading flows in case swapping within a frame is not possible (as in 
IWFQ, both credits and deb-its should be bounded in order to provide separation), (d) since errors are 
known to be bursty in wireless channels, it should spread the slots allocated to each flow as well as 
possible within the frame, (e) since errors are bursty, flows which perceive channel error should not 
be repeatedly polled in subsequent slots (the tagging mechanism in IWFQ will end up doing this since 
it gives higher precedence to flows that have been lagging the longest), (f) well-behaved flows with 
error-free channels should be affected es less as possible while still accommodating flows which perceive 
errors, and (g) the scheduling algorithm should be simple. The major departure in WPS from IWFQ is that 
we have moved from the fair queueing to the weighted round robin paradigm. Thii was motivated by the 
fact though weighted round robin is much simpler to implement, in our environ-ment, weighted round robin 
and fair queueing will result in identical error-free service for the following reasons: (a) the base 
station allocates slots only among known backlogged flows, (b) packets are of fixed size, (c) the base 
station can only periodically know when an empty flow has been back-logged (for the uplink case); in 
particular, if a backlogged flow drains its queue during a frame, it drops out of con- tention for slots 
until the next new queue phase even if it becomes backlogged subsequently, and (d) when all flows contending 
for the channel are backlogged, by spreading slot allocation appropriately within each frame, we can 
exactly replicate the WFQ or WF2Q service for error-free service. Thus, WPS modifies the basic weighted 
round robin schedul-ing algorithm in order to accommodate location-dependent and bursty wireless channel 
errors. The following are the key features of the WPS algorithm: . Spreading: generates a slot allocation 
identical to WFQ [ll] or WF Q when all flows are backlogged. . Swapping within frame: when a flow cannot 
transmit in its slot because of channel error, it tries to swaps its slot with another backlogged flow 
which has (a) been allocated a slot later in the same frame, and (b) per- ceives a good channel at the 
current time; intra-frame In Section 6, we have used the term slot to mean physical time slots as well 
as the logical slots which compose a frame. In this sec- tion, we only deal with logical slots when we 
refer to slots . swapping is a %rst level mechanism to accommodate location-dependent errors. . Cr-edit 
adjustment: when a flow fr cannot transmit during its slot and cannot swap slots within a frame, but 
there is at least one backlogged flow fz that can transmit at the current time (fz does not have any 
slots during the remainder of the frame), fl s credit is in- cremented and f2's credit is decremented 
(both within bounds); the effective weight of each flow at the start of a frame is the aggregate of its 
default weight and its credit, and the spreading algorithm generates a slot allocation with respect to 
the effective weights of the flows. Thus, credit adjustment compensates lagging flows at the expense 
of leading flows in future frames. . One-step prediction: predicts that the channel state for the current 
time slot will be the same as the monitored channel state during the previous time slot. We show through 
intuitive arguments and simulation re-sults that a combination of the above features will address all 
of the above requirements for fair slot allocation in wire- less channels, while also closely approximating 
IWFQ for the average case. ochadulo,franoO /* nain procedure to schedule packets in frana . / conputo,offoctivc-woightu0; 
aprond,nou,frona() ; /* npraud nccording to WFQ; ignore flow with effective credit < 0 */ narkor CJ fir&#38;-olot,of,fra.me; 
uhilo(ourkor I= NULL) f . got,noxt,flovO; Darkor->norkor->noxt; If (f I= NULL) trunonit-hcnd-of-line-packet 
(f 1;  /1 trunnnit tho ho1 puck&#38; and increment f->uttempts / computo,offoctivo,croditoO /* conputc 
effective credits for all flown at the start of a franc */ for onch flow f, f-Scrodit = ain(mnx(f-)offoctive,voight 
-f->attenpto. -(f->dobit,linit)), f->credit-linit); f-?offoctivogoight s f->uoight + f->crodit; f->nttompto 
= 0;  got,noxt-olot() /* return flow that can tranonit in current slot; porfom swapping and credit/debit 
allocation */ uhilo (nurkor != IIULL) if (quouo,ompty(norksr->flov)) /* cuuc 1: flow hao no queue */ 
doloto,flou,nloto,fron-framo(markor->flou); doloto,flov,fron~bnckloggcd,net(marker->flov); morkor = nurkor->noxt 
; 0100 if (oxcoption,cooo()) /* cuse 2: no flow cau transmit */ (nurkar->flov)->nttomptn tt; /t no credit 
for niosod slot / murkor = nnrkor->noxt; roturn UULL; 0100 if (olot,ntnto(narkcr->flou) == ERROR) /* 
cam 3: channel in in error */ for(a = markor->ncxt;o != UULL;o 5 s->next) if ((! (qucue~onpty(o->flov)))R~~olot~otata(s->flou)==CWI) 
1 brouk; /* flov pointed by u can nvup vith marker->flov */ if (n I= MULL) uuap,flovo(markcr, a); /* 
~880 3a: intra frms swap */ oloo roturn find,ncxt-good,flov(); /* case 3b: no svap; credit/ dobit is 
implicit duo to how f->attcnptn is updated I/ oloo /* cauo 4: connection has packet and slot io good 
. / roturn nurkor->flow; roturn NULL;  The above pseudo code describes the essential parts of the WPS 
algorithm. We now comment briefly on some note- worthy points in the WPS algorithm. A %ovr which is unable 
to transmit in its slot receives credit only if some other flow is able to transmit in its place. When 
a flow transmits more slots in a frame than it is initially granted, its credit becomes negative. Hence, 
even when we cannot swap within a &#38;me, the system of credit/debit adjustment implicitly preserves 
the notion of a swapping, just ss lag and lead implicitly preserves the notion of swapping in IWFQ. A 
flow fi with a negative credit of ci will not receive any channel capacity for [l~lJ/ri frames (where 
the size of wi dots, and B(t) is the set of knom a frame iS CieB(t) backlogged flows at the start of 
the frame, and wi is the effective weight of flow f;). The credit adjustment policy above compensates 
all the credits of a lagging flow in the next frame. For a flow that has accumulated a large number of 
credits, thii could po-tentially result in the flow capturing the channel for a long time after its channel 
becomes good (IWFQ also has a sim- ilar effect in case a flow has been lagging for a long time). In order 
to compensate lagging flows over a longer period of time, we could bound the number of credits that can 
be reclaimed in a single frame by any flow, thus amortizing the compensation over several frames. In 
the average case, WPS closely approximates IWFQ because is tries to achieve separation and compensation 
by similar instruments (credit/debit similar to lag/lead, and bounds as in IWFQ). However, there is a 
difference in worst- case delay since we compensate by swapping rather than by giving precedence in channel 
access to longest lagging flows. By swapping, we delay the next attempt of the flow to access the channel 
to a later slot, not necessarily the next slot. Thus, swapping loses the precedence history which IWFQ 
maintains. While this is good for a practical implementation (otherwise the base station will need to 
poll the most error-prone channel most often), it can starve out a flow under some pathological conditions. 
Consider an example in which a flow always perceives an error in precisely the exact slots when it is 
scheduled to transmit, but has some good slots in between when other flows are scheduled to transmit. 
In IWFQ, the flow will eventually have the minimum service tag and gets highest precedence for transmission 
in any slot; in WPS, the %ow can contend only in designated slots and will be starved. Thus, though we 
expect the average delay of packets to be very close for WPS and IWFQ, the worst case delay of WPS is 
co. One important question is the following: when a back- logged is unable to transmit because of channel 
error, %ON and is unable to swap slots within its current frame, how does it choose a %ow in a future 
frame with which it can swap slots? Of course, if we generated slot allocations for several future frames 
in advance, it would be possible to simply pick the %rst %ow in a future frame that can trans-mit in 
the current slot. HoNever, we do not maintain future frames. Instead, we generate a weighted round robin 
ring (with WF Q spreading) based on the default weights for all known backlogged flops after each new 
queue phase. A marker in this ring identifies the last %ON that nras selected for swapping across frames. 
When intra-frame swapping fails, we simple advance the marker around the ring until we %nd a %ow that 
can swap with the current slot.  8 Simulation Results This section presents the simulation results 
for the WPS algorithm. As described in Section 7, there are four key components of the algorithm: spreading, 
swapping, credit adjustment, and prediction. In order to isolate the effect of each of these components, 
we simulated several different algorithms, with different combinations of the above com-ponents. The 
following are the algorithms we simulated and com-pare in this section: . Ble ad WRR spreads slots according 
to WF Q, but does not attempt to predict the channel state. . WRR modifies Blind WRR by skipping the 
slot if the channel for the flow is known (in m-1) or predicted (in WRRP) to be in error. . NoSwap combines 
spreading and credits (but no deb-its), but does not have any intra-frame swapping. If the channel for 
the current flow is known (NoSwap-I) or predicted (NoSwap-P) to be in error, it gives a credit to the 
flow and skips to the next slot. . Swap W combines spreading, swapping and credits (but no debits). If 
the channel for the current flow is known (SwapW-I) or predicted (SwapW-P) to be in error, it first tries 
to swap within the frame. Otherwise, it gives a credit to the flow and skips to the next slot. . SwapA 
combines spreading, swapping, and credit/debit adjustment. SwapA is identical to the WPS algorithm described 
in Section 7. We start by illustrating the key ideas using examples with only two sources. This allows 
us to demonstrate the effect of the various parameters clearly. Later we consider examples with more 
sources to illustrate some differences in the performance when there are small number of sources as opposed 
to a large number of sources. Example 1: We consider an example with two loss-sensitive sources with 
WFQ weights TI = 1, TZ = 1. For the purposes of simulation, we assume that the channel for Source 2 has 
no errors and the channel for Source 1 evolves according to a two-state discrete Markov Chain. Let ps 
be the probability that the next time slot is good given that the current slot is in error, and pe be 
the probability that the next time slot is in error given that the current slot is good. Then, the steady-state 
probabilities PG and PE of being in the good and bad states, respectively, are given by  f-k pE=Pe. 
Pa = - Pn 4Pe Pg+Pe The arrival processes are assumed to be as follows: . Source 1 is a Markov-modulated 
Poisson process (MMPP) where the modulated process is a continuous-time Markov chain which is in one 
of two states ON or OFF. The transition rate from ON to OFF is 9 and OFF to ON is 1. When the Markov 
chain is in the ON state, arrivals occur according to a Poisson process of rate 2. . Source 2 has constant 
inter-arrival time of 2. Note that the channel being Markov is not necessary for our algorithm, it is 
just used for the purposes of simulation. For the two-state Markov chain describing the channel process 
for Source 1, if we let the bad state be 0 and the good state be 1, it is easy to see that the one-step 
autocovariance function is C(1) E E(X(t)X(t f 1)) -E(X(t))E(X(t + 1)) = PaPJr(1 -(ps l-p,)). If p. fpe 
5 1, then C(1) 1 0. Further C(1) is a decreasing function of p. + pe, and therefore, as pg + pe t 1, 
successive time slots become less correlated. Thus, it is a natural to Table 1: Example 1. Results for 
p, -l-pe = 0.1 Table 2: Example 1. Results for ps +pe = 0.5 test our prediction algorithm for various 
values of p, + pe with PG and PB fIxed. We fix the steady-state probability for Channel 1 as PC = 0.7. 
For each packet, we limit the maximum number of retransmissions to 2, i.e., a packet is dropped if it 
is not successfully transmitted after three at-tempts. We also limit the number of credits and number 
of debits to four. Simulation results are presented in Tables l-3. The per- formance of the various scheduling 
algorithms are compared using the following three performance measures for Source i, i = 1,2: . &#38; 
: Average delay of successfully transmitted packets . Ii : Loss Probability, i.e., fraction of packets 
that are dropped after four transmission attempts . dy= : Maximum delay of successfully transmitted pack-ets 
. udi: The standard deviation of the delay Our main conclusions from the simulation results are: . The 
scheduling algorithms assuming perfect state in-formation, WRR.4, NoSwap-I, SwapW-I and SwapA-alnrays 
perform better than the Blind 1JRR algorithm. This means that our basic idea of credits significantly 
improves performance if we have perfect knowledge about the state of each channel. Note that me have 
assumed that the sources are loss sensitive, and thus our objective is to make the loss probabilities 
close to zero. Table 3: Example 1. Results for ps +pe = 1.0 . In all the cases where p, $ pe < 1, the 
one-step pre-diction algorithms WRR.-P, NoSwap-P, SwapW-P and SwapA-P perform significantly better than 
the Blind WRR. Thus, when consecutive time slots are posi-tively correlated, our simple one-step prediction 
works remarkably well. In general, prior studies of wire- less channel errors have indicated that errors 
occur in bursts although models for the error can vary. Thus, our algorithm works very well if the channel 
errors are indeed bursty. 0 When pu + pe = 1, our prediction algorithms per-form poorly. In fact they 
perform worse than Blind WRR. The fact that p, + pe = 1 implies that the channel states are Bernoulli 
random variables with the probability of being good chosen as PG. Thus, channel states at successive 
time slots are uncorrelated and it is not surprising that the one-step prediction performs poorly. However, 
this is a very unrealistic model of a wireless channel which contradicts all prior claims of bursty channel 
errors. We chose to perform this exper- iment to show when our algorithms can break down. . NoSwap, SwapW 
and SwapA perform better than WRR in the following sense: They all reduce the delay for Source 1 significantly 
while increasing the delay slightly for Source 2. This illustrates the advantage of compen- sating a 
source for time slots during which its chau-nel is in error. The difference between the algorithms NoSwap, 
SwapW and SwapA as compared to WRR is even more dramatic when one does not have perfect knowledge of 
the channel state. 9 As is to be expected, swapping will perform better when the channel errors are bursty. 
The idea is that when a channel is in a sustained bursty state, it is ad- vantageous to let other channels 
transmit and get com- pensation later, Thus, the tables show that SwapA is the preferred algorithm especially 
when the chan-nel state is strongly positively correlated. However, Source 2 s delay is slightly higher 
with swapping as compared to the case when there is no swapping. HON-cvcr this can be adjusted by changing 
the debit param-eter as illustrated in a later example with five sources. Example 2: We consider an 
example with the same pa-rameters as in Example 1 , except that instead of setting an upper limit on 
the number of retransmission attempts per packet, we set an upper limit on the maximum delay of a packet 
to be 100. If a packet is in the system for more than 100 time slots, then it is dropped; this could 
possibly hap-pen even before it reaches the head-of-the-queue. Thus, we now assume that the sources are 
also delay-sensitive. We let Pu +Pe = 0.1 and the results are shown in Table 4. From the results, it 
should be clear that this example complements Example 1 by leading to the same conclusions regarding 
the relative performances of the various algorithms when the sources arc both delay and loss sensitive. 
Example 3 We consider a three-source example with the channel and source parameters as in Table 5. Source 
l s ar-rivals occur according to an MMPP process with the modu- lating Markov chain as in Example 1, 
Source 2 s arrivals are Poisson and Source 3 s arrivals have a constant inter-arrival time. The arrival 
rate for Source i is denoted by Xi. The maximum number of credits, debits and retransmissions are chosen 
as in Example 1. The delay and loss performance of Blind WRR, WRR-P and SwapA-P are shown in Table 6. 
Again this exam- Table 4: Results for Example 2 Source Xi 1 1 0.2 \ 0% 0:;3 2 0.25 1 0.095 0.005 3 
0.25 1 0.09 0.01  Table 5: Example 3. Source and Channel Parameters ple illustrates that SwapA-P trades 
off the performance of a severely errored channel (in this case Channel 1) against the less error-prone 
channels in a better fashion than WRR-P. For instance compare to WRR-P, dr is decreased by 26% while 
the delay for dz increase 6% and the de increases by 15%. The increases in the delays of Sources 2 and 
3 can be further controlled by suitable choice of upper limits on cred- its and debits as will be shown 
in a later example. The main conclusion from the examples so far is that SwapA-P alloys greater flexibility 
in hiding errors from a source. However, when the number of sources is larger, the differences between 
WRR and SwapA depend on how heavily loaded the system is as shown in the following examples. Example 
4: We consider an example with five sources. The channels for all five sources are assumed to evolve 
accord-ing to independent d&#38;ret&#38;me two-state Markov chains. Sources 2 and 4 are assumed to generate 
arrivals according to independent Poisson processes, and Sources 1, 3 and 5 are MMPP sources with the 
underlying Markov chain hav-ing the same parameters as that of the MMPP source in Example 1. The arrival 
rate of source a is denoted by Xi and the parameters of the sources and channels are given in Table 7. 
The WFQ weights T;, i = 1,2,3,4,5, are all assumed to be 1, the maximum number of retransmissions for 
Sources 1,2,3 and 5 is set to 2, and is set equal to zero for Source 4. Note that the arrival rates for 
Sources 2 and 4 are so high that their delays would be unbounded. In other words, Sources 2 and 4 have 
packets to send almost all the time, and the only measure of performance that is relevant to these sources 
is the throughput, or equivalently, packet loss probability. The maximum number of debits and credits 
for all sources is set equal to 4. The performance re-sults are presented in Table 8. In order of avoid 
excessive use of space, we only present average delay and loss prob- ability in the table. The main conclusion 
from Table 8 is that SwapW-P is clearly superior to the other algorithms that use one-step prediction. 
Both Examples 1 and 4 show that there is not a significant advantage to swapping slots within a frame, 
but swapping slots across multiple frames, and using credits and debits is clearly superior to WRR.. 
dl 11 dz 12 ds 1 1s Blind WFlFL 41.6 0.134 1.1 0.021 10.9 I 0.038 WRR-P 59.2 0 1.7 0 8.0 1 0 SwapA-P 
44.0 0 1.8 0 9.2 1 0 Table 6: Example 3. Average Delay and Loss Performance Table 10: Example 6. Source 
and Channel Parameters Table 7: Example 4. Source and Channel Parameters Table 8: Example 4. Delay and 
Loss Performance Example 5: We now present a situation where WM.-P performs as well as SwapA-P. Consider 
the same parameters as in Example 4, except that the arrival rates for Sources 2 and 4 are now assumed 
to be equal to 0.07. This system is stable since 6 ClXiPGt C 1, i=l and Ai < PO; 9 Vi Average delay 
and loss probabilities are shown in Table 9. The performance of WRR-P and SNapA-P are virtually identical. 
The reasons for this are two-fold: . Since the number of sources is 5, the frame size for WRB is 5 and 
thus the chance of the same channel being in error in multiple frames is small. Thus, credits are not 
accumulated very often in SwapA-P. . WRR-P naturally allocates credits because of the stability of the 
system. In other NOrd.5, if a source is skipped over during a frame due to error in its chan- nel, it 
will automatically be compensated later because other sources will eventually run out of packets due 
to the system stability condition. However, as the following example shows, it would be erro-neous to 
conclude from the previous example that WRR and SwapA are virtually identical when the number of sources 
is large and the stability condition is satisfied. Example 6: Consider a five source example with the 
chan- nel and source parameters given in Table 10. Note that Sources 1 through 4 are identical and Source 
5 s channel has a higher steady-state error probability than the rest. We limit the maximum delay to 
200, and the number of credits and debits to 4 each. The average delay and loss proba- bilities for Source 
1 (recall that Sources 1 through 4 are identical) and Source 6 are shown in Table 11 as a function of 
the maximum number of debits D for Sources 1-4, and the maximum number of credits c for Source 5. Swap&#38;P 
per-forms much better than WRR-P in the sense that Source 5 s performance can be traded off more effectively 
against the (a ds 1s wrtTc.P 1 4,: 1 0 1.3 0 7.3 0 6.7 0.03 19.7 0 YwnpkP 1 4.6 1 0 1.6 0 7.3 0 6.0 0.03 
19.1 0 Table 9: Example 5. Delay and Loss Performance Table 11: Example 6. Delay and loss performance 
 performance of the other sources. Further, it alloysone to control thii trade-off by using upper bounds 
on credits and debits. For example, Sources 1-4 could be IONpriority video sources which are loss tolerant. 
Thus, Source 5 s quality has been dramatically improved by Swap&#38;P without sig-nificantly degrading 
the performance of Sources 1 through 4 as compared with WRR-P. The reason for this is as follows: . Under 
WRR-P, even though the system is stable, it takes a long tie for Source 5 to be compensated for errors 
in its channel since the other sources load the system rather heavily. . In contrast, SwapA-P hides short 
error bursts from Source 5 by providing compensation over shorter time-scales by using credits. It further 
avoid penalizing the less error-prone sources (Sources 1 thorough 4) by up- per bounding the number of 
credits and debits. 9 Related Work Fair packet scheduling algorithms have been the subject of intensive 
study in networking literature, particularly since the weighted fair queueing (WFQ) algorithm ~a.s proposed 
in [5]. The properties of WFQ were analyzed in [5,12]. Sev-eral modifications to WFQ have been proposed 
to address its computational complexity or improve the performance of WFQ, two notable ones being the 
self-clocked fair queueing (SCFQ) [7] algorithm and WF2Q(1]. Recent modifications to accommodate time-varying 
server capacity include STFQ [S]. While most of the above algorithms can handle time-varying server capacity 
with minor or no modifications, none of them handle the case when the variation in the capacity is location-dependent. 
In particular, we are not aware of any scheduling approach that reasons carefully about what to do when 
the shared channel is available to only a subset of the backlogged flops at any time. In addition to 
the fair queue- ing paradigm, there are several other paradigms for fair al- location of a shared channel 
(surveyed comprehensively in [14]). One such approach, which proposes to achieve long-term fairness at 
the expense of penalizing flonrs that use oth- erwise idle capacity, is Virtual Clock [13]. Section 3 
points out the fundamental differences in the compensation model for IWFQ and Virtual Clock. While wireline 
fair scheduling has been extensively re-searched, wireless fair scheduling is relatively unchartered 
territory. Typically, most related -ivork on fairness in wire- less channels approach it from a network-centric 
view (e.g. probability of channel access is inversely proportional to the contention perceived at the 
location of the host [3]), or pro- vide fairly simplistic definitions of fairness [6]. In particular, 
most of the work in this area has been performed from the perspective of wireless medium access, where 
the empha-sis has been on the mechanisms of channel access once the scheduling algorithm has been worked 
out [6, 9,101, rather than the other way around. In recent related work, some solutions to providing 
per-formance guarantees in the presence of the channel con-tention and dynamic reservation problems have 
been ex-plorcd [4, 10). The underlying idea is to combine the best features of some contention-based 
schemes like CSMA and contention-free schemes like TDMA. Performance analysis in terms of throughput 
and delay has been obtained [4, lo]. However, there are three major limitations of this approach. Firstly, 
channel errors and packet loss during transmission are ignored; second, the issue of location-dependent 
channel capacity is addressed only partially (as a function of con- tention); lastly, the scheduling 
issues in the higher level are typically unaddressed. As we argue in Section 1, they have to be studied 
together for an effective solution in the wireless domain. Finally, a recent work on channel state dependent 
packet (CSDP) scheduling does address the issues of wireless medium access with a view to handling not 
only contention but also location-dependent error bursts [2]. However, it does not address the issues 
of fairness, throughput and delay guaran- tees,  10 Conclusions Emerging indoor and outdoor packet cellular 
networks will seek to support communication-intensive applications which require sustained quality of 
service over scarce, dynamic and shared wireless channels. One of the critical requirements for providing 
such support is fair scheduling over wireless channels. Fair scheduling of delay and rate-sensitive packet 
flows over a wireless channel is not addressed effectively by most contemporary wireline fair scheduling 
algorithms be-cause of two unique characteristics of wireless media: (a) bursty channel errors, and (b) 
location-dependent channel capacity and errors. Besides, in packet cellular networks, the base station 
typically performs the task of packet schedul-ing for both downlink and uplink flows in a cell; however 
a base station has only a limited knowledge of the arrival pro-cesses of uplink flows. In thii work, 
we first propose a new model for fairness in wireless scheduling. This model adapts fluid fair queueing 
to wireless channels. We then describe an idealized wireless packetized fair queueing algorithm which 
approximates the wireless fluid fairness model under the as-sumption that the channel error is fully 
predictable at any time, For our idealized algorithm, we derive throughput and delay bounds for both 
error-free and error-prone chan-nels. Finally, we describe a practical wireless scheduling algorithm 
which closely emulates the idealized algorithm, addresses several implementation-related wireless medium 
access issues, and uses simple one-step channel prediction. We observe that though the worst-case performance 
of the scheduling algorithm is much worse than the idealized algo-rithm, the average-case performance 
is remarkably close. References [l] J,C.R. Bennett and H. Zhang, WF2Q: Worst-case fair weighted fair 
queueing, Proc. IEEE INFOCOM 96, March 1996. [2] P. Bhagwat, P. Bhattacharya, A. Krishma and S. Tri- 
pathi, Enhancing throughput over wireless LANs us-ing channel state dependent packet scheduling, to ap-pear 
on Proc. of IEEE INFOCOM 97. [3] V. Bharghavan, A. Demers, S. Shenker and L. Zhang, MACAW: A Medium Access 
Protocol for Indoor Wire-less LANs, Proc. ACM SIGCOMM 94. [4] C. Chang, J. Chang, K. Chen and M. You, 
Guaranteed quality-of-service wireless access to ATM, preprint, 1996. [5] A. Demers, S. Keshav and S. 
Shenker, Analysis and simulation of a fair queueing algorithm, Proc. ACM SIGCOMM 89. [6] M. Gerla and 
J. T. Tsai. Multicluster Mobile Mul-timedia Network, ACM Bdtzer Journal of Wireless Network, August 1995. 
[7] S. Golestani, A self-clocked fair queueing scheme for broadband applications, Proc. IEEE INFOCOM 
94, June 1994. [S] P. Goyal, H. Vin, and H. Cheng, Start-time fair queu- ing: a scheduling algorithm 
for integrated services packet switching networks, Proc. of SIGCOMM 96, August 1996. [91 M.J. Karol, 
2. Liu, and K.Y. Eng. An efficient demand-assignment multiple access protocol for wire- less packet (ATM) 
networks, ACM Gourd on Wire-less Networking, December 1995. [lo] A. Muir and J. J. Garcia-Luna-Aceves, 
?Supporting real-time multimedia traffic in a wireless LAN, Proc. SPIE Multimedia Computing and Networking 
1997, February 1997. [II] A. Parekh. A Generalized Processor Sharing Approach to Flow Control in Integrated 
Services Networks, PhD Thesis, MIT LAboratory for Information and Decision Systems, Technical Report 
LIDS-TR-2089 1992. [12] A. K. Parekh and R. G. Gallager, A generalized pro-cessor sharing approach to 
flow control in integrated services networks: the single-node case, IEEE/ACM !hnsnctions on Networking, 
l(3), pp. 344-357, June 1993. 1131 L. Zhang, Virtual Clock: a new traffic control al-gorithm for packet 
switching networks, ACM lhns. Comput. Syst., vol. 9, pp. 101-124, May 1991. [14] H. Zhang, ?%rvice disciplines 
for guaranteed perfor-mance service in packet-switching networks, Pmt. of IEEE, 83(10), October 1995. 
 
			