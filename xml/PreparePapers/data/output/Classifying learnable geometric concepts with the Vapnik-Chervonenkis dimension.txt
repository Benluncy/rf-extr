
 Classifying Learnable Geometric Concepts with the Vapnik-Chervonenkis Dimension (extended abstrac0 
Anselm Blurner I) Andrzej Ehrenfeucht 2) David Haussler 3) Manfred Warmuth 4) 1) and 3) Department of 
Mathematics and Computer Science, University of Denver, Denver, Colorado 80208. 2) Department of Computer 
Science, University of Colorado, Boulder, Colorado 80302. 4) Department of Computer and Information Sciences, 
University of California, Santa Cruz, California 95064. Abstract We extend Valiant's learnability model 
to learning classes of concepts defined by regions in Euclidean space E". Our methods lead to a unified 
treatment of some of Valiant's results, along with pre- vious results of Pearl and Devroye and Wagner 
on distribution-free convergence of certain pattern recog- nition algorithms. We show that the essential 
condition for distribution-free learnability is finiteness of the Vapnik-Chervonenkis dimension, a simple 
combina- torial parameter of the class of concepts to be learned. Using this parameter, we analyze the 
complexity and closure properties of learnable classes. Authors A. Blumer and D. Haussler gratefully 
acknowledge the sup- port of NSF grant IST-8317918, author A. Ehrenfeucht the support of NSF grant MCS-8305245, 
and author M. Warmuth the support of the Faculty Research Committee of the University of California at 
Santa Cruz. Part of this work was done while A. Blumer was visiting the University of California at Santa 
Cruz and M. Warmuth the Univer- sity of Denver. Permission to copy without fee all or part of this material 
is granted provided that the copies are not made or distributed for direct commercial advantage, the 
ACM copyright notice and the title of the publication and its date appear, and notice is given that copying 
is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires 
a fee and/or specific permission. &#38;#169; 1986 ACM 0-89791-193-8/86/0500/0273 $00.75 1. Introduction 
Valiant has recently introduced a new complexity-based model of learning from examples and illustrated 
this model by exhibiting and analyzing several learning algorithms for classes of Boolean functions [V84,V85]. 
In this paper we extend Valiant's model to learning concepts defined by regions in Euclidean space E 
r. Our methods lead to a unified treatment of some of Valiant's results, along with pre- vious results 
of Pearl [P78,P79] and Devroye and Wagner [DW76]. In learning a class C of concepts from examples, a 
single concept is selected from C and we are given a finite set of points that are in the concept (positive 
examples) and a finite set of points that are outside the concept (negative examples). This entire set 
is called a labeled sample. A learning algorithm for C is a func- tion that, given a labeled sample of 
a concept in C, returns a region in E r that is its hypothesis as to which concept the sample represents. 
As in [V84], we let P be a fixed probability distri- bution on E r and assume that samples are created 
by drawing points at random based on this distribution. After drawing m points, the algorithm forms a 
hypothesis. The error of this hypothesis is taken to be the probability that it disagrees with the real 
concept on the next point drawn, i.e. the error is just the probabil- ity of the region that represents 
the symmetric differ- ence between the hypothesis and the real concept. What we want in a learning algorithm 
is the following: (1) For large enough m we should get arbitrarily small error with arbitrarily high 
probability, no matter which concept from C we are trying to learn. The bounds on m should be independent 
of the underlying 273 distribution P. (2) In addition, the bounds on m should be poly- nomial in the 
inverse of the error probabilities, and the computation of the hypothesis should be polynomial in the 
length of the sample. A class of concepts that fulfills (1) is called learn-able. If (2) holds as well 
then the class is polynomially learnable. Condition (1) is formalized by demanding error greater than 
e with probability at most 5 for small e and 5, uniformly for all concepts in C. Condition (2) implies 
that the number of required samples is a func- tion m (e,5) that grows polynomially in lie and 1/5. Our 
main result gives necessary and sufficient conditions on a class of concepts C for the existence of a 
learning algorithm satisfying (1). Our criterion is a simple combinatorial one, based on the work of 
Vapnik and Chervonenkis on uniform approximation by empir- ical distributions [VC71]. This approach was 
pioneered in [DW76] [P78]. We define a parameter called the Vapnik-Chervonenkis dimension of the class 
C of concepts 1 and show that there is a learning algorithm satisfying (1) if and only if this dimension 
is finite. Moreover, if C does have finite dimension d then there is a learning algorithm for C, uniformly 
achieving error no more than e with probability at least 1 -~, using ~sample size m (e,5) = max/[~,og~,.~_log.~_-j. 
Hence the polynomial restric- tion on the growth of m (e,5) is automatic in this case. In fact any function 
from labeled samples into the class C that gives hypotheses consistent with the sample is a learning 
algorithm for C using this many samples. As a corollary, we obtain significantly better distribution- 
free bounds on the convergence of many well-known pattern recognition algorithms including the classic 
perceptron learning algorithm and its successors. 2 In relating the Vapnik-Chervonenkis dimension to 
learnability, we use the concept of an z-net for C with respect to a fixed distribution P. This is a 
finite set of points N such that every concept in C of probability greater than e contains a point in 
N. This generalizes the z-net idea from [HW85], which lead to improved time bounds for half-space range 
queries in all Euclidean spaces with dimension greater than one. The key to both the query and the learning 
results is to prove bounds on the frequency of occurrence of small e-nets for classes of concepts (ranges) 
with finite Vapnik-Chervonenkis dimension. We are confident that i This number is one less than the number 
V(C) defined in [D78]. a 1 .L_L I Previous bounds were O (max(~Iog e' e2z°s'g)) [DW761, obtained using 
the results of [VC71 ] directly. this will have other applications in computational geometry besides 
those given in [HW851. Finally we deal with the case of polynomial lear- nability. Following Pearl [P78] 
and Valiant [V84], we introduce the idea of the complexity, n, of a concept and consider the learning 
problem for a sequence of concept classes {Cn}n a z, where C,, represents the set of concepts of complexity 
at most n. We show that if the Vapnik-Chervonenkis dimension of C,, grows fas- ter than polynomial in 
n then this sequence of concepts classes is not learnable in the sense of Valiant. On the other hand, 
we identify certain general strategies related to the principle of preferring the simpler hypothesis 
(Occam's Razor) that in many cases give learning algorithms that satisfy conditions (1) and (2). (Condition 
(2) is modified to include polynomial dependence on the complexity n.) These results are illustrated 
with several learning algorithms for various concept classes. Our analysis also leads to a host of open 
problems, some of which are outlined in the final section. Notation: SAT denotes the symmetric difference 
of sets S and T. For ScX, Is denotes the indicator function for S onX, Is(x)= 1 ifx ~ S,Is(x)= 0 oth- 
erwise. X" denotes the m-fold Cartesian product ofX. Elements of X = will be denoted by barred variables, 
e.g. ~ or y. We assume ~ denotes (x 1,...,x,,) where xi ¢ X, 1 _< i _< m, when m is clear from the context, 
and similarly for other barred variables. If P is a pro- bability measure on X then P m denotes the m-fold 
pro- duct probability measure on X =. Natural logarithms are written "In", all other logarithms are base 
2. 2. Learnabifity We will use the following notions of learning algorithms and learnability. Definition. 
A concept class is a set C ~ 2 x of con-cepts. In this paper it is assumed that X is a fixed set, usually 
finite, countable, or E" (Euclidean r-dimensional space) for some r _> 1. In the latter case, we assume 
that each c e C is a Borel set. Elements of X m will be called m-samples (or just samples). A labeled 
m-sample is an m -tuple (<xl,al > ..... < xm,a,,,>), where ai E {0,1}, 1 _< i _< m, and ai = aj if xi 
= xj. For x'= (xl,...,xm)~ X m, the labeled m-sample of c ~ C generated by ~-is given by samc(x~ = (<x 
b/c(X 1) > ..... < XmJc(Xm)>). The sample space of C, denoted Sc, is the set of all labeled m-samples 
over all c e C and all ~ e X m for all m>_l. A denotes the set of all functions A : Sc ~ H, where H is 
a set of Borel sets on X. (In fact, we would like the range of A to be C, but we may gain some power 
by allowing A to approximate concepts in C.) Elements in H are called hypotheses. A e A is con-sistent 
if its hypothesis always agrees with the sample, i.e. whenever h -- A ( < x l,a I > "'" < Xm,am > ) 
then for all i, 1 _< i _< m, ai -- Ih(Xi). For anyA ~ A, proba- bility measure P on X, c ~ C, and ~ ~ 
X m, the error of A for concept c on sample ~ (w.r.t. P ) is given by errorA,c,p(X~ = P (c AA (same(T))). 
Thus A's error is measured as the probability of the region that forms the symmetric difference between 
the real concept and A's hypothesized concept, which is just the probability that A's hypothesis will 
be inconsistent with the real con- cept on a randomly drawn point (w.r.t. P ). Let m (e,5) be an integer-valued 
function of e and 5 for 0 < e, 5 < 1 and P be a probability measure on X. A e A is a learning algorithm 
for C (w.r.t. P) with sample size m (e,5) if for all 0 < e,5 < 1 and for all c E C, Pm(~,~)(errorA,c,e(Y) 
> e) _< &#38; where ~e Xm(e,S). Thus we insist that using a randomly drawn sample of size m (e,5), any 
concept in C can be learned with error no more than e with probability at least 1-&#38; If A is defined 
in such a way that Z = {Y ~ X m(~,s) : errorA,c,e(x~ > e} is not measur-able for some c e C, then we 
require that there exist a measurable Z" such that Z c Z" and Pm(e,8)(Z') <_&#38; We say A ~ A is a learning 
algorithm for C with sam- ple size m (e,5) only when A is a learning algorithm for C w.r.t. P with sample 
size m (e,~5) for all probability measures P on X. If such an A exists, we say C is learnable. Example. 
Consider the problem of learning concepts such as the concept of "medium build", defined (for men) as 
having weight between 150 and 185 pounds and height between 5' 4" and 6' 0". By looking at a finite database 
of randomly chosen men that gives their weight, height and classification (medium build or not), we want 
to form a rule that approximates the true con- cept of medium build, and we want our approximation to 
be accurate independently of the underlying distribu- tion on height-weight pairs (height and weight 
values are not assumed to be independent). This type of learning problem is formalized and solved as 
follows. Let X be E 2, C be the set of all orthogonal closed rectangles, (i.e. products of closed intervals 
on the x- axis with closed intervals on the y-axis) and P be any arbitrary probability distribution on 
E 2. A simple algo- rithm to learn a concept c e C is the following. Keep track of the minimum and maximum 
x and y coordi- nates of any positive sample point. Let l', r', b', t" denote these four coordinates. 
After drawing a number of sample points, predict that the concept is [1 ,r ]x[b ,t ]. If no positive 
samples are drawn, let h = ~. Call this algorithmA. Theorem 1. A is a learning algorithm for C with sam- 
4 4 ple size .vln (-~ ). Proof. Assume the concept c to be learned is the pro- duct of the intervals 
[l,r] on the x axis and [b ,t] on the y axis. Since A's hypothesis h is always contained in c, ifP(c) 
< e then errora,~,v of any labeled sample of c is always less than e. Otherwise we define four minimal 
side rectangles within c that each cover an area of probability at least ¼: left := m/n ({[l,x ]x[b ,t] 
:P ([l,x ]x[b ,t ])_>~}) and right, bottom, and top are defined similarly. If the sample size is m, the 
probability that a particular side rectangle contains no sample point is at most (1-e/4) m. The probability 
that some side rectangle contains no sample point is bounded by 4 times this quantity. This latter quantity 
is smaller than 4e-m~4, so ff the sample size is at least 4 4 -~-ln(~), then with proba- bility at least 
1-5 we will draw a sample point in each of these four rectangles. If this occurs, then the proba- bility 
of the region given by the symmetric difference ofA's hypothesis and c will be less than e, thus bound- 
ing the error of the hypothesis. Since m is independent of the particular distribution P, the result 
follows. [] This proof readily generalizes to r-dimensional 2r 2r rectangles for r > 2, with a bound 
of -~-ln(-~-), on the sample size. However, it is not clear how to generalize it even in 2 dimensions 
to other types of concepts, e.g. circles, half-planes or rectangles of arbitrary orienta- tion. To show 
that these classes are also learnable, we use a concept first introduced in [VC71]. Definition. Given 
a concept class C and finite S c X, Hc(S) denotes the set of all subsets of S that can be obtained by 
intersecting S with a concept in C, i.e. TIc(S)f{Snc:ceC}. If TIc(S)ffi2 s, then we say that S is shattered 
by C. The Vapnik- Chervonenlkis dimension of C (or simply the dimen- sion of C) is the smallest integer 
d such that no S c X of cardinality d + 1 is shattered by C. If no such d exists, the dimension of C 
is infinite. [VC71], [D78], [WDS1], [A83] and [HW85] give numerous examples of concept classes of finite 
dimen- sion. 3 For example, the dimension of the set of all ~When C is of finite dimension, Dudley calls 
C a Vapnik-Chervonenkis Class (VCC~ [D78], [WD81]. The Vapnik-Chervonenkis number of this class, denoted 
V(C), corresponds to the dimension of C plus one. orthogonal rectangles in E r as defined above is 2r, 
and the dimension of half-spaces and balls of E, is r + 1. Restricting ourselves to the real line, the 
set of all half- lines open to the left has dimension 1, the set of all intervals has dimension 2 and 
in general, the set of all sets of at most k intervals has dimension 2k. In gen- eral, whenever C is 
of finite dimension, then Ck, the set of all Boolean combinations formed from at most k concepts in C, 
is also of finite dimension [D78]. Thus for example, since the set Ck of k -gons in E" for fixed k > 
r is formed by k-fold intersections of half-spaces, Ck is of finite dimension for any finite k. Wenocur 
and Dudley [WD81] also prove more general results that imply e.g. that the concept class formed by set 
of all half-spaces bounded by polynomial curves of fixed degree also has finite dimension. On the other 
hand, the set of all finite sets of intervals, like the set of all open sets or all Borel sets, obviously 
has infinite dimension. Definition. A concept class C is trivial if C is one concept, or two disjoint 
concepts. It is clear that a sample size of one is the most that is required to learn C when C is trivial. 
We can now state our main result. Theorem 2. The following are equivalent for all non- trivial, well-behaved 
4 concept classes C: (i) The dimension of C is finite. (ii) C is learnable.  (iii) If d is the dimension 
of C, (a) for sa~nple size greatel~ than 4 2 8d 8d  max L-~iog~,-rlog~ j , any consistent function 
A : Sc ---> H is a learning algorithm for C and for e < ½ and sample size less than (b) max [~log~,d(1-2(e(1-5)+6)) 
t no function A : Sc ~ H, where C c_ H is a learning algorithm for C. The proof will be given in a series 
of lemrnas and theorems. We begin by showing that (i) implies (iii(a)). The critical property of finite 
dimensional concept classes we use is the following: Definition. For all d _>0 and m _> 1, ~d(m) = ~ 
{tin" 1 ifm >_ d, ~Pd(m ) = 2 m otherwise. Proposition 3. (a) If the dimension of C is d then I Ylc 
(S) I -< ~d(m) for all S ~ X of cardinality m. (b) ¢bd(m)_<md+l for all d_>0 and m~l. ~Pd(m)_<mdforalld 
_>2 and m ~2. A short proof of part (a) above can be found in 4 S~ Appendix. [HW85] or (along with its 
history of independent discoveries) in [A83]. Part (b) is easily verified. To characterize learnable 
concept classes, we use the following, which generalizes the corresponding notion in [HW85]. Definition. 
For any concept class C, probability meas- ure P on X, and e>0, Cpe={c e C:P(c)>e}. S c X is an z-net 
for C (w.r.t. P ) if S contains a point in every c e Cp,~. Examples. IfX is the interval [0,1], P is 
the uniform distribution and C is the set of closed intervals in X, then the set of all points ek, 0 
< k < 1 is an z-net for C for any e > 0. In fact C has a net of this size for any distribution P on X. 
On the other hand, if C is all open sets, then clearly there are no finite z-nets for C w.r.t, the uniform 
distribution. For the next two lemrnas and the following theorem, let C be a fixed well-behaved concept 
class and P be a fixed probability measure on X. The proofs of these lemmas are analogous to those of 
Lernma and Theorem 2 of [VC71]. These results generalize Lem- mas 3.5 and 3.6 and Theorem 3.7 of [HW85]. 
Definition. For any m _> 1 and e > 0, Q ~ denotes the set of all ~- e X = such that the set of distinct 
elements of £- do not form an z-net for C w.r.t. P, i.e. such that there exists c e Cp,~with ~ n c = 
0. j~m denotes the set of all ~ e X ~, where ~-,y e X ", such that there exists c e Ct,,~, where T n 
c = O and I {i : Yi e c, 1 _< i _< m } I -> --~, i.e. no element of c occurs in the first half of the 
sequence, but elements of c occur with frequency at least -~- in the second half. Lemma 4. For any e 
> 0 and ---~-,m > 8 pm(a g,) _< 2p 2m(j 2m). Lemma 5. If C is of finite dimension d, --O'~t P2m(Je2=) 
_< ~d(2m)2 -y- for all m >_ 1. Proof. For each j, 1 _< j _< (2m)[ let roy be a dis- tinct permutation 
of the indices 1 ..... 2m. It is clear that for all permutations rcj. Hence 1 ( Thus it suffices to show 
that 1 (/~_! --e,. ~. Ij~(nj (xD) _< 't'd(2m )2 -r- for all~e X ~. Consider a fixed £- e X 2m. Let S 
be the set of dis- tinct elements of X that appear in ~. Since IS I -< 2m and C is of dimension d, there 
are at most Oa(2m) dis- tinct subsets of S induced by intersections with c ~ Cp,e (by Proposition 3 part 
(a)). Each such subset T of S is a witness to the fact that certain permutations of ~-are in j~,n. Specifically, 
whenever all occurrences of members of T appear in the second half of ~j(T) and there are at least -~ 
such occurrences, then ~j(X~ E jam, otherwise ~j(~')~j~,n. However, for a given T this can occur in only 
a small fraction of all permutations of ~. In particular, if there are l occurrences of members of T 
in ~, then T is a witness for at most m(m-1)...(ra-l+l) <2-z <2T = 2m(2m-l)...(2m-t+l) of all permutations 
of ~. It follows that I (2n~O! _ < --on j _-~ I.l ~ rc J ( X~ - dP a ( 2m ) 2--T-. [] Directly from the 
above two lernrnas we get Theorem 6. If C has finite dimension d and m > 8 g --gin then P ~(Q ~) _< 2Od(2m)2 
-~-. Since the negative exponential term dominates the polynomial ~d(2m), this shows that the probability 
of not getting an e-net for C in an m-sample goes very rapidly to zero for large m. The m required for 
this probability to be less than 8 is estimated in the follow- ing, m _> max log , log then 2Oa(2m)2 
T _< 8. Theorem 8. If C is well-behaved and has finite dimen- sion d then for any .d~istributi°n P o~ 
X and any 0 < e,8 _< 1, if m _> max:[-~loge "~2 ,--~log--~-e ej'-I points in X are drawn at random according 
to P then the probabil- ity that these points do not form an z-net for C w.r.t. P is at most 8. Proof. 
Follows directly from Lemma 7 and Theorem 6.[] To complete the proof that (i) implies (iii(a)), we need 
the following. Lemma 9. For any z ~ X, the dimension of C equals the dimension of C~, where Ct~ = {cAz 
: c ~ C}. Proof. This follows from the result in [W86] that the dimension of C is unchanged if for any 
x ~ X, each c ~ C is replaced bycu{x}ifx~c andc-{x}if XEC.~ Proof that (i) implies (ill(a)). Let A : 
Sc ~ C be a consistent function in A, P be a distribution on X and c be a fixed concept in C to be learned. 
If the dimen- sion of C is d, then the dimension of Ca~ is d by the above lemrna, where C~ = {cAc': c'e 
C}. Hence by Thegrem 8, if w~ draw a sample of size > K[4 2 8d 8d] m _max .~-log~,-~-log-E-, we get 
an e-net for C~ w.r.t P with probability at least 1 -6 for any distribu- tion P. If we get an e-net for 
C ~ then for any c" e C produced by A on the corresponding labeled sample we must have P (c Ac').ge, 
otherwise the hypothesis would be contradicted by a sample point. Hence the error of A is at most e with 
probability at least 1 -8. [] Proof that 0ii(a)) implies (ii). We can simply well- order the concepts 
in C and for each labeled sample choose the first concept that is consistent with this sam- ple. (Note 
that we are not concerned with the computa- bility of A here.) [] It remains to show that (i) implies 
(iii(b)) and (ii) implies (i). Lemma 10. (i) implies (iii(b)): Let C be a non-trivial concept class of 
finite dimension d and e < 3" Then any learning a~gorithm for C must use sample of size at least m _> 
max [~,og~r, a(1-2(~1- s) + 8))]. Proof. We begin with the first term of the lower bound, considering 
two cases for the non-trivial con- cept C. The first is that C contains two distinct con- cepts c 1 and 
c 2 that have a non-empty intersection. The second case is that all concepts in C are pairwise dis- joint, 
and there are at least three concepts in C. We give the proof only for the first case, the other argu- 
rnent being similar. Let A be a learning algorithm for C and let a e X be a point in c tnc2 and b~X be 
a point in (c 1 u c 2) - (c 1 n c 2). Without loss of generality, assume that b E c 2. Let P be the probability 
distribu- tion such that P (b) = e, P (a) = 1 -e and P (x) = 0 for any other point x e X. With respect 
to this distribu- tion, we can effectively replace X with the set {a,b } and H with the four subsets 
of {a,b }, modifying A accordingly. It is easily verified that if the sample size m is less than 1 1 
-~-log..~-, then the probability of drawing the point a each time is greater than 8. Since e < 3' this 
holds for m < .~log~. 1 1 Assume the concept to be learned is either {a } or {a,b }. We can divide the 
pos- sible learning algorithms for C into four types, depend- ing on how they respond to a labeled m-sample 
in which each point is a (the label will always be 1). If the algorithm responds by producing the hypothesis 
{a }, then for this sample it has error e ff the concept to be learned is {a ,b }. If it responds {a 
,b }, {b } or then it has error at least e if the concept to be learned is {a }. In any case, there is 
a concept in C such that the probability that A produces a hypothesis of error at least e with sample 
size m is greater than 8. By increasing the probability of a slightly and decreasing the probability 
of b by the same amount it follows that that the previous statement also holds for error strictly larger 
than e instead of error at least e. We conclude A cannot be a learning algorithm for C with sample size 
 1 1 m < -~log-ff. For the second term in the lower bound, note that since C is non-trivial, the dimension 
d of C is at least I. There must exist a set Xd of d points in X that are shattered by C. Let the probability 
distribution P on X be uniform on these points and 0 everywhere else. With respect to this distribution, 
we may replace X with Xa and C with 2 xd. Suppose we draw an m-sample ~"of X and I dif- ferent points 
are observed in this sample. For each of the 2 t possible labelings of T, there are 2 a-t concepts consistent 
with this labeling. Whatever the hypothesis of the learning algorithm, for every point of X not observed 
in ~, it will be correct for exactly half these concepts. Thus, the average error of the learning algo- 
 rithm on ~-over all concepts in C is at least d-t >_ ..~_. This implies that the average error of the 
learning algorithm over all ~- ~ X m and all concepts in C is at least -~-. Hence there must be a concept 
with average error at least -~. To make the frequency of m-samples in which the error on this concept 
is greater than e at most 5, the error can be greater than e (i.e. I) on at most 152 m of the m-samples, 
and must be at most e on the remainder. Hence the average error can be at most e(1 - 5) + 5, which gives 
the second part of the lower bound. [] Proof that (ii) implies (i) This follows from the second lower 
bound in the above lernma. This completes the proof of Theorem 2. [] The above theorem shows a significant 
gap in the complexity of geometric learning problems as meas- ured by the sample size needed: either 
the set of con- cepts is learnable with sample size O (Tlog-~) or it is I 1 not learnable at all. Furthermore, 
this result is easily extended to probabilistic learning algorithms (i.e. algo- rithms that for a given 
sample choose a hypothesis pro- babilistically) and to algorithms that use an oracle to query the values 
of particular points (as defined by Valiant [V84]). We need only observe that if the Vapnik-Chervonenkis 
dimension is not finite, then no matter how the ra points are drawn in the proof of Lemma 10 and no matter 
how the hypothesis is formed from the values on these points, the algorithm can do no better than random 
guessing on the points that it has not seen. Theorem 2 has immediate consequences for several well-known 
pattern recognition algorithms. For example, since the Vapnik-Chervonenkis dimension of positive half-spaces 
in E' is r, the sample size required for the perceptron learning algorithm (see e.g. [YC74], pg. 120) 
to achieve error at mostle with probability at least 1-5 is max i, independent of the underlying distribution 
governing the selection of sam- pies. Since 5 appears only in a log term, this implies that good linear 
separators can be found with very high probability from relatively small random samples. Furthermore, 
the same bound applies to any learning algorithm for half-spaces that eventually produces con- sistent 
hypotheses, e.g. the Ho-Kashyap algorithm [YC74] or Meggido's prune-and-search linear pro-gramming technique 
[M84]. The best previous distribution-free bound for these algorithms was -~-trm--~--16"" derived directly 
from the 16r + InS) [DW76], results given in [VC71]. In particular cases, such as the orthogonal rectan- 
gles example in Theorem 1, we can tighten and sim- plify the bounds on the sample size given in Theorem 
2. The same is true for certain linear separator algo- rithms in E 2 [L85]. 3. Occam's Razor In this 
section we examine the computational feasibility of learning various concept classes of finite dimension 
and extend the learnability definitions of the previous sections by adding a parameter that defines the 
size or complexity of the concept to be learned [P78] [V84]. Definition. A concept class C is polynomially 
recog- nizable if there exists a polynomial time algorithm that, given any labeled m-sample, produces 
a concept c e C consistent with this sample, or returns false if no such concept exists. An algorithm 
of this type will be called a recognition algorithm. A concept class C is polynomially learnable if there 
exists an algorithm A and a function m(~,5), polynomial in T I and {, such that A is a learning algorithm 
for C using m (e,5) sam- ples and A runs in time polynomial in the length of the sample. If C is polynomially 
recognizable and has finite dimension, Theorem 2 shows that C is polynomially learnable. Since finding 
a consistent hypothesis can often be reduced to linear programming, Meggido's results [M84] show that 
many interesting concept classes have linear recognition algorithms. For exam- pie, ifX = E 2 then the 
classes of circles, and in general concepts defined by curves of degree at most k (i.e. regions of the 
form p (x,y) <_ 0 for some k-degree poly- nomial p, fixed k) have linear recognition algorithms and hence 
are polynomially learnable by Theorem 2. Similar results hold for X -- E" for fixed r, however the time 
complexity of Meggido's technique grows doubly exponentially in the dimension of the linear programming 
problem [LP84]. Definition. {Cn}n a I will always denote a sequence of concept classes Cn with Cn ~ 
C,~+I, for all n >_ 1. {Cn },~ a 1 is polynomially learnable ff there exists an algorithm A and a function 
m (e,8,n), polynomial in I 1 and n, such that for all n > 1, A is a learning T'~' algorithm for C,, using 
m(e,8,n) samples andA runs in time polynomial in the length of the sample. If C = ,~1C,~, then C,, represents 
the set of all concepts of C of complexity at most n. We do not assume that a sample of a concept from 
Cn ~ C contains any specific information about the index n. If the dimension of C,, grows faster than 
polyno- mially in n then {Cn }nal is not polynomially learn- able, by Theorem 2. This shows for example 
that when X is a countably infinite dimensional real space and for every n _> 1, Ca is the set of all 
regions in the Boolean closure of at most n half-spaces in E '~, where for any point in c E C,,, only 
the first n coordinates are nonzero, then C, is not polynomially learnable. To see this, consider 2 n 
points at the comers of the n-cube. Any subset of these points can be defined as a Boolean combination 
of n-dimensional half-spaces whose defining hyperplanes are each normal to one of the ffirst n coordinate 
axes. Thus these points are shattered by Cn and hence the dimension of C,~ is at least 2 n. This also 
shows that if Cn is the set of all regions defined in a countably infinite dimensional discrete space 
by Boolean formulas of n variables, then {Cn}n ~ 1 is not learnable. On the other hand, ff the dimension 
of Cn grows polynomially in n and there is an algorithm that, given an m-sample of a concept in Cn, always 
produces a consistent hypothesis in C,~ and runs in time polyno- mial in the length of the sample, then 
Theorem 2 shows that {Cn}nal is polynomially learnable. For example, if C,, is the set of orthogonal 
rectangles in E n (which has dimension 2n), where, as above, for any point in c E Cn, only the first 
n coordinates are nonzero, then {C,,},,al is polynomially learnable using the algorithm of Theorem 1. 
To show polynomial learnability when C,~ is the set of n-dimensional half-spaces or balls, one must abandon 
Meggido's technique for finding consistent hypotheses in favor of a linear programming technique that 
is polynomial in both the number of con- straints and the (linear programming) dimension, e.g. Karmarker's 
technique [K84]. However, this polyno- mial bound depends on restricting the real numbers involved to 
a fixed finite precision, which runs contrary to the usual model adopted in computational geometry (which 
we have also implicitly adopted here). In a discrete space, Theorem 2 shows that when- ever the cardinality 
of the set of functions that define the regions in C,~ is O(~(n)) for some polynomial p(n), then any 
polynomial algorithm that produces consistent hypotheses in Cn for any sample from a con- cept in C,, 
is a polynomial learning algorithm for {Cn},~al. This is because when I Cnl -< 2e00, at most p(n) points 
can be shattered by Cn, and hence the dimension of C,, is at most p (n). This result applies, e.g., when 
Cn is the set of all functions of a particular type that can be described using at most p(n) bits [P78] 
[V84]. As another example, let Cn be the set of all regions defined in a countably infinite dimensional 
discrete space by k-CNF (DNF) Boolean formulas of at most n variables ([V84]). Since each clause in a 
k- CNF (DNF) involves at most k variables, I C,~l -< 2 ((2n~). This shows that for fixed k, any algo- 
rithm that produces a k-CNF (DNF) formula consistent with the sample in polynomial time (and in particular, 
the algorithm given by Valian0 is a polynomial learn- ing algorithm for {Cn }n~l. However there are simple 
examples where Theorem 2 does not apply directly. Let Cn be all unions of at most n (possibly overlapping) 
closed orthogonal rectangles in E 2. Lemma 13 (below) shows that the dimension of Cn is polynomial in 
n. However, given a set of points in E 2 labeled with O's and l's, it is NP-hard to determine the smallest 
n such that the set of l-labeled points can be covered by n closed orthogonal rectangles, where none 
of these rectangles contains a 0-labeled point [M78]. Hence given positive and nega- tive points from 
a concept in Cn for unknown n, it is NP-hard to determine the smallest n such that there exists a concept 
in C,~ consistent with the sample. Any learning algorithm that always gives a consistent hypothesis in 
C,, for any sample of a concept in Cn implicitly solves this optimization problem. The upshot of this 
is that to apply Theorem 2 directly, we need to find an efficient learning algorithm that works according 
to the principle of always prefer- ring the simplest hypothesis that explains the data, usu- ally called 
Occam's Razor. Yet in many cases this is not possible. To address the cases when it is not feasi- ble 
to find the simplest hypotheses, we will show that it suffices to settle for simpler rather than simplest 
 hypotheses, i.e. it suffices to produce hypotheses that are significantly simpler than the sample data 
itself. Definition. An Occam algorithm for {Cn}nal is an algorithm that, given any m-sample of a concept 
c Cn, produces in polynomial time a consistent hypothesis h H,,,m, where Hn,~ is a concept class that 
includes Cn and has dimension at most f(n,m). f (n,m) is called the range dimension of the Occam algorithm. 
In this version of Occarn, the dimension of the hypothesis class C, is the measure of simplicity and 
the range dimension f (n ,m) tells how well the Razor is applied. If the dimension of Hn,m is equal to 
that of C,~ for all n and m then the Occam-algofithm implements the strictest form of Occam's Razor. 
An example of this occurs when H,~,,~ = C,,, as in Valiant's algorithm for learning k-CNF, or the algorithm 
given above for learning orthogonal rectangles in E n. In this case the range dimension does not depend 
on m. For faster growing range dimensions, especially those that depend on m, the Occam-algofithm can 
take advantage of the larger range of hypotheses to simplify its search for a consistent hypothesis. 
The following shows how fast the range dimension can grow, still maintaining poly- nomial learnability. 
Theorem 11. IfA is an Occam algorithm for {Cn},~ a 1 with range dimension f (re,n) _< rn km a, for some 
k _> 1, 0 _< ct < 1 and r _> 2n ~, then A is a polynomial learning algorithm for {C~}, z 1 with sample 
size [~ 2 8rn k . 8rnkl "1"~] m_>max log~,.~-,og.~.l..~3-j j If f (re,n) is bounded by rn k(log re)t, 
then the second bound on m becomes 2t+4rnt [l°g 8(21+2~'+lrnk ] In some ways Theorem 11 can be viewed 
as showing a relationship between learning and data compression. In the discrete case, i.e. for learning 
Boolean functions, let Hn,m be the set of all hypotheses that can be described with n km a bits, for 
some k _> 1 and 0_< a < 1. Then Theorem 11 says that if, for any m-sample of a function that can be described 
in n bits, we can effectively find a hypothesis that "explains" the sample and uses O (n km ~) bits, 
then we can learn. For fixed n, this amounts to a kind of data compression on the sample. Numerical bounds 
on the number of sam- pies needed for learning algorithms in this discrete case that are slightly better 
than those given in Theorem 11 are derived in [BEHW85] using a simpler argument Theorem 11 can be used 
to show that many kinds of concept classes are learnable. In the remainder we will demonstrate how this 
is done when C,, is built by n-fold unions of concepts for a fixed class C 1 of finite dimension. Similar 
results for n-fold intersections fol- low by considering the class of n-fold unions of com-plements of 
concepts in C1. From Lenmia 9, the dimension of the complements of concepts in a class is the same as 
the dimension of the class itself. Lemma 12. If C is polynornially recognizable and the dimension of 
C is finite then for any finite set S ~ X the sets of Hc(S) can be listed in time polynomial in the cardinality 
of S. Proof. Assume S ={xl, x2 ..... xm}. The size of Hc(S) is polynomial in m by Proposition 3. To pro- 
duce a listL of IIc(S) we proceed as follows. Initial- ize L to the one element list consisting of just 
the empty set. This corresponds to the case m = 0. Now by induction, assume that the list L = IIc({Xl, 
X2 ..... xi}) has been produced for some i, 0 _< i < m. L is updated to the list 1Hc({xl, x2 ..... x/+l}) 
as follows. For each element T of L, test the sets T and T u {xi+1} for membership in Hc({xl, x2 ..... 
xi+l}). Since C is polynomially recog- nizable, this can be done in polynomial time by creat- ing the 
appropriate labeled samples and running the recognition algorithm on them. Now replace the ele- ment 
T in L with either one or both of these sets, according to the outcome of this test. (Note that it is 
possible that T e Hc({xl, x2 ..... xi}) but T¢Hc({Xl, x2 ..... xi+l}).) The time for each complete update 
is polynomial since by Proposition 3 the size of L remains polynomial in m. Hence the entire pro- cedure 
is polynomial. [] Lemma 13. If the dimension of C i is k and for all n >_ 1, C, is the set of all n-fold 
unions of concepts in C 1 then the dimension of C, is at most 2kn log(kn ). Proof. Follows from Lemma 
4.5 of [HW85]. [] The following theorem shows for example that n - fold unions of orthogonal rectangles 
in E 2 are polyno- mially learnable even though finding the smallest set of orthogonal rectangles that 
explains the data is NP-hard [M78]. Theorem 14. Assume C 1 is polynomially recogniz- able and the dimension 
of C1 is finite. Let Cn be the set of all n-fold unions of concepts in C1. Then {C,~}n z 1 is polynomially 
learnable. Proof. Let S be the set of points in a labeled m-sample of a concept in C,,. Our strategy 
will be to find a hypothesis consistent with S that is formed from the union of relatively few concepts 
in C 1, i.e. not many more than n. This problem can be formulated as a set cover problem. The set to 
be covered is the set of posi- tive points of S and the sets allowed in the cover are the elements of 
Hc,(S) that contain only positive points. To lind the smallest set cover is NP-hard [GJ78] and remains 
NP-hard for simple geometric ver- sions such as covenng with rectangles [M78]. For-tunately, there is 
a simple greedy algorithm [N69], [J74] that produces a cover using at most n In p sets, where n is the 
minimum number of sets needed for any cover and p is the size of the set to be covered: pick the set 
that covers the largest number of points; after this pick the set that covers the largest number of points 
which haven't been covered previously, and so forth. Since the sets of Hc,(S) can be listed in polyno- 
mial time (Lernma 12), the largest set that contains only positive points can be found in polynomial 
time. Hence the above algorithm is polynomial in m and given any m-sample of a concept in Cn, produces 
a consistent hypothesis for this sample in Cn~(m). Since the dimension of C,, is at most 2kn log(kn ) 
by Lemma 13, this gives an Occam-algorithm with range dimen- sion 0 (n log(m )(log n + loglog m)). Thus 
by Theorem 11, {C,,}n a x is polynomially learnable. [] The above result can also be used to show that 
concept classes not directly formed by n-fold unions of concepts from a fixed class are learnable. For 
example, let X = E 2 and let Cn be the set of all concepts created in the Boolean closure of n half-planes. 
These correspond to subsets of the set of cells formed by an arrangement of at most n lines. Since there 
are O (n 2) vertices in the arrangement, any subset of cells can be triangulated using O(n 2) triangles 
(some possibly unbounded), hence each concept in C,~ can be represented as a union of 0 (n 2) triangles. 
Let C',, be n-fold unions of triangles in E 2. It follows that C,~ ~ C'o(,~ ~. Since the class of triangles 
is polynomi- ally recognizable and has finite dimension (it is 7), {C'n},~l is polynomially learnable 
by Theorem 14. It follows that {Cn},al is polynomially learnable as well.  4. Open Problems One major 
question of interest that remains is the polynomial learnability of {Cn}n~1when Cn is built by n-fold 
unions of _< n-dimensional orthogonal rectan- gles. This is of particular interest because a special 
case of this is the learning problem for DNF formulas of at most n variables and n clauses [V84]. A more 
res- tricted problem is the case that C,, is built by n-fold unions of n-dimensional positive orthogonal 
half-spaces. The discrete version of this problem is the learning problem for monotone DNF formulas. 
Valiant proved that this class is learnable provided that we are given certain oracles [V84], but it 
is not clear whether these formulas can be learned without oracles. It is shown in [VP 86] that if the 
sample is from a monotone boolean function that is the sum of two monomials, then it is already NP-hard 
to produce two monomials whose sum is consistent with the sample. Thus we do not expect to find a polynomial 
time algorithm that finds a consistent monotone DNF with the minimal number of monomials. The techniques 
in the proof of Theorem 14 show that it would suffice to have a poly- nomial algorithm that finds the 
monomial that covers the largest number of positive samples. However, this is also NP-hard by a simple 
reduction from maximal independent set. The aim of this paper has not been to provide tight bounds on 
the number of samples and the computation time needed for various learning algorithms. Certainly there 
are interesting tradeoffs here, hence many open problems remain in this area. The question of incre- 
mental learning, in which individual sample points are processed one at a time and only the current hypothesis 
is maintained and updated, has also not been addressed here. This can be incorporated into the learning 
model by demanding that the space used by the hypothesis be bounded by some function of the complexity 
of the concept to be learned as in [H85]. How does this res- trict the classes of learnable concepts? 
Finally, using results from [GGM84] on poly-random collections, it can be shown that there are sequences 
of concept classes {C,,},,al with dimension increasing polynomially in n that are not polynomially learnable, 
given the existence of 1-1 one-way func- tions. Are there more natural examples of such sequences? n-clause, 
n-variable DNF could be one such example. Note that this latter example is polyno- mially learnable if 
P=NP (by Theorem 2).  Appendix For Theorem 2 to apply, we require that the con- ccept class C have some 
additional properties related to measurability, beyond the assumption that all sets in C are Borel. Definition. 
C is well-behaved if the sets Q p and J~ defined before Lemma 4 above are measurable for all £ > 0, m 
_> 1 and distributions P on X. An example of a concept class C that is not well- behaved is the following. 
Let X be the closed interval [0,1] and let X be well-ordered such that all prefixes of the well-ordering 
are countable. 5 Let C consist of all suffixes of the well-ordenng. It is readily verified that the dimension 
of C is 1, yet Theorem 8 fails even for s'nais requires the Continuum Hypothesis. the uniform distribution 
on X. In fact, in this case no finite set of points in X form an e-net for C for any e < 1, since there 
is always a suffix of the well-ordering that avoids the set, yet has measure 1. Theorem 1 of [VC71] also 
fails for this case. The problem is that je2,. is not measurable, even for m -~ 1. On the other hand, 
virtually any concept class that one might consider in the context of machine learning applications will 
be well-behaved. Proofs of good behavior for most common concept classes can be derived from the following 
lemma. Definition. A subset Co of C is a dense approximation of C if for every finite S ~ X and c e C, 
there exists c0eC0 such that c0nS=cnS, i.e. ff 1Fic,(S) = IIc(S) for all finite S. Lemma 15. If C has 
a countable dense approximation then C is well-behaved. Proof. It suffices to show that the sets Q~ and 
J~" are Borel sets. We show this for Q p, the argument for j2,, being similar. Q~ can be rewritten as 
 __~')c.{~ : T n c = O}. Since each c ~ C is Borel, each term in the union is Borel. Since C has a count- 
able dense approximation, the union can be replaced by a countable union of sets defined using concepts 
in the approximation without affecting the result. Hence Q is Borel. [] Classes of rectangles, half-spaces, 
etc. clearly have countable dense approximations.  Acknowledgements We would like to thank Emo Welzl 
for many help- ful discussions and ideas on e-nets and classes of finite dimension, and for pointing 
out [A 83] to us. Les Vali- ant, Jan MycielskJ, Janet Blumer, Herbert Edelsbrunner and Nick Littlestone 
also provided valuable sugges- tions at various stages of this investigation.  References: [A83] Assouad, 
P., "Densite et Dimension," Ann. Inst. Fourier, Grenoble 33 (3) (1983) 233-282. [BEHW85] Bhmer, A., A. 
Ehrenfeucht, D. Haussler and M. Warmuth, "Occam's Razor," Technical Report UCSC-CRL-86-2, Computer Research 
Laboratory, University of California at Santa Cruz, February 1986. [DW76] Devroye, L.P. and T.J.Wagner, 
"A distribution-free performance bound in error estima-tion," IEEE Trans. Info. Th., 22, 1976, pp. 586-7. 
[D78] Dudley, R.M., "Central limit theorems for empirical measures," Ann. Prob., 6(6), 1978, pp. 899- 
929. [GJ79] Garey, M. and D. Johnson, Computers and Intractability: A Guide to the Theory of NP- Completeness, 
W.H.Freeman, San Francisco, 1979. [GGM84] Goldreich, O., S. Goldwasser, and S. Micali, "How to construct 
random functions," 25th FOCS, 1984, pp. 464-79. [H85] Haussler, D., "Space Efficient Learning Algo- 
rithms," unpublished manuscript. [HW85] Haussler, D. and E. Welzl, "Epsilon-nets and range queries," 
in preparation. [J74] Johnson, D.S., "Approximation Algorithms for combinatorial problems," Journal of 
Computer and Systems Sciences, Vol. 9, 1974. [K84] Karmarkar, N., "A New Polynomial-Time Algo- rithm 
for Linear Programming," Proc. 16th Symp. on Th. of Comp., 1984, pp. 302-311. [LP85] Lee, D.T. and F.P.Preparata, 
"Computational geometry -a survey," IEEE Trans. Comp., 33(12), 1984, pp. 1072-101. [L85] Littlestone, 
N., unpublished manuscript. [M78] Masek, WJ., "Some NP-Complete Set Cover Problems," MIT Laboratory for 
Computer Science, unpublished manuscript. [M84] Meggido, N., "Linear Programming in Linear Time When 
the Dimension is Fixed," JACM, 31(1), 1984, pp. 114-127. [N69] Nigmatullin, R.G., "The Fastest Descent 
Method for Covering Problems (in Russian)," Proceedings of a Symposium on Questions of Precision and 
Efficiency of Computer Algorithms," Book 5, Kiev, 1969, pp. 116-126. [P78] Pearl, J., "On the connection 
between the com- plexity and credibility of inferred models," Int. J. Gen. Sys., 4, 1978, pp. 255-64. 
[P79] Pearl, J., "Capacity and error estimates for Boolean classifiers with limited capacity," IEEE Trans. 
Patt. An. Mach. Intell., 1(4), 1979, pp. 350-5. [V84] Valiant, L.G., "A theory of the learnable," Comm. 
ACM, 27(11), 1984, PF. k134-42. [VP86] Valiant, L.G., L. Pitt, Technical Report, Aiken Computing Lab., 
Harvard University, to appear. [V85] Valiant, L.G., "Learning disjunctions of conjunc- tions," Proc. 
9th IJCAI, Los Angeles, CA, August 1985, vol. 1, pp. 560-6. [VC71] Vapnik, V.N. and A.Ya.Chervonenkis, 
"On the uniform convergence of relative frequencies of events to their probabilities," Th. Prob. and 
its Appl., 16(2), 1971, pp. 264-80. [VC74] Vapnik, V.N. and A.Ya.Chervonenkis, Theory of Pattern Recognition 
(in Russian), Nauka, Moscow, 1974. [W86] Welzl, E., "Some properties of the Vapnik- Chervonenkis dimension," 
in preparation. [WD81] Wenocur, R.S. and R.M.Dudley, "Some spe- cial Vapnik-Chervonenkis classes," Discrete 
Math., 33, 1981, pp. 313-8. [YC74] Young, T. and T. Calvert, Classification, Esti- marion, and Pattern 
Recognition, Elsevier, New York, 1974.  
			