
 Improvecl Approximations of ~a&#38;in~ and Covering Pro~lems* Aravind Srinivasant Abstract Several important 
NP-hard combinatorial optimization prob­ lems can be posed as packing\couerirag integer pragramq the 
rarzrfomized rouradingtechnique of Raghavan &#38; Thompson is a powerful tool to approximate them well. 
We present one elementary unifying property of all these integer programs (IPs), and use the FKG correlation 
inequality to derive an improved analysis of randomized rounding on them. Thk also yields a pessimistic 
estimator, thus presenting deter­ ministic polynomial-time algorithms for them with approx­ imation guarantees 
significantly better than those known. introduction Several important NP-hard combinatorial optimization 
prob­ lems such as basic problems on graphs and hypergraphs, can be posed as packing/covering integer 
progmms; the ran­domized rounding technique of Raghavan &#38;Thompson is a powerful tool to approximate 
them weil [22]. We present an elementary property ofallthese IPs-positiue correlation-and usethe FKG 
inequality (Fortuin, Kasteleyn &#38; Ginibre [10], Sarkar [23]) to derive an improved analysis of randomized 
rounding on them. Interestingly, this yields a pessimistic estimator, thus presenting deterministic polynomial 
algo­ rithmsfor them with approximation guarantees significantly better than those known, in a unified 
way. *A preliminary version of this work is available as a DIMACS tech­nical report. The full version 
will also be available at the DIMACS site soon (URL http:\\dimacs.rutgers. edu). tschoolof Mathematics, 
Institute for Advanced Study, princetcm, NJ OS540, USA, and DIMACS (NSF Center for Discrete Mathematics 
a~d Theoretical Computer Science), Rutgers University, Piscataway, NJ 08855, USA. Supported by NSF grant 
NSF-STC91-19999 to DI-MACS, by support to DIMACS from the New Jersey Commissionon Science and Technology, 
and by grant 93-6-6 of the Alfred P. Sloan Foundation to the Institute for Advanced Study. Part of this 
work was done while visiting the Department of Computer Science, Univer­sity of Warwick, Coventry CV4 
7AL, UK, supported in part by the ESPRIT Basic Research Action Programme of the EC under contract No. 
7141 (project ALCOM 11). Pe.nnission to copy without fee all or part of this material is granted provided 
that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice 
and the title of the publication and. fis date appear,, and notice is given that c~yin is by permission 
of the Assoclahon of Computing Machinery. o cop otherwise, or to republish, requires 0 a fee ancffor 
spec ICpermission. STOC 95, Las Vegas, Nevada, USA @ 1995 ACM 0-89791 -718-9/95/0005 ..$3.50 1.1 Previous 
work Let Z+ and R+ denote the non-negative integers and the non-negative reals respectively. For a (column) 
vector v, let v= denote its transpose, and vi stand for its ith component. We first define the packing 
and covering integer programs. Definition 1 Given A ~ [0, 1]  , b c [1, OO)n and c E [0, 1] with maxj 
Cj = 1, a packing (resp. cowering) integer program PIP (resp. CIP) seeks to maximize (resp. mini­ mize) 
cT .z subject to z ~ Z~ and Az s b (resp. Ax z b). Furthermore if A G {O, l} X , we assume that each 
entry of b is integral. Finally, we define B = mini b~. Though there are usually no restrictions on the 
entries of A, b and c aside of non-negativity, it is easily seen that the above restrictions are without 
loss of generality (w.l.o.g. ), because of the following. First, we may assume that Vi, j, Aij is at 
most bi. If this is not true for a PIP, then we may as well set Zj := O; if this is not true for a CIP, 
we can just reset Aij := bi. Next, by scaling each row of A such that maxj Ai,j = 1 for each row i and 
by scaling c so that maxj cj = 1, we get the above form for A, b and c. Fi­nally, if A e {O, l} X-, 
then for a PIP, we can always reset bi := Lbij for each i and for a CIP, reset bi := [b;]; hence the 
assumption on the integrality of each bi, in this case. Remark. The reader is requested to take note 
of the pa­rameter B; it will occur frequently in the rest of the paper. As mentioned above, PIPs and 
CIPS model some basic problems in combinatorial optimization, but most of these problems are NP-hard; 
hence we are interested in efficient approximation algorithms for PIPs and CIPS, with a good performance 
guarantee. We now turn to an important tech­nique for approximating integer linear programs- relaxing 
their integrality constraints , and considering the resulting linear program. Definition 2 The standard 
LP relaxation of PIPs/CIPs lets z E R?; given a PIP/CIP, Z* and y* denoter resp., an optimal solution 
to, and the optimum value of, this relax­ation. (For packing, we also allow constraints of the form Z; 
E{ O, l,... , di}, for any set of positive integers {di}; the LP relaxation sets z; E [0, d~] here.) 
Given a PIP or a CIP, we can solve its LP relaxation efficiently. However, how do we handle the possibility 
of possibly fractional entries in z*? We need some mechanism to round fractional entries in z* to integers, 
suitably. One possibility is to round every fractional value z: to the closest 268 integer, with some 
tie-breaking rule if z: is half of an integer. However} it is known that such thresholding methods are 
of limited applicability y. A key technique to approximate a class of integer pro­gramming problems via 
a new rounding method-randomized roundin~was proposed in [22]. Given a positive real v, the idea is to 
look at its fractional part as a probability-round v to Lv] + 1 with probability v Lvj, and round v 
to [v] with probability 1 -v + iv]. This has the nice property that the expected value of the result 
is v. How can we use this for packing and covering problems? Consider a PIP, for instance. Solve its 
LP relaxation and set X; := z; /a for s)me parameter a > 1 to be fixed later; this scaling down by a 
is done to boost the chance that the constraints in the PIP are all satisfied-recall that they are all 
< constraints. Now define a random z c Z~, the outcome of randomised rounding, as follows. Independently 
for each i, set z; to be lz~] + 1 with probability z; lzjJ, and [z:] with probability 1 (z: 12:]). 
We now need to show that all the constraints in the PIP are satisfied and that CT . z is not much below 
V*, with reasonable probability; we also need to choose a suitably. This is formalized in [22] as follows. 
As seen above, an import ant observation is that E[z;] = z j. Hence, E[(AZ)i] = (A~ )i < bi/~ and l?[c~ 
. z] = y*/a. For some ~ >1 to be fixed later, define e~,e:ts J%, 1%, . . . . En by E; - (Az)i > b?, and 
let Em+l ~ c . z < Y*/(c@) . Now, z is an (cz~)-approximate solution to PIP if n+ 1 ;=1 holds. How small 
a value for (a/?) can we achieve? Bounding n+1 n+1 (1) i=l i=l we can pick a, /3 > 1 such that ~~~1~ 
PT(Ei) < 1 holds, using the Chernoff-Hoeffding (CH) bounds. This gives us an (c@) approximation z with 
nonzero probability, which is also made deterministic by Raghavan, using pessimistic estimators [19]. 
Similar ideas hold for CIPs-t he fractions {z;} are scaled up by some a > 1 here. Similar approx­imation 
bounds are derived through different methods by Plotkin, Shmoys &#38; Tardos [18]. See Raghavan [21] 
for a survey of randomized rounding, and Crescenzi &#38; Kann [7] for a comprehensive collection of results 
on NP-optimization problems. Though randomized rounding is a unifying idea to derive good approximation 
algorithms, there are better approxi­mation bounds for specific key problems such as set cover (Johnson 
[14], LOV5SZ [15], Chv5tal [6]), hypergraph match­ing (Aharoni, Erd6s &#38; Linial [1]) and file-sharing 
in dis­tributed networks (Naor &#38; Roth [1 7]), each derived through different means. One reason for 
this slack stems from bound­ ing P~(V~~ll Ei) by ~~~11 PT(-%): to quote Raghavan [19], Throughout, we 
naively (7) sum the probabili­ties of all bad events although these bad events are surely correlated. 
Can we prove a stronger result using algebraic properties (e.g., the rank) of the coefficient matrix? 
A tighter bound for the probabilistic existence proofs should lead to tighter approximation algorithms. 
 1.2 Proposed new method We make progress in the above-suggested direction by ex­ploiting an elementary 
property-positive corTe~at;on-ofCIps and PIPs. This is the intuitively clear fact that kk vkvl<il<i2 
<... < ih < n, PT(~ ~) > ~pT(~). j=l j=l (2) In other words, (2) claims that the constraints are positively 
comehated-given that all of any given subset of them are sat­isfied, the conditional probability that 
any other constraint is also satisfied, cannot go below its unconditional probabil­ity. (Of course, it 
can also be shown that given that all of any given subset of the constraints are uiohzted, the condi­tional 
probability that any other constraint is also violated, cannot go below its unconditional probability; 
but we will not have to deal with this situation!) We prove (2), which seems plausible, using the FKG 
inequality. Thus, n+l PT(V E i) < PT(~ Ei) + PT(J%+l) i=l i=l ~ I (fi(l F r(l?i))) -t. PT(-%+1 ), (3) 
;=1 which is always as good as, and most often much better than, (1). (For a detailed study of the FKG 
inequality, see, e.g., Graham [11] and Chapter 6 of Alon, Spencer &#38; Erd6s [2].9 More surprisingly, 
though this new formulation usually only guarantees that z is a good approximation with very low (albeit 
positive) probability y-in fact, it does not even seem to provide a randomized algorithm with any good 
suc­cess probability the structure of PIPs and CIPS implies a sub-additivity property which yields a 
pes~simistic estima­tor; we thus get deterministic polynomial-time algorithms achieving these improved 
approximation bounds. The prob­lem in proving this is that while the previous estimator ~~~~ p~(~i) (Le., 
the one used in [19] and in related pa­pers) is upper-bounded by E[Z] (for some random variable Z) on 
applying the CH bounds, such a fact cloes not seem to hold here. Nevertheless, the structure of CIPs/PIPs 
helps in providing a good pessimistic estimator. Thus we get, in a unified way, improved bounds on the 
integTaMy gap rn={(cT z)/v*l Y*/(cT z)] and hence, improved approximation algorithms for all PIPs and 
CIPS. In particular, we improve on the aLbove-mentioned results of [14, 15, 1, 17]; our bound is incomparable 
with that of [6]. 1.3 Approximation bounds achieved Our best improvements are for PIPs. For PIPs, the 
stan­dard analysis of randomized rounding delivers integral so­lutions of value tl = ~(y*/nlfE) and tz 
= C2(y*/nl/fB+ J) respectively, if A E [0, l]nx and A 6 {O, I} xrn (An im­port ant problem of the latter 
type is sim,ple B matching 269 in hgpergraphs [15]: given a hypergraph with non-negative edge weights, 
finding a maximum weight collection of edges such that no vertex occurs in more than B of them. Usual 
hypergraph matching has B = 1.) Our method provides B+ I)/B) bounds resp., thus improvingW(W) and ~(t~Sl(tl 
 well on the previous ones e.g., in the latter case if y = @(n) and B = I, we get an integral solution 
of value @(n), as opposed tO the previous Q(W) bound. Our work general­ izes that of [1], which gets 
our bounds if Ai,j c {O, 1}, B = 1, and c~ E {0, 1}. This method also gives Tur$n s classi­ cal theorem 
on independent sets in graphs [26] to within a constant factor. For covering, we prove an 1 + O(max{ln(nB/y*) 
/l?, @n(nB/y*)/B}) (4) integrality gap, and derive the corresponding deterministic polynomial-time approximation 
algorithm. This improves on the 1 + O(msx{(ln .)/B, ~-}) bound given by the standard analysis of randomized 
round­ ing. Also, Dobson [8] and Fisher &#38; Wolsey [9] bound the performance of a natural greedy algorithm 
for CIPS in terms of the optimal integral solution. Our bound is incompara­ ble with theirs, but for 
any given A, c, and the unit vector ~/1 1~112 pointing in the direction of b, our bound is always better 
if B is more than a certain threshold thresh(A, b, c). See Bertsimas &#38; Vohra [4] for a detailed study 
of approxi­ mating CIPS; our work improves bn all of their randomized rounding bounds except for the 
minimum multi-cut problem on graphs, for which our bound is incomparable with theirs. An important subclass 
of the CIPS models the unweighed set cover problem Vi, j, A;,j E {O, 1}, bi = 1 and Cj = 1, here. The 
combinatorial interpretation is that we have a hypergraph H = (V, E), and wish to pick a minimum cardi­ 
nality collection of the edges so that every vertex is covered. The rows correspond to V and the columns, 
to E. Clearly, this problem requires that z c {O, l}m, which is not guar­ ant eed by Definition I; however, 
note that for this problem, any z G 2$ with Az ~ b trivially yields a y E {O, 1} with Ay~bandcT. y<c~. 
z. For set cover, we tighten the constants in (4) to derive an ln(n/y*) + O(ln ln(n/y*)) + O(1) approximation 
bound for it. The work of Lund &#38; Yan­nakakis [16] and Bellare, Goldwasser, Lund &#38; Russell [3] 
shows a constant a > 0 such that approximating this prob­lem to within a In n is likely to take super-polynomial 
time. However, this problem is important enough to study ap­proximations parametrized by other parameters 
of A, b and c, that are always as good as and often much better than, El(log n); for instance, the work 
of [14, 15, 6] shows a in d + O(I) approximation bound, where d is the maximum col­umn sum in A-note 
that d ~ n. Also since there is a trivial solution of size n for any set cover instance, n~y is a sim­ 
ple upper bound on the approximation ratio. Our bound is a further improvement-it is easily seen that 
n/y* < d al­ways, and that there is a constant 1> 0 such that for every non-decreasing function ~(n) 
with 1 < ~(n) <1 in n/ in in n, there exist families of (A, b, c) such that ln(n/y*) < min{n/y*, in d}/$(n). 
 Thus our bound is never more than a multiplicative (1+.(1)) or an additive 0(1) factor above the classical 
bound, and is usually much better; in the best case, our improvement is by @(log n/ log log n). (For 
instance, we can construct instances with d = no(l) and y* = n/loge(l) n, giving a @(log n/ log log n) 
improvement.) Corresponding improve­ ments also hold for facility location problems, which are es­ sentially 
formulated as set cover problems in [4]. Another noteworthy class of CIPS is related to the B domination 
problem: given a (directed) graph G with n vertices] we want to place a minimum number of facilities 
on the nodes such that every node has at least B facilities in its out-neighborhood. This is also a key 
subproblem in sharing files in a d~tributed system [17]; under the assumption that G is undirected and 
Ietting A be its maximum degree, an 1 + O(max{ln(A)/B, ~-}) approximation bound is presented in [IT], 
improving on the standard analysis of randomized rounding. Bound (4) im­ proves further on this; in particular, 
even if G is directed with maximum in-degree A ~ (4) shows that the Naor-Roth bound holds. Furthermore, 
the comments regarding the @(log n/ log log n) improvement for set cover, hold even in the undirected 
case. All of this, in turn, provides better bounds for the file-sharing problem. 2 Preliminaries Let 
r.v. abbreviate random variable and for any posi­tive integer k, let [k] denote the set {1, 2, ..., k}. 
If a uni­verse N= {al, az, . . . , al} is understood, then for any S ~ N, x(S) denotes its characteristic 
vector: x(S) G {O, 1}4 with x(S)j = 1 iff aj E S. For a sequence S1, w, . . . and any integer i > 1, 
s( ) denotes the vector (SI, sz, . . . , s;). In our usage, 91, 92, . . . could be a sequence of reals 
or of ran­dom variables. As usual, e denotes the base of the natural logarithm. Remark. Though the following 
pages seem filled with for­mulae and calculations, many of them are routine. The real ideas of this work 
are contained in Lemmas 1, 5, and 6. The reader might even consider skipping the proofs of most of the 
rest of the lemmaa, for the first reading. We first recall the Chernoff-Hoeffding (CH) bounds, for the 
tail probabilities of sums of bounded independent r.v.s [5, 13]. Theorem 1 presents these tail bounds; 
see, e.g., Raghavan [20] for the proofs. Theorem 1 [20] Let Xl, X2,..., Xl be independent r.u.s, each 
taking values in [0, 1], with R = ~~=1 Xi and E[R] = p. Then for any 6>0, P,(R ~ p(l + $)) < E[(l + rf)~-p( 
+f)] < G(/J, 6) + (e&#38;/(l + 6)(1+6))~, and ifO < 6< 1, then Pr(R ~ P(1 6)) < E[(l 6)R-P(l-6)] ~ 
.H(p, 6) &#38; e-P62i2. 0 It is easily seen that Fact 1 (a) G(p, 6) < (e/(l + 6))( +6)P. 82~13 if 8< 
~. (b) G(p, 6) < e­ (C) G(A, b) ~ .-( +f) n( +f~K/ if6 > ~. (d) If 0< ,u1 S M, then G@, 6) >_?7(I.Jz,6). 
 270 Call a family F of subsets of a set N monotone increasing (resp. monotone decreasing) if for all 
S C T ~ N, S E F implies that T c 7 (resp., T E Y imp~es that S E F). We next present Theorem 2, a special 
case of the powerful FKG inequality [10, 23]; for a proof, see, e.g., the proof of Theorem 3.2 in Chapter 
6 of [2]. Theorem 2 Given a finite set N = {al, az, ..., at} and some p = (pl ,pz, . . . ,pt) E [O, I]f, 
suppose we pick a random ~ ~ N by placing each ai in Y independently, with proba-Wty pi. For any F ~ 
2N, let Pr=(7) = Pr(Y G F). Let R, Fz,. ... F. ~ 2N be any sequence of monotone increasing families, 
and let GI, GZ, . . . . G. G 2N be any sequence of monotone decreasing families. Then, ss PrP(~ F;) z 
~Prp(F;), i=l i=l and sa PrP(A Gi) ~ ~Prp(Gi). 0 i=l i=l Finally, we recall the notion of pessimistic 
estimators [19]. For our purposes, we focus on the case of independent binary r.v.s. Let Xl, X2, ..., 
Xl c {O, 1} be independent r.v.s with Pr(Xi = 1) = pi, for some p ~ [0, l]l. Suppose, for some implicitly 
defined L ~ {O, I}z, that Pr(X(i) E L) <1. How do we find some v c {O, 1}4 -L? Theorem 3 now presents 
the idea of pessimistic estimators applied to the method of conditional probabilities. See [19] for a 
det ailed discussion and proof. Notation 1 Vq E [0, 1]1 Vi ~ {O} u [1] VUJ c {O, l};, let ~( i, W,q) 
~ (Wl, W2, . ..10i)q{+l)qi+2j . . . . ql), and for any j c {O, 1}, define wj E {O, 1}*+1 as (w, w2, . 
. . . ~i,j). Returning to the Xis, p and L, we define Definition 3 A function U : [0, 1]4 -i R+ is a 
pessimistic estimator w.r. t. (Xl, . . . . Xz) and L if: (1) U(P1, P2,..., P1) <1, and (2) vi c{o} u[t]Vwc{o, 
l}i, (a) U(u(i, w,p)) ~ Pr(Xf4J c LIX(;) = w), and (b) if i s 1 1, then U(u(i, w,p)) is at least 
 min{U(u(i + 1, wO, p)), V(u(i+ 1, wl, p))}. Theorem 3 [19] Let an efficiently computable U be a pes­9zmistic 
e9timator w.r. t. (x,,..., XC) and L. Defining, W E {O} U[t-1] Vw c {O,l}i, n(i, w) =j C {0,1} by U(u(i+l, 
wj, p)) = min{U(u(i+l, wO, p)), U(u(i+l, wl, p))} by breaking ties arbitrarily, the following algorithm 
pToduces av~L: For i := O to .4 1 do: vi+l := n(i, v(~)). PROOF. It is not hard to see by induction 
on i, that vi E {o} u[t], P?(x@) c qx(~) = Ji)) <1. Using thk for i = t in conjunction with property 
2(a) of Definition 3, completes the proof. 0 3 Approximating Packing Integer Programs Let a PIP be given, 
conforming to Definition 1. We assume that z c Z? is the constraint on z. (Clearly, even if we have constraints 
such as Zi c {O, 1, . . . . di}j we will get identical bounds since scaling down by a > 1 and then performing 
an randomized rounding cannot make zi $! {O, 1, . . . . di}.) Lemmas 1 and 6 are crucial, wherein the 
structure of PIPs is exploited. It is essential to read this section before reading Section 4-most proofs 
are omitted in Sectim 4 since they are very similar to the ones in this section. We solve the LP relaxation, 
and let the scaling by a, events E1, E2, ..., En+I, and vectors z, x etc. be as in Sec­tion 1.1; a and 
@ will be determined later on. The main point of this section is to present a good candidate for a pessimistic 
estimator (see (5)), and to show that it indeed satisfies the conditions of Definition 3. We may then 
in­voke Theorem 3 to show that not only do vve get improved ezis tential results on the integrality gap-that 
we can also const ructivize the existence proof. The work of t hk section culminates in Theorem 4. We 
first setup some notation. For every j e [m], let $] = l.~~~~and pj = z; sj c [0, l). Let Ai denote the 
ith row of A. Let Xlj Xz, ..., X-c {O, 1} be independent r.v.s with Pr(Xj = 1) = pj Vj E[m], and let 
X = X(m). It is clear that Ei ~ A; .X > p;(l + &#38;i) Vi E [n], and that En+l ~ CT oX < P-+1(1 c$n+l) 
, where pi = E[Ai . X] and 6i = (bi Ai.~)/~i 1 for i c [n], w~+i = E[cr . X], and $m+~ = 1 (g*/(a~) 
 cT o 8)/f~n+I. It is readily verified that that fi > 0 Vi E [n] and that 0< 6n+l <1. Our first objective 
is to prove (2) and hence (3), using Theorem 2; this will then suggest potential choices for a pessimistic 
estimator. In the notation of Theorem 2, N = [m] and Y = {i EN :Xi = 1}. For each ic [n], define Fi ~2N 
as {S ~ N :(Ai .x(S)) < ~i(l + fi)}. A little reflection shows the crucial property that each F~ is monotone 
deceasing. Noting that E -(Y E Fi) for each i, we deduce (2) from Theorem 2. In fact, a similar proof 
shows that since the components of X are picked independently, we have Lemma 1 For any j E{O} U[m] and 
any w c {O, 1}~, Pr(vn+ 1 Ei~X(j) = W) < 1 (fi(l-Pl (EilX(~) = W))) i=l ;=1 +Pr(En+llX(;) = w). 0 Let 
F.+, = {S c [m] : CT . x(s) > pn+l(l 6TL+1)}. In the notation of Definition 3, the set to lbe avoided, 
L, is We are now ready to define a suitable pessimistic estima­ tor; we first introduce some useful notation 
to avoid lengthy formulae. 271 Notation 2 For all i c [n], j c {O} U [m] and w c {O, I}J, let A;.x-jL;(l+&#38;) 
@~) = w], hi(j, W) = E[[l + &#38;i) j-;(j, tu] = E[(l + fi)~ x-~ (l+$ )lx(~+ ) = WI)], ~~~ A;. X #;(l+&#38;)lx(j+l) 
= wI]. 9i(j, V] = .E[(l + Ji) When j and w are clear from the context, we might just refer to these 
as hi, fi and gi. From Theorem 1 and Lemma 1, a natural guess for a pessimistic estimator, U(u(j, w, 
p)), Vj ~ {O} U [m] Vw E {O, 1}~, might be TI 1 (~(1 h~(j, w))) + i=l .~.~ ~m+l(l ~m+l)lx(j) = w]. 
-E[(l &#38;z+l) However, this might complicate matters if hi(j, w) > 1 and hence we first define h~(j, 
w) = min{hi(j, w), 1}, ~~(~, w) = min{fi(j, w), 1}, and = min{g; (j, w), 1}. 9~(jj W) We now define 
U(u(j, w,p)), Vj 6 {O} U [m] Vw e {O, 1}~, to be 1 (fi(l h;(j, w)))+ ;=1 ~T.x #=+1(1 6m+ 1)lx(j) = 
w].E[(l &#38;t+l) (5) To make progress toward proving that U is a pessimistic estimator w.r. t. X and 
L, we next upper-bound Pr(Ei) for each i. Recall that for each i E [n], ~~(E; ) < G(pi, fi); also, F 
~(E~~l) ~ H(y*/a, l 1/~). Lemma 2 upper-bounds these quantities. Lemma 2 (a) For every i E [n.], G(~i, 
fi) < G(bi/ff, a 1) ~ G(B/~, ~ 1). (b) ~(kz+l, ~n+l) < ~(v*/~, 1-1/6). PROOF. (a) Note that pi +Ai 
. s < bi/et, with ~i, Ai .s ~ 0. Subject to these constraints and that a >1, we will show that G(~i, 
&#38;i) is maximized when Pi = b;/a and Ai . s = O; this will prove (a). Now, G(Pi, f;) = Pi i-~ e-Mi(e/(bi 
 A; . S)) b -d . (6) If A; . s is held fixed at some ~ z O, (6) is maximized at pi = A = bi /a y, 
under the constraint that p; c [0, A]. Thus, G(J&#38;, 6i) ~ e~i b~fa ((hi/~ _ Ai . ~)/(bi -Ai . S)) 
b -d s, which is readily shown to be maximized when A: . s = O. A similar proof holds for (b). 0 Now 
that we have good tail bounds, we set a, P >1 such that (c@) is small and such that for the PIP, U(pl, 
pz,.. .,pm)<l (property (1) of Definition 3). Note that the bound of Lemma 3 makes sense only if B > 
1. Lemma 4 handles the common case where Ai,j G {O, 1} Vi, j, to get improved bounds which, in particular, 
work even if B = 1. We have not attempted to optimize the constants. Lemma 3 There ezist constants c1 
> 3 and C2 ~ 1 -for PIP, such that if a = cl(c2n/y*)ll(B-1) and ,6 = 21 then U(Pl, Pz, . . .. PTn) <l. 
PROOF. By Lemmas 1 and 2, it suffices to show that H(y*/a, 1/2) < (l G(B/a, a l)) . Furthermore, Fact 
l(a) shows that -Y*/(8u) < (1_ e-B(k a-l)~n e suffices. Now since B ~ 1 and in a 1 ~ in 3 1 > 0, there 
exists a fixed d > 0 such that ~ _ e B(ln a l) ~ e-de B@u-l) and hence, it suffices if y*/(8a) > ride-B@ 
 1). Solving for a gives the claimed bound. 0 Lemma 4 There exists a constant c1 z 3 for PIP instancea 
with Ai,j c {O, 1} Vi, j, such that if cs = cl(n/g*)l JB and /3=2, then U(pl, p2,..., pm) <l. PROOF. 
Note, since Ai,j E {O, 1}, that for any i c [n], ((Az); > bi) + (Az)i ~ b~ +1. Hence, B essentially 
gets replaced by B + 1 in Lemma 3, leading to the strengthened bounds. 0 As remarked in the introduction, 
it can be seen that the bounds (on the approximation ratio (c@)) of Lemmas 3 and 4 significantly strengthen 
the corresponding bounds achievable by the standard analysis of randomized rounding. At this point, we 
have exhibited suitable a and P such that our function U satisfies properties (1) and 2(a) of Def­inition 
3. We now turn to proving property 2(b), which is more interesting. Before showing Lemma 6 which proves 
this, we first establish a simple lemma which faci.lit ates the proof of Lemma 6. Lemma 5 For all i&#38;[n], 
j c{O} u[m] and wE{O,1}~, (i) O~ f~(j, w) ~ g~(j, w) ~ 1, and (ii) M(j, w)> (1 Pj+l)f~(j, w) + Pj+19;(j, 
w). PROOF. We drop the parameters j and w for the rest of the proof. Part (i) is easily seen. For part 
(ii), we first note that hi = (1 Pj+l)fi ~ Pj+lgit (7) by the definition of these quantities. Now if 
hi < I and gi S 1, then fi <1 by (i ) and hence part (ii) above follows from (7), with equality. Instead 
if hi <1 and gi >1, note again that ~i < 1 and furthermore, that gi > g; = 1; thus, part (ii) follows 
from (7). Finally if hi = 1,note that h; = 1, g{ = 1, and that fl S 1, implying (ii) again. 0 Lemma 6 
For angj E {O} U[rn 1] and for aU w E {O, l}j, ?7( U(j,10, p)) ~ (1 pj+~)U(74(j + 1, WO, p)) + Pj+l U(4j 
+ l,wl, p)). Thus in particular, U(u(j, w, P)) 2 min{U(u(j + 1, wO, P)), U(u(j + 1, wl, P))}. 272 PROOF. 
Let T = Pj+l, for convenience. Note that E[(l 67+1) =T x-~*+l(l J-+l)~x(j) = w] = Omitting the parameters 
j and w in ~i, gi etc., it is sufficient to show that Thus from Lemma 5(ii) and since h; s 1, it suffices 
to show that which we now prove by induction on n. Equality holds in (8) for the base case n = 1. We 
now prove (8) by assuming its analogue for n 1, i.e., show that i=l ;=1 Simplifying, we need to show 
that which holds in view of Lemma 5(i). 0 By now, we have fulfilled all the requirements of Defini­tion 
3 and thus present Theorem 4 There exist constants C3, CA >0 such that given any PIP conforming to the 
notation of Definition 1, we can produce, in determtnistzc polynomial time, a jeaszble 8olution to it, 
of value at least LIB B/(B-l) c3(c4y*/n ) . If A c {0, 1}- , the guarantee orI the solution value is 
at least 11(~+1) (B+I)/B c3(y*/n ) PROOF. Lemmas 3 and 4 show property (1) of Defini­tion 3. Properties 
2(a) and 2(b) of Definition 3 are shown by Lemmas 1 and 6 respectively. Theorem 3 now completes the proof. 
0 Finally, we consider the classical NP-hard problem of finding a maximum independent set in a given 
undirected graph C = (V, E), by posing it naturally as a packing prob­lem. Turin s classical theorem 
[26] shows that G always has an independent set of size at least lv12/(21El + IVl). The standard packing 
formulation described below, com­bined with our approach, shows the existence of an inde­pendent set 
of size Q( ]V12/] E] ). The constant factor hidden in the 0(. ) is weaker than that of Turin s theorem 
however our reason for presenting thk result is just to show that our approach proves a few other known 
results too, in a unified way. We remark that we do not use the standard notation of graphs having n 
vertices and m edges, as it will go against our notation for PIPs and CIPs-the packing formulation has 
~E~ constraints and ~V~variables, Define an indicator variable Z; G {0,1} for each vertex i, for the 
presence of vertex i in the independent set (1S). Subject to the constraint that Z; + Zj < 1 for every 
edge (~, j), we want to maximize xi z;. For specific problems hke thk, we can get better bounds than 
does the analysis for Theorem 4, which uses the generrd CH bounds. The fractional solution X; = 1/2 for 
each i, is optimal to within a factor of 2. Suppose we scale z* down by some a > 1 and do the randomized 
rounding as before. Then for any given edge (i, j), Pr(zi + Zj > 1) ~ l/(4a2), a bound much better than 
the CH bound. Analysis as above then shows that a = @(]E1/lVl) and /3 = @\l) suffice, thus producing 
an IS of size f2(y*/(o#3)) = Cl({V~ /~El). 4 Approximating Covering integer Programs Given a CIP conforming 
to Definition 1, we show how to get a good approximation algorithm for it. Since most ideas here are 
very similar to those of Section 3, we borrow a lot of notation from there, skim over most details and 
just present the essential differences. The idea here is to solve the LP relaxation, and for an a > I 
to be fixed later, to set z; = csz~, for each j c [m]. We then construct a random integral solution z 
by setting, independently for each j G [m], Zj = lz~J +1 with probability Z; [z;], and zj = lz~j with 
probability 1-(z; lz~j ). Let Ai, si, and X1, X2,..., X~ be as in Section 3. The bad events now are 
E; = Ai .X < V;(1 $i) Vi G[n], and J%+l= CT . x > pn+l(l + c%+!.) , where ~; = .E[Ai . X] and for i 
~ [n], pm+l = -E[cT ~X], and Analogously to PIPs, O ~ 6i <1 Vi c [n] and 6~+1 20. For any i G [m], let 
Each of these families is monotone increasing now, and thus Theorem 2 again guarantees Lemma 1, for the 
present defi­nition of El, E2, . . . . E*+l also. Remark. We pause to think why we have been so lucky 
as to have positive correlation among the constraints of PIPs and CIPs-a very desirable form of correlation. 
The features of PIPs and CIPS which guarantee this are: All the entries of the matrix A are nan-negative, 
and  all the constraints point in the same directzon.  Returning to CIPS, suppose we ,define, given 
some j G {0} u [m 1] and s~me w c {O, 1} , h~(j, w), f~(j, w) and g; (j, ~) for every i e [n] analogouslyasin 
INotation2: h~(j, W) = min{l, E[(I 6~)A x-W (l-J [X(j) = w]}, 273 ~, x-gi(I-&#38;)@tl) = wo]}, and 
Now since a ~ 1, d = eln(e/(e 1)) satisfies i~(j,~) = min{l, E[(l-6~) d;.X-#;(l-&#38;)@j+lj = wl]}. 
9/(~,~) ~ min{lj E[(l f~) 1 e-a ~ e de- . As can be expected, the pessimistic estimator U(u(j, w, p)), 
Vj e {0} U [m] Vw E {O, l}J, is now Also if we agree to make @ s 2, we then have G(y*a, ~ 1) ~ e y*a(@-1)2/3, 
1 (fi(l h:(j, w)))+ i=l by Fact l(b). So from (12), it suffices to choose a ~ 1 and E[(l + 6n+I) #. 
X._pn+I(I+6n+I )1X(3) = w]. (lo) 1 s/3 s 2 such that Now for the analogue of the important Lemma 6. It 
is ~de-a < y a(~ 1)2/3. (13) easily checked that Lemma 5(ii) holds again and that instead It can now 
be verified that by choosing of part (i) of Lemma 5, we have a = In(n/v*) + al lnln(n/y*) + O(1) and 
O<g:<f; <l. (11) Thus, (11) guarantees (9) even now! This shows that Lemma 6 holds for the current definition 
of U also. Thus to establish that U is a pessimistic estimator, we only have to exhibit, as do Lemmas 
3 and 4, a, L3 >1 which ensure that V(P1, . . . . pm) < 1. We first present a lemma similar to Lemma 
2, whose proof is simple and omitted. Lemma 7 For all i E [n], %(1%) s H(Ba, 1 l/a). Also, %(-1%+1) 
S G(y*a, P 1). We now present the main theorem on covering problems. Since set cover is an important 
problem, we present the pre­cise approximation bound for this problem as a distinct part of the theorem. 
Theorem 5 Given a CIP conforming to the notation of Definition 1, we can produce, in deterministic polynomial 
time, a feasible solution to it with value at most v*(I + 0(maz@(ni3/y*)/B, Jln(nB/y*)/B})). For the 
unweighed set cover problem, we can improve this to g*(ln(n/y*) + O(ln ln(n/y*)) + O(l)). PROOF. (Sketch) 
For general CIPS, there are two cases: In(nB/y* )/B is at least one or at most one. In the former case, 
we set a = @(ln(nB/y*)/13) and @ = ~(l). For the latter case, we set both a and ~ to be of the form 1 
+ O(<(ln(n/y*) + 0(1))/B). The proofs follow from standard CH bound analysis using Theorem 1 and Fact 
1 with Lemma 7, and the details are omitted in this preliminary version. For the important unweighed 
set cover problem (see Sec­tion 1.3 for the definition), we observe that for any i G [n], E~ holds iff 
A; . s = A~ oX = O; this makes the calculations easier. If A; has j non-zeroes (ones) in it, say in columns 
!l,. G,..., tj, then it is not hard to see that I%(A; . X = O) is maximised when z~~ =;, ford IcE [j]. 
 Thus, Pr(Ei) < (1 a/j)3 < e-= and hence, by Lemma 7, it suffices to pick a, ~ z 1 such that G(y*a, 
/3 1) < (1 e-a) . (12) P = 1 + (ln(Tz/y*))-a for some suitable positive constants al and az, we will 
satisfy (13). Hence, the approximation guarantee a/3 can be made as small as In(n/y*) + O(ln In(n/y*)) 
+ O(l). It is also worth looking at some concrete improvements brought about by Theorem 5, over existing 
algorithms. In the case of unweighed set cover, suppose d ~ n is the max­imum column sum-the maximum 
cardinality of any edge in the given hypergraph. Then, by just summing up all the constraints, we can 
see that y d ~ n. (14) Thus, our approximation bound for the set cover problem­see the second statement 
of Theorem 5-is never more by a multiplicative (1 + O(1)) or an additive 0(1) factor above the classical 
bound of min{n/y*, In d + O(l)}. On the other hand, n/y* << d is quite likely, and it is easy to construct 
set cover instances with min{n/y*, in d} = @(log n/ log log n) In(n/y* ). For instance, we can arrange 
for just a few edges to have the maximum edge size of n ( ~), while keeping y* as high as n/logo(l) n. 
Thus in the best case, we get a @(log n/ log log n) factor improvement in the approximation ratio. An 
important case of the unweighed set cover problem is the dominating set problem: given a (directed) graph 
G, the problem is to pick a minimum number of vertices such that for every one vertex v, at least one 
vertex in v U Out(v) is picked, where Out(v) denotes the out-neighborhood of v. We next consider a more 
general domination-type prob­lem on graphs, modeling a class of location problems. Given a (directed) 
graph G with n nodes and some integral param­eter B > 1, we have to place the smallest possible number 
of facili~es on the nodes of G, so that every node has at least B facilities in its out-neighborhood-multiple 
facilities at the same node are allowed. We use the symbol B here and for the file-sharing problem discussed 
below, since in 274 the natural CIP formulations arising in the analysis of these problems, B plays 
the same role as it does in Definition 1. For the case where G is undirected with maximum degree A, an 
approximation bound of 1 + O(max{ln(A)/B, ~w}) is presented in [17], improving on the bound given by 
the standard analysis of randomized round­ ing. For us, Theorem 5 gives a bound of 1 + O(max{ln(nB/y*)/B, 
<~}). Even if G is directed, this new bound is as good or better than 1 + O(max{ln(A~n)/B, ~w}), where 
A;~ denotes the maximum in-degree of G; this is eas­ily seen from the fact that which follows from the 
same reasoning as for (14). We thus get a generalization of the Naor-Roth result. In the case of undirected 
graphs, it is not hard to show families of graphs for which the present bound is better than that of 
Naor &#38; Roth s by a factor of upto El(log n/ log log n). In addition to its independent interest, 
the above prob­lem is a crucial sub-problem in the folIowing file-sharing problem in distributed networks 
[17]. Given an undirected graph G with maximum degree A and a file F of B bits, F must be stored in some 
way at the nodes of G, such that every node can recover F by examining the contents of its neighbor s 
memories; the aim is to minimize the to­t al amount of memory used. (Note that solving the above domination 
problem is not sufficient for this task. ) An ap­proximation bound of I + O(mzx{ln(A)/B, ~m}] (15) is 
presented in [17] for this problem. Letting g be the opti­mum of the above domination problem on G, we 
derive an approximation bound of 1 + O(max{ln(A)/B, @(nB/y*)/B}), which is always as good as (15), and 
better if B >> in(A). Finally, another type of (canonical) facility location prob­lem can be modeled 
almost as a CIP [4]. Given a directed graph G with vertex-weights {d;} and edge-costs {ci,j }, the problem 
is to place facilities on the nodes of G such that for every vertex v, at least one vertex in v U Out(v) 
houses a facility; the aim is to minimize the the sum of the total weight of the sites of the facilities 
and the total distance (in terms of edge-costs) traveled by all the vertices to their closest facilities. 
Here again, we get an approximation ratio of O(I + log(n/y* )), doing better than the known O(log n) 
bounds (Hochbaum [12], [4]). Concluding Remarks We have presented a simple but very useful property 
of all packing and covering integer programs-positive correlation. Thk naturally suggests a better way 
of analyzing the perfor­mance of randomized rounding on PIPs and CIPS. However, the provable probability 
of success-of satisfying all the con­straints and delivering a very good approximation can be extremely 
low; so, in itself, thk approach may just prove an existential result. Fortunately, the structure of 
PIPs and CIPS in fact suggests a pessimistic estimator, thus convert­ing this existence proof into a 
(deterministic) polynomial­time algorithm. In our view, this is very interesting, and gives evidence 
of the utility of de-randomizaticm techniques. A common objection to de-randomization is that most of­ten, 
it converts a fast randomized algorithm that has a good probability of success, to a somewhat slower 
deterministic algorithm. However, note that the opposite is true here! The randomized algorithm suggested 
by the existence proof can have an extremely low probability of success; second, solv­ing the LP relaxation 
heavily dominates the running time, and the time for running the de-randomization is compar­atively negligible. 
(This observation about running the LP relaxation, also suggests that in practice, it would be better 
to quickly get an approximately optimal solution to the LP relaxation, since we are anyway dealing with 
approximate solutions. ) Anoth er conclusion is that studying correlations helps. In the case of PIPs 
and CIPS, we have benefited from the fact that the constraints help each other , by being posi­tively 
correlated. The precise reasons for such a correlation are spelled out in the remark in Section 4. It 
is a chal­lenging open question to use the structure of correlations in more complicated scenarios; one 
such problem is the set discrepancy problem [24, 2]. Given a system of n subsets S1, S2, ..., Sm of a 
ground set A with n elements, the prob­lem is to come up with a function @ : A -+ { 1,1},such that the 
discrepancy is small , where jES; While randomized rounding and the method of conditional probabilities 
can be used to produce a # with dkcrepancy 0(<-) [24, 2], a classical non-constructive result of Spencer 
shows the existence of a + with disc(~) = O(X) [25]. This is best possible, and it is an important open 
prob­lem to make this constructive. If we write dc,wn the natural integer programming formulation for 
this problem, we can see that each constraint is positively correlated with some subsets of the constraints, 
and negatively correlated with others. It wouM. be very interesting if such more compli­cated forms of 
correlation can be used to get a constructive result here. Finally, as we had seen before, our bouncis 
are incompa­rable with known results for some weighted CIPS, e.g., those considered in [6, 4]. It would 
be interesting if our method could be extended to include these results also. Acknowledgements We thank 
Moni Naor, Babu Narayanan and David Shmoys for their valuable comments. Thanks in particular to Prab­hakar 
Raghavan for his insightful suggestions and pointers to the literature. References [1] R.. Aharoni, P. 
Erd6s, and N. Linial. Optima of dual integer linear programs. Combznatorzca, 8:13-20, 1988. 275 [2] 
N. Alon, J. H. Spencer, and P. Erd6s. The Probabilistic Method Wiley-Interscience Series, John Wiley 
&#38; Sons, Inc., New York, 1992. [3] M. Bellare, S. Goldwasser, C. Lund, and A. Russell. Efficient probabilistically 
checkable proofs and applica­tions to approximation. In Proc. ACM Sympo9ium on Theory of Computing, pages 
294-304, 1993. [4] D. Bertsimas and R. Vohra. Linear programming relax­ations, approximation algorithms 
and randomization; a unified view of covering problems. Technical Report OR 285-94, Massachusetts Institute 
of Technology, 1994. [5] H. Chernoff. A measure of asymptotic efficiency for tests of a hypothesis based 
on the sum of observations. Annals of Mathematical Statistics, 23:493-509, 1952. [6] V. Chwital. A greedy 
heuristic for the set covering problem. Mathematics of Operations Research, 4:233 235, 1979. [7] P. Crescenzi 
and V. Kann. A compendium of NP op­timization problems. Technical Report SI/RR-95/02, Department of Computer 
Science, University of Rome La Sapienza , 1995. [8] G. Dobson. Worst-case analysis of greedy heuristics 
for integer programming with nonnegative data. Mathe­matics of Operations Research, 7:515 531, 1982. 
[9] M. L. Fisher and L. A. Wolsey. On the greedy heuristic for continuous covering and packing problems. 
SL4M J. on Algebraic and Discrete Methods, 3:584 591, 1982. [10] C. M. Fortuin, J. Ginibre, and P. N. 
Kasteleyn. Corre­lational inequalities for partially ordered sets. Commu­nications of Mathematical Physics, 
22:89-103, 1971. [11] R, L. Graham. Application of the FKG Inequality and its Relatives. In A. Bachem, 
M. Grotschel and B. Korte Ed., Mathematical Programming: The State of the Art, Springer Verlag, 1983. 
[12] D. S. Hochbaum. Heuristics for the fixed cost me­dian problem. Mathematical Programming, 22:148 
162, 1982. [13] W. Hoeffding. Probability inequalities for sums of bounded random variables. American 
Statistical As­sociation Journal, 58:13 30, 1963. [14] D. S. Johnson. Approximation algorithms for combi­natorial 
problems. Journal of Computer and System Sciences, 9:256-278, 1974. [15] L. LOV6SZ. On the ratio of optimal 
integral and frac­tional covers. Discrete Mathematics, 13:383 390, 1975. [16] C. Lund and M. Yrmrmkakis. 
On the hardness of ap­proximating minimization problems. Journal of the ACM, 41:960-981, 1994. [17] M. 
Naor and R. M. Roth. Optimal file sharing in dis­tributed networks, In Proc. IEEE Symposium on Foun­dations 
of Computer Science, pages 515-525, 1991. To appear in SIAM Journal on Computing. [18] S. A. Plotkin, 
D. B. Shmoys, and E. Tardos. Fast ap­proximation algorithms for fractional packing and cov­ering problems. 
In Proc. IEEE Symposium on Foun­dations of Computer Science, pages 495-5o4, 1991. To appear in Mathematics 
of Operations Research. [19] P. Raghavan. Probabilistic construction of determin­istic algorithms: approximating 
packing integer pro­ grams. Journal of Computer and System Sciences, 37:130-143, 1988. [20] P. Raghavan. 
Lecture notes on randomized algo­rithms. Technical Report RC 15340 (#68237), IBM T. J. Watson Research 
Center, January 1990. Also available as CS661 Lecture Notes, Technical report YALE/DCS/RR-757, Department 
of Computer Sci­ence, Yale University, January 1990. [21] P. Raghavan. Randomized approximation algorithms 
in combinatorial optimization. In Proc. FST @ TCS Con­ference, pages 300-317, 1994. Lecture Notes in 
Com­put er Science 88o, Springer-Verlag, Berlin. [22] P. Raghavan and C. D. Thompson. Randomized round­ing: 
a technique for provably good algorithms and al­gorithmic proofs. Combinatorics, % :365-374, 1987. [23] 
T. K. Sarkar. Some lower bounds of reliability. Tech­nical Report 124! Department of Operations Research 
and Statistics, Stanford University, 1969. [24] J. H. Spencer. Ten Lectures on the Probabilistic Method. 
SIAM, Philadelphia, 1987. [25] J. H. Spencer. Six standard deviations suffice. Trarasac­tions of the 
American Mathematical Society, 289:679 706, 1985. [26] P. l?ur~n. On an extremal problem in graph theory. 
Matematicko Fizicki Lapok, 48:436 452, 1941. See, for instance, pages 81 and 82 in N. Alon, J. H. Spencer 
and P. Erd6s, The Probabilistic Method, Wiley-Interscience Series, John Wiley &#38; Sons, Inc., 1992. 
 276 
			