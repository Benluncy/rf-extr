
 Integrated Parallel Prefetching and Caching Tracy Kimbrel * Pei Cao t Edward Introduction Prefetching 
and caching are widely-used approaches for improving the performance of file systems. A recent study 
shows that it is important to integrate the two, and proposed an algorithm that performs well both in 
theory and in practice [2, I]. That study waa restricted to the case of a single disk. Here, we study 
integrated prefetching and caching strategies for multiple disks. The interaction between caching and 
prefetching is fur­ther complicated when a system has multiple disks, not only because it is possible 
to do multiple prefetches in parallel, but also because appropriate cache replacement strategies can 
alleviate the load imbalance among the disks. We present two offline algorithms, one of which has provably 
near-optimal performance. Using trace­driven simulation, we evaluated these algorithms un­der a variety 
of data placement alternatives. Our re­sults show that both algorithms can achieve near lin­ear speedup 
when the load is distributed evenly on the disks, and our best algorithm performs well even when the 
placement of blocks on disks distributes the load un­evenly. Our simulations also show that replicating 
data, even across all of the disks, offers little performance ad­vantage over a striped layout if prefetching 
is done well. Finally, we evaluated online variations of the algorithms and show that the online algorithms 
perform well even with moderate advance knowledge of future file accesses. *Department of Computer Science 
and Engineering, University of W&#38;~hington, Seattle WA ({tracyk,karlin }@cs.Washington .edu) tcomputer 
Sciences Department, University of Wisconsin - Madison, Madison WI (cao@cs.wise.edu) $Department of Computer 
Science, Princeton University, Princeton NJ ({felten,li}Qcs. princeton.edu) Permission to make digital/hard 
copy of part or all of this work for personal or classroom usa is grantad without fee provided that copies 
are not made or distributed for profit or commercial advantage, the copyright notice, the title of the 
publication and its date appear, and notice is given that copying is by permission of ACM, Inc. To copy 
otherwise, to republish, to p@t on servere,, or to redistribute to lists, requires prior specific permieelon 
and/or a fee. SIGMETRICS 96 5/96 PA, USA e 1996 ACM 0-89791 -793 -619610005 . .. S3.50 W. Felten I Anna 
R. Karlin* Kai Li$ Model and algorithms We consider the following model of the parallel prefetch­ing 
problem. There is a two-level memory hierarchy: a cache of k data blocks and d disks. A program makes 
a known sequence, called a reference sequence, of refer­encesrl, rz, . . . rn to a set of m data blocks, 
If a refer­ence hits in the cache, it takes one time unit. Otherwise, the missing block must be fetched 
from a disk, which takes F time units; some other block must be evicted to make room for the requested 
block. Blocks may be prefetched in advance of the time they are needed, to avoid stalling until the fetch 
is completed. Prefetches can be overlapped with references, but each disk may fetch only one block at 
a time. The performance mea­sure is the elapsed time required to serve the entire re­quest sequence; 
this is equal to the number of references plus the total stall time. The goal of an integrated strat­egy 
is to decide when to fetch a block from a disk, which disk to fetch from, which block to fetch, and which 
cache block to evict so that the stall time is minimized. YVe consider algorithms both when the entire 
re­quest sequence is known in advance (the offline setting) and when only a limited subsequence of is 
known at a time (the limited lookahead online setting). Regu­lar Aggressive is an offline algorithm that 
works as fol­lows: whenever a disk is free, it prefetches the first miss­ing block that resides on that 
disk, replacing the block whose next reference is furthest in the future among all cached blocks. However, 
it starts a fetch only if the next access to the replacement is after that to the block being fetched. 
Reverse aggressive is an offline prefetching aJgo­rithm that performs aggressive prefetching on the re­verse 
of its input sequence, then derives a schedule to serve the forward sequence from the schedule serving 
the reverse sequence. Space limitations preclude describing the details of this transformation and the 
reasons for its good performance. A companion paper [3] presents a proof that for any choice of data 
layout, and for any se­quence of accesses, the elapsed time incurred by reverse aggressive is close to 
optimal. Regular aggressive does not enjoy such a favorable performance guarantee. 262 Simulation results 
We used trace-driven simulation to evaluate the algo­rit hms performance under varying assumptions about 
fetch cost, data placement, lookahead, and replacement information. The reference streams are taken from 
traces of real file system behavior (in the Sprite [4] file system and on a DEC 5000/200 workstation 
under Ul­trix 4.3). We consider several data placement schemes: striping (disk hoMing the first block 
of each file chosen randomly); random (each block resides on a random disk); biased-striping (first block 
more likely to be on a hot disk); by-file (each file resides entirely on a ran­domly selecteci disk); 
and total-replication (every file block is replicated on every disk). Of the four practical schemes, 
striping and random balance the disks loads best. Biased-striping concentrates references to smaJl files 
(which occupy only one block) on the hot disks, but the load from large files is well clistributed. By­file 
can lead to severe short-term load imbalance, as a sequence of accesses to a file all require the same 
disk. With the striped layout, reverse aggressive is usu­ally close to optimal; in many cases its performance 
is nearly identical to that of regular aggressive under total replication, which implies that its performance 
is close to optimal among all policies and data layouts[2], An in­teresting implication is that total 
replication offers only a slight advantage over striping for these traces. On average, regular aggressive 
performs close to reverse ag­gressive. However, as the fetch time increases, reverse aggressive performs 
better as it balances the loads be­tween the disks more effectively. With the random layout, reverse 
aggressive is again very close to optimal. However, ~egulur aggressive per­forms much worse in some cases, 
especially when the fetch time is large. These results indicate that reverse aggressive is a more robust 
a~gorithm than regular ag­gressive, even when the distribution is fairly well bal­anced, as it is under 
a random distribution. Uncier the biased-striping distribution, the results are between those of the 
striping ancl random layouts. On average, reverse agflressive still achieves near-linear speedups. Though 
biased-striping favors the hot disks for the first block of a file, it places blocks within a file on 
different disks. Thus, for applications that use large files, it still balances the load on the disks 
well. We found a clear difference between regular aggres­sive and reverse aggressive for the by-file 
layout. Unless the application uses mostly smaU files, this layout can create serious load imbalance 
among the disks. Reverse aggressive deals with this imbalance much better. Our results show that the 
distribution of file data on disks is critical for parallel prefetching. Under striping, which distributes 
the loaci evenly, the speed up from par­allel prefetching on d disks can be very close to d, ancl regular 
aggressive can perform almost as well as reverse aggressive. When the load is unbalanced, reverse ag 
gressive clearly outperforms regular aggressive. We also performed experiments in which only lim-. ited 
lookahead information is available. Online versions of wgular aggressive and reverse aggressive perform 
be­tween 10% and 5070 worse than the offline algorithms:, depending on the lookahead distance, and not 
too dif­ferently from each other. Optimal replacement informa­tion improves their performance significantly. 
Applica­tions can often use knowledge of their file access patterns to provide such optimal replacement 
information [1]. Combined with our results in the offline case, we think that reverse aggressive with 
optimal replacement (if possible) is the algorithm of choice even in the more practical online case. 
Future directions This study has several limitations. First, most of our results assume full knowledge 
of the request stream is available in advance. Although this is in general not re­alistic, there are 
some applications that have this sort of knowledge. Better understanding of limited-lookahead versions 
of the algorithms is an area of future work. Second, we consider only read traffic. Writes make the problem 
much harder, but the write problem must be faced eventually. Finally, our simulations do not model disk 
behavior in detail. Acknowledgements This project is sponsored in part by ARPA under con­tract NOO014-95-1-1144, 
by NSF under grants MIP-9420653, CCR-9204242, GER-9450075 and CCR-9301186, by Digital Equipment Corporation 
and by Intel Corporation. Felten is supported by an NSF National Young Investigator Award. References 
[1] Pei Cao, EclwMd W. Felten, Anna Karlin, and Kai Li. Implementation and Performance of Integrated 
Application-Controlled Caching, Prefetching and Disk Scheduling. Technical Report TR-CS95-493, Princeton 
University, 1995. [2] Pei Cao, Eclward W. Felten, Anna Karlin, and Kai Li. A study of Integrated Prefetching 
and Caching Strategies. In Proceedings of the ACM SIGMETRICS, May 1995. [31 Tracy Kimbrel and Anna Karlin. 
Near-optimaJ Paral­ lel Prefetching. Technical Report UW-CSE-96-O1-1O, University of Washington, 1996. 
 [4] Michael N. Nelson, Brent B. Welch, and John K. ousterhout, Caching in the Sprite File System, ACM 
IYansactions on Computer Systems, 6(1):134­ 154,February 1988. 263  
			