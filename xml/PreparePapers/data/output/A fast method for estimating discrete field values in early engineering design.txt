
 A FAST METHOD FOR ESTIMATING DISCRETE IIELD VALUES IN EARLY ENGINEERING DESIGN Jovan Zagajac The Sibley 
School of Mechanical and Aerospace Engineering Cornell University Ithaca, NY 14853 Abstract Much of 
the analysis done in engineering design involves the solution of vartial differential actuations (PDEs) 
that are subject to ini~ial-value or boundary-value conditions; generically these are called field problems 
. Finite-element and finite-difference methods (FEM, FDM) are the predominant solution techniques, but 
these are often too expensive or too tedious to use in the early phases of design. What s needed is a 
fast method to compute estimates of field vafues at a few critical points that uses simple and robust 
geometric tools. This paper describes such a method. It is based on an old technique -integrating PDEs 
through stochastic (Monte Carlo) sampling -that is accelerated through the use of ray representations. 
In the first (pre-processing) stage, the domain (generally a mechanical part) is coherently sampled to 
produce a ray-rep. The second stage involves the usual stochastic sampling of the field, which is now 
enhanced by exploiting the semi-discrete character of ray-reps. The method is relatively insensitive 
to the complexity of the shape being analyzed, and it has adjustable precision. Its mechanics and advantages 
are illustrated by using Laplace s equation as an example. 1. Introduction Mechanical design is an iterative 
process in which the form and configuration of mechanical artifacts are refined repeatedly until some 
design objectives are met. Compliance with goals usually is established through analysis, which also 
provides hints for incremental design improvements] -1. Several important types of mechanical anaIysis 
require the solution of partial differential equations (PDEs); these are called field problems . Finite 
element and finite difference methods (FEM, FDM) are the predominant techniques for solving such problems 
in practice. Because design is iterative, analysis tools are needed frequently (ideally, whenever a design 
is modified). In practice however, using stnrdysis tools in this way is not possible for at least two 
dktinct reasons: First, the predominant field solvers (e.g. FEM, FDM) ordinarily require that the domain 
geometry be represented via cell decompositions, whereas most CAD systems are based on Boundary (b-rep) 
or Constructive Solid Geometry (CSG) representations. Consequently, the coupling between CAD and analysis 
requires representation Permission to copy without fee all or part of this material is granted provided 
that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice 
and the title of the publication and its date appear, and notice is given that copyin is by permission 
of the Association of Computing Machinery. o cop otherwise, or to republish, requires a fee ardor f spea 
rIc permission. Solid Modeling 95, Salt Lake City, Utah USA @ 1995 ACM 0-89791 -672-7/95/0005...$3.50 
conversion (commonly done by mesh or grid generators) that is tedious and expensive. Second, these methods 
produce global solutions, The designer must solve the problem in the entire region even though he may 
want vahres at only a few selected parts of the domain. Global solutions are an over-kill in the early 
stages of design when design parameters are fuzzy and the viability of options can be determined through 
quick check-point analysis. What is needed is an easy-to-use method that (1) produces quick estimates 
of field values at a few critical points in early design, and (2) does not require baroque or expensive 
geometric pre-processing. The Monte Carlo (MC) method for solving field problems, which is the main theme 
of this paper, is a well known technique based on statistical sampling that has the desired characteristics. 
Specifically, some MC methods for solving field problems do not require geometric pre-processing, The 
main computational task they pose is similar to the classical problem in solid modeling of classifying 
a (half-) line against a solid [Til 80]. In principle this operation, known in the context of MC methods 
as parricle-~racking, can be supported on any complete representation of solids [Req 80]. Moreover, the 
method can be used to determine the value of the field (or its derivatives) at an arbitrary point of 
a region without having to find the solution in the entire domain. MC methods are typically used to solve 
problems that involve many independent variables but simple geometry. For example, they have been a standard 
tool for solving some difficult problems in physics, such as neutron transport and statistical mechanics 
(see for instance [KW 86]). This is because the precision of MC calculations does not depend on the dimensionality 
of the problem under analysis. (The com utational accuracy of MC methods varies inversely with / N, where 
N is a measure of sampling density.) Complex geometry, on the other hand, often renders random sampling 
prohibitively expensive. MC methods have been used sporadically in engineering, typically when the stochastic 
nature of the method has an obvious physical interpretation (for instance, in tolerance analysis for 
assembly flur 87]). This is partly due to the fact that real engineering problems usually involve complex 
domains. Nevertheless, the use of MC methods in solving a 1-1 We are forced to design by iterative analysis 
because we do not know in general how to generate form and configuration directly from design goals. 
 wide range of engineering we introduce an efficient solving field problems by overall appeal. The paper 
is organized Monte Carlo method by describing a stochastic game based on boundary sampling for solving 
Laplace s equation with Dirichlet boundary conditions. In Section 3, some opportunities for efficient 
boundary sampling are identified. Section 4 explains how the necessary computations can be sequenced 
to improve overall efficiency by organizing them into ( 1) a coherent stage, in which the region of interest 
is coherently sampled to produce a ray-representation, and (2) an incoherent stage, wherein the usual 
stochastic sampling of the field is enhanced by exploiting the semi-discrete nature of ray足reps. Sections 
5 &#38; 6 illustrate this approach through examples and summarize some advantages of the method.  2, 
A toy problem Consider the following Dirichlet boundary Some physical property -temperature, say 足Laplace 
s equation in a convex, connected value problem. is modeled by subset $2 of Euclidean space En parametrized 
by position coordinates x: see Figure 1. The field T(x) is prescribed on the piece-wise smooth boundary 
of the region: T(x)la = f(x). We wish to determine ~(x) at a point P in the interior of the domain2-1. 
X2: boundaryof Q problems is growing. In this paper boundary sampling strategy for MC methods that increases 
their as follows. Section 2 introduces the (1) From point P we launch at random a heat particle , denoted 
by a , toward Z! the boundary of Q: see Figure 2a. We assume that all directions of propagation are equally 
probable. (2) We record the temperature j(a~ ) at the point tzj where the particle hits X)2-3. (3) 
At aj the particle bounces from X) toward the interior of f2 (Figure 2b). All directions of prop足agation 
are again equally probable (this time, over half of a solid sphere). (4) We repeat steps 2 and 3 until 
some preset number of collisions (say M) take place. The last collision site is denoted by aj. (5) We 
repeat steps 1 to 4 (Figure 2c) until we have traced a sufficiently large number of such particles (say 
~.  D(a)D a   P P a 1 a; .--- . ..-. i----足ai a a Q a 1 a; (b) (c) Figure 2: A Monte Carlo game. 
 W (x s ifiedby x) Figure 1: A Dirichiet bounckz~ value problem. 2.1 A Monte-Carlo game This problem 
can be solved with any of the standard tools of engineering analysis (e.g. FEM, FDM, ... or when the 
region is simple enough -symbolic integration). However, such approaches are sometimes over-kill for 
reasons given earlier. Let s solve the problem by playing the following stochastic game2-2 that simulates 
the diffusion of imaginary heat particles (see [HB 74a]). 2-1 The solution method described in this section 
is not restricted to convex domains and can be generalized easily to arbitrary domains, We assume convexity 
merely to simplify an instructive explanation, given in the appendix, of why the method works. 2 2 The 
main idea behind this algorithm has appeared in the literature (often independently) a number of times. 
See, for instance [HG 69], where the relationship between Brownian motion and Potential theory is illustrated 
with a similar argument. An estimate of the field at P can now be computed from the expression: T(P) 
=j, +~(-l)m(7m+l-@ (1) where (2) is the expected value of the temperature probed on &#38;2 by heat particles 
on their m -th collision with the boundary. The stochastic nature of Equation (2) permits one to use 
standard statistical tools to establish when a sufficiently large collection of field samples (step 5 
of the algorithm) has been tracked. Equation (1), on the other hand, hints at the significance of the 
recursion depth in the algorithm (step 4): accuracy grows with M. (The question how big should M be can 
be answered rigorously through standard analysis, but for our purpose it suffices to say that M=6 usually 
is adequate). 2-3 If the collision happens to be at a vertex or an edge of ~fl, or at a discontinuity 
of j?.r), the game resumes with step 1. Removing the convexity requirement entails minor but important 
mocMications to the outlined procedure [HB 74b]. For non-convex domains all boundary points along the 
ray must be identified and classified as entry or exit . One intercept is then selected at random as 
the next collision site and the magnitude of the statistical weight of the particle is increased by a 
factor equal to the number of intercepts along the ray. If the chosen intercept is entry , the weight 
also changes sign. (For convex domains the weights of all particles are +1 ). The algorithm just described 
is hardly intuitive. Formally, the method is based on the Neumann solution of the boundary integral form 
of Laplace s equation, expressed in terms of double layer potentials [Kel 29]. An informal but instructive 
justification of the algorithm s validity is outlined in the appendix. 2,2 Computational requirements 
An inspection of the algorithm reveals the following computational tasks. Represent the geometry of the 
domain (2 in a computationally tractable manner. Generate random propagation directions. Calculate the 
boundary collision sites and for each compute neighborhoods (surface normals). Accumulate results per 
Equations (2) and (1). Perform variance monitoring via statistical analysis. By far the most demanding 
chore in problems with non足trivial domains is determining collision sites, i.e. tracking particles as 
they bounce around in the interior of the domain. We call this task bounda~ sampling. TM is a special 
case of Set Membership Classification [Til 80], wherein a path (a candidate set) is classified against 
a domain (a reference set). Any complete representation of solids may be used in calculating this function 
[Req 80]. However, the efficiency of boundary sampling is rep-scheme dependent. It is therefore important 
to choose a domain representation judiciously.  3. Domain representations for boundary sampling The 
main computational load in the described MC procedure is calculating and classifying boundary intercepts 
of many random rays launched from points in the boundary or in the interior of the domain. The effectiveness 
of the procedure hangs on our ability to minimize the costs of these calculations. The most suitable 
representation for the task at hand is a set of generalized Gauss maps for all points of the domain from 
which rays can be launched. (A generalized Gauss map is a complete representation that specifies, in 
a polar coordinate system centered at a point P, the distances from P to af 1 boundary intercepts along 
all rays emanating from P: see Figure 3). Intercept identification thus becomes an exercise in constant-time 
table lookup that is essentially independent of the complexity of the domain. Unfortunately, this representational 
option is wildly unrealistic, for reasons too obvious to elaborate. + J Figure 3: A snapshot of a generalized 
Gauss map representation at an interiorpoint for a few directions. Intercept analysis on conventional 
b-reps or CSG reps is similar to linekolid classification. The asymptotic worst case time-complexity 
of such algorithms is 0( F) where F is the number of faces in a b-rep, and 0( HlogH ) where H is the 
number of halfspaces in CSG. This is prohibitively expensive if we must process thousands of rays on 
complex mechanical solids whose b-rep or CSG representations may involve many thousands of primitives. 
We need a working representation that has Gauss-map-like characteristics but does not impose impractical 
storage or processing requirements. The computer graphics community has faced a similar problem in trying 
to accelerate ray-tracing for realistic rendering. (Ray-tracing is a variant of lineAolid classification 
in which only the first intersection of a ray with a collection of objects is sought). A powerful method 
for accelerating ray足tracing is based on cataloging b-rep or CSG primitives into addressable cells of 
spatial decompositions. Voxels (uniform spatial subdivision), octrees (hierarchical spatial subdivision) 
and BSP (Binary Space Partitions) are used frequently for this purpose [AK 88]. 1 234567 a b c d e 
f Figure 4: Accelerated ray-tracing via hashing of b-rep primitives in a uniform spatial subdivision. 
For example, imagine tracing ray A in the b-rep object in Figure 4. The brute force approach is: find 
the intersections of the ray with all the faces of the object, sort them and select the closest. This 
involves unnecessary work since many faces will not be intercepted by the ray at all, or may yield intercepts 
that are far away. Instead, we may simulate ray-tracing by traversing the ray while performing intersection 
calculations only with primitives that are found in the traversed spatial compartments (cells c2, c3, 
C4 in Figure 4). If the cost of identifying neighboring compartments ( grid walking , see [Glass 84, 
FTI 86, CW 88, YCK 92]) does not outweigh the gains achieved by decreasing the number of candidate primitives 
through cataloging, speedups in computing are possible. Ray-tracers that utilize such means to render 
scenes of arbitrary complexity a[ a fixed cost have been reported [OM 87, CW 88]. However, such performance 
is possible only because seeking just the first boundary intercept considerably simplifies the problem. 
When such techniques are used in boundary sampling, the resulting processing enhancements may not be 
adequate. Two sources of extraneous work can be easily identified in the described method: (1) repeated 
intersection calculations performed in a small region of space, and (2) redundant querying of cells that 
are not interesting , Both can be identified in Figure 4. Specifically observe that rays A and B both 
terminate in the same cell C4 which contains a circle in its list of cataloged faces. The location where 
the rays intersect the circle is typically computed independently in each instance. This is done in spite 
of the fact that intercepts will be only a small distance apart. In computer graphics, such practice 
is common because ray/object intercepts usually must be determined exactly. Accuracy is paramount as 
many common optical phenomena (e.g. specular reflection, shadows caused by point and distributed light 
sources, .,, ) are sensitive to geometrical perturbations. Many boundary value problems encountered in 
engineering analysis are more forgiving because elliptic PDEs tend to have smooth solutions that respond 
smoothly to timid boundary perturbations or changes in boundary conditions3- . This fact is significant 
because, when high precision is not warranted, it permits us to trade accuracy for efficiency in boundary 
sampling. For example: we can catalogue represen足tative, pre-computed boundary intercepts (e.g. one boundary足point-sample 
per spatial cell in Figure 4) rather than represen足tational primitives. To see redundant cell querying 
in Figure 4, consider ray A . Identifying all boundary intercepts along this ray, entails querying cells 
c2-c4. But cell C3 does not contain any useful data. Usually many such cells will be queried before one 
containing a boundary intercept is encountered. Avoiding redundant cell queries in the case of 2-D parallel 
rays is akin to exploiting scan coherence. Suppose that all rays that we wish to process are (almost) 
parallel, like rays A and C in Figure4: we may then exploit scan coherence by using a different indexing 
structure for boundary intercepts. One such structure is illustrated in Figure 5, Here we have imposed 
a grid of lines having a particular direction (horizontal) and cataloged boundary intercepts along these 
lines. Approximate ray/solid 1-1 The most famous embodiment of this observation 1s St. Venant s principle 
of linear elasticity which states that small local perturbations of loading (or equivalently -geometry) 
set up stresses and strains of detectable magnitude only within proportionately small distances, intercept 
calculations for almost horizontal rays (e.g. ray A and ray C in Figure 4) then becomes a simple matter 
of table look足up. The cataloging structure for boundary intercepts shown in Figure 5 is called a ray-representation 
(ray-rep for short) [EKLTMMV 91 ]. rayB a b c /.. / d e f 9 h i Figure 5: An alternative way of hashing 
boundary intercepts. A ray-rep can be interpreted as a slice of a discrete Gauss map representation providing 
precise boundary information in a single direction for all points in the object that Iie in a grid of 
lines, A single ray-rep is therefore a long way from our ideal representational option. But, unlike the 
conventional spatial su-bdivision schemes, ray-reps do represent explicitly (in a storage-wise realistic 
way) some directional data. Their usefulness in boundary sampling hinges on our ability to process rays 
that are not aligned with the scan-line direction (e.g. ray B in Figure 5). An efficient technique for 
doing this that uses multiple ray-reps and conventional grid-walking is described in the next section. 
 4. A ray-rep strategy for approximate boundary sampling Efficient boundary sampling can be done in two 
stages. In the first (pre-processing) stage, ray-reps are generated by classifying regular grids of parallel 
lines. In the second stage, ray-intercepts along arbitrary rays are identified by using a technique that 
exploits the semi-discrete nature of ray-reps to do a form of grid-walking . 4.1 Coherent sampling: 
Ray-representations Figure 6 shows a 2-D example of a ray-rep obtained by clipping a uniform grid of 
regularly spaced parallel lines against a 2-D solid. The representation consists of the in足solid line 
segments, each defined by endpoints and optional tags carrying some useful symbolic information (for 
instance, tags may point to equations of half-spaces, describe material properties, or specify boundary 
conditions in boundary value problems), The target solid is usually described in CSG or b-rep and the 
ray-rep is computed by classifying the lines of the grid with respect to the solid. Formally, let G denote 
a set of parallel 3-D rays arranged in a regular grid determined by G={N, O, V, X) where N={nl, n2) is 
the grid density, O is the grid origin, V specifies the ray direction and W={ H 1, H2 } are vectors representing 
the spacing between the rays (see Figure 7). The set membership classification function M (G,S) partitions 
the grid G with respect to a solid S into disjoint segments of parallel lines that lie entirely inside 
(GinS), outside (GourS) or in the boundary (GorrS) of S [Til 80]. The set consisting of the in (GinS) 
and on (Gon S) segments, together with tags appended to the endpoints of the in variety, is R[G,S] -the 
ray-rep of S.4-1 Ray-repofsolid (X,y,tag)J Figure 6: A ray-representation in E2. .足  x /-referenceTM 
 fwodd)coordinatefem z Figure 7: A grid Gin E3. The boundary intercepts in a single ray-rep may not be 
sufficient to represent well the boundary of an arbitrary domain. The reason for this is the uni-directional 
nature of the generating ray-grid. For example, parts of domain boundaries where the surface notmal is 
almost perpendicular to the grid direction are usually under-sampled. This can introduce severe biasing 
in MC calculations and must be avoided. To alleviate this problem, we use a ray-rep tuple T=(RIGl ,S], 
R[G2,S], .... RIGn,Sl ), where the generating grids Gl, G2, .... Gn are chosen to obtain a reasonably 
uniform sampling of the boundary. One practical choice is a set of three mutually orthogonal ray-reps. 
  4.2 Incoherent sampling: The Multiaxis 2.5 Dimensional Digital Differential Analyzer Ray-rep tuples 
hash directional data for a limited number of directions (for example, three in the case of orthogonal 
ray足reps). The Multiaxis 2.5 Dimensional Digital Differential 4-1 The generation of on segments cart 
be prevented by a proper choice of ray-direction [EKLTMMV 91 ], Analyzer (Multiaxis 2.5D3A) is a mechanism 
for identifying boundary intercepts for arbitrary ray directions from data in ray-rep tuples. The Multiaxis 
2.5 D3A exploits the spatial addressability of the ray-rep grid, and is a generalization of a classic 
in足cremental procedure for scan-converting lines on pixel displays [Bre 65]. The identification of boundary 
intercepts for a ray with direction D (Figure 8a) begins by selecting the ray足rep from a ray-rep tuple 
for which the trace of the vector D on V is maximum (the max(V,D) ray-rep). The vector D is then projected 
on the grid plane of the ray-rep and a slightly modified DDA (Digital Differential Analyzer [Bre 65]) 
is run (Figure 8b). As grid points are traversed, interval tests on ray足rep segments are performed to 
detect boundary crossings4-2. When a crossing is found, the surface normal N at the boundary point is 
computed. (Methods for calculating surface normals with ray-reps are briefly described below. ) If N 
and D are nearly orthogonal, the ray-rep may not hash a boundary intercept that is close enough to the 
actual boundary crossing. In that case the grid walk is switched to the rnax(V,N) ray-rep (Figure 12c) 
and is continued (starting from the last in point in the nrax(V,D) ray-rep) until the domain boundary 
is crossed again. This event triggers the resumption of the walk in the nmx(V,D) ray-rep. The walk is 
terminated when the projection of D on any grid plane has been traversed.  /l k-b- pof ids Ps+2K v 
 + ray-rep of S: i- R[G2S] (b) (c) Figure 8: The Multiaxis 2. 5@A. The described procedure uses surface 
normals to detect when a ray-rep may not hash a boundary intercept accurately enough. Surface normals 
are also used in the MC game to determine the direction in which heat particles bounce after they collide 
with the domain boundary. We can obtain surface normals from ray-rep data in a number of ways. One technique 
is to perform a local search to identify neighboring boundary points, tit an algebraic surface through 
them and then infer the 4-2 This can be done efficiently because boundaryhtEXCq3h in ray-reps are pre-sorted. 
 normal. Typically planar fits suffice. Alternatively, we can use ray-rep tags pointing to the algebraic 
equation of surfaces sampled during ray-rep generation. Normals can then be computed analytically, or 
drawn from pre-computed tables. 4.3 On the Practicality of the Ray-rep Strategy The described strategy 
for boundary sampling raises the following issues: How expensive is ray-rep generation? How efficient 
is the Multixaxis 2,5D3A? How accurate is boundary sampling? Careful complexity and error analyses are 
beyond the scope of this paper, but in the sequel we discuss some of the relevant issues and provide 
experimental data. Pre-processing Ray-rep generation is a computationally intensive operation that involves 
many repetitive and inherently independent calculations. It is therefore a task that is ideally suited 
for parallel processing. This fact is exploited in the Ray-Casting Engine (RCE) -a highly parallel special 
purpose computer in which the CSG divide and conquer algorithm for line/solid classification is embedded 
in silicon [EKLTMMV 91]. The RCE can produce swiftly ray-reps of domains described by millions of quadratic 
halfspaces [MMZ 94]. When ray-reps are generated by using such a computer, pre-processing costs are usually 
negligible. However, parallel computation is not a prerequisite for reasonably efficient preprocessing, 
We have experimented with various RCE software emulators that perform well on a single general purpose 
computer (see Table 1 in Section 5). Moreover, all modeling systems that support line/solid classification 
can be used (at least in principle) as ray-rep pre足processors. Although the complexity of individual 
ray-rep generators can be analyzed (see for instance [MMV 94] for an RCE study), in general it is not 
possible to compare their performances because we do not know how to compare formally algorithms that: 
( 1) operate on different kinds of input (i.e. representations in line/solid classification), or (2) 
rely on dissimilar models of computation. Efficiency An elementary analysis of the Multixaxis 2.5D3A 
algorithm for a single ray-rep shows that its worst case time complexity (assuming negligible ray-scanning 
costs) is 0( GRID ) where GRID is the linear ray-rep size (using the notation introduced earlier GRID 
= max(nl, n2) -see Figure 7). This kind of performance4-4 characterizes most grid-walking algorithms 
that use spatial decompositions (see Section 3). We argued that ray-reps may be practical in boundary 
sampling because they explicitly encode some directional information. How effectively is such information 
used? We give an elementary analysis of how well directional coherence is used by comparing grid-walking 
on voxels and on ray-reps. 4-4 The cost of grid-traversal is independent of the shapes that the cell 
decomposition represents. This is the reason why cell足decomposition techniques are valuable in ray-tracing 
and boundary sampling of complex objects. Suppose that the cost of ray-scanning is commensurate to the 
cost of detecting intercepts in a single voxel (this will be true for example if a ray-rep dces not contain 
many segments along individual grid lines). The relative efficiency of ray-rep and voxel grid-walking 
procedures will then be proportional to the ratio of the number of cells that are queried in each method. 
Grid-walking algorithms that use voxels are at a disad足vantage over those based on ray-reps because of 
their higher dimensionality. To estimate by how much, consider grid足walking in 2-D along rays of length 
L randomly distributed in a 2-D cone of aperture 6 (Figure 9). Such rays in the Multixaxis 2. 5D3A will 
be processed by selecting an appropriately aligned ray-rep (e.g. with gridlines parallel to the y-axis). 
Observe that 8 is determined by the number and choice of directions of ray足reps in a tuple (e.g. for 
a pair of orthogonal ray-reps in 2-D f?= 7r/4). A straight-forward analysis reveals that the average 
projection of such rand~m rays on the x and y axes are X = L(I -cos (3)/ O and Y = Lsin 0/ @ respectively, 
The per足formance ratio R for voxel to ray-rep grid-walking is then roughly: R=I+7 ~+ sinfl ~. (3) x I-cose 
Y t I i x Figure 9: A cone for grid-waiking. For 6 = 7r/2, which corresponds to the use of a single ray足rep, 
R=2. For O = x14, R=3.4 and for t? = x18, R=6. O. In the limit of f3 approaching O, ray-scanning is used 
exclusively and the ray-rep method becomes infinitely times more efficient. We stress that Equation (3) 
was derived by making a number of simplifying assumptions that in practice may not always be justified. 
Moreover, the given performance estimates should not be mistaken for global ones (a more careful analysis 
must address issues such as pre-processing costs, grid switching rates, etc.). Accuracy The issue of 
accuracy of the Multiaxis 2.5D3A is relevant in the context of Monte Carlo analysis and we address it 
in the next section through numerical experiments.  5. Numerical experiments In this section we investigate 
several performance issues through boundary sampling experiments conducted by using the models of six 
parts of a small vacuum pump, shown dis足assembled in Figure 10. The complexity of these parts is quantified 
in the first column of Table 1, where the number of b足rep faces and the number of CSG halfspaces that 
one may need to specify in order to describe the parts are listeds-1. All computing was done on a DEC 
3000/600 APP workstation equipped with 96 Mb of memory. c Figure 10: Benchmark object~. Sampling performance 
Traditionally, boundary sampling in MC analysis is done via line/solid classification using boundary 
or CSG rep足resentations of solids. To provide a standard for comparison, we implemented the MC algorithm 
described in Section 2 using the ParaSolidTM linekolid classification utility. (ParaSolid is a commercially 
available b-rep based solid modeler.) The rates at which boundary samples were collected in this implementation 
for random rays bouncing between the boundaries of the benchmark objects are shown in Figure 11 (data 
at the bottom of the graph marked brep ). As expected (see Section 3), the sampling rate decreases steadily 
as the complexity of objects increases. The close to linear trend quickly renders this approach to boundary 
sampling impractical. Rates at which boundary samples were produced by using the Multiaxis 2.5D3A and 
pre-computed ray-reps are shown in the same figure for four different ray-rep GRID sizes (128, 256, 512 
and 1024). The observed sampling rate is almost independent of object-complexity and exhibits an 0( GRID 
) behavior. (For instance, the 512x5 12 and 1024x1024 sampling rates differ roughly by a factor of 2). 
Also, note that the ray-rep based boundary sampling method outperforms the one that uses b-rep line/solid 
classification for all tested objects. Ray-reps are simple and easier to process than the baroque graph-based 
data structures of boundary and CSG representations, and this is reflected in the results. Fast boundary 
sampling in the ray-rep based technique is achieved by sacrificing accuracy. We illustrate the effects 
of discretization errors through a simple numerical experiment conducted by solving Laplace s equation 
(with Dirichlet boundary conditions) inside a unit box centered at the origin. 5-1 B-rep and CSG representations 
are not unique and measuring object complexity in this way may be misleading. However, the numbers listed 
in the first column of Table 1 are close to minimal and will do for our purposes. All sides of the box 
were held at a uniform temperature T=O, except the face at 2=0.5 for which T= 1. An exact analytical 
solution was calculated by classical methods for reference purposes. A MC numerical solution was obtained 
using a single ray-rep with grid direction parallel to a diagonal of the cube. The recursion depth was 
M=6 and the number of traced histories N=1OO,OOO. Figure 12 shows the error of the MC results at a few 
points in the cube as a function of the ray-rep GRID size. The shaded region is a 36 confidence interval 
-a statistical barriel set by N=1OO,OOOthat can not be defeated by varying the parameters of ray-rep 
tuples. Observe that all runs with ray足reps whose linear GRID size was greater than 100 produced statistically 
correct results. 100I 10 100 Objectccfnc.lexity (b-repfaces) Figure 11: Ray-rep and b-rep boundary sampling 
rates for benchmark objects in Figure 10. 1 0.1 0.01 n MI 10 1000 Ray:: GRIDdaneity Figure 12: Field 
value errors at a few locations in a unit box as a function of ray-rep GRID densi/y. Ray-rep generation 
If ray-reps were free, the reported results would seem to suggest that the ray-rep based boundary sampling 
technique is a clear winner. Ray-rep generation costs may be negligible when parallel computation is 
used (e.g. special purpose hardware such as the RCE; see examples in [MMZ 94]). But we can pose a more 
interesting question: is the ray-rep based boundary sampling strategy attractive if we consistently use 
a single model of computation (e.g. serial)? When ray-reps are archived rather than discarded after a 
single use, their generation can be viewed as a one-time pre-processing investment with a fixed price-tag. 
The actual price depends mainly on the efficiency of utilities available to do line/solid classification. 
If objects are defined via boundary representations, then ray-reps can be generated by using Iinek.olid 
classification utilities such as the one available in ParaSolid. Plots similar to those shown in Figures 
11 and 12 can then help us to decide when ray-reps should be generated and used in boundary sampling. 
Here is a simple example. Figure 12 suggests that ray-reps whose GRID size is 100 are dense enough . 
Using the b-rep performance data shown in Figure 11 we can then estimate the cost of ray-rep generation. 
Thus, an object having 60 faces may require roughly 100 x 100 / 200 = 200 seconds of pre-processing time. 
If J is the number of boundary samples that we need in MC analysis, the total cost will be 200 + J/ 8,000. 
If, on the other hand, we take a direct approach and use line/solid classification on b-reps, the total 
cost will be roughly J / 200, The direct approach is better when the number of boundary samples that 
we need to generate is less than 41,000-or more generally, when the ray-rep generation time exceeds the 
ray-rep based MC anafysis time. Such a threshold can be reached easily in practice. (The exemplary numerical 
values given here are mainly indicative because they are based on data collected with untuned code). 
When objects are defined in CSG, pre-processing may be more efficient. We generated ray-reps using an 
experimental RCE emulator executing on a generaf purpose computer. The emulator simulates an RCE in software 
by (1) spawning UNIX processes to imitate the processors of the RCE parallel computer (see [EKLTMMV 91 
]), and (2) using UNIX interprocess communication facilities (sockets and shared memory) to mimic the 
RCE silicon paths between them. Table 1 lists the times needed to generate a single ray-rep via RCE emulation 
as a function of grid size for objects shown in Figure 10. The worst case time complexity of our algorithm 
is 0( HlogH GRID2 ), where H is the number of halfspaces in the CSG description of the object and GRID 
is the linear ray-rep density [MMV 94]. Observe that this kind of performance characterizes the timings 
reported in Table 1. These results suggest that modern workstations may provide adequate processing power 
for generating ray-reps of relatively simple objects. They may also be sufficient when objects have special 
properties that can be used to advantage. (For example: we have ray-cast in a few hours of CPU time Minkowski 
summation and difference of hundreds of thousands of quadric halfspaces), However, brute-force parallel 
processing appears to be the most attractive route for generating ray-reps of real objects, e.g. engine 
blocks. Our results suggest that close to interactive solutions to boundary value problems are within 
reach, For example, by consulting Figure 11 and Table 1, we can estimate the net cost of solving Laplace 
s equation for the mechanical objects shown in Figure 10. For a grid density GRID=l 28, recursion depth 
M=6 and total number of histories N= 10,000 which correspond to an estimated 390 error in computed field 
value, the total CPU time for part B is roughly 7 + 0.00001 x 6 x 10,000 = 13 seconds (preprocessing 
time + sampling rate x recursion depth x number of samples). 1 Rav-reo GRIDxGRID densitv I I fwesh~aces 
I 128x128 I 256x256 I 512x512 I 1024x1024 I Al 6112 1,83 6.35 22.98 94.97 I 1I I BI 59/51 7,03 25.67 
102.97 404.90 c 19 I 30 448 16.67 66,7S 261.07 D 31152 6,73 25.98 102.77 416.42 E ls /21 2.58 8.78 
33.25 133,85 FI 416 I 1,12 I 2.68 I 11.15 I 42.0S I Table 1 Times required to compute, via RCE emulation 
on a DEC 3@0/6# APP workstation, single ray-reps for the solids shown in Figure IO Further processing 
enhancements can be achieved by fine tuning the procedure (for instance by using variance reduction techniques 
or by improving the ray-rep traversal method). With parrdlel processing, which can be used effectively 
in all stages of the MC method, very fast engineering analysis may be possible. This aspect of the technique 
is being investigated further.  6. Other boundary value problems The Monte Carlo method for obtaining 
point solutions of Laplace s equation with Dirichlet boundary conditions is reafly more general than 
the discussion in Section 2 indicates, and can be used to solve any Boundary Integral Equation (BIE) 
of the Fredholm variety [Kan 71]. Because many boundary value problems in engineering can be put into 
this form, the scope of the method is quite broad and includes problems in linear elasticity, diffusion 
and computer graphics (see [Zag 95]). In addition problems involving Neumann and mixed boundary conditions 
can be also solved. The various solution procedures differ mainly in the way in which we select random 
ray directions and in the character of estimators that we accumulate, But, they all use boundary sampling, 
and can therefore exploit the ray-rep based, two-stage approach we have described.  7. Summary and remarks 
This paper illustrates an approach for coupling CAD and engineering analysis that avoids the pitfalls 
of traditional techniques that use mesh and grid generation. By relaxing the expectations of engineering 
analysis (global vs. point solutions), we were able to identify an analysis method that uses one simple 
geometrical utility. We then concentrated our efforts on improving the tools available to support it. 
In Section 2 we described an old technique for solving field problems that uses Monte Carlo sampling. 
The method has two prominent characteristics: it produces point solutions and it Nonconvex Regions , 
Trans. Am. Nucl. Sot., vol. 19, pp uses a simple geometric utility -boundary sampling. 164, 1974. The 
pointwise character of the method is desirable in early design when global solutions to field problems 
are usually an over-kill . In addition, the technique permits us to solve boundary value problems without 
meshing, and without inverting large matrices. Instead, we merely need to accumulate a small number of 
estimators and fast, potentially interactive, local solutions to field problems are within reach, provided 
we can make our principal computational utility efficient enough. In Sections 3 and 4 we argued that 
the performance of boundary sampling can be improved by carefully structuring geometrical computations. 
An examination of the context in which boundary sampling is done led us to the two-stage sampling strategy 
based on ray-representations. Our strategy provides means for analyzing common field problems swiftly 
in very complicated domains, and makes parallelism easy to exploit in all stages of computation. This 
aspect of the method is being investigated further. 8. Acknowledgments I would like to thank Herb Voelcker 
for his suggestions, criticism and encouragement during the course of this work, and K. Suresh for helpful 
ideas. The research reported in this paper was supported by the National Science Foundation under Grants 
MIP-90-07501 and MIP-93-17620, by the AT&#38;T Bell Lab足oratories (Manufacturing Affiliates Program), 
and by the Unigraphics Division of EDS (ParaSolidTM). 9. References [AK 88] Arvo, J., Kirk, D., A survey 
of ray tracing acceleration techniques , ACM SIGGRAPH 88 tutorial notes, vol. 7, New York, 1988. [Bre 
65] Bresenham, J.E., Algorithm for computer control of a digital plotter , IBM Sysr. 1, vol. 4, no. 1, 
pp. 25足30, 1965. [CW 88] Cleary J. G., Wyvill, G., Analysis of an algorithm for fast ray-tracing using 
uniform space subdivision , Visual Computer, Vol. 4, pp 65-83, 1988. [EKLTMMV91] Ellis J. L., Kedem, 
G., Lyerly, T, C., Thielman, D. G., Marisa, R. J., Menon, J. P., and Voelcker H. B., The RayCasting Engine 
and ray representations: a technical summary , International Journal of Computational Geometry and Applications, 
vol. 1, no. 4, pp. 347-380, December 1991. [ITI 86] Fujimoto, A., Tanaka, T., and Iwata, K., ARTS: accelerated 
ray tracing system , IEEE CG&#38;A, vol. 6, no. 4, pp. 16-26, April 1986. [Glass 84] Glassner, A. S,, 
Space Subdivision for Fast Ray Tracing , IEEE CG&#38;A, vol. 4, no, 10, pp 15-22, October 1984. [HB 74a] 
Hoffman T. J., and Banks N. E., Monte Carlo Solution to the Dirichlet Problem with the Double-Layer Potential 
Density , Trans. Am. Nucl. Mc., vol. 18, pp 136足137, 1974. [HB 74b] Hoffman T. J,, and Banks N. E., Extension 
of the Surface-Density Monte Carlo Technique to Heat Transfer in [HG 69] Hersh, R., Griego, R. J., Brownian 
Motion and Potential Theory , Scientific American 220(3), pp. 67-74, 1969. [Kan 71] Kanwal R. P., Linear 
Integral Equations , Academic Press, 1971. [Kel 29] Kellogg O. D., Foundations of Potential Theory , 
Dover Publications, Inc. 1929. [KW 86] Kales M. H., Whitlock P. A., Monte Carlo Methods, Volume I: basics 
, John Wiley &#38; Sons, A Wiley足interscience publication, New York, 1986. [MMV 94] Menon, J. P., Marisa, 
R. J., and Voelcker, H. B., Theoretical Relative Efficiency of the RayCasting Engine, A Parallel CSG 
Classification Computer , Proc. CSG94: Set-theoretic Solid Modeling Techniques and Applications, published 
by Information Geometers, Winchester, UK, pp. 225-250, April 13-15, 1994. [MMZ 94] Menon, J. P., Marisa, 
R. J., and Zagajac, J., More powerful solid modeling through ray representations , IEEE CG&#38;A, vol. 
14, no. 3, pp. 22-35, May 1994. [OM 87] Ohta, M., Mamouru, M., Ray Coherence Theorem and Constant Time 
Ray Tracing Algorithm , Computer Graphics 1987 (~roc. of CG lntemat. 87), ed. T. L. Kunii, pp. 303-314, 
1987. [Req 80] Requicha, A. A, G., Representations for rigid solids: theory, methods and systems , ACIU 
Computing Surveys, vol. 12, no. 4, pp. 437-464, December 1980. ~11 80] Tilove, R. B,, Set membership 
classification: a unified approach to geometric intersection problems , IEEE Trans. on Computers, vol. 
C-29, no. 20, pp. 874-883, October 1980. ~ur 87] Turner, J. U., Tolerances in computer-aided geometric 
design , Ph.D. Dissertation, Dept. of Computer and System Engineering, Rensselaer Polytechnic Institute, 
May 1987. [YCK 86] Yagel, R., Cohen, D., and Kaufman, A., Discrete Ray Tracing , IEEE CG&#38;A, vol. 
12, no. 5, pp 19足28, September 1992. [Zag 95] Zagajac J., Image Rendering: A Monte Cario Method for a 
Boundary Value Formulation , DRAFT Tech. Report CPA93-7, Sibley School of Mechanical Engineering, Cornell 
University, Ithaca, NY, June 1995 (expected). Appendix To prove that the algorithm described in Section 
2.1 solves Laplace s equation with Dirichlet boundary conditions we need to show (in the limit of many 
tracked particles and collisions) the following, ( 1) T(x), i.e. the value T(P) generalized to a function 
in the neighborhood of a point P, is harmonic lo-1 , and (2) the imposed boundary condition is satisfied, 
i.e. T(x)=flx) on K? Given these conditions, we can then conclude that the algorithm produces the result 
we seek, because solutions to Dirichlet problems are unique. First we outline the harmonicity argument, 
Harmonic functions exhibit the following (mean value) properry: the value of a harmonic function at a 
point P inside a domain is equal to the average of its values on any sphere contained in the domain and 
centered at P. Our task is to show that the algorithm produces field estimates that satisfy this property. 
This -in conjunction with continuity, which is not difficult to demonstrate -is a necessary and sufficient 
condition for harmonicity [Kel 29]. In order for T(x) to be harmonic, it is sufficient for each of the 
terms in Equation (1) to be harmonic. To show this, we first need to establish the following fact: The 
probability of a particle emerging from a point F in S2 and colliding with i)~ at Q is equal to the average 
probability that a particle emerging from a point P on a circle of radius R centered at P will impact 
the boundary of the domain at Q, * Figure 13: The mean value property of harmonic -tiusctions. We shall 
prove this claim for a two-dimensional convex domain fl; generalizing the argument to higher dimensions 
is not difficult, and convexity is merely a convenience2-1. 10 1Functions that satisfy Laplace s equation 
are said to be harmonic. The probability p(QIP) that a particle from P will collide with afl at Q is 
equal to the angle subtended by a differential element do of X2 at Q projected onto a circle of radius 
6 centered at P (see Figure 13), i.e. (4) P(Qlp) = ~COS @ The probability that a particle emerging from 
P on a circle of radius R centered at P will impact-d; at Q is p(Q[P; P,R)=~ Cos($l+ lp). (5) 2 np If 
particles are emitted randomly from random points on the circle with uniform probability, then the average 
probability of them impacting dm at Q is F(QIP,R) = +~P(QIF ;P,R)N3 o Thus p(QIP) = ~(QIP, R) We can 
now show that the algorithm generates field values with the mean value property. Observe that T(P) is 
obtained in Equation (1) by averaging boundary temperatures probed by heat particles moving from point 
P in random directions. But these particles are indistinguishable from those that are emitted at random 
from a circle centered at P. Thergfore, on average they produce the same first-collision score f 1. Moreover, 
all further collisions are indistinguishable since particle pro~足agation is not biased by prior collision 
history. Thus f~ (m> 1) in Equation (2) is same for these two kinds of particles and we may conclude 
that the mean value property holds. This inference is true regardless of what each ~m in Equation (I) 
represents (or how we add them up ) as long as they are harmonic. To see that the algorithm also respects 
boundary conditions, imagine that we are playing the MC game at some point P arbitrarily close to the 
boundary of the domain (approaching in the limit a boundary point Q; see Figure 14a). The collection 
of all particles that are racing away from point P can be subdivided into two heats of equal size: those 
that are moving towards the nearby boundary (let us call them heat A particIes) and those that are moving 
away from it ( heat B particles). These are shown in Figures 14b and 14c, Heat A particles will first 
bounce from the nearby boundary. In the limit of P approaching Q they will continue their voyage along 
trajectories that are statistically indistinguishable from the paths of particles in heat B that have 
not collided yet (Figures 14d and 14c respectively). At the same time, heat B particles will collide 
with far away boundaries and continue along different trajectories (Figure 14e). This process will continue 
indefinitely with particles in heat A always lagging those in heat B by one bounce (Figure 14f). Q Ocollisicm 
Q (b) 8 ,.: :. 8 (c) > * \ P4.足 . P + (d) Q 1collisions Q (e) 8 \ \ P足 . . *e /+ Q 2colltitons (f) Figure 
14: The race of particles in two heats  If we write the temperature that the particles sense after 
they experience m collisions as jm=~~(heat A)+~~(heat B), (7) then the above argument implies ~~(heat 
B) = ~M+l(hear A). (8)  Moreover, because of an effective randomization of heat particle trajectories, 
in the limit of many collisions, we have lim ~m (heat A) = lim ~~ (heat B). (9) m-+. m+-  Now, suppose 
M in equation (1) is odd (similar reasoning can be usedforMeven).WecanrewriteEquation(1) as: (lo) Then, 
by substituting Equation (7) into (10), and using (8) and (9) we obtain: lim T(P)= 2~I(hear A) = 2~ = 
T(Q), (11) M+. which is the result we were seeldng. +  
			