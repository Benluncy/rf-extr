
 The Distributed Virtual Windtunnel Steve Brysont and Michael Gerald-Yamasakitf Applied Research Branch, 
Numerical Aerodynamics Simulation Division NASA Ames Research Center, MS T045-1, Moffett Field, Ca. 94035 
-.- -. --- t Employee of Computer Sciences Corporation. Work supported under government contract NAS 
2-12961 tt Employee of NASA Abstract An implementation of a distributed virtual environment for the 
shared interactive visualization of large unsteady three-dimensional flowfields is described. Computation 
of the visualizations is performed on a Convex C3240 computer, and the visualization data is transferred 
over a high-speed network to a Silicon Graphics Iris workstation for rendering. A boom­mounted six degree 
offreedom head-position-sensitive stereo CRT system is used for display. A hand position sensitive glove 
controller is used for controlling various tracers (e.g. smoke ) for the visualization of the jlow. User 
commands are sent to the Convex, which interprets these commands and computes the corresponding visualization. 
With this architecture, several users may share and cooperatively control the visualization generated 
by the Convex. The distributed architecture is also interesting to those using conventional screen and 
mouse interfaces. This work extends the work of Bryson and Creon Levit [11 in the development of a virtual 
environment for the visualization ofjluidflow to large data sets. 1: Introduction In a previous work 
[1], a virtual environment for the visualization of three-dimensional unsteady fluid flows was described. 
This system uses virtual environment interaction techniques, that allow fully three-dimensional display 
and control of computer generated graphics, to explore the global structure of pre-computed unsteady 
simulated flow fields. All control, computation and rendering in this previous work took place on a multi-processor 
high performance graphics workstation. In this environment single grid unsteady data sets were studied. 
Due to performance requirements, these data sets were limited to fewer than 240 megabytes in size. Interesting 
unsteady data sets are, however, considerably larger than this, often in the tens of gigabytes range. 
Visualization of these larger data sets using virtual environment techniques is not practicable on current 
high-performance workstations. This paper describes a distributed architecture for a virtual environment 
system designed to visualize these larger data sets. This architecture also supports shared, cooperative 
visualization of flow data by two or more researchers using virtual environment displays at separate 
locations In the remainder of this section, the problem of visualizing unsteady flowfields is described. 
Section 2 describes the visualization techniques used in the virtual windtunnel [1]. Section 3 describes 
the virtual environment hardware used for control and display. Section 4 describes Distributed Library 
[3] which is used for the communications between the graphics workstation and the host supercomputer. 
Section 5 describes the design of the distributed virtual windtunnel. Section 6 describes current performance. 
Further work and conclusions are described in section 7, 1.1 Unsteady Fluid Flow Visualization Unless 
otherwise noted, aflowfield, is a numerical solution to a three-dimensional computational fluid dynamics 
(CFD) simulation, and in particular, the time­dependent velocity vector field part of that solution. 
Complicated geometrical and topological situations abound in such fields. Methods of visualizing these 
phenomena inspired by the visualization of real flows in real wind (or water) tunnels [4] can be used 
for simulated flow. Examples include smoke injection, dye advection, and time exposure photographs. The 
flowfields considered in this paper are pre-com­puted solutions of the time-accurate Navier-Stokes equa­tions 
of fluid motion. These unsteady flowfields are represented as a sequence of successive three­dimensional 
velocity vector fields. Each of these velocity vector fields is considered as a timestep. 1063-9535/92 
$3.0001992 IEEE Modern engineering data sets can be quite large. One example is the Harrier Jump Jet 
[2], which contains 100 timesteps each with 2.8 million points of data on 18 overlapping grids. The velocity 
vector field part of the this data is about 3.2 megabytes of data per timestep. This data set has been 
chosen as the test data set for the virtual windtunnel. 1.2 Virtual Environments Virtual environments 
[5] are a new approach to user interfaces in computer software. This approach involves integrating a 
variety of input and display devices to give the user the illusion of being immersed in an interactive 
computer generated environment (figure 1). The computer generated scene is displayed in stereo to create 
the illusion of depth, and is rendered from a point of view that tracks the user s head. The user also 
has an input device, typically an instrumented glove, which enables direct manipulation of objects in 
the computer generated environment. Virtual environments provide a useful interface for analyzing unsteady 
fluid flow phenomena which involve complex three-dimensional structure. By providing the user with the 
illusion that the elements of the computer generated environment are real, interactive objects, the user 
becomes more directly involved in the investigation of the phenomena in that environment. By providing 
true three-dimensional control over objects in the environment, they can be directly manipulated to perform 
as the user desires. By providing head-tracked stereoscopic wide field of view displays, the three­dimensional 
structure of virtual objects can be unambiguously perceived. The task of providing an illusion of reality 
places severe demands upon the entire computer system used to generate the virtual environment. Informal 
studies have shown that to sustain the illusion, the system must repeatedly react to the user s commands 
and display the virtual scene in stereo to the user in less than l/8th of a second. Slower performance 
destroys the illusion, removing essentially all of the advantages of virtual environments. Thus the input 
of the user commands, including user head position, access to the data that is being visualized, computation 
of the visualizations on that data, and rendering of those visualizations from the user s point of view 
must all occur in less than l/8th of a second. Faster performance is highly desirable, though a tradeoff 
must be made between a rich environment and frame rate. Ten frames/second will be taken as the desired 
frame rate. These performance demands place strong constraints on the design of a system. The flow visualization 
techniques that can be used in a virtual environment are limited to those that can be computed in the 
time allowed. For example, interactive streamlines of a flow computed with fast integration methods can 
be used, but high-resolution interactive isosurfaces, which require computationally intensive algorithms 
such as marching cubes, can not. The computer platform used in the virtual environment system must be 
sufficiently powerful and respond with sufficient speed. In a distributed implementation, there must 
be enough network bandwidth to deliver the data moving in both directions in the time required. The remote 
machine must also be available for very fast response, and so must not have a heavy user load. The satisfaction 
of these constraints is the primary consideration in the design of the system described in this paper. 
Both streamline calculation and flowfield data 1/0 must be accomplished in less than 1/8 of a second 
to sustain sufficient frame rates for the virtual environment. 2: The Virtual Windtunnel The stand-alone 
virtual windtunnel [1] has demonstrated that virtual environment techniques are useful in visualizing 
complex fluid flows. The stereo head-tracked display is a very effective way of displaying the complex 
three-dimensional features of a fluid flow. Input via an instrumented glove is a useful and intuitive 
way to position the various flow visualization tools. The idea is to create the illusion that the user 
is actually in the flow manipulating the visualization tools (figure 1). Unlike someone in a real flow 
field, however, the user s presence in no way disturbs the flow. Thus, sensitive areas of flow, such 
as boundary layers and chaotic regions, can be investigated easily. The flow can be investigated at any 
length scale, and with control over time. The time evolution of the flow can be sped up, slowed down, 
run backwards, or stopped completely for detailed examination. The stand-alone virtual windtunnel is 
based on a Figure 1: The virtual environment in use. The Silicon Graphics Iris 380GT VGX system. This 
is a researcher is visualizing the flow around a harrier jump jet [2], using streamlines and streaklines. 
 (For color plate see page 842) The performance of the machine is rated at approximately 200 MIPS and 
37 megaflops. Our sys­tem has 256 MBytes of memory. The VGX has a drawing speed rating of about 800,000 
triangles/second. The display device for this system is a boom­mounted six degree of freedom head-position-sensitive 
stereo CRT system. The control device is an instrumented glove which provides the position and orientation 
of the user s hand as well as the degree of bend of the user s fingers. These devices are the same as 
those used in the distributed virtual windtunnel, and are described in more detail in section 3. 2.1 
Visualization Tools The tools considered in this paper for visualizing unsteady velocity vector fields 
are inspired by classical techniques used in real wind and water tunnels. Currently, the visualization 
methods, or tools, that we have implemented are streaklines, particle paths, and streamlines. A streamline 
is formally defined as the lWUS of in­finitesimal fluid elements that have previously passed through 
a given fixed point in space [4]. Streaklines are analogous to smoke or collections of bubbles. A particle 
path is formally defined as the locus of points occupied over time by a given single, infinitesimal fluid 
element [4]. This corresponds to a time exposure photograph of the motion of a single small particle 
injected into the flow. A streamline is formally defined as the integral curve of the instantaneous velccity 
vector field that passes through a given point in space at a given time [4]. They provide direct insight 
into the instantaneous geometry of the flowfield. All of these techniques involve injecting virtual particles 
into the flow. We call the point of injection for a particular tool the seed point for that tool. Each 
of the techniques described above are computed by selecting a set of initial positions and integrating 
the vector field to compute a final set of positions. The difference between the visualization techniques 
described above is the order in which the integrations are performed. The streaklines take as input the 
current positions of all the particles, including those recently added at the seed points, All of the 
particles are moved by integrating each one once using the data in the current time step. The particles 
may be rendered as individual points or connected in a way to simulate smoke. Particle paths take as 
input the seed point(s) and interactively integrate the particle position, incrementing the timestep 
with each integration. This results in an array of positions which is displayed as the particle path. 
Streamlines take as input the seed points and interactively integrate the particle position without incrementing 
the current timestep. This results in an array of positions which is displayed as the streamline. In 
the case of streamlines and particle paths, it is the paths that are of interest, not the positions of 
individual points in the path. The researcher uses these tools to explore the flow field by moving the 
seedpoint and observing the path from that seedpoint. Thus the virtual environment system must be capable 
of computing the entire path in a single frame time. The usefulness of these tools are greatly enhanced 
when several paths can be computed within the required time, Control over the seed points for all of 
the above tools are provided by lines of seed points called rakes. Rakes may be manipulated with the 
glove through finger gestures and hand motion. These rkes are grabbed at one of three points: center 
for rigid translation of the rake, or at either end for movement of that end of the rake. In this way 
rakes may be oriented in an arbitrary manner. Several rakes may be defined simultaneously. The type and 
number of seedpoints in a particular rake is determined by the user. It has been found useful to use 
rakes of several different types in combination when studying a flow. The integrations described above 
are made more complicated by the fact that the fluid flow data are provided on curvilinear grids, which 
contain the physical position of each grid point and the velocity vector at that point. If the position 
of a particle is known in physicat space, a search of the curvilinear grid must be performed to locate 
the grid coordinates nearest that point and obtain the corresponding velocity vector data. This search 
involves unacceptable performance overhead. It is avoided in the virtual windtunnel by converting the 
velocity data to grid coordinates and performing all integrations in grid coordinates. The resulting 
paths are easily converted to physical coordinates by using their known grid coordinates to directly 
lookup their corresponding physical coordinates, using trilinear interpolation if necessary. The computation 
of the visualization tools involves integrating particles throughout the flow field. Thus the data of 
the flowfield must be quickly accessible for the integration to be performed at the speeds required. 
Construction of particle paths in particular requires the entire data set for all timesteps, as the particle 
paths may extend throughout the entire data set. In the virtual windtunnel the flow data is placed in 
physical memory so that it can be randomly accessed with little time penalty. As the physical memory 
of our workstation contains 256 gnegabytes of memory, the data sets that can be examined with the stand-alone 
virtual windtunnel are limited to about 250 megabytes in size. As described in section 1, this is a severe 
constraint and prevents us from examining many interesting flows. One of the primary motivations for 
the distributed virtual windtunnel is the ability to access the larger physical memories of remote supercomputers. 
3: The Virtual Environment Interface The virtual environment interface provides a natural three-dimensional 
environment for both display and control of rakes. This interface allows intuitive exploration of rich, 
complex geometries. It is very similar to the interface used in the stand-alone virtual windtunnel [1]. 
The basic components of the environment are a high-performance graphics workstation for computation and 
rendering, a head­ tracked stereoscopic wide-field BOOM2CW for display, and a VPL Dataglove Model II 
for control (figure 2). The display for our virtual environment is the BOOM2Cm, manufactured by Fake 
Space Labs of Menlo Park CA., and fashioned after the prototype developed earlier by Sterling Software, 
Inc. at the VIEW lab at NASA Ames Research Center [6]. The BOOM2Cm uses two 1000x1OOO pixel CRTs viewed 
through wide-field optics for stereoscopic display. The CRTs provide superior image quality in terms 
of brightness, contrast, and resolution than conventional head-mounted LCD-based displays. Two field-sequential 
color channels are provided via tinted LCD shutters. The weight of the CRTs are borne by a counterweighted 
yoke assembly with six joints, which are designed to allow easy movement of the head with six degrees 
of freedom within a limited range. Optical encoders on the joints of the yoke assembly are continuously 
read by the host computer providing six angles of the joints of the yoke. These angles are converted 
into a standad 4x4 position and orientation matrix for the position and orientation of the BOOM2CW head 
by six successive translations and rotations. By inverting this position and orientation matrix and concatenating 
it with the graphics transformation matrix stack, the computer generated scene is rendered from the user 
s point of view. As the user moves, that point of view changes in real-time, providing a strong illusion 
that the user is viewing an actual three-dimensional environment. For user control in our virtual environment, 
the user s hand position, orientation, and finger joint angles are sensed using a VPL datagloveTM model 
II, which Figure 2: The boom and glove interface used in the virtual windtunnel. incorporates a Polhemus 
3SpaceTM tracker. I ne Polhemus tracker gives the absolute position and orientation of the glove relative 
to a source by sensing multiplexed orthogonal electromagnetic fields. The degree of bend of knuckle and 
middle joints of the fingers and thumb of the user s hand are measured by the VPL DatagloveTM model II 
using specially treated optical fibers. These finger joint angles are combined and interpreted as gestures, 
The glove requires recalibmtion for each user, and the polhemus tracker has limited accuracy and is sensitive 
to the ambient electromagnetic environment. The dataglove works satisfactorily with user training and 
within a limited range. The local computation and rendering for our virtual environment is provided by 
the same Silicon Graphics Iris described in section 2. 4: Distributed Library The main vehicle for distributing 
computation between supercomputers and workstations in the distributed virtual windtunnel is Distributed 
Library (dlib) [3]. The workstation acts as a client and the supercomputer acts as the server. Like many 
systems which provide for distributed processing, dlib is a high level interface to network services 
based on the remote procedure call (RPC) model [7] [8] [9] [10]. However, unlike most of these systems, 
dlib was developed to provide a service which allows for a conversation of arbitrary length within a 
single context between client and server. The dlib server process is designed to be capable of storing 
state information which persists from call to call, as well as allocating memory for data storage and 
manipulation. While RPC protocols are frequently likened to local procedure calls without side effects, 
dlib more closely resembles the extension of the process environment to include the server process. The 
use of dlib is much like developing a library of routines, say, an 1/0 library, on a local system. Application 
code is linked to routines in an 1/0 library. The 1/0 library contains simple routines which give access 
to the 1/0 devices controlled by the operating system device drivers. The 1/0 device drivers in turn 
control the somewhat more complicated exchange of data with external devices. To execute a routine on 
a remote host, all the information necessary to execute the routine in the remote environment must be 
transmitted over the network to a remote server process. After execution of the routine is invoked, results 
of the execution must also be transmitted back to the local client process. Dlib provides utilities to 
automatically create the code which uerforms the network transactions required to invoke-and execute 
the routine in the remote environment and exchange information between the client and server processes. 
 (For color plate see page 842) Due to the persistent nature of the remote environment, dlib is able 
to coordinate allocation and use of remote memory segments and provide access to remote system utilities. 
The application, through dlib, can link to the remote system s 1/0 library, for example, to utilize the 
remote system s 1/0 devices. (figure 3). The illustrated client process can utilize the virtual reality 
niterface via the local 1/0 library and operating system. The client process can also utilize the remote 
disk via dlib which communicates to a remote server process. The remote server process has access to 
the remote disk via the remote 1/0 library. The distributed virtual windtunnel uses dlib to allow the 
workstation clients to gain access to the supercomputer s processing power, large memory, large disk 
storage capacity, and fast disk access. Dlib was originally designed on a model of one client to one 
server. To allow multiple clients to share the server process environment, the dlib server was modified 
to accept more than one connection. Each connection is selected for service by the server process in 
the sequence that the dlib calls are received. The dlib calls are executed by the server in a single 
process environment as though there were only one client. 5: Implementation of the Distributed hand uosition 
and gesture yr 1/0 Device Drivers ................................ *   m!2&#38; iv : dlib . . . . 
. . . . . . . . . . . . . . . . . . . . . .l Client Processes Workstation Figure 3: Access to remote 
1/0 devices using Virtual Windtunnel 5.1 Design Considerations The performance constraints discussed 
in section 1 determine many aspects of a distributed virtual windtunnel implementation. In choosing a 
remote computer system, computational speed, accessibility, disk bandwidth, and compatibility with the 
stand-alone system must all be considered. Further design choices are motivated by available high-speed 
networks and the local workstations. What information is sent across networks is designed so that several 
workstations and virtual environment systems can access the same data on the host system, The format 
of the information is driven by the demand that as little data is transferred over the networks as possible. 
Finally the software architecture should be designed so that the individual computer systems perform 
the tasks that they are best suited for, and there are as few bottlenecks as possible. The choice of 
the Convex C3240 system is motivated by several considerations. The primary consideration is the availability 
of this system, which is dedicated to visualization of large data sets in the NAS project at NASA Ames. 
While the NAS project also has two Cray supercomputers, they are heavily used, so m Network R  1/0 
Device Drivers ........................... ,. Kernel Entry Points A I . 0 1/0 Library  KL!i!i 1------..............-. 
 a ~ Client Processes  dlib and remote 1/0 Library in the distributed virtual w~dtunnel. The solid lines 
represent actual d~ta paths, while the stippled line is the effective data path from the client processes 
to the remote disk. #of particles #of bytes transferred Required bandwidth for 10 fps (Mbytes/see) 10,000 
120,000 1.144 50,000 600,000 5.722 100,000 1,200,000 9.537 Table 1: Network constraints for transferring 
the three-dimensional positions of particles over a network at ten frameskecond interactive response 
time cannot be guaranteed. Dedicated time on the Convex C3240 has been made available for this project. 
The Convex C3240 has four vector processors. These processors can process vector arrays of up to 128 
entries in length. The high performance of the Convex relies on the vectorization of code, which will 
be discussed in section 5.3. Our Convex has one gigabyte of physical memory and 100 gigabytes of disk 
storage. The disk bandwidth has been measured to be between 30 and 50 megabytes/second sustained rate, 
depending on the size of the file being read. Two workstations are used for the local computation and 
rendering of the visualizations. This allows the implementation of a shared virtual windtunnel. The workstations 
are the Silicon Graphics (SGI) Iris 380GT VGX workstation described above and the Silicon Graphics Iris 
440GT Skywriter. The skywriter has four 40 MHz MIPS R3000 CPUS with R301O floating point chips and two 
independent VGXT graphics rendering pipelines. These workstations have been chosen because they are available 
and used in stand-alone virtual windtunnel development. For our current implementation of the distributed 
virtual windtunnel, a workstation with two processors and a high-performance graphics pipeline would 
suffice. The UltraNet high-speed network was chosen for the communication between the workstations and 
the remote system. This network is rated at 100 megabytes/second, but the UltraNet VME interface to the 
SGI workstation limits the bandwidth to 13 megabytes/second. This rate should be sufficient for most 
visualizations. The specification of the data that is returned by the remote system is determined by 
the requirements of minimat data transfer and that each computer do what it point vectors in three dimensions. 
The workstation receives these arrays and renders them from the point of view determined by that workstation 
s virtual environment interface. This requires the transfer of 12 bytes per point in each array. Experience 
has shown that typical visualization scenarios involve tens of thousands of particles implying the transfer 
of several hundred thousand bytes of data per timestep. One might consider projecting these vectors on 
the screen, computing their screen coordinates on the remote system and sending this data to the workstation. 
This would reduce the transfer to eight bytes/point. In the virtual environment scenario, however, stereoscopic 
display requires two projections per point which implies 16 bytes/point. Thus sending the three-dimensional 
position of the points with 12 bytes/point in the arrays is optimal. The network transfer rates required 
for an update rate of ten frames/second neglecting processing and rendering overhead are summarized in 
table 1. does best. The remote system computes the visualization tools as described in section 2, and 
sends the resulting paths out the network as arrays of floating In the specification of what information 
is sent to the remote system, the desire for a shared environment capability was the primary consideration. 
Ideally, at any time during the use of the distributed virtual windtunnel another workstation with a 
virtual environment interface should be able to sign up and interact with the already existing virtual 
environment. This requires that control over all objects in the virtual environment take place on the 
remote system. Thus the information that is sent to the remote system are those user commands which effect 
the virtual environment. These include hand position, hand gestures, keybom-d and mouse commands, and 
any other control data that may be implemented. In the shared scenario, the position of the users heads 
would also be sent so that are limited only by the disk storage space, There are they may be displayed 
as part of the virtual many interesting data sets which are still larger than environment, indicating 
to participants in the this, however. An example is the hovering Harrier data environment where everyone 
is. set computed at NASA Ames, whose flow velocity data I #of points in grid I #of bytes in a timesteu 
I #of timestem. that fit I requiral disk bandwidth I in a gigabyte (Mbytes/see) 131,072 (tapered cyl.) 
1,572,864 682 15 436,906 (current max) 5,242,880 204 50 1,000,000 12,000,000 89 114.4 3,000,000 36,000,000 
29 343.32 10,000,000 360,000,000 2 (actually 2.9) 3,433.2 Table 2: Disk bandwidth constraints In this 
design, the information about the virtual has about 36 megabytes per timestep. Visualization of control 
devices such as rakes must also be sent from the this data set will require a disk bandwidth of about 
600 remote system to the workstation so that the current megabytes per second. Thus we are still a long 
way state of these devices may be correctly rendered, This from interactively visualizing very large 
unsteady data adds a small overhead to the transfer of the visualization sets in a virtual environment. 
The disk bandwidth data described above, but this is typically minor bottlenecks for an update rate of 
ten frames/see are compared to the visualization data itself. summarized for velocity grids in table 
2. Because the computation of the environment state Loading timesteps from disk as described above is 
performed by a single machine, possible conflicting precludes the computation of particle paths of arbitrary 
commands from different workstations are easily length in real time, as they require a different timestep 
handled. As described in section 4, the dlib process on for every point in the path. All the timesteps 
required the remote system that handles the network traffic takes for the computation of a particle path 
must be resident the input from various workstations in serial order. in memory. Thus the number of timesteps 
that can fit This allows conflicts to be resolved by a first come in physical memory places a limit on 
the length of the first served rule. For example, if two users grab the particles paths. The timestep 
that would be loaded into same rake, the user who grabbed it first gets control of memory in this case 
would be the current timestep plus that rake and the second user is locked out of interaction the maximum 
particle path length. with that rake until the first user lets the rake go. Other Another approach to 
the large data problem is to rakes are unaffected by this locking, so the second user choose a spatial 
subset of the data, and to fill the can interact with them. Other control conflicts can be physical memory 
with as many timesteps of this subset similarly resolved. as possible. As in many data sets interesting 
unsteady The problem of large data sets can be handled in a behavior is within a relatively small volume, 
subsetting variety of ways. With one gigabyte of physical can potentially prove quite useful in solving 
the data memory, data sets can be loaded into memory that are size problem. Subsetting capability is 
being developed four times as large as in the stand-alone virtual by Gerald-Yamasaki and will be incorporated 
into the windtunnel case. This allows for the visualization of virtual windtunnel. interesting data sets 
not available to the stand-alone A final consideration is the desire that the virtual windtunnel. Having 
the entire data set resident development of the stand-alone and distributed virtual in memory is the 
easiest method of managing the data. windtunnels be as close as possible. In this way When the data sets 
are larger than physical memory, development of the visualization tools and environment however, the 
data must reside on a mass storage device, can occur on both systems in parallel. In the current usually 
disk. The Convex C3240 with its disk 1/0 implementation both the distributed and stand-alone bandwidth 
of 30 megabytes/second can load datasets of versions are compiled from the same source code using up 
to about three and a quarter megabytes in l/8th of a DEFINE statements where differences in architecture 
second. Thus datasets of whose timesteps are this size occur. This is possible because of the compatibility 
of If the entire data set is not in physical memory Processor 1 Remaining processors Processor n From 
Y ELoad next timestep> workstations To workstations Handle network traffic <buffers for incoming> L commands 
J I Update environment tools and compute visualizations using current timestep data into B available 
buffer buffer buffer 1 2 . <buffers for outgoin~ + Place data in buffer .· data ~ buffer n . Figure 
4: The software architecture of the remote system. The data flow on the left side of the figure is over 
the Ultra network. The interprocess communication is via shared memory. The rightmost process is present 
only when the data set is not stored in physical memory. 281 Processor 1 Processor 2 From remote system 
Read input devices and handle network traffic Render environment visualization tools and from To remote 
system ~ current point of view as determined by head tracker uffers for outgoing command> Figure 5: 
The software architecture of the workstation. The data flow on the left side of the figure is over the 
Ultra network. The interprocess communication is via shared memory. the Convex C3240 and SGI internal 
architectures, tied to the network and remote computation specifically the sharing of the IEEE floating 
point performance, so the head-tracked display of the virtual number format (a compile time option on 
the Convex). environment can run at very high rates. Note that even though the head-tracked display is 
updating at very high 5.2 System Architecture rates, the entire computation cycle is still required 
to take place in less than l/8th of a second due to the user In the design motivated by the above interaction 
with the visualization tools. considerations, each workstation reads its input devices For the shared 
architecture, another virtual and sends their commands to the remote system. The environment system is 
attached to the high-speed remote system updates the virtual environment network (figure 6). The architecture 
on the second including if necessary loading the data for the current workstation is the same as above. 
The remote system timestep, computes the current visualizations, and sends a second copy of the visualization 
data to the transfers the environment state back to the second display system. Only the overhead associated 
workstations. Each workstation renders this state to its with the second transfer of data is added to 
the overall virtual environment display device. Many of these performance of the system. tasks can be 
performed in parallel. On the remote system, computation of the 5.3 Optimization visualizations can occur 
while the data from the previous computation is sent to the network (figure 4). To take advantage of 
the computational power of If the timesteps are being loaded from disk, that loading the Convex, considerable 
code optimization is required. can also occur in parallel. The timestep required for the The main body 
of the computational code must be next computation is loaded into a buffer. This vectorized so that the 
full power of the vector registers parallelization will impact the time required for the can be used. 
The integration algorithm for the computation so a careful study must be performed to computation is 
second-order Runge-Kutta, which determine the optimal balance of tasks. requires two accesses of the 
vector field data from On the workstation, at least two processors are memory each involving eight floating 
point loads to set desirable so the rendering of the graphics and the up for trilinear interpolation, 
two trilinear handling of the network traffic can be run in parallel interpolations, and two simple computations 
per (figure 5). In this way the graphics performance is not component per point integrated. To convert 
the current Remote system (data, computations) E ; Workstation fgraphics,I/0) + -:a High-speed Network 
Workstation (graphics,I/0) Figure 6: The architecture of the shared virtual windtunnel. 282 Time to 
compute 10,000 particles maximum # of particles # of streamlines w/ 100 particles 0.25 seconds 4,000 
40 0.19 seconds 5,200 52 0.13 seconds 7,600 76 0.10 seconds 10,000 100 0.05 seconds 20,000 200 Table 
3: implications of benchmark performance for a frame rate of ten frames/second, assuming no other overhead 
point s location from grid coordinates to physical coordinates, another 8 floating point loads plus a 
trilinear interpolation are required per component per point. This algorithm described above has been 
vectorized and parallelized across the four processors of the Convex. The parallelization is across groups 
of streamlines, and the vectorization is across streamlines within a group. This vectorization works, 
however, only for data sets which use single structured computational grids. Many modern data sets use 
multiple overlapping structured grids, however. The virtual windtunnel has been extended to multiple-grid 
data sets. This extension requires checking to see if a particle has fallen off of one grid onto another 
at every stage of integration, and a search of the new grid if a transition have taken place. The determination 
of grid transition is facilitated by the use of lookup tables which encode the overlap information of 
the grids. The use of the lookup table and the grid search is a grid transition takes place makes vectorization 
much more difficult. At this date, multiple grid integration has not been vectorized on the Convex, though 
it still runs in parallel. To evaluate the computational performance, a benchmark computation of 100 
streamlines each containing 100 points was performed (figure 7). This scenario contains 10,000 points 
with a transfer over the networks of 120,000 bytes of data. The code as originally written for the stand-alone 
vktual windtunnel Figure 7: 100 streamlines each 100 points long in the harrier data set used as a benchmark. 
uses optimized scalar C techniques such as pointer manipulation and striding. In the single-grid case, 
this code successfully vectorizes and parallelizes across the four processors of the Convex 440 by distributing 
the streamlines among the processors. In this case the benchmark computation can be performed in about 
0.10 seconds on the Convex. This is comparable to the performance of the stand­alone virtual windtunnel 
distributing the streamlines across 8 processors, which performed the same computation in 0.10 to 0.11 
seconds. In the multiple grid case, the code is not vectorized on the Convex. It is parallelized over 
streamlines. This benchmark was run in the harrier data set, which has 18 grids for a total of 2.8 million 
points per timestep. In the positions tested, the streamlines would have performed about five to ten 
grid transitions. The computation in this case takes about 1.2 seconds when the seedpoints of the streamlines 
are stationary to 2.1 seconds when the seedpoints are moving. The additional time in the latter case 
can be attributed to the time required to determine the new grid and grid location of each of the 100 
seedpoints as the controlling rake is moved. The workstation performed the same computations in 0.5 seconds, 
in the stationary case, and 0.7 seconds in the moving case. Thus vectorization of the computations on 
the Convex is required. The further optimization of the computation is under study. Generally, the speed 
of the computation places a limit on particle number. Examples of this constraint for various performance 
parameters are shown in table 3 for a frame rate of ten frames/second, assuming that the performance 
scales with the number of particles. 6: Conclusions This paper describes an initial implementation of 
an interactive distributed virtual environment. This implementation has been successful in several ways. 
The feasibility of real-time virtual environment interaction over a high-speed network involving transfers 
of hundreds of kilobytes of data has been demonstrated. Using the large memory of remote supercomputers, 
larger data sets have been visualized in a virtual environment than has been previously possible. An 
architecture which supports data sets read (For color plafe seepage 842) dynamically from disk and multiple 
virtual environment users has been designed. These successes suggest that distributed computation should 
be persued for the visualization of very large unsteady data sets. It is possible that with more work 
the computational power of the Convex C3240 will provide much higher performance virtual environments. 
The most significant constraint on the number of particles that can be used for visualization is the 
computation time. Optimization of computations and higher performance hardware are required to solve 
this constraint. Further work includes optimization of the computations, shared environment interactions, 
and development of greater user control over the virtual environment. Finally, the usefulness of virtual 
environments in the visualization of fluid flow must be formally studied. While the Convex cannot provide 
the disk bandwidth required for very large data sets, the methods developed in this project will extend 
to those machines in the future which can provide the required performance. These methods are useful 
in contexts other than virtual environments, such as the visualization of unsteady flows in the conventional 
screen and mouse environment. 6: Acknowledgements Much thanks and credit goes to Creon Levit for fruitful 
design discussions and planting many of the seeds that lead to this paper. Thanks also to Tom Woodrow 
and David Lane for assisting with the distribution to the Convex. Tom Lasinski is appreciated for his 
leadership and support of this project. Thanks to Jeff Hultquist and Tom Lasinski for helpful suggestions 
on early versions of this paper. Finally, thanks to the NAS division at NASA Ames for general support 
of this work. References [1] S. Bryson and C. Levit, The Virtual Windtunnel: An Environment for the Exploration 
of Three-Dimensional Unsteady Fluid Flows , Proceedings of IEEE Visualization 91, San Diego, Ca. 1991, 
Computer Graphics and Applications July 1992 [2] M. Smith, K. Chawla, and W. Van Dalsem, Numerical Simulation 
of a Complete STOVL Aircraft in Ground Effect , paper AIAA-91­3293, American Institute of Aeronautics 
9th Aerodynamics Conference, Baltimore (1991). [3] Yamasaki, M. Distributed library. NAS Applied Research 
Technical Report RNR-90­008 (Apr. 1990). [4] W.J. Yang (editor), Handbook of Flow Visualization, Hemisphere 
Pub., New York (1989) [5] Fisher, S. et. al., Virtual Environment Interface Workstations, Proceedings 
of the Human Factors Society 32nd Annual Meeting, Anaheim, Ca. 1988 [6] I.E. McDowall, M. Bolas, S. Pieper, 
S.S. Fisher and J. Humphries, Implementation and Integration of a Counterbalanced CRT-Based Stereoscopic 
Display for Interactive Viewpoint Control in Virtual Environment Application , in Proc. SPIE Conf, on 
Stereoscopic Displays and Applications, J. Merrit and Scott Fisher, eds. (1990) [7] Birrell, A. D., and 
Nelson, B. J. Implementing remote procedure calls. ACM Trans. on Comp. Sys. 2, 1 (Jan. 1984), 39-59. 
[81 Dineen, T. H., Leach, P. J., Mishkin, N. W., Pato, J. N., and Wyatt, G. L. The network computing 
architecture and system: an environment for developing distributed applications. Proceedings of Summer 
Usenix (June 1987), 385-398. [9] Sun Microsystems. Request for Comment #1057 Network Working Group (June, 
1988). [10] Xerox Corporation. Courier: the remote procedure call protocol. Xerox System Integration 
Standard (XSIS) 038112, (Dec. 1981).    
			