
 The EMOTE Model for Effort and Shape Diane Chi, Monica Costa, Liwei Zhao, Norman Badler Center for Human 
Modeling and Simulation University of Pennsylvania {chi | monicac | lwzhao | badler}@graphics.cis.upenn.edu 
 ABSTRACT Human movements include limb gestures and postural attitude. Although many computer animation 
researchers have studied these classes of movements, procedurally generated movements still lack naturalness. 
We argue that looking only at the psychological notion of gesture is insufficient to capture movement 
qualities needed by animated characters. We advocate that the domain of movement observation science, 
specifically Laban Movement Analysis (LMA) and its Effort and Shape components, provides us with valuable 
parameters for the form and execution of qualitative aspects of movements. Inspired by some tenets shared 
among LMA proponents, we also point out that Effort and Shape phrasing across movements and the engagement 
of the whole body are essential aspects to be considered in the search for naturalness in procedurally 
generated gestures. Finally, we present EMOTE (Expressive MOTion Engine), a 3D character animation system 
that applies Effort and Shape qualities to independently defined underlying movements and thereby generates 
more natural synthetic gestures.  Keywords Animation systems, human body simulation, gestures, procedural 
modeling, expression 1. INTRODUCTION Human movement ranges from voluntary, goal-oriented movements to 
involuntary, subconscious movements. Voluntary movements include task-driven actions, such as walking 
to get somewhere or speaking. Involuntary movements occur for physiological or biological purposes; for 
instance, blinking, balancing, and breathing. A wide class of movement falls in between these two. In 
general, this class is characterized as consisting of movements which occur in concert and perhaps unconsciously 
with other activities. We note two interesting subclasses of this class of movements. One subclass consists 
of low-level motor controls that assist the accomplishment of a larger coordinated task. For instance, 
unconscious finger controls form grasps, leg and foot coordination enable walking or running, and lip 
movements generate speech. Another interesting subclass is the set of movements that accompany communicative 
acts: facial expressions, limb gestures, and postural attitude. While computer animation researchers 
have actively studied all these classes of human movements, it remains difficult to procedurally generate 
convincing, natural limb and postural movements . We pose the problem as follows: What parameters characterize 
body or limb motions in real people performing communicative acts? The foremost computational approach 
to this issue has been through the gesture models proposed by McNeil [27], and elaborated with computer 
implementations primarily by groups led by Cassell [14,39,13], Badler [2,3], and Thalmann [8,12]. McNeil 
s approach is to characterize communicative arm gestures into several categories: Iconics represent 
some feature of the subject matter, such as the shape or spatial extent of an object.  Metaphorics represent 
an abstract feature of the subject matter, such as exchange, emergence, or use.  Deictics indicate a 
point in space that may refer to people or spatializable things.  Beats are hand movements that occur 
with accented spoken words and speaker turn-taking.  Emblems are stereotypical patterns with understood 
semantics, such as a good-bye wave, the OK-sign, or thumbs-up.  Such an approach has served to make 
conversational characters appear to gesture more-or-less appropriately while they speak and interact 
with each other or actual people. The impression that one gets when watching even the most recent efforts 
in making convincing conversational characters is that the synthetic movements still lack some qualities 
that make them look right . Indeed, the characters seem to be doing the right things, but with a kind 
of robotic awkwardness that quickly marks the performance as synthetic. It is not a computer animation 
problem per se . conventional but skilled key-pose animators are able to produce excellent gestures in 
3D characters. So there is some gap between what such an animator intuits in a character (and is therefore 
able to animate) and what happens in a procedurally synthesized movement. Key pose animators have managed 
to bridge the technology gap by careful application of classic rules for conventional animation [35,25]. 
The McNeil/Cassell approach to gesture is rooted in psychology and experimental procedures that use human 
observers to manually note and characterize a subject s gestures during a story-telling or conversational 
situation. The difficulty in this approach is hidden within the decision to call something a gesture. 
That is, the observer notes the occurrence of a gesture and then records its type. This kind of recording 
fails to capture the parameters of movement that makes one particular gesture appear over another, as 
well as what makes the gesture appear at all. This issue is crucial in the studies of Kendon [19], who 
tries to understand the deeper question: What makes a movement a gesture or not? In his work, a gesture 
is a particular act that appears in the arms or body during discourse. There may be movements that are 
not gestures and there may be movements that are perceived as gestures in some cultures but not in others. 
So clearly, the notion of gesture as a driver for computer­generated characters cannot be - in itself 
- the primary motivator of natural movements. Further, we note that these approaches are limited by their 
basis in linguistics. To address this, we look toward movement representations outside the constraints 
of communicative acts. We find that the Effort and Shape components of Laban Movement Analysis (LMA) 
[22,23,17,7,28] provide us with a more comprehensive set of parameters for describing the form and execution 
of the qualitative aspects of movements. Our approach to gesture augments the McNeil/Cassell approach 
by addressing a missing dimension: movement exists not just because it has underlying linguistic relationships 
but also because it has some distinctiveness in its Effort and Shape parameters. Effort and Shape provide 
a means to describe the aspect of human movement that relates to individual character, cultural norms 
and distinctions. Our approach meshes perfectly with the perspective offered by the LMA proponents: Gesture 
... is any movement of any body part in which Effort or Shape elements or combinations can be observed 
[7]. Our approach to gesture also complies with two other important LMA concepts. The first one is synthesized 
by Bartenieff when she observes that it is not just the main movement actions that let us identify behavior 
but it is the sequence and phrasing of Effort and Shape components that express and reinforce content 
[7]. The other concept is best expressed by Lamb: a gesture localized in the limbs alone lacks impact, 
but when its Effort and Shape characteristics spread to the whole body, a person appears to project full 
involvement, conviction, and sincerity [24]. We present EMOTE (Expressive MOTion Engine), a 3D character 
animation system that allows the specification of Effort and Shape parameters to modify independently 
defined arm and torso movements. The underlying movements of the arms and torso are specified through 
key time and pose information much like conventional computer animation. However, rather than performing 
a simple linear interpolation, we apply Effort and Shape parameters to these motion templates to create 
expressive movements. Our approach allows users to specify separate parameter values for different body 
parts, as well as phrasing parameter values across the key poses. We note that the key pose values may 
be generated synthetically, by inverse kinematics, motion capture, or otherwise pre-stored movement patterns. 
In the next section, we present related work, followed by a brief overview of the Effort and Shape components 
of LMA. Then, we present the EMOTE model for Effort and Shape. Next, we discuss several animations that 
were created to demonstrate the power of our approach. Finally, we point to some directions that guide 
our future investigations and conclude with the main contributions of our work. 2. RELATED WORK In addition 
to the use of computational implementations of gesture models to animate synthetic humans during communicative 
acts [14,39,13,3,8,12], many researchers have addressed the issue of generating more natural movements 
in broader contexts. Several researchers have suggested methods of adding expressiveness to animated 
motions using such methods as stochastic noise functions [31], Fourier function models [38], or emotional 
transforms [1]. Such methods require an off-line modeling process for each different type of expression. 
Others have implemented tools that modify or interpolate existing motions to display different expressions 
or fit other constraints [10,40,33]. Various researchers have developed behavioral animation systems 
to generate animations of multiple creatures with varying personalities and/or goals [32,36,5,8]. Although 
creatures in behavioral animation systems display different high-level behaviors, their low-level movements 
are often very simple, non-expressive, or drawn from a small library of movements. A task-level animation 
system that generates arm motions of a human figure moving an object to a goal location has been developed 
using an inverse kinematics algorithm based on neurophysiological studies [21]. The focus of this system 
is on the intention of moving an object from one location to another and not on the underlying movement 
qualities of the character. The use of secondary motions has been proposed as a way to enliven computer 
generated animations. One approach adds secondary movements to walking characters based on user­specified 
personality and mood [29]. Another approach focuses on passive motions like the movement of clothing 
and hair, generated in response to environmental forces or the movements of characters and other objects 
[30]. Badler originally proposed (but did not implement) the use of Effort to provide users with expressive 
movement control of articulated figures [2]. Bishko suggested analogies between the Twelve Principles 
of Animation [35] and Laban Movement Analysis [8]. She shows that there is an abstract relationship between 
LMA and traditional animation techniques, but does not provide a computational means of exploiting this 
relationship. Others have done work with computerizing Labanotation (a notation, primarily used for recording 
dance, based on Laban s work that focuses on the structural aspects of movement) [4,11], but are only 
beginning to address the more qualitative aspects of movement provided by the Effort and Shape components. 
 3. BACKGROUND Rudolf Laban (1879-1958) made significant contributions to the study of movement, bringing 
together his experiences as a dancer, choreographer, architect, painter, scientist, notator, philosopher, 
and educator. He observed the movement of people performing all types of tasks: from dancers to factory 
workers, fencers to people performing cultural ceremonies, mental patients to managers and company executives. 
His theories on movement, which were significantly extended and applied by his students and colleagues 
have resulted in a rich vocabulary for describing and analyzing movement, leading to the development 
of Laban Movement Analysis1 [7,28,17,26]. LMA has evolved into a comprehensive system that has been used 
in dance, drama, nonverbal research, psychology, anthropology, ergonomics, physical therapy, and many 
other movement-related fields [6,15]. Laban Movement Analysis has five major components: Body, Space, 
Shape, Effort, and Relationship. Together these components constitute a textual and symbolic language 
for describing movement. Body deals with the parts of the body that are used and the initiation and sequencing 
of a motion. Space describes the locale, directions, and paths of a movement. Shape involves the changing 
forms that the body makes in space. Effort describes how the body concentrates its exertion while performing 
movements. Effort is often compared to dynamic musical terms such as legato, forte, dolce, etc., which 
give information on how a piece of music should be performed. Relationship describes modes of interaction 
with oneself, others, and the environment. 1 LMA is promoted by the Laban/Bartenieff Institute of Movement 
Studies (LIMS), 234 Fifth Avenue, Room 203, New York, NY 10001; (212)477-4299; www.limsonline.org. Relationship 
examples include facings, contact, and group forms. As part of our approach to gesture, we developed 
a computational model of the Effort and Shape components of LMA. Effort comprises four motion factors: 
Space, Weight, Time, and Flow. Each motion factor is a continuum between two extremes: (1) indulging 
in the quality and (2) fighting against the quality. In LMA these extreme Effort Elements are seen as 
basic, irreducible qualities, meaning they are the smallest units needed in describing an observed movement. 
The eight Effort Elements are: Indirect/Direct, Light/Strong, Sustained/Sudden, and Free/Bound. The eight 
Elements can be combined and sequenced for innumerable variations of phrasings and expressions. Table 
1 illustrates the motion factors, listing their opposing Effort Elements with textual descriptions and 
examples. Space: attention to the surroundings Indirect: flexible, meandering, wandering, multi-focus 
Examples: waving away bugs, slashing through plant growth Direct: single focus, channeled, undeviating 
Examples: pointing to a particular spot, threading a needle Weight: sense of the impact of one s movement 
Light: buoyant, delicate, easily overcoming gravity, marked by decreasing pressure Examples: dabbing 
paint on a canvas, describing the movement of a feather Strong: powerful, having an impact, increasing 
pressure into the movement Examples: punching, pushing a heavy object, expressing a firmly held opinion 
Time: lack or sense of urgency Sustained: lingering, leisurely, indulging in time Examples: stretching 
to yawn, stroking a pet Sudden: hurried, urgent Examples: swatting a fly, grabbing a child from the path 
of danger Flow: attitude towards bodily tension and control Free: uncontrolled, abandoned, unable to 
stop in the course of the movement Examples: waving wildly, shaking off water Bound: controlled, restrained, 
able to stop Examples: moving in slow motion, tai chi, carefully carrying a cup of hot liquid Table 1: 
Motion Factors and Effort Elements The Shape component involves three distinct qualities of change in 
the form of movement: Shape Flow, Directional Movement, and Shaping. A Shape Flow attitude primarily 
reflects the mover s concern with the changing relationship among body parts. These changes can be sensed 
as the increasing or decreasing volume of the body s form or a moving toward or away from the body center. 
Shape Flow can be seen from these two different perspectives. The first one emphasizes the torso, which 
can be said to Grow or Shrink. A continuous breathing pattern reveals changes in Shape Flow as seen from 
the torso perspective. The other perspective emphasizes the limbs, which are said to be Opening or Closing 
with respect to the longitudinal axis. Shrinking from the cold or stretching to wake up would be characterized 
as having a Shape Flow quality. While Shape Flow is mainly concerned with sensing the body s shape changes 
within itself, Directional Movement describes the mover s intent to bridge the action to a point in the 
environment. These movements can be simple spoke-like or arc-like actions to reach a direction or object, 
such as a reach to shake a hand or to touch an object or to move to a specific location. Shaping Movement 
depicts the changes in movement form that demonstrate a carving or molding attitude as the body interacts 
with the environment. This form can be dictated by objects in space or simply created by the mover. An 
active adapting of the body shape in order to move through a crowd, or a gesture describing an elaborately 
carved sculpture might illustrate a Shaping mode. Shape changes in movement can be described in terms 
of three dimensions: Horizontal, Vertical and Sagittal. Each one of these dimensions is in fact associated 
with one of the three main dimensions (Width, Length, and Depth) as well as one of the three planes (Horizontal, 
Vertical, and Sagittal) related to the human body. Changes in Shape in the Horizontal dimension occur 
mainly in the side-open and side-across directions; as the movement becomes planar there would be more 
of a forward-backward component added to the primary side component. Changes in the Vertical dimension 
are manifested primarily in the upward-downward directions; the plane would add more sideward component 
to the up-down. Finally, changes in the Sagittal dimension are more evident in the body s depth or the 
forward-backward direction; planar movement would add an upward-downward component. We note that while 
there is distinct vocabulary for each quality Shape Flow, Directional Movement, and Shaping in the 
various dimensions, we have merged these three concepts (using them interchangeably) and chosen to use 
the Shaping terminology. The terms we are using to describe the opposing changes in these dimensions 
are Spreading and Enclosing, Rising and Sinking, Advancing and Retreating. It is important to point out 
that limbs and torso movements are not required to involve the same Shape qualities at a given time. 
In this way, Shape Flow functions as a breathing baseline to support Directional and Shaping movement 
of the limbs. In another example, a traffic officer might hold up one arm with a Directional reach, while 
the other arm gestures in a circular Shaping mode, and the head does small tilting Shape Flow actions 
to accompany the Shaping arm. Horizontal Spreading: affinity with Indirect Examples: opening arms to 
embrace, sprawling in a chair Enclosing: affinity with Direct Examples: clasping someone in a hug, huddling 
in the cold Vertical Rising: affinity with Light Examples: reaching for something in a high shelf Sinking: 
affinity with Strong Examples: stamping the floor with indignation Sagittal Advancing: affinity with 
Sustained Examples: reaching out to shake hands Retreating: affinity with Sudden Examples: avoiding a 
punch Table 2: Shaping Dimensions and Affinities Another LMA concept is Reach Space in the Kinesphere 
(near, middle, and far). Our current approach regards Reach Space only from the perspective of the limbs 
in relation to the distance from the body center. Though this is a simplified view, it adds an important 
feature to the limb range of movement. Shape changes can occur in affinity with corresponding Effort 
Elements. Table 2 shows the opposing attitudes towards Shape, some examples, and their affinities with 
Effort Elements. 4. THE EMOTE APPROACH TO GESTURE Our current implementation of EMOTE uses a commercially 
available, fully articulated, human model [18]. At this point, we focus on expressive gestures involving 
arm and torso movements. EMOTE has four features which we believe are essential for creating gestures 
that convey naturalness and expressiveness: 1. A given movement may have Effort and Shape parameters 
applied to it independent of its geometrical definition. 2. A movement s Effort and Shape parameters 
may be varied along distinct numerical scales. 3. Different Effort and Shape parameters may be specified 
for different parts of the body involved in the same movement. 4. The Effort and Shape parameters may 
be phrased (coordinated) across a set of movements.  The underlying movements of a gesture are specified 
through key time and pose information defined for the arms and the torso. An external process, such as 
using a specific gesture stored in a motion library, a procedurally generated motion, or motion captured 
from live performance, could be used to generate these underlying movements. Key pose information could 
be extracted from these movements and used as input into EMOTE. With the key pose information, the EMOTE 
parameters could then be applied to vary the original performance (property 1). Effort and Shape qualities 
are expressed using numeric parameters that can vary along distinct scales (property 2). Each Effort 
and Shape factor is associated with a scale ranging from -1 to +1. The extreme values in these scales 
correspond to the extreme attitudes of the corresponding factors. For example: a +1 value in Effort s 
Weight factor corresponds to a very Strong movement; a -1 value in Shape s Vertical dimension corresponds 
to a Rising movement. Effort parameters are translated into low-level movement parameters, while Shape 
parameters are used to modify key pose information. By interactively using one or many of the Effort 
and Shape dimensions, we can search for the desired quality of a particular movement. During procedural 
synthesis, EMOTE parameters can be applied directly based on parameter values dependent on a character 
s particular utterance, reactions, or personality. EMOTE permits independent specification of Effort 
and Shape parameters for each part of the body (property 3). In its current implementation however, Effort 
parameters do not apply to torso movements. Although Shape parameters have proven to be effective in 
the specification of expressive torso movements, further investigation should be carried out to identify 
how Effort qualities are manifested in the torso. Moreover, the Shape parameters are mainly applied to 
torso movement. The general Space concept of Kinespheric Reach Space is used in the arms. Table 3 summarizes 
which dimensions of Effort and Shape can be used to modify the movements of the different parts of the 
human body. Allowing the definition of expressive gestures that include the legs can be similarly done, 
however, additional constraints need to be carefully considered in order to provide static and dynamic 
balance and stability. Moreover, using Effort and Shape parameters to modify locomotion is a more complex 
task and involves the identification of a different set of low-level movement parameters, including an 
exploration of the pelvic­femoral movement rhythm. Furthermore, including the legs may also affect the 
movement of the torso and arms, because changing the qualities in the legs may result in a reworking 
of the posture. For instance, the additional effort in the legs is reflected and reinforced by the exertion 
of the torso and placement of the arms. Right Arm Left Arm Torso Effort Space yes yes no Weight yes yes 
no Time yes yes no Flow yes yes no Shape Horizontal yes yes yes Vertical yes yes yes Sagittal yes yes 
yes Reach Spc yes yes no Table 3: Body Parts and Effort and Shape Dimensions Figure 1: Effort Phrase 
Editor Finally, our approach allows the specification of different sets of values for the Effort and 
Shape parameters across any series of keys that define the underlying motions (property 4). By property 
(3), this can be done separately for each part of the body. Figure 1 depicts a graph editor used to specify 
Effort parameters across a series of keyframes defined for the arms. 4.1 Expressive Arms The underlying 
key poses of the arms are defined as end-effector positions (keypoints). Keypoints can be defined as 
being global or local. Local keypoints are defined relative to the human s shoulders. Global keypoints, 
on the other hand, establish a constraint relative to the environment. Keypoints can also be classified 
into Goal or Via points. Goal points define a general movement path; the hand follows this path, stopping 
at each Goal point. Via points direct the motion between keyframes without pausing. For instance, a Via 
point might be used to generate a semi-circular path between two Goal points. EMOTE uses an arm model 
with a 1 degree-of-freedom (DOF) elbow joint and spherical (3 DOF) shoulder and wrist joints. An analytical 
inverse kinematics algorithm (IKAN) computes the shoulder and elbow rotations, given a goal specified 
by three­dimensional position coordinates and an elbow swivel angle [36]. Wrist rotations are determined 
according to Effort settings (as described below). Reflecting Effort and Shape definitions provided by 
the LMA system, Shape parameters are used to modify the keypoints that specify arm movements, while Effort 
parameters affect the execution of those movements resulting from the modified keypoints. 4.1.1 Applying 
Shape to Arm Movements Let us first consider the Horizontal, Vertical and Sagittal dimensions of Shape 
and show how the parameters associated with them are used to modify the keypoints. Because we are striving 
to simulate volume-like changes in the movement we are associating the Shape changes more with planar 
action than with strictly dimensional movement. For a particular keypoint, let the variables hor, ver 
and sag in the interval [-1, +1] represent the parameters corresponding to the Horizontal, Vertical and 
Sagittal dimensions, respectively. We define two constants abratio > 1 and maxd. . For each one of the 
above dimensions, we find an ellipse containing the keypoint and lying in a plane parallel to the plane 
associated with that dimension (as described in Section 3). The center of the ellipse is the projection 
of the shoulder joint position on that plane. The major axis of the ellipse is parallel to the direction 
mostly affected by changes in that dimension and its minor axis is parallel to the other direction affected 
by such changes. The quotient between its major radius a and its minor radius b is abratio. We calculate 
the angle .. [0, 2p ) formed by the major axis of the ellipse and the segment whose endpoints are the 
center of the ellipse and the keypoint. We find the contributions of that dimension to the modified keypoint 
by rotating the keypoint by d. , a fraction of maxd. determined by the numeric parameter associated with 
the dimension being considered. Figure 2 illustrates how we calculate vdy and vdz, the contributions 
of the Vertical parameter ver to a particular keypoint. Figure 2: A Keypoint Modified by theVertical 
Parameter Let x, y and z be the coordinates of the keypoint in Figure 2. We find . such that . y .. = 
atan( * abratio) (1) . -z .. 0 =. < 2p The major radius a of the ellipse is calculated by the following 
equation: - z a = (2) cos(. ) The angle formed by the rotated keypoint and the major axis of the ellipse 
is given by the function rot defined as follows: . 0 ver = 0 . min(.- ver * maxd. ,2p ) ver < 0,. =p 
. rot(. ) =. max(.+ ver * maxd. ,0) ver < 0,. <p (3) . max(.- ver * maxd. ,p ) ver > 0,. =p . min(.+ 
ver * maxd. ,p ) ver > 0,. <p Finally, the contributions vdy and vdz are calculated as follows: vdz =- 
(a * cos(rot(. ))) - z (4) 1 vdy = (a ** sin(rot(. ))) - y (5) abratio We use the same model as described 
above to determine the contributions of the Horizontal (hdy, hdx) and Sagittal (sdx, sdz) parameters 
to the modified keypoint. We find the x , y and z coordinates of the modified keypoint by adding the 
appropriate contributions to the coordinates of the original keypoint. Then, x = x + hdx + sdx (6) y 
= y + hdy + vdy (7) z = z + vdz + sdz (8) Let us now consider how the Kinespheric Reach Space parameter 
affects a particular keypoint. When considered from the perspective of the arms, Reach Space design describes 
the limb relationship with the body as it moves toward or away from the body center. Therefore, our Shape 
model modifies a particular keypoint by moving it along the direction that passes through the keypoint 
and the center of mass of the human figure. We use the Reach Space parameter rs to calculate the amount 
by which the keypoint is moved toward or away from the center of mass. This Reach Space modifier is considered 
after the keypoint has been modified according to its Horizontal, Vertical and Sagittal parameters. When 
the achievement of the modified keypoint requires shoulder angles outside the human body limits, stored 
joint limits avoid unattainable configurations of the body. As they establish a constraint relative to 
the environment, Global keypoints are not affected by the Shape parameters. 4.1.2 Applying Effort to 
Arm Movements The translation of the qualitative Effort Elements into quantitative, low-level movement 
parameters was the key task in defining a computational model of the Effort component of LMA. Initially, 
we tried to deduce movement characteristics from motion capture data. We collected 3D motion capture 
data of a Certified Movement Analyst (CMA) trained in LMA performing numerous examples of combinations 
of Effort Elements. Analysis of the motion capture data led to only the most obvious conclusions; i.e.: 
Sudden is short in duration, Sustained is longer in duration, and Strong tends to have large accelerations. 
The inability to deduce the more subtle characteristic qualities of Effort arose from several factors. 
First, Effort reflects complex inner physiological processes that are related to a being s inner drive 
to respond to the physical forces in nature. Thus, Effort is embodied in the whole person and manifested 
in all body parts, whereas we were interested solely in the physical embodiment and visual result of 
inner attitudes on movement, particularly that of the arms. Furthermore, numerous other movements such 
as visual attention, changes in muscular tension, facial expressions, and breath patterns are not adequately 
captured by current motion capture technology. As a result, we turned to other methods for deducing the 
low-level movement parameters and corresponding settings for Effort. We defined underlying quantitative 
structures that model each Effort Element. Visual analysis of the motion capture data played an important 
role in extracting other manifestations of Effort and focusing our attention solely on the influence 
of Effort on arm movements. Other methods we used to derive an empirical model of Effort included descriptions 
of Effort from the literature [7,17,26,28], application of traditional animation principles [25,35], 
and much experimentation with feedback from a CMA. First, we describe the set of low-level, quantitative 
movement parameters. Then, we show how these parameters are set based on the settings for the Effort 
parameters. There are three types of low-level movement parameters: those that affect the arm trajectory, 
those that affect timing, and flourishes that add to the expressiveness of the movement. 4.1.2.1 Trajectory 
Definition We define the arm trajectory for a given animation with two parameters: Path curvature: determines 
the straightness or roundness of the path segments between keypoints. We control the path curvature using 
the tension parameter introduced by Kochanek and Bartels for interpolating splines [20]. The tension 
parameter Tval ranges from -1 to +1.  The interpolation space: defines the space in which the interpolation 
is performed: end-effector position, joint angle, or elbow position.  For end-effector interpolation, 
we use the end-effector position and swivel angle stored for each keypoint. We define an interpolating 
spline between the positions at keypoints using the tension parameter to determine the curvature of the 
path. We also interpolate between swivel angle values with an interpolating spline. For joint angle interpolation, 
we compute and store the shoulder and elbow rotations at keypoints. We then generate an interpolating 
spline between the elbow angle values at keypoints and perform spherical linear interpolation to determine 
the shoulder rotations. For interpolation in elbow position space, we compute and store the elbow position 
at keypoints using the posture defined by the end-effector position and swivel angle. We then define 
an interpolating spline between these positions, which are later used to set the shoulder rotations. 
The elbow rotations for elbow position interpolation are the same as those for end­effector interpolation. 
Interpolation in elbow position space gives smooth elbow motions, but a less path-driven movement than 
interpolation in end-effector position space. The Effort settings determine which interpolation space 
is used. The default interpolation space uses end-effector position. Free movements use angular interpolation 
to achieve a less path-driven and less controlled movement. Our empirical studies show that Indirect 
movements tend to be driven by the elbow, and thus use interpolation in elbow position space. 4.1.2.2 
Parameterized Timing Control We separate timing control from trajectory definition by using a variation 
of the double interpolant method introduced by Steketee and Badler [34]. The interpolating splines that 
define the trajectory (described in the preceding section) compute values between keypoints using an 
interpolation parameter s that varies from 0 to 1 over the interval from keypoint i to keypoint i +1 
[20]. Let the trajectory be defined by some function P(s,i). We now need a method of translating frame 
numbers into s and i. At the ith keypoint, s = 0 . For in-between frames, we define a variable t . [0,1], 
a frame s relative time between the previous and following keypoints. Let prev equal the frame number 
of the previous keypoint, next equal the frame number of the next keypoint, and curr equal the current 
frame number. Then, curr - prev t = (9) next - prev We define a frame number-to-time function Q(t , I 
) = s , parameterized by a set of variables I to achieve various timing effects (described further below). 
For each in-between frame, we normalize the frame number to produce t', use function Q to compute s, 
and then input s and the corresponding keypoint number i into function P to compute the position values 
(or joint angle values for angular interpolation) for the given frame. We provide several parameters 
for timing control: The number of frames between keypoints is initially set according to the user s 
specified key times, but these values get adjusted according to the Effort settings.  Input variables 
to the keyframe-to-time function (I) include inflection time ti, time exponent texp, start velocity v0, 
and end velocity v1.  Our parameterized frame number-to-time function Q assumes every movement (from 
one Goal keypoint to the next) starts and ends at rest. Also, every movement has a constant acceleration 
a until time ti, followed by a constant deceleration. We introduce velocities v0 at time t0 and v1 at 
time t1 to achieve the traditional animation effects of anticipation and overshoot [25]. This model gives 
us the following velocity function (Figure 3): .- v0 t [0,t0) . t . 0 .- (v + t )t +vt + tt 0 i 0 i 
0 i [t ,t ) . 0 i (10) t - t v(t ) =. 0 i - (v + t )t +vt + tt 0 i 1 i 1 i . [ti ,t1). t1 - ti .- v1t 
+v1 [t1,1] . .t - 1 . 1 where ( )t exp t = t . (11) The function Q is the integral of Equation (10). 
ti -v0 -v1 Figure 3: Velocity Function The set of input variables I to the frame number-to-time function 
Q provides control to the acceleration/deceleration pattern of the movement, as well as allowing for 
anticipation and overshoot. The inflection point ti represents the point (between 0 and 1) where the 
movement changes from accelerating to decelerating. A value of 0.5 gives a basic ease-in/ease-out curve. 
A value greater than 0.5 corresponds to a primarily accelerating motion, while a value less than 0.5 
gives a decelerating motion. The default time exponent (texp) value of 1.0 does not affect the velocity 
curve; however, values greater than 1.0 magnify an acceleration, while values less than 1.0 exaggerate 
a deceleration. The start (v0) and end (v1) velocities2 default to 0. Increasing v0 generates movements 
with anticipation, where the hand pulls back before extending in preparation for a Strong movement. Decreasing 
v1 generates movements with overshoot, such as in Free movements where an indulgence in flow causes one 
to swing out past a target before hitting it. We set t0 to 0.01 and t1 to 0.99, which gives us natural-looking 
anticipation and overshoot effects; however, these values can easily be included in I as variable low-level 
movement parameters. 4.1.2.3 Flourishes Flourishes are miscellaneous parameters that add to the expressiveness 
of the movements. These are listed below: Wrist bend is determined by the wrist bend multiplier wbmag 
and the wrist extension magnitude wxmag. The wbmag parameter is a multiplier that represents the magnitude 
of the wrist bend. If the wbmag is set for a flexed wrist, the wrist bend is set to 0.6 radians about 
the x-axis. Otherwise, the wrist bend is set using wrist _ bend = wbmag * sin(2p (t +0.75)) +1 - wxmag) 
(12) where t . [0,1] and represents the normalized time between two keypoints. This results in a wrist 
that gradually bends inwards and back out. The value of wxmag shifts the sinusoidal graph, setting the 
beginning wrist extension to be positive (outward) or negative (inward). 2 As mentioned, each movement 
begins and ends at rest. The start and end velocities represent shortly after the beginning or shortly 
before the end of a movement, respectively. They are so named to emphasize that they are not initial 
and final velocities, which remain 0. Arm twist is parameterized by wrist twist magnitude wtmag, wrist 
frequency wfmag, elbow twist magnitude etmag, and elbow frequency efmag. The wrist twist is measured 
in radians about the z-axis and is determined by: wrist _ twist = wtmag * sin(wfmag *p t ) . (13) Elbow 
twist is set using a similar equation, replacing wtmag and wfmag with etmag and efmag, respectively. 
Displacement magnitude is a multiplier dmag that adds a sinusoidal displacement to the elbow angle elbow 
_ angle = elbow _ angle * (1 + dmag * sin(2p t )) (14) where t is the normalized time between two keypoints. 
 4.1.2.4 Parameter Settings To determine the mapping of the four Effort Elements into our low-level movement 
parameters, we first determined the default settings for each of the eight Effort Elements by trial and 
error using visual analysis and testing by a CMA. For example, the default interpolation space is set 
to elbow position for Indirect, joint angle for Free, and end-effector for the other Effort Elements. 
The default tension of the path curvature Tval is set to -1 for Indirect, +1 for Direct, and 0 for the 
other Effort Elements. Once we had the default settings for the individual Effort Elements, we generated 
the range between opposing Effort Elements by interpolating continuous variables and using the nearest 
value for discrete variables such as the interpolation space. We note that these may lead to discontinuities 
in the animation if Space, Weight, or Flow cross zero when they are phrased across the keyframes. In 
[15], we show that such discontinuities occur only if the zero crossings occur at points that are not 
Goal keypoints, which is fairly uncommon in real human movements. In general, combinations of Effort 
Elements are achieved in a straightforward manner. The magnitude of an Effort Element is used to weight 
its contribution for a parameter setting. If more than one Effort Element contributes to a parameter 
setting, we take the maximum value of the weighted contributions. Several parameters undergo minor adjustments 
when combining Effort Elements from different motion factors. Finally, we express our Effort model as 
a set of equations. Let the variables ind, dir, lgt, str, sus, sud, fre, and bnd represent the magnitudes 
for Indirect, Direct, Light, Strong, Sustained, Sudden, Free, and Bound, respectively. Each of these 
variables is in [0,1]. Variables within the same motion factor are related as such: if one Effort Element 
variable is positive, then its opposing Effort Element variable is zero. To adjust parameters for combined 
Effort settings, we use the function f: . aa = b f (a, b) = (15) ..b otherwise Our model for translating 
Effort into low-level movement parameters is given by the following equations: Tval = (- 1* ind + 1* 
f (ind , fre)) + dir (16) wbmag = max(0.6*ind ,0.5*lgt,0.4* fre) (17) wxmag =- 0.3*lgt + (0.3* fre - 
0.9* f (str, fre)) (18) dmag = etmag = wtmag = 0.4 * ind (19) efmag = wfmag = 2* ind (20) ti = 0.5 + 
0.4* max(str, sud ) (21) - 0.4* max(lgt, sus) + 0.8* f (bnd,lgt) v0 = 0.1* str - max(0.06 * f (sus, str),0.1* 
f ( fre, str)) (22) v1 = max(0.03* max(lgt, sus),0.2 * fre - 0.1* f (ind, fre)) (23) texp = 1 + 2* sud 
+ (0.2 * f (str, sud ) - f ( fre, sud )) - 0.2 * max(str, f (dir, sus)) (24) - 0.4 * fre - 0.5* f (ind, 
fre)  4.2 Expressive Torso The underlying key poses of the torso involve, in fact, the neck joint, 
the spine, the pelvis and the two clavicle joints. The neck has 3 DOF, the spine has 17 joints with 3 
DOF each, the pelvis has 3 DOF and each clavicle has 2 DOF. A key pose consists of angles for the neck, 
the pelvis, and the clavicles, in addition to the configuration of the spine [18]. When, for a particular 
keyframe, no pose information is provided, the system assumes a neutral posture, where all the angles 
are 0. We use an ease-in/ease-out curve to interpolate the angles in the keyframes and hence calculate 
the angles in the in-between frames. In summary, Shape changes essentially provide squash and stretch 
within the limits of a fixed segment length articulated skeleton. 4.2.1 Applying Shape to Torso Movements 
As seen before, EMOTE allows the definition of Shape parameters for the torso corresponding to the Horizontal, 
Vertical and Sagittal dimensions, which are used to modify the key poses. Each parameter lies in a scale 
ranging from -1 to +1. Our Shape model for the torso associates each main body dimension (upward/downward, 
sideward-open/sideward-across, and forward/backward) with specific parts of the body. We note that our 
Shape model was designed considering the available controls in our selected articulated figure model 
[18]. In particular, the torso could not be expanded and contracted for breath and other volume-changing 
movements. We based our Shape to body part associations on the suitability of each body part in producing 
changes in the form of the body in given directions. Thus, we associate the upward-downward direction 
with the neck and the spine; the sideward direction with the clavicles, and the forward-backward direction 
with the pelvis. Therefore, changes in the Horizontal dimension, which occur mainly in the sideward direction 
but also have a forward-backward component as the movement becomes planar, affect mostly the angles of 
the clavicles but also slightly alter pelvis rotations. Changes in the Vertical dimension, which are 
manifested primarily in the upward-downward direction but also have a sideward component in planar movement, 
affect mostly the angles of the neck and the spine but also change clavicle angles. Finally, changes 
in the Sagittal dimension, which are more evident in the forward-backward direction but also involve 
an upward-downward component in planar movement, mainly affect pelvis rotations but also change the angles 
of the neck and spine. For each opposing attitude associated with the above dimensions (Spreading, Enclosing, 
Rising, Sinking, Advancing, and Retreating), we define maximum displacement angles for all the body parts 
that are affected by changes in that dimension. For instance, for the opposing attitudes in the Horizontal 
dimension, we define the following constants: spreading_clavicle_angle, enclosing_clavicle_angle, spreading_pelvis_angle, 
and enclosing_pelvis_angle. The first two angles represent clavicle rotations about the z-axis and the 
latter represent pelvis rotations about the y-axis. For a particular keyframe, let the variables spr, 
enc, ris, sin, adv, and ret represent the magnitudes for Spreading, Enclosing, Rising, Sinking, Advancing, 
and Retreating, respectively. Each of these variables is in [0,1]. Variables referencing the same dimension 
are related such that if one variable is positive, then its opposing variable is zero. We modify the 
angles in the key pose by adding the weighted contribution of all the dimensions that affect the particular 
body part being considered. For instance, if the clavicle rotation about the z-axis is represented by 
the variable clavicle_angle and the pelvis rotation about the y-axis is represented by the variable pelvis_angle, 
then we modify those angles as follows: clavicle_a ngle = clavicle_ angle + ris*risin g_clavicle _angle 
 (25) + sin*sinking_clavicle _angle + spr*sprea ding_clavi cle_angle + enc*enclo sing_clavi cle_angle 
pelvis_ang le = pelvis_an gle + spr*spread ing_pelvis _angle (26) + enc*enclos ing_pelvis _angle + 
adv*advanc ing_pelvis _angle + ret*retrea ting_pelvi s_angle  where rising_clavicle_angle and sinking_clavicle_angle 
are the maximum displacement angles of the clavicles corresponding to the opposing attitudes towards 
the Vertical dimension, and advancing_pelvis_angle and retreating_pelvis_angle are the maximum displacement 
rotations of the pelvis corresponding to the opposing attitudes towards the Sagittal dimension.  5. 
EXAMPLES To demonstrate the power of our approach to gesture we have created a series of animations shown 
on the accompanying video. All the examples were generated in real-time. The first series of animations 
are all generated from the same set of key poses and try to mimic an actor during an actual performance. 
We vary the values of the Effort and Shape parameters across the animations and show how these variations 
can completely alter the meaning of the dramatization enacted by the synthetic actor. By suppressing 
its Shape parameters, we also show the important role that the torso plays in gesture and in the depiction 
of a convincing character. The second video series emphasizes the slight differences in dynamic qualities 
of movements superimposed on American Sign Language phrases (from an ASL sign library) and tries to capture 
the nuances of meaning represented by these differences. The movement of the hands in the video is implemented 
using forward kinematics and linear interpolations. 6. DISCUSSION Our EMOTE computational model of Effort 
and Shape components allows the animation of characters with natural­looking gestures through the usage 
of high-level parameters that represent qualitative aspects of movements. By using EMOTE interactively 
we hope to avoid the hassle that the animator goes through while working with a large number of low-level 
parameters. In order to further assess the advantages of using Effort and Shape parameters from the perspective 
of user interaction, formal methods of evaluation of our approach should be devised. We did a preliminary 
evaluation of the Effort Elements of EMOTE [15]. Using a stylized character with head, arms, and hands, 
we created a 16-minute video of randomly selected Effort Elements. In the first part of the tape, Effort 
gestures with 16 two-keypoint and 16 five-keypoints were paired with a neutral (no Effort Element) animation. 
The second part of the tape consisted of 30 long (5 keypoint) animations with various Effort combinations. 
The tape was given to 3 CMAs and the project consultant CMA. They were asked to view it once to get a 
feeling for the presentation and then a second time while marking a coding sheet. They were asked to 
mark the primary overall Effort Element(s) they observed as present (-1 or 1) or neutral (0). The results 
are presented in Table 4. The first row indicates the percentage of correct responses where the CMA 
either marked the Effort that we were trying to display in the animation or marked neutral when we were 
trying to display neutrality along a given motion factor. The second row indicates the percentage of 
neutral responses where the CMA marked neutral when we were trying to display an Effort or where the 
CMA marked an Effort when we were trying to display neutral along a give motion factor range. The third 
row indicates the percentage of opposite responses where the CMA marked the Effort opposite from the 
one we were trying to portray. The low but significant percentage of neutral responses is partially attributed 
to the fact that most of the animation segments on our video showed combinations of the Effort Elements 
 thus, a more prominent Effort may have masked other displayed Effort Elements. One consequence of this 
experiment for us was to increase the maximum movement rate for the limbs. For example, the Sudden movements 
did not appear fast enough to trained observers. Also, the Shape elements were not included in this experiment. 
Note that the normal CMA observer situation is to watch motions repeatedly; by limiting their samples 
to two we were forcing them to pick distinctive Effort features in a slightly unnatural setting. The 
results were encouraging enough, however, for us to proceed with refinements to the Effort Elements and 
the incorporation of the torso and Shape components. Consultant CMA 1 CMA 2 CMA 3 Correct 76.6 55.6 53.2 
60.1 Neutral 22.6 38.7 39.1 37.1 Opposite 0.81 5.6 7.7 2.8 Table 4: Overall Percentages for Effort Element 
Evaluation Our attempt to bridge the gap between characters manually animated and characters animated 
by procedures establishes a new layer in the motion control process in which expressiveness is represented 
by a small number of parameters. We expect that this layer of control will give rise to yet another layer, 
where characters controlled by natural language commands show different performances according to adverbs 
that convey manner. These adverbs should be automatically mapped into Effort and Shape parameters. For 
example, carefully might translate into Light and slightly Sustained Effort portrayed during arm movements 
and a little Retreating Shape displayed by the torso; proudly might translate into a Rising posture. 
Furthermore, we expect to find connections between emotions and personality and our high-level parameters 
and so be able to synthesize movements that reflect these inner states. 7. CONCLUSIONS We have introduced 
a new approach to procedural human animation that tries to close the gap between characters animated 
by the use of manual techniques and characters animated procedurally. This approach goes beyond the realm 
of psychology of gestures and linguistic-based approaches by exploring the domain of movement observation. 
This approach uncovers the movement qualities, which can be combined together to reveal different manners, 
inner states, personalities and emotions. The EMOTE approach to gesture proposes a computational model 
of the Effort and Shape components of Laban Movement Analysis and associates with each one of their dimensions 
numerical parameters that modify pre-defined movements. Two other important aspects of EMOTE are inspired 
by the tenets of movement observation. The first is the ability to phrase Effort and Shape parameters 
across a set of movements. We believe that a character s gestures should be phrased similarly to communicative 
phrasing with an expressive content consonant with the principal utterance; for example, a strong accent 
in speech should be correlated by a strong Effort in gesture. Since Effort plays a key role in the interpretation 
of a character s action, a gesture must display Effort qualities that match his/her intentions, motivations, 
and mood. Otherwise, the character s message appears conflicted and confused. Furthermore, EMOTE reflects 
our belief that, even if a character moves its arms with appropriate gestures, it will lack conviction 
and naturalness if the rest of the body is not appropriately engaged. If the empirical principles of 
movement science hold up when transformed into computer code implementations, we should be able to animate 
engaging, committed, expressive, and believable characters consistently and automatically. 8. ACKNOWLEDGEMENTS 
Janis Pforsich (courante@juno.com) was our LMA consultant for EMOTE. She played a key role in the development 
of our Effort and Shape model, was an enthusiastic teacher of movement observation, ensured the accuracy 
of the project and its documentation with LMA theory, and acted as a devoted proponent of our work. We 
are very grateful for her generous contributions. This research is partially supported by U.S. Air Force 
F41624-97-D-5002, Office of Naval Research K-5­55043/3916-1552793, and AASERTs N00014-97-1-0603 and N0014-97-1-0605, 
NSF EIA98-09209, SBR-8900230, and IIS­9900297, NASA NRA NAG 5-3990, and Engineering Animation Inc. (EAI). 
Support for Monica Costa by the National Scientific and Technological Development Council (CNpq) of Brazil 
is also gratefully acknowledged.  9. REFERENCES [1] Amaya, K., Bruderlin, A., Calvert, T. Emotion from 
motion. In Davis, W.A., Bartels, R., editors, Graphics Interface 96, pp. 222-229. Canadian Information 
Processing Society, Canadian Human-Computer Comm. Society, May 1996. [2] Badler, N. A computational alternative 
to effort notation. In Gray, J.A., editor, Dance Technology: Current Applications and Future Trends. 
National Dance Association, VA, 1989. [3] Badler, N., Chi, D., Chopra S. Virtual human animation based 
on movement observation and cognitive behavior models. In Computer Animation Conf., Geneva, Switzerland, 
May 1999. IEEE Computer Society Press. [4] Badler, N., Smoliar, S. Digital representations of human movement. 
ACM Computing Surveys, 11(1):19-38, March 1979. [5] Badler, N., Webber, B., Becket, W., Geib, C., Moore, 
M., Pelachaud, C., Reich, B., and Stone, M. Planning for animation. In N. Magnenat-Thalmann and D. Thalmann 
(eds), Interactive Computer Animation, Prentice-Hall, pp. 235-262, 1996. [6] Bartenieff, I., Davis, M. 
Effort-Shape analysis of movement: The unity of expression and function. In Davis, M., editor, Research 
Approaches to Movement and Personality. Arno Press Inc., New York, 1972. [7] Bartenieff, I., Lewis, D. 
Body Movement: Coping with the Environment. Gordon and Breach Science Publishers, New York, 1980. [8] 
Becheiraz, P., Thalmann, D. A model of nonverbal communication and interpersonal relationship between 
virtual actors. Pro. Computer Animation 1996, IEEE Computer Society Press, pp.58-67, 1996. [9] Bishko, 
L. Relationships between Laban Movement Analysis and computer animation. In Dance and Technology I: Moving 
Toward The Future, pp. 1-9, 1992. [10] Bruderlin, A., Williams, L. Motion signal processing. In Proc. 
of SIGGRAPH 95, pp. 97-104, August 1995. [11] Calvert, T.W., Chapman, J., Patla, A. Aspects of the kinematic 
simulation of human movement. IEEE Computer Graphics &#38; Applications, 2:41-48, November 1982. [12] 
Capin, T., Pandzic, I., Magnenat-Thalmann, N., Thalmann, D. Avatars in Networked Virtual Environments. 
Wiley, Chichester, England, 1999. [13] Cassell, J. Not just another pretty face: Embodied conversational 
interface agents. Comm. of the ACM, 2000. (to appear). [14] Cassell, J., Pelachaud, C., Badler, N., Steedman, 
M., Achorn, B., Becket, W., Douville, B., Prevost, S., Stone, M. Animated conversation: Rule-based generation 
of facial expression, gesture and spoken intonation for multiple conversational agents. In Computer Graphics, 
Annual Conf. Series, pp. 413-420. ACM, 1994. [15] Chi, Diane. A Motion Control Scheme for Animating Expressive 
Arm Movements. PhD thesis, University of Pennsylvania, 1999. [16] Davis, M. Effort-Shape analysis: Evaluation 
of its logic and consistency and its systematic use in research. In Bartenieff, I., Davis, M. and Paula, 
F., editors, Four Adaptations of Effort Theory in Research and Teaching. Dance Notation Bureau, Inc., 
New York, 1970. [17] Dell, C. A Primer for Movement Description: Using Effort-Shape and Supplementary 
Concepts. Dance Notation Bureau, Inc., New York, 1970. [18] Jack 2.2 Toolkit Reference Guide, Engineering 
Animation, Inc., 1999. [19] Kendon, A. Gesticulation and speech: Two aspects of the process of utterance. 
In Key, M.R., editor, The Relation between Verbal and Nonverbal Communication, pp. 207-227. Mouton, 1980. 
 Permission to make digital or hard copies of part or all of this work or personal or classroom use is 
granted without fee provided that copies are not made or distributed for profit or commercial advantage 
and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, 
to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. SIGGRAPH 
2000, New Orleans, LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00 [20] Kochanek, D.H.U., Bartels, 
R.H. Interpolating splines with local tension, continuity, and bias control. In Proc. of SIGGRAPH 84, 
volume 18, pp. 33-41, July 1984. [21] Koga, Y., Kondo, K., Kuffner, J., Latombe, J. Planning motions 
with intentions. In Proc. of SIGGRAPH 94, pp. 395-408, July 1994. [22] Laban, R. The Mastery of Movement. 
Plays, Inc., Boston, 1971. [23] Laban, R., Lawrence, F. C. Effort: Economy in Body Movement. Plays, Inc., 
Boston, 1974. [24] Lamb, W. Posture and Gesture: An Introduction to the Study of Physical Behavior. Duckworth 
&#38; Co., London, 1965. [25] Lasseter, J. Principles of traditional animation applied to 3D computer 
animation. In Proc. of SIGGRAPH 87, volume 21, pp. 35-44, July 1987. [26] Maletic, V. Body, Space, Expression: 
The Development of Rudolf Laban s Movement and Dance Concepts. Mouton de Gruyte, New York, 1987. [27] 
McNeil, D. Hand and Mind: What Gestures Reveal about Thought. University of Chicago, 1992. [28] Moore, 
C.-L., Yamamoto, K. Beyond Words: Movement Observation and Analysis. Gordon and Breach Science Publishers, 
New York, 1988. [29] Morawetz, C., Calvert, T. Goal-directed human animation of multiple movements. In 
Proceedings of Graphics Interface 90, pp. 60-67, May 1990. [30] O Brien, J. F., Zordan, V. B., Hodgins, 
J. K. Combining Active and Passive Simulations for Secondary Motion. IEEE Computer Graphics &#38; Applications. 
In Press. [31] Perlin, K. Real time responsive animation with personality. IEEE Transactions on Visualization 
and Computer Graphics, 1(1):5-15, March 1995. [32] Reynolds, C.W. Flocks, herds, and schools: A distributed 
behavioral model. In Proc. of SIGGRAPH 87, volume 21, pp. 25-34, July 1987. [33] Rose, C., Cohen, M.F. 
and Bodenheimer, B. Verbs and adverbs: Multidimensional motion interpolation. IEEE Computer Graphics 
&#38; Applications, 18(5), September-October 1998. [34] Steketee, S., Badler, N. Parametric keyframe 
interpolation incorporating kinetic adjustment and phasing control. In Proc. of SIGGRAPH 85, volume 19, 
pp. 225-262, July 1985. [35] Thomas, F., Johnston, O. Illusion of Life: Disney Animation. Hyperion, New 
York, 1995. [36] Tolani, D. Inverse Kinematics Methods for Human Modeling and Simulation. PhD thesis, 
University of Pennsylvania, 1998. [37] Tu, X., Terzopoulos, D. Artificial fishes: Physics, locomotion, 
perception, behavior. In Proc. of SIGGRAPH 94, pp. 43-50, July 1994. [38] Unuma, M., Anjyo, K., Takeuchi, 
R. Fourier principles for emotion­based human figure animation. In Proc. of SIGGRAPH 95, pp. 91-96, August 
1995. [39] Vilhjalmsson, H.H., Cassell J. Bodychat: Autonomous communicative behaviors in avatars. In 
Proc. of the Second International Conference on Autonomous Agents, pp. 269-277. ACM, May 1998. [40] Witkin, 
A., Popovic, Z. Motion warping. Proceedings of SIGGRAPH 95, pp. 105-108, Los Angeles, CA, August, 1995. 
  
			