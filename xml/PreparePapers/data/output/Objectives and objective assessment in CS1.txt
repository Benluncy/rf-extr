
 Objectives and Objective Assessment in CSl Raymond Lister School of Computing and Information Technology 
University of Western Sydney Kingswood, NSW 2747, Australia r.lister(~uws.edu.au Abstract When designing 
a first semester "CSI" programming subject, I advocate "truth in sentencing". That is, the objectives 
should be explicit, and the assessment tasks should reflect the objectives. This may appear to be a statement 
of the obvious, but few subjects satisfy these criteria. The traditional CS1 approach is to set students 
the task of writing extensive code, as early as possible. On closer inspection of such subjects, one 
finds marking schemes for exams and assignments that are generous to the point of being inconsistent 
with the subject objectives. Instead, students should not write any original code in CS1, and should 
be examined by multiple choice question. Introduction Every CS1 teacher faces two incompatible constituencies 
the students and the faculty. If the teacher attempts to satisfy the expectations of "down stream" teachers, 
most of the class will absorb very little of the material. If the teacher sets more realistic goals, 
then the "down stream" teachers complain that students have not been adequately prepared. In many institutions, 
this incompatibility has led to official objectives that are demanding, and even assignments and examinations 
that appear to reflect these objectives, but on closer inspection, one finds an absence of "truth in 
sentencing". That is, the marking schemes of assignments and exams are very generous. In reality, a student 
with a marginal passing score knows a small subset of the official syllabus. Such students have little 
chance of succeeding in "down stream" subjects ... unless this game continues in those subjects. Permission 
to make digital or hard copies of all or part of this work for personal or classroom use is granted without 
fee provided that copies are not made or distributed for profit or commercial advan-tage and that copies 
bear this notice and the full citation on the first page To copy otherwise, to republish, to post on 
servers or to redistribute to lists, requires prior specific permission and/or s fee. SIGCSE 2001 2101 
Charlotte, NC, USA &#38;#169; 2001 ACM ISBN 1-58113-329-4/01 0002... $5.00 The solution is two-fold: 
explicit agreement within the faculty of the objectives for CS1, and assessment procedures that genuinely 
reflect those objectives. This paper examines both parts of the solution. 2 The Unstated Objectives 
Traditionally, the explicit objectives of first year programming subjects are stated in terms of technology. 
For example, the sequence of chapter headings of a popular Java textbook is "Methods, Statements and 
Expressions", "Program Design with Methods and Graphics", "Selection Structures", "Repetition Structures", 
"Arrays and Vectors", etc. The subject objectives must refer to the technology, but in this paper, we 
argue that the subject objectives must go further. We elaborate in the remainder of this section. 2.1 
Mastery versus Development The objectives should address explicitly whether the subject attempts to address 
student "mastery" or "development" [5]. Mastery objectives are concerned with simple learning tasks on 
which students are expected to demonstrate a uniformly high level of performance. Developmental objectives 
are concerned with complex outcomes toward which students can be expected to show varying degrees of 
progress. CS1 teachers, faced with the day-to-day reality of how little their students know, are frequently 
drawn to address mastery. Many "down stream" teachers are more interested in the skills of the best students, 
and so are drawn to development. At some point of a degree program, students must be challenged by subjects 
that emphasise development. The question is whether this needs to happen from the very first subject. 
Both options are justifiable, depending upon the institutional context. The failure is that most faculties 
do not have an explicit policy on this issue. Debate emphasises technology --whether or not students 
should be taught a particular programming construct in CS1 --when the real difference of opinion is at 
this more fundamental level of mastery versus development.   2.2 Student Cognitive Development When 
only technological objectives are explicit, the implicit assumption is that students are "blank slates" 
upon which the technology is to be imprinted. Since the 1950s, however, educational theorists have argued 
that students acquire cognitive skills by passing through a series of stages. These stages are known 
as Bloom's Taxonomy of Educational Objectives [2]. The taxonomy contains six levels. Each successive 
level depends upon behaviours acquired at the earlier levels. When describing each of these levels, it 
is common practise to include a list of words illustrating the behaviours that a student is expected 
to exhibit. The following description is generic, not specific to computer programming. The levels, from 
lowest to highest are: Knowledge: the recall of specifics and universals, the recall of methods, and 
processes, or the recall of a pattern, structure or setting. A word list of activities is .,. arrange, 
define, duplicate, label list, memorise, name, order, recognise, relate, recall, repeat, reproduce, state. 
Comprehension: a type of understanding when the individual knows what is being communicated and can make 
use of the materials or idea being communicated without necessarily relating it to other material or 
seeing its fullest implications ... Class~y, describe, discuss, explain, express, ident~y, indicate, 
locate, recognise, report, restate, review, select, summarise, translate. Application: is the use of 
abstractions in particular and concrete situations and may include general ideas, rules of procedures, 
generalised methods, technical principles, ideas, and theories that must be remembered and applied ... 
Calculate, choose, demonstrate, dramatise, employ, illustrate, interpret, operate, practice, prepare, 
schedule, sketch, solve, use, write. Analysis: emphasises the breakdown of the material into its constituent 
parts and the detection of the relationships of the parts and of the way they are organised ... Appraise, 
calculate, categorise, compare, contrast, criticise, differentiate, discriminate, distinguish, examine, 
experiment, question, test. Synthesis: the putting together of elements and parts so as to form a whole 
... Arrange, assemble, collect, compose, construct, create, design, develop, formulate, manage, organise, 
plan, prepare, propose, set up, write. Evaluation: the making of judgments about the value, for some 
purpose, of ideas, works, solutions, methods and materials. It involves the use of criteria as well as 
standards for appraising the extent of which particulars are accurate, effective, economical, or satisfying 
... appraise, argue, assess, attach, choose, compare, conclude, defend, estimate, evaluate, judge, predict, 
rate, score, select, support, value. Many computer scientists are initially uncomfortable with the taxonomy, 
as it is from the social sciences, and so lacks the unambiguous, precise, context-free meaning, that 
computer scientists expect within their own technological domain. Teaching, however, is a social process, 
and so computer science teachers need to come to terms with such vague concepts as this taxonomy. I take 
a pragmatic view of the taxonomy, rather than a strict ideological view. The taxonomy is merely a conceptual 
tool. It is a framework for lesson design and assessment design. Perhaps most importantly, it is a framework 
for discourse with peers. The taxonomy is a useful tool for structuring debate with a faculty, as part 
of curriculum design. Given the context-sensitive nature of the taxonomy, not all faculties will take 
the taxonomy and map it onto programming in the same way. For example, Buck and Stucki [3] map the taxonomy 
in a much more ambitious fashion than I do, within my institutional context. The next section will elaborate 
upon that point. 3 Objectives Within the Author's Context Objectives and assessment techniques do not 
stand alone. The correct choice depends upon the innate abilities of the class, their commitment, the 
class size, teaching resources, and the goals of the institution. In the state of New South Wales, all 
high school graduates are rated on a numeric system, from zero to one hundred, with the median students 
scoring fifty and the best students scoring one hundred. Eighty percent of entrants into my CS 1 subject 
have come immediately from high school, with a rating from sixty to eighty. The class size is over 400. 
In addition to a two-hour weekly lecture, each student is allocated a one-hour weekly laboratory session, 
with twenty students per lab. The major resource constraint is quality tutors to take these lab sessions. 
Most graduate students have part time jobs within industry, at better pay rates than that for a tutor. 
I have resorted to using senior undergraduates in the labs, but the University wisely restricts the assessment 
responsibilities of such tutors. They cannot be expected to exhibit sophisticated judgement. Any marking 
they perform must take the form of checking that a well specified task has been performed. My School 
aims to graduate students who will go on to be professional programmers and systems analysts. The major 
degrees of the School are accredited by the Australian Computer Society. 3.1 Technological Objectives 
Like most institutions, my School is explicit on the technological objectives of the first semester programming 
subject. The emphasis is on algorithms. Students are expected to demonstrate fac'dity with the classic, 
array searching and O0N 2) sorting algorithms. The algorithms are encoded in Java, but are consistent 
with a traditional Pascal approach to CS1. As policy, we teach "Pascal in Java". 3.2 Mastery versus 
Development Given my institutional context, I advocate mastery over development, in this the very first 
subject of a sequence of subjects that teach programming. Students who pass this subject have demonstrated 
a natural aptitude for programming, and acquired basic skills. That foundation can then be built upon 
in subsequent subjects. When setting a program as an assessment task, it is easy to set a task that appears 
ambitious. However, the marking scheme truly defines the level of difficulty. It is common practise to 
design marking schemes that reward the student with a passing mark, if the student's submission 1) is 
syntactically correct, 2) follows indentation conventions, 3) is copiously commented, and 4) correctly 
implements a small subset of the functionality of the entire task set.  3.3 Bloom and Student Cognitive 
Development There is little to be gained by attempting to pigeon hole every specific aspect of programming 
into a specific level of Bloom's taxonomy. I merely map the taxonomy onto elementary programming in a 
broad-brushed fashion. The lower two "knowledge" and "comprehension" levels emphasise the skill of reading 
and comprehending code. The intermediate "application" and "analysis" levels emphasise the writing of 
small fragments of code, but within a well-defined context. The upper two "synthesis" and "evaluation" 
levels emphasise the writing of complete non-trivial programs. The objective of my CS1 is to address 
the first two levels of the taxonomy, and no more. Traditionally, the teaching of introductory programming 
has emphasised the writing of extensive code from the earliest possible stage. The week-by-week breakdown 
of topics is explicitly organised according to the technology, not the cognitive development of the student. 
Bloom's taxonomy suggests a different approach --students should first be explicitly assessed on their 
knowledge and comprehension of all the basic programming constructs before they are expected to write 
original code. On first consideration, this may not seem different from what is already done. For example, 
many textbooks use multiple choice and short answer questions at the end of each topic, to give students 
some feedback on whether they have absorbed the material presented. However, the end-of-chapter exercises 
in those textbooks require students to write original code. I propose that we go further: students shouM 
not write any original code at any stage of the first semester programming subject. Draconian as this 
may seer it sends an unambiguous message about student capabilities to any teacher "down stream" ... 
there can be no delusion that, after one semester, students are accomplished programmers. The CS 1 objectives 
are clear. Objectives and Objective Assessment Educational theorists break assessment techniques into 
two broad categories, "subjective" and "obj~tive". Within the context of programming, subjective assessment 
refers to the writing of non-trivial code, probably an entire program. Under such circumstances, there 
is no "truth in sentencing". Students soon learn that they are part of a game, where we pretend to teach 
and they pretend to learn. Plagiarism becomes common. What point is there to setting a demanding assignment 
if students routinely plagiadse? Multiple choice questions (MCQs) are the most widely used of objective 
testing techniques. For many computing teachers, MCQs are the assessment technique of last resort. MCQs 
are the refuge of academics who are over-whelmed by student numbers, and also those academics who are 
merely lazy or incompetent. To admit to using MCQs is to invite disapproval from one's peers, without 
those peers feeling any need to actually look at the questions themselves. Most MCQs are indeed poorly 
designed. Consider the foilowing example of a poor MCQ, taken from a formative assessment task at an 
Australian university, where the programming language was C: When passing an array to a function, why 
is it necessary to pass the length of the array as well? 1. Because the function cannot deduce the size 
of the array. 2. Because this tutorial suggests this is done. 3. Because most programmers do it that 
way. 4. All of the above.  This is a poor MCQ because it does not require any knowledge of programming, 
simply the application of innate cunning. Choices 2 and 3 are trite "game playing" [1]. Eliminating those 
two choices also eliminates the fourth choice, leaving choice 1 as the only remaining option. (We take 
this opportunity to introduce some terminology. Incorrect choices, such as 2, 3, and 4 above, are referred 
to as distractors.) Multiple choice questions enjoy a better reputation among education researchers. 
Much of that literature is devoted to simple rules for assessing the quality of MCQs. An example of such 
a rule is all distractors should be plausible to the student who has not achieved the learning objective 
being tested [5]. MCQs have a poor reputation among computing lecturers because most computing lecturers 
have not studied these rules. It is tempting to believe that a single multiple choice question must be 
intrinsically easier than asking students to write a short piece of code. Educational research, however, 
suggests otherwise [4].   5 Assessing "Knowledge" and "Comprehension" (d) for (int i=last; i>first; 
i--) The following MCQ is appropriate for assessing a student's knowledge of the "for" loop, midway through 
the first semester: What output is produced by the following Java code: for (int i=O ; i<=3 ; i+=2) System.out.print(i 
+ .... ); a)012 b)0123 c)0 2 d)023 e)024 The above MCQ tests the students knowledge of a) the basic 
principle of a for loop, b) the += operator, and c) the student's grasp of exactly when the for loop 
will terminate. All of the distractors would seem plausible to someone lacking the knowledge being tested. 
The above multiple choice question is appropriate in the first half of semester. In the second half, 
the emphasis shifts to knowledge and comprehension of algorithms. MCQs are an effective means of assessing 
the student's comprhension of algorithms. Consider the following MCQ, based upon "skeleton" code for 
a Java method "maxPos": public static ira maxPos(int[] y, int first, int last) { /* Returns the position 
of the maximum element in the * subsection of the array "y", starting at position "first" * and ending 
at position "last".  */ int bestSoFar = first; *** missing code goes here *** return bestSoFar; } if 
( y[i] > y[bestSoFar] ) bestSoFar = i; (e) for (int i=first+l; i<=last; i--) if( y[i] > bestSoFar ) bestSoFar 
= i; The distractors of this MCQ take advantage of the most common errors made by students, as recommended 
in the literature [5]. For example, some of the distractors exploit the common confusion between an array 
index ("bestSoFar") and the contents of the array element indexed by that variable ("y[bestSoFar]"). 
Students complain that MCQs are too brutal -they are either right or wrong -that the difference between 
the correct answer and some of the incorrect distractors is subtle. The rejoinder is, "When you eventually 
write code, the computer will be no less brutal on your subtle errors".  6 Debugging Skills In assessing 
a student's preparedness for one very important aspect of programming -debugging -quality MCQs are better 
than "written" exams. A quafity MCQ assesses the student's readiness for inspecting their own code carefully, 
to find typing errors. Careful reading is only one aspect of debugging, but in written exams, profoundly 
incorrect code can still attract a passing mark, and no aspect of debugging is assessed. The previous 
MCQ, concerning "maxPos", implicitly tested nascent debugging skills. Consider the following MCQ, aimed 
explicitly at testing nascent debugging skills: The following is a correct implementation of Bubble Sort: 
Which of the foUowing is the missing code ffi'om "maxPos"? The loop should run "backwards". That is, 
the code should search the array from the high subscripts to the low subscripts. Given that, the missing 
code for "maxPos" is: (a) for (int i=last; i>first; i-) if( y[i] < y[bestSoFar] ) bestSoFar = i;  (b) 
for (int i=first+l; i<=last; i--)  if ( y[i] < y[bestSoFar] ) bestSoFar = i; (c) for (int i=last; i>first; 
i--) if( y[i] < bestSoFar ) bestSoFar = i: public static void BubbleSort(int[] a) { for (int right=a.length-1; 
fight>=0 ; right--) //fine 1 for (int i=0; i<fight; i++) //line 2 if ( a[i] > a[i+l] ) { //line 3 int 
temp = a[i]; a[i] = a[i+l]; a[i+l] = temp; } Which of the following changes to the above code does NOT 
produce ArraylndexOutOfBoundsException: a) In line 1, "a.length" instead of"a.length-l". b) In line 1, 
"right>O" instead of"right>=0 ''. c) In line 2, "i<=right " instead of"i<right". d) In line 3, "a[i-1] 
> a[i]" instead of"a[i] > a[i+l]". 7 Practical Work As children, when we first learnt to read, we also 
learnt to write. However, our writing skills lagged well behind our reading ability. When we laboriously 
copied our first few characters onto the page, the teaching objective was not just to begin the development 
of writing motor skills, but also to reinforce the development of our reading skills, by involving the 
whole body. I apply a similar philosophy in the laboratory work of CSI. Students do not write original 
code. I rewrite into pseudo- code the Java-encoded algorithms from lectures. I give the pseudo-code to 
the students in the labs. Their task is to recreate the Java in lectures from that pseudo-code. Apart 
from reinforcing the lecture material, they are introduced to an editor, and encounter the joys of syntax 
error messages.  8 Class Experiences I have tried this approach to CS1 twice. The first time, I prepared 
forty MCQs, in the above style, for a two-hour final exam (3 minutes per question). When it was first 
announced in class that the exam would be entirely MCQs, the initial relief and immediate confidence 
of students was palpable; for the first time in weeks there were more smiles than frowns in the theatre. 
On emerging from the exam room, however, the students were tired and less confident. One student spoke 
for many, when she said, "I never thought I'd say that the hardest exam I've ever done was a multiple 
choice exam." That student gained a credit grade in the exam (ie. 65-75%). Over 60% of students failed. 
After a long condemnation of MCQs, Bigg's [1] final word was "Unfortunately, they are convenient", (his 
italics). He was undoubtedly correct about poor quality MCQs, but there was no short term convenience 
in the forty MCQs I wrote for my exam. Devising subtle distinctions for the distractors was especially 
time consuming. The writing of the entire exam paper was my dominant work activity for over two weeks 
... a production rate worse than one MCQ an hour. Quality MCQs do pay off, however, in the long term, 
because quality MCQs can be recycled. The second time I ran CS1 in this new form, I taught the students 
who had failed the first time. The exam paper from the first time was available on the web, and was used 
extensively for tutorial exercises. (While many academics complain that students equate "assessment" 
with "syllabus", this reality of student nature can be turned to our advantage.) At the end of semester, 
most exam MCQs were simple variations on the questions from the previous exam. In some cases, the choices 
were placed in a different order. In other cases, the question was changed in a simple way. For example, 
"rnaxPos" in the above MCQ might be replaced with "minPos". Over 40% of students failed a second time. 
These high failure rates might at first seem to indicate that my approach has failed. The students are 
not required to write code, and yet the majority failed. The principal goal is not to design an easy 
subject with a low failure rate. The goal is to have clear objectives and "truth in sentencing". If the 
faculty regarded the failure rate as too high, then any subsequent review of the subject must focus upon 
the pool of MCQs. Which MCQs do students find hardest? Why? Do they indicate that a change of teaching 
emphasis is required? Any decision to delete MCQs from the pool is an unambiguous revision of the subject 
objectives.  9 Conclusion CS1 cannot produce accomplished programmers. That is the task of an entire 
sequence of programming subjects. What is crucial is that the first subject in that sequence must have 
clear objectives and an assessment system that tests those objectives. In addition to the technological 
objectives, it should be clear whether the subject aims to bring all pupils to a uniform level of performance 
on the minimum essentials, or whether the aim is to assist pupils to achieve their maximum development. 
The cognitive development of the student should also be addressed in the objectives, in terms of Bloom's 
taxonomy. In my CS1, the objectives are 1) to teach iterative processes on arrays via the classic sorting 
and searching algorithms, 2) to bring all pupils to a uniform level of performance on the minimum essentials, 
and 3) to only address the lower "knowledge" and "comprehension" levels of Bloom's taxonomy. From those 
objectives, the use of multiple choice questions (MCQs) follows naturally. To anyone objecting to MCQs, 
I say, "is it really the MCQs that trouble you, or is it the objectives?"  References [1] Biggs, J. 
Teaching for Quality Learning at University. England: Open University press, 1999. [2] Bloom, B.S. (Ed.). 
Taxonomy of Educational Objectives Handbook 1: Cognitive Domain. New York: Lort~aan, Green &#38; Co, 
1956. [3] Buck, D. and Stucki, D. Design Early Considered Harmful: Graduated Exposure to Complexity and 
Structure Based on Levels of Cognitive Development. Proceedings SIGCSE 2000 (March 2000), ACM Press, 
75-79. [4] Ebel, R.L and Frisbie, D.A. Essentials of Educational Measurement. 4oaedition. New Jersey: 
Prentice-Hall, 1986. [5] Gronlund, N and Linn, R (1990). Measurement and Evaluation in Teaching. New 
York: MacMillan, 1990.   
			