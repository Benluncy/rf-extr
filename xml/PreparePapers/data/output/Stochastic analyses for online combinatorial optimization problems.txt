
 Stochastic Analyses for Online Combinatorial Optimization Problems Naveen Garg* Anupam Gupta Stefano 
Leonardi Piotr Sankowski Abstract In this paper, we study online algorithms when the in­put is not chosen 
adversarially, but consists of draws from some given probability distribution. While this model has been 
studied for online problems like pag­ing and k-server, it is not known how to beat the T(log n) bound 
for online Steiner tree if at each time instant, the demand vertex is a uniformly random ver­tex from 
the graph. For the online Steiner tree prob­lem, we show that if each demand vertex is an indepen­dent 
draw from some probability distribution p : V . [0, 1], a variant of the natural greedy algorithm achieves 
E.[A(.)]/E.[OPT(.)] = O(1); moreover, this result can be extended to some other subadditive problems. 
Both assumptions that the input sequence consists of independent draws from p, and that p is known to 
the algorithm are both essential; we show (almost) loga­rithmic lower bounds if either assumption is 
violated. Moreover, we give preliminary results on extending the Steiner tree results above to the related 
expected ra­tio measure E.[A(.)/OPT(.)]. Finally, we use these ideas to give an average-case analysis 
of the Universal TSP problem. 1 Introduction The notion of competitive analysis is perhaps the central 
notion of online algorithms [41, 26, 30]: given an online algorithm A (perhaps a randomized one), the 
*Indian Institute of Technology, New Delhi, India. Part of this work was done when visiting the Dipartimento 
di Informatica e Sistemistica, University of Rome La Sapienza , and the Max­Planck-Institut f¨ur Informatik, 
Saarbr¨ucken, Germany. Computer Science Department, Carnegie Mellon University, Pittsburgh, PA 15213. 
Research was partly supported by NSF awards CCF-0448095 and CCF-0729022, and an Alfred P. Sloan Fellowship. 
Part of this work was done when visiting the Dipartimento di Informatica e Sistemistica, University of 
Rome La Sapienza . Dipartimento di Informatica e Sistemistica, University of Rome La Sapienza , Via Salaria 
113, 00198 Rome, Italy. This work was partially supported by the EU within the 6th Framework Programme 
under contract no. 001907 Dynamically Evolving, Large Scale Information Systems (DELIS) competitive ratio 
is de.ned as Er[A(.,r)] (1.1) max. , OPT(.) where r is the set of random coins .ipped by the algorithm, 
the maximum is taken over all inputs ., and OPT(.) is the optimal solution value on the input .. This 
idea of comparing the performance of an online algorithm (which knows nothing about the future) to the 
optimal solution built in hindsight has led to crisp and clean problems, and strong upper and lower bounds 
on the competitive ratio are known for most problems of interest. However, while the strict de.nition 
of competitive ratio has led to much progress, it has had its drawbacks. The results in the online model 
are often unduly pessimistic, and the strict de.nition of competitive ratio does not allow us to make 
.ne-grained distinctions between competing algorithms. One of the reasons for the introduction of compet­itive 
analysis was that classical online problems like list-update, paging and k-server were easy when the 
inputs were drawn from a known probability distri­bution, and the hope was to extend our understand­ing 
when we did not know the input distribution pre­cisely. (See [6, 13, 22] or Section 1.2 for many refer­ 
ences to weaken the competitive analysis framework and strike a happy median between full-information 
and no­information.) However, for some combinatorial opti­mization problems, we do not understand even 
the case when the inputs are independently drawn from a proba­bility distribution p given as input. An 
excellent exam­ple is the online Steiner tree problem, which has been studied both in the o.ine (see 
[37] and the references therein) and the online [38, 20, 3] case. In this work, we seek answers to the 
following question: If the online Steiner tree requests are vertices of a graph drawn uni­formly at random, 
can we do better than the T(log n) online greedy algorithm? (E.g., one may be stream­ing a video over 
a network to customers arriving from a known distribution; when a customer arrives she has to be served 
immediately.) In general, the goal is to for­mally study the interplay between stochastic informa­tion 
and online algorithms for combinatorial optimiza­tion problems in general. (Individual problems along 
these lines have indeed been studied before [25, 31, 32]: see Section 1.2 for similarities and di.erences.) 
  Our Model. In order to state the questions and an­swers, let us formalize the model in which we work. 
We assume that requests are drawn from the universe U, and there is a probability distribution p : U 
. [0, 1] over the space of requests.1 The adversary generates the input sequence . = .1,.2,..., by deciding 
on the length k of the input sequence, and then drawing k re­quests independently and identically from 
the probabil­ity distribution p. The adversary is allowed to choose any distribution p over the space 
of requests, but the online algorithm are allowed to sample from this distri­bution. Given I . Z, we 
use p£ to denote the set U£ of length-I sequences from U endowed with the natural product measure, and 
hence . . p£ will imply that . is chosen by taking I i.i.d. samples from p. Given an online algorithm 
ALG for problem ., the ratio-of-expectations (RoE) is E..pk,r[ALG(., r)] (1.2) RoE(ALG) = maxmax . pk 
E..pk [OPT(.)] Here r are the random coins of the algorithm. Hence, given any sequence length k (unknown 
to the algo­rithm), this quantity measures the expected cost of the algorithm over length k sequences, 
compared to the ex­pected optimal cost for these sequences. This is a spe­cial case of the di.use adversary 
of [29], and has been recently studied in the context of several scheduling problems (see, e.g., [39, 
33]). Also, note the syntac­tic/philosophical similarity to the Max/Max objective of [5].) 1.1 Our Results 
We show this result for the stochas­tic online Steiner tree problem: Theorem 1.1. (Steiner Tree RoE) 
There is an on­line algorithm A that takes as input a graph G =(V, E) and probabilities {pv}v.V so that 
if at each time step the input vertex is v with probability pv (i.e., the input consists of i.i.d. draws 
from p), then RoE(A)= O(1). These algorithms are related to work done on the may­becast problem, and 
work in o.ine stochastic problems. As for those cases, we extend the Steiner tree result to other problems 
as well. Theorem 1.2. (Extending to Other Problems) There are online algorithms for Steiner Forest, Facility 
Location and Vertex Cover with RoE = O(1). 1In the following, we use requests, demands, and clients 
interchangeably. We show that some access to the distribution, and also the independence is necessary: 
if the input consists of i.i.d. draws from a .xed but unknown distribution, or if the draws are not independent 
but drawn from some log n given Markov chain, then there is an O( ) lower log log n bound for Steiner 
tree. We also give initial results in the model where we are given the input length k and distribution 
p (given only as a black-box), and we want to minimize the expected ratio EoR(A)= E..pk [A(.)/OPT(.)]. 
Theorem 1.3. There is an EoR = O(log log n)­algorithm for the Steiner tree problem in the above model. 
This result, which appears in Section 5, uses fairly di.erent techniques from the results above: it is 
an interesting question to extend this result to match those in the RoE setting. Finally, we show how 
to get results for the so-called Universal TSP problem, where we have to give a single permutation t 
such that if vertices arrive independently of each other (each with probability pv), then the expected 
length of the tour induced by t on this random set is O(1) times the expected length of the optimal tour. 
1.2 Related Work For books and surveys on online algorithms, see, e.g., [6, 13, 1, 2], and the many refer­ 
ences therein. There have been many prior attempts to relax the notion of competitive analysis: e.g., 
us­ing access graphs and working sets [7, 11, 12, 23, 4] or Markov distributions [27] to capture locality 
of ref­ erence in paging-type problems, comparative analysis to compare against the best algorithm in 
some given class [29], resource augmentation to level the playing .eld by giving the online algorithm 
more power than the o.ine one [24, 34, 43], or the idea of statistical ad­versaries [35, 9] (where the 
online sequences have cer­ tain statistical properties) or di.use adversaries [29, 44] (where the sequences 
are generated from a small set of distributions known to the algorithm). In most of these cases, it was 
easy to handle the case when each request is an i.i.d. sample from a given distribution, and the challenge 
was to obtain a happy median between this simple case and the unbounded adversary. In this pa­per, we 
consider problems which are interesting even with i.i.d. draws from a .xed distribution. A di.erent approach 
was the Max/Max objective of Ben-David and Borodin [5], which has similarities to our RoE objective. 
While stochastic optimization problems have a long history in the operations research community, they 
have been studied in the algorithms community only for a few years; see, e.g., [21, 36, 19, 40]. The 
setting sounds similar to the online case: the input is gradually revealed over k stages, and partial 
decisions are made as time goes on however, the goal is to have a feasible solution at the end and only 
the rising costs force us to make decisions early in the game, whereas in the online case we need a feasible 
solution after every pre.x but do not have to worry about the costs increasing. Also, while the o.ine 
work sought to enlarge the scope of inputs instead of one input, we wanted to handle a class of inputs 
on the average whereas this work on stochastic online algorithms seeks to to restrict the power of the 
adversary: instead of having the adversary hand-pick the demands in the input stream, we now posit that 
these are drawn at random.  Some of these lines of work are especially close to ours: Karger and Minko. 
[25] (see also [16]) give an O(1) approximation for the maybecast problem: given a graph and probabilities 
pv for each vertex, one needs to .x a path Pv for each vertex v of the graph to the root, such that if 
each vertex is added to a set S with probability pv (independently of all other vertices), the cost ES[c(.v.SPv)] 
is minimized. A minor di.erence between our problem and theirs is that vertices arrive independently 
in their case and it is a one-shot problem, whereas we sample a single random vertex each time, and the 
process goes on for some unknown number of rounds. Another di.erence is that they seek an approximation 
algorithm and hence compare to the optimal expected value achievable by a union of paths .xed before 
the random set S is chosen, whereas we need to compare our solution to the best Steiner tree on S .{root} 
itself. (These two quantities turn out to be the close, which is part of what we have to prove.) The 
papers of Meyerson [31] (for facility location) and Meyerson, Munagala and Plotkin [32] (for the access 
network design problem) consider the random­permutation model . Here the adversary picks a set S of demand 
points (kept secret from the algorithm); the points in S are then presented to the online algorithm in 
random order. In these papers, the competitive ratio is O(log n) in the purely adversarial case, but 
with the random permutation the expected cost of the algorithm becomes O(1) × OPT(S). These results are 
quite strong, since in a loose sense, the adversary is choosing the distribution pS which is uniformly 
spread over its support S, and their algorithms gets their performance guarantee without access to this 
distribution. However, this generality of the de.nition is also its shortcoming: log n in the full version 
of the paper, we give an O( ) log log n lower bound for Steiner tree in this model. Finally, some recent 
work in online scheduling and paging tries to extend work in the RoE model to the expected-ratio model 
EoR (from Section 5) see, e.g., [39, 42, 33]. 2 Online Steiner Tree In this section we develop algorithms 
that are competi­tive under the ratio-of-expectations (RoE) measure (1.2) for the the online Steiner 
tree problem. We show that while the standard greedy online algorithm performs badly in the stochastic 
setting, we can devise algorithms that obtain RoE = O(1) for the problem. In the next section, we extend 
these ideas to general subadditive problems that admit certain kinds of good algorithms. As a warm-up 
to illustrate the concepts we will use, we consider the Steiner tree problem; our results would be applicable 
to a much wider variety of problems. In the online Steiner tree problem, we are given a graph G =(V, 
E) with and a root vertex r and edge costs/lengths c : E . R=0; w.l.o.g., we assume that the edge costs 
satisfy the triangle inequality. The input clients U are the vertices V of the graph, and hence the adversary 
provides a sequence of vertices (possibly with repetitions) v1,v2,... from V . At each point in time 
t, the online algorithm must maintain a subgraph St which connects the root r and all vertices {v1,...,vt} 
seen so far in the input sequence; note that the decisions of the online algorithm are irrevocable, and 
hence St . St+1. 2.1 Online Steiner: Known Sequence Length We .rst suppose we know the length k of the 
input sequence: . A1. Choose a set of vertices D by drawing from the distribution p independently k 
times. A2. Construct a .-approximate Steiner tree TM over the set D .{r}, and set S0 = TM . A3. When 
the actual input sequence of k vertices ar­rives, run the greedy algorithm on this sequence namely, connect 
the tth input vertex vt to the clos­est vertex in the subtree St-1. The algorithm is very similar to 
those used for o.ine two-stage stochastic problems. Although the proofs from the o.ine stochastic case 
cannot be used here directly, since here we have to build the tree in an online fashion as the vertices 
arrive, we can give the following guarantee: Theorem 2.1. (Steiner Tree for Fixed k) The ratio-of-expectations 
of the above algorithm is (2 + .). Proof. Let D be the sequence of k dummy requests in the anticipatory 
solution, and let R be the set of k real requests arriving online: recall that both are identically distributed. 
Note also that in the Steiner tree problem, repetitions are irrelevant, and hence we can associate both 
D and R with the set of vertices contained in these sequences. It is clear that the cost of the optimal 
Steiner tree on D has expected cost E[OPT(R)], and hence the cost of the anticipatory solution is at 
most .E[OP T (R)].  Now, to consider the cost of connections added in Step A3: since we run the greedy 
algorithm, if the ith vertex of R is some node v, then the cost incurred is upper bounded by d(v, D .{r}), 
the distance from v to the nearest vertex in D .{r}. Since each node is an i.i.d. sample from p, by linearity 
of expectations, the expected cost of the real solution is at most k × Ev. p,D. pk [d(v, D .{r})]. In 
turn, this expected cost does not decrease if we were to take a smaller set D" . pk- 1 of k - 1 samples. 
Finally, the expression Ev. p,DI. pk-1 [d(v, D" .{r})] is upper bounded by the expression EDII. pk [ 
1 ·MST (D"" . k {r})]. To see this, we can imagine picking D" and v by picking a set D"" of k samples, 
and randomly choosing one of them to be the vertex v; moreover, for each vertex in the MST, we assign 
it its parent edge, and note that the distance to its closest vertex is bounded by the length of this 
edge. Hence the cost of the greedy connections is at most k × Ev. p,D. pk [d(v, D)] = k × Ev. p,DI. pk-1 
[d(v, D" )] = k × EDII. pk [ 1 · MST (D"" )] = ED. pk [MST (D)]. k And hence the cost of the greedy connections 
is at most the cost of the MST on the anticipatory set. Since the MST is a 2-approximation to the Steiner 
tree, this is at most twice the optimum cost, and we get the (2 + .)­approximation, as claimed. As in 
earlier proofs of this form [18, 19], we can optimize the number of samples taken in Step A1 depending 
on the approximation guarantee .; however, we skip these minor optimizations in the interests of clarity. 
Moreover, since we are in the online setting, we could conceivably compute the optimal Steiner tree to 
get . = 1. Lazy Version: We can change the algorithm so that the edges of TM are bought only when they 
are needed for the actual demands: in that case the algorithm is very similar to that of [25]. ( Section 
1.2 gives a detailed comparison of that work and ours: the main di.erence is that while we compare the 
cost of the tree built to ER[OPT(R)], the proof in [25] compares it to the best performance achievable 
by a set of .xed paths.) Also, if we use a light approximate-shortest­path tree (LAST) [28] in this lazy 
version, we can ensure that no single demand vertex has to use a path that is much longer than his shortest 
path to the currently­built network. Details of these extensions will be in the .nal version.  2.2 Online 
Steiner: Unknown Sequence Length In this section, we show how to remove the assump­tion that we know 
the number of points in the input sequence. The .rst idea one might try is to guess-and­verify : guess 
that the number of points in the input sequence, and if that is incorrect, double the guess and try again. 
This turns out to be a bad idea (with our standard example of the line being a counterexample). So instead 
of scaling on the input length, we scale on the cost of the anticipatory solution we build each time 
we build an anticipatory solution costing about twice as much as before, and wait until we see as many 
vertices as in that solution. If we allow ourselves to perform non-polynomial computations, doing this 
is quite easy: but in the following we will show how to achieve this with polynomial amounts of computaiton. 
For the following, let us de.ne Z£ = E.. pd[OPT(.)] to be the cost of the optimal Steiner tree on a random 
sequence . of I i.i.d. draws from the distribution p. Sub-additivity immediately implies that for any 
c = 1, Zc£ = cZ£. For clarity of argument, we distinguish between sequences ., the multiset of vertices 
in it (denoted by Mset[.]), and the set of distinct vertices contained in it (denoted by Set[.]). Using 
arguments used in Theorem 2.1, we know that it is enough to buy a tree on some dummy set D at the beginning 
(such that r . D), and then connect each of the real clients in Set[.] to its closest point in this tree. 
In fact, the theorem implies that (2.3) Z"= min (c(MST (D£)) + E.. pd[d(x, D£)]) £ Dd.(Vd ) x. Set[.] 
r.Dd is at least Z£ (since it is the cost of a tree that connects I random points), and at most 4Z£ (by 
Theorem 2.1). Moreover, if we have a sequence length I in mind, while one can imagine complicated online 
algorithms that buy edges at various points of time the analysis shows that it is enough to choose from 
one of 2n- 1 distinct policies, each de.ned by the set D£ . V such that r . D£. Lemma 2.1. Given a length 
I, the graph G =(V, E), and black-box access to the distribution p, there is an - 1 algorithm that runs 
in time I×poly(n,C, log d- 1), and with probability 1-d outputs a set D £ which contains the root r, 
and which is an 2.92 + C-approximate minimizer of (2.3). Proof. To .nd a minimizer of (2.3), instead 
of choosing the input string from the distribution p£ , sample N sequences .1,.2,...,.N from p£ (for 
a value of N to be .xed later), and consider the set D that c-minimizes the sample average  (2.4) 1 
N min c(MST (D)) + d(x, D). D.(V ) N d i=1x.Set[.i] First, we claim we can .nd a solution D that approx­imately 
minimizes (2.4): note that we merely have an instance of the so-called single-source rent-or-buy prob­lem. 
For this problem, we have a deterministic 2.92­approximation [10]. Secondly, we want to claim that this 
minimizer for (2.4) is also a minimizer of (2.3) with high prob­ ability. This is not true in general, 
but if we repeat the above sampling process t = C-1 log d-1 times and take the minimizer that results 
in the lowest value of (2.4) among these t runs, we can use a sample average the­orem of Charikar et 
al. [8, Theorem 2] to argue that for a suitable value of N, this minimizer will also be an 2.92 + C-minimizer 
of the expression (2.3) with proba­ bility 1-d. Let us call this set DD£. (Note that we do not claim 
that the values of the expressions (2.3) and (2.4) are close for this set, just that this approximate 
mini­mizer for the latter is also one for the former.) It remains to .gure out a suitable value of N: 
in our case, there are 2n-1 possible choices for D. Also, for each choice D, we want the di.erence in 
the objec­tive function value (2.3) between picking D and pick­ing the empty set Ø to be at most . × 
c(MST (D)); in this case it is clear that this value . = min{I, n}. Plugging these values into [8, Theorem 
2], it su.ces to choose N to be T(C-4.2t log(2n-1) log d-1, which is poly(n, C-1 , log d-1). Hence, if 
we can sample each se­quence in time T (I), we can .nd a 2.92 + C-minimizer for (2.3) in time T (I) × 
poly(n, C-1 , log d-1) with prob­ability at least 1 - d. Since T (I) = I, the claim follows: in the full 
version of the paper, we show how to reduce the time to get one sample from I-length strings of a product 
distribution. Lemma 2.2. Given a length I, the graph G =(V, E) and probabilities pv of each vertex arriving, 
there is a poly(n, I, log d-1)-time algorithm that outputs an esti­mate ZD£ . [1, 11.68 + 4C] × Z£ with 
probability 1 - d. Proof. To obtain a 11.68 + 4C-approximation to Z£, it enough to .nd a 2.92 + C-approximate 
minimizer DD£ for the expression Z " in (2.3), and this is done in £ Lemma 2.1. Now, it remains to calculate 
the value of (2.3) for this set DD£: this can be easily done given that we know the probabilities pv 
and can calculate the probability pv =1 - (1 - pv)£ that v appears in a sequence of length I, and then 
use linearity of B1. Set i = 0 and S0 = Ø. We process the input clients {v1,v2,...} one by one. B2. 
Let vk be the k-th client and let Sk := Sk-1. B3. If k>ti then goto Step B5 B4. Connect the k-th input 
vertex vk greedily to the closest vertex in the tree Sk. B5. Set i . i + 1. B6. Construct a .-approximate 
Steiner tree Tti over the set DDti . Set Sk := Sk . Tti . Goto Step B4. Figure 1: Algorithm for Unknown 
Sequence Lengths expectation. (Note that this is the .rst time we used the actual values pv.) This is 
our estimate ZD£. In order to compute the estimate ZD£, when it is 1 needed, we use the above lemma with 
d =. (£+1)£ log n 8 11 Note, that = . Hence, we get: £=1 £(£+1) log n log n Fact 2.1. For the setting 
d-1 =(I + 1)I log n, all estimates ZD£ are correct with probability 1 - 1/ log n. Now we de.ne t0 = 0, 
t1 = 1, and ti to be the D= 2i D smallest value I such that Z£ · Z1. The following fact also uses subadditivity: 
Fact 2.2. c(MST (DDti )) = ZDti = ZD1 +2iZD1 = ZD1 + 2 D= 3 D Zti-1 Zti-1 . We run the algorithm from 
Figure 1 in parallel with the standard greedy algorithm 2: The following theorem is almost immediate 
from the discussion above. Theorem 2.2. (Steiner Tree for Unknown k) Suppose the .nal length of the input 
sequence is k: the expected total cost incurred by the above algorithm is O(1) × Z£. D Proof. When all 
estimates Z£ fall into the C bound we can bound the cost of the algorithm in the following way. Let the 
.nal length k . (ti*-1,ti* ]. For any j = i* , the expected cost for the input elements at positions 
(tj-1,tj ] is at most ZDtj note that here we account for both the anticipatory network built in Step 
B6, 2By run both algorithms in parallel , we mean that we run the two algorithms as independent instances 
feeding the input stream to both instances, but pausing the more expensive run when the cheaper one has 
.nished serving all the requests seen thus far. This standard trick ensures that we never pay more than 
2× the cost of the greedy algorithm in the worst case: as we see, we do much better on average.  as 
well as the connection costs incurred in Step B4. Summing this over all the relevant values of j, the 
total cost is at most 2 ZDti* = 2 · 3 · ZDti*-1 = 6ZD£ = (70.08 + C) × Z£. However, with probability 
at most 1/ log n (see Fact 2.1) we fail to compute the estimates ZD£, we pay O(log n) times the optimum 
cost (since we are running in parallel with the greedy algorithm). Since for each input sequence we pay 
O(log n) times 1 more with probability at most , this contributes log n only O(OPT) more to the cost, 
and completes the proof of the theorem. 3 Extending to Sub-additive Problems In this section, we show 
how to extend the results in the Steiner tree case to a general subadditive problem .. Let U denote a 
universe of clients (these make up the input sequence) and E a set of elements used " to de.ne a solution. 
For a subset of clients U . U, de.ne Sols(U ") . 2E as the set of all possible solutions " of U . The 
cost of a solution F . E is given by c(F )= e.E c(e) where c(e) is the cost of element e . E. Let OPT(U 
") . Sols(U ") be the solution of minimum cost. We assume the presence of two algorithms associ­ated 
with the problem .: " The .rst algorithm, A, takes a set of clients U . U and outputs a solution A(U 
") . Sols(U "), which is an a-approximation to OPT(U ").  The second is the augmentation algorithm B, 
which takes a solution E " . Sols(E "), and an input client x . U, and outputs B(E " ,x) . E such that 
E " .B(E " ,x) . Sols(U " .{x}).  The augmentation procedure is monotone if for . E "" any extra elements 
E "" . E, c(B(E " ,x)) = c(B(E " ,x)). I.e., adding extra elements in the ini­tial solution does not 
increase the cost of augmen­tation. We will also assume the presence of a cost-sharing " scheme .(U ,x) 
which assigns a cost share to each x . U " " such that .(U ,x) = 0for x . U , and the fairness property: 
" (3.5) x.UI .(U ,x) = c(OPT(U ")). The main property of interest for the cost-sharing scheme will be 
the strictness with respect to (A, B): " Given a set U . U and client x . U, (3.6) c(B(A(U "),x)) = ß 
· .(U " .{x},x), i.e., the cost c(B(A(U "),x)) of taking the solution " on U and augmenting it to serve 
yet another client x is not much more than the cost share of e in the client set U " .{e}. 3.1 Subadditive 
Problems: Known Sequence Lengths The general algorithm (given in Figure 2) stays essentially the same: 
let the clients arriving online be x1,x2,..., let Rt denote the pre.x {x1,x2,...,xt}, and R = Rk be the 
.nal sequence. C1. Choose a set of vertices D by drawing from the distribution p independently k times. 
C2. Compute and buy S0 = A(D). C3. When the clients in the input sequence arrive online (with xt being 
the tth request) set St = St-1 . B(St-1,xt) . Sols(D . Rt). Figure 2: Algorithm for Subadditive Problems 
Theorem 3.1. Given an a-approximation algorithm A, a monotone augmentation procedure B, if there exist 
ß-strict cost-shares . (w.r.t. (A, B)), then the above online algorithm is an (a + ß)-approximation algorithm 
for the subadditive problem .. The proof of the above theorem is deferred to the full version of the 
paper, as is the proof of the following theorem which essentially follows from previous results on strict 
cost-shares in [17, 19, 14]. Lemma 3.1. (Applying General Framework) For the following problems, there 
exists an a-o.ine approximation algorithm A, a monotone augmentation procedure B, anda cost-sharing mechanism 
. that is ß-strict with respect to (A, B): For Uncap. Facility Location (UFL) problem, a = 3 and ß =3; 
 For Steiner Forest, a =2, and ß =3.  For Vertex Cover, a = ß =3. (For t-hypergraph vertex cover, a 
= ß = t +1.)  3.2 Subadditive Problems: Unknown Sequence Lengths Just as in Section 2.2, we can extend 
algo­ rithms tailored for a .xed value of the sequence length to general cases where we do not know the 
sequence length in advance. The ideas are similar to those for Steiner tree: the only problem-dependent 
facts that we need to show are the following. The number of samples N we need depends on log M, where 
M is the number of di.erent antic­ipatory solutions possible. Note that for Facility Location and Vertex 
Cover, say, M = 2n , since each anticipatory solution is merely a set of nodes; hence this gives us a 
polynomial dependence.  The problem should not be too sensitive. In particular, for every choice of 
the anticipatory solution D, the change in cost between running the augmenting algorithm B after building 
D and running B starting with the empty anticipatory solution, should be at most . × cost of building 
D. The number of samples N required depends on poly(.). For Facility Location and Vertex Cover, we can 
bound . by n.  We should be able to de.ne and solve a rent-or­buy versions of the problem .. Again, 
for Facility Location and Vertex Cover, this can be done by modifying the current LP rounding solutions. 
The details appear in the full version.  Of course, we need all these facts only if we want the running 
time of the online algorithm to be polynomial: if we are allowed to perform unbounded computation (as 
is sometimes allowed in the design of online algorithms, when we are only interested in the price of 
the lack of foresight), moving to unknown sequence lengths is easy. 4 Lower Bounds One may ask whether 
our assumptions are too restric­tive: suppose we did not insist that the requests were independent of 
the past history, or if the draws were from some .xed but unknown distribution could our results be proved 
in such models? We answer this in the negative: not only do our results make use of these features, they 
are necessary to get O(log1-E n)-type re­sults. Here is a summary of results, the details for which will 
be included in the full version of the paper. All the following results also hold in the RoE model from 
the following section. Consider a model where the (random) demand point vt at time t is dependent only 
on vt-1, the demand point at time t-1. In other words, given a probability distribution p(v) for each 
vertex v . V , the tth demand point is drawn from the distribution p(vt-1) . log n For such models, we 
show a O( ) lower bound log log n on the performance of any online algorithm for the Steiner Tree problem, 
and the Uncapacitated Facility Location problem. These results almost follow easily from known lower 
bounds for the two problems [15, 3]. Another natural question to ask is whether the al­gorithm really 
needs (black-box) access to the dis­tribution p: suppose there exists some distribution from which the 
clients are being drawn indepen­dently at each step, but we do not allow the al­gorithm to sample from 
this distribution. For the Steiner tree, there is an O(log n) lower bound in this model. Finally, a stochastic 
model that has seen success is the random permutation model: here the adversary chooses a set S of clients 
which is initially not known to the algorithm. The clients in S are then presented to the algorithm in 
a random order. In this model, O(1)-competitive ratios were shown for facility location and the access-network 
design problem [31, 32]. Here we show an O(log n) lower bound for Steiner tree. 5 Expectation of Ratios: 
Online Steiner Tree An objective function that is often more challenging to work with is the expected 
ratio (EoR) Er[A(.,r)] (5.7) EoR(ALG) = maxp maxk E..pk. OPT(.) Note that the outer expectation is over 
the random choice of inputs, and the inner one is over the random coins r of the online algorithm. Loosely, 
it turns out ot be somewhat more di.cult, since we have to compare the expected ratio of the cost of 
the built solution to the cost of the optimal one on the same set of clients. The analysis techniques 
from the RoE case do not seem to extend to this case; for RoE, we were counting on the fact that if we 
su.er a large cost on an instance, the optimum also su.ers a large cost on some instance. Clearly such 
analyses are not enough to prove results for EoR. In this paper, we show that when the sequence length 
k is known to the algorithm, we can get substantial improvements over previous results; it remains an 
open question to extend this to cases where the sequence length is not known in advance. You should note 
that the bounds on EoR might be not comparable to the bounds on RoE . The main di.erence in our algorithm 
is that instead of generating one dummy set D, we generate L such sets and choose the best of them (for 
L to be speci.ed later). The algorithm given in Figure 3 is again run in parallel with the greedy solution, 
to ensure that the worst-case competitive ratio is still O(log n). Let us denote by . the sequence given 
to the algorithm as input, and R = Mset[.] the corresponding multiset. The following lemma is immediate 
from symmetry. 1 Lemma 5.1. With probability at least 1 - , the cost L+1 of least expensive tree Ti* 
is no more than .OPT(R).  D1. Sample L di.erent k element multisets D1,...,DL from the distribution 
p. D2. For each i, .nd .-approximate Steiner tree Ti on the set Di .{r}, but do not buy these edges. 
D3. Choose i* such that the cost of Ti* is the least, and buy these edges: i.e., set S0 = Ti* . D4. Connect 
the k input vertices greedily: connect the tth input vertex vt to the closest node in the tree St-1 to 
get the tree St. Figure 3: Algorithm for EoR Steiner Tree Now we prove that the cost of connecting the 
real vertices R to the tree Ti is small as well. First, we need the following technical lemma that each 
vertex of the graph sees at most O(log nL) real vertices from R closer to it than the closest dummy vertex 
from any one of the L sets Di. Formally, given a parameter I . Z, for a vertex v . V and each index i 
.{1,...,L}, let N £ i,v be the (random) multiset of I nearest vertices to v (excluding v) in Zi = R . 
Di. (Proof will be included in the full version of the paper). Lemma 5.2. Let I = i3 log nLl. Then for 
all i . {1,...,L} and for all v . V , Ln 1 P[N£ i,v . R] == , 2£ n2 where the probability is taken over 
the choices of R and 1 Di. In other words, with probability at least 1 - 2 , all nthe sets N£ contain 
at least one vertex from Di. i,v -2 Hence, with probability at least 1 - n, every vertex v has a vertex 
from the anticipatory set Di* that is no further than its Ith-closest vertex in the actual demand set 
R. Having this fact at hand, we are ready to prove the following. Lemma 5.3. The total cost of connecting 
the demand vertices in R incurred in Step D4 is O(OPT(R) · 1 log log(nL)) with probability at least 1 
- 2 . n Proof. Let us take the multiset R, and let us denote by TR the optimal Steiner tree on it. Take 
the tree TR, build an Eulerian tour, and short-cut to get the tour TR = (v1,...,vk,v1). Note that the 
cost of the tour isat most 2OPT(R). Divide TR into contiguous I = i3 log kl-element segments Tj = (vj£,...,v(j+1)£), 
for k each index j .{1,..., }: note that the length of these £ pieces adds up to the length of TR, which 
is at most 2OPT(R). Now consider the run of the above algorithm on the k demand set R. For every .xed 
j .{1,..., } consider £ the .rst vertex wj in the segment Tj that is given to the algorithm (and suppose 
this arrives at time t). Applying the trivial union bound to Lemma 5.2 gives us that with 1 probability 
at least 1 - n2 , the set Ni£ * ,wj around wj contains a vertex from Di* . This in turn implies that 
the distance from this .rst node wj . Tj to the anticipatory set Di* is no more than the length of Tj 
, since by the construction of the segments the node wj sees at least I nodes within distance equal to 
the length of Pj. (In fact, in the worst case this set N£ of I nearest nodes i,wj to wj contains just 
the vertices of Pj.) Since we initialized S0 = Ti* to be the tree on this set Di* , the distance from 
wj to the current set St-1 is d(wj,St-1) = d(wj ,S0) = d(wj ,Di* ), which by the preceding argument is 
no larger than the cost of Tj . Now the cost of connecting the other vertices in R lying in the segment 
Tj is no higher than the cost of the greedy algorithm run solely on the nodes from the segment Tj . Since 
the competitive ratio of the online Steiner tree is logarithmic in the number of input nodes, and we 
are looking at I = O(log nL) input nodes from the segment Tj , the cost we incur is at most O(Cost(Tj 
) · log I)= O(Cost(Tj ) · log log nL). Summing k up over all the segments j .{1,..., } we get the total 
£ cost of connecting R to the anticipatory solution S0 is at most O(OPT(R)·log log nL) with probability 
at least 1 - 1 2 . n Now we are ready to prove that the above algorithm has a good expected competitive 
ratio for the case when the sequence length k is known. Theorem 5.1. Setting L = O(log n), the expected 
com­petitive ratio of the above algorithm is O(log log n). Proof. Suppose either of Lemma 5.1 or Lemma 
5.3 fails: 12 this happens with probability at most + L = . L+1 n2 log n In this failure case, we use 
the fact that we ran our algorithm in parallel with the greedy online Steiner tree algorithm, which is 
O(log k)= O(log n) competitive. Hence, the contribution of these two bad cases to the EoR is only a constant. 
If neither of the two lemmas fail, we see that the cost of the antipatory solution on Di is at most .ST 
OPT(R) by Lemma 5.1; the cost of connecting the actual demand set R having built the anticipatory solution 
is O(OPT(R) · log log n) by Lemma 5.3. Hence the expected ratio is O(log log n). 6 Stochastic Universal 
TSP Using the techniques from the previous sections, we can obtain results for the average case of the 
Universal TSP problem. To de.ne this problem, for a set R and a permutation t , de.ne t |R to be the 
length of the tour induced by t on R: i.e., start from any vertex in R, and keep visiting vertices in 
R in the order prescribed by t until you hit the start vertex; we let c(t|R) be the length of the tour. 
In the classical Universal TSP problem, given a metric space (V, d), we are asked for a  c(t|R) permutation 
t such that maxR.V is as small c(T SP (D)) as possible. In the stochastic variant we study here, we are 
given a probability pv of node v . V arriving (independent of the other nodes) and needing service from 
the salesper­son.3 The measure of goodness is: ER[Cost(t|R)] RoET SP (t )= , ER[Cost(TSP (R))] We can 
obtain the following result using techniques from the previous sections: Theorem 6.1. (Universal TSP 
Result) Given a metric space with n points, and probabilities pv for each vertex v independently demanding 
service. If the expected number of demands pv = O(1), there exists v a permutation t1 such that RoET 
SP (t1)= O(1). For lack of space, the proof of the theorem is deferred to the full version of the paper. 
The above theorem has been independently proved by Shmoys and Talwar (unpublished manuscript). Acknowledgments 
The .rst-and second-named au­thors would like to thank the University of Rome for its generous hospitality. 
Many thanks to Avrim Blum, Moses Charikar, Chandra Chekuri, Rajmohan Rajara­man, Tim Roughgarden, David 
Shmoys, and Kunal Tal­war for discussions and questions over the years on the topic of stochastic online 
algorithms. References [1] Susanne Albers. Online algorithms: a survey. Math. Program., 97(1-2, Ser. 
B):3 26, 2003. ISMP, 2003 (Copenhagen). [2] Susanne Albers and Stefano Leonardi. On-line algo­rithms. 
ACM Comput. Surv., 31(3es):4, 1999. [3] Noga Alon and Yossi Azar. On-line Steiner trees in the Euclidean 
plane. Discrete Comput. Geom., 10(2):113 121, 1993. 3Note the slightly di.erent model than in the previous 
sections: we could have assumed that the set R is obtained by taking some i P samples from a distribution 
p over vertices such that pv = 1, v instead of drawing one set R where each vertex independently arrives 
with probability pv. We feel that the pv model is better for the Universal TSP prolem, though many of 
the results continue to hold in pv case. [4] Luca Becchetti. Modelling locality: a probabilistic analysis 
of LRU and FWF. In Algorithms ESA 2004, volume 3221 of Lecture Notes in Comput. Sci., pages 98 109. Springer, 
Berlin, 2004. [5] S. Ben-David and A. Borodin. A new measure for the study of on-line algorithms. Algorithmica, 
11(1):73 91, 1994. [6] Allan Borodin and Ran El-Yaniv. Online computation and competitive analysis. Cambridge 
University Press, New York, 1998. [7] Allan Borodin, Sandy Irani, Prabhakar Raghavan, and Baruch Schieber. 
Competitive paging with locality of reference. J. Comput. System Sci., 50(2):244 258, 1995. 23rd Symposium 
on the Theory of Computing (New Orleans, LA, 1991). [8] Moses Charikar, Chandra Chekuri, and Martin P´al. 
Sampling bounds for stochastic optimization. In Ap­proximation, randomization and combinatorial opti­mization, 
volume 3624 of Lecture Notes in Comput. Sci., pages 257 269. Springer, Berlin, 2005. [9] Andrew Chou, 
Jeremy Cooperstock, Ran El-Yaniv, Michael Klugerman, and Tom Leighton. The statis­tical adversary allows 
optimal money-making trading strategies. In SODA 95: Proceedings of the sixth an­nual ACM-SIAM symposium 
on Discrete algorithms, pages 467 476, Philadelphia, PA, USA, 1995. Society for Industrial and Applied 
Mathematics. [10] F. Eisenbrand, F. Grandoni, T. Rothvoß, and G. Sch¨afer. A tighter analysis of random 
sampling for connected facility location. Submitted, 2007.  [11] Amos Fiat and Anna R. Karlin. Randomized 
and multipointer paging with locality of reference. In STOC 95: Proceedings of the twenty-seventh annual 
ACM symposium on Theory of computing, pages 626 634, New York, NY, USA, 1995. ACM Press. [12] Amos Fiat 
and Ziv Rosen. Experimental studies of access graph based heuristics: beating the LRU stan­dard? In Proceedings 
of the Eighth Annual ACM-SIAM Symposium on Discrete Algorithms (New Orleans, LA, 1997), pages 63 72, 
New York, 1997. ACM. [13] Amos Fiat and Gerhard J. Woeginger, editors. Online algorithms, volume 1442 
of Lecture Notes in Computer Science. Springer-Verlag, Berlin, 1998. The state of the art, Papers from 
the Workshop on the Compet­itive Analysis of On-line Algorithms held in Schloss Dagstuhl, June 1996. 
[14] Lisa Fleischer, Jochen K¨onemann, Stefano Leonardi, and Guido Sch¨afer. Simple cost sharing schemes 
for multicommodity rent-or-buy and stochastic steiner tree. In STOC 06: Proceedings of the thirty-eighth 
an­nual ACM symposium on Theory of computing, pages 663 670, New York, NY, USA, 2006. ACM Press. [15] 
Dimitris Fotakis. On the competitive ratio for online facility location. In Automata, languages and program­ming, 
volume 2719 of Lecture Notes in Comput. Sci., pages 637 652. Springer, Berlin, 2003. [16] Sudipto Guha, 
Adam Meyerson, and Kamesh Muna­gala. Hierarchical placement and network design prob­  lems. In Proceedings 
of the 41th Symposium on the Foundations of Computer Science (FOCS), pages 603 612, 2000. [17] Anupam 
Gupta, Amit Kumar, Martin P´al, and Tim Roughgarden. Approximations via cost-sharing. In Proceedings 
of the 44th Symposium on the Foundations of Computer Science (FOCS), pages 606 615, 2003. [18] Anupam 
Gupta, Amit Kumar, and Tim Roughgarden. Simpler and better approximation algorithms for net­work design. 
In Proceedings of the 35th ACM Sympo­sium on the Theory of Computing (STOC), pages 365 372, 2003. [19] 
Anupam Gupta, Martin P´al, R. Ravi, and Amitabh Sinha. Boosted sampling: Approximation algorithms for 
stochastic optimization problems. In Proceedings of the 36th ACM Symposium on the Theory of Computing 
(STOC), pages 417 426, 2004. [20] Makoto Imase and Bernard M. Waxman. Dynamic Steiner tree problem. SIAM 
J. Discrete Math., 4(3):369 384, 1991. [21] Nicole Immorlica, David Karger, Maria Minko., and Vahab Mirrokni. 
On the costs and bene.ts of procras­tination: Approximation algorithms for stochastic com­binatorial 
optimization problems. In Proceedings of the 15th ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 
684 693, 2004. [22] Sandy Irani and Anna R. Karlin. On online compu­tation. In Dorit Hochbaum, editor, 
Approximation Al­gorithms for NP Hard Problems. PWS publishing Co, 1996. [23] Sandy Irani, Anna R. Karlin, 
and Steven Phillips. Strongly competitive algorithms for paging with lo­cality of reference. SIAM J. 
Comput., 25(3):477 497, 1996. [24] Bala Kalyanasundaram and Kirk Pruhs. Speed is as powerful as clairvoyance. 
J. ACM, 47(4):617 643, 2000. [25] David R. Karger and Maria Minko.. Building Steiner trees with incomplete 
global knowledge. In Proceedings of the 41th Symposium on the Foundations of Computer Science (FOCS), 
pages 613 623, 2000. [26] Anna R. Karlin, Mark S. Manasse, Larry Rudolph, and Daniel D. Sleator. Competitive 
snoopy caching. Algorithmica, 3(1):79 119, 1988. [27] Anna R. Karlin, Steven J. Phillips, and Prabhakar 
Raghavan. Markov paging. SIAM J. Comput., 30(3):906 922, 2000. [28] Samir Khuller, Balaji Raghavachari, 
and Neal E. Young. Balancing minimum spanning and shortest path trees. Algorithmica, 14(4):305 322, 1995. 
[29] Elias Koutsoupias and Christos H. Papadimitriou. Beyond competitive analysis. SIAM J. Comput., 30(1):300 
317, 2000. [30] Mark S. Manasse, Lyle A. McGeoch, and Daniel D. Sleator. Competitive algorithms for server 
problems. J. Algorithms, 11(2):208 230, 1990. [31] Adam Meyerson. Online facility location. In 42nd IEEE 
Symposium on Foundations of Computer Science (Las Vegas, NV, 2001), pages 426 431. IEEE Computer Soc., 
Los Alamitos, CA, 2001. [32] Adam Meyerson, Kamesh Munagala, and Serge Plotkin. Designing networks incrementally. 
In Pro­ceedings of the 42nd Symposium on the Foundations of Computer Science (FOCS), pages 406 415, 2001. 
[33] Konstantinos Panagiotou and Alexander Souza. On adequate performance measures for paging. In STOC 
06: Proceedings of the thirty-eighth annual ACM sym­posium on Theory of computing, pages 487 496, New 
York, NY, USA, 2006. ACM Press. [34] C. A. Phillips, C. Stein, E. Torng, and J. Wein. Opti­mal time-critical 
scheduling via resource augmentation. Algorithmica, 32(2):163 200, 2002. [35] Prabhakar Raghavan. A statistical 
adversary for on­line algorithms. In Online Algorithms, volume 53 of DIMACS Ser. Discrete Math. Theoret. 
Comput. Sci., pages 79 83. Amer. Math. Soc., Providence, RI, 1991. [36] R. Ravi and Amitabh Sinha. Hedging 
uncertainty: Approximation algorithms for stochastic optimization problems. In Proceedings of the 10th 
Integer Pro­gramming and Combinatorial Optimization Conference (IPCO), pages 101 115, 2004. [37] Gabriel 
Robins and Alexander Zelikovsky. Tighter bounds for graph Steiner tree approximation. SIAM J. Discrete 
Math., 19(1):122 134, 2005.  [38] Daniel J. Rosenkrantz, Richard E. Stearns, and Philip M. Lewis, II. 
An analysis of several heuristics for the traveling salesman problem. SIAM J. Comput., 6(3):563 581, 
1977. [39] Mark Scharbrodt, Thomas Schickinger, and Angelika Steger. A new average case analysis for 
completion time scheduling. J. ACM, 53(1):121 146, 2006. [40] David B. Shmoys and Chaitanya Swamy. An 
approxi­mation scheme for stochastic linear programming and its application to stochastic integer programs. 
J. ACM, 53(6):978 1012, 2006. [41] Daniel D. Sleator and Robert E. Tarjan. Amortized e.ciency of list 
update and paging rules. Comm. ACM, 28(2):202 208, 1985. [42] Alexander Souza and Angelika Steger. The 
expected competitive ratio for weighted completion time schedul­ing. Theory Comput. Syst., 39(1):121 
136, 2006. [43] Neal Young. On-line caching as cache size varies. In Proceedings of the Second Annual 
ACM-SIAM Sym­posium on Discrete Algorithms (San Francisco, CA, 1991), pages 241 250, New York, 1991. 
ACM. [44] Neal E. Young. On-line paging against adversarially biased random inputs. J. Algorithms, 37(1):218 
235, 2000. Ninth Annual ACM-SIAM Symposium on Dis­crete Algorithms (San Francisco, CA, 1998).  
			