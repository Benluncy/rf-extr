
 METHODS FOR SELECTING THE BEST SYSTEM David Goldsman Barry L. Nelson School of Industrial &#38; Systems 
Engineering Department of Industrial &#38; Systems Engineering Georgia Institute of Technology The Ohio 
State University Atlanta, Georgia 30332 Columbus, Ohio 43210 Bruce Schmeiser School of Industrial Engineering 
Purdue University West Lafayette, Indiana 47907 ABSTRACT In this tutorial we consider three methods for 
select­ing the best of a set of competings yst ems: interactive analysis, ranking and selection, and 
multiple compar­isons. We describe each method; discuss assumptions, implement ation aspects, advantages, 
and disadvan­tages; and demonstrate the use of each method with an airline-reservation-system simulation 
example. 1 INTRODUCTION Stochastic simulation is often used to compare com­peting systems. In this tutorial, 
we discuss concepts and methods to select the best system. The goal is to design an efficient experiment 
and to provide a sound data analysis. We consider three approaches: interactive analysis (IA), ranking 
and selection (R&#38;S), and multiple com­parison procedures (MCPS). The concept of standard error underlies 
all three approaches, but they differ in that IA, R&#38;S, and MCPS are based on estimation, opt imization, 
and inference, respectively. After considering factors affecting problem context, we define a specific 
example involving the selection of an airline-reservation system. We pursue the exam­ple with Schmeiser 
discussing IA in Section 2, Golds­man discussing R&#38;S in Section 3, and Nelson dis­cussing MCPS in 
Section 4. Common notation is used throughout. 1.1 The Problem Context The problem context can vary 
dramatically. Some factors that affect the choice of design and analysis methods include: Number of 
competing systems, k. In o,ur exam­ple k is small, less than ten or twenty, and the systems are given. 
Another possibility y is k large, possibly infinite, as is the case when some deci­sion variables are 
cent inuous.  Question to be answered. In our example the goal is to find the single best system, where 
best is some criterion of goodness determined by the decision-maker. Other possibilities include find­ing 
all systems satisfying a set of criteria or rank­ing the best r of the k systems.  Number of performance 
measures. In cmr exam­ple the comparison of systems is based on only one criterion so the definition 
of best is unam­biguous. When multiple performance param~e­ters are considered, the definition of best 
ble­comes more subjective. Scatter plots and other graphical aids are particularly useful for compar­isons 
in two, and sometimes more, dimensions.  Type of performance measures. In our exam­ple the single performance 
measure is estimated  by a sample mean of independent observations. Comparisons based on dependent observatic,ns 
are sometimes more efficient, but complicate the statistical analysis. The statistical analysis allso 
would change if the performance measure were estimated with a non-mean, such as a sample standard deviation 
or quantile. Computational cost. In our example the stoch:is­tic elements of the system and the system 
co m­plexit y produce a high computational cost, mea­sured typically in elapsed time, money, or both. 
If the computational cost is not high, then the 177 comparison is easy-simply simulate each sys­tern 
until the sampling error is negligible. Constraints. In our example we make various as­sumptions about 
constraints placed on the anal­ysis caused by computing time, analyst time, and analyst sophistical ion. 
 Simulation environment. The combination of computer hardware and software can support or hinder simultaneous 
runs of various systems, stopping and rest arting of the simulations, inter­active anal ysis, graphical 
analysis, and statistical analysis.  Common random numbers. Often the k sys­tems have similar logical 
structure, in which case the use of common random numbers can reduce  the computational cost. In our 
example com­mon random numbers are not used; therefore, the data from each system are independent from 
the other systems data. Use of the results. The choice of analysis method can depend upon whether its 
purpose is to con­vince the analyst or another decision-maker.  1.2 The Airline-Reservation Example 
We consider k = 4 different airline-reservation sys­tems. The single measure of performance is the ex­pected 
time to failure, E[TTF] the larger the bet­ter. The system works if either of two computers works. Computer 
failures are rare, repair times are fast, and the resulting E[TTF] is large. The four sys­tems arise 
from variations in parameters affecting the time-to-failure and time-to-repair distributions. We know 
from experience that the E[TTF] s are roughly 100,000 minutes (about 70 days) for all four systems. We 
are indifferent to expected differences of less than 3000 minutes (about two days). The large E[TTF] 
s, the highly variable nature of rare failures, the similarity of the systems, and (as it turns out) 
the small indi~erence zone of 3000 min­utes yield a problem context with reasonably large computational 
costs. Although the similarity of the systems suggests the use of variance reduction tech­niques such 
as common random numbers, for tutorial simplicity we have agreed to restrict ourselves to in­dependent 
replications of the systems. In all cases the point estimator for system i is the sample aver­age over 
the replications allocated to system i by the experiment. Goldsman, Nelson and Schmeiser 2 INTERACTIVE 
ANALYSIS The interactive analysis described here is an estima­tion approach. It considers the k = 4 point 
estima­tors for the respective E[TTF] s and the estimates of their sampling errors. The goal is a vague, 
but well­founded, sense of confidence in the selected system. IA is similar in spirit to the procedure 
in Schmidt and Taylor (1970, pp. 524 528), except that we suppress their explicit confidence-interval 
logic. Neither the assumptions nor the cleanly-stated statistical conclu­sions of R&#38;S and MCPS are 
found here. 2.1 The Method The method is to successively experiment with the systems, roughly comparing 
the four point estimators and their associated standard errors with each other until we are comfortable 
that the chosen system is the best or negligibly close to the best. A more-detailed discussion of the 
underlying concepts can be found in Schmeiser (1990). An interactive driver program reads a specified 
sys­tem i, a set of random-number seeds, a number of microreplications m, and a number of macroreplica­tions 
b. It runs the simulation model for n = bm replications, producing the TTF observation Yije on microreplication 
1 of macroreplication j. The driver also produces the point estimator for pi, the E[TTF] of system i, 
 b bm wit h associated sample variance of the macro replica­tion estimators and standard error sei = 
Si/ti. (The SERVO soft­ware described in Schmeiser and Scott 1991 could be used to automatically compute 
these statistics.) The relevant part of the output report is . . parameters. . . . . system = 1 . . macroreplicat 
ions = 5 . . microreplicat ions = 5 . .monte carlo estimates. . . ,, estimated E [TTFI 111086. = . 
. With standard error = 21211.0 In this example, twenty-five replications of system 1 have been run. 
The interpretation is that system 1 has estimated E[TTF] of 11. x 104 minutes. The last digit included 
is in the most-significant position of the standard error. The last digit is meaningful only in that 
11. x 104 is a better guess than 1, x 105, The digits not included are meaningless, since they are dominated 
by sampling error. 2.2 The Assumptions The significant digits interpretation of the last para­graph is 
loosely based on the mathematics of the con­fidence interval on the E[TTF] of system i, Yi +t&#38;l,l-@/.2 
-sei, which under assumptions of normality and indepen­dence of the Yij S covers the true mean pi with 
confi­dence 1 cr. If these assumptions hold, then for b >4 [b > 15] and the typically-used 90% to 99% 
confi­dences, the t-values range from 1.6 to 4.6 [1.6 to 2.9]. Since the range of values is small, and 
since most peo­ple choose a arbitrarily, there is little precision lost by stating significant digits 
rather than a confidence interval. Put another way, lit tle practical (compared to statistical) confidence 
results if varying a (over the typical values) affects our decision whether to stop or simulate further. 
Nevertheless, some care must be taken, since in extreme cases the significant-digit guideline is invalid. 
Small values of b and a can lead to large t-values; for example, b = 2 with a 99% confidence level yields 
a t-value of 64. In simulations with serially dependent data, large values of b (with fixed n) result 
in high correlation of the Yij s, typically biasing estimators of the standard error to the low side. 
Choosing 10< b < 30 is often wise (Schmeiser 1982). The independence in our example implies that we can 
safely choose b relatively large, since only normality of the Yij S (via the central limit theorem) is 
gained with low values of b. Interactive experimentation leads to sequential sampling, another source 
of error in confidence­interval computations. But if the value of b is kept reasonably large, the secondary 
effects of sequential sampling are negligible in a method that does not specify a confidence level. 
 2.3 The Example In this section we summarize a log kept during an analysis of the airline-reservation-system 
example. The analysis spanned two days. The initial run, which produces the example out­put shown above 
for system 1, is designed just to gain a sense of the magnitude of the required pro­duction experiment 
in terms of time per replication 17 9 and number of replications. The twenty-five replica­tions require 
about thirteen minutes on a dedicated SPARCstation 1, so each replication costs about 30 seconds of real 
time. We extrapolate this time esti­mate to the other systems, since all four systems are similar. The 
standard error estimate of 21000 minutes has only b 1 = 4 degrees of freedom, but that is fine for the 
following rough analysis. We are interested only in detecting differences between the best system and 
inferior systems when the differences are in fact at least 3000 minutes. So the standard errors will 
need to drop to at most 1500 minutes, or 1/14 the current value. Therefore, the number of replications 
needs to be about 142 times longer: 5000 replications. This means that the worst-case projected experi­ment 
time is 5000 replications times half a minute per replication times four systems. Seven days. The standard 
error has few degrees of freedom, so the a,c­tual experiment time may be from five to nine days. Rather 
than begin a seven-day experiment so soon after starting the comparison, and since it is time to go home 
for the day, we first run an overnight experiment: For each of the four systems, we run b d 15 macroreplications 
of m = 15 microreplications; as agreed upon for this tutorial, we use different random numbers for each 
system. The overnight results are given in Table 1. Stated in significant digits, Y1 = 11.1 x 104, 92 
= 10.3 x 104, Y3 = 9.4x 104, and Y4 = 0.8 x 104. System 4 appea,rs to be an order of magnitude from being 
competitive; however, inspection shows an input error, so ignore the system 4 results for now, while 
being pleased this was an overnight run rather than a week-long run. System 1 looks better than systems 
2 and 3, regard­less of the indifference value. System 3 already is close to being eliminated, so the 
total experimentation is looking shorter than the worst-case projections. Table 1: IA Overnight-Experiment 
Results ill 234 z 110762. 103265. 93968. 7!194. Sei 5757. 4622. 5226. 538. Now would be a good time to 
create some graphics, e.g., boxplots of the macroreplication estimators Vij. (We have not included graphics 
in this paper due to space constraints.) Needing to go home in about an hour, let us devcke about 30 
minutes exclusively to system 4, for which wetake b= 10and m = 6. It turns out that Y4= 8.8 x 104 minutes, 
which is not competitive, especially if we remember the indifference value of 0.3 x 104 minutes. We will 
be back in about five hours, so we have time to make a run of systems 1 and 2, each for b = 30 and m 
= 10. The results are Y1 = 10.9 x 104 and Y2 = 10.5 x 104. This is more cumulative evidence indicating 
system 1 is best, but the evidence from this run alone is not conclusive. Since the additional in­formation 
lowers the standard error of the estimators for systems 1 and 2, we decide that it is safe to elim­inate 
system 3 from further consideration. In fact, since the indifference value is 3000 minutes, we could 
at this point select system 1 as best, albeit with low (undefined, subjective) confidence. We are now 
far ahead of schedule, which (arbitrar­ily for this tutorial) is to finish before the Memorial Day weekend. 
With the extra time, we devote an­other night s run to the comparison. We simulate systems 1, 2, and 
4, each for b = 30 and m = 20. (System 4 is included since the time is available; our initial look was 
probably sufficient, despite it be­ing only sixty microreplications.) The results are YI = 10.6x 104, 
~z = 10.4x 104, and 74 = 9.1 x 104. System 4 is clearly noncompetitive. Our confidence in system 1 has 
risen yet more, since it is again best, although still not conclusively. With the extra assur­ance of 
the 3000-minute indifference zone, we declare system 1 our choice with a confidence easily great enough 
for this tutorial application. 2.4 Discussion IA carries a variety of advantages compared to the formal 
methods such as those to be discussed in the next sect ions. Noncompetitive systems are elimi­nated quickly 
in IA, and so require less computation than bet ter systems; many formal methods assume the worst case 
and devote equal numbers of observa­tions to each system. IA allows heavy computation to be conveniently 
scheduled while the analyst is away; many formal methods require an inconvenient single large run. IA 
forces the analyst to think about the results, which helps to detect mistakes. IA extends directly to 
estimators other than means (Schmeiser, Avramidis, and Hashem 1990); new mathematics is required for 
most formal methods. Common random numbers can be incorporated into IA by computing standard errors on 
differences or by using the same analysis with some increased confidence in the results. Adding new systems 
is easy with IA s informality; most formal methods require the number of systems k to be fixed. Three 
disadvantages exist. First, the lack of a pre- Goidsman, Nelson and Schmeiser cise confidence statement 
causes discomfort for many people. Second, the analyst ignoring or misusing the standard errors allows 
incorrect conclusions. Third, using significant digits as a measure of sampling er­ror is crude, since 
a significant digit is added only by simulating 100 times longer. Of course, the standard errors are 
available for more precise computation if the analyst desires. 3 RANKING AND SELECTION Ranking and selection 
procedures are statistical methods specifically developed to select the best sys­tem from a set of competing 
systems. Provided certain assumptions are met, these methods usually guarantee that the probability of 
a correct selection will be at least some user-specified value. This sec­tion discusses the normal means 
procedure of Rinott (1978). We then apply the Rinott procedure to the airline-reservation-system problem 
at hand. 3.1 The Method The general goal behind R&#38;S methods is to select the best system from among 
k competitors. Here, we have k = 4 airline-reservation systems. By best, we mean the system having the 
largest underlying E[TTF]. Suppose we denote the E[TTF] arising from system i by pi, i = 1, 2, . . . 
. k, and the associated or­dered pi s by p[ll ~ pt21 ~ . . . < p[~l. (We assume that the pi s, p[il s, 
and their pairings are completely unknown. ) Since we prefer the E[TTF] to be as large as possible, the 
mean difference between the two best systems in the ongoing airline-reservation example is P[~] Pp -11. 
The smaller this difference is, the greater the amount of sampling that will be required to dif­ferentiate 
between the two best systems. Of course, if p[~l p[~ 11 is very small say less than ~ >0, then for all 
practical purposes, it would not matter which of the two associated systems we chose as best. In other 
words, we regard 6 as the smallest difference worth detecting. In the airline-reservation exam­ple, we 
have taken 6 = 3000 minutes. We would also like to be assured that the probabil­ity that we make a correct 
selection (CS) of the best system will be at least a certain high value, P*. The greater the value of 
P*, the greater the number of ob­servations that will be required. We take P* = 0.90 in our example. 
We will apply a two-stage procedure due to Rinott (1978) to the airline-reservation problem. The pro­cedure 
assumes that system i produces independent and identically distributed (i.i.d.) normal (pi, a?) out­put, 
where pi and u; are unknown, i = 1,2, . . . . k, and where the k systems are independent. If p[k] W-II 
> ~, the Procedure guarantees that ~{cs} 2 P*. The procedure runs as follows. In the first stage of sampling, 
we take a random sample of b. observations from each of the k normal populations. We use as our observations.-from 
system i the macroreplication estimators, Y~l, Y~z, . . .. defined as in Subsection 2.1; for now, we 
will assume that they are i.i.d. normal. Calculate the first-stage sample means, and sample variances 
s; = ~(fijYy)2/(bo 1),  j=l fori=l,2 ,. ... k. The sample variances are used to determine the number 
of observations (macrorepli­cations) which must be taken in the second stage of sampling; the larger 
a sample variance, the more macroreplications must be taken in the sec­ond stage from the associated 
system. Now set bi = max{bo, [(hSi/d)21 }, where [.1 is the ceiling function, and h is a constant that 
solves a certain in­tegral, and is tabled in, e.g., Wilcox (1984). During the second stage of sampling, 
take bi b. additional observations from the ith system, i = 1,2, . . . . k. ~inally, we calculate the 
grand means Yi = ~j~l ~j/bi, ~ = 1,2,.. .,k, and select the sYstem having the largest ~i m best (which 
is certainly intu­itively appealing).  3.2 The Assumptions Rinott s procedure requires that the observations 
(macroreplications) taken within a particular system be i.i.d. normal. We discuss these assumptions in 
this subsection. The macroreplication estimators, Yil, Yiz, . . . . Yib,, from the ith system are assumed 
to be i.i.d. with expectation pi. This is trivially true since the macroreplicat ions are independent 
of each other. The macroreplications from across all systems are assumed to be independent; i.e., if 
i # i , all Yij S are independent of all ~i,j s, j = 1,2, . . .. This require­ment is also satisfied 
trivially since different random­number streams are chosen for each system s simula­t ion. (See Subsection 
4.5 for a discussion concerning the use of common random numbers.) The macroreplication estimators, Yij, 
for i = 1,2,..., kandj=l,2,. . . , bi, are assumed to be nor­mall y distributed. If the number of microreplications 
181 m is large enough, say at least 20, then the central limit theorem yields approximate normality for 
the macroreplication estimators. We make no assumptions on the variances of the macroreplications. Although 
there are a number of R&#38;S procedures devised for the special case of normal populations with unknown 
but common variance, we will not resort to those procedures here.  3.3 The Example Our goal is to find 
the system having the largest E[TTF]. To achieve the goal the following sequence of experiments was performed: 
1. A debugging experiment to check the computer code and assess execution speed. 2. A pilot experiment 
to study characteristics of the data and aid in planning the production run. 3. A production run to 
produce the final results.  All R&#38;S experiments and analyses were performed on various SPARCstations. 
3.3.1 Debugging Experiment Five macroreplications, each consisting of five m i­croreplications of system 
1, produced a sample mean TTF of 129182. minutes and a sample standard de­viation 69417.2 minutes. Each 
microreplication took about 24 seconds of real time on a (non-dedicated) SPARCstation 1. Since the sample 
variance was so large, we decided to conduct a somewhat larger p i­lot study; this would also serve as 
the first stage c)f the Rinott procedure. The pilot study would take 20 macroreplicat ions, each consisting 
of 20 microreplica­tions, for each of the k = 4 systems. We anticipated that the pilot study would use 
at most 10 hours of real time.  3.3.2 Pilot Experiment By dividing the pilot study among various SPARC­stations, 
we were able to complete it in less than 3 hours. The results are given in Table 2. To check our normality 
assumption, we used the pilot study to conduct Shapiro-Wilks tests on the 20 macroreplications from each 
system; the tests passed at all reasonable levels. We mention in passing that we could have also conducted 
Bartlett s test to check for equality of variances among the systems; we did not do so since Rinott s 
procedure haa no restrictions on the variances of the macroreplication estimators. We also remark that 
an IA interpretation of the pilot Table 2: R&#38;S Pilot Experiment (bO = 20) ill 234 jy) ~ogzse. ~of7&#38;3f3. 
96167 .7 897 47.9 Sj 29157.3 24289.9 25319.5 20810.8 se~ 6519.8 5431.4 5661.6 4653.4 bi 699 485 527 356 
 study would have immediately eliminated system 4 (and maybe even system 3) from further considera­tion. 
For the case k = 4 and P* = 0.90, the critical con­stant from Wilcox (1984) is h = 2.720. This enabled 
us to calculate the hi-values for the second stage of sampling. Since the pilot study was also intended 
to be used as the first stage of Rinott sampling, we have displayed the resulting hi-values in the table. 
For ex­ample, for system 2, we needed to take bz b. = 465 additional macroreplications in stage two, 
each con­sisting of m = 20 microreplications. The total num­ber of microreplications over all four systems 
was about 40000. A worst-case scenario of 24 seconds of real time per microreplication (as in the debugging 
experiment) implied that the production run might take 250 hours; luckily, we had access to a number 
of SPARCstations, some faster than the SPARCsta­tion 1. 3.3.3 Final Results By dividing the production 
runs among the various SPARCstations, we were able to complete them in less than 2 days. The results 
are given in Table 3. These results clearly establish system 1 as the winner. We can make the formal 
statement that we are at least 9070 sure that we have made the correct selection (with the proviso that 
the true difference between the best and second best E[TTF] s is at least 6 = 3000 minutes). Table 3: 
R&#38;S Production Run ill 234 E 110816.5 106411.8 99093.1 86568.9 Se; 872.0 1046.5 894.2 985.8 Goldsman, 
Nelson and Schmeiser  3.4 Discussion There are a number of reasons to use R&#38;S techniques when seeking 
the best of a number of competing sys­ tems. Procedures such as Rlnott s guarantee the user of a correct 
selection with high probability when the true difference between the best and second best sys­ tem is 
at least 6; even when the true difference is less than 6, Rlnott s procedure insures selection with high 
probability of a good system (i.e., one that is within 6 of the best). This guarantee compares favorably 
to the simple yes or no answer that a classical hy­ pothesis test is likely to provide. R&#38;S procedures 
are also easy to use, as our Rinott example demon­ strated; little more than one tabled constant look-up 
and a sample-mean calculation is required. One drawback to Rinott s procedure is that it tends to be 
conservative, i.e., it sometimes takes more ob­servations than necessary in the presence of (favor­able 
system mean configurations (i.e., configurate ions in which the largest mean and the others differ by 
more than 6). This drawback arises from the fact that Rlnott guarantees P{CS} > P for all configurations 
of the system means for which the best is at least 6 better than the second best. But Rinott is just 
one of many R&#38;S techniques for the normal means problem. For instance, if we were to assume (or force) 
common variances among the competing systems, we could use a variety of more efficient R&#38;S procedures, 
some of which enjoy the capability of sequentially eliminating systems deemed as noncompetitive; these 
procedures capitalize on favorable system mean configurations by terminating sampling early. Bechhofer, 
Dunnett, Goldsman, and Hartmann (1990) study a number of such procedures. For additional reading, Gibbons, 
Olkin, and Sobel (1977) is a nice introductory R&#38;S text, and Law and Kelton (1991) shows how to apply 
R&#38;S techniques in a simulation context.  4 MULTIPLE COMPARISONS Multiple-comparison procedures treat 
the optimiza­tion problem as an inference problem on the per­formance parameters of interest. MCPS account 
for the error that arises when making simultaneous infer­ences about differences in performance among 
k sys­tems. This section introduces a specific MCP, multi­ple comparisons with the best (MCB), and applies 
it to the airline-reservation-system example. 4.1 The Method MCB provides simultaneous statistical inference 
on either Pi max~~i pl or pi minz#i pl, for i = 1,2 , . . . . k, where pi is the performance parameter 
of system i. In the airline-reservation example, pi is the E[TTF] for computer configuration i. This 
collection of parameters is particularly rele­vant for optimization. For example, suppose a larger performance 
parameter is better (as in the airline­reservation system). If pi maxl#i pl > 0, then system i is the 
best, because all other systems have smaller performance parameter. On the other hand, if pi maxl#i 
pf < 0, then system i is not the best, since there is another system with larger per­ formance parameter. 
Even when pi m~#i pl <0, if Pi maxl#i pl > 6, where 6 is a positive number, then system i is within 
6 of the best. If optimization is the goal, then inference on the k parameters Pi maxz+~ pt, i = 1,2, 
. . . . k, is superior to inference on the k(k 1)/2 parameters Pi pl, Vi # 1. Simultaneous inference 
is typically sharper (smaller differences in performance can be detected) when fewer parameters are included 
in the inference. The parametric version of MCB described here as­sumes that the output data from the 
simulation ex­periment is approximated by the oneway analysis-of­variance model: for systems z = 1,2, 
. . . . k and replications j = 1,2,..., ni, where the &#38;ii s are i.i.d. normal (O, o*) random variables. 
(We make no distinction between micro and macroreplications for the moment.) Under model (1), Hsu (1984) 
derived simultaneous (1 a)100% confidence intervals for pi maxt~i pt fori=l,2 ,.. ., k. In the balanced 
case (nl = nz = . ..= nr = n), the form of the ith interval is where ~i is the sample mean of the outputs 
from system i, S2 is a pooled estimator of U*, d = dl_a,k(n_l),k is a critical value, z-= min{O, z} and 
z+ = max{O, x}. The intervals for the unbalanced case are similar, but difficult to write compactly. 
Notice that the MCB intervals are constrained in­tervals, meaning that each interval either contains 
O. or one of its endpoints is O. In a maximization prob­lem, if the confidence interval for pi maxl#i 
pl con­tains O it means that relative to the sampling error in the point estimators system i is not significantly 
different from the best system, and may be the best. If the upper endpoint of the interval is O, then 
system i is not the best system. On the other hand, if the lower endpoint is O, then system i is the 
best system; at most one system will have lower endpoint O. These statements are made with confidence 
level 1 a. Hochberg and Tamhane (1987) describe the the­oretical development and practical application 
of MCB. Nelson (1992) gives a detailed algorithm for the balanced case. Both references provide tables 
of critical values. Statistical analysis packages that compute MCB intervals include J MP (version 2) 
and Minitab (release 8).  4.2 The Assumptions The assumptions implied by model (1), and their in­ terpretation 
in simulation experiments, are discussed in this subsection. The output data from within each system 
i, E1, K2,..., Y.~n, ~ are assumed to be i.i. d. with com­mon expectation pi. This will be true if the 
outputs are from replications (as in the airline-reservation ex­ample), or approximately true if they 
are batch means from within a single replication of a stationary pro­cess. The output data from across 
all systems on repli­cation j, Ylj, Y2j, . . . . Ykj, are assumed to be inde­pendent. This will be true 
if different random num­ber streams or seeds are chosen for the simulation c,f each system. Since simulators 
sometimes use common random numbers (CRN) to sharpen comparisons, we comment on the effect of CRN on 
MCB in Subsec­tion 4.5. All the outputs Mj, for j = 1,2, . . .,ni and i Z= 1,2,..., k, are assumed to 
be normally distributed. This may be approximately true if each Yij is an av­erage of many outputs. All 
the outputs ~j, for j = 1,2,.. .,ni and i = 1,2, . . . . k, are assumed to have common variance U2 = 
Var[Y~j], Vi, j. There is no reason to believe this assumption holds in general. The assumptions of normally 
distributed data and common variance are clearly the most tenuous. How­ever, since simulators can often 
obtain a large number of replications, n, the method of batch means can be used to improve the approximation 
to both assump­tions. Consider the outputs from system i in the balanced case: Y~l,Yiz,..., Yin. If bimi 
= n, then we can trans­form the data into bi batch means of mi outputs as follows: (2) forl=l,2,. . 
. . bi. Notice that the batch means are identical to the macroreplication estimators described in the 
previous sections. We use different terminology because we batch the data after collecting it to improve 
the approximations of normality and equal variance. Let u? = Var[~j]. Then Var[zy] = u~/m~; that is, 
the variance of the batch means can be controlled by the choice of batch size, m~. In addition, the batch 
means will tend to be normally distributed due to the central limit theorem effect. Therefore, by choos­ing 
the batch sizes ml, mz, . . . . mk (equivalently the numbers of batches bl, b2, . . . . bk) appropriately, 
the batch means ~1 will be more nearly normally dis­tributed, and the variances of the batch means more 
nearly equal, than the original data Yij. A drawback of batching is the loss of degrees of freedom from 
~~=1 (ni 1) to ~~=1 (bi I) which affects how sharp the inference is. Goldsman and Nelson (1990) analyzed 
the effect of batching on MCB and found that, for 3 ~ k < 10 systems, very little is lost as long as 
the number of batches is bi z 20; this result holds no matter how many replications, n, are available. 
Therefore, bat thing can (and should) be used provided the simulator has enough data to form at least 
20 batch means.  4.3 Planning The IA approach Schmeiser advocates proceeds until the estimation error 
is small enough that a winner can be declared. The R&#38;S procedure Goldsman ad­vocates is also designed 
to yield a winner. MCPS provide information about the relative performance of the systems, but they are 
typically not designed to produce a winner. However, careful planning of the experiment can insure that 
useful results are ob­tained. Suppose that a difference in performance of more than 8 is considered 
important. Hsu (1988) derived an expression for the sample size, n, required to guar­antee that, with 
high probability y, the system that MCB infers to be the best is in fact within 8 of the true best. This 
power calculation requires an es­timate of 6/a, and is similar in spirit to the second­stage sample-size 
calculation of Rinott s procedure. Since software for Hsu s sample-size calculation is not readily available, 
a crude approximation is to take AL+n-l)jk fJ 2 n> (3) 6 () which approximates the sample size required 
to ob­tain a confidence-interval halfwidth less than or equal to 6. An estimate of u may be obtained 
from a pilot study, and a conservative (small) value of n can be chosen to determine a value of dl_e, 
~(n _ I),k to insert in the formula. Goldsman, Nelson and Schmeiser  4.4 The Example The outline of 
experiments we performed is the same as that described in Subsection 3.3. The data was written to a file, 
and the analysis was conducted using Splus. All experiments and analyses were performed on a DECstation 
3100. 4.4.1 Debugging Experiment Ten replications of system 1 produced a sample mean TTF of ~1 = 60761. 
minutes and a sample standard deviation of S = 71450. minutes. Each replication took about 3 seconds 
of real time. Under the (arbi­trary) constraint of running and analyzing the pilot experiment within 
an hour, this implied that a pilot experiment of n = 200 replications for each system could be performed 
(approximately 40 minutes of real time to generate the data).  4.4.2 Pilot Experiment For each system, 
n = 200 replications of TTF were generated. Different random number seeds were used to initialize the 
experiment for each system to obtain independent data, as assumed by model (1). The simulations act uall 
y used nearly the entire hour. Histograms and quantile-quantile plots of the data from each system indicated 
that the data was not normally distributed. Boxplots of the data indicated that system 1 was somewhat 
more variable than the others; the ratio of the largest sample variance (sys­tem 1) to the smallest sample 
variance (system 4) was almost 2.5. To improve the approximation of normality, the data from each system 
were batched into b = 40 batch means of m = 5 outputs. Visually the data appeared to be more normally 
distributed after the transforma­tion. Using the batch means, 95% simultaneous MCB confidence intervals 
were formed for pi maxl#~ pl, i = 1,2,3,4; the results are displayed in Table 4. Table 4: MCB Pilot 
Experiment lower upper i limit Yi m~#~ Y~ limit 1 -11349. 7091. 25530. 2 -31242. -12802. 5638. 3 -25530. 
-7091. 11349. 4 -36172. -17732. 708. System 1 was the sample best, but since all the intervals contained 
O no system could be declared to be the best. The lower limit on the interval for PI ma~#l ,u~ indicated 
that, if system 1 is not the best, its E[TTF] could be as much as 11349. minutes less than the best; 
since this was greater than the in­ difference zone of 6 = 3000 minutes, a production run was planned. 
Using the pooled standard deviation estimate from the pilot run, S = 92449., the indifference zone 8 
= 3000, and the critical value d0,95)4t40J,4 = 2.078, formula (3) predicted that n x 8200 replications 
would be required to distinguish differences of 3000 minutes. Approximately 42 hours of real time would 
be required for the computer to complete that many replications. Rather than save all of the TTF data 
from the production run, the outputs were batched as they were generated into b = 300 batch means of 
m = 27 outputs each (implying n = 8100 replications; the 200 pilot replications were available to add 
to the data set if needed). Having 300 batch means allowed some flexibility for further batching to achieve 
ap­proximately equal variances whiIe still keeping the number of batches above 20. The production runs 
were executed over a weekend.  4.4.3 Final Results The batch means from all four systems appeared to 
be normally distributed. Applying Bartlett s test for equality of variance yielded a test statistic of 
13.11, which is larger than the 0.995 quantile of a X2 random variable with 3 degrees of freedom; the 
variances did not appear to be equal. However, the test statistic without system 4 was only 0.99, which 
is not signifi­cant, The ratio of the pooled sample variance from sys­tems 1,2, and 3 to the sample variance 
of system 4 was about 1.5. Therefore, approximately equal vari­ances could be obtained by rematching 
the 300 batch means from systems 1,2, 3 and 4 into 61 = b2 = b3 = 100 and b4 = 150 batch means, respectively. 
Bartlett s test on the rebatched data yielded a test statistic of 1.03, which is not significant. The 
95% simultaneous MCB confidence intervals based on the rebatched data are given in Table 5. System 1 
wss conclusively identified as the best sys­tem (with confidence level 0.95), and the best guess is that 
it is superior by 5288. minutes. System 2 was the sample second best, but it may be inferior to sys­tem 
1 by as much as 8284. minutes, which is greater than the indifference zone of 3000 minutes. Table 5: 
MCB for Production Run 1 0 5288. 8284. 2 -8284. -5288. () 3 -12747. -9751. () 4 -25409. -22675. ()  
4.5 Discussion MCPS recognize that selecting the best system is a multivariate-estimation problem, and 
they explicitly account for the joint (overall) error inherent in mak­ing statements about multiple performance 
parame­ters. Informal methods for measuring sampling errclr do not account for the possibility of simultaneous 
er­rors in different directions. MCPS provide inference about not only the best system, but also relation­ships 
among all the systems. In the case of MCB, the difference between the expected performance of each system 
and the best of the other systems is boundecl. These insights are useful when the performance pa~­rameter 
of interest (E[TTF] in the example) does nc,t account for all of the differences among the systems (e.g., 
cost of installation). MCPS can provide infer­ence from a single stage of sampling, although, as the 
example illustrates, experiment planning may be re­quired to guarantee useful results since a winner 
is not guaranteed. The primary disadvantage of MCPS is their reliance on rather strict distributional 
assumptions. Non­parametric procedures exist that remove assumptions (such as normality) on the marginal 
distributions, but they typically have lower power to discriminate dif­ferences. A particularly nettlesome 
limitation is the assumption of independence across systemls, which rules out the use of CRN. CRN is 
a variance re­duction technique for sharpening estimators of dif­ferences. The improvement is obtained 
by inducing positive dependence across replications from each sys­tem, Ylj, Yzj, ..., Ykj, which violates 
an assumpticm of model (l). We have been able to show that MCB is conser­vative, under fairly general 
conditions, when CRN is employed, which means that a true confidence level greater than 1 a is achieved. 
Ideally, the MCP should incorporate GRN to make the inference sharper while preserving the desired confidence 
level. Some progress has been made in developing MCPS that incorporate CRN (e.g., Yang and Nelson 1991). 
ACKNOWLEDGMENTS David Goldsman s work was supported by Na­ tional Science Foundation Grant No. DDM-9012020. 
Barry Nelson s work was supported by National Sci­ ence Foundation Grant No. DDM-8922721. Bruce Schmeiser 
s work was supported by National Science Foundation Grant No. DMS-8717799. The authors acknowledge the 
helpful comments of Jon R. Hill, Ja­ son C. Hsu, Yu-Hui Tao, and Jin Wang. REFERENCES Bechhofer, R. E., 
C. Dunnett, D. Goldsman, and M. Hartmann. 1990, A Comparison of the perfor­mances of procedures for selecting 
the normal pop­ulation having the largest mean when the popula­tions have a common unknown variance. 
Commu­nications in Statistics Simulation and Computa­tion B19, 971 1006. Gibbons, J. D., I. Olkin, and 
M. Sobel. 1977. Select­ing and Ordering Populations: A New Statistical Methodology. New York: John Wiley. 
Goldsman, L., and B. L. Nelson. 1990. Batch-size effects on simulation optimization using multiple comparisons 
with the best. In: Proceedings of the 1990 Winter Simulation Conference, eds. O. Balci, R.P. Sadowski, 
and R.E. Nance, 288 293. Institute of Electrical and Electronics Engineers. Hochberg, Y., and A. C. Tamhane. 
1987. Multiple Comparisons Procedures. New York: John Wiley. Hsu, J. C. 1984. Ranking and selection and 
multiple comparisons with the best. In: Design of Experi­ments: Ranking and Selection, eds. T. J. Santner 
and A. C. Tamhane, 23 33. New York: Marcel Dekker. Hsu, J. C. 1988. Sample size computation for de­signing 
multiple comparison experiments. Compu­tational Statistics &#38; Data Analysis 7, 79 91. Law, A. M., 
and W. D. Kelton. 1991. Simulation Modeling and Analysis. New York: McGraw-Hill. Nelson, B. L. 1992. 
Statistical analysis of simulation results. In: Handbook of Industrial Engineering, Second Edition, ed. 
G. Salvendy, in press. New York: John Wiley. Rinott, Y. 1978. On two-stage selection procedures and related 
probability inequalities. Communica­tions in Statistics A7, 799 811. Schmeiser, B. W. 1982. Batch size 
effects in the anal­ysis of simulation output. Operations Research 30, 556-568. Schmeiser, B. W. 1990. 
Simulation experiments. In: Handbook of Operations Research and Management Science, Volume 2: Stochastic 
Models, eds. D. Hey- Goldsman, Nelson and Schmeiser man and M. Sobel, 295 330. Amsterdam: North Holland. 
Schmeiser, B. W., T. Avramidis, and S. Hashem. 1990. Overlapping batch statistics. In: Proceedings of 
the 1990 Winter Simulation Conference, eds. O. Balci, R. P. Sadowski, and R. E. Nance, 395-398. Institute 
of Electrical and Electronics Engineers. Schmeiser, B. W., and M. D. Scott. 1991. SERVO: Simulation experiments 
with random-vector out­put. In: Proceedings of the 1991 Winter Simula­tion Conference, eds. B. L. Nelson, 
G. M. Clark, and W. D. Kelton, this volume. Institute of Elec­trical and Electronics Engineers. Schmidt, 
J. W., and R. E. Taylor. 1970. Simulation and Analysis of Industrial Systems. Homewood, Illinois: Richard 
D. Irwin. Wilcox, R. R. 1984. A table for Rinott s selection procedure. Journal of Quality Technology 
16, 97 100. Yang, W., and B. L. Nelson. 1991. Using common random numbers and control variates in multiple­comparison 
procedures. Operations Research 39, in press. AUTHOR BIOGRAPHIES DAVID GOLD SMAN is an Associate Professor 
in the School of Industrial and Systems Engineering at the Georgia Institute of Technology. His research 
in­terests include simulation output analysis and rank­ing and selection. He is Secretary-Treasurer of 
the TIMS College on Simulation. BARRY L. NELSON is an Associate Professor in the Department of Industrial 
and Systems Engineer­ing at The Ohio State University. His research inter­ests are experiment design 
and analysis of stochastic simulations. He is Vice President of the TIMS Col­lege on Simulation, and 
is Proceedings Editor for the 1991 Winter Simulation Conference. BRUCE S CHMEISER is a Professor in the 
School of Industrial Engineering at Purdue University. His research interests include input modeling, 
random­variate generation, out put anal ysis, and variance re­duction. He is the current Simulation Area 
Editor of Operations Research and a Member of the Council of the Operations Research Society of America. 
He is an active participant in the Winter Simulation Con­ference, including being Program Chairman in 
1983 and Chairman of the Board of Directors during 1988­1990.  
			