
 SLICC: A Low Latency Interface for Collective Communications Man D. Knies Purdue University 1285 Electrical 
Engineering Building West Lafayette, IN 47907-1285 William J. Harrod Cray Research Into rated 655F Lone 
Oak % r. Eagan, MN 55121 Abstract Several recent parallel computers have implemented logically shared 
physically distributed memory systems which allow processors to directly access memory in other processors 
without interrupting the referenced PE. Because this kind of architecture provides greater jlexibili~ 
for interprocessor communications than private aaliress space computers, different sojlware models can 
be &#38;veloped to take advantage of these machines. In this papec we describe a low-level collective 
communications inteflace specifically tailored for shared aaWess space parallel computers. We also provtie 
performance results for an implementation on the Cray T3D and compare its performance to Cray5 PVM implementation. 
These results show that SLICC can be more than 10 times fmter than PVM on the T3D. 1.0 Introduction The 
most common programming styles used on parallel machines are task-tiented message passing systems such 
as PVM [4] and MPI [7], and data paratlel languages such as CM Fortran [9] and MPL [6]. These styles 
are well suited to the type of machine for which they were originally intended. However, neither of these 
approaches are ideal for shared address space MIMD computers such as the Cray T3D [3] and Convex SPP1 
[2]. To more fully exploit the capabdities of these machines, we have designed and implemented an interface 
called SLICC (Shared address-space Library Interface for Collective Communications). The primary design 
goals of SLICC are low-latency addressing and low-overhead flexible processor partitioning. SLICC achieves 
low latency by specifically tailoring the semantics of the interface to allow implementors to take advantage 
of globally addressable memory. This differs from message passing approaches where the source and target 
addresses of a message maybe F. Ray Barriuso Cray Research Into rated 655F Lone Oak T r. Eagan, MN 55121 
George B. Adarn~ III 12!%% ??=?!?%?%7%!;!?3 unknown until runtime, which makes it difficult to efficiently 
exploit a globally addressable memory. Low-overhead partitioning is achieved by providing differen~ but 
very similar, interfaces to support regular and inegular partitions of processors. A regular partition 
is a set of processing elements (PEs) that can be described by a three element tuple containing the lowest 
numbered PE in the partition, the distance between consecutive PEs in the partition, and the tokti number 
of PEs in the partition, Examples of regular partitions are the set of even numbered PEs, the set of 
odd numbered PEs, and the set of PEs numbered 7 through 34. Such partitions can be specified on-the-fly 
and do not require explicit partition aeation. An irregular partition is any partition which cannot be 
specified using the above technique. Irregular partitions in SLICC require explicit group creation as 
in standard message passing systems. However, once seated, an irregular partition can be subpartitioned 
on­the-fly using the regular partitioning model. This provides SLICC with a hierarchical model for processor 
partitioning. To avoid dictating the kinds of algorithms that will be useful for implementing collective 
communications, the SLICC interface minimizes the semantic requirements of its constituent operations 
while providing a standard addressing and partitioning system, Thus, any library implementing a SLICC-like 
interface is characterized by its functionality, rather than its execution semantics. For example, message 
passing systems can be characterized by their private memory and synchronous communications; data parallel 
languages can be characterized by their statement-level synchronous execution semantics; SLICC is characterized 
by its global addressing and hierarchical partitioning models. Section 2.0 describes the underlying software 
and hardware models and their effect on the semantics of SLICC. Sections 3.0 and 4.0 discuss how SLICC 
implements regular and irregular processor partitioning. Section 5.0 summarizes SLICC performance results. 
1063-9535/94 $4.00 G 1994 IEEE 2.0 The SLICC model and other approaches The SLICC model is applicable 
to any machine that meets the following two requirement any PE can directly access any other PE s memory 
(load and store) without requiring the remote processor s participation and any global address can be 
derived from a PE mnrkr and local address. Because of these requirements, the interface is not well suited 
for machines where each processor has only private addressing capabilities. However, the advantage gained 
is that SLICC has very low overhead communications and hierarchical partitioning mechanisms that provide 
support for both ~gular and irregular partitioning. 2.1 Addressing model Many language approaches have 
been developed that reflect the instruction stream model of the architecture(s) for which they are intended 
(e.g., CM Fortran for Thinkiig Machines computers, MPL [6] for MasPar computem, and message passing for 
Intel architectures). Rather than specifically designing an interface based on the instruction stream 
model, we have concentrated on the data addressing model. Thus, the addressing requirements in the SLICC 
interface are the most stringent pmt of the modek any argument that specifies source or result data must 
reside at the same local address on each processor in the partition for a given routine call. This requirement 
differentiates SLICC from message passing models where there are no restrictions on the address of the 
arguments passed to a function call. The benefit of this restriction is that it allows the library implementor 
to take advantage of the addressing capabilities of a shared address space architecture. While this may 
require the programmer to manually rearrange data locally before using the routines, it also eliminates 
the inefficiencies of more general addressing schemes. Finally, this addressing model allows communications 
to take place without system buffers being implicitly allocated by the SLICC routines or without the 
extra copying associated with implicit buffering or compiler­generated temporary arrays. One drawback 
of the data addressing restriction is that this likely precludes using SLICC in heterogeneous systems. 
In general, ease of use has been traded to gain partitioning flexibility and communications with very 
low overhead.  2.2 Partitioning One expressive technique for writing clear and concise parallel programs 
is to group PEs into partitions and specify an operation on that partition. In message passing systems 
such as MPI or PVM, groups of processors are explicitly created by the programmer before collective communications 
occur. While this provides a natural and easy to understand interface, there is overhead associated with 
creating partitions. In the case of a fine-grained computation, execution may be required on many different 
partitions in rapid succession. For example, suppose an N by N matrix is stored in the memory of a parallel 
machine with P processors such that each processor contains N/P columns of the matrix. Now suppose the 
computation being performed require the diagonal elements of the matrix to be broadcast to the processors 
containing the elements to the right of the diagonal as in Figure 1 below: PEo PE1 PE2 PE3 I-i=-l=--l 
  EE131 U.-l_kl Figure 1 N=4, P=4 Example In this example, the programmer would need to be able to specify 
0(P) different partitions during the calculation, where P is the number of PEs over which the matrix 
is d~tributed. In a message passing system, mid­computation creation of partitions can incur additional 
overhead because inter-PE communications may be required. Preallocation of partitions may not be practical 
for large problems where the number of partitions requimxl is a function of the problem size. In SLICC, 
the programmer still has the simplicity of using a collective communications interface, but none of the 
partition creation overhead is required. This partitioning capability provides SLICC with fine-grained 
collective communications capabilities reminiscent of array slice notation in languages such as High 
Performance Fortran (HPF) [5], CM Fortran, Vienna Fortran [11], and MPP Fortran [8]. Thus, while the 
SLICC programmer has access to a structured partitioning interface to allow concise expression of complex 
operations, they are not constrained by a tightly synchronous execution model. This allows the programmer 
to avoid extra synchronizations and thus optimize for a sum of max versus max of sums rnntime advantage. 
 23 Relaxed semantic restrictions 3.1 General format of collective routines Because SLICC is intended 
to provide a low level software base, the routines have very little support for error checking or other 
user level features. For example, SLICC routines do not perform a synchronization before or after collective 
operations. Thus, if the data required by one operation is needed in the nexg the programmer is responsible 
for manually adding barriers or otherwise insuring that separate collective operations do not interfere 
with each other. While this approach places a greater burden on the programmer, this loosely synchronous 
model allows non-conflicting operations to be initiated on overlapping partitions of processors even 
if a previous collective operation may not have completed on all its PEs. l his is consistent with the 
fact that most parallel computer designs are MIMD rather than SIMD. SLICC is intended to provide a layer 
of software that optimizes communications intensive operations. As such, its current implementation allows 
only one datum per processor per function call (integers or reals for computational routines, blocks 
of data for data movement routines). It is however, possible to specify multiple operations on independent 
data in a single function call. This allows more sophisticated algorithms that provide better communication 
and computation load balancing (a good example of such an algorithm is described by van de Geijn in [10]). 
Thus, the design is intended to allow efficient implementation on a small scope of functiontilty. This 
allows the interface to be implemented quickly using a variety of algorithms and for other software libraries 
to be layered on top of it. One of our major design criteria was to create a model that allowed the programmer 
access to lower level management of data and addressing. Another approach to parallel programming which 
allows similar generalky is the split-phase communications approach [1]. With the split-phase model, 
the programmer can explicitly avoid redundant setup operations and overlap communications and computation. 
Thus, the split-phase model allows the programmer to hide and optimize overhead in their program and 
is applicable to a wide set of architectures. 3.0 Regular partitioning model The SLICC partitioning model 
is designed to allow the masking and partition creation mechanisms of data parallel and message systems 
to be replaced by a unified partitioning tuple. In this section, we will provide several examples of 
a regular partitioning interface and its use. In Figure 2 below, the data u&#38;ress arguments, source 
and target, are used to indicate the locat address on each processor where the source data can be found 
and where the results are to be written. These data addresses must be the same on all processors participating 
in the call.: some_collective_operat ion ( target, /* Data address */ source, MI_info, 1* Multi-issue 
info */ PE_f irst, / Partitioning tuple*/ PE_stride, PART_s i ze pWrkD, pWrkS ) / * Workspace / Figure 
2 General format of SLICC routines The multiple issue informatwn, MI_inf o, argument provides the number 
of independent operations that are to take place. If this argument has value n, then a vector of n independent 
operations are performed the first operation uses the datum at offset zero from the source address on 
each PE and the results written at offset zero from the target address on each PE, the second operation 
uses the datum at offset one from the source address and the results are written at offset one from the 
target address, and so on. The partitioning tuple arguments specify which PEs are active in a given call 
to a SLICC routine. The first tuple parameter, PE_f irst, indicates the lowest physically numbered PE 
to be active. The second parameter, PE_s t ride, indkates the log2 of the difference between participating 
processors PE numbers. The log2 is used (rather than the linear difference) to enforce a positive power-of-hvo 
stride for our implementation (which helps to make addnxs calculations fast). The third parameter, PART_S 
i ze, indicates how many processors are active in the call. For example, a partitioning tuple of [ O, 
0, 8 ] would specify PEs O through 7. If the partitioning tuple were [ 7, 1,5 ], then PEs, 7,9, 11, 13, 
and 15 would be implied. The following restrictions are placed on the partitioning tuple rdl values specified 
in a tuple must be non-negative, PART_s i ze must be greater than zero, and the squence of PEs implied 
by a tuple must be monotonically increasing. The third restriction implies that the value of the equation 
below: 1 + PEJirst + PAR T_size x 2pE-stride must be less than the total number of PEs in the system. 
In the SLICC model, the programmer is responsible for makhg sure that each of the PEs specified by the 
partitioning tuple calls the same routine and that the tuple parameters have the same values on each 
PE in the partition. The partitioning tuple allows a programmer to use many different partitions without 
the extra interPE commun ications associated with partition management. his is possible because the regular 
partitioning model allows the SLICC implementation to easily virtualize the processor numbers active 
in a given opemtion. When a SLICC routine is calle~ each processor computes its virtual PE number (offset 
within the active partition) ax mvp = (mype -PE_first) >> PE_stride; Where mype is the processor s physical 
PE number, mvp is the processor s virtual PE number, and >> is the C language bit shift right operator. 
The virtual PEs for a partition are thus numbered from O to PART_size-1. This allows algorithms which 
have already been written for an unpartitioned machine to be used to implement a SLICC routine with no 
algorithmic changes. When an algorithm needs to communicate with a particular virtual PE, the above formula 
is inverted to yield a physical PE number and the communication takes place. Thus, the only additional 
work needed to implement this interface is converting between physical and virtual PE numbers within 
the routines (which is only a few cycles on current RISC processors). The workspace parameters, pWrkD 
and pWrkS, provide shared data buffers and workspace to be used for memory locks and scratch space. This 
means that setup for the values passed through these arguments will generally be system specific, but 
the semantics of the operation performed are not. This allows the interface to be implemented efficiently 
on a variety of machines without completely fixing the interface. Thus, porting could require different 
handling and setup of the workspace arguments.  3.2 Multiple partitions and non-unit striding In the 
following example, we demonstrate two concepts in one picture: multiple partitions executing different 
operations and non-unit stride partitions. Suppose we have a 4 by 4 grid of processors (numbered consecutively 
across rows) and wish to execute a reduction on each even column of the processor grid and broadcast 
blocks of data of size nwords up the columns (see Flgule 3). [] Broadcast Raductbn Figure 3 Example To 
do this, each processor needs to compute PE_first, the number of the first PE in each column. Along with 
PE_f irst, a PE_stride of 2 (distance of 4) and a PART_s i ze of 4 are needed to specify a column. In 
the code below, broadcast ( ) takes a block of data of size nwords starting at location &#38;from on 
processor broadcast_PE and copies it to location &#38;to on the other PEs specified by the partitioning 
tuple [ PE_f i-rst,2,4 ]. The arguments to reducetoall ( ) are exactly those shown in Figure 2. The pwrkS 
argument provides shared scratch SW(X. /* first PE in each PE s column */ PE_f irst = MOD ( My_ PE_number, 
4 ) ; / compute reduction on even COIS / if ( MOD(PE_first, 2) == O ) { reducetoall ( &#38;targetV, 
&#38;sourceV, . 1, PE_first, 2, 4, pWrkD, pWrkS ) ; / * broadcast on odd columns / } else { / last PE 
in each column / broadcast_PE . PE_f irst + 12; broadcast ( &#38;to, &#38;from, nwords, broadcast_PE, 
PE_first, 2, 4, pWrkSl ) ; } There is no requirement that the processors be physically arranged in a 
processor grid although that is the appearance that is provided in this example. Also, although the code 
for the reductions and broadcasts are written as parts of the same conditional, there is no requirement 
that they be executing synchronously. Thus, if there were operations previously executed on these subpartitions 
or on overlapping subpartitions, execution can proceed without having to wait for all the PEs to arrive 
at thk code segment. This provides a form of collective communication pipelining when a sequence of operations 
does not require synchronization between events. For example, if we wanted to follow the calculations 
shown in Figure 3 with an operation across the rows of the matrix (see Figure 4), step1: Step 2 A If 
1 Figure 4 Continued example then we would add the following code at the end of the previous example 
/ division with truncation *I PE_first . My_PE_number / 4; reducetoall ( &#38;targetV2, .&#38;sourceV2, 
1, PE_first, O, 4, pWrkD2, pWrkS2 ); In this example, each oftherow reductions couldproceed as soon 
as the operations from the previous code were completedon that row since there is no data depmdence between 
the operations. That is, each PE involved in a row operation could proceed without having to wait for 
operations on other rows to complete. If there were data dependencies between the operations, the programmer 
would need to explicitly separate them with a synchronization point. The addressing restriction requiring 
the use of the same address within an operation is shown in the examples above. However, the source and 
target addresses of the operations can be different provided the calls are in different partitions. 4.0 
Irregular partitioning model Irregular partitions are defined to be any group of PEs that the programmer 
wishes to group together (thus, the set of irregular partitions is a superset of the set of regular partitions). 
The interface for dealing with irregular partitions is identical to the regular interface except for 
the adWion of a partition identifier argument, Simitarly, the semantics, parameter lists, and implementation 
can be easily understood in terms of the regular interface. The main difference in usage of the irregular 
interface is the use of group identifiers. Any operation on an irreguhir partition must specify a processor 
group which has previously been created. To create a processor group, each PE that is to be a member 
of the group must call create_group ( ) and pass a list of all the other PEs that will be in the group. 
The order of this list is important as it defines the virtual to physical processor numbering translation 
map (within the partition). Thus, each PE must call create_group ( ) with the same PE lis~ and the list 
must be in the same order on each PE. The group identifier returned by crest e_group ( ) is a strictly 
private value in the sense that one cannot send the group identifier s value to another processor and 
then have that processor use the value in a collective catl (the exact value type returned by crest e_group 
( ) k unspecified for this discussion, but it could be a pointer, an integer, or some other value type 
that can identify a partition). Once a group has been created, use of irregular SLICC routines is the 
same as for the regular partitions except for the addition of the group identifier argument. The following 
example shows code that will create three separate irregular partitions and execute a reduce toall ( 
) operation across each one independently. All variables are assumed to be private (there is a copy on 
each PE): / virtual PEs O -4 in group A / int A[] = ( 0 ,6,12,2,7 ), sizeA = 5; / virtual PEs O -7 in 
group B / int B[] = { 15,1,3,5,8,9,10,11 }, sizeB = 8; /* virtual PEs O -2 in urou~ C / int C[] = ( 4,14,13 
}, ­sizeC = 3; int size; if ( in_list(mype, A) ) { grp_id = create_group ( A); size = sizeA; } else 
if ( in_list (mype, B)){ grp_id . create_group ( B); size = sizeB; ) else if ( in_list (mype, c)){ grp_id 
= create_group ( c); size = sizeC; } / * execute 3 independent reductions on partitions A,B, C */ reducetoall 
( grp_id, &#38;target, &#38;source, 1, 0, 0, size, pWrkD, pWrkS ) ;  Similarly, we could have executed 
the reduction on only the even virtual PEs in each partition (PEs [0, 12, 7 ] in group A, PEs [ 15, 3, 
8, 10] in group B, and PEs [4, 13 ] in group C) by writing: / execute 3 independent reductions on only 
the even virtual PEs in partitions A, B, and C / reducetoall( grp_id, &#38;target, &#38;source, 1, 0, 
1, (size+l)/2, pWrkD, pWrkS ); Another feature of this model is that the implementation of the irregular 
routines is straightforward. Since the regular routines are implemented by lint virtualizing physical 
PE numbers relative to the current partition, the same routine implementations can immediately be adapted 
to the hregular interface by having each PE first lookup up ita virtual number in its group (this would 
be computed at group creation time, rather than actually searching the PE list) and then proceed with 
the same code as in the regular implementation using the virtualized PE number as the physical PE number. 
When a processor needs to communicate with another processor, it reverses the process by looking up the 
physical PE number of the desired virtual PE in its local PE list for that partition. Thus, the entire 
irregular interface can be implemented with ahnost the same code as used for the regular case, and the 
only software overhead is the cost of an extra local array reference for each remote processor reference. 
The performance will also be affected by the topological aspects of the given irregular partition. For 
example, if the partition is allocated along a linear row of processors in a mesh connected computer, 
network conflicts are likely to be gwter than if a more tightly packed group of PEs were used. On the 
other hand, since each PE has a complete list of the other physical processor numbers in the partition, 
the implementor could choose to try to optimize communications with knowledge of the target system s 
interconnect topologies and attributes. Thus, the interface does not preclude system specific topological 
optimization. This approach also allows for easy remapping of partition processor numbers by changing 
the order of the PE partition lists in the processors. This allows easy instrumentation and study of 
different network performance issues and mapping decisions. 5.0 SLICC performance results The SLICC 
interface for regular partitions is fully implemented on the Cray T3D (including routines for reduction, 
broadcast, barrier, and global collect), We present performance numbers for SLICC and compam with the 
PVM broadcast and barrier routines of the Cray PVM library (the version optimized for the T3D, not the 
network version). We have not implemented the irregular interface, but expect that its performance should 
be nearly that of the regular case based on the discussion above (and subject to network delays). The 
following tables show results from SLICC on the Cray T3D. The times for the PVM message passing communications 
interface do not include any setup overhead --only the time for the actual operation. Table 1. Cray T3D 
configurations used PEs Configurations (XxYxZ) 4 1X2X2 16 4x2x2 64 8x2x4 256 16x4x4 The interconnection 
network of the Cray T3D is a three dnensional torus of nodes. Each node has two processors that share 
its link into the network. Table 1 above describes the shapes of the partitions used for the experiments 
in this section. The performance experiments were conducted on non­dedicated Cray T3D with 16 megabytes 
of memory per PE running UNICOS-MAX release 1.1. All PVM routines Table 2. Broadcast performance on the 
Cray T3D (in microseconds) PES System 8 bytes 32 bytes 128bytes 512 bytes 2K bytes SK bytes 32K bytes 
4 PVM 156 157 164 182 229 402 1,089 4 SLICC 8 9 10 16 41 140 529 16 PVM 215 216 229 237 351 887 3,030 
16 SLICC 12 14 17 29 77 276 1,047 64 PVM 278 277 291 381 928 3,240 12,667 64 SLICC 13 16 22 40 112 405 
1,401 were called from the Cray PVM-3 release 1.1 library (which implements PVM version 3.2). The SLICC 
routines were called from Cray s Shared Memory Access library (libsma.a) release 1.1. Each expaiment 
was run 100 times and consisted of the following sequence of operations: flush cache, hardware barrier 
to synchronize the PEs, timer starL SLICC or PVM operation, timer stop. The time from the slowest PE 
from each run was recorded and the average over the 100 runs is reported in the tables. All times are 
in microseconds. The results in Table 2 demonstrate the low-overhead character of SLICC relative to PVM 
on the T3D. For small broadcasts, SLICC is as much as 20 times faster than PVM. Additionally, PVM time 
increases linearly with the number of processors for larger message sizes. The semantics of SLICC allow 
it to use a pipelined version of broadcast which is much more efficient for larger numbers of PEs. Table 
3. Reducetoall time on Cray T3D (in wsec) Number of Sbnuttaneous Reductions 4 PES 16 PEs 64 PEs 256 PES 
1 26 38 57 69 4 29 44 66 81 16 42 63 88 110 64 56 86 115 138 256 126 166 201 225 1,024 366 480 530 573 
4,096 1,363 1,733 1,886 1,931 16,384 5,309 6,827 7,265 7,412  Table 3 shows the timings for reducetoall 
( ), which computes a sum reduction across a set of active PEs. There is only one element per PE for 
each reduction although the interface allows for vectors of independent reductions to be performed with 
one function cd. Notice that the problem size scales linearly with the number of PEs implicitly, so that 
at 256 PEs, the problem being solved is 4 times larger than that at 64 PEs. The numbers above demonstrate 
the jxxformance advantage gained by this functionality as the throughput of the system is much greater 
for larger numbers of reductions. The reducet oal 1 ( ) data above k not available for PVM as the only 
collective routines supported in Cray s current release of PVM are broadcast ( ) and barrier ( ) (although, 
a new release implementing a newer version of PVM containing these routines will be available in the 
future). At the time of this writing, we are unaware of a version of MPI or similar message passing system 
which would provide for a more complete comparison on the T3D. Table 4. Barrier performance on Cray T3D 
(in psec) System 4 PEs 16 PEs 64 PES 256 PEs PVM 15 27 39 51 SLICC 7 12 18 24 In Table 4, PVM and SLICC 
use only software to execute a barrier (none of the T3D barrier hardware is used). This is to demonstrate 
the speed of a barrier implemented only in software. For comparison, the speed of the T3D hardware barrier 
network (which is accessible from both PVM and SLICC) is approximately 1.7 microseconds. 6.0 Conclttsions 
In order to utilize low-overhead, fine-grained features of shared address space architectures, we have 
developed and implemented SLICC, a new implementation driven model for collective communications, This 
interface is intended for machines that support shared address spaces (such as the Convex SPP1 and Cray 
T3D). However, its restrictions on the data passed to library calls allows all library operations to 
be executed without any interprocessor communications for setup and minimizes the amount of software 
overhead required to perform the operations. Because SLICC is a low level software layer, it may not 
always be suitable for general applications development but it can provide a foundation for the development 
of other libraries or tools. Although SLICC places a greater burden on the progmmmer than either the 
message passing or data pamllel approaches to collective communications, given the large communications 
performance improvement this is a reasonable trade-off. Whh this in mind, programming details such as 
synchronization and work space management have been intentionally left to the programmer. 7.0 Acknowledgments 
The work in this paper would not have been possible without the support, knowledge, and advice of Sandra 
Carney. The design was influenced by Peter Rigsbee, Paul Fredrickson, John Kyle, Tom MacDonald and Brad 
Kline. We also thank Peter Rigsbee for providing the performance numbers for PVM on the T3D. 8.0 References 
[1] Vasanth Bala, Shlomo Kipnis, Larry Rudolf, and Marc Snir, Designing Efficient, Scalable, and Portable 
Collective Communications Libraries; SIAM 93 Conference on Parallel Processing for Scientific Computing, 
Norfolk, VA, March 1993, pp. 862-872. [2] CONVEX Computer Corporation, Exemplar Architecture, CONVEX 
Press, Richardson, TX, 1993. [3] Cray Research Incorporated, Cray T3D SystemArchitecture Overview, 1993. 
[4] Al GeisL Adam Beguelin, Jack Dongarra, Weicheng Jiang, Robert Mancheck, and Vaidy Sunderam, PVM 3.0 
User s Guide and Reference Manual. available from netlib@oml.gov. [5] High Performance Fortran Forum, 
High Performance Fortran Language Specification/ version 1.0, May 3, 1993. [6] MasPar Computer Corporation, 
MasPar Parallel Application Language (MPL) Reference Manual, MasPar Computer Corporation, 1990. [7] Message 
Passing Interface Forum, DRAFT Document for a Standard Message Passing Interface, August 14, 1993. [8] 
D. Pase, T. MacDonald, and A. Meltzer, MPP Fortran Programming Model, Cray Research Inc., March 1992. 
[9] Thinking Machines Corporation, CM Fortran Reference Manual, 1991. [10] Robert van de Geijn, LAPACK 
Working Note 19 , University of Tennessee. [11] Hans Zima, Pete Brazany, Barbara Chapman, Piyush Mehrotxa, 
and Andreas SchwaId, Vienna Fortran -A Language Specification Version 1.1, Technical Report ACPC/TR 92-4, 
Austrian Center for Parallel Computation, March 1992.  
			