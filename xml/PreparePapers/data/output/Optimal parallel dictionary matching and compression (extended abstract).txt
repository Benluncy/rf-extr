
 Optima] Parallel Dictionary Matching and Compression (Extended Abstract) Nlartin Farach* S. Mutlmkrislman~ 
Rutgers University DIMACS April 26, 1995 Abstract Emerging applications in multi-media and the Human 
Genome Project require storage and searching of large databases of strings a task for which parallelism 
seems the only hope. In this paper, we consider the parallelism in some of the fundamental problems in 
compressing strings and in matching large dictionaries of patterns against texts. We present the jirstwork-optimal 
al­ gorithms for these well-studied problems including the classical dictionary matching problem, optimal 
com­ pression with a static dictionary and the universal data compression with dynamic dictionary of 
Lempel and Ziv. All our algorithms are randomized and they are of the Las Vegas type. Furthermore, they 
are fast, working in time logarithmic in the input size. Additionally, our algorithms seem suitable for 
a distributed implementa­tion. 1 Introduction Large data bases of strings from multi-media applica­tions 
and the Human Genome Project are now avail­able on-line. The size of such information makes com­pression 
essential. Furthermore, one must be able to search through such data bases quickly. AS the size of data 
bases increases and users demand quicker turn­ *farach@cs. rutgers. edu; Work done while this author 
was a Visitor Member of the Courant Institute of NYU. t~uthu@dtiacs. rutgers. edu; Supported by DIMACS 
(Center for Discrete Mathematics and Theoretical Computer Science), a National Science Foundation Science 
and Technology Center un­der NSF contract STC-880964S. Permission to mzke digitalfhard copies of all 
or plrt of this material willv out fee is granted provided thot the copies are not made or distributed 
for profit or commerckil aclvmt:lge, the ACh4 copyrighUscrver notice, the title of the publimtion and 
ils (i:de tippwr, :md notice is given that copyright is by permission o f tbe Association for Computing 
Machinery, Inc. (ACM). To copy otherwise, to republish,to post on servers or to redistribute to lists, 
requires specific permission mci/ur fee. SPAA 95 Santa Barbara CA USA(C 1995 ACM 0-89791-717-0/95/07.$3.50 
around times, parallelism offers the only hope of meet­ing both challenges. In this paper, we consider 
par­allelism in some of the fundamental problems in com­pressing strings, and in matching large dictionaries 
of patterns against texts. These two areas of study have an intimately linked history and are amongst 
the most intensively studied problems in Computer Science. (For compression see e.g. [25] and for dictionary 
matching see e.g. [3, 18, 22, 5, 4]). In this paperj we present the first work-optimal algorithms for 
these problems in a paral­ lel setting. Furthermore, all of our algorithms are fast, working in time 
logarithmic in the input size. Compression S themes A wide variety of compres­sion schemes exist in the 
literature [25]. Amongst the most powerful, and the ones that will be addressed in this paper, are the 
so-called dictionary schemes. These schemes can generally be described as follows. Suppose the prefix 
S[1, i] of string S has been compressed. Then replace a prefix of S[i + 1, n] with a reference to some 
word from a dictionary . If this word is of length k, then we will have compressed the prefix S[1, i 
+ k]. Two issues arise: what is the dictionary> and how do we pick the best word from this dictionary 
so as to minimize the number of references in such a parsing. The well-known LZ1 compression scheme of 
LempeI and Ziv [20] makes the following choices. The dictio­nary is all substrings S[z, y] of S such 
that z < i. In this case, since the dictionary is always changing, that is, new strings are added to 
the dictionary as longer prefixes get compressed, this scheme is known as a dy­namic dictionary compression 
scheme. As far as how a match is chosen from the dictionary, it is greedily taken to be the longest match. 
Since matches are substrings of strings, rather than say prefixes, this greedy heuristic is provably 
optimal. By optimal we mean that it gives tile fewest diet ionary references. By contrast, static compression 
schemes are those in which the dictionary of words is fixed. Then the goal is to minimize the number 
of dictionary references needed to parse the string. A typical assumption, and one we make in this paper 
as well, is that the dictionary consists of the input words and their prefixes. Such dictionar­ies are 
said to have the prefix pTopeTty. The question of how a match is selected now becomes problemati­cal. 
The greedy heuristic of always choosing the longest mat ch need not give optimal compression. A variety 
of other sub-optimal heuristics (Longest Fragment First, etc.) have been proposed [25]. In this paper, 
we will only consider optimal parsing. One of the crucial subtask for static dictionary com­pression 
is to find matches from a dictionary of patterns at some locations in the text. Of course this is simply 
the dictionary matching problem which is defined be­low. This problem has a rich independent history 
in the string matching literature. The Dictionary Matching Problem. Given a set of patterns called the 
dictionary that can be pre­processed, the problem is to find, for each position in a text string given 
on-line, the longest pattern that occurs at that position. The natural parameters for dictionary D = 
{Pl, . . . . Pk} are d = ~~=1 \Pil and m = m-{ lPi I}. The goal is to make the matching work independent 
of the dictionary size. 1.1 Our Results Our results are obtained on the arbitrary CRCW PRAM. All our 
algorithms are randomized and they are of the Las Vegas type. Dictionary Matching: First consider the 
case when the strings are drawn from a constant-sized alpha­bet. We present a randomized algorithm that 
pre­processes the dictionary in O(log d) time and O(d) work. Following that, it matches a text string 
of length n against this dictionary in O(log d) time and O(n) work. The preprocessing and matching work 
bounds are clearly optimal. The previously known work bound for dictionary processing and text matching 
in this case are O(d@@@) and O(n/@@, respectively [22]. A natural model in string matching is the compar­ison 
model, commonly referred to as the case when the alphabet size is unbounded. We derive an al­gorithm 
for this case that has an additional log 1X1 multiplicative factor over the bounds above in both the 
time and the work; here, E is the alphabet set. On this model too, the work bounds of our algo­rithm 
are optimal. For the special case when the alphabet size is poly­nomial in the input size, the classical 
algorithm of [3] for dictionary matching can be implemented with randomization in O(n) sequential time 
and space. For this case, we obtain a suboptimal al­gorithm: our algorithm has an O(log log d) extra 
factor in the dictionary and text processing work (with no penalties on time). An important task in our 
algorithm is the construction of a suflix tree this has the well-known bottleneck of parallel inte­ger 
sorting for which the best known algorithm is suboptimal by an O(log log d) factor in this case [8]. 
The work bound of our algorithm improves on the previously best algorithm [22] unless the alphabet size 
is superexponential in m. In what follows, we assume that the alphabet size is constant, as is standard 
in the literature on compression techniques [25]. However, the bounds we quote below can be easily modified 
to include other cases in a manner analogous to the dictio­nary matching case above. Optimal Static 
Compression: We give a work op­timal algorithm for static dictionary compression. Our algorithm pre-processes 
the dictionary of size d in O(log d) time and O(d) work. Given a string of length n, our algorithm generates 
its optimally compressed string in O(log d+log n) time and O(n) workl. The previously known best algorithm 
for this prob­lem takes time 0(log2 n) and 0(n3 log2 n) work, or alternately, takes time O(log n) and 
0(n4 log n) work [2] after Q(d log d) work for preprocessing. Dynamic Dictionary Compression: We present 
the first known optimal algorithm for LZ1 com­pression as well for its uncompression. Our alge­rithm 
constructs the LZ1 compression of a given string of length n in O(log n) time and O(n) work. Also, given 
the LZ1 compressed version of a string of length n, our algorithm reconstructs the string in O(log n) 
time and O(n) work. Here we make the standard assumption [23] that n is known. The previously known best 
algorithms perform O(n log n) work for compression [23, 10] as well as for uncompression [23].  1.2 
Technical Contribution The suffix tree of a string (defined in Section 2) is a versatile data structure 
in string processing. All our algorithms crucially rely on a recently discovered algo­ rithm for constructing 
the suffix tree of a string [11]. While this work-optimal algorithm paved way for our work optimal algorithms 
for dictionary matching and compression, we stress that the suffix tree construction was not the inherent 
bottleneck in the previously best 1Recall that the optimally compressed string is one with the fewest 
possible dictionary references. results we have quoted for these problems. For instance, the dictionary 
matching algorithm in [22] does not rely on suffix trees at all. The bottleneck in the previosuly known 
best bounds for static optimal compression [2] lies in computing shortest paths in graphs. Dictionary 
Mat thing. Within two years of the dis­covery of the classical linear time string matching algo­rithm 
due to Knuth, Morris and Pratt [19], Aho and Corasick [3] designed a linear time (hence, optimal) al­gorithm 
for dictionary matching by generalizing the fi­nite automaton construction in [19] to a set of strings. 
In the mid-eighties, Galil [12] and Vishkin [27] designed the first work-optimal string matching algorithms, 
which have since been extended significantly [28, 13, 9]. How­ever, a work-optimal algorithm for dictionary 
match­ing has remained elusive. As in the case of [19], the finite automaton based approach of [3] is 
inherently se­quential. Recent progress on parallel dictionary match­ing [4, 5, 18, 22] based on alternate 
techniques has only yielded suboptimal bounds. Our work-optimal results for dictionary matching are obtained 
by judiciously combining ideas from [5] and [22] to solve a generalization of dictionary matching, called, 
dictionary substring matching (see Section 3). We crucially employ a novel data structural primitive 
that we call the nearest colored ancestors problem on trees. The sequential version of our solution for 
this problem has already been applied successfully in com­pilers for Object Oriented Programming Languages 
[21]. Also, interestingly, we have devised a very fast proce­dure that checks the output of the basic 
Monte Carlo dictionary matching algorithm: therefore, our dictio­nary matching algorithm is of the Las 
Vegas type. Static Dictionary Compression. We obtain our work-optimal algorithm for this problem by discovering 
and exploiting the structural properties of optimal pars­ings. In particular~ we show that it suffices 
to consider certain dominating dictionary references to determine the optimal parsing (See Section 4). 
In contrast, previ­ous approaches to this problem have relied on applying a general purpose shortest-paths 
routine [2]; this step is work-inefficient due to the well-known transitive closure bottleneck [16]. 
Dynamic Dictionary Compression. LZ1 [20] and LZ2 [30] are two well-known dynamic compression schemes. 
LZ 1 is known to give better compressions in practice; for example, see Unix compress and gnuzip. Nonetheless, 
LZ2 is implemented in practice because of the simplicity of its sequential implementation. Curi­ously, 
while we provide optimal work RNC algorithm for LZ 1 compression, LZ2 is P-Complete [1] (hence un­likely 
to have (R)NC algorithms). Versions of our algorithms seem suitable for dis­tributed implementation on 
a network of workst~ tions [24]. In fact, in this setting, we can conclude from Communication Complexity 
that even checking equal­ity of strings requires randomization for efficiency [29]. Thus the randomization 
in our algorithms seems well justified. Section 2 contains a review of results we use. Our al­gorithms 
for dictionary matching, dynamic compression and static compression are described in Sections 3, 4, and 
5 respectively.  2 Models, Tools and Definitions For a string s, its suffix tree T is a compacted trie 
of its suffixes. Each edge in T is labeled with a substring of s and the substrings marking the edges 
out of a node all have distinct first characters. The child of u that has the edge marked by a substring 
with leading symbol a is called the a-child of u; any node in the subtree of the a-child of u is called 
an a-descendant of u. We denote the concatenation of the labels on the path from the root to a node u 
as a(u). T has Is I leaves which are in one-to-one correspondence with the suffixes ofs. For a leaf u, 
a(u) is the suffix that corresponds to u. For each node u for which a(u) = aa for some alphabet a and 
string a, a sufiz link is a pointer from node u to node v in the suffix tree where a(v) = ct. we call 
this as the a­slink. ~From the properties of suffix trees, it follows that the suffix links are well-defined. 
In referring to strings, we write S1 S2 to denote the suffix of S1 after removing the prefix S2. All 
our algorithms are derived on the arbitrary CRCW PRAM. All the expected time and work bounds of our algorithms 
hold with high probability y, that is, the failure probability is polynomially small is the input size. 
We use the following results in parallel algorithms. Lemma 2.1 ([11] ) Given a string of length n, its 
suf­fix tree can be constructed in O(log n) ezpected time and O(n) expected work if the string has constant-sized 
al­phabet, and in O(log n) ezpected time and O(n log log n) ezpected work if it has a polynomial-sized 
alphabet. Lemma 2.2 ( [14]) Given a graph on n vertices and m edges, its connected components can be 
determined in O(log n) time and O(m) work.. Lemma 2.3 ([7] ) Given an array A of n numbers, it can be 
pre-processed in O(log n) time and O(n) work so any range-mazima query (that is, given [i, j], return 
the mazimum vazue in A[i], A[i + 1], . . . . Au]) can be pro. cessed in O(1) time and work. Lemma 2.4 
([6] ) Given an mmy, A, of n numbers, we can compute, for each location i, the nearest posi­tion j such 
that j < i and Au] < A[i] in O(log logn) time with O(n) woTk. Lemma 2.5 ([26] ) A subset of numbers from 
-the uni­verse 1 . . . N can be maintained under insert, delete, extract mazimum or minimum and jind 
predecessor or successor queries in O(log log N) time using O(s) space where s is the size of the subset. 
Lemma 2.6 ([7] ) Given a sufiz tree of a string T, it can be p? e-p? ocessed in O(log n) time and O(n) 
work foT string equality queries (that is, is T[i 0.. (i + 1)] = T~. . . j + /] for some i, j, l), and 
more generally, longest common prefiz quem es (that is, what is the longest com­mon prejiz between T[u. 
. . u + /1] and T[ZJ. . . z + /2] for some nonnegative integers u, z, il, 12) can be answered in O(1) 
time and wor k per query. Lemma 2.7 ([11]) Given a tree of n nodes with some marked nodes, the nearest 
marked ancestor of each node in the tree can be computed in O(log n) time and O(n) work. 3 Dictionary 
Matching 3.1 Algorithmic Outline In this section, we provide optimal parallel algorithms for dictionary 
matching. Formally, the dictionary matching problem is as follows. A dictionary D = {P,,..., P~} such 
that ~~=1 IPi [ = d is given for preprocessing. Queries of the form T[l, n] are pre­sented and the output 
is ill[l, n] such that iW[i] = j if T[i, i + [Pj I 1] = Pj; that is, if Pj matches at T[i], and no longer 
pat~ern from D also matches at T[i] 2 In what follows, let D be the concatenation of the patterns in 
the dictionary. We solve this problem in two steps. Step 1. We first solve the dictionary substring prob­ 
lem. That is, for each text position T[~, we determine S[i], which is the longest substring in D (not 
necessar­ily a pattern in the dictionary) that occurs there. This sub~tring S[i] is specified as a pointer 
into the sut%x tree of D, that is, as a pointer to the edge (u, v) and a length 1, such that S[i] is 
a prefix of a(v) of length [a(u) I + 1. Step from 2. S[i]. For each text position T[i], we determine 
ilf[i] 2We matches can at take T[i]. M[i] to be some length O pattern if no Pj Outline of Step 1. 
We perform this in two substeps. Fix C, an integer to be determined later. Step 1A. We determine S[i], 
for each location i = kC, for integer k G [1, n/z]. This substring is specified as an edge (u, v) and 
a length 1. This is done as in [5]. We briefly sketch the approach there. We first co~struct a separator 
decomposition of the suflix tree of D. Then we trace down from the root starting from each of the desired 
text locations independently. The key is that string comparison along the edges and separators are done 
using fingerprints [17]. Step lB. We now compute S[i] for all positions which are not a multiple of ,C. 
We process each window T[k,C-(,C -1),..., k~] independently, for integer k E [1, n/L]. Given S[i], we 
show how to compute S[i -1] via a procedure called EXTEND LEFT procedure. Start­ing with S[k,C] and applying 
EXTEND LEFT repeatedly, we can compute S[i] for all i within the window under consideration. To implement 
EXTEND LEFT we make the following observation. Observation 1 S[i 1] is th:longest prefiz of T[i 1] o 
S[i] that is a $ubstring in D, where o denotes string concatenation. We also use the following structural 
property of the suf­fix links. Let p(w) denote the nodes in order on the path from the root to node w. 
Let S[i] fall on edge (u, v) with length 1. Observation 2 The possible substrings S[i 1]form a path 
in Tfi. F?Mhermo? e, each node on this path has a T[i 1]-link to the nodes in p(u). We do EXTEND LEFT 
in two substeps. Step lB.1,. We determine the deepest node in TD, say w, that has a T[i 1]-link to an 
ancestor, say Ua, of u. Furthermore, let T be the node in TB, if any, that is the q-child of w if u is 
the q-descendant of u=. Then u(w) is a prefix of S[i 1]. In order to implement Step lB. 1, that is, 
determine w, T and u., we abstract a problem on trees that we call the nearest colored ancestors prob­ 
lem. In Section 3.2, we state the problem and present our algorithm. Step lB.2. In Step lB. 1, we have 
determined a prefix of S[i 1], namely, a(w). We now determine the sufEx of S[i 1], namely, S[i 1] 
 a(w). We claim without proof that this suffix is the longest prefix of a(u) C(U. ) and CT(T) a(w). 
This can determined by Lemma 2.6. Details of Step 2. We perform Step 2 in two sub­steps. Both the substeps 
require precomputation on the dictionary, as described below. In what. follows, consider only a single 
text location Z?[i]; the other text locations are. considered similarly. Step 2A. In this step, we determine 
B [i], that is, the longest prefix of S[i] that is a prefix in D. For this, we perform the following 
precomputation. We mark each node in TD that is a prefix in the dictionary; this is per­formed by a table 
look-up using the fingerprints. Then, we determine, for each node v, its nearest marked an­cestor, A[v]. 
Furthermore, for each leaf lj, we determine L[j], its legal length, defined as follows. L[j] = 1 if i 
is the minimum number such that a new pattern starts at j + 1 1 in ~. For each node v, we precompute 
L[v] to be the maximum legal length of the leaves in the subtree rooted at that node. This is performed 
by Lemma 2.3. Given L and A, Step 2A proceeds as follows. Recall that the substring S[i] is provided 
as an edge (u, v] and a length i. We determine the longest prefix of a(v) a(u) that leads to a prefix 
in the dictionary while tracing down starting from A[u] along an edge. We now claim, without proof, that 
this is of length exactly L[A[u]]. That completes the descriptio~ of Step 2A. Step 2B. We will now compute 
&#38;f[i]. For this, we precompute, for each pattern prefix, its longest prefix which is a pattern. This 
is done by modifying a sim­ilar step in [22]. Following that, we can look up M [i] directly.  3.2 Data 
Structures The Nearest Colored Ancestor Problem. Con­ sider a rooted tree with n nodes, each node being 
marked with several colors such that the number of dis­ tinct colors used over the entire tree is C and 
the total number of colors used is C. The problem is to prepro­ cess this information so as to answer 
FIND (p, c) queries quick~y. The FIND (p, c) operation returns the nearest ancestor of the node pointed 
to by p (possibly the node itself) which is marked with color e G C. The Algorithm Let T be the initial 
tree. Let ? be the augmented tree derived from T by adding two chil­dren to each leaf of T, and an extra 
leaf to each in­ternal node with only a single child. The leaves of ? are called the out-leaves. Note 
that the number of out­ leaves is O(n). The cth naive skelefon tree, denoted TC, is the tree obtained 
by restricting ? to only those nodes which are colored c; in addition, all out-leaves are retained in 
each T. and the root of ~ is retained regard­less of whether it is colored c, The cth real skeleton tree, 
denoted R., is obtained by deleting the out-leaves from TC. In what follows, we describe the algorithm 
as if the tree T and the query are presented at the same time. It is easy to modify it to the case when 
the tree is available for preprocessing and the query is presented later. First we describe the algorithm 
using naive skeleton trees. Later we will show how to use the real skeleton trees instead to improve 
the efficiency. Step 1. Generate 2?. For each internal node v in p, determine two out-leaves 1. and r,, 
whose least common ancestor (L CA) is v; clearly two such leaves exist, by construction. Step 2. Generate 
the naive skeleton tree Tc for each c E C. Process each T. so LCA queries can be answered efficiently. 
Step 3. To implement FIND (p, c), determine the LCA of lV and rv in Tt. This is the answer to the query. 
That completes the algorithm. The preprocessing takes time O(log d) and work ~TCl) = O(nlCl). (~all naive 
skeleton trees T= Each query takes time O(l). Both these bounds follow from Lemma 2.6. In what follows, 
we show how the preprocessing work can be reduced to (~all real skeleton trees ~. IRCI) = O(n+ c) while 
taking O(log log n) time for each query. TO achieve this, we use the van Erode Boas result [26]. For 
any c E C, associate with each node in the naive skeleton tree TC ~ the group of out-leaves which are 
its descendant but which are not the descendant of any other internal node in T.. The key observation 
is that these groups form a partition of the out-leaves of@ into sets of disjoint ranges in their Euler 
tour numbering. Moreover, the sets of disjoint ranges associated with an internal node u can be assigned 
to distinct instances of u in the Euler tour unambiguously (as described below). As a result, we can 
look upon the out-leaves as being just partitioned into disjoint ranges associated with dis­ tinct instances 
of nodes, Using this observation, the algorithm can be changed as follows. Step 1. Generate ? and assign 
Euler tour numbering to the vertices. For each node v in T, determine two out­leaves /. and TV whose 
least common ancestor (LCA) is v; clearly two such leaves exist, by construction. Step 2. For each c, 
generate the real skeleton trees R. and also generate its nodes in the Euler tour order with the same 
numbering as that in ?. Note that a node which has two descendant nodes marked c can appear several times 
in the Euler tour on Z! between the last and the first occurrence of the left and the right descen­dant 
respectively; in the Euler ordering for R., picking its number for any one such occurrence (say the left­most) 
will do. For each node v in the real skeleton tree R., define G. to be the set of out-leaves that are 
direct children of node v in the corresponding naive skeleton tree T.. Step 3. We implement FIND (P, 
c) as follows. Say p points to v and 1. and T. are as defined above. By de­termining the smallest number 
greater than 1. and the largest number smaller than i. in the Euler ordering for Rc, we can easily determine 
the node u such that G, contains IV. We do a similar computation for TV. Now, we determine the LCA of 
the node(s) which contain lV and r . This is the answer to the Find query. That completes the algorithm. 
All operations can be done in O(n + C) work and O(log n) time. For each R., predecessor and successor 
queries can be determined us­ing a Van Erode-Boas structure where N = n and s ~ n. Each query takes O(log 
log n) time. We remark that the entire algorithm has two main ideas. One is to show how finding the nearest 
colored an­cestor can be implemented using LCA queries by adding extra leaves. The second is the structural 
observation that the extra leaves can be considered in groups of ranges, which reduces our problem to 
the Van Emde-Boas structure. This algorithm has already been used for dynamic method look-up in object 
oriented program­ming languages like the smalltalk [21]. 3.3 Results In this section, we state the complexity 
bounds for each step in our dictionary matching algorithm. All time bounds stated below are expected 
time bounds that hold with high probability. Also, the bound below do not in­clude that for constructing 
the suffix tree for TD from Lemma 2.1 [11]. Step 1. From the bounds in [5] it follows that Step 1A can 
be done in O(log d) time and O(n log d/,C) work. The precomputation required can be done in O(log d) 
time and O(d) work. Using the complexity bounds in Section 3.2, Step lB can be done in O(L log log d) 
time and O(n log log d) work if the string has polynomial­sized alphabet, and O(L) time and O(n) work 
if it has binary alphabet; the required precomputation for this can be done in O(log d) time and 0(0!) 
work. Step 2. Step 2A can be done in O(1) time and O(1) work per text location. The preprocessing can 
be done in O(log d) time and O(d) work. @om the results in [22], the preprocessing for Step 2B can be 
done in O(log d) time and O(d) work. The text processing in Step 2B can be done in O(log d) time and 
O(d) work. To sum up, the text processing takes time O(log d + L) and work O(n + n log d/,C) if D has 
binary sized al­phabet and it takes time O(log d + L log log d) and work O(n + n log d/Z) if D has a 
polynomial sized alphabet. Therefore, Theorem 3.1 The dictionary matching problem over constant-sized 
alphabet can be solved in O(log d) time and O(n) wodc following O(log d) time and O(d) work preprocessing. 
The woTk bounds are optimal. Proof: By encoding each symbol in the alphabet in binary, any string over 
a constant-sized alphabet can be replaced by another of same asymptotic size over a binary-sized alphabet. 
Then, we set C = log d to get the theorem. Setting L = log d/ log log d we get, Theorem 3.2 The dictionary 
matching problem for strings over a polynomial sized alphabet can be solved in O(log d) time and O(n 
log log d) work following O(log d) time and O(d log log d) work preprocessing. Theorem 3.3 The dictionary 
matching p? oblem for strings over an unbounded alphabet (that is, in the com­ parison model) can be 
solved in O(log d log lx 1) time and O(n log 1x1) woTk following O(log d log 1X1) time and O(d log 121) 
woTk preprocessing. Here E is the size of the alphabet set. These work bounds are optimal. Proof: First 
we use the randomized renaming pro­cedure in [11] that remaps the symbols into the range 10 o.IXI in 
O(lognloglZl) time and O(nloglZl) work in the comparison model. Following that, we can replace the n 
length text by a string of length O(n log 1X1) and apply Theorem 3.1. 3.4 Checking the Pattern Matches 
We are given, for each z E [1, n], a pointer Rf[i] to suffix tree T of dictionary D which is claimed 
to represent the longest pattern prefix from D that matches at T[ij. Theproblem is to check the correctness 
of the claimed matches. First, we can check in constant time if any of the matches can be extended by 
a character, by reference to T~. For technical reasons, we make the following small change. If at some 
location i, &#38;f[i] is undefined, that is, there is no match at i, then we treat &#38;f[i] to be a 
special pointer to the singleton character T[i]. Now? we can check in constant time if the first character 
of &#38;f [i] matches text position T[i]. If neither test fails, the we proceed as follows. Let L[i] 
be the match length at i, that is, the string length of ikf[i]. We say that i dominates j if i < j and 
i+ L[i] z j + L[j]. If a position j has some dominating position i~ then we say that j is dominated, 
and if no such i exists, we say that j is dominating. We can find, for each posi­tion j, a dominating 
edge, by techniques described in Lemma 5.2. Now, for each j, let A[j] be a dominating match of j, if 
such a match exists. If so, we can check if the match at j is consistent with the overlapping sub­string 
given by kf[A[i]] over the same positions. For ex­ample, suppose Ikf[i] = P~, [1, L[i]], lbf[j] = P~, 
[l, L[j]] and i = A[j]. Then we check to make sure that Pk2 [1, L[j]] = Pkl [j i, j i + L[i]]. Such 
a check takes constant time, by Lemma 2.6. Now we consider only dominating matches, since, by the previous 
step, if the dominating matches are correctq so are the dominated matches. Furthermoret we need only 
check that the dominating matches are all pairwise consistent, since we have already checked to make 
sure that each text position is consistent with some pattern. So if the patterns are all pairwise con­sistent, 
then they are all correct. For each dominating match i, we find the next dominating match AJ[i] via all 
nearest ones (See Lemma 2.4). For each dominating i, we do the following. Let j = iV[i], J4[i] = F kl 
[1, L[i]] and Ikf[j] = Pkz [1, L[j]]. Then we check to make sure that Pk, [l, L[i] j+i 1] = Pkl [j 
i, L[i]]. Once again, such a check takes constant time, by Lemma 2.6. Lemma 3.4 If an away M passes the 
above te~ts, then it is COTT.5 Ct. Proof: We need to show only that for dominating matches u < v < w 
~if u is consistent with v and v is con­sistent with w, then v is consistent with w. Let MIU1 = PkU[l, 
L[u]], lkf[v] = Pka[l, L[v]], kf[W] = PkW[~, ~[w]]. Then PkW[l, L[u] w+u 1]= PkO[w v,L[u] v+ u l]P&#38;[w 
 u, L[u]].  4 Dynamic Compression 4.1 Compression We wish to compute the LZI parsing of a string 
S[1, n]. Recall that the LZ1 parsing is defined as follows: If we have parsed S[1, i], then the next 
phrase to be parsed is the longest substring of S which starts at both S[Z + I], and at some S[j], for 
j < i + 1, that is, we must find the pair (j, k) such that S[i + 1,i + k + 1] = S[j, ~+ k], j < i + 1, 
and k maximized. Given such a (j, k), we can replace S[i + 1, s + k + 1] with (j, k), and we will have 
parsed up until S[i + k + 1]. If we ever encounter a new character a, e.g. when we parse the first character 
in the string, then we output (a, 0) and advance one characters. Suppose that for each i c [1, n], we 
have a pair .M[i] =(j,k)suchthat j<i,S[i,i+k]=S[j,j+k] and k is maximized. Notice that this is a superset 
of the information needed for the LZ 1 parse? since we need the M[i] for only a (hopefully small) subset 
of the positions of a string. We can easily find those i which partic­ipate in the LZ1 parse in O(log 
n) and O(n) work as follows. Let G = ([1, n], 1?) be a directed graph such that (i,i+k) EEif&#38;f[i] 
= (j,k)and k> 1. For &#38;f[i] = (a, 0), (i, i + 1) c E. Notice that such a graph is a tree, since it 
is acyclic and each node i has a par­ent i + k. Also, the node 1 is a leaf, and n is the root. Then the 
positions that participate in an LZ1 parsing of S are exactly those on the path from 1 to n in G. These 
nodes can be determined in O(log n) time and O(n) work by many methods, e.g. tree contraction, level 
ancestors, Euler tour techniques, etc. So we must show how to compute the lkf[i] within the desired bounds, 
As in any string matching problem, we begin by building a suffix tree Ts. Let /i be the leaf representing 
the suffix S[i, n]. For each interval node v? let L[V] = min{illi is a descendant of w}. Notice that 
if L[v] = i, then some child w of v will have L[w] = i. Conversely, for each li, there will be a (possibly 
empty) chain of ancestors v such that L[v] = i. For each i, let A[i] = v if w is the deepest ancestor 
of li such that L[v] # i. Notice that L[A[i]] < i. The importance of these functions is summarized as 
follows: Lemma 4.1 Vi E [1, n], M[i] = (L[A[i]], lA[i]l). Proof: First, by the observations above, L[A[i]] 
< i, and so the L[A[i]]th suffix is to the left of the ith suf­fix. Now suppose that there is a (j, k) 
such that j < i, k > lA[i] I and S[i, i + k] = S[j, -j+ k]. Let w be the least common ancestor of 1, 
and lj. 3There are several variants on how new characters are han­dled, but they are easily convertible, 
and the algorithms in this section serve to compress and uncompress according to any of the standard 
LZ1 variants. For L, mark a node v if A[v] > A~(v)], where p(v) is Lemma 5.1 The shortest path from 
1 ton in Gd is the the parent of v. Then, for each leaf l(i), L[i] is the parent game length as the shortest 
path from 1 to n in G. of the nearest marked ancestors of li. By Lemma 2.7, we can therefore compute 
L within the desired time Proof: Let SG [i] be the length of the shortest path in bounds.  G from 1 
to i. Theorem 4.2 A string of length n can be compressed via LZ1 in O(log n) ezpected time and with 
O(n) ez­pected work. 4.2 Uncompression Given C[l, m], and LZ1 parsing of a string of length n, we we 
will construct the uncompressed string 2 [1, n]. First, we do a prefix sum on the lengths of the blocks 
in C -taking the special blocks of the form (a, O) to have length 1. We place a 1 at each i in an array 
I?[i] if i is the beginning of a block in T. .B[i] = O otherwise. Now, for each i E [1, n] we compute 
which block it is in, that is, if B[i] = O,we set -B[i] = j such that j < i, B[j] = 1 and j is maximized. 
This is done by Lemma 2.4. And for each B[i] = 1, we set .P[i] = j if the block start­ing at T[i] is 
represented by C [j]. Now we set up the directed graph G = ([1, n], l?) as follows: (i, j) c E if (z, 
y) = P[B[i]] and j = z+ i B[i]. We get aforest in which the root of each tree is a node of the form (a, 
O) and all nodes in the tree are as in the text. Thus, to uncompress, we need only find out which tree 
each node is in, which we do by Lemma 2.2. Theorem 4.3 Given the LZI compressed representa­ tion of an 
n character string, the uncompressed repre­ sentation can be generated in O(log n) time and O(n) work, 
 5 Static Compression For each position, i, let &#38;f[i] be the length of the longest pattern prefix 
from D that occurs at T[i], that is, if iW[i] = j, then there is some k such that P~ ~ D and T[i, i + 
j] = pk [1, j]. Similarly, &#38;f[i] is the length of the longest matching pattern in a dictionary with 
the prefix property. Let G = ([1, n], l?) be a directed graph such that (i,i+ k) GE if k < M[i] + 1. 
Now those posi­tions, i, that participate in an optimal parse of T are those i along a shortest path 
from 1 to n in G. How­ever, it would be grossly inefficient to apply a general shortest paths routine 
to this problem. Instead, we take advantage of the structure of G, as follows. Let edge (i, j) dominate 
edge (k,/) if i < k and j > 1. Let Gd = ([1, n], Ed) be a directed graph such that (i, i+k) c Ed if (i, 
i+k) G .?3 and there is no e c E which dominates (i, z + k). Then: Claim 5.lA l<i<j~n~S~[i]~SG~] Proof: 
Let PI, . . . ~PS~ ~1 be a shortest path from 1 to j in G. Let kbe such that pk < i ~ pk+l. Then PI,. 
. .tf%, i is path from 1 to i of length k + 1 ~ SG(j]. Hence, SG [i] < k + 1 ~ SG[j]. Claim 5.lB There 
is a path of length SG [i] j+om 1 to i in G which uses only dominating edges. Proof: By induction: For 
i = 2, (1,2) is a dominating edge. Assume the claim holds for all i < i. Suppose it fails for i and for 
some path, P = PI, ..., p.s~ [i], pl = 1, psG[i] = > let (%7 Pj+l) be he lMt ominated edge used. If pj+l 
< i, the we can replace Pl, . . . ~pj+l with a path of length SG ~i+l] using only dominating edges, and 
hence get a shortest path to i using only dominating edges. Now consider the case where Pj+l = i. Let 
(1, i) be the dominating edge ending at i -the existence of such an edge follows directly from the construction 
of Lemma 5.2. Now Claim 5.1A implies that SG [/] s SG ~j] and so the dominating path to 1 followed by 
edge (1, i) is a shortest path to i, hence establishing our claim. 9 This also finishes the proof of 
the lemma. The questions is how to find the shortest 1 to n path in Gd, and how to construct Gd directly. 
Notice, however, that each node in Gd has a single incoming edge, so Gd is a rooted tree in which the 
edges are directed away from the root. In this case, the 1 to n path is unique and can be computed in 
O(log n) time and O(n) work. As for the construction: Lemma 5.2 Gd can be constructed in O(log log n) 
time and O(n) work from array M. Proofi Let ikf [i] be the prefix maximum (see Lemma 2.3) of array M[i]. 
For each i, let L[i] = min{j[fif [j] > i}. Then, (z, y) c Ed if L[y] = z. So we must find the rank of 
each i E [1, n] in M . If r(i) is the rank of i, then L[i] = r(i) + 1.The rank of each i can be computed 
in constant time and linear work. All steps are therefore executable within the desired time bounds, 
thus finishing the proof. M We conclude with Theorem 5.3 Given a dictionary, D, of size d, and a string, 
T, of size n, then, Static Dictionary with PTefix Property Optimal Parse of S with respect to D can be 
computed in O(log d + log n) time and O(n) work, after O(Iog d) time, O(d) work preprocessing of D. 
 References [1] S. De Agostino. P-complete problems in data com­pression. Theoretical Computer Science, 
127:181­186, 1984. [2] S. De Agostino and J. Storer. Parallel algorithms for optimal compression using 
dictionaries with the prefix property. Proc. of the 2nd IEEE Data Com­pression Conference, pages 52 6 
1, 1992. [3] A.v. Aho and M.J. Cora­sick. Efficient string matching. Communications of the ACM, 18(6):333 
340, 1975. [4] A. Amir and M. Farach. Adaptive dictionary matching. Proc. of the 32na! IEEE Annual Symp. 
on Foundation of Computer Science, pages 760 766, 1991. [5] A. Amir, M. Farach, and Y. Matias. Efficient 
ran­domized dictionary matching algorithms. Proc. of 3rd Combinatorial Pattern Matching Conference, pages 
259 272, 1992. Tucson, Arizona. [6] 0. Berkman, D. Breslauer, Z. Galil, B. Schieber, and U. Vishkin. 
Highly parallelizable problems. Proc. of the 21si Ann. ACM Symp. on Theory of Computing, pages 309-319, 
1989. [7] 0. Berkman and U. Vishkin. Recursive *-tree par­allel data-structure. In Proc. of the 30th 
IEEE An­nual Symp. on Foundation of Compute~ Science, pages 196-202, 1989. [8] P.C.P. Bhatt, K. 13iks, 
T. Hagerup, V.C. Prasad, T. Raznik, and S. Saxena. Improved deterministic parallel integer sorting. Information 
and Compu­tation, pages 29-47, 1991. [9] R. Cole, M. Crochemore, Z. Galil, L. Gasieniec, K. Park, S. 
Muthukrishnan, H. Ramesh, and W. Rytter. Optimally fast parallel algorithms for preprocessing and pattern 
matching in one and two dimensions. PTOC. of the 3~th IEEE Annual Symp. on Foundation of Computer Science, 
pages 248 258, 1993. [10] M. Crochemore and W. Rytter. Efficient paral­lel algorithms to test square-freeness 
and factorize strings. Information Processing Letters, 38:57-60, 1991. 252 [11] M. Farach and S. Muthukrishnan. 
An optimal, logarithmic time, randomized parallel suflix tree construction algorithm. Technical report, 
DIMACS, 1995. [12] Z, Galil. Optimal parallel algorithms for string matching. In Proc. of the 16th Ann. 
A Cikf Symp. on Theory of Computing, pages 240 248, 1984. [13] Z. Galil. A constant-time optimal parallel 
string­matching algorithm. Proc. of the ,2Jth Ann. ACM Symp. on Theory of Computing, pages 69-76, May 
1992. [14] H. Gazit. An optimal randomized parallel algo­rithm for finding connected components in a 
graph. In Proc. of the 27th IEEE Annual Symp. on Foun­dation of Computer Science, pages 492-501, 1986. 
[15] M. E. Gonzalez-Smith and J. A. Storer. Parallel algorithms for data compression. Journal of the 
ACM, pages 344-373, 1985. [16] R. M. Karp and V. Ramachandran. Parallel al­gorithms for shared-memory 
machines. In J. van Leeuwen, editor, Handbook of Theoretical Com­puter Science, Vol. A, pages 869-941. 
North-Holland, Amsterdam, 1990. [17] R.M. Karp and MO. Rabin. Efficient randomized pattern-matching algorithms. 
IBM Journal of Re­search and Development, 31:249 260, 1987. [18] Z. M. Kedem, G. M. Landau, and K. V. 
Palem. Optimal parallel suffix-prefix matching algorithm and application. Ist Annual ACM Symposium on 
Parallel Algorithms and Architectures, 6:388-398, 1989. [19] D.E. Knuth, J.H. Morris, and V.R. Pratt. 
Fast pat­tern matching in strings. SUM Journal on Com­puting, 6:323 350, 1977. [20] A. Lempel and J. 
Ziv. On the complexity of fi­nite sequences. IEEE lkansactions on Information Theo~y, 22:75-81, 1976. 
[21] S. Muthukrishnan. A time and space efficient al­gorithm for dynamic method look-up in object ori­ented 
programming languages. manuscript, 1995. [22] S. Muthukrishnan and K. Palem. Highly efficient parallel 
dictionary matching. 5th Annual ACM Symposium on Parallel Algorithms and Architec­ tures, 1993. [23] 
M. Naor. String matching with preprocessing of text and pattern. Proc. of 18th International Collo­quium 
on Automata Languages and Programming, pages 739-750, 1991. [24] M. Papadipoyli. A distributed dictionary 
matching implementation. 1994. [25] J, Storer. Data compression: methods and the­ory. Computer Science 
Press, Rockville, Maryland, 1988. [26] P. van Erode Boas, R. Kaas, and E. Zijlstra. Design and implementation 
of an efficient priority queue. Math. Systems Theory, 10:99-127, 1977. [27] U. Vishkin. Optimal parallel 
pattern matching in strings. Proc. of 12th International Colloquium on Automata Languages and Programming, 
Springer LNCS, pages 91-113, 1985. [28] U. Vishkin. Deterministic sampling -a new tech­nique for fast 
pattern matching. SIAM Journal on Computing, 20:303-314, 1991. [29] A. C. C. Yao. Some complexity questions 
related to distributed computing. Proc. of the Ilth Ann. ACM Symp. on Theory of Computing, pages 209­213, 
1979. [30] J. Ziv and A. Lempel. A universal algorithm for sequential data compression. IEEE Transactions 
on Information Theory, IT-23(3):337 343, 1977. 
			