
 Generating Comprehensible Summaries of Rushes Sequences based on Robust Feature Matching Ba Tu Truong 
Dept. of Computing Curtin University of Technology Perth, W. Australia truongbt@cs.curtin.edu.au ABSTRACT 
This paper describes our .rst attempt at tackling a pilot task in Trecvid: video summarization of rushes 
data [3]. Our method is based on the tight clustering produced via SIFTmatching.Inthis .rst attempt,wetry 
toexaminehow our approach performs without complex implementation in terms of concept detection and excerpt 
assembly (i.e, no picture-in-picture, split screen and special transitions). Al­though we do not perform 
very well in terms of concept inclusion, we rank very well in terms of the summary being easy to understand 
and relevancy of included segments.  Categories and Subject Descriptors H3.1[InformationStorage andRetrieval]: 
ContentAnal­ysis and Indexing Abstraction methods General Terms Algorithms, Experimentation Keywords 
Video summarization, Trecvid, Rushes 1. INTRODUCTION The system taskin rushes summarizationis, given 
a video from the rushes test collection, to automatically create an MPEG-1 summary clip less than or 
equal to a maximum duration of 4 percent of the duration of the original video thatshowsthemainobjects(animate 
andinanimate) and events in the rushes video to be summarized. Trecvid also requiresthatthesummarytominimizethenumber 
offrames used and topresenttheinformationinwaysthatmaximizes the usability of the summary and speed of 
objects/event recognition[3]. Trecvid groundtruth a set of concepts that should be in­cludedinthesummary; 
concepts aredividedintofourtypes: 1. object(s)(no event or camera event), e.g., antique car, old women, 
the room. Permission to make digital or hard copies of all or part of this work for personal or classroom 
use is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage 
and that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, 
to post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. TVS 07, 
September 28, 2007, Augsburg, Bavaria, Germany. Copyright 2007 ACM 978-1-59593-780-3/07/0009 ...$5.00. 
Svetha Venkatesh Dept. of Computing Curtin University of Technology Perth, W. Australia  svetha@cs.curtin.edu.au 
2. object(s) + camera event. e.g., pan to the car, zoom into the man. 3. object(s) + event. e.g., red 
hot air balloon ascending, people talking. 4. object(s) + event + camera event.  Trecvid limits the 
set of allowable camera events to: zoom in, zoom out, or pan. In ourreviewofexisting videoabstractiontechniques[6], 
we describe a taxonomy of video summarization, shown in Figure1(adaptedfrom [6]). Ascan beseenfromthis 
.g­ure, there are .ve major aspects of a skim generation tech­nique: its overall process, the nature 
of skim length, the genre/domain of the video sequence to be summarized, the underlying mechanism and 
the features used.  na   m n e r  t znht t Figure 1: Attributes of video skimming techniques. 
In this paper, we propose a method for video summary generation based on the tight clustering produced 
by SIFT matching. Our method maps well to the de.ned taxonomy. Weusevisual and visualdynamicfeatures, 
and theperspec­tive is information/coverage and event/highlight. The main mechanismsforgenerating thesummary 
areeventdetection and redundancy elimination. The summarization process is divided into the following 
steps. 1. Segmentation. This is often a standard step where a video sequence is segmented into shots. 
However, rushes sequences sometime contain special e.ects which needtobetreateddi.erently(Section 2.2). 
 2. Filtering Irrelevant Shots. Shots that are irrelevant to the content to be summarized need to be 
.ltered out atthisstep(Section 2.3). 3. Clustering for redundancy elimination at shot level (Section 
2.4). 4. Shot selection(Section 2.5).  5.UnconstrainedSub-shotsegmentation and selection(Sec­tion 2.6). 
6.Satisfying durationconstaints(Section 2.7)). We set our focus on the detection of: setting and cam­era 
views, presence of main characters in the scene, camera framming, camera events. Due to the resource 
constraints, wechosetoignoretheexplicitdetection of object+event con­cepts, such as a subject entering 
or exiting the scene or a subject picking up or putting down the phone. We set additional constraints 
for our summaries: naturalplayback. Thisis oppositetotechniqueswhich try to fasten the playback speed 
to reduce the dura­tion. We believe techniques should be compared at natural playback speed, at least 
in the pilot year of the task.  nosplit-screen,compositeviews and specialtransitions betweensegments. 
Eachframeinthesummary willbe anexact copyof one and only oneframeinthe original video sequence.  visual-audiosynchronization. 
Thesynchronizationbe­tween audio and visual streams is exactly as the orig­inal video sequence.  smoothness/coherent 
in viewing a priority. We would like user to have the same experience watching a sum­mary as with a normal 
video.  2. METHOD &#38; ALGORITHM 2.1 Pre-processing First, we apply some existing techniques to the 
data set to extract basic components for the summarization process. Shot segmentation. Shot boundaries 
are detected by a simplemethodofapplying anadaptive-threshold onthedis­continuity curve[4]. ForRushes 
video sequences, this simple methodisquitereliableinthedetection ofhard cuts,giving only a couple offalsedetection(clapboard 
segments) and missed boundaries. Shot-based keyframe extraction. We use a simple, e.­cient method for 
extracting representative frames of a shot describedin[6]. Inthismethod,the .rstframeisalwaysa keyframe, 
and the current frame is selected as a keyframe, if its visual appearance signi.cantly di.ers from that 
of the last keyframe. We also force the last frame of the shot as the keyframe. 2.2 Detecting Special 
Transitions Sinceweperformclustering attheshotlevel(seeSection 2.4), it is important that a shot boundary 
is not missed; otherwise,twoclusters ofunrelated contentsmaybemerged together. Black-frames transitions. 
Special transitions of this type areproduced accidentally betweentwocamerashots. They are characterized 
by the existence of black frames and grey images. We can detect these transitions by .rst locating black 
frames based on the average image brightness. Once theblackframes arelocated,weextenditsboundary toboth 
sides as long as the average saturation of the current frames are still low (i.e., grey scale images). 
To be more robust, we do allow certain tolerance till the extension process is stopped, i.e., if the 
saturation of the current frame is high, but the saturation of the next frame is low, we still keep going. 
Fade transitions. Fade transitions are relativelycommon insomeRushessequences(MS220460,MS216210). Most 
of them belong to TV series Between the lines . These fade transitions are all fade to white color. We 
.rst locate the middle of the transition, the brightest part, by applying thresholds on the brightness. 
Once the middle part is iden­ti.ed, its boundaries are extended on both sides based on the lightness 
di.erence between successive frames. When thedi.erencebecomesinsigni.cant theboundaryis marked. As it 
turns out, this simple technique is e.ective enough for Rushes sequences. 2.3 Shot Filtering It is often 
useful if irrelevant content is .ltered out before applying the clustering procedure. Testing sequences. 
Testing shotsappear atthestart(oc­casionally inthemiddle) ofaRush sequence. They arerela­tively easy 
tobedetected accurately asthe audio and visual characteristics are quite distinctive. We de.ne a color 
histogram of 6 bins for 6 colors: bright­red,bright-yellow,bright-green,bright-blue,bright-pink and bright 
magenta and count the number of pixels in each bin. This simple technique identi.es testing shots with 
100 per­cent accuracy. Flash-back sequences. Flash back sequences are often very short and often inserted 
between two takes for some production reasons that are unknown to us. They contain short segments of 
scripted contents or segments of contents that are completely unrelated to the current content. These 
segments are surrounded by fade-out e.ects, so their removal is relatively easy once fade-out e.ects 
have been identi.ed in Section 2.2. 2.4 Clustering onssnr n   a erh drh d F d The advantages of 
feature-based matching such as SIFT (Scale Invariant Feature Transform [2]) for clustering are two-fold. 
First, we can visually examine why clusters are formed and subsequently derive the most suitable feature 
set for the task. From our experience in the similar domain of home video, clusters produced by the SIFT 
matching are generally more accurate than those produced by histogram similarity. Secondand moreimportantly,wecanconstruct 
a clustery hierarchy based ontransformparametersproduced during the matching process. For example, in 
this work we can derive the hierarchy produced in Figure 2. a e1 Leve 2 Figure 2: Cluster hierarchy 
produced by SIFT matching. The latter is very useful for further exploitation of cluster relationships 
for excerpt segmentation/selection or excerpt assembly. For example, by knowing the relative camera dis­tancebetweenclusterswecan 
assembleclipsin aninduction pattern where longer/setting shots are presented .rst and closer/emphasis 
shots are introduced later, which according to Film Theory literature can present content with better 
clarity. Furthermore, we can use segments closer to the be­ginning of long distance shots and segments 
closer to the end of close-up shots to preserve temporal .ow of dialogue (see Figure 3). However, with 
our summaries submitted for evaluation, these have not yet been integrated. fc w  o r e w v ffc w f 
 C eeup1   Figure 3: Induction excerpt segmenta­tion/selection/assembly and dialogue presevation. 
In our recent work [5], robust shot clusters can be ex­tracted via the use of SIFT features. Each cluster 
generally corresponds to one view of the action, possibly with di.er­ent shot distances or camera focal 
lengths, and they often lie in the same side of a 180-degree axis. However,in ordertowork onrushesdata 
and thesumma­rization problem as de.ned by the Trecvid committee, we need to adjust the clustering algorithm 
in two aspects. First, it is important that keyframes selected for match­ing do not belong to the clap 
board segment. Since clap boards themselves can be matched with each other even the contents of the shot 
are completely di.erent. We implement a simple method where the segment surrounding a high mo­tion region 
is marked and no keyframes can be assigned to the marked segment. Second,transformparameters ofRansacclustersproduced 
by two matching frames are evaluated to see if they in fact have the same framing, i.e., same shot size. 
Therefore, clus­tersproducedin[5] areoftenbrokenintoanumberofclus­ters. Forexample,clusters4 and5inFigure4belong 
to one singleclusteriftheproceduredescribedin[5]is applied. Figure 4 shows typical results for our SIFT-based 
cluster­ingtechniques, eachframe represents a shotin the sequence. The sequence shown is MS212920. Seven 
clusters produced for this sequence are very accurate.  Figure 4: Shot clusters of sequence MS212920. 
Ransac cluster. Random Sample Consensus(RANSAC) [1] is a technique for .tting a model to experimental 
data. Traditionally, RANSAC procedure aims to .nd only one good consensus set. However, we observe that 
in natural setting of home videos, objects lie in di.erent image planes and each plane tends to produce 
di.erent good consensus sets. Therefore, we use the iterative execution of RANSAC, which removes a good 
consensus set each time till no more good consensus sets can be found. Given a set of keypoint matches 
from two images as the input with a set of model parameters (t,T,e) denoting the size of the seeding 
set, the minimum size of the consensus set and theerrortolerance,werandomlypick t matches, and compute 
the similarity transform f, and check the number of targetkeypoints in the matching set thatlie near 
thepre­dicted position of source keypoints. These are added to the consensus set and the similarity transform 
is updated. This is iterated until there is no change in the consensus set. If the number of elements 
in the consensus set is large enough, itisconsideredaRANSAC cluster andall matchesintheset are considered 
correct. This consensus set is removed from the original matching set, and the RANSAC procedure is repeated 
till no more consensus sets can be produced. Al­ternatively we can apply Houghtransforminthepose space 
asdescribedin[2]; however, ourexperimentationshowsthat our RANSAC version is much more reliable in this 
task. In this work, RANSAC clusters and their transform pa­rameters are used for deciding if two images 
match or a camera event has occurred. 2.5 Shot Selection After shots are clustered into di.erent non 
overlapping groups,weselectthemaximumof oneshotfromeach cluster and discard the rest as they are considered 
redundant once the shot is selected. If the cluster contains more than one shot, the longest shot is 
selected as they tend to cover all contents presented inothershots. This assumptiongenerallyholdsunlessthere 
are very long unscripted segments in the shot. However, this is not always the case as shown in Figure 
4. In the third cluster, the .rst shot is not the longest shot but it is the only segment containing 
the concept {the man walks toward the sofa and sits down}. If the cluster contains one single shot, the 
shot is selected only if its duration is su.ciently long. 2.6 Excerpt Segmentation and Selection Once 
we select the rightshotfrom each cluster, we need to locate segments to be included in the summary. These 
seg­ments need to map to concepts de.ned in the groundtruth. Currently, we only aim at concepts involving 
objects and camera events and we make no attempt at detecting con­cepts of object+event class. Concepts 
of theobject class tend tobedetectedby cluster­ing and cameraextractionitself,sinceobjects are associated 
with framing and framing can only be changed via camera movements. For concepts of object+camera or onject+camera+event 
class, we do not try to detect camera movement frame by frame, but rely on extracted keyframes in Section 
2.1 to reduce the computational time. First we compute the sim­ilarity transform parameter set (tx,ty,s) 
which represents the translation and scaling of each Ransac cluster between two successive key frames 
fri and fri+1. A camera event is claimed, if for each Ransac cluster, at least one of these transform 
parameters is too large. The thresholds are set at(20,30,1.4). Temporally close camera events are merged 
into onesingleeventtoimprovethecoherence(seeFigure 5b). Figure 5: Detected camera events. Overall this 
technique performs relatively well. Close-up shots, however, are rather problematic because of two rea­sons. 
First, as the head becomes so dominant, its slight movement may lead to one wrongly detected camera event. 
Second, even if camera movements do occur, they merely try tokeep thesubjectintherightframing(seeFigure 
5c). They are not considered as an event of object+camera class. Incorrect camera events picked up for 
close-up shots lead to less comprehensible summary. We have recently resolved this issue by checking 
the consistency in the direction of cameramovement acrossmultipledetectedcamerasegments within a shot. 
For example, a short pan right followed by ashortpanleft,suggesting they arenottruecameraevents (Figure 
5c). If nocameraevents aredetected,weextract asthemiddle 4 seconds of the shot to be included in the 
summary. 2.7 Satisfying Time Constraints Trecvid committee set target duration of 4 percent of the original 
video and submitted summaries are required not to exceed this target duration. In our work, if the total 
duration of excerpts selected in the previous step does not exceed 4 percent then we do no further processing 
and excerpts are assembled like they are in the order of their temporal occurrence. This occurs to more 
than half of the sequences in the Rushes set. If the total duration exceeds the target duration we trim 
each excerpt proportionally to their length. This is quick, but not an ideal solution. A better solution 
should be based on some modelling of the duration with respect to concept recognition, concept class 
and story comprehension. In oursubmissiontoTrecvid,wedonottrimcameraevent excerpts any di.erently to 
other excerpts. This may lead to concepts of object class being missed at the start and end of a camera 
operation (see Figure 6a). Our recent experi­mentation shows that for long camera-related excerpts it 
is better to retain the start and end of a camera-related ex­cerpts and fast forward the middle segment 
to the desired duration, asshowninFigure6b. Thisretainscoherence and make sure concepts of object class 
at the start and end of a camera operation are not being missed. m e      mte    d Figure 6: 
Trimming camera event segments. 2.8 Excerpt Assembly Afteridentifying excerptsfromeach selected shot, 
weedit them together in a simple fashion. Excerpts are joined in their originaltemporal orderviahard 
cuts and audiostream is synchronized with selected excerpts as in the original video. If Trecvid allows 
more than one submission, more sophisticated approaches (picture-in-picture, split screen) would certainly 
be tried. 3. EXPERIMENTAL RESULTS In terms of concept inclusion (IN), our technique does not rank very 
well (16/24) as shown in Figure 7. This is understandable due to the fact that we set out from the start 
to completely ignore the detection of concepts of ob­ject+event class, which as it turns out is very 
dominant in the groundtruth. 140 120 100 80 60 40 20 0 Figure7:Conceptinclusion score(IN). Reasons 
behind missed inclusions can be divided into the following categories: object+event concepts for which 
we have not yet de­signed a explicit detector (OE). This contribute sig­ni.cantly to ourlowperformanceinconceptinclusion, 
since concepts of this class are very dominant in the groundtruth set. We miss concepts such as {a subject 
exiting the scene} or {a subject walking towards the camera}. Incorrect clustering. Concepts are not 
included in the summary because produced clusters represent more than one single unit of objects. This 
can be further divided into two categories:  Parameters not set correctly (CL-I). This error is correctable 
and after the correction we have recovered a lot of concepts. Unfortunately, this is discovered only 
after evaluation results come back to us.  Identical camera framing and setting but di.er­ent eventsoccuringatdi.erenttimes(CL-II).For 
example, in sequence MRS048779, there are two identical shots of the door, one when the woman comesbytheapartment 
andthe otheriswhenthe mancomesby. Errors ofthistypecannotbecor­rected via adjusting theimagematchingparame­ters. 
However, enforcing shots in each cluster to be temporally close to each other may help. Nev­ertheless, 
they only account for a small fraction of errors due to incorrect clustering results.   Incorrectshotselection(SL).Errors 
ofthistypeoccur whenshots areclusteredcorrectly,but wedoselectthe right shot leading to some inclusions 
actually belong­ing to unselected shots in the cluster. This is because the longest shot does not always 
contain all relevant contents of the cluster. Errors of this type are uncom­monin ourresults(MRS043400). 
 Incorrect sub-shot segmentation and selection (SG). Errors of this type are produced when we select 
the right shot to be in the summary, but the selected seg­ments do not contain the desired events. For 
example, itcontains unscriptedsegments. Theyare not common in our results.  Incorrectshot .ltering(FI).Shots 
are .ltered outin­correctly.  Assessormistakes(AS).Thisoccurswhenthereis an inconsistency in results 
by assessors and we check that a concept was actually included.  70  Table 1 shows our analysis of 
missed concepts in seven Rushes sequences where our technique is worst in terms of 60 concept inclusion. 
The last row shows that 15 Type-I clus­ 50 tering errors(CL-I)arepresent. Sincethey canallberecov­ered,theoverallimprovement 
withinthissubsetis 15/(12*8) 40 = 15 percent. This clearly indicates that our technique can 30 potentially 
reach the same performance level in concept in­clusionofthecurrent .vebesttechniques. Further,ifwecan 
20 design an object event concept detector that can recover 20 10 percent of events that have strong 
optical .ow signatures 0 such as people entering and leaving a scene or people mov­ing towards acamera,theimprovementin 
conceptinclusion can be up to 20 percent, making our technique better or Figure9:Duration score(DU). 
 equivalent to the best technique. Sequences OE CL-I CL-II FI SL SG AS MS237650 6 0 1 0 0 s 3 MRS158380 
6 0 0 0 0 1 3 MRS156211 2 5 0 0 0 1 0 MRS157443 2 6 0 0 0 0 2 MRS157445 6 3 0 0 0 0 0 MRS043400 4 1 1 
1 2 0 0 MRS042548 0 0 0 10 0 0 0 Total 26 15 2 11 2 2 8  3.2 Table 1: Errors in worst performing sequences. 
 3 As shown in Figure 8, our technique is ranked third in terms ofhumancomprehension ofthesummary. Thismeans 
Figure10:Redundancy score(RE). our system has produced desired results. This also means that, although 
we do not rank very well in terms of concept inclusion, most of concepts we detect are important to the 
 4. CONCLUSIONS understanding of the summary. For example, concepts we We have described our .rst attempt 
at tackling the sum­misssuch as nursepickingupthephone and nurseputting marizationtask. Although, ourtechniqueiscurrentlygivingdownthephonedonot 
a.ecttheunderstanding ofthesum­mixed results, we have identi.ed areas where improvementsmaryasthemostimportant 
conceptisthe nursetalkingon can be made. the phone. The generated summaries are relatively smooth Wecanintegratedi.erentmethodsfor 
assemblingselectedand do not contain many fragmented segments, which also excerpts(e.g.,PIP, split screen)to 
reduce the summarytime. contribute to the EA ranking. For example, shots of the same objects at di.erent 
framing sizecanbeshown atthesametime,whilesegmentscontain­ing camera motions can be fast forwarded. We 
also need a better detector for camera events, especially with close-up shots. We are currently designing 
a object+event detector basedonthe analysisofoptic .owpatterns,whichallowsthe detection of events such 
as subjects moving toward camera, entering or exiting a scene. 5. REFERENCES and automated cartography. 
Commun. ACM, 24(6):381 395, 1981. [2] D. G. Lowe. Distinctive image features from scale-invariant keypoints. 
International Journal of Computer Vision, Figure8:Easytounderstand score(EA). 60(2):91 110, 2004. [3] 
P. Over, A. F. Smeaton, and P. Kelly. The TRECVID 2007 With respect to the .nal duration, Figure 9 shows 
that BBC rushes summarization evaluation pilot. In Proceedings of the TRECVID Workshop on Video Summarization 
our summaries are relatively long. This is somewhat ex­ (TVS 07), pages 1 15, New York, NY, September 
2007. ACM pected as we make no attempt at this stage to fast-forward Press. the summary or apply picture-in-picture/split-screen 
edit-[4] B. T. Truong and S. Venkatesh. Finding the optimal segmentation of video sequences. In ICME 
05, 2005. ing. Also, the duration of each excerpt, initially set at four [5] B. T. Truong and S. Venkatesh. 
Linking identities and seconds, maybe abitlongfor the contentidenti.cation task viewpoints in home videos 
using robust feature matching. In via play-and-stop approach as currently trialled by Trecvid. International 
Multimedia Modeling Conference (MMM 07), Singapore, Jan 9-12 2007. [6] B. T. Truong and S. Venkatesh. 
Video abstraction: A Although our summaries are relatively long in duration, systematic review and classi.cation. 
ACM Transactions on theperceptionofcontentredundancy islow, aswerank7th Multimedia Computing, Communications 
and Applications as shown in Figure 10. This further demonstrates that our (ACM TOMCCAP), 3(1), Jan 2007. 
 
			