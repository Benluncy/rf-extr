
 An Honors Course in Data Compression Debra A. Lelewer California State Polytechnic University Pomona, 
CA 91768 (714)869-3443 lelewer@ics.uci .edu Cheng Ng California State University Fullerton, CA 92634 
and University Irvine, of California CA 92717 Abstract This paper describes a course in Data Compres­sion 
to be offered as an enrichment experience to ju­nior and senior Computer Science majors at Cal Poly Pomona. 
Data compression is the process of reducing the amount of space needed to store files on computers or 
of reducing the amount of time taken to transmit information over a network. It is a topic of obvious 
interest to computer professionals and the subject of a great deal of current research. However, there 
are no available textbooks that present a complete and balanced coverage of the subject. This paper provides 
recommendations for course content and reference ma­terials. 1. Int roduct ion The honors program at 
Cal Poly is in its infancy and is current 1y very informal. Students are not se­lected for the program, 
but rather for individual hon­ors course offerings. The topic of an honors course should involve either 
subject matter outside the regu­lar curriculum or advanced study of some area within the regular curriculum. 
The subject and method of in­struction should call for independent study and critical thinking on the 
students part. The criteria for selec­tion of students is left to the instructor s discretion; evidence 
of success in the major and intellectual cu­riosity are desirable, The prerequisites for this course 
will be a grade point average of at least 3.0 (B), and B or better in each course of the four-quarter 
introduc­tory sequence for CS majors (beginning programming through data structures and file management). 
The Permission to copy without fee sII or part of this material is granted provided that the copies are 
not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the 
publication and its date appear, and notice is given that copying is by permission of the Association 
for Computing Machinery. To copy otherwise, or to republish, requires a fea and/or specific permission. 
e 1991 ACM 0-89791-377-9/91 10002-0146...$1.50 instructor may override the default prerequisites in the 
case of alternative evidence of outstanding ability. In any case, proficiency in programming and a good 
grasp of data structures and algorithms are required. We see several reasons for honors students to find 
the proposed course attractive. The obvious practical importance of data compression heads off the familiar 
what do we need to know this for? question; most students are acquainted with public domain archival 
programs and know from personal experience the need to conserve disk space. Most will see immediately 
the benefit in compressing data to be transmitted over mo­dem or network, again calling on personal experience. 
The first author s involvement in an active research project in data compression provides motivation 
in several ways. First, students at a teaching institution like Cal Poly do not often see journal articles 
authored by their instructors (e.g., [LH87] and [HLgo]). Second, the author s participation and the number 
of recent publications dealing with data compression serve as evidence that the material being presented 
represents the state of the art in an area of significant interest. We plan to emphasize the research 
nature of the course through the inclusion of a term project and expect that students will welcome the 
opportunity to contribute to advancing the field. Although our motivation as instructors is primar­ily 
in presenting the history, theory and methods of data compression, we believe that students will also 
benefit from the exercise of implementing the algo­rithms discussed. Implementing data compression al­gorithms 
provides an opportunity for a challenging programming experience. In particular, the idea of two programs 
(encoder and decoder) communicating with each other is not one with which students have a great deal 
of experience. The t~k of debugginga program that produces output that is meaningful only as input to 
another program also provides considerable potential for learning. The rest of this paper will describe 
the content andorganization of the course, including resource ma­terials, course outline, and assignments. 
2. Course Materials There will be no required text for the course; four primary references will be placed 
at the library re­serve desk for students to check out ([ LH87], [s88], [BWC89], [BCW90]). A bibliography 
on data com­pression (about 60 items) will also be provided; the entries will not be required reading 
but students will be able to check them out from the instructor or from the library. The four primary 
references provide very good coverage of all but the most recent developments in data compression. The 
bibliography provides the latest developments as well as increased depth on many topics. In addition, 
students will receive copies of the instructor s class notes. The technique suggested by Reek of using 
transparencies during lecture and then copying them for students will be employed [R90]. Students will 
have access to a VAX cluster. The C language will be suggested for use in implement­ing data compression 
algorithms because of the facility for using low-level bit operations. In addition, access to personal 
computers and the Unix operating sys­tem would permit students to experiment with existing compression 
packages. 3. Annotated Course Outline Overview (1 lecture) This annotated outline will be presented 
as the first lecture, providing a skeleton of the rest of the course. We believe that it is helpful for 
students to have a description of the topics that make up the course and an idea of how they relate to 
each other even if all of this is very vague without the accompanying details. In addition, motivation 
for the course will be provided. Definitions and History (1 lecture) In order to provide a manageable 
starting point, we present a simplified definition of encoding (compressing): the process of constructing 
a mapping (code) from a source alphabet S to a code alphabet A and then applying that mapping to a file 
whose alphabet is S. Decoding is defined as the process of reconstructing the original data from the 
encoded form. Later in the course we will reveal the complexities that these definitions con­ceal. We 
present Morse code, the Shannon-Fano code, and Huffman s algorithm as methods of constructing codes and 
give examples of encoding files using them. Terms from information theory, including uniquely de­codable, 
instantaneously decodable, entropy and opti­mality will be defined using our simple model of data compression. 
All of the material in this lecture is con­tained in [LH87]. Modeling versus Coding (1 lecture) The 
el­ements of the source alphabet S may be symbols or groups of symbols and the mapping from S to A may 
be static or dynamic. We divide the encoding prob­lem into two phases: constructing a model of the input 
file (selecting the elements to be coded); and coding the elements of the model. Examples will be used 
to clarify the concepts and to indicate advantages of cod­ing words rather than characters and using 
dynamic models. More on Static Coding (2 lectures) Arithmetic coding is an alternative to Huffman coding 
that is opti­mal in a stronger sense. We will see later that it is more appropriate for use with dynamic 
(adaptive) models. Variable-length representations of the integers, includ­ing Elias codes, Fibonacci 
codes, and start-step-stop codes, will be described. The survey by Lelewer and Hirschberg defines Elias 
and Fibonacci codes, and the Bell book contains good presentations on arithmetic coding and start-step-stop 
codes [LH87] [BC W90]. We will show that while the representations of the integers may be used instead 
of Huffman coding in our current model, they are poor substitutes. Evidence of their value in data compression 
will be postponed until later in the course. Adaptive Huffman Coding (2 lectures) Algo­rithms FGK and 
A will be described using [V87] as a resource. The benefits of adaptive coding will be illus­trated by 
example. In addition, proofs for time com­plexities and compression guarantees can be sketched. Lempel-Ziv 
Coding (2 lectures) The Lempel- Ziv family of codes will be described. Storer s book is a comprehensive 
reference on this topic [S88]; Fiala and Greene have proposed the most recent variation [FG89]. Earlier 
Lempel-Ziv schemes use fixed-length codewords (unlike any of the other methods we dis­ cuss), and the 
Fiala and Greene algorithm uses start­ step-stop codes. Algorithm BSTW (1 lecture) The concept of locality 
of reference in text files is the idea behind algorithm BSTW [BSTW86]. Variable-length coding of the 
integers is employed here, as are some very sophisticated data structures. Lecture material can be found 
in [LH87]; students will be encouraged to read [BsTw86] for data structure details. Context Modeling 
(2 lectures) The concept of context modeling (using the context in which a sym­bol occurs to condition 
the probability and thus the number of bits used to represent the symbol) will be presented. A new book 
by Bell, Witten, and Cleary is the definitive work in this area [BCW90]. We could easily devote half 
of the course to context modeling; instead we will settle for an explanation of the con­cept, discussion 
of the advantages, and mention of the implement at ion issues. So Which Is Best? (2 lectures) At this 
point in the course (if not long before) students will be clam­oring for us to make some order of the 
clutter of data compression schemes we have presented. It is time to confess that there is no best method 
and to discuss the parameters involved in evaluating and comparing methods. We define as performance 
parameters: com­pression achieved, run time for code construction, run time for encoding, run time for 
decoding, and amount of memory needed by the algorithm. Considerations to be discussed include: in a 
dynamic model code con­struction is concurrent with encoding and decoding; in a static model the code 
must be transmitted along with the file so that the decoder may use it; a particular data compression 
application may require on-line en­coding and/or decoding, or may constrain the amount of memory available 
to the encoding and/or decoding algorithm. The type of data to be compressed may also affect the performance 
of a method. It is likely that these questions will come up earlier in the course so that this section 
may be a thread running through the course rather than the subject of two lectures. Empirical Comparisons 
(1 lecture) It is difficult to make any precise and meaningful statement about the expected performance 
of a given method. Instead we present time, space, and compression results ob­tained by running the algorithms 
we have presented. In most cases we are reporting data gathered by other researchers. The point will 
be made that comparisons involving results obtained by encoding and decoding different files on different 
machines must be viewed with suspicion. Additional empirical comparisons will be contributed by the students 
in the last week of the course when they report the results of their program­ming projects. Special-Purpose 
Techniques (1 lecture) Brief discussion of run-length encoding, difference coding, image compression, 
lossy compression and others will be presented as time permits. Robustness (1 lecture) It is easily 
seen that loss or corruption of a single bit can prevent the decoder from recovering the original text 
from its compressed form. While the Fibonacci codes provide some protection against error propagation 
and many Huffman codes tend to desynchronize quickly, it is unlikely that either of these facts will 
mitigate the problem of errors in adaptive codes. Summary (2 lectures) As we begin the course with 
a preview, we end with a wrap-up. We review the methods we have discussed in view of the perspec­tive 
gained along the way. Presentations on results of student projects will add to this perspective. 4. Course 
Organization Cal Poly is on a 10-week quarter system. Com­puter Science courses are usually 4-unit courses. 
This class will meet for two hours twice a week. Honors courses typically have enrollments of 15 20 students. 
As mentioned above, the instructor will use an over­head projector. Some transparencies will be prepared 
in advance and others will be constructed in class. After class the transparencies will be copied for 
the students so that they may spend class time focusing on the presentation rather than writing notes. 
The grade for the course will be based on several home­work assignments, a project, and a final exam. 
The project is discussed in the next section. The homework assignments will include one or two exercises 
in which the student summarizes a paper from the bibliography. The student will benefit from the experience 
of reading a journal article and will gain either additional depth on a lecture topic or exposure to 
extra material. 5. The Project Students will be allowed to organize themselves into groups of 1 to 4 
people and to define their own class project. The lectures and the bibliography offer many project opportunities. 
The first-lecture overview of the course provides students a basis for reading from the bibliography 
and for reading up on course topics in advance of their occurring in lecture. Students will be required 
to select a topic and discuss it with the instructor during the first three weeks of the quarter. The 
instructor will suggest topics during lecture, will help groups to more carefully define their topics, 
and may define topics for groups having trouble deciding. Ideally, the class will be small enough that 
each group can select a different project. We expect that all projects will involve implement­ing a data 
compression system. A research aspect will be required for each project so that rather than im­plementing 
an existing algorithm the group will be expected to try something new. As mentioned ear­lier, there are 
many issues involved in implementing a context model. A group may select a particular set of parameters 
that have not been investigated yet. A more challenging project is to supply an encoder to ac­company 
the decoding algorithm described in [H L90]. Students with especially good background in systems programming 
may elect to use an existing algorithm in constructing an archival system that allows com­pression of 
directories in addition to single files, and self-extracting decompression. Students with interest in 
graphics may choose to explore image compression. In addition to the programs, each group will turn in 
a project report that will also be presented to the class during the final week of the quarter. The re­port 
will include empirical results, analysis of these results and descriptions of the problems encountered 
and lessons learned from the project. Each group will also make its system available to the instructor. 
The instructor will run the systems on a test suite of files and report winning methods in categories 
such as best overall compression, best compression on a single file, best compression time, best decom­pression 
time, least memory required, best memory­ use/compression-achieved ratio. We expect that the contest 
aspect of the programming project will add to its appeal. 6. Expansion to a Semester Course We are hoping 
that this course can be success­fully adapted for an advanced undergraduate course at California State 
University, Fullerton, which operates on a 15-week semester system. There is sufficient time in a semester 
for expanded coverage of some of the topics and inclusion of additional topics. We outline these changes 
below. Error-Correcting Codes (2 lectures) Prior to the discussion on robustness, we present introduc­tory 
material on error-correcting codes. This mate­rial allows a more concrete discussion of robustness, since 
many data compression techniques embed the codes in an error-correction scheme to achieve robust­ness. 
The material on error-correcting codes includes parity checks, CRC checksums, and Hamming codes. Students 
are assigned additional reading on coding theory [H80]. Recent Results in Context Modeling (2 lec­tures) 
 The coverage on context modeling is ex­panded to include recent work on order-2 and order-3 models by 
the first author. Additional reading is re­quired from [L90]. Adaptive Huffman Coding Proofs (2 lectures) 
The proofs for time complexities and compression guarantees of the FGK and A algorithms are of con­siderable 
theoretical interest and can be examined in greater detail, given the additional time available. Revisiting 
BSTW (1 lecture) The BSTW algo­rithm is a general framework that allows for variations in implementation. 
For example, we could examine the effect of different list organization strategies on BSTW. Additional 
Project A small project is required in addition to the large project mentioned in Section 5. This project 
is completed prior to the larger project and could serve as a launching point for the latter. Appropriate 
projects include evaluating existing data compression packages, such ss compact and compress available 
in Unix, Stuflli available on Macintosh com­puter systems, and ARC and PKZIP on PC compat­ible systems. 
When source code is available, it is also possible to experiment with some modifications and evaluate 
their impact on performance. 7. Summary We are looking forward to teaching this course. We are confident 
that the students will learn a great deal and enjoy doing so. We are certain that we will learn from 
the experience as well. Instructors interested in obtaining our bibliography on data com­pression or 
a report of the success of this course are invited to contact the authors. Note: Due to sched­uling changes, 
the course will be offered at Cal Poly during the winter quarter rather than spring. The conference presentation 
will include a preliminary as­sessment of the success of our proposed course. References [BWC89] Bell, 
T., Witten, I. H., and Modeling for text compression. put. Surv. 21, 4 (Dec., 1989), Cleary, J. G. ACM 
Com­557-591. [BCW90] Bell, T. C., Cleary, J. G., and Witten, I. H. Text Compression, Prentice-Hall, Englewood 
Cliffs, N. J,, 1990. [BSTW861 Bentley, J. L., Sleator, D. D., Tarian, E., and Wei, V. K. A locally adaptive 
compression scheme. Commun. ACM (Apr., 1986), 320-330. R. data 29, 4 [FG89] Fiala, E. R. and Greene, 
D. H. Data com­pression with finite windows. Commun. ACM 92, 4 (Apr., 1989), 490-505. [H80] Hamming, 
R. W. Coding and Information Theory, Prentice-Hall, Englewood Cliffs, N.J., 1980. [HL90] Hirschberg, 
D. S. and Lelewer, D. A. Ef­ficient decoding of prefix codes. Commun. ACM 33, 4 (Apr., 1990), 449-459. 
[L90] Lelewer, D. A. An order-2 context model for data compression with reduced time and space requirements. 
Tech. Rep. 90-33. Dept. of Information and Computer Science, University of California, Irvine (1990). 
[LH87] Lelewer, D.A. compression. (Sept., 1987), and Hirschberg, ACM Comput. 261-296. D. S. Surv. Data 
19, 3 [R90] Reek, M. M. An undergraduate oper­ating systems lab course. In Proceedings of the 21st SIGCSE 
Technical Symposium on Computer Science Education, SIGCSE Bulletin 22, 1 (Feb., 1990), ACM SIGCSE, Washington, 
D. C., 171-175. [s88] Storer, J. A. Data Compression: Meth­ ods and Theory, Computer Science Press, 
Rockville, MD, 1988. [V87] Vitter, J. S. Design and analysis of dy­ namic Huffman codes. J. ACM 34, 
4 (oct., 1987), 825-845. 
			