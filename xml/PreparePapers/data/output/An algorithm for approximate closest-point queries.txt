
 An Algorithm for Approximate Closest-point Queries Kenneth L. Clarkson AT&#38;T Bell Laboratories Murray 
Hill, New Jersey 07974 e-nrail: clarkson@research. att. corn Abstract This paper gives an algorithm for 
approximately solv­ing the post ofice problem: given n points (called sites) in d dimensions, build a 
data structure so that, given a query point q, a closest site to q can be found quickly. The algorithm 
is also given a relative error bound e, and depends on a ratio p, which is no more than the ratio of 
the distance between the farthest pair of sites to the distance between the closest pair of sites. The 
algorithm builds a data structure of size 0(nq)O(l/t)(d-ljt2 in time 0(n2q)O(l/c)(d-lJ. Here q = log(p/~). 
With this data structure, a site is returned whose distance to a query point q is within 1 + e of the 
distance of the closest site. A query needs O(log n)O(l/c)(d-lJ12 time, with high probability. 1 Introduction 
The post-office problem is the following: given a set S of n points (called sites) in d dimensions, build 
a data structure so that given a query point q, the closest site to q can be found quickly. Many data 
structures have been proposed for this problem; their query times generally are faster than the trivial 
O(n), and often are O(log n), as n ~ co. The dependence of the query time on the dimension d is generally 
very steep for the nontrivial algorithms, at least 2n(d). Even heuristic algorithms that are fast in 
practice, such as bucketing and kd-trees, also have this expo­nential dependence. This is unfortunate, 
since many Permission to cop w thout fee all or part of this material is granted provided ti at the 
copies are not made or distributed for direot commercial advanta$e, the ACM copyright notice and the 
title of the publication and its date appear, and notice is given that copying is by permission of the 
Association of Cornputitlg Machinery. To copy otherwise, or to republish, requires a fee andlor specific 
permission. 10th Computational Geometry 94-6/94 Stony Brook, NY, USA @ 1994 ACM 0-89791 -848-4/8410006..$&#38;5O 
of the most interesting potential applications have d at least 10, and often much greater. Thus the promise 
of a query time of O(log n) is crushed by the curse of dimensionality. This paper gives a modest but 
significant improve­ment in the dimension dependence of one recent algo­rithm, that of Arya and Mount. 
[AM93] They attack the problem of approximate solution to the post-office problem; their procedure returns 
an c-closest site, one whose distance to the query point q is within 1 + c of closest. Here c > 0 is 
input when their data structure is built. The algorithm given here takes their ap­ proach to what is 
arguably its logical conclusion, re­ duces the query time from O(log3 n) to ~(log n),l and reduces the 
constant factors for the dimension from 0(1/~)~ to about 0(1/c)(~-l)i2. (The latter reduc­tion applies 
to the storage and query-time bounds.) The new algorithm has an additional factor in storage and preprocessing 
time of log p, where p is bounded above by the ratio of the distance between the sites farthest apart 
to the distance of the sites closest to­gether. (In fact p is roughly the maximum, over all sites s, 
of the ratio of the distance to s to its farthest Delaunay neighbor, to the distance ofs to its closest 
Delaunay neighbor. Hence log p is negligible relative to other factors.) Arya et al. have more recently 
described a dif­ferent approximation algorithm, based on quadtree techniques, that has better storage 
and preprocess­ing bounds than those given here, but with a query time that has a Cl(l/c)d dependence. 
[AMN+ 94] The new algorithm uses a technique previously used for polytope approximation [Cla93] in order 
to approximate a certain Voronoi region for each site by a simpler Voronoi region for the site. The approxi­mation 
problem is solved using randomization. The algorithm also builds a data structure similar to a skip list, 
and uses randomization for that. [Pug90] 1Here d(g(n)) means O(g(n)) with probability at least 1 l/n2. 
 The following section first describes the general ap­proach, and then develops that approach in the 
fol­lowing subsections.  2 The algorithm Following Arya and Mount, the general idea is to find, for 
each site s, a list of sites N, with the following property: ifs is not the closest site to the query 
point q, then there is a site in N, closer to q than s. With this property, a simple search procedure 
will lead to the closest site: pick any site s; if a site t E N, is closer to q, assign t to s and repeat; 
otherwise return s aa closest. This approach is not so interesting just yet. The list N, must be the 
set of Delaunay neighbors ofs, as the interested reader can easily show. This makes for a space requirement 
of f2(n2 ) in the worst case, for d >2. Also, the query time is $_I(n) in the worst case: there is no 
speedup over the obvious algorithm. (For uniformly distributed points, the query time is more like O(nli~), 
so this approach is not entirely useless, however.) For more interesting results, make the problem eas­ier: 
instead of the closest site, find an c-closest site. (Again, such a site has a distance to the query 
point that is within 1 + c of the distance of the closest site s, for c > O.) This is the approximate 
query problem solved by Arya and Mount. They used a collection of narrow cones to obtain their lists, 
in a way similar to Yao s use of them for finding minimum spanning trees [Yao82]. Here the approach is 
to go from the de­sired conditions on the lists to a problem similar to polytope approximation. The modified 
construction begins aa follows. For each site s, consider a list L~ with the following prop­erty: for 
any q, if there is b 6 S with d(g, s) > (1+ c)d(q, b), then there is b c L. with d(q, s) > (1+ e )d(q, 
b ), where c z c/2. (Note that if 6 = O, then L, = N,. ) Using such lists, the search procedure starts 
at any site s. If there is tE L, with d(q, s) > (1 + c )d(q, t), then assign t to s and repeat. Otherwise, 
return s. With L, as defined, the returned s is c-closest. Note that the procedure makes progress at 
each step: the distance of the current site decreases by 1/(1+ d). Consider the condition satisfied by 
L, in a contra­positive way. Fixing s E S, let NC(S) be defined by N,(S) s {q I d(q, s) s (1+ ~)d(q, 
b) for all b ~ S}, so that N, I(L, ) = {q I d(q, s) s (1 +c )d(q, b) for :dl b E L,}. The condition 
on L, is equivalent to ,ti,t (L, ) C N,(S). The set N,(S) is the Voronoi region of s in a certain multiplicatively 
weighted Voronoi diagram. The L, we want is a small one that such th~at NCI(L$) is inside NC(S); the 
problem of finding such an L* can be solved by techniques previously applied to poly­tope approximation. 
[Cla93] The following subsection discusses the problem of finding L,; 32.2 bounds the size of such a 
list; $2.3 shows how to use the lists to obtain fast query times. Finally, $3 makes some concluding remarks. 
 2.1 Finding L. To find L., we ll translate the problem to d + 1 di­mensions using standard lifting map 
techniques. If we put s at the origin, the region NC(S) is the intersection of all regions of the form 
{z I .Z2 s (l+c)2(z-b)2}, where b c S. The condition here is z2/(1 + C)2 ~ (z ­b) , or az ~ 2b. z b2, 
where a ~ l 1/(l+e)2 % 2c. Now let (z, y) denote a point in Rd+l, with z E Rd and yE R. Wehave N,(S) 
={zlayz 2bz-b2andy=z2}~ So for b E S, let fit,b denote the halfspace ti,,b + {(%, y) I (Yy~ 2b. z b2}, 
and PC(S) denote the polytope which is the intersec­ tion of such halfspaces, p,(s) -n ?&#38;,b. bcs 
Then y := z }. N,(S) = {z \ (z, y) E P,(S) and Let V s {(z, y) I y z Z2}. Then for N,l(L, ) c N,(S), 
it is enough that P,~(L, )nW c P,(S) nW. (1) The problem of finding L, satisfying this con­dition can 
be solved using techniques as for poly­tope approximation [Cla93], which are very simi­lar to techniques 
described long age) for linear programming[Cla88]. The general idea is this: give each site an integral 
weight, initially one. Take a ran­dom subset R of S, choosing each site with probabil­ity proportional 
to its weight, and making R about the right size. Test if R satisfies (1); if so, return it. Otherwise, 
we can derive from the testing of R a subset of S that we can expect to be small, and that must contain 
a site of an appropriate L,. Dou­ble the weights of the sites in that subset. Repeat this process until 
done. Here are a few more details of the algorithm: fix some optimal L satisfying (1), where the size 
c of C is as small as possible. Assume for the moment that c is known. (By using exponentially increasing 
esti­mates of c, we lose only a small factor in the size of the returned set, and in running time.) Give 
each P c S a weight, initially equal to one. Choose a ran­dom subset R c S of size CC l d log c, where 
C l is a small constant, choosing a site with probability pro­portional to its weight. For each b c S, 
check that .   P,I(R) n w c ?iC,b. (2) This is a convex programming problem, and as shown by Adler 
and Shamir, it is solvable in O(n) ex­pected time using a randomized procedure similar to one for linear 
programming. [AS90] (The base case, with 0(d2) constraints, can be solved in polynomial time. [Vai89]) 
If all b c S satisfy (2), then return R as Ls. Suppose the condition fails for some b E S. Then there 
will be some vertex v of PCI(R) n Wnot in H,,;; such a vertex can be returned as part of the output of 
the convex programming algorithm. Find all the halfspaces %Cl,b that do not contain v, for b E S. This 
set V of halfspaces will have relatively few members, with high probability, and will contain a member 
of the optimal set C . If the size of set V is less than Czdn/r, then double the weights of the correspond­ing 
sites. (Here C2 is another small constant.) Repeat this procedure until a list L. is returned. An analysis 
of this procedure appears in [Cla93], and will appear in the full paper. 2.2 The size of L, This algorithm 
returns a set L, of size within O(d log c) of the best possible size c, as can be shown as in [Cla93]. 
How large can c be? We have the fol­lowing bound, which this subsection will prove. Theorem 2.1 The optimal 
size c of L, is 0(1/c) (~-l JJ2dlog(p/c). We ll need the following lemma, due to Dudley.[Dud74] Lemma 
2.2 Let P c Rd be compact and convex, and contained in a ball of radius 1. For c with O < e <1, there 
is a convex polytope P > P with 0(1/c)td-llj2 facets, and with P within Haussdorf distance E of P. We 
ll apply this lemma to bound the size of L,. To do so, split W into slabs Vi ~ {(z, ~)Idi <v<di+l}j fori= 
O.. .m, where fi s (1/2) min~es Ilbll, and di = 3di_l/2, and m is large enough that ~ > (2/cr) maxb~s 
Ilbll. We have m = log O(p/a). The following lemma implies that only the points of these slabs need be 
considered. Lemma 2.3 Every halfspace ~C,b contains alJ points of@ not in some Qi. Proof We need to show 
that g ~ Z2 and either y ~ dm or y ~ do imply %!C,bfor any b E S. since b. z is maximized for given Ilzll 
when z = bllzll/l]bl[, when showing that (z, y) 6 h!~,b we can assume z = yb for some -y. If Z2 > dm, 
the minimum valiie of g to consider is Z2, sonlt s enough to show that crzs ~ 2b . z b2, or cq2b2 > 
2-yb2 bz. This is implied by ~ ~ (2/a). If Z2 ~ dm, then similarly we need ay ~ 4b2/a z 2-yb2 b2, which 
holds for y ~ 2/a. When y~ do,wehave Z2~ y~ b2/4, and soweneed azz ~ 2yb2 b2, where y ~ 1/2. Since a.zz 
~ O, the result follows. Cl Consider a given slab Vi. We have 5 ~di ~ ~ di+l , (3) 4 for sufficiently 
small c. Consider the sets 7 = n{(z, y) I cr di+, Z 2b ~Z -b }, LIES and ?b~fl{(~jy)l~di>2b.~-b2}. bEs 
 Then (considering only points in Vi) P,/(s) c P c Pa c P,(s). Moreover, condition (3) implies that there 
is a gap between Pa and Pb that we ll use shortly. Now let ball B s {Z 1Z2 < di+l}, so that all points 
in Vi have z projections in B , and let B -{Z I Z2 < (1+ 2~)2di+1}. Let p; ~ {z I (z, y) c Pa}, and 
similarly define P;. Then P; n B c P: n 1?,and by (3), every point of the former is at least cidi+l/4 
2\lbll from any point not in the latter. If a plane ~et,h does not contain ~i, and so is relevant here, 
then as in the proof of Lemma 2.3, we have di+l ~ b2/4. This says that gap between T; fm and Tjnll is 
proportional to c~, and so by appropriate scaling we can apply Lemma 2.2 to find a polytope p; such that 
and PC has 0(1/e)td-1112 facets. This implies that there is a polytope p in Rdtl of the same complexity, 
such that relative to Qi, PC,(S) c P c P,(s). It not hard to show that there is a coarsening poly ­tope 
p,, (Lf ), with at most d times as many facets as P , such that P, I(L\) n Wi c P,(S). By putting the 
lists L$ together into L,, we obtain the size bound of the theorem of this subsection. 2.3 Solving closest-point 
problems How can a fast query time be obtained using the lists L,? Just as with Arya and Mount s work, 
a skip­list approach is helpful. [Pug90] Choose a family of subsets of S as follows: let R. ~ S; to obtain 
Rj+l from Rj, pick each element of Rj tobe in Rj +1with probability 1/2. Repeat this process until an 
empty R~ is obtained. If s E Rj but not Rl+l, say that s hss level j. Construct the lists L, ,j for each 
Rj, and so s c S has lists for each subset up to its level. To answer a query, start with some s E Rh..-1, 
and find the ~-closest site t~_l in Rk_l using the lists L$,k _ 1. Now find an c-closest site th -z in 
Rk _ z, starting with t~_l. Repeat until to is found, and return to as an c-closest site in S. The correctness 
of this procedure should be clear. How much time does it take? Since each list is bounded in size by 
O(dc log c), where c is 0(1/c)(d-lJi2d log(p/c), the query time is equal to O(dc log c) times the number 
of sites visited in the procedure. It is worthwhile to compare this procedure with one that finds the 
closest site in R, at stage j, not just the e-closest. Suppose we have tjasthe c-closest at some stage, 
but indeed a site t is closest in Rj. When finding the e-closest in Rj +1, the approximate procedure 
will in two steps find a site t in Rj+l such that d(g, t ) s d(q, t)/(1 + C )2. (Here we assume that 
the search in Rj +1 takes at least two steps.) Since (1+ 6 )2 ~ (1+6) for c ~ O, we know that t is closer 
to q than t, The number of sites visited at stage j + 1 for the exact procedure is proportional to the 
number of sites of Rj+l closer to q than t; hence the number of sites visited for the approximate procedure 
in Rj +1 is no more than 2 plus the number for the exact procedure. To analyze the exact search procedure, 
we can follow Sen s analysis of skip lists. [Sen] Look at the search procedure backwards : starting at, 
the clos­est site to q in Rj, visit sites in Rj in order of in­creasing distance, until a site also in 
Rj +1 k encoun­ tered. Call this a level jump. Once the level jump occurs, only sites in Rj +1 are visited. 
The proba­bility of a level jump at a given visited node is 1/2. Thus the probability that at least k 
level julmps occur in v node visits is the probability that a binomially distributed random variable 
has at least k successes in v trials. The query time can be greater than V only if either the number 
of level jumps exceeds K or if fewer than K level jumps occur in V attempts; the former probability is 
no more than n/2K, which we ll need less than some probability J l. This im­plies K ~ lg(n/P1). The probability 
of fewer than K level jumps in V trials can be bounded using Cher­noff bounds for the binomial; letting 
~ z 2K/V, it is exp( V(l 7)2/2). The probability that the query time exceeds 2 lg(n/Pl)/-y for a given 
point q is therefore at most PI + exp( lg(n/P1)(l y)2/~). Hence, setting P1 = l/nQ, an O(Q) log n query 
time is achievable with failure probability 0(1/nQ). This analysis applies only to a single given point 
q; what about arbitrary points? As with similar situ­ations in randomized geometric algorithms, a good 
query time holds for all points because there are nolo( ) combinatorially distinct classes of pcjints. 
That is, in an exact search algorithm, two points ql and q2 will have the same sequence of visited sites, 
and so the same query time, if the distance order on the sites induced by the two points is the same. 
In other words, whether we sort the sites in order of distance from ql, or sort them in order of distance 
from q2, we get the same sorted order. How many classes of points are distinct in this way? Let B be 
the set of ~) perpendicular bisector hyperplanes of pairs of sites, and let A(B) be the subdivision of 
.Rd induced by those bisectors. Then all points in one cell (block) of A(B) induce the same distance 
orders, and so have the same query time, The number of cells of A(B) ( ) is(~)<n d. Thus a query time 
for any point of O(log n) occurs with probability 1 l/#(1). Queries can be made a bit faster by splitting 
up the each list L~ ,j into lists L: j, where the superscript corresponds to the slabs ~i in j2.2. When 
searching for a given site s at a given stage j, the list L~,j with z = 2 lg d(s, q) can be used. This 
gives a query time independent of p. [Dud74] [Pug90] R. M. Dudley. Metric entropy of some classes of 
sets with differentiable bound­aries. J. Approximation Theory, 10:227 236, 1974. W. Pugh. Skip lists: 
a probabilistic alter­native to balanced trees. Comnaun. ACM, 35:668-676, 1990. 3 Concluding Remarks 
It should be possible to have an algorithm that is polynomial in d, by making c a sufficiently large 
con­stant, as in Bern s note. [Ber93] [Sen] [Vai89] S. Sen, Some oberservations on skip lists. P. M. 
Vaidya, A new algorithm for minimizing convex functions over convex sets. In Proc. 30th Annu. IEEE Sym­pos. 
Found. Comput. Sci., pages 338-343, 1989. References [AM93] S. Arya and D. M. Mount. Approximate nearest 
neighbor queries in fixed dimen­sions. In Proc. ~ih ACM-SIAM Sym­pos. Discrete Algorithms, pages 271-280, 
1993. [Yao82] A. C. Yao. On constructing minimum spanning trees in k-dimensional spaces and related problems. 
SIAM J, Comput., 11:721-736, 1982. [AMN+94] S. Arya, D. M. Mount, N. S. Ne­tanyahu, R. Silverman, and 
Angela Wu. An optimal algorithm for approximate nearest neighbor searching. In Proc. 5th ACM-SIAM Sympos. 
Discrete Algo­rithms, 1994. [AS90] I. Adler and R. Shamir. A randomiza­tion scheme for speeding up algorithms 
for linear and convex quadratic program­ming problems with a high constraints­to-variables ratio. Technical 
Report 21­90, Rutgers Univ., May 1990. To appear in Math. Programming. [Ber93] M. Bern. queries in cess. 
Lett., Approximate high dimensions. 45:95 99, 1993. closest-point Inform. Pro­ [Cla88] K. L. Clarkson. 
A Las Vegas algorithm for linear programming when the dimension is small. In Proc. 29th IEEE Symp. on 
Foundations of Computer Science, pages 452-456, 1988. Revised version: Las Ve­gas algorithms for linear 
and integer pro­gramming when the dimension is small (preprint). [Cla93] K. L. Clarkson. Algorithms for 
polytope covering and approximation. In Proc. tlrd Workshop Algorithms Data Struct., Lec­ture Notes in 
Computer Science, 1993.  
			