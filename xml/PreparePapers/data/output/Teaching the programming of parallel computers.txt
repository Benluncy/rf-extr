
 Teaching the Programming of Parallel Computers AlIan L. Fisher and Thomas Gross School of Computer Science 
Carnegie Mellon University Pittsburgh, PA 15213 Abstract Parallel computers are becoming increasingly 
important for scientists and engineers in a wide range of dis­ciplines. However, most undergraduate students 
rarely have the opportunity to program a parallel computer. Furthermore, the field of parallel computers 
is rapidly developing, yet most classes on programming parallel machines that are offered by the vendors 
of various parallel computers are geared towards their computer models and loaded with machine-specific 
details. Early academic texts, precisely because they were early, have tended either to concentrate on 
algorithms more than programming per se, or to concentrate on a collection of languages or systems rather 
than on the underlying principles. Carnegie Mellon University has offered an under­graduate course on 
the programming of parallel machines for undergraduate students for the last two years. This course is 
designed to expose the students to the principal models of programming parallel machines, and to provide 
hands-on experience (on an Encore Mul­tiMax and Cray YMP) using these models. The course emphasizes conceptual 
models, as well as measurement and evaluation techniques, over machine-specific details to achieve a 
high degree of host machine in­dependence. The course concentrates on the principles that are the foundation 
for various programming styles. Speeitically, the course addresses the programming styles of message 
passing, shared memory, (SIMD) data parallelism, and the extraction of parallelism (vector parallelism). 
This work was suppmedin pm by a grant fmm Cray Re.wxmh Inc., by &#38;e Pittsburgh Suparemnpwing Cantsr 
(pSC), and by ttre School of Compumr Scisnce of Carnegie MeUon Uekwsity. Permission to copy without fee 
all or part of this material is granted provided that the copias are not made or distributed for direct 
commercial advantage, the ACM copyright notice and the title of the publication and its date appear, 
and notice is given that copying ie by permieeion of the Association for Computing Machinery. To copy 
otherwise, or to republish, requires a fee and/or specific permission. @ 1991 ACM 0.89791 .377 -9/91 
/0002-0102,.. $1 .50 1. Introduction Parallel computers are becoming increasingly more important as host 
machines for science and engineering applications. Students who graduate with a degree in engineering 
or science should be well versed in the use of such computers to take advantage of the oppor­tunities 
afforded by these powerful computer systems. At the same time, the demand for computer science graduates 
experienced in programming such parallel computers increases as well; as new architectures are developed, 
new software must be developed to accom­pany them. Thus students may approach parallel com­puters from 
two directions: They can learn about them strictly from a user s perspective, or they can learn about 
them as machines for which they have to develop support software such as compilers, programming en­vironments, 
or operating systems. These goals overlap; a developer of software tools must understand how a system 
is used if the tool is to be useful to the applica­tion programmer. Although the use of parallel computers 
is spreading rapidly, there has been no agreement on the best parallel architecture. At this time, numerous 
types of parallel architectures are available, and new architec­tures continue to be developed. We expect 
that this trend will continue for the foreseeable future, and that no single architecture style will 
dominate the field for quite some time to come. 2. Objectives From these two observations, we infer two 
major re­quirements for any course in programming parallel machines: The course cannot be geared toward 
a single architecture, the architectures of a single manufacturer, or even a specific style of architecture. 
Instead, a course must emphasize Conceptual models of architectures suitable for a programmer s use. 
 Performance measurement and evaluation of real programs so that the gmduates can form their own in­formed 
opinion on the merits and usefulness of parallel architectures for real problems that they are asked 
to solve in practice. . The course must provide sufficient hands­on experience so that the graduates 
can use a parallel computer in practice. It is clear that a course on the programming of parallel machines 
must include a sizable laboratory component to provide the students with an opportunity to program a 
parallel system. No class lecture can duplicate the experiences of designing and debugging parallel programs, 
or of observing a real speedup on a parallel system. The role of the classes is then to provide guidance, 
overview, and the base material that is neces­sary to turn the laboratory exercises into a meaningful 
learning experience. The details for any specitlc ar­chitecture are relevant only to the extent that 
they allow students to reflect on their practical experience with a system. The objective of the course 
is then to establish 1. Some practical competence. 2. A understanding of the basic theoretical models. 
 3. The tools to judge and assimilate new technology in the area of parallel com­puters.  This course 
differs horn some others described earlier [1, 2] in that it does not cover the breadth of parallel processing, 
but focuses specifically on the parallel programming process, broadly construed. 3. Course outline The 
major challenge for the design of this course was then to meet these two conflicting goals: to minimize 
the architecture details that have to be mastered by the student (so that there is ample of time to discuss 
the basic principles of parallel programming) but to provide sufficient background that the practical 
experience can be reflected upon and can be explained in terms of the underlying models. We have addressed 
this challenge by choosing the notion of a programming model as the framework to organize the practical 
component of the class. We have also chosen a sequence ofprogramming styles that allows us to order the 
different architecture and programming approaches that students are exposed to in the course of the semester. 
A programming model is the model of parallel com­putation seen and used by a programmer. Rather than 
basing our discussion on specific machines or km­guages, we discuss algorithms, programming tech­niques 
and computer architecture issues in the context of programming models. The sequence of the course is 
based on the explicitness of the models discusse~ that is, the degree to which they require a programmer 
to specify the location and timing of the primitive opera­tions performed by her or his program. The 
course begins with a discussion of algorithms and the prediction, measurement and analysis of program 
performance. We describe a number of usage models that have been used successfully with various parallel 
machines, e.g. input partitioning or output partitioning. There are three reasons to introduce these 
usage models early on: To convince students that finding a way to implement a problem in parallel is 
often not at all difficult. . To indicate the importance of insight gained from practical experience, 
. To provide a common vocabulary and to generate some ideas for the laboratory ex­ercises. After explaining 
the usage models, we introduce a set of programming problems that will be visited during the course of 
the semester. The examples have included . Dymmic programming (determine optimal order to multiply matrices), 
Sorting, . Searching, . Dense matrix multiplication, . Sparse matrix multiplication, . 2D convolution 
and other local image operators, . Path planning (routing on chip or PC board), . Fast Fourier transform. 
 After that, the lectures and assignments cover four programming models: message passing (MP), shared 
memory (SM), data parallel (DP), and extraction of parallelism (l@. A topic outline is given in Figure 
1. Each of the four segments follows the same pattern. First the basic ideas of the model are introduced, 
fol­lowed by an introduction to the specific software en­vironment used in the course. Students work 
on a pro­gramming assignment in parallel with class discussions of algorithms, programming issues, and 
architecture issues. One class period at the end of each unit is dedicated to the presentation and discussion 
of student programs and their performance. As stated earlier, the programming styles are intro­duced 
in order of decreasing explicitness. For the mes­sage passing style, the programmer has to handle a large 
amount of detail: the sequencing of messages, the destination for each message, and the packaging (protocol) 
are all under the control of user programmer. Programs written in this style are typically long, often 
much longer than the equivalent sequential program. The shared memory style simplifies the programmer 
s view; since there is shared address space among all tasks, the main task is to deal with synchronization 
and the assignment of work to processes. Accordingly, the program text becomes shorter, and students 
are nor­mally relieved when they have put the gory details of the message passing style behind them. 
(Performance and scalability are another matter.) The third style is labeled as data parallel and covers 
the style of programming usually associated with SIMD machines. This programming model emphasizes style 
more than the others, and given the right problem, programs in this style can be both compact and ef­ficient. 
There is no need to include synchronization, and there is no explicit communication. However, not all 
problems lead to an elegant solution in this style, and this provides the lead in for the extraction 
of paral­lelism. The idea behind the fourth model, the automatic ex­traction of parallelism, is both 
simple and powerful. Let the user describe the problem in a way that is natural for him or her, e.g. 
in a sequential language like Fortran or C, and have a compiler transform the program. This style has 
been used quite successfully on processors with vector units and some small-scale shared-memory multiprocessors. 
However, as the stu­dents soon realize, the current state of the art only ap­proximates this concept, 
and a programmer actually must often provide specific hints in the form of direc­tives to the compiler. 
4. Course implementation Since we consider the practical experience gained from programming real parallel 
machines to be an in­tegral part of the learning experience of this class, all assignments include a 
sizable programming compnent. The programming assignments are drawn from the set of problems discussed 
in the Introduction section of the class and consist usually of three programming projects (different 
problems). At least one problem is used as a running example , i.e. students have to implement the same 
problem several times during the class for dif­ferent programming styles. In our experience, this or­ganization 
has two advantages: Since the students know the problem al­ready, all energy can be spend on mapping 
the problem onto the specific parallel style. Little time is spent understanding the com­putation. . 
The same problem implemented in a dif­ferent style provides an ideal reference fmme to assess the difficulty, 
drawbacks, and advantages of each programming style. To make sure the students understand the problems, 
we have given out the task to come up with a sequential implementation as zeroth programming assignment. 
Throughout the course, we use the C programming language for all example programs. The decision in favor 
of C (over Fortran or Pascal) is motivated by three reasons: c C is clearly the language of choice for 
all Unix environments and most multiproces­sors. All undergraduates in computer science and engineering 
programs at Car­negie Mellon have a working knowledge of c. Vectorizing compilers for C on vector computers 
are becoming competitive with Fortran compilers; there are no such com­pilers for Pascal.  C is increasingly 
used for scientific com­puting.  By keeping the programming language constant, and by selecting the 
programming assignments from a small set of repeating problems, we allow our students to apply their 
energy towards the parallelization of the problems. For each problem, we usually expect measurements 
and discussion of speed-up, efficiency, and mapping strategies. Students results are presented and discussed 
in class. This process has been critical in helping students to develop intuition and skill in under­standing 
why programs behave the way they do, and and predicting the behavior of new programs. The actual hardware 
base for the course consists of an Encore MultiMax and a Cray YMP. The Encore Mul­tiMax is a shared-memory 
multi-processor with 16 processors, running the Mach operating system and a processor allocation system 
developed locally. The processors are connected, through their caches and a common bus, to the shared 
memory system. In ad­dition, all students at Carnegie Mellon have access to a workstations and a common 
campus-wide file system. The students in our class use the workstations as front­ends to access the special 
class machines and are en­couraged to keep their files in the common file system. 1. Introduction . Why 
parallel programming is difficult . Performance Measwes of performance Issues affecting performance &#38; 
its measurement Analysis of performance, asymptotic &#38; constant factors . Usage models (Input partitioning 
(static load balancing), Output partitioning, Pipelining, Task queues , Divide-and­conquer, Data parallelism, 
Functional parallelism.) . Sample problems . Pitfalls (Race conditions, Deadlock, Livelock, Use of compiler 
directives) 2. MP --Message Passing Programming Basics  Protocol design issues: multicast,  Synchronizations 
styles (Deadlock, Circular wai~ Buffering)  naming, ports, and problems Rendez-vous, Programming language 
and tools issues  Examplex CSP, Occam-2, Ada . Architecture issues  Topology  Routing  Message transfer 
mechanisms:  store&#38;forward, cut-through, cimuit­ switched, Message generation and OS overhead, 
systolic communication, memory communication Examples Intel iPSC, Ncube, Transputer, iWarp. 3. SM --Shared 
Memory Programming Basics  Theoretical model(s): PRAM: like RAM, but with P concurrent processes. CREW. 
EREW.  Figure 1: Syllabus for 105 Figures of merk eftlcient , optimal parallel algorithms  Synchronization: 
methods applications (Semaphores, Sign% Barriers, Fetch and Add)  Example: C Threads  Architecture 
issues Implementation of synchronization (Compare and swap, Fetch &#38; add via combining)  Memory 
(Bandwidth, Structure, Caches, Cache coherence)  Examples: Encore MultiMax, Sequent Balance, Silicon 
Graphics Titan, Cray YMP  Asymptotic limits to shared memory performance  4. DP --Data Parallel Programming 
 Basics  Language issues Examples: Fortran 8x, C*  Architecture issues  SIMD machines  Examples: 
Illiac IV, MPP, Connection Machine   5. XP --Extraction of Parallelism Basics (Pipelining, Vector 
units, Memory interleaving, Multiple operations, Multiple processors)  Dependence analysis  Basic definition; 
flow dependence, anti dependence, output dependence  Dependence in loops  Implications of dependence 
on parallel execution   Vectorizing/Parallelizing compilers . Architecture and performance  Examples: 
Cray 1, Cray YMP,  performance evaluation (Nl~ half­vector length, Los Alamos model for   vector-register 
based machines, Memory ~ontention for real machines) semester course The processor allocation system 
allows a user to reserve up to 12 processors for his or her job and provides thereby a good environment 
to perform realis­tic measurements. To the user the system appears in stand-alone mode, with minimal 
interference by other user tasks. For this class, we developed a communica­tions library (with the usual 
set of primitives like send and receive ) that turns the Encore MultiMax into a message passing system. 
A user program creates a set of processes with distinct address spaces, communicat­ing only via messages. 
With this library, the Encore has most of the characteristics of a distributed-memory computer, thereby 
eliminating the need to introduce yet another machine for the class. (When we offered a graduate level 
precursor of this course for the first time, we used an Intel iPSC hypercube for this class segment. 
However, introducing one more system to the students tcmk additional time away from the lectures, and 
the current solution works well enough for our purpose.) The second segment (shared memory programming) 
uses the C Threads package on the Encore MultiMax. This package is also used by various research groups 
in Carnegie Mellon, and recently it has been used in the introductory operating systems class, so several 
stu­dents have usually been exposed to it before. This package is also nearly interchangeable, call-for-call, 
with the forthcoming Posix threads standard. Using the threads package, a user program creates a set 
of lightweight processes running in a shared address space and synchronizing via simple primitives. For 
the third segment, we developed a simulation en­ vironment (in the horn of a set of C extensions) that 
 provides a set of data parallel primitives but executes them sequentially. Any system with a standard 
C com­piler can serve as a targe~ in practice, most students use their workstation and only a few run 
their programs on the Encore. For the programming assignment for this segment, we don t expect speed-up 
measurements but encourage students to write the most beautiful program. Future iterations of the course 
will add facilities to count the number and size of SIMD operations invoked by a program, and may move 
this segment onto a real SIMD machine. The final segment, extraction of parallelism, employs the compiler 
for the vector capabilities of the Cray YMP. The compiler provides complete information on vectorization, 
and the UNICOS system offers excellent performance analysis tools. 5. Portability We have been fortunate 
in the computing facilities available to us via the Carnegie Mellon School of Com­puter Science and the 
Pittsburgh Computing Center, and we recognize that good access to parallel computers has been limited 
to a relatively small number of institut­ions. One short-term solution to this problem is to use parallel 
programming models to program serial machines, and portable versions of the support software for all 
but the extraction of parallelism unit run on a wide variety of Unix platforms. This approach has the 
disadvantage that performance evaluation is hamperet approximations can be made, but the exhaustive simula­tion 
required to get accurate results is prohibitively ex­pensive. Nonetheless, it is possible for students 
to get a feel for the models and discover some of the pitfalls of parallel programming in such an environment. 
In the longer term, we can expect machines exhibiting interesting forms of parallelism to become com­monplace 
(indeed, this expectation is the motivation for the present course.) A variety of manufacturers offer 
multiprocessor workstations in both shared memory and message passing flavors, and their affordability 
y is in­creasing. Furthermore, advances in compiler technol­ogy and trends in hardware design suggest 
that paral­lelism extraction techniques similar to those used on the Cray will become commonplace on 
microprocessors in the near future. 6. Discussion The primary goal of this course has been to provide 
students with some real-world competence in parallel programming that can be transferred to new architec­tures 
and languages as students encounter them. Our experience thus far has been encouraging. The key reasons 
for the success of the course seem to be these . The discussion of an abstract set of pro­gramming models 
with respect to what decisions they require of the programmer. These programming models provide a bet­ter 
way to introduce different computer ar­chitectures than an approach that is based on the features of 
machines. A feature­based approach distracts from the program­ming problem, classifies machines incor­rectly 
(a shared-memory machine can be used as a distributed-memory machine), and leaves the student with the 
impression of being guided through a zoo of architec­tures. The focus on programming models directs the 
student to always pay attention to the fact that computers exist to run programs rather than vice versa. 
 . The implementation and measurement of a small handful of medium-size programs in each model, allowing 
both comparison of algorithms within a model and styles across models. A suitable selection of problems 
gives the student a chance to work on problems that can be parallelized trivially as well as problems 
that are difficult to run on any parallel computer. This experience will also, we hope, immunize our 
students against outlandish performance claims based on irrelevant programs or based solely on the peak 
performance of a com­puter. An emphasis on the interpretation of ob­served program behavior in terms 
of the decomposition of the problem and the fea­tunxs of the programming model and its im­plementation. 
Learning how to gather and interpret experimental performance data prepares students for the day they 
en­counter a new architecture or have to de­velop a new parallel program. It is also an important step 
towards understanding the limitations of a specific machine or map­ping of a problem onto a parallel 
machine. Almost as important as the above was the decision of what to leave out. Given the goal of providing 
a prac­tical skill, the constraint of fitting the course into one semester limited our presentation of 
parallel algorithms, programming languages and architecture to just enough to underpin the desired skills. 
Our experience in teaching these areas in more detail to students with­out these practical skills leads 
us to believe that a course like the present one would provide valuable background and that we should 
find a sequencing that allows the students to take this course first. (Our en­rollment has included sophomores 
and juniors, and these students have done as well as or better than more experienced students.) Note 
that it is customary for students to take an introductory programming course before taking courses in 
automata theory or computer architecture. Similarly, we find it much easier to teach this course to students 
with little or no theoretical knowledge than to teach the theory to students without practical experience. 
The teaching of programming parallel computers is an important addition to the undergraduate cumiculum. 
Our experience indicates that a course organized along progrrunming models, backed by a solid coverage 
of the important theoretical foundations, provides students with practical experience as well as with 
a solid foun­dation to master the challenges of parallel computing. References 1. Daniel C. Hyde. A Parallel 
Processing Course for Undergraduates. Twentieth SIGCSE Technical Sym­posium, February, 1989, pp. 170-173. 
 2. Chris Nevison. An Undergraduate Parallel Process­ing Laboratory. Nineteenth SIGCSE Symposium, February, 
1988, pp. 68-72.  
			