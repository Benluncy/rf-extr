
 A Performance Evaluation of Cluster Architectures* Xiaohan &#38;in and Jean-Loup Baer Department of 
Computer Science and Engineering University of Washington, Seattle, WA 98195-2350 {xqin,baer}@cs.washington.edu 
Abstract This paper investigates the performance of shared- memory cluster-based architectures where 
each cluster is a shared-bus multiprocessor augmented with a protocol processor maintaining cache coherence 
across clusters. For a given number of processors, sixteen in this study, we evaluate the performance 
of various cluster configu-rations. We also consider the impact of adding a remote shared cache in each 
cluster. We use Mean Value Anal-ysis to estimate the cache miss latencies of van ous types and the overall 
execution time. The service demands of shared resources are characterized in detail by examin- ing the 
sub-requests issued in resolving cache misses. In addition to the architectural system parameters and 
the servicedemands on resoz1rces, the analytical model needs parameters pertinent to applications. The 
latter, in par-ticular cache miss profiles, are obtained by trace-driven simulation of three benchmarks. 
Our results show that without remote caches the per- formance of cluster-based architectures is mixed. 
In some configurations, the negative effects of the longer latency of inter-cluster misses and of the 
contention on the protocol processor are too large to counter-balance the lower contention on the data 
buses. For two out of the three applications best results are obtained when the system has clusters of 
size2 or 4. The cluster-based ar-chitectures with remote caches consistently outperform the single bus 
system for all 3 applications. We also ezercisethe model with parameters reflecting the CUT-rent trend 
in technology making the processor relatively faster than the bus and memory. Under these new con-ditions, 
our results show a clear performance advantage for the cluster-based architectures, with OT without re-mote 
caches, over single bus systems. 1 Introduction Single bus shared-memory multiprocessors, or multis [4], 
have enjoyed a tremendous success. However, the number of processors that can be incorporated in the 
system is limited since the single bus soon becomes an *This work was supported in part by NSF grant 
CCR-94-01689 Permission to make digital/hard copy of part or all this work for personal or classroom 
use is granted without fee provided that copies ara not made or distributed for profit or commercial 
advan- tage, the copyright notice, the title of the publication and its date appsar. and notice is given 
that copying is by permission of ACM, Inc. To copy otherwise, to republish, to post on servers, or to 
redistribute to lists, requires prior specific permission and/or a fee. SIGMETRICS 97 Seattle, WA, USA 
 0 1997 ACM 0-89791-SOS-2/97/0006...$3.50 overcommitted resource. The lack of expansion is exac- erbated 
by the fact that the rate of increase in proces-sor speed grows faster than the corresponding increase 
in bus bandwidth and decrease in memory latency. For example, systems that were balanced with a dozen 
pro-cessors of the x86 vintage might become saturated if more than four processors of the Pentium Pro 
class are attached to the single bus. One way to expand the number of processors while keeping the shared-memory 
paradigm is to consider each multi as a cluster and to link clusters together using some interconnection 
net-work such as one (or two) mesh [12] or an SC1 ring [13]. In cluster systems, memory requests are 
satisfied ei-ther locally, i.e., within a cluster (intra-cluster), or ex-ternally, i.e., by another cluster 
(inter-cluster). Intra-cluster cache coherence is enforced using a snoopy pro-tocol while some form of 
directory-based coherence is used for inter-cluster transactions. Intra-cluster misses require the utilization 
of the internal resources of the cluster, namely the common bus, the local memory at-tached to the bus, 
the snooping caches of the other pro-cessors in the cluster, and, on occasion, the protocol pro-cessor 
(see below). Inter-cluster misses require the uti-lization of the network and additional logic for manag-ing 
the directory information and for transmitting con-trol information and data between clusters. The current 
trend[8,13,16] is to use programmable processors, here-after called protocol processors, rather than 
hardwired controllers for the management of inter-cluster transac-tions. This choice is motivated by 
the ease in expansi-bility, the opportunity of tailoring coherence protocols to the needs of an application, 
and the possibility of in- troducing user-directed communication primitives. The drawback is that inter-cluster 
misses, that already suf-fer a delay an order of magnitude longer than that of intra-cluster misses because 
of the network latency, will be more costly than in the hardwired approach. A bal- anced system should 
then be such that the number of intra-cluster misses does not saturate the cluster buses and at the same 
time the number of inter-cluster misses should not be too large since it takes much longer to resolve 
them. In this paper, we evaluate cluster-based architectures where each cluster is a shared-memory single 
bus mul-tiprocessor to which is associated a protocol processor. We use Mean Value Analysis (MVA) to 
estimate the contention at major shared resources, the cache miss latencies, and the overall execution 
time. The input pa-rameters of the analytical model include architectural parameters such as the cache 
and line sizes as well as the number of cycles needed for primitive, contention-lation. We compare the 
performance of various cluster-based configurations with different application parame-ters, including 
systems that contain a remote cache for reducing inter-cluster references. 1 Businty , , , . . . N/ Ei 
ci I- I m f---l\ NETWORK (a): The base architecture Figure 1: The architectural (b): The protocol models 
processor free, operations, and application dependent parameters such as the cache miss profiles. These 
application de- pendent parameters are obtained via trace-driven simu- The rest of the paper is organized 
as follows. In sec-tion 2, we present the basic architectural model and its variations. In section 3, 
we introduce the analytical model showing how to get an estimate of the cache miss latencies and overall 
execution time. In section 4, we describe our choice of architectural parameters and how we obtained 
the application parameters needed for the evaluation. In Section 5, we show the quantitative re-sults 
obtained by exercising the model and we highlight where performance bottlenecks might arise and what 
configurations are best for various parameters. Section 6 presents related work. Finally, we summarize 
our re-sults in Section 7. 2 Architectural models   2.1 Base architecture and alternatives The base 
architecture, shown in Figure la, consists of clusters connected to each other by an interconnection 
network such as a mesh. Each cluster is a single bus shared-memory multiprocessor augmented with a pro-tocol 
processor. A private cache (or cache hierarchy) is associated with each processor in a cluster. The caches 
are kept coherent via a snoopy protocol. Memory mod-ules can be accessed through the bus and through 
the protocol processor. Thus, local memory accesses, such as cache misses for private data, can be directly 
placed on the bus instead of being routed through the proto-col processor. In addition to data, the memory 
also contains the coherence directory for the (shared) mem-ory associated with the cluster. From the 
directory s viewpoint, nodes are clusters not individual processors. Data that is not local is called 
remote. The protocol processor in the base architecture, Fig-ure lb, contains a protocol processing core 
for running protocol handlers (we assume a full directory coherence protocol), a network interface for 
sending and receiving inter-cluster messages, a bus interface for communicat-ing with the intra-cluster, 
or local, processors, and a memory interface for accessing memory. In order to de-crease the load on 
the protocol processing core and to reduce inter-cluster latencies, we include a forwarding module between 
the bus and network interfaces for those messages where it is easy to decide that they do not need access 
to local memory and/or directory. Examples of such messages would be a read request whose home node is 
another cluster and the from the home cluster. Because there is a large tencies of intra-cluster and 
formance of cluster-based the proportion of the two corresponding write-back discrepancy between the 
la-inter-cluster misses, the per-architectures is sensitive to kinds of misses. Intuitively, this proportion 
is closely related to the number of pro- cessors in the cluster. If we increase this number, the ratio 
of intra-cluster misses to inter-cluster misses will increase. At one extreme, as all processors reside 
in a single cluster, all cache misses become intra-cluster. At the same time, contention on the bus will 
degrade per-formance. On the other hand, if the number of pro-cessors per cluster is too small, then 
we might have too many inter-cluster misses with long latency. At the other extreme, each cluster has 
only one processor and all cache misses not serviced by the local memory are 3 Analytical model Misses 
Rsrs Situations and operations Intra Bus Requested Data is in one local cache. It.1 Cache Data transferred 
cache to cache. Intra Bus Home node is local, requested data R2 PP not in local caches. Home issues Mem 
memory read, and updates directory. Inter Bus Home node is local, data owned It3 PP remotely. Home node 
sends writeback Net to owner. As data returns, it is Cache supplied to requester, written to Mem memory. 
Home updates directory. Inter Bus Home node is remote, data is clean. R4 PP Request forwarded to home. 
Home Net issues memory read, sends data to Mem requester, and updates directory. Inter Bus Home node 
is remote, data owned R5 PP by home. Request forwarded to home. Net Home issues writeback locally. As 
Cache data returns, it is forwarded to Mem requesting node and written to memory. Home updates directory. 
Inter Bus Home node is remote, data owned R6 PP by a third cluster. Request forwarded. Cache to home. 
Home requests owner to Net write back. As data arrives, it is Mem forwarded to requester, written to 
memory. Home updates directory. Table 1: Classification of read misses. This table lists six types of 
read misses, resources used by each type, and a high-level description of the protocol followed on a 
miss. inter-cluster misses. One purpose of this study is to evaluate cluster-based architectures when 
we vary the number of processors per cluster; we will monitor how the cache miss distribution reacts 
to this parameter and what impact it has on overall performance. One variant to the base architecture 
that we will also study is clusters with large remote caches like those found in two recent systems [13, 
161. The remote caches are shared by all processors in the cluster and contain only remote data. Including 
a remote cache in a cluster will reduce the number of inter-cluster misses; we will assess the benefits 
of introducing this new resource in the system. 2.2 Classification of misses Cache coherence in the hierarchical 
architecture is maintained by using a combination of snoopy and directory-based cache coherence mechanisms. 
Detailed protocol operations are similar to those found in [12]. For our modeling purposes, we need to 
classify the cache misses according to their service demand on the shared resources. In Table 1 we summarize 
the actions needed for each type of read misses. A similar table for write misses can be found in [17]. 
3.1 Estimating the execution time A program s execution time can be estimated as: T ezecute = Tinstrs 
+ M, x L c T where Tinstrs is the time to execute the instructions on each processor (assuming good 
load balance), M, is the number of type T cache misses and L, is the latency for type T misses. If there 
is no contention, L, is the summation of service times of the resources used by a type r miss. If there 
is contention, then L, is the sum-mation of service times plus the queuing times on those resources, 
i.e.: L = ccw,r + Sk,,) (2) k In Equation (2), Wk,, is the total waiting time of a re-quest of type 
r on a resource of type k, and similarly Sk,, is the total service demand of request of type T on resource 
of type k. As shown in Table 1 each miss request of a given type generates a specific sequence of sub-requests. 
These sub-requests, the resources they use, and their semantics are shown in Table 2. Table 3 is a synthesis 
of Tables 1 and 2 showing in detail the sub-requests and associated resources for each miss type (similar 
tables for write misses are given in [17]). For example, a read miss of type R3 first generates the fol-lowing 
4 sub-requests in its home cluster: 1. Request for data on cluster s address bus (Areq on resource Abus) 
 2. Insertion in the input queue of the bus interface (BreqI on BI(1)) 3. Protocol processing operation 
(PPops on PPcore) 4. Send a write-back request to a remote cluster (NreqO on NI(0))  Then in the remote 
cluster, the write-back request will pass through the network input queue (NreqI), the for-warding module 
(Freq), the bus interface output queue (BreqO), and be placed on the address bus (Areq). From there on, 
there will be another sequence of sub- requests to get the data from one of the caches or local memory 
of the remote cluster, pass it to the bus in-terface, forwarding module, network interface, and net-work. 
Finally, there will be a last sequence of sub-requests in the home node. 3.2 Architectural assumptions 
As evidenced by the example in the previous para-graph and by Tables 2 and 3, our architectural model 
is quite detailed. Nonetheless, we make a few assumptions and simplifications to keep the analysis computationally 
tractable and effective. To start with, we consider only symmetric systems, i.e., each cluster contains 
the same number of proces- sors, same amount of memory, and remote cache if any. Then looking at each 
component of a cluster, we assume the following: . The instructions executed on the compute proces-sor 
are perfectly pipelined. Therefore the CPI is one under an ideal memory system. Sub-requests Resources: 
Abbrev. Meaning Cycles Areq Address bus: Abus Request data/write-back 2* invalidation/ownership Xdat/Xack/ 
Xown Data bus: Dbus Transfer data/acknowledgement/ 2* ownership 1 L2 cache: L2 1 Read data from L2 cache 
I 4 I Mem I Read/Write memory I 8 I RC 1 Read/Write remote cache BI PPops PP core: PPcore Protocol processing 
e.g., PPrecv/PPsend, PPsched, Table 2: Definitions of sub-requests and resources required. This table 
shows the sub-requests, their names, the resources they use with their abbreviated names, the actions 
and latencies of the sub-requests. Only a subset of the protocol operations are displaved. Bus transactions 
are counted in bus clock cvcles (marked with asteristks). The rest operations are counted-in CPU cycles. 
The latencies of data transactions are for the first 8 bytes. L2 cache misses block the compute processors 
that ing times of the sub-requests issued to k by a type T must wait for the memory requests to complete 
be-miss. While the Sk,r ~ are architectural parameters, fore executing their next instructions. contention 
is present m the determination of the Wk,r ~. The L2 caches have dual ported tags. Hence, We use a closed 
queuing network to model the cluster ar-snooping on the bus does not interfere with a pro-chitectures, 
with compute processors as customers, and cessor accessing its L2 cache if the processor and buses, memories, 
and protocol processors as service cen-the snoop controller access different cache lines. We ters. We 
will use the MVA technique and the following will ignore contention at the L2 level since the con-notations 
to solve Wk,,. current access to the same L2 cache line by the processor and by the snoop controller 
is extremely 0 N is the total number of processors in the system. rare. b N, is the number of processors 
per cluster, i.e., the The cluster buses are split transaction [7]. The ad- cluster size. dress bus 
and the data bus are two separated re- . P, is the probability that a miss is of type T. sources and 
can be used simultaneously for different b I is the average number of instruction cycles be-transactions. 
tween cache misses. b R is the mean total time between cache misses. Finally, we do not consider the 
contention on the net-b uk is the utilization at type k center. work for three reasons: (1) The bandwidth 
provided by . Qk,loc (Qk,rmt) is the arrival queue length at k ob-current network technology appears 
to be sufficient for served by a local (remote) request. the size of the systems investigated in this 
paper and b Sk,r,i,loc ( sk,r,i,rmt) is the service demand of the contention is not an important factor; 
(2) Many mod-ith sub-request to k from a local (remote) T miss. els have been developed to analyze the 
performance of Recall that a cache miss may issue multiple sub- various networks [ 1, 51; and (3) The 
MVA (Mean Value requests to a service center. Analysis) technique we use cannot cope easily with the 
. Sk is the average sub-service demand on center k. network contention directly. If need be the results 
of the . ( mk,r,rmt) is the number of sub-requests contention models just alluded to could be incorporated 
mk,r,loc in the analysis (cf. Section 5). to k issued by a local (remote) T miss. If T does not require 
local (remote) k resource, nz~,~,l~~ = 0 3.3 Modeling contention (~k,r,rmt = 0). Returning to Equation 
(2), Sk,r and Wk,, are respec-b mk,r is the number of local and remote sub-tively the summations of the 
service times and wait-requests issued to type k resource by a type T miss. Type of Sub-requests Misses 
Abus Dbus L2 1 Mem WI) BI(O) WI) WO) Fwd PPcore IntraRl Areq Xdat RI2 IntraR2 Areq Xdat Rmem BreqI PPops 
Inter-R3 Areq Xdat Wmem BreqI BdatO NdatI NreqO PPops Areq Xdat RI2 BdatI BreqO NreqI NdatO neq Fdat 
InterR4 Areq Xdat BreqI BdatO NdatI NreqO fieq Fdat Rmem NreqI NdatO PPops Inter-R5 Areq Xdat BreqI BdatO 
NdatI NreqO neq Fdat Areq Xdat BI2 Wmem BdatI BreqO NreqI NdatO PPops InterR.6 Areq Xdat BreqI BdatO 
NdatI NreqO J+eq Fdat Wmem NreqI NreqO PPops NdatI NDatO Areq Xdat RI2 BdatI BreqO NreqI NdatO fieq Fdat 
 Table 3: Service demands of read misses on shared resources. Rows of this table show the sub-requests 
and the resources needed (each resource has its own column) in a given cluster for a particular type 
of read miss (cf. Table 2 for the meanings of abbreviations). For misses involving multiple clusters, 
the first row shows the service demands for the local cluster; the second and third rows (if present) 
are the service demands for the second and the third clusters involved. mk,,. = mk,,.,loc + mk,r,rmt. 
If T does not use type k resource at all, mk,r = 0. . wk,loc (Wk,rmt) is the waiting time at k for a 
sub-request issued by a local (remote) miss. Based on the assumption that a cache miss blocks the compute 
processor, each processor alternates be-tween executing instructions and waiting for miss re-quests served 
by the memory system. Thus the mean total time between cache misses can be expressed by the instruction 
cycles between two misses under an ideal memory system plus the time spent in the memory sys-tem: R = 
1 + c(P, x c(wk,r + sk,T)) (3) 7 k where 10 if k is L2 cache or network wk,, = mk,r,loc x wk,loc+ (4) 
mk,r,rmt x Wk,rmt otherwise mL...lac~~. ~- rnk.r.rmt Sk,r = c Sk,r,i,loc + c Sk,r,j,rmt (5) i=l j=l 
Equations (4) and (5) express the total waiting time and total service time in terms of the waiting time 
and the service time of sub-requests respectively. We now consider the queuing effect at each type of 
service center on the waiting time of each sub-request. Suppose a type T miss issued from cluster C requires 
resource k of the local cluster, and the same type of resource k of a remote cluster. Since the system 
is symmetric, for each service performed at a remote cluster, resource k of C will need to perform the 
equivalent service on behalf of a request of type T issued from a remote cluster. Therefore, for any 
request issued from the local cluster (C), the arrival queue length at k (Qk,loc) is contributed by (1) 
all the sub-requests issued by the other N, -1 local processors and (2) by the remote requests of the 
N -N, remote processors, with each request having a probability &#38; of requesting cluster C. Similarly, 
for any request issued from a remote cluster, the arrival queue length at k (&#38;?k,rmt) is contributed 
by the sub-requests issued by the local N, processors and $&#38; of the sub-requests c issued by the 
other N-N,-1 remote processors. Hence, the arrival queue lengths at center k observed by a local or a 
remote request are: Cr Pr X(Nc-l)XC~ir oc(W~,~oc+Sk,r,i,~oe) Q k,loc = R + C, Pr XNcxC2irvrrnt (Wk,rmt+Sk,r,j,rmt) 
R C, Pr XNc XC~irgloc (Wk,loc+Sk,r,i,loc) + Q k,rmt = R CrPrx(Nc-~)X~~ir rmt(wk,rmt+Sk,rj,rmt) R (6) 
 A Datapath width of PP 64blts Data bus width 64blts CPU clock/bus clock 2 Table 4: Architectural parameters 
Finally, we can estimate the waiting time for each sub-request by the arrival queue lengths and utlization. 
uk) x Sk + u, x Sk/2 WkJoc = (&#38;k,toc - (7)    wk,rmt = (Qk,rmt-uk) x Sk + uk x Sk/2 where Sk = 
(C PTX $)A c PT) (8) P use k P use k uk = NC x CT R sk7 Since Equations (3)-(g) contain cyclic interdepen-dencies, 
wk,, will be solved iteratively with u&#38;&#38;c and wk rmt initialized to 0. be will vary the number 
of processors in each clus-ter, or cluster size, from a single processor per cluster (as in FLASH [S]), 
to 2, 4, 8, and finally 16, i.e., a sin-gle cluster corresponding to a conventional shared-bus multiprocessor. 
In the alternative designs that employ remote caches, we will keep the remote cache size per processor 
a constant while altering the cluster size, so that comparisons between various configurations are as 
fair as possible. Other architectural parameters such as those related to the L2 caches and the cluster 
bus are given in Table 4. The latencies of the sub-requests are given in Table 2. The network latency 
is 24 cycles. 3.4 Applications parameters In order to exercise the model properly we need rea-sonably 
balanced workloads. To that effect, we selected FFT, RADIX, and RAYTRACE from the SPLASH-2 benchmark 
suite [23]. The algorithms for FFT and RADIX are given in [3] and [6] respectively. RAY-TRACE is an image 
processing program which renders a three-dimensional scene using ray tracing [18]. Table 5 summarizes 
some pertinent statistics about the appli-cations: problem size, number of instructions, number of read 
and write references to shared data. For determining the application dependent param-eters, we have performed 
trace-driven simulations in which we assume that cache misses are insensitive to the accurate timing 
information, hence we use constant latencies for cache misses. The assumption is reason-able for many 
applications for which false sharing is a rare case such as FFT or for those where memory ac-cesses are 
randomized and evenly distributed across the The version we use has been optimized to eliminate false 
shar- ing and to facilitate bulk data transfer. nodes such as RADIX and RAYTRACE. The applica-tion dependent 
parameters of principal interest are the numbers of cache misses of each type and the probabil-ity that 
a replaced cache block is dirty and, in that case, whether its home node is local or remote. To gather 
these statistics, we use Mint [20] as the simu-lation tool. Mint is a software package that emulates 
multi-processing execution environments and generates memory reference events which drive a memory system 
simulator. Figures 2,3, and 4 show the cache miss profiles of the three applications. The four and half 
pairs of cache miss profiles correspond to architectures of cluster size 16, 8, 4, 2, and 1 respectively; 
the left (right) bar of each pair corresponds to an architecture not containing (contain-ing) remote 
caches. Since the chosen applications have not been programmed to take advantage of the clus-ter architectures, 
the numbers of intra-cluster misses of shared references drop, as expected, when the cluster size decreases. 
In the cases of FFT and RADIX, when the cluster size halves, the intra-cluster misses reduce by half 
approximately. A plausible explanation for this is that data in processor caches and/or cluster memories 
is uniformly distributed and that the average number of processors sharing a piece of datum is 2. Therefore 
when the cluster size reduces by half, nearly one-half of the intra-cluster misses have to cross the 
cluster bound-ary. For RAYTRACE, the average number of processors sharing a datum is higher. As a result, 
when a cluster splits into two, a smaller portion of the intra-cluster misses turn into inter-cluster 
misses. Adding remote caches changes significantly the cache miss profiles. The presence of remote caches 
increases the retention of remote data. When a line, whose home is in a remote cluster, is replaced in 
an L2 cache, it may still exist in the remote cache either because of some inclusion property or because 
on a replacement the first option is to write to the remote cache. Thus, the remote caches can transform 
some of the inter-cluster misses caused by conflict mapping or capacity limitation in the L2 caches into 
intra-cluster misses. The extent of the reduction in the inter-cluster misses is determined by the proportion 
of conflict and capacity misses in these inter-cluster misses. From [23], we know that when processor 
caches are of limited size, both RADIX and RAYTRACE encounter significant amounts of capacity misses. 
For these two applications, the remote caches are very effective in transforming the inter-cluster misses 
into intra-cluster misses. On the other hand, FFT has a smaller propor-tion of capacity misses, therefore 
the remote caches are Table 5: Applications: problem sizes, number of in-structions executed (in millions), 
number of shared read references (in millions), number of shared write refer-ences (in millions).  
 $1.5 5 5 1.0 e ti .G 0 P 0.5 I % 0.0 16 6 4 2 1 Cluster size Figure 2: The cache miss profile of FFT. 
The left (right) bar of each pair corresponds to an archi-tecture not containing (containing) remote 
caches. Rl, R2 and Wl,W2 are intra-cluster read and write misses. R3, R4, R5 and W3,W4, W5, W6 are inter-cluster 
misses which involve two clusters. R6 and W7, W8 are misses involving three clusters. Table 1 gives detailed 
explanation of each type of read misses. For write misses, please refer to [17]. 6.0 f m g 6.0 Fj .; 
4.0 .g 2 2 2.0 2 .- 6 4 2 1 Cluster size  Figure 3: The cache miss profile of RADIX. The left (right) 
bar of each pair corresponds to an ar-chitecture not containing (containing) remote caches. Rl, R2 and 
Wl, W2 are intra-cluster read and write misses. R3, R4, R5 and W3, W4, W5, W6 are inter-cluster misses 
which involve two clusters. R6 and W7, W8 are misses involving three clusters. Table 1 gives detailed 
explanation of each type of read misses. For write misses, please refer to [17]. 3.0 IntraJtl,P 0 Inlra-Wl,2 
E m 2.0 Inter-R3,4,5 tl 9 Inter-W3,4,5.6 .E 0 Inter-R6 P 1.0 Inter-W7,6 : I - 0.0 16 Cluster size 
Figure 4: The cache miss profile of RAYTRACE. The left (right) bar of each pair corresponds to an ar-chitecture 
not containing (containing) remote caches. Rl, R2 and Wl, W2 are intra-cluster read and write misses. 
R3, R4, R5 and W3, W4, W5, W6 are inter-cluster misses which involve two clusters. R6 and W7, W8 are 
misses involving three clusters. Table 1 gives detailed explanation of each type of read misses. For 
write misses, please refer to [17]. not as helpful. As we increase the size of L2 caches (not shown), 
the benefit of the remote cache diminishes. Fi-nally, we note that remote caches also bring a negative 
impact. Because they tend to retain data in remote clus-ters longer, they increase the 3-hop inter-cluster 
misses (misses involving three clusters). 4 Exercising the model: performance evaluation The cache miss 
profiles by themselves are not suffi-cient to assess the performance of the architectures. For example, 
in a single bus system, all cache misses are intra-cluster misses. However if the bus is saturated, the 
miss latency of the intra-cluster misses will signifi-cantly increase. When we use a balanced multi-cluster 
architecture, the inter-cluster misses may still incur a latency longer than that of the misses of the 
saturated single bus system but the overall performance may be better since the intra-cluster misses 
of the multi-cluster architecture can be resolved faster. 4.1 Average service demand and con-tention 
We have shown in Table 3 the service demands of read misses. Based on this and other tables found in 
[17], the parameters given in Table 2, and the cache miss profile, we can compute an application s average 
service demand (per processor) for each resource (cf. Equation (8) of Section 3.3). Table 6 displays 
average service demands for the RADIX application. The magnitude of the resource service demands determines 
the relative intensity of contention. The resource with the highest demand is subject to the highest 
contention and will be the first to saturate among all. As can be seen in Table 6, of the 10 shared resources, 
the data bus (Dbus) and the protocol processing core are the two resources with Average service demands 
NO remote cache N, Abusm) 1 NI(0) 1 Fwd 1 PPcore Table 6: RADIX s average service demands. the heaviest 
demand. This is true not only of RADIX but also of the other two applications FFT and RAY-TRACE. We will 
therefore focus our attention on these two resources. As the cluster size becomes smaller, the number 
of intra-cluster misses decreases. The reduction in the number of intra-cluster misses does not imply 
a reduc-tion in service demand on the data bus. Independently of whether the miss is intra or inter-cluster, 
the data has to transit on the local cluster s bus. In fact, the de-mand on the bus might increase in 
inter-cluster misses if the data is owned by a cache of the remote cluster. In that case, the data bus 
of the remote cluster is used rather than the direct link from protocol processor to memory. According 
to the symmetry principle that we used earlier, the overall demand on the data bus ser-vice center is 
therefore increased. In our experiments, most of the remote requests were serviced by memory and the 
demand on the data bus remained practically unchanged with cluster size. Of course, as the number of 
requesters in the cluster decreased, contention for the bus decreased. For the protocol processing core, 
a larger inter-cluster miss ratio certainly leads to a higher service demand. Like in the case of the 
data bus, the increase is rather limited. For example, looking at RADIX, the percent-age of the inter-cluster 
misses rises from 50% to 75% (cf. Fi ure 3), but the service demand increases by less than 10$ o when 
the cluster size is halved from 8 to 4. There are two reasons that can explain this fact. First, some 
of the intra-cluster misses, specifically R2 or W2, also require the service from the protocol processing 
core. Second, the design of the forwarding module offloads the service demand from the protocol processor 
for the inter-cluster misses. Note than when remote caches are employed, the de mand on the data bus 
is barely altered while the demand on the protocol processing core drops significantly. In this architectural 
variation, the contention on the pro-tocol processor will be governed by the cluster size as well as 
by the presence of the remote caches. 4.2 Overall performance In order to assess the overall performance, 
i.e., execu-tion time, of the various configurations we exercise the model presented in Section 3.3. 
We first compute the waiting times of each cache miss type on a given resource (cf. Equation (4) that 
has to be solved iteratively) and combine them with the service demands computed as in the previous section 
to obtain the cache miss latencies. These latencies are shown in Table 7 for the RADIX application. We 
obtain the execution time by apply-ing Equation (1). The results for the three applications are shown 
in Table 8, which shows the execution times of the 3 applications normalized to the execution in a single 
cluster. From Table 7 we see that both the intra and the inter-cluster miss latencies decrease monotonically 
as more resources are added to the system. The large dif-ferences (from 60% to 270%) between the intra-cluster 
miss latency of the single bus system and those of other architectures indicate that the contention in 
the former system is substantial. When the single cluster system splits into two, the contention on the 
data bus drops dramatically but the intra-cluster miss latency remains high. This is because the contention 
on the protocol processor is severe. As mentioned before, some of the intra-cluster misses (R2 or W2 
and local replacement of a dirty line) of the multi-cluster architectures call for the service of the 
protocol processor. In other words, the intra-cluster miss latency is affected by the contention on the 
protocol processor. The normalized execution time is determined prin-cipally by the number of instructions 
executed between cache misses (I in Equation (3)) and the average service demand on each resource, rather 
than by the shared miss ratio. RADIX and RAYTRACE have almost the same miss issuing rate. Therefore, 
even though RADIX has a much higher miss ratio (6.8%) than RAYTRACE does (2.7%), their normalized execution 
times do not differ considerably as long as there are at least 4 clus- ters. The reason that RAYTRACE 
has slightly better normalized execution times is that its average service demand on the protocol processor 
is significantly less. As a result, when there are as many as 8 processors in each cluster, the overall 
performance of RAYTRACE surpasses the single cluster system, while for RADIX, the turning point occurs 
at the cluster size of 4. FFT, on the other hand, has a miss issuing rate lower than RADIX and RAYTRACE. 
Its contention on the single Table 7: RADIX s average cache miss latencies Miss latencies No KC KC N, 
lntra ] inter lntra Inter 16 259 ] - 8 158 I 416 119 384  Table 8: Execution times of the 3 applications 
for vari- ous configurations. The execution times are normalized to the execution time on a single cluster. 
RC stands for remote cache. bus system is not as high as those of the other two ap-plications. Thus, 
for this specific application, when no remote caches are employed, the performances of clus- ter architectures 
do not match up that of the single bus system. When remote caches are included the cluster archi-tectures 
consistently perform better than the single clus-ter system on all three applications from 15% to 33%. 
The benefits incurred by the presence of the remote caches derive from the fact that they increase the 
pro-portion of the intra-cluster misses and they also reduce the intra-cluster miss latency while increasing 
the inter-cluster miss latency to a smaller extent (cf. Table 7). The latency of intra-cluster misses 
decreases because the data supplied by remote cache do not involve the pro-tocol processor. The slightly 
larger inter-cluster miss latency is caused by higher S-hop inter-cluster miss ra-tio, not by the contention 
on the protocol processor, which, in fact, is reduced by the presence of the remote caches. In summary, 
our model shows that cluster architec-tures, with no optimization in the software to take ad-vantage 
of the architectural organization, would pro-vide little benefit over a flat architecture. However, by 
increasing the configurations with remote caches, the performance improvements are quite noticeable. 
This last observation certainly justifies the design decisions in recent cluster architectures [13, 161. 
The architectural parameters of Table 4 are justifi-able with the current technology for processors, 
caches, and buses. As newer processor designs adopt more ag-gressive superscalar techniques the miss 
issuing rate will increase. In other words, the number of instruction cy-cles between misses decreases. 
Second, CPU clocks are getting faster, making the bus and memory relatively 0.69 0.62 0.60 1 0.43 1 
Table 9: Normalized execution time for the faster ar-chitecture. slower. This translates into larger 
(in terms of CPU cycles) service demands on these resources. Obviously, both trends tend to increase 
the contention on these shared resources. A feature that would counter balance these effects would be 
the presence of wider cluster data buses and wider data paths in the protocol processor. Data transfers 
in these resources that are potential bot-tlenecks would be sped up. As a simple experiment to see the 
effects on cluster architectures of these technological trends, we doubled four parameters: The ideal 
miss issuing rate (or l/1), i.e., we assume that the CPI of each processor was halved. The ratio of CPU 
clock rate vs. bus clock rate, i.e., we assumed that the CPU clock will improve at twice the rate of 
the bus clock. The width of the data bus. The width of the data path of the protocol pro- cessor, with 
the increase of this parameter reduc- ing the latency of data transactions in the protocol processor. 
 With these changes in parameters, the service demand on the data bus remains approximately the same 
since the doubling of the CPU to bus clock ratio is counter-balanced by the doubling of the width in 
the data bus. However, contention is increasing since the issue rate is also doubled. The results of 
exercising the model with these new parameters are shown in Table 9. Now cluster architec-tures, with 
and without remote caches, are clearly more advantageous. When there are 4 processors in each clus-ter, 
the cluster architectures perform 29%-46% better than the single bus system without the remote caches, 
and 43%-57% better with the remote caches. This is because the contention on the data bus of the single 
bus system will be intensified at a rate faster than the contention on the protocol processor of the 
cluster ar-chitectures. 5 Related work The behaviors of the service centers contained in our model generally 
satisfy the two assumptions required by the MVA technique, i.e., routing homogeneity and service time 
homogeneity [ll]. Vernon, Lazowska and Zahorjan have shown that the MVA technique applied to shared-bus 
multiprocessor systems is of remarkable accuracy [22]. Vernon et al. applied the same tech-nique to studying 
the performance of purely bus-based hierarchical multiprocessors [21]. Our model and theirs share some 
common elements because of the similarity in the architectures. Torrellas, Hennessy and Weil also developed 
analytical models to investigate the impact of several architectural and application parameters in shared-memory 
processors based on DASH [19]. The main differences between their model and ours are that they used an 
open queuing network and that the inter-cluster interface was hardwired [12]. They found that contention 
for the bus dominated that of the cluster interface . We show that this conclusion is no longer true 
in all cases when the design shifts from hardwired controller to protocol processor. Holt et al. studied, 
via simulation, the effects of the occupancy of the protocol processor and of the la-tency of the network 
for a Flash-like architecture where the number of processors per cluster is one [9]. They showed that 
the time consumed by the protocol proces-sors did have an impact on the performance, especially with 
a fast network. However it is not clear whether con-tention causes considerable delay in protocol processors 
or not. Cluster-based architectures where inter-cluster communication is provided by a (hierarchy of) 
bus and a shared cache(s) have been studied also via simulation by Nayfeh et al. [14, 151 and by Anderson 
21. Conclu- sions on the viability of clusters are mixe d depending on the amount of memory requests 
and their locality. As several processors share a protocol processor, the latter is likely to become 
the performance bottleneck. Some researchers have looked at the idea of using one of the processors in 
the cluster as a protocol processor. If a cache or page miss blocks the computation, the com-pute processors 
may as well be used to resolve the fault. Karlsson and Stenstrom simulated such a scheme for a multiprocessor 
which used an ATM network to link clus-ters of bus-based multiprocessors [lo]. The intra-cluster coherence 
is maintained by cache snoopy hardware while inter-cluster coherence is maintained at the page level 
by distributed virtual shared memory software. With a network latency of lOOus, the chance of finding 
a com- pute processor idle is quite high (52%-92% for 4 pro- cessors/cluster) even when a simple round-robin 
policy is used for scheduling the task of protocol processing. The bottleneck is in the ATM interface 
and the soft-ware must be tailored to the application for significant speed-ups to occur. The authors 
conclude that in the context of such an architecture, and accompanying soft-ware, the presence of a separate 
protocol processor is not necessary. In our study we assumed that the interconnec-tion network is of 
sufficient bandwidth and used a contention-free network latency in the model. However a network model 
can be easily incorporated in the eval-uation if the contention is not negligible. There exists a rich 
set of analytical models for various network topolo-gies [l, 51. These models usually take the request 
rate as an input parameter and compute the network latency taking into effect the contention. This latency 
could be used in our model instead of the contention-free net-work latency to achieve a more accurate 
response time for inter-cluster misses. The response time then would in turn affect the request rate 
issued to the network. The two models would have to iterate until they both converge. 2The number of 
processors per cluster was fixed at four. 6 Conclusion In this paper, we have applied Mean Value Analysis 
to assess the performance of cluster-based architectures. We have accurately characterized the service 
demand of shared resources by examining in detail the sub-requests involved in the resolution of cache 
misses. In addition to the overall system parameters and the service demands on shared resources, the 
analytical model needs param-eters pertinent to applications, i.e., the cache miss pro-files. These parameters 
were obtained via trace-driven simulation for three applications. We have compared the performance of 
cluster-based architectures of various cluster sizes with and without remote caches. Our simulation and 
analytical results shed some light on three different aspects of the im-pact of cluster-architectures3: 
the applications cache miss profile, the service demand and the contention for shared resources, and 
the overall performance normal- ized to the single bus system. We summarize these re- sults as follows: 
 Many existing parallel programs, and our exam- ple programs, are written for machines with a flat architectural 
model. For these applications, and for the cluster-based architectures without remote caches, a si nificant 
portion of intra-cluster misses (33% to 50 ff0) becomes inter-cluster misses of longer latency when the 
cluster size is reduced by half. When large remote caches are present, they retain, as expected, much 
of the remote data in the cluster. They prevent part of the intra-cluster misses (ca-pacity misses and 
conflict misses) from becoming inter-cluster misses when the cluster size decreases. In the cluster-based 
architectures which employ protocol processors and maintain cache coherence in software (i.e., the protocol 
processor is distinct from the bus and network interfaces), the protocol processor and the data buses 
are the resources for which there is the greatest service demand. Service demand on the data bus remains 
approxi-mately the same independently of the cluster size (although contention decreases with cluster 
size). The service demand on the protocol processor in-creases as the cluster size decreases, but it 
does not increase as fast as the rate of inter-cluster misses. Remote caches reduce the service demand 
on the protocol processor significantly but have little im-pact on the service demand on the data bus. 
The contention for shared resources is determined mainly by three factors: the miss issuing rate, the 
cluster size, and the presence of a remote cache. The contention on the data bus and on the proto-col 
processor are the prime factors influencing the performance of the cluster-based architectures. Without 
the remote caches, the performance of the cluster-based architectures is mixed. For two out of the three 
applications, a cluster size of 2 or 4 gives 3We are in the process of validating the models with simulations. 
 the best results (improvements in the range of 6% to 18%. . When remote caches are present, all configurations 
consistently outperform the flat architecture (im-provements in the range 15-33%). . Finally, if we modify 
the service demand parame-ters to reflect the technological trend of faster pro-cessors with respect 
to the speed of buses and la-tency of memories, we see that the cluster-based architectures will have 
an increasing performance edge over the single bus system. References A. Agarwal. Limits on interconnection 
network performance. IEEE Trans. on Parellel and Dis-tributed Systems, 2(4):398-412, April 1991. PI 
C. S. Anderson. Improving Performance of Bus- Based Multiprocessors. PhD thesis, Dept. of Com- puter 
Science, Univ. of Washington, 1995. D. H. Bailey. FFT in external or hierarchical mem- 131 ory. J. of 
Supercomputing, 4(1):23-35, March 1990. G. Bell. M&#38;is: A new class of multiprocessor com- PI puters. 
Science, pages 462467, April 1985. L. N. Bhuyan, Q. Yang, and D. P. Agrawal. Perfor- PI mance of multiprocessor 
interconnection networks. Computer, 22(2):25-37, February 1989. G. E. Blelioch et al. A comparison of 
sorting algo- 161 rithms for the connection machine CM-2. In Proc. of the Symposium on Parallel Algorithms 
and AT-chitecture, pages 3-16, 1991. M. Galles and E. Williams. Performance opti- VI mizations, implementation, 
and verification of the SGI challenge multiprocessor. In Proc. of the 27th Hawaii International Conference 
on System Sci-ences. Vol. I: Architecture, pages 13443, 1994. J. Heinlein, K. Gharachorloo, S. Dresser, 
and A. Gupta. Integration of message passing and shared memory in the Stanford FLASH multipro-cessor. 
In PTOC. of 6th International Conference on Architectural Support for Programming Languages and Operating 
Systems, pages 38-50, 1994. C. Holt et al. The effects of latency, occupancy, PI and bandwidth in distributed 
shared memory mul-tiprocessors. Technical report, Dept. of Computer Science, Stanford Univ., 1995. CSL-TR-95-660. 
M. Karlsson and P. Stenstrom. Performance PO1 evaluation of a cluster-based multiprocessor built from 
ATM switches and bus-based multiprocessor servers. In PTOC. of 2nd Conf. on High-Performance Computer 
Architecture, pages 4-13, 1996. E. D. Lazowska, J. Zahorjan, G. S. Graham, and Pll K. C. Sevcik. Quantitative 
System Performance. Prentice-Hall, Inc., 1984. M D. Lenoski et al. The Standford DASH multipro-cessor. 
IEEE Transactions on Computer, 25(3):63-79, March 1992. M T. Lovett and R. Clapp. STiNG: A CC-NUMA computer 
system for the commercial marketplace. In PTOC. of 23rd International Symposium on Com-puter Architecture, 
pages 308-317, 1996. I141 B. A. Nayfeh and K. Olukotun. Exploring the de-sign space for a shared-cache 
multiprocessor. In PTOC. of 2lst International Symposium on Com-puter Architecture, pages 166-175, 1994. 
1151 B. A. Nayfeh, K. Olukotun, and J.P. Singh. The impact of shared-cache clustering in small-scale 
shared-memory multiprocessors. In PTOC. of 2nd Conference on High-Performance Computer Archi-tecture, 
pages 74-84, 1996. A. Nowatzyk et al. The S3.mp scalable shared memory multiprocessor. In PTOC. of the 
1995 In-ternational Conference on Parallel Processing, Vol II, pages l-10, 1995. (171 X. &#38;in and 
J.-L. Baer. A performance evalution of cluster architectures. Technical report, Dept. of Computer Science 
and Engineering, Univ. of Wash- ington, 1997. UW-CSE97-01-02. P31 J. P. Singh, A. Gupta, and M. Levoy. 
Parallel visu-alization algorithms: performance and architecture in&#38;ications. IEEE Computer, 27(7):45-55, 
July J. Torrellas, 3. Hennessy, and T. Weil. Analysis  I191 of critical architectural and program parameters 
in a hierarchical shared-memory multiprocessor. In PTOC. of the 1990 ACM SIGMETRICS Conference on Measurement 
and Modeling of Computer Sys-tems, pages 163-72, 1990. J. E. Veenstra and R. J. Fowler. MINT: a front 
end PO1 for efficient simulation of shared-memory multipro-cessors. In Proc. of the 2nd International 
Workshop on Modeling, Analysis, and Simulation of Com-puter and Telecommunication Systems, pages 201-7, 
1994. WI M. Vernon, R. Jog, and G. S. Sohi. Performance analysis of hierarchical cache-consistent multipro-cessors. 
In PTOC. of the International Seminar on Performance of Distributed and Parallel Systems, pages 111-126, 
1988. M. K. Vernon, E. D. Lazowska, and J. Zahor- P21 jan. An accurate and efficient performance anal-ysis 
technique for multiprocessor snooping cache-consistency protocols. In PTOC. of 15th Intema-tional Symposium 
on Computer Architecture, pages 308-315, 1988. S. C. Woo, M. Ohara, E. Torrie, J. P. Singh, and 1231 
A. Gupta. The SPLASH-2 programs: Characteri-zation and methodological considerations. In Proc. of 22nd 
International Symposium on Computer AT-chiteckre, pages 24-36, 1995.  
			