
 Generating Local Addresses and Communication Sets for Data-Parallel Programs Siddhartha Chatterjee * 
John R. Gilbert t Fred J. E. Long ~ Robert Schreiber * Shang-Hua Teng S Abstract Generating local addresses 
and communication sets is an import­ant issue in distributed-memory implementations of data-parallel 
languages such as High Performance Fortran. We show that for an array A affinely aligned to a template 
that is distributed across p processors with a cyclic(k) distribution, end a computation in­volving the 
regular section A(t: h: s), the local memory access sequenw for any processor is characterized by a finite 
state me­chine of at most k states. We present fast algorithms for computing the essential information 
about these state machines, end extend the framework to handle multid@ensional arrays. We also show how 
to generate communication sets using the state machine ap­proach. Performance results show that this 
solution requires very little runtime overhead and acceptable preprocessing time. 1 Introduction Languages 
such as Fortran D [4] and High Performance Fortran [5] allow the programmer to annotate programs with 
alignment and drk­tribution statements that describe how to map arrays to processors in a distributed-memory 
environment. Both languages introduce an auxiliary Cartesian grid called a templat< arrays are aligned 
to templates, and templates are distributed across the processors. The alignment of an array to a template 
and the distribution of the tem­plate determine the final mapping of the array. lle compiler must partition 
the arrays end produce node code for execution on each processor of a distributed-memory parallel computer. 
Consider an array A of size n aligned to a template such that array element A(z) is aligned to template 
cell as + 11.The template Reaesrch lrmtituta for Advsnced Computsr Science, Mail Stop 1045-1, NASA Ames 
Research Center, Moffett Field, CA 94035-1030 (sc@riacs.edu, schreibxtf?riacs.edu). The work of these 
authom was supported by the NAS Systems Dmision vis Ccqerative Agrsemmt NCC 2-3S7 bstweerr NASA and tbe 
Universities Space Resaerch Association (USRA). txmox pdo Alto Research Centar, 3333 Coyota Hitl Road, 
Palo Alto, CA 94304­1314 (gilbert@parcxmx.core). Copyright @1992 by Xerox Corpomtion. All rights reservrd. 
+~-at of cmpu~ ~d Womation Science, Univemity of California, .%nta CIUZ, CA 95o64 (ilong@cm.ucsc. rdu). 
This work was petfonned whiie the author wss a summer studmt at RIACS. s ~-=t of ~~aati~, h@.sachus@b 
Institute of Technology, ~~idge. MA 02139 (sreng@hemy.lcs.mit.edo). lMs work was pacfonnsd while the 
author was a postdoctoml scientist at Xerox PARC. Permission to copy without fee ell or part of this 
material is grsnted provided that the copies are not made or distributed for direct commercial advantage. 
the ACM copyright notice and the title of the publication and its date appear, and notice is given that 
copying is by permission of the Association for Computing Mschinery. To copy otherwise, or to republish, 
requires a fee and/or specific permission. 4th ACM PPOPP,51931CA,USA 0 1993 ACM 0-89791 -589 -5/93 /0005 
/0149 . ..$1 .50 A a distributed array n size of array A a stride of alignment of A b offset of eligtrment 
of A P number of processors to /s block size of distribution f lower bound of regular h upper bound of 
regular s stride of regular scxtion m processor number to the template to the tcrnplate which the template 
is dishlmted of the template section of A section of A of A Table 1: Symbols used in this paper. is 
distributed across p processors using a cyclic(k) distribution, in which template cell i is held by processor 
(i div k) mod p. (We number array elements, template cells, and processors starting from zero.) Thus 
the array A is split intop local arrays residing in the local memories of the processors. Consider a 
computation involving tlte regular section A(1: h: s), i.e., the set of array elements {A(4 + js) :0 
S j < (h t)/s}, wheres >0. (Table 1 summarizes the notation.) In this paper we consider the following 
problems What is the sequence of local memory addresses that a given processor m must access while performing 
its share of tbe computation? c What is the set of messages that a @en processor m must send while performing 
its share of the computation? The cyclic(k) distributions generalize the familiar bbck and cyclic distributions 
(which are cyclic( [n/pi) and cyclic( 1) respec­tively). Dongarra et al. [3] have shown these distributions 
to be es­sential for efficient dense matrix algorithms on distributed-memory machines. High Performance 
Fortren includes directives for multi­dimensional cyclic(k) distribution of templates. Many spwial cases 
of these problems have been solved, but the general solution does not appear in the literature or (to 
the best of our knowledge) m any implementation. Koelbel and Mehrotra [8] treat special cases where the 
array is mapped using a block or a cyclic mapping, and the regular section has unit stride. Koelbel [7] 
handles non-unit-stride regular sections with block or cyclic map­pings. MacDonald et al. [9] discuss 
the problem in the context of Cray Research s MPP Fortran, and conclude that a general solu­tion requires 
unacceptable address computation overhead in inner loops. They therefore restrict block sizes and number 
of processors to powers of two, altowing tbe use of bit manipulation in address computation. Experimental 
implementation of our solution shows that address computation and rnterprocessor canrmmication csn be 
performed efficiently without these restrictions. The paper is organized as follows. Section 2 solves 
the problem for a one-level mapping (a = 1, b = O). Section 3 reduces the problem for two-level mappings 
to combining the solutions from two subproblems involvrng one-level mappings. Section 4 extends the tla.ntework 
to handle multidimensional arrays, aud Section 5 extends it to handle generation of communication sets. 
Section 6 discusses performance results, and Section 7 presents conchtsions. One-level mappings We first 
consider the problem where a one-dimensional array A is aligned to a template with a = 1 and b = O. This 
is called a ondevel mapping. In this case, the array and the template are in one-to-one correspondence, 
aud we specify the problem by the pair (p, k) describing the distribution of the template across the 
processors. As shown in Figure l(a), we visualize the layout of the array in the processor memories as 
courses of blocks. lltree functions P, C, and 0 describe the location of array element A(i). P(i; p, 
k) is the processor holding A(i), C(+ p, k) is the course of blocks within this processor holdrng A(i), 
and 0(;; p; k) is the offset of A(i) witidm the course. The layout of array elements in the local processor 
memories is as shown m Figure l(b). These functions are defined as follows. P(i;p, k) = (i modpk) div 
k = (i div k) modp (1) C(i; p, k) = adivpk=(idivk)divp (2) O(i;p, k) = i mod k (3) Therefore, we have 
i =pk .C(i; p,k) + k. P(tp, k) +O(i; p,k). The folIowing function gives the local memory location (with 
respect to the base of the local array) where element A(i) is stored within processor P(i; p, k): M(i; 
p,k) = k . C(tP, k) + ~(i; p,k). (4) We are interested in the sequence of local memory locations that 
an individual processor, numbered m, accesses while performing its share of the computation on the regular 
section A(Z: h:s). We specify the sequence of accesses by its starting location and by the differences 
AM(iI, 22) = M(iz; p, k) M(il; p, k), where iI and iz are two successive indices of the part of the 
regular section mapped to processor m. As the examples in F@re 2 illustrate, the spacing AM is not constant. 
l he key insight is that (for a particular p, k, and stride s) the offset O(i I; p, k) of one element 
of the reguku section determines the offset O(iZ; p, k) of the next element of the regular section in 
the same processor, and also determines the spacing AM(il, iz). Thus we can represent the offset sequence 
and the AM sequence as a simple table indexed by offset. We visualize this table as the transition diagram 
of a tinite state machine (FSM) whose k states are the offsets. This FSM is particularly simple because 
it has no inpuL and each state has only one outgoing trausitim, in fact, it is just a union of state-disjoint 
cycles. Thus the sequence of AM s is periodic, with period at most k. The contents of the table, or the 
transitions of the FSM, depend on p, k, snds, but not on the section bounds 1 and h or the processor 
number m, The same FSM describes the offsets for every processor, though each processor sees at most 
one cycle of the FSM. The first memory location used by the section in processor m (and its offset which 
is the start state for that processor s view of the FSM) &#38;pends on 1 and m as well asp, k, ands. 
Followrng equation (4), we can write AM in terms of the dif­ferences m course and offset AM(iI, i2) = 
k .AC(il, i2) +AO(il, ZZ), (5) where the definitions of AC and AU are analogous to that of AM. Here are 
some examples. We assume that 1 = O and that h is large. Example 1 k = 4, p = 4,s = 3. The distribution 
is shown in Figure 2(a). The transition table T is as follows. For convenience, we also tabulate AC and 
AO. This shows a simple case where all processm access local memory with constant stride. 0 3 3 STATE 
NEXl AM AC AD 1 o3 3033 1 0 31 1 3 2 1 31 1 3 3 2.31 1 2 Q Example 2 k = 4, p = 3,s = 3. l he distribution 
is shown in Figure 2(b). The transition table T is as follows. This demonstrates that the FSM for a problem 
csn consist of disjoint cycles. Each processor sees at most one cycle of the FSM, depending on its start 
state. 0 3 4 ISTATEINEXT IAM lACIAO  C@81 EEIEE36 Example 3 k = 4,p = 3,s = 5. The distribution is shown 
in Figure 2(c). The transition table T is as follows. The AM sequence has period k, and local memory 
accesses vary in stride. I STA~ I NEXT ] AM IACIAC21 EEIEEl 2.1 The algorithm Preparatory to finding 
a processor s AM sequence, we solve the problem of finding its starting memory location. Given a processor 
number m, regular section A(l: h : s), and layout parameter p and k, we wish to find the ftrst element 
(if any) of the regular section that resides on that processor. This is equivalent to tiding the smallest 
nonnegative integer j such that 7(/ + sj; p, k) = m. Substitudng the definition of P from equation (l), 
we get ((1 + sj) modpk) div k = m. This reduces to km<(l+sj) modpk<k(m+l) 1 Local memory location o . 
. . k 1 k ... 2k-1 ... Course Processor O Processor 1 . . . Processor p -1 0 0 . . . k-1 k . . . 2k 
1 . . . (y -l)k . . . pk-1 (a) 1 ~k . . .  . . . @+l)k-l @ + l)k  @+2)k-1  ~2~ -l)k  2pk -1 Offset 
O . . . k~l o .:. k~l .:. 0 .:. k~l Array in&#38;x (processor O) o . . . k 1 pk . . . (p+l)k-1 . . . 
0) Array index (processor 1) k 2k 1 (p+ l)k . . . (p+2)k-1 . . . Array index (processor p 1) (p l)k 
::: pk-1 (2p l)k ... 2pk 1 ... Figure 1: A one-level mapping. (a) Layout parameters. Processors run 
across the page, while memory is organized as a two-dimensional array of courses aud offsets. Each box 
represents a memory cell, aud the number m the cell indicates the array element it holds. (b) Layout 
of array elements in local processor memories. Each processor s memory is one row of the diagrsm. Proc. 
o ~12~1145~7 1121 13 14 II 11511 116 Proc. 1 17 1181 19 118~ 1120 Proc. 1211 2 10 22 11 23 \ 101 Proc. 
1 o 2 3 Proc. 1 4I5I67II89I1OI Proc. 2 11 (c) z 24 13 [ 14 26 [ 27 36 37 38 39 E 48 49 ~ 51 52 Figure 
2 One-level array mappings. The corresponding state k=4, p=3, s=3. (c)k= 4,p=3, s =5. which is equivalent 
to Iinding au integer q such that km l<sj pkq~km l?+k-1. (6) We rewrite inequalky (6) as a set of k 
linear Diophantine equa­tions in the variables j and q, {sj-pkq =A:km l <A<km-l+k-1}. (7) l he equations 
in this set are to be solved independently (rather thsn simultaneously). Solutions exist for tin individual 
equation if and only if A is divisible by GCD (s, pk). The general solution to the liiear Diophantine 
equation sj pkq = A is found as follows. Let d = GCD (s, pk) = crs + /3pk, with a, /3 E ~. The quantities 
a, j?, and d are determined by the extended Euclid algorithm [6, page 325]. Then the general solution 
of the equation is the one-psmmeter family j = (X-Y+ pky)/d and q = ( M + s-i)/d, for 7 C Z. For each 
solvable equation inset (7), we iindthe solution having the smallest nonnegative j. The minimum of these 
solutions gives us tie starting index value I + js, which we then convert to the starting memory location 
usiug equation (4). We compute the ending memory location similarly, by finding the largest solutions 
no greater than h, and tdcing the maximum among these solutions. 29 ~ 31 32 33 34 ~ 41 42 43 44 [ 46 
47 53 54 ~ 56 57 58 59 tables sre shown in Examples 1-3. (a) k = 4, p = 4,s = 3. (b) Now we extend this 
idea to build the AM sequence for the processor. A sorted version of the set of smallest positive solutions 
determined above gives us the sequence of indices that will be successively accessed by the processor. 
A linear scan th~otrgb this sequence is sufficient to generate the AM sequence for the cycle of the FSM 
visible to processor m. If the FSM has multiple cycles, the local AM sequences maybe different for different 
processors; otherwise, the sequences are cyclic shifts of one snorher. To avoid following the NEXT pointers 
at mntime, the AM table we compute contains the memory spacings in the order seen by the processor. All 
of these elements are combined in Algorithm 1, which pro­vides the solution for the memory address problem 
for a one-level mapping. The mtmingtime of Algorithm 1 is O(logmin(s, pk)) + O(k log k), which reduces 
to O(min(k log k + logs, k log k + logp)). As the output can be of size k in the worst case, any algorithm 
for this problem takes Q(k) time. Finding an algorithm with O(k) running time remains an open problem. 
Our implemen­tation recognizes the following special cases that can be handled more quickly p = 1, s 
divides k, k = 1, pk divides s, a single course of blocks, ands divides yk. Algorithm 1 can be simplified 
by noting that the rigbthsnd sides of successive solvable equations differ by d, and that the identity 
of the first solvable equation is easily determined from the value of (km -1) mod d. This removes the 
conditional from the loop in lines 5-12. Similar incremented computations can also be used to simplify 
the floor and ceiling computations m that loop. lltenode code for each processor consists of the fo~owing 
loop. base + startrnem; i ~ o while (base < lsstmem) do compute {ith address base base ~ base+ AM[iJ 
 i -(i+ 1) mod length enddo Algorithm 1 (Memory access sequence for a one-level mapping.) Input: Layout 
parameters (p, k), regular section A(I: h: s), processor number m. Output: fhe AM table, the length 
of the table, starting memory location, ending memory location, and ~ AM for processor m. Metkod: 1 integer 
AM[kJ Iengtb 2 integer A, i, star~ end, d, CY,/3, lcourse, loffset, course, offset 3 integer loc[k+l], 
sorted[k+l] 4 (d a, ~) -EXTENDED-EUCLID(S, pk); length +-O; st@-h+l; end+l l 5 for A=km 1tokm l+k 1 do 
6 if Amodd=O then /* Solvable equation*/ 7 Ioc[lengtb] + 1 + ;(AcY + pk [*1 ) 8 start + min(start, loc[kmgth]) 
9 end + max(end, I + ~(kY + pkl @-%; sl)) 10 length + length+ 1 11 endif 12 enddo 13 ifkngth -Oor start 
> b then 14 return AM, L, 1, 1, 1/* No work for processor */ 15 endif 16 sorted ~ SORT (lot, length) 
17 sorted[lengthl t sorted[Ol +pks/d; lcourse + C(sorted[O]; p, k); loffset + @(sorted[O]; p, k) 18 fori-Otolength-fdo 
19 course t C (sorted[i + 1]; p, k); offset i-0 (sorted[i + 1]; p, k) 20 AM[i] +-k.(course lcourse) 
+ (offset loffset) 21 lcourse + course; Ioffset + offset 22 enddo 23 return AM, length, M (start; p, 
k), M(end; p, k), sk/GCD(s, @) We illustrate the actions of Algorithm 1 on Example 3 for pro­cessor 
O. Here, p = 3, k = 4, ands = 5. The extended Euclid algorithm returns d = 1, a = 5, and /3 = 2. Since 
each linear Diophaudne equation is solvable, the loop in lines 5 12 is executed four times. At the end 
of this loop, Ioc -[0, 25,50, 15], length -4, start -0, and end -50. After line 17, sorted -[0, 15, 25, 
50, 60]. At the end of the loop in lines 18 22, AM -[7,2,9, 2]. 3 Two-level mappings Two-level mappings 
are solved by two applications of Algorithm 1. As an example, let A be an array with element A(i) aligned 
with template cell 3i, let the template be mapped with p = k = 4, and let the regular section be A(O: 
42: 3), which we abbreviate to B. The picture is shown in Figure 3(a). The boxes represent template cells, 
the number j indicates the location of A(j), sndnumbers in squares are part of the regular section A(O: 
42: 3). Two-level mappings encompass all mappings pssible with the linguistic constructs of nested sections, 
aflirte alignment and cyclic(k) distribution. Unlike the one-level case, all template cells do not contain 
array elements. However, we allocate local storage only for the occupied cells of the template. Thus, 
the local memory storage for A in the example above is as shown in Figure 3(b). We handle tlds complicatim 
by tirst doing all our computations with respect to the template (the bcal tensplare space), and then 
switching back to the local memory space in the 6M1 step. The picture in local template space is shown 
m Figure 3(c). Note that A and B are both aligned to regular sections of the template A(i) is aligned 
to template cell 3i, and l?(i) is aligned to template cell 92. We therefore create state tables for A 
and B with respect to the local template space using Algorithm 1. The AM entries returned by the algorithm 
are actually spacings in 9 locrd template space we therefore rename this column AT. This givea the tables 
for A and 1? shown below. n 0 3 3 31 TA 3 3 1 0 Now all we need is to switch back to local memory space 
and fill in the AM entries. As we traverse tbe state table TA for any processor m, we access each element 
of A stored on that processor, in order. But srnce storage is allocated based on A, this translates to 
accessing consecutive locations in local memory. Thus the AM entries for TA are all unity. Now it is 
a simple matter to find the AM entries for the desired section B. For instauce, the tirst line of TB 
says that if we are at an element of the section that has offset O, the next element has offset 2 and 
is 6 template cells away. To fid the corresponding AM, we start in state O of TA, and walk the state 
table, accumulating AT and AM counts. We stop when we have accumulated the desired AT count. (It is easy 
to show that at this point we will have arrived at the correct offset as well.) The accumulated AM value 
is the desired entry for that line of TB. This procedure results in the following table for B. 1 [sTATEl 
NExTlfi7_ll AMl 01216112 1 3 6 2 2 1 15 5 3 0 9 3 The algorithm as presented above requires O(k) time 
to com­pute each AM entry of TB. Two optimization reduce this com­plexity to O(l), allowing the tinal 
fixup phase to run in O(k) time. In this case, AM turns out to be AT divided by the alignment stride 
a .3, but tAisisnottrue rngeneml. T~tbefollowingcase a-3, b-0,p-3, k-!3, s-3. 1 Processor O Processor 
1 Processor 2 processor 3 1o11 1 2 131 4 5 I 11116 7 8 191 10 11 112[ 13 14 115[ (a) 16 I I 17 ml18 19 
20 121/ 1 1221I I [1 231 I 1 124[ 25 26 t 1271] 28 I 29 130\ 31 32 I 133]11 34 1351 1361 I I 37 [ 38 
13911 I 40 I I 41 II 1421 I Localmemorylocation o 1 2 3 4 5 6 7 8 9 10 ProcessorO 0 1 6 11 16 1722 27 
32 33 38 Processor1 2 7 12 13 18 23 28 29 34 39 Processor2 38 91419242530354041 Processor3 4 5 10 15 
20 21 26 31 36 37 42 Localtemplatecell o12345 67 8 91011... Processor O 0 1 6 11 ... (c) Processor 1 
2 7 12 13 .-. Processor 2 3 8 9 14 ... Processor 3 4 5 10 15 ... Figure 3: A two-level mapping of an 
array A aligned to the template with stxide 3. (a) Layout of template cells. Numbers are array elements; 
boxed numbers are in the regular section A(O: 42: 3). (b) Local memory storage for array. (c) Local template 
space for array. I Processor (0. 0) Processor (0. 1) I (o,o) (o,1) (o, 4) (o, 5) (o, 2) (o, 3) I (O, 
6) ] (o, 7) (1, 1) (1,4) (1,5) (1,2) (1,3) (1,7) () () I(::)I (2, 1) (2, 4) (2,5) (2, 2) I (2,3) [ &#38; 
(2, 7) () ()()() () () () () I J;) I (: ,;) (:;:4) (:;:5) (1%) I (1%:3) I I (;;~6) I (;;V7) (11,0) (11, 
1) (11,4) (11,5) (11,2) (11, 3) (11,6) (11,7) Processor [1. 01 Process or (1,1) w, (3,0) (3, 1) I (3,4) 
(3,5) ()() ()() (4, o) (4, 1) (4, 4) (4,5) (i:) I(::)1 I(2:)I (t;) (5, 1) (5, 4) (5, 5) (5, 2) (5,3) 
(5, 6) (5,7) (:;:0) (12,1) (12, 4) (12,5) (12, 2) (12,3) I I (12,6) (12,7) (13,0) (13,1) (13, 4) (13,5) 
(13,2) (13,3) (13,6) (13,7) 1(14,0) I (14,1) (14,4) (14,5) (14,2) I (14,3) I 1(14,6) I (14,7) Processor 
[2. 0) Processor (2. 1) [ (6, O) (6, 1) (6, 4) (6,5) (6, 2) (6, 3) I (6,6) (6, 7) (7, 1) (7, 4) (7,5) 
(7, 2) (7,3) (7, 7) I (:::) I (8, 1) (8, 4) (8,5) (8, 2) I (8,3) I I (;::) 1 (8, 7) (15, o) (15, 1) 
(15, 4) (15,5) (15, 2) (15, 3) (15,6) (15,7) (16,0) (16, 1) (16, 4) (16,5) (16, 2) (16,3) I I (16,6) 
(16,7) (17,0) (17,1) (17, 4) (17,5) (17,2) (17,3) (17,6) (17,7) () () Figure 4 Mapping a two-dimensional 
array. Layout of array elements on processor arrangement with p = k = 3 in the vatical dimension and 
p = k = 2 in the horizontal dimension. The boxed pairs are A(O: 17:2,0:7: 3). Fwst, we compte the sum 
of the AT and the AM around the state machiue cycle, so that we cau cast out any full cycles that must 
be traversed. Second we tabulate the distance of each offset from a fixed origin (the numarcs table m 
Algorithm 2), allowrng the com­putation of the distance between any two offset positions by two table 
lookups and some simple arithmetic. Consider the third line of TB, which has a AT of 15. Using tbe precomputed 
sums, we determine in constant time that this corresponds to one full cycle plus 3 more template cells, 
and that the full cycle contributes a memory offset of 4. Now all we need is to determine the num­ber 
of memory cells encountered in gorng from offset position 2 to offset position 1 in TA. We tid this distance 
in constant time using the rtumarcs table. Algorithm 2 shows the method with these optimization incorporated. 
Algorithm 2 (Memory access sequence for a two-level mapping.) Input: Alignment parameters (a, b), layout 
parameters (p, k), array length n, regular section A(t: h: s), processor number m. Output: The AM table, 
length of the table, starting memory locatio~ ending memory location, aud the length of A in processor 
m. Method: 1 integer i, nun, rrumarcs[k] 2 integer AM[k], length, startmem, lastrnem 3 integer AM 1[k], 
lengtbl, startrneml, kwtmeml, memcyclel 4 integer AM2[k], kmgth2, startmem2, lastmem2, memcycle2 5 fuuction 
residual(x)= Iengthl -1 (x-startmeml)/memcyclel J 6 function major(x) = (numarcs[x mod k] numarcs[startmemi 
mod k]) mod lengthl 7 function reslrnem(x) -major(x)+ residual(x) 8 (AM 1, lengt.hl, startmeml, lastmeml, 
memcyclel) + Algorithm I@, a(n 1) + lJ, a,p, k, m) 9 (AM2, length2, sUrtruem2, last2nem2, memcycle2) 
 ~ Algon thm l(al + b, ah + b, as, p, k, m) 10 if kngthl -J_ or length2 -J_ then 11 return AM, J_, 1, 
1, O/* No work for processor */ 12 endif 13 mm + startmeml 14 fori-Otokngthl-ldo 15 mrmsrcs[mm mod k] 
~ i 16 mm -rnm+AMl[i) 17 enddo 18 mm + startmem2 19 fori=Oto length2-ldo 20 AM[iJ + realmem(inrn + AJ42[i]) 
 rehem(m) 21 mm + mm+AM2[i] 22 enddo 23 return AM, leugth2, reaLrnen@artrnem2), realmem(lastmem2), realrnem(lastrneml) 
 realmem(startmeml)+l Multidimensional arrays So far, we have characterized the access sequence of a 
single pro­cessor by a start address and a memorv stride Dattem. the latter being g;nerated by the transitions 
of a-FSM. F;r a multidimen­sional array, each dimension will be characterized by a (p, k) pair and have 
a FSM associated with i~ and each processor will execute a set of nested loops. Instead of the start 
address being fixed as it was before, it too will be generated by the transitions of a FSM. For simplicity, 
we will deal with a two-dimensional array. The exten­sion to higher dimensions is straightforward. An 
array dimension with p = 1 is mapped to memory . Consider a one-level mapprng of an May A(O: 17,0: 7) 
with parameters (pI, kl) -(3,3) in the tintdimension and (p2, k2) ­(2, 2) m the second dimension, and 
let tbe section of interest be A(O: 17:2,0:7:3). Thepicture isshownin FQure4. Let each block be laid 
out in cohmm-major order m local mem­ory. Thus, the sequence of in&#38;x values corresponding to successive 
memory elements of processor (0,0) is {(0,0), (1,0), (2,0), (9,0), (10,0), (11,0), (0,1), .: .}. Such 
a decomposition is used by Don­garra et al. for dense linear algebra codes on distriiuted-memory machines 
[3]. Then the tables corresponding to the two dimensions ofA are ISTATEINFXTI AM I  m, I I and STATE 
INEXTI AM I 2= EEEEI (Had the section been larger in the second dimension, the col­umn accessedby processor 
(O, O) immediately after column Owould be column 9. fhe elements (O, O) and (O, 9) would be 30 locations 
apart in local memory. fhis explains the AM entry in the Erstrow of T2.) TheFSM T2 is used to generate 
memory spacings for successive base addresses, while the FSM T1 is used to generate memory soaci.mm between 
successive elements with the same base address. k.lgori%m 3 computes these FSMS. Algorithm 3 (Memory 
access sequence for a multidimen­ sional array with a two-level mapping.) Input: Number of dimensions 
d; aligument parameters (aj, bj), layout parameters (pj, kj), dimension length n j, regular section A(li 
: hj : sj ), processor number mj, for each dimension j (1 < j < Q?). Ouput: AMj, Imgth j, st~tmemj, lastmemj 
(1 < j < d). Method: 1 integer j, mem, m, k 2 mem -1 3 for j­ 1 toddo 4 (AMj, lengthj, s~emj, Iastmemj, 
m) 5 ~ &#38;gorithm 2(aj, Startmtmlj + Startmanj bj, pj, kj, . mem nj, 1 h1, ~,sj,mj ) 6 Iastmemj * 
l~dllemj . mem 7 fork -Otokrgthj-ldo 8 AMj [k] -A.A4j[k] . mem 9 enddo 10 mem t mem. m 11 enddo 12 return 
AM j, lengthj, startmemj, la.stmemj (l<j<d) For the two-dimensional case, each processor executes the 
fol­lowing doubly-nested loop. Higher-dimensional cases follow the same pattern, with one loop for each 
dimension. bsstz + startmern2 &#38;to while ba.w < bstmerm do basel t startrneml i~+o while baseI ~ 
Jastrneml do compute with address bas~ + basel basal t basel +AM l[il] il -(il + l)mod krtgtlu enddo 
b-e bax% +AM2[i23 i2 + (fi + I)mod Jength2 enddo 5 Generating communication sets The set of messages 
that a given processor m must send while per­forming its sham of the com@ation is called its conrmunicatwn 
set. We now extend the FSM approach to generate communication sets for regular communication patterns 
such as shhls and stride changes. In this section, we assume a one-dimensional array with a srngle-level 
mapping. Extensions to multilevel and multidiruen­sioml cases areas in the address generation problem. 
We adopt Koelbel s execution model for parallel loops [7], in which a processor goes through four steps: 
1. Send those data items it holds that are required by other processors. 2. Perform local iterations, 
i.e., iterations for which it holds alI data. 3. Receive &#38;ts items from other processors. 4. Perform 
nonlocal iterations.  We combme the Enal two steps into a receive-execute loop. Our approach puts all 
the intelligence on the sending side, keep­ing the receive-execute loop simple. We wish to minimiie the 
number of messages sent and maximize cache benefits. To this end, each processor makes a single pass 
over the data in its local memory using the FSM technique &#38;scribed above, figuring out the desti­ 
nation of each data element and building messages. The messages contain (address, value) pairs which 
the receiving processor uses to perform its computations. All the data that must go from processor i 
to processor j are communicated in a single message. The data iterns corresponding to the local iterations 
are conceptually sent by the processor to itself. Of course, this message is never sent rather, the local 
iterations are performed out of the message buffer. The idea is similar to the rnspector-executor model 
[10] for irregular loops. 5.1 Shifts Consider the computation A(O:h:s) =A(O:h: s)+ A(l:l+h:s), where 
the regular section A(.t: Z + h:s) is to be moved to align with A(O: h:s). (For simplicity, assume 1 
> O.) In this case, a processor communicates with at most two processors. We show how to augment the 
FSM with a A P and a A.C column, such that adding these quantities to the processor and memory location 
of the current element gives the remote processor number and the memoty location in the remote processor 
of the element with which it interacts. Suppose we know that element A(i) is located on processor m at 
memory location M(i; p, k) with offset Oi = O(i; p, k). Then tbe problem is to find the processor and 
local memory location of theelementA(i-E). Letl = q ,pk+~, audAP = ((r-Oi)/kl. Thm P(i-gp, k)=(m-AP+p)modp. 
Sh.tce O<Oi <k, AP can assume only two values as the offset Oi varies. We also define the quantity AC 
such that M(i -&#38;p, k) = M(i; P, k) + AL(Oi). AL(Oi) = ((Oi -r)modk) Oi kq v, k if(mk r+O:) <O, =Q 
O otherwise. { 5.2 Stride changes Now consider the communication required to move the regular sec­tion 
A(O: s2n: S2) to align with the regular section A(O: Sln: Sl). This is harder than a shift there is no 
simple lookup technique for generating the communication sets. The problem is that the pattern of destination 
processors can have period longer than k. The only fact that appears to be true in this case is that 
the sum of tbe periods over all processors divides pk. We therefote resort to a simple technique. We 
generate the FSM to reflect the storage properties of the s2­ th element of this section interacts with 
index section. Now, the j i=j. slof A. Letql=i divpk, rl=imod pk, qz=rldivk, and T2 = T I mod k. Then 
the &#38;sired element is stored in memory location ql . k + T2 on processor qz. Many modem RISC microprocessors 
perform integer division and remainder operations in software, which are rather slow. The equations above 
can be rewritten to require only two integer divi­sions, which substantially speeds up the implementation. 
If either pork is a power of two, the computation can be sped up further by using shifts and bit masks 
instead of the division operations. Our implementation incorporates these optirnizations.  5.3 Termination 
Each processor must determine when it has completed all its opera­tions. As we do not send messages between 
every pair of processon, we cannot use the number of messages to determine termination. However, it is 
easy to figure out the total number of operations that processor m will execute (by a variant of Algorithm 
1). A simple termination test initializes a counter to this value, decrements it by the number of amay 
elements received in each message, and exits when the counter reaches zero. 6 Experimental results In 
this swtion we present experimental results of implementing the algorithms above. We measure botb the 
preprocessing cost of table generation and the runtirue loop overhead. Our computational kernel is the 
DAXPY operation y(l:n:s) = y(l:n:s) + a*x(l:n:s). The ratio of memory operations to computation is high 
in this kernel, making it unfavorable for our algorithms. The overhead due to irregular memory access 
patterns would decrease with more computation in the loop. AU experiments reported in this section were 
done on a Sparcstation 2, for which Dongarra [2] reports a 100 x 100 double-precision LINPACK performance 
of 4.0 Mflops. (We measured 3.3 Mflops.) (A repetition of the experiments on a single processor of an 
iPSC/860 showed simiisr characteristics.) We used the GNU C compiler, with optimization at the -02 level. 
 a=l D=32 3 1 I I k=17 $ :, :, : k=64 k=l ----­----­ k=block ---------­ 2.5 .,., :; ;? 2 Ii w 1.5 (eb-l 
al E 1 0.5 0 16 32 48 64 stride s Figure 5: Performance of DAXPY for various block sizes k. A higher 
optimization level increased compilation time without improving the quality of the generated code. With 
s = 1 the loop compiled into 9 instructions, while the general case with table lookup compiled into 17 
instructions. Our measured performance for thes = 1 case was 2.0 IVftlops for long vectors. 6.1 Local 
address generation Wc have seven parameters for a one-dimensional array: two align­ment parameters a 
and b, two layout parameters p and k, and three section parameters 4, h, ands. This is too large a space 
over which to present results. We now explain how we reduce the number of variables. c Alignment stride 
a: For any constant c, the memory layout for parameters ca and ck is exactly the same as that for a and 
k. Thus we experiment only with a = 1; the effect of changing a can be simulated by changing k instead. 
 Al@rnent offset 11: lltis is irrelevant for large sections. For small sections where end effects could 
be importaut, we observed that changes in b affected results by less than 5%. We therefore take b = O. 
 Section lower bound f: Again, this is irrelevant for large sections and has negligible effect even 
for small sections. We therefore take t = O.  Section upper hound h: This affects the number of elements 
in the DAXPY, which in turn affects the tradeoffbetween loop startup and iteration time in a single processor. 
We vary h along witi s, keeping h/s fixed, so that all our experiments do tbe same amount of work regardless 
of stride.  Section strides: This is the z-axis of our plots, as one of the two independent variables. 
 Number of processors p: We run our experiments with a fixed number of processors. Varying the n~ber 
of proces­sors has the effect of scaling the plots.  c Block size k: This varies from plot to plo~ as 
the second of the two independent variables. We use the execution time per element of the DAXPY loop 
as our measure of performance. For a given set of parameters, we compute the tables for each processor 
and time the execution of the distributed loop. Let r~ be the time taken by processor m to execute the 
TZmiterations it receives. Then the execution time per element is t. =max~. = nn This factors out the 
effect of load imbalance, We plot 2/t. (the megaflops per second per processor) against stide s for various 
block sizes k. The plots are shown in Figure 5. The curves are the sum of two effects. .-1 .-7 .-32 8000 
 A *, #&#38; #$ 7000 * *W* * 6000 ~~.d .* *++ .**** 5000 a. * :+ *: :* 4000 -* *.++ * !K 16 4864 
bl..;2. ize k Figure 6 preprocessing overhead for memory stride computations. The graph corresponds 
to an alignment stride of 1. The multiple points for each block size show the processing times for each 
of 32 processors. 1. Baseline The lower envelo~ is a smooth, relatively flat function ofs. The value 
of the envelope is approximately the asymptotic performance for the single-processor DAXPY. 2. Jiggles: 
The plot jiggles up and down some, presumably due to cache stride effects [1].  Although the general 
loop has ahnost twice as many instructions as the unit-stride loop, the dtiference in performance is 
much less. This is because the execution time is dorninatedbythe floadng-pornt operations and references 
to operands in memory. The references to the AM array usually hit in cache, and the additional loop control 
instructions probably execute in overlapped fashion. The preprocessing time required to compute the AM 
tables is shown in Figure 6, where we plot the running time of Algorithm 1 against block size k. The 
running time is approximately O(k), with certain special cases being handled much faster. Experimental 
observation confirmed that the running times for Algorithm 2 were no more than twice those of Algorithm 
1. If the AM sequence is known at compile time, unrolling tbe loop bodies avoids the indirection into 
the AM array. Figure 7 shows that the benefits of this are mixed. While loop unrolling removes the indirection, 
it also increases the size of the loop body, thus the benefits of a simpler loop body maybe undone by 
misses in the insbuction cache caused by the larger loop body. This is a factor for larger block sizes, 
since the amount of unrolliig may be as large as k. We recommend unrolling only if the AM sequences are 
short, or can be compacted, e.g., by run-length encoding. 6.2 Communication set generation For the communication 
set generation algorithms, we measured the time required to generate the FSM tables and tie time required 
to marshal the array elements into messages. We dld not measure the actual communication time, since 
that varies wi&#38;ly among machines. The results are presented in Table 2. 7 Conclusions Our table-driven 
approach to the generation of local memory ad­dresses and communication sets unifies and subsumes previous 
so­lutions to these problems. The tables required in our method are k s e J FSM gen. time (PS) Run timelelt 
(#s) 1 3 10 360 3.3 173 10 1831 3.1 (a) 17 3 40 1870 3.1 643 10 6632 3.2 64 3 100 6508 3.2 blk 3 10 
408 3.2 k .91 S2 FSM gen. time (ps) Run tirdelt (KS) 1 3 5 360 5.3 (b) 17 3 5 1891 14.3 64 3 5 6542 
5.4 blk 3 5 400 11.3 Table 2 Performance of communication set generation algorithms. All numbers are 
for 32 processors. (a) Shift commttnicatiorx the shift dktance is 1. (b) Change of stride the stride 
is changed from Sztosl. easy to compute, and the runtime overheads due to indirect address­ing of data 
are small. Our data access patterns exploit temporal locality and make effective use of cache. We also 
support blocking of messages to reduce message startup overheads. Our prototype implementation has demonstrated 
our perfor­mance claims. An optimized implementation of our techniques should deliver close to maximum 
performance for general cyclic(k) distributions of arrays. Our algorithms are fully parallel in that 
each processor of a parallel machine can generate its tables independently. Finding a fully parallel 
algorithm running in O(k) parallel time or with O(p + k) total work remains an open problem. Acknowledgments 
J. Ratnarnt@n proofread an early draft and suggested the simplifi­cations to Algorithm 1. We also thank 
the referees for their helpful comments. References [1] BAILEY, D. H. Unfavorable strides in cache memory 
sys­tems. RNR Technical Report RNR-92-015, Numerical Aero­dynamic Simulation Systems Division, NASA Ames 
Re­search Center, Moffett Field, CA, May 1992. [2] DONGARRA, J. J. Performance of various computers using 
standard linear equations software. Computer Architecture News 20,3 (June 1992), 22-44. [3] DONGARRA. 
J. J.. VAN DE GEIJN, R., AND WALKER, D. W. A look at scalable dense linear algebra libraries. In Proceed­ings 
of the Scalable High Perfonrrance Computing Confer­ence (Wiiiarnsburg, VA, Apr. 1992), IEEE Computer 
Soci­ety Press, pp. 372 379. Also available as technical report ORNIJTM-12126 from Oak Ridge National 
Laboratory. [4] Fox, G. C., HIRANANDANI, S., KENNEDY, K., KOELBEL, C., KREMER, U., TSENG, C.-W., AND 
Wu, M.-Y. Fortran D lan­guage specification. Tech. Rep. Rice COMP TR90-141, De­partment of Computer Science, 
Rice University, Houston, TX, Dec. 1990. 3 2.5 I a=l D=32 .- 1 1 k=17 (unrolled) k=17 (table) k=64 (unrolled) 
k=64 (table) ----­----­----­ 2 ? Q s w Id m a G 1.5 1 0.5 0 I 16 I 32 stride s I 48 64 Figure 7 Effect 
of loop unrolling on perfomxmce of DAXPY for various block sizes k. [5] HIGH PERFORMANCE FORTRAN FORUM. 
High Performance Fortrrm language specidcation version 0.4. Drafi Nov. 1992. Also available as technical 
report CRPC-TR 92225, Center for Research on Parallel Computation, Rice University. [6] [7] [8] KNTJTH, 
D. E. Seminumerical Algorithms, second cd., vol. 2 of The Art of Computer Programming. Addison-Wesley 
Pub­lishing Company, Reading, MA, 1981. KOELBEL, C. Compile-time generation of regular communi­cation 
patterns. In Proceedings of Supercomputing 91 (Albu­querque, NM, Nov. 1991), pp. 101 1 10. KOELBEL, C., 
AND MEHROTRA, P. Compiling global name­space parallel loops for distributed execution. IEEE Tramac­tions 
on Parallel and Distributed Systems 2, 4 (Oct. 1991), 440-451. [9] [10] MACDONALD, T., PASE, D., AND 
MELIZER, A. Addressing in Cray Research s MPP Fortran. In Proceedings of the Third Workshop on CompikrsforParalkl 
Computers (Vienna, Aus­tria, July 1992), Austrian Center for Parallel Computation, pp. 161 172. MIRCHANDANEY 
R., SAL EL J. H., s= R. M., CRow-LEY, K., AND NICOL, D. M. Principles of mntime support for parallel 
processors. Jn i%ird Internatwnal Conference on Supercomputing (July 1988), ACM Press, pp. 140-152, 158 
  
			