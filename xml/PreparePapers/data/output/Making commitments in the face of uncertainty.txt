
 Making Commitments in the Face of Uncertainty: How to Pick a Winner Almost Every Time (Extended Abstract) 
 Baruch Awerbuch* Yossi Azart Amos I?iat$ Torn Leighton~ Abstract In this paper, we formulate and provide 
optimal solu­ tions for abroad class of problems in which a decision­ maker is required to select from 
among numerous com­ peting options. The goal of the decision-maker is to select the option that will 
have the best future perfor­ mance. This task is made difficult by the constraint that the decision-maker 
has no way to predict the future performance of any of the options. Somewhat surpris­ ingly, we find 
that the decision-maker can still (at least in several important scenarios) pick a winner with high probability 
y. Our result has several applications. For example, consider the problem of scheduhng background jobs 
on a network of workstations (NOW) when very little is known about the future speed or availability of 
each workstation. In this problem, the goal is to schedule each job on a workstation which will have 
enough idle capacity to complete the j ob within a reasonable or spec­ified amount of time. This task 
is complicated by the fact that any particular workstation might become sat­urated by higher priority 
jobs shortly after one of our jobs is assigned to it, in which case progress will not Johns Hopkins University 
and Lab. for Computer Science, MIT Supported by Air Force Contract TNDGAFOSR-86-O078, ARO contract DAAL03-86-K-0171, 
NSF contract 9114440-CCR, DARPA contract NOOO14-J-92-1799, and a special grant from IBM. E-Mail: baruch@theory.lcs. 
mit,edu. tDepartment of Computer Science, Tel Aviv University. E-Mad azar~math. tau.ac.il. Research supported 
in part by AlIon Fellowship and by the Israel Science Foundation administered by the Israel Academy of 
Sciences tDepartment of Computer Science, Tel-Aviv University, Is­ rael E-Mail fiat@?math. tau ac 11. 
Research supported m part by the Israel Sc]ence Foundation admimstered by the Israel Academy of Sciences. 
SMathematics Department and Lab for COmPuter SCI­ ence. MIT E-Mad: ftl@math.mit.edu Research SUDDOrted 
in part by ARPA Contract NOO014-95-1-1246 and Arm~ Contract DAAH04-95-1-0607. Permissiontomakedlgkal/hnrdcopiesofallorpartoftils 
materialfor peraonelor classroomuseisgrantedwhhoutfeeprovidedthalthecopies are not made or distributed 
for profit or commercial advantage, the copy­right notice, the title of the publication and its date 
appear, and notice is given that copyright is by permission of the ACM, Inc. To copy otherwise, tn republish, 
to post on sewers or to redktribute to tists, requires specific permission andlor fee. STOC 96, Phdadelphia 
PA,USA 0 199fj ACM @sgTgl-78$S/9fj/OS ..$.3.50 be made on our job. Thus, in order to complete the jobs 
within a specified amount of time, we need to be able to accurately guess (or predict) which workstations 
will be idle and when. Somewhat surprisingly, it is pos­ sible to make such guesses with a very high 
degree of accuracy, even though very little is assumed about the future availability y of the workstations. 
For example, if at least k of n workstations will be available for at least D units of time each (spread 
out over some interval of 1 units of time), then with probability at least 1 0(1/n), we will be able 
to complete k log n jobs with duration !2(D/ log n) within the interval. The result holds for all k, 
d, n, and 1, and only knowledge of n is needed in order to schedule the tasks. For small values of k, 
the result is far superior to the (seemingly optimal) dart­throwing approach in which each job is assigned 
to a random workstation in the hope that it will be idle. Our results can also be used to provide the 
first com­petitive algorithm for the video-on-demand scheduling problem as well as the more general on-line 
set cover problem. The results may also be of int crest in the context of investment planning, strategic 
planning, and other areas where it is important to be able to predict the future moves of an adversary 
or a market. 1 Introduction In this paper, we consider a class of optimization prob­lems in which there 
is a decision-maker who is required to choose from among numerous competing options. The goal of the 
decision-maker is to select the option that will have the best future performance. The task is com­plicated 
by the fact that the decision-maker only has information about the pastperformance of each option, and 
little or no information about future performance can be assumed or inferred based on the past perfor­mances. 
Nevertheless, we will show how to make opt i­mal or near-optimal selections in a variety of settings. 
Decision-making problems arise in numerous appli­cations. For concreteness, we will focus most of our 
attention on one such application in this paper. In par­ we will focus on the problem of scheduling back­tic~ar, 
ground or low-priority jobs on a network of workstations (NOW) when very little information is known 
about the future speed or availability of each of the workstations. We will then mention how the results 
can be generalized 1.1 Scheduling Jobs on Networks of Worksta­tions Consider a company in which there 
is a network of n workstations and where there is a hierarchy of priorities for the various users of 
each workstation. For exam­ple, the owner of a workstation might have top prior­ity for his/her workstation, 
the president of the com­pany might have second priority for all workstations, and so forth. Sometimes, 
an employee (call him Bob) might have more work than can be handled by his own workstation and so he 
will schedule some of his jobs on other workstations in a background or low-priority mode. When those 
workstations are not busy working on higher-priority tasks (say those of their owners), they will work 
on Bob s background job. Such job scheduling on NOWS has become very com­mon, In fact, the unused capacity 
on NOWS has be­come increasingly viewed as a major computational re­source within companies. Accordingly, 
it is often desired that this resource be used (and used efficiently) before funds are allocated to purchase 
additional equipment. In this paper, we describe efficient algorithms for scheduling background jobs 
on NOWS. To begin, we focus on the problem faced by a single user who needs to schedule a set of jobs 
(perhaps with precedence con­straints) on the workstations so that all of the jobs can be accomdished 
within some interval of time. In order to be effic;ent, the user is restricted to assign each job to 
a single workstation. (E.g., the user is not allowed to make n copies of a job and then simultaneously 
assign one copy to each workstation. ) If the user is unsatis­fied with the progress of a workstation 
on a job, he may elect to kill the job and start over on another worksta­tion, however. Of course, the 
user will be reluctant to make frequent switches because of the effort lost when the job is killed and 
the overhead incurred when the job is moved elsewhere. The goal of the user is to assign the jobs to 
worksta­tions that will be able to comdete them auicldv. The difficult y is that the user does nat know 
dead ;f time which workstations will be able to finish a job quickly. The user can monitor the NOW to 
see which worksta­tions are currently available, and which were available at various times in the past, 
but we da not assume that there is any connection (probabilistic or otherwise) be­tween the past ar present 
and the future. (E.g., the fact that the workstation was available for the last T seconds does not imply 
anything about the praspect that it will be available in the next second.)2 For simplicity, we will partitian 
time inta units of equal length called steps, and we will assume that every workstation is either available 
or unavailable for each 1 Such a constraint may also be needed to insure coherency in cases where the 
job makes use of external data or where the job could have side effects. In some scenarios, it might 
be remonable to assume that such a dependency does exist (I.e., that workstations that were idle in the 
past are more likely to be idle m the future), but we will show that euch assumptions are not necessary 
in order to ob­tain good performance, Hence, we will do without them, thereby obtaining algorithms with 
stronger guarantees of performance. step. We will also assume that all workstations are equally fast 
and that there is no penalty incurred when a workstation swaps back and forth between a low-priority 
job and a higher-priority job. Hence, a workstation W will complete a d-step background job in the dth 
step that W was available after the job was assigned to W.3 We also assume that an adversary decides 
which work­stations will be available at each step and that this ad­ versary is aware of our scheduling 
algorithm, but not the results of our random coin tosses. We first consider the scenario when Bob wants 
to schedule a single background job of duration d on the NOW. Bob may schedule the jab on any machine 
that he chooses and he is assumed to have full knowledge of the past availability of every machine. After 
scheduling the job, Bob may later kill the job and restart elsewhere if he is not satisfied with the 
performance of his first choice. The number of times that Bob must restart his job is samething that 
we would like to minimize (in addition ta the total time needed to get the job dane). ln order for Bob 
to get his job done in some interval af 1 steps, it is necessary that at least one workstation be available 
far d steps within the interval. Unfortunately, this condition is not sufficient ta guarantee that Bob 
can get his job done since Bob does not know until it is toa late which workstation(s) will be available 
far d steps. Even if many (say m) af the n workstations will be available for d steps, the probability 
that Bob will be able to pick one of the m good workstations can be limited to m/n by the adversary. 
(In this case, the adversary need only make n m workstations available for fewer than d steps and m 
workstations available for exactly d steps. The m good workstations can be chosen randomly. Bob s best 
strategy is ta schedule the jab on a random workstation at the beginning of the interval and then to 
hope for the best. It will not help Bob to make any changes in this case.) The preceding example is very 
discouraging since it demonstrates that Bob will not be likely ta get his job done even though there 
may be a great deal af unused capacity in the NOW. Even worse, the adversary need not be malicious since 
even a randam adversary is suf­ficient to thwart Bob. In the paper, we show that if the adversary is 
re­ stricted slightly, however, then Bob can find a goad workstation with very high probability. In particular, 
if at least one workstation will be available for ad log n steps (where a = ~(l)), then Bob will be able 
to get his d-step job done with probability 1 0(1/n). In fact, even if Bob has log n cl-step jobs which 
must be done in sequence, he will still be able to get them all done with probability 1 0( I/n). Moreover, 
in the latter case, the number af times that Bab will need to kill a job and switch workstations will 
be very small compared to the number of times that Bob picks a good workstation for one of his jobs. 
In other words, Bob will almost always be able to pick a good workstation when he 3Most of these assumptions 
can be relaxed without changing the results that we prove, although the analysis becomes more difficult. 
For example, our methods can also be apphed to see. narios where machines run at different speeds and 
where there is a penalty for swapping between jobs. In fact, the methods even work for a model where 
Job swapping is not allowed (e.g., if a job is swapped out, it is killed). goes to schedule a job. Viewed 
in another way, our re­sult shows that if Bob partitions his work into jobs of size O(D/ log n), where 
D is the amount of time that the most idle workstation will be available, then he will almost cert airily 
be able to get @(D) work done. More­over, if Bob is allowed to schedule 0(1) jobs at once, then he will 
be able to get (1 e) D work done for any constant c >0 with high probability y.4 The results can be 
extended to a scenario where there are multiple users with varying priorities. In this case, if the ith 
most available machine is available for D, steps, then for all i, the user with the ith prior­ity can 
be assured (with high probability) of getting at least (1 e)~, work done. Knowledge of the D, s is not 
needed in order to obtain these results and no coordi­nation is needed among the users. Our scheduling 
algorithm is quite simple and tends to schedule jobs on machines that have been available in the past 
(thus confirming that what may be considered a standard practice is, in fact, good practice). Although 
the past is not related to the future, we will show that such a strategy makes it difficult for the adversary 
to prevent any of the users from making progress while also making sure that i workstations are available 
for at least D, steps. (This is provided, of course, that the strategy is randomized and that the random 
choices are made in the right way. )  1.2 A General Problem Formulation The problem just described can 
be formulated in a more general context. For example, consider a problem in which there are n commodities. 
A decision-maker or trader (call her Alice) is allowed to hold at most one commodity at any time. At 
each step, each commodity may or may not issue a dividend or a return of, say, $1. Allice collects the 
dividend as profit if and only if she is holding the commodity at the time that the dividend is paid. 
Alice may or may not be allowed to change the commodity that she is holding and there may or may not 
be a steep penalty or cost every time that Alice makes a change. Alice s goal, of cpurse, is to maximize 
her profit while minimizing the number of changes that she makes. 5 4Although the constraint that some 
workstation be available for D = ad log n steps may seem strange at first, It is really a mdd assumption. 
For example, such a condition would be likely to arm if workstations were avadable with some constant 
prob­ability at each step. Such a condition may also be likely to arise in pract]ce since some workstations 
may be idle for long periods of time. By focusing on the D-step avadabi lit y constraint, we are merely 
identifying a salient feature that makes the problem tractable. The fact that we have identdied a salient 
feature is demonstrated by the matching lower bounds that are proved in the paper Finally, the constraint 
M useful in guiding the choice of job sizes. In particular, we find that by partitioning maxlmal­length 
Jobs mto El (log n ) chunks, we will be assured of scheduling them efficiently, even w]thout knowledge 
of which workstations wdl be available 5The connection between the trading problem and job scheduhng 
on NOWS M quite close The workstations correspond to commodities and dividends correspond to workstation 
avail­abdlty. The only ddference M that a Job M completed on a work­station only when a threshold is 
reached m terms of avadabdlty. Such a threshold can be modeled by a steep trading cost. Our methods can 
also be apphed to a scenario m which a Job 1s com­pleted iff a workstation is available for d comecuttue 
steps, in which case the dividends can be collected Iff they are Issued for Let D denote the number of 
dividends paid by the best commodity. At first glance, it seems as though Alice s best strategy is to 
select a random commodity, in which case her expected profit might be as small as @(D/n). In fact, Alice 
can do much better. For example, even if Alice is not allowed to make any trades (i.e., she is allowed 
to hold only one commodity) and D > log n, then our selection algorithm will obtain a profit of Q ( D 
/ log n) for Alice with probability y at least 0.99. (This is optimal.) By makhg O(log n) trades, Alice 
s profit can be increased to @(D) with probability 1 0(1/n). (This is also optimal.) If Alice is allowed 
to hold O(1) commodities at any step, then her profit can be increased to .99D with high probability. 
The value of D need not be known in advance, although then the number of trades may increase to O(log 
n log(D/ log n)) if D is not known.6 The results can also be extended to a scenario where there are multiple 
traders or to where Alice is allowed to own several commodities at the same time. In each case, we will 
show how Alice can almost always pick a winner every time. 1.3 On-line Set Cover and Related Applications 
 It turns out that our methods can also be used to give a @(log n log ~) competitive algorithm for the 
on-line set cover problem. 78 In the on-line set cover problem, we are given a family of n sets F = {SI, 
SZ, ....S~}. Elements v1, W, . . . . v~ arrive one at a time where each u, belongs to at least one of 
the sets. As each element arrives, the sets to which it belongs are revealed to the player. The goal 
of the player is to pick k sets so as to maximize the number of elements that are covered. In this version 
of the problem, the player can make selec­tions at any time, but cannot change a selection once it is 
made. In addition, the player only gets credit for elements that are contained in a set that was selected 
by the player before or during the step when the element arrived. Moreover, the player only gets credit 
for each element at most once, even if he/she has selected many sets that contain the element. In the 
case when the sets are constrained to be dis­joint, we have the special case of the commodity trading 
problem described in the previous section where Alice is allowed to own k commodities but cannot make 
any trades. Alice s profit is simply the number of elements covered. g d consecutive steps during which 
Alice holds that commodity 6 If D is not known and Ahce is not allowed to make trades, then her probabdity 
of obtaining a profit of f2(D/ logn) is decreased This bound is optimal for many values of n, m, and 
k. 8We use the convention that the competitive ratio IS always greater than one, even though we cons]der 
benefit problems rather than cost problems. Thus, an upper bound of c on the competitive ratio really 
means that the ratio between the on-hne benefit and the adversary benefit is at least I/c A lower bound 
of c on the competitive ratio means that the above ratio M at most I/c. Technically, Alice only gets 
credit for dividends pa]d on com­ modities that she owned m prevzous steps, in which case, we need to 
assume that the return of the off-line player is CJ(k log n) m order to guarantee that Alice will be 
O(log n log ~) competitive 1.3.1 Video-on-Demand Scheduling The disjoint version of the set cover problem 
is also equivalent to the video-on-demand scheduling prob­lem posed in [AGH94]. In this problem, customers 
issue requests for movies (which will start at a prearranged later time) to a video server with limited 
capacity k. Each customer demand is immediately accepted or re­jected. If the demand is accepted, that 
movie must be shown. If the demand is rejected, the customer is lost. The goal is to be able to accept 
the largest number of demands subject to the capacity constraint k. The on-line algorithms in [AGH94] 
achieve linear (O(m)) competitive ratio; the semi-offline algorithms with look-ahead (where the decision 
can be postponed to the time when future demand is known) achieve loga­rithmic ratio. In this case, the 
movies correspond to the sets, and the customers correspond to the base elements. The more general (non-disjoint) 
set cover problem cor­ responds to a setting in which every customer discloses a list of altern ative 
movie titles that he/she wants to watch. 1.3.2 Investment Planning The disjoint set cover problem also 
has applications in the context of investment planning. For example, previ­ous work on the competitive 
analysis of financial prob­lems [EFKT92, EK93, CEL93] focused on trading prob­lems, where algorithms 
were allowed to make partial in­vestments, to retract from previous decisions (at some cost), or were 
based on some statistical knowledge of the input. In contrast, on-line set cover captures sit­uations 
where investment decisions are indivisible and irrevocable: once an investor has decided to invest in 
building a new factory, he/she must hope that there will be high demand for the product produced by the 
fac­tory, in which case the dividends paid to investors will be high. In our model, investment decisions 
do not have to take place immediately; investors can postpone the decision until he/she get convinced 
that the company is doing well and indeed paying lofty dividends. Still there is risk involved since 
the future and the past are not necessarily related and once the investor decides to invest in a company, 
demand for its product could drop to zero, in which case the investor is stuck with the fac­tory building 
and a warehouse full of merchandise that nobody wants. Hence, with limited financial resources, the investor 
has to decide what investments to make and at what stage along the curve of increasing demand. The more 
general (non-disjoint ) set cover problem captures scenarios where there may be complex relation­ships 
between between one company s growth and an­other s (e.g., they are producing a competitive product or 
mutually exclusive lines of products). For example, the marginal revenues of investing in a factory produc­ing 
answering machines may be too low after purchase of a factory producing answering machines with a cordless 
phone. To analyze such problems, we view companies as sets of products, and arriving customers as base 
ele­ments. Upon their arrival, customers disclose mutually exclusive lists of products they would like 
to purchase. If, by that time, the investor has already has purchased a company producing any one of 
these products, the investor makes a profit. Otherwise, the customer (and the money) are lost, since 
the client may not wait. 1.3.3 Strategic Planning The methods developed in this paper may also be of 
interest in the domain of strategic planning and war gaming. For example, consider the following oversim­plified 
battlefield scenario where a general needs to de­cide where and when to attack an enemy defender. In 
the scenario, the general wants to find a single soft spot from among n enemy defensive positions. The 
enemy does not have the resources to defend all n positions, but he can shift his forces on a daily basis. 
The attack­ing general has access to intelligence data that reveals which enemy positions were well defended 
in all previous days, but this data does not provide the general with any information about which positions 
will be defended during the coming day(s). The general s problem is to pick a position p c [1, n] and 
a day q 6 [1, Tl for the attack. The general wins if site p is not well defended on day g, and he loses 
oth­erwise. The only knowledge possessed by the general about the future is that over the course of the 
next T days, at least one of the n positions will not be defended for at least @(log nT) consecutive 
days. 1° In this case, our results provide a strategy for the general that will result in victory with 
probability near one, no matter what strategy is used by the enemy. The high success probability holds 
even if the enemy knows the strategy being followed by the general (but not the result of the coin tosses 
being used by the general each morning when deciding whether or not to attack). In other words, it is 
not possible for the enemy to trick the general into at­tacking a well-defended position by, for example, 
leaving a position undefended for several consecutive days, only to switch and suddenly ramp up defenses 
in anticipation of an attack. Our results can also be applied to scenarios where multiple attacks are 
being planned and/or where attacks can be broken off quickly if a position is attacked that is well defended. 
In such cases, the general can achieve success against even less restricted enemies. The gen­eral s decision 
about where and when to attack does rely on random coin tosses each morning (which might be disconcerting 
to some) but it is the randomness in the coin tosses that insures that the general will be suc­cessful 
wit h high probabifit y. (For inst ante, wit bout the randomness, the general will have no chance of 
success against an enemy who has acquired the general s strat­ egy. ) 1.4 Additional Previous Work Various 
on-line optimization variants of many combina­torial problems have been subject to competitive anal­ 
10In some cases, the restriction that a position be left uncle. fended for El(log nT) consecutive days 
M unrealistic, Indeed, the enemy may decide to continually move his forces so that no site ]s left undefended 
for more than o(log nZ ) days at a t]me How­ever, continual movement of forces might be costly, and even­tually, 
the enemy may be forced to leave some sights less well defended for longer periods of t]me. If this ever 
happens, the general will win with high probabdity, ysis [ST85a, KMRS88]. Such problems include, for 
ex­ample, on-line matching [KVV90], partition [FKT89], on-line steiner tree and generalizations [1W91, 
CV92, ABF93, WY93], and also on-line graph coloring [Vish90, Irani90, HS92]. The subject of job scheduling 
in the face of uncer­tainty has also been studied previously [BL94, KP94, BCLR95], though our results 
differ from those of prior researchers in several respects. The closest prior work is that of Kalyanasundaram 
and Pruhs [KP94], who devise competitive algorithms for task scheduling of unrelated jobs on a NOW where 
processors may become faulty. In the case when most processors are faulty and replication is not allowed, 
the results are mostly negative. In con­trast, the positive nature of the results in this paper are possible 
because we consider a different adversary and objective function. Bhatt et al. [B CLR95], on the other 
hand, study a related cycle-stealing problem in which the zeal is to maximize the amount of zminterrwted. 
work that can be stolen from a single workstation. 1.5 Outline of the Paper The remainder of the paper 
is divided into sections as follows. In Section 2, we describe an optimal algorithm for picking a winner 
with probability close to 1 on the first try. We then show how this algorithm can be used to provide 
an optimal solution to the on-line set cover problem. In Section 3, we describe a more efficient algo­rithm 
for picking winners repeat edly if we are allowed to kill and reschedule jobs. Several extensions of 
the results are presented in Section 4. We conclude with some acknowledgements and references. 2 Picking 
a Winner on the First Try We begin by considering the scenario where Bob wants to run a single d-step 
job and he is only allowed to schedule the job once. (This corresponds to the scenario where Alice is 
allowed to own only one commodity and to the on-line set cover problem where we can select just one set. 
) We will assume that at least one of the n work­stat ions will be available for at least D > 3d log 
n steps. In what follows, we will show how to select a worksta­tion so that the job is completed with 
probability at least 1 O(w). The algorithm for selecting the workstation is quite simple. Label the 
workstations as WI, ) V2, . . . . Wn. At each step, Bob checks the status of each workstation to see 
which are available. For each i (i = 1,2, . . . . n in se­quence), if W, was available at the end of 
the last step, Bob flips a coin to decide whether or not to assign the job to W,. In particular, Bob 
will assign the job to W, SXID-Z / d, where x is the number of with probability n steps for which ~, 
has been available thus far. lf the job is ever assigned to some workstation, then Bob stops ilipping 
coins and just waits for the job to be completed. In what follows, we will show that with probability 
at least 1 O((cllog n)/D + l/rz), the job will be assigned to a workstation that will be available for 
d or more steps following the assignment, no matter what strat­ egy is employed by the adversary in determining 
when machines are available. Let S denote a sample space of coin tosses where there will be one coin 
toss for each pair (z, ~) for which Wi is available at step j. If W, is available for the xth time at 
step j, then the probability y of Heads for the (z, J flip is set to be n3xlD-2/d. There is a sample 
point in S for each possible com­bination of Heads and Tails for the flips. Each sam­ple point also corresponds 
to an outcome of the coins flipped by Bob to determine when and where to sched­ule hk job. In particular, 
Bob will schedule his job on W, at step j if the (i, j) coin is Heads and if the (z , j ) coin is Tails 
for all (i , j ) such that j < j or j = j and i <i. Let Swin denote the subspace of S consisting of sam­ple 
points for which Bob is successful in getting his job done. These are the sample points for which there 
is at least one Head among the flips and for which the first fiip to result in Heads occurs for a workstation 
that is available for at least d subsequent steps (i.e., for which there are at least d subsequent flips). 
The probability y that Bob is successful is then Pr[Swin]. In order to show that .? r[SWin] is close 
to 1, we will consider a second space S ~ S for which it is easy to show that Pr[S ] is large and that 
Pr[Swin] is nearly as large as % [S ]. In partictiar, we define S to con­ sist of the sample points for 
which there is at least one Head and for which the first d tlips for each workstation result ed in Tails. 
Lemma 2.1 Pr[S ] z 1-0(1/rz). Proof: The probability of getting a Head among the (at most) dn flips for 
which z s d is at most dn(n 3dl D-2/d) < 2/n. The probability that there are no Heads among the last 
d flips for the workstation that is available for D steps is either O or at most ) < (1-~)d ~ e-nj2, 
0 (1-n3(D-: D-2 Lemma 2.2 Pr[Swin] z (1 O(w)) Pr[S ]. Proofi We will construct an injection f : S + 
Swin for which Vs e s Pr[f(s )] > (1 o(W)) Pr[s ]. The lemma will then follow. Consider any sample point 
s c S . Let W, be the workstation for which a flip is first Heads in s and let x denote the number of 
steps that W, had been avail­able up to and includkg the step when the first Heads occurred. By the definition 
of S , we know that z > d. Let z = n3=JD-2/d ~3(x-d)/D 2/d z= ~ 3df Dz! = = (1 -El(W)) Z . If z > 1/2, 
then define ~(s ) to be the sample point which is identical to s in every way except that the out come 
of the (z d) t h flip for W, is changed from Tails to Heads. By definition, this sample point is in 
swin. Moreover, Pr[f(s )] z Pr[sI] = l z (1 e(%))z = 1 (1 e(Q&#38;))z L_@(*) >2 *+ @(* ) . l.@(*), 
since z ~ 1/2. If z ~ 1/2, then define f(s ) to be the sample point which is Identical to s except that 
the outcome of the (z d)th flip for W; is changed from Tails to Heads and the outcome of the zth flip 
for W, is changed from Heads to Tails. Once again, it is easy to see that f (s ) E Swin. It is also easy 
to check that f is an injection. Moreover, Pr[.f(s )] = z 1 2 l z P?-[s ]  z > (1 e(w)) 1 2 x 1 .2 
+ El(Q.&#38;z ) since z ~ 1/2. Q Theorem 2.3 Pr[Swzn] ~ 1 O(w + ~). Proofi Follows immediately from 
Lemmaa 2.1 and 2.2. Remark 2.4 By being more careful with the asymp­totic analysis and adjusting the 
probability of the iiips slightly, it is possible to make Pr[Swin] > 1 % ~. Remark 2,5 The probability 
bound of Theorem 2.3 cannot be improved by more than a constant factor, no matter what algorithm is used 
to select a worksta­tion. This is because the adversary can select pJn of the workstations at random 
to be available for the first dj steps for 1 ~ j ~ D/d where p = 1 6)(%). (In other words, (p~ p~+ 1) 
n machines wilI cease to be available immediately after step dj , ) No matter what selection algorithm 
is used, it can have at most a p chance of picking a winner. More generally, a similar adversary can 
be used to show that if D G [td, (t + l)d 1], then Bob can select a good workstation with probability 
at lJt mostn . 2.1 Application to On-line Set Cover Next, we show how to adapt the preceding algorithm 
to provide an optimal O(log n log ~)-competitive algo­rithm for the on-line set cover problem. In the 
set cover problem, we are given n sets F = {S,, S2, . . . . S~}, of which we are allowed to choose k. 
The elements WI, VZ, . . . . v~ arrive one per step (without loss of generalit y), and as each element 
arrives, we learn what sets it belongs to. In what follows, we will assume that credit is given for an 
element if we have chosen a set containing the element in the past or if we choose a set containing the 
element during the step when the element arrives. (E.g., in the video-on-demand prob­lem, we can get 
credit for a request for a movie after seeing the request, but only if we immediately accept the request.) 
For clarity of exposition, we will think of the k choices as being made by k different people PI, PZ, 
. . . . 7,$, each of whom will make at most one choice. When a person PI chooses some set S, he will 
get credit for all elements v c S that arrived during or after the step when he se­lected S and that 
are not credited to Pl, for 1 <1. (In other words, if v is contained in a set that was selected by Pl, 
for some 1 <1 before or during the step when v arrived, then v is not credited to Pt. ) By prioritizing 
the allocation of credit in this way, we can ensure that we only get credit for each element once, even 
if the same set is selected more than once. The prioritization also allows the analysis to proceed as 
if PI selects first (see­ing all the elements before P2 selects), Pz selects from what remains, and so 
forth, even though all the play­ers make their decisions as each element arrives. (so, in fact, P1 might 
select the same set as PZ after it is selected by P2, In this case, P2 stops getting credit for the set 
as soon as PI selects the set,) Pi makes his selection using the protocol followed by Bob in the previous 
section, but with the following mod­ifications. First the value of D is replaced by D1 = 23 1 where j 
is selected at random from [1, log +]. (Each D1 is chosen independently from the others.) The value of 
d is replaced by dl = 1*J, where a is a suitably large constant. Next, we identify set S, with workstation 
W, for each i. W, will be considered to be available by ?? at the jth step iff WYc S, and Vj # S,/ for 
all i such that S,, was previously selected by PIJ for some 1 <1. (In other words, S, is considered to 
be available at step j by Pt if Pt would have gotten credit for Vj had PL chosen S, in the past or if 
P1 chooses S, now. ) If dl = O and this is the first time that Pi has a chance to get credit for an element, 
then P1 selects S,. Otherwise, P~ selects S, at step j with the same probability that Bob would select 
W, at step J. In what follows, we use Rl to denote the random variable that counts the number of covered 
elements that are credited to PZ. We also define R = R1 + R2 +... + Rk to be the size of the cover produced 
by the algorithm. Our goal is to show that EX[R] ~ 11The ~e,uits ,-an be ~~dified to handle a scenario 
where credit is only given for elements that are contained in previously-held sets, but we will then 
need to assume that the optimal k-cover contains L ~ ak log n elements where a is a sufficiently large 
constant, although we will not assume that the value of L is known in advance,  ), where L is the size 
of the optimal off­ wlOgnlO; (7n/k) line k-cover. Let Lt be the random variable that denotes the max­imum 
amount that any workstation (or set) appeared to be available to P~. If LI z L/2k, then D[ will be chosen 
so that &#38; < D! < Lt with probability at least 1/ log ~. If this happens (i.e., if &#38; < Dt ~ Ll) 
and CJ z 1, then we can use Theorem 2.3 to show that RI > dl > Q(A) > Q(A) with probabil­ ity close to 
1. If &#38; < 11 ~ L1and dl = O,then R1 = 1 ~ Q(%) ~ Q(A) with certainty. Com­bining the preceding facts 
yields the conclusion that ) with probability at least 0(1/ log ~) R 2 Q(klo; n (all provided L1 ~ L/2k). 
We next show that if LI < L/2k for some 1, then R > L/2. The proof is by contradiction. Suppose that 
R < L/2. Then R1 +Rz+. . .+ RI_l < L/2. Since there are k sets that cover L items, this means that some 
set must cover at least ~ ~ &#38; items that were not credited to PI, PZ, . . .. or Pl_l. By the definition 
of availability y to PI, this means that L1 ~ L/2k. Hence, if L{ < L/2k for some 1, then R > L/2, as 
claimed. Let T l = R, + R/k. Then EZITI] ~ Q( ~log ~lo~ ~ ) since we get that contribution from Ez[R~] 
if LI > L/2k and we get more than that from Ex [R/k] otherwise, Let T denote the sum of the Tl. Then 
Ez[~ = 2Ez[R] and EzITl z O(log ~~og ~ ). Hence, Ez[R] z ), as claimed. (log nfog ~ The preceding algorithm 
allowed the same set to be chosen more than once, even though each element was credited only once. In 
fact, it never helps to select a set a second time and so we can easily restrict our algorithm to select 
each set only once. In this case, we may select fewer than k sets overall. 2.2 Lower Bounds The lower 
bounds we give hold even in the special case where the sets are disjoint, i.e., every element belongs 
to precisely one set. We first show that m/(2k) is a lower bound on the competitiveness of any deterministic 
algorithm. This bound is obviously tight up to a factor of 2. The adversary first presents elements taken 
from set number 1. Once the on-line algorithm commits to this set, no further elements from set 1 arrive, 
but elements from set number 2 are presented. This process is re­peated until the on-line algorithm commits 
to k sets or at total of m elements have been presented overall. For every set chosen by the on-line 
algorithm, the on­line benefit is 1 giving a total benefit of no more than k for all sets. If after k 
such sets at least m/2 elements were presented then the off-line algorithm accepts all these sets in 
advance and obtains a benefit of at least m/2. Otherwise at least m/2 elements belonging to set k + 1 
arrive. The off-line algorithm accepts this set obtaining a benefit of at least m/2 whereas the on-line 
algorithm remains at benefit k as it cannot accept any more sets. In any case the off-line benefit is 
at least m/2 whereas the on-line benefit is at most k. This implies the lower bound. The randomized lower 
bound is more complicated. We prove it for the case when k = 1. A similar result can be proved for some 
larger values. The competitive ratio of a randomized algorithm is defined as the supremum over all sequences 
of the ratio bof ~/E(bon ). To prove the lower bound we apply a vari­ation of Yao s theorem to the competitive 
ratios under consideration. (See ABM-93 for this variation). This allows us to replace randomness in 
the algorithm with randomness in the input. We will choose a distribution on the input sequence such 
that the expected competi­tive ratio of any deterministic algorithm is at least the desired lower bound 
when averaging over the possible sequence inputs. Consider the following probability distribution over 
sequences of elements from the sets: 1. Choose integers g and z such that n ~ 2Vz and m ~ 2Z2Y. 2. 
Ghoose an integer 1 < Z* < z uniformly at ran­dom. 3. Construct sequences that consists of i phases 
while each phase consists of y steps. Associated with phase istepj, o<i<i , l~~ <y, isa setof 2Y ~ sets 
S(i, j) (S(i, j) c {S1, ..., S~}). During  phase z step j, 2 different elements are presented from every 
set S C S(i, j). Throughout the sequence, no element is ever pre­sented twice. (In fact, elements names 
are not sig­nificant, only the sets to which they belong, and every element belongs to exactly one set). 
The set of sets S(Z, 1) consists of 2Y 1 sets and is pairwise dkjoint with sets of sets S(r, 1) for all 
r#i, O~i <i*. The set of sets S(i, j), j >1, (elements from whose elements are presented at phase i step 
j), is a ran­dom set of size 2Y J out of the 2Y J+ 1 sets associ­ated with the previous step (S(i, j 
 l)). In other words S(i, j) is a random half of S(i, j 1). Note that the number of new sets in each 
phase is precisely 2Y-] and there are at most z phases. Thus the number of sets is at most 2Y-l Z < n. 
Moreover, the number of elements requested in phase i is less than 2Y 2;. Hence the total number of elements 
is bounded by 2Y2Z < m. Thk justifies the choice of y and z as a function of m and n. Since the off-line 
algorithm knows the value of i* and the set that will be used in the last step of phase Z*, it is easy 
for the off-line algorithm to get a benefit of 2  1y. Matters are more difficult for the on-line algorithm, 
however, since it knows neither piece of information. In fact, we will next show that the on-line algorithm 
can not do any better than picking a predetermined point at which to select the set, where the selected 
set is the set containing the item just presented. Consider simple deterministic on-line algorithms of 
the following form: wait until the element 1 is pre­sented, choose the set to which it belongs. We can 
now argue that given any deterministic on-line algorithm A for this problem with inputs drawn from the 
above dis­tribution, there exists a simple deterministic algorithm A such that the expected benefit of 
A , over the above distribution on input sequences, is at least the expected benefit of A on the same 
distribution. This is because nothhg is significant in one sequence over another. (Up to reordering the 
input and the set lables, every input of a fixed length in the same. ) Hence, we can successively modify 
A, without decreasing the expected benefit, un­til a simple algorithm is obtained. It follows that to 
prove a lower bound on the com­petitive ratio it suffices to prove an upper bound on the expected ratio 
between the on-line benefit of simple deterministic algorithms and the adversary benefit. A simple deterministic 
on-line algorithm has a sin­gle parameter 1 as discussed above, this translates to choosing a set which 
has an element presented in phase 2 ,step ~ for some O~ i < z, 1<j < y. Then the probability that i = 
i + s for 1 Z < s ~ z i is ~. Now, if s < 0 then the on-line bene­fit 1s zero because the sequence 
ends before the on-line algorithm chooses any set. If s ~ 1 then the on-line algorithm chooses some set 
S c S(i , j ). The on-line benefit depends on the maximal value ~ s k s Y for which S c S(Z , k), in 
which case it is no more than (k j + 1)2 . The conditional probability that S c S(i ,lc +r), r z O, 
given that S c S(Z , k), is 2- . It now follows that the expected ratio between the on-line benefit and 
the adversary benefit is no more than 1 2 2 . ;+ 2,/+2 + 22/+1 + ()(z ) ~ 2-0 +2-1 +2-2+... < ~. yz 
Y ( ) This implies the yz/4 lower bound on the competitive ratio. Note that in terms of n and m the lower 
bound is log n log m for wide range of values. 3 Picking Winners Repeatedly We next consider the scenario 
where Bob has log n d­step jobs to run and where he may elect to kilI a job and restart it on another 
workstation if he is not sat­isfied with the progress thus far. To make matters more difficult, we will 
assume that the jobs, call them Jl,..., frog., must be run in sequence (i.e., that Yt+l cannot be scheduled 
until $, is completed for all z < log n). In what follows, we will show how to schedule the jobs so 
that all of them are completed with probabil­ity 1 0(1/n). We will assume only that at least one workstation 
will eventually be available for D > ad log n steps where a is a sufficiently large constant. As before, 
the scheduling algorithm is quite sim­ple. In this case, we flip (1 + e) log n coins (call them that 
will be specified later.) The probability of a Heads for ch will be 2-c, hnc2xjD-2/d where c1 = @(log 
cr) and cz = @(log a) are large con­stants that will be specified later and z is the number of steps 
that W, has been available thus far. Whenever one of the coins is Heads for the first time (i.e., if 
the hth coin is Heads for the first time at step j of Wi ), then Bob schedules a job on W,. If a job 
was still running on another workstation, then it is killed and restarted on W,. Otherwise, the next 
job in the queue is scheduled on W,. The proof that this algorithm works is similar to but more complicated 
than the proof for the case when Bob schedules a job only once. The complication arises because we need 
to overcome the log n barrier described in Remark 2.5 as well as dependence problems caused by occasionally 
having to kill jobs in order to be more efficient. We begin by getting good bounds on when each coin 
is likely to first come up Heads. Each time that some workstation is available at a step, there is a 
chance (specifically, the probability is Z C1hnC2$lD-2/~) that Ch will come up Heads. Let mh denote the 
sum of these probabilities up to and includ­ing the current flip. (In the case that 2 c1hnc2 D-2/d > 
1, we still add the full amount into mh. Then, it will be the case that m~+l = 2 C mh for au h.) We say 
that a coin is early if Ck comes up Heads for the first time when mh < 2 =112 and iate if eh does not 
come up Heads while mh < 2 1 2. In what follows, we show that the probability that a coin is either early 
or late is small. Lemma 3.1 For ang h, the probability that eh is early 2 -.1 i2 ~n~ the probability 
that Ch is ~~te is at is at most _2ql~ moste . Proofi The probability that there is a Heads among the 
first r tosses of Chis at most PI + pZ + ~. + p, where p~ is the probability of a Heads on the lth flip 
of C~ (over all workstations and steps). For eh to be early, one of these tosses must result in Heads 
where CI 12. Thus, the probability mh=Pl+P2+ +pr<2 that eh is early is at most 2- /2. By similar reasoning, 
the probability that eh is late is at most 6 1=1 c112. Thus, the probability _2.llZ where mh=p~+. ..+p, 
~2 that Ch is late is at most e .0 Lemma 3.2 With probability l 0(1/n), at most ~ log n coins will be 
either early or late. Proofi By Lemma 3.1 and the independence of the coins, the probability that ~ log 
n or more coins are ei­ther early or late is at most Cl, cl,..., C(I+C) ~Og~) for every pair (i, J where 
W, is available at step j. (e = @(l/ log a) is a small constant (ws9(2-c 2+e-2c1 2 g This probability 
is 0(1/rs) provided that e ~ 1 and that c1 ~ 36/e. Henceforth, we will assume that a is large enough 
and that c1 and c are selected so that both conditions are satisfied. 0 Lemma 3.3 For eachh < (l+e)logn, 
theprobabd­ity that ch first becomes Heads afterch+l first becomes Heads is at most 2-=11 + e- . Proofi 
The step at which mh first reaches 2= /2 is the same as the step at which mh+l first reaches 2-=1/2 (since 
mh+l = 2- mh by definition). Thus in order for ch to first become Heads after ch+ 1 first becomes heads, 
it must be the case that either ch is late or C~+ 1 is early. The result then immediately follows from 
Lemma 3.1. Lemma 3.4 For any h ~ (1+ c)log n, with probability 1-o(c2/a + 2- 2 + ~/n), ch will eventua[iy 
become Heads and it will first become Heads when flivoed for a ... . workstation which will be availab!e 
for at least d steps before Ch+ 1 first becomes Heads. Proofi The proof is similar to the proof in Section 
2. In particular, let S denote the sample space of all flips for Ch and Ch+ 1. Let S denote the subspace 
of sample points for which Ch first becomes Heads at or before the step where Ch+ 1 first becomes Heads 
and for which the fist d flips of Ch for each workstation results in Tails. Let Swin denote the subspace 
of points for which the condition of the Lemma holds; namely, that ch will first become Heads when flipped 
for a workstation that will be available for at least d steps before Ch+ 1 first becomes Heads. We first 
show that Pr[S ] is close to 1. The proba­bility of getting a Heads among the (at most) dn flips of &#38; 
for which x ~ dis at most ~n(2-clhnMD-2/~) ~ n.zdJD l zc 1~ =_ = O(;/n) since c = O(log a). The probability 
that there are no Heads among the last d flips of ch for the workstation that is available for D steps 
is either O or at most (1 _ 2-cl~ncz(~-d)/D-2 /d)d ~ (1 _ ~ ci(l+c)+cz-c, d/D-2 /d)d _nc~(l-d/D) -cl(l+. 
)-2 <e ~ 0(1/n)  provided that cz ~ 3+2c1, a ~ 1 and e <1. Henceforth, we will assume that a is large 
enough and that c and e are selected so that these conditions are satisfied. Combining the previous two 
bounds with the bound of Lemma 3.3, we find that Pr[S ] > 1 2- 1 2 e-  0(1/n) We next show that 
Pr[Swin] is large. The proof is nearly identical to that of Lemma 2.2. In particular, we construct an 
injection f ZS + S~in for which Vs E S Pr[$(s )] ~ (1 @(cz/a))Pr[s ]. The injection is constructed 
by identifying the worksta­tion W, and flip x for which ch is first Heads, amd then changing the (x 
d)th flip of &#38; for W, to be Heads instead of Tails. If z = 2 1hnc2Xl D 2/d is less than 1/2, then 
we also change the zth fllp of ch for W, to be Tails instead of Heads. As a result, we can conclude that 
p~[Swin] ~ (1 -0(c2/a))pr[s l > 1 0( C2/CI + 2 1/2 + l/n). 0 Lemma 3.5 With probability 1 0(1/n), for 
all but $ log n values of h ~ (1+ c) log n, ch will become Heads and will first become Heads for a workstation 
which will be available for at least d steps before ch~l first becomes Heads. Proofi We first consider 
the case when h is even. Then the probability that the condition of Lemma 3.4 holds will be independent 
for each h. In particular, the probability that the condition of Lemma 3.4 fails for more than ~ log 
n even values of h is at most ~l$nn (o(c /a + 2- /2 + I/n)) ~ 10 . ()  This probability is 0( l/n) 
provided that c ~ 1, c1 is a sufficiently large constant multiple of 1/.5, c is 0(1/c), and t is a sufficiently 
large constant multiple of 1/ log a. All of these conditions (as well as the prior con­straints in the 
constants) can be met provided that a is a sufficiently large constant and where c = @(1/ log a), c1 
= @(logs), and cz = @(logs). An identical argument can be use to show that the probability that the condition 
in Lemma 3.4 fails for more that ~ log n odd values of h is at most 0(1/n). Thus, with probability y 
1 0(1/n), the condition of Lemma 3.4 fails for at most ~ log n values of h. 0 Theorem 3.6 With probability 
l 0(1/n), the schedzd­ing algorithm wiil result in ail log n jobs being completed and at most clog n 
instances where a job is killed and restarted on another workstation. Proofi Let lh denote the interval 
of time between the step when ch is first Heads and the step when Ch+ 1 is first Heads. If both of ch 
and C~+ 1 are neither early nor late, we say that lh is good. Otherwise, we say that Ih is bad. By Lemma 
3.2, we know that with probability y 1 0(1/n), there are at most ~ log n bad intervals. Since no two 
good intervals overlap, we can again use Lemma 3.2 to show that with probability 1 0(1/n), there are 
at most ~ log n good intervals 1~ during which another coin (other than C~ or Ch+ 1) first becomes Heads. 
By Lemma 3.5, we know that with probability 1 0(1/n), for all but at most slog n values of h, Ch will 
become Heads for a workstation which will be available for at least d steps before ch+ 1 fist becomes 
Heads. Combining the previous three facts and adding fail­ure probabilities, we can conclude that with 
probability y 1 0(1/n), there are at least (l+c)logn ;logn ;logn :logn=logn values of h for which ch 
first becomes Heads for a work­station that will be available for at least d steps before any other coin 
first becomes Heads. The job that is assigned to such a workstation is guaranteed to be com­pleted before 
the algorithm attempts to schedule an­other job. Since only (1 + e) log n attempts are made to schedule 
a job, only e log n can end in failure. 0 The result in Theorem 3.6 can be shown to be tight or nearly 
tight in several respects. For example, by con­sidering a randomized adversary of the type outlined in 
Remark 2.5 (with p = 1/ log n), it can be shown that if D = O((d log n)/ log log n), then no scheduling 
algo­rithm will have better than a @(1/ log log n) chance of scheduling even one job, no matter how many 
swaps and restarts it makes. 4 Extensions 4.1 Obtaining Higher Efficiency The algorithm described in 
Section 3 is inefficient by a factor of a. This is because there is a workstation that was available 
for D = ad log n steps, but we only com­pleted log n d-step jobs with high probability. In what follows, 
we will show how to attain higher efficiency by allowing the user to schedule up to a/c = O(l) jobs at 
the same time. (Each workstation still only actively works on one job at a time, of course, and there 
are no precedence constraints between jobs scheduled at the same time.) The improvement is quite simple. 
Assume that some workstation will be available for D* steps. Then the user runs cY/e versions of the 
algorithm, described in Sec­ tion 3 with d = &#38; and D = D*/(2 + 1/6). Each version is assigned a unique 
priority and works inde­ pendently from the others. (In fact, we can think of the cY/e versions as if 
each was being run by a separate user. ) Whenever a higher-priority job is scheduled on a workstation, 
that workstation simply appears to be unavailable to all lower priority versions. (The reverse e) log 
n jobs, each of which has length at most d.) Hence, even if all cr/e versions consume (1 + c)d log n 
steps on a single workstation, some workstation will still be avail­able for at least D* :(l+c)dlogn 
= D(2+ l/~) :(1 +6) =D steps, which means that every version of the algorithm will be able to complete 
log n jobs with high probability by Theorem 3.6. The total amount of work accomplished by the a/c versions 
is D* ~ (1 2e)D*. = 1+26 By making e be small, this amount can be made arbi­ trarily close to D*. The 
preceding analysis ignored the scenario when there is more than one workstation that is available for 
D* steps. We show how to exploit the available capacity in multiple workstations in the following section 
where we also handle the case of multiple users. 4.2 The Case of Multiple Schedulers One particularly 
nice aspect of our scheduling algorithm is that (with only small modifications) it can be used simultaneously 
by multiple individuals without coordi­nation. (Alternatively, it can be used by a few individ­uals who 
want to use multiple workstations this is reducible to the case where there are multiple users.) For 
example, consider a scenario where there are at least k workstations that will be available for at least 
D* steps each. In this case, each user will be instructed to use the algorithm of Section 3 with D = 
D*/(2 + 1/6). For simplicity, the users will be prioritized, but they will not otherwise interact. Each 
user runs the scheduling algorithm as if he/she were the only individual running background jobs. Workstations 
will be considered to be available to a user iff they are not running a higher­ priority job. Higher-priority 
jobs will always interrupt lower-priority jobs. Knowledge of k is not needed and is used only for the 
purposes of the analysis. In order to attain near maximum use of the available time in the k workstations, 
we will assume that there are at least (a/c)k users, each with log n d-step jobs where d = D/(a log n). 
Somewhat surprisingly, the (cr/c)k users will get all of their log n jobs run (in sequence) with high 
probability. This is because each of the (cr/e)k users can consume at most (1 + e)d log n available time 
using the algorithm of Section 3. This means that at least one of the k workstations will still be available 
for jobs done with high probability. (Actually, we need to boost the success probability of the analysis 
in Theo­rem 3.6in order to make the preceding result hold with probability l O(l/n), but this iseasyto 
do by adjust­ing the constant factors. ) is not true. In low-priority job priority user.) other words, 
a workstation will appear to be available running a to a higher- D* - (cY/e)k(l + k e)dlog n The key 
to proving that (a/e) log n jobs are com­ = (2+ l/6)D - ( ~)D = D pleted with high probability rests 
on the fact that each version can consume at most (1 + c)d log n available steps no matter what the (a/c) 
k users do. Hence, by steps. (This is because a version starts at most (1+ Theorem 3.6, each of the (cr/c)k 
users will get all log n It is worth noting that the preceding result attains greater eficiency since 
weareable to accomplish kD* ~kdlogn=~ = 6(1/6 + 2) = = >(1 26)kD* l+2t productive work. 4.3 The Case 
when D* is Unknown The algorithms described thus far used knowledge of D* in order to schedule the jobs. 
In what follows, we show how to modify the algorithms so that dependence on D* is no longer required. 
In the case when D* is not known, the scheduler partitions time into intervals as follows. The first 
inter­ val lasts until some workstation has been available for a log n steps. During this time, the scheduler 
ruus the algorithm for d-step jobs where d = 1. For z > 1 the ith interval starts after the (i l)st 
interval has finished and lasts until some workstation has been available for a2i -1 log n steps (counting 
from the beginning of the interval, only). During this time, the scheduler runs the algorithm for d-step 
jobs where d= 2 . Even without knowing the value D*, log n jobs of length O(D*/ log n) will still be 
completed with proba­bility 1 0(1/n). Jobs of shorter length will also be completed, although the number 
of killed jobs could grow as large as e log n log (D*/ log n). The algorithms in Sections 4.1 and 4.2 
can be modified in a similar man­ner in order to improve efficiency. In fact, if the users each have 
a unique priority, and if each user is allowed to run a/t jobs at the same time, then for all k, the 
kth user will be able to get (1 2c)D~ work done with high probability where D: is the amount of time 
available on the kth most available machine. 5 Acknowledgments We would like to thank Allan Borodin, 
Leonid Levin, Prabhakar Raghavan, Mike Sipser, Bob Tarjan, Al Vezza, and Joel Wein for helpful remarks, 
suggestions, and ref­erences. References [ABFR94] B. Awerbuch, Y. Bartal, A. Fiat, and A. Ros6n. Competitive 
Non-Preemptive Call-Control. In Proc. oj the 5th Ann. ACM-SIAM Symp. on Dis­crete Algorithms, pages 312-320, 
January 1994. [ABF93] B. Awerbuch, Y. Bartal, and A. Fiat. Competitive Distributed File Allocation. In 
Proc. of the 25th Ann. ACM Symp. on Theory o.f Computing, pages 164-173, May 1993. [ABM93] Y. Azar, 
A. Broder, and M. Mannase. On-line choice of on-line algorithms. In Proc. lth ACM-SIAM Symp. on Discrete 
Algorithms, pages 432 440, 1993. [AGH94] A. Aggarwal, Juan Garay, and Amir Herzberg. Adaptive video on 
demand. In Proc. Thirteenth ACM POD C Syrnp., page 402, 1994. also appeared as an IBM Research Report, 
RC19770, Ott, 1994. (BCLR951 S. Bhatt, F. Chun~, T. Lei~hton, and A. Rosen­ berg. Optimal stra~egies 
fox stealing cycles. Un­published manuscript, 1995. [BL94] R.D. Blumofe and C.E. Leiserson. Scheduling 
Mul­tithreaded Computations by Work Stealing. In Proc. oj the 95th Ann. IEEE Symp. on Founda­tions oj 
Computer Science, pages 356-368, Novem­ber 1994, [CEL93] J. Cooperstock, R. E1-Yaniv, T. Leighton. The 
Statistical Adversary Allows Online Foreign Ex­change with no Rkk. Proceedings of SODA 95. [CV92] B. 
Chandra and S. Vishwanathan. Construct­ing Reliable Communication Networks of Small Weight On-line. Journal 
of Algorithms, 1992. [EFKT92] R. E1-Yaniv, A. Fiat, R. Karp, and G. Turpin. Competitive Analsvs of Financial 
Games. In Proc. of the 33th Ann. IEEE Symp. on Foundations oj Computer Science, pages 327-333, October 
1992. [EK93] R. E1-Yaniv and R. Karp. The Mortage Problem. In Proc. of the 2nd Ann. Israeli Symp. on 
Theo­retical Computer Science, May 1993. [FKT89] U. Faigle, W. Kern and Gy6rgy Tur&#38;n. On the Performance 
of On-Line Algorithms for Parti­tion Problems. Acts Cybernetics 9, pages 107-1I9, 1989. [HS92] M.M. Ha11d6rsson 
and M. Szegedy. Lower Bounds for On-Line Graph Coloring. In Proc. oj the 3rd Ann. ACM-SIAM Symp. on Discrete 
Algorithms, pages 211-216, January 1992. [Irani90] S. Irani. Coloring Inductive Graphs On-Line. In Proc. 
oj the $lst Ann. IEEE Symp. on Founda­tions oj Computer Science, pages 470-479, OctO­ber 1990. [IW91] 
M. Imase and B.M. Waxman. Dynamic Steiner Tree Problem. In SIAM .lournal on Discrete Mathematics, 4(3):369-384, 
August 1991. [KP94] B. Kalyanasundaram and K.R. Pruhs. Fault-Tolerant Scheduling. In Proc. oj the 26th 
Ann. ACM Symp. on Theory oj Computing, pages 115­124, May 1994. [KMRS88] A.R. Karlin, M.S. Manasse, L. 
Rudolph, and D.D. Sleator. Competitive Snoopy Caching. In Al­gorithmica, 3(1):79-119, 1988. --­ [KVV90] 
R.M. Karp, U.V. Vazirani, and V.V. Vazirani. An Optimal Algorithm for On-Line Bipartite Match­ing. In 
Proc. oj the 22rd Ann. ACM Symp. on Theory oj Computing, pages 352-358, May 1990. [LT94] Richard J. Lipton 
and Andrew Tomkins. On­line interval scheduling. In Proc. 5th ACM-SIAM Symp. on Discrete Algorithms, 
pages 302 311, Ar­lington, VA, January 1994. [ST85a] D.D. Sleator and R.E. Tarjan. Amortized Effi­ciency 
of List Update and Paging Rules. In Co m­munications o.f the A CM, 28(2) pages 202-208, 1985. [ST85b] 
D.D. SLEATOR AND R.E. TARJAN. Self-Adjusting Binary Search Trees. Journal oj the A CM, 32:652 686, 1985. 
 [WY93] J. Westbrook. and D.K. Yan. Greedy On-Line Steiner Tree and Generalized Steiner Problems. In 
Proc. of the 9rd Workshop in Algorithms and Data Structures, Also Lecture Notes in Computer Sci­ence, 
vol. 709, pages 622-633, Montr6al, Canada, 1993, Springer-Verlag. 
			