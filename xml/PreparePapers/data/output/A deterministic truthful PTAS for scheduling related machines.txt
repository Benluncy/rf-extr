
 A deterministic truthful PTAS for scheduling related machines. George Christodoulou* Annam´aria Kov´acs 
Abstract Scheduling on related machines (Q||Cmax) is one of the most important problems in the .eld of 
Algorithmic Mechanism Design. Each machine is controlled by a sel.sh agent and her valuation can be expressed 
via a single parameter, her speed. Archer and Tardos [4] showed that, in contrast to other similar problems, 
a (non-polynomial) allocation that minimizes the makespan can be truthfully implemented. On the other 
hand, if we leave out the game-theoretic issues, the complexity of the problem has been completely settled 
the problem is strongly NP-hard, while there exists a PTAS [9, 8]. This problem is the most well-studied 
in single­parameter Algorithmic Mechanism Design. It gives an ex­cellent ground to explore the boundary 
between truthfulness and e.cient computation. Since the work of Archer and Tardos, quite a lot of deterministic 
and randomized mech­anisms have been suggested. Recently, a breakthrough re­sult [7] showed that a randomized, 
truthful-in-expectation PTAS exists. On the other hand, for the deterministic case, the best known approximation 
factor is 2.8 [10, 11]. It has been a major open question whether there exists a deterministic truthful 
PTAS, or whether truthfulness has an essential, negative impact on the computational complexity of the 
problem. In this paper we give a de.nitive answer to this important question by providing a truthful 
deterministic PTAS. Introduction Algorithmic Mechanism Design (AMD) is an area origi­nated in the seminal 
paper by Nisan and Ronen [14, 15] and it has .ourished during the last decade. It stud­ies combinatorial 
optimization problems, where part of the input is controlled by sel.sh agents that are either unmotivated 
to report them correctly, or strongly mo­tivated to report them erroneously, if a false report is pro.table. 
In classical mechanism design more emphasis has been put on incentives issues, and less to computa­tional 
aspects of the optimization problem at hand. On the other hand, traditional algorithm design disregards 
the fact that in some settings the agents might have incentive to lie. Therefore, we end up with algorithms 
that are fragile against sel.sh behavior. AMD carries challenges from both disciplines, aiming at the 
design of qualitative algorithms that, at the same time, make Max-Planck-Institut f¨ur Informatik, Saarbr¨ucken, 
Germany. This author has been partially supported by DFG grant Kr 2332/1-3 within Emmy Noether Program 
and by EPSRC grant EP/F069502/1. Email: gchristo@mpi-inf.mpg.de. Department of Informatics, Goethe University, 
Frankfurt M., Germany. Email: panni@cs.uni-frankfurt.de. sel.sh users interested in reporting truthfully, 
and so are also immune to strategic behavior. A fundamental optimization problem that has been suggested 
in [15] as a ground to explore the design of truthful mechanisms, is the scheduling problem, where a 
set of n tasks need to be processed by a set of m machines. There are two important variants with respect 
to the processing capabilities of the machines, that have been studied within the AMD framework. The 
machines can be unrelated, i.e., each machine i needs tij units of time to process task j; or related, 
where machine i comes with a speed si, while task j has processing requirement pj , that is, tij = pj 
/si (we will use the settled notation Q||Cmax to refer to the latter problem). The objective is to allocate 
the jobs to the machines so that the maximum .nish time of the machines, i.e. the makespan is minimized. 
In the game-theoretic setting, it is assumed that each machine i is a rational agent who controls the 
private values of row ti. It is further assumed that each machine wants to minimize its completion time, 
and without any incentive it will lie, if this can trick the algorithm to assign less work to him. In 
order to motivate the machines to cooperate, we pay them to execute the tasks. A mechanism consists of 
two parts: an allocation algorithm that assigns the tasks to the machines, and a payment scheme that 
compensates the machines in monetary terms. We are interested in devising truthful mechanisms in dominant 
strategies, where each player maximizes his utility by telling the truth, regardless of the reports of 
the other players. The scheduling problem provides an excellent framework to study the computational 
aspects of truth­fulness. It is a well-studied problem from the algorith­mic perspective with a lot of 
algorithmic techniques that have been developed. Moreover, it is conceptually close to combinatorial 
auctions, so that solutions and insights can be transferred from the one problem to the other. Indeed, 
the scheduling problem comes with a variety of objectives to be optimized, that are di.erent than the 
objectives used in classical mechanism design. The mechanism design version of scheduling on re­lated 
machines was .rst studied by Archer and Tar­dos [4]. It is the most central and well-studied among single-parameter 
problems, where each player controls a single real value and his objective is proportional to this value 
(see also Chapters 9 and 12 of [13]). In particular, in the scheduling setting the cost of player i is 
ti · wi, where ti =1/si is his private value, and wi is the to­tal processing requirement (sum of the 
pj ) allocated to machine i. The pro.t that the agent tries to maximize is the payment he receives minus 
his cost. Myerson [12] gave a characterization of truthful algorithms for one­parameter problems, in 
terms of a monotonicity condi­tion. Archer and Tardos [4] found a similar monotonic­ity characterization, 
and using it they showed that a certain type of optimal allocation is monotone and con­sequently truthful 
(albeit exponential-time). A mono­tone allocation rule for related scheduling is de.ned as follows: 
 Definition 1.1. An allocation rule for Q||Cmax is monotone, if increasing the input speed si of any 
sin­gle machine i, while leaving the other speeds unchanged, monotonically increases the workload wi 
allocated to this machine, i.e., si <s.= wi . wi. i The fact that truthfulness does not exclude opti­mality, 
in contrast to the multi-parameter variant of scheduling (the unrelated case)1, makes the problem an 
appropriate example to explore the interplay between truthfulness and computational complexity. It has 
been a major open problem whether or not a deterministic 2 monotone PTAS exists for Q||Cmax . In this 
work, we give a de.nitive positive answer to that central question and conclude the problem. 1.1 Related 
Work Auletta et al. [5] gave the .rst deterministic polynomial-time monotone algorithm for any .xed number 
of machines, with approximation ratio 4. This result was improved to an FPTAS by Andelman et al. [2]. 
For an arbitrary number of machines, Andelman, Azar, and Sorani [1] gave a 5-approximation deterministic 
truthful mechanism, and Kov´acs improved the approximation ratio to 3 [10] and to 2.8 [11], which was 
the previous record for the problem. Randomization has been successfully applied. There are two major 
concepts of randomization 1With the scheduling on unrelated machines, we are more in the dark (see [6] 
for a recent overview of results). There are impossibility results that show that there does not exist 
any truthful mechanism with approximation ratio better than a constant even in exponential time. Therefore, 
more primitive questions need to be answered before we settle the complexity of the problem. The only 
known algorithm for the problem is the VCG that has approximation ratio equal to the number of machines. 
2We say that a mechanism runs in polynomial time when both the allocation algorithm and the payment algorithm 
run in polynomial time. for truthful mechanisms, universal truthfulness, and truthfulness-in-expectation. 
The .rst notion is strongest, and consists of randomized mechanisms that are probability distributions 
over deterministic truthful mechanisms. In the latter notion, by telling the truth a player maximizes 
his expected utility. Only the sec­ond notion of randomized truthfulness has been applied to the problem. 
Archer and Tardos [4] gave a truthful­in-expectation mechanism with approximation ratio 3, that was later 
improved to 2 [3]. Recently, Dhangwat­notai et al. [7], settled the status for the randomized version 
of the problem by giving a randomized PTAS that is truthful-in-expectation. Both mechanisms ap­ply (among 
other methods) a randomized rounding pro­cedure. Interestingly, randomization is useful only to guarantee 
truthfulness and has no implication on the ap­proximation ratio. Indeed, both algorithms can be eas­ily 
derandomized to provide deterministic mechanisms that preserve the approximation ratio, but violate the 
monotonicity condition. 1.2 Our results and techniques We provide a deterministic monotone PTAS for Q||Cmax. 
The cor­responding payment scheme [4] is polynomially com­putable3 , and with these payments our algorithm 
in­duces a (1+3a)-approximate deterministic truthful mechanism, settling the status of the problem. As 
op­posed to [10], where a variant of the LPT heuristic was shown to be monotone, our algorithm is the 
.rst de­terministic monotone adaptation of the known PTAS s [9, 8]. Next we describe the main ideas of 
the paper. We start by .xing a common basis for our sub­sequent considerations. We always assume that 
input speeds are indexed so that s1 . s2 . ... . sm holds. For any set of jobs P = {p1,p2,...,pj }, the 
weight or .j workload of the set is |P | = r=1 pr. We will view an allocation of the jobs to the machines 
as an (ordered) partition (P1,P2,...,Pm) of the jobs into m sets. We search for an output where the workloads 
|Pi| are in non-decreasing order. The PTAS in [8] which is a simpli.ed and polished version of the very 
.rst PTAS [9] de.nes a directed network on m + 1 layers depending on the input job set, where each arc 
leading between the layers i - 1 and i represents a possible realization of the set Pi, and directed 
paths leading over the m + 1 layers correspond to the possible job partitions. An optimal solution is 
then found using a shortest path computation in this network. The di.culty in applying any known PTAS 
to con­ 3This is intuitively clear, since our work curve is a step function with a polynomial number 
of steps.  struct a deterministic monotone algorithm for Q||Cmax is twofold. First, in all of the known 
PTAS s, sets of input jobs of approximately the same size form groups, s.t. in the optimization process 
a common (rounded or smoothed) size is assumed for all members of the same group. Second, jobs that are 
tiny compared to the total workload of a machine do not turn up individually in the calculations, but 
just as part of an arbitrarily divisible (e.g., in form of small blocks) total volume. Note that it must 
be relatively easy to .nd an allocation procedure that is in a way approximately monotone . However, 
(exact) monotonicity intuitively requires exact determination and knowledge of the al­located workloads. 
To justify this, we just point out that in every monotone (in expectation) algorithm for Q||Cmax provided 
so far, the (expected) workloads ei­ther occur in increasing order wrt. increasing machine speeds, or 
constitute a lexicographically minimal opti­mal solution wrt. a .xed solution set and a .xed ma­chine 
indexing. Thus, both of the mentioned simpli.cations of the input set which, to some extent, seem necessary 
to admit polynomial time optimization appear to be con­demned to destroy any attempt to make a deterministic 
adaptation monotone. (The authors of [7] used ran­domization at both points to obtain the monotone in 
expectation PTAS.) Our ideas to eliminate the above two sources of inaccuracy of the output are the follow­ing, 
respectively: 1. As for rounding the job sizes, note that grouping is necessary only to reduce the (exponential) 
number of di.erent outputs. We can achieve the same goal if for any group of jobs of similar size we 
.x the order of jobs in which they appear in the allocation (e.g., in increasing order), and calculate 
with the exact job sizes along the optimization process. Notice that not even the fact is obvious that 
such a solution with near-optimal makespan, and increasing workloads exists. Now, if reducing a machine 
speed increases the makespan of the (previously optimal) solution, that means that this machine became 
a bottleneck, so a new upper bound on the optimum makespan over the considered set of outputs is induced 
exactly by the (previous) workload of the changed machine (the same argument as used in [4, 2, 7]). With 
this idea we derandomize the .rst type of randomization (job smoothing) of [7]. 2. Concerning tiny jobs, 
we observe that with these we can .ll up some of the fastest machines nearly to the makespan level. On 
the other hand, it is easy to show [3] that pre-rounding machine speeds to powers of some prede.ned (1+ 
a) does not spoil monotonicity and increases the approximation bound by only a factor of (1+ a). Assuming 
now that the coarsity of tiny blocks is  small jobs S i large jobs L i  Figure 1: The type of schedule 
we look for much .ner than the coarsity of machine speeds, we can be sure that (full) machines of higher 
speed receive more work than slower machines. Moreover, having reduced the speed of such a machine, tiny 
jobs in its workload .ow to other machines to provide a makespan much smaller than implied by the previous 
workload of this machine. It is quite a technical challenge to combine these two ideas so smoothly that 
in the end yields a correct monotonicity proof. We accomplish this task as follows (see Figure 1). We 
.x (for the proof argument) a set Li of non-tiny jobs on each machine, so that the L1,L2,...,Lm have 
increasing and exactly known weights, and they ful.l the constraints suggested in 1. On top of the sets 
Li, each machine has a set Si of small jobs (due to necessary conditions for rounding the total volume 
of tiny jobs, some of these are uniform blocks, while some are known exactly). The total set of small 
jobs is .exible (along the proof), in particular we can always move a small job to a higher index machine, 
and obtain a valid schedule. Moreover, we set the objectives so that in an optimum solution the small 
jobs are moved to the higher index machines as much as possible (and so, make them full). We note that 
the same job size may be large for a slower machine, while it is small on a faster machine. Our monotonicity 
proof becomes subtle in case of the .rst (and so, not necessarily full) machine contain­ing small jobs. 
It is especially so when this .rst ma­chine is m, not leaving space for manipulating the small jobs in 
the output as needed. In order to circumvent this problem we restrict the search to allocations where 
at least two machines do have some tiny blocks (un­less too few tiny jobs exist). Moreover, it seems 
crucial in our monotonicity argument that every machine has the possibility to get rid of all the tiny 
blocks (i.e., those inducing uncertain workload) if this is provoked by a re­duction of its speed. Combining 
these two requirements we treat the last three machines as a single entity. A carefully optimized assignment 
of an obligatory set of tiny blocks, and later of the actual tiny jobs to these machines then implies 
monotonicity.  1.3 Preliminaries The input is given by a set PI of n input jobs, and a vector s (or 
.) of input speeds s1 . ... . sm. For a job p . PI we use p both to denote the individual job, and the 
size of this job in a given formula. For a desired approximation bound 1+3a, we choose a 5 . a, that 
will be the rounding precision of the job sizes. For ease of exposition, we will assume that 4 (1 + 5)t 
= 2 for some t . N. Furthermore, we de.ne d as the unique integer power of 2 in [5/6, 5/3]. We use the 
interval notation (e.g., [1,m]) for a set of consecutive machine indices. Definition 1.2. (job classes) 
If p denotes (the size of) a job, then p denotes this job rounded up to the nearest integral power of 
(1 + 5). A job p is in the job class Cl, i. p = (1+ 5)l . Let Cl = {pl1,pl2,...,plnmax } be the jobs 
of Cl in l some .xed non-decreasing order of size. We use the max notation Cl(a)= {pl1,...,pla} for 0 
. a . n, and l max Cl(a, b)= Cl(b)\Cl(a) for 0 . a . b . n. l If P = {p1,p2,...,pj } is a job set, then 
P = {p1,p2,..., pj } denotes the corresponding set of rounded jobs. The weight or workload of P is |P 
| = j ; the rounded weight is |P | = j . As­ r=1 pr r=1 pr suming that the jobs are in non-increasing 
order of size, we denote the subset of the r largest jobs by P r = {p1,p2,p3,...,pr}. 2 Canonical allocations 
This section characterizes the type of allocations we call them canonical that we will consider. De.ni­tions 
2.1 and 2.2 describe the necessary restrictions on the output job partition P1,...,Pm. 5 Subsequently, 
our .rst main result, Theorem 2.1 states that for any in­put, and any 5> 0, a canonical allocation exists 
that provides a 1 + O(5) approximation to the optimum makespan. Definition 2.1. (5-division) We say that 
a given set of jobs P is 5-divided into the pair of sets (L, S) (or P =(L, S)) if (D1) P = L u S and 
L . S = =, 4This simplifying assumption is unrealistic for computations, but it is not necessary for 
the result to hold. We could equally well use the rounding function of [8] or [7]. Also, since our result 
is of purely theoretical interest, we do not try to optimize the ratio 0/E; it will be clear that, e.g., 
300<E su.ces in the proofs. 5More precisely, the partition Q1,Q2,...,Qm obtained in step 4. of Ptas will 
be canonical (see Figure 2). U·|L| (D2) p> (1+U)2 for every p . L, and (D3) q . 5|L| for every q . S. 
The subsets L and S will be the large resp. the small jobs on a single machine. Note that we cannot set 
a sharp border between their sizes. For instance, having uniform jobs, 1/5 of them will form L, while 
the rest (arbitrarily many) belong to S. Definition 2.2. (canonical allocation) For a given input, an 
allocation P1,P2,...,Pm is called canonical, if for every i . [1,m], the set Pi can be 5-divided into 
(Li,Si), so that the following properties hold: (A1) if i<i, then |Li|.|Lit |; (A2) for jobs p and q 
of the same job class p . q holds if and only if (a) p . Li and q . Sit for some i, i . [1,m], or (b) 
p . Li and q . Lit and i . i, or (c) p . Si and q . Sit and i . i.  Theorem 2.1. For arbitrary increasing 
input speeds and input jobs, a canonical allocation inducing a sched­ule with makespan at most (1 + 35)OP 
T exists, where OP T is the optimum makespan of the input. In the proof of Theorem 2.1, we modify an 
optimal partition of the rounded input jobs P I to get the canonical allocation. First we take the core 
set of each set in the partition, which yields a preliminary 5-division of the sets: Definition 2.3. 
(core) Given a set P of jobs, we de.ne the core cr(P ) of P as follows. Consider the jobs P = {p1,p2,...} 
in a .xed non-increasing order of size. Let j be minimum with the property that U def pj . 1+U |P j-1|, 
then cr(P )= P j-1 = {p1,...,pj-1}. def If no such j exists, then cr(P )= P. Then we order the partition 
sets by increasing order of core size, and apply the following result to make the cores (now with original 
job sizes) ful.l (A2) (b). Proposition 2.1. Let (Q1,...,Qm) be a partition of a subset Q of the input 
jobs such that |Q1|. ... .|Q|. m There exists a partition (L1,...,Lm) of Q that satis.es 1. |L1|. ... 
.|Lm|; 2. for any job class Cl, if job plj belongs to Li and job plk to Lit where i<i, then j<k;  
 3. for all i |Qi| < |Li|.|Qi|. 1+ 5 Finally, it is easy to show that small jobs (those outside the 
cores) can be shifted towards fast machines, in order to .t below the original makespan again. After 
shifting, the cores and the small jobs induce a 5-division on each machine. 3 Con.gurations Like in [8, 
7], we introduce so called con.gurations ß(w, µ, .no, .n1) in order to represent any possible job set 
Pi of the partition, up to 5 accuracy. We use the con.gurations to de.ne the vertices of a directed graph 
H. A well-de.ned optimal path in this graph will then specify our output schedule.6 The .rst component 
of any con.guration is a mag­nitude w which is an integer power of 2. As we proceed from slow machines 
to fast machines in a schedule, the monotonically increasing magnitude keeps track of the largest job 
size allocated so far, which must be some size in the interval (w/2,w]. Thus, the current mag­nitude 
also shows, which (larger) job sizes are not yet relevant, and which (tiny) jobs need not be taken into 
account individually anymore in the con.guration (note that here we exploit that the |Li| must be increasing, 
cf. [8]). This motivates the next de.nition. Definition 3.1. (valid magnitude) The value w = 2z (z . 
Z) is a valid magnitude if an input job p . PI exists so that w/2 <p . w. Let wmin and wmax denote the 
smallest and the largest valid magnitudes, respectively. We call a job tiny for w if it has size at most 
dw. Recall that d is the integer power of 2 between 5/6, and 5/3. Having a magnitude w .xed, let A = 
log(1+U) dw = t · log(dw), and A=log(1+U) w = t · log w, where (1+5)t =2. Notice that both A and A are 
integers, and by De.nition 1.2, the jobs of size in (dw, w] belong to the classes C.+1,...,CA. These 
will constitute the relevant job classes, if the largest jobsize on the current or slower machines is 
between w/2 and w. If the con.guration ß represents the set Pi in o a job partition, then the so-called 
size vector .n= ooo (n.,n.+1,...,nA) describes the jobs in the cumulative def Ui-1 job set Ai-1 = Ph 
as follows. For A<l . A, h=1 6Roughly speaking, our graph can be thought of as the line graph of the 
graph G de.ned in [8] (with simple modi.cations). That is, the vertices of H correspond to edges of G. 
This is the reason why our con.gurations include two vectors n 1 and n 2 instead of only one. o (l Øµ, 
µ + 1), exactly the .rst (smallest) njobs of= l o the class Cl are in the set Ai-1. The meaning of n. 
is U that in Ai-1 the total weight of jobs from l<. Cl is oo in the interval ((n. - 1) · dw, (n. + 1) 
· dw). However, the particular subset of these small jobs inside Ai-1, is 1 not determined by ß. The 
vector .nrepresents the set def Ui Ai = Ph, analogously. h=1 A major di.erence to the con.gurations of 
[8], is that our con.gurations should not only represent a job set Pi, but also its 5-division (Li,Si). 
In particular, we will distinguish four types of job sizes in a con.guration. Tiny jobs have size at 
most dw, and, as already seen, are represented by the .rst coordinates n. of the two size vectors with 
their total size rounded to an integer multiple of dw. Correspondingly, we will sometimes talk about 
blocks of size dw which are simply re-tailored tiny jobs so as to make our procedure e.cient. Definition 
3.2. (block) Blocks are imaginary tiny jobs, each having size dw for some valid magnitude w. We use S(n., 
dw) to denote a set of n. blocks of size dw. Small jobs are those that (together with the tiny jobs), 
can only appear in the set Si of the 5-division, whereas large jobs can only be in the set Li. However, 
there must exist job classes we will call them middle size jobs , which might occur in both Li and Si, 
since by De.nition 2.1 there is a .exible border between the job sizes in Li and in Si. Therefore, exactly 
two job classes, µ and µ + 1 will be represented by a triple of (increasing) non-negative integers instead 
of scalar values in both of the vectors .no, .n1 . For instance, in the o ooo case of n=(nµ.,n,n), the 
meaning of the three µ µmµs numbers will be that in the set Ai-1, from the job class oo exactly the jobs 
in Cµ(n,n) are allocated as Cµ µmµs small jobs, that is, to one of the sets S1,S2,...,Si-1, o and exactly 
the jobs in Cµ(nµ.) as large jobs, i.e., in one 1 of L1,...,Li-1, and similarly in case of .nfor the 
set Ai. µ The meaning of the numbers for µ + 1 is analogous. We summarize the above description in the 
next, somewhat technical de.nitions. Definition 3.3. (size vector) A size vector .n = (n.,...,nA) with 
middle size µ . [A +1, A], is a vector of integers, with the exception of the entries n=(nµ.,nµm,nµs) 
and n= µµ+1 (n(µ+1).,n(µ+1)m,n(µ+1)s) both of which consist of three integers, so that nµ. . nµm . nµs, 
and n(µ+1). . n(µ+1)m . n(µ+1)s holds. All integer entries belong to [0,n]. Definition 3.4. (configuration) 
A con.guration ß(w, µ, .no, .n1) consists of four components: a valid  oo o magnitude w, and two size 
vectors .n=(n.,...,nA), 11 and .n1 =(n,...,n) with middle size µ, such that .A o 1 max (C1) n. n. nfor 
A<l . A,l Ø. {µ, µ +1}; l ll 1 (C2) if w =Øwmin then n> 0 for at least one l . l (A - t, A]; P |Cl| 
o 1 l (C3) n. n.+ 3; . . dw o 1 o 1 o 1 max (C4) n. n. n= n. n. n. n, and µ. µ. µm µm µs µs µ analogously 
for µ + 1; def 1 o = S(n - n ,dw). Tß .. def o 1 o 1 = (nµ.,n )u Lß Cµµ.) u C(µ+1)(n(µ+1).,n (µ+1). A 
U o 1 u Cl(nl ,n ), and l l=µ+2 def o 1 o 1 = (n ,n ,n )u Sß Cµµsµs) u C(µ+1)(n(µ+1)s(µ+1)s µ-1 U o 1 
u Cl(nl ,n ) u Tß. l l=.+1 o 1 (C5) either .nØ= .n, and (1 + 5)(µ+1) . 5 ·|Lß| < (1 + 5)(µ+2); ..where 
Amin = t · log(dwmin). or ß is the empty con.guration (wmin,Amin+1,0,0) Notation 1. We refer to the whole 
represented job set Lß u Sß (including virtual blocks) simply by ß (abusing notation), and |ß| stands 
for the total work of the set ß. We denote the set without tiny blocks by ß = ß \ Tß. o 1 Clearly, (C1), 
(C3), and (C4) re.ect how .nand .n represent the cumulative job-sets; (C2) implies that w is always the 
smallest possible magnitude for representing these job-sets. (C5) is di.erent in .avor from the previous 
prop­ erties: it establishes the relation between the weight of Lß and of the job sizes at the boundary 
of Lß, and Sß, according to De.nition 2.1. It is easy to verify that for every set Pi = Ai \ Ai-1 (and 
corresponding w) in a canonical schedule a unique µ>A exists that ful.ls (C5), and vice versa, that due 
to (C5) any con.guration ß =(Lß,Sß) represents a 5-division. Finally, we stress that the sets Ai do not 
possess a 5-division, and a single size vector .n does not represent an (L, S) division at all. 4 The 
directed graph HI In this section, for arbitrary input instance I, we de.ne a directed, layered graph 
HI . All vertices of this graph are con.gurations, selected, numbered, and chained to form the graph 
in an appropriate way. First, for an arbitrary con.guration ß, we de.ne a set Scale(ß) of con.gurations. 
These are the pos­sible con.gurations of an end-vertex of any arc with a starting vertex having ß as 
con.guration. In par­ticular, if ß =(w, µ, .no, .n1),f =(w ,µ ,.n o, .n 1), and o f . Scale(ß), then 
.n must represent the same job 1 set Ai, as .n, from the point of view of a (possibly) in­creased magnitude 
w and a (possibly) increased middle size µ. Whenever w >w, this involves collecting the re­maining jobs 
of classes that become tiny, and remaining tiny blocks, and forming the proper number of new big­ger 
tiny blocks out of this job set. Importantly, the size dw of tiny blocks at least doubles (unless unchanged) 
as we proceed to faster machines. This fact prevents that rounding errors in the total (tiny) job size 
cumulate to an unpredictable error (see also [8]). Furthermore, the middle size µ is allowed to increase 
if all large jobs in 11 the class µ have been allocated, that is, n= n. We µ. µm omit the exact de.nition 
of Scale(ß). The vertices of HI (i.e., the con.gurations) are arranged in m layers, and in levels I and 
II, which are orthogonal to the layers. The con.gurations on level I must have an empty set of small 
jobs, i.e., Sß = =, and here the layers {m-2,m-1,m} are empty. Level II has m - 1 real layers, and we 
add a single dummy vertex vm adjacent to every vertex on layer m - 1, that alone 7 forms the last layer 
m.In general, the ith layer stands for the ith set Pi. Any directed path of m nodes leads to vm over 
the m layers, and from level I (or II) to level II. Such a path we call an m-path. The m-paths represent 
partitions of the input PI . For a given m-path, the very .rst vertex on level II is in some layer k 
. m - 2; we will call it the switch vertex, and k the switch machine. (Note that k is thus the .rst machine 
possibly receiving small jobs.) We shall denote the vertices on the two levels by VI , and VII , respectively. 
Notation 2. For any directed path (v1,v2,...vr), the corresponding con.gurations of the nodes will be 
denoted by (ß1,ß2,...,ßr). In what follows, we discuss about the last three sets Pm-2,Pm-1,Pm of the 
partition. First of all, observe that for an m-path the last con.guration ßm-1 U m-1 alone represents 
Ph. Thus, we can use ßm-1 to h=1 7More precisely, we will unite layers m - 2 and m - 1, and use double 
vertices in the united layer, but it simpli.es the discussion to think of these as pairs of individual 
vertices.  uniquely determine the hidden con.guration ßm (not appearing explicitely in the path). We 
de.ne ßm to have wm = wm-1,µm = µm-1, and all jobs not allocated in U m-1 Ph. In particular, note that 
ßm includes all jobs h=1 of class higher than Am-1, and that this job set can be handled as a single 
huge chunk without violating the running time bounds. For technical reasons we overestimate the amount 
of tiny blocks on m and set P |Cl| n1 = l +3. We omit the details. . dw Furthermore, our monotonicity 
argument requires that even the last three workloads ßm-2,ßm-1, and ßm be dealt with as a single entity. 
Among other restrictions, we will demand that either all of them have the same magnitude, and therefore 
use w= m-1 wm-2 instead of wm-1, or that wm-2 is much smaller than wm-1, so that all jobs on m - 2 (if 
exist), are tiny for machines m - 1 and m. Correspondingly, ßm-2 and ßm-1 of any path must adhere to 
either type (A) or (B) as speci.ed next. Observe that in case (A) on the last three machines, resp. in 
case (B) on the last two machines the size of tiny jobs mentioned is the same (i.e. well-de.ned). (A) 
wm-2 >d2 · wm-1; in this case we modify the last magnitude to be w:= wm-2, and require m-1 |ß m-2|.| 
ß m-1|.|ß m|, and |ßm-2|.|ßm-1|. |ßm|; moreover, (i) either all tiny jobs (measured by blocks) are on 
machines m - 1 and m, or (ii) machines {m - 2,m - 1,m} have at least 18 tiny blocks, and at least two 
of them have each at least 6 tiny blocks.  (B) wm-2 . d2 · wm-1; then |ß m-1|.|ß m|, and |ßm-1|.|ßm|, 
and (i) all machines but {m - 1,m} are empty, or (ii) m - 1 and m together have at least 6 tiny blocks. 
 The requirements (A) and (B) can be included in the graph de.nition, e.g., by using (polynomially many) 
special double vertices vwith double con.gurations m-2 (ßm-2,ßm-1) in the layers (m - 2,m - 1) on level 
II. Applying w:= wm-2 >d2 · wm-1 can be done by m-1 using size vectors of triple length for the double 
vertices of type (A). Clearly, all restrictions can be represented by the con.gurations (ßm-2,ßm-1). 
The following de.nition (sketch) of graph HI is independent of the speed vector s, and depends only on 
the job set PI . We assume, w.l.o.g. that m . 3, otherwise we include a machine of speed (close to) 0. 
Definition 4.1. (graph HI ) HI (V, E) is a directed graph, where every vertex v Øvm is a triple v = = 
(d, i, ß), so that d .{I,II}, and i . [1,m - 1], and ß =(w, µ, .no, .n1) is a con.guration. o (V1) if 
i =1, then for l Øµ, µ +1 n= = l 0, while for o oo l .{µ, µ +1} n=0 and n= n; l. lm ls (V2) if d = I, 
then i . [1,m - 3], and Sß = =; There is an arc from v =(d, i, ß) to v =(d ,i +1,f) if an only if (E1) 
f . Scale(ß), and (E2) |Lß|.|L1 |, and (E3) d . d ; Finally, for d = II, and combined layers (m - 2,m 
- 1), include double vertices v with double con.gurations (ßm-2,ßm-1) so that for (ßm-2,ßm-1,ßm) the 
require­ments (E1) (E2) and either (A) or (B) hold. Next we assign a weight to each vertex, called .nish 
time, and de.ne the makespan of a path accordingly. Obviously, these values do depend on the machine 
speeds s. Definition 4.2. (finish time of a vertex) Let v =(d, i, ß) be a vertex of HI , where ß =(w, 
µ, .no, .n1). |ß|+dw The .nish time of v is then f(v)= if ß does si o |ß| have tiny blocks (n<n1 ), and 
f(v)= otherwise. . .si Definition 4.3. (makespan of a path) Let Q = (v1,v2,...,vr) be a directed path 
in HI . If QC VI , or QC VII , then the makespan of Q is M(Q)= r maxf(vh). If Q is an m-path with switch 
vertex vk = h=1 (II, k, ßk), then M (Q) = max{ |ßk| , maxh= k f(vh)}. sk The following theorem, saying 
that an m-path having makespan close to the optimum makespan of the scheduling problem always exists, 
is a consequence of Theorem 2.1. The obligatory amount of tiny blocks (or all tiny jobs for wmax) are 
collected at the beginning and later allocated to the 3 fastest machines to ful.l (A) or (B). The rest 
of the proof is rather straightforward, and requires a technical translation of real schedules to m­paths 
of HI , which involves creating blocks of size dwi from the actual tiny jobs. Theorem 4.1. For every 
input I =(PI ,s) of the scheduling problem, the optimal makespan over all m­paths in HI is at most OP 
T · (1 + O(5)), where OP T denotes the optimum makespan of the scheduling prob­lem.  5 The deterministic 
algorithm This section describes the deterministic monotone algo­rithm, in form of two procedures and 
the main algorithm Ptas. We will make use of an arbitrary .xed total or­der n over the set of all con.gurations 
ß, such that con.gurations of smaller total workload |ß| are smaller according to n . One can easily 
check that every m-path in the graph represents a canonical allocation (extending this con­cept to tiny 
blocks). Among the m-paths of HI , the al­gorithm selects an m-path having minimum makespan, as the primary 
objective. Among paths of minimum makespan, we maximize the index of the switch ma­chine k. A further 
order of preference, is to be of type (A), then (B). This selection of the optimal path is done by Procedure 
OptPath (see Appendix A). On the high level, this procedure is a common dynamic pro­gramming algorithm 
that .nds an m-path of minimum makespan in HI . However, we do not simply proceed from left to right 
over the m graph layers, but select an optimal path from the .rst layer to every node in VI , and similarly, 
an optimal path from layer m to each node in VII . Finally, we test each vertex in VII to provide a po­tential 
switch vertex (i.e., we .nd optimal paths leading to the switch vertex from both end-layers). When the 
makespan of two pre.x (or su.x) paths is the same, we break ties according to n . We choose a switch 
vertex vk providing optimum makespan, and of maximum pos­sible k. The case k = m - 2 needs careful optimization. 
Roughly, we choose deterministically by some .xed or­der of the double con.gurations (ßm-2,ßm-1), but 
min­imize the makespan on the last three machines by re­distributing the tiny blocks. The .exibility 
provided by three machines with many tiny blocks, facilitates monotone allocation in this degenerate 
case as well. Once an optimal m-path is found, we have to allocate the jobs of PI to the machines. This 
is obvious for jobs that appear individually in some con.guration of the path, but we need an accurate 
description of how the tiny jobs are distributed, given the block representation. Procedure Partition 
is detailed in Appendix A. Importantly, depending on whether the switch machine k is .lled high (above 
(1 - a/2) times the makespan) or low, it gets .lled with tiny jobs below, resp. above |ßk|. This, again, 
will play an important role when showing monotonicity. Distributing the tiny jobs when k = m - 2, is 
a slightly more subtle procedure, operating with the same principle (a low machine is .lled over |ßi|). 
In general, by having a careful look at Partition, one can see that the machines never get .lled above 
the makespan of the input path, that is, |Qi| . M(Q). This is trivial for machines without tiny si jobs, 
and follows from the de.nition of .nish time with Algorithm 1 Ptas Input: machine speeds .1 . .2 . ... 
. .m, and job set PI = {p1,p2,...,pn}, desired precision a. Output: A partition P1,P2,...,Pm of PI . 
 1. for each i . [1,m], round the speed .i up to the nearest power of (1 + a); 2. based on the jobs 
PI , rounded speeds s1 . s2 . ... . sm, and an appropriate 5 . a, construct the graph HI ; 3. run Procedure 
OptPath in order to obtain the optimal m-path Q =(v1,...,vm); 4. from Q compute the partition Q1,Q2,...,Qm 
by Procedure Partition; 5. let P1,P2,...,Pm be the sets of {Q1,Q2,...,Qm}, sorted by increasing order 
of weight |Qi|; output P1,P2,...,Pm.  Figure 2: The deterministic monotone Ptas. the extra tiny block, 
for other machines. Finally, the monotone Ptas is presented in Fig­ure 2. A substantial property of the 
output is that on machines i < k, the workloads Qi do not get permuted in step 5. of Ptas (see also Appendixapp:mono). 
This is due to the fact that the sets Li of large jobs are in­creasing by (E2). On the other hand, the 
machines i>k have .nish time close to the makespan M(Q) (resp. .n­ish time of small di.erence in (A)). 
As a consequence, we obtain that in step 5. the sets Qi are permuted only among machines i . k of equal 
rounded speed si. There­fore, even for the permuted workloads, |Pi|/si . M(Q) holds for all i. This, 
in turn, together with Theorem 4.1 implies the approximation bound: Theorem 5.1. For arbitrary input 
I, and any given 0 <a . 1, the deterministic algorithm Ptas out­puts a (1 + 3a)-approximate optimal allocation 
in time P oly(n, m). 5.1 Monotonicity A high-level formulation of the monotonicity argument is the following. 
Since the work­loads |Pi| of the output partition are non-decreasing, it is easy to see that it su.ces 
to prove monotonicity in the special case when a single rounded speed si = (1+ a) is reduced to si =1. 
Let the output path of OptPath be Q in the .rst, and Q in the second case, more­over let f and M denote 
.nish time and makespan for input (si,s-i) and f and M for input (si,s-i), re­spectively. Our argument 
is based on the following ob­servation. The makespan of any (sub)path in HI can­not decrease by reducing 
a machine speed. The objec­tives concerning the optimal path are de.ned so that  Q Ø = Q may occur only 
if machine i becomes a bottle­neck machine, either concerning the whole path (mean­ing M(Q) <M (Q)= f 
(vi)), or some relevant subpath (e.g., pre.x path, or the subpath (vm-2,vm-1,vm)). In any of these cases, 
Q is a possible solution of (local) makespan f (vi), and Q is preferred only in case the re­spective 
(sub)path of Q has no higher (local) makespan than f (vi), implying that i gets not more workload than 
f (vi) = (1+ a)f(vi). This proves monotonicity if (1+ a)f(vi) is the exact workload, or at least a lower 
bound on what i received with the original speed (e.g., if i<k). On the other hand, if (1+a)f(vi) was 
just a 5­estimate, then the machine was close to full with input si, and, by reallocating many or all 
tiny blocks, the new makespan (no matter how the output path looks like!) becomes much smaller than f(vi)(1 
+ a). The other way round, the output path Q changes to QØ= Q only if i became a bottleneck machine w.r.t. 
some relevant subpath. As long as all considered subpaths remain optimal, there is no reason to change 
by some other optimization criterion: a better path Q would have been better than Q with input si as 
well. Our main result is thus the following theorem. See Appendix B for a more detailed proof. Theorem 
5.2. Algorithm Ptas is monotone. References [1] Nir Andelman, Yossi Azar, and Motti Sorani. Truth­ful 
approximation mechanisms for scheduling sel.sh re­lated machines. In 22nd Annual Symposium on The­oretical 
Aspects of Computer Science (STACS), pages 69 82, 2005. [2] Nir Andelman, Yossi Azar, and Motti Sorani. 
Truth­ful approximation mechanisms for scheduling sel.sh related machines. Theory of Computing Systems, 
40(4):423 436, 2007. [3] Aaron Archer. Mechanisms for Discrete Optimization with Rational Agents. PhD 
thesis, Cornell University, January 2004. ´ for one-parameter agents. In 42nd Annual Symposium on Foundations 
of Computer Science (FOCS), pages 482 491, 2001. [4] Aaron Archer and Eva Tardos. Truthful mechanisms 
[5] Vincenzo Auletta, Roberto De Prisco, Paolo Penna, and Giuseppe Persiano. Deterministic truthful approx­imation 
mechanisms for scheduling related machines. In Volker Diekert and Michel Habib, editors, STACS, volume 
2996 of Lecture Notes in Computer Science, pages 608 619. Springer, 2004. [6] George Christodoulou and 
Elias Koutsoupias. Mecha­nism design for scheduling. Bulletin of EATCS, 97:39 59, February 2009. [7] 
Peerapong Dhangwatnotai, Shahar Dobzinski, Shaddin Dughmi, and Tim Roughgarden. Truthful approxima­tion 
schemes for single-parameter agents. In FOCS, pages 15 24, 2008. [8] Leah Epstein and Jiri Sgall. Approximation 
schemes for scheduling on uniformly related and identical par­allel machines. Algorithmica, 39(1):43 
57, 2004. [9] Dorit S. Hochbaum and David B. Shmoys. A polyno­mial approximation scheme for scheduling 
on uniform processors: Using the dual approximation approach. SIAM J. Comput., 17(3):539 551, 1988. [10] 
Annam´aria Kov´acs. Fast monotone 3-approximation algorithm for scheduling related machines. In Algo­rithms 
-ESA 2005, 13th Annual European Symposium, pages 616 627, 2005. [11] Annam´aria Kov´acs. Tighter approximation 
bounds for lpt scheduling in two special cases. J. Discrete Algorithms, 7(3):327 340, 2009. [12] Roger 
B. Myerson. Optimal auction design. Mathe­matics of Operations Research, 6(1):58 73, 1981. [13] N. Nisan, 
T. Roughgarden, E. Tardos, and V.V. Vazi­rani. Algorithmic Game Theory. Cambridge Univer­sity Press, 
2007. [14] Noam Nisan and Amir Ronen. Algorithmic mechanism design (extended abstract). In Proceedings 
of the Thirty-First Annual ACM Symposium on Theory of Computing (STOC), pages 129 140, 1999. [15] Noam 
Nisan and Amir Ronen. Algorithmic mechanism design. Games and Economic Behavior, 35:166 196, 2001. Appendix 
A The procedures OptPath and Partition The pseudo-codes are presented in Figures 3 and 4. Observe, that 
by the de.nition of M() and opt() values, M(vk)= M(Q). Moreover, since in each case the pointers pred() 
and succ() determine an incoming, and an outgoing path of minimum makespan, respectively, we can make 
the following observation: Observation 1. For all v . VII , the value M(v) is the minimum makespan over 
all m-paths having vertex v as switch vertex. Consequently, M(vk)= M(Q) is the minimum makespan over 
all m-paths of HI . B Monotonicity The next lemma is a descriptive characterization of the .nal output 
allocation P1,...,Pm to be readily used in the monotonicity proof.  Procedure 2 OptPath Input: The directed 
graph HI . Output: The optimal m-path Q =(v1,...,vm) of HI . 1. for every double vm-2 =(ßm-2,ßm-1) . 
VII do opt(vm-2) := max{f(vm-2),f(vm-1),f(vm)} (where ßm-1 determines ßm, see Section 4) |ßm-1||ßm| M 
(vm-2) := max{ |ßm-2| ,, }; sm-2 sm-1 sm for i = m - 3 downto 1 do for every v =(d, i, ß) . VII do (i) 
succ(v) := w, if opt(w) is minimum over all vertices y with (v, y) . E, and among such vertices the con.guration 
ß of w is n-minimal. (ii) opt(v) := max{f(v), opt(succ(v))};  M(v) := max{ |ß| , opt(succ(v))}. si 
2. for every v =(d, 1,ß) . VI do opt(v) := f(v); for i =2 to m - 2 do for every v =(d, i, ß) . VI u VII 
do (i) pred(v) := w . VI , if opt(w) is minimum over y . VI with (y, v) . E, and among such vertices 
the con.guration ß of w is n-minimal. (ii) if v . VI then  opt(v) := max{f(v), opt(pred(v))}; if v 
. VII then M(v) := max{M(v), opt(pred(v))}. 3. select an optimal switch vertex vk =(II, k, ßk) . VII 
, by the following objectives: (i) M(vk) = min{M(v) | v . VII }; (ii) k is maximum over all v of minimum 
M(v);  (iii) if k . m - 3 then ßk is minimal wrt. n over all v of minimum M(v) in layer k; (iv) if k 
= m - 2, then among all double vertices vm-2 =(ßm-2,ßm-1) of minimum M(vm-2) select vk = vby the following 
objectives: m-2 (a) keep the order (A) (B) (cf. Section 4); (b) in case of (A), select an ( ßm-2,ß m-1,ß 
m, (Tßm-2 u Tßm-1 u Tßm )) (i.e., with a common pool of tiny blocks) by some prede.ned ordering, and 
then minimize (also) the second highest .nish time among  |ßm-2||ßm-1||ßm| ,, and (by redistributing 
sm-2 sm-1 sm the tiny jobs); (c) in case of (B), select an (ßm-2,ßm-1,ßm) by some prede.ned ordering, 
but so that |ßm-1| + |ßm| is maximized; 4. for i = k - 1 downto 1 do vi := pred(vi+1); for i = k +1 to 
m do vi := succ(vi-1); Q := (v1,...,vk,...,vm). Figure 3: Procedure OptPath .nds an m-path Q of minimum 
M(Q) in the directed graph. Lemma B.1. Let PI denote the set of input jobs and s be the vector of rounded 
speeds. Let Q =(v1,...,vm) be the path output by OptPath with con.gurations (ß1,...,ßm) in the vertices, 
and vk =(II, k, ßk) be the switch vertex of the path. If P1,...,Pm is the .nal partition output by Ptas, 
then (a) for i<k, Pi = Qi = ßi. Moreover, if k . m - 3, then |Pi| (b) for i > k, . (1 - 65) · M(Q); si 
(c) for k either Pk = Qk or (b) holds. Lemma B.2. In case k = m - 2, step 3b. of Partition can be realized 
so that |Qm-2|.|Qm-1|.|Qm| holds. Corollary B.1. In step 5. of Ptas, the sets Qi are permuted only among 
machines i . k of equal rounded speed, and only if k . m - 3. Consequently, |Pi|/si . M(Q) for all i. 
Below we show an incomplete proof of the mono­tonicity, in that we omit the proof of Lemma B.3, treat­ing 
the case k = m - 2. The omitted proof is neither longer nor more di.cult than the presented part; it 
op­erates with the same type of arguments, and needs a slightly more complex case analysis. Theorem 5.2. 
Algorithm Ptas is monotone. Proof. Assume that machine i alone decreased its speed .i to .i in the input. 
If the vector of rounded speeds (s1,s2,...,sm) remains the same, then the determin­istic Ptas outputs 
the same allocation, and i receives the same, or smaller workload, since the output work­loads P1,P2,...,Pm 
are in increasing order. Assuming that the rounded speed si decreased as well, it is enough to consider 
the special case when i is the .rst (small­est index) machine of rounded speed si = (1+ a) in input I(P, 
s), and after reducing its speed, it becomes the last (highest index) machine of rounded speed 1 in input 
I (P, s ). Since the workloads in the .nal alloca­tion P1,...,Pm are ordered, this implies monotonicity 
for every one-step speed change (like (1 + a) -1). Monotonicity in general can then be obtained by ap­plying 
such a step repeatedly. Note that for both in­puts the algorithm constructs the same graph, indepen­dently 
of the speed vector. We assume that with in­puts I, and I OptPath outputs Q =(v1,...,vm), and Q =(v1,...,v), 
where the switch vertices have index m k and k, respectively. Finish time, makespan, etc. wrt. the new 
speed vector s are denoted by f (),M () etc. We prove that |Pi|.|Pi |.  Procedure 3 Partition Input: 
The job set PI , and an m-path Q =(v1,...,vm) with switch vertex vk in the graph HI . Output: A partition 
Q1,Q2,...,Qm of the set PI . Case Low-k : |ßk|/sk . (1 - a/2) · M(Q) Case High-k : |ßk|/sk > (1 - a/2) 
· M(Q). 1. for i =1 to m do Qi := ßi; U m 2. let T = {t1,t2,t3,...,tµ} = PI \ Qi, so that i=1 the jobs 
tj are in increasing order; 3a. if k . m - 3, then let W = 0 and r = 0; for i = k to m - 1 do given ßi 
=(w, µ, .no, .n1) and A = log dw, let 1 o Wi := (n- n) · dw; .. (i) W := W + Wi; (ii) if High-k then 
let u be the maximum index  u in T so that j=1 tj . W ; if Low-k then let u be the minimum index in 
 u T so that j=1 tj . W ; (iii) Qi := Qi u{tr+1,tr+2,...,tu}; (iv) r := u. Qm := Qm u{tr+1,tr+2,...,tµ}. 
3b. if k = m - 2, then start with an allocation of tiny jobs to {m - 2,m - 1,m} (in the given order) 
s. t. each machine i gets at most |Tßi | amount of tiny jobs (this is doable, because the total number 
of tiny blocks is overestimated by 3 blocks in ßm) |ßm-1||ßm| let M = max{ |ßm-2| ,, }; sm-2 sm-1 sm 
call i .{m - 2,m - 1,m} low, if |ßi|/si . (1 - 2a/3) · M, and high if |ßi|/si . (1 - a/2) · M; Correct 
the partition of tiny jobs (with keeping the job order) so that (i) if there is one low machine i, and 
two high machines, then i receives at least |ßi| work; (ii) if there are two non-high machines, then 
both receive at least 6dw work of tiny jobs.  Figure 4: Procedure Partition allocates the jobs based 
on path Q output by OptPath. We start with a simple observation. Since we decreased a machine speed, 
it follows from the de.nition of makespan that for any path R =(v1,v2,...,vr) within level VI , or within 
level VII , and for any m­path, M (R) . M(R). Similarly, for any vertex v, opt (v) . opt(v), and for 
any v . VII M (v) . M(v) (cf. Procedure OptPath). Obviously, also the optimum makespan over all m-paths 
could not decrease. We elaborate on the subtle case of k = m - 2 in a separate lemma; in what follows, 
we assume k . m - 3. CASE 1: M (Q) >M(Q) In this case machine i with the new rounded speed si =1, becomes 
a bottleneck in path Q. That is, |ßi|(+dw) M (Q)= f (vi)= . 1 If i < k, then ßi = Pi, so the machine 
received exactly |ßi| work with speed si, and now Q is a path with makespan |ßi|, so by Corollary B.1, 
and by the optimality of Q we have |Pi | = |Pi |/si . M (Q ) . M (Q)= |ßi| = |Pi|. def Let us introduce 
the notation B = (1 - 65) · M(Q) for the lower bound in Lemma B.1 on .nish times. Recall that si = (1+ 
a). Assume now that i . k and |Pi|/(1 + a) . B. Due to (E2), for the job partition Q1,...,Qm (before 
ordering the sets by size), it holds that |Qh|. |Lßi | for every h . i. Therefore, for the m - i + 1st 
largest set Pi, we have |Pi|.|Lßi |, and so |Pi|. max{|Lßi |, (1 + a) · B}. We modify the path Q and 
construct a new path Q by putting small jobs from Sßi (of machine i) onto machine i +1, until Sßi becomes 
empty, or the moved jobs have total weight of at least (a/3)·(1+a)·M(Q). For the new .nish time we have 
f (vi ) . max{|Lßi |, (1+a)· M(Q)(1-a/3) . max{|Lßi |, (1+a)·B}.|Pi|. If i = m, then we put only tiny 
blocks of the common magnitude wm-1 onto m - 1, and use |ß m| instead of |Lßi | in the calculation. It 
is easy to see that with speed si = 1 machine i is still a bottleneck machine in path Q , since it is 
.lled up to about (1+2a/3) · M(Q), while other machines are .lled not higher than (1 + a/3) · M(Q), even 
with the jobs received from i. Thus, f (v ) is an upper bound on the new optimal path-makespan, and so 
on |Pi |, while it is less than |Pi|. By Lemma B.1, it remains to consider the case i = k, and Pi = Qi. 
Given M (Q) >M(Q), we have M (Q ) . M (Q)= |ßi| as an upper bound on |Pi |. Assuming that for i = k Low-k 
holds with speed si, |Pi| = |Qi|.|ßi| by Partition 3a, and we are done. Assuming High-k, |Pi| = |Qi|. 
max{|ß i|, |ßi|- dwi}. On the other hand, |ßi| > (1 - a/2)M(Q) · (1 + a) > (1 + a/3)M(Q). By putting 
one tiny block onto the next machine (if there are any), we still obtain a path Q , so that max{|ß i|, 
|ßi|- dwi} = |ßi | = M (Q ) . M (Q ) .|Pi |, and we are done. Note that here again we used that with 
ßi machine i is still bottleneck, by |ßi| > (1 + a/3)M(Q), and so |ßi | is the makespan.  As follows 
from Lemma B.1 (b), the only remaining case is CASE 2. M (Q)= M(Q), and i . k. If M (Q)= M (Q), and Q 
= Q , then the output of Partition can only be di.erent if i = k. Since the makespan did not change, 
and sk decreased, the change is from Low-k to High-k, and machine i = k receives less work with sk, by 
Partition 3a. If, on the other hand, Partition outputs the same partition, then every machine gets the 
same workload. Suppose M (Q)= M(Q), but Q =ØQ . Since Q has minimum makespan, M (Q )= M (Q). We claim 
that also k = k. Otherwise Q would have been better than Q for input s as well, because M(Q ) . M (Q 
)= M(Q) and k >k. Similarly, also vk = vk, otherwise ß n ß would hold, and Q would have been better for 
input s as well. Now, since QØ = Q , a maximum h<k exists so that vh Øvh. This means that pred(vh+1) 
Øpred (vh+1). == If vh was preferred in Q because ßh n ßh, then opt(vh) < opt(vh) . opt (vh) . opt (vh). 
The .rst inequality holds, otherwise vh = pred(vh+1) would have been the choice. The second holds for 
every vertex. The third holds, otherwise vh = pred (vh+1) would have been the choice of OptPath. Similarly, 
if v h was preferred in Q because opt (v) < opt (vh), then h opt(vh) . opt(vh) . opt (vh) < opt (vh). 
In both cases we obtained opt(vh) < opt (vh). Recall that opt(vh) is the optimum makespan over all paths 
leading to vh from layer 1. This could strictly increase only if i . h, and i with workload ßi = Pi and 
speed si = 1 became a bottleneck machine in (v1,v2,...,vh). Therefore, |Pi |. opt (vh) . opt (vh) . |ßi| 
= |Pi|. t s i Lemma B.3. If on input I(P, s), for the output path Q of OptPath the switch machine is 
k = m - 2, then |Pi|.|Pi |.  
			