
 Extracting Classification Knowledge of Internet Documents with Mining Term Associations: A Semantic 
Approach Shian-Hua Lin, Chi-Sheng Shih*, Meng Chang Chen*, Jan-Ming Ho*, Ming-Tat Ko*, and Yueh-Ming 
Huang Department of Engineering Science, National Cheng Kung University, Tainan. Taiwan Institute of 
Information Science, Academia Sinica, Taipei, Taiwan* Abstract In this paper, we present a system that 
extracts and generalizes terms from Internet documents to represent classification knowledge of a given 
class hierarchy. We propose a measurement to evaluate the importance of a term with respect to a class 
in the class hierarchy, and denote it as support. With a given threshold, terms with high supports are 
sifted as keywords of a class, and terms with low supports are filtered out. To further enhance the recall 
of this approach, Mining Association Rules technique is applied to mine the association between terms. 
An inference model is composed of these association relations and the previously computed supports of 
the terms in the class. To increase the recall rate of the keyword selection process. we then present 
a polynomial- time inference algorithm to promote a term, strongly associated to a known keyword, to 
a keyword. According to our experiment results on the collected Internet documents from Yam search engine, 
we show that the proposed methods In the paper contribute to refine the classification knowledge and 
increase the recall of keyword selection. 1. Introduction Recently, Information Retrieval (IR) systems 
have become popular due to plentiful information services available on the Internet. The IR systems are 
designed to facilitate rapid retrieval of information for diverse users using Internet browsers. By applying 
keyword-based index/search approaches, terms in the document are extracted, indexed and stored in the 
database. Then the index is employed to rapidly retrieve relevant documents matched the terms in a query. 
However, the conceptual gap between document authors and the Internet users makes the retrieval results 
from many conventional IR systems far from user s information needs so that both recall and precision 
are low. Due to various usage of words, authors and users frequently represent the same semantic with 
different terms, or describe different meanings by the same word. Therefore, in IR systems, positive 
term-matched documents are probably wrong answers, such as the term apple has different meanings in Apple 
computer and apple pie . Therefore, many Internet keyword-based search In the paper, we use term as word 
or phrase, which is extracted from the Internet documents, rather than use keyword . Keyword is representative 
to the concept, while term is probably non-representatlve. This work was supported pa in part by NSC 
grant NSC 87-2213-E-00 1 -W3. Permission to make digitalihard copy of all or part of this work for persona1 
or classroom use is granted without fee provided that copies are not made or distributed for profit or 
commercial advantage, the copyright notice, the title of the publication and its date appear, and notice 
is given that copying is by permission of ACM, Inc. To copy otherwise, to republish, to post on servers 
or to redistribute to lists, requires prior specific permission and/or fee. SIGIR 98, Melbourne, Australia 
0 1998 ACM l-581 13-015-5 S/98 $5.00. engines usually retrieve hundreds of documents with the desired 
few. In contrast, negative matches may be right. For instance, the terms airline schedule , which does 
not match with query term flight schedule . However, both terms have the same semantics. A thesaurus 
database of terms for a specific domain is probable able to handle the problem. However, because of diverse 
topics of documents with different culture background on the Internet, no static thesauri can cover the 
shifting semantics to deal with the term mismatch problem. From the perspective of retrieval efficiency, 
keyword-based IR systems are useful to handle a large document-base. However, documents collected from 
the Internet are extremely numerous. In such case, for a query with two words* submitted to a search 
engine implemented with VSM (vector space model) [3], thousands of documents are probably retrieved for 
the query. In addition, ranking of so many documents according to one or two words is not always meaningful. 
Therefore, Internet IR systems should be able to efficiently shrink the size of retrieval and to enhance 
the semantic of documents ranking. Assigning classifications to documents is essential to the efficient 
management and retrieval of knowledge [2]. In our system, for every class, the classification knowledge 
is learned from the training data, which are Internet documents collected and manually classitied by 
Yam3. By mining and inferring term associations in each class, the hidden semantics of terms are discovered. 
With the classification knowledge, a two-phase search engine is proposed that performs class-level search 
first, and then object-level search. In this way, a linear ranking model of documents used in conventional 
IR systems becomes a structured ranking model. In the rest of the paper, we discuss related work of IR 
and mining association rules in Section 2. In order to illustrate our ideas clearly, our system, Automatic 
Classifier for Internet Resource Discovery (ACIRD), is described in Section 3. In Section 4 and Section 
5, we present the details of mining term associations and applying associations to efficiently refine 
the semantics of terms in a class. From the experiments on the actual categorization of Yam in Section 
6, we found that the proposed approaches meet the claimed goal. Finally, we conclude the contributions 
and point out related future work.  2. Related Work The past studies in IR systems improved retrieval 
efficiency by applying techniques of index and query formulation. According to index methods of documents, 
we can categorize IR systems into two types of systems. According to the statistics in [4], the average 
query length is 1.3 words. 1 htto.//WWW., which is very popular search engine in Taiwan. Full-text search 
systems Full-text search technology has been used in Information Retrieval for many years, which is applied 
to find matched strings and sub-strings in the documents. Some string-index technologies, such as PAT-tree 
[ 1 I], are proposed to improve the performance of various search functions, such as prefix searching, 
proximity searching, range searching, longest repetition searching, most significant and most frequent 
searching, and regular expression searching [lo]. These searching functions are rarely used on the Internet 
environment; the improvement is seldom used in the Internet. As a novel application, PAT-tree can be 
used to discover new Chinese terms such as names or terminology [12]. The main disadvantage of full-text 
search is its inefficient retrieval. Another drawback is low recall and precision rate, since this approach 
only finds documents with exact matched strings that is usually far from user s information needs. Keyword-base 
search systems Keyword-based search systems usually extract terms from documents based on a pre-constructed 
dictionary, and index the terms in inverted file [l3]. The extracted terms are regarded as keywords and 
used to represent the document. When a query consisting of one or more terms is submitted to the system, 
the index is applied to rapidly locate documents that contain these terms. Once documents are retrieved, 
a VSM-based [3] similarity measurement is employed to rank the documents. A popular ranking algorithm, 
TFxIDF [ 141, combines Term Frequency and Inverse Document Frequency to estimate the metric of relevance 
between the terms of documents and the query. Those systems can search relevant documents efficiently. 
However, the index size is usually larger than the size of original documents. Huge physical memory (up 
to several giga bytes) is required to retain good performance. Otherwise, the page faults of operating 
system will dramatically degrade the searching performance. Another disadvantage of the systems is that 
queries with non-indexed terms only can not find any documents. Thus, a complete dictionary with the 
capability of automatically discovering new terms is critical for keyword-based systems. For the example 
of Chinese IR environment, the terminology and names are formed by more than one Chinese character and 
most of them do not exist in the dictionary. An automatic identification of the names is demanded. Mining 
Association Rules Mining association rules [5,6,7] is applied to discover the important associations 
among items in transactions. A well- known application of item association mining is to find an optimal 
item arrangement in the supermarket to allow customs to collect the needed grocery conveniently. According 
to the definition of association rule in [6], elements in the problem are items, transactions, and the 
database. Let I={i,,i,,...,i,,,} beasetofitemsand D be a set of transactions (the transaction database) 
in which each transaction T is a set of items such that T c 1 An association rule is an implication of 
the form x + Y , where X C / , Y C 1, and X n Y = 4. The rule _&#38; + y holds in the transaction set 
D with confidence c, if co/o of transactions in D that contains X also contains Y . The rule X + Y has 
support s in the transaction set D ifs% of transactions in D that contain X u j We follow the above definition 
and map the problem of mining term associations to the specification. The detail of mining term associations 
is described in Section 4.  3. The ACIRD System The system, Automatic Classifier for Internet Resource 
Discovery (ACIRD4) [9], is implemented to automatically classify documents collected by Yam Web Server. 
The system is motivated to improve the poor performance of the current manual classification. The classification 
process of ACIRD is composed of two phases, training phase and testing phase. In the first phase, documents 
with their manually assigned classes in Yam are employed as the training set to learn the classification 
knowledge of the classes in the lattice. Then the newly collected documents manually categorized by Yam 
s experts are applied to verify the learned classification knowledge in the second phase. Results of 
the current training and testing phase is shown in Figure I. Given a document with a target class among 
5 I3 classes of Yam, the accuracy of exact match (Topl) is 13.2%. The probability that the target class 
appears in the first 5 classes (Tops) among 513 classes is about 30.5%. If we apply the same classification 
process to classify Level-l classes of There are 12 main categories in Yam s homepage (http://taiwan.iis.sinica.edu.tw/en/yam/): 
Arts , Humanities , Social Sciences , Society and Culture , Natural Sciences , Computer and Internet 
, Health , News and Information . Education . Government and State , Companies . and Entertainment and 
Recreation . Yam , the result is shown in Figure 2. Among 12 Level-l classes, the accuracy of exact 
match (Top]) is 53.9%, and the probability of accurate classification to the half of Level-l classes 
(Top6) is about 86.1%. Currently, the automatic classifier is used as assistance to Yam s colleagues 
to speed up the performance of categorization process. Learning by advice is also applied to refine the 
classification knowledge during the period of manual interactions. In the following, we describe the 
system based on the issues of IR systems [IO]: document operation, conceptual model, term operation, 
file structure, query operurion, and hardware. The following terminology is used throughout the paper. 
Class corresponds to a Yam s category. The classification knowledge of a class is represented by a group 
of keywords. Object corresponds to an Internet document (Web page). Term is the word or phrase extracted 
from objects or generalized into classes by the learning process. Support is the improtance degree of 
a term that supports some object or class. The value is normalized to [0, 11. Keyword corresponds a representative 
term, i.e., its information quality is better than a term Memership grade (MG) is the supporting degree 
of a keyword to some object or class as if support is the supporting degree of a term. Document Operation 
Each document (object) is assigned a unique OID (object ID) in the database. An object can be instantiated 
from several classes according to Yam s current categorization. Before performing the classification 
process, objects are preprocessed into terms with supports su,,,,,,, , l, is a term extracted from object 
0. Web document authors highlight the content with HTML tags, such as title, heading, list, table, italic, 
bold, underline, etc. Additionally, META tag allows users to add extra information for the documents 
such as classification and keyword . Apparently, HTML tags provide significant information to index and 
classification. We categorize HTML Tag into four types: I, Informative: Contents in these tags are regarded 
most important, such as <META Classification Keyword>, <TITLE>, <HI-6~. <B>, <I>, etc. Thus, the weight 
of content enclosed by these tags is increased. 2. Skippable: These tags have no effects, such as <BR>, 
<P>, etc. These tags are omitted during processing. 3. Uninformative: Contents enclosed by these tags, 
such as <AREA>, <COL>, <COMMENT>, etc., are not processed and indexed. 4. Stafisticuf: Contents included 
in these tags, such as <!DOCTPYE>, <APPLES>, <OBJECT>, cSCRIPT>, etc., are extracted and stored in database 
as statistical information in the future.  The content is usually included in nested tags (called TagPattern 
), and only the tag with maximal weight in There are 12 main categories in Yam s homepage (http:l/taiwan.iis.sinica.edu.tw/en/yam/): 
Arts , Humanities , Social Sciences , Society and Culture , Natural Sciences , Computer and Internet 
, Health , News and Information , Education , Government and State , Companies , and Entertainment and 
Recreation . TagPattern is considered as the weight of the content. Both weight and frequency are used 
to measure sUp,,,o of a term extracted from the object. With normalization, the measure of suP ,,.,) 
is shown in (3.1). UP;,,0 = F@ti w ; 9 (3.1) r, is a term enclosed by TagPattern T, in 0, (fu is the 
term frequency in T,, and +, is the maximal weighed tag in T,. Conceptual Model Probabilistic document 
conceptual model is applied to the system. For each object, terms are extracted from its context. Objects 
categorized by Yam are the training set of the classification process. By learning terms from the training 
set of a class, terms with supports to the class are used to represent the classification knowledge of 
the class. The support of a term to a class C is measured by (3.2). sup, +,, 7 I, (3.2) , f, is a term 
in C, n;r;a$(&#38;p,,+ )  s,,c = I and an object 0, in C containing I, is donoted as 01. For s,, , 
a threshold 6 is defined to divide terms into two types. Representarive: a term r; is representative, 
if s,,. 2 6 Non-representative: a term t, is non - representative, if S,,. < 0. For each class, terms 
are applied to mine associations between terms, which is named term associations. Based on the digraph 
constructed from term associations and term supports, an inference model of terms to the class is formed 
to promote some terms from non-representative to representative states. A term t becomes a keyword ki 
of a class, if representative. MG,, is used to substitute s,,., if [, is promoted to k, Thus, MG,,. is 
the maximal value between s,, and the value inferred from term associations. The detail of the inference 
model is described in Section 5. In query model, a query is formulated with terms concatenated with Boolean 
operators (AND, OR, NOT). Similarity match based on VSM is applied to two-phase search (introduced in 
the following query operation) to retrieve hierarchical results. Term Operation Terms in an object are 
extracted according to our developed Term-Segmentation Rules based on the term database. The database 
is mainly originated from the analysis of past query logs of Yam. Besides, during the periods of automatic 
classification and mining term associations, the system also constructs term semantic networks (TSN) 
as the system s thesaurus. File Structure In the system, keyword-based inverted indexes of keywords 
(or terms) in classes and objects are implemented by the relational database system, SQL Server 6.5. 
Given a term or a keyword, relevant classes (indicated with CID) or objects (indicated with OID) can 
be retrieved efficiently. IN the experiment, there are 37986 indexed terms associated to 8706 indexed 
objects and 5 12 indexed classes. In average, given a query term, the system costs 1.132 ms to retrieve 
relevant objects, and 0.04 1 ms to retrieve relevant classes.  Query Operation By applying refined 
classification knowledge (keywords in the class), two-phase search is used in the system. In the first 
phase, query terms are used to search qualified classes. Those classes form a shrunk view of Yam s hierarchical 
lattice to shrink the searching domain of the following object search. In the second phase, for each 
matched class, query terms (except that terms are matched with the class) are employed to match objects 
in the class. By integrating qualified classes and matched objects in these class, a tree (or lattice) 
with probabilities on branches is shown to the user. Thus, the two-phase search mechanism shrinks the 
searching domain and presents a hierarchical view to users. Hardware Currently, the system is implemented 
on a Pentium II 233/128M SDRAM machine with NT Server 4.0 (SP3) and SQL Server 6.5 (SPI) software systems. 
In Fig. 3, we summarize the overview of the system. First, Document Collection and Class Lattice are 
borrowed from Yam inventory. Then, terms with supports are extracted from documents and are further generalized 
as Classification Ktzowledge. Finally, the classification knowledge is refined to a set of keywords (i.e., 
Refined Classification Knowledge) by employing mining Term Association Rules in each class. For query 
processing, the two-phase search module parses Query into term-based Query Representation that is used 
to match with Refined Classification Knowledge and documents indexed by terms. The semantics of synonym 
or alias have included the knowledge base. -r* R.lh,d c49 mlk4 (Cl dCdhlI0u.y R,, , ., a ,Cb~TL , Figure 
3: Overview of the Process Flow~CIRD  4. Mining Term Associations After generalizing terms of the class, 
these terms form a graph with direct edges from term vertices to the class vertex. The edge is labeled 
with support S, c of term 1, to the class c . Thus, we call the graph as Term Support Graph (TSG). In 
average, there are only a few representative terms (e.g.. s 2 0.5) for a class, therefore, the recall 
of the class knowledge is low. Thus, mining the hidden semantics among terms can alleviate the problem. 
In our system, we apply Mining Association Rules to discover those excluded but representative terms. 
Based on mining associations from those terms, the system constructs Term Association Graph (TAG). Combining 
TAG and TSG, Term Semantic Network (TSN) is constructed as shown in Figure 4. In this section, we describe 
the process of mining term associations. In next section, the algorithm of inference on TSN is introduced. 
Tern Semantic Network (TSN) Term Support Term Association Extracting 1 Figure 4: Construction of Term 
Semantic Network Two important issues should be considered before mining term associations is described. 
One is what granularity should be used to mine associations. The other is what boundary should be to 
generate association rules, which is the transaction database defined in [6]. The granularity of mining 
associations In [8], authors restricted the granularity of generating associations to 3-10 sentences 
per paragraph. It is infeasible since a paragraph may be hundreds of sentences, the system should not 
skip later sentences. Additionally, the importance of a sentence of Internet document depends on HTML 
tags (HTML weights defined in the system). Therefore, we can regard an object s informative paragraphs 
as a transaction defined in [6]. The boundary of generating association rules As Internet documents are 
published by a diversity of people. the semantics of words alternate case by case. It is possible to 
use the same word for different concepts, which arise from various cultures of Internet users. For example, 
if a query with apple and computer , the semantics can not be apples of food (As we know, it means Macintosh 
). Thus, in the system, we apply mining term associations based on documents of each class rather than 
all documents in the database. Based on those reasons, we translate our problem domain into the domain 
of mining association rules [6]. Terms are corresponding to items. Documents in the class are corresponding 
to transactions. The class is corresponding to the transaction database. Concentrating on documents 
of a class instead of all classes also takes the advantage of small database size, as the complexity 
of mining association problem is exponentially increased by the size of transaction database. If the 
size of database is not very large, a simple mining algorithm, such as Apriori [5], can be efficiently 
applied to our system. As for the confidence and support defined in [6], the former is used to promote 
non-representative term to refine the classification knowledge, and the latter is recognized as a threshold 
to filter out noisy term associations. For example, after mining term associations in class Art , the 
confidence and support of exhibition + art are 0.826 and 0.1. By experience, an association rule with 
10% supports is useful. The association can be applied to refine the classification knowledge. In the 
classification knowledge, s,, of term exhibition to class Art is 0.13, and s,,. of term art to class 
Art is 1. According to the inference of TSN, introduced in Section 5, in the class Art , s,~ of term 
exhibition to class Art is promoted from 0.13 to 0.826 (The promoted s,,, of the term exhibition = 0.826*1 
= 0.826.). Based on the original support, the term exhibition may be filtered out due to its low support 
(0.13). However, the promoted value (0.826) is enough to support the term exhibition as a keyword of 
the class Art . Thus, the classification knowledge is refined. Moreover, by storing the information of 
term-location in a document, new Chinese terms (or compound terms) can be easily discovered. For example, 
in the class Computer Hardware , $$ t+ &#38; (A t+ B represents if and only if ). @ t+ @$, and &#38; 
t+ @$ are with (conJdence, support) = (1, 0.25). After retrieving their term-locations from the database, 
the system detects that all documents satisfy @ is followed by &#38;, and &#38; is followed by @ . Thus, 
we discover that @&#38;t~ (Macintosh) should be a new Chinese term used in the class Computer Hardware 
. A new terminology is learned in this way. In our system, all term associations in a class are automatically 
applied to refine the classification knowledge of corresponding classes (in Section 5). In this way, 
no query expansion process is necessary. As we mentioned in previous example, a query with the term exhibition 
does not need to be expanded as exhibition and Art , since exhibition has been included in the class 
Art by promoting its support to the class. Also, by directly applying term associations to refining knowledge 
(instead of expanding queries), the system is never confused by which domain (class) thesaurus should 
be employed. Moreover, the performance will be deteriorated that the query expansion should be performed 
every time to match with each class, if different classes own their domain thesaurus. 5. Optimization 
of Term Semantic Network We want to promote some non-representative terms to promoted-representative 
after mining term associations and constructing TSN. In other words, the term s support is larger than 
the threshold after the optimization process. The optimization process of TSN is shown in Figure 5. In 
the left-hand side graph (TSN), there are four non-representative terms. In the right-hand side graph 
(Optimized TSN), three terms are promoted to the promoted-representative state based on their associations 
with the representative term. The optimization of TSN is based on the inference chain in the graph. As 
we can see in the figure. three terms with supports are promoted to keywords with MC after optimizing 
TSN. Before describing how to optimize TSN, we first give a formal definition of TSN.  Graph Notations 
Referring the graphic notations using in [ 151, for a graph G , the vertex set of G is denoted as V(G) 
and the edge set of G is denoted as E(G). A path in G is an alternative sequence of vertices and edges, 
beginning and ending with vertices. p:v,,,e,.v,.e,,v, ,..., v,_,,e,,v,(n>O)l e, =v,_,v,? i=l,2,...,n. 
A path from vertex U to vertex V is denoted as P(U.V) and the set of all possible paths is denoted as 
P(~,~). ,/.--...-.xupp,n__ .. . TSN Optimized TSN ------MG- Figure 5: Optimization of TSN Term Association 
Graph (TAG) TAG, a strongly connected digraph, describes the association among terms. Each vertex u in 
V(TAG) represents a term. and each edge UV in E(TAG) denotes the mined association rule U + v with confidence 
confU, as the edge s label. cclJlf"yy$ where ,f(u)stands for= the number of documents that contain term 
U. The association between U + v is asymmetric. A path (U,v, w) concatenated by edge Us and edge YW forms 
the inference chain that indicates the probability of co-existence of these three terms. Based on the 
assumption of conditional independence, the combination rule to reason co+fUV,is defined below. 0 5 
confu,.,= mnf,, 'clJ"f"WI I, where U,Y,WEV(TAG), and uv, VW,UWEE(TAG)  Term Support Graph (TSG) TSG 
is a digraph in which one class-vertex c represents the class, and a set of term vertices u that also 
exists in TAG denote these terms supporting the class. Only one type of edge, from a term U to the class 
c , is used to represent the term s support to C. Each edge is labeled with the support s,, (0 5 s,,, 
s 1) as defined in Section 3.  Term Semantic Network (TSN) TSN is the union of TAG and TSG as shown 
in Figure 4. V(TSN)=V(TSG)uV(TAG)=V(TSG) E(TSN) = E(TSG) u &#38;TAG) As we described previously, only 
a few terms in TSN are representative, many non-representative terms should be promoted by the association-chain 
from these non-representative terms to representative terms. Through the association-chain, a term has 
many alternarive-supporrs that can be use to decide term promotion. Alternative support Association-chain 
is a path started from a term vertex u via the other term vertices finally ended in C. V,.V~.....V and 
 The inference on the chain denotes an alternative support of II to C. Olas For any term vertex ,,, 
there are many possible paths (alternative supports) to C. The set of these supports is denoted as ASvc. 
While optimizing TSN. we want to find a term s maximal measure among s,,, and AS,, . The maximal measure 
is regarded as MC of the keyword (if the term is promoted) to the class. Membership Grade (MC) Mti,,,. 
= MAX(S,,,.,AS,,,.),if MAX(S,,,AS,,)2 threshold else MC,,< = 0, i.e., u is not a keyword of C after the 
promotion Considering TSN with n terms, since TAG is strongly n-l connected, there are alternative paths 
. Suppose a n Zen- I=, class has 10 terms, there are 2.3~10 possible paths. Unfortunately, after leaning 
the classification knowledge, the average number of terms of a class is 43. It s the problem of combination 
explosion, and the exhaustive search is not possible. Thus, based on greedy approach, we propose Perfect 
Term Support (PTS) algorithm to find each term s MC in polynomial time.  Perfect Term Support (PTS) 
Algorithm Since MC of U is measured by determining the maximum among s,,,. and AS,,, (which is equal 
to co,qU , .S,,) of u , MC is fully dependent on s,,,,~ V(TSN). Based on the greedy heuristic, the first 
selected term is the one with maximal s,,, , u E v(mv) T and s,, is just the term s MC. Theorem I proves 
MG,,~ = s,,, , if u is the term with maximal s,,, Theorem 1: In TSN, if u exists such that S,,, = MAX(S,<), 
where u, YE V(TSN) and VCE E(TSN), then MC,, = S,, Proof: O-l)! (n-l)! -+-+...+- 2! (n-2)! I n-1 =n.zP, 
- ,whereP; - =(n-l)! (n-l-r)! Porapath(u,v,;~~,v,,C)~ P&#38;C), S,, ,..,, c =conf,, ,,, .S,,, .: S,,, 
= MAX(S,,.), for VVE V(TSN) and VCE E(TSN), i.e.,S,,.>S,, wherevtu,vCE E(KSN)-UC Therefore, we say MC,,, 
= S,,, Based on Theorem I, PTS algorithm is a greedy algorithm that guarantees MC of each term is founded 
after the algorithm terminates. Starting from the class vertex C in TSN, PTS selects the vertex, called 
u,, with the maximal support s,, to vertex C in the initial state. Then uI is added into the set termed 
Set,. PTS guarantees the term s support is MC to the class while it is added into ser,, . In the following 
loops, through , to C, alternative supports of remaining terms are estimated. For each term, if the maxima1 
alternative support is larger than its original support, the support is replaced by the maximal alternative 
one. Among all remaining terms. the term vertex with maximal (updated) support is added into set,, The 
algorithm is terminated while ser,, = v(TsN,-C, and MC of each term is found.  PTS Algorithm: I. lnitinf 
state: [This step sets the labels ( ptu), the current maximal support of u) of all term vertices in TSN 
to s,,,, , I.e., e(u) = s,,, . Vertex u, with the maxima1 support to C is added into ser, that contains 
vertices of TSN whose MC to C is determined up to now. The set or remaining terms, Set, t is initialized 
as the set V(TSN)-(C,u,) .I Let the loop index i t I; P(u, ) t MAX (S,, ); Set, +u,;Set,, tV(TSN)-(C,u,); 
e(u) t S,, for allus V(TSN) -u,; If Set,, is empty (i.e., V(TSNJ = (C,u, J) then stop; otherwise, continue; 
 2. [For each vertex u in set,, , this step updates ecu,, if necessary, on account of the path from u 
to C via u in Set,, Fufiher. if e(u) is changed, then ANTECEDENT(~) is assigned to vertex u to keep the 
previous vertex of u .I For each u E Set,, such that uu, E E(TSN ), If conf u,, , eru, ) > ew. then e(u) 
t conf ,, .t (u,)and ANTECEDENT (u) t Us; 3. [This step finds the vertex with MAx(p(u)) from Sel,,. 
and set the vertex as u,+, .] Determine m = MAX (e(v) I VE Set, ); If v, E Set, is selected as a vertex 
with e(v, ) = m, then output m as MC of v, to C and ui+, t vi; 4. [This step adds u,+, into set,, and 
removes it from Set,, .I By tracing u s antecedent vertex recursively, PTS can identlty vertices of 
the path that supports u s MC. WW, t Set,, Uu,,, and Set, t Set, -IA,+,; 5. [This step increases the 
loop index and determines whether the algorithm finishes.] i t i + 1; If Set,, is empty, then stop;otherwise, 
go to step 2;   An Example of PTS As an example, we apply PTS algorithm to optimize TSN of the class 
Placard &#38; Photography shown in Figure 6. Table 1 indicates initial supports of terms to the class. 
The first column indicates supports of terms, and the other columns show the term associations. Table 
2 has columns headed from u, to us, one for each term. The ordered pair in the column corresponding to 
the vertex u, indicates (e(u, ), AN~-ECEDE~~(U, 1) for i = 1,2,3,4,5. The last row compares the initial 
support and the promoted support. With term associations, most of the supports are promoted in the case. 
For example, the support of ug raises from 0.10 to 0.83 because of the high term association from u, 
to Ll, Figure 6: TSN of Class * Placard &#38; Photography . Table 1: Initial Supports of Terms in Class 
Placard &#38; 0 19 Work ( u5) 0.78 0.22 0.1 I 0.44 - Table 2: Loops of PTS for Class Placard &#38; Photography 
. Set,, UI u2 u1 u4 u5 I=1 (1.o,c)(o.13,c)(o.11.c)(o.1o,c)(o.19,c) u, i=2 (0.25,~,)(0.75,,,)(0.83,~,) 
(0.78.u,) (u,,t+ 1 {u,*u,.u, i=3 (0.39,uJ (0.75, u, l(O.83, u,) I I I I I , 1 U, Us.Uq i=4 (0.39. u,: 
(0.75,u, ) UT1 (u,,u,.u, i=s (0.39. us) U3.UI I ,:~~~~, l.O/l.O 0.13/0.390.11/0.7.50.10/0.830.19/0.78 
Using Table 2, we can obtain the following paths that MC of terms are optimized with maximal supports: 
p, :u,.c; p* :u,,u,,u,.c; px :u,,u,,c; p4 :u,,u,,c; Ps :u+,,c; Completeness of PTS Algorithm Now, we 
verify the validity of PTS algorithm. Theorem 2 describes that as PTS terminates MC of each term vertex 
is found. The proof of Theorem 2 is given as the validity of the algorithm. Theorem 2: When thealgorithm 
terminates, (5.1) MC,, =~(u)foralluEV(TSN)-C. Further, if e(u) f 0 and u # C. thenu=w,;...w,,w,,w,,=Cisapath 
(5.2) with maximum support (i.e., MC), where w,_, = ANTECEDENT (w, ) for i = I.2 . . . . . k  Proof: 
We begin with verifying (5.1). We prove based on induction of the loop index ( i, where 1 s i 2 N - 1 
and N = Ivf TSN)- CI 1 and show hat3 after Ui is determined, ~ (u)=MG,,~ foralluE Se&#38; =(u,,.,.,u,), 
(5.3) where i indicates Set,, in ith loop. This is certainly true for i = 1, based on Theorem I, e (~,)=MAX(S,~).where 
PI(v)sta for the current maximal support in ith loop. According to Theorem 1, because e (u,) is the maximum 
support of edge vc for all VE Set:, where Se&#38; =V(TSN)-c. therefore MC,,, = e &#38;)= s,,,. Assume 
Theorem 2 is true for i = N -1. uN_,is the vertex such that e(u._,)=e - (u._,)=MAX (e - (v)lv~ Set:<; 
) and MG,#_,,. = ecu,_,) And for all v E Set:, -u+, 7 we update eN~ (v)=MAX(~o~~,,;MG,,.Iw~Set~, )S~,.) 
In other words, ~N-I(~,_,) is the maximal value among  eN-l(V), where VE Set:: when i = N -1. Then we 
prove the theorem is true when i = N For each VE serLG =ser&#38;l _uN_, , by PTS, we can obtain f%) = 
MAX(eN- (v),conf,,,~_, MC,,_,,). Based on the assumption of induction, eNmI(U,_,) is the maximum of 
[ -I(~),~~ Set:: , and fN(V) is the maximum of P (V) ad clJ,qVuN_,.MG,,~_,. . we can say @ (v) is the 
maximal support of edge VC and all alternative paths via WE set;(. By PTS, UN is a vertex such that 
 ~(u,~)=MAX(e(v)Iv~Ser~ ]. Because the alternative support of the path (u, >kv,,,.c) where N!E seriG 
is larger than that of any other path (v.w,...C) where WE Set&#38; and YE Se&#38; -uN Y by definition 
of MC, we can say MG ,,v<.= MAX ( ( cocf,,N,. MG, I v E Set:, 1, Su,(. ) . = eNb,) By the induction hypothesis, 
we can say when PTS terminates, MC,,. =f'(v), for all VE V(KSN)-C. To verify (5.2). let VE V(KSN) such 
that Q~)+o, and L1+ c At the completion of the algorithm, if ANTECEDENT(v) = C, (5.2) is certainly true; 
otherwise  C(V)=cmf,,, .E(V,)T where ANTECEDENT(V)=", and MC,,.=cmf,,, MC,,, The fact implies that 
VI is the second to last vertex on some ( ,c) path with MC. Continuing in this manner, we produce a ( 
,c) path with MC. P :Y,V,, ,vn_l,Vn =cv where ANTECEDENT(V,)=V,+, for i = 1,2,...,n-1. Let WY = v,_, 
for i=o,l,..,, n, Then we see that (5.2) follows. As observing PTS algorithm, the time complexity is 
 o(,i?) which provides a better performance comparing with the exhaustive inference mechanism whose time 
complexity is exponential 6. Experiment Results We examined the performance of our algorithms using 
the collected data from YAM. Experiments are designed to verify if ACIRD can learn and promote representative 
terms (keywords), which should approach to concepts of human experts for each class. Thus, recall and 
precision of the keywords extracted by ACIRD are compared with the keywords manually selected by experts. 
Four criteria are used to qualify class keywords generalized and promoted by PTS. Top lb: the membership 
grades of all terms are sorted first. The first 10 ranked terms are selected to be the class keywords. 
Top 20: the first 20 ranked terms from the sorted terms are selected to be the class keywords. Threshold 
= 0.5: this criterion selects tho terms whose MC is larger than 0.5. Threshold = 0.7: this criterion 
selects tho terms whose MC is larger than 0.7.  Experiments are tested for the precisoin and recall 
of classes keywords before and after the PTS algorithm is applied respectively. Experiment results are 
shown in Table 3. Observing the results, before applying PTS, we can find that the precision of all criteria 
is larger than 0.73 no matter TSN is optimized or not. Based on these high thresholds (Top 10, Top 20, 
T = 0.5, and T = 0.7), most of qualified terms are represenkztive, thus the precision is high. Such result 
also supports that our classification learning approach can generalize representative terms based on 
these thresholds. However, as we predicted, the recall is very low. It implies that the learning approach 
can not learn the implicit semantic without PTS. As for PTS, in comparison with the case without applying 
PTS, the precision is slightly decreased, but the recall is dramatically increased. Thus, by the learning 
approach, mining term associations and PTS, the system can dig out the hidden semantic (i.e., increase 
the recall) without losing the precision. Also, in comparison with these criteria, the criterion, Threshold 
= 0.5 , can be employed in ACIRD based on the evaluation of recall and precision. Table 3: Simulation 
Results Based on Precision/Recall.  7. Conclusion and Future Work Based on the experiment, we conclude 
that mining term associations from the documents is effective to refine the classification knowledge. 
By the inference of TSN, meaningful terms with low supports, which will be filtered out before PTS is 
applied, are promoted to enhance recall and precision. In addition, with the term promotion, the meaning 
of ranking algorithm is much close to the perception of users. Thus, the inference model of TSN indeed 
represents the semantics of terms. In the future, we will enhance TSN to model the knowledge representation 
of thesaurus. In our model. the edge on TSN model only indicates the co-occurrence probability. Observing 
the constructed term semantic network models, we found that the co-occurrence terms usually have different 
meanings. Some terms support positive concept but some are negative. With only considering the co-occurrence 
probability, pro or con of the concept are confused. With the enhancement of these formal semantic relationships 
in the model, edges can hold specific meanings. According to these specific meanings, we can construct 
the precise conceptual representation for classes. Also, based on mining term associations and heuristics, 
new terms with semantic associations can be discovered by the system to improve and enlarge the thesaurus. 
 8. References [II Mostafa, J., Mukhopadhyay, S., Lam, W. and Palakal, M., A Multilevel Approach to Intelligent 
Information Filtering: Model, System, and Evaluation , ACM Transactions on Information Systems, Vol. 
15, No. 4, October 1997, pp. 368-399. PI Apte, C., Damerau, F., and Weiss, S. M., Automated Learning 
of Decision Rules for Text Categorization , ACM Transactions on Information Systems, Vol. 12. No. 3, 
July 1994, pp. 233-25 I. [31 Salton, G., Automatic Text Processing , Addison Wesley, 1989. [41 Yuwono, 
B., Lam, S. L. Y., Ying, J. H., and Lee, D. L., A World Wide Web Resource Discovery System , World Wide 
Web Journal, Vol. I, No. 1, Winter 1996. [5] Agrawal, R. and Srikant. R., Fast Algorithms for Mining 
Association Rules , Proceedings of the 201h International Conference on VLDB, September 1994. [6] Agrawal, 
R., Imielinski, T., and Swami, A., Mining Association Rules between Sets of Items in Large Databases 
, Proceedings of the ACM SIGMOD International Conference on Management of Data, May 1993. 171 Srikant, 
R. and Agrawal, R.. Mining Quantitative Association Rules in Large Relational Tables , Proceedings of 
the ACM SIGMOD International Conference on Management of Data, June 1996. [8] Jing, Y. F. and Croft, 
W. B., An Association Thesaurus for Information Retrieval , UMass Technical Report 94- 17. 191 Lin, 
S. H.. Chen, M. C., Ho, J. M.. and Huang, Y. M., The Design of an Automatic Classifier for Internet Resource 
Discovery , International Symposium on Multi-technology and Information Processing (ISMIP 96), December 
1996, pp. 181-188. [lo] Frakes, W. B. and Baeza-Yates, R., Information Retrieval -Data Structures &#38; 
Algorithms , Prentice Hall. 1992. [ 1 l] Connet, G., Unstructured Data Bases or Very Efficient Text Searching 
, ACM PODS, Vol. 2, pp. 117- 124. [ 121 Chien, L. F., PAT-Tree-Based Keyword Extraction for Chinese Information 
Retrieval , Proceedings of the ACM SIGIR International Conference on Information Retrieval, 1997. [13] 
Cutting, D., and Pedersen, J., Optimizations for Dynamic Inverted Index Maintenance , the 13 International 
Conference on Research and Development in Information Retrieval. [14] Shasha, D., and Wang, T., New Techniques 
for Best-Match Retrieval , ACM Transactions on Office Information Systems, Vol. 8, No. 2, January 1990. 
pp. 140-158. [IS] Chartrand, G. and Oellermann, 0. R., Applied and Algorithmic Graph Theory , McGraw-Hill, 
Inc., 1993.  
			