
 Evidence-Based Practice in Human-Computer Interactionand Evidence Maps Bonnie E. John Human-Computer 
Interaction Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15217, USA 
1-412-268-7182 bej@cs.cmu.edu ABSTRACT At the onset of evidence-based practice in software engineering, 
prospective disciples of this approach should inspect and learn from similar attempts in other disciplines. 
Having participated in the National Cancer Institute s multi-year effort compiling evidence-based guidelines 
for information-rich web-site design, I bring my personal experiences as a member of that group to the 
discussions at the workshop. From my experience doing other empirical research, I propose using an evidence 
map to communicate research questions, the available evidence to answer those questions, the relationship 
between the questions, and the meaning of different paths through the evidence map. I have used this 
device for several empirical studies, both in HCI and in software engineering, and have found it to be 
a useful organization tool that could help in pursuing evidence-based software engineering.  Categories 
and Subject Descriptors H.5.2 [Information Interfaces and Presentations]: User Interfaces evaluation/methodology, 
style guidelines. D.2.8 [Software Engineering]: Metrics process metrics. General Terms Measurement, 
Experimentation, Human Factors.  Keywords human-computer interaction, research-based guidelines, evidence 
maps 1. INTRODUCTION At the onset of evidence-based practice in software engineering, prospective disciples 
of this approach should inspect and learn from similar attempts in other disciplines. Medicine has often 
Permission to make digital or hard copies of all or part of this work for personal or classroom use is 
granted without fee provided that copies are not made or distributed for profit or commercial advantage, 
and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, 
to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. REBSE 
05, May 17, 2005, St. Louis, Missouri, USA. Copyright 2005 ACM 1-59593-121-X/05/0005 $5.00. been cited 
as an example, but we can look closer to home in another software development field concerned with methods 
and tools as well as product: Human-Computer Interaction (HCI). In reaction to a preponderance of expert 
opinion about web design guidelines, the National Cancer Institute (NCI) sponsored a multi­year effort 
to produce evidence-based guidelines in a systematic, yet pragmatic, manner. The NCI s primary intention 
was to produce citizen-based guidelines for medical information websites, but additional side effects 
of this work were to (1) produce more trustworthy guidelines for all information-rich websites, and (2) 
provide an example of evidence-based practice in this particular area of software development. I participated 
in this effort as an expert reviewer on two rounds of review of the available evidence, and as a contributor 
of some of the evidence itself. The first half of this position paper details some of my experiences 
with NCI s process for amassing and reviewing evidence, and my judgments as to where the NCI s process 
does and does not apply to software engineering. It is a personal statement of opinion and should not 
be construed as expressing the views or conclusions, expressed or implied, of the National Cancer Institute 
or the United States Government. After presenting these views, the second half of this position paper 
draws on other experience doing empirical research to propose using an evidence map to communicate research 
questions, the available evidence to answer those questions, the relationship between the questions, 
and the meaning of different paths through the evidence map. I have used this device for several empirical 
studies, both in HCI and in software engineering, and have found it to be a useful organization tool 
that could help in pursuing evidence-based software engineering.  2. CONSTRUCTING THE NCI EVIDENCE-BASED1 
WEB GUIDELINES Historically, HCI came from a confluence of psychology and 1 The NCI officially calls 
its web guidelines research-based instead of evidence-based , but the credibility of each guideline is 
based on a combination of controlled experiments, other observational studies, and expert opinion. Thus, 
the broader term evidence-based is appropriate and I will use it throughout this paper. computer science; 
the psychologists brought with them a culture of empiricism. Thus, HCI literature (and Human Factors 
before it) has a wealth of publications empirically testing the products of design. The effects of color 
and layout, different strategies for information organization, the performance and comfort of different 
input devices, etc. etc. have been studied empirically for decades. Along this dimension, HCI has a much 
richer literature than software engineering, and these studies are the raw material for evidence-based 
practice. However, HCI is also concerned with methods, e.g., iterative design through prototypes and 
usability studies, expert review, predictive human-performance models, among others. Methods have been 
proposed both by academics and consultants. Methods have also been empirically studied in HCI but much 
less frequently, with the same difficulties as SE methods (e.g., using student groups rather than professionals, 
classroom types of projects instead of real-world development, so few participants that results are likely 
to have occurred by chance, etc.). Thus, for methods, the evidence is more sparse, less controlled, less 
generalizable, and is probably more similar to the state of the evidence in SE. NCI compiled and merged 
a substantial body of published evidence for both content, presentation, and methods for developing web 
guidelines. They produced two sets of guidelines: a preliminary set currently available on the web at 
http://usability.gov/guidelines/, (henceforth, the preliminary guidelines) and a more fully assessed 
set published in book form in 2003 and downloadable from http://usability.gov/pdfs/guidelines.html (called 
here the subsequent guidelines, rather than final guidelines, because the intention is for this set to 
grow as new research emerges). The preliminary guidelines were constructed by a committee of researchers 
and web designers internal to the US Department of Health and Human Services (described at http://usability.gov/guidelines/intro.html). 
This group collected published evidence, did empirical studies to fill in gaps in evidence, and rated 
each guideline as to its strength of evidence. The subsequent guidelines were constructed by going to 
researchers and web designers outside the US government; a description of the process of creating the 
subsequent guidelines can be found in Background and Methodology (p. xx) of the book. In short, NCI collected 
the studies and then presented them for review and assessment importance to a panel of experts from both 
academe and industry, consisting of both researchers and practitioners. The first round of experts used 
a web-based survey system to review hundreds of guidelines with several supporting articles or reports 
apiece, judge the importance of the guideline for the success of a website, and suggest additional papers 
when they spotted something missing. The results of this round formed the input to a second round where 
the guidelines were culled to a smaller set of the most important guidelines, and sent out to a different 
panel of experts, all skilled in research methodology and review, who worked both individually on-line 
and face-to-face to come up with the final set of guidelines. In addition to re-rating the importance 
of each guideline, this group also created a strength of evidence scale and rated each guideline for 
its level of evidence. With respect to judging the strength of evidence, the initial group had a clear 
bias towards controlled experiments, which were termed Category A Experiments (Hypothesis Testing) . 
Only guidelines with at least one Category A experiment supporting them can be implemented with confidence 
. It is well-accepted that controlled experiments give the greatest confidence that a result is causally 
connected to the manipulation, but also well­known that tight control runs the risk of not generalizing 
to real­world situations. To expand the evidence to techniques that can be used more readily in real-world 
situations, Category B Studies (Observation Evaluation/Performance-Based Usability Tests) were weighted 
next most highly. Expert opinions were given the least weight. This rating scheme was hotly debated at 
the face-to-face meeting of researchers from both academe and industry, involved in the subsequent guideline 
creation. It was eventually changed in the final round to give more weight to an expanded set of evidence 
as follows. Rigorous observational study (e.g., ethnographic evaluation) Hypothesis-oriented experiment 
 Model-based evaluation  Expert opinion with no or few references  Reference-base literature review, 
chapter in a book, or meta-analysis  Survey  Textbook with many references  Usability test results 
or summary of several usability tests (e.g., lessons learned)  Exploratory study (e.g., How long will 
people wait for a page to download? )   (pp. xxi-xxii, [8]) In addition, the subsequent guidelines 
gave more weight to converging evidence from several sources. For example, a guideline supported by a 
hypothesis-oriented experiment, seen in an ethnographic study, and consistent with a computational human 
performance model of the phenomena, would receive the highest strength rating, whereas a guideline with 
two hypothesis­oriented experiments alone could receive the second highest strength rating (depending 
on the validity of the particular experiments). In the preliminary guidelines, the order of strength 
ratings would have been reversed. I believe the subsequent rating scheme improved the chance of generalizing 
to new design situations without sacrificing the rigor that comes from controlled experiments. However, 
the subsequent rating scheme, though far better than the preliminary, still had some problems. For example, 
both schemes required a hypothesis-oriented experiment in support of the guideline to attain the highest 
strength rating, but the truth is that controlled experiments are not done, nor could they find a venue 
for publication were they to be done, on aspects of design that are obvious . This is a problem because 
some guidelines that practitioners need to have at their fingertips (or as ammunition when fighting the 
battle for resources in their development process) are considered obvious . For example, one famous HCI 
consultant exhorts the design world to speak the users language [9]. This is most often interpreted to 
mean that interfaces (or websites) should use plain language instead of computer jargon. However, it 
also extends to natural language when considering health-related websites accessed by immigrant populations. 
No controlled experiment would ever be conducted, or published, that tested whether a person who is known 
to not read English would be able to get information from a website in English. Therefore, this guideline 
could not receive the highest strength ratings in the NCI guidelines (in fact, the closest guidelines, 
Use Familiar Words and Avoid Jargon received strength ratings of 3 and 4, respectively). Translate a 
health website into several different natural languages might be essential for getting the information 
to the people who need it most, but resources might not be assigned given that the NCI does not seem 
to fully endorse this guideline2. It seems that a strong category of self-evident , judiciously applied, 
was missing. As mentioned earlier, the preponderance of empirical work in HCI has been on the product 
of user interface design rather than the processes involved in creating that content. This is evidenced 
by the NCI subsequent guidelines. Only 1 section in 17 was on design process; only 16 of the 187 guidelines 
were about process. Further, only 5 of those 16 design process guidelines received the highest strength 
rating and only 2 of those were dos (Use an Iterative Design Approach, and Provide Useful Content) instead 
of don ts (Recognize Tester Bias, Use Heuristics Cautiously, and Use Cognitive Walkthrough Cautiously). 
Thus, the HCI research literature provides more guidance about what processes are not dependable than 
about what processes to use. It is possible that research and academic forces are at play in HCI that 
will also effect SE research: it is easier to evaluate a process and find its faults than it is to credit 
its strengths, let alone to create a new process and formally validate its contributions. The NCI s processes 
in creating both sets of guidelines were well­defined and painstaking executed. I believe the subsequent 
guidelines benefited from active inclusion of industry and academic researchers who were keenly knowledgeable 
about the strengths and weaknesses of many different types of evidence. The SE community does not seem 
to have as rich a history of multiple empirical and analytic methods as HCI has given its roots 2 In 
this particular case, the NCI Guidelines stated explicitly that these Guidelines apply to English language 
websites (p. xix) in the prefacing section labeled How to Use this Book and the Guidelines . in the behavioral 
sciences as well as CS. SE may have to enroll experts in other disciplines to enrich its judgment of 
available evidence, to identify the gaps in the current work, and to assist in filling those gaps.  
3. EVIDENCE MAPS Separate from my role on the NCI s web guideline panel, my research centers around the 
usefulness and usability of HCI methods. I have published empirical studies on such techniques as think-aloud 
usability testing [4], Cognitive Walkthrough [6, 7], computational human performance modeling [3, 5, 
6], Heuristic Evaluation [6], and usability-supporting architectural patterns [1, 2]. In my experience, 
demonstrating the efficacy of a method is difficult, and compelling cases most often come from a convergence 
of different types of data. As a tool for organizing evidence from many different sources, I offer what 
I call evidence maps. I have used evidence maps for several years both as explanatory tools when describing 
the results of an empirical and analytic study and as a tool for my own team to guide our analyses of 
evidence. An evidence map is constructed by first listing the questions your research program seeks to 
answer. For example, in [6] we were assessing whether then current HCI methods for usability evaluation 
were useful in a software development process that involved UI designers assessing a prose specification 
and feeding usability problems and suggested solutions to developers. The questions were (1) are the 
methods predictive of usability problems that would be found if they were not fixed during development?, 
(2) are the problem reports sufficiently convincing to developers that they would expend the effort to 
fix the problems?, (3) are the suggested solutions sufficiently convincing to developers that they would 
implement them or do they prefer their own solutions, and (4) are the resulting systems more usable than 
that which was implemented from the original spec without involvement of UI designers. These questions, 
whatever they are for any particular research program, become the columns in an evidence map. Figure 
1 shows an evidence map for a case study we are currently performing about the efficacy of usability-supporting 
architectural patterns (USAPs). USAPs provide software architects with patterns that encapsulate information 
about the implications of usability concerns on software architecture design. Introducing USAPs into 
a real-world development project, we asked the following questions. (1) Are our USAPs relevant to this 
project?, (2) Had the developers already thought of these issues before our involvement? (3) Did our 
USAPs have an effect on their software architecture design? (4) Did they implement the usability features 
they judged important to the project? (5) Did they implement them in accordance with the architecture 
they had designed (with our help)? And finally, (6) were the usability features needed by the end users 
of the software product? These questions are the columns of Figure 1. In each column, all the logically 
possible answers to these questions are laid out. In general, given the form of the research questions, 
the answers could potentially be some form of yes , some form of no and some form of maybe . For example, 
with respect to the first question, Are our USAPs relevant to this project? the possible answers are 
yes, immediately relevant , yes, but can wait until the next release and no, not expected to be relevant 
to this project ever. With respect to the final question, Were the usability features needed by the end 
users? , the possible answers are yes, the users needed the implemented features , yes, the users needed 
the implemented features but they had even more need than was implemented , and there is no evidence 
of need . In this case, no evidence of need does not mean that users would not need these features under 
different circumstances of use, but only that the actual observed circumstances did not provide evidence 
of need. Thus, the general answers of yes, no, and maybe are tailored to each research question and 
listed in each column. Finally, the sources of evidence are listed at the bottom of each column. These 
sources can be diverse. Figure 1 shows sources of notes taken during an architecture review meeting, 
source code, user s manual, and videos of actual use. All of the possible paths through the space of 
research questions and answers are indicated with lines joining the boxes. Different paths through the 
map have different meanings. In fact, the different paths often have contradictory meanings and are therefore 
a natural way to encode conflicting evidence. For example, the path shown by the heavy black line means 
a direct  Figure 1. Evidence map of a real-world case study of a project s use of usability-supporting 
architectural patterns. 4 positive influence on the project. Each USAP that follows this path was immediately 
relevant to the project, had not been thought of by the developers independently, changed the architecture 
design, was implemented according to that design, and was needed by the end-users. In contrast, if a 
USAP follows the heavy gray line it was of no use to the project because they had thought of it themselves 
and the USAP had no impact on their architecture or implementation (irrespective of the path after implementation). 
As a third example, each USAP that follows the heavy black dotted path was judged to be not relevant 
by the development team and therefore not implemented, but was needed by the end-users, a result that 
says USAPs could have had more positive effect were it not limited by the development team s ability 
to estimate what end-users would ultimately need. We are in the process of analyzing all these data for 
a real-world project, and do not yet know which USAPs follow which paths, but we do know what questions 
to ask, what evidence to examine, and what the results will mean when the analysis is completed. As our 
research program progresses, other sources of evidence may be brought to bear on the same questions, 
for example, a controlled study might be added that could help answer the question as to whether USAPs 
effect architecture design (e.g., [2]). Other real-world cases could be added that follow the same map. 
Thus, the evidence map is a tool that communicates research questions, the combination of evidence, and 
the meaning of results. It guides analysis, and it can accumulate sources of evidence as they accrue. 
 4. CONCLUSION As SE embarks on evidence-based research and practice, it should look to other disciplines 
for methods, pitfalls, and lessons learned, for example, HCI. It will also need tools for directing the 
accrual of evidence and communication of results, for example, evidence maps. The workshop at ICSE 2005 
promises to be a step further towards evidence-based software engineering. 5. ACKNOWLEDGMENTS I thank 
the National Cancer Institute for the opportunity to participate in creating the Research-based Web Guidelines. 
This paper is a personal statement of opinion and should not be construed as expressing the views or 
conclusions, expressed or implied, of the National Cancer Institute or the United States Government. 
I also thank Len Bass of the Software Engineering Institute for encouraging me to propose evidence maps 
as a tool for evidence-based practice.  6. REFERENCES [1] Adams, R. J., Bass, L., &#38; John, B. E. 
(in press) Applying general usability scenarios to the design of the software architecture of a collaborative 
workspace. In A. Seffah, J. Gulliksen and M. Desmarais (Eds.) Human-Centered Software Engineering: Frameworks 
for HCI/HCD and Software Engineering Integration. Kluwer Academic Publishers. [2] Golden, E, John, B. 
E., &#38; Bass, L. (2005) The value of a usability-supporting architectural pattern in software architecture 
design: A controlled experiment. To appear in Proceedings of the 27th International Conference on Software 
Engineering, ICSE 2005 (St. Louis, Missouri, May, 2005. [3] Gray, W. D., John, B. E., &#38; Atwood, M. 
E. (1993) Project Ernestine: Validating a GOMS analysis for predicting and explaining real-world task 
performance. Human-Computer Interaction, 8, pp. 237-309. [4] Jacobsen, N.E., Hertzum, M., &#38; John, 
B.E. (1998). The evaluator effect in usability studies: problem detection and severity judgments. Proceedings 
of the Human Factors and Ergonomics Society 42nd Annual Meeting: Santa Monica, CA: Human Factors and 
Ergonomics Society. [5] John, B. E. (1994) Toward a deeper comparison of methods: A reaction to Nielsen 
&#38; Phillips and new data. In Proceedings Companion of CHI, 1994 (Boston, MA, April 24-28, 1994) ACM, 
New York, 1994. pp. 285-286 [6] John, B. E., &#38; Marks, S. J. (1997). Tracking the effectiveness of 
usability evaluation methods. Behaviour and Information Technology, 16(4/5), pp. 188-202. [7] John, B. 
E., &#38; Mashyna, M. M. (1997) Evaluating a Multimedia Authoring Tool with Cognitive Walkthrough and 
Think-Aloud User Studies. Journal of the American Society of Information Science, 48 (9) pp. 1004-1022 
[8] Koyani, S. J., Bailey, R. W., &#38; Nall, J. R. (2003) Research­based web design and usability guidelines. 
National Cancer Institute. Available for download from http://usability.gov/pdfs/guidelines.html. [9] 
Nielsen, J., &#38; Mack, R. L. (1994), Usability inspection methods, John Wiley.  
			