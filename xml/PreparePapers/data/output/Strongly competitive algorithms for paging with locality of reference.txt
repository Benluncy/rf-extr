
 Chapter 29 Strongly Competitive Algorithms for Paging with Locality of Reference Sandy Irani* Anna R. 
Karlin t Steven Phillips$ Abstract What is the best paging algorithm if one has partial information about 
the possible sequences of page re­quests? We give a partial answer to this question, by presenting the 
analysis of strongly competitive paging algorithms in the access graph model. This model re­stricts page 
requests so that they conform to a notion of locality of reference; given by an arbitrary access graph. 
We first consider optimal algorithms for undirected access graphs. Borodin et al. [2] define an algorithm, 
called FAR, and proved that it is within a logarithmic factor of the optimal. We prove that FAR is in 
fact strongly competitive, i.e. within a constant factor of the optimum. For directed access graphs, 
we present an algorithm that is strongly competitive on all struc­tured program graphs graphs modeling 
the request sequences of structured programs. Introduction Many computer systems have a two-level store 
consist­ing of a small fast memory and a large slow memory. The abstraction of a large, fast virtual 
memory is often implemented by dividing the slow memory into pages, and keeping those pages that are 
likely to be referenced soon in the fast memory. A page fault occurs when a reference is made to a page 
that is not resident in fast memory. Handling a page fault is typically expensive since the page must 
be brought into fast memory. A page replacement strategy specifies which page to replace on a page fault. 
If the future sequence of page requests is known in advance, the optimal page replacement strategy is 
clear: replace the page whose next request is farthest in the future [1]. Unfortunately, the decision 
about which page to replace must usually be made on-line, without detailed information about future requests. 
How can we analyze such an on-line algorithm? * Dept of Computer Science, UC Berkeley, Berkeley, CA. 
t DEC Systems Research Center, 130 Lytton Ave., palo Alto, CA 94301. $Dept of Computer Science, Stanford 
University and DEC Systems Research Center. Partially supported by NSF Grant CCR-901O517 and an OTL grant. 
Straightforward worst-case analysis is useless: if arbi­trary request sequences are allowed, then an 
adversary that always requests the last discarded page can force any paging algorithm to fault on each 
request. Average­case analysis is also problematic, since it requires a sta­ tistical model of the sequence 
of requests. It is ex­ tremely difficult to devise a realistic model, since the pattern of accesses changes 
dynamically with time and with different applications. Nonetheless, several of the early analyses of 
paging algorithms were performed W­suming a fixed probability distribution on the request sequences [5, 
11, 1]. In order to get around these problems, the notion of competitive analysis was introduced by Sleator 
and Tarjan [10]. An on-line page replacement strategy A is said to be c-competitive if there exists a 
constant /? such that for every request sequence a, A(u) < c . OPT(u) + /3, where A(o) is the cost incurred 
by the algorithm A in processing the request sequence a and OPT(C) is the cost incurred by the optimal 
off-line algorithm in processing u. The competdiueness of A, denoted by cA, is the infimum of c such 
that A is c-competitive. Competitive analysis avoids the assumptions of probabilistic analysis, and has 
the power of differenti­ating between different paging algorithms. Sleator and Tarjan showed that no 
deterministic paging algorithm can achieve a competitiveness better than k, where k is the number of 
pages of fast memory. They also showed that some practical algorithms, such as first­in-first-out (FIFO) 
and least-recently-used (LRU) are k-competitive, and hence best possible in their model. 1.1 Locality 
of Reference. The Sleator-Tarjan results conflict with practical experience on paging in at least two 
ways. First, FIFO and LRU have the same competitiveness, even though in practice LRU usually outperforms 
FIFO. Secondly, LRU usually incurs much less than k times the optimal number of faults, even though its 
competitiveness is k. The reason for the practical success of LRU has long been known: most programs 
exhibit locality of reference [1, 3, 9]. This means that when a page is 228 referenced by a program, 
the next request is very likely to come from some small set of pages. Indeed, a two­level store is only 
useful if request sequences are not arbitrary. Motivated by these observations, Borodin, Irani, Raghavan 
and Schieber [2] proposed a technique for incorporating locality of reference into the traditional Sleator-Tarjan 
framework. Their notion of an access graph limits the set of requests the adversary is allowed to make. 
An access graph G = (V, E) for a program is a graph that has a vertex for each page that the program 
can reference. Locality of reference is imposed by the edge relation the pages that can be referenced 
after a page p are just the neighbors of p in G or p itself. Thus a request sequence a must be a walk 
on G, determined by the data given to the program. The definition of competitiveness remains the same 
as before, except for this restriction on the request sequences. Let cA,~ (G) denote the competitiveness 
of an on-line algorithm A with k pages of fast memory on the access graph G. We denote by c~ (G) the 
infimum (over on-line algorithms A with k pages of fast memory) of cA,k (G). Thus ck (G) is the best 
that any on-line algorithm can do. We say an algorithm A is strong{~ competitive if cA,~ (G) = O(c~(G)). 
1 An access graph may be either directed or undi­rected. A typical situation where an undirected access 
graph is a suitable model is when the page reference patterns are governed by the data structures used 
by the program. For instance, a program performing op­erations on a tree data structure might have an 
access graph that is a tree. Alternatively, if the access graph of a program is determined by the flow 
of control inherent in the structure of the program, a directed access graph is a more suitable model. 
1.2 Summary of Results. In this paper, we show that there are strongly competitive page replacement strategies 
in two important settings: 1) undirected ac­cess graphs and 2) directed access graphs representing the 
stream of instruction references made by a struc­tured program. Undirected Access Graphs: Borodin et 
al. proved that on any undirected access graph G, cLRu,k (G) < 2cF1Fo,~(G), and LRU is often better than 
FIFO. How­ever, on some graphs the competitiveness of both LRU and FIFO is much greater than c~ (G). 
For example, it has been observed [7] that LRU and FIFO behave 1This terminology differs slightly from 
that of [6, 8], where strongly competitive was defined as achieving the competitive ratio ck (G). badly 
on loops just larger than k. This is substantiated by the following result in the access graph model: 
if G is the k + l-node cycle, then ck (G) = Pog(k + 1)], while CLRU,~ = k. Thus LRU can be far from optimal 
among on-line algorithms. We seek a universal algorithm, with a succinct description, whose competitiveness 
is close to c&#38;(G) on every graph G. Importance is lent to this question by recent operating system 
research, in which facilities are provided for user-defined page replacement strategies [12, 13]. When 
these facilities are available, one must consider the following question: What is the best paging algorithm 
if one has partial information about the possible sequences of page requests? When the partial information 
is represented by an access graph this question asks for a universal paging algorithm that is strongly 
competitive. Borodin et al. describe a simple and natural algo­rithm called FAR, and prove that it achieves 
a com­petitiveness within O(log k) of c~ (G) for every graph G. They leave open the question of finding 
an algorithm with competitive ratio O(ch (G)). Our main result for undirected access graphs is that FAR 
is optimal, within a constant factor, among on-line algorithms for paging with locality of reference. 
THEOREM 1.1. For any graph G and memory size k, the algorithm FAR is strongly competitive, i.e. CFAR,~(@ 
= O(Ck(G)). The proof relies on a graph decomposition called a vine decomposition [2]. Borodin et al. 
show how to find a lower bound on ck (G) using a vine decomposition of G. We show that this lower bound 
is essentially optimal, by using the fault pattern of FAR to construct a vine decomposition of G. Directed 
Access Graphs: In Section 3 we consider optimal algorithms for directed access graphs that rep­resent 
the stream of instruction references made by a structured program. We use the notion of structured program 
graphs as in [2]. Structured program graphs are defined recursively as follows: Every structured program 
graph has a designated start and a designated stop node. A single directed edge is a structured program 
graph, where the node with out-degree one is the start node and the node with in-degree one is the stop 
node. A structured program graph can be constructed by the following two rules. A structured program 
graph can be built from two structured program graphs G1 and G2, by either identifying the start node 
of GI and the stop node of G2 or identifying the start nodes of GI and Gz and the stop nodes of G1 and 
G2. Identifying a node of a directed loop with a node of a structured program graph. The first operation 
represents sequential operations and branching statements (if, case, etc.). The second operation represents 
loops. Notice that there is no way to represent arbitrary GOTO statements. The access graph of a structured 
program can be determined at compile time. Hence, it is reasonable to assume that a paging algorithm 
will have access to the information necessary to determine the graph in advance. Borodin et al. analyze 
a simple generalization of FAR, called 2FAR, that is optimal for structured pro­gram graphs in which 
all strongly connected compo­nents have at most k + 1 nodes. We introduce a variant of 2FAR, called EVEN, 
and prove that it is strongly competitive for all structured program graphs. THEOREM 1.2. The algorithm 
EVEN is strongly competitive on the class of structured program graphs. In other words, for any structured 
program graph G, ck ,EvEN(G) = ~(cornp(G)) . 2 Undirected Access Graphs 2.1 Background. We review some 
terminology and results on competitive paging. A sequence of page requests and the resulting behavior 
of a paging algorithm can be divided into phases. At the beginning of a phase, all pages are unmarked. 
When a page is requested (or hit) it is said to be marked. A phase ends just before the request to the 
k + 1 ~ distinct node. At the end of a phase all nodes become unmarked again. We call the nodes requested 
in this phase but not in the previous phase new nodes. A node is said to be evacuated if it has been 
evicted from the fast memory during the phase. If a page is in the fast memory, we will say it is covered 
by a server. The algorithm FAR is a marking algoriihm. A marking algorithm is an algorithm that, on a 
fault, always evicts an unmarked page. The unmarked page that FAR chooses to evict is the page that is 
furthest (in the access graph) from the set of marked nodes. PROPOSITION 2.1. (a) If gi new nodes are 
re­ quested in the i th phase, then the cost, of the adversary during the first i phases is at least 
(~~=1 gj)/2, and at most ~~=1 gj. (b) If A is a marking algorithm then for any graph G, CA,~(G) < k. 
The proof of part (a) is due to Fiat et al. [4], while part (b) is due to Karlin et al. [6]. A vine 
decomposition V(H) = (T,?) of any graph H is a tree T in H together with a set P = PI, P2, ... of node 
disjoint paths, called vines such that (i) the IRANI ET AL. nodes of H are partitioned between T and 
the Pi s; (ii) each end of each path is an edge incident to a node in T. If a tree node is incident to 
the last edge in a vine, then it is called an anchor. Let nT be the number of leaves of T that are not 
anchors. The value of a path P, denoted v(P), is defined to be 1 + hog IPIJ. Let H be a connected subgraph 
of G containing k + 1 nodes. Then the value of a vine decomposition V(H), denoted v(V(17)) (or v(V) for 
short) is THEOREM 2.1. 1. For any graph G and memory size k, let V be a vine decomposition on k + 1 nodes 
in G. Then ck(G) > v(V). 2. Let H be a graph containing a cycle on k +g nodes, g < k. Then ck(G) = Cl(logk 
 logg). Proof. The proof of Part 1. is due to Borodin et al. [2]. For completeness we give an outline 
of the proof here. W.1.o.g. assume that G has k + 1 nodes. An on­line algorithm A always leaves one node 
of G uncovered the hole. The bound on Ch(G) follows from noting that during a phase, the adversary can 
make A incur v(V) faults before k + 1 nodes are visited, and the adversary incurs a fault. When A moves 
its hole to a new position following a fault, the adversary sequence walks to that position. If the new 
location of the hole is a tree node, the adversary walks to the hole on a path that contains only marked 
nodes or internal tree nodes. If the new location of the hole is on a vine, the adversary walks to the 
hole on a path that contains only marked nodes, The hole can be reached at least nl times by requests 
to non-anchor leaves, and at least v(P) = 1 + [log IPIJ times by requests to each path P. For part 2, 
the adversary plays against an on-line algorithm A. There are g holes (nodes where A doesn t have a server) 
in H at any time. A phase starts with the adversary and A covering the same set of cent iguous vertices. 
If A ever has a hole on a marked node, the adversary requests the path to the hole, passing only through 
marked nodes. The phase begins with the adversary issuing requests to all g of the uncovered nodes. At 
any point, the set of unmarked nodes forms a contiguous path. The adversary then repeats the following 
pattern: he requests all the nodes in whichever half of the path of unmarked nodes haa more holes, causing 
the on-line algorithm to incur at least g/2 faults. This pattern can be repeated until k nodes are requested. 
The adversary incurs only g faults during this process: at the start of the phase he vacates the g nodes 
that will remain unmarked at the end of the subsequence. The number of times the on-line algorithm incurs 
at least g/2 faults is Q(i) such that k/2~ = g. Therefore the ratio between the number of faults incurred 
by the on-line algorithm and the adversary is fl(log k log g). Since the situation at the end of this 
phase is the same as at the start, the adversary can repeat this behavior ad infinitum, hence c~(G) = 
Q(log k logg). Note that if we replace rz~ in the definition of v(V) by the total number of leaves 
of the tree, then the value v(V) at most triples. Since we are not concerned with small constant factors 
in the next section, we will regard the value of a vine decomposition as being the number of leaves in 
the tree plus the sum of the values of the vines. 2.2 Proof of FAR s Strong Competitiveness. In this 
section we prove that FAR is a strongly com­petitive algorithm, i.e. that cF~~,~(G) = O(c~(G)). The proof 
works by showing that for each phase of the algorithm, there is a vine decomposition whose value is within 
a constant factor of the ratio between the number of faults incurred by FAR in that phase and the number 
of faults incurred by OPT in that phase. Suppose that g new nodes are requested in a phase. By proposition 
2.1, the adversary incurs at least g/2 faults for this phase, amortized. Note that the number of faults 
FAR makes in the phase is just the number of nodes it vacates (since it is a marking algorithm). We use 
the concept from [2] of representatives or reps for short, The sequence of nodes vacated in the jth phase 
is divided into blocks of g + 1 successive vacations. Because at any time, the number of nodes that have 
been evacuated during the phase and are currently outside the fast memory is at most g, at least one 
node of each block is marked by the time any node of the subsequent block is vacated. We pick such a 
node for each block and call it a rep. Suppose that the number of reps for this phase is R. We will 
obtain the theorem by either constructing a vine decomposition on k + 1 nodes whose value is Q(R) or 
else by finding a cycle on k(l i-2-Q(R)) nodes. There are two main steps in constructing such a vine 
decomposition or finding a large cycle: First construct a vine decomposition V on at most 2 .5k nodes 
whose value is Q(R). Second, find within it the desired vine decomposition or cycle. Constructing the 
Vine Decomposition. We begin by showing that there is a vine decomposition V on at most 2 .5k nodes whose 
value is f2(R). Let G be the directed subgraph of G containing every node that is marked during the phase, 
with 231 a directed edge from u to v if the first request at v immediately follows a request at u. Therefore, 
a directed path from x to g in G indicates that x was marked before y. Let G be the subgraph Gt augmented 
as follows: Suppose that u is a degree 2 vertex in G that has degree three or more in G. Arbitrarily 
pick one vertex, say v, that is a neighbor of u in G but not in G . Then if v is in G , add an undirected 
edge between u and v in 6. Otherwise (v isn t in G:), add the vertex v and the undirected edge (u, v) 
to G. At most k nodes can be added to G1 by this process. ~ has a number of nice properties that are 
useful in constructing the vine decomposition, We describe these properties in the following lemma. Define 
a chain to be a directed path in ~ all of whose nodes have degree 2. For brevity we will call a node 
of degree not equal to 2 a non-chain node. LEMMA 2.1. Let P be a directed path in 6 consist­ing of r 
chains separated by non-chain nodes. Let fij 1< i < r be the number of reps on the ith chain, and let 
n(v), for v a node in one of the chains, be the dis­ tance from v to the end of the chain. Then 1. If 
v, w and z are three consecutive reps on a chain, then n(z) < n(v)/2. Hence fi = O(log ii), where /i 
is the length of the ith chain. 2. If u is the second to last rep on a chain and z is the third rep 
on any subsequent chain in the entire path, then n(z) < n(u).  9. Xl<i<, fi = O(r + log~l) 7 ~~ere ~1 
is t~ ~ n9th of t~e-first chain with at least 5 reps. Proof Part 1 is proven as follows: Suppose that 
v, w and z are three consecutive reps on a chain and that Wt is the last marked node on w s chain at 
the time that w is evicted. Then at the time that w is evicted the unmarked portion of the chain extends 
from w until the end of the chain, Note, however, that some of these nodes may not have servers on them. 
Since FAR evicts an unmarked node furthest from the set of marked nodes, if w is less then half the way 
from w to the end of the chain, then every vertex between w and the midpoint must be a hole. Furthermore, 
none of them can be reps since they were vacated before w was marked. Therefore, .z must be beyond the 
midpoint of the chain from Wt to the end and so n(z) < n(w )/2 < n(v)/2. (The last inequality follows 
from the fact that v is marked before w is evicted, and so w can not precede v in the chain.) Hence, 
fi = O(log /i). For part 2, suppose that u and v are the last two reps on a chain and that x, y and z 
are the first three reps on any subsequent unmarked chain in the entire path. Since u is marked before 
v is evicted, v is at distance at most n(u) from the set of marked nodes at the time it is evicted. Also, 
by part 1, n(z) < n(z) n(z), Now suppose that n(z) > n(u). Then n(x) n(z) > n(u) also. But then the 
server at z is further from the set of marked nodes at the time w is evicted than v is, so would be vacated 
before v, so z cannot be a rep. For part 3, consider the following subset R of the reps along P: For 
each chain c in P, R includes every other rep along C , excluding the first four and the last one. Let 
vi be the site of the ith rep in R , ordered by occurrence along P. Then using parts 1 and 2 of this 
lemma it is easy to prove by induction that n(w; ) < 7z(vl)2-(~-1). Since n(vlR)l) < 9Z(V1)2-(IR I-lJ, 
n(vl) < 11/2, and Zl<i<r fi  = ~(lR l+ ) weobtain We omit the proof of the following well known graph-theoretic 
proposition that we will need. PROPOSITION 2.2. Jet n2(G) be the number of nodes in G that are not of 
degree 2. Then, there ex­ists a tree T C G such that the number of leaves in T, 1(T), satisfies I(T) 
= Q(n2(G)). We now describe a proce~ure for constructing a vine decomposition of the graph G with value 
Q(R), possibly adding a small number of additional nodes. First we dispose of an easy case, when the 
graph ~ is tree-like . Divide the reps into two types: A rep is of type 1 if it is the last or one of 
the first four reps on a chain, or it is a non-chain node. Type 2 is all the rest (any rep in the middle 
of a chain). If there are more reps of type 1 than type 2, then we can use Proposition 2.2 to build a 
tree in G with Q(R) leaves. It is easy to then prune the tree until there are k + 1 nodes, while keeping 
the number of leaves Q(R). The resulting tree is the desired vine decomposition. Hence we can assume 
that reps of type 2 dominate, so we need to find a vine decomposition whose value is proportional to 
the number of type 2 reps. This is achieved by the Procedure FindVineDecomp. The procedure works by repeatedly 
collapsing a subgraph of the graph into a supernode, each time identifying a vine in the subgraph whose 
value is proportional to the number of reps in the subgraph. When a subgraph has been collapsed, it functions 
as a single node in the graph for the remainder of the algorithm. (Edges to and from vertices within 
a supernode are replaced by edges to and from that supernode, without multiple edges.) IRANI ET AL. Procedure 
Find VmeDecomp Initialization: Partition G into maximal components of non-chain nodes (nodes with degree 
not equal to 2), and directed paths between these maximal components. If any directed path between components 
haa at most 5 reps on it, combine that path, and the two max­imal components it connects into one larger 
component. As soon as no more combining is possible, ~ has been partitioned into a set of components 
and directed paths, where all of the reps of type 2 are on some directed path. Reduce each component 
to a single supernode. Loop: Call the (super)node containing the first node marked in the phase Root. 
Repeat the following steps until Root contains all of the nodes of ~. Follow a maximal directed path 
from Root. This di­rected path, called P, consists of a set of chains sep­arated by supernodes. Suppose 
that the directed path ends at (super)node s. Let SP be the short­est path in G froms to ~ P (possibly 
just a single edge) and let v be the node in G P to which SP connects. Let P1 be a directed path from 
Root to v. Let s be the last supernode in P at which P and PI intersect. Let ~ be the portion of P ex­tending 
from s on, and let ~ be the portion of P extending from s on.  Now suppose that C l is the longer of 
the first chain of ~ and the first chain of ~1. We identify C I aa a vine, and reduce s U } U 8 U SP 
to a single supernode.  Output: The vine decomposition is the collection of vines that were identified 
in the loop, together with a spanning tree of the remaining graph. In order to prove that this procedure 
achieves the desired results we need a preliminary lemma. LEMMA 2.2. In any execution of the loop of 
Pro­cedure Find VineDecomp, the length of the path SP is at most 12-r, where 1 is the length of the first 
chain in ? and r w half the number of type 2 reps along P. Therefore the number of nodes added to the 
graph G in such an iteration through the loop is at most 1/2, and the total number of nodes added during 
the execution of Find Vine Decomp is at most k/2. Proof. Let x, y and z be the last three reps ~n the 
path ~. Ifs is not adjacent to a (super)node in G P, then no vertices in s can be marked before z is. 
Let y be the last node marked at the time that y is evicted. (Note that y cannot precede z.) Suppose-that 
d is the length of the shortest path from s to G P. At the time that y is evicted its distance from 
the set of marked nodes is at most n(y ) n(y). z s distance from the set of marked nodes at the time 
that y is evicted is at least the minimum of d and n(y ) n(z). Since n(y ) n(z) > n(y ) n(y), if d 
> n(y ) n(y), then at the time that y is evicted, z s distance from the set of marked nodes is larger, 
contradicting the choice of y for eviction. Therefore d < n(y ) n(y) < n(y ) < n(z) < 12- , where the 
last inequality follows from lemma 2.1. Since r ~ 1, the length of SP can be at most l/2. Furthermore, 
since there are at most k nodes that are in chains, we add at most k/2 nodes during Find VineDecornp. 
LEMMA 2.3. Procedure Find VineDecomp produces a vine decomposition V with value Q(R2), where R2 is ihe 
number of reps of type 2. The number of nodes in V is at most 2.5k. Proof. First we show that Find VineDecomp 
is well defined: To see that the loop can be executed as described, we first observe that if the directed 
path P does not terminate at Root (if it did SP and P would be empty) the path SP must exist. If not, 
there could not be a rep in the interior of the last chain in P. There must also exist a directed path 
P from Root to v (possibly empty if v = Root), since there is a directed path from Root to any (super)node 
in ~. We now prove the claim that v(V) = Q(R2) by noting that during each iteration through the loop, 
the number of (type 2) reps that become incorporated into a supernode is proportional to the value of 
the vine that is identified. Indeed, this follows by the proof of lemma 2.1. LEMMA 2.4. Let V be the 
vine decomposition con­structed above. Then either V contains a vine decom­position V on k + 1 nodes 
whose value is Q(Rz) or V contains a cycle on k(l + 2-n(~2J) nodes. Proof. We decompose the proof into 
two cases: Case 1: There is some vine V such that the number of reps r on that vine is at least R2/4: 
Suppose that V has length 1, and that it has r reps. Suppose also that the shortest path in G from one 
endpoint of V to the other has length a. As soon as the number of unmarked nodes that remain in V decreases 
to a, no more vertices will be vacated along V. Therefore, by the proof of Lemma 2.1, @ < TI(V, ) < ~(vl)z-( 
- 52/2 <= L2-o( ). Since 1< k and r = !2(R2), V contains a cycle on k(l + 2-n(R )) nodes. Case 2: No 
single vine has value more than w(V)/4. In this case, we fiyst throw out all the leaves added in going 
from G to G above. Thus we are left with a vine decomposition on k + ~ ~ 2k nodes whose value is still 
0( Rz). We then remove vines in order of decreasing length until the number of nodes is k + 1. Suppose 
that VI, . . . Vr are the vines in order of decreasing 233 length, and 11,... , lr are their lengths. 
Suppose further that ~l<i<m 1z < ~ and ~l<;<m+l Ii ? ~. BY Lemma 2.2; /3 < (1/2) ~l<i<,-li~ Therefore, 
m < r/2. Furthermore, since t~e value of a node in v; (i.e. its average contribution to the value of 
the vine decomposition) is log(li )/li and this value decreases the larger ii k, ~l<i<m log Ii < v(V)/2. 
Since the value of Vm+l is at fi&#38;t v(V)/4, we then have that the remaining vines have value at least 
v(V)/4. We summarize the results of this section: THEOREM 2.2. For any graph G and memory size k, the 
algorithm FAR is strongly competitive, i.e. CFAR,~(G) = O(Ck(G)) Proof. Let g be the number of new nodes 
requested in the current phase. By proposition 2.1, the adversary incurs at least g/2 faults for this 
phase, amortized. Suppose that R is the number of reps for the phase. R is an upper bound on the competitive 
ratio for this phase. But Q(R) is also a lower bound on the competitive ratio of G. Indeed, either there 
are many type 1 reps, so we can find a tree with value Q(R), or by Lemma 2.3, we can construct a vine 
decomposition on at most 2.5k nodes whose value is Q(R). By Lemma 2.4 we can use that vine decomposition 
to either construct a vine decomposition on k + 1 nodes whose value is still $2(R) or else we can find 
in G a cycle on k(l + 2-n(RJ) nodes. In either case, theorem 2.1 gives a matching bound, completing the 
proof of the theorem. 3 Directed Access Graphs The problem of devising competitive paging algorithms 
for directed access graphs is an important one. Unfor­tunately, in full generality this problem seems 
very dif­ficult. There is, however, a restricted class of directed access graphs that is both important 
and tractable, rep­resenting the stream of instruction references made by a structured program. We examine 
the class of structured program graphs as defined in the introduction. The access graph of a structured 
program can be determined at compile time. Hence, it is reasonable to assume that a paging algorithm 
will have access to the information necessary to determine the graph in advance. The information that 
is not available until run time is the number of times a particular loop will be executed or the particular 
branch an execution path will take. In other words, it is unknown until run time exactly which path through 
the access graph will represent the access sequence for a particular execution. Fortunately, our analysis 
is a worst case over all paths through a particular access graph. The algorithm we introduce, called 
EVEN, is strongly competitive on all structured program graphs. EVEN is a variation of an algorithm introduced 
in [2] called 2FAR. A strongly connected subgraph of a structured program graph consists of a connected 
set of cycles. If F is a strongly connected subgraph of G, then we define a graph UF which we call the 
underlying graph of F. UF is an undirected graph whose vertex set is just the set of cycles in F. There 
is an edge between two nodes if and only if their corresponding cycles in F share a node. A node in UF 
is said to be a peripheral node if its removal does not disconnect the graph. A cycle in F is said to 
be a peripheral cycle if it corresponds to a peripheral node in Up. Notice that UF has the property that 
the shortest path between any two nodes does not contain any peripheral nodes (except perhaps for the 
endpoints of the path). We define l(F) to be the number of peripheral cycles in a subgraph F. Let F be 
a strongly connected subgraph on k + g nodes. We say that F is compact if every peripheral cycle in F 
has at least g nodes, The following theorem gives a lower bound on the competitive ratio achievable on 
a structured program graph G. Let Hi(G) be the set of strongly connected compact subgraphs of G on i 
nodes. Let cornp(G) = maxFmax{(~(F) I)/g, (~~(p) l)}, where the maximum is taken over F c Hk+g (G), 
k ~ g ~ 1,and Hi is the j th harmonic number. THEOREM 3.1, If G is a structured program graph, then c~ 
(G) ~ COrnP(G) . Proof. Let F be any strongly connected subgraph of G on k + g nodes, and let A be a 
paging algorithm. The adversary operates in phases (slightly different from the phases referred to elsewhere). 
We show that the adversary can force A to incur g cornp(G) faults in a phase while the adversary only 
incurs g faults in a phase. A phase starts with all of the adversary s holes on one peripheral cycle 
and the last request on a non-peripheral cycle. There are two csses: 1 (1(F) -1)/g z (lZl(F~ -1): In 
a phase, the adversary forces the algorithm to fault on every peripheral cycle except one: if there is 
a cycle that has not been hit where the algorithm has a hole, the adversary requests the shortest path 
to the hole. The shortest path is measured by the distance in UF. In particular, no peripheral cycles 
are hit along the way. A peripheral cycle is only hit if the algorithm has a hole on it. The phase ends 
when the adversary has hit all but one peripheral cycle. The algorithm incurs at least I(F) 1 faults. 
IRANI ET AL. Meanwhile, the adversary services the sequence by initially moving all g of its holes to 
the peripheral cycle that will not have been hit at the end of the phase (and thus incurs at most g faults 
during the phase). 2. (I(F) 1)/g < (~~(F) 1): If the algorithm ever maintains a hole on a non-peripheral 
cycle, then the adversary requests a path to the hole through only non-peripheral cycles. Thus we can 
assume that all of the algorithm s holes are on peripheral cycles. The adversary requests the shortest 
path to the peripheral cycle with the most holes. Again the phase ends when all but one of the peripheral 
cycles have been hit. When the ith peripheral cycle is hit, the algorithm incurs at least g/(l(F) i 
+ 1) faults. Therefore, the algorithm incurs at least g/l(F ) + g/(Z(F) 1)+. ~+9/2 = g(~((F) 1) faults. 
Again the adversary services the sequence by initially moving all g of its holes to the peripheral cycle 
that isn t hit during the phase. We now introduce an algorithm whose competitive­ness on structured program 
graphs is within a constant factor of the lower bound of Theorem 3.1. The algo­rithm is called EVEN because 
it keeps the holes evenly distributed around the graph. EVEN works in two modes. The first mode is used 
when the set of nodes occupied by the servers and the present request are not contained in a strongly 
connected component. In this case, EVEN simply services a fault by using any server which is on a node 
that is not reachable from the cur­rent request. The second mode is used when all the servers are in 
a strongly connected component. Then EVEN works like a marking algorithm. Phases are de­fined only when 
EVEN is in the second mode: a new phase begins either when EVEN switches from mode one to mode two or 
when k + 1 nodes have been marked in the previous phase. A few definitions are necessary to describe 
this mode of the algorithm. Let C be the set of nodes requested in the current phase. Let P be the set 
of nodes requested in the previous phase. Let P be the smallest set of cycles that cent ains all of P. 
A cycle is empty if it does not have any servers on it. A branch in ~ is a path of cycles such that the 
first cycle in the path is a peripheral cycle and all the cycles except the last are empty. The last 
cycle is called the active cycle. A branch dies at the moment that the active cycle is hit or becomes 
adjacent to two non-empty cycles. A live branch is one that has not died. The value of a branch is the 
number of nodes in the branch that have been evacuated during the phase (i.e. the number of holes in 
the branch that are also in P). At the beginning of a phase, each peripheral cycle is a branch. The 
peripheral cycle is the active cycle for that branch. EVEN s policy for deciding which server to vacate 
when servicing a fault is determined as follows: pick the live branch of minimum value. Evacuate the 
farthest (from the request) node on the active cycle. If the active cycle becomes empty then add the 
next non-empty cycle to the branch. The new cycle becomes the active cycle. Note that the values of any 
two live branches differ by at most 1. Thus if there are i live branches, the value of any branch is 
bounded above by [9/~1 . The following theorem bounds the competitive ratio of EVEN on any structured 
program graph. THEOREM 3.2. The algorithm EVEN is strongly competitive on the class of structured program 
graphs. In other words, for any structured program graph G, C~,EVEN(G) = ~(co~p(G)). Proof, Consider 
a structured program graph G. If EVEN incurs a fault on node v when in mode one, then it is the first 
time that node v has been requested during the entire sequence. As a result, both EVEN and the optimal 
algorithm fault on the request to node v. Thus, when EVEN is in mode one, the number of faults that it 
incurs is equal to the number of faults the optimal algorithm incurs. We must now bound the number of 
faults incurred by EVEN in mode two. Let P be the set of nodes requested in the phase before the last 
phase. (Recall that P is the set of nodes requested in the previous phase and C is the set of nodes requested 
in the current phase.) If the algorithm just switched from mode one to mode two, then P is the set of 
nodes currently occupied by the servers, and P) are the k distinct nodes requested previous to the first 
request to any node in P. Let g be the number of new nodes requested in the last phase (g = 1P P 1) 
and let g be the number of new nodes requested in this phase (g = IC Pl). If the number of faults EVEN 
incurs for an arbitrary phase is bounded by O(g + g ), then by Proposition 2.1, c~,EvEN(G) = O(c~(G)). 
If P nC =0,then g + g ~ k. In this case, EVEN incurs O(g + g) faults during the phase, since no marking 
algorithm incurs more than k faults in a phase. If P n C # 0, then there is a strongly connected component 
cent aining P, contained in C U P U P . Indeed, there is a directed path from the first node requested 
in C to a node z in P nC. (This path contains at most g nodes outside P.) Since x G PI there is a path 
from ~ to the first node requested in the previous phase. (This path contains at most g nodes outside 
P.) Finally there is a path entirely inside P from the first node requested in the previous phase to 
the first node requested in the current phase. Therefore, if P is the smallest strongly connected component 
containing P that is contained in C UPUP , ~ has at most k+g+g nodes. Trim ~ by removing small peripheral 
cycles until it is compact. Call the resulting graph ikf. We show that the number of faults that EVEN 
incurs in the current phase is bounded by (g +g +l(lkf) +gllf(M)). We count the number of faults by counting 
the number of nodes evacuated in &#38;f. Note that there are at most g+g other faults in the phase: at 
most g + g nodes were trimmed to get M. Each evacuated node in P is contained in some branch in P. The 
total number of evacuated nodes in ~ in any particular branch is the value of the branch when it dies. 
The set of l(~) branches of ~ induces a set of l(ikl) branches in M such that every branch in M corresponds 
to a branch in ~. The branches in M are just the branches in P restricted to M. Note that some branches 
in P may have been completely removed by the trimming. The value of each branch in M is upper bounded 
by the value of its corresponding branch in P. Consider the ~ th branch in ~ that dies. The number of 
evacuated nodes on that branch is at most g/(~(~) i+ 1), which in turn upper bounds the number of evacuated 
nodes on the corresponding branch in M. Thus the total number of evacuated nodes in M is at most 1(M) 
j=l Theorem 3.1 gives us a matching lower bound, complet­ ing the proof of the theorem. 4 Open Problems 
We assume in all of our work that the algorithm hss access to the complete access graph. Is there a strongly 
competitive algorithm that learns the access graph as more requests are seen? Also, are there good algorithms 
that only maintain a portion of the access graph, say, the subgraph generated by the pages in fast memory? 
The access graph model can be generalized in a number of ways, to make it a more realistic model of page 
request sequences. For instance, the algorithm may have a small set of pointers moving through the access 
graph: one for data and one for instructions say, or a collection of pointers into arrays. It would be 
interesting to combine the competitive and probabilistic approaches. If the request sequence forms a 
Markov chain, is there a natural algorithm whose performance is close to optimal? If the transi­tion 
probabilities of the Markov chain are not known in IRANI ET AL. advance, is there an algorithm that performs 
well com­pared to the adversary who chooses the Markov chain? Lastly we restate the two remaining open 
questions from [2]: Open Question 1. Show that for all G and k, CLM_J,~ (G) < WIFO,~ (G). Open Question 
2. Is there a universal randomized algorithm that is close to optimal on every G? Is there a graph-theoretic 
lower bound on Ck (G) for randomized algorithms against an oblivious adversary? References [1] L.A. Belady. 
A study of replacement algorithms for virtual storage computers. IBM Systems Journal, 5:78 101, 1966. 
 [2] A. Borodin, S. Irani, P. Raghavan, and B. Schieber. Competitive Paging with Locality of Reference. 
In Proc. .Z5 rd Annual ACM Symposium on Theory of computing, pages 249-259, 1991. [3] P.J. Denning. 
Working sets past and present. IEEE Trans. Software Engg., SE-6:64-84, 1980. [4] A. Fiat, R. Karp, M. 
Luby, L. McGeoch, D. Sleator, and N. Young. On competitive algorithms for paging problems. To appear 
in Journal of Algorithms, 1991. [5] P.A. Franaszek and T.J. Wagner. Some distribution­free aspects of 
paging performance. Journal of the ACM, 21:31-39, 1974. [6] A. R. Karlin, M. S. Manasse, L. Rudolph, 
and D.D. Sleator. Competitive snoopy caching. Algorithmic, 3(1):70-119, 1988. [7] T. Kilburn, D.B.G. 
Edwards, M.J. Lanigan, and F.H. Sumner. One-level storage system. IRE Trans. Eiect. Computers, 37:223 
235, 1962. [8] M.S. Manasse, L.A. McGeoch, and D.D. Sleator. Com­petitive algorithms for on-line problems. 
Journa/ oj Algorithms, 11:208 230, 1990. [9] G.S. Shedler and C. Tung. Locahty in page reference strings. 
SIAM Journal on Computing, 1:218 241, 1972. [10] D.D. Sleator and R.E. Tarjan. Amortized efficiency 
of list update and paging rules. Communications of the ACM, 28:202 208, February 1985. [11] J.R. Spirn. 
Program Behavior: Models and Measure­ments. Elsevier Computer Science Library. Elsevier, Amsterdam, 1977. 
 [12] D. Cheriton and K. Harty. Application-controlled physical memory using external page-cache manage­ment. 
Technical Report draft, Department of Com­puter Science, Stanford University, 1991. [13] D. McNamee 
and K. Armstrong. Extending the Mach external pager interface to accommodate user-level page replacement 
policies. Technical Report 90-09­05, Department of Computer Science and Engineering, University of Washington, 
1990.  
			