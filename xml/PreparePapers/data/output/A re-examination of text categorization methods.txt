
 e B a e B a e B a y a h is ho e a sk w m t o f e b een a p - r w th e a w th e e ev e w havenotbeendirectlycomparedtogetherinacontrolled 
studywiththoroughstatisticalsignifcanceanalysis,which isthefocusofthispaper.Specifcally,thispapercontains 
thefollowingnewcontributions: Providesdirectlycomparableresultsofthefvemeth­odsonthenewbenchmarkcorpus,Reuters-21578.Cur­rently,publishedresultsofLLSF,NNetandNBonthis 
corpus(withthefullsetofcategories)arenotavail­able.ForkNN,publishedresults[12,14]areavailable butare\mysteriously"lowerthantheresultsbyoth­ersonapreviousversionofthiscollection[31].Asfor 
SVM,thepublishedresultsdonotcontainsufcient detailsforstatisticalsignifcanceanalysis.  Proposesavarietyofstatisticalsignifcancetestsfor 
diferentstandardperformancemeasures(includingF  1 measuresforcategoryassignmentsandaveragepre­cisionforcategoryranking),andsuggestsawayto 
jointlyusethesetestsforcross-methodcomparison. Observestheperformanceofeachclassiferasafunc­tionofthetraining-setcategoryfrequency,andan­alyzestherobustnessofclassifersindealingwitha 
skewedcategorydistribution. 2Task,CorpusandPerformanceMeasures Tomakeourevaluationresultscomparabletomostofthe 
publishedresultsinTCevaluations,wechosetopicspotting ofnewswirestoriesasthetaskandtheReuters-21578cor­pusforthedata.Thiscorpushasbecomeanewbenchmark 
latelyinTCevaluations,andistherefnedversionofseveral olderversions,namelyReuters-22173andReuters-21450,on 
whichmanyTCmethodswereevaluated[10,16,1,28,6,33, 22,31],buttheresultsontheolderversionsmaynotbedi­rectlycomparabletotheresultsonthenewversion.Forthis 
paperweusetheApteModversionofReuters-21578,which wasobtainedbyeliminatingunlabelleddocumentsandse­lectingthecategorieswhichhaveatleastonedocumentin 
thetrainingsetandthetestset.Thisprocessresultedin 90categoriesinboththetrainingandtestsets.Afterelim­inatingdocumentswhichdonotbelongtoanyofthese90 
categories,weobtainedatrainingsetof7769documents,a testsetof3019documents,andavocabulary24240unique wordsafterstemmingandstopwordremoval.Thenumber 
ofcategoriesperdocumentis1.3onaverage.Thecate­gorydistributionisskewed;themostcommoncategoryhas atraining-setfrequencyof2877,but82%ofthecategories 
havelessthan100instances,and33%ofthecategorieshave lessthan10instances.Figure1showsthecategorydistri­butioninthistrainingset. 
Documents Frequency 3000 2500 2000 1500 1000 500 0 Category rank (from common to rare) Figure1:CategorydistributioninReuters-21578AptoMod 
Forevaluatingtheefectivenessofcategoryassignments byclassiferstodocuments,weusethestandardrecall,pre­cisionandF1 
measure.Recallisdefnedtobetheratio ofcorrectassignmentsbythesystemdividedbythetotal numberofcorrectassignments.Precisionistheratioofcor­rectassignmentsbythesystemdividedbythetotalnum­berofthesystem'sassignments.TheFmeasure,initially 
1 introducedbyvanRijsbergen[26],combinesrecall(r)and precision(p)withanequalweightinthefollowingform: 
2rpF1(rp) : r+p Thesescorescanbecomputedforthebinarydecisionson eachindividualcategoryfrstandthenbeaveragedovercat­egories.Or,theycanbecomputedgloballyoverallthe 
nxmbinarydecisionswherenisthenumberoftotaltest documents,andmisthenumberofcategoriesinconsider­ation.Theformerwayiscalledmacro-averagingandthe 
latterwayiscalledmicro-averaging.Themicro-averagedF1 havebeenwidelyusedincross-methodcomparisonswhile 
macro-averagedFwasusedinsomecases[15].Itisunder­ 1 stoodthatthemicro-averagedscores(recall,precisionand 
F1)tendtobedominatedbytheclassifer'sperformance oncommoncategories,andthatthemacro-averagedscores aremoreinfuencedbytheperformanceonrarecategories. 
Providingbothkindsofscoresismoreinformativethanpro­vidingeitheralone,asweshowinourevaluationandcross 
methodcomparison(Section5). Wealsouseerrorasanadditionalmeasure,whichis defnedtobetheratioofwrongassignmentsbythesystem 
dividedbythetotalnumberofthesystem'sassignments (nxm). 3Classifers 3.1SVM SupportVectorMachines(SVM)isarelativelynewlearning 
approachintroducedbyVapnikin1995forsolvingtwo-class patternrecognitionproblems[27].ItisbasedontheStruc­turalRiskMinimizationprincipleforwhicherror-boundanal­ysishasbeentheoreticallymotivated[27,7].Themethodis 
defnedoveravectorspacewheretheproblemistofnda decisionsurfacethat\best"separatesthedatapointsintwo classes.Inordertodefnethe\best"separation,weneedto 
introducethe\margin"betweentwoclasses.Figures2and 3illustratetheidea.Forsimplicity,weonlyshowacaseina 
two-dimensionalspacewithlinearlyseparabledatapoints, buttheideacanbegeneralizedtoahighdimensionalspace 
andtodatapointsthatarenotlinearlyseparable.Ade­cisionsurfaceinalinearlyseparablespaceisahyperplane. Thesolidlinesinfgures2and3showtwopossibledecision 
surfaces,eachofwhichcorrectlyseparatesthetwogroupsof data.Thedashedlinesparalleltothesolidonesshowhow 
muchonecanmovethedecisionsurfacewithoutcausing misclassifcationofthedata.Thedistancebetweeneachset ofthoseparallellinesarereferredtoas\themargin".The 
SVMproblemistofndthedecisionsurfacethatmaximizes themarginbetweenthedatapointsinatrainingset. Moreprecisely,thedecisionsurfacebySVMforlinearly 
separablespaceisahyperplanewhichcanbewrittenas w~·~x;b0 ~xisanarbitrarydatapoint(tobeclassifed),andthevec­tor~theconstantbarelearnedfromatrainingsetof 
wandthiscomparisonisquiteinformative,severalpointsaremiss­ingorquestionable: Thoroughstatisticalsignifcancetestswerelacking. 
 Performanceanalysiswithrespectivetocategorydis­tribution,especiallyonrarecategories,wasnotpro­vided. 
 ThekNNresultreportedbyhimislowerthanthekNN  Figure2:Adecisionline(solid)withasmallermarginwhichis 
results byothers[31]. thedistancebetweenthetwoparalleldashedlines. For addressingtheabovepoints,wedecidedtore-test 
 Figure3:Thedecisionlinewiththemaximalmargin.Thedata light SVMusingtheSVMsystembyJoachims2 andourown 
versionofkNN. 3.2kNN kNNstandsfork-nearestneighborclassifcation,awell­knownstatisticalapproachwhichhasbeenintensivelystud­iedinpatternrecognitionforoverfourdecades[8].kNNhas 
beenappliedtotextcategorizationsincetheearlystagesof theresearch[17,29,11].Itisoneofthethetop-performing 
methodsonthebenchmarkReuterscorpus(the21450ver­ pointsonthedashedlinesaretheSupportVectors. linearlyseparabledata.LettingDf(yix~i)gdenotethe 
trainingset,andyi f1gbetheclassifcationfor~x(+1 2 forbeingapositiveexampleand-1forbeinganegativeex-ampleofthegivenclass),theSVMproblemistofndwand 
~bthatsatisfesthefollowingconstraints w~·x~i ;b.+1fory+1 (1) i ~·xi ;b; 1foryi ;1 (2) w~ andthatthevector2-normofwisminimized. 
~ TheSVMproblemcanbesolvedusingquadraticpro­grammingtechniques[27,7,23].Thealgorithmsforsolving linearlyseparablecasescanbeextendedforsolvinglinearly 
non-separablecasesbyeitherintroducingsoftmarginhyper­planes,orbymappingtheoriginaldatavectorstoahigher 
dimensionalspacewherethenewfeaturescontainsinterac­tiontermsoftheoriginalfeatures,andthedatapointsinthe 
newspacebecomelinearlyseparable[27,7,23].Relatively light efcientimplementationsofSVMincludetheSVMsys­tembyJoachims[12]andtheSequentialMinimalOptimiza­tion(SMO)algorithmbyPlatt[24]. 
AninterestingpropertyofSVMisthatthedecisionsur­faceisdeterminedonlybythedatapointswhichhaveex­actlythedistance1 
fromthedecisionplane.Thosepoints k~ wk arecalledthesupportvectors,whicharetheonlyefective elementsinthetrainingset;ifallotherpointswerere­moved,thealgorithmwilllearnthesamedecisionfunction. 
ThispropertymakesSVMtheoreticallyuniqueanddiferent frommanyothermethods,suchaskNN,LLSF,NNetand NBwhereallthedatapointsinthetrainingsetareused 
tooptimizethedecisionfunction.Itwouldbeinteresting toknowwhetherornotthistheoreticaldistinctionleadsto 
signifcantperformancediferencesbetweenSVMandother methodsinpractice. JoachimsrecentlyappliedSVMtotextcategorization, 
andcompareditsperformancewithotherclassifcationmeth-odsusingtheReuters-21578corpus.Hisresultsshowthat 
SVMoutperformedalltheothermethodstestedinhisex­perimentsandtheresultspublishedbythattime 1 .While 1Apteetal.laterpublishedbetterresultsofadecisiontreeap­proachusingboosting[2]. 
sion,Apteset);theothertop-performingmethodsinclude LLSFbyYang,decisiontreeswithboostingbyApteetal., andneuralnetworksbyWieneretal.[2,12,14,31,28]. 
ThekNNalgorithmisquitesimple:givenatestdocu­ment,thesystemfndstheknearestneighborsamongthe trainingdocuments,andusesthecategoriesofthekneigh­borstoweightthecategorycandidates.Thesimilarityscore 
ofeachneighbordocumenttothetestdocumentisusedas theweightofthecategoriesoftheneighbordocument.If severaloftheknearestneighborsshareacategory,thenthe 
per-neighborweightsofthatcategoryareaddedtogether, andtheresultingweightedsumisusedasthelikelihood scoreofthatcategorywithrespecttothetestdocument. 
Bysortingthescoresofcandidatecategories,arankedlist isobtainedforthetestdocument.Bythresholdingonthese 
scores,binarycategoryassignmentsareobtained.Thede­cisionruleinkNNcanbewrittenas: X ~~ y(~xcj) sim(~xdi)y(di 
cj);bj ~di2kNN ~~ wherey(di cj)2 01gistheclassifcationfordocumentdi f withrespecttocategorycj (y1forYES,andy0for 
~ NO);sim(~xdi)isthesimilaritybetweenthetestdocument ~ ~andthetrainingdocumentdi;andbj isthecategory­ 
x specifcthresholdforthebinarydecisions.Forconvenience, weusethecosinevalueoftwovectorstomeasurethesimilar­itybetweenofthetwodocuments,althoughothersimilarity 
measuresarepossible.Thecategory-specifcthresholdbj isautomaticallylearnedusinga\validationset"ofdocu­ments.Thatis,weusedasubsetofthetrainingdocuments 
(notusedthetestdocuments)tolearntheoptimalthreshold foreachcategory.Byoptimal,wemeanthethresholdthat 
yieldedthebestF1 scoreonthevalidationdocuments. Notethatthereisadiferencebetweenthethreshold­ingmethodaboveandthethresholdingmethodusedby 
JoachimsinhiskNNexperiment.Joachimssimplysorted theconfdencescorespertestdocumentandassignedthe top-rankingcategoryasthecorrectcategory.Thissimple 
2light The SVMsystem is publicly available via http://www-ai.cs.uni-dortmund.de/FORSCHUNG/ VERFAHREN/SVMLIGHT/svmlight.eng.html. 
methoddoesnotallowthesystemtoassignmultiplecate­goriestoanydocumentandisnotnecessarilytheoptimal strategyforkNN,oranyclassifer,becausedocumentsoften 
havemorethanonecategory[31].Wesuspectthissimplif­cationbyJoachimsisthereasonforthelowperformanceof hiskNN;thisassertionwasconfrmedbyourexperiments 
withbothversionsofkNNonReuters-21578(seetheresults inSection5). 3.3LLSF LLSFstandsforLinearLeastSquaresFit,amappingap­proachdevelopedbyYang[32].Amultivariateregression 
modelisautomaticallylearnedfromatrainingsetofdoc­umentsandtheircategories.Thetrainingdataarerepre­sentedintheformofinput/outputvectorpairswherethe 
inputvectorisadocumentintheconventionalvectorspace model(consistingofwordswithweights),andoutputvec­torconsistsofcategories(withbinaryweights)ofthecor­respondingdocument.Bysolvingalinearleast-squaresft 
onthetrainingpairsofvectors,onecanobtainamatrixof word-categoryregressioncoefcients: 2 F LS argmin F;Bk 
kA F wherematricesAandBpresentthetrainingdata(thecor­respondingcolumnsisapairofinput/outputvectors),and 
matrixFLS isthesolutionmatrix,defningamappingfrom anarbitrarydocumenttoavectorofweightedcategories.By 
sortingthesecategoryweights,arankedlistofcategoriesis obtainedfortheinputdocument.Bythresholdingonthese 
categoryweights,categoryassignmentstotheinputdocu­mentareobtained.Again,thesystemautomaticallylearn theoptimalthresholdforeachcategory,whichhasthesame 
defnitionasinkNN. AlthoughLLSFandkNNdiferstatistically,wehave foundthesetwomethodshadsimilarperformanceinall 
theapplicationswherewecomparedthesetwomethods,in­cludingthecategorizationofReutersnewsstories,MED-LINEbibliographicalabstractsandMayoClinicpatient­recorddiagnoses[32,29,31].Whatwehavenotcompared 
yetistheirrobustnessindealingwithrarecategories;this istheoneofthemainfociinthisstudy. 3.4NNet Neuralnetwork(NNet)techniqueshavebeenintensively 
studiedinArtifcialIntelligence[19].NNetapproachesto textcategorizationwereevaluatedontheReuters-21450cor­pusbyWieneretal.[28]andNg.etal.[22],respectively. 
Wieneretal.triedbothaperceptronapproach(without ahiddenlayer)andthree-layeredneuralnetworks(witha hiddenlayer).Ngetal.onlyusesperceptrons.Bothsys­temsuseaseparateneuralnetworkpercategory,learning 
anon-linearmappingfrominputwords(ormorecomplex featuressuchassingularvectorsofadocumentspace)toa category.Wiener'sexperimentssuggestedsomeadvantage 
forcombiningamultiple-classNNet(forhigher-levelcate­gories)andmanytwo-classnetworks(forlowest-levelcate­gories),buttheydidnotcomparetheperformanceofusing 
amultiple-classNNetalonetousingatwo-classNNetfor eachcategory. InourexperimentswithNNetonReuters21578,aprac­ticalconsiderationisthetrainingcost.Thatis,training 
NNetisusuallymuchmoretimeconsumingthantheother classifers.ItwouldbetoocostlytotrainoneNNetper category,wethendecidedtotrainoneNNetonallthe90 
categoriesofReuters.Inthissense,ourNNetapproachis notexactlythesameasthepreviousreportedones(wewill leavethecomparisonforfutureresearch).Wealsodecided 
touseahiddenlayerwithknodes,wherekisempirically chosen(Section5).WeimplementedourownNNetsystem forefcienthandlingofsparsedocumentvectors. 
3.5NB NaiveBayes(NB)probabilisticclassifersarecommonlystud­iedinmachinelearning[19].Anincreasingnumberofeval­uationsofNBmethodsonReutershavebeenpublished[16, 
20,13,3,18].ThebasicideainNBapproachesistousethe jointprobabilitiesofwordsandcategoriestoestimatethe 
probabilitiesofcategoriesgivenadocument.Thenaivepart ofNBmethodsistheassumptionofwordindependence,i.e., 
theconditionalprobabilityofawordgivenacategoryisas­sumedtobeindependentfromtheconditionalprobabilities 
ofotherwordsgiventhatcategory.Thisassumptionmakes thecomputationoftheNBclassifersfarmoreefcientthan theexponentialcomplexityofnon-naiveBayesapproaches 
becauseitdoesnotusewordcombinationsaspredictors. ThereareseveralversionsoftheNBclassifers.Recent studiesonamultinomialmixturemodelhavereportedim­provedperformancescoresforthisversionoversomeother 
commonlyusedversionsofNBonseveraldatacollections, includingReuters-21578[18,3].Thisimprovedmodel,how­ever,wasonlyevaluatedonafewcommoncategories(the 
10mostcommononesoutofthetotalof90categories)of Reuters;itsresultsthereforedonotallowacompletecom­parisontothosepreviouslyreportedforNBmethodsor 
othermethodsonthefullsetofReuterscategories.An­otherconfusingaspectoftherecentevaluationswithNBis anon-conventional\accuracy"measure{theproportionof 
thecorrectcategoryassignmentsamongthetotalofnas­signments(nisthenumberoftestdocuments)whereeach documentisassignedtooneandonlyonecategory[13,3,18]. 
Thisnarrowlydefned\accuracy"isindeedequivalenttothe standardprecisionundertheone-category-per-documentas­sumptiononclassifers,andalsoequivalenttothestandard 
recallassumingthateachdocumenthasoneandonlyone correctcategory.Itisnotequivalent,however,tothestan­darddefnitionforaccuracyintextcategorizationliterature, 
whichistheproportionofcorrectassignmentsamongthebi­narydecisionsoverallcategory/documentpairs[26,16].The 
standardaccuracymeasureiswell-defnedfordocuments withmultiplecategories;thenarrowlydefned\accuracy" isnot.Thelatterleadstoconfusionandnon-comparable 
performancemeasuresintextcategorizationevaluationson severalcollections,contributingtothedifcultyofcross­methodand/orcross-collectioncomparisons. 
ToprovidecomparableresultsofNBonReuters-21578, weranthemultinomialmixturemodelofNBbyMcCallum3 , andevaluateditsoutputusingthestandardperformance 
measuresintroducedinSection2. 4SignifcanceTests Wedesignedasetofsignifcancetestsforcomparingtwo systemsusingvariousperformancemeasures.Wewilldefne 
thetestsfrst,andthendiscusstheirsuitabilitywithrespect totheperformancemeasures. 3TheNBclassi.ersbyMcCallumetal.arepubliclyavailablein 
theBowlibraryatcmuviahttp://www.cs.cmu.edu/mccallum/bow/. 4.1Microsigntest(s-test) 4.3Macrot-test(T-test) 
Thisisasigntestdesignedforcomparingtwosystems,A Thisisat-testforcomparingtwosystems,AandB,using andB,basedontheirbinarydecisionsonallthedocu-thepairedF1 
valuesforindividualcategories.Forthiswe ment/categorypairs.Weusethefollowingnotation: usethesamenotationasdefnedforS-test,andaddthe 
 Nisthenumberofbinarydecisionsbyeachsystem, i.e.,theproductofthenumberoftestdocumentsand thenumberofcategories; 
 ai 2f01gisthemeasureofsuccessforsystemAon theithdecision(i1,2...N),where1meanscorrect and0meansincorrect; 
 bi 2f01gisthemeasureofsuccessforsystemBon theithdecision;  nisthenumberoftimesthatai andbi difer; 
 kisthenumberoftimesthatai islargerthanbi.  Thenullhypothesisisk0:5n,orkhasabinomial distributionofBin(np)wherep0:5.Thealternativehy­pothesisisthatkhasabinomialdistributionofBin(np) 
wherep>:5,meaningthatsystemAisbetterthansys­temB.Ifn12andk>0:5n,theP-value(1-sided) iscomputedusingthebinomialdistributionunderthenull 
hypothesis: .. n Xn P(Zk) x0:5n : i i=k Symmetrically,ifn12andk0:5n,theP-valueforthe otherextremeiscomputedusingtheformula 
.. k Xn P(Zk) x0:5n : i i=0 TheP-valueindicatesthesignifcanceleveloftheobserved evidenceagainstthenullhypothesis,i.e.,systemAisbetter 
(orworse)thansystemB. Ifn>12,theP-value(1-sided)canbeapproximately computedusingthestandardnormaldistributionfor 
k;0:5n Z: p 0:5n 4.2Macrosigntest(S-test) Thissigntestisdesignedforcomparingtwosystems,Aand B,usingthepairedF1valuesforindividualcategories.We 
usethefollowingnotation: Misthenumberofuniquecategories;  ai 2[01]istheF1scoreofsystemAontheithcate­ 
 gory(i1,2...M);  bi 2[01]istheF1scoreofsystemBontheithcate­ gory(i1,2...M);  nisthenumberoftimesthatai 
andbi difer;  kisthenumberoftimesthatai islargerthanbi.  ThetesthypothesesandtheP-value(1-sided)computa­tionarethesameasthoseasinthemicros-test. 
followingitems: di ai ;bi isthediferenceofai frombi; e  disthesimpleaverageofthedi valuesfori12:::n. 
 e Thenullhypothesisisd0.Thealternativehypothesis e isd>0.Ifn40,theP-valueiscomputedusingthe t-distributionwiththedegreeoffreedom(d.f.)ofn;1for 
e d T ; s:e:(de) otherwise,thestandardnormaldistributionisusedinstead. 4.4Macrot-testafterranktransformation 
TocomparesystemsAandBbasedontheF1valuesafter ranktransformation[4],inwhichtheF1 valuesofthetwo systemsonindividualcategoriesarepooledtogetherand 
sorted,thenthesevaluesarereplacedbythecorresponding ranks.TomakeadistinctionfromtheT-testabove,werefer 
tothistestasT'-test.Weusethefollowingthenotation: 0 ai istherankoftheF1scoreofsystemAontheith category(i1,2...M); 
b0 i istherankoftheF1 scoreofsystemBontheith category(i1,2...M); d00 0 b0 i ;b0 i ai istherankdiferenceofai 
fromi; 0 b0 nisthenumberoftimesthata6 ii; de0isthesimpleaverageofthed0 12:::n. i valuesfori de0 Thenullhypothesisis0.Thealternativehypothesis 
de0 is>0.Ifn40,computetheP-value(1-sided)for de0 T>s:e:(de0) usingthet-distributionwithadegreeoffreedom(d.f.)of 
n;1;otherwise,thestandardnormaldistributionisused instead. 4.5Comparingproportions(p-test) Fortheperformancemeasureswhichareproportions,such 
asrecall,precision,errororaccuracy,wecomparetheper­formancescoresofsystemsAandBasbelow. Letpa andpb 
betheperformancescoresbysystemsA andB,respectively,  Letna andnb bethenumbersoftrialsinthetwo  samplesthatareusedtoevaluationsystemsAandB, 
respectively.Thedefnitionofna ornb dependsonthe performancemeasures: {forrecall,itisthenumberoftrueYESesfor 
categories; {forprecision,itisthenumberassignedYESes bythesystem;and {foraccuracyorerror,itisthenumberofdocument­categorypairs. 
naXpa+nbXpb Computep ,theobservedproportion na+nb ofthetotalofnna +nb trials. Thenullhypothesisispa pb 
p.Thealternative hypothesisispa >pb.Ifn40,computetheP-value(1­sided)for a ;pb Zp p(1;p)(1 na +1 nb) usingthet-distributionwithadegreeoffreedom(d.f.)of 
n;1;otherwise,usethestandardnormaldistributionin­stead.Thep-testallowsystemsAandBtobeevaluated usingtwodiferenttestcollectionsaswellasusingasame 
testcollection.Inthecaseofna nb,thecomputationis pa+pb pa;pb simplifedasp,andZp. 2 2p(1;p)/n Amongthetestingmethodsdescribedabove,s-testand 
p-testaredesignedtoevaluatetheperformanceofsystems atamicrolevel,i.e.,basedonthepooleddecisionsonin-dividualdocument/categorypairs.Ontheotherhand,S­test,T-testandT'-testaredesignedtoevaluateatamacro 
level,usingtheperformancescoresoneachcategoryasthe unitmeasure.Amongthesethreemacrotests,S-testmay bemorerobustforreducingtheinfuenceofoutliers,but 
risksbeinginsensitive(ornotsufcientlysensitive)inper­formancecomparisonbecauseitignorestheabsolutedif­ferencesbetweenF1 
values.TheT-testissensitivetothe absolutevalues,butcouldbeoverlysensitivewhenF1scores arehighlyunstable,e.g.,thoseforlow-frequencycategories. 
TheT'-testisacompromisebetweenthetwoextremes;it islesssensitivethanT-testtooutliers,butmoresensitive 
thanthesigntestbecauseitreservetheorderofdistinct 1 values.Noneofthetestsis\perfect"foralltheperfor­mancemeasures,orforperformanceanalysiswithrespectto 
askewedcategorydistribution,sousingthemjointlyinstead usingonetestalonewouldbeabetterchoice. Inrelatedliterature,signtestswerereportedbyCo­henformethodcomparisonbasedonmicro-levelcategory 
assignments[5],andbyLewisetal.foracomparisonbased onpairedF1 valuesofindividualcategories[15]. 5Evaluation 
5.1Experimentssetup Weappliedstatisticalfeatureselectionatapreprocessing stageforeachclassifer,usingeitheraX2 
statisticorinfor­mationgaincriteriontomeasuretheword-categoryassoci­ations,andthepredictivenessofwords(features).Diferent 
feature-setsizesweretested,andthesizethatoptimizedthe globalF1 scoreforaclassiferwaschosenforthatclassifer. 
Asaresult,weselected1000featuresforNNet,2000fea-turesforNB,2415featuresforkNNandLLSF,and10000 featuresforSVM. 
Otherempiricalsettingswere: ThekinkNNwassetto45.Thischoicewasbased onourpreviousparameteroptimization(learnedfrom 
trainingdata)onReuters-21450[31].  ThenumberofsingularvalueusedforLLSFcompu­tationwassetto500,whichisalsobasedonprevious 
parameteroptimization(learnedfromtrainingdata) onReuters-21450[31].  Thenumberofhiddenunitsinthemiddlelayerof 
NNetwassetto64.Thischoiceproducesthebest 1 scoreforNNetonavalidationset(apartofthe trainingdata)ofReuters-21578whenwevariedthe 
numberofthehiddenunitsbetween16,64and160.  ForSVMwetestedthelinearandnon-linearmodels oferedbySVMlight,andobtainedaslightlybetter 
resultwiththelinearSVMthanwiththenon-linear models.Weusethelinearversionastherepresentative forSVMinthecross-methodcomparison. 
 5.2Results Table1summarizestheglobalperformancescores.Several pointsinthistableareworthdiscussion. 
 Themicro-averagedF1score(.8599)ofSVMisslightly lowerthanthebestoftheSVMscores(.860-.864)re­portedbyJoachims,possiblybecauseofadiference 
inourtermweightingschemefordocumentpresenta­tion,orduetominordiferencesindatapreparation. Joachimsusedthewithin-documentfrequencyofterms 
(tf)directly,whileweusedlog(tf)instead.Thisdif­ferenceisnotsignifcant.  Ourmicro-averagedF1 score(.8567)forkNNisno­ticeablyhigherthanthekNNscore(0.823)reported 
byJoachims.WealsotestedthesimplifedkNN(fol­lowingJoachims)whichassignonlythetop-ranking categorytoeachdocument,andobtainedaF1 
score of.8140.Thesecontrastingtestssuggestthatthissim­plifcationisneitheroptimalnornecessaryforkNN. 
 Ourmicro-averagedF1 score(.7956)forNBissignif­icantlyhigherthantheNBscore(0.720)obtainedby Joachims.AccordingtotheanalysisbyMcCallumat 
al.[18]whoimplementedbothmodels,themultinomial mixturemodelwhichwetestedisbetterthanthemulti­variateBernoullimodelwhichJoachimstested.Soour 
experimentconfrmedMcCallum'sconclusion.How­ever,thebettermodelofNBstillunderperformsSVM, KNNandLLSF. 
 5.3Cross-classiferComparison Table2summarizesthestatisticalsignifcancetests.Using eachcolumnofthistable,onecanobtainacompleteor 
partialorderoftheclassifers.Themicro-levelanalysis(s­test)onpooledbinarydecisionssuggeststhat SVM>kNN 
fLLSFNNet NB g wheretheclassiferswithinsignifcantperformancedifer­encesaregroupedintooneset.Ontheotherhand,the 
macro-levelanalyses(S-test,T-testandT'-test)onthe 1 scoressuggestasomewhatdiferentgroupingandorder­ingoftheclassifers: 
fSVMkNNLLSFg fNBNNetg Theseinconsistentgroupingandorderingsofclassifersre­fectthebiasesintheperformancemeasures.Themicro­levelsignifcancetestisdominatedbytheperformanceof 
theclassifersoncommoncategories,whilethemacro-level signifcancetestsaremorerefectiveoftheperformanceof 
theclassifersonrarecategories.Thisdoesnotnecessarily meanthatthesignifcancetestsareinvalid;onthecontrary, 
Table1:Performancesummaryofclassifers method miR miP miF1 maF1 error SVM .8120 .9137 .8599 .5251 .00365 
KNN .8339 .8807 .8567 .5242 .00385 LSF .8507 .8489 .8498 .5008 .00414 NNet .7842 .8785 .8287 .3765 .00447 
NB .7688 .8245 .7956 .3886 .00544 miRmicro-avgrecall;miPmicro-avgprec.; miF1micro-avgF1;maF1macro-avgF1. 
Table2:Statisticalsignifcancetestresults sysA sysB s-test S-test T-test T'-test SVM kNN > . . . SVM LLSF 
. . . kNN LLSF . . . SVM NNet kNN NNet LLSF NNet . NB kNN « « « « NB LLSF « « « « NB SVM « « « « NB NNet 
« . . . \"or\«"meansP-value0.01; \>"or\"means0.01P-value0.05; \."meansP-value>0.05. Table3:p-testappliedtomultiplemeasures 
sysA sysBmiRmiP error SVM kNN« . SVM LLSF« « kNN LLSF SVM NNet « kNN NNet . « LLSF NNet « NB kNN« « NB 
LLSF« « NB SVM« « NB NNet. « itmeansthatthosediferenttestsprovidecomplementary analysesabouttheclassifers.Moreover,onecancombine 
evidencefromdiferenttests.Forexample,whentheS-test, T-testandT'-testallagreeonaP-valuerange(e.g.,), onecanbemoreconfdentaboutthesuggestedsignifcance 
thanthecasewhentheydonotagree. Table3showsadditionalmicro-levelperformanceanal­ysisusingp-testwithmultiplemeasures.Error-ratebased 
comparisonleadstoanorderedlistofequivalentclasses fSVMkNNg>LLSF>NNetNB where\>"or\"indicatesabetterclassifer(withsmaller 
error)ontheleft-handsidethantheoneontheright-hand side.Theorderamongclassiferssuggestedbytheerror­basedtestsissimilartotheoneobservedinthemicro-level 
signtests,exceptthatSVMandkNNaregroupedtogether here,andthatLLSFandNNetarenolongergrouped.P-test onrecall(miR)andprecision(miP)canalsobeinformative 
whentheobservationsarejointlyused.Forexample,since NB«kNNinbothrecallandprecision,weconcludethat kNNissignifcantlybetter;ontheotherhand,forkNNver­susSVM,wecannotdecidewhichoneisbetterbecausethe 
p-testoutcomesonrecallandprecisionarenotinagreement. Figures4and5comparetheperformancecurvesofthe fveclassiferswithrespecttothetraining-setfrequencyof 
categories.Thesecurvesareobtainedbydividingthehor­izontalaxisintoequal-sizedintervals,averagingtheper­categoryF1 
scoresperintervalforeachclassifer,andinter­polatingtheper-intervalaveragescores.Figure4focuseson thetrainingsetfrequencyintherangefrom1to60(cover­ing67%ofthetotaluniquecategories),whereNNetandNB 
areclearlyworsethantheotherthree,butthesethreeare lesseasytorank.Figure5showstheperformancecurveson thefullrangeoftraining-setfrequenciesofcategories,where 
theefectivenessofalltheclassifersaremoresimilartoeach otheroncommoncategories(withatraining-setfrequency 
of300orhigher),comparedtotheirrelativeperformanceon rarecategories. training-set frequency of category 
Figure4:Performancecurvesonrarecategories. Figure5:Performancecurvesonallthecategories. 6Conclusions 
Inthispaperwepresentedacontrolledstudywithsignif­canceanalysesonfvewell-knowntextcategorizationmeth­ods.Ourmainconclusionsare: 
 Signifcanceanalysescanbeappliedtobothamicro­ levelandmacro-levelevaluationoftextcategorization systems,andjointlyusedforcross-methodcompari­son. 
 Theoutcomeofasignifcancetestdependsonthe choiceofperformancemeasure,thesensitivityofthe test,andthetraining-setfrequencyofcategoriesbeing 
tested.  Forthemicro-levelperformanceonpooledcategory assignments,bothasigntestandanerror-basedpro­portiontestsuggestthatSVMandkNNsignifcantly 
outperformtheotherclassifers,whileNBsignifcantly underperformsalltheotherclassifers.  Withrespecttothemacro-level(category-level)per­formanceanalysisusingF1,allthesignifcancetests 
weconductedsuggestthatSVM,kNNandLLSFbe­longtothesameclass,signifcantlyoutperformingNB andNNet. Acknowledgment 
WewouldliketothankBoraCenkGazenforverifyingour resultsinthestatisticalsignifcancetests,andYuePanfor extendingtheNBmethodtoallowcategoryrankingand 
multiplelabelsperdocument. References [1]C.Apte,F.Damerau,andS.Weiss.Towardslanguageinde­pendentautomatedlearningoftextcategorizationmodels. 
InProceedingsofthe17thAnnualACM/SIGIRconference, 1994. [2]C.Apte,F.Damerau,andS.Weiss.Textminingwithde­cisionrulesanddecisiontrees.InProceedingsoftheCon­ferenceonAutomatedLearningandDiscorery,Workshop6: 
LearningfromTextandtheWeb,1998. [3]L.DouglasBakerandAndrewK.Mccallum.Distributional clusteringofwordsfortextcategorization.InProceedingsof 
the21thAnnIntACMSIGIRConferenceonResearchand DevelopmentinInformationRetrieval(SIGIR'98),pages 96{103,1998. 
[4]D.BerryandB.W.Lindgren.Statistics:TheoryandMeth­ods.Brooks/Cole,PacifcGrove,California,1990. [5]WilliamW.Cohen.Textcategorizationandrelationallearn­ing.InTheTwelfthInternationalConferenceonMachine 
Learning(ICML'95).MorganKaufmann,1995. [6]WilliamW.CohenandYoramSinger.Context-sensitive learningmethodsfortextcategorization.InSIGIR'96: 
Proceedingsofthe19thAnnualInternationalACMSIGIR ConferenceonResearchandDevelopmentinInformation Retrieval,1996.307-315. 
[7]C.CortesandV.Vapnik.Supportvectornetworks.Machine Learning,20:273{297,1995. [8]BelurV.Dasarathy.NearestNeighbor(NN)Norms:NN 
PatternClassifcationTechniques.McGraw-HillComputer ScienceSeries.IEEEComputerSocietyPress,LasAlamitos, 
California,1991. [9]N.Fuhr,S.Hartmanna,G.Lustig,M.Schwantner,and K.Tzeras.Air/x-arule-basedmultistageindexingsys­temsforlargesubjectfelds.In606-623,editor,Proceedings 
ofRIAO'91,1991. [10]P.J.HayesandS.P.Weinstein.Construe/tis:asystemfor content-basedindexingofadatabaseofnewstories.InSec­ondAnnualConferenceonInnovativeApplicationsofArti-fcialIntelligence,1990. 
[11]MakatoIwayamaandTakenobuTokunaga.Cluster-based textcategorization:acomparisonofcategorysearchstrate-gies.InProceedingsofthe18thAnnIntACMSIGIRCon­ferenceonResearchandDevelopmentinInformationRe­trieval(SIGIR'95),pages273{281,1995. 
[12]ThorstenJoachims.TextCategorizationwithSupportVec-torMachines:LearningwithManyRelevantFeatures.In 
EuropeanConferenceonMachineLearning(ECML),1998. [13]D.KollerandM.Sahami.Hierarchicallyclassifyingdocu-mentsusingveryfewwords.InTheFourteenthInterna­tionalConferenceonMachineLearning(ICML'97),pages 
170{178,1997. [14]W.LamandC.Y.Ho.Usingageneralizedinstancesetfor automatictextcategorization.InProceedingsofthe21th 
AnnIntACMSIGIRConferenceonResearchandDevel­opmentinInformationRetrieval(SIGIR'98),pages81{89, 1998. [15]DavidD.Lewis,RobertE.Schapire,JamesP.Callan,and 
RonPapka.Trainingalgorithmsforlineartextclassifers. InSIGIR'96:Proceedingsofthe19thAnnualInternational 
ACMSIGIRConferenceonResearchandDevelopmentin InformationRetrieval,1996.298-306. [16]D.D.LewisandM.Ringuette.Comparisonoftwolearn­ingalgorithmsfortextcategorization.InProceedingsofthe 
ThirdAnnualSymposiumonDocumentAnalysisandInfor­mationRetrieval(SDAIR'94),1994. [17]B.Masand,G.Linof,andD.Waltz.Classifyingnewsstories 
usingmemorybasedreasoning.In15thAnnIntACMSIGIR ConferenceonResearchandDevelopmentinInformation Retrieval(SIGIR'92),pages59{64,1992. 
[18]A.McCallumandK.Nigam.Acomparisonofeventmodels fornaivebayestextclassifcation.InAAAI-98Workshopon 
LearningforTextCategorization,1998. [19]TomMitchell.MachineLearning.McGrawHill,1996. [20]I.Moulinier.Islearningbiasanissueonthetextcatego­rizationproblem?InTechnicalreport,LAFORIA-LIP6, 
UniversiteParisVI,1997. [21]I.Moulinier,G.Raskinis,andJ.Ganascia.Textcatego­rization:asymbolicapproach.InProceedingsoftheFifth 
AnnualSymposiumonDocumentAnalysisandInformation Retrieval,1996. [22]H.T.Ng,W.B.Goh,andK.L.Low.Featureselection,per­ceptronlearning,andausabilitycasestudyfortextcate­gorization.In20thAnnIntACMSIGIRConferenceon 
ResearchandDevelopmentinInformationRetrieval(SI­GIR'97),pages67{73,1997. [23]Osuna,R.Freund,andF.Girosi.Supportvectormachines: 
Trainingandapplications.InA.I.Memo.MITA.I.Lab, 1996. [24]J.Platt.Sequetialminimaloptimization:Afastalgorithm 
fortrainingsupportvectormachines.InTechnicalReport MST-TR-98-14.MicrosoftResearch,1998. [25]K.TzerasandS.Hartman.Automaticindexingbasedon 
bayesianinferencenetworks.InProc16thAnnIntACM SIGIRConferenceonResearchandDevelopmentinInfor­mationRetrieval(SIGIR'93),pages22{34,1993. 
[26]C.J.vanRijsbergen.InformationRetrieval.Butterworths, London,1979. [27]V.Vapnic.TheNatureofStatisticalLearningTheory. 
Springer,NewYork,1995. [28]E.Wiener,J.O.Pedersen,andA.S.Weigend.Aneural networkapproachtotopicspotting.InProceedingsofthe 
FourthAnnualSymposiumonDocumentAnalysisandIn­formationRetrieval(SDAIR'95),1995. [29]Y.Yang.Expertnetwork:Efectiveandefcientlearning 
fromhumandecisionsintextcategorizationandretrieval.In 17thAnnIntACMSIGIRConferenceonResearchandDe­velopmentinInformationRetrieval(SIGIR'94),pages13{ 
22,1994. [30]Y.Yang.Samplingstrategiesandlearningefciencyintext categorization.InAAAISpringSymposiumonMachine 
LearninginInformationAccess,pages88{95,1996. [31]Y.Yang.Anevaluationofstatisticalapproachestotext categorization.JournalofInformationRetrieval(toappear), 
1999. [32]Y.YangandC.G.Chute.Anexample-basedmapping methodfortextcategorizationandretrieval.ACMTransac-tiononInformationSystems(TOIS),12(3):252{277,1994. 
[33]Y.YangandJ.P.Pedersen.Featureselectioninstatistical learningoftextcategorization.InTheFourteenthInter­nationalConferenceonMachineLearning,pages412{420, 
1997.
			