
 Proceedings of the 1998 Winter Simulation Conference D.J. Medeiros, E.F. Watson, J.S. Carson and M.S. 
Manivannan, eds. GRANULARITY, AFFORDABILITY AND UTILITY IN BUSINESS PROCESS SIMULATION Mark Grabau Andersen 
Consulting, LLP 1666 K Street, NW Washington, DC 20006-2873 USA  ABSTRACT This paper seeks modeling 
guidelines by which project teams may respond confidently to the client s urgency while simultaneously 
assuring valuable and valid analyses. We conclude that de-scoping is the only reliable approach for accelerating 
a business process simulation project in response to client pressures. A thought experiment supports 
this conclusion. INTRODUCTION Recognition of the value of business process simulation has emerged in 
recent years (Hansen, 1998; Profozich, 1998). However, as appreciation rises, so does urgency. Appalled 
by the realization of how costly, non-competitive, and otherwise ineffective current processes are, client 
executives often demand rapid results from business process simulation projects. Time-to-market for business 
process engineering work is every bit as important as the cycle time of the modeled process. Drawing 
on our experience with the project process of business process simulation development, we seek modeling 
guidelines by which the project team can respond confidently to clients urgency while still assuring 
valuable, valid analysis results. In the following section, project management and control realities 
suggest that generally de-scoping the simulation is the only reliable approach to accelerating a business 
process simulation project. We then consider how the process experts on whom the simulation project team 
must rely also encourage de-scoping the process model. We then present our experience-won insights about 
approximating activity times as a technique to de-scope the data collection phase of a business process 
simulation project. Lastly, we discuss our disappointing thought experiments to infer guidelines about 
when and how to use high-level process modeling (i.e., with few details) while still assuring utility 
of the simulation results. Frank Grange Andersen Consulting, LLP 1225 17th St., Suite 3200 Denver, Colorado 
80202-5532 USA  2 PROJECT BALANCE Every project manager confronts a reality packaged neatly in the old 
saw: You can have it fast, you can have it good, you can have it cheap pick two. Figure 1 presents an 
influence diagram depicting the usually conflicting relationships among cost, quality, and schedule in 
business process simulation projects. Analysis of this figure exposes complex and unpleasant realities 
of a balance in simulation project management. Figure 1: Influence Diagram of Project Balance As the 
breadth and depth (i.e., scope) of a business process simulation increase, so will the effort, schedule, 
and cost of the project. As the schedule extends (e.g., through delays or other difficulties in obtaining 
needed data), the cost of dedicating the project resources will rise. The reverse relationships to cost 
overruns are not always consistent. As the cost or effort alone increase (e.g., due to misestimating 
the productivity of a simulation staff, or to misestimating the labor or other costs), the project manager 
may crash (accelerate) the schedule (e.g., I m spending all this money anyway, so ), or conversely stretch 
the schedule by assigning less costly, less productive, inexperienced staff. Also, as the perceived cost 
increases, either the project manager may decide to de­scope the project by eliminating features from 
the business process simulation, or the manager may decide to expand the scope in an attempt to justify 
the increased project cost by delivering more value. Finally, project delays trigger immediate cost increases 
(project labor tied up unexpectedly longer), which typically provoke a de-scoping response from the project 
manager or client. However inconsistent all the cause-effect relationships may be, the balance among 
cost, schedule, and scope in business simulation projects tends to force a de-scoping in reaction to 
unexpected cost or schedule overruns or, equivalently, to client urgency. Reducing project scope in business 
simulation generally takes one or more of the following forms: Devote low effort to estimating the activity 
times and their associated probability distributions  Model the business process at a high level, i.e., 
with little detail  Simplify the business rules that define jointly the structure of a modeled business 
process and the conditional flow of work among the resources, queues, and activities of the process 
 Taking one or more of these actions, the project manager can minimize damage to the cost and schedule 
of the business process simulation project. However, the decision-making value of the business process 
simulation results from accurate estimates of process cycle times and resource bottlenecks. Can the project 
manager rely on any of the methods above to control project schedule and cost while assuring the decision-making 
value of the results? APPROPRIATE EXPECTATIONS In contrast to most manufacturing and distribution applications, 
business process simulations rely on human beings to provide nearly all information that could comprise 
a model. Understanding the unique human constraints of the process experts can help the project team 
formulate an appropriate perspective on limits to business process simulation quality. Early in all business 
process simulation work, the project team will interview process experts who acquaint the simulation 
modelers with the process subject matter, activities, inputs, outputs, intermediate entities, resources, 
queues, arriving workload, and governing business rules. Both current process performers ( as-is ) and 
prospective process designers ( to-be ) are generally comfortable talking about the what of the business 
process. Difficulties do often arise in identifying queues, which are frequently invisible or physically 
subtle in business processes (and in contrast to manufacturing). In addition, business rules (especially 
those regarding queues) are challenging to elicit from the process experts. The process immaturity of 
the parent organization explains some of these difficulties (Paulk, et al, 1994). With reference to the 
Software Engineering Institute (SEI) Capability Maturity Model, business process engineering efforts 
often attempt to assist organizations at Level 1 and Level 2 of process maturity. Processes of a Level 
1 organization succeed through non-repeatable, heroic performances of individuals. Process communication 
in a Level 1 organization is ad hoc in the sense that process performers talk to one another about the 
non-repeatable specifics of their immediate work, and not about the overall process they perform. How 
could we make this process better? is a question that arises only informally and infrequently. An organization 
of Level 2 maturity often has defined, repeatable, even documented management processes, but has its 
core business processes (i.e., those that produce the goods and services of the enterprise) only documented 
at the task level (e.g., task instructions). Functional departments organize, manage, and reward core 
business process performers, who lack perspective and real visibility of the end-to-end business process. 
Business process simulation projects that address organizations of Level 1 or 2 maturity inevitably encounter 
process experts who have an incomplete picture, and who have given little or no thought to a project 
team s tough questions about queues and business rules. Questions of a quantitative focus, such as activity 
times or workload arrival rates, are even more vexing with reference again to the SEI Capability Maturity 
Model, only an organization of Level 4 maturity would have institutionalized and managed itself with 
quantitative measurements across the enterprise. Thus, the business process simulation project team can 
almost always expect to encounter individuals who really do not know off-hand the information necessary 
for modeling a current or future process design. Without question, Level 1 process performers find the 
typical data call a provocative, intimidating experience ( Why don t I know these things? I do this job. 
), and both Level 1 and 2 performers realize that obtaining answers will take a great deal of time away 
from very difficult, urgent, essential everyday work. With ineffective leadership, process performers 
may become extremely reluctant or even intransigent about providing modeling information at a realistic 
level of detail. Thus, not only cost-and schedule­pressures but also the inherent inertia of process 
experts favor de-scoping measures as a path of least resistance to complete the business process simulation. 
The psychological challenge to effective business process information gathering is to prevent accidental 
de­scoping because the process experts cooperation wanes or dies. Enthusiastic process experts will volunteer 
vital process information. Checklists can help the simulation project team ask complete questions. However, 
we have found that business rules can be so process-specific that we have yet to define or discover a 
perfect checklist or question that guarantees we have properly captured every business rule about resources 
and queue management. APPROXIMATING ACTIVITY TIMES Approximating activity times can reduce a simulation 
project scope. Reduced quantitative data elicitation from process experts will shorten the lead-time 
to build the model. Simple, approximate probability distributions may shorten simulation run-times over 
those involving more complex probability distributions (e.g., generating gamma variates). Finally, simple, 
approximate probability distributions may streamline model validation by process experts who can understand 
the parameters of the simple probability distributions used, in contrast to the more exotic probability 
distributions that objective maximum likelihood and goodness-of-fit analyses suggest (e.g., Johnson, 
gamma, or beta distributions). In a business process simulation project, activity time data may be available 
from: Extensive interview with process performers, owners, or other experts  Direct observation via 
industrial engineering time measurement (e.g., time-and-motion study or work sampling)  Comparison and 
construction with standards (e.g., Methods Time Measurement )  Quick-and-dirty, subjective estimates 
 Subjective activity time estimates require the least effort and time. Consequently, business process 
simulation projects frequently collect activity time data according to any of the following simple distributions: 
 An exponential distribution based only on an estimated average activity time  A uniform distribution 
based on the lowest and highest possible activity times (or, for example, on subjective estimates of 
10-percentile and 90-percentile activity times)  A triangular distribution based on the lowest, most 
likely (mode), and highest activity times (again, possibly using 10- and 90-percentiles to infer the 
extremes) In the authors (and others ) experience, assuming any of these distributions for activity 
times can yield several project balance and quality impacts on a business process simulation project: 
 Exponential distributions for activity time imply a mode at the left. This shape rarely corresponds 
with a  real process, which is likely to exhibit both a left and right tail in its activity times. The 
simulation will overestimate small activity times. In a simulation run, exponential distributions for 
activity time will occasionally yield high outlier values. The generation of such outlier values create 
a long, slow warmup period for steady-state simulations, and high variances for performance measures 
which require many replications to achieve a target confidence level. In contrast, activity time distributions 
with finite tails will converge to steady­state relatively quickly.  To achieve steady-state more quickly, 
uniform and triangular distributions are preferable to exponential distributions for activity times. 
 The uniform distribution is often an accurate reflection of a process expert s uncertainty and ignorance 
about an activity time. However, the uniform also does not present the simulation a mode, which is likely 
to exist for the real activity time.  Irrespective of whether the project team elicits 10/90­or 0/100-percentiles 
for the extreme activity times, process experts tend to overestimate minimum and underestimate maximum 
activity times though the process experts will not reliably offer such restricted ranges.  Many process 
experts are uncomfortable about stipulating only a range for a uniform distribution, and would prefer 
to provide three values for a triangular distribution.  When the performance criteria for deciding among 
process designs involve high percentiles (e.g., 99­percentile order cycle time ), the finite uniform 
and triangular distributions may prevent generation of the extreme activity time values that could be 
responsible for the extreme end-to-end process performance measures. The difficulty of process experts 
to estimate extreme values accurately compounds the adverse quality impact of using inappropriate uniform 
and triangular distributions in such a situation.  Despite these drawbacks, we continue to recommend 
using the simple, subjective probability distributions for activity times. The positive benefits to business 
process simulation project schedule and cost overshadow the drawbacks. Moreover, the process experts 
limitations with quantitative estimates, and the experts limited comprehension of other distributions 
in a model validation, create project uncertainties and delays that destroy any timely decision-making 
value of more objectively selected distributions. Mindful of the limitations of these distributions and 
process experts, the business process simulation project team can somewhat mitigate the adverse impacts 
of using these simple, subjective distributions for activity time. AN AGGREGATION EXPERIMENT Unfortunately, 
guidance for effective high-level process and simplified business rule modeling is not as straightforward 
as activity time approximation. To illustrate the difficulty, we hypothesized three descriptions of the 
same process: Case 1: A worker serves a customer who arrives on average every 3.6 minutes. The worker 
requires an average of 10 minutes to complete his activity. Five identical workers perform this activity. 
Expo(3.6) Worker (5) Expo(10) Figure 2: Case 1 High-Level Process Model Case 2: In fact, the customers 
of Case 1 are of two varieties. On average, customers still arrive every 3.6 minutes. An arriving customer 
has a probability of 1/3 of being Type 1, which one of three identical Worker 1 s will serve for the 
same average of 10 minutes. Either of two identical Worker 2 s will serve the other arriving Type 2 customers 
(probability of 2/3) for an average of 10 minutes. Worker 1 (3) 1/3 Expo(10) Expo(3.6)  Worker 2 (2) 
2/3 Expo(10) Figure 3: Case 2 Intermediate-Level Process Model Case 3: Instead of the simplification 
of Case 2, the two customer types actually undergo some series and parallel activities. Again, on average 
customers arrive every 3.6 minutes. Three Worker 1 s serve Type 1 customers (probability 1/3 in the arrivals) 
in two series activities, one for an average of seven minutes followed by another for an average of three 
minutes. One of the two Worker 2 s who process the Type 2 customers (probability 2/3 in the arriving 
customers) first perform an activity with the customer for an average of seven minutes. However, the 
Type 2 customers are, in turn, of two types, 2A (probability 0.75) and 2B (probability 0.25). Either 
Worker 2 is able to process the Type 2A customers in their final activity, which lasts an average of 
three minutes. However, only one of the Worker 2 s is certified to process the Type 2B customers, though 
that activity also requires an average of three minutes. No special queuing logic pertains to customers 
competing for the different Worker 2 s (i.e., availability is first-come, first-served). Worker 1 (3) 
Worker 1 (3) 1/3 Expo(7) Expo(3) Expo(3.6) Worker 2 (2)  2/3 3/4 Expo(3) Worker 2 (2) Expo(7) Worker 
2B (2) 1/4 Expo(3) Figure 4: Case 3 Detailed Process Model From the point-of-view of most process experts 
typically encountered by the authors, Cases 1, 2, and 3 represent progressively more detail describing 
the same process. In each case, the total customer process time averages 10 minutes. Furthermore, the 
parallel activities in Cases 2 and 3 have the same average activity times. The level of detail in Case 
3 exposes the business rules of specialized customers, queues, and resources. Simulations of the three 
cases provided the following steady-state averages: Table 1: Simulation Analyses of the Three Process 
Modeling Levels Case 1 Case 2 Case 3 Time in Queue at Entry 0.70 43 88 Cycle Time 12.65 40.9 71.8 Throughput 
1381 1387 1407 Queue Length at Entry 0.2 8.1 16.6 Expected to the simulation modeler, the differences 
among these models of the same processes first shock most process experts. In organizations of Level 
1 or 2 process maturity, business process experts often do not think about queues and their impacts on 
cycle time bottlenecks are an annoying aspect of daily work, not part of the process. Hence, the process 
experts find the striking differences in cycle time extremely jarring, and until the project team educates 
them, the experts tend initially to declare the detailed models invalid. The throughput numbers are 
certainly equivalent, so that Case 1 would be a suitable simulation model if process capability is defined 
only on throughput, and none of the other measures matters for decision-making about process design. 
Thus, if providing space for the queue is a key issue in the facility design, Cases 1 and 2 would severely 
underestimate the space required, and only the detailed Case 3 would be acceptable. To date, we have 
found no satisfactory generalized rules for accepting the high-level process model with casual or no 
representation of business rules. To be sure, discovering and modeling the detailed process structure, 
resources, and business rules hedges all bets. Few real­world business process simulations would be as 
simple as this example. Suppose this example were part of a larger end-to-end business process, and that 
a longer process activity chain dominated the cycle time of the sub-process in this example. Then, other 
than possible facility issues of queue length, modeling this example with the detail of Case 3 would 
add little value to the business process analysis. Yet again, merely shifting the customer arrival pattern 
and mix (the branching probabilities) could place the sub-process squarely on the critical path. Regrettably, 
we have consistently observed or contrived plausible counterarguments to every candidate guideline that 
could justify a high-level business process model such as Cases 1 or 2. CONCLUSIONS Beginning the thought 
experiments for this paper, we hoped to discover robust, useful guidelines for process simulation that 
would assure valuable results while minimizing the cost and time of building and using the model. We 
believe the goal is worthy, and we continue the quest. For the present, we conclude the following: Growing 
client appreciation of the value of business process simulation only increases schedule pressures on 
the business process simulation project team to yield analysis results.  Because of inconsistencies 
in the relationships among schedule, cost, and scope ( project balance ) in business process simulation 
projects, de-scoping remains the preferred project management response to client urgency.  The typical 
knowledge, ability and psychology of process experts in organizations with low process maturity confounds 
information gathering about the process. Understanding process experts in a process  maturity context 
helps anticipate information collection problems that could lead to free-fall de­scoping, and to design 
risk-mitigating tactics. From our experience, we continue to place cautious confidence in approximating 
activity time as an effective de-scoping method.  Despite de-scoping motivations, the best approach 
for assuring valuable business process simulation results is to model thoroughly all process structure, 
activities, resources, queues, and business rules.  To date, we have found no other viable guidelines 
regarding when and how to model business processes at a high level while still confidently assuring valuable 
analytical results.  REFERENCES Hansen, Gregory A., 1997, Automating Business Process Reengineering: 
Using the Power of Visual Simulation Strategies to Improve Performance and Profit, Prentice-Hall, Upper 
Saddle River, NJ, USA. Paulk, Mark C., Charles V. Weber, Bill Curtis, and Mary Beth Chrissis, 1994, The 
Capability Maturity Model: Guidelines for Improving the Software Process, Addison-Wesley, Reading, MA, 
USA, 441 p. Profozich, David, 1998, Managing Change with Business Process Simulation, Prentice-Hall, 
Upper Saddle River, NJ, USA. AUTHOR BIOGRAPHIES MARK GRABAU is a Consultant in Andersen Consulting s 
Centre for Process Simulation and Modelling. He has over 6 years of experience in simulation modelling. 
He earned a B. S. in Operations Research at the United States Air Force Academy in 1992, and an M. S. 
in Operations Research and M. S. in Statistics at the Colorado School of Mines in 1997. FRANK GRANGE 
is a Manager in Andersen Consulting s Centre for Process Simulation and Modelling. He has more than 20 
years of experience applying simulation and optimization modeling to applications in business engineering, 
information technology, telecommunications, manufacturing, defense/aerospace, petroleum, and mining. 
He is currently an Adjunct Professor at the Colorado School of Mines, where he received a B. S. degree 
in Chemical and Petroleum Refining Engineering in 1972, and an M. S. and Ph. D. in Mineral Economics 
in 1974 and 1977, respectively. 
			