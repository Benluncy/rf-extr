
 Scaling and Related Techniques for C, eometry Problems Harold N. Gabow l Department of Computer Science 
University of Colorado at Boulder Boulder, CO 80309 Abstract. Three techniques in computational geometry 
are explored: Scaling solves a problem by viewing it at increasing levels of numerical precision; acHvat/ovt 
is a restricted type of update operation, useful in sweep algo- rithms; the CaTtex'hzR tree is a data 
structure for prob- lems involving maximums and minimums. These tech-niques solve the minimum spanning 
tree problem in R~ and ]~ in O(zz(lg n)rlg lg n) time and O(n) space, where for R~ and k ~3, r =k-2; 
for R~, r = 1,2,4 for k =3,4,5 and r =k for k >5. Other problems solved include R[ and l~ all nearest 
neighbors, post office and maximum spanning tree; Re maxima, R t rectangle searching problems, and Z~ 
all nearest neighbors (tip ~ ~). 1. In~oduction. This paper explores three techniques in computa-tional 
geometry: scaling, a general method for algo- rithm design; activation, a restricted dynamic data structure 
for sweep algorithms; and the Cartesian tree, a data structure for maximums and minimums. Seal/rig has 
been applied to network problems (Edmonds and Karp, 1972; Gabow, 1983). Consider a problem defined on 
nonnegative integers; in geometry these are the point coordinates. Let N be the largest integer and I 
= Jig N]+I. View each number as its l bit ~:binary expansion b t : '"b~. The method works by consid- 
 =:ering l+l scales s. s = 0,I ..... l, where scale s takes the numbers to be b I .. b.. This paper 
applies scaling in two ways. The first is to view the solution to the scale s-1 problem as an approximation 
to the scale s prob- lem. The solution is refined over l + 1 scales until it is correct. This is similar 
to network scaling and gives an efficient all nearest neighbors algorithm. The second x This research 
was supvorted in paa:t by the National Science Foundation under Grant IdeS- 8302648 and in part by Bell 
Laboretccies. Permission to copy without fee all or part of this material is granted provided that the 
copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the 
title of the publication and its date appear, and notice is given that copying is by permission of the 
Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific 
permission. &#38;#169; 1984 ACM 0-89791-133-4/84/004/0135 $00.75 by Jon Louis Bentley Robert E. Tarjan 
AT&#38;T Bell Laboratories AT~kT Bell Laboratories Murray Hill, NJ 07974 Murray Hill, NJ 07974 application 
uses scaling to isolate events: If two numbers are equal in scale s-i but their be-bits disagree, the 
number with 58 = 1 is larger. This princi- ple is useful in rectangle searching problems Scaling assumes 
that the numbers are integers and N is not too large (e.g , N:0(n r) for some T, so l 9 N = 0(Ig n).) 
This can be reasonable in geometry, e.g., integrality holds for problems on a grid; VLSI appli- cations 
deal with small sizes and hence small N. We make these assumptions in the all nearest neighbors algorithm. 
However in the other applications sorting is used to map the problem from R~ to a "rank space" [1..n] 
~ in which the scaling assumptions hold. In this paper bounds given for R e need no assumptions. Bounds 
given for Z~ (and the parameter N) rely on the above scaling assumptions. Ac~vo.tio~t is an operation 
for database search prob- lems that is a restricted type of insert. The points V of a database are known 
at the outset, but are initially ~m.ac- Hve. The operation activRte(z), for z E V, makes z active. The 
answer to a query is given with respect to the currently active points. Activation is the operation needed 
in geometric sweep algorithms, and can often be done more efficiently than insert. One data structure 
for activation is based on the queues of van Erode Boas (1977a, 1977b). The Ozrtesi~ tree was introduced 
by Vuillemin (1980) in the context of average time analysis of search- ing and linear list algorithms. 
Searching for maximums can be done by nearest common ancestor calculations on the Cartesian tree. This 
is accomplished using the algorithm of Harel and Tarjan (1982). It gives efficient algorithms for rectangle 
intersection, rectangle contain- ment and others. The Cartesian tree generalizes the idea of the right-to-left 
mmdma of a sequence. Data structures for the latter are useful for spanning trees and other problems. 
 The contents of this paper are as follows. Section 2 discusses sealing. The_all nearest neighbors problem 
in Z~, i~ p ~ .= is solved in 0(~t Ig N) time and 0(rt) space. This generalizes Clarkson's bound (1983) 
for p = 2. The I~ rectangle searching problem, a generalization of range searching, is solved in the 
same resource bounds as those known for range searching. This improves a result of Edelsbrunner and Maurer 
(1981) by a Ig rt fac- tor. 135 Section 3 applies the Cartesian tree and activation to searching problems 
involving orthants or searching for maximum values. R* range searching for maximum is solved in O((lg 
n) ~-~) query time, a lg n improvement of previous results. R~ orthant searching with reporting is solved 
in O((lg n)t-~+m) query time (m is the number of points reported). This applies to the problem of rec- 
tangle containment searching, giving a Ig n improve-ment of (Lee and Wong, 1981). The maxima of a set 
of points in l~ are found in O(n(lg n)~-~lg lg n) time for k ~ 4. This improves (Kung et al., 1975) by 
~ for lg ~n k ~ 4, i.e., when their algorithm is not optimal. Section 4 discusses the region approach 
to geometric minimum spanning trees. It begins with an improved version of Yao's region theorem for closeness 
problems (1982). An algorithm for R~ and R~ minimum spanning trees is given. The time is O(n(lg n)rlg 
lg n) and the space is 0(n), for k >- 3. For 1~ the exponent r is/c -2. For R~, r = 1,2,4 when k = 3,4,5 
and r increases rapidly for higher dimensions; r = k is achieved in Sec- tion 5. These bounds improve 
Yao's bound (1982) of O(nZ-aC~)(Ign) '-ac~)) time, where a(/c):2 -(*+'), and Unanalyzed but superlinear 
space. Hwang (1979) and Lee and Wong (1980) give asymptotically optimum (O(n lg n)) algorithms for k 
= 2, which our algorithm matches. Guibas and Stolfi (1983) implement Yao's region approach in time 0(n 
lg n) for k = 2, using a different method than the one of this paper. Section 5 discusses an orthant-oriented 
approach. The R~'and R~ post office and all nearest neighbors prob- lems are solved, the latter improving 
Bentley's bound (1980) by a factor Iglglgnn" (This contrasts with the Sec- lion 2 bound for Z~.) The 
R~ minimum spanning tree algorithm with r = k is given. Section 6 discusses furthest point problems. 
The R~ and R,~ furthest post office problems ar9 solved in 0(1) query time, and R~ and.R~ all. furthest 
neighbors and maximum spanning tree are solved in 0(n) time. The all furthest neighbors bound improves 
Yao's bound (1982), which is the same as that cited above. The notation I~(77~) denotes the space of/c-tuples 
of real numbers (integers) under the ~ metric, I ~ p ~ ~. For two vectors c~, z ]~, az denotes their 
scalar pro- duct. l 9 n denotes logarithm base two. [a..b ], where a,b ~ denotes the set of integers 
a, a+l ..... b. The ira level of a tree consists of all nodes at distance i from . the root; the root 
is at level 0. In the resource bounds, n denotes the number of input objects, i.e., the number of points 
or rectangles on which the problem is defined. When the output is a list of objects, m is the length 
of the list. Resource bounds given in this paper explicitly show the dependence on all relevant quantities, 
e.g., R~ minimum spanning tree uses O(n(2lg n)~lg lg n) time. This is important since the algorithms 
are usually exponential in the dimension k. 2. ,~..aling geometry problems. This section gives basic 
applications of scaling, including the Z~ all nearest neighbors problem, 1~* rec- tangle searching and 
other rectangle problems. In the all nearest neighbors problem, a set of points V C 1~ is given. For 
every v e V we seek all points v' Y-v at minimum distance d(v', v). Such a point v' is a nearest neighbor 
(nn) of v and d(v', v) is the nearest neighbor distance, denoted ~ (with v under-stood). We solve the 
problem for integral coordinates, V~Z~, i ~ p ~ -. (If V ~ P~ the algorithm can be transformed to an 
approximation method). The algorithm is similar to Clarkson's adaptive cell technique for the L z metric 
(1983). The algorithm works by successively examining scales s =0, 1 ..... l, where I = Jig N]+I. For 
a fixed scale s, if z e V let denote the point that z scales to (i.e.,~ = [~--~'s );,.et~denoteE'snndistanceintheset 
PP. The algorithm maintains a set B(V) for each ~ VP, !_ which contains points of P that are tess than 
~ + 2/cP units from ~. When passing from scale s to scale s+l the lists are pruned: For each ~7 P the 
new value ~ is computed as the distance in Scale s+l from ~ to a point ~l,u~rexxtly in B('D). Then the 
points of B(~) that are too far from ~ in scale s+1 are removed. The correctness of this algorithm follows 
from the fact that at every scale, every nn of v (in the original space) is in the list B(~). This is 
demonstrated as fol- lows: For a given scale s, let z' be the point that z scales to under exact scaling, 
i.e., z i' -21_ , zt . The trian- gle inequality implies1_ 1-d(~,,q)-kP < d(z'.y') < d(~,ff)+kP, for 
z, y ]~. This in turn implies the result. The efficiency is based on the fact that the sets B07) have 
total size 0(n). This is demonstrated as fol-lows: Theorem 4.2 below shows that space at v can be partitioned 
into regions R such that two points I- x, y R--v satisfy d(z, y) ~ max(d(x,v), d(y,v ))-21cP. This implies 
that @ is in at most one set B(~) for a point R (If .~,~ are in the same region R of scaled space, d(~,~)md(g,~), 
and "~ is e's nn distance, then !_ l_ d(~;D) _> d(E,.q)+2kP ~ ~+2kP, i.e., F ~ B(.~).) Thus ff the algorithm 
is implemented so that two points v,~u e V scaling to the same point F = ~ share one copy of the set 
B(F) = g(~), the sets have total size O(NSn). Here N# is the number of regions, and depends on k andp 
but is independent of n. The total time is O(N# n lg N), since each scale is O(N~n) and there are/+1 
scales. Theorem 2.1. The all nearest neighbors problem for a set of points in Z~ (l~p~-) can be solved 
in 0(Nk~ lg N) time and 0(N~) space. Here N is the larg- est coordinate of a point and Np ~ is the number 
of regions given by Theorem 4.2 (e.g., N~ = 5~). Next we consider range searching (also called orthogonal 
range query). Although this problem is well- solved it is a convenient starting point to introduce ideas 
that are useful in other rectangle problems. Given is a database V~R ~ subject to queries that are orthngonal 
k rectangles, R = 1[I [/-t, ~]- The answer to query R is a list of the points V('~R, or alternatively 
a commutative group function ~B of point "values", ~B val(x) (for z e V{'~R The algorithm transforms 
V to a set V' in an integral "rank space" [1..vt] ~. A point v E V corresponds to a point of V' whose 
ith coordinate is the rank of vi in ~ I z E Vl; equal coordinate values v t have equal rank. A query 
is translated into rank space by doing /c binary searches. It is convenient to assume that queries are 
open rectangles ¢~.(l~,=t= h~); this entails no loss of general- ity or efficiency. The algorithm processes 
dimensions sequentially. A point z with zi c (/~, h~) "scales" from the boundary into the interval. More 
precisely there is a scale s where one of two alternatives holds: in scale s-l, ~ = l~ <h i l and ins, 
~.+1 =zt <~' (i.e., the s th bit is 0 for /.~ and 1 for z~); or in scale s-1, l~ <h~ =~i and in s, ~- 
< ~ = ~.-1 (the s ~ bit is 0 for x~ and 1 for h~). The containment ~ ~ (/~, h~) is detected at scale 
s. This approach is implemented with a seal/rig tree, which can be viewed as a variant of Bentley's range 
tree (1980) with Willard's refinement (1982). A scaling tree for dimension i is a binary tree of height 
at most [lg n]+l. It represents the grid hyperplanes z~ = b in the various scales. The root represents 
the hyperplane zt = 0 in scale 0, i.e., the entire database. A node on level s that represents z~ = b 
(in scale s) has two chil- dren, representing zi = 2b and x~ = 2b + 1 in scale s + 1, respectively. (A 
child is null if no database points scale to its hyperplane.) Each node has an associated data structure 
(on the points it represents). For range searching a node in the dimension i scaling tree has a data 
structure that is a scaling tree for dimension i-l, for i ~ 3. The data structure for two-dimensional 
range searching consists of scaling trees for dimensions 1 and ~. A node l in the dimension i tree represents 
a line = b in scale s. Its data structure is a list of all (scaled) points ~(z ~ V) with ~i = b in scale 
s, sorted on the opposite coordinate ~; has a list of all points of V that scale to it. Further, has 
two pointers: if node l has children lj, j = 0, 1 then linepointe.rj for points to the last node on 
l~'s list that scales from a node preced- ing (on l's list). A two-dimensional query is processed by 
scaling the rectangle (li, hi)x(l~,h~), i.e., using line pointers to track /~ and ht in the dimension 
i tree, i = 1,2. When-ever a line x~ = b "scales into" the rectangl e its points are reported. A k-dimensional 
query is processed by generating a (k-1)-dimensional query whenever a hyperplane ~ = b scales into the 
rectangle. (2 lg n) ~-~ two-dimensional queries are generated. The algorithm matches the best- known 
bounds for range searching: query time 0((2 lg n)~-l), preprocessing time and space  o(n(~g ~)~-i) (Willard, 
19S2).  This algorithm generalizes to rectangle se~zrching: Here the database consists of grid rectangles 
in R e. A query is a rectangle R; its answer is a list of all database rectangles intersecting R (or 
some function of them). The algorithm is based on the fact that two rectangles k R~ =l~ [/.~, h~, i = 
0,1, intersect if and only if in every dimension i, /z~ ~ [/~-~, h~ l-~] for j = 0 or 1. This condi- 
tion is verified by scaling each dimension i > 1. If the 1 Recall xi scales to ~, etc. goal is reporting 
the first dimension is done with the data structure of Theorem 3.2; for commutative group functions, 
a list data structure similar to range search- ing is used. Theorem 2.2. In 1E e. k ~ 1, rectangle searching 
(with reporting or commutative group functions) can be done in O((2lgn)m~(~-I'l)+m) query time (no zrt 
for functions), O(n (2lgn) m~C~-LO ) preprocessing time and O(n(2/gn)/~-1) space (ke., the same as range 
searching). The bounds increase by lg ~ for commutative semigroup functions. The reporting result improves 
(Edelsbrunner and Ma~r, 198I), by Ig n in preprocessing time and space (or query time and preprocessing 
time.) Similar group bounds follow from (Edelsbrunner and 0vermars, 1982, and Willard, 1982). These ideas 
also solve the rectangle intersection problem: Given n grid rectangles in ~, report (or count) all intersecting 
pairs. The time is O(n(lg n)~-l+m) and the space is 0(n) (no m for count- ing). The depth-first search 
method of Section 4 ensures linear space. Edelsbrunner and 0vermars (1983) have achieved the same bound 
for reporting, using "memory streaming" to reduce space. The data structure for range searching with 
report- Lug can be extended to Support activation. This uses the linear time split-find data structure 
of (Gabow and Tar- jan, 1983). Adatabase with d.euzt./.~at/or~ can also be sup- ported, using (linear) 
Un/trn-find. The bounds given above for range searching hold with activation or alternatively, with deactivation. 
Another extension is 45 ° ~ght-triangle searchi~tg. The database consists of points in Z ~. A query is 
a 45 ° right triangle, i.e., hyperplanes xt = bt, i = 1 .... , k and ax = b, for some a ~ ~±1~ ~. Activation 
works. This gives results slightly better than Section 5, if the space is Z~ and not R[. The data structure 
is a sealing dog, similar to a scaling tree but a node has more than one parent. 3. Searchin~ problem: 
orthants and run--mum values. This section applies the Cartesian tree to several searching problems. 
Among these is orthant searching for maximum with activation. This gives an effmient algorithm for finding 
the maxima of a set in K e . It is also used in Sections 4-5 to solve the minimum spanning tree and other 
problems. The Caz'tesian tree for a sequence of real numbers x4, i = 1 .... , n, is a binary tree with 
nodes labelled by the numbers. The root has label zm where z m =max~ li =1 ..... 7tl. Its left subtree 
is a Carte-sian tree for ~zi I i = 1 ..... n,t-ll and its right subtree is a Cartesian tree for ~:r.i 
I i = zn.+l ..... n~. (The tree for an empty sequence is null.) This definition is due to Vuillemin (1980). 
In what follows it is convenient to iden- tify nodes with their labels. We apply the Cartesian tree to 
the problem of ra~tge sea.rch~.ng for zrtazon.~m. This is range searching where the goal is to find the 
maximum value point in a query rectangle. If z and y are two nodes of a tree, nca(x,y) denotes their 
nearest common ancestor. In a Cartesian tree for any indices i, j, 1 ~- / ~ ~ ~ n, 137 max}xt I i ~/c 
~j] = nca(zt, zi). (This follows from the inductive definition of the Cartesian tree.) Hence one-dimensional 
searching for maximum can be done by answering nca computations on the Cartesian tree. (Actually discrete 
one-dimensional searching for max-imum is equivalent to rtea queries on a tree: If the tree is binary 
the sequence for searching for maximum is derived bY listhug the heights of the nodes in inorder; general 
trees use an analogous order.) The Cartesian tree is found by constructing sequence Ti, i = 1 ..... n 
where Ti is the Cartesian tree for ~zi I J = 1 ..... i I. Ti is derived from Ti-~ by ascend- ing along 
the rightmost path of Ti-~ until x#, the first node larger than z i, is found; the right subtree of xe 
is made into the left subtree of :r. i, and xi is made the right child of zt. The total time to construct 
Tn = T is 0(n), since a node leaves the rightmost path immediately after it is traversed. The Re range 
searching for maximum algorithm works as follows. Dimensions /c,/c-I ..... 2 are scaled. The data structure 
for dimension k (k ~ 2) is a scaling tree, where each node has a data structure for Re-~ range searching 
for maximum. Additionally in dimension 2, each node has a list of its points sorted on z~ value, with 
line pointers to the lists of its children. The data structure for dimension 1 is a Cartesian tree prepro-cessed 
for w.va queries. Harel and Tarjan (19B2) show that a fixed tree can be preprocessed (in linear time 
and space) so that ztccz queries can be answered in 0(i) time. Theorem 8.1. In Re, /c ~ i, range searching 
for max-Lmum can be done in 0((2 Ig n) maz(~-m)) query time, O(n(tg vt) raax(~-Ll)) preprocessing time 
and O(zt(lg r~) ~-1) space. Section 5 uses the special case of orthant searching for minimum. In this 
case vtca searching can be replaced by simple linear list manipulations, as in the range searching algorithm 
Section 2. The rest of this section is concerned with orthant searching. (Special eases of this are 
called dominance searching and ECDF searching.) Consider orthant search- ¢s'tgj zv/2h report/~.g. This 
is the special case of range searching where query regions are orthants of R e (i.e., ~ t vt, i = 1 ..... 
~:, where v E Re and ~ E }~_ , _< I, i = 1 ..... /c) and a query answer is a list of all database points 
in the given orthant. For/c = 2 the data structure is a Cartesian tree for a set of points in R e, defined 
as in Vufllemin (19B0): Let ~:i,i = 1 ..... n, be a list of the database points in increasing order of 
first coordinate x~. The Cartesian tree is for the sequence z~, i = 1 ..... n, i.e., a point's ~alue 
is its_ second coordinate. To report all points z i with z~ ~ ~, I = 1, 3, where v l = z~, compute zr 
= ~,ca(zl, z"). If z~ < ~e the orthant is empty. Oth-erwise z" is in the orthant. The remaining points 
are found recursively, beginning by probing vmu(zl, z ,-~) and vtea(z ~÷I. z"). The orthant searching 
algorithm for Re works by scaling dimensions/c ..... 3, and processing dimensions 2 and i with the Cartesian 
tree. The data structures arc similar to Theorem 3.1. Theorem 3.2. In Re, ]c ~ 1, orthant searching 
with reporting can be done in O((lg n)~{~-2,0+m) query time, 0(n (Ig vt) maz(~-z.l)) preprocessing time 
and O(n(Ig n) ma=(~-2.°)) space. The result is more general: it applies to any range search (for reporting) 
where the query region is infinite in some dimension. It also applies to rectangle prob- lems: For rectangle 
searching with reporting it provides the base case (k = i) for the algorithm of Theorem 2.2. (The tree 
stores intervals. It 'appears that the data structure of (Chazeile, 1983) for interval overlap could 
also be used here.) Another problem is rectangle covt-taiztrnertt sewrch ~nith reporting (a rectangle 
is reported if it iS contained in the query rectangle). This problem (and any "rectangle relationship" 
problem) amounts to orthant searching in I~ e. The bound (i.e., Theorem 3.2 for 2k) improves (Lee and 
Wong, 1981) by a Ig rt factor in query time, preprocessing time and space. Now we consider algorithms 
based on the r/ght-to- left maxima of a sequence S. This is the subsequence of elements that are larger 
than any of their successors in S (Knuth, 1973); equivalently it is the rightmost branch of the Cartesian 
tree. This sequence will be represented using the queues of van Emde Boas (1977a, 1977b). First consider 
orthant searching for maximum with activation. In this problem the database is subject to operations 
act/vote(z), where x is a point of the data- base, and query(v), where v ElY. The answer to the query 
is the database point x with maximum value in the orthantz i (~ (zs(.i= 1 .... ,k. For/¢ = 1 assume the 
points x i are sorted in order of increasing coordinate. This gives the sequence of point values va./(z~), 
i = 1 ..... n, of which the active ele- ments form a subsequenee. The algorithm maintains the list M 
of right-to-left maxima of the subsequence of , active elements; To query .~, return the first element 
'in M with i ~3.=To activate zj, find the above element z i, If ~a/(zj) > va/(~) then insert zj immediately 
before r.~ and delete all elements before zj that are not larger than zj. The algorithm for IRe orthant 
searching for max- irnum with activation works by scaling dimensions ~: ..... 2. The data structure 
for these dimensions is a scaling tree, as in Theorem 3.1. In dimension 1 the data structure is the list 
M, implemented as a priority queue over [1..n]: Van Emde Boas (1977a; 1977b) shows how to support the 
operations needed for M (predecessor, suc- cessor, insert, delete) in O(lg lg n) time per operation. 
The integer i is in the queue if zi is in M. (The subscript refers to the index of the element in the 
one- dimensional universe at hand; vt is the number of ele- ments in this universe. If /c > I this universe 
can be smaller than the original. Storage efficiency depends on making this distinction.) Theorem 3.3. 
In Re, k ~ 1, orthant searching for max-imum with activation can be done in O((tg vz)~-llgj lg vt + tg 
n) query time, a total of O(n(lg n)~-ltg lg n) activation time, and O(rt(lg n) ~-i) preprocessing time 
and space. TM Next consider the problem of finding the maxima of aset of points V£Re. Apointv E Vis amaxima 
if no other point dominates it, i.e., ~x ] = E v,~ ~v(.i = 1..... kl = {v]. Suppose/c = 3. The algorithm 
does one-dimensional orthant searching for maximum with activation, where x~ gives the value of z and 
x, gives the position of x in the sequence. The points are processed in order of decreasing xa. For each 
value of xs, the corresponding points x are activated; then a query is done for each of these elements 
x. x is a maxima if the returned point is not in its orthant. (Tins are handled carefully so that a point 
does not interfere with itself.) The Re maxima algorithm works by scaling dimen- sions k ..... 4. To 
achieve linear space the scaling trees are generated and explored depth-first, with only the relevant 
portion of a scaling tree explicitly stored. Details of this technique are illustrated by the spanning 
tree algorithm of Section 4. Theorem 3.4. In Re, k ~ 4, the maxima of a point set can be'found iv/~(~{/g 
vZ)~-:slg"Ign)time and O(n) space. " This improves the bound of (Kung, et al., 1975), O(~z(Ig nit-z). 
Their optimal bound of 0(rt lg n) for k ~ 3 is also achieved by this approach. 4. The region approach 
and $~.~nnin~trees. This section explores the region approach. It proves a new version of Yao's region 
theorem (1982) and applies it in an algorithm for R~ and 1~ minimum spanning trees. Several algorithms 
for nearesL neighbors (Bentley, 1980; Papadimitriou and Bentley, 1981; Clarkson, 19B3) and minimum spanning 
trees (Yao, 1982) are based on a partition of space into "narrow" regions: For a given metric d and a 
point v, region R is narro~u if for any two points x, y E R--v, d(z,y) < max(d(x,v), d(~/Ju)). Yao 
(1982) shows that for any point v E P~, space can be covered by a finite set of narrow, convex polyhedral 
cones centered at v. He gives an effective construction, based on baryecntric subdivision. We present 
several partitions that can be explicitly stated and appear to give better algorithmic bounds than Yao's. 
The p,. reg/ons for a point v e R e are formed by the coordinate hyperplanes x i = vi: a ~ region has 
the form ~x J x~v i, l~i~k], where each ~ E ~>,<, = I. The lz~regions fore are formed by the "octahedrai" 
hyperplanes ax = ¢zv, a E !:t=llU: a /zi region has the form ~x Iax ~ ~av, a E I:t=l]~l where each (~ 
~ E ~ > , <, = ]. The p~ reF~o'ns for v are formed by the hyperplanes ax = Re, a E [0, :t=1]~: a p~, 
region has the form Ix I a.z~ haY, a E 10, :i:l]~], where each (~. C~>. <.=i. Theorem 4.1. The ~.~ regions 
are narrow for any //~ metric, 1 ~p ~ ~. The pq regions are narrow for L~ and the/&#38;, regions are 
narrow for L.. Proof. We give the argument for the pp regions and for 1 < p < m. The /~l and ~ regions 
are handled similarly. For x E R e use the notation z,.a = ~ xi. Consider two points x, y in a p~ region 
R. Take coordinates with v as the origin and R having constraints zi > 0 (or z~ = 0), for i = 1 ..... 
k. By renaming coordinates, write d(=. vF = (~,-wF+...+(~ ~,) ~ +(1/r+1--ZT+i)e+  +(!&#38;--x~) p, 
where 0~" ~/~. Region R does not have the constraint xl, r = Xr+l. ~. (If it did then d(x.y F ~ (xt.r-Xr+t,-(V,.,-njj,÷l.~) 
F = 0.) Assume R has the constraint xl. r >xr+L~, else rename points and coordinates. Choose the largest 
index i ~ r so that R has the constraints, ~.r > Zr÷l~ and either zi÷Lr < xr+L~ or z~l,r =xr+~. (i =r 
is a degenerate case). Then (~-y~F+"  +(x~--yrF+(~,÷t-~÷~F+ .. +(~_~#)e (~,-~+~-(zA,-~+~)) ~ < (x,.,-z,+~F 
= (~+~+L~-=,+,~)~ ~ ~. So ~(x. u)~ ~ (x~-wF+' + (~-~-~(-~F+~=~+  +~ = a(x. ~)p. The number of regions 
can be reduced by using a perturbation technique to handle the possibility of equal- ity in the region 
constraints. This is done for p.. as fob lows. We need only be concerned with a given finite set of points 
V. Index these points arbitrarily from 1 to n by a function t(z). A /~.reg/o~ for ~ has the form ~Vl~@,v,. 
or x,=v, and t(=)@,t(v). l~i~kl, where each > i e~ >, < I- The lexleo-Fraphtc L, distance is a vector 
defined as ~(V,~) = (d.(v,~V), s't(v), -S't(~)), where names v and ~ are chosen so that t(v)>t(~u) and 
s is 1 if d.(v,~) can be written as vi--zvi for some ~., and -1 if d.(v,~) can only be written zui--v 
i. a. values are com- pared lexicographicaily. The ~. regions are narrow for ~... This construction 
reduces the number of regions from 3 ~ to ~. The Theorem can be refined to justify the scaling algorithm 
of Section 2 for all nearest neighbors (Theorem 2.1): Consider a point v E Z~. Partition Z~ nsin~r the1.L]linear 
forms az, ~ E ~0,±iI ~, as follows. Let a =12pIc PI. A region R is defined by choosing one of the following 
constraints for every a E ~0,~llk: ax = ax + 6, e [-a+l..a-l], ~r ~ mJ+a, mr ~ av-a. rneorem 4.2. For 
any two points .L x.zl e #-v. d(z.zl) ~ max(d(=.~), d(~.v))-a~p. Proof. The argument follows Theorem 
4.1. using the identities aP--pbQ p-1 ~ (a-by ~ aP-bcz~-! ~ ~ 1). Yao (1982) originated the region approach 
to the minimum spanning tree problem: Let V be a finite set of points in 1~. If v E V a (nearest) region 
ne'ighbor (rn) of v is a closest point of V-v in some ~ region at v. For every v e V and every/,~ region 
at v, choose an rn. Let E=~lrv' I v' is a chosen rrt of v I. Yao showed that E contains a minimum spanning 
tree on V. (Alternatively this can be seen by applying Jarnik's algorithm (also known as Prim's algorithm: 
see (Graham and Hell)) to V: Whenever the algorithm adds a vertex ~u and an edge ~v to the tree, ~u is 
an rn of w.) This theorem also holds for lexicographic L.: if E is constructed for ~. and ~., it contains 
an L. minimum spanning tree. The spanning tree algorithm of this Section works by fixing a region R 
and, for every point v e V, finding an rn (in the R region at ~). To do this effmiently we use a de'rived 
d/stance funct/on 6(x) such that, for x in o's region, the distance d(x,v) is monotonic in 6(z). This 
 139 allows o's rn to be chosen as a point that minimizes 6. The/~, and/~, regions can be modified so 
~ exists, as fol- lows. For/a,, define a partition of space using the hyper- planes ~ ± z$, i # j. More 
precisely a ~ region at v has the form ~z I for i x ..... /, a permutation of [1..k] and s, ..... s~ 
a collection of signs, sj ~±1], sjzg-sj÷xa~t. , ~ s tv½-sj÷xv½, ~, j = 1 ..... le -1 and s~ ~ s~v~]. 
Setting u~/ = z%--v%, the above inequalities are equivalent to sxu q ~ s~u~ ~ . . ~ s~u~ ~ 0. So the 
~= regions partition space and 6(x) = stzq can be used as the derived distance function. A ~,, region 
is included in a /~, region, except for the boundary. We shall see below this suffices for the algorithm. 
For /z x consider an orthant z~ (~ ivy, £ = 1 ..... k. Let~ = lif (~ ~is ~ and-lif (~is ~. Thenforany 
point z in the orthant, dx(x,v ) = ax-om. Hence 6(z) = az can be used as the derived distance function. 
For any partition of space into regions let N be the number of regions z. Let C be the greatest number 
of nonredundant constraints for a region. (Alternatively C is the greatest number of facets plus the 
number of equality constraints.) For p~., N = ~k!, C = k. For the above /z~ regions the numbers are much 
larger: 2~"~N~ ~, Cm[lk k~ ]. (The uppe rbound on N l[ ~ IJ comes from the fact that every feasible 
linear program has a basic feasible solution. The lower bound on N is a construction viewing the 2 ~ 
oet~hedral hyperplanes as vertices of the Re cube. The lower bound cn C comes from a "central" region.} 
The run time of the spanning tree algorithm is linear in N and exponential in C. For small values of 
k we can add new hyperplanas to the collection to decrease C. For example define the ~t regions by adding 
the same hyperplanes z~ ± z1 as in ~,.. Then C = 4 for I~ and C = 6 for R ~. The proof is a tcdicus case 
analysis. Also C = 3 for I~ (even without the new planes). Now we present the algorithm to find rn's. 
Consider a region defined by C constraints txx ~ txv, with derived distance function ~(z). (Any ~, region 
has this form. A ~x region may have > and = constraints, which are han- dled in a similar fashion). The 
rn problem in R e is transformed to a minimum value problem in Z O as fol- lows. Each point ~u V corresponds 
to a point t~' [I..~%] c. The i xh coordinate of ~u' is the rank of txv in the set ~az ] z ~ V I, where 
a is the vector for the i tn constraint; equal numbers av are assigned the same rank. The value of v' 
is 6(v). Hence an rn cfv is a minimum value point z V' in the orthant of v' defined by~ ~v~',i = 1 ..... 
C. The value problem is solved by sealing dimensions C ..... 3, sweeping on dimension 2 and using orthant 
 arching for minimum with activation (Theorem 3.3) on dimension 1. The details are as follows. Each point 
in V' plays two roles: it defines a region (to calculate its r-n) and it belongs to regions (and can 
be another point's rn). To distinguish these roles form m N~ in Theorem 2.1. two sets: the "region" set 
R = {(?..z x ..... 2zc) I z V' I and the "point" set P = {(Zzt+l ..... 2zc+l)[ z V' I. The mcdified 
coordinates used for R and P ensure that a point always scales into the interior of a region. The recursive 
algorithm RN below has several argu- ments. R and P are interpreted as above (although they change with 
each call). These sets are stored as linked lists of points sorted in order of decreasing z~. s indi- 
o ates the current scale, i.e., a point v RI, JP is represented by its leading s bits. k indicates the 
dimen- sion of space, i.e., onl F the first k coordinates are relevant, b specifies the hyperplane z~ 
= b that con- rains all points in R kiP. The algorithm maintains a tentative rn for each point v R. 
It is a global variable. On exit it is the desired rn. The initial call is RN(R,P,C,O,O). R and P are 
ini-tialized as described above: 6 is set to the derived dis-tance for the region of interest; tentative 
rn's are set to a dummy point with infinite 6 value. Algarithm RN(R, P, k, b,s) If k = 2 do Step 1 and 
return; if k ~ 3 do Steps 2-4 and return. Step 1. (h = 2) Initialize a data structure for R 1 orthant 
searching for minimum with activation. Use the points of RUP with coordinates given by z I and values 
6(z). Then do Steps 1.1-1.2 for the distinct values of z 2 in decreasing order. Step 1.1. Activate all 
points v E P withy a = z 2. Step 1.,?. Do a query for each point v c R with v~ = z2, using the answer 
to update o's rn. Step 2. (k ~ 3). For ff = 0,1, set Rj(Pj) to the points of R(P) that are in the hyperplane 
z~ = 2b + ] in the next scale s +1. (R i and Pt are lists sorted on zz.) Step 3. If R 0 and Pi are nonempty, 
call RN(Ro, Pl, k-1, 0, 0) (do lower dimensional queries for the points that scale into regions in dimension 
k ). Step 4. Ifs < ~/gn]+2then forj =0,1, ifR$ andPj are nonempty call RN(Rj, Pj, k, 2b +j, s +1) (process 
points and regions that scale into the same hyperplane). For a dimension/c >_ 3, RN does a depth-first 
traver- sal of the scaling tree (Step 4). A node in the scaling tree that represents hyperplane z~ = 
b scales to hyper- planes z~ = 2b and z~ = 2b+l. The region constraint zk ~ v~ is satisfied by points 
z E Pcn the hyperplane z k = Zb+l and ~ 6 R on x~ = 2b. So the recursive call of Step3 correctly updates 
rrCs. Thus RN calculates rn's correcUy. The time for RN is O(Tt(Ig ~)~-~Ig Ig rt). First observe there 
are constants c 1, e 2 so that the time for Step 1 is c ln/g lg ~, and the time for Step 2 is ezn. We 
show by induction on k that the time for RN(R, P, k. O, O) is cxn(lg n)~-~tg tg n+czn~(lg n)J. j=l where 
n = ]RI+IP]: The nodes of one level of the scal- ing tree partition the original points. Hence for a 
given scale s, the recursive calls in Step 3 have R and P sets of total size n. Thus the total time 
for these calls is e ln(lg n )~-31g lg n + cen ~ (lg n )i ; additionally, Step j=l 2 is c2n. The total 
time for lg n scales is k-2 cln(lg n)~-~lg lg n + c2n~ (lg n)i, as desired. 1=1 The space for RN is 
O(kn): At any instant in the depth-first traversal of the scaling tree, a point of the original set RUP 
is represented only once. The overall outline for the spanning tree algorithm is as follows. For every 
region, generate the correspond- ing space V' and use RN to find an v-rt for each point. Then construct 
a graph with edges ~xyj I Y is an rn of x I, and distances using the metric of interest. Find a minimum 
spanning tree on the graph. The resource requirements for the entire spanning tree algorithm are as follows. 
As before, let there be N regions, each with at most C constraints. Suppose that 5(x) can be computed 
in 0(/c) time and C ~ k. Then the time is O(Nn(lgn)C-elglgn) and the space is  0(n(k +N)). For R[ we 
apply this algorithm to the ~l regions. Theorem 4.3. A minimum spanning tree in R~, for k : 2,3,4,5, 
can be found in 0(n (lg n ÷ (lg n)rlg Zg n)) time, for r = 0,1,2,4, respectively, and 0(n) space. For 
k > 5 the approach of Section 5 is more efficient. For L, the above algorithm is modified slightly. 
Use the lexicographic ~, regions, defined by setting t(&#38;) to the rank of x in a stable sort of the 
multiset ~v~ ] v E V]. To calculate /~, rn's, execute the RN algorithm on the regions. Recall that a 
~ region is included in some /~ region, with the possible exception of points on the boundary, i.e., 
z i = re. These points are handled correctly if Step 1 of RN breaks ties in xu according to t(x), i.e., 
Steps 1.1-1.2 are executed in order of decreas- ing t(x). This ensures that all points x in v's region 
have t(x)> t(v), as lexicographic /~, requires. Hence for each 1~ region, rn's are calculated by executing 
RN on the corresponding ~ regions. Theorem 4.4. A minimum spanning tree in R~, k ~ 2, can be found in 
0(~k! n(lg n)~-2lg lg n + n lg n) time and 0(2~n) space. " A different approach to L. minimum spanning 
tree is faster for a small range of k. Consider anrn for v in a region R. The rn distance r/is the smallest 
radius of an L, ball at v that contains a point of R ~ V--~. Further- more W] = ] ~'-"v~t, for some i 
E [1..k] and z (the Tn) E V. Since an L® ball is a cube and R has the form xi (~ i vi, r/ can be determined 
by doing k binary searches, one for each coordinate i. The run time of this method is 0(k n(4/og n) ~) 
and the space is O(n(lg n)~-l). The terms in k alone make this method faster than Theorem 4.4 (but not 
more space efficient) when k = ~(log log n/log log log n). (When lc = Q(log n/log log n) the naive 0(/c 
n e) method is superior.) 5. Post offices and sl~nning trees. This section presents an orthant-based 
approach to RI~ and ItS closeness problems, including the post office problem (sometimes called nearest 
neighbors search-ing), all nearest neighbors, and minimum spanning tree. The latter improves the algorithm 
of Section 4 for R~. k > 5. We begin with R~ post olTme. The algorithm works by solving value searching 
problems in [1..n] ~. Each point v ~ V corresponds to a point v' ~ [1..n] k, where v i' is the rank of 
W in Ix~ I x c VI; equal coordinates v i have the same rank. The algorithm constructs 2 ~ .uniyerses 
for orthant~earching for mi~nimum (Theorem 3.1) on V', one universe for each arthant. The value function 
for the orthant z~ (~ t v~, ~, = 1 ..... k, is the derived distance function 5(z)= mr where a i = 1 if 
(~ i is ~ and-1 if (~ i is ~. A query for v ERe converts v to v'c [1..n] by doing k binary searches. 
Then it does ~ orthant searches for minimum. The desired closest point in V is the orthant minimum point 
with smallest distance to v in Re. Theorem 5.1. In R[, k ~ 1, the post office problem can be solved in 
O((2lg n) maz(~-Ll}) query time, 0(n(E/y n) maz(k-I'l)) preprocessing time and O(n(2lg n) ~-l) space. 
The ~ post office problem can be solved in the same bounds times/c!. A previous bound for the post office 
problem in R[ and ~ is O(lg n) query time and 0(n 2k*1) preprocessing time (Dobkin and Lipton, 1976; 
Yao, 1982). The R1 ~ all nearest neighbors problem is solved in a similar way. For each orthant the points 
of V are represented by their counterparts in the rank space [1..n] ~ with values given by the derived 
distance function ax. The RN algorithm of Section 4 is run on this universe to compute tentative nn's. 
Theorem 5.2. In R~,/c ~ 2, the all nearest neighbors problem can be solved in 0(n((2 lg n)e-Zlg lg n+lg 
n)) time and 0(Ion) space. In R, the time bound increases by a factor k! The spanning tree algorithm 
is based on Boruvka's algorithm (also known as Sollin's algorithm; see (Graham and Hell)) for graphs. 
At a given moment in this algo- rithm the points are partitioned into trees that are all contained in 
some minimum spanning tree. A (nearest) foreign neighbor (fn) of a point v is a closest point in a different 
tree; an fn of a tree is a closest ffn of one of its points. One pass of Boruvka's-algorithm adds an 
fn edge for each tree. The algorithm finds a minimum spanning tree in at most lg n passes. To implement 
this algorithm in R~, let T i, i=l ..... r. be the trees at the start of a pass. It suffices to find 
an .fn for each vertex. As in the post office problem this is done by considering each orthant separately; 
the points of V ~ R e are represented by their counterparts in the rank space [1..n] ~ with values given 
by the derived dis- it, ante: function ax. 'Pne' algorithrrf-for an orthant is organized similar to algorithm 
RN of Section 4: The points are placed into a region set R and a point set P. A tentative .fn is maintained 
for each point v c R. The algorithm scales dimension k ..... 2. The sets R and P are maintained as lists 
sorted by tree Ti. For dimension 1 the algorithm uses one--dimensional orthant searching for rninimum 
with activation (Theorem 3.3), as follows: 'INvo passes are made over the points R~P, one for the tree 
index Ti increasing and the other for index decreas- ing.~ The index-increasing pass finds, for each 
vertex E R(-~Ti, a closest point in a tree of smaller index T$, # < i. To do this, when Ti is processed 
a minimum value query is made. for each v ~ R ~ T~ to compute it closest point. After these queries all 
point of P~T~ are activated. The index-decreasing pass finds a closest point in a larger index tree in 
a similar way (It starts with all points inactive). The two points found for v in the two passes include 
a closest point in the current universe and are used to update v's tentative fn. The time to process 
one orthant is O(n(lgn)~-ilg lg rt) and the space is 0(/on), by an analysis similar to Section 4. Since 
there are 2 ~ orthants and lg n passes, the following result holds. Theorem 5.3. In R~, k ~ 1, a minimum 
spanning tree can be found in 0(n(2lg n)~lg lg rt)time and space 0(/cn). &#38; Furthest point problems. 
This section shows that R~ and P~ furthest point problems, such as furthest neighbors and maximum spanning 
trees, are easier than their closest point coun-terparts. We start with a useful observation for furthest 
point problems in R~ and P~. Let V~ R[ and for each ae~±ll ~ let v a e V be a point that maximizes av 
~. Then for any point ~ ERa, one of the points v a is a furthest point from w in V. For if v E V and 
d(v .~ ) = a(v -'w ), then d(v =, %u) ~ a(v=--%u) ~ a(v-'w) = d(v ,'tu). The same argument shows that 
if V~P~ and for each i e [1..k], vi(v "-i) e Visa point that maximizes v~(-v~), then any E R,~ has some 
v ~ or v -t as a furthest point. The furthest post office problem is defined on a set F ~ P~; given a 
query point ~ E I~, find a furthest point from ~ in V. In R~ this is done by first computing the points 
v a of V. The answer to a query w is a point va furthest from ~. The same method works for R,. The all 
furthest neighbors problem is, given a set V ~ 1~, for each v E F find a furthest point v' e V. In R~ 
and 1~ this is done by making furthest post office queries for eachv e Y. Theorem 8.I. In R[, k ~ 1, 
the furthest post olTme prob- lem can be solved in 0(2 ~) query time, O(~n) prepro-cessing time and O(kn+2 
~) space. In ~ the bounds are 0(/¢) query time, O(k'n) preprocessing time and space. In the all furthest 
neighbors problem can be solved in 0(2Jtn) time and 0(/on) space; in 17~ the bounds are 0(kvt) time and 
space. For the maximum spanning tree problem we refine our initial observation. If V and W are sets in 
R~, define points v a and w a as above. Then there is a pair of points va w--a that are as far apart 
as any point of V from a point of W. For ifv e V,~ e W, andd(v,~)=a(v-~w), then d(v=, ~u "-Q) >- a(v='-~ 
-a) -> a(v--~u) = d(v.~). Similarly in R,, the maximum distance between V and W is achieved by two points 
v i and ~v ~. A maximum spanning tree is found using Boruvka's algorithm. (For maximum spanning trees, 
an fr~ in Boruvka's algorithm has furthest, rather than closest, distance). Suppose the trees at the 
start of a pass are Ti, i = 1 ..... ~-. With each tree T~ the algorithm stores the 2 ~ maximizing points 
avG. An fn for Ti is a point v~, j ~ i, that maximizes a(v~.-v~-a). To find fn's the algorithm considers 
each orthant, i.e., each a E }±11 ~, in turn. If v~ is the point that maximizes mJ~ (over all trees), 
then it is the maximizer (i.e., the above v~) in the a orthant for all trees T~, i ~ s; if v~ is the 
point with second highest av~, it is the maximizer in the a orthant for Ts. In one pass, the algorithm 
processes all orthants this way and finds all fzt's. Then it adds all J'vt edges, merges trees and updates 
the points v~. To estimate the time for this algorithm, recall that in the i :h pass of Boruvka's algorithm 
there are at most n 2~_1 trees. So the total time is at most a constant times The space is 0(2~n). This 
can be reduced to O(/on+2~): The first pass eanbe done in 0(2~) time and O(k~t) space, as in.Theorem 
6.1. After this there are at most 2~ trees, one for each maximizing point va. The reduced space bound 
follows. Clearly further reductions in space can be achieved. The algorithm for 1~ is similar. Theorem 
6.2. In R~, k ~_ 1, a maximum spanning tree can be found in O(2~n) time and 0(/on + 2 ~) space. An maximum 
spanning tree can be found in 0(/cn) time and space.  Acknowledgmen~ The authors thank Professor D.T. 
Lee for his inspir- ing conversations on maximum spanning trees, and Pro- fessors R.H. Byrd and A. Ehrenfeueht 
for the lower bound on N1k ,  Bibliography J.L Bentley, "Multi~mensional divide-and-conquer." CACM 33, 
4, 1980, pp. 214-229. B. Chazelle, "Filtering search: a new approach to query- answering," Pr0c. 34th 
Aztnual Syrnp. on Found. of Camp. Sci., 1983, pp. 122-132.  K.L Clarkson, "Fast algorithms for the all 
nearest neigh- bors problem", Proe. 34th Annual Syrnp. on Foundations of Corap. Sci., 1983, pp. 226-232. 
 D. Dobkin and ~J. Lipton, "Multidimensional search prob.- lJsms,",SIAM J. Comput. 5. 1976, pp. 161-186. 
 I-L Edelsbrunner and H.A. Maurar, "On the intersection of orthogonal objects," In./. Proc..Letters, 
13, 4, 1961, pp. I77-181. H. Edelsbrunner and M.H. 0vermars, "On the equivalence of some rectangle problems,". 
Inf. Proc. Letters 14, 3, 19B2, pp. 124-27. H. Edelsbrunner and M.H. 0vermars, "Botched dynamic solutions 
to decomposable searching problems," Report FllB, Inst. fur Informationsvararbeitung, Tech. Univ. Graz, 
1983. J. Edmonds and R.M. Karp, "Theoretical improvements in algorithmic effmiency for network flow problems," 
J. ACId 19, 2, 1972, pp. 248-264. H.N. Gabow, "Scaling algorithms for network problems," Proc. 34th Annual 
Syrnp. on Found. of Cornp. Sci., 1983. pp. 248-257. H.N. Gabow and P,.E. Tarjan, "A linear-time algorithm 
for a special case of disjoint set union," /:'roe. 15th Aztnuat ACM Syrnp. on Theory of Cornp., 1983, 
pp. 248-251. R.L. Craham and P. Hell, "0n the history of the minimum spanning tree problem," Ann. History 
of CornputiTtg, to appear. LJ. Guibas and J. Stolfi, "On computing all north-east nearest neighbors in 
the Li metric," Inf. Pr0c. Letters 17, 1983, pp. 219-223. D. Harel and R.E. Tarjan, "Fast algorithms 
for finding nearest common ancestors, SIAM J. Comput., to appear. F.IL Hwang, "An O(n log n) algorithm 
for rectilinear minimal spanning trees," J.ACM. 26, 2, 1979, pp. 177-182. D.E Knuth, The Art of Computer 
Progrararnirtg, Vol. 3: Sorting and Searchi:tg, Addison-Wesley, Reading, Mass., 1973. H.T. Kung, F. 
Luccio and F.P. Preparata, "0n finding the maxima of a set of vectors," J. ACM 22, 4, 1975, pp. 469- 
 476. D.T. Lee and C.K. Wong, "Voronoi diagrams in Li(L,) metrics with 2-dimensional storage applications," 
SIAM J. ~rnpu/. 9, Feb. 1960, pp. 200-211.  '.T. Lee c.K. Wo , "nnd g interseeUons of rectan- glesby 
range search," J. of A/gar/thms g, 1961, pp. 337- 347. C.H. Papadimitriou and J.L Bentley, "A worst-case 
analysis of nearest neighbor searching by projection", in Lecture Notes in C0m/mter Science, 85, Springer-Vering, 
Berlin, 1930, pp. 470-482. P. van Emde Boas, Kaas, R., and Zi]lstra, E., "Design and implementation of 
an efficient priority queue," Math. Sjsterns Theory 10, 1977, pp. 99-127. P, van Erode Boas, "Preserving 
order in a forest in less than logarithmic time and linear space," In f. Prsc. Letters 6, 3, 1977, pp. 
60-62. J. VuiUemin, "A unifying look at data structures," C. ACM 33, 4, 1968, pp. 229-239. D.E. Willard, 
"New data structures for orthogonal range queries," SIAM J. Cornput., to appear. A.C. Yes, "On constructing 
minimum spanning trees in k-dimensional spaces and related problems," SIAM J. Corn/mr. ii, 4, 1982, pp. 
721-736. ]43  
			