
 1979 Conference on Simulation, Measurement and Modeling of Computer Systems CRITERIA FOR COMPUTER PERFORMANCE 
ANALYSIS  Jozo J. Dujomovi6 Department of Computer and Information Science Department of Electrical 
Engineering University of Florida Gainesville, Florida 32611 Abstract. Computer evaluation, comparison, 
and selection is essentially a decision process. The decision making is based on a number of worth indi- 
cators, including various computer performance in- dicators. The performance indicators are obtained 
through the computer performance measurement proce- dure. Consequently, this procedure should he com- 
pletely conditioned by the decision process. This paper investigates various aspects of computer performance 
measurement and evaluation procedure within the context of computer evalua- tion, comparison and selection 
process based on the Logic Scoring of Preference method. The set of elementary criteria for performance 
evaluation is proposed and the corresponding set of perfor- mance indicators is defined. The necessary 
perfor- mance measurements are based on the standardized set of synthetic benchmark programs and include 
three separate measurements: monoprogramming per- formance measurement, multiprogramming performance 
measurement, and multiprogramming efficiency mea- surement. Using the proposed elementary criteria, the 
measured performance indicators can be trans- formed into elementary preferences and aggregated with 
other non-performance elementary preferences obtained through the evaluation process. The ap- plicability 
of presented elementary criteria is illustrated by numerical examples. I. INTRODUCTION The selection 
of anew computer, tuning of an existing computer, and design of a new computer are three basic reasons 
for the computer perfor- mance measurement and evaluation [4, 7]. Perfor- mance measurements are organized 
by running the so-called drive workload [7], which can be real- ized either as a natural workload (benchmark 
pro- grams selected from the set of existing user's application programs), or an artificial workload 
(synthetic programs developed especially for per- formance measurement). The scope of this paper will 
be restricted to the case when synthetic pro- grams are used for the performance evaluation, within the 
procedure for computer evaluation and selection which is realized using the Logic Scoring of Preference 
(LSP) method [I0, 13]. To our knowledge, no authors, except Lucas [4], have attracted attention to the 
simple fact that in the case of computer evaluation and selec- tion, performance measurements cannot 
be consid- ered isolated, but within the context of the com- puter evaluation procedure. This is rather 
sur- prising if one takes into account that it is evi- dently meaningless to apply sophisticated proce- 
dures for workload representativeness analysis and to perform highly accurate and costly measurements 
in order to obtain the values of performance indi- cators, and then to use all these results in some 
ill-defined intuitive decision process. According to the LSP method, when selecting a new computer the 
basic steps within the selection procedure should be organized as shown in Fig. I. FORMULATIONOF THE 
CRI'IERION FOR COMPUTER EVALUATION _ ,~ ,. PREPARATION INDICATORS REFLECTING THE POSSIBILITIES OF  '++ 
iF o+++++sY+++ L PERFORMANCE A A MI'ASUREMENTS HARDWARE SOFTWARE VENDOR SUPPORT I COMPUTER EVALUATION 
(Co.~putation of the resultant global preference of the computer from the values of performance varlables 
derived using performance L indicators and worth indicators of hardware, software and vendor support) 
GLOBAL GLOBAL~PREFEHENC~ COST ICOST-PREFERENCE ANALYSIS (Selection of the COST IEDICATOR best alternative 
from the set of cost--preference pairs of all competitive ANALYSIS omputers) I (i ACCEPTANCE TEsT VERIF¥IR~TME 
RE-~ i SULTS,, OF,,PERFORMANCE MEASUREMENTSJr t INSTALLATIONOF EqUiPMENT Figure 1 The contents of the 
steps termed "workload prepar- ation" and "performance measurements" result from the following facts: 
 (I) Performance measurements themselves cre- ate generally only various timing tables, and do not automatically 
solve the decision problem of selecting the most convenient computer system*.  (2) The performance indicators 
to be mea- sured, as well as the conditions of the measurement procedure,should be deter- mined according 
to the requirements de-  *Approximately the same conclusion was reached by Joslin and Aiken [2]. However, 
they did not pro- pose any kind of formalized quantitative procedure for computer evaluation and selection, 
leaving the decision problem clearly formulated, but unsolved.  259 ly) estimated by the evaluator. 
This type of the AEC is to be used only when evaluating some compo- nents of the computer proposal which 
are important (i.e. cannot be omitted), but are difficult to be quantified -the quality of the organization 
of performance measurements, the efficiency of the staff in a local agency, or the quality of various 
manuals, can serve as typical examples. The REC is defined as the criterion for evalu- ating the performance 
relations of two or more com- petitors -for example, Cr(x=) is the REC if x represents the relative performance 
variable J xj = tj/tmin, j&#38;{1,...,n} defined as a ratio of the measured elapsed time t. corresponding 
to the given computer, and the bes~ time t . corresponding to the fastest competitive mln  computer. 
Consequently, the REC cannot be used in the case when only one computer is evaluated. The last step 
within the LSP method is the logic aggregation of preferences. After trans- forming the values of performance 
variables into the values of elementary preferences, the value of the global preference of evaluated 
computer (Eo) can be derived from the values of elementary pre- ferences: E ° = L(EI,...,En) = L(gl(Xl),...,gn(Xn) 
). Here L denotes the composite extended continuous logic function. Mathematical presentation of ex- 
tended continuous logic functions can be found in [I0], and accordingly we will mention here only that 
L can be realized by superposing the weighted po~er means: k = r I/r ~r~+~, O<Wi<l , i=l,. .,k, eo 
( ~ Wiei ) , -m _ _ i=l k W.=I, z i=l where r is the adjustable parameter, WI,...,W. are k  the adoustable 
weights, el,...,e k denote some in- put preferences, and e is the corresponding out- o  put preference 
in some preference aggregation block within the hierarchical preference aggrega- tion structure which 
determines the function L. Therefore, according to the LPS method, the com- plex criterion for computer 
evaluation is synthe- sized using the criteria aggregation technique pre- sented in Fig. 2. Elementary 
preferences x I c  Global preference f . Figure 2 The four basic groups of elementary criteria for 
computer evaluation include: vendor support, performance and reliability, hardware, and soft- ware. Each 
group is partitioned into subgroups which are presented in Fig. 3. Some of these sub- groups contain 
elementary criteria related to per- formance measurements, apart from other elementary criteria. The 
set of measurement-based elementary criteria for batch systems is presented in Fig. 4. (numbers in parentheses 
denote the code of the sub- group to which the elementary criterion belongs). The complete definitions 
of measurement-based ele- mentary criteria will be introduced in Section 4. BASIC STRUCTURE OF THE CRITERION 
FOR COMPUTER EVALUATION I. VENDOR SUPPORT I.I. Training of the user "a staff 1.2. System engineer 
support I.3. Maintenance of hardware and software 1.4. Local agency of the vendor   I.5. Auxiliary 
and backup configuration 2. COMPUTER PERFORMANCE AND RELIABILITY 2.1, Measured operating speed of computer 
2.2, Measured reserves of computer resource8 2.3. Redundant unite and their reconfiguration 2.4. Basic 
components of hard, are and softwcme reliability 2.5. Measured reliability indicators 3. HARDWARE 
 3.1. Processor Z.2. Internal memory 3.3. Channels and control units 3.4. Disk memory 3.3. Magnetic 
tape memory 3.6. Input devices in the computer center 3.7. Output devices in the computer center 3.8. 
Consoles 3.9. Terminals and related remote equipment  4. SOFTWARE  4.1. Operating System 4.2. Progrc~r~ing 
Zangungee 4.3. Utility programs  4.4. Application progre~ng systems Figure 3 ELEMENTARY CRITERIA FOR 
COMPUTER EVALUATION, BASED ON PERFORMANCE MEASUREMENT RESULTS 1. Monoprogramming performance (2. I) 
 2. Multiprogra~ng performance (2. I) 3o Processor speed (2.1 or 3.1)  4. Processor idle time (2.2) 
 5. Disk channel idle time (2,2)  6. Tape channe~ idle time (2,2)  7. Memory utilization (3.2 or 4.2) 
 8. Accuracy of computation (3. I)  9. Multiprogra~ng efficiency (4.1)  10. Relative multiprogramming 
elapsed time (4. I)  11. Operating system overhead {4.1)  12. FORTRAN compiler speed {4.2) 13, COBOL 
compiler speed (4.2)  14. Efficiency of the SORT system (4,4)  15. EfficiencB of application programs 
(4.4)  16. Quality of organization of performance measurements (1.49 I?. Acceptance test (1.4)  Figure 
4 3. PRACTICAL REALIZATION OF PERFORMANCE MEASURE- MENTS Various methods for constructing drive work- 
loads are described in the literature, and rela- tive merits of synthetic programs in comparison with 
other approaches to performance evaluation within computer selection procedure are discussed (see for 
example [4, 5, lll). Some authors have described synthetic programs designed for evalu- ating only the 
processor speed (e.g. [8, 12]), while more frequently the synthetic programs  having adjustable use 
of various computer re- sources were proposed [3, 5, 9, II]). The syn- thetic program which includes 
adjustable activ- ities of input/output, processor and external storage devices within an idealized file 
mainten- ance procedure was proposed by Buchholz [3]. This synthetic program was used later by Wood and 
Forman [5], and Sreenivasan and Kleinman [9], as the basic element for building the synthetic job stream 
having the prescribed characteristics. The discussion of methods for workload charac- terization and 
the synthesis of synthetic jobs is not the primary aim of this paper. Accordingly, only those characteristics 
of drive workload which are relevant to the elementary criteria for perfor- mance evaluation of batch 
systems will be briefly discussed. The realistic approach to the drive workload development should take 
into account that the majority of modern computers serve concurrently a number of users, and therefore 
the multiprogram- ming efficiency is generally the chief objective of performance evaluation. Multiprogramming 
effi- ciency of a computer can be insufficient either because of the limited possibilities of the selected 
synthetic workload to overlap the use of system resources, or because of an ineffective hardware/software 
design. Since the evaluator is primarily interested in evaluating the limits of system's possibilities, 
the synthetic programs should be realized in a way which enables the maxi- mal (and controllable) simultaneity 
in the use of computer resources. To that end the set of so- called monoresource synthetic programs, 
convenient for multiprogramming measurements was developed. Monoresource synthetic program (MSP) is 
de- fined as a standardized independent synthetic pro- gram which predominantly uses only one system 
re- source. Its basic properties can be summarized as follows: (I) Each MSP must be as simple and short 
as possible in order to eliminate conver- sion costs (typically one page per pro- gram).  (2) The set 
of MSPs should be portable (ma- chine independent) and even as language independent as possible in order 
to mini- mize the unintentional influence of the quality of compiler.  (3) All necessary files should 
be generated by auxiliary programs in order to avoid file prepiration activities and file con- version 
problems.  (4) If T denotes the time during which  r~s  the gzven MSP uses the selected re- source, 
and T ~ denotes the total e .  elapsed time for ~hzs program, then gen- erally the MSP should be designed 
in a way which gives the ratio T &#38;/Tel as much close to I as possible ~ms ratio varies from computer 
to computer depen- ding primarily on the processor speed).  (5) Each MSP must represent some typical 
processing (for example scientific pro- cessor-bound computation, commercial processor-bound computation, 
sequential file processing, random processing of file, etc.). This processing can be con-sidered as 
derived from the most impor- tant classes of problems, namely engi-neering problems, business problems, 
mathematical problems, etc. In our case, the basic hardware and software resources considered are: processor, 
disk memory~ tape memory, card reader (or equivalent input de- vice), line printer, compilers (FORTRAN, 
COBOL, etc.), and sort system. The set of monoresource synthetic programs used for performance measure- 
ments is shown in Table I. "TABLE I. Reso~e P~ogz~m~ Activi~ of the p~ogr~, ,~me =========================================-~=. 
_--=-_===_-=======-~== ProcesNor PSCI T~c~Z proceesor-bo~d scientific computation Processor PCOM T~plca~ 
processor-bo~ commerolal computation Disk DSEQ Seq~nt~l file pmocesai~ Disk DRAN R~ndom processing of 
records in the fiZe Tape TSEQ Sequential file procesNin~ Card reader CRAC Card reader ~ctivit~ Line printer 
LPAC Line printer cctiui~7~ Sort s~stem SORF Sortin 4 of the file The workload characterization problem 
was carefully defined and analyzed [7], and general methods for its solution were proposed [l, 5, 9]. 
Workload characterization can be organized either from the "user's standpoint" (i.e. according to the 
machine independent type-of-processing classi- fication [1], or from the "operating system's standpoint" 
(i.e. according to the magnitude of demands on the system resources [5, 9]). The com- bination of these 
methods can be applied in the case of the HSP-based synthetic workload synthe- sis: First approach is 
convenient for the selec- tion of a subset of MSPs being adequate as a drive workload, and the second 
approach can be applied for the drive workload parameters adjustment. BASIC PEREORMANCE MEASUREMENTS 
OF BATCH SYSTEMS Monop~ng performance Multlprograz~ing performance MuZtip~ogra~nlng TABLE 2 m~s~nt 
measurement efficisnc y me~sumement DRIVE PSCI, POOH, DSEQ, bRAN, MDW := NI*PSCI+N2*PCOM+N3* DR2~, 2*DRAB, 
 WORKLOADS TBEQ, SORE *DSE~+Ni*DRAN+N5*TSS~ 3~ORAN,... (sepo2~ed NI,. ,, N5 = n~nbe~8 Of .., BMAX*DRAN 
by commas) copies of indiuidm~Z OeneraZZH. p~o~[m~m8 6 ~ N~iX ~ 12 Beq~ntlaE co.~i~atlon. Si~l~eous 
e~scu~ion of SimuZtaneo~ lo~di~g~ Zinkin~, and NI+N2+N$+Ni+N5 progr~s in execution of COM~UTER exertion 
of eac.h workload multip~ogrammin~ enViron- N copies of ACTIVITY ~c~rw~,m~r~ merit progro~ BRAN environment. 
(N = I,. . . N~NX) in mu~tiprogr~w- min~ environment a) CPU time :~d eZaps~ a) CPU time for uaer "s 
C~ for: t~cpu ~ t °s -collation (t~°op~" teOez), ~or~oad (t~u)op -~e~t£~g sNstem (~i~ b) CPU ~ime 
for epe~tlng tc~ " fez PEREORMANCE ~nd Zlnkln~)(t~u ~ t Os 8Hst~m (tOga) INDICATORS ¢1)" O) TO~Z CPU 
t~me ~uMEo =.o~tlo. :t~w t~J FOR EACH -p (t opu=tusu+op t°sopu ) ~t~ (s~e WORKI~OAD b) Int~Z m~ o~a-d) 
TotaZ eZ~rpsed time (tsz) indicators as tion fOr: ~) CPU idle tim~ in al~iproH~ ~ng performance measurement) 
 -compiled progr~. (m o) (tpi = te ~ -tcpu ) -data f~d ) f) ~ek channoZ idZ¢ time -~ood rnoduZa (m l) 
(tdei) c) aoc~a~y indicator g) T~pe o~anne~ idle time (PSCX o,~) (*tel) NOTE: FOr muZtlprooeasor a~stemsj 
CPB timee ~nd CPU idZe times should be ezpreased a8 ~er~ge t42nee per cent~Z p~ooe88or.   Cr(Q(~ )) 
:= {(0,0), (100,100)} P seems to be more convenient. However, in the case o . . max < f small zdle 
tlmes (e.g. %i 10~), the discrim-. inatory effect obtained by Cr(Q(~ )) cannot be ac- pl cepted as meaningful. 
Therefore, the criterion C ,^(2)) should be used only in the presence of rtqpi . max . max  sufficient 
value oi q.; ge.g. q . > 25g). Other- wise, the use of Cr(Q~i') was shown to be more con- venient. 
 (5) Disk channel idle time (Qdci) is defined ana- logously to processor idle time: / max Qdci := I00 
qdci qdci ' qdci := lOOtdci(MDW)/tel (MDW)' Cr(Qdci) := {(0,0), (100,I00)} (6) Tape channel idle time 
(Qtci) is defined as follows: max Qtci := 100 qtci/qtc i ,  qtci := lOOttci(NDW)/tei (l~w)' Cr(Qtci) 
:= {(0,0), (100,100)} The criteria for monoprogramming performance, multiprogramming performance and 
idle times repre- sent the basis for realization of the subcriterion for computer performance and reliability 
evalua- tion. The proposed structure of that criterion is presented in Fig. 5. (7) Memory utilization 
(Qmem) is defined as fol-lows : Qmem := M/Mmin' Qmem > I, M := blmc(PSCI) + b2mc(PCOH) + b3[mc(DSEQ) 
+ + m c(DRAN) + m c(TSEQ) + m c(SORF)] + + b4mI(PSCI) + b5mI(PCOM) + b6[ml(DSEQ) + + m l(DRAN) + m 
I(TSEQ)] + b7m I(SORF),  where b.,...,b_ are scaling factors derived from the wor~load c~aracterization 
analysis. The pro- posed elementary criterion is <2.5 Cr(Qmem) := {(1,100), (Bmem,0)}, 1.5 < Bme m 
= (8) Accuracy of computation In the cases when the computer will be used for the numerical computations, 
the accuracy of computation has to be measured and evaluated. If some convenient accuracy indicator cannot 
be di- rectly derived from the real workload, the syn- thetic benchmark should be organized in such a 
way~ that the accuracy indicator is generated as a by-' product in the course of the processor speed 
mea- Meuni~ of ope~torg: A ÷ ~" = 1 C-+ r = 0.1983 C-+ + ~' = -0.24?8 OPERATING SPEED CA + v = -0.?803 
 zi+2 PROCES,,~RIDLE zi+3 DISK C&#38;tNNEL ~ IDLE TI+ Zi :~ TAPE CHANNEL IDLE TIME 1~ RESERVES OF COMPUTER 
6+__15. PERFORMANCE AND zi+ 5 RESOURCES 50 60 RELIABILITY ~ RELIABILITY 60 60. Ftgure 5 surement. Consequently, 
an adequate technique for accuracy evaluation can be based on matrix inver- sion (program PSCI). To that 
end let a regular matrix be given A(aij)m, m , aij := 1.0001, i ~ j = 2.0001, i = j. The double inverted 
matrix B = (bij)m, m = (A'I) "1 will differ from A because of errors introduced by floating point arithmetic 
operations. The average error 1 m m -bl Eavr(m) := m-~ i~I j=1 ij lj can be used for the evaluation 
of accuracy. If E°Pt(m) denotes the error obtained on the most ac- avr curate among competitive computers, 
the error ratio of an analyzed computer with respect to the most accurate computer, obtained using the 
se- quence of K various sizes of matrices A and B, can be defined as follows: K Eavr(mk))I/K R := ( 
fl , R> I, k=l E°Pt(m_) avr K or logarithmically: RLOG := log(R), RLOG ~ 0. The proposed relative 
elementary criterion for the case of uniform accuracy level (generally the stan- dard precision) is 
Cr(RLOG) := {(0,100), (3,0)}. An example illustrating the applicability of the error coefficients R 
and RLOG is presented in Table 3. (9) Multiprogramming efficiency (MEF) Let N monoresource synthetic 
programs P.,...,P. be given, whose individual elapsed execu- tlon tlmes are TI,...,T.. Sequential execution 
of these programs will takeNthe following time  and the equivalent REC is TABLE 3 ANALYSED COMPUTERS 
WORD~MANTISSA [bit] R RLO0 ......................................................................... 
 ......................................................................... HONEYWELL 66/10 36/2? 1 0 
DEC 2040, ~050~ 1090 36/27 1.44 0.I~8 UNIVAC 1110, 1100/11 36/27 2.09 0.320 BURROUGHS 1714 26/24 8.01 
0.804 DEC POP 11/34 32/23 8.~4 0.931 IBM 1130 32/23 202 2,305 IBM 370/138 32/24 999 2°998 UNIVAC 90/60 
32/84 1017 3.007 N TMON0 := ~ T.. i i=l Since in a multiprogramming environment the main goal is 
to overlap various parallel processes, the multiprogramming elapsed time TMULTI, necessary for overlapped 
execution of programs PI,...,PN, will satisfy the following inequality: max (Ti) ~ TMULTI ~ TMONO ]~i~N 
 (the case when TMULTI > TMONO, although possible, represents an irregular operation). Operating system 
and computer architecture should he de- signed in a way which enables TMULTI to be as close as possible 
to the lower bound of the pre- vious interval. Therefore, the extent to which this goal is attained 
represents the multiprogram- ming efficiency of computer hardware and software, and can be defined 
as follows: realized time savin 8 MEF := 100 maximal possible time saving [%] TMONO - TMULTI = I00 
TMONO -max ._(Ti~ , 0~MEF~I00~. l~i~N The programs P-""'PN can be tuned so to have equal elapsed 1 
times, i.e. TI=T2=...TN=T. Using the results of basic performance measure- ments introduced in Table 
2, equal elapsed times can be realized using multiple copies of the pro- gram DRAN: PI=P2 .... PN=DRAN, 
T=teI(DRAN), TMULTI=teI(N*DRAN), TMONO=NteI(DRAN). Since in this special case TMONO=NT, the multipro- 
 gramming efficiency can he expressed in the fol- lowing way:  MEF = N RMTS, RMTS := IO0(I - TMULTIITHONO) 
[%], where RMTS represents "the relative multiprogram- ming time saving." In fact, RMTS indicates the 
reduction of elapsed time, caused by multiprogram- ming, expressed with respect to the sequential execution 
time TMONO, while fIEF indicates the re- duction of elapsed time expressed with respect to the corresponding 
theoretical maximal reduction TMONO-T. The proposed AEC is Cr(MEF) := {(0,0), (BMEF, I00)}, 50 &#38; 
BME F ~ 100, Cr(MEFrel) := {(O,0), (100,100)}, MEFre I := I00 NEF/MEF max. The use of AEC requires 
generally some a poste- riori adjustment of B. ~, and the use of REC is restricted to the cases~ when 
MEFmax is not too small (an acceptable condition could be MEF max >40~). Since the value of MEF depends 
on N, the minimal value min2~N<__NMAXMEF(N) is most frequent- ly used in practical evaluations. An illustration 
of TMULTI=t .(N*DRAN) and the related MEF(N) is e . presented in ~ig. 6. IMLLTI[aao[l E~O ~D 45~ 
UNI 1100/18 ~D ~D IBM 370/148 aD KD S N  Figure 6 (10) Relative multiprogramming elapsed time (RMET) 
is defined using either the workload N*DRAN: teI(N*DRAN) RMET(N*DRAN) := I00 N tel (DRAN) ' or using 
the workload MDW: RMET(MDW) := I00 teI(MDW))[NI teI(PSCI) + + N2 tel(PCOM) + N3 teI(DSEQ) + N4 teI(DRAN) 
+ + N5 teI(TSEQ)] The proposed elementary criterion is based on the use of RMTS :=IO0-RHET, having 
the AEC version: Cr(RMTS) := {(0,0), (BRMTs,IO0)}, 40~BRMTS~70 , and the REC version: Cr(RMTSrel) := 
{(0,O), (100,I00)}, RHTsmaX>25~, RMTSre I :=I00 RMTS/P~MTS max. The source version of the criterion is 
also avail- able: Cr(RMET) := {(BRMET, I00), (I00,0)}, 30 < = BRMET < = 60. Statistical indicators similar 
to MEF, RMTS, and RMET, proposed in [6] can also be applied in the synthesis of corresponding criteria. 
 (II) Operating system overhead (Qovh) is defined as follows:   qovh := I00 t~u(~W)/tcpu(~W). The proposed 
elementary criterion is either Cr(Qovh) := {(0,100), (Bovh, 0)3, 20 ~ Boy h Z 50, or max Cr(Ous) := 
{(0,0), (100,100)3, Qus := 100qus/qus ' qus := lO0"Qovh" (12, 13) Compiler speed (Qco) is defined as 
follows: , min Qco := qco/qco ' qco := Tco/Tmono' where T elapsed time for compiling all programs 
co written in the same high-level language (FORTRAN, COBOL, etc.) T = monoprogramming elapsed time used 
as an mono indicator of the overall hardware perfor- mance of the given computer (see criterion (1)). 
The proposed elementary criterion is Cr(Qco) := {(1,100), (Bco, 0)}, 3 ~ BCO <5. (14) Effi?iency of 
the SORT system (Qsort) is defined as follows: / min Qsort := qsort qsort' qsort := t:~(SORF)/Tmono" 
 The proposed elementary criterion is Cr(Qsort) := {(I,I00), (Bsort , 0)3, 2 ~ Bsort < 3. (15) Efficiency 
of application and utility pro- grams can be defined analogously to the efficiency of the SORT system. 
 (16) Quality of the organization of performance measurements (Q ) is generally evaluated by the  
 org dlrect preference estimation of Qor ' so that the corresponding elementary criterion I~ Cr(Qorg) 
:= {(0,0), (100,100)3. Q is estimated on the basis of the vendor's re- a~[~ation of all given conditions 
of performance measurements (completeness of results, deadlines, etc.), since the quality of organization 
of perfor- mance measurements seems to be highly indicative for the possibilities of the future vendor's 
sup- port. (17) Acceptance test represents the final part of the computer evaluation process, and takes 
place immediately after the installation of equip- ment. The first basic requirement of the accep- tance 
test is to reproduce all results of perfor- mance measurements which are obtained by the ven- dor, and 
used for evaluation. In the case that the equivalent results obtained during the accep- tance test are 
worse than those presented by the vendor, then the vendor is obliged either to change the delivered equipment, 
or to reduce accor- dingly its cost. The second basic requirement of the acceptance test is to realize 
the up-time greater than 95~ during several consecutive working days (usually from 5 to 15 days). The 
con- ditions of the acceptance test are precisely spec- ified in the appropriate technical documentation, 
and after the evaluation these conditions must be included in the contract. The proposed elementary criterion 
is Cr(Qat) := {(0,0), (1,100)3, where Qat = I if the vendor accepts the conditions of the acceptance 
test defined by the user, other- wise Qat = O. 5. CONCLUSION The proposed elementary criteria represent 
the basis for the systematic aggregation of the performance measurement results with the values of all 
other relevant worth indicators of the evalu- ated computer, making possible the decision pro- cess for 
computer evaluation and selection, based on the LSP method. Some of the presented elementary criteria 
are partially redundant, and the present list of ele- mentary criteria is not definitive, since the cri- 
teria for evaluation of the time sharing computer systems are still missing. However, the proposed approach 
makes possible the definitions of addi- tional elementary criteria in the manner similar to those previously 
described. Moreover, the pro- posed criteria clearly determine which indicators must be measured during 
the performance measure- ments, and give the guidelines for the preparation of drive workload. REFERENCES 
 [1] Joslin, E.O., Application Benchmarks/The Key to Meaningful Computer Evaluations, Proc. 20th ACM 
Nat. Conf., pp. 20-37, 1965. [2] Joslin, E.O. and Aiken, J.J., The Validity of Basing Computer Selections 
on Benchmark Results, Computers and Automation, Vol. 15, No. 1, pp. 22-23, January 1966. [3] Buchholz, 
W., A Synthetic Job for Measuring System Performance, IBM Syst. J., Vol. 8, No. 4, pp. 309-318, 1969. 
[4] Lucas, H.C. Jr., Performance Evaluation and Monitoring, Computing Surveys, Vol. 3, No. 3, pp. 79-91, 
September 1971. [5] Wood, D.C. and Forman, E.H., Throughput Mea- surement Using a Synthetic Job Stream, 
AFIPS Conf. Proc. (FJCC), Vol. 39, pp. 51-56, November 1971. [6] Kummerle, K., Characteristic parameters 
for the description of performance and efficiency of EDP systems, (In German). Elektronische Rechenanlagen 
14(1972), H.I, pp. 12-18. [7] Ferrari, D., Workload Characterization and Selection in Computer Performance 
Measurement, Computer, pp. 18-24, July/August 1972. Is] Bell, A.G., Hallowell, P.J. and Long, D.H. A 
Universal Benchmark, Software-Practice and Experience, Vol. 3, pp. 355-357, 1973. [9] Sreenivasan, K. 
and Kleiuman, A.J., On the Construction of .a Representativ e Synthetic Workload, Communications of the 
ACM, Vol. 17, No. 3, pp. 127-133, March 1974. [lOl Dujmovic, J.J., Extended Continuous Logic and the 
Theory of Complex Criteria, Publ. Elek. Fak. Ser. Mat. Phys. No. 498-541, (1975), pp. 197-216. [11I Waiters, 
R.E., Benchmark. Techniques/A .Contruc- tive Approach, The Computer Journal, Vol. 19, No. 1, pp. 50-55, 
February 1976. [12] Curnow, H.J. and Wichmann, B.A., A Synthetic Benchmark, The Computer Journal, Vol. 
19, No. 1, pp. 43-49, February, 1976. [13] Dujmovic, J.J., Professional Evaluation and Selection of Compute 
L Systems , Proceedings of the 12th Yugoslav International Symposium on Information Processing, Paper 
4-001, October 1977. 267   
			