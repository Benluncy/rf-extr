
 AN ENH AN-CED RSM ALGORITHM USING GRADIENT-DEFLECTION AND SECOND-ORDER SEARCH STRATEGIES Shirish S. 
Joshi Hanif D, Sherali Center for Transpiration Analysis Department of Industrial and Systems Engineering 
Oak Ridge National Laboratory Virginia Polytechnic Institute and State University P. ~. BOX 2008 Blacksburg, 
Virginia 24061 Oak Ridge, Tennessee 37831-6206 Jeffrey D. Tew ISQ Department P.O. BOX 6696 Consolidated 
Freightways, Inc. Portland, Oregon 97208-6696 ABSTRACT This paper presents a new methodology for improv­ing 
the search techniques currently being used in standard Response Surface Methodology ( RSM ) al­gorithms. 
RSM is a collection of mathematical and statistical techniques used for experimental optimiza­tion. Our 
improved RSM algorithm incorporates cer­tain gradient deflection methods, augmented with ap­propriate 
restarting criteria, as opposed to using the path of steepest descent as the only search direction. In 
order to empirically investigate our new RSM al­gorithm in comparison with the standard RSM tech­niques, 
a set of standard test functions is used. We consider two cases for each test function; with a ran­dom 
perturbation added to the function and without a random perturbation added to the function. Com­ putational 
results exhibit the improvements achieved under the proposed algorithm. INTRODUCTION The focus of this 
paper is on improving the standard algorithm of response surface methodology (RSM ) as it is applied 
to the optimization of simulation models. First proposed by Box and Wilson (1951) in the con­text of 
optimization problems concerned with chemi­cal process engineering, RSM is a collection of n]ath­ematical 
and statistical techniques for experimental optimization. The search procedures current ly em­ployed 
by RSM use the method of steepest descent only. This paper develops a new RSM algorithm which incorporates 
certain gradient deflection meth­ods and is further augmented with suitable restart­ing criteria. We 
use a set of standard test functions taken from the literature to empirically investigate the merits 
of this new RSM algorithm relative to the standard R,SM algorithm. Cornput ational results ex­hibit the 
improvements achieved under the proposed algorithm. This paper is organized as follows. The remainder 
of this section presents an overview of RSM and de­fines the standard RSM algorithm. A discussion of 
various competitive gradient deflection techniques is also given. Section 2 gives a definition of our 
proposed RSM algorithm. A summary of extensive empirical results of a comparison study are given in Section 
3. Section 4 gives conclusions and recommendations for future research. 1.1 Definition of Response Surface 
Method­ology Throughout, this paper we focus only on an uncon­strain ed minimization problem and that 
the func­tional form of the objective function is unknown. Typically, to start the RSM procedure, an 
experi­ment is designed in a small sub-region of the fac­tor space, and a low-degree polynomial (usually 
first­order) is used to represent the data obtained from the responses. This polynomial helps the experimenter 
297 decide the next region of the factor space that should be explored by identifying a search direction 
along the path of steepest descent (in the case of minimization). If the goal is to minimize the response, 
this method tries to climb down the response surface toward the overall, response minimum rather than 
exploring a large region of the factor space. Its success depends on the assumption that the overall, 
response mini­mum can be reached via such a path of descent (see Davies 1956, p. 503). (Throughout the 
remainder of this paper, we assume that problem at hand is to mmirnize the expected mean response over 
the factor space of interest. ) If the experimenter haa a prior notion of the gen­eral vicinity of the 
location of the minimum, then Myers (1976) gives the following five-step procedure (see Myers 1976, p. 
88): STEP 1: Fit a first-order regression model to the mean response over some restricted (usually taken 
to be small) region of the factor space. STEP 2: Based on the results of Step 1, estimate a path of 
steepest descent. STEP 3: Perform a series of experiments along the estimated steepest descent path 
until no addi­tional decrease in the mean response is evident. STEP 4: Based on the results of Steps 
1, 2, and 3, estimate the overall, minimum mean response. STEP 5: Repeat Steps 1, 2, 3, and 4 over a 
new region centered at the current estimate of the overall, minimum mean response. If curvature is evident 
and the experimenter is satisfied that little or no significant, additional improvement in the estimate 
of the overall, minimum mean response can be obtained from conducting fur­ther searches, a more elaborate 
experiment and analysis study is conducted using a second-order design. From this design, a final estimate 
of the minimum mean response is obtained. In Step 1, typically, replications of a factorial or fractional 
factorial experiment are performed. The unknown parameters of the fitted model are then computed. A judicious 
selection of an experimental design having desirable properties such as minimum variance of model parameter 
estimates is needed in order to obtain better estimates of the unknown pa­rameters. The first-order model 
linear model is rep­resented as (1) 1=1 and Tew where yij is the response of the jth replication at the 
ith design point, ~[ is the lth unknown model param­eter, ZI is the lth experimental factor, and Cij 
is the error term at the ith design point and jth replica­tion, (for i=l,2, . . ..rn. j=l,2,.. .,r=; 
and 1= 1,2,..., k). If, for example, a full 2~ factorial experiment is used in Step 1, then estimates 
of all the /3[ can be obtained. We denote these estimates by b{ (for 1 = 1,2,..., k). These estimates 
are then used in Step 2 to locate the path of steepest descent, which is given by the vector d = ( bl, 
bz, . . . . b~). Responses are oberved along this path using some prescribed search strategy until there 
is no observed, significant improvement in the mean response. In the standard RSM approach, a simple 
sequence of fixed steps along the direction d are taken. If curvature is evident, then a second-order 
model is fitted. The second-order model is represented as: kk 1=1 h>l 1=1 where flh[ is the coefficient 
of interaction between fac­tors h and 1, /311is the second-order coefficient for fac­tor 1, and all other 
terms are as defined in (1). The model in (2) is used to evaluate a stationary point of the response 
surface. A canonical analysis is per­formed that typically involves locating the stationary point as 
well as determining the nature of this point. From this analysis, an estimate of the optimum is ob­t 
ained; perhaps following additional investigation in the case of a detected ridge system. The principal 
shortcomings of the standard RSM algorithm described in this section are two-fold. First, the inital 
gradient search can he prone to zig­zagging and can be slow to converge (see Bazaraa, Sherali, and Shetty, 
1993). Moreover, there is no information from previous iterations that is succes­sively employed to provide 
improved search direc­tions (the process is essentially memory less). Second, in the event that curvature 
is detected, there is no attempt to conduct, a continuous, iterative, second­order search using the information 
available from the canonical analysis. In this paper we present a revised RSM algorithm which attempts 
to rectify these short­comings using a technique called gradient deflection. 1.2 Gradient Deflection 
Methods This section discusses the four gradient deflection methods we used in revising the standard 
RSM al­gorithm presented in the previous section. Origi­nally developed by Hestenes and Stiefel ( 1952), 
conju­gate gradient (gradient deflection) methods were first 299 applied to unconstrained minimization 
problems by Fletcher and Reeves (1964). In this approach. a se­quence of design points, xj, and a sequence 
of direc­tions dj, are generated iteratively as xj+l = XJ +Ajdj, (3) where dj = gj +~jdj_l) (4) and do 
= O, ttj is a scalar multiplier that scales the direction vector of the previous iteration (the calcu­lation 
of Kj varys under different gradient deflection methods), gj is the gradient of the objective function 
at the operating point, xj (aasumed to be nonzero, or else the method is terminated), and Aj is the step 
length adopted along the descent direction dj. As seen from (4), a deflected direction dj at the jth 
it­eration is comprised of a linear combination of the negative gradient, at the jth iteration and the 
direc­tion vector of the previous iteration. This implies a possible advantage in this method over the 
method of steepest descent, since the deflection strategy at­tempts to capture the second-order curvature 
effects over successive iterations. For a quadratic function in k variables, this approach can be made 
to converge to an optimum within k iterations (see Bazarra et al., 1993). Various gradient deflection 
methods use different techniques for computing the deflection parameters Kj at the jth iteration. In 
this paper, we consider four such techniques that have been suggested in the literature. A brief description 
of each of these tech­niques follows. A gradient deflection method proposed by Sherali and {Jlular (1989) 
that was found to be promising even for nondifferentiable functions (where gj repre­ sents a subgradient 
of the function) uses the average dzrection strdegy and is denoted here as GD1. The deflected direction 
at the jth iteration for this method is given by (5) where dj _ 1 is the search direction adopted at 
the (j l)st iteration, with dl = gl. In the context of proposing a conjugate gradient al­ gorithm that 
adopts quasi-Newton types of updates and permits inexact line searches, Sherali and [Hular (1989) develop 
yet another scheme for computing ~j. Their method, denoted in this paper as GD2, gener­ ates the deflected 
direction at the jth iteration as where p~ =Xj Xj ], (7 ) qj =Fiy .ffj-1, (8) and sj is a scale parameter 
that, if suitably chosen, permits SJdj to be the Newton direction at the jth iteration, given that this 
direction is spanned by gj and dj 1. From a computational viewpoint, the pa­rameter sj is prescribed 
as Sj = Aj 1. (9) Motivated by the computational success of BFGS quasi-Newton updates, Shanno proposed 
a related conjugate gradient strategy for which the direction at the jth (j = 2, 3, . . .) iteration 
is given by d, =_ ~_ Pjq j+qjP ~ 1 gj~ (10) [ q j P~ 1 where pj and ~ are as defined in (7) and (8), 
respec­tively. We denote this gradient deflection method as GD3. Finally, a fourth gradient deflection 
method we considered, denoted by GD4, is a modification of GD2 suggested by Sherali and lJlular (1990) 
that converts this strategy into a symmetric, memoryless BFGS type of an update. The prescribed search 
directicm under this modification is given by dj = -1- q ~ jp ~ + [ q jPj +  [:+%3%3 H) where pi, qj, 
and sj are as defined in (7), (8), and (9), respectively. The four gradient deflection methods described 
above were used in our emprical study whose partial results are summarized in Section 3. 1.3 Restarting 
Techniques In addition to the gradient deflection techniques pre­sented in the previous section, restarting 
techniques, in conduction with gradient, deflection methods, can significantly enhance their performance. 
Beale (1972) and Powell (1977) have offered various criteria for im­proving the performance of gradient 
deflection meth­ods. In the present, context of RSM, restarting at any iteration would imply following 
the path of steep­est descent, at that iteration: although, as suggested by Powell (1977), in the context 
of conjugate gra­dient methods, one could restart using the current  deflected direction in order to 
preserve the accumu­lated second-order information. However, this would require an additional term in 
the deflection scheme. We considered two restarting techniques, RSA and RSB, which were used in conduction 
with the four gradient deflection methods discussed in the previous section. First, we consider RSA. 
For a k-factor problem, the optimization procedure is restarted at, the jth itera­tion, by setting dj 
= gj, if any one of the following three conditions is satisfied: CONDITION 1: j = k, CONDITION 2: ]Ig 
jgj+lll Z (0.2)ilgj 1/2, CONDITION 3: (-1.2) llgil]2 < Ci proj,jgproj,j < (-o.s)(g jgpr~j,j) is false, 
 where dproj,j and gproj ,j, respect ively, project dj and gj onto the box (boundary) constraints by 
zero­ing out the components corresponding to the active bounds that would be immediately violated by 
mov­ ing along the corresponding direction dj or gj. The rationale for these conditions is provided in 
Section 4 of Powell (1977) and in Bazaraa et al. (1993). Next, we consider RSB. For a k-factor problem, 
the optimization procedure is restarted at the jth it­eration if any one of the following two conditions 
is satisfied: CONDITION 1: j = k, COND1TION 2: d proj,j > ( O.S)(g jgProj,j). Notice that the conditons 
given for RSB are a subset of the conditions given for RSA, and that in particular, since restarting 
is triggered whenever d proj,j gj > ( O.$)(g jgProj,j ), the adwted direction dproj,j is always a descent 
direction hav% a Suffi­ciently negative directional derivative. In the next section we formally present 
an enhanced RSM al­gorithm that incorporates the deflection and restart schemes discussed in this section 
and also adopts second-order search directions whenever significant curvature is detected. 2 AN ENHANCED 
RSM ALGORITHM This section presents an enhanced version of the stan­dard RSM algorithm outlined in Section 
1. It incor­porates the gradient deflection methods which were presented in Section 1.2, along with second-order 
search directions. In the following, we assume that the problem is a minimization one in k factors. All 
 factors are assumed to have upper and lower bounds that cannot be violated at, a prescribed solution; 
al­ though, in any experimental design we can set the levels of a factor above or below that factor s 
bounds (i.e., when the center of a design lies on the boundary of any factor). However, along a search 
direction if some factors reach their lower or upper bound, then their levels are set to that respective 
boundary value and the search is continued. The steps of the en­ hanced RSM algorithm are given below: 
STEP 1: Select a starting point as the current in­cumbent solution. STEP 2: Construct a 2~ full factorial 
experiment, using the current incumbent solution as the cen­ter of the design. If the first-order model 
is a good fit, then proceed to Step 3. Otherwise, go to Step 4. STEP 3: For the first pass of the algorithm 
(j = 1), adopt the path of steepest, descent as the search direction dl. For subsequent iterations, adopt 
the appropriate deflected gradient direction, dj, as prescribed in Section 1.2, using any one of the 
deflection techniques GD1, GD2, GD3, or GD4 unless a restart is triggered by the selected crite­rion 
RSA or RSB in which case follow the path of steepest descent (reset dj = gj ). If any vari­able is at 
its upper or lower bound, and if the search direction being followed immediately vio­lates this bound, 
then fix the corresponding com­ponent of the search direction, dj, to zero. If the appropriate restarting 
criterion is triggered, then restart with the negative gradient direc­tion. (Note that this restarting 
check is per­formed only when using deflected diretions. ) If the direction adopted is a projection of 
the nega­tive gradient direction, and this direction is zero, then proceed to Step 9. Otherwise, the 
present direction is a descent direction. Call this the revised search direction dj. Determine a maxi­mum 
step-length, ~~ar, that can be taken along this direction without violating any bounds and conduct a 
line search on this interval to deter­mine a suitable step-length ~ E (O, Amaz]. If the optimal step-length 
A* c (O, &#38;aZ], return to Step 2 with the resulting solution as the current incumbent solution. Otherwise, 
if A* = ~~.x, then further project the current direction by ze­roing out the components corresponding 
to the factors that have just hit their bounds and con­tinue searching along such projected directions 
until no further improvement, can be obtained. An Enhanced RSM Return to Step 2 with the current incumbent 
so lution. In the rare instance that J* = O, due to inaccuracies in experimental, functional evalua­tions, 
terminate the algorithm by proceeding to Step 9. STEP 4: Construct a second-order design and check if 
the second-order model represents the data adequately. If the second-order model is a good fit, then 
go to Step 6. Otherwise, proceed to Step 5. STEP 5: Expand (or contract,) the design size ap­propriately 
and return to Step 2. STEP 6: Suppose the center of the current design is located at x, let gj = V.f(x) 
as determined by the second-order model. Perform a canonical analysis. Let H denote the Hessian of the 
pre­dicted quadratic response function with k eigen­values ~l)}z,...,~~. Also, let vl, v2, . . ..v~ be 
the corresponding k normalized eigenvectors, Q = [VI, VZ,..., Vk], A = diag[~l,~z,...,~~], I+= {i: Ai 
> 0 and v igj # O}, and I_= {i: Ai <o}. If gi # O, then proceed to Step 7. Otherwise, go to Step 8. Step 
7: If 1+ = +, then return to Step 3 with dj = gj. otherwise, let (12) so that d jgj = ~ v ~)z <O. (13) 
t iEI+ If d proj,j gj < 0, then perform Step ~ using dj as the prescribed search direction (no restarting 
criterion is checked in this case). Otherwise, per­ form Step 3 after resetting dj = gj ml return to 
Step 2 with the current incumbent solution. If the optimal step-length J* equals zero, then terminate 
the algorithm by proceeding to Step 9. Step 8: If 1-= ~, then terminate the algorithm by proceeding to 
Step 9. Otherwise, let At = min{~i : i E 1 }. Pick dj = Vf. In this case, we note that, ~(x + ~dj) = 
?(x)+ Ag ldj + +A211dj1120(~ -0), (14) Algorithm 301 where O(A -O) is a function that approaches zero 
as A d O. Since gj = O, this yields ~im f(x+ ~dj )-f(x) = d j Hdj A-+o ~2 5­= v t QAQtvt = etj Aet = 
At <o, (15) where et is a vector having a 1 in position t and 0 s everywhere else. Thus, dj is a descent 
direction. Perform a search as in Step 3 using dj as the prescribed search direction (no restart­ ing 
criterion is checked in this case). If an im­ provement results, then return to Step 2. Oth­ erwise, 
if }* = O (which would be the case if d proj,j gj > 0) repeat this step by replacing ~­ with I_ {t}. 
STEP 9: Terminate the algorithm and report the incumbent solution. The next section contains a summary 
of a compu­tational assessment of the enhanced RSM algorithm and draws some conclusions about the relative 
per­formance of the four gradient deflection methods dis­cussed in this paper. Recommendations for adoption 
of a suitable restart policy under each of the gradient deflection methods are also given.  3 EMPIRICAL 
RESULTS This section provides a brief summary of experimen­ tal results obtained as a result of empirical 
evalua­ tions of the enhanced RSM algorithm to the stan­ dard one presented in Section 1. First, we performed 
a comparative study of the four gradient deflection methods with no restarting criterion in the context 
of the RSM algorithm using ten deterministic test functions given by Sherali and Ulular (1990). Com­ 
prehensive tables of results obtained as a result of the experimental trials are given in Section 3 of 
Joshi, Sherali, and Tew (1993). Table I contains computational results showing the best (smallest response 
value) solutions obtained us­ ing the four gradient deflection methods and the method of steepest, descent 
along with the number of simulation runs needed to reach these solutions. Overall, GD1 performed as well 
as or significantly better than PO!3D on 9 of the 10 test functions and, sornet imes with significantly 
fewer simulation runs. Moreover, GD 1 performed the best of all methods considered in 6 of the 10 cases. 
In particular, inter­ esting results were obtained for test functions 3. 6, and 8. For functions 3 and 
6, GD1, GD2, and GD3 exhibited superior performance to the POSD; again, oft en with significant, 1y fewer 
simulation runs. For function 8, all gradient deflection methods performed worse than the method of steepest 
descent. For the other test functions, in general, POSD did not, per­form significantly worse or better 
than the gradient deflection methods. The focus for the remainder of the empirical study was therefore 
restricted to test functions 3, 6, and 8. In order to improve the performance of the en­hanced RSM algorithm, 
the starting criteria dis­cussed in Section 2.2, RSA and RSB, were employed. The results for these two 
criteria were tabulated in Section 3 of Joshi, Sherali, and Tew (1993) and indi­cated that GD2 performs 
better with RSA and GD1, GD3, and GD4 work well with RSB. This allocation of the restarting criteria 
to the gradient deflection methods was preserved throughout the remainder of the study. A point of interest 
for function 8 is the number of second-order models employed under the gradient deflection method. Recall 
from Section 1 that the standard RSIM algorifihm stops after the jir-st second­order model adequately 
fits the data and reports the most favorable design point obtained using that de­sign as the optimum. 
The added feature of the mod­ified algorithm that allows the search to be contin­ued using more designs, 
if needed, after thedata has been adequately represented by a second-order model at any stage of the 
algorithm, improves the mean re­sponse of the best design point reported. Table 5 of Joshi, Sherali, 
and Tew (1993) illustrates such an improvement achieved on function 8 under different starting conditions. 
In particular, note that, while POSD achieves a minimum response value of 3.42 if only one second-order 
model is employed, it attains an improved objective function value of 0.06 if a suc­cession of second-order 
model runs (5 in this case) are employed. Similarly, GD3 uses 6 second-order models under the restarting 
criterion R,SB to reach a minimum value of 0.02, with the true optimal value being O. If the same algorithm 
is used in conjunction with GD3, but cmly one second-order model is em­ployed, then the minimum value 
obtained was 4.71. This illustrates the improvement possible under the enhanced second-order search steps. 
The three test, functions were then modified to in­clude a measure of random behavior and all five algo­rithms 
(POSD, GD1, GD2, GD3, and C,D4) were per­formed on them. For these runs, at each design point, the response 
function was augmented by a random er­ror term that was normally distributed with mean O and variance 
equal to 10% of the he reponse value at the center of the corresponding design. Two indepen­dent replications 
were performed at each design point and the mean of these two responses was recorded. Independent replications 
were also performed across design points. While performing line searches, the variance on the error term 
was the same as that used for the first-order design. For all test functions, the uncoded values of the 
factors for the first-order model were set at +0.1 and O. 1 as the high and low levels, respectively. 
The axial points for the second-order model were selected so as to make the design rotat­able (see Section 
7.3 in Myers 1976 for a discussion on rotatable designs). Computational results for test functions 3, 
6, and 8 with random error terms added are given in Tables 6, 7, and 8 of Joshi, Sherali, and Tew (1993). 
With the pressence of a random error term for func­tion 3, GD1 performed considerably better than all 
of the other methods. Also, GD2, GD3, and GD4 failed to perform as well as POSD in this situation. For 
function 6, GD1 again performed superior to all other methods. C,D4 performed competitively with POSD. 
For function 8, all methods performed well in terms of the minimum value returned. However, GD 1 requ 
ired significantly more simulation runs than the other methods. The results for function 8 indi­ cated 
that (contrary to the deterministic case) POSD nolonger performed better than the gradient deflec­tion 
methods and on average did not perform as well w GD2, GD3, and GD4 while performing only marginally better 
than GD1. This indicates, as ex­pected, that the relative effectiveness oft he methods is significantly 
affected by the level of random vari­ation present in the response. Overall, C,D1 contin­ued to perform 
better than the other methods and is therefore recommended for future use in RSM opti­mization probIems. 
These results, although preliminary, illustrate the relative performance of the enhanced RSM algorithm 
to the standard RSM algorithm and indicate that sig­nificant improvements can be obtained when using 
gradient deflection methods. The numerical results also indicate that GD1 offers, perhaps, the most po­tential 
improvement over PO SD. 4 SUMMARY AND CONCLUSIONS The standard RSM approach uses a pure gradient search 
method that can be prone to zig zaging (i.e., slow to converge). There is no information from pervious 
iterations that is successively employed to provide improved search directions; the process is es­sentially 
memoryless. hIoreover, after curvature is detected there is no attempt to conduct a contin­An Enhanced 
RSM i able I Minimum Response with no Restarts uous iterative second-order search using the infor­mation 
available from the canonical analysis. This paper presents a new RSM algorithm which rectifies these 
shortcomings by incorporating gradient deflec­tion techniques in conjunction with certain restart­ing 
criteria as well as adopting second-order search directions whenever significant curvature is evident. 
An empirical study comparing our revised RSM algo rithm to the standard RSM algorithm using a set of 
test functions taken from the literature suggests us­ ing the average direction strategy proposed by 
Sherali and Ulular (1989) in conjunction with the criterion of RSB. REFERENCES Bazaraa, M. S., H. D. 
Sherali, and C. M. Shetty. 1993. Nonlinear programming: theory and algo­rithms, 2d. ed. New York: John 
Wiley &#38;. Sons. Beale, E. M. 1972. A derivation of conjugate gradi­ents, Numen cal Methods for Nonlinear 
Optzmzza ­tton, F. A. Lootsma, ed. London: Academic Press, 39-43. Box, G. E. P. and K. B. Wilson. 1951. 
On the experi­ment al attainment of optimum conditions. Jourrr al of the Royal Stattsttcal Society, B13:1 
38. Davies, O. L. 1956. Desagn and analysis of zndustria! experiments, 2d. ed. New York: Hafner Publishing 
Co., Inc. Fletcher, R. and C. M. Reeves. 1964. Function min­imization by conjugate gradients. Computer 
.Jour­nal, 7:149 154. Hestenes, M. R. and E. Stiefel. 1952. Methods of con­jugate gradients for solving 
linear systems. journal of Reasearch of the Nationai Bureau of Standards, Algorithm 303 48:409 436. 
 Joshi, S., H. D. Sherali, and J. D. Tew. 1993. An Enhanced Response Surface Methodology Algo­rithm Using 
Gradient-Deflection and Second-Order Search Strategies. Technical Report, Department of Industrial and 
Systems Engineering, Virginia Poly­technic Institute and State University. Myers, R. H. 1976. Response 
surface methodology. Ann Arbor: Edwards Brothers, Inc. Powell, M. J. D. 1977. Restart procedures for 
the corrr­jugate gradient method. Mathematical Prograni­ming, 12:241 254. Sherali, H. D. and O. Ulular. 
1989. A primal­dual conjugate subgradient algorithm for specially structured linear and convex programming 
prob­lems. Applied Mathematics and Optimization, 20:193-221. SheraJi, H. D. and O. Ulular. 1990. Conjugate 
gradi­ent methods using quasi-newton uptdates with in­exact line searches. Journal of Mathematical AnaL 
ysas and Applications, 150:1359-377. AUTHOR BIOGRAPHIES SHIRISH JOSHI is a post-doctoral research as­sociate 
at the Oak Ridge National Laboratory in the Intelligent Vehicle Highway Systems (IVHS) re­search group. 
He received his B.S. in Electrical En­gineering from Bornbay University, Bombay, India, in 1986, a M.S. 
in Mathematical Sciences from Vir­ginia Commonwealth University, Virginia, M .S. and Ph.D in Industrial 
and Systems Engineering from Virginia Polytechnic Institute and State University, Blacksburg, Virginia 
in 1991 and 1993, respectively. His current research interests include output anal­ysis of traffic simulations, 
variance reduction tech­niques, simulation-opt irniz at ion t echnques, and de­sign of simulation experiments. 
HANIF D. SHERALI is the Charles O. Gordon Professor of Industrial Engineering in the Depart­ment of Industrial 
and Systems Engineering at Vir­ginia Polytechnic Institute and State University. He has published numerous 
articles in many archival journals such as Applied Mathernatzcs and Optim$za­tton, EJOR, IIE, Journal 
of Mathematical Analysis and Appltcattons, Management Sctence, and Opera­tions Research, among other. 
He is also a co-author of text Nonlinear Programming: Theory and A!go­m thms 2nd. edition. His research 
interests are in ap­plied mathematics and mathematical programming. He is also a member of numerous professional 
soci­eties. JEFFREY D. TEW is a Senior Systems Engi­neer at Consolidated Freightways. Inc. and an Ad­junct 
Associate Professor of Computer Simulation at the Oregon Graduate Institute. Prior to these po­sitions 
he was on the facluty in the Industrial and Systems Engineering Department at Virginia Poly­technic Institute 
and State University. He has pub­lished numberous articles in many archival journals such as EJOR, JSCS, 
Management Science, Opera­tions Research, and SCS Transactions among others. He is the 1994 WSC Proceedings 
Editor as well as the Vice President/President Elect of the TIMS College on Simulation. He received a 
B.S. in mathematics from Purdue University in 1979, a M .S. in statistics from Purdue University in 1981, 
and a Ph.D. in indus­trial engineering from Purdue University in 1986. His current research interests 
include variance reduction techniques, simulation optimization, and the design of simulation experiments. 
He is a member of Alpha Pi Mul ACM, ASA, IIE, IMS, ORSA, SCS, Sigma Xi, and TIMS.  
			