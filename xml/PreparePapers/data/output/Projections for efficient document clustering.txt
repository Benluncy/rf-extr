
 Projections for Efficient Document Clustering Hinrich Schutze, Craig Silverstein Xerox Palo Alto Research 
Center 3333 Coyote Hill Road palo Alto, CA 94304 emaik schuetze~parc.xerox.cent, csilvers6!cs .stanford.edu 
URL:ftp://parcftp .xerox.com/pub/qca/papers Abstract Clustering is increasing in importance, but linear-and 
even constant-time clustering algorithms are often too slow for real-time applications. A simple way 
to speed up clustering is to speed up the distance calculations at the heart of clus­tering routines. 
We study two techniques for improving the cost ofdistance calculations, LSI and trrmcation, and deter­mine 
both how much these techniques speed up clustering and how much they affect the quality of the resulting 
clus­ters. We find that the speed increase is significant while surprisingly the quality of clustering 
is not adversely af­fected. We conclude that truncation yields clusters as good as those produced by 
full-profile clustering while offering a significant speed advantage. Introduction Clustering is becoming 
increasingly widespread: It is finding applications in browsing [8, 7], in improving the performance 
of similarity search tools [16, 19], and in automatically gen­erating thesauri [5, 6], In query analysis, 
clustering has been used for transforming a free-text query into a fuzzy Boolean constraint [25]. The 
popularity of Yahoo! demonstrates the potential of categorization for presenting information on the World 
Wide Web. Clustering can only approximate a man­ual categorization like Yahoo! s, but in many cases such 
an approximation is still beneficial while at the same time being cheap to install. Many of these clustering 
applications demand rapid re­sponse times while utilizing data sets too large for Iinear­tirne clustering 
algorithms. Even constant-time clustering algorithms such as constant-time Scatter/Gather [7] can, because 
of large constants, be too slow for very large data sets. It is possible to decrease the constants used 
in cluster­ing routines. We concentrate on doing so in the context of clustering text documents, which 
we consider as vectors of terms. The bottleneck in clustering text documents is calcu­lating the dktance 
between term vectors. This calculation takes time proportional to the number of distinct terms in Permission 
to make digitallhard copies of all or pan of thk material for personal or Ckeroonr use is granted without 
fiw provided that the copies are not made or distributed fw profit or cannmercialedvaotage,tbecopy­ right* 
~ titleof* PuMiiioo anditsdateappear.andnotice is givtm that copyri@t is by permission of the ACM, bte. 
To copy otherwise, torepublish,toposton.servemorto redistributeto lists. requiresspecific permission 
@or f= S7GJR 97 PhiladelphiaPA, USA q&#38;@ 1997 ACM0-$9Wl-836-3197~..$3 .5O the smaller docnment. One 
obvious way to speed up cluster­ing, then, is to project each document onto a small subspsce of the total 
term space, thereby reducing the average num­ber of terms in each document. For another application of 
clustering, word sense disarnbiguation, it has been shown that projection onto a smaller subapace does 
not affect per­formance [24]. This is further motivation for exploring pro­jection for clustering in 
information retrieval. There are two different approaches to projecting docu­ments. One is a bad method, 
where for each document we excise a number of unimportant terms. This type of prr+ jection, called truncation, 
is called locaJ because each doc­ument is projected onto a dMerent subepace. In practice, we only truncate 
cluster centroids, 1 which are often rather dense and thus benefit greatly from truncation. Document 
vectors are usually quite sparse and benefit minimally from the sparsification provided by truncation. 
The alternative to local projection is glolml projection, in which the terms to delete are chosen first, 
and then these terms are deleted from each document. This type of pro jection is called dimension rwdaction. 
The disadvantage of dirttension reduction is that it does not adapt to the unique characteristics of 
each document; its advantage is that it better preservea the ability to compare even dissimilar doc­uments, 
since all documents undergo an identical projection. It is possible to preprocess the documents before 
project­ing them. One common preprocessing step for truncation is weighting, whereby each term in a document 
is assigned a weight based on its fkquency in that document and, poe­sibly, in other documents. Usually, 
terms with the lowest weight are then deleted. There is no equally obvious pre­processing step for dimension 
reduction, but an increasingly popular step haa been to map the documents from term space to an orthonormal 
space by means of Latent Semantic Indexing (LSI), an application of Singular Value Decomposi­tion (SVD) 
to this problem. An advantage of the orthonor­mal space (which we call LSI space ) is that the dmen­sions 
are ordered, in that projecting the set of documents onto the d lowest dimensions is guaranteed to have, 
among all possible projections to a d dimensional space, the lowest possible least-square distance to 
the original documents. In this sense, LSI finds an optimal solution to dimensicmdity reduction. See 
[9] for a further discussion of LSI artd [1] for a description of SVD and the algorithms we use to compute 
it. There has been a fair amount of work on comparing LSI and term-based d~tance measures, but this work 
has been 1A cIuster s centrotd is the vector sum of itsmemberz in the context of similarity search 
in ad hoc retrieval rather than in clustering. In the similarity search context, dimen­sion reduction 
via LSI haa proven to be effective in terms of retrieval performance [9, 11]. Truncation, on the other 
hand, is not interesting as an optimization technique for speed in similarity search, since queries generally 
are shorter than 20 key wordsz aad access to the inverted index of documents is independent of the number 
of terms per document. While both similarity search and clustering require dis­tance measurements between 
text objects (either queries or documents) represented as vectors. the goal in each case is different. 
This is because the two problems cMer subatan­tialiy in how much they depend on the dwtance measure­ment. 
Similarity search is very sensitive to the distance or rather, similarity formula used.3 Even the slightest 
modification of the d~tance calculation can swap the rela­tive positions of two documents with respect 
to the query, affecting the quality of the search result. Clustering, on the other hand, is leas sensitive 
to the distance calculation. Only at the fringes oft he clusters, where clustering decisions are already 
somewhat arbitrary, is a slightly perturbed distance calculation likely to afkct the cluster in which 
a document is placed. Therefore, we might hope that projection tech­niques speed up clustering with a 
tolerable deterioration of cluster quality. Projecting documents is the simplest way to speed up the 
distance calculation, but it is not the only one. Cohen and Lewis have looked at using a modMied matrix 
multiplication routine for calculating approximate Euclidean distance [3]. Such an approach can be used 
in conjunction with document projection but may be more costly. Regardless, the fact that the algorithms 
community is expressing interest in this problem is an indication of its increasing importance. In Section 
2 we describe the projection techniques eval­uated in this paper. Section 3 describes the experimental 
cent ext used to evaluate these methods. We present and evaluate the results in Section 4. Finally, we 
recommend a specific projection technique in Section 5. ProjectbmTechniques At its heart, the clustering 
of text documents consists of clustering m vectors in an n-dimensional space, where m is the number of 
documents and n is the number of terms. For a given vector d, the value dt is the number of times term 
t occurs in document d. Clearly, most document vectors are sparse, and under a sparse representation 
each vector has size in line with the document length. We define projecting to be the act of converting 
some non-zero values of dt to O, possibly first modifying the vector d in an arbitrary way. If we choose 
not to convert any value, then we obtain the trivial projection, called FULL for full profiles. The simplest 
non-trivial projection is truncation. Trun­ cation works as folIows: consider, for each vector d, the 
 2An exception is the very long queries that may be generated via relevance feedback and pseudo-feedback. 
In this caae, truncation is not just an .fficiency tool but, as discussed in [2], is also probably necessary 
to maintain the quality of the search result set. We use the concepts of similarity and distance interchangeably 
here since Euclidean distance and correlation coefllcient produce the same ranking for normalized vectom: 
c largest components of the vector. (Note that the com­ponents chosen are different for each vector. 
) Keep these fixed, and set all other components to O. As is common. we weight the vector d before truncation. 
(One reason weight­ing is common is that it has been shown to give improved results for similarity search 
[23]. ) The weighting technique we use is term fmquencv weighting, in which we replace dt by 1+log dt. 
We call this technique TF weighting to c term, or TF-c. It is common to weight based on Inverse Document 
Fre­quency (IDF) in addition to term frequency, but we chose not to do so after preliminary studies indicated 
IDF weight­ing slightly degraded cluster quality. IDF weighting down­ weights frequent words and upweights 
rare words, which may be useful for similarity search but complicates clus­tering since clusters tend 
to be formed based on patterns of frequent words. We test two instantiations of TF-c, TF-50 and TF-20. 
TF-50 is similar to the truncation method recommended in [7 J. (Cutting et al. also truncate to 50 terms, 
but they use ~ for term frequency weighting instead of I + log dt. ) We test TF-20 to explore how a substantial 
reduction in the truncation constant affects time efficiency and clustering effectiveness. In addition 
to truncation, we consider LSZ, a global pro­jection scheme. In LSI, we convert the documents to LSI 
space and take the d lowest dmensions of each document. Thw dimension reduction method is called LSI 
dimension reduction to d dimensions, or LSI-d.4 We call the d in LSI­d, and the c in TF-c, the truncation 
constant. Nevertheless, we reserve the term truncation for the term truncation methods TF-c to avoid 
confusing these methods with the LSI methods. We test three instantiations of LSI-d, LSI-150, LSI-50, 
and LSI-20. The constant 150 is typical for the range of truncation constants in which LSI is competitive 
with or superior to term-based sirnilarit y search [9, 11, 12]. W e test the two lower truncation constants. 
LSI-20 and LSI­50, to explore how a substantial reduction in the constant afkcts time etliciency and 
clustering effectiveness. 3 ExperimentalDedgn We would like to compare projection techniques both ac­ 
cording to time etliciency and according to clustering efkc­ tiveness. It is easy to measure time efficiency 
by recording the CPU time for the various techniques when embedded in a fixed clustering algorithm. Measuring 
clustering efktiveness is harder, One mea­sure, used in probabfistic clustering methods such as EM clustering 
[10], is the probability that the vectors are gener­ated by a particular cluster model. Another measure, 
used in groupaverage agglomerative clustering, is the average dis­tance between members of a cluster. 
Instead of using these rather abstract measures, we base our evaluation on ckter retrieval, a measure 
that is closely aligned with information retrieval performance. Cluster retrieval is a retrieval strat­egy 
based on the cluster h~thesis [18, 29, 16] which states that closely associated documents tend to be 
relevant to the same requests [28]. In cluster retrieval, instead of rank­ing all documents of the collection 
according to similarity to the query, only documents in one selected cluster are ranked 4Computing the 
transformation matrices used in LSI is memory intensive, so we subsampled by ignoring terms occurring 
in fewer than = 1-2c0rr(d, ;) + 1 = 2(1 -corr(a,~))  5 documents. and presented to the user. (There 
are several ways of select­ing this cluster; see below. ) Cluster retrieval enhances the pretilon of 
ad hoc searches to the extent that relevant doc­uments are concentrated in one region of document space 
and that region is well represented by the selected cluster. In order to evaluate dtierent clustering 
procedures via cluster retrieval, we need a setof documents; a set of queries; and, for each query, an 
exhaustive list of the documents relevant to the query. To evaluate a clustering, we pick one cluster, 
using a method to be described below. We then turn the cluster into a ranked list via similarity search 
on that cluster.5 The quality of the clustering is then equated with the quality of the ranked list of 
the documents in the selected cluster, as measured by average precision. We evaluate the performance 
of each projection method by embedding it in a fixed distance metric Euclidean dktance and using this 
metric in a fixed clustering algo­rithm Buckshot [8].6 Buckshot first chooses a random sample of size 
@, where n is the number of vectors to be clustered, and then clusters this sample with an 0(n2 ) algorithm. 
Hence, overall time complexity of this step is 0(n). The algorithm used for clustering the sample is 
groupaverage agglomerative clus­tering (GAAC). GAAC initially forms a cluster out of each vector and 
then in each step merges the two clusters which give rise to the lowest average distance between members. 
After GAAC has clustered the fi sample, a centroid is com­puted for each cluster. In the final step, 
all n vectors are assigned to the cluster whose centroid they are closest to. The centroid computation 
and reassignment steps have time complexity O(n), so overall complexity is O(n). Our test corpus is the 
Wall Street Journal subpart of the TREC-4 collection [14]. The queries we use are queries 202 through 
250 of that collection; these are the 49 queries used for ad-hoc evaluation in TREC-4. The corpus consists 
of 74520 documents. In order to make sure that our findings are not baaed on a particular setting of 
an experimental parameter, we vary several experimental conditions systematically. First, we look at 
both global and 10CS.Iclustering to control for the size of the set. that is to be clustered. Second, 
we consider two types of information requests long and short queries. The queries (or topics) in TREC 
are long, but in many ap placations of information retrieval users supply much shorter queries. Finally, 
we also vary the way the best cluster is selected, using both a procedure similar to the one suggested 
in [16] and two automatic methods. The factors are sum­marized in Table 1 and described in more detail 
below. The scope factor describes the type of clustering. GLOBAL clustering clusters the 74520 documents 
in the corpus into about 400 clusters. In the first phase of Buckshot, 400 clus­ter centers are computed. 
Then all 74520 documents are assigned to the closest cluster center. Documents from clus­ters with fewer 
than 5 documents are aasigned to the closest surviving cluster. Thus, somewhat less than 400 clusters 
are act ually created. Thw procedure prevents unusual, h@ly dissimilar documents from occupying their 
own cluster. Lo-CAL clustering, on the other hand, first chooses the 1000 documents that are closest 
to the query according to Inc.ltc We use Inc.ltc weighting, a form of term frequency and inveme document 
frequency weighting, for similarity search (26]. We did not experiment with other ranking algorithms, 
but we do not believe that the choice of ranking algorithm would interact with any of our exper­imental 
conditions, such as the size of queries, the cluster selection method, or the scope of clustering. *Euclidean 
distance is equivalent to the more common cosine met­ric for length-normalized vectom as pointed out 
above, weighted search. T hese documents are then clustered into 5 clusters. As in GLOBAL, the parameters 
for LOCAL are only approximate; in particular, if there are fewer than 1000 documents with non-zero similarity 
to the query, fewer than 1000 documents will be clustered. Note that the diference between the global 
and local scope (clustering all documents vs. a selected set) should not be confused with the contrast. 
between global and local truncation (truncating the same dimensions throughout vs. truncating different 
dimensions in each case). The queries factor dwtinguishes between long and short queries. LONG queries 
are taken directly from the descrip tion field of the TREC topics. The SHORTqueries are mod­ified versions 
of the LONG queries, where each query is re­duced to a few keywords. For example, the long form of query 
216 is as follows: What research is ongoing to reduce the effects of oateoporosia in existing patients 
as well as pre­vent the disease occurring in those unafllicted at this time? while the short form is 
the single word osteoporosis.fi The average length of long queries is 10.8 terms, while short queries 
average 2.3 words. The cluster se~ection factor describes, as promised, the method for picking the cluster 
used by cluster retrieval. There are three levels for this factor. In the first, CLOS-EST cluster selection, 
we select the cluster that is closest to the query, or more exactly, the cluster whose centroid is closest 
to the query. ThM type of strategy has been used by [22, 4, 29, 13] among others. In FEEDBACKselection, 
we modify the query with paeud~feedback before selecting the cluster closest to it. We use the method 
suggested in [2]: We expand the query with the 20 documents that are ranked at the top in an initial 
retrieval with the unmodifwd query, and then we delete all but the 50 highest weighted terms of the expanded 
query. Finally, in DENSITY selection, we select the cluster wit h the highest proportion of relevant 
documents (cf. [16]). Note that DENSITY selection presumes that some amount of relevance information 
is available to the selection procedure whereas CLOSESTand FEEDBACKdo not. The final factor, projection 
technique, haa already been discussed in Section 2. Unlike for the other factors, which serve aa controls, 
for this factor we wish to discover which levels of the factor significantly difer in terms of efficiency 
and performance. In order to teat for significant differences we use Tukey s W procedure. (For a detailed 
description of this test, see e.g. [20].) This test takes into consideration that when a large number 
of difference tests are performed independently, there is high probabilityy of error. For exam­ple, if 
the probability of error of an individual test is 5% and 100 tests are performed, then on average five 
teats will have erroneous results. Tukey s W procedure guarantees that, for a 95% significance level, 
the probabfity of error for any of the differences between levels is only 5% regardless of the number 
of levels under consideration. Tukey s W procedure is similar to the Schefh$ test used in [27]. 4 Results 
and Diauuaion 4.1 Efficiency The experiments consisted of clustering the WSJ subcol­kction using each 
of the 6 projections for global clustering. Shnilarly, for local clustering, each of the 98 result sets 
(49 factor ] levels description projection I FULL unmoddied vectors .  L TF-20 term frequency weighting, 
20 terms/document TF-50 term frequency weighting, 50 terms/document LSI-20 LSI conversion, 20 dimensions 
LS1-50 LSI conversion, 50 dimensions LSI-150 LSI conversion, I so dimensions c ustermg LOCAL the 1000 
documents closest. to the auerv are clustered scope GLOBAL the entire corpus is clustered -­query LONG 
the original query; average length is 10.8 terms SHORT a shortened query; average length is 2.3 terms 
c uster CLOSEST pick cluster closest to the query selection FEEDBACK CLOSEST preceded by pseudo-feedback 
t-DENSITY pick cluster with highest proportion of relevant dots Table 1: The factors that we modify in 
studying the six projection techniques. There are 6x2x2x3 =72tests inall FULL I F-20 T F-SO Txl-20 LSI-50 
LSI-150 GLOBAL 78537 5132 8925 1543 4716 2241 LOCAL LONG 8120 1219 1932 350 464 897 LOCAL SHORT 6822 
1168 1861 324 443 859 Table 2: Clock Times in seconds for the six projection techniques. for short queries 
and 49 for long queries) were clustered us-measure is much more time consuming than computing a ing each 
of the 6 projections. The experiments were run reduced-profile measure. In full-profile clustering, centroids 
on a dedicated Sun Ultra-1, running Solaris 5.5, with 316 can include several tens of thousands of terms. 
In Buckshot Megabytes of main memory. Table 2 shows the CPU times clustering, most time is spent on computing 
distances, which for the 1S experiments. CPU times for local clustering are is roughly linear in the 
length of the profiles. Since the summed over all 49 queries. Average times for local chw-cent roid profiles 
are much longer without projection, full­tering range from 7 seconds per query for LSI-20 and short profile 
dist ante calculations are much slower. queries (324 seconds total) to almost 3 minutes per query The 
reason for the efficiency advantage of LSI-d is that for FULL and long queries (8120 seconds total). 
Processing even when we use TF-c, we do not project document profiles. times for long queries are generally 
longer because, as men-So only for LSI-d do document profiles have a fixed, small tioned above, the result 
sets of some of the short queries length. have fewer than 1000 members. However, the CPU times presented 
do not include the It is obvious from Table 2 that LSI and truncation are compile-time operation of Latent 
Semantic Indexing. It much more efficient than FULL. For global clustering the im-took roughly 20,000 
seconds (about 5.5 hours) to compute provements range from 8.8 times faster for FULL vs.TF-50 the LSI 
for this experiment on the Sun Ultra-1. If that to S1 times faster for FULL vs.LSI-20. For local cluster-time 
were included, then truncation would be the winner in ing the improvements range from 3.7 times fast 
er for FULL terms of time efficiency. [t was not included because the vs. TF-50 (SHORT)to 23 times faster 
for FULL vs. LSI-20 LSI analysis does not have to be repeated when clustering a (LONG). new local set 
of documents or choosing a different number To confirm our impression that LSI and truncation are of 
clusters or (smaller) number of dimensions. Nevertheless, futer than FULL, we performed an analysis of 
variance (ANO% A) the time for the analysis has to be taken into account in on Table 2. Since ANOVA is 
an additive model we converted judging the overall cost of LSI-baaed clustering. all clock times to logarithms 
under the assumption that the In summary, clustering after projection is an order of effects of the factors 
are multiplicative rather than additive. magnitude faster than full-profile clustering for .globaf clus-We 
performed Tukey s W test, comparing the average log-tering. Although the improvement for local clustering 
is not arithms of the clock times of the projections. The results quite as large as for global clustering, 
even moderate im­of the test are summarized in Table 3. This analysis yields provements in response time 
have a dramatic impact in an a critical value for a given alpha level, chosen to be 0.05 interactive 
setting, the most likely application setting for here. If the difference between two averages is more 
than local clustering. the critical value, then they are different at a significance level of 0.9s. The 
critical value for this analysis is 0.550. 4,2 Retrieval Performance Hence, there is no significant difference 
between TF-50 and TF-20 (group b), between TF-20 and LSI-] 50 (group c), As discussed above, we measure 
the quality of a cluster­ and between LSI-50 and LSI-20 (group d). For all other ing by ranking the documents 
in the selected cluster and pairs of projections. there is a significant difference in time evaluating 
this ranking as the response to an information efficiency. retrieval query. Table 4 gives performance 
results for the 72 It is not surprising that computing a full-profile distance possible combinations 
of projection (major rows of Table 4), cluster selection method (major columns of Table 4), scope projection 
average I group 1 group 2 group 3 group 4 FULL 9.7 Ia TF-50 8.064 b TF-20 7.571 bc LSI-150 7.338 c LSI-50 
6.649 d LSI-20 6.327 d rngmficant difference for Q = 0.05: 0.550 Table 3: Efficiency: Groups of averages 
that are not significantly difTerent. FEEDBACK CLOSEST DENSITY LONG SHORT LONG SHORT LONG SHORT G 0.044 
42.3 0.029 44.3 0.017 44.4 0.040 44.1 0.040 44.1 0.102 26.6 FULL L 0.062 39.5 0.068 37.6 0.054 38.1 0.066 
37.9 0.136 16.7 0.107 21.5 TF-20 G L 0.020 0.063 47.5 37.2 0.014 0.061 48.0 39.5 0.011 0.019 47.4 43.3 
0.011 0.059 47.6 41.8 0.069 0.118 28.0 19.4 0.071 0.098 30.9 22.6 G 0.023 45.9 0.031 44.0 0.023 45.9 
0.022 46.4 0.096 25.2 0.089 28.7 TF-50 L 0.084 33.8 0.052 40.6 0.073 35.7 0.061 37.2 G 0.023 47.2 0.018 
48.7 0.015 45.4 0.018 49.0 LSI-20 L 0.077 35.8 0.064 38.3 0.097 27.3 I 0.068 34.3    3H13!EE G 0.020 
49.6 0.010 49.0 0.022 47.5 0.010 48. 7 0.108 22.0 I 0.107 24.9 LSI-50 L 0.061 40.3 0.057 39.4 0.095 27.0 
0.068 35.3 0.142 15.7 I 0.104 20.6 0.022 46.9 0.014 48.1 0.018 47.6 0.013 47.6 0.101 22.5 I 0.097 26.6 
LSI-150 ; 0.068 37.3 0.051 41.3 0.078 32.9 0.044 40.4 0.141 16.1 I 0.112 19.2  slm. search 0.1118 (LONG), 
0.100 (SHORT) Table 4: Performance of Clustering Methods. Each entry is average precision followed by 
average rank. G and L stand for GLOBAL and LOCAL, respectively. (minor rows of Table 4), and query type 
(minor columns of We again use A NOVA and Tukey s W procedure to an-Table 4). alyze these results in 
terms of dMerences between the six We give two performance measures for each of the 72 projections.8 
Two sets of tests were performed, one for av­methods: uninterpolated average precision and average rank 
erage precision and one for ranks as shown in Tables 5 and 6. of uninterpolat ed average precision. Uninterpolated 
average Both analyses agree that there is no s nificant differ­precision is computed by taking the precision 
at each rele-ence between LSI-20, LSI-50, LSI-150, T ? -50, and FULL. vant document in the ranked list 
(number of relevant docu-In both ardyses, TF-20 is significantly worse than some ments up to this point 
divided by total number of documents projections (worse than LSI-20, LSI-50, TF-50, and FULL up to this 
point) and averaging these measurements over all when tested on average prtilon, and worse than LSI-20 
relevant documents. For each of the 72 experiments, the when tested on on ranks). average prti~on number 
given in Table 4 is the average of This result is quite surprising. A great deal of informa­uninterpolated 
average precision over the 49 queries. tion is lost when cluster centroids are reduced from thou- The 
disadvantage of taking such au average over queries sands of terms to lees than a hundred terms as we 
do in is that one method may score higher than another because truncation. Yet truncating to 50 terms 
has no measurable of its exceptionally high performance for a small number effect on cluster quality. 
Only with extreme truncation, to of queries, despite poor performance in general. For this 20 te~ms, 
is clustering performance affected. reason, we use a second rank-based score which compares The good 
performance of LSI is not as surprising, since methods on a query-by-query basis [17]. For this score, 
the LSI finds an optimrd dimensionality reduction in the ~nse average pr&#38;lon results of the 72 methods 
are ranked for described in Section 2. This optimality property suggests a particular query. The beat 
method receives rank O, the that LSI is Iess likely than trunc@on to djscard informa­second rank 1, and 
so on until the worst result receives rank tion crucial for clustering. However, it is surprising that 
71. After repeating this process for all queries, we have there is no significant dtierence between LSI-20, 
LSI-50 49 ranks for each method. The rank-based measure for a and LSI-150: In the similarity search Uena, 
LSI with 50 method is the average of these 49 ranks. Note that bet­aWe ahao ran an A NOVA on the 5-way 
table of dimenaiona 49 x ter performance corresponds to higher average prec&#38;on but 2 x 2 x 3 x 6 
with a separate average precision reeult for each query lower rank (since rank O is the best possible 
rank and rank (U oppoaad to averaging average precition over 49 queriq). The 71 the worst possible rank). 
ruwdysis gave the same result M the one reported below, namely, no performance difference except for 
TF-2fI. However, the distribution If a relevant document is not in the selected cluster the precision 
of mesaurements for this 5-way table wan clearly not nornvd, eo we for that document is aasumed to be 
zero. decided to report the results for the 4-way table only. m TF-20 0.0511 I a .sIgnificant difference 
for a = 0.05: 0.0138 1 Table 5: Retrieval Performance (average precision): Groups of averages that are 
not significantly different. projection average Ill roup 1 group 2 LSI -20 34.3 I a LSI-50 35.0 a b FULL 
35.2 a b TF-50 35.2 a b LSI-150 35.6 a b TF-20 37.8 I b significant difference for a = O.O5: 3.o3 I 
Table 6: Retrieval Performance (rank measure): Groups of averages that are not significantly different. 
dimensions has been shown to perform worse than LSI with 150 dimensions [9]. We conclude that for clustering, 
as op­posed to for similarity search, a relatively small number of dimensions is sufficient to achieve 
optimal results. In fact, LSI-20 performed better than LSI-50 and LSI-150 under both the goodness measures 
we used, though we attribute this small, insignificant difference to noise. A major motivation for using 
LSI in similarity search is that it addresses the vocabulary problem in term-based re­trieval. An example 
(adapted from [9]) is given in Figure 1. The query in the figure will only retrieve documents 1 and 3 
for term-baaed similarity search. However, LSI will rep­resent the terms interfacen and HCI by vectors 
that are close to each other because these two words have a similar distribution in the documents of 
the collection. As a result, the query will also retrieve document 2 when both query and documents are 
represented in LSI space. It is not so clear that, in clustering, there is a similar vocabulary problem 
that LSI could be beneficial in solving. In other words, is it likely that two documents with similar 
content but non-overlapping (or only slightly overlapping vocabularies will be assigned to two different 
clusters? The answer, at least for Buckshot, is no. This is made clear by examining how Buckshot computes 
clusters. After the ini­tial random sample has been clustered, cluster centroids are computed as sums 
of members of the clusters. As a result, terms that frequently c~occur in documents will also c­occur 
in centroids, since the centroid contains all the terms in the cluster s documents. For the example in 
Figure 1 thw means that if a cluster contains many documents with terms user and interface, then some 
of these documents wilf also contain HCI and interaction. Consequently, all four terms will be part of 
the cluster s centroid, and documents with both sets of terms will be assigned to the cluster. We conclude 
that clustering or at least clustering based on centroid computation and centroid-based reassignment 
 is not susceptible to the vocabulary problem. Clustering ex­ploits the co-occurrence structure of terms 
implicitly with­out the need of additional comput ation.9 The behavior of centroid-based clustering 
techniques ex­plains why LSI does not give rise to better clusters than FULL, but it does not explain 
why it performs equivalently to truncation. In the terminology introduced above, LSI is a global projection 
while truncation is local. The global pro­jection performs the same dimensionality reduction for each 
cluster cent roid. In cent rast, the local projection selects a dWerent set of terms to be excised for 
each centroid. As a result, in our experiment the cluster centroids in giobaf clustering for TF-50 had 
2248 dwtinct terms. On average, each of these 2248 terms is found in only 12.5 percent of the 360 centroids! 
Although the identical truncation constants in LSI-50 and TF-50 suggest that the dimensionafity of the 
reduced spaces is the same, in act ua.lity the dimensiomdit y of the truncation space, created by a locaf 
projection, is 45 times as high as the space created by the global projection. Even if the local projection 
of truncation is not optimal in the sense that LSI is, the effective dimensionality of the re­sulting 
space for a given truncation constant is substantially higher for truncation than for LSI. Another way 
to look at the same phenomenon is to re­member that reassignment, the key operation for determin­ing 
cluster membership, is a local calculation. Moderate truncation preserves the locally important terms 
and thus does not much affect remsignment (and hence cluster qual­ity). Since (excepting TF-20) there 
is no difference in clus­tering effectiveness between the projections, the main selec­tion criterion 
for an implementation should be efficiency. As shown above, TF-50, LSI-20, LSI-50 and LSI-150 are the 
most efficient projections. However, the LSI projections re­quire an expensive compile-time calculation. 
Our recom­mendation is therefore to use moderate truncation, to about 50 terms per document. for clustering. 
Moderate trunca­ 9We have made the assumption here that semantic similarity can be deduced from the co-occurrence 
structure. If this is not the case. then neither LSI nor clustering will be able to correctly handle 
doc­uments with similar content but different vocabulary Figure 1: Example for the vocabulary problem. 
For the query their respective rows. tion combines optimal clustering effectiveness and impres­ sive 
time efficiency. There is one weakness in our argument in favor of TF-50. Although computing the LSI 
transformation for our exper­ iments took more than 5 hours on a powerful machhe, the time complexity 
of Singular Value Decomposition (the un­ derlying numerical calculation for LSI) is cubic in the num­ 
ber of singular values computed. While we have determined that radical truncation harms clustering effectiveness 
(TF­ 20 vs. TF-50), we don t know for which truncation con­ stmt cluster quslity will deteriorate for 
LSI clustering. If this constant is very small (e.g., LSI-10), then the compile­ time operation of LSI 
may take so little time that it would cease to factor as a major concern when selecting a cluster­ ing 
algorithm. In future work, we will perform additiorud experiments with LSI with even fewer dimensions 
in order to further clarify the behavior of LSI. 4.3 The Impect of Other Factors Although it is not the 
focus of the paper. a brief analysis of factors other than projection may be of interest.10 Scope, LOCAL 
clustering (mean average precision: 0.083) iS significantly better than GLOBAL clustering (mean aver%e 
precision: 0.046) (a = 0.001). This result is to be expected since LOCAL clustering is performed for 
a dtierent selected set for each query. In contrast, one set of clusters is used for d queries in GLOBAL 
clustering. since LOCAL isdao faster than GLOBAL, it may seem that there is no reason to use GLOBAL. 
However, the times we report for GLOBAL in­clude the time needed to crest e the global clustering, which 
like the LSI calculation need only be done once in a pre­processing step. For a given query, GLOBAL isactually 
much faster than LOCAL ifthe global clustering already exists. The best solution to this time/quality 
trade-off might be a hy­brid scheme where a pre-processed clustering is adaptively modified based on 
the result set. One possibility, described in [2]], is significant] y faster than a local clustering 
scheme but equal to it in quality. Query. LONG queries (mean average precision: 0.069) are significantly 
better than SHORT queries (mean average preci­sion. 0.060) (a = 0.001). This result is to be expected 
since longer queries provide a better specification of the user s information need than short queries. 
ChISteredection. @NHTY sdection (mean average preci­sion: 0.108) is better than CLOSEST (mean average 
preci­siom 0.042) and FEEDBACK (mean average precision: 0.043) (CV= 0.01). This result suggests that 
it can be quite hard 10W= only ~poti ~~ults for awr~e precision, but all si6nifimce results below were 
confirmed by the analysis on ranks: FEEDBACK, CLOSEST < DENSITY, OLOBAL < LOCAL; and SHORT < LONG, and 
the three documents, the terms they contain are listed in to find automatic cluster selection methods 
(like the one in [19]) that perform as well as partially man ual ones like DENSITY. FEEDBACK isslightly 
better than CLOSEST,but this dif­ ference is not significant for cr = 0.05. S Conclusion In this paper, 
we have shown that projecting documents via LSI and truncation offers a dramatic advantage over fuU­profile 
clustering in terms of time efficiency. The improved efficiency, surprisingly, is not accompanied by 
a loss of clus­ter quality. In fact, with the exception of rrdcal truncation (TF-20), there is no significant 
difference in the cluster qual­ity of any of the projections we studied. This means that, in contrsst 
to similarity search, clustering can proceed suc­cessfully even if vector representations have been reduced 
at a considerable loss of information. We explain this re­sult by the fact that clustering is a less 
fine-grained tssk than similarity search and therefore requires leas precision in determining the distances 
of objects with respect to each other. In future work, we plan to investigate the impact of one important 
factor that we have neglected here: cluster sise. The number of clusters was fixed at 400 for global 
and s in 10CSIclustering, resulting in an average cluster size of about 200. However, smaller cluster 
sizes have been found to be effective in [19] for local clustering, and the effect of cluster size on 
global clustering remains to be explored. In addi­tion, we are interested in globol term truncation, 
in which one global set of, say, 1000 terms is chosen to be retained after truncation. This may well 
shed light on how important the choice of weighting scheme is for the quality of global truncation. Our 
recommendation for implementations of clustering is to use the truncation projection with a moderate 
amou~t of truncation, around 50 terms. This projection ia the most ef­ficient of the ones investigated 
here when both compile-time and run-time computations are taken into account. Despite its speed, truncation 
creates clusters of the same quality as the other projections. We hope that lightweight yet effective 
clustering algorithms based on the truncation projection will make clustering even more widely applicable 
than it ia now. Acknowledgments. We thank Jan Pedersen, Marti Hearst, David Hull, Mehran Sahami and three 
anonymous reviewers for helpful comments. References [1] Michael W. Berry. Large-scale sparse singular 
value computations. The hatemationul Journal of SupercQm­puter Applications, 6(1):1349, 1992. [2] Chris 
Buckley. Amit SinghaL Mandar Mitra. and Ger­ ard Sa3ton. New retrieval approaches using SMART: TREC 4. 
pages 25-48.1996. In [15]. [3] Edith Cohen and David D. Lewis. Approximating ma­trix multiplication 
for pattern recognition tasks. In Pro­ceedings o-f the Eight Annual A CM-SIAM Symposium on Discrete Algorithms. 
pages 682-691, 1997. [4] lT. B. Croft. A model of cluser searching baaed on classification. Information 
Systems. 5:189-196, 1980. [5] C. J. Crouch. An approach to the automatic construc­tion of global thesauri. 
Information Processing EdMan­agement, 26(5):629-640, 1990. [6] Carolyn J. Crouch and Bokyung Yang. Experiments 
in automatic statistical thesaurus construction. In Pro­ceedings oj SIGIR, pages 77 88, 1992. [7] Douglas 
R. Cutting, David R. Karger, and Jan O. Ped­ersen. Constant interaction-time scatter/gather brows­ing 
of very large document collections. In Proceedings of SIGIR 99, June 1993. [8] Douglaa R. Cutting, Jan 
O. Pedersen, David Karger, and John W. Tukey. Scatter/gather: A cluster-based approach to browsing large 
document collections. In Proceedings of SIGIR 92, pages 318-329, New York, 1992. Association of Computing 
Machinery. [9] Scott Deerwester, Susan T. Durnais, George W. Fur­nas, Thomas K. Landauer. and Richard 
Harshman. Indexing by latent semantic anaJysis. Journal of the American Society for Information Science, 
41(6):391­407, 1990. [10] A.P. Dempster, NM. Laird, and D.B. Rubin. Maximum likelihood from incomplete 
data via the EM algorithm.  J. Royal Statistical Society Series B, 39:1-38, 1977. [11] S. T. Dumais. 
Improving the retrieval of information from external sources. Behavior Research Methods, in­struments, 
and Computers, 23:229 236, 1991. [12] Susan T. Dumais. Latent semantic indexing (lsi): Trec­3 report. 
pages 219-230, 1995. In [15]. [13] A. Grifliths, H. C. Luckhurst, and P. WiUett. Using inter-document 
similarity information in document re­trieval systems. Journal of the American Society for Information 
Science, 37:3-11.1986. [14] D. K. Harman, editor. The Fourth Ted REtrieua/ Con­ference (TREC-~). U.S. 
Department of Commerce, Washington DC, 1996. NIST Special Publication 500­ 236. [15] D.K. Harman, editor. 
The Second Test REtriewal Con­~erwnce (TREC-2). U.S. Department of Commerce, Washington DC, 1994. NIST 
Special Publication 500­ 215. [16] Marti A. Hearst and Jan O. Pedersen. Reexamining the cluster hypothesis. 
In Proceedings of SIGIR 96, pages 76-84, Zurich, 1996, [17] David A. Hull, Jan 0. Pedersen, and Hinrich 
Schiitze. Method combination for document filtering. In Pm­ceedings oj SIGIR 96, pages 279 287, Zurich, 
1996. [18] N. Jardine and C. J. van Rijsbergen. The use of hier­archic clustering in information retrieval. 
h~ormation  Stomge and Retrieual, 7:217-240, 1971. [19] AlIan Lu, Maen Ayoub, and Jianhua Dong. Ad hoc 
experiments using EUREKA. In Proceedings of TREC­5, Gaithersburg MD, 1992. NIST. [20] Lyman Ott. An introduction 
to statistical methods and data analysis. Wadsworth, Belmont CA, 1992. [21] Jan O. Pedersen and Craig 
Silverstein. Almost­constant-time clustering of arbitrary corpus subsets. In these proceedings, 1997. 
[22] G. Salton. Cluster search strategies and the optimiza­tion of retrieval effectiveness. In G. Salton, 
editor, The SMART Retrieval System, pages 223-242. Prentice-HaU, Englewood Cliffs NJ, 1971. [23] Gerard 
SaJton and Chris Buckley. Improving retrieval performance by relevance feedback. Journal oj the American 
Society for hformation Science, 41(4):288­297, 1990. [24] Hinrich Schiitze. Ambtguitg Resolution in Language 
Learning. CSLI Publications, Stanford CA, 1997. [25] Hinrich Schiitze and Jan O. Pedersen. A cooccurrence­baaed 
thesaurus and two applications to information retrieval. hjormation Processing t? Management, 1997. To 
appear. [26] Amit SinghaJ, Gerard !?mJton, and Chris Buckley. Length normaJization in degraded text coUections. 
In Fijth Annual Symposium on Document Analgsis and Information Retrieval, pages 149-162, Las Vegas NV, 
1996. [27] Jean Tague-Sutcliffe and James Blustein. A statisti­cal analysis of TREC-3 data. In D.K. Harman, 
editor, The Third Tezt REtrieval Conference (TREC-3), pages 385-398, Washington DC, 1995. U.S. Department 
of Commerce. [28] C. J. van IUjsbergen. Information RetrieuaL Butter­worths, London, 1979. Second Edition. 
[29] EUen M. Voorhees. The cluster hypothesis revisited. In Proceedings of SIGIR 85. pages 188-196, 1985. 
 
			