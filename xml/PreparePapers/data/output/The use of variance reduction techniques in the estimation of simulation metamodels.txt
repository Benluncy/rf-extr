
 THE USE OF VARIANCE THE ESTIMATION OF REDUCTION SIMULATION TECHNIQUES METAMODELS IN Joan M. Donohue 
College of Business Administration University of South Carolina Columbia, South Carolina 29208, U.S.A. 
 ABSTRACT Variance reduction techniques can be useful strategies for improving the estimates of simulation 
metamodel coefficients. Depending upon the goals of the experi­menter, the type of metamodel being estimated, 
and the characteristics of the system being simulated, an appropriate variance reduction technique can 
be ap­plied. This paper provides a review of recent research that investigates the application of variance 
reduction techniques in the simulation met amodeling context. One strategy, Schruben and Margolin s ( 
1978) assign­ment rule, which utilizes a combination of antithetic and common random number streams, 
is found to be a particularly useful variance reduction technique for the estimation of simulation met 
amodels. INTRODUCTION In this paper, we use the term simulation meta­model to describe a mathematical 
equation that re­lates the input and output variables of a computer simulation model. Since a computer 
simulation model is, in itself, only a model of the true system, the term metamodel is generally used 
to describe the mathe­matical model that approximates the relationship be­tween the variables of a simulation 
model. For example, in the research of Galbraith and Stan­dridge (1994), the true system of interest 
is an elec­tronics assembly plant where circuit boards are man­ufact ured. Their computer simulation 
model mimics the operation of the true system, over time, using in­put distributions that are estimated 
from the actual manufacturing facility. The stochastic model com­ponents (those requiring random number 
generators) include assembly times, routing times, time between equipment failures, etc. After validating 
and verify­ing the computer simulation model of the manufac­turing facility, a metamodel is then used 
to explain the relationship that exists bet ween the input vari­ables and the simulated output response. 
The out­put variable, denoted g, is typically a performance measure of the system (e.g.; mean production 
time for circuit boards), The controllable input variables (e.g.; type of solder and the method of component 
placement on circuit boards) are termed factors and denoted Zi (i = 1,.. ., k). An experimenter may be 
interested in maximizing or minimizing y, determin­ing the sensitivity of y to the input variables, or 
sim­ply studying the nature of the relationship between y and various Zi. The type of metamodels most 
commonly used in simulation studies are polynomial regression models, such as the first-order model k 
Y. = Po+ ~bu +6. (1) i=l and the second-order model kk (2) ~<j where u= l,..., n is the simulation run 
number, yu is the simulated response (often the mean of obser­vations collected during a simulation run), 
Xiu is the setting of the ith input factor on the uth simulation run, the ,6 s are model coefficients 
to be estimated us­ing regression anal ysis, and CU is unexplainable error in the regression model. Barton 
(1994) identifies some alternative meta­models for simulation studies, including frequency do­main approximations, 
kernel smoothing models, ra­dial basis functions, spatial correlation models, and spline models. The 
advantage of these alternative models is their ability to fit a wide variety of curvilin­ear relationships, 
often in a piecewise manner. The disadvantage of the alternative metamodels is that 194 VRT for Metamodels 
their use requires sophisticated statistical knowledge. Not many simulation practitioners are familiar 
with the alternative models and few statistical packages provide routines for the estimation and analysis 
of such models. In addition, using variance reduction techniques with the alternative models has not 
yet been investigated. Polynomial regression models, on the other hand, are familiar to many experimenters, 
most statistical packages support their estimation and analysis, and the application of variance reduc­tion 
techniques has been investigated in many re­search studies. In terms of the type of metamodel, the scope 
of this paper is limited to traditional polynomial regres­sion metamodels. It is assumed that the experimenter 
has developed a valid simulation model, identified the output variable of interest y, and selected a 
set of k controllable input variables xi. The experimenter s objective is to develop a polynomial regression 
meta­model that can be used for optimization, prediction, or sensitivity analysis. The question we address 
in this paper is: What type of variance reduction tech­niques would be appropriate for the experimenter 
s simulation metamodeling study? We present an overview of recent fundamental ad­vances on the application 
of variance reduction tech­niques in a simulation metamodeling context. The paper is organized as follows. 
Section 2 provides ref­erences to some examples of simulation metamodel­ing studies. Section 3 describes 
commonly used ex­perimental design plans and variance reduction tech­niques. Section 4 discusses recent 
research findings on the use of variance reduction techniques in simulation metamodeling studies. 2 APPLICATIONS 
OF METAMODELS There are many articles in the simulation literature that use simulation metamodels to 
study real-world systems. A few such studies are mentioned here. Gor­don, Ausink, and Berdine (1994) 
develop a simulation model of a spacecraft in orbit. The simulation model has 6 stochastic components 
(e.g.; changes in solar radiation and changes in transmission voltage) and 5 controllable input variables 
(e.g.; tracking error and thrust input) that effect the output variable of in­terest (cost of controlling 
the spacecraft s orbit). A second-order metamodel is developed in order to ef­ficiently learn about the 
relationships that exist be­tween cost and the controllable input variables. Kuei and Madu (1994) and 
Madu and Kuei (1992) develop simulation models of 2 machine maintenance queueing systems in order to 
determine the num­ ber of machines and repair persons needed for var­ious service levels (the output 
variable of interest). The simulation models have a number of stochastic components (e.g.; machine repair 
times and time be­tween failures) and a number of controllable input variables (e. g.; number of repair 
persons and num­ber of standby machines). Second-order metamodels of the simulations are developed in 
order to advise management on issues related to the service levels of the maintenance systems. Friedman 
and Pressman (1988) develop simulation models of 3 systems with known theoretical solutions in order 
to ascertain whether simulation metamodels can be trusted. The first system is an M/M/s queue­ing system. 
A metamodel relating time-in-system to 3 input variables (arrival rate, service rate, and num­ber of 
servers) is developed. The second system is time-shared computer system with a single central processing 
unit. A metamodel relating job response time to 3 input variables (arrival rate, service rate, and proportion 
of time used for overhead) is devel­oped. The third simulation is an order-level inven­tory control system. 
A metamodel relating annual inventory costs to 3 input variables (item demand, review period, and target 
inventory level) is devel­oped. For each of the simulated systems, a first-order metamodel using natural 
logarithms of the variables lnyv = ln,f30 + @l lnzlU + /3zlnzzU + P3in %3. + in Cu (3) or, equivalently, 
the multiplicative model Yu = Po (xlU)@ (z2u)~ (x3u)~3 Cu (4) provides results similar to the analytic 
solution. There are many other studies that develop simu­lation metamodels of real-world systems. Excellent 
sources of such applications can be found in the Pro­ceedings of the Winier Stmulatton Conference and 
in the Simulation journal. 3 BACKGROUND INFORMATION In this section we discuss experimental design plans 
and variance reduction techniques that are particu­ larly useful in simulation metamodeling studies. 
3.1 Experiment al Design Plans In order to estimate the @ coefficients of the poly­nomial regression 
model in (1) or (2), experimental data must be collected. Specifically, information on the response variable 
y at a variety of input conditions xiU is needed. Experimental design is a scientific ap­proach to deciding 
how to collect such information. Classical experimental design procedures involve find­ing efficient 
approaches to collecting and analyzing data for the development of a mathematical model of a system. 
In a simulation context, an experimental design must specify the values of the k controllable input factors 
(z~ ; i = 1, ..., k) on each simulation run such that the ~ coefficients can be efficiently estimated. 
For the first-order model in (1), the most commonly used experimental designs are the 2-level (full or 
frac­tional) factorial plans. These designs minimize the variances of the estimated /3 coefficients. 
Other first­order designs have been developed for specific experi­ mental goals (e. g.; screening designs) 
but the proper­ties of the factorial plans make them the most widely used first-order designs. For the 
second-order model in (2), a number of experimental designs have been developed for the ef­ficient estimation 
of the P coefficients. The 3-level factorial designs are often used because of their intu­itive appeal, 
but these designs have the drawback of requiring a large number of experiments. For exam­ple, if a simulation 
model has k=7 input factors, then a full 3-level factorial plan requires 37 = 2187 simula­tion runs for 
1 replication of the experimental design. A 1/3 fractional replication would require 729 runs. For complex 
simulation models, the computer time needed to collect the data for 3-level factorial designs may be 
prohibitively large. There are many other commonly used second-order designs that have desirable properties 
in certain situ­ations (e. g.; Box-Behnken, central composite, equira­dial, Notz, and small composite 
designs). Thus, un­like the case of fitting a first-order metamodel, an experimenter has a wide variety 
of choices for an ef­ficient second-order experimental design. However, despite the design choices available, 
the central com­posite design is the most popular of the second-order designs. The central composite 
designs require that each in­put variable xi be set at 5 different levels, but require far fewer runs 
than the 3-level factorial designs. A central composite design consists of 3 parts: a 2-level factorial 
design, an axial design, and one (or more) experimental run at the center of the design. The number of 
runs required is 2~ + 2k + 1, or 143 when k=7. Depending upon the levels of the variables cho­sen for 
the axial portion, the central composite design can be developed to have a variety of desirable prop­erties. 
Since one part of the design is a first-order 2-level factorial, experimenters often fit a first-order 
model before augmenting the design with an axial portion in order to fit a second-order model. This ca­pability 
of being performed sequentially is one of the main reasons for the popularity of the central com­posite 
design. Table 1 illustrates a central composite design for a metamodel with k = 3 input variables. The 
top por­tion of the design (runs 1 8) is a 2-level factorial plan with the xi levels specified as +1 
and 1 for each variable, (The experimenter must code the levels of each controllable input variable 
such that +1 cor­responds to the highest value and 1 corresponds to the lowest value within the region 
of experimenta­tion.) The bottom of the design (runs 10 15) is the axial portion with the 3 levels specified 
as +a, O, and a. The design also includes 1 experiment (run #9) at the center of the region, xl = X2 
= *3 = O. Table 1: k = 3 Central Composite Design Run # Input Variables u x3 1 +1 +1 +1 2 +1 +1 1 3 +1 
1 +1 4 +1 1 1 5 1 +1 +1 6 1 +1 1 7 1 1 +1 8 1 1 1 900 o 10 +Cro 0 11 a!o 0 12 0+CY 0 13 o a0 14 00+a 
15 00 a Classical experimental design procedures require that the experimenter choose the high and low 
lev­els of each factor. These levels would generally be selected such that the output variable y is ad­equately 
described by the second-order metamodel in (2). Other issues facing the experimenter are the number of 
experiments to perform at the center and the number of times to replicate each design point. In a simulation 
context, the experimenter has a number of additional experimental design consider­ations. Here we assume 
that the length of the sim­ulation runs, the length of any warm-up period, the VRT for Metamodels initial 
conditions, and other such tactical issues have already been addressed. (See Nelson 1992 for recom­mendations 
concerning these tactical issues. ) There is also the strategic issue of how to assign random number 
streams to the stochastic model components. The use of independent streams for each stochastic model 
component on each simulation run would re­sult in independent output responses (similar to most real-world 
studies). However, due to the high vari­ability often associated with simulation output, the use of a 
technique to reduce variability through the manipulation of random number streams can be de­sirable. 
The next section discusses the variance reduction techniques that are applicable to simulation meta­modeling 
studies. 3.2 Variance Reduction Techniques Variance reduction techniques were originally devel­oped 
for the estimation of integrals in mathematics and physics (Kleijnen 1977). In order to use these techniques 
in simulation, changes were needed due to the autocorrelation of simulated observations and the complex 
relationships between stochastic model components and simulated response output. Fishman (1974) appears 
to be the first researcher to investigate the use of variance reduction techniques in the design of simulation 
experiments. Unfortunately, the re­sults of that study were pessimistic. Since that time, new research 
studies have brought optimism into the area. In this paper, we briefly describe 4 variance reduction 
techniques (common random numbers, an­tithetic random numbers, the assignment rule, and control variates) 
that are potentially useful in simu­lation metamodeling studies. Consider the simulation of an M/M/l 
queueing sys­tem. Let the output variable y be the steady state time-in-system, and let the single input 
variable z be the server utilization. Suppose the experimental de­sign involves 2 replications of the 
following 2 design points: x=O.5 and x=O.7. Another aspect of the ex­perimental design is the assignment 
of random num­ber streams to the 2 stochastic model components: arrivals and services. Let the vector 
l?i (i = 1,. . .) denote the ith stream of uniform (O, 1) random num­bers used to generate stochastic 
inputs. If no vari­ance reduction technique is used, the experimental design might be performed using 
the stream assign­ments shown in Table 2. This assignment of a unique random number stream to each stochastic 
component on each simulation run is termed independent random numbers, resulting in independent output 
responses, yu; u=l, ...,4. Table 2: Independent Random Numbers Run Repli-Arrival Service # cation Stream 
Stream 1 1 OX5RI R5 2 1 0.7 R2 R6 32 0.5 R3R7 420.7 R4R8  The variance reduction technique of common 
ran­dom numbers uses the same stream more than once in order to induce positive correlations and reduce 
the variances of certain output statistics. The technique can be used within a simulation run (generating 
data for 2 or more stochastic model components with the same random number stream) and/or between simu­lation 
runs (generating data for different sets of input conditions using the same random number stream). For 
the M/M/ 1 queueing system considered here, Ta­bles 3 and 4 respectively illustrate experimental de­signs 
that use common random numbers within and between simulation runs. Table 3: Common Random Numbers Within 
Runs Run Repli-Arrival Service # cation Stream Stream 1 1 0X5RI RI 2 1 0.7 R2 R2 32 0.5 R3R3 42 0.7 
R4R4  Table 4: Common Random Numbers Between Runs Run Repli-Arrival Service # cation Stream Stream 11 
0:5 RIR3 2 1 0.7 RI R3 32 0.5 R2R4 42 0.7 R~Rd  The variance reduction technique of antithetic ran­dom 
numbers uses antithetic pairs of random num­bers in order to induce negative correlations that lead to 
reduced variability of certain output statistics. An­tithetic streams, defined as ~ = 1 F&#38;, are also 
uni­formly distributed on the (O, 1) interval. Unlike com­mon random numbers, replications of design 
points Donohue can be made with antithetic streams (see Table 5). Table 5: Antithetic Random Numbers 
Run Repli-Arrival Service # cation x Stream Stream 11 0.5 RIR3 21 0.7 R2R4 32 0.5 RIR3 42 0.7 R2R4 
  There are many possibilities for combining the strategies of independent, common, and antithetic random 
number streams. The third variance reduc­tion technique discussed in this paper, Schruben and Margolin 
s (1978) assignment rule, is just one way of combining these strategies. The assignment rule uses common 
and antithetic random number streams in pairs of orthogonal blocks in order to induce both positive and 
negative correlations that result in re­duced variability of certain metamodel coefficients. (See Box 
and Draper 1987 for the design requirements of orthogonal blocking.) If we incorrectly assume that the 
2 design points, x=O.5 and x=O.7, of the M/M/l queueing example represent orthogonal blocks, then Table 
6 illustrates the assignment rule strategy. Table 6: The Assignment Rule* Run Repli-Arrival Service # 
cation z Stream Stream 1 1 0.5 RI R3 210.7El Es 32 0.5 RzR4 42 0.7 R2%  * A design with k=l factor 
does @ form orthogonal blocks so this design only illustrates the assignment rule, in concept. Another 
variance reduction technique that is used in simulation metamodeling is control variates. Un­like the 
3 techniques described previously, the use of control variates does not affect the random number stream 
assignments. The control variates technique only changes the statistical estimators of the meta­model 
coefficients. The basic idea of control variates is to identify one or more random variables whose expectations 
are known and correlated with the sim­ulated output variable of interest. The new estimator is computed 
as the old estimator plus a linear combi­nation of the control variables. See Bauer and Wilson (1992) 
and Lavenberg, Moeller, and Welch (1982) for more information on control variates. 4 RECENT RESEARCH 
In this section, we summarize the findings of current metamodeling research that investigates the variance 
reduction techniques described above. Each of these 19 studies makes a significant contribution to the 
sim­ulation metamodeling literature. Schruben and Margolin (1978) develop and present the assignment 
rule, thereby sparking research inter­est in this area. The authors appropriately comment that the assignment 
rule s true value for simulation has yet to be fully realized. Hussey, Myers, and Houck (1987a, 1987b) 
investi­gate the assignment rule in comparison to indepen­dent streams and common streams using 4 variance­related 
design criteria (generalized variance, inte­grated variance, prediction variance, and variance of slopes). 
For both first-and second order metamod­els, the assignment rule is found to be the preferred variance 
reduction technique in most, but not all, ex­perimental settings. Nozari, Arnold, and Pegden (1987) develop 
statisti­cal inference procedures for analyzing metamodeling data obtained with the assignment rule strategy. 
Ap­propriate confidence intervals and hypothesis tests on the ,B coefficients of linear models are derived. 
Tew and Crenshaw (1990) examine the effect that the absence of a pure error component has on the statistical 
analysis procedures associated with the es­timation of metamodel coefficients. They show that in order 
to legitimize a proper statistical analysis, at least one random number stream must be randomly selected 
across all design points. The authors also point out that inducing too much correlation within a simulation 
design results in a poor estimate of the experiment al error variance. Using first-order designs, Tew 
(1991) investigates the use of independent versus correlated replications of the assignment rule strategy. 
Correlated replica­tions are achieved by using various combinations of common and antithetic stream sets. 
Tew illustrates that the variances of the metamodel coefficients can be reduced by using correlated replications 
but, un­fortunately, the bias of the coefficients was not con­sidered in this study. Tew and Wilson (1992) 
develop statistical proce­dures for checking the assumptions associated with the assignment rule, which 
include multivariate nor­mality and the assumed correlation structure. Addi­ VRT for Metamodels tionally, 
statistical tests for lack-of-fit to the assumed metamodel are presented. Joshi and Tew (1995) ex­tend 
these statistical procedures to the common ran­dom number streams strategy. Kleijnen (1992) compares 
ordinary and estimated generalized least squares for computing the meta­model s ~ coefficients when common 
random number streams are used. Interval estimates computed using the ordinary least squares estimators 
have good cov­erage probabilities. Also, it is found that common random number streams reduce the confidence 
inter­val widths of all /? coefficients except the intercept, Po ~ Donohue, Houck, and Myers (1992, 1995) 
compare the assignment rule with common and independent streams using 2 mean squared error criteria (MSE 
of predicted responses and MSE of slope coefficients). For second-order metamodels, the assignment rule 
performs well in terms of both design criteria; com­mon streams perform well only in terms of the MSE 
of slopes criterion. Using central composite designs, Tew (1992) in­vestigates the use common random 
numbers across design points and antithetic random numbers across replications in comparison to independent 
random number streams. The variances of the second-order metamodel coefficients are reduced by using 
Tew s common/antithetic combination strategy. Bias of the metamodel coefficients was not considered here. 
Schruben et al. (1992) consider the use of antithetic random number streams in the context of Taguchi 
s parameter design framework. A simple example illus­trates that this variance reduction technique may 
be beneficial for robust designs in a simulation setting. However, further research using variance reduction 
techniques combined with Taguchi analysis strategies is called for. Donohue, Myers, and Houck (1993a, 
1993b) in­vestigate the use of independent streams, common streams, and the assignment rule for fitting 
a first­order metamodel and for sequentially fitting a first­and second-order metamodel using a central 
compos­ite design. In terms of 2 different mean squared error criteria, the assignment rule was found, 
in general, to perform the best of the 3 variance reduction tech­niques. Extending the earlier work of 
Crenshaw and Tew, Zeimer and Tew (1994) address the problem of select­ ing an appropriate method for 
generating experimen­ tal error when correlated replications of design points are used. The authors find 
that the selection of such a generator is closely linked to its ability to maintain a prescribed correlation 
structure. Benefits can be achieved from the use of correlated replications if the desired correlation 
structures are achieved. Control variates, in combination with common and antithetic random number streams, 
are investigated by Tew and Wilson (1994). For first-order met amod­ els, the combined technique is shown 
to be superior over any of the techniques used individually. Kwon and Tew (1994) extend this research 
by comparing, 3 different methods of combining control variates with common and antithetic streams. The 
use of both con­trol variates and antithetic streams is shown to per­form the best in terms of prediction 
variance. Lastly, the most recently published research in­volves the application of yet another variance 
reduc­tion technique in a simulation metamodeling study. Hesterberg (1995) uses the importance sampling 
tech­nique in a case study of oil inventory reliability at a large electric power plant and finds the 
technique to be very efficient. Further research on this variance reduction technique appears warranted. 
REFERENCES Barton, R. B. 1994. Metamodeling: A state of the art review. In Proceedings of the 1994 Wznter 
Simu­lation Conference, ed. J. D. Tew, S. Manivannan, D. A. Sadowski, and A. F. Seila, 237 244. Lake 
Buena Vista, FL. Bauer, K. W., and J. R. Wilson. 1992. Control variate selection criteria. Naval Research 
Logistics 39:307-321. Box, G. E. P., and N. R. Draper. 1987. Emptrical model-budding and response surfaces. 
New York: John Wiley and Sons. Donohue, J. M., E. C. Houck, and R. H. Myers. 1992. Simulation designs 
for quadratic response surface models in the presence of model misspecification. Management Sczence 38:1765-1791. 
Donohue, J. M., E. C. Houck, and R. H. Myers. 1993a. Simulation designs and correlation induc­tion for 
reducing second-order bias in first-order re­sponse surfaces. Operations Research 41:880 902. Donohue, 
J. M., E. C. Houck, and R. H. Myers. 1993b. Sequential experimental designs for the estimation of first-and 
second-order simulation metamodels. ACM Transactions on Modeling and Computer Szmulatton 3:190-224. Donohue, 
J. M., E. C. Houck, and R. H. Myers. 1995. Simulation designs for the estimation of response surface 
gradients in the presence of model misspec­ification. Management Science 41:244 262. Fishman, G. S. 1974. 
Correlated simulation experi­ments. Simulation 23:177-180. Friedman, L. W., and I. Pressman. 1988. The 
meta­model in simulation analysis: Can it be trusted? European Journal of Operational Research 39:939­ 
948. Galbraith, L., and C. R. Standridge. 1994. Analysis inmanufacturing systems simulation: A case study. 
Simulation 63:369-376. Gordon, S. C., J. A. Ausink, and R. J. Berdine. 1994. Using experimental design 
techniques for space­craft control simulation. Simulation 62:303 309. Hesterberg, T. 1995. Weighted average 
impor­tance sampling and defensive mixture distribu­tions. Technometrics 37:185 194. Hussey, J. R., R. 
H. Myers, and E. C. Houck. 1987a. Pseudorandom number assignments in quadratic response surface designs. 
IIE Transactions 19:395 403. Hussey, J. R., R. H. Myers, and E. C. Houck. 1987b. Correlated simulation 
experiments in first­ order response surface design. Operations Research 35:744-758. Joshi, S., and 
J. D. Tew. 1995. Validation and statistical analysis procedures Under the common random number correlation-induction 
strategy for multipopulation simulation experiments. To ap­pear in European Journal of Operational Research. 
Kleijnen, J. P. C. 1977. Design and analysis of simula­tions: Practical statistical techniques. Stmulaiion 
29:81-90. Kleijnen, J. P. C. 1992. Regression metamodels for simulation with common random numbers: 
Com­parison of validation tests and confidence intervals. Management Science 38:1164-1185. Kuei, C. H., 
and C. N. Madu. 1994. Polynomial metamodelling and Taguchi designs in simulation with application to 
the maintenance float policy. European Journal of Operational Research 72:364­ 375. Kwon, C., and J. 
D. Tew. 1994. Strategies for com­bining antithetic variates and control variates in de­signed simulation 
experiments. Management ScZ­ence 40:1021 1034. Lavenberg, S. S., T. L. Moeller, and P. D. Welch. 1982. 
Statistical results on control variables with application to queueing network simulation. Oper­ations 
Research 30:182 202. Madu, C. N., and C. Kuei. 1992. Simulation meta­models of system availability and 
optimum spare and repair units. IIE Transactions 24:99 104. Nelson, B. L. 1992. Designing efficient 
simulation ex­periments. In Proceedings of the 1992 Winter Sim­ulation Conference, ed. J. J. Swain, D. 
Goldsman, R. C. Crain, and J. R. Wilson, 126 132. Arlington, VA. Nozari, A., S. F. Arnold, and C. D. 
Pegden. 1987. Statistical analysis for use with the Schruben and Margolin correlation induction strategy. 
Opera­ tions Research 35:127 139. Schruben, L. W., and B. H. Margolin. 1978, Pseu­dorandom number assignment 
in statistically de­signed simulation and distribution sampling exper­iments. Journal of the American 
Statistical Asso­ciation 73:504 520. Schruben, L. W., S. M. Sanchez, P. J. Sanchez, and V. A. Czitrom. 
1992. Variance reallocation in Taguchi s robust design framework. In Proceed­ings of the 1992 Winter 
Simulation Conference, ed. J. J. Swain, D. Goldsman, R. C. Crain, and J. R. Wilson, 548-556. Arlington, 
VA. Tew, J. D. 1991. Correlated replicates for first-order metamodel estimation in simulation experiments. 
Transactions of The Soczety for Computer Simula­tion 8:218 244. Tew, J. D. 1992. Using central composite 
designs in simulation experiments. In Proceedings of the 1992 Winter Simulation Conference, ed. J. J. 
Swain, D. Goldsman, R. C. Crain, and J. R. Wilson, 529 538. Arlington, VA. Tew, J. D., and M. D. Crenshaw. 
1990. Heuristic di­agnostics for the presence of pure error in computer simulation models. In Proceedings 
of the 1990 Wzn­ter Sz mulatzon Conference, ed. O. Balci, R. P. Sad­owski, and R. E. Nance, 337 343. 
New Orleans, LA. Tew, J. D., and J. R. Wilson. 1992. Validation of simulation analysis methods for the 
Schruben-Margolin correlation-induction strategy. Opera­tions Research 40:87-103. Tew, J. D., and J. 
R. Wilson. 1994. Estimating sim­ulation metamodels using combined correlation­based variance reduction 
techniques. IIE Trans­actions 26: 2 16. Zeimer, M. A., and J. D. Tew. 1994. Selection of a pure error 
generator for simulation experiments. Transactions of The Society for Computer Simula­tion 11:132 158. 
AUTHOR BIOGRAPHY JOAN M. DONOHUE is Associate Professor of Management Science at the University of South 
Car­ olina. Her research interests include the design of simulation experiments, response surface metamod­ 
eling, and variance reduction techniques. 
			