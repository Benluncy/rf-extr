
 COMPOSITION FOR MULTIVARIATE RANDOM VARIABLES Raymond R. Hill AFSAA/SAGW 1570 A. I . Pentagon Washington, 
D. C., 20330, U.S.A. ABSTRACT We show how to find mixing probabilities, or weights, for composite probability 
mass functions (pmfs) for k-variate discrete random variables with specified marginal pmfs and a specified, 
feasible population correlation structure. We characterize a joint pmf that is a composition, or mixture, 
of 2k -1 extreme­correlation joint pmfs and the joint pmf under inde­pendence. Our composition method 
is also valid for multivariate continuous random variables. We con­ sider the cases where all of the 
marginal distributions are discrete uniform, negative exponential, or contin­ uous uniform. INTRODUCTION 
We consider the problem of generating samples of a k-variate discrete random variable via composition 
when the marginal probability mass functions (pmfs) and a feasible population correlation structure are 
specified. We are interested in this problem mainly because we want to be able to generate coefficients 
for synthetic optimization problems in which the depen­dence between each pair of coefficient types is 
con­trolled. Many computational evaluations of solution pro­cedures are conducted exclusively on synthetic 
opti­mization problems whose coefficients are generated independently. Results from other computational 
studies indicate that the statistical properties of the coefficients in synthetic optirnization problems, 
e.g., the marginal distribution families and the popula­tion correlation structure, can affect the performance 
of solution methods (Loulou and Michaelides, 1979; Martello and Toth 1979, 1988; Balas and Martin, 1980; 
Balaa and Zemel, 1980; Potts and Van Wassen­hove, 1988; John, 1989; Moore, 1989; Reilly, 1991; Pollock, 
1992; Rushmeier and Nemhauser, 1993; Moore and Reilly, 1993). Charles H. Reilly Department of Industrial, 
Welding and Systems Engineering The Ohio State University 1971 Neil Avenue Columbus, Ohio, 43210, U.S.A. 
Our primary goal is to show how synthetic opti­mization problems with a prescribed population cor­relation 
structure among the coefficient types can be generated via composition. But, our work is not rel­evant 
to only one application. Our composition ap­proach can be used for continuous random variables also, 
making this approach useful for many simulation applications. For example, the generation of values of 
multivariate continuous random variables can be cru­cial to realistic simulation models of manufacturing 
systems. In this paper, we extend recent work on bivariate composite pmfs to the muitivariate case. We 
pay par­ticular attention to the case where all of the marginal pmfs are uniform because the discrete 
uniform distri­bution is very often used to represent the marginal distributions of coefficient values 
in synthetic opti­ mization problems. We point out that our composi­tion approach is valid for multivariate 
continuous ran­dom variables. We also consider the cases where all of the marginal distributions are 
negative exponential or continuous uniform because these cases may be im­portant for simulations of tandem 
queueing systems and manufacturing systems. 2 BACKGROUND in this section, we review the concepts of conventional 
mixtures, extreme mixtures, and parametric mixtures for bivariate discrete random variables (Yl, Y2). 
We also discuss methods for generating values of a mul­tivariate random variable Y = (Yl, Y2,..., Yk). 
In $2.1-2.3, we assume that Y,, i = 1,2, is a finite discrete random variable distributed over the support 
Si = {yil, yiz,. . . , Yin, } according to the pmf fi(yi). We denote the maximum and minimum possible 
val­ues of the Pearson product-moment correlation, p= Corr(Y1, Yz), asp+ and p-, respectively. Also, 
we de­note the minimum-and maximum-correlation pmfs for (Yl, Y2) as g1(y11y2) and gz(yl, yz). We let 
p be 332 a specified value of p. 2.3 Parametric Mixtures for (YI, Y2) Let 6 be the smallest joint probability 
associated with 2.1 Conventional Mixtures for (Yl, Y2) any (yl, y2) G S1 x S2, and assume that .fi(yi) 
> 0, Vyi c S i, i = 1,2. Peterson and Reilly (1993) show We can generate values of (YI, Yz) with population 
that a piecewise linear curve that plots the maximid correlation p such that p-< P < p+ by mixing value 
of O versus p can be constructed from the so­ values of (Yl, Y2) generated under independence and lution 
to a parametric linear program. They go on values of (Yl, Y2) generated with extreme correlation. to 
show that, if max{nl, n2} ~ 3, this curve outlines The prnf for (YI, Yz) upon which such a generation 
a parametric envelope that contains all points that method is based is () correspond to feasible combinations 
of p and 0. Let0 = .fl(yli*)~z(yzj*), where i = 1 $ fl(Yl)f2(Y2) + $ 91(Y1>Y2)> (1) arg mini {fl(yli)} 
and j = arg minj {j2(y2j)}. SuP­ () pose that (p , 0°) is a point in the parametric enve­lope such that 
0° < 0 and (1 OO/O )p-< p ~ ifp < 0, and (1 -eo/#)p+. Peterson and Reilly show that, if 91(Yl~*,Yzj*) 
= 92( Yli*, YV ) = 0, hen P = PO and 1 $-fl(yl)fz(yz) + $ 92( Y1, Y2)> (2) 0 = 0° for the pmf () () 
Aojl(yl)j2(Y2) + J191(Y1, Y2) + J292(Y1)Y2)I (4) if p > 0. We refer to pmfs (1) and (2) as conventional 
mixtures. where A. = 6 0/0 , Devroye ( 1986) defines a family of distributions for a bivariate random 
variable to be comprehen- AI = ((1 @ /e )p+ p J) /(p+ p-), sive if the family includes $1( Y1)~2(Y2), 
91(Y1, Y2), and g2(y1, yz). So conventional mixtures constitute and a comprehensive family for (Yl, Y2 
). Conventional mixtures are easy to use. There is a AZ= (po -(1 -eo/o )p-) /(p+ -p-). unique pmf (1) 
or (2) for every feasible value of p. As a result, when a conventional mixture is used to Even if gl 
(yli* , Y2j* ) #Oor gz(yli., y2j. )# O, p=pO represent the distribution of (YI, Yz) with p = O, Y1 for 
the pmf (4). We refer to pmfs (4) as parametric and Y2 are independent. Conventional mixtures are mixtures. 
useful for bivariate continuous ranclom variables too. Parametric mixtures include conventional mixtures 
(1) and (2) and extreme mixtures (3). Therefore, parametric mixtures form a comprehensive family for 
2.2 Extreme Mixtures for (Yl, Y2) (Yl, Yz). For all values of p except p+ and p-, there is Reilly (1994) 
describes how samples of (Yl, Y2) can an infinite number of parametric mixtures. Yet, there be generated 
by mixing values of (Yl, Y2) generated is only one parametric mixture for every point (p, 0) based on 
gl(yl, y2) and values of (Yl, Y2) generated such that 0<0 and (1 6/&#38;)p-< p < (1 fl/@)p+. based on 
g2(y1, y2). Let p-< p < p+. In this c~e, the composite pmf for (Yl, Y2) is  2.4 Multivariate Variate 
Generation One way to generate values of a multivariate random variable Y = (Y1, Y2, . . . . Yk) with 
a specified popula­ ($3 )91(Y1Y2 +(;:=5)92(Y]Y2) 3) tion correlation structure is to generate a value 
for Y1 We refer to pmfs (3), which were first suggested by based on its marginal pmf or probability density 
func-Fr6chet (1951), as extreme mixtures. tion (pdf), ~l(yl), then generate a value for Y2 b~~d Like 
conventional mixtures, extreme mixtures are on the conditional pmf or pdf ~ql (Y21Y1 ) *SOCiat ed wit,h 
p12 n Corr(Yl, Y2) z p~2, and so on. Althou/~h quite easy to use. There is a unique extreme mixture for 
every feasible value of p. However, it is impossi-certainly effective, this approach can be time consum­ble 
to use an extreme mixture to generate value~ of ing and tedious. Furthermore, a fully specified joint 
 (Yl, Yz) with Yi and Yz independent, so extreme mix-pmf or pdf is required to begin. t ures do not forma 
comprehensive family for (Yl, Y2). A method for generating values of rnultivariate nor-Extreme lnixtures 
are also useful for bivariate contin-mal random variables with a specified population cor­uous random 
variables. relation structure is well known. Johnson, Wang, and Ramberg (1984) and Devroye (1986) describe 
meth­ods for generating values from several multivariate distributions. Iman and Conover (1982) present 
a method for gen­erating values of a multivariate random variable with a specified Spearman rank correlation 
structure. 3 EXTREME-CORRELATION Ph4Fs Let Y = (Y1, Y2,..., Yk) be a multivariate dis­crete random variable. 
We assume that each Yi, i= 1,2 ,.. ., k, is distributed over the support Si = {?Jilj Yiz, . . . ,Vin, 
} according to the pmf $i(~i). According to Whitt (1976), the maximum and min­imum possible values of 
pij = Corr(Yi, Yj ) are p; = (Var(K)Var(\)); and K,; E(fi)E(~) p; = (Var(fi)Var(lj))* respectively, 
where q = J1 Fi-l(u)F :l o (u) du, I{,; = 1Fi-l(u)F ~l(l u) du, J o and Fi-l (u) and Fj (u) are 
the inverse cumulative distribution function (cdfs) for Y, and ~. Peterson and Reilly (1993) show that 
Kt~ and KZ~ can be de­termined by solving one factored transportation prob­lem with the Northwest Corner 
Rule and one with the Southwest Corner Rule. We define the extreme-correlation pmfs for Y to be the joint 
pmfs for Y for which p~j = p: or pij = p;, Vi < j. Each extreme-correlation pmf is associated with a 
possible assignment of extreme values for the plj, j = 2,3, . . . ,k. Hence, there are 2k-1 extreme-correlation 
pmfs for Y. Let gt(y), e=l,2 , ., , , 2k-1, be the extreme-correlation pmfs. Table 1 shows the ~lj values 
associated with the eight extreme-correlation pmfs when k = 4. MULTIVARIATE COMPOSITION Define for each 
extreme-correlation pmf, gt(y), t = 1,2 ,...,21,1, 61 = 1 ifptj =~~; t] O ifp,j =p~;{ and Reilly Table 
1: Possible plj combinations when k = 4. Table 2: Values of 6~j when k = 4. l,j ij e1,2 1,3 1,4 2,3 2,4 
3,4 1 000 111 2 001 100 3 010 010 4 011 001 5 100 001 6 101 010 7 110 100 8 111 111  to indicate whether 
ptj = p~ or pij = p,j. First, we assign values to6~j, j=2,3, -.. .,k, l= 1,-2, ...,21,1, that are consistent 
with the plj values found in a table like Table 1. Then, to find the remaining A:js, we use the following 
simple rule: ln other words, if 15fi = &#38;fj, then 6~j = 1; otherwise, b~j = O. Table 2 shows the i$fJ 
value~ when k = 4. Let go(y) = I_I!=l .fi(yt). The pmf Zk-l 9(Y) = ~ h91(Y)> (5) e=o where~~~~ ~e=land~e 
~0, e=O,l, ... ,2k 1, is a composite pmf for Y. In fact, pmfs (5) could be said to constitute a comprehensive 
family of multivariate distributions for Y. Let p~j be the desired value of p~j, Vi < j. A valid pmf 
(5) for Y with the desired population correlation structure exists if there is a solution to ~k-l (7) 
Al >0, l=o,l,...,2l-l. (8) In most cases, there is an infinite number of solutions to (6)-(8). Hence, 
we could use an appropriate crite­rion as an objective function, along with (6)-(8), to select a composite 
pmf (5) with desired characteris­tics. An interesting question is what criterion should be used to select 
a pmf for a given application. Let Fi-l(u) be the inverse cdf for Yi, i = 1,2,... ,k. The procedure KVAR 
allows us to generate values of Y easily based on the joint pmfs (5). Procedure KVAR 1. Generate ul, 
u2, . . . . u~+l -/7(0, 1). 2. Iful<JO, then fori=l,2,. ... k,  Yi -~i-l(Ui+I) and go tO SW ~. Otherwise, 
set / = 1, A = ~. + Al. 3. If U1 > A, go to Step 4. Otherwise, go to Step 5. 4. /el+l, A+ A+ Al, Goto 
Step3. 5. Generate y with UZ based on gl(y). (a) y, + F~l(u~). (b) Foryi, i=2,3,..., k, y,+ F~ (u,) 
if?if, = 1, or   Yi-~i-l(l W) if~fi=0. 6. Return y. 5 SPECIAL CASES In this section, we consider 
the special cases where Y is either a bivariate or a trivariate random variable. 5.1 Specinl Case 1: 
k = 2 Suppose we wish to find a pmf (5) with p = p such that AO is minimized, i.e., the frequency of 
inde­ pendent sampling is minimized. We can show that Ao=o, Al=(p+ PO)/(P+ P-)! and A2 = (P -P-)/(p+ 
-p-). In other words, the desired prnf is an extreme mixture (3) and a special case of a parametric mixture 
(4) where O = O. Suppose instead that we wish to find a pmf (5) for which p = p such that )0 is maximized. 
Then, we can show that A. = 1 pO/p+, Al = O,and AZ = pO/p+ if p ~ O and Jo = 1 pO/p-, J1 = pO/p -, and 
J2 = O if p < 0. Hence, the desired pmf is a conventional mixture (1) or (2). 5.2 Special Case 2: k 
= 3 Suppose that we seek a pmf (5) with a specified popu­lation correlation structure such that ~. = 
O, or there is no independent sampling. Let ~~j = (pi + p; )/2, Vi < j. Then the unique solution to (6)-(8) 
is 4 1 = 1,2,3,4. The factor 26fi 1 assures that ea,ch term in the numerator has ;he appropriate sign. 
It can be shown that (5) is a valid pmf for Y with tlhe desired population correlation structure because 
~~ 2 0,1= 1,2,3,4, ~~=1~1= l,andpij =p~j, yi <j. Suppose that Yi -U{l,2,.. .,n, },i= 1,2,3. That is, 
suppose that Yi is uniformly distributed over the integers from 1 to ni. In this case, pi = p,; and pij 
= O, Vi < j. Therefore, the mixing probabilities ~t, /= 1,2,3,4, become There is less setup required 
to use the pmfs (5) when all of the marginal distributions are discrete uniform. Example 1. Let Y = (Yl, 
Y2, Y3), where Yi N u{l,2,. ... n}, i= 1,2,3, and RI bethe desired pop­ ulation correlation matrix, where 
In this case, p; = 1,Vi < j. Suppose that Jo = O. Then, JI = 0.275, ~2 = 0,425, AS = 0.075, and AA = 
0.225. 0 The mixtures (5) for which Jo = O have a short­ coming. For some values y 6 S1 x S2 x SS, g(y) 
= O. Consequently, there are some values of Y that can never be generated. By including go(y) in our 
comp­ osite pmf with a nonzero mixing probability, we can generate all of the possible values of Y, i.e., 
all y~.$lxszx.%. Consider the following mixing probabilities: ,yo, where To > 0, and for f? = 1,2,3,4, 
Figure 1: Example pmf g(y) with yO = O Y1] o 1 2 3 I 1 0.1657 0 0.0233 0.0110 2 0.0132 0.1607 0.0261 
0 3 0 0.2000 0 0 4 0,0028 0.0713 0.1259 0 I 5 I 0.0343 0 0.1127 0.0530 where 4Ae 70~y*= mjn ~{} A valid 
pmf for Y is ~k-1 h(y) = ~ -ftgt(y). (9) e=o If -yO = y , then at least one of the other mixing probabilities, 
-yl, 1 = 1,2,3,4, will be zero. If Y~U{l,2,... ,nj}, i = 1,2,3, then dl = 1 for 1= 1,2,3,4, and -y* = 
4ming{Ag}. Example 2. Recall Example 1. In this case, 7* = 0.30. If TO = 7*, then 71 = 0.20, 72 = 0.35, 
73 = o, and 74 = 0.15. 0 Suppose that k = 2. Peterson and Reilly (1993) show that pmfs (9) give the user 
control over the smallest joint probability, 0, in parametric mixtures (4). If -YO = y , then the pmf 
(9) is a conventional mixture (1) or (2). The next example illustrates how the value of yO affects the 
nature of the pmf for (Y,, Y,). Example 3. Let Y1 N U{1,2,3,4,5} and Y2 be a binomial random variable 
with 3 independent trials and success probability 0.4. Suppose that the desired value of p = 0.6 and 
Jo = O. Then, Al = 0.1715 and AZ = 0.8285. The pmf shown in Figure 1 is the mixture (5), and the prnf 
shown in Figure 2 is the mixture (9) with 7rJ = 7* = 0.3431, 71 = O, 72 = 0.6569. We see in Figure 1 
that g(yl, yz) = O for 7 of 1 Y2 Y1o12 3 1 0.1462 0.0296 0.0198 0.0044 2 0.0254 0.1504 0.0198 0.0044 
3 0.0148 0.1610 0.0198 0.0044 4 0.0148 0.0614 0.1194 0.0044 5 0.0148 0.0296 0.1092 0.0464 Figure 2: Example 
pmf h(y) with yO = y .  6 EXTENSIONS In this section, we describe the limitations of our com­position 
approach for k ~ 4. We also show how to use composition for continuous multivariate random variables. 
6.1 General case: k ~ 4 It would be convenient if we could use mixing proba­bilities of the form / = 
1,2, . . . . 2k -1, to construct composite pmfs for general multivariate discrete random variables. How­ever, 
there is no guarantee that mixing probabili­ties of this form will be nonnegative. For example, suppose 
that k = 4 and that Ya N U{l,2, . . ..n}. i = 1,2,3,4. Also suppose that the desired popu­lation correlation 
structure is that shown for gl (y) in Table 2. It should be that Al = 1 and the re­maining mixing probabilities 
are all O. But, if we assume that A. = O, then we find that our for­mula yields Al = 0.875, A2, ,A3, 
~5,As = 0.125, and ~d,&#38;,~T = 0.125. The next example illustrates that the general for­mula (10) for 
the mixing probabilities can work in some cases. Example 4. Let Y = (Yl, Y2, Y3, Y4), where Yi, i = 1, 
2, 3, 4, is a discrete uniform random variable. Suppose that the desired correlation matrix is ( 10 Pf3/4 
P14/16 o1 P;318 oRZ = Pt3/4 P;3/8 1 P;418 p;4/16 O Pi4/8 1 ) 20 members of S l x S2, while all members 
of S1 x S2 Then, if A. = O, the mixing probabilities for the have positive probability with the pmf in 
Figure 2. 0 extreme-correlation pmfs depicted in Table 1 are: Al = 13/128; A2, A4 = 15/128; J3 = 21/128; 
At = 9/128; A6= 11/128 ;A7=25/128; and A8 =19/128. We can generate values of Y with the population cor­relation 
structure given by Rz using a joint pmf (9) with any TO such that O < Y. < y = 9/16. The mixing probabilities 
in this case are -yt = Ae 7./8, 1= 1,2,...,8. 0 In a case where the mixing probabilities (10) com­puted 
are invalid, we can resort to trying to find a solution to (6)-(8) with an appropriate criterion to guide 
our selection of a composite joint pmf. We are also investigating procedures for adjusting the invalid 
mixing probabilities so that they become nonnega­t ive. 6.2 Mixtures for continuous random vari­ables 
Let X = (X1, X2, . . . . X~) be a continuous random variable, and let Fi(zi), i = 1,2, . . . ,k, be the 
cdf of Xi. We assume that Var(X~) < co, i = 1,2,..., k. Then, if we refer to pdfs rather than pmfs, the 
mix­tures (5) or (9) and the generation procedure KVAR can be used to generate values of X. Many opportunities 
to use the composite pdfs (5) and (9) will arise in simulations of manufacturing sys­tems. Two of the 
important distributions in these simulations may be the negative exponential and the continuous uniform. 
Suppose that Xi, i = 1,2,. ... k, is a negative expo­nential random variable with arbitrary expectation. 
Then, pi = 1 and p; = 1 7r2/6 = 0.6449, Vi < j (Page, 1965). There is little setup required to use the 
composite pclfs (5) and (9) because the extreme cor­relation values are independent of the parameters 
of the marginal distributions of the Xi, i = 1,2, ..., k. Example 5. Let X1 and X2 be negative exponen­tial 
random variables with identical mean 1. Suppose that p = 0.4. Figure 3 shows a plot of 1000 points generated 
with the composite pdf (5) with AO = O, Al = 0,365, and A2 = 0.635. Figure 4 shows a plot of 1000 points 
generated with the pdf (9) with TO = 7* = 0.6, 71 = O, 72 = 0.4. Along the lines of our observation about 
Example 3, we see in Figure 4 that many more of the possible values of (Xl, X2 ) are generated when 70 
= ~ than when 70 = O. 0 Example 6. Let X = (Xl, X2, X3) be a trivariate random variable where each of 
the Xi, i = 1, 2,3, is a negative exponential random variable. Suppose that the desired ]Joplilatioll 
correlation structure is given by 1 0.4 0.2 1<3= 0.4 1 0.1 . 0.2 0.1 1 i ) Random Variables 7 I X2 j 
I \ \ .. . ./ ,, 0 Figure 3: ---------Exponential ... x, Marginals, p = 7 0.4, 70 = O. 7 1. Figure 4: 
Exponential Marginals, p = 0.4, TO = 7*. For a joint pdf of the form (5) with AO = O, we find that Al 
= 0.2128, ~z = 0.1520, AS = 0.5167, and AA = 0.1185. Furthermore, y = 0.5, and if YO = y , then yl = 
0.0608, 72 = O, 73 = 0.3648, -y4 = 0.0744. Suppose now that Xi, i = 1,2,. . . ,k, is a con­tinuous uniform 
random variable. For all i < j, + = 1 and p; =. 1. Itfollows that dt = 1, Pij 1 =1,2 , . . . . 2k-1. 
Like the case where the marginal distributions are all negative exponential, there is very little setup 
required for our composition method when the marginal distributions are all continuous uniform, and the 
parameters of the marginal distri­butions do not affect the value of the mixing proba­ bilities At and 
~e. Example 7. Let X = (Xl, X2, X3) be a trivariate random variable where each of the Xi, i = 1,2, 3, 
is a continuous uniform random variable. Suppose that the desired population correlation matrix is given 
by 1 0.25 0.2 Rq= 0.25 1 0.4 . 0.2 0.4 1 ( ) For a joint pdf of the form (5) with ~. = O, we find that 
Al = 0.3625, A2 = 0.2625, AS = 0.0375, and AA = 0.3375. Suppose we let To = 0.10<0.15 = ~ , then -yl 
= 0.3375, Y1 = 0.2375, 73 = 0.0125, 74 = 0.3125. 0 CONCLUSIONS Consider an optimization problem with 
k 1 con­straints. Suppose that A~, i = 1,2, . . . . k 1, is the random variable that represents the 
values of the co­efficients in the ith constraint and that C is the ran­dom variable that represents 
the values of the objec­tive function coefficients. We can generate coefficients with a specified population 
correlation structure for synthetic optimization problems if we use our com­position approach as the 
basis for generating values of(A1, A2, . . .. Ak_l. C). Although the motivating application for us is 
the generation of synthetic optirnization problems, the composite pmfs and pclfs that we present can 
be use­ful in many practical simulation models of, for exam­ple, manufacturing systems. More research 
is needed to find mixing probabilities for composite pmfs and pdfs for general multivariate random variables. 
REFERENCES Bala.s, E. and C. II. Martin. 1980. Pivot and Comple­ment -A Heuristic for O-1 Programming. 
Manage­ ment Science 26(l): 86 96. Balas, E. and E. Zemel. 1980. An Algorithm for Large Zero-One Knapsack 
Problems. Operatzons Research 28(5): 1130-1154. Devroye, L. 1986. Non-uniform Random Variate Generation. 
New York: Springer-Verlag. Fr6chet, M, 1951. Sur les tableaux de corrdation dent les marges sent donn6es. 
Annales de l University de Lyon, Section A 14:53-77. Iman, R.L. and W .J. Conover. 1982. A Distribution-Free 
Approach to Inducing Hank Correlation Among Input Variables. Communications m Statistics, Simulation 
and Computation 11: 311­ 334. John, T.C. 1989. Tradeoff Solutions in Single Ma­chine Production Scheduling 
for Minimizing Flow Time and Maximum Penalty. Computers and Op­erations Research 16(5): 471 479. Johnson, 
M. E., C. Wang, and J.S. Ramberg. 1984. Generation of Continuous Multivariate Distribu­tions for Statistical 
Applications. Amertcan Jour­nal of Mathematical and Management Sciences 4(3-4): 225-248. Loulou, R. and 
E. Michaelides. 1979. New Greedy Heuristics for the Multidimensional O-1 Knapsack Problem. operations 
Research 27(6): 1101-1114. Martello, S. and P. Toth. 1979. The O-1 Knapsack Problem. Combinatorial Optimization, 
eds. N. Christofides, A. Mingozzi, and C. Sandi, 237-279. John Wiley and Sons, New York. Martello, S. 
and P. Toth. 1988. A New Algorithm for the O-1 Knapsack Problem. Management Science 34(5) :633-644. Moore, 
B.A. 1989. Correlated O-1 Knapsack Prob­lems. IND ENG 854 Course Project, Department of Industrial and 
Systems Engineering, The Ohio State University, Columbus, OH. Moore, B.A. and C.H. Reilly. 1993. Randomly 
Gen­erating Synthetic Optimization Problems with Ex­plicitly Induced Correlation. Working Paper 1993­002, 
Department of Industrial and Systems En­gineering, The Ohio State University, Columbus, OH. Page, E.S. 
1965, On Monte Carlo Methods in Conges­tion Problems: II. Simulation of Queuing Systems. Operations Research 
13(2): 300-305. Peterson, J.A. and C.H. Reilly. 1993. Joint Proba­bility Mass Functions for Coefficients 
in Synthetic optimization Problems. Working Paper 1993-006, Department of Industrial and Systems Engineering, 
The Ohio State University, Columbus, OH. Pollock, G.A. 1992. Evaluation of Solution Meth­ods for Weighted 
Set Covering Problems Gener­ ated with Correlated Uniform Random Variables. Undergraduate Honors Thesis, 
Department of In­ dustrial and Systems Engineering, The Ohio State University, Columbus, OH. Potts, C.N. 
and L.N. Van Wassenhove. 1988. Algo­rithms for Scheduling a Single Machine to Minimize the Weighted Number 
of Late Jobs. Management Science 34(7): 843-858. Reilly, C.H. 1991. Optimization Test Problems with Uniformly 
Distributed Coefficients. FYoceedings of the 1991 Winier Simulation Conference, eds. B. L. Nelson, W. 
D. Kelton, G. M. Clark, 866-874. Institute of Electrical and Electronics Engineers, Phoenix, Arizona. 
Reilly, C.H. 1994. Alternative Input Models for Gen­erating Synthetic Optimization Problems: Analy­sis 
and Implications. Working Paper 1994001, De­partment of Industrial and Systems Engineering, The Ohio 
State University, Columbus, OH. Rushmeier, R.A. and G.L. Nernhauser. 1993. Exper­iments with parallel 
branch-and-bound algorithms for the set covering problem. Operations Research Letters 13(5): 277-285. 
Whitt, W. 1976. Bivariate Distributions with Given Marginals. Annals of Statistics 4(6): 1280-1289. 
AUTHOR 1310 GRAPHIES RAYMOND R. HILL is a Weapons and Tactics Analyst for the U.S. Air Force stationed 
at the Pen­tagon. He is also a Ph.D. candidate in the Depart­ment of Industrial, Welding and Systems 
Engineer­ing at The Ohio State University. He earned his B.S. in mathematics from Eastern Connecticut 
State Uni­versity and his M .S. in operations research from the U.S. Air Force Institute of Technology. 
Capt. Hill is a member of ORSA, TIMS, and SCS. CHARLES H. REILLY is an Associate Profes­sor and Acting 
Chair in the Department of Indus­trial, Welcling and Systelns Engineering at The Ohio State University. 
He earned a B.A. in mathematics and business administration at St. Norbert College in 1979 and M.S. and 
Ph.D. degrees in industrial engi­neering at Purdue University in 1980 and 1983, re­spectively. His current 
research interests are in the areaa of empirical evaluation of solution methods for optimization problems 
and applications of optimiza­tion in manufacturing and telecommunications. Dr. Reilly is a member of 
ORSA, TIMS, IIE, and ASEE and Director-Elect of the Operations Research Divi­ sion of IIE.   
			