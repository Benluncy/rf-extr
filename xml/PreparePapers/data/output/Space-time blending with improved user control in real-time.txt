
 Space-Time Blending with Improved User Control in Real-Time Figure 1: Animation sequence illustrating 
the pumpkin-to-coach controlled space-time blending with applied textures. 1 Introduction Most of the 
existing methods of metamorphosis are based on the interpolation schemes, space-time blending is a geometric 
oper­ation of bounded blending performed in the higher-dimensional space. It provides transformations 
between shapes of different topology without necessarily establishing their alignment or cor­respondence. 
The original formulation of space-time blending had several problems: fast uncontrolled transition between 
shapes within the given time interval, generation of disconnected compo­nents, and lack of intuitive 
user control over the transformation pro­cess. We improve the original technique to provide the user 
with a set of more intuitive controls. The problem of the fast transition be­tween the shapes is solved 
by the introduction of additional control­lable af.ne transformations applied to initial objects in space-time. 
The approach is further extended with the introduction of an addi­tional non-linear deformation operation. 
The proposed techniques have been implemented and tested within an industrial computer animation system. 
We have also implemented our method on the GPU, so that it can be employed in real-time applications. 
 2 Related work A detailed survey on 3D metamorphosis can be found in [Lazarus and Verroust 1998]. Practically 
all existing approaches are based on one or several of the following assumptions: equivalent topology, 
polygonal shape representation, shapes alignment and possibility of establishing vertex-to-vertex correspondence. 
Turk and O Brien [Turk and O Brien 1999] proposed a more sophisticated approach based on interpolation 
of surface points (with assigned time coordinates) using radial bases functions in 4D space. This method 
is applicable to misaligned surfaces with arbitrary topology. However, for the initially given implicit 
surfaces this requires time consuming surface sampling and interpolation steps. We consider the following 
aspects of the general type meta­morphosis problem: two given initial shapes can have arbitrary topology; 
the alignment or overlapping of the shapes is not required; one-to-one correspondence is not established 
between the boundary points or other shape features. The solution to this metamorphosis problem has been 
proposed in [Pasko G. 2004], where a space-time blending operation was introduced, which is a geometric 
bounded blending [Pasko et al. 2005] operation in a higher-dimensional space in contrast to other existing 
meta­morphosis operations based on the interpolation. It is based on * dkravtsov@bournemouth.ac.uk the 
increase of object s dimensionality, function-based bounded blending [Pasko et al. 2005], and consequent 
cross-sectioning for animation. Figure 2: Frames of the animation based on the controlled bounded blending. 
 3 Method Outline Our .rst improvement over the original space-time blending method is the extrusion 
of the initial 2D/3D shapes along an arbitrary axis in space-time. One way is the interpolation along 
the axis going through the centers of both objects (.g. 2). This is similar to the .rst order interpolation 
of the offsets used to shift both objects. This can be especially useful when both objects are placed 
at a signi.cant distance from each other. In certain circumstances it is desirable to manipulate the 
transition from one shape to another in a speci.c way. For instance, the user may want to align particular 
features of the shapes or have more control over the intermediate transformation process when sizes of 
the shapes vary signi.cantly. For these situations, we introduce rotation and scale af.ne transformations 
de.ned along the time axis (.gs. 3). This also helps reducing the in.uence of fast transitions of the 
de.ning function values of both objects and thus results in smoother transition. Without these transformations 
the volume of the intermediate shape needs to be signi.cantly increased to avoid having disjoint components. 
But increase of Figure 3: User guided rotation around time axis to align object features. Copyright 
is held by the author / owner(s). SIGGRAPH Asia 2010, Seoul, South Korea, December 15 18, 2010. ISBN 
978-1-4503-0439-9/10/0012 the volume leads to even faster transitions between the shapes. Thus af.ne 
transformations provide more control over the interim modi.cations of the shape as well as help reducing 
the rate of the transition. All required time-dependent af.ne transformations can be generated automatically 
based on the estimated bounding shapes of the objects. Alternatively, the user has an opportunity to 
modify these parameters in order to achieve the desired effect. We can apply the same technique to 3D 
objects. Fig. 4 demonstrates two initial 3D objects and a number of steps of the transition between them. 
We have applied af.ne trans­formations to adjust the size and location of the intermediate object. Figure 
4: User guided scale along time axis. Another way of resolving the issue of disjoint components appearing 
between the source and the destination objects is the addition of user controlled deformations. The appearance 
of the disconnected components can usually be explained by the signi.cant difference in the distances 
between the objects. We can improve the transition through the introduction of time-dependent deformations 
in addition to the pure space-time blending. We apply deformations in the direction from the source object 
to the destination object. This can be done with the help of a non-linear space mapping ( warping ) intuitively 
controlled by two points as illustrated by .g. 5. Although the user can de.ne the control points for 
the deformation interactively, these points can be also generated automatically based on the properties 
of the objects being blended. To do so we .nd a set of internal points with extreme values of the de.ning 
function. These points are located inside thick features of the model, i.e. the areas situated at the 
the extreme distances from the object s boundary. We .nd the locations of the these points performing 
distance transform of the functional object using Euclidean metrics, i.e. performing voxelization of 
the data set to retrieve a data set containing Euclidean distances to the surface of the resulting object. 
We then .lter the retrieved dataset in order to .nd a set of internal voxels with the highest distance 
value. The desired number of points is then extracted from a subset of points located in the proximity 
of these thick features of the object (.g. 6). The retrieved points are located on the medial surface 
of the object. We establish correspondence between the points of the source and destination object using 
the distance metrics and mutual locations of these points. Due to non-linear nature of de.ning functions 
of the objects Figure 5: Metamorphosis of a torus into the union of two cylinders using two additional 
deformations. and the properties of the bounded blending operation, transition between the objects can 
not be expected to be a linear process. We linearize it performing non-uniform sampling over time. In 
the simplest case the time step can be adjusted depending on the estimated change of the area or volume 
of the shape. Discretization Figure 6: Examples of extracted thick features . can be done using a simple 
voxelization or polygonization proce­dure in 2D or 3D space. It allows us to get an approximate measure 
of space occupied by the intermediate object. This information can then be used to estimate the rate 
of change of the occupied area/volume at different moments of time. 4 Implementation and discussion 
We have implemented our method and integrated it into a popular animation modeling system Autodesk R@. 
@Maya RIt allows us to comfortably work with the model continually receiving visual feed­back. We are 
able to produce 2D and 3D animation sequences in near-real time. Finally, parameters of the model are 
exported as a kernel for NVIDIA CUDA SDK. All the evaluations and rendering of the model are performed 
on the GPU in real-time using the ap­proach described in [Kravtsov et al. 2010]. There are a number of 
issues that require additional considera­tion. First of all, space-time blending does not by itself guarantee 
the elimination of unwanted disjoint components. It only provides a method allowing the user to overcome 
this problem and have more control over the intermediate process of shape transformation. It is thus 
desirable to extract the topological critical points in space­time. This information can then be employed 
for the generation of more detailed animation sequences in the interval of the fast shape transitions. 
This information could also prove to be useful for the creation of a tool simplifying the exploration 
of 4D and higher di­mensional objects. We also plan to research the problem of vol­umetric attributes 
transfer in order to apply extended space-time blending to hypervolume objects. Another direction of 
further re­search is the application of space-time blending for advanced shape modeling operations such 
as lofting. We believe that this approach can be useful for computer anima­tion and games and illustrate 
it by several video sequences. References KRAVTSOV, D., FRYAZINOV, O., ADZHIEV, V., PASKO, A., AND COMNINOS, 
P. 2010. GPU Pro: Advanced Rendering Tech­niques. Polygonal-Functional Hybrids for Computer Animation 
and Games. Ed. Wolfgang Engel. AK Peters Ltd. LAZARUS, F., AND VERROUST, A. 1998. Three-dimensional metamorphosis: 
a survey. The Visual Computer 14, 8/9, 373 389. PASKO, G., PASKO, A., AND KUNII, T. 2005. Bounded blend­ing 
for function-based shape modeling. Computer Graphics and Applications, IEEE 25, 2, 36 45. PASKO G., PASKO 
A., K. T. 2004. Space-time blending. Com­puter Animation and Virtual Worlds 15, 2, 109 121. TURK, G., 
AND O BRIEN, J. F. 1999. Shape transformation using variational implicit functions. In SIGGRAPH 99: Proceedings 
of the 26th annual conference on Computer graphics and inter­active techniques, 335 342.  
			