
 Dynamic Scheduling with Incomplete Information HANNAH BAST* Max-Planck-Institut fiir Informatik D-66123 
Saarbriicken, Germany hannah@npi-sb.mpg.de ABSTRACT, We consider the following scheduling problem: Our 
goal is to execute a given amount of ar- bitrarily decomposable work on a distributed machine as quickly 
as possible. The work is maintained by a central scheduler that can assign chunks of work of an arbitrary 
size to idle processors. The difficulty is that the processing time required for a chunk is not exactly 
predictable--usually the less, the larger the chunk-and that processors suffer a delay for each assignment. 
Our objective is to minimize the total wasted time of the schedule, that is, the sum of all delays plus 
the idle times of processors waiting for the last processor to finish. We introduce a new deterministic 
model for this set-ting, based on estimated ranges [01(w), ,0(w)] for process- ing times of chunks of 
size w. Depending on (Y, p, and a measure for the overall deviation from these estimates, we can prove 
matching upper and lower bounds on the wasted time, the former being achieved by our new bal-ancing strategy. 
This is in sharp contrast with previous work that, even under the strong assumption of inde- pendent, 
approximately normally distributed chunk pro- cessing times, proposed only heuristic scheduling schemes 
supported merely by empirical evidence. Our model nat- urally subsumes this stochastic setting, and our 
generic analysis is valid for most of the existing schemes too, proving them to be non-optimal. 1. INTRODUCTION 
 The present paper deals with the fast parallel exe-cution of computations that can be arbitrarily decom-posed 
in such a way that the constituent chunks may be processed independently from each other and in any or-der. 
Examples of this are numerical integrations, Monte Carlo simulations, matrix computations, or any kind 
of loops with essentially independent iterates. To be re- alistic, we take into account that processing 
times of *Supported by a Graduiertenkolleg graduate fellowship of the Deutsche Forschungsgemeinschaft 
(DFG), and in part by ESPRIT LTR Project no. 20244 - ALCOM-IT. krmk.h to make digital or hard copies 
of all or pa oftis wd for personal Or hssroom me is granted without fee Provided tha copies are not made 
or distributed for profit or commercial advan%e and that copies bear this notice and the full citation 
on the first page. To copy othemise, to republish, to post on servers or to redistribute to lists, requires 
prior specific permission and/or a fee. SPAA 98 &#38;t-to Vallarta Mexico CoPyright ACM 1998 O-89791-989-0/98/ 
6...$5.00 chunks are not fully predictable. First, there can be al- gorithmic variance inherent to the 
computational prob-lem; numerical integration, for example, can be consid- erably harder for an area 
where the integrand changes rapidly. And second, there will always be system-induced variance caused 
by memory accesses, interrupts, varying processor latencies, etc. Under such circumstances, so-called 
static chunking that distributes all work as evenly as possible among the processors before the computa-tion 
starts is inappropriate; it cannot avoid that some processors finish long before others, thus wasting 
their time for the rest of the computation. More intelligently, some work should always be left unassigned, 
to be al- located later to processors that finish early. This, how-ever, takes us to dynamic scheduling, 
where each assign- ment of a chunk to a processor is associated with some scheduling overhead. To minimize 
the makespan of the computation, it is then necessary to find, depending on the degree of variance, the 
right balance between the to- tal number of scheduling operations and the processors load imbalance. 
Dynamic scheduling of decomposable computations needs to be distinguished from other lines of scheduling 
research, where either a fixed set of jobs (with known execution times) is given, or jobs arise in an 
online fash-ion and no a priori information whatsoever is available. Results from these areas yield little 
useful for our (very practically motivated) setting. Research on our variant of the scheduling problem 
is not new, and more and more sophisticated strategies have been invented over the last two decades [7, 
8, 10, 3, 2,9, 1, 6,5]. All of these, however, were developed on the basis of heuristic arguments and 
are not supported by a rigorous analysis. Except for the very simple scheme of [7], that is not practical 
in general, no nontrivial bounds on the makespan were proved. For a more detailed ac-count of previous 
work, see Section 3. We shall present a new strategy that is significantly simpler than comparable schemes, 
while performing prov-ably well in theory and practice; it also adapts to a much wider variety of environments. 
Our work is the first extensive theoretical contribution to the considered field, with the result of 
a very general, precise rnodel based on processing time estimates a, P : Jw+ + IL!+. Roughly speaking, 
[a(w), P(w)] represents the variance of the time required for chunks of size w, and our theory 182 shows 
the exact tradeoff between cr,p and the achiev-able makespan. For details see the Main Theorem in Section 
4.2; a foretast,e is given by the following rudi-mentary table, where we assume o(w) = w, and both the 
scheduling overhead and the number of processors are considered to be fixed. Each entry actually states 
the (order of the) wasted time, which is just the sum of idle times of processors that finish early plus 
the to-tal scheduling overhead. The two middle rows compare our optimal balancing strategy (BAL) with 
the aforemen- tioned static chunking approach (SC). The last row per- tains to previous work2 (PRE), 
all of which is based on the rather restrictive assumption of normally distributed chunk processing times. 
In our model, this would corre-spond to a very low variance. FIGURE 1: Wasted times for total work n 
 The practical value of t,his work is apparent by the fact that our theory reveals previously unknown 
weak points of existing schemes, compared to which our new strategy, with its strong mathematical foundation, 
always per-forms better or at least equally well. We expect that the idea of estimated ranges underlying 
our model can be usefully employed in different contexts as well. The next section will explain our model; 
based on this, Section 3 sheds light on previous work and intro- duces the new BAL scheme. Section 4 
presents our theo- retical result, the intuition behind it, and its application to some stochastic models. 
The final two sections are dedicated to the proofs of the upper and lower bound. Some of the statements 
in the first, rather informally written half of the paper may be hard to follow before the second, technical 
part is understood. We apologize to all readers who find this annoying. 2. THE SCHEDULING MODEL We consider 
a distributed machine consisting of p identical, asynchronously working processors together with a central 
scheduler that maintains a given amount of work to be processed in arbitrary decomposition by the processors. 
The functionality of the scheduler is that it can hand out (synonymously: schedule, allocate, as-sign) 
chunks of work of an arbitrary size to requesting *The entry > logn is not valid for the scheme of [5]; 
see Section 3.2 for commentson this. processors that must not be currently working on a pre- vious chunk. 
For each request served, the respective pro-cessor suffers a fixed scheduling overhead. The goal is to 
schedule the given work with minimum makespan, i.e., such that the whole computation is finished as quickly 
as possible . The difficulty is that, while, naturally, the processing time required for a chunk is roughly 
propor-tional to its size, there can be considerable variance and the exact time is not known a priori. 
The remainder of this section is dedicated to modeling this uncertainty. We first present the approach 
underlying all previous work; then WC shift to a much more general viewpoint. 2.1. Stochastic scheduling. 
The strategies of [7, 3, 2, 9, 6, 51 were designed for a rather specialized setting, where the total 
work consists of n separate tasks, the processing times of which are independent random vari-ables, identically 
distributed with mean 1 (for a suit- able time scale) and known variance cr2. From this it is typically 
deduced (by the central limit theorem) that processing times of chunks of size w are approximately normally 
distributed with mean w and variance wcr2; schematically, time(w) t N(w, WC?). More generally, one could 
investigate the case time(w) t F., for arbitrary (possibly less well-behaved) distributions F,. Previously, 
only [5] took a step in this direction, by running some experiments for F, = Exp(w), the expo- nential 
distribution with mean w and variance w2. But there is a snag in even the most general stochastic ap-proach: 
First, when there is no true randomness but rather nondeterministic behaviour, it may not at all be meaningful 
to assign probabilities. And second, even if the behaviour is quasi random, it can be very difficult 
to find a suitable distribution. 2.2. Scheduling against an adversary. We here as- surne that processing 
tirnes of chunks may be fixed ar-bitrarily by an adversary; as before, the scheduler does not learn of 
these choices before the respective chunk is fully processed. As a base for its decisions, the sched-uler 
is provided with some estimate of the adversary s viciousness, the so-called uariunce estimator [a, ,0]. 
Notation: For cy,p : II?? -+ IL?+ with o(w) < w < [j(w) for w > 0, [a,@] denotes the function w t+ [a(w),P(w)]. 
Intuitively, working with [cr, p] means that for chunks of size w we reckon on processing times between 
o(w) and p(w), called lo,wer limit and upper limit, respectively; in our schematic notation, time(w) 
t [a(w), p(w)]. 3Note that other performance measures such as average job completion time only make 
sense when jobs are of individual importance. The goal is now to find a strategy that, for given [a,@], 
is optimal for the setting time(w) c [a(w), /3(w)], while, roughly speaking, it performs only gradually 
worse when the adversary deviates from its estimated behaviour. In particular, the makespan should be 
close to optimal for only slightly misbehaving adversaries. The following key notion, used in our theorems 
and proofs, quantifies this deviation. Definition: For x, a, b > 0, the distance of z from the interval 
[a, b] is xL b gf max{O, a -x} + (p -l)max{O, x -b}, where p is the number of processors. For a collection 
C of 1 chunks of sizes WI, . . . , wl and with processing times Tl,... ,Tl, and for (Y, /3 : W+ -+ lR+, 
the distance of C from [o, p] is First observe that this (nonnegative) distance is zero if and only if 
the processing times of all chunks are within the intervals corresponding to their sizes. The reader 
will also note that very short and very long process-ing times are weighted differently. The intuitive 
reason is that a very quickly processed chunk only affects the processor working on it, while excessively 
long process- ing may affect all other processors as well (as is obvious for the very last chunk). Schematically, 
we will denote by time(w) &#38; [o(w), p(w)] an adversary that fixes the processing times of a col- lection 
C of chunks such that Cl: = 6. Note that this characterisation, unlike the previous ones, cannot be for- 
mulated for individual chunks. Let us finally verify that our approach covers the sto- chastic setting 
time(w) c F,, for known distributions F,. We simply use appropriate tail estimates in order to find cr,/3 
: lR+ + lR+ and a constant e such that, for all w, where X, is distributed according to F,. Then also 
the expected distance of any collection of chunks from [a, p] will be bounded by E and our Main Theorem 
yields a sharp bound on the expected makespan (depending on Q, /3 and E); examples will be given in Section 
4. In contrast to previous work, no independence between processing times of different chunks is required 
here. But given this is indeed the case, the bounds can be easily seen to hold even with high probability. 
2.3. Discussion. We immediately find three strong ar-guments in favour of the adversary-based approach: 
First, and as a matter of fact, it is very general; in particu-lar, an upper bound found in this model 
easily trans-lates to the stochastic model. Second, our approach en-ables an elegant mathematical analysis, 
while so far a direct probabilistic analysis seemed to be intractable. The third argument is somewhat 
more subtle: one might argue that it is as difficult to obtain suitable ranges [a(w),p(w)] than to estimate 
the mean and variance of an assumed underlying distribution; in fact, this would be true if our goal 
were to model reality very precisely, in which case extensive profiling information is required for both 
approaches. The point is that our model is especially suited for expressing weak a priori informa-tion, 
while in the stochastic model the method of choice would be to heavily overestimate the variance, which 
(provably) gives bad results in general. If, for exam-ple, we work with variance estimators of the form 
w c-) [w/a, b. w], there is a simple strategy that merely needs to know an estimate c on a . b, and its 
wasted time will be linear in c (and logarithmic in the total work). 2.4. Input convention and objective 
function. In our model, a problem instance Z is given by the total amount n of work, the number p of 
processors, the sched- uling overhead h, and an infinite sequence Tl, TX, T3, . . . , representing the 
adversary, where Tj 2 0 is the process- ing time of the jth chunk assigned. We write ZN(n, p, h) for 
such instances, and S(Z) for the schedule, that is, the sequence of chunks, produced by a strategy S 
on input Z. Given such a schedule, a processor s wasted time is defined as the makespan of the schedule 
minus the total processing time required for all chunks assigned to this processor. Since the scheduler 
has no influence on pro-cessing times, instead of the makespan, we can choose the average wasted time 
per processor (the total divided by p), briefly called waste, as our objective function. 3. PREVIOUS 
AND NEW STRATEGIES As mentioned before, the right decomposition of the given work into chunks is crucial 
for achieving optimal balance between scheduling overhead and even proces-sor load. To decide on suitable 
chunk sizes, a variety of strategies has been invented so far. Among the simpler ones are fixed size 
chunking (FE) [7], guided self sched- uling (Gss) [8], and trapezoid self scheduling (Tss) [lo]; more 
involved are the factoring schemes (FAc, FAC2) [3], TAPER [9], and BOLD [5]. The last four (and most 
recent) of these were born from complicated, often some-what arbitrary heuristics, and were only supported 
by empirical evidence. Only the simplest scheme, FSC, came equipped with a mathematical analysis, however, 
with a weak result only. Further, it is a matter of fact that all listed schemes are not able to adapt 
to the gen- eral adversary scenario. Even worse, the schemes are not even optimal for the standard stochastic 
setting time(w) t N(w, wg2) (cf. Section 2.1), for which all of them except GSS and TSS were actually 
designed; this is demonstrated next. 3.1. Parameterized Self Scheduling. With the only exception of BOLD, 
all the named st,rategies may be viewed as instances of a generic scheme called pamm-eterized self scheduling 
with parameter Q : I?+ --+ I%+, PSS(Q) for short. In reply to a processor request, PSS(Q) assigns a chunk 
of size @(W/p), where W is the amount of currently unassigned work; while information about the sizes 
of earlier chunks may be incorporated into Q, their processing times are always ignored. The limits of 
this scheme are easily explored: it is natural to have &#38;V/p) 5 W/p, at least for W/p 2 h (consider 
a situ- ation, where all processors request at roughly the same time: they should all get a chunk of 
about the same size then). This will become clearer in Section 4.1; for now, take note of the fact that 
all of Gss, FAC, FAc2, and TAPER have this property. Given it, each assignment decreases W by at most 
a factor of 1 - l/p, and p as- signments decrease it at most by l/3, for T/V > 3ph. The waste incurred 
by any such scheme on input ZN(n,p, h) is hence at least (1) h.log,$ On the other side, the analysis 
in Section 5.1 will show that for any C > 1 and for Q : II: c) max{x/C2, h -I- E}, P%(Q) incurs a waste 
of at most (2) C(h + ~1. ln cp(t+ E) + C(C + 2). (h + c), for adversaries time(ul) &#38;-[w/C, Cw]; observe 
that, for C + 1 and E + 0, this bound is just h. ln(e3n/ph). Since EX, IzTc = O(l), for X, distributed 
according to N(w, wcr2), most previous work is hence subsumed by (1) and (2), the latter being just a 
special case of our Main Theorem. 3.2. Comparison. More informally, let us next con-sider the named schemes 
with respect to the following two principles, that will also be very helpful later. conservative: always 
leave enough work unassigned to be able to achieve even finishing times Both of these principles are 
obviously crucial: not being conservative risks long idle times, while not being pro-gressive causes 
too many scheduling operations. We may (loosely) formulate in these terms a set of four criteria, which, 
according to both our theoretical and practical investigations, are necessary and sufficient for any 
effi- cient scheme. very careful start: the choice of the first p chunk sizes is of utmost importance, 
and it is crucial to be maxi- mally progressive and conservative at the same time. conservative middle: 
after the bulk of the work has been assigned, being a bit less progressive does not harm. Conservatism, 
however, is still crucial: the assign-rnent of too large a chunk here can spoil the whole performance. 
progressive end towards the end, conservatism becomes neurotic, i.e., it is more important to process 
the re-maining work in as few chunks as possible. This can always be achieved by introducing a suitable 
minimal chunk size. simple formulas: For many applications, it is desirable that no time-consuming calculations 
(like evaluating square roots or logarithms) are required of the sched- uler. It is a matter of fact 
that even for adversaries of the type time(w) t N(w, wg2), each existing scheme fails for at least two 
of these criteria; a comprehensive summary is given in the following table. scheme optimal at start middle 
end simple analyzed Fsc t t --h J Tss -3 -+ + J Gss -+ -+ t J FAC -+! + t FAC2 t (4 +- J TAPER t (4 J 
- BOLD J ? t 1-. BAL J (4 J J J BAL J J J J FIGURE 2: Comparison of existing schemes in the standard 
stochastic model The symbols t and -+ give an indication of whether the respective scheme is too conservative 
or too progres- sive in the respective phase of the scheduling process; (J) says that the difference 
to the theoretical optimum is negligible except for very large n. The last column indicates whether any 
analysis was given prior to the present work. The last two rows refer to the new bal-ancing strategy 
BAL, and its relaxed version BAL , both introduced in the next section. BAL is simpler, and-though not 
asymptotically optimal-performs best for all practical purposes. The exclamation mark in the row pertaining 
to FAC refers to a flaw in its heuristic; under certain circumstances, in particular when the number 
of processors is small, already the first chunk assigned by FAC is so large that it is very likely to 
finish last. The question mark is due to the fact that nontrivial an-alytical statements seem to be completely 
out of reach for the extremely complicated BOLD scheme. However, the author of [5] conjectures a bound 
on the waste of O(loglogn), when time(w) t N(w, O(w)). 3.3. The balancing scheme. We now describe an 
op- timal strategy for the general adversary scenario with given variance estimator [o,/3]. It will become 
clear by our analysis of PSS in Section 5.1 and our lower bound that an optimal PSS scheme only exists 
if p(w) -o(w) = Q(w), that is, if we assume relatively large variance. Typically, this will be the case 
for smaller chunks, while initially, and for large problems, we often reckon on pro- cessing times inside 
relatively narrow limits. In the lat- ter case, a strategy must consider additional state in-formation 
in order to be optimal, and the BAL strategy in fact uses the time of request (ignored by all the PSS 
schemes) for this purpose. Namely, if o(m) < w < p(w) and I -o(w) _< w/18 (the constant stems from our 
analysis), BAL works by grouping a number of consecu- tive processor requests, served in what we call 
a round, and it tries to maintain the invariant that all requests in a round are issued at roughly the 
same time (which is certainly true for the initial p requests). Like Pss(Q), BAL is also guided by a 
parameter Q : lR+ -+ IL!+, and the first chunk assigned in a round is of size w = @(W/p), if W work is 
still unassigned. Indeed, since o(w) x w, Q should be chosen approximately as p-l, because we expect 
all other processors to request soon also, so that about (p-1) *,0(w) work should be left for them. Now 
we expect this first chunk, requested at some time to, to finish around time h + to + w, which we set 
as the target t trs for this round. Any further request that occurs at time T is then served by allocating 
a chunk of size ttrg -T -iz, so that all processors will finish at about the same time, namely ttrs. 
For each round, BAI, also set,s a maximal time of request t"lax < ttrg; initially tmax = -1. With the 
arrival of the first request, after Pax, BAL starts a new round, unless ,0 - cy has become large relative 
to the (next) chunk size. In this case, the next p chunks can no longer be assured to finish almost simultaneously, 
but instead the variance is now large enough, so that a PSS scheme may be employed to schedule the remaining 
work optimally. For an exact specification of BAL(Q), see Figure 3. Here Wmin = lim,je Q(X) is the minimal 
chunk size as specified by Q. Once Pax has been assigned the spe-cial value PSS-MODE, the remaining work 
is sched- uled according to Pss(Q). The correct choice of Q for given cr,j3 is described in Section 5.2; 
approximately, e(z) = max{,LVi(z), h}. if (Pax == PSS-MODE) return @V/p); else if (T < tmax) return ttrg 
-T -h; else { /* new round */ w = ew/P); ttrg = T + w + h; if (W > max(0.4. W/p, Wmi,}) tmax = ttrg -(W/p 
-w)/9; else Pax = PSS-MODE; return w; > FIGURE 3: The chunk size computed by BAL(Q) for a request at 
time T BAL is a simplification of BAL which unconditionally sets tmzLx to PSS MODE after the first round, 
and pro- ceeds with a linear approximation of Q from below (if necessary). BAL is thus slightly less 
progressive than BAL in the middle part, which makes little difference, if only the first p chunks were 
chosen optimally. 4. MAIN RESULTS This section provides matching upper and lower bounds on the optimal 
waste in the adversary model. To lead us to the Main Theorem, we first present a simplified but intuitive 
analysis, introducing on the way essential terminology. 4.1. Intuitive analysis. Assume that we are 
given a variance estimator [G, p] together with a constant A > 1 such that G(W) 2 w/A. Such a bound is 
in fact essential, because the scheduler s base of decision is almost com-pletely lost if processing 
times can be fixed (arbitrarily close) to zero. In this section we will restrict atten-tion to the case 
of relatively large intervals [G(w), P(W)] (of size 0(w)), in which case it turns out that we may as 
well work with the (weaker but simpler) estimator w I-$ b/4&#38;41, or, with time re-scaled by a factor 
of A, w I--) [w,/l(w)], where p(w) = AD(w) = Cl(w). Definition: The imbalance of a schedule is defined 
as P p. max{TF , . . . , Tin} -c TF , k=l where Tkfi is the finishing time of the kth processor. The 
imbalance of a schedule at time t is the imbalance of the partial schedule consisting of all chunks assigned 
(not necessarily finished) before time t (see Figure 4). f t = aum of all - FIGURE 4: The imbalance at 
time t By the above time scaling, we expect each unit of work to take at least one unit of time. Therefore, 
if at any point in the scheduling process the imbalance is I, the amount of unassigned work W should 
be at least 1. On the other hand, no more is required, and it appears that an ideal strategy would maintain 
the invariant I = w. But this is impossible: while it is very easy to keep track of W, it is the inherent 
property of I that its actual value is unpredictable. The four criteria of the previous section already 
gave an idea of suitable relaxations; for details, see Section 5. For the purpose of this section, we 
focus on a very particular scheduling process, where indeed I = W throughout. Namely we consider an, 
in some sense, worst possible adversary that lets all of 11 processors finish their chunks at the lower 
limit, while for a distinct (zeroest) processor it fixes the processing time at the upper limit. (4 (b) 
(cl +.--;5 = P(w)--rf FIGURE 5: How the imbalance decreases Figure 5 illustrates how the schedule develops 
over each p subsequent allocations; for simplicity, the scheduling overhead is neglected here (compared 
to the processing time). When the current imbalance is I, the scheduler should choose a chunk size w 
such that, on the one hand, p(w) < I/p, in order to be conservative, and such that, on the other hand, 
w is maximal with respect to this condition, in order to be progressive. By taking w = ,B- (W/p) both 
is provided, and indeed, this will be the choice of the optimal PSS scheme. Now all p chunks finish as 
early as possible-in a sense, this is the worst that can happen-and I/p drops to I/p -w = I/p -0-l (I/p) 
=: 7(1/p); schematically, I/P -5 Y(I/P). Notation: For given invertible4 ,0 : Rf + IR+, the function 
y : I@ + lR+ with y E id -p-l, i.e., y : z ++ z -,K (x), is called the relative variance associated with 
p. Figure 5 nicely illustrates the intuition behind y: a chunk of a size chosen maximal such that the 
processing time is at most Z, may finish at most y(z) before its upper limit. Continuing our analysis, 
we see that r 9ounds of our scheduling process, according to Figure 5, decrease the 4this can be relaxed 
to increasing , in which case the inverse is defined as p- (y) = sup({O} U {CE > 0 : b(z) < y}). average 
imbalance I/p as follows; note that the zeroest processor takes time ,0(p- (n/p)). n/p -3 y(n/p) -% . 
.. -% # )(n/p). Naturally, a further round r + 1 should be executed only if the decrease of I/p, namely 
$ )(n/p) -y(T+l)(n/p), outweighs the overhead per processor in the round, which is h. We should hence 
choose the number of rounds as y*(n/p, h), according to the following Notation: For all z,y > 0, y*(x, 
y) +if min{i E N : yci) (x) -yci+l) (xl I Y>. After the last round, we then have ,Fl(I/p) = I/p -7(1/p) 
5 h, hence I 5 pa(h) work is still unassigned. According to our lower bound (Section 6), this is opti- 
mally assigned in some O(p.@( II) / h) chunks of a common size 0 (h) (the minimal chunk size). Let us 
finally calculate the waste of the complete schedule: The number of allocations performed was p . y*(n/p, 
h) +O(p.P(h)/h), and for a minimal chunk size of h the final imbalance is at most h + P(h). The waste 
is hence correctly quantified by the following compact formula: h. y*(n/p, h) + W(h)). But recall that 
we merely proved this for a very partic-ular adversary that gave rise to a very nicely structured scheduling 
process. Besides, the underlying intuition is not valid for estimators [a, p] with p(w) -a(w) = o(w), 
in which case o may no longer be substituted by its lower bound id/A, and no schedule will ever develop 
accord-ing to Figure 5. Quite surprisingly though, the optimal wasted time achievable for the general 
problem deviates no more than a small constant factor from the above formula. 4.2. Theoretical results. 
The Main Theorem below is stated for symmetric variance estimators with a lower bound of w t+ w/2 imposed 
on the lower limit, that is, of the form 7u Ij [max{w/2, w -6(w)}, w + 6(w)]. This allows us to cover 
the whole spectrum of useful variance estimators by a single parameter S : lR+ -+ I@; a typical estimator 
will be coarse (6(w) > w/2) for small chunk sizes and rather fine (6(w) < w/2) for larger chunk sizes. 
Merely considering symmetric estimators means no loss of generality, since every variance esti-mator 
can be replaced by a weaker but symmetric one without a significant amount of information being lost 
(interval sizes at most double). Setting the lower bound to id/2 is pure convention, and in fact, our 
Main The-orem holds for any id/A, A > 1. Concerning the up-per bound below, note that although there 
is in fact an optimal choice for the minimal chunk size, namely %nin = h + e, the upper bound is formulated 
for arbi-ap/2fi (this requires an estimate of the convergence trary given Wnlin > h, as our model does 
not (and should rate of the central limit theorem [ll, Theorem 5.16]), not) assume a priori knowledge 
of e. and the waste bound becomes Main Theorem. Let p E W, 7~, h 2 1, and 6 : Rf + K?.+ such that S and 
id -6 (hence also id + S) are increasing. Then, for (II : w + max{w/2, w -6(w)}, /3 : w I-) w + 6(w) 
and y G id -,/3-l, the relative variance associated with /3, > h, there exists a strategy BAL whose waste 
on all instances Z-(72,3), h) is h+t (a) for all Wnin _ 0 (h + 6) . r*c;, Wmin) + (1 + n;,,,i,,) P(r~hlir~d), 
( where E = BAL(Z)~$ (b) for every strategy S and all E 2 0, there exists an instance Z~(n,p, h) with 
S(Z)TI = e such that the waste of S(Z) is O((h+t).y*(;,h+c)+/3(h+e)). The following table should provide 
a feeling for the trade- off between the given variance estimator, represented by /?, and the corresponding 
optimal waste, which is order of y*(n/p, h + e). The last column merely indicates the behaviour of y* 
as a function of n/p. For more detailed evaluations, see the following application of the Main Theorem 
in the context of stochastic scheduling. P(x) Y(X) Y* x++ Mfi log log 2x xl2 log zlogx %::-Cr/logx log2 
X2 X-6 J FIGURE 6: How the optimal waste depends on ,0 For the standard stochastic setting time(w) t 
N(w, wo2), use the well-known tail estimates [4] to verify that if S, is distributed according to N(w, 
w(T~), then (3) wJ:::~~&#38;w-~(w)) = O(l), where 6(w) = ad-. According to part (a) of the Main Theorem 
(separately applied to each pos- sible fixing of the random choices), the expected waste incurred by 
the corresponding BAL strategy is then O(hloglog; + d-). (4) If we replace, for integral w, N(w,wa2) 
by the w-fold convolution of some distribution with mean 1 and vari- ance g2, the analogous bound to 
(3) holds for 6(w) = (5) O(hloglog; + 6). Finally, to give an example of a more ill-behaved dis-tribution, 
if time(w) t w + Exp(w), then w + 6(w) = w log(pw) is an appropriate upper limit and the expected waste 
is O(hlognlog;). 4.3. Simulation results. Due to a lack of space, we have to omit any detailed report 
on our findings. As a very rough conclusion, let us mention though that in our experiments concerning 
the standard stochastic setting the strategies basically rank according to their order in Figure 2 (with 
the topmost scheme performing worst); the wasted times achieved by BAL, BAL and BOLD differ only insignificantly. 
5. UPPER BOUND PROOFS We first give a complete analysis of the generic PSS scheme introduced in Section 
3.1. Recall that, in partic- ular, this implies (tight) bounds for t,he existing schemes Fsc, Gss, FAC, 
FACT, and TAPER, which are just spe-cial instances of PSS. The second part of this section investigates 
the complete BAL scheme. 5.1. PSS. In the context of some schedule, let Ij and Wj be the imbalance and 
the yet unassigned work, re-spectively, at the time of request for the jth chunk, whose size will be 
denoted by wj. In particular, I1 de- notes the initial imbalance (so far, we assumed the initial p requests 
to be issued simultaneously; this, however, is not the case when PSS takes over after the last round 
of BAL). For a given variance estimator [id, fi], the follow- ing lemma suggests that chunk sizes should 
be chosen such that ,0(wj) = Wj/p + ,0min, for some imbalance slack ,&#38;in 2 0. Lemma 1. For any schedule 
with 1 chunks and for all Pmin 2 0, if Wj I Wj/P + Pmin, A+1 5 max{O,Il -WI} +ph+ W~+I +pPmin +cq, j=l 
 where tj = T,T~ f  and Tj is the processing time of the jth chunk; h is the scheduling overhead. Proof: 
The proof is by induction on 1. The base case 1 = 0 is easy; for the induction step 1 - 1 + 1 we distinguish 
between two cases: If the lth chunk finishes last, we have 4+1 I (P-l).(h+Z) 5 pll+(P-l) (~/P+P,i,)+~1, 
5 ph + Wl + PAnin -~1 + ~1 and Wl -wl = Wl+l. If the lth chunk is not the last to finish, then I[+1 
= IE - (h + Tl) I 4 -WE + EL, and we can use the induction hypothesis. 0 For variance estimators of the 
form [id, p], we ana-lyzed in Section 4.1 the waste incurred by Pss(Q), for e(x) = max{P- (x), wmin}, 
against a very particular adversary. Let us now consider this very scheme for increasing p, and hence 
increasing Q. Then p(wj) = P(Q(Wj/P)) = max{Wj/p,P(wkd} I Wj/P + P(Wrnin)7 and Lemma 1 proves that for 
arbitrary Z~(n,p, \L) the final imbalance is bounded by (7) max{O, 11 - n} + ph + pp(wmin) + 1. C, where 
E = Pss(Q)(Z)TL, and 1 is the total number of scheduled chunks. In order to bound 1, first observe that 
Wj+l = W, -Q(Wi/p) > (1 -l/p) . Wj if only Wj > W,in. Hence if w~+~-I > Wmin, Wjfp-l 2 Wj/3, and because 
e is increasing, wj+p 5 Wj -P @(l/3 wj/p) = P T(Wj/P), where y E id -(3p)- . Hence Wipfl /p 2 yci) 
(n/p) if Wp > Wnin, and it follows that for all j > p.y* (n/p, w ,in), Wj/P 5 3P(%nin ). Using that w,nin 
is the minimal chunk size, we thus obtain l/p < y*(n/p, wmin)+3P(wmin)/w ,i*,. By (7), the waste incurred 
by PSS(Q) on Z is max{O,Il -n)lp + h + P(wmin) + (h + 6) . I/P, which is now easily bounded by max{O, 
I; /p -n/p} + h + ( ) (h + 6) y* (n/p, wmin) + (1-t 3 2) . P(wmin)* 5.2. BAL. We are now ready to prove 
part (a) of the Main Theorem in its full generality. For this purpose, assume we are given Z~(n,p, h), 
a minimal chunk size w,rn 2 h, and a variance estimator w I-+ [max{w/2, w -S(w)}, w + 6(w)], with 6 
: LR+ -+ lR+ such that all of 6, id -S, id + 6 are increasing. It will be demonstrated that BAL(Q), for 
Q E max{ (id + 96)-i , w,in}, achieves the bound stated in the Main Theorem. We first give a full analysis 
for the case e = 0, that is, when time(w) t [max{w/2, w -6(w)}, w + S(w)], and afterwards comment on 
the extension necessary for the general case. Make sure to have Figure 3 handy, and let r denote the 
number of rounds, that is, the number of times, the instruction Pa = ttrg -(W/p -w)/9 is executed. With 
indices now pertaining to rounds (instead of scheduling operations), we use Wi for the amount of work 
left before the first chunk is assigned in round i, the size of the latter is denoted by wi; note that, 
according to Figure 3, wi > max(0.4 . Wi/p,wmin}. Also, ti and trax will denote the values of ttrg and 
Pax in round i. We will often use that (9) Wi/p = Wi + 9S(Wi), i E [r], which, by wi > max(0.4. Wi/py 
wmin}, implies (10) 96(Wi) = (id -~)(Wi/p) 5 3/5. Wi/p, i E [r]. For i E [r], call tkrg -S(wi) and t~ 
g+c5(wi) the (absolute) limits of round i, and verify that because any chunk in this round has size at 
most wi, it finishes within these limits. In particular, since tplax = tirg -S(wi), at most p chunks 
are assigned per round, so that by (9), (11) Wi+l 2 Wi -pWi = Spd(Wi), i E [r -11. It is now easy to 
prove the crucial fact that the limits of successive rounds form a sequence of non-intersecting intervals 
with increasing bounds, i.e., (12) t;yl -d(Wi+l) > ti + S(Wi), i E [r -11. This is because the target 
of round i + 1 is never set before tyax = tIrg -S(wi), so that ti; l >_tirg -6(wi) + wi+i + h, and by 
(ll), Wifl > 0.4 * Wi+l/p > 36(Wi) > 2S(Wi) + d(Wi+l). With the help of (12) we may now conclude that 
a processor s last chunk finishes somewhere in [t:g -S(w), cg + S(wr)l, so that by (ll), the imbalance 
Ir+i after the last round is (13) I r-+1 5 2P6(%) < w,+1/4. It remains to investigate BAL S progressiveness, 
that is, how fast the unassigned work decreases over the rounds. Assuming that in a round i, 1 < i < 
r, all processors request for a chunk simultaneously, a maximum of pwi work would be assigned, and by 
(9), Wi+l = SpS(wi) then. But actually, each chunk from a previous round is finished by time ttrg Z-l 
+ 6(wi-1) at the latest (use (12)), which is at most 2S(wi-1) after round i was started. Therefore, (14) 
Wi+l 5 9$(Wi) + 2pd(Wi-1)) l<i<r, and by (lo), 9p6(wi) 2 3/5. Wi < 27/5 . pS(~i-1) + 6/5. pd(wi-z), which 
implies Wi+z/p < 96(wi), for i E [r -31. Taking yi G id -Q, so that by (lo), 9S(wi) = yi (Wily), we can 
now easily bound the amount of work left unassigned after the last round: (15) WTfl /p < Yy3 --l)(n/p). 
According to Figure 3, the remaining work Wr+l is sched- uled by Pss(Q). In order to apply the analysis 
of Sec- tion 5.1, we will temporarily re-scale our time unit to half its original value and relax the 
lower limit of our variance estimator, which then becomes w cj [w,2p(w)], where @ : w e w + 6(w). But 
all chunks assigned when Pax = PSS-MODE have size w < max(0.4 . Wr+l/p,wmin} (check with Figure 3), and 
for w > Wmin we can show5 that S(w) 2 3/18. w, SO that either w = Wmin or 2@(w) 5 w + 96(w). R.ecalling 
that, I, E max{(id + 96)-i,wlnin}, our analysis of PSS(Q) hence applies. Using (8) and re-turning to 
the original time scale, we can bound the average wasted time of BAL(Q) by where 72 f id -(S/3- . By 
(13), the last term is zero, and using (15) it can be shown that6 06) r + Y2*(WfllP I Wnin) = 0 (Y*( 
L/P> 7L min)) 3 where y E id -p-l. We now easily obtain a bound on the wasted time 0 (h . y* (n/p, wmin) 
+ (I+ h/wmin) P(wmin)) ) which ends the analysis of BAL(Q) for the case E = 0. The analysis for the 
general case proceeds along the same line of argumentation, but meets with major com-plications. Namely, 
the nice property (12) on round lim-its is no longer guarantueed, and the unassigned work may decrease 
in a much more irregular manner (for ex-ample, only one chunk might be scheduled in a round). Due to 
lack of space, let us here consider a variant of BAL, whose implementation is less elegant but for which 
the above analysis nicely extends to arbitrary adver-saries. For any fixed processor, let us define the 
ear-liness of its ith chunk, with (absolute) finishing time T, as max{O, t: g-G(wi)-T}, and its lateness 
as max{O, T- tfrg -6(wi)}; in the above analysis both of these were al- ways zero. First, we modify BAL 
such that chunks with positive earliness do not immediately request another chunk, but, rather pwait 
for a period of time amounting to their earliness, after which the current round will be over. This very 
simply adds the earliness of a chunk to the total wasted time of the schedule. The other modi-fication 
is that if the ith chunk of a processor has pos- itive lateness, the processor will be assigned the i 
+ 1st chunk as if the former chunk finished just in time, i.e., at tirg + 6(wi). While this requires 
some additional book-keeping (namely the ti $ -tirg -6(wi) have to be remem- bered), the nice effect 
is that (15) (progressiveness) holds without change and that concerning (13) (imbalance) the lateness 
of chunks simply aggravates over the rounds. More precisely, with the modifications just sketched, it 
is not hard to prove that for arbitrary adversaries the wasted time increases by at most rp Cl:, where 
C is 5To be honest, we here require that z H ~(s)/z -2/S has at most one zero; this could be avoided, 
however, if BAL employed Pss(2p) instead of PSS(Q) after its last round. 6This bound would be straightforward 
to prove if y1 E yz 3 y; but due to the constant factors involved in y1 and yz, (16) becomes a rather 
complicated exercise. the collection of the (exactly rp) chunks scheduled in the r rounds of BAL. Putting 
this together with the (full) analysis of PSS, just as done before, we obtain the bound stated in part 
(a) of the Main Theorem. 6. THE LOWER BOUND PROOF At last, we are in the adversary s role and our goal 
is to fix, within given limits, the processing times of chunks assigned by the scheduler, so as to enforce 
a schedule with maximal waste. As in the Main Theorem, assume we are given p E N, n, h 2 1, E > 0, and 
6 : llX+ -+ Iw+ such that 6 and id -S are increasing; the same then applies to Q : w I-+ m;lx{w/2, w 
-d(w)} and p : w e w + 6(w). We will (incrementally) construct, for a given strategy S, an instance Z~(n,p, 
h) with S(Z)If = E such that the waste of S(Z) is at least a constant factor times where y z id -/3-l 
and h = max{ h + E, 6h). For the purpose of this section, the lower and upper limit of a chunk of size 
w assigned at time T, will be the absolute times T + h + Q(W) and T + h + p(w), respectively. Though 
the adversary need not fix finishing times of chunks right at the time of their allocation, it usu-ally 
does so, except for one designated peak chunk, the Yeader , for which the decision is postponed. Initially, 
the very first chunk allocated becomes the (first) peak chunk. Whenever a new chunk is scheduled, the 
ad-versary compares its upper limit T to the peak upper limit Tpeak, that is, the upper limit of the 
current peak chunk. If T 2 Tpeak, the finishing time of the new chunk is fixed at its lower limit; otherwise, 
it becomes the new peak chunk, and the finishing time of the old peak chunk is fixed at the maximum of 
its lower limit and the ac-tual time. Only for the very last (peak) chunk a finishing time outside its 
limits is fixed, namely I.c/(p- 1) beyond its upper limit, where 1 is the total number of chunks that 
have been assigned then. The resulting schedule S(Z) will satisfy S(Z)If = E, and the wasted time is 
increased by 1 . E, which turns out to be just what we want. At any given time, the peak A states the 
difference between the current peak upper limit and the maximal finishing time of the other processors 
(which are already fixed). We will in the following demonstrate that, in-tuitively, large chunks cause 
large peaks, while reducing a peak of A to y(A) requires at least p -1 scheduling operations. We then 
argue that either many chunks are allocated, or the final peak is high. Lemma 2. Just after the allocation 
of a chunk of size w, the peak is at least /3(w) -20. Proof: After the considered allocation, at some 
time T, the peak chunk has upper limit Tpeak 2 T + h + p(w). Consider any of the other chunks, with size 
w and lower limit Tc,,. For w < 20, Te,, 5 T + h + a(w), otherwise Tpeak - Tc, > /3(w ) -a(~ ) 2 ,L~(w 
) -w . In any case, Tpeak -Tfi, 2 /j(W) -w. 0 Given a schedule and a pair of chunks (Cl, Ca) such that 
C, is the next chunk scheduled after Cl on the same processor, define the gap of (Cl, Cz) as [T/hj, where 
T is the time that elapses after Cl is finished until processing of C.2 starts (in case processors never 
wait to request unless all work is assigned, the gap is always 1). The cost of a schedule is then defined 
as the sum of all such gaps. We easily verify that a schedule with cost C and imbalance 1 has total wasted 
time of at least C . h + I. Let us further agree to use the notation A 4 A as a convenient shorthand 
for the following assertion: If the peak at some time T is at least A, and the cost of the schedule increases 
by at most C from T to a later time T , then the peak at time T is at least A . Lemma 3. Let 7 -id-max{P- 
, 6h}, then for positive A with y(A) > 2h, A 2 y(A). Proof: Denote by Tpeakr Tieak the peak upper limit 
at time T,T , respectively; then Tieak 2 Tpeak. Consider an arbitrary chunk-except the peak chunk at 
time T --- scheduled between T and T , with size w and finishing time Tp,, fixed at time t. The central 
observation is that because the peak at time T is A and the schedule s cost increases by at most p -1 
until T , always t < Tpeak -A + h. Now let us first, assume that /3(w) < A. Then if A < P(4h), Tc,, 5 
t+h+w 5 t+5h 5 Tpeak-A+6h; if otherwise A > P(4h), then because $A) 2 y(A) > 2h, both of w/2 and 2w -p(w) 
are bounded by ,0-l (A) -2h, so that Tfi,, 5 1 + h + max{lu/2,2w -a(w)} 5 t+h+P- (A) -2h < Tpeak - A 
+ P- (A) Hence always Tpeak - Te,, 2 A -max{,@ (A),6h} = ;Y(A) if p(w) < A, and in the opposite case 
Tpeak -Tfi, 2 P(w) -a(w) 2 A-FIW. cl For any T E N, r-fold application of the previous lemma yields A 
=+ y(A) =+ . . . q $ (A), provided that T( )(A) 2 2h. Hence A ?k+!) $ (A) -2h. (17) Now let T = [C/(p-l)] 
+ 1, where C is the total cost of the schedule, and denote by wlrlax the maximal size of an allocated 
chunk; clearly then C 1 n/w,,,. Using (17) together with Lemma 2, and that p(w,,,) -20,~~ = Y(P(wmax))> 
we can bound the waste by where i = max{ h + E, 6h). This formula is minimized for T = y* (p(w,,,), ?z), 
and it remains to show This results from a tricky calculation (omitted here) us-ing that, for all i E 
N and 2 > 0, Y(~- )(x) 5 p(~/i). This ends the proof of part (b) of the Main Theorem. A final remark 
is concerned with the significance of this lower bound. Indeed, Lemma 3 requires the full power of the 
adversary model and it seems that weaker (maybe more realistic) adversaries might not suffice to establish 
a similarly strong result. While this is certainly true for very large p, it can be proven that, for 
example, the (expected) bound (4) of Section 4.2 is indeed optimal for adversaries of the type time(w) 
t N(w, a2w). Acknowledgments: The author is greatly indebted to Volker Priebe for kindly sacrificing 
so much of his valu- able time for proof reading and discussions. She thanks Torben Hagerup for introducing 
her to the subject. And she loves Elisa. REFERENCES DURAND, M. D., AND JALBY, W., Load balancing perfor-mance 
of dynamic scheduling on NUMA Multiprocessors, IEEE fians. Par. Distrib. Syst. (1996), pp. 1201-1214. 
PI FLYNN, L. E., AND HUMMEL, S. F., A mathematical analysis of scheduling parallel loops in decreasing-size 
chunks, man-uscript. A preliminary version is available as IBM Research Report No. RC 18462, Oct. 1992. 
[31 FLYNN, L. E., HUMMEL, S. F., AND SCHONBERG, E., Factor- ing: A method for scheduling parallel loops, 
Comm. Assoc. Comput. Mach. 35:8 (1992), pp. 90-101. [41 GRIMMETT, G. R., AND STIRZAKER, D. R., Probability 
and Random Processes (2nd ed.), Oxford University Press, Ox- ford, 1992. [51 HAGERUP, T., Allocating 
Independent Tasks to Parallel Pro-cessors: An Experimental Study, J. Parallel Distrib. Comput. 47 (1997), 
pp. 185-197. [61 HUMMEL, S. F., SCHMIDT, J., UMA, R. N., AND WEIN, J., Load-sharing in heterogeneous 
systems via weighted factor-ing, in Proc. 8th Annual ACM Symposium on Parallel Algo-rithms and Architectures 
(SPAA 1996), pp. 318-328. [71 ill KRUSKAL, C. P., AND WEISS, A., Allocating independent sub- tasks on 
parallel processors, IEEE TYans. Software Eng. 11 (1985), pp. 1001-1016. WI I<UCK, D. J., AND POLYCIIRONOPOLJLOS, 
C. D., Guided self- scheduling: A practical scheduling scheme for parallel super-computers, IEEE ??ans. 
Comput. 36 (1987), pp. 1425-1439. [9] Lucco, S., A Dynamic Scheduling Method For Irregular Par-allel 
Programs, in Proc. SIGPLAN Conference on Program- ming Language Design and Implementation (1992), pp. 
200- 211. [lo] Nr, L. M., AND TZEN, T. H., Dynamic loop scheduling for shared-memory multiprocessors, 
in Proc. International Con-ference on Parallel Processing (ICPP 1991), pp. II 247-11250. [ll] PETROV, 
V. V., Limit theorems of probability theory, Oxford University Press, Oxford, 1995. 
			