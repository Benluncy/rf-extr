
 Improving TCP Throughput over Two-Way Asymmetric Links: Analysis and Solutions Lampros Kalampoukast 
Anujan Varma K. K. Ramakrishnan Bell Laboratories Computer Engineering Department AT&#38;T Labs -Research 
Lucent Technologies University of California Florham Park, NJ 07932 101 Crawfords Corner Rd. Santa Cruz, 
CA 96064 kkrama@rcsearch.att.com Holmdel, NJ 07733 varma@cse.ucsc.edu lamprosk@bell-labs.com Abstract 
1 Introduction The sharing of a common buffer by TCP data segments and The Transmission Control Protocol 
(TCP) has become the acknowledgments in a network or internet has been known most widely used transport-layer 
protocol today, due to the to produce the effect of aclc compression, often causing dra-explosive growth 
of the TCP/IP Internet in recent years. matic reductions in throughput. We study several schemes An important 
component of TCP is the collection of algo- for improving the performance of two-way TCP traffic over 
rithms used to perform congestion control and recovery [l, asymmetric links where the bandwidths in the 
two direc-21. These algorithms give rise to a variety of interesting dy-tions may differ substantially, 
possibly by many orders of namics, some of which have been studied extensively [3, 4, magnitude. These 
approaches reduce the effect of ack com-5, f-31. pression by carefully controlling the flow of data packets 
In this paper, our interest is in the performance of TCP and acknowledgments. We first examine a scheme 
where traffic over asymmetric access links in the presence of two- acknowledgments are transmitted at 
a higher priority than way traffic. We define two-way or badirectional traffic as the data. By analysis 
and simulation, we show that prioritizing traffic pattern resulting from two or more TCP connections 
acks can lead to starvation of the low-bandwidth connection. transferring data in opposite directions 
between the same Next, we introduce and analyze a connection-level backpres-pair of end nodes over a 
network path. The TCP segments sure mechanism designed to limit the maximum amount of transmitted by 
the connections in one direction share the data buffered in the outgoing IP queue of the source of the 
same physical path with the acknowledgments (acks) of con- low-bandwidth connection. We show that this 
approach, nections in the opposite direction. These data segments and while minimizing the queueing delay 
for acks, results in un-acknowledgments may share a common queue in the end sys- fair bandwidth allocation 
on the slow link. Finally, our tems as well as network switches/routers. This sharing has preferred solution 
separates the acks from data packets in been shown to result in an effect called a&#38; compression, 
the outgoing queue, and makes use of a connection-level where acks of a connection arrive at the source 
bunched bandwidth allocation mechanism to control their bandwidth together [6, 71. The result of ack-compression 
is a marked shares. We show that this scheme overcomes the limitations unfairness in the throughput received 
by the competing con-of the previous approaches, provides isolation, and enables nections, and reduced 
overall throughput compared to what precise control of the connection throughputs. We present could be 
expected without this effect [7]. Aclc compression analytical models of the dynamic behavior of each 
of these may occur either at the end system or in a switch/router. approaches, derive closed-form expressions 
for the expected In either case, the smooth flow of acknowledgments to the connection efficiencies in 
each case, and validate them with source is disturbed, potentially resulting in reduced overall simulation 
results. throughput. Keywords: Transmission Control Protocol (TCP), two-Some of the reduced throughput 
in datagram networks way traffic, ack compression, asymmetric TCP, performance. under two-way traffic 
could be attributed to the bunching of acks at the bottleneck link, behind data packets. Since the +This 
work was done while the author was a Ph.D candidate acks typically take less time to process in the 
routers as com- at the Computer Engineering Department, University of California, pared to data packets, 
the former tend to become bunched Santa Cruz. This research was supported by the Defense Advanced Research 
as they travel through the network. However, even when Projects Agency (DARPA) under Contract No. F19628.96-C-0038 
routers and their links have adequate bandwidth, undesir-and by the NSF Young Investigator Award No. 
MIP-9257103. The able interaction between bidirectional connections can still OPNET modeling tool used 
for simulations was donated to us by Inc. occur in the end systems, leading to ack compression and MIL-3, 
 throughput loss [S]. This is due to the sharing of a com- permission 10 make di9ital or hard copier 
of all or pan of this work for mon queue by data packets and acknowledgments in the personal or classroom 
we is prantsd without fee provided that copies are not made or distributed tar profit or commercial 
sdvsn-end systems. With asymmetric link speeds, the effect of tage and that copies bear this notice and 
the full citation on the firs1 page ack compression is more pronounced at the end system with To copy 
otherwise. 10 topublish, lo post on sar~ers or 10 the lower-speed upstream channel. With a non-preemptive 
redistribute to lists. requires prior specific permission and/or a fee. SIGMETRICS 98 Madison. WI. USA 
FIFO scheduling policy in the end system, acknowledgments B 1999 ACM 0.99791.99%3/98/0006...S5.00 of 
the fast connection may be queued for a long time behind a data packet being transmitted on the slow 
link, causing a large number of acknowledgments to be bunched. The genesis of ack compression can be 
traced to the slow- start phase of the TCP connection that increases the window low link (bandwidth = 
p,, prop. delay = LA>) progressively at startup [I]. The slow-start algorithm sets fast link (bandwidth 
= p3, prop. delay = D, the initial window size to one and increases it by one with every acknowledgment 
received. This effectively doubles the Figure 1: Network configuration. window every round-trip time. 
Thus, during slow start, the receipt of every ack causes the end system to add two seg- ments to its 
outgoing queue. Since the outgoing queue is usually maintained in FIFO order, these two segments must 
be transmitted before an ack to the connection in the op-posite direction can be sent. In addition, when 
the acks to the two transmitted segments arrive after a round-trip de-lay, with no data segments in between, 
four data segments are transmitted in response, which also appear back-to-back. Likewise, the acks of 
the reverse connection are queued be-hind the data segments, causing them also to be bunched. This behavior 
can persist in steady state when the windows reach their final values [8]. In a previous paper [8], we 
analyzed the effect of ack compression resulting from the sharing of a common queue in the end systems, 
and developed models for estimating the throughputs of the TCP connections under two-way traffic. In 
this paper, we consider several solutions to the problem in asymmetric environments where the link speeds 
in the two directions differ significantly. Asymmetric link speeds are likely to be common in the future, 
with the widespread de-ployment of cable and high-speed DSL (Digital Subscriber Line) [9] access networks 
to homes and businesses. These access networks have substantially higher speed in one di-rection than 
the other. The asymmetry in link bandwidth can have a dramatic effect on the performance of the con-nection 
going through the faster down-link. The solutions studied in the paper attempt to control the queueing 
delays of acks in the end system with the slow outgoing link. For each of the solutions, we develop detailed 
analytical models that capture the dynamics of the system and compute the expected throughput efficiency 
for each connection. The so-lutions include providing priority to acks over data packets queued at the 
end node; applying back-pressure to limit the number of data packets transmitted by an end node prior 
to transmitting an ack; and finally, a policy that provides connection-level bandwidth allocation and 
exploits knowl-edge of the asymmetric link speeds. Lakshman et. al. [lo] have also investigated the effect 
of link asymmetry on TCP performance, primarily in the context of networks with one-way TCP traffic (only 
acks are transported over the slow link) and limited buffering. In [lo], the effect of the cumulative 
nature of TCP acknowl-edgements on the burstiness of transmitted data traffic is analyzed in the presence 
of ack losses; and guidelines for the proper sizing of the bottleneck buffers are provided. Our interest, 
on the other hand, is in devising solutions and an-alyzing their performance in networks with turo-way 
TCP traffic. We do this in the context of environments with neg-ligible packet loss. This model is realistic 
in situations where the adapter to the slow link is directly connected to the end system so that backpressure 
can be used to avoid losses. It is also relevant when interfacing to a rate-controlled network with an 
intelligent buffer management strategy that avoids losses. In such environments, the degradation is only 
due to interactions taking place between data and acks. Therefore, our results are complementary to the 
analysis presented in POI. The remainder of the paper is organized as follows: In the next section we 
define our models of the network and the end nodes, and briefly review the dynamics of two-way TCP connections 
in an asymmetric network. In the next three sections we evaluate various solutions to the ack compres- 
sion problem in the asymmetric environment: prioritizing acks (Section 3), using backpressure to control 
the band-width allocation (Section 4) and a connection-level band-width allocation scheme. Finally, we 
conclude in Section 6 with some observations on the results. 2 Network Models and Effects of Two-Way 
TCP Traffic In this section, we describe our models of the network and the end nodes. The basic configuration 
we consider consists of two nodes i and j communicating over a network that guarantees in-order packet 
delivery and provides a fixed-bandwidth pipe for the TCP connections between the end-nodes in each direction 
(Figure 1). Since our primary focus is on the performance of two-way TCP traffic over asym-metric access 
links (especially lower speed uplinks), we can model the path connecting the end-nodes (consisting of 
links and routers) in each direction by single point-to-point links with known delays. The data packets 
transmitted by the TCP connection in one direction share a common FIFO queue with the acks of the opposite 
connection within the IP layer in the outbound direction. We refer to the TCP connection transferring 
data from node i to j as connection i and the connection in the opposite direction as connection j. We 
denote by Dij the constant delay seen by TCP data packets transmitted from i to j, and by Dji the delay 
seen by packets from j to i. We assume that the data packets transmitted by each con-nection are of constant 
size. The bandwidth capacities of the links originating at nodes i and j will be denoted by p; and pj, 
respectively, in units of TCP data packets per sec-ond. We assume that the link from j to i has larger 
capacity compared to that in the opposite direction, that is, pj > pi. For convenience, we often refer 
to connection i, transmit-ting data on the slow link, as the slow connection and its originating node 
i as the slow node. Likewise, we call the opposite connection j the fast connection and its originating 
node j as the fast node. We use tf and t:, respectively, to denote the transmission time of data packets 
and acks over the slow link; similarly, we use ty and t4 to denote the trans- mission time of data packets 
and acks, respectively, over the fast link. Let cr denote tt/ty, the ratio between the packet transmission 
times in the two directions. Similarly, let io, denote the ratio of the packet transmission time to the 
ack transmission time on the slow link, that is, tf/tP. The model of each end node is shown in Figure 
2. A number of receiving and transmitting applications may re-side within each node; sending applications 
generate data for TCP connections originating at that node and receiving ones act as the sinks for data 
arriving from the opposite node. Transmitting applications are assumed to be greedy and receiving ones 
are considered to be able to absorb all received data immediately. A common process handles all TCP traffic 
within the node. The TCP process receives data segments from the network and delivers them to the receiv- 
ing application. In addition, an acknowledgment is gener- ated and added to the outgoing queue for each 
segment re-ceived. The same TCP process also handles transmission of data to the opposite node by queueing 
one or more data scg- ments from the transmitting application into the outgoing Figure 2: Model of an 
end-node used in the analysis. queue each time an ack is received from the opposite node. Finally, the 
TCP process is responsible for controlling the window growth of the sending TCP. Segments of the forward 
connection and acks to the back- ward connection share a common FIFO queue at the IP layer that is serviced 
at the transmission rate of the outgo-ing link. This common queue is key to the occurrence of ack compression 
where acks to the backward connection get bunched while waiting in the queue behind data packets of the 
forward connection (and vice versa), causing the TCP dynamics we study in this paper. We assume that 
the IP ser-vice rate is equal to transmission rate on the link. However, this is not critical if the 
acks of the backward connection and data packets of the forward connection share common outgoing queues 
all the way down the protocol stack. A higher service rate out of the IP queue reduces queueing at the 
IP layer, but the same bunching effect will now occur at the device driver or at the FIFO queue residing 
in the transmitter s data-link layer. The functionality assumed for the IP layer is simple. For incoming 
traffic the IP layer is responsible for forwarding data from the lower layer to the local TCP process. 
Since TCP processing time is assumed to be small, no queueing is required for incoming traffic at the 
IP layer. For the outgo- ing traffic, on the other hand, a queue may be built up at the IP layer awaiting 
transmission on the link, as a result of ack bunching. Thus, in the worst case, an entire window s worth 
of packets may be added to the outgoing queue in quick succession due to a bunch of acks received from 
the opposite encl. Therefore, to avoid packet losses at the source node, we assume that the IP queue 
has a size equal to the maximum window size of the sending TCP. The interaction between TCP and IP described 
above is consistent with the 4.4 BSD-Lite Unix Release [ll]. In our previous work [8], we developed 
an analytical model for estimating the connection throughputs for two-way TCP traffic in both symmetric 
and asymmetric net-work environments. It is easy to see that the sum of the windows of the two connections 
must be large enough to fill the round-trip pipe for ack compression to occur. If this condition is not 
satisfied, no persistent queueing will occur at the IP queues at both nodes in steady state. Thus, if 
Wi and Wj are the window sizes of the two connections in packets, we must have F + 7 > Dij + Dji, (2.1)1 
for ack compression to occur. We will therefore assume that this condition always holds. Under this condition 
the dynamics of two-way traffic may be summarized as follows: as the congestion window for a TCP connection 
increases, so does the number of data packets that are transmitted back-to-back as a batch. Acks for 
a window worth of data transmitted from the connec-tion flowing in the opposite direction are queued 
in the end node s queue behind a window s worth of data sent by the forward connection. The increased 
queueing delay for acks at the destination node increases the round-trip delay for that connection, As 
a result the window size for connection is never large enough to fill the effective round-trip pipe. 
Consequently, the network links remain idle while the TCP connection at the source node awaits acks that 
are queued behind a large amount of data at the destination node. Ack compression leads to periodic behavior 
of the con-nections in steady state, where each of the nodes transmits an entire window of segments as 
a single burst, followed by acks of the opposite connection. We will refer to a maximal interval of time 
during which a node transmits only data segments as a busy period of the connection originating at the 
node. The busy periods of a connection are separated by the acknowledgements of the opposite connection. 
Since the transmission time of an acknowledgment is considerably smaller than the transmission time of 
a data segment we assume the former to be zero in the analysis. We call the ratio of the throughput of 
a connection to the corresponding link capacity as its throughput efficiency, or simply eficiency. The 
following results from [8] estimate the connection efficiencies for two-way TCP traffic under ack compression. 
Theorem 1 The throughput efficiency Fi of connection i in a two-way trafic configuration with asymmetric 
link rates is given by ( 1, if Wi/pi > Wj/pj + (Dij + Dji); 7 if Wj/pj > Wi/pi + (Dij + Dji); (2,2) 2 
Wi pi) (Wi/pi + W:/pj,/+ (Dij + Dji) otherwise The first and the second cases of Eq. (2.2) occur when 
the duration of a busy period of one connection is longer than the duration of a busy period of the other 
plus the round-trip delay. In this case, the first connection achieves perfect throughput while the second 
is able to transmit only one window-full of packets during a busy period of the other connection. The 
last case of Eq. (2.2) occurs when the win- dow of each connection is not large enough to overflow a 
pipe consisting the round-trip delay plus a full window of the other connection. In this case the behavior 
is more com- plex: Each connection is now able to transmit two windows worth of packets during a period 
equal to the sum of the busy periods of the connections and the round-trip delay. A formal proof of Theorem 
1 can be found in [8]. Eq. (2.2) asserts that the throughput degradation seen by a connection transmitting 
on the fast link can be dra- matic. For example, consider the case when the capacities of the fast and 
slow links are 5 Mbits/set and 100 Kbits/sec, respectively; and the packet and ack sizes are 1500 and 
28 bytes, respectively. With a maximum window size of 64 Kbytes for each connection, and a link delay 
of zero, the efficiency of the fast connection is under 2%. This out- come can be explained as follows: 
the slow connection takes 5.1 seconds to transmit a maximum window of packets (64 Kbytes) at 100 Kbits/sec. 
Due to ack compression, all the Figure 3: Network configuration used in the simulations. acks of the 
fast connection queue up for access to the slow link at node i, behind this full window of packets, encoun-tering 
a queueing delay of 5.1 seconds. When these acks arrive at node j, the fast connection transmits a full 
win-dow of packets in response. These packets are transmitted in about 90 milliseconds down the fast 
link. Thus, the fast connection is able to transmit only for 90 milliseconds within an effective round-trip 
delay of 5.1 seconds, an efficiency of approximately 2%. 2.1 Simulation Model Our analytical results 
in the following sections are validated by extensive simulations. We used the OPNET modeling tool for 
performing detailed TCP simulations. The model of TCP used in the simulations is based on the TCP-Reno 
version. It supports the congestion control mechanism de-scribed by Jacobson [l], exponential back-off, 
enhanced round-trip (RTT) estimation based on both the mean and the vari- ance of the measured RTT, and 
the fast retransmit and fast recovery mechanisms. However, some adjustments had to be made to the TCP 
timers; since the RTT values in some of our simulations are of the order of just a few milliseconds, 
the coarse-grain timers used in Unix TCP implementations (typically with a granularity of 500 ms) would 
make the comparison of the schemes difficult. To avoid the mask-ing of the performance differences of 
TCP (which we be-lieve will eventually be the case when we have finer-grained timers) due to coarse-grain 
timers, we used double-precision floating-point arithmetic in the RTT estimation algorithm. Therefore, 
both the RTT measurements and the timeout de-lays are represented by double-precision floating-point 
num-bers. For each data segment received, an ack is generated immediately. The service rate at the IP 
layer is set to be equal to the transmission rate of the physical link. This will cause all the interactions 
between data packets and acks to be confined to the IP queue, with no queueing at lower layers. Such 
an assumption will allow us to focus on a single queue and model its dynamics. In case the bottleneck 
is at a different layer, the same analysis applies to the queue built up at that layer. The network configuration 
used in all simulations is shown in Figure 3. In our simulation models all links are full duplex. The 
transmission rates in the two directions can be set independently. The capacity of the fast link (Fast 
node + Switch-2 + Switch-l + Slow node) was set to pj = 5 Mbits/set and that of the slow link (Slow node 
-+ Switch-l -+ Switch-2 --+ Fast node) to p; = 100 Kbits/sec. The packet size for both TCP sessions was 
set to 1500 bytes and the maximum window size to 64 Kbytes. 3 Reducing Ack Compression by Prioritizing 
Acknowledg-ments An obvious solution to overcome the blocking experienced by the fast connection s acks 
in the slow node s IP queue is is to service data and ack packets selectively from the IP queue. Perhaps 
the simplest means of achieving this is to assign strict priority to acknowledgments over data pack-ets 
while servicing the outgoing IP queue. When service is non-preemptive, this results in the lowest queueing 
delay for the acknowledgments. In this section, we analyze this solu-tion and determine the connection 
efficiencies under such a scheme. It is easy to show that prioritizing acks over data pack-ets for transmission 
maximizes the throughput of the fast connection. The waiting time for a group of acks in the slow node 
is now within the transmission time of a single data packet on the slow link, as compared to an entire 
win-dow of packets in the original system. This improvement, however, is achieved at the expense of degrading 
the slow connection s throughput. With priority queueing, the slow connection will be able to use only 
the portion of the up-stream link that is not used for transporting acks of the fast connection. Thus, 
while the fast connection is free to in-crease its throughput until its acks use the entire available 
bandwidth on the slow link, the slow connection is made to adjust its throughput to the capacity left 
over on the slow link. In the extreme case, the slow link may be used ex-clusively for transporting acks, 
starving the slow connection completely. In Section 3.1 we develop a simple analytical model to analyze 
the expected throughput of the fast and slow con-nections when acks are transmitted with higher priority 
over data. In Section 3.2 we validate the analytical results with simulations. 3.1 Analysis We assume 
that the window sizes of the two connections satisfy the necessary condition (2.1) for ack compression. 
As before, let (Y denote t$/t,d, the ratio between the packet transmission times in the two directions. 
Similarly, let p denote the ratio of the packet transmission time to the ack transmission time on the 
slow link, that is, tf/tT since in steady state the departure rate of packets from the fast node becomes 
equal to the arrival rate of acks to that node. The portion of the slow link s bandwidth that will be 
used for transporting acks of the fast connection will be approxi- mately tF/t,d. The rest will be available 
to the slow connec-tion for transmitting data. Therefore, the efficiency of the slow connection under 
the priority queueing scheme is given by Fi=l-$=I-;, whentT<t,d. (3.1) 3 Note that the throughput is 
zero if tP 1 t;. More careful analysis is needed to determine the through- put of the fast connection. 
Let us first calculate the number of acks bunched behind a data packet being transmitted by the slow 
node. Since the fast node is transmitting data pack- ets at the rate of l/t;, the maximum number of acks 
that can be generated by node i during the transmission time of an outgoing data packet is tf/tf., In 
addition, this number can never exceed a full window of packets of the fast con-nection. Thus, the number 
of acks bunched behind a data packet on the slow link is given by N, = min(nj, tt/tjd), (3.2) where nj 
is the window size of the fast connection in number of packets. Immediately after transmitting the data 
packet, the slow node will begin clearing the bunched acknowledg-ments at the rate of l/t?. While this 
clearing is in progress, new acks continue to join the outgoing queue at the rate of l/t:. Thus, the 
number of acknowledgments transmitted by Connection Efficiency the slow node before starting transmission 
of the next data packet is given by l+($)+($)2+,,. J .l = Y &#38;< a =N, 3 1 1 (3.3) and the total transmission 
time of the acks is given by The elapsed time between transmission of two consec-utive data packets 
of the slow connection will be equal to tf + T,. Meanwhile, the fast connection is able to maintain a 
steady flow of data as long as acks are flowing back and its congestion window is not exhausted. The 
maximum time the fast connection will be able to sustain a continuous flow of data will be n,t,d + T,. 
Thus, the efficiency for the fast connection will be min(n,t,d, 6 + Ta = min 1 nj (P/a) Fi = - p-a+ni 
. (3.5) tf +Ta When t: 2 t;, the slow link is used exclusively for trans- porting acks of the opposite 
connection, and therefore the efficiency of the slow connection will be zero. The efficiency of the fast 
connection in this case reduces to the ratio of the data packet transmission time on the fast link to 
the ack transmission time on the slow link, that is Fj = tf/tp = /3/a.  3.2 Simulation Results We performed 
simulations to verify the analysis presented in Section 3.1, using the simulation model described in 
Sec- tion 2.1. In the first experiment, we consider the case where the acks have a size of 15 bytes. 
With this set of param- eters, the packet transmission time on the fast link will be 2.4 msecs and the 
ack transmission time on the slow link 1.2 msecs. This suggests that the slow link will be used 50% of 
the time for transporting acks from the fast connec-tion and the remaining capacity is available for 
data packets sent by the slow connection. The calculated efficiency from Eq. (3.5) is close to 92%. The 
simulation results for this case are shown in Figure 4, where the measured through-puts closely follow 
the analytical estimates. In this first experiment, giving acks priority improved the performance of 
the fast connection significantly, while allowing the slow connection to acquire a fair portion (approximately 
50%) of the slow link s bandwidth. An undesirable consequence of prioritizing acks over data segments 
is that the slow connection becomes highly sensi-tive to the ratio of the data packet transmission time 
on the fast link to the ack transmission time on the slow link. To demonstrate this, we performed a second 
experiment after increasing the ack size to 28 bytes, keeping the data packet size the same. This caused 
the transmission time of acks on the slow link to be only slightly lower than that of data packets on 
the fast link. This has no significant effect on the fast connection, but the throughput of the slow 
connection is severely degraded, as shown in Figure 5. If we further in-crease the ack size to 40 bytes, 
the ack transmission time on the slow link will now exceed the packet transmission time on the fast link, 
and the slow connection will be completely starved (Figure 6). Note that 40 bytes is close to the ac-tual 
ack size when the IP header compression is turned off and no compression takes place at the data-link 
layer. The 0 5 10 15 20 25 30 35 40 45 50 Time (sets) Figure 4: Throughput efficiencies of the two connections 
with priority queueing in end systems (Window size = 64 Kbytes, packet size = 1500 bytes, and ack size 
= 15 bytes). Connection Efficiency 0 5 10 15 20 25 30 35 40 45 50 Time (sets) Figure 5: Throughput efficiencies 
of the two connections with priority queueing in end systems (Window size = 64 Kbytes, packet size = 
1500 bytes, and ack size = 28 bytes). efficiency of the fast connection in this case (approximately 75%) 
is determined by the ack transmission time and not by the capacity of the fast link. Thus, in summary, 
priority queueing of acks can im-prove the throughput of the fast connection, but may make the performance 
of the slow connection considerably worse. Thus, to find a general solution to the problem, we must look 
beyond simple priority queueing to a scheme that pro-vides some flexibility in controlling the throughput 
of both connections. We continue this investigation in the next scc- tion by evaluating the use of backpressure 
in the outgoing IP queue. 4 Use of Backpressure in Controlling Ack Compression A more general solution 
to controlling the queueing delay of outgoing acknowledgments is to limit the number of data packets 
in the outgoing IP queue by applying backpressure to the TCP layer. Such an approach is realistic in 
con-figurations where the network adapter to the slow link is directly attached to the end-system. It 
is also appropriate in networks where the buffer occupancy is controlled by us-ing buffer management 
schemes that avoid losses [12]. The backpressure approach has the potential to control of the throughput 
efficiency of the connections in both directions by varying the maximum allowable number of data packets 
in the outgoing IP queue. In this section we analyze the behavior of the TCP connections under such a 
scheme, de-rive their throughput efficiencies, and validate the results by simulation. We show that applying 
backpressure to data Figure 6: Throughput efficiencies of the two connections with priority queueing 
in end systems (Window size = 64 Kbytes, packet size = 1500 bytes, and ack size = 40 bytes). Connection 
Efficiency 1 , I I 1 t 1 I 1 I 0.9 0.6 _ : .: - :: : Fasticoruwtion Slow,connection P k -+--- -- 0.7 
0.6 0.5 0.4 0.3 0.2 0.1 - 0 , , I, ,I , ,, 0 5 10 15 20 25 30 35 40 45 50 Time (sets) packets affords 
greater flexibility than the simple priority scheme of the last section, but still makes the throughput 
of the slow connection highly sensitive to the network and connection parameters. The backpressure scheme 
we consider operates as follows: The transmitting TCP process in each node is allowed to send data packets 
to the IP layer as long as the backpressure threshold is not exceeded. When the backpressure threshold 
is reached, the transmitting TCP is suspended until space is available in the IP queue. Acks can be sent 
at any time. The backpressure threshold can be set taking into account the target throughput efficiencies 
of the fast and slow con-nections. We will now analyze this backpressure scheme with re-spect to the 
asymmetric network model of Figure 1 with two connections. We will derive simple analytical expressions 
for the throughput efficiencies of the fast and slow connections as a function of the link capacities 
in each direction and the parameters of the two connections. We will first ignore the link propagation 
delays in our analysis, but will later extend the analysis to cover non-zero propagation delays. 4.1 
Analysis for Single-Packet Backpressure Threshold We first consider the simple case when the backpressure 
threshold is set to one data packet, and then extend the re-sults to the general case in Section 4.2 
for multiple packets. To derive the throughput efficiencies of the connections, we need to determine 
the queueing delay seen by a data packet in the outgoing IP queue. Otherwise it is easy to see that the 
backpressure scheme has no effect within the slow node i. Figure 7 illustrates the evolution of the outgoing 
IP queue in i. The data packets are numbered sequentially from 0. Assume, for simplicity, that the slow 
and fast connections open simultaneously. When the backpressure threshold is set to one packet, the queueing 
delay seen by any data packet is the time re-quired to transmit acknowledgments accumulated in front 
of it. Thus, to calculate the queueing delay, we must deter-mine the number of bunched acks in front 
of it. This can be calculated iteratively as follows: Let us denote by Q the queueing delay of the kth 
data packet added to the outgoing IP queue of node i. Let Qp = 0, as shown in Figure 7(a). While packet 
0 is under service, node i keeps receiving data packets of connection j at the rate of l/t,d and an acknowl- 
edgment is added to the outgoing IP queue for every packet received. Thus, the number of acks accumulated 
behind    A=O. time (4 t/me Q _ Qf + tf ,m t! Figure 7: State of node i s IP queue for a threshold 
equal to one packet. packet 0 is given by tt/t$ = LY. The state of the IP queue just when packet 0 completes 
transmission is shown in Fig- ure 7(b). Transmission of packet 1 can begin only after all these acks 
are cleared from the queue. Therefore, the queueing delay of packet 1 is given by the transmission time 
of acks accumulated behind packet 0, and is given by Q; = St; zz ; tf. 3 0 Acknowledgments entering 
the outgoing IP queue after packet 1 will be delayed not only by the transmission time of packet 1, but 
also by the transmission time of the acks accumulated in front of it. Therefore, the queueing delay of 
the first ack accumulated behind packet 1 will be Qi + tf, and the number of acks that will be bunched 
together behind packet 1 will be Qt+tl t, Consequently, the queueing delay of packet 2 is given by &#38;? 
= Qt + t$,, 2 ---q-a This is illustrated in Figure 7(c). The behavior for subse-quent packets is similar. 
Thus, the queueing delay for the lath data packet is given by Qk = Of- + tf t (4.1) tj Expanding Eq. 
(4.1) as a series, (4.2) If o/p < 1, the queueing delay will converge to a steady- state value given 
by Qf+(;)m-l) =t:&#38;. (4.3) On the other hand, if o//3 2 1, then acknowledgments for an entire window 
of connection j will be accumulated behind each data packet in the outgoing IP queue of node i. The steady-state 
queueing delay of a data packet in node i s queue is then given by Qf = njt:, (4.4) where nJ is the size 
of connection j s window in number of packets. Knowing the steady-state queueing delay of outgoing data 
packets in node i, we can compute the throughput ef-ficiencies of both connections: Theorem 2 The throughput 
eficiencies of connections i and j under the single-packet backpressure scheme are given by if o/p < 
1; (4.5) otherwise. if o/P < 1; (4.6) otherwise. Proof: The efficiency of each connection is simply the 
fraction of time during which the originating node transmits its data packets. Since each data packet 
of the slow connec-tion, in steady state, experiences a queueing delay of Q , the efficiency of the slow 
connection will be Fi = tP Q +td t L Substituting for Q from equations (4.3) and (4.4) for the cases 
of o/p < 1 and o/p 2 1, respectively, this becomes (4.7) The efficiency of the fast connection can be 
computed by calculating the maximum delay seen by its acknowledgments in the outgoing IP queue of the 
slow node. This maximum delay is the queueing delay of a data packet plus its trans-mission time, that 
is, Qf + tf. The fast connection may transmit its entire window before receiving an ack. Thus, the efficiency 
of the fast connection will be Fi =min(I,&#38;), Again, substituting for Q from equations (4.3) and (4.4) 
for the cases of o/p < 1 and cu/p 2 1, respectively, this becomes min (l,nj(k -i)) , if o/p < 1; Fj = 
(4.8) otherwise. This concludes the proof of Theorem 2. 4.2 Analysis for Multiple-Packet Backpressure 
Threshold We can now generalize Theorem 2 for the case when the backpressure threshold is set to more 
than one data packet. Let bi and 6j denote the backpressure thresholds in the out- going IP queues of 
nodes i and j, respectively. As before, t; tf Batch 0 - time (4 Batch 0 -ooood- time (b) Batch 2 Figure 
8: Example evolution of node i s outgoing IP queue when the backpressure threshold is two packets. we 
need to calculate the queueing delay of data packets in the outgoing queue by computing the number of 
acks trans- mitted between data packets. To aid this computation, we can group the data packets transmitted 
out of node i into batches, with the number of packets in each batch equal to the backpressure threshold 
bi. Figure 8 illustrates the evo-lution of the outgoing IP queue in node i, for bi = 2. The first batch 
of data packets, batch 0, consists of packets 0 and 1. Since the queue is initially empty, these packets 
will be transmitted back-to-back with no intervening acks (Fig- ure 8(a)). During the next batch, packets 
2 and 3 are trans- mitted. However, during the transmission time of packet 0, a number of acks equal 
to tt/tj = (Y will be added to the IP queue and will be queued between packets 1 and 2 (Figure 8(b)). 
Thus, packet 2 will undergo an additional queueing delay of at: because of these acks. Similarly, dur-ing 
the transmission time of packet 1, a! acks will be queued behind packet 2. Thus, it can be seen that 
every data packet in batch 1 will have a bunch of Q: acks queued ahead of it. When the first packet of 
the next batch, packet 4, arrives into the queue, the number of acks accumulated in front of it is determined 
not only by the transmission time of packet 2, but also by the time needed to clear the acks in front 
of packet 2. This is evident from Figures 8(c) and (d). Let T, denote the total transmission time of 
the acks in front of a packet in the kth batch in node i s IP queue. Then, the number of acks accumulated 
in front of packet 4 will be (T: + tf)/t,d, where T,! is the total transmission time of the acks accumulated 
in front of any packet in the previous batch. Therefore, the total transmission time of the acks in front 
of packet 4 is given by T2 = CT,! + 6 ta, 2 t; z Proceeding similarly, we can express T) in terms of 
T,k- by the recurrence Tk = T,k- + tf t; = $ T;- + cd;. 1 (4.9) tj 0 Note that this is the same recurrence 
as in Eq. (4.1). There-fore, the steady-state value of T, is is given by TS = tt&#38;, if o/p < I; (4.10) 
, njtt, otherwise, In order to compute the maximum queueing delay of a data packet in node i s outgoing 
queue, we need to consider the transmission times of both data packets and acks ahead of them. At most 
(bi -1) data packets can be in the IP queue when a new packet arrives, contributing to a delay of (bi 
-l)tt. An additional delay of biTis is incurred in clearing the acks. Thus, using Eq. (4.10), the maximum 
queueing delay of a data packet in node i s queue in steady state is given by given by Lj = (Dij + Dji 
+ tf)/ max(t,d, t:). Consequently, at most Xj = nj -Lj acks may be queued behind a data packet in the 
slow node s outgoing IP queue. The efficiency of the fast connection can thus be determined by replacing 
nj with Xj in Eq. (4.6) derived for the zero-delay case. The second case occurs when the window size 
of the fast connection is not able to fill the round-trip pipe. Since the data packets from the slow 
connection are subject to back- pressure, the efficiency of the fast connection in this case is simply 
(Tljt,d)/(LJ max(t,d, t:)). The efficiency for the slow connection can be derived directly from that 
of the fast connection by computing the percentage of time the slow link will be transmitting acks of 
the fast connection. The connection efficiencies calculated taking into account the link delays are summarized 
below: tt&#38;+(bi-l)tf, ifcu/,B<l; &#38;" zz (4.11) ,nt; + (bi -l)t:, otherwise. Knowing the queueing 
delay, the throughput efficiencies of the connections can be determined by proceeding as in the single-packet 
backpressure case. Theorem 3 When the ba&#38;pressure thresholds in the nodes i and j are bi and bj, 
respectively, the throughput eficiencies of connections i and j are given by (4.12) min (1, z(i -+)), 
if o//3 < 1; Fj = njP (4.13) otherwise. (nj+Obt)u > The proof of this theorem is similar to that of 
Theorem 2 and is therefore omitted. We can make some important observations from this theorem. The efficiency 
of the fast connection can be controlled by adjusting the backpressure threshold in the slow node s IP 
queue. A higher thresh-old, in general, increases its throughput. More important, however, is the fact 
that the efficiency of the slow connec-tion is insensitive to the backpressure thresholds bi and bj. 
This establishes the important result that the backpressure scheme cannot overcome the basic weakness 
of the priority scheme in the last section, namely its inability to control the throughput of the slow 
connection. We provide further evidence of this behavior in Section 4.4 using simulation. 4.3 Effect 
of Link Propagation Delay So far we ignored link delays in our analysis. In this sec-tion we show that 
non-zero link propagation delays do not change the fundamental behavior of the connections when backpressure 
is used to limit the number of data packets queued in the nodes outgoing IP queues. For simplicity, we 
consider only the single-packet backpressure threshold; extension to the multiple-packet case is straightforward 
and is omitted because of space constraints. With non-zero link delays, the efficiency of the fast con-nection 
depends on its window size and the round-trip delay, among others. We need to consider two distinct cases: 
In the first case, the window size of the fast connection exceeds the bandwidth-delay product of the 
round-trip pipe plus the transmission time of a single packet from the slow connec-tion. In this case, 
the maximum number of packets and acks of the fast connection that can be in transit at any time is l-$, 
ifa//3< 1 andnj > Lj; F = l-smin(l,E), otherwise. I (4.14) F, = min (l,Xj(d -i)) , if cr/p < 1 and nj 
> L,; 3 2 min (1, t) , otherwise. i (4.15) We have verified these analytical results with simula-tions. 
However, since the basic behavior of the connections was found to have little sensitivity to link delays, 
we show results only for the case of zero propagation delays in the next section. 4.4 Simulation Results 
To validate the results from our analysis of the backpressure scheme, we performed simulations using 
the simple asym-metric configuration of Figure 1 with the same parameters used previously. The size of 
acknowledgments was set as 28 bytes after IP header compression [13], and including link layer overhead. 
The backpressure threshold is set to one data packet. For the first experiment, the size of data packets 
for both connections was set to 1500 bytes. Thus, nj = 43 packets, ,0 = 54, and a = 50. Using Theorem 
2, the expected ef-ficiency of the slow connection i should be approximately 56% and the efficiency of 
the fast connection j about 48%. Figure 9 shows the efficiency for the two active TCP connec-tions from 
simulation, matching our analysis. In this specific example, the transmission time of a window s worth 
of data from the fast connection is approximately the same as the transmission time of one data packet 
for the slow connec-tion. Therefore, all the acks for data packets transmitted from the fast node j will 
be queued behind a single data packet in the slow node i, as in Figure 7. The transmission time for all 
the acks is about the time to transmit a sin- gle data packet on the slow link, resulting in approximately 
50% efficiency for the slow connection. The efficiency of the fast connection j is significantly improved 
because of the backpressure mechanism. The exact efficiency of the slow connection depends on the amount 
of data transmitted from the slow connection for a window s worth of acks from the fast connection. Thus, 
re-ducing the size of data packets for the slow connection will have an even more significant impact 
on its performance The efficiency of the two TCP connections, when the size of data packets for the slow 
connection is reduced to 256 bytes is shown in Figure 10. The efficiency of the fast connection improves 
further, but that of the slow connection plummets. This is because a large portion of the slow link s 
bandwidth Connection Efficiency Connection Efficiency - 0 5 IO 15 20 25 30 35 40 45 50 Time (sets) Figure 
9: Efficiencies of the two connections under the back- pressure scheme (Max window size = 64 Kbytes, 
packet size = 1500 bytes, ack size = 28 bytes). Connection Efficiency Slow cpnnection 2 + -.. Slow cfJnnec~n3 
h+-. ,.. ..L ,. j ,.. : i 0 5 10 15 20 25 30 35 40 45 50 Time (sets) (4 Time (sets) Figure 10: Efficiencies 
of the two connections under the back- pressure scheme with 1500-byte packets for the fast conncc-tion 
and 256-byte packets for the slow connection (Max win-dow size = 64 Kbytes, packet size = 1500 bytes, 
ack size = 28 bytes). Connection Efficiency 0 5 10 15 20 25 30 35 40 45 Time (sets) (b) Figure 11: Efficiencies 
of six connections (three with a slow upstream link and three with a fast one) with the backpressure 
scheme for 64 Kbyte maximum window size, 1500-bytes packet size for the fast connection, 256-byte packets 
for the slow one, and 28 bytes ack size. (a) Efficiencies of individual connections and (b) aggregate 
efficiencies of connections in each direction. is used to carry acks for the fast connection. The analyti-cally 
derived efficiencies would have been 17.2% and 87%, matching the simulation results quite well. Notice 
that the dynamics of the backpressure mechanism are independent of the number of connections that have 
been setup in each direction and also independent of their rela-tive timing. That is, the exact time 
when a new connection opens does not affect the fundamental system dynamics. To illustrate this point 
we simulated a situation where three connections were started in each direction in a staggered fashion, 
with a delay of 1 second between successive con-nections. Each connection is backpressured independently. 
The rest of the system parameters are the same as for the re- sults shown in Figure 10: the data packet 
size is 1500 bytes and the ack size is 28 bytes. Figure 11(a) shows the effi-ciency achieved by each 
individual connection when three connections are set up in each direction. Observe that, in a given direction 
each connection gets a fair portion of the bandwidth available for data packets. The total efficiencies 
for the fast and slow connections (the sum of the efficiencies for connections transmitting in a given 
direction) are shown in Figure 11(b). The aggregate behavior is similar to the case where there is a 
single TCP session operating in each direction, shown in Figure 10. To demonstrate the sensitivity of 
the performance of the slow connection to the number of acks generated for a win- dow of the fast connection, 
we reduced the size of the data packets sent by the fast connection to 512 bytes. This caused the number 
of acks for a window worth of data sent by the fast connection to triple. Observe in Figure 12 that the 
effi- ciency of the slow connection is now only about 6%. That is, only about 6 Kbits/sec out of the 
100 Kbits/sec link capac-ity is available to the slow connection for transporting data packets. The remaining 
capacity is used for carrying acks for the fast connection. More importantly, the efficiency of the fast 
connection is also hurt. However, this is for a dif- ferent reason: the transmission of a 512 byte packet 
over the fast link takes approximately 0.8 msecs while the trans-mission of an ack over the slow link 
takes about 2.2 msecs. Therefore, the ack transmission time on the slow link con-trols the throughput 
for the fast connection. This limitation however is simply due to the asymmetry in link speeds, and is 
not just due to bi-directional TCP effects. This suggests that the packets on the fast down-link should 
be sufficiently large to limit the bandwidth demand for sending acks on the slow up-link, even in the 
uni-directional case. The simulations suggest that backpressuring the data can improve the efficiency 
of the fast connection. However, it results in an undesirable sensitivity of the two connections to each 
other s parameters, such as packet and ack sizes. A significant change in the packet size for the one 
connection, for example, results in a change in the throughput achieved by the other connection. There 
is no simple means by which the slow connection can get a fair portion of the capacity of Connection 
Efficiency 0 5 10 15 20 25 30 35 40 45 50 Time (sets) Figure 12: Efficiency for the two connections 
computed in 3 second intervals for 64 Kbytes maximum window size, 512 bytes packet size for the fast 
connection, 256 byte pack- ets for the slow connection, and 28 bytes ACK size (data packets are backpressured). 
the slow link even though acks and new data packets have equal priority, without undue dependence on 
variables that we cannot control, such as the packet size. Connection-Level Bandwidth Allocation Both 
priority queueing of acknowledgments and the use of backpressure are able to mitigate the effects of 
ack compres- sion, but are inherently unfair to the slow connection. Both schemes make the throughput 
of the slow connection highly sensitive to parameters of the fast connection, such as its packet size 
and window size. An ideal solution to the ack compression problem should allow control the throughput 
efficiencies of both the slow and fast connections. That is, such a scheme should maintain the slow link 
always fully utilized but allow control of the fraction of the data packets to acks transmitted on it. 
None of the previous schemes al- low this flexibility. In this section, we propose and evaluate a simple 
connection-level bandwidth allocation scheme that achieves this objective. The basic goal of our scheme 
is to provide a guaranteed minimum efficiency to the slow connection while providing the maximum possible 
throughput for the fast connection under this constraint. This is achieved by limiting the max-imum number 
of acks a node is allowed to transmit before transmitting a data packet when one or more data packets 
are queued. This controls the fraction of time the slow link is used for transmitting acks when data 
packets are waiting, and vice-versa. Such a scheme can be implemented using two queues -one for data 
packets and the other for acks -and using a scheduler to make the choice between the two queues for transmission. 
A fair-queueing scheduler can be used to perform this task, but we can avoid the complex- ity of the 
scheduler by resorting to a simple counter-based implementation: This implementation simply counts the 
to- tal number of bytes transmitted in a sequence of acks, and will force the transmission of a data 
packet if that number exceeds a preset threshold representing the desired ratio of data to ack bandwidth. 
Similarly, after the transmission of a data packet, waiting acks are given priority over data until the 
threshold on acks is again reached. The pseudocode presented in Figure 13 illustrates how such a mechanism 
can be implemented with two counters data-bytes-tz and ack-bytes-&#38; that keep track of the dis-crepancy 
between the number of data and ack bytes trans-mitted, and two flags indicating the presence of data 
or acks if (system empty) then wait for next arrival; if (data-present) and (! ack-present) then { ack-bytes-tx 
t 0; data-bytes-tx t data-pktsize; /* in bytes */ send( data packet ); } if (ackpresent) and (! data-present) 
then { data-bytes-tx t 0; ack-bytes-tx t acksize; /* also in bytes */ send (ack); } if (data-present) 
and (ack-present) then { if (ack-bytes-tx > data-bytes-tx) then { /* more acks transmitted than data 
*/ send (data packet); data-bytes-tx += data-pktsize; if (data-bytes-tx > ack-bytes-tx) then { data-bytes-tx 
-= ack-bytes-tx; ack-bytes-tx = 0; } } else { /* more data transmitted than acks */ send (ack) ; ack-bytes-tx 
+= acksize; if (ack-bytes-tx > data-bytes-tx) then { ack-bytes-tx -= data-bytes-tx; data-bytes-tx = 0; 
} } } Figure 13: Example pseudocode of the bandwidth allocation mechanism to guarantee a mininum of 
50% allocation to data packets sent from the slow connection. in the two outgoing queues (data-present 
and act-present). The pseudocode shown is executed whenever the transmis- sion of a data packet or ack 
is completed. The implementa-tion shown in Figure 13 assumes that available bandwidth on the slow link 
will be shared equally among data pack-ets and acks. If data and acks are to be allocated different fractions 
of the available bandwidth, then the ack-byte-ts counter needs to be scaled appropriately. For example 
if we want to guarantee 80% of the bandwidth to data from the slow connection then the a&#38;byte-tz 
counter must be multiplied by a factor of four. The objective of the mech-anism is to minimize the difference 
between the two coun-ters, data-bytes-tx and ack-bytes-tx. After sending a data packet, the counter data-bytes-tz 
is increased by the length of the packet. If the updated count crosses the value in ack-bytes-&#38; both 
counters are decreased by ack-bytes-tx so as to bound their range. The operations after sending an ack 
are symmetric. It can be shown easily that the above mechanism is able to provide a throughput guarantee 
to the slow connection. Let f denote the fraction of the capacity of the slow link that is to be allocated 
to the slow connection s data. The remaining fraction 1 - f is then available for transporting the acks 
of the fast connection. Given f, we can compute the threshold on the number of acks, y, which triggers 
the transmission of the next data packet, from the following equality: td L = f, tf + rt; which gives 
 y - (1 - f)P, (5.1) f We will now derive closed-form expressions for the through- put efficiencies 
of the two connections under this scheme. The maximum time T,,, between successive transmissions of data 
packets from the slow connection will be T maz --min(nj t:, rt:). (5.2) Therefore, the efficiency of 
the slow connection i will be 1Fi = T t% = -ax(&#38;,f) maz + tf = min(nj , y) 3 + 1 (5.3)Notice from 
Eq. (5.3) that the efficiency of the slow connec-tion is guaranteed to be at least f. The efficiency 
of the fast connection j can be computed by observing that at most min(nj, y) packets may be ac- knowledged 
within in an interval of time min(nj, -y)ty + tf. Therefore, the efficiency will be min(n, , -y)t,dFj 
= min 1, (5.4) min(n,, --y)tF + tf After some manipulation, this reduces to h (1 -f)P (5.5) a(nj + p) 
 ----k-- . We now discuss the results from a series of simulations we performed to validate the above 
analysis. In all of the simulation experiments discussed below the parameter f is set to 50%, unless 
otherwise specified. This allows the slow connection to reach a minimum efficiency of 50%. Figure 14 
considers the case where the size of data pack-ets in each direction is set to 1500 bytes. The link capacities 
arc the same as those considered in the previous section, that is, 5 Mbits/set and 100 kbits/sec. The 
window size for both connections is set to 64 Kbytes, so that nj = 43, (Y = 50, and p = 50. Therefore, 
the expected efficiency for connection i from analysis is approximately 54%, and that of connection j 
is approximately 46%. The simulation results in Figure 14 are in close agreement with these estimates. 
In this case both connections achieve acceptable performance. Note that the throughput of the fast connection 
is limited only by the bandwidth available for transporting its acks over the slow link. To test the 
sensitivity of the throughput on the parame- ters of the opposite connection, we decreased the packet 
size of the slow connection to 256 bytes, maintaining the 1500-byte packet size for the fast connection. 
The results are shown in Figure 15. Note that the efficiencies of both con-nections remain almost unaffected 
because the bandwidth needed for the additional ack traffic on the fast link is easily available. The 
slight discrepancy between the observed effi-ciency of the slow connection and its ideal value (50%) 
is due to the rounding error resulting from the size of data packets not being an exact multiple of the 
size of acknowledgments. On the other hand, use of the backpressure scheme in this Connection Efficiency 
0 5 10 15 20 25 30 35 40 45 50 Time (sets) Figure 14: Efficiencies for the two connections with connection-level 
bandwidth allocation (Max window size = 64 Kbytes, packet size = 1500 bytes, ack size = 28 bytes). Connection 
Efficiency 0 5 10 15 20 25 30 35 40 45 50 Time (sets) Figure 15: Efficiencies for the two connections 
with connection-level bandwidth allocation with 1500-byte packet size for the fast connection and 256-byte 
packets for the slow connection (Max window size = 64 Kbytes, ack size = 28 bytes). case reduces the 
efficiency of the slow connection to about 20%. In Figure 16 we consider the case where the size of con- 
nection j s data packets is also reduced to 512 bytes. With these parameters, the slow connection was 
able to get only about 7% of the link capacity when the backpressure scheme was used. On the other hand, 
with the bandwidth allocation mechanism, the throughput of the slow connection remains almost unaffected 
(Figure 16). The smaller packet size of the fast connection, however, causes a reduction in its own efficiency. 
This is because the smaller packet size causes an increase in ack traffic for the same amount of data 
transmit-ted; since the bandwidth available to ack traffic on the slow link is fixed, this results in 
a reduction in the fast connec-tion s throughput. This is in fact the desired behavior, since the throughput 
of a connection is now sensitive to only its own parameters. 6 Conclusion Asymmetric link speeds are 
likely to be common in data communication networks in the future, particularly with the deployment of 
cable and high-speed Digital Subscriber Line (DSL) technologies. With the increasing use of powerful 
PCs and workstations at homes and small businesses, it is not unrealistic to imagine users running multiple 
applications simultaneously (download files, transfer data on the uplink) that result in two-way TCP 
traffic. In such an environment, Connection Efficiency oz 6 E w 0 5 10 15 20 25 30 35 40 45 50 Time 
(sets) Figure 16: Efficiencies for the two connections with connection-level bandwidth allocation with 
512-byte packet size for the fast connection and 256-byte packets for the slow connection (Max window 
size = 64 Kbytes, ack size = 28 bytes). TCP has been known to suffer a reduction in the overall throughput 
of one or both connections due to the well-known effect of ack compression. This effect is exacerbated 
in an environment where the link speeds in the two directions are widely different. In this paper, we 
examined several schemes for improving the performance of bi-directional TCP connections, by care-fully 
controlling the flow of packets and acknowledgments, particularly on the slow link. We first examined 
a scheme that provides higher priority to acks waiting for transmis-sion on the slow link, so as to minimize 
the degradation in throughput for the fast connection. Through both analysis and simulation, we showed 
that the throughput for the fast connection consistently improves, in one case going from 2% when there 
was no control to 92% with priority queueing of acks. Unfortunately, this is achieved at the expense 
of the slow connection. In addition, as the total bandwidth con-sumed by acks on the slow link becomes 
a significant portion of its capacity, the slow TCP connection in this direction may become completely 
starved. This is clearly unaccept-able behavior. As a logical extension of the mechanism to provide pri-ority 
to acks, we then examined a scheme that limits the number of data packets in the outbound queue for the 
slow link. This form of backpressure modulates the priority given to acks by allowing the slow connection 
to place no more than a given maximum number of data packets in the out-bound queue. Our analysis accurately 
predicts the through- put for the two connections for different values of the back- pressure threshold, 
validated by simulations. The backpres-sure mechanism, while improving the throughput for the fast connection, 
again results in an undesirable sensitivity of the two connections to each other s parameters, such as 
packet and ack sizes. A decrease in the packet size for the fast con-nection, for example, results in 
a corresponding degradation of the throughput achieved by the slow connection. Finally, our preferred 
solution to the problem is to use a connection-level bandwidth allocation mechanism to over- come the 
sensitivity of the throughput of each connection to the parameters of the opposite connection. Through 
a simple counter-based implementation, we showed that it is possible to guarantee a minimum throughput 
for the slow connection, making the fast connection s throughput sensi-tive to only its own parameters. 
Lakshman et. al. [lo] have also shown that flow isolation provided by the use of an appropriate scheduling 
policy is required in order to achieve the desired degree of fairness in the network. With our work we 
show that the use of a connection-level bandwidth alloca-tion scheme is beneficial in improving the fast 
connection s efficiency by properly controlling the bandwidth allocation between data packets and acknowledgments 
over the slow link. Our contribution in this paper is thus two-fold: our ana-lytical results, and the 
simple closed-form expressions de-rived can be used to accurately predict the performance of the TCP 
connections in the two directions. Secondly, our proposal to use the simple bandwidth allocation mech-anism 
allows us to not only improve, but also to tune the throughputs of the two connections in a flexible 
way using our analytical results for guidance. References V. Jacobson, Congestion avoidance and control, 
in PI Proc. of ACM SIGCOMM 88, pp. 314-329, 1988. V. Jacobson, Modified TCP congestion avoidance al- 
 PI gorithm. message to end2end-interest mailing list, April 1990. S. Floyd and V. Jacobson, On traffic 
phase effects in 131 packet-switched gateways, Internetworking: Research and Ezperience, vol. 3, no. 
3, pp. 115-156, September 1992. J. C. Mogul, Observing [41 works, in Proc. of ACM August 1992. L. Zhang 
and D. D. Clark, [51 work traffic: A case study Research and Ezperience, December 1990. TCP dynamics 
in real net-SIGCOMM 92, pp. 305-317, Oscillating behavior of net- simulation, Intenetworking: vol. 1, 
no. 2, pp. 101-112, L. Zhang, S. Shenker, and D. D. Clark, Observations 161 on the dynamics of a congestion 
control algorithm: The effects of two-way traffic, in Proc. of ACM SIG-COMM 91, pp. 133-147, September 
1991. R. Wilder, K. K. Ramakrishnan, and A. Mankin, Dy- [71 namics of congestion control and avoidance 
of two-way traffic in an OS1 testbed, ACM Computer Communi-cation Review, vol. 21, no. 2, pp. 43-58, 
April 1991. L. Kalampoukas, A. Varma, and K. K. Ramakrishnan, PI Two-way TCP traffic over ATM: Effects 
and analysis, in Proc. of IEEE INFOCOM 97, April 1997. PI K. Maxwell, Asymmetric Digital Subscribe Line: 
In-terim Technology for the Next Forty Years, IEEE Communications Magazine, vol. 34, no. 10, pp. lOO-106, 
October 1996. WI T. V. Lakshman, U. Madhow, and B. Suter, Window-based error recovery and flow control 
with a slow ac-knowledgement channel: A study of TCP/IP perfor-mance, in Proc. of INFOCOM 97, April 1997. 
WI W. R. Stevens and G. R. Wright, TCP/IP Illustrated, vol. 2. Addison-Wesley Publishing Company, 1995. 
P21L. Kalampoukas, A. Varma, and K. K. Ramakrishnan, Explicit Window Adaptation: TCP performance, in 
Proc. V. Jacobson, Compressing speed serial links, Request February 1990. 1131 A method to enhance of 
Infocom 98, 1998. TCP/IP headers for low-for Comments: 1144,  
			