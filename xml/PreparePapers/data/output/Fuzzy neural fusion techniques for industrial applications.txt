
 Fuzzy Neural Fusion Techniques for Industrial. Applications S. K. Halgamuge and M. Glesner Darmstadt 
University of Technology, Institute of Microelectronic Systems, Germany Abstract Three different fuzzy 
neural fusion methods for real world problems are presented with results for compari- son, The fuzzy 
controlled backpropagation converges faster in comparison to the conventional backpropagation. The cascade 
system fuzzy step net extends the abiLity of inte- grating neural learning with fuzzy logic statements, 
giving an insight view over the complexity of separation of out- put classes. The neural network based 
fuzzy system genera- tor FuNe I analyses the training data automatically indicat- ing superfluous input 
and generating a knowledge base. All three systems are tested with Iris flower species classification 
data and applied to industrial problems. Keywords ]uzzy-neural, cascade systems, fuzzy step net, backpropagation, 
Iris classification, rule generation Introduction Fuzzy systems ([Zad65], [Zim85], [DHR93]) and neural 
net- works popularly described as model free estimators are im- portant areas in soft computing. The 
fusion of these tech- niques in order to enhance the effectiveness is an exciting area to explore ([BK92]). 
Fusion techniques for neural net- works and fuzzy systems can be categorized as follows: Application 
9~l.ZZ.y systems in neural networks  Cascade systems  Use of neurM networks in constructing fuzzy systems 
 In this paper three different methods from the research directions mentioned above are presented. The 
fuzzy step net and the fuzzy controlled backpropagation are new meth- ods described in the next sections. 
The third method FuNe I developed earlier by authors is also briefly described in the context of a new 
application for comparison. Application of fuzzy systems in neural networks Use of "fuzzy neurons'[GQ91] 
or "fuzzy weights" [IKT93] with modified leashing algorithms belong to this category. The first approach 
pre~sented in this paper, fuzzy controlled ba~ckpropagation describes a method for dynamic adaptation 
of network parameters using fuzzy control Computation of the weight updates in a multilayer per- ceptron 
is described in ([RM86]): Permission to copy without fee all or ~ of this material is granted provided 
that the copies arc not made or distributod for direct commercial tdvantag¢, the ACM copyright notice 
and the tide of the publication and its date appear, and notice is giwm that copying is by Permission 
of the Association for Computing Machine'Py. To copy otherwise, or to republish, requires a fee and/or 
sp¢~iilc permission. 0 1994 ACM 089791-647-6/94/0003 $3.50 OE"  w,~(~ + 1) -~Y~,7 + ~,,(0 (1) where 
w~j stands for the weight value from the jth neuron to the ith neuron, r/for the learning rate and t 
denotes the iteration count. The output of a neuron is computed using the sigmoid activation function: 
1  g(ne~,) = 1 + exp[--~ - ,~et,] (2) where neL is the sum of the weighted inputs to the neuron and/3 
the steepness parameter. The computation method mainly defined by (1) is known as gradient descent and 
its properties have been documented by many authors ([HKP91], [R.M86]). Since it is inherently slow, 
great efforts have been taken to improve the speed by introducing new parameters (momentum [PNH86], adaptive 
parameters [Jac88], individual learning rate [SAg0] etc.). However, there are only few parameters suitable 
for im- provement, and their interaction is rather complex. This is the reason why most methods, though 
based on mathemat- ical properties of the formulae, are empirically derived. The parameters considered 
are Learning Rate 7: scales the computed weight change and has therefore the biggest influence on learning 
speed." It must be large enough to ensure fast convergence towards the error minimum without overshooting 
it. Steepness t3: determines the steepness of the activation function in the sensitive area (input between 
::t:6 ap- proximately). The larger this value is, the closer the activation function gets to the step 
function, which means increased insensitivity to changing input val-ues. Adaptive methods change parameters 
in each iteration by observing one or more criteria. Commonly employed in non- fuzzy methods is the partial 
derivative of the error function with respect to the single weights. The idea is to use the change in 
the sign of the partial derivative as an indicator that the minimum was overshot. A previous paper [Jac88] 
collected a number of heuristic rules for the adaptation of individual learning rates (each weight is 
assigned its own learning rate) which form the basis of most adaptive ap-proaches. Fuzzy controlled parameter 
adaptation method Most of the suggested improvements are often derived from empirical observations. This 
is an ideal field for applica- tion of fuzzy logic, the idea being to integrate the empirical knowledge 
in a fuzzy rule base that allows an automated adaptation of the speed influencing parameters. An addi- 
tional benefit will be the fact that the derived perceptron is an auto adaptive system where the parameter 
initialization does not effect the efficiency of the learning run. The use of fuzzy logic for parameter 
adaptation has been proposed by several authors, e.g. [ACMC92], whose algo- rithm will be referred to 
as FA1, and [XWB92]). The method introduced in this paper (called FA2) is an extension of their research 
work suggesting an entirely different rule base. The major departure with this rule base is the computatio 
n of a learning rate virtually within an unlimited range. Com- pared to the non-fuzzy methods mentioned, 
the fuzzy meth- ods proposed so far use only one learning rate parameter for all weights. The fuzzy rule 
base evolved from the following qualitative observations: I. A high error means being far away from 
the minimum. Hence the learning rate should be high. 2. The change of error (CE) is the most important 
and significant measure: CZ(t) = E(t) -E(t-1). (3) As long as it is high the learning rate can be increased 
quite safely. Negative CEindicates that the minimum has been passed. 3. Since the idea is to use a learning 
rate as large as possible, it is important to know when to decrease it to avoid an overshot. The experiments 
have shown that the most reliable measure is the second change of error. At the beginning only the sign 
of this value was regarded. Positive sign means an increase in CE which in turn means it is safe to increase 
the learning rate. But it also proved beneficial to take the magnitude of this value into account because 
it contains information about the trend in CE: Even if CE remains positive for consecutive iterations, 
its decrease gives an early hint that the minimum is being approached. To be independent of the magnitude 
of CE itself the QCE-measure is introduced: QCE = CE(t)/CE(t -1) . (4) Values of QCE smaller than 1 should 
lead to a de- creasing learning rate. These statements are introduced under the assumption that the 
error surface for each weight can be approximated by a parabola, so that an approximation of the minimum 
means a flattening of the curve and hence a decrease in the change of error. Now the rule base for the 
learning rate can be easily de- rived and is given in Table 1. The result of the fuzzy com- putation 
is to use a factor *7/ to compute the learning rate of the next step according to the formula: .(t + 
1) = .. .(t). (5) The membership functions introducing quantitative knowl-edge are depicted in Figure 
1. In [XWB92] it was shown that further speed improvement can be achieved by combin- ing the learning 
rate adaptation with that of the steepness parameter. A rule base for the steepness can be derived from 
the following statements: Tru~ Valu very imail m~li~m laa,gz very 1~ 0 0.0 0.00005 0,0005 0.05 0.1 LO 
CE (a) Fuzzificationof CE Truth Value small medium h.rge v~v/l~tge  0.0 0.25 0.5 1.0 1.5 2.0 5.0 8.0 
I0.0 20.0 500.0 -QCE (b) Fuzzificationof QCE Truth VLhe small medium large 0 0.0 0,1 0.4 0.6 1.0 4.0 
6.0 7.0 10.0 (c) Fuzzification of *~ Figure 1: Membership Functions for Computation of the Learning 
Rate QCE CE small [medium large very large very small small i small medium large small small i medium 
medium large -medium small i medium medium large large small, medium large large very large small medium 
large large Output Parameter : .T/ Table h Rule Base for the Learning Rate 137 1. If the error is 
large then the output of the neurons is far from the expected result. Therefore the steepness should 
be quite small so that the interconnections of the neurons are not driven into saturation. 2. When the 
error becomes smaller the steepness value should grow to adjust the weights quickly and pre- cisely towards 
the correct values. In the actual implementation of the outlined principles it proved necessary to add 
some enhancements. The high learning rate will, despite the measures described, occasion- ally lead to 
an overshot. Hence only steps that decrease the error function are allowed. Otherwise the algorithm will 
re- turn to the second last iteration and compute a new learning rate by simply decreasing the current 
one. The experiments have shown that the most reliable way to ensure a successful iteration is to decrease 
the rate by an order of magnitude. It is also necessary to observe the learning rate in order to prevent 
it from falling below a minimum level. Otherwise the algorithm will get stuck, since no significant changes 
to the results can be made. So a lower bound for the learning rate is introduced and if it is reached, 
the learning rate is set to a fixed value instead. Finally, when dealing with a variable steepness parameter, 
it is important to understand the following connection: In the update formula for the weights (1) the 
steepness occurs as a factor multiplied with the learning rate, in effect increas- ing or decreasing 
it depending on whether it is larger/smaller than 1 (this equation is a part of the computation formula 
for the error derivatives; see [HKP91]):  g'(.~t,) = ~g(na,)(1 g(na~)) . (6) - This is probably the 
main reason for the non-convergence of conventional backpropagation (BP) if the steepness is cho- sen 
too high, since the resulting effective learning rate can easily lead to oscillations. To prevent this 
effect the imple- mentation proposed in this paper usesa modified update formula: aE + w,j(t). (r)w~j(t+l)= 
#aw~  Results A set of botanical data, the Iris data of Anderson land35] has been selected to compare 
the results of all three meth- ods. The data are four-dimensional vectors, each of which representing 
sepal length, sepal width, petal length and petal width of one of the three Iris subspecies Setosa, Versicolor, 
and Virginica. Measurements are taken from 50 plants of each subspecies. The data set is divided into 
a training set and a test set, each of them consisting of 75 vectors. One of the main problems in comparing 
results is the wide variety of parameters that have to be chosen initially. This includes many that have 
not been considered in this research work such as weight initialization and number o/hidden layer neurons. 
This method w~ tested with Iris data. The initial weight values were chosen with a randomiza- tion algorithm 
in the range of ::t=1 but were identical for all compared methods. A two layer perceptron was imple- 
mented with 4 inputs, 3 outputs and 3 hidden layer neurons. The initial values for the crucial parameters 
learning rate and steepness were set to 1. 138 Figure 2 shows the development of the error for the fuzzy 
controlled methods FAt and FA2 compared with the con-ventional backpropagation BP. 0.4 I I ' '* I ! 
I I I 0.35 ¢onv.B P -- FAt -- 0.3 . FA2 -- 0.25 error 0.2 0,15 0.1 0.05 0 I ! f f P I ,,t f | 0 200 
400 "600 800 1000 1200 1400 1600 1800 2000 iteration Figure 2: Error Function comparison for three imple-mented 
methods The diagram shows how the learning process benefits from the learning rate adaptation. There 
is a high gain of speed for FA1 already. The proposed new method FA2 performs even better, decreasing 
the error rapidly from the very be- ginning. The error function is an important indicator for the overall 
learning speed of an algorithm. According to Figure 2 FA2 only needs 400 iterations (i.e. 5 times faster) 
to achieve the best results obtained by the method FA1. It was already mentioned that for every learning 
run there is a certain number of failed iterations. The ratio of failed to successful iterations is around 
15% which having only little effect on the performance of the algorithm. Figure 3 presents statistics 
of the learning rate 77 collected over a complete learning run. b2'~.td' I .'! .......... ... , /I R 
i I1 g Rta 400 e 200 o .~I[ut,H_,,l ~t..................................... ~,~jj...... o 2oo 4o0 ~,o 
.00 ,ooo ~oo ~4oo ~oo tsoo 000 i~ration Figure 3: Learning Rate variation during a learning run Bearing 
in mind that a lower limit has been implemented for the learning rate (0.2) it can be seen that ~ varies 
over a range of five orders of magnitude. This is a considerable difference to the learning rates reached 
with FA1 or conven- tional BP. Having obtained these promising results with the new method the simulations 
were expanded to other data sets in order to confirm the applicability to general problems. Two new data 
sets were used: sensor data for road surface recognition of a moving car ([HHG93]) laser scanner data 
for recognition of solder joints ([HPG93]) Both data sets are from real world problems of which more 
details axe to be taken from the cited papers. The results obtained under the same circumstances as described 
for the Iris data. are shown in Figures 4 and 5. I I I ! I I ! I I 0.45 conv.BP -- 0.4 0.35 FAI --FA2 
-- 0.3 error 0.25 0.2 0.15 0.1 0.05 0 I I 1 I I  I I 200 400 600 800 1000 1200 1400 1600 1800 2000 
itelration Figure 4: Error Function comparison for road surface recognition 0.3 t I I I I conv.BP 
  0.25 ~ FAI 0.2 ¢zror 0.15 0.I  0.05 0 I I I I I 50 100 150 200 250 300 iteration  Figure 5: 
Error Function comparison for solder joint classification Cascade systems  There are not many applications 
using cascaded neural and fuzzy systems. One problem in cascading systems is the adding up of approximation 
error that can be critical for some applicatiom. However, authors present a method of cascading step 
net described in [KPD92] with fuzzy logic. The step net proved to be a structured way of using feed 
forward neural networks with gradient descent learning for classification problems. Building and training 
of step net consists of three consecutive steps: 1. linear separation is attempted using single layer 
net- works, training each class separately 2. pairwise linear sepaxation is attempted for all the classes 
not successfully trained in the previous step. 3. the remaining classes axe trained either through build- 
ing binary decision tree or using multilayer perceptron networks for pairwise separation  All the neurons 
used in first two steps are binary. The final layer of step net includes binary logic operations such 
as AND that conclude the results of all the steps. Class Step performance Setosa 1 I00 % Versicolor 3 
96 % Virginica i 3 92 % Table 2: Fuzzy Step Net Performance on Iris Data  Unlike many classical neural 
networks with "black box" behavior this strategy gives insight into the complexity of the problem. Fuzzy 
step net deviates from the conventional step net on the following characteristics:  instead of binary 
neurons, sigmoid neurons are used with output varying from 0 to 1  final layer contains fuzzy logic 
operations such as the T-norm: Min, the T-conorm: Max and the fuzzy nega- tion   Results Fuzzy step 
net is tested with the Iris classification problem. The class Setosa could be trained well using linear 
separa: tion by a single layer network (step 1).The classes Versicolor and Virginica could only be successfully 
separated from es~ch other using a mnltilayer perceptron (step 3). The results axe summarized in Table 
2. The overall performance on IRIS data is 96%, hence slightly better than the performance of conventional 
back- propagation (94.7%). The application of the same method on numerical digit recognition was less 
successful than ex- pected (Table 3). Handwritten numerical digit recognition Numerical characters 
from several writers are preprocessed and 36 input features are selected. After the normalization of 
the image at the beginning, the center of gravity is cal- culated. The main guide in preprocessing is 
the 8 scan lines drawn from this point, each in 45 ° of difference. The dis- tances from the center of 
gravity to the two cutting points in each scan line are considered as inputs to the network. Fur- thermore 
the number of gray points and corner points are also considered as input features. The preprocessed data 
are divided into a training set and a test set. The performance reached is shown in Table 3. The overall 
performance (94.1%) is beyond the perfor- mance shown by conventional bax:kpropagation (99%) algo- rithm. 
A general difficulty in this problem is the similarity of the handwritten digits 5 and 6 leading to frequent 
mis- classification. Use of neural networks in constructing fuzzy systems  Exploiting the learning 
capabilities of neural networks, pa- rameters of fuzzy systems such as rule weights, slopes and positions 
of membership functions can be tuned. The extrac- tion of the knowleclge base, otherwise to be formulated 
by experts, is the more interesting task in this area. The Fuzzy- Neural system FuNe I is capable of 
extracting the knowledge base as well as tuning the parameters. 199 Class Step performance 0 3 100 % 
1 1' 100 % 2 2 100 % 3 2 90 % 4 3 100 % 5 3 63 % 6' 3 90 % 7 3 100'% 8 2 90 % 9 2 I00 % Table 3: Fuzzy 
Step Net Performance on Numerical Digit Recognition FuNe I generates rules from a prewired network in 
the first phase (FuNe I training network), and tunes the parameters and the already extracted rules in 
the second phase (FuNe I fuzzy system). The basic structure of the FuNe I training network con- rains 
four neuron blocks. The input block feeds the pre- processed input signals to the fuzzification block 
where mem- bership functions are realized. The rule block receives all the membership values, rules are 
formed, and the rule strengths are passed to the defuzzification. The output block produces defuzzified 
results ~ter a post-processing step. Fuzzification block A set of bell shaped membership functions were 
created con- sisting of shiKed, scaled, or reflected sigmoid transfer func- tions [HG92]. Three or more 
user s electable membership functions can be formed according to the users preference. for each input 
(e.g. Low (L), Medium (M), and High (H)). Only one sigmoid neuron is needed to create a membership function 
at the lower or higher end of the input range, while two sigmoid functions are used to build a membership 
func- tion in the range. Generation and tuning of membership functions are further discussed in [HG92]. 
Rule extraction Three types of fuzzy rules axe considered for extraction from sample data in FuNe I: 
simple rules (with premises consisting of a single fuzzy variable), conjunctive rules aald disjunctive 
rules. By considering the fact that the rules are weighted and accumulated to the output, a conjunctive 
.(disjunctive) rule with premises containing more thaxt two fuzzy variables can be approximated by a. 
number of conjunctive (disjunctive) rules vdth only two fuzzy variables in their premises each. The rule 
block and the method of rule extraction is further illustrated in [HPG93]. Non-transparent defuzziiication 
The classifier FuNe I performs a non-transparent, but cus- tomized defuzzification, which is characterized 
by the rule weights achieved during the second phase. Customized de- fuzzification in general is axtvantageous 
to the conventional methods, since no standard defuzzification method has so far proved to be the best, 
independent of the application  t.GgS]. Application i, IRISdata ,, Classification rate .l 95 % Feature 
reduction 2S % 99 % solder joints 99 % 43 % load surface 99 % - numerical digit 99 % 14 % Table 4: FuNe 
I Performances Results FuNe I is successfully employed in various applications, e.g. the road surface 
recognition of a moving car [R}IG93], the recognition of solder joints [HPG93] and the numerical digit 
recognition described previously.' The results obtained are summarized in Table 4. The most important 
advantage in the results is the feature reduction capability which re- duces hardware complexity inherently. 
 Discussion Though it is not easy to implement resulting systems of fuzzy controlled backpropagation 
in hardware, the results obtained are encouraging. Additional benefits of the new method are the simplification 
in the initial choice of para- meters and the fact that the computation of the adaptive parameters is 
independent of the size of the network. With continuously increasing performance of parallel computing 
systems it would be very interesting to find out how fuzzy control can also improve on single learning 
rates. There is also a potential for improvement in the connection between learning rate and steepness 
that can lead to a better rule ba.se than the somewhat intuitive one used by authors. Fuzzy step net 
proved to be an interesting alternative to the backpropagation, especially for problems containing a 
number of linear separable classes in addition to complex" .... ": classes. Hardware implementation is 
easier if many classes can be successfully trained with the first two steps. FuNe I is already employed 
in many industrial applica= tions for generating fuzzy systems, and seems to be supe- rior to the other 
two methods. It integrates the learning capabilities of multilayer perceptrons with the structured knowledge 
base in a fuzzy system. FuNe I Fuzzy systems are implemented in commercially available fuzzy proces- 
sors (e.g. FCl10 from Togai [Tog91], Fuzzy-166 from In- form [Inf92]) and in application specific designs 
using Xil- inxs 4005 Field Programmable Gate Arrays. An extended fuzzy-neural classifier based on FuNeI 
isunder development adding user selectable alternative methods for the training phase (e.g. qnickpropagation 
algorithm). The fuzzy neural controller FuNe II, containing a transparent global defuzzi- fication approximation 
is also under development. Authors: S. K. Halgamuge and M. Glesner Darmstadt University of Technology 
Institute of Microelectronic Systems Kadstr. 15, D-64283 Darmstadt, Germany Tel.: -.}-.}-@49 6151 16-5136 
Fax.: -F+49 6151 16-4936 email: saman@rnicroelectronic.e-technik.th-darmstadt.de [ACMC92] [And35] [BK92] 
 [DHR93] [GQ91] [HG92] [HG93] [HHG93] [HKP91] [HPG93] [IKT93]  [Inf92] [Jac88] [KPD92] REFEREN CES 
 P. Arabshahi, J. J. Choi, R. J. Marks, and T. P. Caudell. Fuzzy Control of Backpropagation. In IEEE 
International Cor~fer~nce on Fuzzy Sys- tems, March 1992. E. Anderson. The irises of the Gaspe Peninsula. 
Bull. Amer. Iris Sot., 59, 1935. H. R. Berenji and P. Khedkar. Learning and Tuning Fuzzy Logic Controllers 
Through Rein- forcements. IEEE Transactions on Neural Net- works, 3(5), 1992. D. Driankov, H. Hellendoorn, 
and M. Reinftank. An Introduction to Fuzzy Control. Springer-Verlag, USA, 1993. M. M. Gupta and J. Qi. 
On Fuzzy Neuron Mod- els. In International Joint Conference an Neural Networks, Seattle, July 1991. S. 
K. Halgamuge and M. Glesner. A Fuzzy- Neural Approach for Pattern Classification with the Generation 
of Rules based on Supervised Learning. In Neuro.Nimes 92, Nimes, France, November 1992. S. K. Halgamuge 
and M. Glesner. The Fuzzy Neural Controller FuNe II with a New Adaptive Defnzzification Strategy Based 
on CBAD Dis- tributions. In European Congress on Fuzzy and Intelligent Technologies'93, Aachen, Germany, 
September 1993. S. K. Ha.lgamuge, H.-J. Herpel, and M. Glesner. An Automotive Application With Neural 
Net- work Based Knowledge Extraction. In Mecha-tronical Computer Systems ]or Perception and Action' 93, 
Halmstad, Sweden, June 1993. J. Hertz, A. Krogh, and R. G. Palmer. Intro-duction to the Theory of Neural 
Computation. Addison-Wesley, 1991. S. K. Halgamuge, W. Poechmueller, and M. Glesner. A Rule based Prototype 
System for Automatic Classification in Industrial Qual- ity Control. In IEEE International Conference 
on Neural Networks' 93, San Francisco, USA, March 1993. H. Ishibuchi, K. Kwon, and H. Tanaka. Im-plementation 
of Fuzzy If-Then Rules by Fuzzy Neural Networks with Fuzzy Weights. In Eu-ropean Congress on Fuzzy and 
Intelligent Tech- nologies'93, Aachen, Germany, September 1993. INFORM GmbH. FUZZY-166 Hybrid Fuzzy Processor. 
Aachen, Germany, 1992. R. A. Jacobs. Increased Rates of Convergence Through Learning Rate Adaptation. 
IEEE Transactions on Neural Networks, 1988. S. Knerr, L. Personnaz, and G. Dreyfus Hand- written Digit 
Recognition by Neural Networks with Single Layer Training. IEEE Transactions on Neural Networks, 3(6), 
1992.  [eNri86] [RM86] [SAg0] [Toggl] [XWB92] [Zad6S] [Zim85] D. Plant, S. Nowla~n, and G. Hinton. 
Report CMU-CS-86-I~6: Experiments on Learning in Back Propagation. Carnegie Mellon University, 1986. 
D. E. Rumelhart and J. L. McCle]land. Par-allel Distributed Processing: Explorations in the Microstructure 
of Cognition. MIT Press, USA, 1986. F. M. Silva and L. B. Almeida. Acceleration Techniques for the Bazkpropagation 
Algorithm. In Lecture Notes in Computer Science, 1990. Togai Infralogic Inc. FCllO Togai Fuzzy Proces- 
sor. Irvine, U.S.A., 1991. H. Y. Xu, G. Z. Wang, and C. B. Baird. A Fuzzy Neural Networks Technique with 
Fast Back Propagation Learning. In International Joint Conference on Neural Networks, June 1992. L. A. 
Zadeh. Fuzzy Sets. In Information and Control 8, 1965. H.-J. Zimmermann. Fuzzy Set Theory and Its Applications 
Kluwer Academic Publishers, Boston, 1985. 141  
			