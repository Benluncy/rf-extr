
 Construction of Vector Field Hierarchies Bjoern Heckel* Center for Image Processing and Integrated 
Computing (CIPIC) Computer Science Department University of California, Davis Gunther Weber AG Graphische 
Datenverarbeitung and Computergeometrie Fachbereich Informatik Universit¨ at Kaiserslautern, Germany 
Bernd Hamann Kenneth I. Joy§ Center for Image Processing and Integrated Computing (CIPIC) Computer Science 
Department University of California, Davis Abstract We present a method for the hierarchical representation 
of vector .elds. Our approach is based on iterative re.nement using cluster­ing and principal component 
analysis. The input to our algorithm is a discrete set of points with associated vectors. The algorithm 
generates a top-down segmentation of the discrete .eld by splitting clusters of points. We measure the 
error of the various approxima­tion levels by measuring the discrepancy between streamlines gen­erated 
by the original discrete .eld and its approximations based on much smaller discrete data sets. Our method 
assumes no particular structure of the .eld, nor does it require any topological connectiv­ity information. 
It is possible to generate multiresolution represen­tations of vector .elds using this approach. Keywords: 
vector .eld visualization; Hardy s multiquadric method; binary-space partitioning; data simpli.cation. 
1 Introduction The rapid increase in the power of computer systems coupled with the improving precision 
of computational simulations now produce terascale data sets. The critical research problem encountered 
in the visualization of these data sets is the development of methods for storing, approximating, and 
rendering them. The crux of the *heckel@cs.ucdavis.edu weber@informatik.uni-kl.de hamann@cs.ucdavis.edu 
§joy@cs.ucdavis.edu 0-7803-5897-X/99/$10.00 Copyright 1999 IEEE problem is to reduce the size of the 
data sets while preserving es­sential features. We create different representations (or approxima­tion 
levels) of a data set, each of which can be substituted for the complete set, depending on the requirements 
of a visualization tech­nique. The given discrete vector .eld may be represented by a few data elements 
or by several billion elements if necessary, with each of the various representation levels containing 
as many as possible of the essential features contained in the original data set. In this paper, we address 
simpli.cation of vector .elds. Our method creates a disjoint set of clusters, and thereby a simpli­.ed 
discrete vector .eld, where a single point and vector is used to represent a cluster. We de.ne a bisection 
strategy for clusters that utilizes a plane to effectively split a discrete data set. The tiling that 
is implied by this repeated bisection procedure creates a parti­tioning of space into convex regions. 
Each of these convex regions, bounded by certain split planes used during the split process, con­tains 
a particular discrete subset of the original data. Each cluster has an associated error measure that 
depends on the differences in the streamlines generated from the points of the dis­crete vector .eld 
that are contained in the cluster. We measure the deviation of these simpli.ed vector .elds from the 
original discrete .eld and base our cluster generation process on these errors. Our algorithm splits 
clusters recursively, and splitting continues until a certain error condition is met. Most standard hierarchical 
visualization and data representation schemes require a mesh de.ning the connectivity of data points 
at varying levels of resolution. Except in very specialized situations where the connectivity among points 
is implicit (e.g., rectilinear and curvilinear grids), one must store connectivity information ex­plicitly. 
This is a major overhead concerning storage and also pro­cessing requirements. Our approach is among 
the very .rst stress­ing the concept of gridless representation. Our vector .eld data hierarchy is solely 
based on positional and vector information with­out the need to store or compute any connectivity information. 
One of the main motivations for this work is to provide tools that enable interactive and real-time browsing 
of massive scienti.c data sets. Our method supplies a very .exible means for produc­ing coarser levels 
of approximation of massive data, thus enabling interactive exploration. The vector .eld hierarchies 
generated with our approach can easily be adapted to support local level-of-detail rendering, or user-steered 
local re.nement operations to focus on certain details present in a vector .eld. In Section 2, we review 
mesh simpli.cation algorithms that ap­ply directly to our work. In Section 3, we survey approximation 
techniques that we use for vector .eld data. In Section 4, we dis­cuss error measures that differentiate 
a simpli.ed vector .eld from the original one. In Section 5, we describe the procedure that de­termines 
the bisection plane used to split a cluster. More practical aspects of our algorithm are covered in Section 
6. Results of our algorithm are provided in Section 7. Conclusions and future work are presented in Section 
8. 2 Related Work Given a discrete vector .eld as a set of points {xi =(xi,yi,zi): i =1, ..., n} and 
a corresponding set of vectors {vi =(ui,vi,wi): i =1, ..., n}, the goal of our simpli.­cation method 
is to construct a new discrete representation of the vector .eld containing fewer points. We present 
an algorithm based on a top-down re.nement approach by using an adaptive clustering method. This type 
of simpli.cation requires no topological knowledge or connectivity information. A number of methods have 
been developed that simplify scalar .elds, but research concerning simpli.cation of vector .elds is still 
in its infancy. Several simpli.cation algorithms use a bottom-up or a top-down strategy for the construction 
of multiple approximation levels of a given .eld. In a bottom-up strategy, the given (high­resolution) 
grid is examined to identify regions where the mesh can be simpli.ed, and the mesh is decimated in these 
areas. This pro­cess continues until an error threshold is reached. In a top-down strategy, an initial 
mesh is iteratively re.ned. One starts with a very coarse mesh and inserts points into the mesh until 
a desired error condition is met. Most of these methods require topological con­nectivity information 
for the data points. Nielson et al. [12, 13] have used a wavelet approach to simplify vector .elds over 
the sphere and over curvilinear grids. In [12], they de.ne a class of Haar wavelets over triangular domains 
and apply these techniques to simplify a vector .eld over a sphere. In [13], they utilize lifted Haar 
wavelets and apply them to curvilinear grids. Helman and Hesselink [7] have developed methods to simplify 
the visualization of two-dimensional vector .elds by visualizing the topology of the .eld. The topology 
is visualized by specify­ing a collection of tangent curves that separate a .ow into regions. The tangent 
curves connect critical points, where the .ow is zero. These methods provide a good way to simplify two-dimensional 
vector .elds, but these techniques have not been extended to three dimensions. The algorithm we present 
in this paper utilizes clustering, simi­lar to the work of Heckel et al. [5, 6], who use these techniques 
for the generation of surface triangulations for a given set of scattered surface data. In these works, 
they use adaptive clustering methods to construct near-planar point clusters that can be directly triangu­lated. 
The resulting cluster triangulations, together with a triangu­lation of the space between the clusters, 
provides a valid surface reconstruction of a point set. The authors utilize principal compo­nent analysis 
(PCA), see Hotelling [8], Jackson [9], or Manly [10], to .nd a best-.t plane to each cluster, and then 
split perpendicular to this plane. Most of these methods are based on an error measure. However, error 
measures are dif.cult to de.ne for vector .elds. We have decided to utilize an error measure that pertains 
to the visualization itself, i.e., we consider an approximation of a given vector .eld to be a good approximation 
if the visualizations of the original data set and the approximation are very similar. The algorithm 
we use is based on a top-down approach. We uti­lize clustering [10] to generate a binary space partitioning 
(BSP) of the data set. Initially, all points of the given discrete vector .eld are placed in a single 
cluster. Clusters are split using a weighted best-.t plane that splits the clusters so that the variance 
of the error is reduced in the child clusters. Each cluster has an associated error measure that depends 
on the differences in the streamlines gener­ated from the points of the original discrete vector .eld 
that are contained in the cluster. The algorithm splits clusters recursively, and splitting continues 
until a certain error condition is met. The approach is gridless in the sense that we never require any 
point connectivity. 3 Approximation of Unstructured Vector Fields A discrete vector .eld in three-dimensional 
space is de.ned by a set of points xi, i =1, ..., n, and a set of associated vectors vi. We as­sume that 
the data set is a scattered data set, with no mesh de.ning the connectivity of the points. We de.ne an 
analytical approxima­tion to the vector .eld by using Hardy s multiquadric method, see [2, 3, 4, 11], 
which is one of the most effective and most commonly used methods for scattered data interpolation. Given 
n points x1, x2, ..., xn and associated vector values v1, v2, ..., vn, Hardy s multiquadric interpolant 
is de.ned by the solution of the n × n linear system of equations n vi = aj||xi - xj ||2 + R2, i =1, 
..., n, j=1 where the parameter R2 is a positive constant. Once the unknown coef.cients aj are computed, 
one can approximate a vector value v at an arbitrary point x in space by n v(x)= aj||x - xj ||2 + R2. 
j=1 The computation of the coef.cients aj requires the inversion of an n × n matrix. The function v(x) 
smoothly interpolates the vector values at the n data points and provides a vector estimate at any point 
x in space. For large data sets, it is unreasonable to consider all data for the construction of Hardy 
s multiquadric interpolant. Franke and Nielson [1] have shown that for each point xi of a scattered data 
set, an in.uence radius can be de.ned so that its contribution to the Hardy interpolant is zero outside 
this radius. A more common method is to utilize a .xed, small number of data points for the calculation 
of localized Hardy interpolants. Concerning the identi.cation of the data points closest to a par­ticular 
point for which we have to compute a local Hardy inter­polant, we utilize a kd-tree, see [14].  4 Clustering 
and Error Measurements Traditionally, algorithms concerned with the approximation of sci­enti.c data 
consider the numerical deviation of the given data and its approximation as the error. In the context 
of our application, which is the visualization and analysis of a vector .eld, it is more important to 
ensure that the resulting imagery generated from a par­ticular approximation varies very little from 
the imagery obtained from the original data set. We chose to use the deviation of stream­lines as the 
criterion to measure the error of a discrete approxima­tion of a given discrete .eld. Our method separates 
a discrete vector .eld data set into dis­joint clusters. A simpli.ed vector .eld is de.ned by using a 
sin­gle point and associated vector for each cluster and using Hardy s  Figure 1: Cluster-based simpli.cation 
of a vector .eld: (a) The original vector .eld segmented into two clusters. (b) By replacing each cluster 
with a single point and associated vector, a simpli.ed .eld is obtained.  Figure 2: Calculation of the 
error measure at a point x. The error is the sum of the distances between the sequence of point pairs 
si ' and si obtained by a Runge-Kutta method. The streamline based on the original .eld is drawn as 
the solid line, and the dashed line corresponds to the simpli.ed .eld. multiquadric to approximate the 
.eld locally. Figure 1 illustrates a vector .eld subdivided into two clusters and the simpli.ed vector 
.eld. The new data point associated with a cluster C is de.ned by the average of the coordinates of the 
points in C. The single vector associated with C is calculated as the average of the vector values associated 
with the points of the original .eld contained in C. Suppose we have segmented a vector .eld V into a 
set of disjoint clusters C1, C2, ..., Cn and have determined the cluster center points x1, x2, ..., xn 
with associated vector values v1, v2, ..., vn; the set of n points and n vectors de.nes a new, reduced 
vector .eld V'. To measure the difference between V and V' at an arbitrary point x, we compute a difference 
measure for the streamlines emantating from x, one based on the .eld V and one based on V', see Figure 
2. A streamline is an integral curve. It is calculated by solving the equation dx(t) = v(x(t)), (1) dt 
 where t is the parameter along the streamline. Given an initial point x in a .eld, a streamline can 
be calculated by solving this initial­value problem. We apply a fourth-order Runge-Kutta scheme, see 
[15], to integrate the equation stepwise. For a given point x in the .eld, we calculate two streamlines, 
one based on the simpli.ed .eld and one based on the original one. We use the deviation of the two streamlines 
as an error measure for an approximation. The streamlines are calculated using a .xed number of Runge-Kutta 
steps (with a .xed step size), using the same point x as the initial position. We compare the difference 
between the two streamlines by calculating the sum of the distances between the cor­responding Runge-Kutta 
points on the respective streamlines, see Figure 2. Thus, we de.ne the error at a point x to be n E(x)= 
||si - si'||, (2) i=1 where si is the ith Runge-Kutta point in the generation of the ' streamline for 
V, and si is the ith point in the generation of the streamline for V'. To calculate an error measure 
for an entire cluster of points, we use the maximum calculated error value for those points belonging 
to the cluster, i.e., E(C) = max E(x). (3) x.C In Figure 3, we illustrate the difference between streamlines 
for the blunt-.n data set.  5 Splitting Clusters In order to generate the set of clusters whose cluster 
center points and associated vectors de.ne a simpli.ed vector .eld, we recur­sively split clusters until 
the maximum value of all cluster errors is less than a prescribed tolerance. Each cluster is split using 
a bi­section plane, which is placed and oriented to reduce the error in the child clusters. Figure 4 
provides an illustration of a cluster and a desirable splitting plane. To generate the split plane for 
a cluster C, we consider the points xi of the original discrete vector .eld that are contained in C and 
the associated error values E(xi) for these points. To generate position and orientation of the split 
plane, we focus on those points of the cluster where the error is low, and use a weighted least squares 
best­.t plane using these points. First, we determine weights for each point, based on the approx­imation 
error. Those points xi of the original vector .eld where the error is low should be weighted high for 
our split procedure, while Figure 4: The split plane subdivides the cluster such that the errors in the 
child clusters are minimized.   points with high error should be weighted low. Thus, the weight w(xi) 
of a point xi is proportional to the reciprocal of the calcu­lated error E(xi) at the point. In particular, 
if W = E(xj )-1,E(xj ) (4) =0, xj.C then we de.ne the weight w(xi) of a point xi to be 1 w(xi)= . (5) 
WE(xi) We de.ne the orientation of the split plane P as the best-.t plane, in the least squares sense, 
to the set of weighted points {wi(xi)xi : xi . C}, passing through the point 1 x = wi(xi)xi, (6) k xi.C 
where k is the cardinality of the cluster C, see Hotelling [8] or Jack­son [9]. The splitting process 
is done recursively, de.ning a binary space partitioning of the data set. At each step of the partitioning 
pro­cedure, we concentrate on the cluster that has the maximum error. This results in simpli.ed vector 
.elds based solely on the differ­ences between streamlines. We illustrate the splitting process for a 
two-dimensional vector .eld de.ned by the function (x, y) .(x, xy). We limit the vector .eld to the region 
0 = x = 1 and -1 = y = 1. This vector .eld is shown in Figure 5. Figure 6 shows the cluster centers and 
associated vectors after splitting. The errors associated with clusters must be updated (locally) once 
a particular cluster has been split. The computation of a streamline emanating at a particular cluster 
center x depends on a certain number of nearby centers of neighbor clusters. Splitting one of these neighbor 
clusters will affect the streamline computa­tion, and therefore the error for point x. Thus, we locally 
update cluster values whenever splitting is performed.  6 The Algorithm Given a discrete vector .eld 
as a set of points and a corresponding set of vectors, we initially place all vectors in a single cluster, 
thus de.ning a single, constant vector .eld V ' . We calculate the error for all points of the cluster 
and de.ne the maximum point error as the cluster error. Each cluster is placed in a priority queue ordered 
 (a) (b) (c) (d)   Figure 6: The splitting algorithm operating on the vector .eld (x, y) .(x, xy). 
We have displayed the cluster centers after (a) 7 splits; (b) 15 splits; (c) 25 splits; and (d) 45 splits. 
Data Set Number of Points Number of Clusters Time in seconds Spiral 4,000 500 45 Three Singularities 
4,000 1,000 60 Blunt Fin 41,000 5,000 886 Table 1: Statistics for the three vector .elds. by decreasing 
cluster error. We repetitively use the cluster at the front of the priority queue to re.ne the vector 
.eld V ' . Given a cluster C with maximal weight (the cluster at the front of the queue), we calculate 
the split plane for the cluster and split the cluster into two child clusters C1 and C2. We calculate 
the center points and average vectors that represent each of these clusters and obtain a modi.ed vector 
.eld V ' by replacing the point and vector representing C by the new points and vectors representing 
the child clusters. We recalculate the errors for C1 and C2 and insert them into the priority queue. 
This process is repeated until a vector .eld approximation is ob­tained whose maximal cluster error is 
less than a prescribed error tolerance or until a prescribed number of clusters is generated.  7 Results 
We have applied this algorithm to several complex data sets. Pre­processing of the data sets was done 
on an SGI Onyx2 computer system, using one 195MHz R10000 processor. The statistics for each data set 
are shown in Table 1. The .rst data set is given by a discrete .eld of 4,000 samples taken from the vector 
.eld de.ned by the equation (x, y) . (-0.03x +0.1y, -0.1x - 0.03y, 0) . (7) We have illustrated the simpli.ed 
vector .eld at two resolutions in Figure 7. The streamlines for the original .eld and the simpli.ed .elds 
are nearly identical in each case. Here, the simpli.ed .elds represent the spiral well, even though few 
clusters are generated near the center. The second data set is given by a discrete .eld of 4,000 samples 
taken from the vector .eld de.ned by the equation 11 11 (x, y) .-x 2 + xy + y + ,x 2 + xy - y - . (8) 
24 24 This vector .eld contains three critical points. We have illustrated the simpli.ed vector .eld 
at two resolutions in Figure 8. Here, the light-colored streamline represents the data of the original 
.eld, and the dark-colored streamline represents the data of the simpli.ed .eld. The third data set is 
the blunt-.n data set. This is a curvilinear data set containing 41,000 points. Figures 9a d show streamsur­faces 
generated for this vector .eld and for simpli.ed versions of it. In each .gure, the streamsurface of 
the original vector .eld is shown in blue, while the streamsurface for the simpli.ed .eld is shown in 
red. The simpli.ed vector .elds are based on (a) 200 clusters; (b) 500 clusters; (c) 1500 clusters; and 
(d) 2000 clusters. We have found that the illustration of these simpli.ed vector .elds, where a short 
streamline is displayed at each cluster cen­ter, is a very useful tool for visualizing the entire .eld. 
Since the simpli.cation routine generates clusters where the error is high, we obtain many streamlines 
in areas of high error, and few in regions of low error. Figures 9e and 9f illustrate this technique 
with two simpli.ed .elds at different resolutions for the blunt-.n data set.  (a) (b) Figure 7: A two-dimensional 
spiral vector .eld. The streamlines (shown here as tubes) for the simpli.ed .elds and the original one 
are nearly identical. The simpli.ed vector .elds are based on (a) 40 clusters, and (b) 300 clusters. 
 (a) (b) Figure 8: Segmentation of a two-dimensional vector .eld having three critical points. (a) Using 
100 clusters, the simpli.ed .eld cannot reproduce the streamline generated from the original .eld. The 
streamline generated from the original .eld is shown in light gray, and the streamline generated from 
the simpli.ed .eld is shown in dark gray. (b) Using 500 clusters, the simpli.ed .eld can reproduce the 
streamline generated from the original .eld. The streamlines are generated at the centers of the clusters. 
The simpli.ed vector .elds are based on 170 clusters in Figure 9e and 300 clusters in Figure 9f. 8 Conclusions 
We have presented an algorithm suitable for the generation of a hierarchical representation for discrete 
vector .elds. This method uses a clustering approach, segmenting the vector .eld into a hi­erarchy of 
disjoint clusters. This allows us to represent the origi­nal vector .eld a varying resolutions. We utilize 
an error measure that measures the error between the streamlines generated from the points of the original 
discrete vector .eld and points of the approx­imate .eld, and we use this measure to generate split planes 
for iterative segmentation. Our approach is innovative in two regards: (1) Our method is en­tirely gridless, 
leading to signi.cant savings in memory and storage overhead, and (2) our error computation is based 
on the difference in streamlines between the original vector .eld and an approxima­tion. The gridless 
paradigm is extremely space-saving, and an error metric based on the visual differences between approximations 
is de.nitely advantageous for visualization applications. We have experimented with a variety of approaches 
for cluster­ing. In the context of vector .elds, the data is characterized by (1) positional information 
and (2) vector information for each in­dividual datum. A multitude of different options exists concerning 
possible clustering criteria. Several approaches, including cluster­ing merely based on vector information 
will, in general, lead to cluster whose members will lie in disjoint regions of the original .eld that 
are far apart from each other. The binary space partition­ing approach that we have chosen creates spatially 
convex clusters and avoids most of the problems encountered with other clustering strategies. We plan 
to develop other strategies for clustering vector .eld data. Our current algorithm generates a BSP tree, 
but there are many other hierarchical data structures that one could use. We plan to evaluate the changes 
in the vector .eld topology under simpli.­cation as well. 9 Acknowledgments This work was supported 
by the National Science Foundation un­der contract ACI 9624034 (CAREER Award), the Of.ce of Naval Research 
under contract N00014-97-1-0222, the Army Research Of.ce under contract ARO 36598-MA-RIP, the NASA Ames 
Re­search Center through an NRA award under contract NAG2-1216, the Lawrence Livermore National Laboratory 
through an ASCI ASAP Level-2 under contract W-7405-ENG-48 (and B335358, B347878), and the North Atlantic 
Treaty Organization (NATO) un­der contract CRG.971628, and Silicon Graphics, Inc. The .rst au­thor was 
also supported by the IBM Alamaden Research Center. The blunt-.n data set is courtesy of NASA Ames Research 
Cen­ter. We would like to thank the members of the Visualization Group at the Center for Image Processing 
and Integrated Computing at the University of California, Davis, for their support.  References [1] 
Richard Franke and Gregory M. Nielson. Smooth interpo­lation of large sets of scattered data. International 
Journal for Numerical Methods in Engineering, 15(11):1691 1704, 1980. [2] Richard Franke and Gregory 
M. Nielson. Scattered data in­terpolation and applications: A tutorial and survey. In H. Ha­gen, H. Mueller, 
and G.M. Nielson, editors, Focus on Sci­enti.c Visualization, pages 131 159. Springer-Verlag, New York, 
1995. [3] R. L. Hardy. Multiquadric equations of topography and other irregular surfaces. Journal of 
Geophysical Research, 76:1906 1915, 1971. [4] R. L. Hardy. Theory and applications of the multiquadric­biharmonic 
method: 20 years of discovery 1968 1988. Computers and Mathematics with Applications, 19:163 208, 1990. 
[5] Bjoern Heckel, Antonio E. Uva, and Bernd Hamann. Clustering-based generation of hierarchical surface 
models. In C.M. Wittenbrink and A. Varshney, editors, Proceedings of Visualization 1998 (Hot Topics), 
pages 50 55. IEEE Com­puter Society Press, Los Alamitos, CA, October 1998. [6] Bjoern Heckel, Antonio 
E. Uva, Bernd Hamann, and Ken­neth I. Joy. Surface reconstruction using adaptive clustering methods. 
Technical Report CSE-99-1, Computer Science De­partment, University of California, Davis, January 1999. 
[7] James Helman and Lambertus Hesselink. Representation and display of vector .eld topology in .uid 
.ow data sets. Visual­ization in scienti.c computing, pages 61 73, 1990. [8] H. Hotelling. Analysis of 
a complex of statistical variables into principal components. Journal of Educational Psychol­ogy, 24:pp. 
417 441 and 498 520, 1933. [9] J. E. Jackson. A User s Guide to Principal Components. Wi­ley, New York, 
1991. [10] B.F. Manly. Multivariate Statistical Methods, A Primer. Chapman &#38; Hall, New York, New 
York, 1994. [11] Gregory M. Nielson, Thomas A. Foley, Bernd Hamann, and David A. Lane. Visualizing and 
modeling scattered mul­tivariate data. IEEE Computer Graphics and Applications, 11(3):47 55, May 1991. 
[12] Gregory M. Nielson, Il-Hong Jung, and Junwon Sung. Haar wavelets over triangular domains with applications 
to mul­tiresolution models for .ow over a sphere. In Roni Yagel and Hans Hagen, editors, IEEE Visualization 
97, pages 143 149, Los Alamitos, October 1997. IEEE Computer Society Press. [13] Gregory M. Nielson, 
Il-Hong Jung, and Junwon Sung. Wavelets over curvilinear grids. In David S. Ebert, Hans Ha­gen, and Holly 
Rushmeier, editors, Proceedings IEEE Visual­ization 98, pages 313 317, Los Alamitos, 1998. IEEE Com­puter 
Society Press, Los Alamitos, CA. [14] Hanan Samet. The design and analysis of spatial data struc­tures. 
Addison-Wesley, Reading, Mass., 1990. [15] Shyh-Kuang Ueng, Christopher Sikorski, and Kwan-Liu Ma. Ef.cient 
streamline, streamribbon, and streamtube construc­tions on unstructured grids. IEEE Transactions on Visualiza­tion 
and Computer Graphics, 2(2), June 1996. ISSN 1077­2626.  (a) (b) (c) (d) (e) (f) Figure 9: Construction 
of Vector Field Hierarchies.   
			