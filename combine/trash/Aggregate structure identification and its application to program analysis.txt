
 Aggregate Structure Identification and its Application to Program Analysis G. Ramalingam John Field 
Frank Tip IBM T.J. Watson Research Center, PO. Box 704, Yorktown Heights, NY, 10598, USA {rama,jfield,tip}@watson.ibm.com 
Abstract In this paper, we describe an efficient algorithm for lazily decomposing aggregates such as 
records and arrays into simpler components based on the access patterns specific to a given program. 
This process allows us both to identify implicit aggregate structure not evident from declarative information 
in the program, and to simplify the representation of declared aggregates when references are made only 
to a subset of their components. We show that the structure identification process can be exploited to 
yield the following principal results: - A fast type analysis algorithm applicable to program maintenance 
;z;lications such as date usage inference for the Year 2000 prob- - An efficient algorithm for atomization 
of aggregates. Given a pro- gram, an aggregate atomization decomposes all of the data that can be manipulated 
by the program into a set of disjoint atoms such that each data reference can be modeled as one or more 
references to atoms without loss of semantic information. Aggregate atomization can be used to adapt 
program analyses and representations designed for scalar data to aggregate data. In particular, atomization 
can be used to build more precise versions of program representations such as SSA form or PDGs. Such 
representations can in turn yield more accurate results for problems such as program slicing. Our techniques 
are especially useful in weakly-typed languages such as Cobol (where a variable need not be declared 
as an aggregate to store an aggregate value) and in languages where references to statically-defined 
subranges of data such as arrays or strings are allowed.  Introduction Many algorithms for static analysis 
of imperative programs make the simplifying assumption that the data manipulated by a pro-gram consists 
of simple atomic values, when in reality aggregates such as arrays and records are usually predominant. 
There are sev- eral straightforward approaches to adapting analysis algorithms de- signed for scalars 
to operate on aggregates: 1. Treat each aggregate as a single scalar value. 2. Decompose each aggregate 
into a collection of scalars, each of which represents one of the bytes (or bits!) comprising the aggregate. 
 Permission to make digital or hard copies ofall or part of this work for personal or classroom use 
is granted without fee provided that copies arc not made or distributed for prolit or commercial advantage 
and that copies bear this notice and the full citation on the first page. To copy otherwise. to republish. 
to post on sewers or to redistribute to lists, requires prior specific permission anJ:or a fee. 3. Use 
the declarative information in the program to break up each aggregate into a collection of scalars, each 
of which represents a declared component of the aggregate containing no additional substructures of its 
own. Unfortunately, each of these approaches has drawbacks. (1) can yield very imprecise results. While 
(2) is likely to produce precise results it can be prohibitively expensive. At first blush, (3) appears 
to be the obvious solution. How-ever, it is unsatisfactory in weakly-typed languages such as Cobol, where 
a variable need not be explicitly declared as an aggregate in order for it to contain composite data. 
Even in more strongly- typed languages, declarative information alone can be insufficient because (i) 
loopholes in the type system (such as typecasts) may permit aggregate values to interoperate with non-aggregate 
values; and (ii) programmers may pack several scalars, each encodedusing one or more bits, into a single 
word. Moreover, (3) may produce unnecessarily many scalar components when the program only ac- cesses 
a subset of those components. In addition, in the presence of unions this approach can produce scalars 
that overlap one an- other in storage inexactly. The operation of determining whether two scalars in 
a program refer to overlapping storage (such checks are often required in the inner loops of analysis 
algorithms) can be costly. In this paper, we present an efficient algorithm for lazily de-composing aggregates 
into simpler components based on the access patterns specific to a given program. This process allows 
us both to identify implicit aggregate structure not evident from declarative information, and to simplify 
the representation of declared aggre- gates when references are made only to a subset of their compo- 
nents. After atomization, each reference to an aggregate can be ex- pressed as a set ofreferences to 
disjoint atoms. The resulting atoms may then be treated as scalars for the purposes of analysis, and 
checks for overlapping storage reduce to equality tests on atoms. Atomization can thus serve as an enabling 
technique for perform- ing various program analyses (e.g., computing reaching definitions [l] and program 
slicing [I SJ), as well as constructing their underly- ing representations (e.g., PDG [6] or SSA form 
[4]) in the presence of aggregates. We also present a variant of the algorithm that can be used to efficiently 
solve certain type analysis problems. One instance of such a problem is date usage inference for programs 
affected by the Year 2000 problem. This is an instance of a general class of problems that require inferring 
undeclared but related usages of type information for various software maintenance and verification activities 
[lo]. The type analysis algorithm described here has been incorporated into several recent IBM products 
. POPL 99 San Antonio Texas USA IBM VisualAge 2000 for Cobol and IBM VisualAge 2000 for PL/I. Copyright 
ACM 1999 I-581 13-095-3/99/01...$5.00 01 A. 05 Fl PIC 99. 05 F2 PIC 99. 05 F3 PIC XX. 05 F4 PIC XX. 
 01 B PIC X(8). 01 c PIG x(8). 01 D. 05 F5 PIC 99. 05 F6 PIG 99. 05 F7 PIC XX. 05 FE PIG xx 01 RESULT 
PIC 99. MOVE 17 TO Fl. MOVE 18 TO F2. MOVE A TO B. MOVE B TO C. MOVE C TO D. MOVE F5 TO RESULT.  Figure 
1: Example Cobol program illustrating assignments be-tween aggregate and non-aggregate variables. 1.1 
Motivating Examples Consider the Cobol fragment shown in in Fig. 1. For Cobol-illiterati, the declarations 
in the example behave as follows: The program contains top-level declarations of variables A, B, C, D, 
and RESULT. Variables A and D are both declared as records of four fields: Fl through F4, and F5 through 
F8, respectively. The types of these fields are declared using so-calledpicture clauses, which are a 
com- pact means of expressing both the length and the allowable types of the sequence of characters that 
constitute the fields. The characters that follow the keyword PIC specify the types of characters that 
are allowed at corresponding locations in the field. For instance, a 9 character indicates numerical 
data, whereas an x character indicates that any character is allowed. Hence, variables A and D both consist 
of 4 numeric characters followed by 4 unconstrained characters. A picture character may be followed by 
a parenthesized repetition factor. The non-aggregate variables B and C thus each consist of eight unconstrained 
characters. The example program contains a number of assignments. Note that in Cobol, it is not necessary 
to name parent structures in data references when field references alone are unambiguous (e.g., in the 
assignment of 17 to field Fl of A). Suppose we are interested in computing the backwards pro-gram slice 
[ 17, 151 with respect to the final value of RESULT, i.e., the set of assignments that could affect the 
final value of RESULT. Since our example program does not contain any control flow con- structs, the 
slice contains any statement on which the final value of RESULT is transitively data-dependent. We assume 
that the fol- lowing model is used to compute these data dependences: . All variables are decomposed 
into disjoint atoms by some means. . Each MOVE statement is modeled as a set of atomic assign- ments 
between corresponding atoms. . Data dependences are determined by tracing def-use chains between atomic 
assignments. Clearly, an atomization that is too crude will lead to redundant statements in the slice. 
For example, treating the statement MOVE 01 DATA-RECORD. 02 DATE. 03 YY PIC 99. //year 03 MM PIC 99. 
//month 03 DD PIC 99. //day 02 PRINTABLE-DATE REDEFINES DATE PIC X(6). 02 . . . 01 OUTPUT-BUFFER. 02 
LINE PIG X(80). 02 COLUMNS REDEFINES LINE. 05 COLUMN-l PIC XX. 05 COLUMN-2 PIC XX. 05 . . . 01 PRODUCT-INFORMATION. 
02 COLUMN-l-INFOPIC XX. 02 COLUMN-2-INFOPIC XX. 02 . . . MOVE FUNCTION CURRENT-DATE TO DATE OF DATA-RECORD. 
M&#38;E PRINTABLE_DATE(1:2) TO COLUMN-l. . . . MOVE PRODUCT-INFORMATION TO OUTPUT-BUFFER. Figure 2: 
Example illustrating type analysis for the Y2K problem. B TO C as a scalar assignment between two atomic 
variables2 will lead to the inclusion of the superfluous statement MOVE 18 TO F2 in the slice. On the 
other hand, if the atomization is too fine-grained, the number of data dependences that must be traced 
to compute the slice will be larger than necessary and represen- tations that capture these dependences 
(such as PDGs) will also be larger than necessary. For example, breaking up each variable into character-sized 
atoms leads to the desired slice (one that omits MOVE 18 TO F2). However, the same result can be achieved 
with the following, much coarser-grained atomization, which is produced by our atomization algorithm: 
atomization(A) = (A[1:2], A[3:4], (~[5:8]) atomization(B) = (B[l:2], B[3:4], B[5:8]) atomization(C) = 
(~[1:2], C[3:4], C[5:8]) atomization(D) = (D[l:2], D[3:4], ~[5:8]) Here, we use array notation to indicate 
subranges of the characters occupied by a variable, E.g., B[8:4] denotes the subrange consist- ing of 
character 3 and character 4 of variable B. There are a few interesting things to note about this solution: 
. Fields Fl and F2 cannot be merged into a single atom with- out a loss of precision, and therefore correspond 
to separate atoms. . Field F3 and F4 are merged, because the distinction between these fields is irrelevant 
for this particular programs. In gen- eral, merging fields can lead to faster dataflow analysis and more 
compact program representations. Note that this is a very reasonable choice, especially if we use only 
declarative information to perform the atomization. Unused fields occur frequently in Cobol applications. 
Cobol-based systems typ- ically consist of a collection of persistent databases and a collection of programs 
that manipulate these databases. Although the declared record structure reflects the format of the database, 
a single application typically only uses a subset of the fields of the retrieved records. Hence, analysis 
of individual applications can benefit by coalescing or eliminating uninteresting fields. YY OF DATE 
OF DATA-RECORD + fear} MM OF DATE OF DATA-RECORD --f {notYear} DD OR DATE OF DATA-RECORD + {notYear} 
PRINTABLE-DATEt1:2] OF DATA-RECORD --f bear} PRINTABLE-DATE[3:4] OF DATA-RECORD + {notYear} PRINTABLE_DATEtS:6] 
OF DATA-RECORD -+ {notYear} LINE[1:2] OF OUTPUT-BUFFER -+ bear} COLUMN-l OF COLUMNS OF OUTPUT-BUFFER 
-+ bear) COLUMN-l-INFO OF PRODUCT-INFORMATION + bear} Figure 3: Result of type analysis applied to example 
in Fig. 2 . Although variables B and C are both declared as scalar vari- ables, both must be partitioned 
into three atoms in order to obtain precise slicing results. Fig. 2 shows a program fragment that manipulates 
dates in ways similar to those of Cobol programs affected by the Year 2000 ( Y2K ) problem. Here, DATA-RECORD 
represents a record con- taining date and non-date information. The storage for date infor- mation is 
redefined in two different ways: DATE is a structured record containing separate fields for month, day, 
and year digits, while PRINTABLE -DATE is an unstructured string of uncon- strained characters intended 
to be used for input or output. Since the YY field of DATE is only two digits long, it would have to 
be expanded to four digits to account for post-1999 dates. In addi- tion, COLUMN - 1 of OUTPUT -BUFFER 
(here representing a multi- purpose string used for input/output purposes) would have to be expanded 
to account for the fact that years are now larger. This could in turn affect PRODUCT - INFORMATION as 
well, since even though the latter never actually contains a year value, it would prob- ably have to 
be updated to account for the fact that the first column of OUTPUT - BUFFER is now two characters wider. 
Section 5 discusses how our aggregate structure identification algorithm can be extended to assist in 
remediating field expan- sion problems such as the Y2K problem by viewing it as a flow- insensitive, 
bidirectional type analysis problem. The basic idea is as follows: We first define a semi-lattice of 
abstract types. In the case of the Y2K problem, a lattice of subsets of the set { year, notYear ) (where 
year and notYear are atomic types representing fields inferred to be year-related or not year-related, 
respectively) would suffice, although more complicated lattices could also be used. Known sources of 
year-related values, such as the year char- acters returned by the CURRENT-DATE library function in Fig. 
2 are initialized to year. Sources of values known not to contain years (e.g., the non-year characters 
returned by CURRENT-DATE) are initialized to notYear. After applying the algorithm described in 5, the 
results of the type analysis are depicted in Fig. 3. The interesting aspect of our analysis is not the 
type lattice it- self, which is trivial, but the way in which the analysis is carried out efficiently 
and accurately on aggregates. This kind of analysis is applicable not only to the Y2K problem, but to 
other problems in which similar types must be propagated through aggregates, e.g., any problem involving 
field expansion of variables holding values of a particular logical (i.e., non-declared) type. Fig. 4 
depicts a more problematic example, in which an ar-ray, MONTH, is overlaid with a record, MONTHS -BY-NAME. 
Each field of MONTHS -BY -NAME corresponds to an element of the ar- ray MONTH).Overlaying of records 
and arrays is a fairly common idiom in Cobol. This allows programmers to refer to array ele- ments by 
name as well as by index (e.g., when iterating uniformly through the collection represented by the array), 
and is also used to initialize arrays, as in this example. The use of such idioms makes it desirable 
to avoid overly conservative treatment of such 01 M. 02 MONTH OCCURS 12 TIMES. 05 NAME PIG X(3). 05 NIJM-DAYS 
PIC g(2). 02 MONTHS-BY-NAME REDEFINES MONTH.  05 JAN. 10 NAME PIC X(3) VALUE IS "JAN". 10 NUM-DAYS 
PIC 9(2) VALUE IS 31. 05 FEB. 10 NAME PIC X(3) VALUE IS "FEB". 10 NUM-DAYS PIC 9(2) VALUE IS 28. . . 
. . . . MOVE NDM-DAYS OF MONTH(I) TO ND.  Figure 4: Example Cobol program illustrating overlaid arrays 
and records. overlays in the context of program analysis. For instance, in the context of reaching-definitions 
analysis, it is desirable to infer that the initializing definition of NAME OF JAN will not reach the 
use of NUM -DAYS OF MONTH [I] , but that the initializing definition of NUM -DAYs OF JAN might reach 
the same use. Our aggregate structure identification algorithm differentiates between references to the 
array as a whole, references to array sub- ranges with statically-determinable indices (references to 
the ele- ments of MONTHS -BY -NAME in the example of Fig. 4 are treated as single-element instances of 
subrange references), and references to arbitrary elements via indices computed at run-time. These dis- 
tinctions can be exploited to yield better atomizations that accu- rately differentiate among these cases. 
1.2 Overview The remainder of the paper proceeds as follows: In Section 2, we describe a tiny programming 
language that contains only the lan- guage features relevant to our results. Section 3 outlines the ba- 
sic ideas behind the structure identification algorithm in terms of solving equivalence constvaints on 
ranges of abstract memory lo- cations; this algorithm manipulates a new data structure called the Equivalence 
DAG. In Section 4, we observe that the algorithm of Section 3 can be viewed as computing the solution 
to a unc@mion problem. Among other things, this alternate view allows certain other problems that can 
be expressed in terms of unification (e.g., Steensgaard s flow- insensitive pointer analysis [11, 121) 
to be incorporated smoothly into our framework. Sections 5 and 6 cover two refinements of the basic algorithm 
and their applications: Section 5 extends the framework of Sec- tion 3 to add inequality constraints 
involving elements of an ab- stract type lattice. The resulting type analysis algorithm is applica- ble 
to the Y2K problem. In Section 6, we formalize the atomization problem, and provide a solution based 
on another extension to the framework of Section 3. The complexity of the Equivalence DAG construction 
algorithm (in its most general form) is discussed in Section 7. Extensions to the algorithm, including 
representation of variables of indeter- minate length, pointer analysis, and uses of SSA renaming, are 
covered in Section 8. Section 9 is devoted to related work. Sec-tion 10 discusses possible future work. 
Finally, the appendix pro- vides the details of the type analysis and atomization algorithms in pseudocode 
form. denote this set by V[cQ Formally, we define V by: Pm ::= E I Stmt Pgm Stmt ::= DataRef t DataRef 
  DataRef ::= ProgVars I DataRef [Int+ : Int+] I  DataRef \ Int+ Figure 5: The mini-language under 
consideration. Here, Int+ de- notes the set of positive integers, and ProgVarsdenotes a set of top level 
program variables. 2 A Mini Language In order to facilitate the discussion of problems studied and the 
algorithms presented in this paper, we will use a small language, the grammar of which is shown in Fig. 
5. The language of Fig. 5 may be thought of as a traditional imperative language trimmed down to the 
bare essentials necessary to discuss the problems at hand. Since the algorithms we present are flow-insensitive, 
control-flow aspects of a program are irrelevant, and we consider a program P E Pgm to simply consist 
of a set of statements. A statement di t CZZE Stmt represents an assignment which copies the contents 
of data reference d2 into dr . A data reference d E DataRef is a reference to some sequence of abstract 
locations ( bytes ) and takes one of the following forms: . a program variable z E ProgVars (the length 
of which will be denoted by Iz]) . a subrange d[Cj] of locations i through j of some data ref- erence 
d . a single, statically indeterminate element of an array of n elements, denoted by d\n, where d is 
a data reference repre- senting the complete array Subranges are used to represent a sequence of locations 
at a statically-determinedposition in a data reference. For example, if d refers to a record, then cZ[i:j] 
can be used represent a reference to a field of the record. A data-reference created by indexing an array 
is abstracted in our mini language into a reference of the form d\n, where d identifies the complete 
array, and n is the number of elements in the array. (Thus, our abstraction omits the actual array index 
ex-pression. If the index expression is a constant, however, the data reference can alternatively be 
represented as a subrange cZ[i:j] of the array, where i and j delimit the range of locations occupied 
by the single array element denoted by the index expression.) The no- tation cZ\n is intended to suggest 
that if we break up the sequence of locations that d denotes into n subsequences of equal lengths, then 
cZ\n denotes one of these n different subsequences. We now define the set of all locations as: Lot = 
((5, i) I I E ProgVars, 15 i < Iz]} (Different elements of Progtirs thus represent disjoint sets of 
loca- tions.) For convenience, we will denote location (z, i) as simply z[;]. At execution time, every 
data-reference denotes a sequence of locations. At analysis time, however, we may not know the precise 
sequence of locations referred to by a data-reference d in the general case (e.g., due to a reference 
to an array element at a statically-indeterminate index). Hence, we treat a data-reference d as a reference 
to one of a set of sequences of locations, and we will D[z] = { z[l] . z[2] . . . z[[z[] } if z E ProgVars 
D[d[i:j]] = { u[i] . c7[i + 11. *. a[j] 1 u E D[dj} D[d\nJ = { CT[S] . a[s + 11. . . u[e] 1 uE?)[cq,lIiI:a, 
s = (i -1) * (]u]/n) + 1, e = i * (]u]/n) } where u[;] indicates the i-th element of sequence u, and 
]u] denotes the length of a sequence u. Note that all elements of D[cZl have the same length, which we 
will denote loll. For example, let z, y E ProgVars. Then, 2[3:5] denotes the singleton set {z[3] . z[4] 
. z[5]}. A more interesting example is ((y[1:10])\2)[2:3]. Here, y[l:10]\2 is a reference to an arbitrary 
element of a 2-element array; the array as a whole occupies the first 10 locations ofy. The subrange 
[2:3] (which could, e.g., represent a single field when the array element is a record) is then selected 
from the element. As a result, the set of locations referred to consists of {y[2] . y[3], y[7] . y[8]}. 
In other words, ((y[l:10])\2)[2:3] is a reference to either locations y[2] and y[3] or locations y[7] 
and  YPI. We will now define an abstract semantics S(&#38; t &#38;) for the assignment statement cZl 
t cZ2, which simply consists of the set of all pairs of locations (21,12), written symbolically as 21 
t 12, such that the assignment statement might copy the contents of location 12 to location 11. This 
semantics is defined as follows: S(&#38; + d2) = {m(i) + c72(4 I Ql E qdl],az E D[dz], 15 i 5 min(lml, 
1~721)) In the rest of the paper, we will assume that for every statement dl i-d2, l&#38;l = Idzl. The 
abstract semantics for assignments that occur in a given program P E Pgm can be used to define an equivalence 
relation on locations that will be useful in the sequel (e.g., as the basis for inferring equivalent 
types). To this end, we first define: E = u S(&#38; t dz) &#38;+daCP Now, let EP denote the smallest 
equivalence relation containing the set of pairs of locations E (i.e., the equivalence closure of E). 
We will omit the subscript P if no confusion is likely. 3 The Equivalence DAG: The Basic Ideas Behind 
the Al-gorithm In this section we focus on the problem of computing the equiva- lence relation up, given 
a program P. The goal of this section is to give the reader an understanding of the essential algorithmic 
con-tributions of this paper, primarily through examples. We will de- scribe extensions and applications 
of this algorithm in subsequent sections. Rather than generate an explicit representation of the equiva- 
lence relation =P, we will actually generate a more compact rep- resentation of the equivalence relation 
that can be used to answer queries about whether two locations are equivalent or not. We will also refer 
to a statement dl c d2 as an equivalence constraint dl 11 d2 for notational convenience. 3.1 The Simple 
Equivalence Closure Problem We start with a simple version of the problem, where every con- straint has 
the form I N y, given CC, y E ProgVars. In this case, -N induces an equivalence relation on ProgVars. 
This is sufficient to answer questions of equivalence of locations since (2, ;) EP (y, j) if and only 
if variables 2 and y are in the same equivalence class and i = j. Thus, in this case, the set of equivalence 
classes of ProgVars provides a compact representation of the equivalence relation on vocations. The partitioning 
of PmgVars can be done in the standard way: initially place every program variable t E PmgVars in an 
equiv- alence class by itself, and then process the equivalence constraints one by one; a constraint 
z -N y is processed by merging the equiv- alence classes to which I and y belong into one equivalence 
class, using the well-known union-find data structure4 (see [14,3]). 3.2 The Range Equivalence Closure 
Problem Now, consider a version of the problem where every constraint is of the form z[i:j] 2: y[k:Z], 
where 2, y E PmgVars. There are two aspects to the original solution that we would like to preserve when 
we address this generalized problem. The first aspect is that the algorithm processes every constraint 
in C exactly once, instead of using an iterative (e.g., transitive-closure-like) algorithm. The second 
is that we would like to identify ranges of locations that are equivalent to each other and partition 
them into equivalence classes. This can represent EC more compactly than a partition of the set of all 
locations into equivalence classes. We now illustrate through an example how we can achieve these goals. 
Assume that W, X, Y, 2 E Pmgtirs, and that (W( = 6,1X1 = 12, IYi = 8, and 121 = 12. Assume that C consists 
of three equivalence constraints, X[5:8] N Y[1:4], 2[1:6] 31 W[1:6], and X[3:12] 21 Z[l:lO]. We begin 
by placing every variable in an equivalence class by itself. We then process the first con- straint X[5:8] 
N Y[1:4] as follows. We split the range X[1:12] into three sub-ranges X[1:4], X[5:8], and X[9:12] and 
place them each in an equivalence class by itself. We refer to this as adding breakpoints . We similarly 
split range Y[1:8] into two sub-ranges Y[1:4] and Y[5:8], placing them each in an equivalence class by 
itself. We then merge the equivalence classes to which X[5:8] and Y[1:4] belong into one. Given this 
kind of a partition of every program-variable into a collection of sub-ranges, every location belongs 
to a unique sub- range of the partition. We can map every location 1 into a pair (el, 01) where et is 
the equivalence class of the unique sub-range containing 1 and 01 is the offset of the location within 
that sub- range. Further, locations II and 12 are equivalent with respect to the relation =p if and only 
if the el, = ela and or1 = 01~. For exam- ple, location X[6] will be mapped to (ec(X[5:8]), 2) where 
et(r) denotes the equivalence class containing sub-ranger. Similarly, lo- cation Y[2] will be mapped 
to (ec(Y[1:4]), 2). Since ec(X[5:8]) = ec(Y[1:4]), these two locations are equivalent. Let us re-visit 
the step where we split a range, say X[1:12], into a sequence of sub-ranges, say X[1:4], X[5:8], and 
X[Q:12]. It turns out to be convenient to keep both the original range and the new sub-ranges around, 
and to capture the refinement relation be- tween these into a tree-like representation (rather than, 
for instance, replacing the original range by the new sub-ranges). Fig. 6(a) and Fig. 6(b) illustrate 
how we represent the refinement of X and Y for the above example. Each rectangle in the figure, which 
we will re- fer to as a node , denotes an equivalence class of sub-ranges, and the number inside indicates 
the length of each sub-range contained in the equivalence class. Fig. 6(c) indicates that the equivalence 
classes containing the nodes representing X[5:8] and Y[1:4] have been merged into a single equivalence 
class . It can be done even more etkiently using the linear time algorithm for comput- ing the connected 
components of an undirected graphs. However,wewill need the flexibility of the union-find data structure 
in a generalized version of the problem. Note that edges whose targets are nodes representing equivalence 
classes to be merged are not literally redirected to a common node, instead, the union-find data stroctore 
is used to merge the classes to which the edges refer. The next constraint (2[1:6], W[1:6]) is processedjust 
like the first constraint, as illustrated by Fig. 6(d-e). In the general case, processing a constraint 
dl N c&#38; consists of the following steps. (i) We first add break-points to the repre- sentation before 
the starting-location and after the ending-location of both $1 and &#38;. (ii) The sub-ranges dr and 
&#38; can then be represented by a sequence of nodes, say ~1 = [sl , . . . , sk] and ua = [t1,*.. , tm] 
respectively. We make these two sequences equivalent to each other as follows: if sr and tr denote ranges 
of the same length, we simply merge the two into one equivalence class and proceed with the remaining 
elements of the sequence. If the two denote ranges of different lengths, we then split the bigger range, 
say ~1, into two sub-ranges, s; and s; , such that si has the same length as tr. We then merge si with 
tl, and continue on, making the sequences [sr, ~2, . + . , sk] and [tz, . . - , t,,J equivalent. The 
third constraint X[3:12] N Z[l:lO] illustrates the more general scenario described above. After adding 
the necessary break- points, the range Z[l:lO] is represented by the sequence [sl, sa] (see Fig. 6(t)), 
while the range X[3:12] is represented by the se- quence [tl , t2, ta]. s1 is longer than tl , and is 
broken up into sub- ranges si and sr, as shown in Fig. 6(g). We then merge tl with s;, t2 with s: , and 
ts with ~2. Fig. 6(h) shows the resulting represen- tation. Clearly, given a location 1, we can walk 
down the DAG (shown in Fig. 6(h)), from the appropriate root to a leaf et to map the loca- tion to a 
pair (er, 01) such that 11 = 12 if and only if (ell ,oll) = (eta, 01~). We call the representation generated 
by this algorithm an Equivalence DAG. In the above description of the algorithm, we assumed that the 
nodes in the sequences ur and (~2 were leaf nodes. Even if that were true when the processing of the 
two sequences begins, when we get around to processing elements s; and tj, the processing of the earlier 
elements of the sequences could have had the effect of adding breakpoints to either si or tj or both, 
converting them into internal nodes. Our algorithm handles this by converting the sub- ranges involved 
into a sequence of leaf nodes lazily rather than ea- gerly. A simple example that illustrates this is 
the single constraint A[1:12] = A[5:16]. Adding breakpoints corresponding to the endpoints of the two 
subranges A[l:l2] and A[5:16] generates the representation shown in Fig. 7(b), The processing of the 
constraint then proceeds as below: A[1:12] N A[5:16] u [u2] N [us, us] u (replace 2~2 by its children) 
[u4, U6] = [ias, 2131 ij- (split ~6 into ~6 and ~7) [u4, u5] N [ue,u7, us] JJ (merge ~4 and ~6) [us] 
= [u7, us] u (replace 21s by its children) [U&#38;217] N [U7,%] .lJ (merge ~6 and ~7) [2L7] N [us] .I) 
(merge u7 and US) 0 = 0 This example illustrates the motivation behind our representa- tion. Note that 
if we maintained for each variable only the list of subranges into which it has been refined (instead 
of the tree 12 I 8 4 4   (W kil 0 (g) Figure 6: An example illustrating our range equivalence algorithm. 
124 (a) Figure 7: Another example illustrating our range equivalence algorithm. representation of the 
refinement), processing constraints such as A[1:12] N A[5:16] will be more difficult. Our algorithm may 
be easier to understand if it is viewed as a sort of unification, with the leaf nodes denoting unbound 
variables and internal nodes de- noting bound variables. We will explore this connection briefly in Section 
4. 3.3 The General Problem In the most general version of the problem we consider here, an equivalence 
constraint dr N da may consist of arbitrary data ref- erences as defined by the grammar in Fig. 5, including 
references to (statically indeterminate) array elements. Consider, for exam- ple, a case in which we 
have P, Q, R E ProgVars, with IPI = 20, IQ1 = 10, and /RI = 2. Assume we have two constraints P[l:lO] 
ci Q[l:lO] and (P[1:20])\10 N R[1:2]. The first con- straint is processed as before, producing the representation 
shown in Fig. 8(a). Processingthe secondconstraint, which includes an ar- ray reference, produces the 
representation shown in Fig. 8(b). The nodes labeled u and w represent arrays consisting of 5 elements 
of size 2 each. We will explain in detail how our algorithm handles arrays and similar constructs in 
Section 6. A complete algorithm for the gen- eral version of the problem appears in pseudocode form in 
the ap- pendix. 4 The Equivalence DAG as a Unifier Readers familiar with unification may have observed 
that our algo- rithm has a unification flavor. Our algorithm can, in fact, be thought of as unifying 
terms belonging to a term language rv (defined be- low), with the following distinction: unlike in standard 
unification, we do not treat the operators @ and @ in this term language as free, i.e., uninterpreted 
operators; instead, we are interested in unifica- tion with respect to a specific interpretation of these 
operators. We explore this connection briefly in this section. For any set X, let rz denote set of terms 
defined by:  rX::= x 1 rxwx 1 Int+ @ix (1) where Int+ denotes the set of positive integers, Occasionally 
we will omit 8, abbreviating i @ r to in. Let V = U;>cV, denote a set of variables. A variable belonging 
to V; is said to have a length i. We will use the notation c : i to indicate that a variable z has a 
length i. Consider now the set of terms I v. Observe that we may in- terpret the trees rooted at any 
node in the Equivalence DAG as terms belonging to rv: leaves are interpreted as variables belong- ing 
to V; internal nodes denoting the concatenation of two ranges, such as those in Figure 6, may be interpreted 
as uses of the oper- ator $; nodes such as u and 2, of Fig. 8, representing arrays, are interpreted as 
uses of the operator 8. Let X denote the set of sequences of elements from X. Given a term T E l?x, the 
value ([T] E X is obtained from 7 by interpret- ing $ as sequence concatenation and @ as repeated concatenation 
(of a sequence with itself, as many times as indicated) Define the length of a term T E l?v to be the 
sum of the lengths of all variables in the sequence [r]I. A substitution u is a length- preserving mapping 
from V to rv:i.e., a function that maps every variable to a term that has the same length as the variable. 
The Equivalence DAG can be thought of as a substitution, restricted to a set of variables denoting program 
variables. For example, the DAG in Fig. S(b) represents a substitution (~8 e 5zR, ZP c) @XR)$@XR), XR 
I-$ XR}. Two substitutions ui and u2 are said to be equivalent if [oi(z)l = [ua(z)~ for all I. Every 
substitution u can be extended to map every term T E rv to a term U(T) E I?v. A substitution u is said 
to be a unifier for a set of unification constraints S c I v x l?v if [Us = l[u(-rz)] for every (~1, 
TV) E S. Further, it is said to be a most general unifier for 5 if every other unifier ur for S can be 
expressed as the composition of some substitution ua with a sub- stitution u3 that is equivalent to u: 
u1 = 62 0 ua. The translation rules in Fig. 9, specified in the Natural Seman- tics style, show how a 
set of unification constraints can be generated from a program. In particular, the translation rule S 
shows how a statement dl t cl2 can be translated into a set of unification con-straints of the form ~1 
rY 72 where ~1~7-2 E rv. The auxiliary rules D1, D2, and Ds show how a data-reference d E DataRef can 
be translated into a variable I E V and a set of unification constraints C constraining variable x (which 
we denote by d *D (x, C)). For example, if we have a data reference of the form d\n (rule Da), then we 
represent d by a variable, say x, and d\n by a vari- able, say y, where the variables x and y are related 
by the constraint n @ y E Z. The constraints generated from a program are simply the union of the constraints 
generated from the statements in the program. Let us illustrate this using the example of Figure 8. Here 
we have P, Q, R E ProgVars, with IP( = 20, IQ] = 10, and IR( = 2. We have two constraints P[l:lO] N Q[l:lO] 
and (P[1:20])\10 1! R[1:2]. We represent every program variable V E ProgVars by a constraint variable 
zv of length IV]. Processing the constraint P[l:lO] N Q[l:lO] producesthe substitution {ZQ I+ ZI : 10, 
XP e (U : lO)$(v : lo)}, where u and 2, are two new variables, which is represented by the Equivalence 
DAG of Figure S(a). Now consider the constraint (P[1:20])\10 N R[1:2]. This is effectively trans-lated 
into the unification constraint GP Z 10 @ CR (ignoring some superlluous variables that may be generated 
by a direct application of the translation rules of Figure 9). Since xp is already bound to (u:lO)@(v:lO), 
our algorithmunifies (u:lO)$(w:lO) with 10 &#38;3 (Z:R:2). Thisrequires splitting up lO@(Zi&#38;) into 
5@(~~:2)$5@ (2&#38;). The subsequent unification binds both u and 2) to 5 @ (2R:2). It is easy to see 
that the Equivalence DAG construction algo-rithm can be interpreted as computing a unifier for the constraints 
 Figure 8: An example illustrating our array equivalence algorithm. V E PmgVars where a unique variable 
xv is used for every pro- Dl VS-D (q7,Cl) gram variable v d =k-D (CZ, c) where ~1, x2, and XJ are fresh 
variables of lengths D2 d[i : j] *D (22, C U {~1@~2~~s 2 s}) i -1, i -i + 1, and 1x1 - j respectively 
d *D (2, C) Da where y is a fresh variable of length /xi/n d\n=F-D (y,CU{n@ysY}) Figure 9: Generating 
unification constraints from a program. generated in Figure 9. We also conjecture that unifier it computes 
is most general. 5 Application I: Type Analysis 5.1 The Problem In this section we extend the basic 
algorithm of Section 3 to ad- dress a generalization of the type analysis problem discussed in Section 
1. Consider the example depicted in Fig. 6. Assume the programmer wants to modify the representation 
of a field of the variable W, say W[1:2]. The question we wish to answer is What other variables are 
likely to be affected by this change, requiring corresponding modifications to their own representation? 
We now present a more precise formulation of the problem. Let ,C denote some semi-lattice with a join 
operator U. We may think of the elements of L as denoting abstract types. An example is the lattice of 
subsets of the set bear, notYear} used for year usage inference in the example of Fig. 2 of Section 1. 
A function A from Lot to L represents a typing for the set of locations. We say that ?r is valid with 
respect to a program P if r(Zr ) = ?r(lz) for all 11~~12, In other words, a typing r is valid with respect 
to P if the expressions on both sides of every assign- ment in P have the same type under 17. Now consider 
a constraint of the form d 2 c, where d C DataRef and c E L. We say that ?r satisfies this constraint 
if and only if ~(2) > c for every 2 E U set(o) u-L4 where set(c) denotes the set of elements in a sequence 
u. Given two typing functions al and 7r2, we say that ?rl 17~ if and only if ~1 (I) 2 ~a@) for all I 
E Lot. Given a program P and a set C of constraints of the form d >_ c, where d E DataRef and c E L, 
we are interested in computing the least typing valid with respect to P that satisfies every constraint 
in C. 5.2 The Solution We now illustrate how we can efficiently compute the desired solu- tion, using 
the data structure and algorithm presented in Section 3. We first process the equivalence constraints 
induced by program P to produce the Equivalence DAG. We then associate every leaf of the resulting DAG 
with a value from L, which is initially the least element of L. We then process every constraint d > 
c in C as follows. The data-reference d can be mapped onto a sequence of leaves in the Equivalence DAG 
(possibly after adding new break- points). We update the value associated with each of these leaves to 
the join of their current value and c. The example in Fig. 10 illustrates this. Assume we start with 
the Equivalence DAG of Fig. 6(h), and process constraint W[1:2] 2 bear). (We are using the lattice L 
of subsets of the set beur, notYear) described above.) W[1:2] corresponds to the single leaf tl (shown 
as a bold rectangle in Fig. lo), whose value is then updated to bear}. The resulting DAG can be viewed 
as a compact representation of tbe desired solution. In particular, it can be interpreted as a func- 
tion 7r from locations to C: the value associated with any location I is obtained by traversing the DAG 
to map location 1to a leaf el in the DAG (as explained earlier), whose value yields n(Z). In par- ticular, 
the DAG in Fig. 10 maps locations X[3], X[4], Z[l], 2[2], W[l], and W[2] to type bear} and every other 
location to type {}. Equivalently, the DAG in Fig. 10 may be viewed as a function mapping every program 
variable to a type term belonging to I c, where I is defined as in Equation 1. 6 Application II: Atomization 
In this section, we address the aggregate atomization problem through another extension of the basic 
algorithm of Section 3. We first con- sider some examples that illustrate several of the more subtle 
as-pects of the problem. @, 8 09 Figure 10: An example illustrating our type inference algorithm. We 
have used N as an abbreviation for {} and YR as an abbreviation for { year}. 6.1 Motivation Overlapping 
Data References Consider the problem of com- puting the reaching definitions for the use of NUM-DAYS 
OF MONTH [II in the example shown in Figure 4. In the absence of aggregates, determining whether any 
two direct (i.e., non-pointer) data references in a program can refer to the same set of locations is 
usually straightforward. However, in this example Cobol s aggre- gate model considerably complicates 
matters: we note that the ini- tialization of NAME OF JAN (M[1:3] in our mini-language) does notreachtheuseofNtJM-DAYS 
OF MONTH111 ((M[l:60] \ 12) [4:5]), but the initialization of NUM-DAYS OF JAN (M[4:5] in our mini-language) 
does. This follows from the fact that M[4:5] overlaps (M[1:60] \ 12) [4:5], while M[1:3] does not. It 
should be evident from this example that testing for overlap between two data references has the potential 
to be quite expensive, especially in the presence of arrays. Partially Killing Definitions Reaching definitions 
analysis is further complicated by the fact that one definition may partially kill another definition. 
Consider the following example in our mini language: Sl: z[l:lO] t y[l:lO] s2: z[1:5] t t[l:5] s3: ul[l:5] 
t z[1:5] S4: 2[1:5] t r[6:10] Clearly, the definition of z[l:lO] at Sl does not reach the use of 2[1:5] 
at S3, but it does reach the use of z[6:10] at S4, because the definition ofz[l:5] at S2 partially kills 
the definition of z[l:lO] at S 1. Although reaching definitions analysis can be performed in several 
different ways, the example illustrates the need to handle partially killing definitions accurately in 
order to compute precise results. The goal of atomization is to transform the input program into a semantically 
equivalent program in which all data references are atomic, thereby simplying program analyses such as 
the computa- tion of reaching definitions. In the case of the example above, we can transform statements 
S 144 so that each of the assignments is defined in terms of atoms zl, ~2, yl, y2, zl, and wl. The set 
of atoms partitions the set of locations such that every atom identifies a set of locations, and distinct 
atoms refer to disjoint sets. For in- stance, atom zl identifies the set of locations 2[1:5], and atom 
z2 identifies the set of locations z[6:10]. (The statement Sl can be thought of as an abbreviation for 
two assignment statements.) Sl: (Zl, 22) t (yl, y2) s2: zl t 21 s3: wl t rl s4: 21 t x2 As with reaching 
definitions analysis, many other standard pro- gram analysis techniques or transformations (e.g., partitioned 
anal- ysis, SSA-form construction, or program slicing) are not imme-diately applicable to programs containing 
overlapping data refer- ences or partially killing definitions. However, once a program is transformed 
into an equivalent one containing only operations on atoms, these complications can be ignored, since 
the atoms can be treated as simple scalar variables. 6.2 The Basic Ideas It should be clear from the 
preceding examples that an atom is in- tended to denote a set of locations that may be treated as a single 
logical unit for purposes of program analysis (we will make this notion more precise in the sequel). 
Recall from Section 5 that the leaves of the Equivalence DAG identify subranges of locations that can 
be treated as a single logical unit during type analysis. It should therefore not be surprising that 
the Equivalence DAG can also be used as the starting point for atomization. However, we need to exercise 
some caution in treating the leaves of the Equivalence DAG as atoms. Due to the sharing present in the 
DAG structure, a leaf may identify a number of different subranges of locations in the program, and each 
of these subranges must be treated as a distinct atom. This can be done by first flattening the Equivalence 
DAG into a forest (by duplicating shared subgraphs), then treating the leaves of the resulting forest 
as atoms. To formalize this intuition, we first note that the Equiva-lence DAG can be interpreted as 
a function cp mapping ev-ery V E ProgVars to a term in I?v. Let p+ be the sub-stitution obtained by renaming 
the variable occurrences in the set of terms {p(x) 1 z E ProgVars} such that no variable oc-curs twice. 
For example, if ProgVars = {A, B}, and Q = {A ti w&#38;yew, B I+ y@z}, then a suitable renaming is 'pr 
= {A I+- x~@Q@x~, B r-) x4$x5}. By renaming multiple occur-rences of the same variable, cpr abstracts 
away type equivalence information, while allowing us to determine for any program vari- able the aggregate 
structure that was inferred during the construc- tion of the Equivalence DAG. In the absence of arrays, 
we can then identify atoms with the variables occurring in the set of terms {Q?(Z) 1 x E ProgVurs}; we 
will use Atoms to denote this set. Once the atoms have been identified, the next step is to ex- press 
all data references in the program in terms of the atoms. In particular, we can replace every data reference 
by a sequence of atomic references. In the above example, the reference to x[l:lO] can be replaced by 
the sequence (xl, ~2). Our atomization algo-rithm guarantees that every assignment statement in the resulting 
programwillhavetheform(al,az,~.~,a,) t (bi,ba,...,b,), where every o; and bi is an atom. If desired, 
such a statement can be replaced by n simpler statements of the form a; t b;. 6.3 Dealing With Arrays 
Consider the following example: Ll: MOVE . . . TO A(5). L2: MOVE . . . TO A(I). L3: MOVE . . . TO A(6). 
 L4: MOVE A(5) TO . . .  A precise reaching definitions analysis can establish that definitions Ll and 
L2 reach the use in L4, while definition L3 does not. To ex- tend the atomization approach discussed 
in the previous section to arrays in such a way that no information is lost, we must ensure that our 
atomization distinguishes references to statically indeterminate array elements from references to statically 
determinate elements. Among other things, this means that A (5 ) and A (6 1 must be rep- resented by 
distinct atoms (say, US and ae), and that A (1) must refer to one of a set of atoms (containing a~, a6, 
and additional atoms as required to represent the other elements of A). More generally, consider the 
example of Figure 8. Here we must be able to model a reference to an entire array (P[1:20]), a reference 
to an array subrange (P[l:lO]), as well as a reference to a statically indeterminate array element (P[1:20]\10. 
To properly account for the various classes of array references discussed above, we must slightly relax 
the assumption in the pre- vious section that all data references in the original program could be redefined 
entirely in terms of sets of atoms. Borrowing from the notation for array element references in our mini-language, 
we will instead replace data references in the original program by atomic data references, which are 
defined as follows:  ::= Atoms \ Int+DataRef,,,;, Intuitively, z\n E DatuRefAtomic represents an indeterminate 
reference to exactly one element of a set of n determinate refer-ences. We will refer to n as the multiplicity 
of the reference. Note that the union of locations referred to by s\n need not itself be contiguous sequence 
of locations, e.g., when c\ta refers to a single field of an indeterminate element of an array of multi-field 
records. In the limit case, z\l is used for all determinate references. Not unexpectedly, such references 
include contiguous ranges of loca- tions such as simple scalar variables and references to entire arrays. 
However, determinate references may also denote noncontiguous ranges of locations, e.g., given an array 
of records, each of which contains two fields Fl and F2,the collection of references to the F2fields 
of all array elements. Consider, for example, the Equivalence DAG of Figure 8, which represents the substitution 
+ = { Q ti 5zR, z:p t-b (5ZR)$(5zR), tR I+ IR) Renaming the variables gives us pr = { Q I-) 5a1, GP 
++ (5a2)$(5as), 2R I-) a4) with atoms ~1~~x2, aa, and ~4. Note that atoms 02 and aa represent different 
parts of the array xp. The data reference P[1:20]\10 rep-resents an arbitrary element of the array. Consequently, 
its repre- sentation in terms of atomic data references is given by (a2 \5, as \5 In contrast, the full 
array P[1:20] is represented by the singleton set {aa\l . aa\l}.We see from this example that if an array 
has been fragmented into several sub-arrays, then a reference to an arbitrary element of the original 
array must be represented as a ref- erence to an element of one of the array subranges resulting from 
the fragmentation. In the general case, a data reference must be replaced by a set of sequence of atomic 
references. Recall from Section 2 that we defined the semantics ZJ[q of a data reference d as a set of 
sequence of locations. The atomization process allows us to think of an arbitrary data references as 
a set of sequence of atoms instead of set of sequence of locations. Because of this connection, we denote 
the function that maps every data reference to a set of sequences of elements of DataRefAtomic by V1. 
The pseudocode in Fig. 13 shows how 2)1 is defined in its full generality. 6.4 Atomization versus Unification 
It is worth noting that atomization imposes stricter requirements on the construction of the Equivalence 
DAG than does type analysis or unification. In the context of type analysis or unification, the two terms 
T@T and 27 are completely equivalent. However, this is not true for atomization. The term T@T leads to 
twice as many atoms as the term 27,since it indicates that atomization should distinguish the first half 
of the array from the second half. Consider the example in Fig 8. Unification of (u: lO)$(v:lO) and 10 
@ (x~:2) creates the bindings u e 5 @ (zR:2) and v e 5 @ (z&#38;) and the unified terms can be represented 
by either 5 @ (z~:2)$5 8 (zR:2) or 10 @ (lR:2). To perform atomiza-tion, however, it is important to 
choose 5 @ (z~:2)$5 @ (dR:2) as the representation of the unified terms. The algorithm presented in the 
appendix takes this stricter requirement into account. 6.5 Atomic Data References and Reaching Definitions 
We note that two atomic data references x\i and y\j are disjoint whenever their atomic components differ, 
i.e., when a: # y. Thus for the purposes of computing reaching definitions, two data ref- erences z\; 
and y\j can overlap if and only if x = y. However, when determining whether a definition of one atomic 
data refer- ence kills (i.e., completely covers ) another, the multiplicity in-formation comes into play: 
z\i kills y\j if and only if x = y and i = 1. In other words, only a determinate reference can kill another 
reference. Thus for the purpose of computing reaching definitions, there is no reason to distinguish 
between different multiplicity val-ues greater than 1. However, the full range of values will be useful 
in the sequel to establish certain formal properties of the atomiza- tion. 6.6 Correctness Properties 
In this section, we formalize a notion of equivalence between a program and its transformed version, 
in which data references are replaced by corresponding atomic data references. We first define a function 
Si that maps every statement s E Stmt to a set of state- ments S C S~t.&#38;rn;c where ShtAtcmic ::= 
DataRefAtomic t DataRefAtcmic  SI(d1 + d2) = { m(i) +-uz(i) I (21 E 2)l[dl], CT2 E np21, 1 5 i I min(lml, 
1~21) 1 By ordering the set of locations identified by an atom a in as- cending order, we get a sequence 
of locations which we denote d[a]. Define a function 2)~ that maps every element ofDatuReS,,_,, to a 
set of sequence of locations as follows: D2[a\nl = { (o[sl, a[3 + 11,. . . , +I) I 1 5 i 5 TZ, u = d[a], 
9= (i-1) * (IuI/n) + 1, e = i * (lul/n) } Define function Se mapping every element of StmtAtomic to 
a set of ordered pairs of locations as follows: S,(a, t Q2) = { (Ul(i), u2(i)) I Ql E Dz[a1],a2 E D2[a2], 
15 i 5 min(lall, 1~21) 1  Observe that: Using SSA Renaming to Improve Precision The flow- for every 
statement dl c d2 in the given program. Thus, we can think of our atomization algorithm as decomposing 
the semantic function D into VI and Vz. In particular, note that the abstract semantics S(s) of a statement 
s can be fully recovered from the atomized statement Sa (s) . This formalizes the sense in which our 
atomization transformation is lossless . Note that S1 oniy models flow-insensitive program properties, 
since it does not distinguish between cases in which one of a set of possible assignments is executed, 
and cases in which all of a set of assignments are executed. It is straightforward to generalize SI to 
yield a program transformation that correctly models Sow-sensitive program properties. 7 Complexity Analysis 
 Let d denote the maximum number of atoms (as defined in Sec- tion 6) and arrays identified in a single 
aggregate. (For example, if we have a single aggregate whose atomization is ~1 $b(za@Ca), then d is 4. 
Equivalently, we may think of d as the maximum size of the atomization-trees produced.) Let f denote 
the total num- ber of atoms identified in the program and let s denote the total number of statements 
in the program. Our algorithm runs in time O(sd.cu(sd, f)) in the worstcase, wherea(., .) denotes the 
inverse Ackermann function. 8 Extensions Variables of Unknown Length Our basic algorithm assumes that 
all variables have a statically-determined finite length. We can extend our algorithm to deal with variables 
of statically indetermi- nate length (e.g., variable-length strings) by representing them as variables 
of (potentially) infinite length. One interesting issue that comes up in this context is the need to 
do an occurs check . Note that the algorithm presented in this paper binds variables only to terms of 
the same length. When all lengths are finite, this ensures that a variable can never be bound to a complex 
term containing the same variable. However, this is no longer true once variables of infinite lengths 
are allowed. We detect the creation of a cyclic term during unification, and replace it by a suitable 
array consist- ing of an unbounded number of elements. For example, unifying (Z : 2)$(y : co) with y 
: co results in binding y to 00 @ (Z : 2). Pointer Analysis Our full algorithm incorporates a points-to 
al- gorithm similar to that of Steensgaard [ 11, 121. Since both our algo- rithm and the points-to algorithm 
are unification-style algorithms, it is straightforward to perform both the analyses in parallel, in 
a common framework. This is not only convenient, it turns out to be necessary since in the presence of 
both (implicit) aggregates and pointers, the points-to analysis depends on atomization information while 
the atomization algorithm requires points-to information. The essential idea in this approach is as follows: 
For each pointer- valued variable p, we maintain a term rp describing the range of locations pointed 
to by p. Whenever two pointer-valued variables p and q need to be unified, the two corresponding pointed-to 
terms TV and TV are also unified. Note that any location (or range of lo- cations) may potentially store 
a pointer value. Hence, we associate a points-to term ~1 with every leaf 2of the Equivalence DAG. This 
effectively amounts to expanding our term language I v to encode points-to information. insensitive nature 
of our algorithm can introduce imprecision, es-pecially when variables are used for completely different 
purposes in different parts of the program. The technique of Static Single Assignment [4] renaming can 
be used to improve the precision of the results produced by our algorithm. One interesting issue that 
arises here is the interdependence be-tween the atomization problem and the SSA renaming problem: our 
atomization algorithm can produce more precise results if ap- plied after SSA renaming, while SSA renaming 
is easier to do after atomization since it does not have to deal with aggregates. One possible solution 
to this issue is to run the atomization al-gorithm once, apply SSA renaming, and then run the atomization 
algorithm again to produce a more precise atomization. (Iterating any further will not improve the atomization 
results.) 9 Related Work A substantial body of work exists in the area of type inference, following [9]. 
While our algorithm belongs to this family, what distinguishes it is that it is aimed at low level languages, 
where aggregate structure has to be inferred from the way memory (loca- tions) are accessed and used. 
The algorithm presented in this paper can be thought of as unification in the presence of an equational 
theory. Much previous work [S] has been done on unification in the presence of equational axioms (e.g, 
associativity) but we are unaware of previous work in this area for the specific equational theory that 
we are interested in. Several other authors [l 1, 10, 7, 161 have explored the appli- cation of type 
inference based techniques to program maintenance and software reengineering as well as program analysis 
for imper- ative languages. van Deursen and Moonen [ 161 present a type inference system for Cobol and 
describe its applications. What distinguishes our al- gorithm from theirs is the way we handle the unification 
of records. In their algorithm, the unification of two records causes the cor- responding fields of the 
two records to be unified only $ the two records have the same structure, i.e., only if they have the 
same number of fields, with corresponding fields having same length. Our algorithm is also similar in 
some respects to a points-to algorithm presented by Steensgaard [1 l] which accommodates C-style s tructs 
and unions. The problem common to both our paper and Steensgaard s is the unification of two aggregates 
with di$ering structure. In our approach, the result of unifying two structures Sl and Sa is a structure 
that is more refined than both &#38; and SZ. For example, unifying 2:4@y:4@2:4 with a:4@:2@:6 results 
in the structure z:4@b:2&#38;~:2@z:4 with the additional bind- ings a I+ Z, y ++ b@w, c t-+ w@z. In Steensgaard 
s algorithm, on the other hand, the unification of &#38; and 5 2 produces a structure that is less refined 
than both Si and Sa. In the above example, Steensgaard s algorithm [131 will stop distinguishing between 
the fields y, .a, b and c, and produce the unified structure ~:4@t:8, with the a being bound to x, and 
y, Z, b and c all being bound to t. As a result, our algorithm computes more precise results than Steensgaard 
s algorithm. Our algorithm was primarily designed to analyze legacy applications written in languages 
such as Cobol and PUI, where variables are commonly used to store aggregate data without necessarily 
declaring the aggregate structure of the vari- ables. We believe that in such a context our approach 
is prefer- able. However, whether our approach produces more precise re-sults when applied to typical 
C or C++ applications remains to be seen. O Callahan and Jackson [IO] use type inference to C programs 
to identify sets of variables that must share a common representa-tion and outline various applications 
based on this. 10 Future Work Future directions we wish to pursue include: [S] KNIGHT, K. Unification: 
A multidisciplinary survey. ACM Computing Surveys 21,1(1989),93-124. [9] MILNER, R. A theory of type 
polymorphismin programming. Journal of Com- puterand System Sciences I7 (1978), 348-375. Other Notions 
of Atomization Does our atomization algo-rithm produce the optimal (i.e., the least refined) atomization? 
We believe that it does, with respect to one reasonable definition of at- omization, though we have attempted 
no formal proof. However, if we relax the notion of an atom implicit in our algorithm, the atom- ization 
produced by our algorithm is not necessarily optimal. As an example, consider the program ~[l:lO] t y[l:lO]; 
2[1:2] t ~[5:6]. In this case, it is possible to generate the following atom- ized program (~1, ~2) t 
(yl, ~2); zl t x2 where atom zl de-notes the union of the ranges ~[1:4] and ~[7:10], while atom 12 denotes 
the range ~[5:6]. (The remaining atoms are defined in a corresponding manner.) However, our algorithm 
breaks up z into three atoms 2[1:4], ~[5:6], and 2[7:10], producing a more refined atomization than is 
necessary. It is possible to take the atomization produced by our algorithm and to improve it further 
by applying an algorithm somewhat sim- ilar to the finite state minimization algorithm (grouping atoms 
that need not be distinguished from each other into equivalence classes). It would be more interesting 
to see if such an improvement can be integrated directly into our atomization algorithm. Applications 
to Sparse Analysis The equivalence class parti- tioning of atoms produced by our algorithm can be used 
to con- struct (flow-insensitive) sparse evaluation representations for vari- ous analysis problems. 
Disjoint Unions Cobol programs may use REDEFINES for two purposes: to define disjoint unions or to define 
multiple views of the same data. The inability to distinguish between these two us- ages forces our algorithm 
to handle REDEFINES conservatively. It would be worth developing analysis techniques to infer the use 
of disjoint unions so that they can be handled less conservatively. More Sophisticated Type Systems One 
could extend the sim- ple type framework introduced in Section 4 in various directions, including adding 
more complex constructors and incorporating in-equality (or set) constraints [2, 51. Such extensions 
might enable more precise treatment of data located at variable offsets, over-loaded operators, and pointer 
arithmetic than are possible with our current approach.  References PI AHO, A., SETHI, R., AND ULLMAN, 
I. Compilers. Principles, Techniquesand Tools. Addison-Wesley, 1986. 121 AIKEN,A., AND WIMMERS, E. Solving 
systems ofset constraints. In Sympo-sium on Logicin ComputerScience (June 1992) pp. 329-340. [31 CORMEN, 
T., LEISERSON, C., AND RIVEST, R. Introduction to Algorithms. MIT Press, Cambridge, MA, 1990. CYTRON, 
R., FERRANTE, J., ROSEN, B. K., WEGMAN, M. N., AND ZADECK, F. Efficiently computingstatic single assignment 
formand the control dependence graph. ACM Transactions on ProgrammingLanguages and Systems z3,4 (199I), 
45 l-490. [41 FAHNDRICH, M., AND AIKEN, A. Program analysis using mixed term and set constraints. in 
Proceedings of the 4th International Symposium on Static Analysis (September 1997), vol. 1302 of Lecture 
Notes in Computer Science, Springer-Verlag, pp. 114-l 26. PI FERRANTE, J., OTTENSTEIN, K., AND WARREN, 
J. Theprogramdependence graph and its use in optimization. ACM Transactions on Programming Lun- guagesandSystems9.3 
(1987),319-349. [61 KAWABE, K., A. MATSUO, UEHARA, S., AND OGAWA, A. Variable clas- sification technique 
for software maintenance and application to the year 2000 problem. In Conference on Sofmare Maintenance 
and Reengineering (1998).J?. [71 Nesi and F. Lehner, Eds., IEEE Computer Society, pp. 44-50. (lo] O CALLAHAN, 
R., AND JACKSON,D. Lackwit: Aprogramunderstandingtool based on type inference. In Proceedings of the 
1997 International Confenxce on Sojiware Engineering (ICSE.96) (Boston, MA, May 1997), pp. 338-348. [I 
l] STEENSGAARD,B. Points-to analysis by type inferenceof programswith struc- tures and unions. In Proceedings 
of the 1996 International Conference on Com- piler Construction (Linkoping, Sweden, April 1996) vol. 
1060 of Lecture Notes in Computer Science, Springer-Verlag, pp. 136150. [12] STEENSGAARD, B. Points-to 
analysis in almost linear time. In Proceedings of the Twenty-Third ACM Symposium on Principles of ProgrammingLanguages 
(St. Petersburg, FL, January 1996), pp. 32-41. [ 131 STEENSGAARD,B. Personal communication, Oct. 1998. 
[141 TARJAN, R. E. Data Sttwtures andNetwork Algorithms. Society for Industrial and Applied Mathematics, 
Philadelphia, PA, 1983. [151 TIP, F. A survey of program slicing techniques. Journal ofProgrammingLan-guoges3,3 
(1995), 121-189. [16] VANDEURSEN,A., AND MOONEN, L. Typeinferenceforcobolsystems. In5th Working Conference 
on Reverse Engineering (1998), IEEE Computer Society, pp. 22&#38;230. [171 WEISER, M. Program slices: 
fomtal, psychological, and practical investiga- tions of an automatic program abstraction method. PhD 
thesis, University of Michigan, Ann Arbor, 1979.  Appendix We now present a complete description of 
our algorithm in SML- like pseudo-code. We assume that an implementation of the fast union-find data 
structure[ 14,3] is available with the signature shown in Fig 11. The function newvar creates a new element 
not equiv- alent to any other element (i.e., belonging to an equivalence class all by itself). The function 
union merges two equivalence classes into one, while the function equivalent indicates if two ele- ments 
are equivalent or not. In addition, every equivalence class has a value associated with it, whose type 
is the parameter a of the parametrized type a eqclass. The value associated with an equivalence class 
can be retrieved and modified by the functions f indval and setval respectively. The functions newvar 
and union take a parameter specifying the value to be associated with the newly created/merged equivalence 
class. We also assume the existence of a semi-lattice L (of types ) with a join operator U. In our implementation, 
we have a set of term variables (repre-sented by the type terravar in Fig 1 l), which are partitioned 
into a collection of equivalence classes (using the union-find data struc- ture). Every equivalence class 
has an associated value, which has the type ter%alue. The function 1x 1 returns the length of a variable 
x. In an actual implementation it will be more efficient to store (cache) the length with the variable, 
rather than compute it every time it is needed. The function . newvar. is a convenient wrapper for function 
It creates a new equivalence class with the initial value w, unless Y denotes a one element array, in 
which case the array element itself is returned. The function split (x, n) adds a breakpoint to the DAG 
rooted at x after position n. The function refine (x, n) returns the children of a concatenation node 
x. If x is not a con- catenation node, it is first converted into one by adding a break- point, preferably 
(but not necessarily) after position n. Note that every leaf of the Equivalence DAG has a value (belonging 
to semi- lattice L)associated with it. The function update (x, c) updates the value associated with every 
leaf of the DAG rooted at x by c. The main unification algorithm as well as our type analysis al- gorithm 
appear in Fig. 12. The basic ideas behind the algorithm were explained earlier in Sections 3, 4, and 
5. The four different unify functions implement the actual unification. The functions // An implementation 
of union-find with the // following signature is assumed type 'a eqClass val newvar: 'a -> 'a eqClass 
val union: ('a eqClass * 'a eqClass * 'a) -> unit val equivalent: ('a eqClass * 'a eqClass) -> boo1 val 
findval: 'a eqClass -> 'a val setval: ('a eqClass * 'a) -> unit // A semi-lattice L of types is assumed, 
with a // join/meet operator U type L val U : L*L->L datatype termvalue = atomic (L, int) 1 termvar $ 
termvar I int @ termvar withtype termvar = termvalue eqClass fun 1x1 = case (findval x) of atomic (c, 
1) => 1 I Xl @ x2 => IX1 + 1x21 I n@e lel => n * fun jTJ = case v of l@e => e 1 otherwise => newva 
 fun split (x, n) = if (0 < n) and (n < 1x1) then case (findval x) of atomic (c, 1) => setvallx, atomic 
(c, n) $ atomic (c, 1-n) ) I x1 $ x2 => if (n < 1x11) then split (xI,n) else split (x2,"-1x11) fi I 
m @ e => let val p = max(1, n/lel) in setval(x, F]e($ /(m-p)); split (x,n) end fi fun refine (x, n) 
= case (findval x) of Xl $ x2 =' [Xl, x21 I atomic (c, 1) => split (x,n); refine hn) I mc%e=> let val 
p = max(1, n/lel) in setval(x, m+GiTG); refine (x,n) end fun update (x, c) = case (findval x) of atomic 
(c', 1) => setval(x,atomic (c U C , 1))) I x1 @ x2 => update (xl, c); update (x2, C) 1 m @I e => update 
(e, c) fun unify (x.y) = if (not (equivalent(x,y))) then // Merge the two vars into one, setting the 
// value of the merged var to . . . unioncx, y, // . . . the join of the values of x and y case (findval 
x, findval y) of (atomic (c, 11, _) => unifyatom(Y,c) I (_, atomic (c, 1)) => Unifyatom(x,C) I (Xl @ 
x2, _I => UnifYlist ( [x1,x21,[Yl ) I (_, yl $ y2) => UnifYli,t([xl, [Yl,Y21) I (nl 63 el, n2 8 e2) => 
unifyarrays((nl,el), (n2,e2)) ) fi; findval x // return the value associated with // the merged equivalence 
class  fun unifyatom(x,c) = update(x,c); findval x fun unifylist(xl::r~, x2::r2) = if (1x11 = 1x21) 
then case (rl,r2) of elsif (1x11 > 1x21) then Unifyli,t( refine(xl,Ix2l) 0 rl, x2::r2) else /* (1x11 
< 1x21) */ Unifyli,t( xl::rl, refine(x2,Ixll) Q '2) fi  fun Unifyarraysc (nl,el), (n2,e2) ) = let fun 
exp (t,l) = t I exp (t,i) = t $ ex] 3 (t,i-1) 1 val m = least-common-mu: ii :iple~le~l,le2l~/le~l val 
x1 = exp(el, m) val x2 = exp(e2, m) val z = (nl* lell / 1x1 I in unify (xl, x2); z end  // The main 
procedures datatype constraint = termvar E+ termvar I termvar > L fun processConstraint (x S? y) = unify 
(x,y) I processConstraint (x k c) = update(x,c) fun solve 1istOfConstraints = apply processConstraint 
1istOfConstraints Figure 12: The unification and type analysis algorithms. Figure 11: Type definitions 
and auxiliary procedures. // Assume some suitable way of generating // names for new atoms. type AtomId 
val gensym: unit -> AtomId datatype AtomicReference = AtomId \ int datatype AtomicTree = atom of (AtomId 
* int * int) 1 AtomicTree gl AtomicTree 1 int &#38; AtomicTree // Assume a suitable implementation of 
program // variables" with the following signature type PgmVar val termVar0f: PgmVar -> termvar val setAtomicTree: 
PgrrVar * AtomicTree -> unit val gethtomiclree: PgmVar -> (AtomicTree option) datatype DataRef = ProgVar 
of PgmVar ( DataRef [ int : int I 1 DataRef \ int fun flatten(x,mu) = case (findval x) of atomic (c, 
1) => atom (gensymO,l,mu) I y $ z => (flatten(y,mu)) $ (flatten(z,mu)) 1 i @ y => i B (flatten(y,mY*i)) 
fun atomicTreeOf pgmvar = case (getAtomicTree pgmvar) of SOME atomicTree => atomicTree 1 NONE => ( 
setAtomicTree (pgmvar, rightAssociate( (flatten (termVarOf pgmvar,l)) ); atomicTreeOf (pgmvar); ) fun 
treesOf(ProgVar x) = { atomicTreeOf x } 1 treesOf(d [i:jl) = { subrange (t, i, j) 1 tEtreesOf(d)} I treesOf 
(d\n) = UtCtreesOf(d) breakup (t, I t I /n) fun leaves(atom (a,l,mu),m) = [ a \ (mu/m) 1 I leavestxl 
g x2.m) = leaves(xl,m) Q leaves(xg,m) leaves(i &#38; x,m) = leaves(x,m*i) fun Dl[d] = { leaves(t,l) 
1 t E (treesOf d) } fun rightAssociate ( (xl e x2) 2 x3) = rightAssociate (xl a (x2 a x3) ) I rightAssociate 
(xl a x2) = (rightAssociate xl) &#38; (rightAssociate x2) rightAssociate (i B x) = i &#38; (rightAssociate 
x) rightAssociate (atom (a,l,mu)) = atom (a,l,mU) fun breakup (t,s) = if (ItI = s) then { t } else case 
t of x1 B x2 = if ( I x1 I > s) then breakup (xl, s) U breakup (x2, s) else { head(t,s) } U breakupc 
tail(t,s+l), s) fi ( i &#38; x => breakup (x, s) fun subrange (t, i, j) = head ( tail(t,i), j-i+1 ) Figure 
13: The atomization algorithm (part 1). fun tail (t, 1) = t I tail (xl e x2, i) = tail (x2, i -1x11) 
 fun head (xl e x2, i) = if (i > 1x11) then x1 &#38;head (x2, i -/xl]) else x11 head (t, s) = t Figure 
14: The atomization algorithm (part 2). solve and processconstraint show how the Equivalence DAG can 
be constructed and how type analysis can be performed. The atomization algorithm appears in Figs. 13 
and 14 and was explained in Section 6. We assume the existence of a function gensym that can be used 
to generate new symbols to represent atoms. As explained earlier, every top level program variable is 
associated with a node (a term variable) in the Equivalence DAG. The function termvarof is assumed to 
return this. The first step in atomization is to take the DAG rooted as this node and flatten it to construct 
an atomization tree . The function flatten (x, mu) shows how a termvar x can be flattened into a AtomicTree. 
The secondparametermu is used to compute the multiplicityof the leaves in the atomic tree, where the 
multi- plicity of a leaf in an atomic-tree is defined to be the product of the cardinalities of all arrays 
in the tree that contain the given leaf. In order to simplify some of the other fimctions, the gener- 
ated atomic-tree is normalized to be in right-associative form (by the function rightAssociate). We assume 
that the imperative functions setAtomicTree and getAtomicTree let us asso- ciate an atomic-tree with 
every program variable. (The function atomicTreeOf constructs the atomic-tree and associates it with 
the corresponding program variable.) Once an atomic-tree has been constructed for a program vari- able 
X, any data-reference based on X can be mapped onto a set of subtrees of the atomic tree corresponding 
to X. The function treesOf does this. Please note that the input data-reference can not be an arbitrary 
one. It is assumed to be one of the data-references in the original program with respect to which the 
Equivalence DAG was constructed. (Hence, the Equivalence DAG and the atomic- tree are guaranteed to have 
breakpoints corresponding to the end- points of data-reference d.) Given an atomic reference a\ i, let 
us refer to i as the denom- inator of the atomic reference. The tin&#38;ion leaves shows how any subtree 
S of an atomic-tree T can be converted into a sequence of atomic references. This essentially is the 
sequence of leaves of the subtree combined with an appropriate denominator. The de- nominator of a leaf 
2 is simply the multiplicity of 1 in the tree T divided by the multiplicity of 1 in the subtree S. 
			
