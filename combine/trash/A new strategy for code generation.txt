
 A New Strategy for Code Generation the General Purpose Optimizing Compiler Permission to make digital 
or hard copies of part or all of this work or personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee.&#38;#169; 1977 ACM 0-12345-678-9 
$5.00 William Harrison IBM Thomas J. Watson Research Center Yorktown Heights, New York 10598 ABSTRACT: 
This paper presents a systematic approach to the problem of generating good code with a compiler that 
is easy to construct. A compiler structure is proposed which relies on interprocedural data flow analysis, 
global optimization, and an intermediate language schema to simplify the task of writing the code generating 
portions of a compiler without sacrificing code quality. This structure is contrasted with a more conventional 
structure to explore the reasons why the new structure solves several problems inherent in the conventional 
structure. Further advantages which accrue from the new structure are also presented. OVERVIEW Considerable 
effort has been spent over the last decade producing a systematic strategy for the syntax processing 
component of compilers. During this time, however, no adequate framework has evolved for struc­turing 
the remaining components of an optimizing compiler. Recent advances ,z, ~ in program analysis have opened 
the way to a new compiling strategy which may provide such a framework. This strategy is the general 
purpose optimizing compiler (GPO compiler) which is being developed by the Experimental Compiling System 
group at the IBM Research Laboratory, Yorktown Heights. The GPO compiler strategy has evolved from the 
synthesis of four fundamental ideas: 1) the development of a schema for intermediate languages, 2) the 
expression of language semantics through defining procedures which take the place of code generators, 
code macros, and library source code, 3) the application of global data flow analysis to the defining 
procedures and source programs, and 4) the repeated application of machine­independent optimizations. 
The role of each of these is discussed in turn. Intermediate Language Schema Most compilers perform the 
translation from source to target in a series of stages using a different representation of the known 
information at each stage. These represent­ations, which include both text and symbol table informa­tion, 
are often referred to as intermediate languages. In order to facilitate the application of global flow 
analysis and optimization at any stage, it is imperative that a GPO compiler embody a schematic representation 
into which all intermediate languages, from source to target, may be embedded. This representation, called 
the IL Schema, meets several important objectives. It supports languages such as PL/1, ALGOL, COBOL, 
and FORTRAN. It makes the execution of a procedure integrated at compile-time equivalent to invocation 
of the procedure at run-time so that the compiler is free to chose whether or not to integrate any procedure 
into any particular invocation, It introduces no ambiguities into control and data flow analysis that 
were not characteristic of the source lan­guage so that the quality of the code resulting from a source 
language will not suffer from its translation through the schema. Finally, it contains a minimal number 
of language constructs with built-in semantics so that the largest number of constructs are available 
for analysis and optimization. Several broad characterizations of this IL Schema can be made. It contains 
no dictionary holding particular information about the data types, bounds, or other attributes of variables. 
Such information is expected to be held as the values of ordinary variables. The instruction format uniformly 
represents the invocation of procedures, whether they are primitives or source language proced­ures. 
No expressions may appear as operands since the call by reference interpretation of these expressions 
at run-time may differ from the call by name interpreta­tion imposed by procedure integration at compile-time. 
There are no built-in constructs for exception handling nor are there even arithmetic or branching instructions. 
The schematic language is somewhat higher than a conventional machine language in that details of storage 
mapping and address computation are suppressed. In addition, the IL schema provides for a naming convention 
which allows the variables containing type information to be associated with the variable they describe. 
It also includes provision for describing the occurrence of block structure in a program, for describing 
possible storage equivalencing relations among variables, for indicating the occurrence of indirect addressing 
or its inverse, and for denoting program points as labels or entries. The following sample procedure 
for generic addition of fixed or floating point numbers illustrates the use of this naming convention: 
ADD: PROCEDURE(X,Y,Z); DO CASE X.TYPE EQ FIXED : FIXADD(X,Y,Z) ; FLOAT : FLOATADD(X,Y,Z); END; END; 
Defining Procedures In order to relate the semantics of a source language to the semantics of a target 
machine, it is necessary for a compiler designer to specify at least one most general code sequence which 
implements each source language operation. This is the same specification which would be used for an 
interpreter of the operation. However, such a general definition ignores many possibilities for optimiza­tion. 
These possibilities arise because the context in which an operation is actually used may make the information 
needed to interpret some parts of the operation sequence available at compile time. For example, the 
most general code for referencing an array element in PL/I would include examination of subscript range 
enablement, the range checking itself, and the formation of a sum of products of the multipliers and 
subscripts. In a GPO compiler, the semantics of a source level operation is expressed as a conventional 
procedure whose invocation at run-time would always produce the correct result. These procedures, called 
defining procedures, may be viewed as the code for an interpreter, or as macros whose expansion produces 
the code desired from a compiler. By using procedure integration, the defining procedure for an operation 
can be incorporated into a calling procedure. This involves performing by-name substitution for parameters 
and renaming of local varia­bles. The integration of defining procedures merely eliminates the overhead 
of procedure call. In traditional compilers for large languages like PL/Is, the macro­expander or code-generator 
generally embodies the ability to execute tests at compile time in order to tailor the code sequences 
to the circumstances of their invocation. For example, the code generator for subscripting in a PL/I 
compiler would usually test the enablement of subscrip­trange at compile-time, since the enablement of 
conditions is known statically. In a GPO compiler, the function of such an interpretive selection mechanism 
is performed by global data flow analysis and optimization. Such analysis and optimization is independent 
of the particular charac­teristics of the language being compiled. For example, in a PL/I compiler, the 
types of data are always known at compile-time. Thus, at any invocation of the previously described ADD 
procedure, the value of X.TYPE would have been established by the sequence of assignments to attributes 
of X made by the DECLARE statement for X. Data flow analysis relates the assignment to X.TYPE to its 
use in ADD, allowing the compiler to determine which case applies at a particular invocation of ADD. 
Although the same ADD procedure might be employed in an compiler for a typeless language, the lack of 
declarations might cause the testing of X.TYPE to be deferred until execution time. Global Data Flow 
Analysis Global data flow analysis is the process of determin­ing how information is generated and used 
in a program. R is then possible to base various optimizations on locally available information derived 
from the global analysis. For example, data flow analysis can be used to derive a relation between those 
operations which set the value of a variable and those operations which use that value. This makes it 
possible for the optimization called constant propagation to carry forward the results of interpreting 
an operation at compile-time by bringing those results directly to the operations which use them. The 
technology for performing such an analysis of a program is becoming well understood2,1A,16, However, 
because the semantics of all operations are expressed as procedures, it is necessary that the analysis 
of a program take cognizance of some summary of preceding analyses of the procedures the program invokes. 
Recent workl~] 131 has also estab. lished techniques for such interprocedural analysis. By using interprocedural 
analysis, summary informa­tion which is asserted for the most primitive operations encountered in the 
compiling process can be mechanically constructed for the defining procedures built from them. Global 
Optimization In the development of a GPO compiler, a collection of optimization forms the basis which 
replaces the manual analysis conventionally applied to determine special code generation cases. Certain 
optimizations, such as constant propagation and dead code elimmationz are vitally necessary in a GPO 
compiler. Other optimizations, such as redundant expression elimination, range analysis6, variable propagation 
, and strength reduction may be part of any optimizing compiler. They assume greater importance in a 
GPO compiler since the code resulting from integrated procedures conttilns many redundtint presumably 
correspond closely to the source language operations. These operations would have been avoided by operations. 
Next, the resulting program is subjected to a human programmer presented with the task of writing repeated 
control and data flow analysis, optimization, the original program entirely in a low-level language, 
It and procedure integrations, The procedure integration has been demonstrated that the application of 
such component and the analysis component are directed by optimization to a procedural description of 
concatenation (heavy lines) a library containing the defining procedures would, in a typical case, produce 
a code sequence in which and their data flow summaries, This transformation 25 instructions are executed 
as compared to the 35 results in an IL/P program, i.e. one completely in terms of instructions required 
by an available optimizing compiler prtmitive operations, Although the choice of this primi­or the 85 
instructions required by an interpreter, tive level is somewhat arbitrary, it is intended to be a regularized 
version of the machine operations. The IL/P STRUCTURE OF THE COMPILER program result may be entered into 
the library as a new As can be seen from Figure 1. a GPO compiler has defining procedure. Finally, the 
storage mapping is three major transformatlorral components. performed. the lL/P is transformed to a 
machine oriented First, the source language translator converts the variant of IL/P called RL and register 
allocation and final input form into its IL language, IL/S. The IL/S operators assembly take place. RL 
preserves the IL/P program but IL/P and IL/S Summaries IL/P Storage Mapping and -Analysis IL toRL Conversion 
I Instruction Scheduling, RegisterSummaries Allocation, Optimization and Final Expansions t Assembly 
Procedure Integration o IL/S IL/P Machine and Code Summary Figure 1 General Structure of a GPO Compiler 
31 annotates the operands of the IL/P program with whatev­er addressing Information is needed. Register 
allocation is then performed, followed by final assembly. A small example will give some insight into 
the interaction of optimizations with defining procedures to produce tailored code. For this example, 
we will consider the translation of two lines of a PL/I program to the IBM System/370. The two PL/I statements 
are: 1. DECLARE I FIXED BINARY, A(10,1O) FIXED BINARY, B(2O,1O) FIXED BINARY; .. 2. A(I,l) = B(I,l); 
Since the emphasis in this example will be on tailoring the subscripting operation, the data type issues 
which would actually arise in PL/I will be ignored. T his is reasonable since it is clear that constant 
propagation of the FIXED BINARY attribute eliminates any conver­sions. The translation of these two statements 
into IL/PLI could then be as shown in Table 1. The SUBSCRIPT operation is a general PL/I sub­scripting 
procedure which computes a pointer to the array element whether the array is based, controlled, or parame­ter, 
or whether the bounds are known or unknown. Consider the integration of the subscript procedure into 
the above program, as shown in Table 2. Examination of the optimization of the code resulting from statement 
2 shows that, in fact, optimal code can be derived automati­cally from the general procedure. Constant 
propagation will drive all of the bounds and multiplier information from the declaration in statement 
1 into the operands of the code for statement 2, Furthermore, when constants are propagated to all input 
arguments of an operation, the result can be computed and propagated further. By keeping the data flow 
information up to date, dead code elimination proceeds simultaneously. Table 2 shows the 1.1 MOVE 1,2 
MOVE 1.3 MOVE 1.4 MOVE 1.5 MOVE 1.6 MOVE 1.7 MOVE 1.8 MOVE 1.9 MOVE 1.10 MOVE 1.11 MOVE 1.12 MOVE 2.1 
SUBSCRIPT  2.2 SUBSCRIPT 2.3 MOVE results of constant propagation, with dead code marked by a bullet 
(.). It should also be noted (hat after constant propagation, all code resulting from sta[ernent 1 has 
become dead. Several other optimizing transformations can be interleaved with constant propagation. These 
are: range analysis, redundant expression elimination. and operation simplification. Operation simplification 
includes collection of operator dependent transformations which are made when various operand criteria 
are met. For example. y=x+O * y = x is an operation simplifica­tion. Table 3 shows the result of applying 
these transfor­mations. A new data flow analysis pass is made to form the information needed for the 
variable propagation optimi­zation. These transformations are applied, opening the way for another round 
of optimization. In the case under consideration, however, optimization of the IL/P program is complete. 
Table 4 shows the final IL/P program corresponding to the sample input program. The process of integration 
and optimization has reduced the original IL/PLI program to an IL/370 program containing only machine-like 
primitives. These primitives are selected to reflect, one-to-one, the semantics of the target machine 
s operators without the complex operand accessing structure present in most machines. This accessing 
structure, which generally includes address formation, indexing, and indirection, is instead exposed 
as instructions in the code stream. This allows the optim­izing transformations to ignore special addressing 
charac­teristics of the target machine. In a GPO compiler, storage mapping completes this process of 
exposing all computations by entering into the instruction stream all instructions needed for address 
calculation. In the exa mple under discussion, this adds the instructions numbered 0.1 through 0.7 in 
Table 5. These A. LBOUNDI = 1 B, LBOUND1 = 1 A. HBOUND1 = 10 B. HBOUND1 =20 A. MULTIPLIER1 =20 B. 
MULTIPLIER1 =20 A. LBOUND2 = 1 B. LBOUND2 = 1 A. HBOUND2 = 10 B. HBOUND2 = 10 A. MULTIPLIER2 = 2 
 B. MULTIPLIER2 = 2 T1 =A(I,l) T2=B(I,1) indirect(T 1) = indirect Table 1 IL/PLI Version of Source 
Program E # Code after Integration Code after Constant Propagation \ I* 2.1.1 SUB T5 = l A. LBOUND2 SUB 
T5= 1-1 II  2.1.2 MULT T6 = A. MULTIPLIER2*T5 MULT T6= 2*0 I I 2.1.3 SUB T7 = I A. LBOUND1 SUB T7=I 
 I I I 2.1.4 MULT T8 = A, MULTIPLIER1 *T7 IMULT T8 = 20*T7 I 2.1.5 ADD T9 = T6+T8 IADD T9 = O+T8 I 2.1.6 
ADD T1 = address-af(A)+T9 I unchan~ed I 2.2.1 SUB TIO = 1-B. LBOUND2 ]SUB TIO= l-1 I* 2.2.2 MULT T1 1 
= B. MULTIPLIER2*T1 O IMULT T] I =2*O I* 2.2.3 SUB T12 = I B.LBOUND1 ISUB T12=I I I 2.2.4 MULT T13 = 
B. MULTIPLIER 1*T12 MULT T13=20*T12 2.2.5 ADD T14=T11+T13 IADD T14=O+T13 I 2,2,6 ADD T2 = address-o~(B)+T14 
unchanged I I 2.3.1 MOVE indtrecdT 1) = indirect(T2 ) unchanged Table 2 Result of Constant Propagation 
# Code after Constant Propagation Code after Redundant Operation Expression Elimination Simplification 
and .1,3 SUB T7=I 1 unchanged .1.4 MULT T8 = 20*T7 unchanged ,.1.5 ADD T9 = O+T8 MOVE T9 = T8 :.1.6 ADD 
T 1 = address-of(A) +T9 unchanged !.2.3 SUB T12=I 1 T12is T7 !.2,4 MULT T13=20*T12 T13 is T8 ;.2.5 ADD 
T14=O+T13 MOVE T14=T8: !.2.6 ADD T2 = addres~-~~(B)+T14 unchanged !.3.1 MOVE indirect(T 1) = indirecl(T2 
) unchanged Table 3 Result of Redundant Expression Elimination and Operation Simplification 33 instructions 
have the form ADD Ti = DSA+offt or ADD T,= SI+off,, where the off, are the the offsets in the automatic 
storage(DSA) or static storage assigned to variable i. Any references to address(i) in the program are 
changed to T,. t This transformation occurs in two steps. Operation The process of converting the primitive 
IL/370 simplification changes the ADD to a MOVE and redun­dant expression elimination replacesT13 with 
T8. program to the corresponding RL/370 program is Code after Redundant Expression Elimination and # 
Final Code After Subsumption/Renaming Operation Simplification 2.1.3 SUB T7=I 1 unchanged 2.1.4 MULT 
T8 = 20*T7 unchanged 2.1.5 MOVE T9 = T8 unchanged 2.1.6 ADD T1 = address-of(A)+T9 ADD T1 = address-of(A)+T8 
2.2.5 MOVE T14=T8 unchanged 2.2.6 ADD T2 = address -of(B)+Tl 4 ADD T2 = address-of(B) +T8 2.3.1 MOVE 
indirect(T 1) = indirect unchanged Table 4 IL/P Code after Optimization # Code after Storage Mapping 
RL/370 Code 0.1 ADD T.= DSA+offA 0.2 ADD T,, = DSA+off,l 0.3 ADD T,= DSA+offl 0.4 ADD T., = DSA+off,, 
0.5 ADD T,g = DSA+off,, 0.6 ADD Tl=SI+off, 0.7 ADD T,,, = SI+off,(, 2.1.3 SUB T7=I 1 SUB DSA,offJT7]; 
DSA,offl[I]; SI,off, [l] 2.1.4 MULT T8 = 20*T7 MULT DSA,off~,[T8]; SI,off,,)[20]; DSA,offr,[T7] 2.1.6 
ADD T1 =T~+T8 2.2.6 ADD T2 = T,,+T8 2.3.1 MOVE indirect(T 1) = indirecf(T2 ) MOVE DSA,offA,T8[indirect( 
Tl)]; DSA,offl,,T8[irzdirecdT l)] Table 5 Result of IL-RL Conversion 34 accomplished by gathering together 
those operations Compiling Past the Machine Interface which can be absorbed into the operand semantics 
of the machine, For example, in addressing a single operand of an instruction, an IBM System/370 can 
perform the equivalent of the code sequence: ADD t=ll+k [for k<4095] ADD f= X+t reference indirect (t 
), The conversion of IL to RL is performed by the matching of such patterns in the data flow graph. When 
applied to the sample code described above, this process results in the the 3-instruction sequence of 
RL/370 code displayed in Table 5. Register allocation will convert this RL/370 code into a sequence of 
at most 5 machine instructions. COMPARISON WITH CONVENTIONAL COMPILERS The structure of a GPO compiler 
may be compared with that of a conventional compiler. [L Schema In the GPO compiler strategy, the IL 
Schema replaces the collection of disparate levels of intermediate language present in a conventional 
compiler. This single schematic representation provides for flexible support of multiple source or target 
languages. In addition, its use for the representation of input, output, and intermediate stages of compilation, 
as well as for the defining procedures themselves, makes it possible to write a single collection of 
analysis and optimization algorithms. As an additional benefit of this flexibility, a GPO compiler is 
free to perform various analyses and optimizations at more than one stage in the compiling process, making 
it unnecessary to find a single best time for optimization. Defining Procedures The GPO compiler strategy 
employs a library which contains, in addition to the bodies of the defining proce­dures, a summary of 
their characteristics. The summary information is a uniform description of the data flow and other characteristics 
of all operations, primitive and expandable. Thus the analysis and optimization algor­ithms are totally 
independent of the precise semantics of the operations. The fact that nonprimitive operations are merely 
procedures in the same IL schematic framework makes it possible for most of the summary characteristics 
to be produced mechanically by the same data flow analysis which later uses them. Those summary items 
which, like commutivity, are difficult to mechanize will be supplied by assertions about the defining 
procedures. A conventional compiler often performs peephole optimizationsz which detect the occurrence 
of certain machine-dependent code patterns which are subject to improvement. In a GPO compiler, a similar 
pattern matching process takes place to convert the primitive IL program to its RL form. The pattern 
match in this case is not limited to small sections of control flow, but rather, is applied in the context 
of the more general data flow graph. This matching and substitution process is per­formed to gather overexpanded 
operations back together into machine primitives. The overexpansion of operations often results from 
the exposure of addressing calculations in the IL/P code. However, such a gathering compo­nent will also 
produce optimizations of any code which was overexpanded by the programmer before being input to the 
compiler. Elimination of Special Casing In the GPO compiler strategy, the code generation phase of the 
conventional compiler has been replaced by a procedure integrator. Defining procedures in a GPO compiler 
contain only those instructions needed to express the semantics of the individual operation which they 
implement. It has been claimeds that code generators should provide for elaborate special casing. Special­casing 
compilers require major re-examination and revision when language changes are made. In fact, the cost 
of a revised compiler often approaches that of the original. This requirement for major revision when 
language changes are introduced seems to be inherent in the special-casing approach to optimization. 
This is caused by the fact that the conceptual model used for the design and specification of each language 
operation does not match the representation of operation semantics used by the compiler designer, In 
particular, the special-case compiler designer has distributed the semantics of each operation into the 
description of all others with which it may interact to produce special cases. This combinatorial effect 
means that although the compiler designer may correctly incorporate a language change into uses of the 
feature specifically changed, it is difficult for him to locate all cases where the semantics of the 
changed operation has been factored into the case analysis of other operations. What generally happens 
is that bugs develop in the compiler from outdated case analyses. These bugs are fixed by removing the 
offending special case and over time the quality of the compiler code degrades. The GPO compiler strategy 
of implementing only the general case operations and allowing the optimizations to derive the special 
case code systematically overcomes the disadvan ­tages of speciil casing. ADDITIONAL ADVANTAGES In addition 
to reducing the problems of cost, com­plexity, reliability, and code quality associated with existing 
compiler technologies, the GPOcompiler str~tegy brings with it numerous additional advantages. Varying 
Degrees of Optimization It becomes possible to use the same body of program­ming to implement varying 
degrees of optimization. For example, interpretive execution is achieved by totally or selectively avoiding 
the integration of defining proce­dures. In the limiting case, all instructions In the IL/S form of a 
program are merely converted into subroutine calls to some interpreter library. On tbe other hand, a 
quick and dirty compilation is achieved by performing rudimentary constant propagations of the data type 
attributes. Simple examination of the summary informa­tion of the defining procedures will indicate that 
no definitions of the value of these attributes occurs, so the constant propagation can be performed 
without extensive analysis. The cost of the alternating sequences of proce­dure integration and dead-code 
elimination which would result may be reduced to produce a quick and dirty compiler. This cost reduction 
can be realized by applying a pre-integration of the defining procedures into one another with an accompanying 
pre-analysis like that which has been suggested for conventional optimization ( 7. Pre-integration and 
pre-analysis is also useful to the fully optimizing compiler because the alternative expansions represent 
tailored library routines from which a selection can be made according to circum­stances. Proofs of Compiler 
Correctness The GPO compiling strategy of using defining procedures assists in proving the correctness 
of an imple­mentation. The correctness of various optimizations may be demonstrated on an overall basis, 
and then a particular language implementation must only demonstrate, using IZ that the defining procedures 
some accepted technique , are faithful to the language. Improved Programming Style Use of a GPO compiler 
also contributes to the use of improved programming style by allowing a program to be constructed from 
many small sub-programs without incurring the overhead associated with numerous proce­dure calls or with 
overly general procedures. Furthermore, the advantages of an abstract data definition can be realized 
without sacrificing the optimizations made possible by exploiting its underlying representation. Program 
Developmerrr Supporr In fact, the GPO compiling strategy can occupy a significant role in an overall 
programming support system. The same summary information and integration techni­ques applied to the library 
of defining procedures can be applied to a library of user programs. The compiler can verify that summaries 
of program behavior fit within their specified behavior. Optimization of Skslems The GPO compiler strategy 
opens the way to the optimization of large programming entities such as compilers, data managements, 
or operating systems. By compiling those components of a system which are totally internal, knowledge 
about all invokers can be used to eliminate inhibitions to optimization which arise from not knowing 
aliasing information about p~rameters and globals. ACKNOWLEDGEMENTS The Experimental Compiling System 
is being built upon a groundwork laid by Frances Allen, John Cocke, and Patricia Goldberg. Recognition 
should be given to J. Lawrence Carter, Paul Loewner, David Lomet, Robert Tapscott, and Mark Wegman for 
their contributions to this project. My thanks are due to these people and especially to Mary Harrison 
for their helpful suggestions in revision of this manuscript, REFERENCES 1) Allen, F.E. Interprocedural 
data flow analysis. Proc. IFIP Congress ~, North Holland Publishing Co., Amsterdam, 1974, 398-402 2) 
Allen, F. E., and Cocke, J. A catalogue of optimizing transformations. QE@ m oPtimi~~tion of Compilers, 
Rustin, R. (Ed.), Prentice-Hall, Englewoo~ Cliffs, N. J., 1972, 1-30 3) Barth, Jeffrey M. Interprocedural 
data flow analysis based on transitive closure. Report UCB-CS-7644, University of California at Berkeley, 
September 13, 1976 4) Carter, J.L. A case study of a new code generation technique for compilers Communications 
of the ACM, to be published 5) Elson, M., and Rake, S.T. Code generation technique for large language 
compilers. IBM Systems Journal, Vol. 9 No, 3 (1970), 166-188 6) Harrison, William Compiler analysis of 
the value ranges of variables. IBM Research Report, RC5544, T.J. Watson Research Laboratory, Yorktown 
Heights, NY., July 1975 7) H~rrison, William Formal semantics of a schematic intermediate language, 
IBM Research Report, RC6271, T.J. Watson Research Laboratory, Yorktown Heights, NY., November 1976 8) 
Herrick, Steven S., Donnely, David V. A survey of optimization techniques In compilers. National Technics] 
Information Service, AD-AO 17-404, Rome Air Development Center, September 1975 9) Liskov, B. H., Zilles, 
S.N. Specification techniques for data abstractions, IEEE Transactions on Software Engineering, Vol. 
1 No, 1, March 1975, 7-fi 10) Lomet, David Predicting the effects of call point information on called 
procedure optimization. Inter­nal Memorandum, October 1975 11) Lomet, D. B. Data flow analysis in the 
presence of procedure calls. IBM Research Report, RC5728, T.J. Watson Research Laboratory, Yorktown Heights, 
NY., November 1975 12) Jones, C.B. Formal definition in compiler develop­ment. Technical Report, TR25. 
145, IBM Laboratory Vienna, February 1976 13) Rosen, Barry K. Dtita flow analysis for procedural languages. 
IBM Research Report, RC5948, T.J. Watson Research Laboratory, Yorktown Heights, NY., April 1976 14) Rosen, 
B. K. Data flow analysis for recursive PL/I programs. IBM Research Report, RC52 11, T.J. Watson Research 
Laboratory, Yorktown Heights, N. Y., January 1975 15) Spillman, T.C. Exposing side effects in a PL/I 
optimizing compiler. Proc. IFIP Congress 7 I North . , Holland Publishing Co,, Amsterdam, 1971, 376-381 
16) Unman, J. D. Data Flow Analysis, Second USA Japan Computer Conference, ( 1975) 17) Urschler, G. Complete 
redundant expression elimina­ tion in flow diagrams. IBM Research Report, RC4965, T.J, Watson Research 
Laboratory, Yorktown Heights, NY., August 1974 
			
