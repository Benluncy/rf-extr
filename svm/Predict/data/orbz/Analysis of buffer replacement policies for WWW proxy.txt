
 ANAYLSIS OF BUFFER REPLACEMENT POLICIES FOR WW'W PROXY Ilhwan Kim, Heon Y. Yeom Joonwon Lee Department 
of Computer Science Department of Computer Science Seoul National University Korea Advanced Institute 
of Technology Seoul, Korea, 151-742 Seoul, Korea, 130-650 {ilhwan,yeom } @arirang.snu.ac.kr joon@glory.kaist.ac.kr 
Keywords: WWW,proxy, buffer replacement, cache, proxy ABSTRACT The advent of the Web service brought 
out the explosive growth of Internet, which necessitates a scheme to reduce the netwrok traffic. Caching 
is a popular method in reducing such traffic, and it is widely used in many Web servers, clients, and 
proxy servers. Since an object of Web service is accessed as a whole, the caching scheme for such object 
must be different from others that usually treat a piece of an object as a unit of caching. In this paper, 
we propose caching schemes for variable size objects. Performance of the proposed schemes are explored 
through trace-driven simulation studies. I. INTRODUCTION WWW service is considered to be the most successful 
and popular service in the Internet world. The explo- sive growth of the Internet is mainly due to the 
pop- ularity of WWW service. The HTTP protocol used for WWW service is designed to transfer only one 
document per one connection. If more than one doc- uments are needed, multiple connections should be 
set up. Though this protocol simplifies the design of servers and clients, it may lead to significant 
network Permission to make digital/hard copy of all or part of this work for personal or classroom use 
is granted without fee provided that copies are not made or distributed lbr profit or commercial advantage, 
the copyright notice, the title of the publication and its date appear, and notice is given that copying 
is by permission of ACM, Inc. To copy otherwise, to republish, to post on servers or to redistribute 
to lists, requires prior specific permission and/or a fee. &#38;#169; 1998 ACM 0-89791-969-6/98/0002 
3.50 latencies for clients and heavy network traffic. Fur-thermore, poplular servers are easily saturated 
with client's requests which aggravate the network laten- cies. Proxy service, which is originally introduced 
to provide Internet service to the nodes within a fire-wall, can be used to mitigate this problem [6]. 
By providing documents from its local disk cache with- out actually connecting to the remote host, the 
proxy server is an effective place to cache Web objects which otherwise experience long network latencies. 
It also helps reducing the network traffic and distributing the server load since the object only needs 
to be trans- ferred once to the proxy and subsequent requests can be handled from the proxy. Since the 
proxy has limited amount of storage space we should only keep the documents that will be requested frequently 
in the future. A Web ob- ject is treated as a whole by the Web protocol while traditional caching schemes 
treat a block of a file as a unit to be cached. Cache management has been extensively studied in areas 
such as file system and hardware cache memory [7],[2]. Main objective of such caching is to reduce access 
latencies by utiliz- ing the locality in reference pattern. In tranditional caching, data objects are 
managed using a fixed size page or block to enhance space utilization and to simplify replacement algorithm. 
We call this fixed size page oriented cache management. Unlike con-ventional cache schemes, the cache 
management for WWW proxy should consider variable size objects. We have identified two metrics which 
can be used to show the effectiveness of the replacement algo-rithms. One is the byte hit ratio which 
is the ratio of the total bytes that was hit to the total bytes re-quested. The other is the request 
hit ratio which is 98 Less Valuable 5+4+2 > New 05 O' 03 O~ O~ 5+4 < New  i .... 3 > New 5+2 > New 
 o .... iiiiiiiiiiiiiiiiiiiiiill Which combination should we choose? Figure 1: An example of VSOC replacement 
the ratio of number of hits to the number of requests to Web services. We then proceed with various algo- 
rithms which can be used for the variable size buffer replacement and analyze their performance implica- 
tions using simulation studies. From our simulation, it is shown that the general idea for the fixed 
size caching can not be directly applied to proxy caching. The size of the objects should be considered 
and the schemes handling the size have large performance im- plications. Another major issue in designing 
WWW caching proxy is the consistency of the cache contents, As discussed in [4] and [5], it is similar 
to the consistency problem in distributed file system. In this paper, we concentrate on the cache replacement 
policy and do not consider the consistency problem. The rest of this paper is organized as follows. In 
section 2, the problem is formally introduced and some algorithms for variable size object caching are 
presented. In section 3, we present trace driven simu- lation results to analyze various algorithms and 
con- clude the paper in section 4. II. VARIABLE SIZE OBJECT CACHE MANAGEMENT 1. A Motivating Example 
Before going into more details with the mechanism of the VSOC(Variable Size Object Cache) scheme, it 
would be illustrative to see an example that needs special care in cache management. Let us consider 
the case shown in Figure 1. The objects stored in the cache are sorted according to some criteria and 
they have different sizes. This cri- eteria could be age(elapsed time since last access) or popularity(frequency 
of requests). Since objects are sorted and a victim object to be replaced is usually found at the tail 
of the sorted list, this list can be regarded as a stack. Suppose a:new object is arrived and it is to 
be stored in the cache. In traditional caching schemes, the object 05 will be chosen as a victim to make 
room for the new object. In VSOC, however, it needs a con- cept of victim set instead of a single object 
since a set of objects may need to be replaced to make room for the new object. When selecting the victim 
set, total size of victim objects should be greater than the size of the object to be cached. Also, the 
aggregate value of the set for the given criteria should be the mini- mum of all possible sets of objects. 
Of course, this value will be less than the value of incoming object. It is apparent that the solution 
space of this prob- lem grows exponentially with the number of objects in the cache. We should select 
a set which has the minimum aggregate value as the victim. This prob- lem is similar to the 0/1 knapsack 
problem where the goal is to find the set of objects in the cache which maximizes the aggregate value. 
 2. The VSOC Management Definition 1. Variable Size Object Cache Man- agement Algorithm Let p = (Pt,P2,...,PN) 
be the given request sequence. Each request pi is the pair of (object-id, object-size). VSOC algorithm 
is an algorithm to manage the cache given the request sequence without partitioning the objects.o Like 
the fixed size paging algorithm, we can con-sider both online and ofltine algorithms for VSOC. The offiine 
algorithm is the one working with the knowledge of the forthcoming request sequences in advance. The 
online algorithm works without such knowledge, and thus it should make decision dynam- ically for coming 
requests. In other words, the online algorithm has to make a decision based only on the current request 
and the history. For the fixed size paging where the pages are of the same size, the cost of the page 
fault is the same for all pages. However, for VSOC, the cost should be set according to the size of the 
page since it is more expensive to transfer a large object over the network. We consider the following 
two metrics for the cost of page faults. 99 Definition 2. Request hit ratio and Byte hit ratio Let PNhit 
= Nhit/Nreq and PNmiss = 1-PNhit, where N~eq is the number of total requests and Nhit is the number of 
cache hits among them. PNhit is called request hit ratio of the caching algorithm. Also let PBhit = Bhit/Breq 
and PBmiss = 1 --PBhit ,where Bhit is the total size of objects serviced via cache hit and B~q is the 
total size of requested ob- jects. PBhit is called byte hit ratio.<> These two basic metrics can be used 
to evaluate the performance of caching system. For the WWW proxy using VSOC system, the average latency 
of WWW request can be expressed as follows. Average latency per object is defined as L = PN h it Tsetup 
( cl ient, proxy) + PBhit Ttrans(proxy, client )Bavg + P Nmiss ( Tsetuv (client, proxy) + Tset,,v (proxy, 
server)) + PBmiss (Ttrans (proxy, client) + Ttrans (server, proxy)) Bavg  where T~et~,v(x, y) : connection 
setup time between x, y in- cluding time spent to initiate request, Ttra,~s(x,y) : time to transmit a 
byte from x to y, Bavg : average size of each object. The next equation can be obtained after a little 
manipulation of the above formula. L = Tsetup(C,p) + Ttrans(p,c)Bavg + Tsetup(P,s)<PNmiss + PBmissRt) 
 where Rt = Tt~,,s(p, s)Ba,g /Tsetup(p, s). The first two terms in this equation are constants since 
they are for the latency from the client to the proxy. The last two terms denote the penalty due to cache 
miss. In the first equation, note that the ratio of PNmiss tO Psmiss in the cost function is deter- mined 
by the ratio between the time for connection establishment plus the request initiation and the time for 
transferring objects. For the WWW server which works with the sequence of TCP 3-way handshake, server 
spawn, sending request, process request, and reply, only the reply part is denoted by Ttra,s and Tset~,p 
accounts for the remaining cost. It is evident that PN,,iss is at least as important as PBm,ss. Of course, 
PNiniss and PBmiss cannot be totally in- dependent. As will be shown in the simulation analy- sis, there 
exists some trade off between them. Unlike caching of fixed size objects, Rt should be considered to 
obtain the optimal algorithm. 3. Experimental Algorithms The fixed size page replacement has been a hot 
topic for some time and the optimal offiine algorithm and several good online algorithms are known [1]. 
As shown in [3], the MIN algorithm based on TTNR(Time to next request) is optimal and the com- petitiveness 
of LRU is derived and verified through experiments. We believe that the same heuristics can be applied 
to our VSOC scheme and have conducted various experiments based on this belief. The follow- ing are some 
of the algorithms studied in this paper. Offline Algorithms TTNR(Time to next request) It is the optimal 
algorithm for fixed size paging known as MIN, OPT. Based on the future reference time, object with the 
latest reference is selected as the victim. FREQ(Frequency of request)For each object, the number of 
references is counted and the ob- jects are sorted according to the number of fu- ture references. The 
object with the fewest fu- ture request is selected as the victim. Online Algorithms LRU(Least Recently 
Used) It is the approxima- tion for TTNR. The object with longest time from last request is selected. 
LFU(Least Frequently Used) The object with smallest number of requests in the past is se-lected. When 
we apply these heuristics to VSOC, we have to decide how to select multiple objects if needed. The selection 
process for the size of the object should be used in conjunction with the above heuristics and is totally 
independent with each other. VSOC algorithms R~ (Replace until fit) This is a greedy algorithm that 
selects objects with lower value one by one from the sorted list until the new object can fit in the 
cache. R1 (Replace the first fitting one) R~ tends to kick out many small objects when a big object comes 
in which results in small number of ob- jects in the cache and relatively low PNhit. For the system with 
small Rt value, it would be bet- ter if small objects could have some advantage. R1 scans the objects 
from the lower value un- til it finds an object that is larger than the new object. CRy(Class based R~) 
CRt (Class based R1) The problem with RI and R~ is that objects with different sizes compete as if they 
are of the same size. One might think that it is fair to classify objects into several disjoint groups 
according to their size and let them compete within the group. We call the ap- proaches with this classification 
CR~ and CRy, respectively. Gsi~e(Greedy on value~size) Gtsize(Greedy on value/log2(size)) R~ can be thought 
of as a greedy algorithm. To take the size into consideration, we divide the val-ues(LRU order, TTN) 
with the size and use the resulting value to run greedy algorithm. It is also possible to use log:(size) 
instead of the size to lessen the effect of size. We call these two approaches Gsize and Gtsi:e respectively. 
  III. SIMULATION RESULTS In this section, we present the simulation results for the algorithms. 1. 
Simulation environment and as-sumptions The trace used for simulation is the proxy log from Seoul National 
University, Korea for the month of September, 1995. Since all the traffic from univer- sities in Korea 
go through SNU, we believe it is of sufficient size. Table 1 shows some of the character- istics of the 
trace. In the table, the Maximum PNhit and Maximum PBhit are the values obtained with the infinite cache. 
In other words hit rates when no object is ever re- placed. For the class based algorithms, we divided 
objects into three groups. The first group is for the objects with size less than 5 K bytes, the sec-ond 
group is between 5 K and 500 K bytes and the Total no. of requests app. 1.6M Total no. of valid requests 
app. 1.4M Total unique no. of objects app. 380K Total bytes transferred app. 23Gbytes Total size of unique 
objects app. 10Gbytes Maxinmm PNhit 0.7172 Maximum PBhit 0.5448 Table h Characteristics of SNU-PROXY 
trace third group is for the objects with larger than 500 K bytes. We have also divided the cache storage 
into three equal size partitions and assigned them to each group. For G~i~ and Gt~ze, the score for LRU 
and TTNR is determined as follows. score : TS/size Gsize , LRU TS/log2(size) Gtsi=~, LRU -TTNR/size G 
size , TT N R -TTNR/log2(size) Gtsi~e, TTNR where TS is the timestamp of last request and TTRN is the 
time to next request. The object with the smallest score is selected for replacement. 2. Results In this 
section, we evaluate the algorithms in-troduced in the last section by Pghit and PBh~t. We have applied 
the 6 VSOC schemes (R~,R1,CR~,CR1,Gsi~e, and Gtsi.-e) to the well known heuristics for the fixed size 
paging, TTNR for offline and LRU for online. Table 2 and Table 3 show the performance of VSOC schemes 
applied to TTNR and LRU respectively. For TTNR, PNhit approaches the maximum for this trace when the 
cache size is 1 Gbyte, which is about 10 % Of the total size of unique objects. It tells us that the 
working set size of this re- quest sequence is about 1 GB. Also, R~ is appreared to be a clear winner 
for Pghit and R1 and Gt~i.-~ are both perform pretty well for PBhit- 3. The effects on Pghit From figure 
3, it clearly shows that CR1 and R1 are superior to R~ or CRo~ in terms of PNhit. We also note that CR~ 
alleviates the unfairness of R~ and its performance is comparable to Rt with large enough cache space. 
i01 Cache Size (MB) 1 4 l0 40 100 400 1000 4000 R1 PNhlt 0.2766 0.3804 0.4956 0.5860 0,6962 0.7165 0.7172 
0.7172 PBhit 0.0810 0.0862 0.1085 0.1494 0.3128 0.4631 0.5429 0.5448 Re¢ PNhlt 0.1610 0.2309 0.3051 0.4406 
0.5289 0.6591 0.7111 0,7172 PBhlt 0.0745 0.1295 0.1896 0.2977 0,3737 0.4859 0.5387 0.5448 CR1 PNhlt 0,2647 
0.3203 0.3726 0.4849 0.5775 0,6649 0.7022 0.7172 PBhlt 0.0751 0.0862 0.0937 0.1235 0.1690 0.2809 0.4095 
0.5403 C}:~oo PNhit 0.2182 0.3343 0.4027 0.5307 0.6049 0.6786 0.7038 0.7172 PBhit 0,0411 0.1078 0.1637 
0.2666 0.3483 0.4621 0.5199 0.5448 Gsize PNhit 0.1142 0.1873 0.2574 0.4250 0.5584 0.6991 0.7172 0.7172 
PBalt 0.0085 0.0154 0.0233 0.0554 0.1145 0.3346 0.5289 0.5448 Glo9 PNhlt 0.1701 0.2480 0.3315 0.4625 
0.5766 0.7008 0.7172 0.7172 PB~it 0.0217 0.0308 0.0451 0.0853 0.1556 0.3612 0.5313 0.5448 Table 2: The 
performance of VSOC algorithms based on TTNR Cache Size (MB) 1 4 10 40 100 400 1000 4000 RI PNhit 0.1963 
0.2592 0.3117 0.4147 0.4883 0,5916 0,6452 0.7027 PBhit 0.0425 0.0541 0.0663 0,1072 0.1577 0,2926 0.3859 
0.5185 Rc~ PNhit 0.0584 0.1019 0.1437 0.2380 0.3195 0.4616 0.5656 0.6890 PBhlt 0.0263 0.0563 0.0856 0.1490 
0.2076 0.3191 0.4034 0.5173 CRI PNhlt 0.1910 0.2430 0.2955 0.3941 0.4695 0.5981 0.6577 0.7000 PBhlt 0.0570 
0.0687 0.0807 0.1213 0,1647 0.2790 0.3684 0.4951 CR¢o PNhlt 0.0872 0.1544 0.2218 0.3298 0.4281 03759 
0.6444 0.6973 PBhit 0.0172 0.0410 0.0732 0.1342 0.1915 0.3023 0.3834 0.4956 Gsize PNhit 0.0868 0.1377 
0.1920 0.3202 0,4292 0.6068 0.6840 0.7164 PBhit 0.0071 0.0092 0.0152 0.0311 0.0573 0.1606 0.2896 0.4805 
Glog PNhlt 0.1983 0.2847 0.3349 0,4342 0.5095 0.6220 0.6738 0.7100 PBaI= 0.0268 0,0573 0.0756 0.1127 
0.1548 0.2678 0.3667 0.4933 Table 3: The performance of VSOC algorithms based on LRU About the LRU type 
algorithms, the performance of Gsi_.e is about the same as the performance of CR~ while Glsi:e shows 
superior PNhit. It is due to the fact that the timestamp and the size are directly used for the cost 
function. For example, in G~i~ealgo-rithm, an object with size 20 KB and the timestamp value 10 has the 
same cost with another object with size 10 KB and timestamp value of 5 which is unfair in some sense. 
Therefore, when we used log2(size) instead of size, it makes the algorithm less sensitive to the size 
and it shows better performance. About TTNR type algorithms, there is not much difference in Pghit between 
different algorithms as shown in figure 2 and figure 3. Also, unlike LRU types, CRoo shows better performance 
than RI. 4. The effects on Pshit For the byte hit ratio(PBhit, Roo type algorithms gen- erally perform 
better as shown in figure 5. It does make sense since we can think of R~ type algorithms as fixed size 
paging algorithms. For fixed size paging, the goal is to maximize the hit ratio which is the num- ber 
of pages being hit. If we consider that objects are being stored in units of pages and the pages are 
con- secutively requested, it is almost the same as the byte hit ratio in VSOC. The reason that RL and 
G~i:e per- Hit rate vs. Cache size LRU VSOC Algorithms 0.8 0.6 ~ ~ El ~-' .-/"JF" ..-~ .~ ;.. ~R.,, .-" 
//</ ¢.--oCR1 0,2 -'" //~' i~-..~ G_$1ze ,i~,...~.~"-~ --Upp~ ~un(~ 0.0 i...... J 10 I00 1000 Cache$,z~(Mbyte=) 
 Figure 2: LRU Hit ratio form poorly is that there are less probability for large objects being in the 
cache for these algorithms. Another important observation is that there is less difference in byte hit 
ratio in LRU type algorithms than the TTNR types. TTNR is known to be optimal for fixed size paging and 
it shows that the schemes handling the size difference in VSOC has large effect on the performance. Hit 
rate vs Cache size Byte Hit rate vs. cache size 08 TrNR VSOC Aloor~ms 0.60 T[NR VSQC i~,~i',hms I ................................... 
~$-_=- = .... . :i---:-, ............................. - / .... // ,/ // / 0.4 ,A / / f= ~:// ~ :~ 
Rinf 2< O--~>CR1 0.2 "/" ~- -- e. CRinf - --q G-size T ~ "~ '~ G-Logs~ze ~" i-~--- ~ Upper bound / 10000 
Cache size (Mbytes) Figure 3: TTNR Hit ratio Byte Hit rate vs, cache size LRU VSOC algorithms 0.60 z~,/ 
 i 0.40 [I // / z / . . ... z / G---~D R 1 °'1 Q,2Q L~ / / . ~ -O Rin| 1 " / <-<~ Gslze . ~f~ "" ,/ 
V-~Glog-slze 1 10 100 1000 t0000 Cache size (Mbytas) Figure 4: LRU Byte Hit ratio   IV. CONCLUSION 
In this paper, we have identified the problem of cache management for the WWW caching proxy and showed 
the VSOC is different from the fixed size cache management. We have identified two metrics for evaluating 
the policies as the byte hit ratio and request hit ratio and the real cost to be some combi- nation of 
these two metrics. Even though these two metrics are closely related, it seems to be not possible to 
optimize both of them at the same time since they show trade-offs. The VSOC policy is essential in de- 
signing the WWW caching proxy an d depending on the algorithm employed, it could affect the caching ef- 
fectiveness greatly. From our experiments, it is cliear that the replacement policies known to be good 
for fixed size caching are not necessarily good for VSOC ~' ~ "i// 3 .. '.: Rinf 0.20 .g/" / <~-- OCR1 
/ ~ ~-: " ~ ~ /,/ .... Upper bound r O00 10 100 1000 10000 Gache $izl (Mbytes) Figure 5: TTNR Byte 
Hit ratio and there are other factors to be considered to make them effective.  References <RefA>[1] [2] 
[3] [4] [5] [6] [7] Y. Rabani A. Fiat and Y. Rabid. Competetive k-server algorithms. In Proceedings 
of the 31st IEEE Symposium of Foundations of Computer Science, pages 454-463, 1990. L. Rudolph A.R. Karlin, 
M.S. Manasse and D. D. Sleator. Competitive snoopy caching. Algorith-mica, 3:79-119, 1988. L. A. Belady. 
A study of replacement algorithms for virtual storage computers. IBM Systems Jour- nal, 5:78-101, 1966. 
M. A. Blaze. Caching in Large-Scale Distributed File Systems. PhD thesis, Princeton University, 1993. 
V. Cate. Alex -a .global filesystem. In Proceed-ings of the USENIX File Systems Workshop, May. 1992. 
A. Loutonen and K. Altis. World-wide web prox- ies. In Preliminary Proceedings of the First Inter- national 
World Wide Web Conference, Apr. 1994. D. D. Sleator and R. E. Tarjan. Armortized effi- ciency of list 
update and paging rules. Commu-nications o] ACM, 28:202-208, 1985. 103  </RefA>
			
