
 An Object-Oriented Modeling of the History of Optimal Retrievals Yong ZHANG1, Vijay V. RAGHAVAN1 and 
Jitender S. DEOGUN2 1Center for Advanced Computer Studies University of Southwestern Louisiana Lafayette, 
LA 70504-4330 2Department of Computer Science and Engineering University of Nebraska Lincoln, NE 68588-0115 
 Abstract Learning techniques are used in IR to exploit userfeed­back in order that the system can improve 
its peflormance with respect to particular queries. This process involves the construction of an optimal 
query that best separates the documents known to be relevant from those that are not. Since obtaining 
relevance judgments and constructing an oplitnal query involve a great deal of effort, in this paper, 
we develop a framework for organizing the history of op­timal retrievals. The j?amework involves the 
identi~cation of a hierarchy of document classes such that the concepts corresponding to higher level 
classes are more general than those of the lower level classes. The ways in which such a hierarchy may 
be used to retrieve answers to new queries are outlined. This approach has the advantage that the query 
specification is concept­based, where as the retrieval mechanism is numerically­oriented involving optimal 
query vectors. It is shown that the construction of a hierarchy of opti­mal queries can correspond to 
an object-oriented modeling of IR objecm. Furthermore, the resulting model can be eas­ily implemented 
using a relational DBMS. 1. Introduction In an Information retrieval (IR) environment, it is im­possible 
to perform perfect retrieval; that is, to precisely select only and atl relevant documents [1]. In order 
to pro­vide reasonable retrieval or optimal retrieval, researchers in IR have introduced learning techniques 
that exploit user feedback in order to improve retrieval effectiveness [1,2,3]. Most of the earlier work 
on the use of learning techniques Permission to copy without fee all or part of this material is granted 
providad that the copies are not made or distributed for direct commercial advantage, the ACM copyright 
notice and the title of the publication and its date appear, and notice is given that copying is by permission 
of the Association for Computing Machinery. To copy otherwise, or to republish, requiras a fee and/or 
specific permission. @ 1991 ACM 0-89791-448-1191 /0009/0241 . ..$l .50 have considered the enhancement 
of the retrieval results separately for each query instance by obtaining an opti­malquery. For the purpose 
of this work, an optimal query is defined to be a decision rule that optimally separates the documents 
judged relevant by a user from those judged non-relevant among a sample of documents examined by the 
user. But, whatever the system learns may be lost at the end of processing that query. It is well known 
that the learning process is very expensive in the sense that it takes time and effort to obtain the 
relevance judgernents that make up the samples in the training set. For an IR system, the knowledge that 
the system gains by the lear­ning process relative to some user need might include lhe optimal query, 
the set of documents retrieved by this query and some properties about the query. If the knowledge IE­lating 
to the history of optimum retrievals is accumulated, the effort for learning could be avoided, when users 
want to repeat the same request as the one in the history. More im­portantly, when a new information 
request is close to one in the history, the learning process for obtaining the corre­sponding optimal 
query would be shortened and the effort could be lessened by using the knowledge in the history. For 
keeping the history of optimum retrieval, Ithe system should support a long-term storage strnctute for 
accumulating the results of the learning process over the past. Obviously, the manner in which the system 
organizes this knowledge critically affects the utility of this effort. In this paper, we propose a hierarchical 
structure in which the knowledge about the past optimum retrievals is organized into conceptual entities 
at different levels of generality. By introducing concepts from the Object­oriented approach, we shall 
denote the conceptual entities as object classes, around which knowledge such as the description of the 
class, the operations allowed on the class and the rules that govern the behavior of the class and its 
operations can be grouped. We regard documents as objects. A set of documents forms an object class. 
The optimal queries that retriev(e a set of documents are treated as the operations on this object class 
and the rules for deciding an optimal query form the rule part of the class. The criterion of forming 
an object class is based on the conceptual cohesiveness between sets of documents which we shall define 
later. When a user has an information requirement in mind, he usually doesn t know the form of the optimal 
query with respect to his need. However he should have a certain intuitive conceptual idea about his 
requirement. Starting with that initiat knowledge, hopefully, the user can navigate through the historical 
hierarchy and locate the documents or a related optimal query through which the desired information can 
be retrieved quickly. This is the motivation of our ~search in this paper. The remainder of the paper 
is organized as follows. In section 2, the notions of conceptual distance and conceptual cohesiveness 
are defined which follow from the ideas pro­posed by Kodratoff and his coworkers [4]. However here we 
specialize the ideas to fit the situations in IR domain. Based on these notions, a hierarchy generating 
algorithm is presented in section 3. The general flow of the algorithm is derived from that proposed 
in [4]. But in our context, we desire a hierarchy of object classes instead of a hierarchy of prototypes. 
From the point of view of object-oriented database design, the automatic construction of a hierarchy 
of object classes is very important in the data modeling step [5]. Therefore, it is natural to consider 
the building of such a hierarchy on top of a database management system in order to take advantage of 
the facilities provided by the DBMS. In section 4, one of the data modeling techniques, called OMT, that 
is useful in implementing a hierarchy over a relational database is briefly illustrated. In addition, 
some strategies for making use of the knowledge in the history of optimum retrievals based on the database 
implem­entation are presented. Finally, the conclusions of this study are given in section 5. 2. Conceptual 
distance and cohesiveness In this section, we first describe an algorithm to learn generalizations from 
a set of samples. Then the notations of conceptual distance and cohesiveness are defined in order to 
determine the preferred generalization. 2.1 Concept learning from example Concept learning from example 
is a kind of induc­tive machine learning technique. The learning system is presented with independent 
instances representing a cer­tain class, and the task is to induce a general description of the class 
[6]. Considering the result of an optimum n%rieval as a potentially good grouping mechanism, the system 
is presented with a set of relevant documents representing a class. By using the technique of concept 
learning from example, the learning system obtains a general description of the class that can be regarded 
as the concept for the class. Based on the concepts learned from the sets of doc­uments that were optimally 
retrieved during the past, the system will cluster similar concepts in the sense that they have small 
conceptual distances between themselves and in turn obtain more general descriptions (concepts) by the 
technique of concept learning from example. This process is called clustering by generalizing and will 
generate a hi­erarchy of concepts. Intuitively, we would like to measure conceptual dis­tance between 
two concepts by inspecting how many com­mon properties they both have. The goal of concept learn­ing 
from example is to induce a preferable generalization of the examples. The generalization is able to 
reflect the con­ceptual distance that we intuitively prefer. Subsequently, we could formally define the 
notions of conceptual distance and cohesiveness based on the generalization. First, we provide an outline 
of the learning algorithm we chose to derive the preferable generalization from ex­amples or concepts. 
2.2 The learning algorithm for obtaining generalization In this algorithm, both examples and the generaliza­tion 
are described in the same representation language, which is of the form of conjunctions of Iiterals. 
All lit­erals have the from (T, x) where T is a term or keyword and x is the argument of T. The algorithm 
is based on the principle of a stmc­tnral matching. That is, the examples (or concepts) are successively 
transformed until they acquire approximately the same form. Then the generalization is obtained by re­taining 
only the common features. This kind of learning algorithm has been developed at LRI [7]. But in the realm 
of IR, some details should be treated diffenmtly. To illustrate this approach, let us consider the follow­ing 
two examples. For the sake of simplicity, the conjunc­tion symbols between literals are omitted. Let 
documents dl and dz have the expressions as below: dl: (Pattern-Recognition , 3)(Classification , 4) 
dz: (Computer-Vision , 2)(Pattern-Recognition , 4) (Scene-Analysis , 5)  One of the operations of the 
algorithm is to rewrite the examples (or concept) so as to reveal their common features. For the given 
examples, we have dl: (Pattern-Recognition , X2)(Classification , 4) dz: (Computer-Vision , 2)(Pattern-Recognition 
,X2) (Scene-Analysis , 5) Next, the algorithm will use the theorems of the rep­resentation language in 
order to identify features of one example that imply those exhibited by the other. For in­stance, documents 
have term Pattern-Recognition should have some importance for term Computer-Vision , Thus, there is the 
theorem V,(Pattern-Recognition, v) * 3X( Computer-Vision, x) Using this theorem one can rewrite the two 
examples as follows: dl: (Computer-Vision , Xl)(Pattern-Recognition , X2) (Classification , 4), dz: (Computer-Vision 
, Xl)(Pattern-Recognition , X2) (Scene-Analysis , 5). When no other common features maybe revealed, one 
simply drops the differing features between the two expres­sions and obtains a generalization of the 
initial examples: G(dl ,dJ: (Cornputer-Vision , Xl)(Pattern-Recognition , x2) 2.3 The contributions of 
terms to similarity and dissimilarity The terms of dl, d2, and G(dl ,d2) could be classified in to one 
of three categories as follows: Common terms Common terms refer to the terms from G(dl ,dz) which were 
initially present in dl and dz, such as (Patternltecognition, X2) For the common terms, their contributions 
to similari­ty and dissimiltity are defined as follows S(dl, dz, T) = 1 D(dl, dz, T) = 1 S(dl, dz, T) 
= O Dropped terms Terms deleted from dl and dz in order to obtain G(dl ,d2) are called Dropped terms, 
for exampkx (Classification ,4) and (Scene-Analysis ,5) The dropped term are regarded as an indication 
of dissimilarity between the two examples. Therefore, lthe contribution of a dropped term T to the dissimilarity 
and similarity between dl and d2 are defined as follows: D(dl, dz, T) = 1 S(dl, dz, T) = l D(dl, dz, 
T) =0 Terms introduced by Theorems Terms in G(dl ,d2) that &#38; introduced by theorems should be treated 
as the common terms. Such terms as (Computer-Vision , Xl) Their contributions to the similarity and dissimilarity 
are the same as those of the common terms.  2.4 Relative importance of the terms The process of obtaining 
a generalization of a set of examples (or concepts) is not a deterministic one. our objective is to find 
a good generalization, which is not necessarily the one that considers all properties common to the examples 
to be equally importan~ but rather the one that gives different weights depending on the relative im­portance 
of properties common to the examples. GeneraJly speaking, one would prefer the generalization that has 
the desirable property of reflecting the most important cctm­monality between the documents. To achieve 
this goal, the relative importance of the various terms should be given by a domain expert in the form 
of a weight associated to each term. In this context, general terms may be regarded as more important 
than specific ones. For instance, if T is a common term with weight U, then its contribution to the similarity 
and dissimilarity will be taken as S(dl, d2, T)=l*w=w, and D(dl, dz, T)= l S(dl, d2, T) = l w If T is 
one of the dropped terms, then its influence should be D(dl, d2, T)=l*w=w, and S(dl, dz, T) = 1 D(dl, 
d2, T) = 1 w 2.5 Conceptual distance and conceptual cohesiveness The total measure of similarity and 
dissimilarity esti­mated for a generalization G are defined respectively as S(dl, dz, G) = S(dl, dz, 
Ti), and z T~G{CM,TH,DR} D(dl, dz, G) = D(dl, d2, Ti), x Ti<{CM,TH,DR] where CM is the set of common 
terms, TH is the set of terms introduced by theorems, and DR refers the set of dropped terms. Now let 
the quality of G be measured by the distance function ~(dl, d,, G) = D(dl, dz, G)/S(dl, d,, G) Then 
the concepmal distance, which corresponds to the preferred generalization, is defined as conceptual 
distance(dl, d2) = The reciprocal of the conceptual distance is called the conceptual cohesiveness of 
dl and d2. That is, conceptual-cohesiveness = I/conceptual-distance Moreover, the generalization for 
which~is a minimum is considered as the concept to be learned from dl and d2. We denote such a generalization 
as G*(dl ,d2). The definitions above can easily be extended for any number of examples. Let us consider 
n documents dl, d2, ... , dn. Let G(dl, d2, ... , dJ be one of their generalizations. Based on this generalization, 
one can compute the similarity S(dl, d2, ... , d.,G), dissimilarity D(dl, d2, ... , d., G), and the distance 
function fidl, d2, ... , dn,G). Then, the conceptual distance is the minimum of ~ over all possible generalizations 
of dl, dz, ... , d.. The reciprocal of the conceptual distance is called the conceptual cohesiveness 
of the set {dl, d2, ... , d,}. The generalization with minimum f is denoted as G (dl, d2, ... , dJ. 3. 
Construction of a hierarchy of object classes In the previous section, we have discussed how to learn 
a concept corresponding to a set of documents. In this section, we describe au algorithm to construct 
a hierarchy of the concepts by learning concepts as generalizations of other concepts. Then the hierarchy 
of the concepts will be transformed to the hierarchy of the object classes. We shall illustrate this 
process by an example. 3.1 Concepts learning from classes of documents As a part of the history of optimum 
retrieval, suppose that the system is presented with many independent classes of documents, say {S1, 
S2, .... S.}. By applying the learning algorithm discussed in the preceding section, the system will 
learn a concept (generalization with minimum f) for each of the classes. The corresponding set of concepts 
is obtained as {Cl, C2, .... C,}, whe~ Ci = G*(Si) for i = 1, 2, .... n. Every concept is of the form 
of conjunction of literals. We shall illustrate the process by an example with a collection of documents 
shown in Table 1. With the representation language described in section 2, for example, dl and d12 can 
be expressed as: dl = (Pattern-Recognition, 2)(Classification, 4) (Parametric, 4)(Bayes-Rule, 3) dlz 
= (Classification, 5)(Parametric, 5)(Probabilistic-Model, 1) Indexing dl d2 d3 d4 d5 d6d~dsdgdlo dll 
d12 Vocabulary Conlputer­ 0010 -  00 Science 1,, I I I Pattern-Recognition l~l~lulululJlulul Boolean-Model 
0050030()0 (1 () (J proyl }i$c-000450000 (J 4 1 Table 1. A collection of documents Suppose that the 
history of optimum retrievals con­tains six optimal queries and sets of relevant documents retrieved 
by the queries. QI: {all, dn} Q2: {all, dz, dl, ds, dg} Q3: {d2} Q4: {dq, ds, %, dlo} Q5: {@, G} Qr5: 
{cL dll } For the learning systew suppose it has the theorems listed as below 1. Vv(Pattern-Recognition, 
v) * 3X(Computer-Science, x)  2. V, (Information-Retrieval, v) * El,(Computer-Science, x)  3. Vv(Probabilistic-Model, 
v) * 3X(Information-Retrieval, x)  4. V. (Classification, v) + 3Z (Pattern-Recognition, x) 5. Vu (Bayes-Rule,v) 
a 3Z(Probabilistic-Model,x)  The relative importances of the terms given by an expert are as shown in 
Table 2. Table 2: Relative importances of terms I Term Name I Weight I I Computer-Science 1.0 I I Pattern-Recognition 
0.8 I I Information-Retrieval I 0.8 I I I I Classification 0.7 I Boolean-Model 0.4 I I I I Probabilistic-Model 
I 0.4 I Then, by using the learning algorithm described in Section 2, we may get two different generalizations 
for the document set {all, dlz } retrieved by Q1. They arc as shown below: Gl(dl, dn) = (Classification,Xl) 
(Parametric, X2) (Probabilistic -Model, X3) with CM I = {Classification, Parametric}; THI = {Probabilistic-Model}; 
DR1 = {Pattern Recognition, Bayes-Rule}; S(dI, dlZ, Gl) = 0.7+0.4+ 0.4+(1 0.8) + (1 -0.3)= 2.4 D(dl, 
dlz, GJ = (1 0.7) +(1 0.4)+ (1 -0.4)+ 0.8+0.3= 2.6 ~(dl, dlz, G,) = 2.6/2.4= 1.08 and Gz(dl, dlz) 
=( Pattern Recognitiio, Xl) (Classification,X2) (Parametric, X3) with CMZ = {Classification, Parametric}; 
THZ = {Pattern-Recognition}; DRZ = { Bayes-Rule, Probabilistic Model}; S(dl, d12, G2) = 0.7+ 0.4+0.8+(1 
 0.3) + (1 -0.4)= 3.2 :1 0.7) +(1 0.4)+ (1 -0.8)+0.3+0.4= 1.8 .8/3.2 = 0.56 Conceptual-distance( dl, 
dlz) = min {1.08, 0.56} = 0.561 Conceptual-cohesiveness= 110.56=1.79 Therefore, Gz(dl ,dlz) is considered 
as the concept learned from the document set {dl ,dlz }. We denote it as cl. Omitting the details of 
the process, the concepts learned from the remaining retrieved sets of relevant documents are listed 
below: Cl = (Pattern-Recognition,Xl) (Classification,X2) (Para­metric,X3) C2 = (Computer-Science,Xl )(Pattern-Recognition,X2!) 
(Classification,X3) Cs = (Pattem-Recognition,3) (Classification,4) (Non­parametric,3) C~ = (Computer-Science,Xl 
)(Information-Retrieval,X2) C5 = (Computer-Science,Xl )(Information-Retrieval,X2) (Boolean-Model,X3) 
C6 = (Information-Etrieval,Xl)(Probabilistic-Model,X2) (Parametric,X3) Where Ci represents the concept 
learned from the set of relevant documents retrieved by Q,, for i = 1, 2, .... 6. 3.2 The clustering 
algorithm Given the set of concepts {Cl, Cz, .... C,}, the clus­tering algorithm will cluster the concepts 
by generalizing in such a way that the conceptual cohesiveness of each cluster is maximized. Given asetof 
concepts {Cl, C2, .... ~}, the algorithm first looks for the two concepts Ci, Cj for which the Con­ceptual 
cohesiveness is maximum. These concepts form the seed of a cluster. A new concept ek is added to this 
cluster only if the conceptual cohesiveness of the set {Ci, Cj, Ck } does not decrease very much. Therefore, 
a thresh­old should be given in order to be able to determine when the conceptual cohesiveness has decreased 
significantly. The threshold is denoted by the symbol p, where O < p < 1. Based on p , the following 
statements are established 1. C(sl) < C(S2) e C(SI) < P * C(S2) 2. C(S2) < C(sl) u C(S2) < p * C(sl) 
 3. C(sl) = C(S2) e lC(SI) < c(s2)&#38;7c(s2) < C(sl)  where c(Si) denotes as the conceptual cohesiveness 
of the set Si. For a particular application, one should experimen­tally determine the value of the threshold. 
The algorithm, which is extended from the one pro­posed by Kodratoff et al[4], is described below. Assume 
that the threshold p has been determined. Step 1: Compute the conceptual cohesiveness of each pair of 
concepts Let C={cl, C2, .... C.} be the set of concepts. For each pair {CP, Cq }, compute its conceptual 
cohesiveness defined previously. Step 2 Choose a seed of the clustering. Determine the pair {CP, C~ } 
for which c(~, C~) is maximum. Let M={%, C~ }. Step 3: Determine the concepts which could be members 
of the cluster represented by the chosen seed That is, determine the set T = {Ck[c(C~, Cq) -c(ck, cp)&#38;c(Cp, 
Cq) = c(ck, Cq)) If c(ck , CP) << C(CP, GJ> then c(cp, Cq, Ck) << c(cp, Cq). Therefore Ck may not be 
the member of the cluster represented by {CP, C~ }. Step 4 Introduce concepts into the cluster For each 
Ck from T, if C(CP, Cq) s c(M, Ck), then introduce Ck into M. Here, if M = {CP, Cq, .... Ct }, then c(M, 
Ck) means C(cp, cq, .... Ct, Ck) At the end of this step one has discovered the cluster represented by 
the seed {CP, Cq}: M = {Cp, Cq,..., C,} and also obtained the generalization (concept) of CP, Cq,..., 
C, as CM=G(q, Cq,..., CJ for which the ~ is minimum. Step 5: Replace the concepts contained in the cluster 
M with the concept CM Remove elements of the discovered cluster M from the set of concept C. That is: 
c+(C M) U{ CM} Step 6: Rerun the algorithm Repeat from stepl with the new set of concepts until C can 
not be further reduced. his clustering algorithm is able to discover a hier­archy of concepts characterizing 
sets of documents and computes a description of each concept in the form of generalization. For the given 
example, the algorithm is presented with a set of COnCeptS C={ Cl, Cz, .... es}, and the resolution is 
determined as p=O.8. Running the clustering algorithm with the set of concepts, we obtained the hierarchy 
of concepts shown in Figure 1. Ctotal /\ C123 C456 /\ A c1 C2 C5C6 Figure 1. The hierarchy of concepts 
In the Figure 1, C~oM = G*(dl, d2, .... d12) = (Computer-Science, Xl) CIM = G*(dl, d2, d7, d8, d9, d12) 
= (Computer-Science, Xl)(Pattern-Recognition, X2) (Classification, X3) CM6 = G*(d3, d4, d5, d6, dlO, 
dll) = (Computer-Science, Xl)(Information-Retrieval, X2) Ferfn Computer-Science I O Q1,Q2,Q3,Q4,Q5,Q6 
Rule . . . . . . I I1 Classification II I, i II I Non-Term Parametric Term Parametric 0Q1 0Q3 Rule ...... 
Rule ...... Figure 2. A hierarchy 3.3 A hierarchy of object classes Objects and object classes are 
the primary components in the paradigm of object-oriented technology [8]. An object class represents 
a set of objects that represent a concept in the real world. Usually, an object class consists of three 
parts; description part, operation part and rule part. The description part contains attributes of the 
class. The operation part exhibits the possible operations that are allowed on the object class. The 
rule part is the knowledge base in which rules for the operations are specified. A hierarchy of object 
classes means that the object classes are ordered in a class-subclass hierarchy in which each object 
class inherits all the properties of its superclass. Thus, the knowledge about objects and object classes 
is or­ganized within the hierarchy in a memory efficient manner. In the domain of information Retrieval 
, an object refers to a document. Therefore, an object class represents a set of documents. Aside from 
sets of documents, the history of optimal retrievals includes the optimal queries and some additional 
knowledge about these queries, such as the restrictions for applying the optimal queries and the characteristics 
of the queries. As a result of the clustering algorithm, we discover many clusters of concepts. Since 
each of the concepts represents one or several sets of documents which were retrieved by one or severat 
optimum queries, we can treat the clusters as object classes. The terms appearing in the 1 Information- 
Term Retrieval 0 Q4, Q5, Q6 Rule ...... I1 [ Boolean- Probability Term -fem -Model Model Parametric 
 0Q5 0Q6 Rule ...... Rule ...... of object classes description of a cluster are the attributes of the 
object class. The corresponding optimal queries are the operations of the object class. The knowledge 
about optimal queries forms the rule part of the object class. The clustering algorithm is used to generate 
a hier­archy of concepts. For transforming it to a hierarchy of object classes, the only step that remains 
is to compute the description specific to each object class (node in the tree). This can be done by removing, 
from the description of each object class, the terms which are already present the descriptions of its 
ancestors. After instantiating the operation and the rule parts of each object class respectively with 
the corresponding optimum queries and the additional knowledge about lhe queries, we finish the task 
of constmcting a hierarchical structme for the history of optimum retrievals. For the example given earlier, 
we obtain the hierarchy of object classes shown in Figure 2, which is transformed from the hierarchy 
of concepts shown in Figure 1. 4. Building the hierarchy in a DBMS The hierarchy of object classes can 
be implemented in a relational DBMS in order to benefit from the facilities it provides and the theoretical 
basis provided by the rela­tional model. We briefly show an implementing techniqpe and describe some 
possible strategies for the application Class-1 Pattem-Reeognition Classification Class-ID f1 Class-1 
Attribute Name Null Domain Class-1 instance ID N ID Pattem-Reeognition Y Weight Classification Y Weight 
 Class-ID N D Candidate keys (Class-1 instance ID),(Class-ID)  E&#38;laclass-2 (a) High level abstraction 
Figure 3 Representations of the history of optimal retrievals on the basis of that implementation. 4.1 
Data Modeling A data model is the first design step towards using a database in an application. It defines 
the structure of a database [5]. Furthermore, the object-oriented approach can be employed to define 
useful abstractions on top of a standard data model. When the underlying data model is relational, object-oriented 
data modeling promotes adher­ence to normal forms and improves integration between a database and its 
applications[l 1]. Generalization is an extremely valuable abstraction mechanism supported by object-oriented 
modeling tech­nique. A hierarchy of object classes based on generaliza­tion allows the inheritance of 
the properties of the object classes in the upper levels of a hierarchy by the classes that are below. 
In the proceeding sections, we discussed the construction of the hierarchy of object classes by gen­eralizing. 
This hierarchy can be considered as a high level abstraction of the database model. Such an abstraction 
can be implemented by an actual DBMS. OMT (Object Modeling Technique) is a relational database design 
approach based on object-oriented concept [111. By OMT, the hierarchy of object classes based on generalization 
we discussed previously can be transfomx% into relational database schema. Attribute Name Null Domain 
 Class-1 instance ID N ID Parametric Y Weight I Candidate key: (Class-1 instance ID) (b) The idael relation 
tables for Generalization Figure 3(b) gives the result of transforming the gen­eralization hierarchy 
of object classes a (high level) shown in Figure 3(a) into ideal relational tables. The triangle in Figure 
3(a) symbolizes generalization. In Figure 3(b), we do not show the table of Class-3 since you may obtain 
it by replacing the attribute Parametric in the table of Class-2 by the attribute Non-Parametric . Although, 
in the representation of OMT, the operation and rule parts are not exhibited, they are implicitly associ­ated 
with each table as secondary information. In addition, the process of tmnsfonnation from high level abstraction 
to a target relational database schema could be automatically carried out. 4.2 Strategies for the application 
of the history By building the history of optimum retrievals on re­lational database, one could benefit 
a lot from the many facilities provided by a DBMS and from the strong theo­retical basis that underlies 
the relational model, Of course, for our application, some extensions to the SQL of rela­tional database 
would be desirable [12]. We don t discuss this issue in this paper. What we are interested in arc the 
strategies for retrieving the information present in the history of optimum retrievals, assuming that 
the necessary extensionshave beendone. Since the idea is to illustrate the concept involved in the strategies 
rather than to show the relational language explicitly, we explain the retrievals with natural language. 
Several strategies are listed below. Boolean Retrieval By Boolean retrieval we mean the use of queries 
in which the usual Boolean operators are applied to specify index term combinations. Operation 1a: Find 
the document ID s for all documents indexed by Pattern-Recognition , Classification and Non-parametric 
, but not by Linear-discriminant . But just retrieving the documents by Boolean retrieval does not fully 
exploit the knowledge in the history of optimum retrievals. When one s information requirement can be 
met by processing an optimal query in the history, the following operations will usually take place. 
Operation lb: Create a table for all documents indexed by Pattern-Recognition , Classification and Non-Parametric 
, but not by Linear-discriminant . (This operation finds the most general object class which has the 
desirable attributes.) Operation lc: Find the optimal queries associated with the object class. Operation 
ld: Select one of the optimal queries according to some knowledge. Operation le: Apply the optimal query 
to the documents contained in the created table to obtain optimum result. If none of the optimum queries 
in the history could obtain a satisfactory result, one could select an optimal query that is the most 
promising. Then, the usual learning process of constructing an optimum retrieval would be performed by 
taking this optimal query as the initial query. By following this strategy, the learning process may 
be expected to converge faster than the process of learning the optimal query from scratch. Weighted 
Boolean Retrieval Here we are interested in retrieving information that satisfies several criteria. For 
instance: Operation 2: Find the optimal queries associated with the documents which have weight of Pattern-Recognition 
lager than 3, and weight of image-Processing less than 2. When users have more accurate information 
about their request this kind of retrieval could be used. Weighted Retrieval In fact, the operation 
le is a form of weighted re­trieval. In order to perform this retrieval, we assume the existence of a 
special function, SIM( ), appropriate to the given query language that computes the similarity between 
a query and a given document. For instance, Operation 3: List and rank the IDs of documents that have 
similarities higher than 0.60 with the query Q. Feedback Searches Feedback searches are those which involve 
the results of a previous search. For example, a user may examine the results of a search and decide 
that he prefers certain docu­ments. For example, suppose that documents numbered 25 and 167 are relevant, 
but the document numbered 2501 is not. TWO kinds of operations, which are illustrated below, may be performed. 
Operation 4a: List the ID s of documents that are like documents 25 and 167 but not like document 2501. 
For this operation, the system will presents the doc­uments in the object class of maximum size that 
contains documents 25 and 167 but not 2501. As discussed pre­viously, documents in a class have high 
conceptual cohe­siveness between them. Operation 4b: Find an optimal query that retrieves docu­ments 
25 and 167 but not document 2501. Since system has some knowledge about optimal queries that are contained 
as a pm of object classes, the system could determine a more appropriate optimal query based on the characteristics 
of the optimal queries and the properties that documents 25, 167 and 2501 have. Further strategies to 
make use of information pro­vided by the history of optimum retrieval could be given. However, most needs 
can be met by the strategies already shown. The object-oriented modeling technique enriches the flexibility 
of the system with respect to both search strategies and information representation. The semantic linkages 
are specified at definition time. Therefore, the structure of the query language is simple and the operat­ions 
shown above can be expressed in a straightfonvard manner [14]. 5. Conclusion Optimal retrieval may be 
based on numerical ap­proaches. It could provide more reasonable retrieval re ­suits with respect to 
user s information requirements than the conceptual retrieval, such as Boolean retrieval, which involves 
a symbolic approach [1]. But the former approach suffers from the following major disadvantage the conc­eptual 
meaning is not explicitly expressed by the optima~ queries. Thus, it is hard for users to obtain and 
manage them. The symbolic approach, in contrast, has the advar~­tage that it could explicitly express 
the users concepts in terms of some keywords (primitive concepts). Such kind of queries can be well understood 
and can be appealing to the intuition of human beings [17]. Several models for IR that include features 
of both symbolic and numerical methods [15,16] have been inves­tigated. Most of them focus on improving 
retrieval per­formance by converting a symbolic query into a numerical expression. But these approaches 
are unable to achieve per­formance that is as good as the pure numerical approach. Therefore, in the 
work of this paper, our attention is focused on a different scheme for combining the features of both 
symbolic and numerical approaches. In order to retain good retrieval performance, the retrieval operations 
are performed by optimal queries that ate obtained by some learning technique. Then, by using the machine 
learning approach, the symbolic concepts relevant to the optimal query can be learned from the set of 
objects retrieved by the query. Based on the concepts generated, a conceptual clustering method is applied 
to construct a hierarchy which organizes the results of optimal retrievals around meaning­ ful conceptual 
entities. Thus, user s information require­ ment can be first expressed by a symbolic query. Based on 
the concepts contained in the query, a corresponding opdrnal query, which will perform optimum retrieval, 
can be selected. In this model, the performance of document retrieval depends solely on the method that 
one adopts to learn the optimal queries, rather than on the conceptual hierarchy constructed. However, 
the problem of how to locate an optimal query that is most appropriate to meet a particular user s requirement 
is still open. We will investigate this problem in our future work. Reference<RefA> 1. Nobert Fuhr. Optimum 
Polynomial Retrieval Functions Based on the Probability Ranking Principle. ACM Trans. on Informdion Systems, 
Vol. 7, No. 3, July 1989. pp 183 204. 2. V. V. Raghavan , J. S. Deogun and P. Rhee. Formu­lation of 
the Term Refinement Problem for User-oriented Information Retrieval. Proceedings of The Annual Al Sys­tems 
in Government Conference, Washington, D. C., March 1989, pp 72 78. 3. S. K. M. Wong, Y. Y. Yao and P. 
Bellman. Linear structure in information Retrieval. ACM 11 th International Conference on Reseaxh &#38; 
Development in Information Retrieval, Grenoble-France, 1988, pp 219 232. 4. Y. Kodratoff and G. Tecuci. 
Learning based on Con­ceptual Distance. IEEE Trans. on Paltern Analysis and  Machine Intelligence. Vol 
10, No. 6, NOV. 1988, pp 897-909. 5. R. Hull, R. King. Semantic Database Modeling: Sur­ vey, applications, 
and Research issues. ACM Computing Surveys,19(3), Sept. 1987, pp 201-260. 6. R. S. Michalski, J. G. Carbonell, 
and T.M. Mitchell, E&#38;.., Machine Learning: An Artipcial Intelligence Ap­proach, Vol 2. Palo Alto, 
CA Morgan Kaufman, 1986 7. Y. Kodratoff, and J. G. Ganasciua. Improving the gen­eralization step in 
learning, In Machine Learning: An Ar­ti~cial Intelligence Approach, Vol. 2, 1986, pp. 215 244.  8.A. 
Goldberg, and D. Robson. Smalltalk-80: The lan­guage and Its Implementation. Addison-Wesley, Reading, 
Mass. 1984. 9. V. V. Raghavan and G. S. Jung. Connectionist Lear­ning in Constructing Thesaurus-like 
Knowledge Structure. AAAI Symposium on Text-Based Intelligent Systems, Spring, 1990, pp 123-127. 10. 
J. S. Deogun and V. V. Raghavan. User-oriented Document Clustering A Framework for Learning in In­formation 
retrieval, Proc. Ninth Annul International ACM Confi on Research and Development in Information re­trieval, 
Piss-Italy, 1986.pp. 157 163. 11. M. R. Blaha, W. J. Premerlani, and, J. E. Rum­baugh. Relational Database 
Design using an Object-Oriented Methodology. Communications of the ACM. Vol. 31, No. 4, April 1988, pp 
414-427. 12. D. D. Chamberlain, R. F. Boyce, SEQUEL A struc­tured English Query Language. Proceedings 
of the ACM-SIGMOD Workshop on Data Description, Access, and Con­trol. Ann Arbor MI, 1974, pp 249 264. 
 13. R. G. Crawford. The Relational Model in Information Retrieval. Journal of #he American Society for 
Information Science. ,1981, pp 51-63. 14. I. A. Macleod, and A. R. Reuber. The Array Model: A Conceptual 
Modeling Approach to Document Retrieval. Journal of the American Society for Information Science. 38(3) 
162-170, 1987. 15. G. Salton, E. A. Fox, and H. Wu. Extended Boolean Retrieval. Communications of the 
ACM, 26: 1022 1036, 1983. 16. S. K. M. Wong, W. Ziarko, V. V. Raghavan, P. C. N.  Wong. On Modeling 
of Information Retrieval Concepts in Vector Spaces. ACM trans. on Database Systems, Vol. 12, No. 2, June 
1987, pp 299 321. 17. C. T. Yu, W. Meng, and S. Park. A Framework for Effective Retrieval. ACM Transactions 
on Database Systems, 14(2), June 1989, pp147-167.  
			</RefA>
