
 An Optical Simulation of Shared Memory Leslie Ann Goldberg* Yossi Matias Satish Rao Sandia National 
Laboratories AT&#38;T Elell Laboratories NEC Research Institute MS111O, PO Box 5800 600 Mcmnt ain Avenue 
4 Independence Way Albuquerque, NM 87185-1110 Murray Hill, NJ 07974 Princeton, NJ 08540 lagoldb@cs .sandia. 
gov matias@research. att .com satish~research. nec.conr Abstract We present a work-optimal randomized 
algorithm forsimu­ lating a shared memory machine (PRAM) OIIZUIOptiCalCOIIW munication parallel computer 
(O CPC). The OCPC model is motivated by the potential of optical communication for par­allel computation. 
The memory of an OCPC is divided into modules, one module per processor. Each memory module only services 
a request on a timestep if it receives exactly one memory request. Our algorithm simulates each step 
of an n lg lg n-processor EREW PRAM on an n-processor OCPC in O(lg lg n) expected delay. (The probability 
that the delay is longer than this is at most n- for any constant a.) The best previous simu­lation, 
due to Valiant, required ~(lg n) expected delay. 1 Introduction The huge bandwidth of the optical medium 
makes it possi­ble to use optics to build communication networks of very high degree. The possibility 
of using such communication networks in parallel computing was first studied by An­derson and Miller 
[21 who introduced the OCPC model: In .. an n-processor completely connected Optical Communication Parallel 
C ornputer (n-ocpc) n processors with local mem­ory are connected by a complete network. A computation 
on this computer consists of a sequence of communication steps. During each communication step each processor 
can perform some local computation and then send one message to any other processor. If a processor is 
sent a single message during a communication step then it receives this message successfully, but if 
it is sent more than one message then the transmissions are garbled and it receives none of them. While 
the OCPC seems a reasonable model for optical computers, it has not been used as a programming model 
to date. The PRAM model, on the other hand, has been exten­sively used for parallel algorithmic design 
(e.g., [16, 19, 30]. The convenience of programming on the PRAM is largely due to the fact that the programmer 
does not have to spec­ify interprocessor communication or to allocate storage in a This work was performed 
at Sandla National Laboratories and was supported by the U S. Department of Energy under contract DE­AC04-76DPO0789. 
Permission to co y without fee all or part of this material is granted provld Jthat the copies are not 
made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication 
and its date appear, and notice is given that copying is by permission of the Association of Computing 
Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. SPAA 94-6194 
Cape May, N.J, USA Q 1994 ACM 0-89791-671 -9/9410006..$3.50 distributed memory. For the very same reason, 
the PRAM is considered as highly theoretical, and the task of emulating the PRAM on more realistic models 
has attracted consider­able attention; emulations may enable automatic mapping of PRAM algorithms to 
weaker models, as well as a better understanding of the relative power of different models. In­deed, 
many emulations of the PRAM on bounded degree net­works were introduced (see, e.g., [1, 33, 34, 20, 29] 
or [21] for a survey). In this paper, we present a simulation of an EREW PRAM on the o CPC. In particular, 
we present a randomized simu­lation of an n lg lg n processor EREW PRAM on an n processor OCPC in which, 
with high probability, each step of the PRAM requires O (lg Ig n) steps on the OCP c. 1 Our simulation 
is work optimal, to within a constant factor. Our results are closely related to previous work on the 
well studied distributed memory machine (DMM) which con­sists of n processors and n memory modules connected 
via a complete network of communication. Each processor can access any module in constant time, and each 
module can service at most one memory request (read or write) at any time. The DMM is thus a weaker model 
than the shared memory PRAM, in that the memory address space is par­titioned into modules with a restricted 
access imposed on them. We remark that there are several variants of DMM models differing in their contention 
rules. Quite a few papers have studied the emulation of a PRAM on various DMM models [28, 31, 18, 35, 
6, 17, 7]. Karp et al. [17] present O(lg lg n) ~xpected delay simulations of var­ious types of PRAM on 
a CRCW DMM in which each memory module allows concurrent read or write access to at most one of its memory 
locations during any step. Dietzfelbinger and Meyer auf der Heide improve upon this paper by presenting 
an O (lg lg n) expected delay simulation of an EREW PRAM on the (weaker) c-collision DMM in which any 
memory mod­ule that receives c or fewer read or write requests serves all of them. Although Diet zfelblnger 
and Meyer auf der Heide require c ~ 3 for their analysis to work, they report that experiments show that 
c = 2 works as well. The l-collision DMM is equivalent to the OCPC. Our result improves on the result 
of [7] in two ways. First, it is work-optimal. Second, it works for the OCPC (or l-collision DMM). The 
previous best known work-optimal simulation of a PRAM on the OCPC is an O (lg n) delay sim­ulation of 
Valiant [36]. 1we ~,11 refer to the t,itne required to simulate one Pram steP as the delay of the .sImulation 
1.1 Related work The OCPC model The OCPC model was first introduced by Anderson and Miller [2], and has 
been studied by Valiant [36], Eshaghian [8], Ger6b-Graus and Tsantilas [9], Gerbessi­otis and Valiant 
[10], Goldberg, Jerrum, Leighton and Rao [14], and Goldberg, Jerrum and MacKenzie [15]. The fea­sibility 
y of the o CPC from an engineering point of view is discussed in [2, 9]. See also the survey paper of 
McColl [26] and the references therein. Computing h-relation on the OC PC A fundamental prob­lem that 
deals with contention resolution on the OCPC is that of realizing an h-relation. In this problem, each 
processor has at most h messages to send and at most h messages to receive. Following Anderson and Miller 
[2], Valiant [36], and Ger4b-Graus and Tsantilas [9], Goldberg et al [14] solved the problem in time 
O(h + lg lg n) for an n-processor OCPC. A lower bound of O(-expected time was recently obtained by Goldberg, 
Jerrum, and MacKenzie [15]. Simulating PRAM on OCPCS Valiant described a simu­lation of an EREW PRAM 
on an OCPC in [36]. More specifi­cally, Valiant gave a constant delay simulation of a Bulk Syn­chronous 
Parallel (BSP) computer on the OCPC (there called the S* PRAM), and also gave an O(lg n) randomized simula­tion 
of an n lg n-processor EREW PRAM on an n-processor BSP computer. A simpler simulation with delay O (lg 
n lg lg n) was given by Ger6b-Graus and Tsantilas [9]. Valiant s re­sult is the best previously known 
simulation of a PRAM on the OCPC. Independently of our work, MacKenzie, Plaxton and Ra­j araman [24], 
and Meyer auf der Heide, Scheiderler and Stemann [23] have shown how to simulate a n processor EREW PRAM 
on an n-processor OCPC. Both simulations have @(lg lg n) expected delay. However, neither simulation 
is work-optimal, and both simulations require n a(l) storage at each processor. Simulating PRAMs on 
DMMs Mehlhorn and Vishkin [28] used a (lg n/ lg lg n)-universal class of hash functions to achieve a 
simple simulation of a CRCW PRAM on a CRCW DMM with expect ed delay O (lg n/ lg lg n). An n-processor 
CRCW PRAM can be simulated on an n-processor EREW DMM in O(lg n) expected delay using techniques from 
[36]. The work of this simulation is thus a @(lg n) factor away from optimrdit y. The best work-optimal 
simulation of a PRAM on an EREW DMM has delay O(nc) [20]. Recently, Karp, Luby and Meyer auf der Heide 
[17] pre­sented a simulation of an n-processor CRCW PRAM on an n-processor CRCW DMM with O(lg lg n) delay. 
They also presented a work-optimal simulation of an (n lg lg n lg n)­processor EREW PRAM on an n-processor 
CRCW 13MM in O(n lg lg n lg n) expected delay, and a nearly work-optimal simulation of an n lg lg n processor 
CRCW PRAM on an n­ processor CRCW DMM with the same delay. Subsequently, Di­ etzfelbinger and Meyer auf 
der Heide [7] presented a simpli­ fied (non-optimal) simulation of an n-processor EREW PRAM on an n-processor 
DMM with O(lg lg n) expected delay. The simulation in [17] introduces a powerful technique that in­ 
corporates the use of two or three hash functions to map the memory address space into the memory modules, 
combined with the use of a CRCW PRAM algorithm for perfect hashing. It heavily uses the concurrent read 
capabihty of the CRCW DMM. The simulation in [7] circumvents the need for using the CRCW PRAM perfect 
hashing by an elegant use of an idea from Upfal and Wigderson [35].  1.2 Overview of the algorithm 
Our simulation algorithm incorporates techniques and ideas from the simulation algorithms of [17, 7], 
as well as from the h-relation routing algorithm of [14], as follows. The simulation in [7] uses three 
hash functions to map each memory cell of the EREW PRAM to three processors (and memory cefls) in the 
DMM. A write on an EllEW memory ce~ is implemented by writing a value and a time stamp to at least two 
out of the three associated DMM memory cells. A read of an EREW memory cell is implemented by read­ing 
two out of three of the memory cells and choosing the value with the most recent time stamp. Diet zfelbinger 
and Meyer auf der Heide s proof that their simulation requires only O (Ig lg n) delay on a 3-collision 
Drmvl relies on the fact that, given a randomly generated tripartite hypergraph on 3n nodes with en edges, 
one can, with high probability, re­ move all the nodes in the hypergraph using the following process. 
Repeat O(lg lg n) times: 1. Remove all of the nodes with degree at most 3. 2. Remove all resulting trivial 
hyperedges (hyperedges in which only one incident node remains.)  Each hyperedge corresponds to a read 
or write of a PRAM memory location: The three vertices correspond to the three processors in the DMM 
associated with that memory loca­ tion. Thus, one step of an en node EREW PRAM is imple­ mented by using 
the process above to deliver at least two out of three of the messages associated with each memory request. 
Since we are simulating an n lg lg n processor PRAM on an n-node OCPC, we must simultaneously implement 
the pro­ cess above for O (lg lg n) 3n-node hypergraphs using only n processors. To do this, we start 
by sparsifying all of the hypergraphs using ideas from the (lg lg n)-relation routing algorithm in [14]. 
That is, we route all but O(n/ lg= n) mes­ sages and we ensure that at most one undelivered message remains 
at any processor. Even so, implementing the pro­ cess above in parallel could still require f2 (lg lg 
n) time steps per iteration since each destination may participate in as many as (lg lg n/c) different 
hypergraphs. Thus, we must also copy each destination in such a manner that each message can locate the 
appropriate copy of its destination. We then perform the process in each hypergraph, ensuring that the 
process delivers at most a constant number of mes­ sages to each copy of a destination. After that, the 
messages can be sequentially forwarded to their true destinations in O(lg lg n) time. We remark that, 
in fact, we cannot directly perform the process above on any of the O (1s 16 n) hypergrapha since our 
processors can only receive one message in a time step whereas the processors in [7] can receive three 
messages in a time step. The details of our solution to this problem can be found in the technical sections. 
 1.3 Paper outline We proceed in Section 2 with a high level description of our simulation. In Section 
3, we present our algorithm in detail and prove correctness. In Section 4 we deal with the evaluation 
of the hash function that maps the virtual shared memory to the memory modules. 2 The Simulation Our 
objective is to show how to simulate one step of an n lg lg n processor EREW PRAM in O (lg lg n) time-steps 
on an n processor Ocpc. Our simulation follows [7] in using the following idea from [35]. The memory 
of the PRAM is hashed using three hash functions, hl, hz, and h3. Thus, each memory cell of the PRAM 
is stored in three memory cells of the OCPC. To write memory cell z, a processor of the OCPC sends a 
message to at least two of the processors in {hl (x), hz (z), Its(z)}. The message contains the new value 
for cell z and also a time stamp. To read memory cell x, a processor p of the OCPC sends a message to 
at least two of the processors in {hi (z), hz (z), hs (z)}. Each of these two processors sends p the 
value that it has for cell x and also its time stamp for cell z. Processor p uses the value with the 
later time step. The hash functions hl, hz, and in are chosen from the the highly universal family ~~n 
from [17], which guarantees random-like behavior. Each OCPC processor will simulate lg lg n PRAM proces­sors. 
Thus, at the start of a PRAM step, each of the OCPC processors will wish to access up to lg lg n cells 
of the PRAM memory. Each processor uses hl, hz and in to obtain the three destinations where each memory 
cell is stored. Thus, each OCPC processor wants to send messages to up to 3 lg lg n destinations. Our 
objective is to deliver at least two of the messages associated wit h every request. As in [14], we will 
divide the processors of the OCPC into target groups of size k = lgc n. We will also divide the n lg 
lg n memory requests into lg lg n/6 groups of m requests each for a sufficiently small constant c. We 
will refer to the set of messages associated with a particular group of memory requests as a (group of 
messages . The messages will be delivered using the following procedures: Thinning and deliver to target 
groups. Initially, the number of messages destined for any given target group may be as high as 4k lg 
lg n. (We will show that, with high probability y, it is no larger than this. ) We will use techniques 
from [14] to route the messages to their target groups. With high probability when thk pro­cedure is 
finished every message will be in the target group of its destination. Furthermore, each processor will 
have at most one message left to send. For a suf­ficiently large constant cz, we will allocate a contigu­ous 
block of C2 processors from the target group to each unfinished destination for that destination. All 
senders will know which processors are allocated for their destination. For a sufficiently large constant 
c1, we will ensure that for any of the lg Ig n/c groups of messages, with high probability, all but 0(n2-cl 
lg g ) of the messages in the group will be delivered to their final destinations. Divide into sub-problems 
and duplicate. We now divide the OCPC into lg lg n/c sub-ocpcs, each with n = nc/ lg lg n processors. 
Each sub-ocpc will work on the sub-problem of delivering the messages corresponding to a particular group 
of messages. For each sub-ocpc we now make lgz n copies of the rel­evant sub-problem, all of which will 
reside in its pro­ cessors 1, . . . . n /2. We will also allocate its processors n\/2, ..., n , as follows. 
For each outstanding memory request (i.e., for each memory request which has the property that at most 
one of its three messages was delivered during the previous procedure), we will allo­cate lg2 n processors. 
These 1g2 n processors will do the book-keeping concerning the request in the lg2 n copies of the sub-problem. 
Each message will know the identity of the processors responsible for the book­ keeping concerning its 
memory request. Route messages for each sub-problem. In each copy of each sub-problem we route messages 
accord­ ing to the cz-collision access schedule from Section of [7]. Dietzfelbinger and Meyer auf der 
Heide prove that with high probability each sub-problem is good (this term will be defined later on). 
We will prove that if a sub-problem is good then for any particular mem­ ory request in any particular 
copy of the sub-problem, the probability that the memory request is satisfied in the cz-collision access 
schedule routing is at least 1/2. Also, no destination in any copy of any sub-problem receives more than 
a constant number (3cz) of mes­ sages during the cz-collision access schedule routing. Combining problem 
copies and combining sub­problems. -In this procedure we identify a subset S of the set of messages that 
were delivered by the var­ious copies of the cQ-collision access schedule routing procedure. The messages 
in S are chosen in such a way that every processor is the destination of O(lg lg n) messages in S. We 
show that with high probability every memory request in every sub-problem that was created in the divide 
into sub-problems and dupli­cate) procedure will be satisfied if the messages in S are delivered. We 
deliver the messages in S using the routing algorithm in [14]. 3 Simulation details and analysis Before 
giving the details and analysis we define the class of hash functions ~~n being used and describe its 
properties that are used in the analysis. In the subsequent subsections we will give the details of each 
of the procedures described in the previous section. 3.1 The hash functions The class z~n is taken from 
[17] and is defined as follows. Definition of ~~.: A function from ~~n is a combina­ tion of functions 
taken from several classes. Carter and Weg­ man [4] introduced H~,n ~ {h : [1, . . . ,rn] ~ [1, . . .,n]}, 
the class of functions j(z) mod n where j is a polynomial of de­ gree d 1 over Zm. Siegel [31] introduced 
a class of functions z ~,,n ~ {h : [1,. ... n~] ~ [1,..., n]}. (More details on this class are given 
in Section 4.1. ) To choose a random hash 4,] function h from Rm, n, one first chooses A function ~, 
chosen uniformly at random from H~,fi  A function r, chosen uniformly at random from En j ,n  A function 
s, chosen uniformly at random from Hi ,mj  0 X integers al, . . . . a ~, each of which is chosen uni­formly 
at random from the range [I,. . . . n]. The function h is defined by h(x) = (r(s(z)) +aj(=)) mod n. 
Property 3.1 Let 1 be cm arbitrary constant and let ] be large enough relative to 1. Let S ~ [1,..., 
m], n < ISI < nil/lo Let ~$n (s) be the restriction of ~~m induced by fixing s ~ H~,n,. If s is chosen 
uniformly at random from H: ~, then s is l-perfect with probability at least 1 n e. Ifs 2s 1-perfect 
then ~~n(s) is (1, @-universal.  Proof. See [17]. Property 3.2 Let f be an arbitrary constant and let 
d be large enough relative to 1. Let S ~ [1,..., m], n ~ ISI ~ nll/10. Let f be drawn randomly from H~,&#38;. 
Then with probability at least 1 n-e every set f 1 (i) n S has size at most 2[S1/fi.  t Proof. See 
[17]. Property 3.3 Let 1 be an arbitrary constant and let d and j be large enough relative to k. Let 
S ~ [1,.. ., m], n ~ [S1 ~ n f o. Let Z be a subset of [1,..., n] and let i be an integer in [1, . . 
. . ~. Suppose that ,b s +. Let h be chosen randomly from ~~n. (That is, let f, r, s, and al, ..., a+ 
be chosen as described above.) The probability that /3 or more members of S n f-l(i) are mapped to Z 
by h is at most 2n-t + ( syq(~)p. Proof. By Property 3.2, with probability at least 1 n e every set f 
l(i) n S has size at most 21S1/@. By Property 3.1, with probability at least 1 n ~, the hash destinations 
are W-wise independent.  3.2 Thinning and deliver to target groups We start out by running the thinning 
procedure from [14], which is based on the algorithm of Anderson and Miller [2]. The procedure runs for 
O(lg lg n) steps. During each step each sender chooses a message uniformly at random from the set of 
messages that it has not yet sent successfully and it sends the message to its destination with a certain 
prob­ability. Let h = 32e lg lg n. We prove the following lemma. Lemma 3.1 With probabdzty at least l 
2n o (for any con­stant a), afler the thinning procedure from [14] terminates, there are at most k/h 
[CSlg lg nl undelivered messages des­tinedfor any particular target group. (c3 is a constant which must 
be sufficiently large; it is the constant C2 from [Id].) The proof of Lemma 3.1 will use the following 
lemma. Lemma 3.2 Wtth probability at least 1 n-c (for any con­stant CY), each target group of size 
k is the destination of at most 4k lg lg n messages. Proof. Consider a target group T and for each i 
in the range 1 < i < 3n lg lg n let x, be a random variable that is 1 if the ith message has a destination 
in T and O otherwise. Let X = ~, z,. Clearly, the probability that any given z, is 1 is k/n, so E(X) 
= 3klg lg n. By Property 3.1 of the hash functions, the z,s are W-wise independent (with high prob­ability), 
so using a limited independence Chernoff bound (Theorem 1 of [32]), we find that Pr(X ~ ~(X)(l + 1/3)) 
~ e ~(x)OT. This probabfity is sufficiently sm~ that we can sum the failure probability over the target 
groups. In order to continue with the proof of Lemma 3.1 we need some notation. For every target group 
T let S(T) denote the set containing all senders that have messages destined for target group T. We will 
say that a sender is bad if it has some message that has the same destination as at least h other messages. 
We will use the following lemma. Lemma 3.3 With probability at least 1 n (for any con­stant a) every 
set S(T) contains at most k/(2h2 (C3 lg lg nl) bad senders. Proof. This proof is similar to the proof 
of Claim 2 in [14]. We include it here for completeness and also to demonstrate how the limited independence 
is handled. Let h = h/2. For a given target group T let AI(S(T)) denote the set of messages that are 
sent by senders in S(T). We will say that a message is externally bad with respect to a target group 
T if the message has the same destination as at least h other messages that are not sent from senders 
in S(T). We will say that a message is internally bad with respect to a target group T if it has the 
same destination as at least h other messages that are sent from senders in S(T), We wish to prove that 
with probability at least 1 n at most k/(2h2 (CSlg lg nl ) of the messages in Nf(S( T)) are either 
externally or internally bad. First we consider externally bad messages. We will say that a processor 
P is externally crowded with respect to a target group T if there are at least h messages which are not 
in M(S(T)) and have destination P. A set of b members of a target group are all externally crowded only 
if at least bh messages have destinations in the set. Property 3.1 of the hash functions tells us that 
the destinations of the messages are @-wise independent. Therefore, as long as b s &#38;/h the probability 
that there is a set of 11members of a target group that are all externally crowded is at most n o (for 
any constant a )2, plus  (a(:)fk?$n)w h We can use Stirling s approximation to show that for b = k/h 
s this quantity is at most (n/k) 2-k/h 5. Therefore, with probability at least 1 n (n/k)2-k/h every 
target group has at most k/h s processors which are externally crowded with respect the T. Suppose that 
this is the case. Then the probability that a message in M(S(T)) chooses a desti­ nation which is externally 
crowded with respect to T is at most h s and the expected number of messages in AI(S(T)) that choose 
a destination which is externally crowded with . respect to T is at most M(S(T))/h O. Property 3.1 of 
the hash functions tells us that the destinations of the messages By Lemma 3.2, n m IS an upper bound 
on the probability that more than 4k lg lg n messages are destined for any target group. are @-wise independent 
(with high probability). Hence, we can use a limited independence Chernoff bound from Theorem 1 of [32] 
to show that with probability at least 1 exp( [M(S(T))l / (12 x h )) at most 2 [M(S(T))l/h messages 
in M (S(T)) choose a destination which is ext er­nally crowded with respect to T. Note that as long as 
n is sufficiently large then 2 liVf(S(Z )) I/h < k/(4h2 [CSlg Ig nl ). Also, as long as IM(S(T)) I z 
k/(4h~cs lg lg n] ) and the constant c (in the definition of k) is sufficiently large, the sum of (n/k) 
2-~/h 5 and (n/k) exp( l M(S(T))l / (12x h )) is at most n a. We now consider internally bad messages, 
We start by calculating an upper bound on the probability that a mes­sage is internally bad. Lemma 3.2 
tells us that with high probability at most 4k lg lg n messages are destined for any target group. Thus, 
with high probability, at most 4k lg lg n messages in M (S(T)) are destined for the same target group 
as the given message. Property 3.1 of the hash functions tells us that the destinations of the messages 
are ./ii-wise inde­pendent. Therefore, the probability y that the ~iven message is internally bad is 
at most < 2 h So the expected number of messages in M(S(T)) which are internally bad is at most IM(S(T)) 
12 h. In order to prove that with high probability the number of internally bad messages is not far from 
the expectation we will use the following theorem of McDiarmid [25]. (The inequality is a development 
of the Azuma martingale in­equality ; a similar formulation was also derived by Bollob&#38;s as [3].) 
Theorem 3.1 [McDiarmid] Let Z1,. . . . zn be indepen­ dent random variables, with x, taking values in 
a set A, for each i, Suppose that the (measurable) function f : ~ A, ~ R satisfies If(E) f(?)l ~ c, 
whenever the vectors E and&#38; chfier only in the ith coordinate. Let Y be the random vari­ able j(zl, 
...,z~). Then for an~ t >0, Pr (IY -E(Y) Iz t)< 2exp ( -2t2/~~=1 c:). If the hash functions hl, hz, and 
hs were chosen uni­formly at random from the set of functions from [1,.. ., m] to [1,..., n], the application 
of the bounded differences inequal­it y would be straightforward. We would take as the random variable 
z, the destination of the ith message in Ikf(S(T)). We would let Y be the random variable denoting the 
num­ber of internally bad messages in M(S(T)). If we change the value of one of the Z,S the value of 
Y would change by at most h + 1. Plugging these values into the inequality, we would get a sufficiently 
small failure probability. However, since h], hz, and hs are in fact drawn from the family ~~n, the X,S 
are not independent so we cannot ap­ply Theorem 3.1 to them. Instead, we follow the approach used in 
the proof of Lemma 6.1 in [17]. Consider the in­dependent random variables al, . . . . a~. As before, 
let Y be a random variable denoting the number of internally bad messages in &#38;f(S(T)). Let Z be the 
set of all destinations of messages in M(S(T)). (The size of Z is at most llf(S(T))l, which is at most 
4k(lg lg n)2 (with high probability y), by Lemma 3.2.) Suppose that we change one of the ats. By Property 
3.3 of the hash functions, the probability that /3 or more members of M ( S(T) ) change destination is 
at most 2n a + (6nlglg n/ 4k(lglg?l)2 This probability is suf- P ?( ) . ficiently small as long as 
th~ constant /3 is sufficiently large. So suppose that at most /3 members of M(S(T)) change des­ tination. 
Each of those may make at most h + 1 members of M ( S (T) ) become internally bad. Therefore, if we change 
one a, we change Y by at most ~(h + 1). Therefore, by Theorem 3.1 the probability that Y ~ k/(4h2 [C3 
lg lg nl ) is at most ( k -E(Y)) 2 4h2 [.3 lglg nl ( 2exp (IM(S(T))I ,B2(h + 1)2) ) Since E(Y) ~ $,.2 
[ca~lgkgnl (for big enough n) and, with high probability (by Lemma 3.2), II14(S(T))I S 4k(lg lg n) , 
the probabtity is at most 2exp( k/(32h4[cs lglgn124(lglg n)2,B2(h + 1)2)). This quantity is at most 
~n-a (k/n) as long as c is suf­ficiently large. This concludes the proof of Lemma 3.3. The following 
}emma is proved on page 19 of [14] (The proof of the lemma uses the fact that IS(T) I s 4k lg lg n, which 
is true with high probability, according to Lemma 3.2.) Lemma 3.4 With probability at least 1 n-a the 
number of messages destined for any target group that start at good senders but are not delivered during 
the thinning procedure from [IJ] is at most k/(2h[cS lg lg nl). Proof of Lemma 3.1. We conclude that 
with probability at least 1 2n the number of undelivered messages destined for any given target group 
after the thinning procedure ter­minates is at most k/(h [C3 lg Ig nl ). After the thinning procedure 
from [14] terminates we will use the spreading procedure from [14] to spread out the unfinished requests 
so that each processor has at most one unfinished message to deliver. As part of the spreading procedure 
we will allocate one processor to do the book­keeping associated with each memory request and we will 
ensure that all messages associated with the request know the identity of this processor. During this 
procedure of our simulation the three messages associated with a request may be sent to various processors 
but they will keep the book­keeping processor informed about their whereabouts. After the spreading , 
we will use the deliver to target groups procedure from [14] to deliver the rest of the mes­sages to 
their target groups in O(lg lg n) steps. With prob­ability at least 1 n-a (for any constant a) every 
message will be in its target group at the end of the deliver to tar­get group procedure. Furthermore, 
each sender will have at most 2 undelivered messages to send and (by Lemma 3.3), the number of unfinished 
messages in a target group will be less than k. At this point we can sort the messages in the target 
groups by destination. After the sorting, each sender will have at most one message to send. We now wish 
to allocate a contiguous block of cz proces­sors from the appropriate target group to each unfinished 
destination (for a sufficiently large constant c ). We wish to do the allocation in such a way that all 
senders know which processors are allocated for their destination. We do this as follows. If a destination 
is the destination of fewer than c requests we simply deliver them. Otherwise, we allocate cz processors 
for the destination. The processors allocated will be the first C2 processors with requests for that 
destination. At this point we wish to send all but O(n2 lslK ) of the messages in any group to their 
final destinations. We will say that a message is bad if its destination is also the destination of at 
least c1 lg lg n other messages. We will use the following lemma. Lemma 3.5 With probability at least 
1 n o (for any con­stant a) at most 0(n2-cl lglg ) of the messages in any group of messages are bad. 
Proof. This proof is similar to the second part of the proof of Lemma 3.3. By Property 3.1 of the hash 
functions, the destinations are @i-wise independent with high probability. In this case, the probability 
that a given message is bad is 3 lg lg n n c ISlg n. By Stirling s approximation,at most (C1 lglg n ) 
 this is at most (3e/cl)c g g n which is at most 2 gig n for c1 ~ 6e. Therefore, the expected number 
of bad messages in a group is at most en2 C1 g g . We now use Theorem 3.1 (the bounded differences in­equality) 
to prove that with high probability the number of bad messages in a group is not much more than the expec­tation. 
As in the case of Lemma 3.3, the bounded differences inequality would be straightforward if the hash 
functions h], h2, and h3 were chosen uniformly at random from the set of functions from [I, ....m] to 
[1, ....n]. We would take as the random variable z, the destination of the ith message and we would let 
Y be the random variable denoting the number of bad messages. If we change the value of one of the z~s 
the value of Y would change by at most c1 lg lg n + 1. Therefore, we would obtain the following inequality. 
Pr(Y ~ 2E) ~ 2exp( 2E2/(cn(cl lglg n + 1)2)). However, since hl, hz, and h3 are in fact drawn from the 
family ~~~, we again follow the approach used in the proof of Lemma 6.1 in [17]. Consider the independent 
ran­ dom variables al, . . . . afi. Let Y be a random variable denoting the number of bad messages. If 
we change the value of one of the a,s then, with high probability at most 6n lg Ig n/@ messages get new 
destinations. (This follows from Property 3.2 of the hash functions. ) Each new desti­nation could cause 
at most c1 lg lg n + I messages to become bad. Thus, changing one of the a,s could change Y by at most 
6filg lg n(cl lg lg n + 1). So, by the bounded differ­ences inequality, Pr(Y ~2E) ~ 2exp( 2E2/(fi36 n(lglg 
n)2(cl lglg n + 1)2)) , which is sufficiently small. Given Lemma 3.5, it suffices to route c1 lg lg 
n messages to each destination. This can be done in O(lg lg n) steps since the messages are sort ed 
by destination. At this point we have finished the thinning and deliver to target groups procedure. The 
book-keeping processor associated wit h ev­ ery memory request now cancels the request if at least two 
of its messages were delivered. If the request is canceled then the third message is deleted. 3.3 Divide 
into sub-problems and duplicate Our goal is to divide the OCPC into lg lg n/c sub-ocpcs, each of which 
has n = nc/ lg lg n processors. Each sub-ocpc will work on the sub-problem of delivering the messages 
corre­sponding to a particular group of messages. For each sub-OCPC we wish to make lgz (n ) copies of 
the rtJevant sub­problem, all of which will reside in its processors 1, . . . . Tz /2. We will use an 
approximate compaction tool to divide the problem into sub-problems and to make copies of the problem. 
(For similar tools see [5, 13, 22].) Given an n-ocpc in which at most s senders each have one message 
to send,  a set of ,8s receivers which is known to all of the senders,  the (s, ,B) approximate compaction 
problem is to deliver all of the messages to the set of receivers in such a way that each receiver receives 
at most one message. The following lemma is from [14]. Lemma 3.6 For any positive constant w there is 
a positive constant C3 such that the (s, [C3 lg lg n]) approximate com­paction problem can be solved 
m O(lglg n) communication a -w + s ~steps with failure probability at most We proved in the previous 
subsection that, with high probability, when the thinning and deliver to target groups procedure terminates, 
the number of undelivered messages is at most 3n lg lg n2-C g lg n. Furthermore, every message is in 
the target group of its destination and each processor will have at most one message left to send. The 
number of unfinished target groups is at most the number of unfinished messages, which is at most -., 
k%n < n /(21g2(n )k2[C3 Iglg ~1) 3nlglgn2 for a sufficiently large c1. Therefore, with high probabil­ity 
(by Lemma 3.6), we can compact one message from the first processor in each unfinished target group to 
the first n /(2 lg2 (n )k2) processors in the n-ocPc. Having done that, we can copy each of the unfinished 
target groups to one of the first n /(2 lg2(n )k) target groups in the n-ocpc. Next, we can use doubling 
to make lgz (n ) copies of each unfin­ished target group. All of these copies will reside in the first 
n /(2k) target groups in the n-ocpc. At this point, the entire problem is copied lg2 (n ) times into 
the first n /(2k) target groups in the n-ocpc. These n /(2k) target groups will form the first half of 
the processors in the first n -processor sub-ocpc. Our objective is to use the first sub-ocpc to solve 
the sub-problem of delivering the messages in the first group of messages. The sub-ocpc will do this 
by simply ignoring all messages that are not in the first group of messages. The lg2 (n ) copies of the 
entire problem can now be copied into the remaining lg lg n/c 1 sub-ocpcs. The jth sub-ocpc will ignore 
all messages that are not in the jth group of messages. Our next goal is to allocate the processors n 
/2, . . . . n of each sub-ocpc such that for each outstanding memory re­ quest (i.e., for each memory 
request which has the property that at most one of its three messages was delivered dur­ ing the previous 
procedure), we allocate lg2 (n ) processors. (These lg2 (n ) processors will do the book-keeping concern­ 
ing the request in the lg2 (n ) copies of the sub-problem.) The allocation can be done in the same way 
that the problem was split and copied because the number of re­ maining requests is at most 3n lg lg 
n2 cl Ig lg . 3.4 Route messages for each sub-problem Consider a particular copy of a particular sub-problem. 
Lemma 3.5 tells us that with high probabtity at most 0(n2 c lg g ) of the memory requests from the m 
mem­ory requests associated with this sub-problem remain. Al­though each processor has at most one message 
to send, there is a book-keeping processor allocated to each memory request and each message knows the 
identity of its book­keeping processor. Furthermore, there is ablockof cz con­tiguous processors allocated 
to each unfinished destination and each sender knows which processors are allocated to its destination. 
For z c {1,2,3} we will say that a message is an %rnessage if it obtained its destination using hash 
function i. We now route messages according to the cz-collision ac­cess schedule from Section 3 of Dietzfelbinger 
and Meyer auf der Heide s paper [7]. Each round of the access schedule is defined as follows. For i = 
1,2,3: a. For all destinations din parallel, repeat (C2 lg(2cz)l times: Each i-message with destination 
d that is not already waiting at one of the cz processors allocated to d picks a random processor from 
those allocated to d and sends there. Each of the allocated processors will only accept one message. 
 b. Each destination d now checks whether there are any other i-messages destined for d (that is, whether 
there are any i-messages with destination d that are not at the allocated processors). To do this, the 
first of the C2 processors allocated to d sends to d. Also, any i­messages with destination d that have 
not yet been successful in reaching one of the cz processors allocated to d send to d. Then the first 
of the cz processors allocated to d tells d whether or not it had a collision.  c, For each destination 
d, if all of the i-messages destined for d are at the processors allocated to d then these messages are 
delivered. Otherwise, no requests are de­livered. d. The book-keeping processor associated with each 
mem­ory request checks which of the messages associated with the requests were delivered. If at least 
2 of the messages associated with the request have been deliv­ered then the request is canceled and the 
third message is deleted. Note that no destination receives more than 3CZ messages during the cz-collision 
access schedule routing. We use the following lemma Lemma 3.7 During one round of the C2-collision access 
sched­ule routing procedure any processor that is the destination of at most CQ i-messages gets ali of 
the i messages with prob­ability at least 1/2 (and none of them with the remaining probability). dray 
processor that is the destination of more than C2 i-messages receives none of them. Proof. If d is the 
destination of at most cz i-messages then the probability that one of them fails to reach the al­located 
processors in 1 = (CZlg (2CZ)1 attempts is at most C2(1 l/c2)1 ~ 1/2. In their analysis of the c2-collision 
access schedule rout­ing procedure (as implemented on a c2-collision DMM), Di­etzfelbinger and Meyer 
auf der Heide define a hypergraph H = (V, E) for a set of memory requests xl, .... Zen with ver­tex set 
V = {vrt I1 ~ r <3, 1~ t < n} and hyperedge set E = {{~l,~,t~i),~z,~z(~,), v3,k3(~iJ} 11 S z < en}. In 
light of Lemma 3.7, we can view the c2-collision ac­cess schedule routing as a process on H. In each 
round, the process removes each node with degree at most C2 (i.e., the i-messages destined for the processor 
are delivered) with probability at least 1/2. Then the process removes each hy­peredge that consists 
of only one node (i.e., memory requests are canceled if at least two of the messages associated with 
the request are delivered). Following Dietzfelbinger and Meyer auf der Heide, we will say that H is s-good 
if 1. The largest connected component in H has at most a = cr(s) lg n nodes. 2. Every set A ~. V intersects 
fewer than IAI + s hyper­edges from E m at least 2 points.  Dietzfelbinger and Meyer auf der Heide prove 
the fol­lowing lemma. (The proof presented in [7] is based on the assumption that hl, hz, and hs are 
chosen uniformly at ran­dom from the set of functions from [1,. . . . m] to [1,..., n]. However, the 
lemma is also true if hl, hz, and hs are chosen randomly from ~~~n.) Lemma 3.8 The probability that H 
is s-good is 1 -O(TZ-S). We will prove the following lemma. Lemma 3.9 Suppose that H is s-good for some 
positive constants. Then the probability that any particular memory request is satisfied after O(lg lg 
n) rounds of routing accord­ing to the cz-co!lision access schedule is at least 1/2. Proof. Let Ht denote 
the hypergraph obtained by ap­plying t rounds of the cz-collision access schedule routing process to 
H. Diet zfelbinger and Meyer auf der Heide have made the following observation [7]. Observation 3.1 If 
H is s-good and A ~ V is a component of H~ for some t ~ O, then A contains at most 31A1/(cz + 1) + 3s/(c2 
+ 1) nodes of degree larger than ct in Ht. We will use the following lemma. Lemma 3.10 Suppose that H 
is s-good. Let r be an edge in a component of size t ~ s of Ht for some t z O. If C2~ 23 then with probability 
at least 1 exp( .t/54) the component of r in Ht+l has size at most 5!/6. Proof. Let b = 3(J + s)/(cz 
+ 1). By Observation 3.1 and Lemma 3.7, the expected number of nodes in the component of r in Ht~l is 
at most t/2 + b/2. Using a Chernoff bound, we see that the probability that there are at most 4/3(1/2+ 
b/2) < 51/6 nodes is at least 1-exp( (t/2 + b/2)/27). ~ U~ing Lemma 3.10, we conclude that for some constant 
CA ~ s, with probability y at least 3/4, O(lg lg n) rounds of the cz-collision access schedule routing 
procedure reduce the size of the component of a given memory request r to at most C4. We conclude the 
proof of Lemma 3.9 by observing that as long as cz > 3s + 2, O(1) rounds will, with probability at least 
3/4, further reduce the component to size 1. 3.5 Combining problem copies and combining sub-problems 
Proof of Lemma 3.11. The fact that (with high probability) Let us focus our attention on the jth sub-problem. 
Let SJ be the set of messages that were in the sub-problem when it was created. Let S; be the subset 
containing all messages in S3 that are delivered in at least lgz (n )/9 copies of the C2-collision access 
schedule routing procedure. Note that when the ca-collision access schedule routing procedure terminates 
the lgz (n ) processors per memory re­quest that were allocated in the divide and copy procedure to do 
book-keeping can inform all of the the messages in S j (in the first copy of the sub-problem) whether 
or not they are in S;. We will prove the following lemma. Lemma 3.11 With probability at least 1 n-a 
(for any positive constant m) each set S; has the following proper-tzes. 1. Each processor is the destination 
of at most 27c2 mess­ages in S:. 2. Each memory request in the jth sub-problem wilt be satisfied if 
the messages in S; are delivered.  If each set S; has the properties described in Lemma 3.11 (as it 
will, with high probability), then we can satisfy all of the memory requests in O(lg lg n) steps by routing 
the mes­sages in S = U3 S;. These messages form a 27c2 lg lg n/6­ relation, so we can use the routing 
algorithm in [14] to route the messages. To prove Lemma 3.11 we use the following lemma and the following 
observation. Lemma 3.12 With probability at least 1 n-a (for any constant a) every memory request in 
every sub-problem is satisfied in at least lg2(n )/3 of the lg2(n ) copies of the c2­collision access 
schedule routing pr-ocedure, Proof. Suppose that every sub-problem is such that the corresponding hypergraph 
is s-good. (Lemma 3.8 shows that this is so with high probabtity, as long as s is cho­sen to be sufficiently 
large.) Consider a particular memory request in a particular sub- problem. Lemma 3.9 shows that the probability 
that this request is satisfied in any given copy of the sub-problem is at least 1/2. A Chernoff bound 
shows that with probability at least 1 ne -*g2fn J154 the request is satisfied in at least lg2 ( n ) 
/3 copies. The lemma follows by summing the failure probabilities over particular memory requests. Observation 
3.2 If xl, X2 and X3 are the three messages in a memory request that is satisfied in at least 1 copies 
of the cz-collision access schedule routing procedure then there is a pair of messages from {xl, x2, 
x3} such that both of the messages in the pair are satisfied in at least l/3 copies of the procedure. 
Similarly, if xl and X2 are the two messages m a memory request that w satisfied in at least 1 copies 
of the C2 -collision access schedule routing procedure then at least one of xl and X2 is satisfied in 
at least l/2 copies of the p rocedur-e. each memory request in the jth sub-problem will be satisfied 
if the messages in S; are delivered follows from Lemma 3.12 and from Observation 3.2. To see that each 
processor is the destination of at most 27CZ messages in S; note that a message is a member of S; only 
if it is delivered in at least lg2 (n )/9 copies of the ca-collision access schedule routing procedure. 
However, we proved in the previous section that each destination will receive at most 3C2 messages in 
each copy of the procedure. Therefore, at most 27CZ messages that have the same destination will be included 
in S;. This completes the proof of Lemma 3.11. 4 Construction and evaluation of the hash function In 
the simulation algorithm we have assumed that a hash function h was chosen uniformly at random from the 
fam­ ily ~~n and is available to every processor for constant time evaluation. When concurrent-read is 
available in the simulating model, a hash function in use can be kept in the shared memory, and be read 
as necessary in constant time. The exclusive-read nature of the OCPC model, together with the fact that 
the function h c ~~n is represented by a polynomial number of memory words, imply a more subtle sit uation. 
A straightforward implementation is t o keep a copy of the function h at each processor. However, this 
im­ plies polynomial overheads in both the time of preprocessing for distributing all copies, and in 
the space dedicated for this function at each processor. In the remainder of this section we describe 
an eiiicient implementation in which the func­ tion requires only a total of linear space, and its evaluation 
increases the simulation delay by at most a constant factor, 4.1 The hash function Our basic approach 
is: (i) replace the class ~~n with a class whose functions h have similar properties, but can be repre­sented 
in O(n ) space, where 1/2 < c < 1; the modified class exhibits only n -universalit y (rather than fi-universalit 
y as in Property 3.1), but this is enough for our purpose; (ii) make O(n]-e) copies of the selected function 
h; and (iii) make sure that at each simulation step the number of pro­cessors that need to read a component 
of h is bounded by O(nl lg lg n), an average of O(lg lg n) per copy, thereby enable the use of an efficient 
lg lg n-relation algorithm for the read operation. (A similar approach of making dupli­cates to reduce 
contention was used in [12]. ) To implement the approach sketched above we first modify the definition 
d,~ of R ~,. Sa follows. Let t = j/c. The function s from the family ~~~n. is re-defined to be the 
tuple (sI, . . . . st), with the operation s(z) = (sl(z), . . . . st(z)), where s,, 1 s i s t, are chosen 
uniformly at random from H~,n. , for an appropriately large constant d. The following lemma shows that 
Property 3.1 still holds for the new family of hash functions. Lemma 4.1 Let 1 ~ 1 be arbitrary and let 
d and j be large enough relative to 1. Let S be a subset of [1, ... m] of size n ~ ISI ~ n]lllO. If s 
is chosen randomly as described above then Prob[s is l-perfect on S] is at least 1 n-e. Proof. The probability 
that two given distinct points z, y E S wiU collide under s, i.e., that s(z) = s(y), is at most (2/nC)t, 
since the s; s are (2, d)-universal. The proba-(2) Each of the nJ outputs of G chooses dt values in bility 
that any pair of points from S will collide is therefore [0, ... n 1]. A set of n - processors is selected 
for at most  ( : )(w) <n-o-,2,-l / ) The lemma follows by taking j >1 + 22/10. The class of functions 
En, ,n from which T is taken is modified next. Siegel [31] defines a (p, c, d, h)-weak concentrator-H 
as a bipartite graph on the sets of vertices 1 (inputs) and O (outputs), where Ill = p, and 101 = p , 
that has outdegree d for each node in III, and that has, for any h inputs, edges matching them one-by-one 
with some h outputs. A (p, c, d, h)-weak concentrator H is used to construct a function F by storing 
d random numbers from [0,..., p 1] at each node of O. On input i, F(i) is computed by evaluating a polynomial 
hash function of degree d 1 whose coefficients are determined by the numbers stored at the neighbors 
of i in O. Siegel showed that the family of hash functions F so defined is a (1, h)-universal family 
of hash functions mapping [O, p 1] w [0, p 1]. Let H be a (n , e, d, n )-weak concentrator. Siegel 
showedl that the Cartesian product G = Ht is a (n~, c, dt, n )-weak concentrator. The graph G can therefore 
be used to con­st ruct a (1, n )-wise independent family of hash functions mapping [1, ....n~] to [1, 
....n~]. The above was used by Siegel to provide a space-efficient construction of the hash function, 
which turns out useful for our needs. To enable approximately uniform contention distribution we will 
need the function to exhibit one more property. Lemma 4.2 There exists a graph H that is (n , c, d, n 
)­wea?c concentrator, and whch also has the property that ev­ ery output of H has degree at most 2dn 
 2. Proof. We use a probabilistic construction, as given in [31] for finding an (n , c, d, n )-weak concentrator. 
Suppose that each input of H chooses its d (distinct) neighbors uniformly at random. Siegel proves that 
the probability that H is not c -(. , ) . (As long as d is sufficiently small.) We can now use a Chernoff 
bound to show that the degree of each output of H is suffi­ciently small as required. a (n , E, d, n 
) weak concentrator is at most n 4.2 Constructing the hash function The graph H is built into the machine 
when the machine is built. Each of the n inputs has d neighbors. A set of nl c processors is selected 
and each processor in the set is given the name of these neighbors. Recall that it may be the case that 
a new function needs to be constructed (a re-hash operation), when the se­ lected one does not satisfy 
the required properties. (This occurs with polynomially small probability for each parallel step, and 
with high probability after a polynomial number of steps. ) A new hash function is constructed in O(lg 
n) steps as follows: (1) Construct s,, ... S, and j and distribute to all proces­sors. each given output 
and each processor in the set is given the values associated with the output. (3) The values al, .... 
afi are generated. ~ sets of G processors are selected and each processor in a set i is given the value 
of a,.  4.3 Evaluating the hash function At each simulation step, the hash function is computed for 
all memory addresses in O(lg lg n) time, as described next. Let S be the set of 3n lg lg n requests from 
[1, ...m]. Recall that h(z) = (r(s(z)) + af(=)) mod n. Each processor executes the following steps for 
each re­quest z: (1) Compute sl(z), ....st(z). (2) ~~~pute the names of the neighbors of (SI (z), .... 
s,(z)) (3) Read the values corresponding to the neighbors of (sI(z), ....s.(z)) in G.  (4) Apply r 
to (s,(z), .... S.(~)). (5) Compute ~(z). (6) Read a$f~). (7) Compute r(s(z)) + af(z).  The executions 
of Steps 1,4,5, and 7 are in constant time. The following lemma of Dietzfelbinger, given in [20], is 
cen­tral to the analysis of the other steps. Lemma 4.3 Let XI, ....X~ be O 1 valued, d-independent, 
equidistributed random variables. Let p = E(X, ). Then, for n k d/(2k), where a is a constant that depends 
on d but not on n. Claim 4.4 In Step .2, with high probability, for every y in [1 , .... n ] (i. e., 
for every input of H) there are at most O(nl-C lg lg n) pairs (i, x) such that z ~ S and s,($) = y. 
Proof. Note that the set of values s,(z) :1 ~ z < m is d­independent. Following Kruskal, Rudolph, and 
Snfi [20] we use Lemma 4.3. Fix a y and i and let xb be a O-1 random variable which is 1 if and only 
if s, maps the b)th member of S to y. p is I/n . Let ~ be lS1/n . Then the probability that s, maps 
more than 2A to y is 0(n-d 2(1-C) ). Choose d large enough to sum over all i and y. We conclude that 
at most O(nl lg lg n) processors want to read the information about input y, and so we have a tar­ get 
group O(lg lg n) relation . The requests can be routed using [14]. Claim 4.5 In Step 3, with high probability, 
for every output y of G there are at most O(n - Ig lg n) values z in S such that (s1(z), ....st(z)) is 
a neighbor of y in G. Proof. Fix y = (VI, . . . w}. Let L, denote the neighbors of y, in H. Note that 
IL, I < 2dn 2. If s(x) has a neighbor g in G then s,(z) isin L,, for 1< i < t. The probability of this 
event is at most (2d/nc ) . Let xb be a O-1 random variable which is 1 if and only if the b-th member 
z of S has s(x) mapped to y in G. Apply Lemma 4.3: p is at most (2d/rze2)t by Lemma 4.2; let A be lSl(2d/nc2 
) . The probability that there are more than J such values x is at most om fd/2~(1-~ J. Given the claim, 
we have a target group O(lg lg n) re­lation . The requests can be routed using [14]. It remains to analyze 
Step 6. By Property 3.2, with probability at least 1 n a each group needs to be read by at most 6 @lg 
lg n of the requests, so we have a target group 6 lg lg n relation . The requests can be routed using 
[14]. Conclusions In this paper we have described a work-optimal algorithm which simulates an n lg lg 
n-processor EREW PRAM on an n­processor OCPC with O(lg lg n) expected delay. The proba­bility that the 
delay is longer than this is at most n-a for any constant cr. It would be interesting to determine whether 
this is the fastest possible work-optimal simulation. It would SJSObe interesting to discover how much 
delay is required in order to simulate a CRCW PRAM. We have recently derived an algorithm that simulates 
an n-processor CRCW PRAM step on an n-processor OCPC in time O(lg k + lg lg n) with high probabihty, 
where k is the maximum memory contention of the CRCW step. The simulation algorithm assumes that k is 
known. Thk assumption can be removed by augmenting the OCPC model to include a single bus which can be 
used to synchronize all of the processors: each processor can broadcast a 1 bit and every processor can 
determine whether or not any processor is broadcasting a 1 at any given time. We note that the lg k 
term in the simulation algorithm is provably necessary, as implied by an Q (Ig k) expected time lower 
bound for broadcasting the value of a bit to k proces­sors on a QRCW PRAM (and hence on an ERCW), by 
Gibbons, Matias and Ramachandran (see [12]). Evidently, the performance of the CRcw simulation de­pends 
on the maximum contention. A model that accounts for memory contention was recently proposed in [11]. 
In this model the run time of each step is a function of the memory contention encountered at this step. 
Thus, in the sub-model of SIMD-QRQW(lOg) PRAM, a step in which the maximum memory contention is k is 
assumed to take lg k time units. The CRCW simulation implies that an n-processor SIMD­ QRQW(log) PRAM 
algorithm OCPC, augmented with a high probability. We note is strictly stronger than the References can 
be simulated on an n-processor bus, with delay O(lg lg n) with that the SIMD-QRQW(log) PRAM EREW PRAM. 
<RefA>[I] H. Alt, T. Hagerup, K. Mehlhorn and F. P. Preparata, Deterministic Simulation of Idealized Parallel 
Comput­ers on More Realistic Ones. SIAM Journal of Computi­ng 16 (1987) 808 835. [2] R.J. Anderson and 
G.L. Miller, Optical Communica­tion for Pointer Based Algorithms, Technical Report CRI 88-14, Computer 
Science Department, University of Southern California, Los Angeles, CA 90089-0782 USA, 1988. [3] B. 
Bollobis, Martingales, Isoperimetric Inequalities and Random Graphs, in Combinatorics (eds A. Haj­nal, 
L. Levi.sz, and V. T. S6s), Coiloq. Math. Sot. Jdnos Bolyai 52 (North Holland 1988) 113-139. [4] J.L. 
Carter and M.N. Wegman, Universal Classes of Hash Functions, Journal of Computer and Systems ScZ­ences 
18 (1979) 143 154. [5] B. S. Chlebus, K. Diks, T. Hagerup, and T. Radzik, New Simulations between CRCW 
PRAMs, Proc. Foun­dations of Computation Theory 7 , Lecture Notes in Computer Science 380 (Springer-Verlag 
1989) 95-104. [6] M. Dietzfelbinger and F. Meyer auf der Heide, How to Distribute a Dictionary in a Complete 
Network, Pro­ceedings of the ACM Symposium On Theory of Com­puting 22 (1990) 117-127. [7] Martin Dietzfelbinger 
and Friedhelm Meyer auf der Heide, Simple, Efficient Shared Memory Simulations, Proceedings of the ACM 
Symposium On Parallel Algo­rithms and Architectures 5 (1993) 110 119. [8] Mary Mehrnoosh Eshaghian, Parallel 
Algorithms for Image Processing on OMV, IEEE Transactions on Computers, 40(7) (1991) 827-833. [9] M. 
Ger6b-Graus and T. Tsantilas, Efficient OpticaJ Communication in Parallel Computers, Proceedings of the 
ACM Symposium On Parallel Algorithms and Ar­chitectures 4 (1992) 41 48. [10] A. V. Gerbessiotis and L. 
G. Valiant, Direct Bulk-Synchronous Parallel Algorithms, Proceedings of the Scandinavian Workshop on 
Algorithm Theory 3 (1992). [11] P. B. Gibbons, Y. Matias, and V. L. Ramachandran. The QRQ W PRAM: Accounting 
for contention in par­allel algorithms. Proceedings of the ACM-SIAM Sym­posium On Discrete Algorithms 
5 (1994) 638-648. [12] P. B. Gibbons, Y. Matias, and V. L. Ramachandran. Efficient low-cent ention parallel 
algorithms. These pro­ ceedings, [13] J. Gil and Y. Matias, Fast ceedings of the ACM-SIAM gorithms 2 
(1991) 271 280. [14] Leslie Ann Goldberg, Mark Hashing on a PRAM, Pro- Symposium On Discrete Al- Jerrum, 
Tom Leighton and Satish Rae, Doubly Logarithmic Communication Algo­ rit hms for Optical Communication 
Parallel Computers, Pre-print, 1994. (A preliminary version of this paper appeared in Proceedings of 
the ACM Symposium On Parallel Algorithms and Architectures 5 (1993 ).) [15] Leslie Ann Goldberg, D. 
MacKenzie, An Cl(=) ing in Optical Networks,  [16] J. JiJci. An Introduction (Addison-Wesley, 1992). 
Mark Jerrum and Philip Lower Bound for Rout-These proceedings. to Par-aliel Algorithms. [17] Richard 
M. Karp, Michael Luby and Friedhelm Meyer auf der Heide, Efficient PRAM Simulation on a Dis­tributed 
Memory Machine, Pre-print, 1994. (A pre­liminary version of this paper appeared in Proceedings of the 
ACM Symposium On Theory of Computing 24 (1992) 318-326.) [18] Anna R. Karlin and Eli Upfal, Parallel 
Hashing an Efficient Implement ation of Shared Memory, Proceed­ings o.f the ACM Symposium On Theory 
of Computing 18 (1986) 160-168. [19] R. M. Karp and V. Ramachandran, Parallel Algorithms for Shared-Memory 
Machines, Handbook of Theoretical Computer Science, Volume A, (J. van Leeuwen, editor, Elsevier, 1990) 
869-941. [20] C. P. Kruskal, L. Rudolph, and M. Snir, A Complex­it y Theory of Efficient Parallel Algorithms, 
Theoretical Computer Science, 71 (1990) 95-132. [21] F. T. Leighton, Methods for Message Routing in Par­allel 
Machines, Proceedings of the ACM Symposium On Theory of Computing 24 (1992) 77 96. [22] Y. Matias and 
U. Vishkin, Converting High Probabil­ity into Nearly-Constant Time with Applications to Parallel Hashing, 
Proceedings oj the ACM Symposium On Theory of Computing 23 (1991) 307-316. [23] F. Meyer auf der Heide, 
C. Scheiderler, and V. Ste­mann, Fast simple dictionaries and shared memory sim­ulation on distributed 
memory machines; upper and lower bounds, Pre-print 1994. [24] P. D. MacKenzie, C. G. Plaxton, and R. 
Rajara­man, On Cent ention Resolution Protocols and Associ­ated Probabilistic Phenomena, Proceedings 
of the ACM Symposium On Theory of Cornputmg 26 (1994) To ap­pear. [25] C. McDiarmid, On the Method of 
Bounded Differences, Surveys in Combinatorics, London Math. Sot. Lecture Notes Series 141 (Cambridge 
Univ. Press, 1989) 148­ 188. [26] W. F. McCO1l. General Purpose Parallel Computing, in Lectures on Parallel 
Computation, Proc. 1991 AL-COM Spring School on Parallel Computation, Edited by A.M. Gibbons and P. Spirakis, 
(Cambridge Univer­sity Press 1993) 337 391. [27] D. E. Muller and F. P. Preparata, Bounds to Complex­ities 
of Networks for Sorting and for Switching, Journal of the ACM 22 (1975) 195 201. [28] Kurt Mehlhorn and 
Uzi Vishkin. Randomized and de­terministic simulations of PRAMs by paraJlel machines with restricted 
granularity of parallel memories. Acts Informatica, 21:339-374, 1984. [29] A. G. Ranade, How to Emulate 
Shared Memory, Jow-­nal of Computer and Systems Sciences 42 (1991) 307 326. [30] J. H. Reif, editor, 
A Synthesis of Parallel Algorithms (Morgan-Kaufmann, 1993). [31] A. Siegel, On Universal Classes of 
Fast High Perfor­mance Hash Functions, Their Time-Space Tradeoff, and Their Applications, Proceedings 
of the IEEE Sympo­sium on Foundations of Computer Science 30 (1989) 20-25. [32] Jeanette P. Schmidt, 
Alan Siegel and Aravind Srini­vasan, Chernoff-Hoeffding Bounds for Applications with Limited Independence, 
Pre-print, 1994. (A pre­liminary version of this paper appeared in Proceedings of the A CM-SIAM Symposium 
On Discrete Algorithms (1993). [33] E. Upfal, Efficient Schemes for Parallel Communica­tions, Journal 
of the ACM 31 (1984) 507-517. [34] E. Upfal, A Probabilistic Relation Between Desirable and Feasible 
Models of Parallel Computation, ProceecZ­ings of the ACM Symposium On Theory of Computing 16 (1984) 258-265. 
[35] E. UpfsJ, A. Wigderson, How to Share Memory in a Distributed System, Journal of the ACM 34 (1987) 
116-127. [36] L. G. Valiant, General Purpose Parallel Architectures, Chapter 18 of Handbook of Theoretical 
Computer Sci­ence, Edited by J. van Leeuwen (Elsevier 1990).  </RefA>
			
