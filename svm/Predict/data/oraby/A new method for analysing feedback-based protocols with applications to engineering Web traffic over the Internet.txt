
 A New Method for Analysing Feedback-Based Protocols with Applications to Engineering Web Traffic over 
the Internet D. P. Heymant T. V. Lakshman* Arnold L. Neidhardt** t AT&#38;T Labs * Bell Labs * * Bellcore 
101 Crawfords Corner Road 101 Crawfords Corner Road 331 Newmans Springs Road Holmdel, NJ 07733, USA Holmdel, 
NJ 07733, USA Red Bank 07701 NJ USA dph@buckaroo.att.com lakshman@research.bell-labs.com arnie@bellcore.com 
 Abstract Most of the studies of feedback-based flow and congestion control consider only persistent 
sources which always have data to send. However, with the rapid growth of Internet ap-plications built 
on TCP/IP such as the World Wide Web and the standardization of traffic management schemes such as Available 
Bit Rate (ABR) in Asynchronous Transfer Mode (ATM) networks, it is essential to evaluate the performance 
of feedback-based protocols using traffic models which are specific to dominant applications. This paper 
presents a method for analysing feedback-based protocols with a Web- user-like input traffic where the 
source alternates between transfer periods followed by think periods. Our key re- sults, which are presented 
for the TCP protocol, are: (1) The goodputs and the fraction of time that the system has some given number 
of transferring sources are insensi-tive to the distributions of transfer (file or page) sizes and think 
times except through the ratio of their means. Thus, apart from network round-trip times, only the ratio 
of aver- age transfer sizes and think times of users need be known to size the network for achieving 
a specific quality of service. (2) The Engset model can be adapted to accurately compute goodputs for 
TCP and TCP over ATM, with different buffer management schemes. Though only these adaptations are given 
in the paper, the method based on the Engset model can be applied to analyze other feedback systems, 
such as ATM ABR, by finding a protocol specific adaptation. Hence, the method we develop is useful not 
only for analysing TCP using a source model significantly different from the com-monly used persistent 
sources, but also can be useful for analysing other feedback schemes. (3) Comparisons of simulated TCP 
traffic to measured Eth-ernet traffic shows qualitatively similar autocorrelation when think times follow 
a Pareto distribution with infinite vari-ance. Also, the simulated and measured traffic have long range 
dependence. In this sense our traffic model, which purports to be Web-user-like, also agrees with measured 
traffic.  Permission to make digital/hard copy of part or all this work for personal or classroom 
use is granted without fee provided that copies are not made or distributed for profit or commercial 
advan-tage, the copyright notice, the title of the publication and its date appear, and notice is given 
that copying is by permission of ACM, Inc. To copy otherwise, to republish, to post on servers, or to 
redistribute to lists, requires prior specific permission and/or a fee. SIGMETRICS 97 Seattle, WA, USA 
8 1997 ACM O-69791-909-2/97/0006...$3.50  1 Introduction With the rapid growth of TCP/IP based Internet 
applica-tions, such as Web servers and browsers, and with the stan- dardization of the ATM ABR trail% 
management scheme, it has become crucial to understand the behavior of these feed- back based flow and 
congestion control protocols in a realis-tic scenario using traffic that corresponds to those produced 
by dominant applications. This is especially true for TCP since it is an end-to-end transport protocol, 
and the traffic offered to a particular TCP connection is typically from one application. However, one 
of the challenges in research on the engineering of networks with feedback controlled traffic is the 
difficulty of analysing feedback flow control proto-cols using bursty traffic typical of most applications. 
Hence, most of the studies of feedback-based flow and congestion control consider only persistent (infinite) 
sources which al- ways have data to send. With persistent sources, the rate from the sources is limited 
only by the allowed window, as in TCP, or allowed rate as in instances of ABR. We do not consider infinite 
data sources as prior studies have done. Infinite data sources have been useful in devel- oping an understanding 
of the effects TCP s dynamic win-dows on performance, and are also the right models when applications 
such as audio streaming, use TCP for trans-fers However, this scenario is not our concern here. Our primary 
interest is in modeling Web-like applications, where users request a file transfer (by clicking on a 
link in a Web browser for instance) and after this transfer is completed enter a think (idle) period. 
Another file transfer request is made on completion of the think period. Consequently, we use an on-off 
source model where an on-state starts with the user requesting transmission of a file (an HTML page for 
instance). The length of the on-period depends on the activity of other connections and it lengthens 
with conges- tion. When the source completes its transfer it enters the off-period. At the end of the 
off-period, another transfer request is made. The basis for our contribution is the Engset model, which 
we modify to develop a method for analysing feedback-based protocols where the offered load of each source 
is as de-scribed above. There is no notion of feedback, in the Engset model and congestion causes blocking. 
In contrast, the use of feedback to backpressure traffic results in rate reduction to sources instead 
of blocking. The rate reduction may trig- ger efficiency impairing effects in the protocol because the 
protocol may not track the available bottleneck link rate accurately. An example is the large window 
oscillations in TCP which impairs efficiency in the absence of sufficient buffering. We adapt the Engset 
model to account for this rate reduction, and to any efficiency-impairing effects spe-cific to the protocol 
of interest. We present adaptations specific to TCP operating in conjunction with various cell or packet 
discard schemes. Our adaptation of the Engset model allows for heterogenous sources that can have arbi- 
trary distributions for the sizes of retrieved Web pages and for the times between retrievals. By picking 
distributions with infinite variance, our model accounts for traffic with long range dependence since 
it has been shown [6] that on-off source models exhibit long range dependence if either the on or the 
off periods have infinite variance. Feedback schemes other than TCP, such as ATM ABR, are similarly analyzable 
using our method provided the protocol specific efficiency-impairing effects are accounted for in the 
adapta- tion. The main contributions of this paper are as follows: To our knowledge, this is the first 
analytical study of feedback- based protocols with Web-like traffic. The Engset model based method that 
we develop is of general applicablity in the study of feedback-based protocols provided some pro-tocol 
specific efficiency reducing factors can be determined. We illustrate this by using the model to derive 
goodputs which accurately match det,ailed TCP simulations (as we will see in Section 6) for TCP and TCP 
over ATM, with d&#38;rent buffer management schemes. The different buffer management schemes have different 
efficiency impairing ef-fects. An important consequence of our mathematical model, which is verified 
by simulations, is that the goodputs, and the fraction of time that the system has some given number 
of transferring sources, are insensitive to the distributions of transfer (file) sizes and think times 
except through the ratio of their means. Thus, apart from network round-trip times, only the ratio of 
average transfer sizes and think times of users need be known to size the network. Hence, our method 
provides a means for Internet traffic engineering with min- imal information about source behavior. Also, 
our traffic model agrees with measured traffic in the sense that com-parisons of simulated TCP traffic, 
with Pareto think times, to measured Ethernet traflic show qualitatively similar auto-correlations. Also, 
the simulated and measured traffic have long range dependence although with different Hurst pa-rameters. 
Closer comparison was not viable due to lack of knowledge of round trip times in the measured traffic. 
2 System Model and TCP Background The TCP code used in the simulations closely parallels Reno implementations. 
The mathematical model captures the effects of the primary factors affecting TCP performance and, as 
we will see, predicts accurately the results obtained from simulations. The insensitivity implied by 
the model is confirmed by the simulations and the goodputs predicted by the model for TCP and TCP over 
ATM, with various buffer management schemes, match the simulation results very well. Hence, the model 
provides a tool for traffic engi-neering the Internet to support Web access using TCP. 2.1 Description 
of the system model Our system model is similar to that used in [7, 10, 131, with the following key difference: 
we do not consider infinite data sources as in previous studies. Instead we use a Web-user- like source 
model, henceforth refered to simply as an on-off model. Note that in our on-off model the on-period can 
get lengthened by congestion, and the off-period starts only after completion of this lengthened on-period. 
An on-state starts when a user clicks on a link in a Web browser and starts a file transfer. For the 
simulatidn results presented, we use deterministic or exponentially distributed file sizes with a 200KB 
mean. Note that the mean file size does not change the analysis unless the file sizes are so short that 
TCP s congestion avoidance is not in effect at all. How-ever, since our analysis is for feedback protocols 
we are in-terested in the case where TCP feedback plays a significant role. The length of the on-period 
depends on the activity of other connections and lengthens with congestion. When the source receives 
the acknowledgement (a&#38;) for the last trans- mitted packet of the file, it declares the transfer 
complete and enters the off-period. For the results presented, off pe- riods are drawn from deterministic, 
Pareto, or exponential distributions. The mean off period is 5 seconds. We present results for 25 and 
50 TCP sources. We have compared re-sults for various other numbers of sources. on-- \-- SrnUn o(( bmrawc.-.w 
Ti+ TW soww -. .* laar -..* *... TCP -I.* - - --.m--w-b.m laap _ _ __ _ _ -_ ------------1,+1 lslrrr*rl 
%--- _,*'  w,,/ /Tcp Figure 1: Simulated System Configuration The simulated system model is shown 
in Fig. 1. Each TCP source i is connected to a router or switch via an access line of capacity ci bits 
per second and propagation delay di seconds. The switch or router is simulated by considering it as a 
multiplexer with a FIFO buffer of size B packets and an outgoing link of capacity C bits per second. 
This outgoing link has a delay D and is connected to the destination nodes. Simulation results are presented 
for both ATM networks and packet based networks. We used packets of size 576 bytes. ACKS are 40 bytes 
long. For the ATM case, each packet is converted to 12 cells of 53 bytes (48 bytes payload + 5 bytes 
ATM header). ACK packets are transported in The transfer and hence the on-period can also be terminated 
by the user clicking on another link during the on-period, aborting the current transfer. one cell. The 
segmentation and reassembly functions are done at the TCP sources and destinations. Hence, ATM transport 
is used over the access lines as well as the multi- plexer outgoing line. The multiplexer buffer holds 
packets for the packet simulations or an equivalent amount of cells for the ATM simulations. The receiver 
advertized window is set high enough that the network is always the bottleneck. The maximum congestion 
window reached is limited only by the capacity of the network. The scenario of interest to us is Web 
server access over ISDN access lines. Hence, we simulate access lines of ca-pacity 128 Kbps (i.e. 2 ISDN 
B channels) and a .lms delay each way. The multiplexer outgoing link rate is set to 1.5 Mb/s (i.e. a 
Tl line). The delay from the multiplexer to the destinations and back is taken to be 300 ms. This delay 
is set to include not only the propagation delay over the 1.5 Mb/s link but also the end-system processing 
delays at the destination. Note that the link delays indicated do not in- clude the service time of the 
packet on the link. It is however included in the round trip time calculations. 2.2 Brief Summary of 
TCP Window Flow Control TCP is a go-back-n retransmission based reliable transport protocol. Each connection 
uses a dynamic window flow con- trol protocol. The window size limits the maximum number of packets that 
can be sent without receiving an acknowl- edgement. The window changes when packets are acknowl- edged 
or when loss is detected. Destinations send cumula- tive acknowledgements for each received packet indicating 
the next expected packet. Packet losses are detected either by timer expiration or by receipt of three 
duplicate acks, i.e. three successive acks indicating the same next expected packet (see [12] for details). 
When a packet loss is detected by timer expiration, TCP-Reno reduces its window to one and retransmits 
the lost packet. The window subsequently is increased by one for every received a.&#38;. This fast growing 
phase, called the slow start phase, continues till the window reaches a slow start threshold equal to 
half the value at which the previous loss was detected. After the slow start phase, the window is increased 
by one for every window s worth of acknowledge- ments. This window growth continues till the maximum 
window size, determined by the receiver advertized window, is reached or till a packet loss is detected. 
Detection of a loss through duplicate acks causes the window to be halved but slow start is not initiated. 
The window evolves as given below. Let W denote the current window size and Wt the slow start threshold. 
(1) After every newly-acked packet, the algo-rithm works as follows: if W < Wt, set W = W + 1; Slow Start 
Phase else set W = W + l/[W]. Congestion Avoid-ance Phase Next expected byte in practice, but since we 
use fixed length packets we dispense with this subtlety. Also, for simplicity of exposi- tion w-e assume 
that an ack is sent for every received packet though this is not necessary in practice. (2) When the 
number of repeated ACKs exceeds a threshold, retransmit next expected packet; set Wt = W/2, then set 
W = Wt (i.e. halve the window); resume congestion avoidance using new window once retransmission is acknowledged 
 (3) Upon timer expiry, the algorithm goes into slow start as before: set Wt = W/2; set W = 1.  TCP-Reno 
permits a temporary expansion of the out-standing number of packets beyond the current window size. This 
is done to prevent a big burst of packets from being sent following a succesful retransmission. (See 
[5] for details.) We omit this from our description. However, it is implemented in the simulations. 3 
Review of the Engset Model This section presents the rudiments of the queueing model where arrivals occur 
in a quasirandom fashion (i.e. finite-source Poisson) and blocked attempts are cleared (denied service). 
This model is the basis of our contribution, and this section serves to record the results we need and 
introduce notation; it is based on Section 3.7 of Cooper [2]. Each source alternates between active and 
idle phases which have random durations. The active periods represent time intervals when the source 
is sending or receiving data. The source is silent during the idle periods. The length of an active phase 
has distribution function G with mean l/p < 00. The length of an idle phase has distribution function 
F with mean l/X < 00. The durations of the phases are mutually independent. This is known as an alternating 
renewal process. Let aon be the long-run probability that the source is active; then aon dg )~;P(source 
active at t] lim total active time by t = (W.P.1) (1)t-m t and VP aon = l/j&#38; + l/X see, e.g. Heyman 
and Sobel (31, Section 5-6. Eq. (1) is the reason a,,,, is called the intended offered load of a source. 
An important feature of (2) is that aon depends on F and G only through their means; the particular functional 
forms and higher moments are irrelevant in computing aon. (These may alIe&#38; the rate of convergence 
in (l), but not the limit.) Formulas with this property are said to be insensitive; we will see more 
insensitive formulas in the sequel. Let N be the number of sources, and suppose they share a channel 
that can carry at most s simultaneous active sources. To fix ideas about s, the simulations in Section 
6 have sources that are connected to a 1.5 Mb/s line via 128 Kb/s access links. Active sources send data 
to the line using the full capacity of the link, so s = 11500/128j = 11. 4 Calculation of TCP effects 
Let Pjbe the steady-state probability that j sources are ac- tive. When N 5 s there is no contention 
among the sources for the channel, so from (1) and (2), Pj has the binomial distribution Pj = &#38;(l 
-aon)N-j,j=O,l ,..., N<s. (3) Pj is insensitive because aon is. When N > s we temporarily assume that 
the idle and ac- tive durations have exponential distributions. The number of active sources fluctuates 
according to a birth-and-death process. Let @j and 6j be the birth and death rates (respec- tively) in 
state j. They are given by (N-j)X j=O,l,..., s-l (4) 0 j=s and &#38;j=jp,j=1,2 ,..., S. (5) The steady-state 
probabilities of a birth-and-death process are given by p. =pOflOP1..-flj-l 2 .,.; 3 ,j=l 1 , 6162 6j 
 PO is chosen to make the probabilities sum to one. It is useful to introduce the offered load per idle 
source, which is denoted by ir and given by When N > s we use (4 -7) to obtain N  (4 kj Pj=,i=oi T)l 
,i=O.lj...,~<N. (8) In terms of aon, (8) is N (4 a,,j(l -a,,)N-j ei = cizo i ; ) aonk(l -aon)N--L 3 
= 011y...7s < N. To develop a quantitative estimate of how TCP can affect performance, this section explores 
an extreme case in which a fixed number of users are transferring files which are in- finitely long. 
We assume that each transfer proceeds over its own TCP connection, and we are particularly interested 
in the case in which enough users are involved to create conges- tion at some network resource with some 
capacity C. In this congestion, as packets are lost and retransmissions become necessary, the successful 
transfers of file contents proceed at a total rate that can be well below the physical capacity C. Quantitatively, 
the file transfers proceed at some average rate PC, with p 5 1 representing an effective efficiency for 
the bottleneck. Based on some of the details of how TCP reacts, our purpose in this section is to form 
an estimate of p in terms of some elementary properties of the network and sources that a person engineering 
the system bottleneck can identify. Population size Window size Service rate i /... . . . . . . . . . 
.___.._...s.----* _.____.__------. . . . . . . . . . . . . ; E B .. I ..-* : *.-* ; : ..--- 4 : i *.- 
. ..* . . ..* if ; I... iF i I/-/1 Figure 2: Evolution of service rate with windows and pop- ulation 
The estimate is based on how windows evolve in TCP. A TCP source employs a window to control its transmis- 
sions, which is dynamic to adapt to different situations. Figure 2 indicates roughly how this window 
evolves dur-ing congestion, growing slowly over long periods punctuated by short episodes of collapse. 
Mostly, these oscillations of the windows of different TCP connections competing for a congested resource 
turn out to be in phase. So the total . . (9) Cohen [l] proved that (9) is insensitive; moreover, insensitiv- 
ity occurs even when the sources have different mean active and idle periods. For heterogeneous sources, 
we must keep track of the number of sources each type that are active; the probability that a total of 
j sources are active is obtained by summing the probabilities for the various ways this can occur. Our 
model for TCP traffic uses different birth and death rates than (4) (5) and possesses the insensitivity 
property. The insensitivity property allows us to present the model in terms of exponential on and off-time 
distributions (which is easy) and have the solution apply to any other distributions with with the same 
means. population of packets competing for the resource remains practically in proportion to the window 
of any one connec- tion, as suggested in the figure by its use of a single trace to represent both a 
population and a window. At any one time, the service that a connection receives equals its win- dow 
divided by a round-trip time. So long as the population of competing packets is enough to keep the resource 
busy, the resource will deliver service at its full capacity, the ex- cess packets constituting the resource 
s queue. When the population is smaller, the resource will be delivering less ser- vice, in proportion 
to the current population of packets as they circulate on their routes. This relation of service rate 
to population is also indicated in the figure. If service could be delivered at capacity for a full cycle, 
there would be no inefficiency. As the figure indicates, however, the resource can easily spend much 
of the time at less than full capac- ity. Indeed, as pictured in the figure, the inefficiency is the 
fraction of the full rectangle C3 ABCD represented by the triangle flAEF. The rest of this section explains 
why the figure reflects TCP performance and calculates the estimate for the efficiency that the figure 
suggests. 4.1 Quantitative aspects of the behavior For this purpose of generating estimates, the assumption 
will be made that all connections share the same round-trip time. 4.1.l instantaneous rates Let J&#38; 
= round-trip time with empty bottleneck queue, w, = current window of connection of user u, P = c W,, 
= total outstanding packet population, B = si , of the bottleneck s buffer, Q = size of the queue currently 
held in the buffer, and 7 = work time of the bottleneck to serve one data unit, where the data unit 
is to be the same (byte, bit, packet, or cell) as that used to measure W,,, B, and Q. Then W,,f&#38; 
is roughly the rate at which the connection of user u re-ceives service, at least during the early part 
of the cycle in which the windows have not grown enough to keep the bottleneck busy. Correspondingly, 
P/R0 is the total rate at which traffic is arriving and being served by the bottleneck in this part of 
the cycle. Indeed, since the physical capacity of the bottleneck is C = l/r, this early part of the cycle 
is defined by the comparison P/h < l/r. In the next part of the cycle, in which growing windows correspond 
simply to a growing queue, the bottleneck is kept busy, serving the connections at its capacity l/r. 
Finally, during an over-flow episode and its immediate aftermath, the bottleneck may carry some fragments 
of packets that overflowed partly while trying to enter the queue, as well as some retransmit- ted packets 
that had actually been delivered successfully. Such activity provides no service to the connections. 
The overflow episode itself should be a very tiny portion of a complete cycle, even if one considers 
the episode to include the immediate aftermath in which connections gradually dis- cover their losses. 
Accordingly, to estimate the service being delivered by the bottleneck, our approach is simply to esti- 
mate the evolution of P over the bulk of a typical cycle in which the connections are growing their windows 
slowly in the congestion-avoidance manner. 4.1.2 Local evolution within congestion avoidance Within 
the congestion-avoidance phase, each connection grows its window by one packet every round-trip time, 
which is &#38; during the early part of the cycle. Thus, during this early part, P is growing linearly. 
When the bottleneck has a growing queue, increased round trip times cause P to grow slightly slower than 
linearly. The deviation from linearity is insignificant except when the buffer can hold very long queues. 
Precise formulas could be developed to account for the decrease, but for simplicity, in our estimates, 
we will ignore the decrease and assume that P grows linearly. 4.1.3 Identification of the peak population 
Let Ph;ph be the population at which the first overflow oc-curs. At the instant of the first overflow, 
the amount B must be residing in the buffer, while the rest must be in circulation on the routes of the 
various connections. Any member of the unbuffered circulating portion of the popu- lation must have passed 
through the bottleneck within the last J7.c time units. Any member of the population that last received 
service within the last &#38; time units cannot have made it back yet to the buffer. Thus, Thigh is the 
sum of B and the population members that have received service in the last &#38; time units. Since the 
bottleneck would have been kept busy in this late phase of the cycle, the number of unbuffered circulating 
members must be Ro/r, so Ra Phigh =B+ 7.  4.1.4 Depopulation from two factors Let Pi,, be the population 
size when the congestion-avoidance phase begins. It is obtained from the larger pre-drop pop-ulations 
as the individual connections experience window halvings. The number h of halvings equals the number 
of packets lost by the connection in the overflow episode. In the spirit of simplicity, we will also 
pretend that every con- nection loses an average number of packets, so that all the windows are cut by 
the same factor 2-h. With this assump- tion, Pi,, can be identified as 2-hPh;gh. This identification 
still leaves the task of calculating h, the average number of packets lost per connection in a typ- ical 
overflow episode. Our estimate of h will be organized into two factors: one to estimate the number of 
collisions in a typical overflow episode, which turns out to depend on the location within the queue 
where discarding is performed, and the other to estimate the damage done per collision. Thus, h = 1. 
d, (11) where 1 is the location-influenced average number of colli- sions created per connection, and 
d is the average damage done by a collision in the form of lost and damaged packets. 4.1.5 Number of 
collisions per connection Collisions are caused when connections raise their windows, which they do once 
per round trip. The overflow episode lasts for roughly a round-trip time. We will perform cal-culations 
only for the extreme possibilities of discarding at the tail or discarding from the front. When discards 
occur at the tail, then the overflow episode lasts a full round trip, and every connection raises its 
window exactly once in this period. Since each window raising corresponds to a collision, the number 
of collisions equals the number of connections. Thus, 1 = 1 with tail dropping. Now consider the case 
in which discards occur at the front of the queue. Here, the episode is shortened by the length of the 
queue, so 1 equals the fraction of the full round-trip time that corresponds to the portion Rc that excludes 
the time in queue. (Recall that &#38; was the zero-queue round-trip time, but the round- trip time governing 
the frequency of window raisings must include the time spent waiting in the queue.) To compute this fraction, 
recall that in this period of a full queue, the bottleneck can serve the full population Ph;,$, = B + 
&#38;/r in a full round trip, but it only serves the amount RQ/T in the portion &#38;. Hence, the desired 
fraction is given by this ratio: 1 = (&#38;/T)/(B + h/r) = l/((Br/&#38;) + 1). At this point, it is convenient 
to introduce the normalized buffer size (12) i.e. the buffer size in units of the bandwidth-delay product 
J&#38;/r. To summarize the collision-count results of the two cases of dropping at the tail or from the 
front, It=1 &#38; I,=&#38; b+l  4.1.6 Damage per collision Since a collision is caused when a connection 
adds one packet to the population through a window increase, the resulting overflow will be by the same 
amount: a packet s worth of data. Unfortunately, this overflow need not be in the form of a single whole 
packet, unless the queue management is sr- ranged precisely to deal only in whole packets. The damage 
per collision depends on the manner of queue management. Packets damaged in a collision  1  I ~ I 
I 3 if by cell 1 if by whole packet (EPD) 1.5 if by packet tail (PPD) Figure 3: Colliding packets We 
will perform calculations for 3 types of queue man-agement: in whole packets, by tails of packets, or 
by individ- ual units or cells within packets. In TCP over ATM, Partial Packet Discard (PPD) and Early 
Packet Discard (EPD) are examples of queue management by tails of packets and by whole packets respectively 
[q]. To estimate the damage from a collision, we imagine, as in Figure 3, the interloping packet colliding 
with the invited stream of packets that should have been arriv- ing and would not have induced an overflow 
on their own. (They are invited in the sense that each was authorized through the acknowledgment of a 
corresponding packet that was served earlier by the bottleneck, as opposed to the in- terloper that was 
only authorized by a window increase.) More precisely, we imagine the interloper beginning to arrive 
while invitee A is entering the queue. Moreover, we imag- ine that the interloper will not finish arriving 
at least until the next invitee (B) would have begun entering the queue. For simplicity, we will also 
assume that the discards will be made at the tail, thereby damaging at most the 3 con- tending arrivals. 
If queue management is by the cell, then cell discards will occur as long as the interloper is arriving, 
with some cells managing to enter the queue as the bottle- neck resource works as fast as the invitees 
arrive. With in-dependent cell-discard decisions, all 3 contenders would be damaged. At the other extreme 
of management by whole packets, the discarding of a packet s worth of data would re- quire only one packet 
to be discarded, so the damage would be just one packet. The intermediate case of management by tails 
of packets involves an extra complication. When the interloper begins to arrive, the queue manager quickly 
encounters an arriving cell that cannot enter the buffer, and so the manager discards the cell. Whether 
this discarded cell belongs to the interloper or to the invitee A is random, being an accident of which 
cell had the manager s attention when a queue overflow became necessary. Thus, one of these two packets 
loses a cell, at random, after which, the queue manager ensures that no more cells of that packet ever 
en- ter the queue. If the damaged packet is the interloper, that suffices to avoid further overflows 
in this collision, and the total damage is just one packet. If, however, the damaged packet is invitee 
A, then another overflow decision will be required when invitee B begins to arrive, so 2 packets will 
be damaged in total. Thus, on average, queue management by packet tails leads to 1.5 damaged packets 
per collision. To summarize the damage-per-collision results, d, = 1 &#38; dt = 1.5 &#38; d, = 3. (14) 
 4.1.7 Combined effect on efficiency These observations can be combined into a formula for es-timating 
the efficiency with which a resource will be used when TCP sources are congesting it. Recall that in 
a typi- cal cycle, the population P of packets, that the TCP sources maintain in circulation in the network, 
grows almost linearly fi0I.U Pl,, t0 Phi&#38;, where Pi,, = 2-ldPh@,. Now SO long aS P is above the number 
RQ/T of packets that the bottleneck could have served in the past round trip, the bottleneck will be 
kept busy. It is convenient to normalize populations to this value, i.e. to introduce Phigh RLW -= b-t 
1 &#38; plow = -= 2- d(b + 1). phigh = fw Rob (15) Correspondingly, the normalized population p = P/(&#38;/T) 
grows almost linearly from plow to ph;gh, where plow = 2-ldphigh. Notice that so long as p 2 I, the bottleneck 
will be kept busy. In particular, if plow > 1, then there will be no inefficiency. On the other hand, 
if pcow< 1, then the bot- tleneck is spending part of the cycle underutilized. In fact, so long as p 
< 1, this normalized population p equals the practically instantaneous occupancy of the bottleneck (in-stantaneous 
relative to the time scale of a full cycle). Thus, as p grows linearly from plow < 1 (the nontrivial 
case) to phigh > 1, there are the following two phases. 1. As p grows linearly from plow to 1, it is 
the bottle- neck s occupancy, which is therefore growing linearly to saturation. 2. AS p grows from 1 
to nhigh, the bottleneck simply re-mains saturated. The average efficiency of the bottleneck is simply 
the aver- age over the cycle of the bottleneck s occupancy, which first g-rows linearly from plow to 
1, and then remains at 1. This average is easily computed as p = 1 _ (0 -PrOw)+)2 (16) 2(Phigh -plow) 
 where the positive-part operator z+ = max{z,O} yields a formula valid even when plow > 1. The formula 
(16) is our estimate for the efficiency at which a bottleneck can work during congestion when the traffic 
comes from TCP connections. 5 TCP-modifled Engset From our perspective of attempting to engineer a network 
resource, such as a link, we imagine the users of our re-source as alternating between an active phase 
of retrieving a Web page across our resource and an idle phase of study- ing the retrieved page (and 
perhaps retrieving other pages across other routes that miss our resource). Without any capacity-related 
bound on the number of simultaneously ac-tive users, individual service rates are effectively reduced 
whenever too many users are active, as they are forced to share the limited capacity of a bottleneck. 
The reduction becomes even more severe as the congestion induces some of the efficiency-impairing effects 
described in the previous sec- tion. Our adaptation of the Engset model accounts for these effects, and 
allows for statistically inhomogeneous users that can have arbitrary distributions for the sizes of retrieved 
Web pages and for the times between retrievals. The arbi-trariness in these distributions turns out to 
imply that our model includes situations with long-range-dependent traffic. 5.1 Explicit adaptations 
The basic adaptation to the model is to change the effect of congestion from service denial to service-rate 
reduction. There are N users in total, of which a random number J(t) are active at a time t. For expository 
convenience we con- sider homogeneous sources: as with the Engset model this is not necessary. Users 
are connected to the network via access lmes with capacity c bps. The mean size of a Web page is w, so 
when there is no congestion at our resource, an individual retrieval can be completed in an average time 
of P -r = w/c. Assume that an individual phase of idle- ness is completed in an average time of X- . 
Let C bps be the capacity of our resource, so s = [C/c] is the number of active users that the resource 
can handle without being a bottleneck. Accordingly, whenever j > a, we expect the efficiency of the resource 
to be reduced to a value p specified in Section 4. In this case, the aggregate service rate is PC, so 
an individual user will receive service at the rate PC/j. We assume that all users are equally affected. 
The insensi- tivity results in the Appendix allows us to make exponential assumptions without loss of 
generality, so we can think of J(t) as a birth-and-death process with birth and death rates &#38; = (N 
-j)X , j = O,l,. . . , N. 07) and j = O,l,. ,s sj= jcL j>s PCl w{ These adaptations determine our model. 
When p = 1 we call this the modified Engset model. The steady-state probabili-ties are given by (6). 
Notice that these probabilities depend on w and X only through the product -y = wX. While the number 
of active users is j, each active user receives service at the rate r(j) given by 1 if j 5 C/c r(j) = 
g (19) if j > C/c. j The throughput of the resource is then given by throughput = 2 P jr(j) = E[Jr(J)] 
(20) j=l  5.2 Quantffylng service quality For users browsing the Web, the primary measure of incon- 
venience is how long it takes a user to browse, or, given the virtually unlimited wealth of information 
available on the Web, how much can be browsed in the time users have. To formalize the discussion, let 
T be the average time it takes the network to complete the delivery of a page requested by a user, and 
let TO= w/c be the ideal time it would have taken if we had not been so cheap in giving our resource 
so little capacity that it became a bottleneck. The figure of demerit for the service users receive, 
that we will employ, is the ratio D = T/To (21) by which the actual average exceeds the ideal. The calculation 
of D requires the consideration of vari- ous average rates. The average rate at which the network delivers 
bits to all the users together is E[.Ir(J)], so these users receive complete pages at an average aggregate 
rate of E[Jr(J)]/w. This rate is also the average rate at which users complete cycles of retrieving pages 
and thinking about them. In particular, it is the long-term average rate at which users enter the active 
phase. By Little s law, the average number E[J] of users in this phase equals the product of this average 
rate with the average time T that a user stays in this phase. Thus. E[Jj = EIJr(J)l T - 1 (22) W so 
the figure of demerit is 4Jl D=x w/c = E(Jro1 From (20), the denominator of (23) is the throughput, 
so (23) relates performance measures of the resource to cus-tomer satisfaction. 5.3 Multiple classes 
of users This section describes the extensions of our model for han- dling users with differences in 
tratfic-generating behavior. In principle, each user u could have their own distributions W, and F,, 
for the retrieved page sizes and the thinking times. More probably, a network engineer will have identified 
at most a few classes 1.X of users with different parameters rk. For definiteness, this discussion will 
scale time so that the users of class k retrieve pages of average size 1 and think about them for an 
average time of 7; . Because of the insen- sitivity established in the appendi, the results are the same 
even if each user u has any individual distribution W, with mean w,, for page sizes and any individual 
distribution F,, with mean X; for thinking times, so long as Xuwu = 7r~ for all users u of class k. The 
model keeps track of the number Jk(t) of class-k users that are active at time t. Some things must be 
the same for all users in our model. First, all users must share the same access-line speed c. As one 
consequence, the criterion of congestion is still the simple criterion of whether J = c, Jk exceeds C/c. 
An-other requirement of our model is that during congestion, i.e. while J = j > s, each user receives 
the same fair share PC/j of the inefficient bottleneck as all the other users, re- gardless of the users 
classes. Thus, the effective service rate delivered to an individual user while j users are active is 
still given by the same function r(j) defined by (19) for the simpler version of our model. In keeping 
track of the numbers Jk(t) of class-k users active at time I!, the state is a vector I= (ji..jK). Let 
Nk be the number of users of class k in total. In the exponen- tial conception of user behavior, Jk jumps 
up by one at the rate (Nk -Jk)rk, and it jumps down by one at the rate JkF(ct JI). For this Markov chain, 
the stationary distribu-tion Pr is given by  l-IL ( ; ) 7-L PJ = Pa SE, jr)  (24) where n S(n) = fi 
F(m). (25) m=l Moreover, this distribution also turns out to satisfy a local- balance condition for 
the rates at which J jumps between any pair of states. Specifically, for any state $ for any class k 
with jk > 0, pf-ck(Nk -jk + l)rk = pfjkF(x jc), (26) 1 where ek is the unit vector along axis k (i.e. 
i- ek cor-responds to having one fewer busy user of class k than j). Although these equations have been 
motivated by the con- ception of exponential users, the insensitivity result estab lished in the appendix 
shows that the same equations still hold for nonexponential users. The appendix also justifies these 
equations, which have merely been stated here. The extension of (23) is cE[Jk]DkAL (27) WkfC E[Jkr(J)] 
 6 Comparison of the Model and Simulations The comparisons demonstrate that the model captures the important 
factors affecting TCP performance and that it predicts accurately the results obtained from simulations. 
An important insight from the analytical model is the insen- sitivity implied by the model and this is 
confirmed by the simulations. Also, the goodputs predicted by the model for different scenarios of practical 
interest such as packet TCP, TCP over ATM, and different buffer management strategies closely match the 
simulation results. Hence, the model pro-vides a tool for traffic engineering the Internet to support 
Web access using TCP. The match in different situations also shows that the model can be adapted and 
used for analysing different feedback-based systems. 6.1 Demonstration of insensitivity To demonstrate 
that the TCP simulations have the the in- sensitivity property implied by the mathematical model, we 
simulated two distributions for the file lengths and three dis- tributions for the off periods. The results 
are then compared to those obtained from the mathematical model. The two file length distributions are 
deterministic and exponential. We fix the mean transfer size to be 200 KBytes. This makes the mean on-time, 
in the free mode, to be 12.5s for the simulation configuration we use (Section 2). The three off-period 
distributions are deterministic, expo-nential and Pareto. The coefficient of variation (standard deviation 
divided by mean) for these distributions are zero, one and infinity respectively. The Pareto distribution 
be-cause of its infinite variance provides a good test of insen- sitivity when its results are compared 
to those obtained for deterministic off periods. With its shape parameter fixed at 2.5, the Pareto distribution 
has the complementary distri- bution function P[size > z] = &#38; ( > -1.5 ,x>-. mean 3 (28) The mean 
off periods are 5s in all simulations. The num-ber of sources was either 25 or 50, and the drop-from-front 
and drop-from-tail queue disciplines were used. In the drop- from-front discipline [8], when a packet 
or cell arrives to a full buffer, space is created in the buffer by dropping a packet or cell from the 
front of the buffer. For comparisons with the mathematical models, we computed results from the modified 
Engset model (i.e., the Engset model modi-fied to change the effects of congestion from blocking to rate 
reduction) and from our TCP-modified Engset model (i.e., the modified Engset model further modified to 
account for TCP not tracking the bottleneck rate accurately because of feedback delays and window oscillations). 
The distribu-tions of the number of active (on) sources obtained from the simulations are compared to 
those obtained from our TCP- modified Engset and the modified Engset models in Figures 6-9. In these 
figures, our TCP-modified Engset model is labled simply as our model. We have solved our model for the 
steady-state perfor-mance measures. To see how soon the steady-state mea-sures describe the simulation, 
we looked at the simulation results every 40 sets. We used two sets of distributions to test convergence: 
exponential on, and either Pareto off or exponential off periods. The goodputs as a function of time, 
as well as the steady-state value is shown in Figure 4. Both simulations reached steady-state after about 
4000 seconds. This indicates that steady-state performance measures are suitable for describing performance. 
Moreover, the rate of convergence to steady-state is not significantly affected by the distributions. 
The two cases are not distinguishable in Figure 4 since their goodputs are almost identical. The subsequent 
simulations, for the results presented below, are much longer and lasted 400000 to 2000000 seconds. Hence, 
we do not show confidence intervals in the plots. -.~n. Il.l on, P.,.Io on ....--. .lpa.lenum, on d on 
-----~~tavm,U I I , 0 ,000 2000 -4000 s-In-4 Figure 4: Goodputs vs. Simulation Time, drop-from-tail, 
25 sources, no ATM links Drop from front -50 sources T-0 10 20 30 .o w Figure 5: Number of busy sources, 
drop-from-front, 50 sources We plot only the distributions from the following cases for the simulations: 
(1) Deterministic file sizes and deter-ministic off times, labeled as deterministic times in the plots 
(2) exponential file sizes and deterministic off times, labeled as exponential on times in the plots 
and (3) deterministic file sizes and Pareto off times, labeled as Pareto off times in the plots. Other 
cases produce similar results. Note that there is only one plot for each model because of the insensitivity. 
Drop from front -25 sources Figure 6: Number of busy sources, drop-from-front, 25 sources Table 1: Goodputs 
for 25 sources: Drop from Front In Figures 6-9 we see that the mean number of active sources has the 
same distribution in all cases considered. The modified Engset model does very well here because the 
TCP degradation factor is very close to one. It is 0.9902 for drop-from-front and 0.9472 for drop from 
tail. The TCP degradation factor, the performance impairment due to TCP not accurately tracking the bottleneck 
link rate, is close to one only for our packet TCP configuration. We will see later that for TCP over 
ATM the degradation factor is much lower and the TCP-modified Engset model becomes necessary to predict 
performance measures accurately. The other independent performance measure that we used is the goodput, 
which is the information rate of suc-cesful transmissions divided by the speed of the access line in 
the same units. The information rate for each source in the simulations is computed by dividing the total 
number of bytes acked (i.e. successfully transmitted ) during the simulated time. To compute goodputs, 
which are expressed as fractions of access line capacity, the source information File sizes ] Detr ] 
Exp ] Detr Model Off times 1 Pareto I Detr I Detr E(active) 20.52 20.45 20.45 20.83 Goodput [%] 45.60 
46.65 45.56 46.42 Demerit 1.80 1.75 1.80 1.80 Table 2: Goodputs for 25 sources: Drop from Tail Drop 
from front -50 sources Drop from tall -50 sources Figure 7: Number of busy sources, drop-from-front, 
50 sources Drop from tall -25 sources Figure 8: Number of busy sources, drop-from-tail, 25 sources rate 
in bits is divided by the volume of data that could have been trasported by the access line during the 
same time (i.e. the volume of data that could be carried during the simu- lated time by the 128 Kbits/s 
access line). The goodput was computed for each source and then averaged over all sources; the results 
expressed in percentages are given in Tables l-4 The figure of demerit or demerit factor, defined in 
equation (21), can be obtained from the mean number of busy sources and the goodput. The tables also 
record the mean number of active sources and the demerit factor. We can also compute the output rate 
of the multiplexer: (29) where I$ is the output rate in Mb/s, N is the number of sources, g is the goodput 
in per cent, c is the access-line speed in Mb/s. The performance measures for the three choices of distributions 
(same cases as in the figures) are al- Figure 9: Number of busy sources, drop-from-tail, 50 sources Table 
3: Goodputs for 50 sources: Drop from Front most identical. The model is an excellent match to the drop- 
from-front simulations, and almost as good for the drop-from-tail simulations. These results demonstrate 
that the insensitivity property of the mathematical model is present when a detailed simulation of the 
TCP protocol is used. 6.2 Accuracy of the model To see how well the model performs with heterogeneous 
sources we ran two simulations using the drop-from-front rule. The off-periods are deterministic or Pareto 
distributed and the on-periods follow a deterministic or exponential probability distribution. Each simulation 
had three groups of sources; these are their characteristics. . Group 1: a burst of packets with mean 
length 1740 followed by an off period of 5 s; burst sizes are deter-ministic and off periods follow a 
Pareto distribution. . Group 2: a burst of packets with mean length 348 followed by an off period of 
5 s; burst sizes and off 1 File sizes I Detr I EXD I Detr I Model 1 Table 4: Goodputs for 50 sources: 
Drop from Tail 1 Class I Class 1 I Class 2 I Class 3 1 goodputs for 25 sources [%] Simulation 1 65.6 
1 47.2 1 26.8 Model [ 64.3 1 52.6 I 28.3 goodputs for 50 sources [%] Simulation 1 29.4 1 25.7 1 17.9 
Model 1 28.6 I 26.1 1 18.4 Table 5: Performance measures for heterogeneous sources File sizes Exp Detr 
TCP-Modified Modified Engset Engset 25 sources with droo from front I 50 sources with drop1 from front 
E(active) 1 46.82 1 46.77 1 46.46 1 45.60 Goodnut [%I 1 16.09 1 1 16.10 1 1 17.08 i 21.23 Demerit 1 5.82 
I 5.81 1 5.44 1 4.30 50 sources with drop from tail E(active) 46.90 46.86 46.77 45.60 Goodput [%] 15.67 
15.66 15.77 21.23 Demerit 5.99 5.98 6.01 4.30 Table 6: Performance measures for TCP over ATM periods 
are deterministic. . Group 3: a burst of packets with mean length 348 fol- lowed by an off period of 
25 s; burst sizes follow an exponential distribution and off periods are determin- istic. One simulation 
had 25 sources, 12 in groups 1 and 3, and 1 in group 2. The other had 50 sources, 20 in group 1, 5 in 
group 2, and 25 in group 3. The mean number of active servers and the goodputs by group are shown in 
Table 5. The model does an acceptable job of estimating these performance measures. It does a better 
job of estimating the total throughput. Using (29) and Table 5, with 25 sources, the simulation gives 
1.48 Mb/s and the model gives 1.41 Mb/s. With 50 sources the corresponding figures are 1.49 Mb/s and 
1.41 Mb/s. These results suggest that the model will be suitable for determining the capacity of a link 
or server receiving TCP traffic (e.g. a Web site) or for doing admission control at network access points. 
Comparisons for TCP over ATM There are two reasons for comparing results from the model to simulations 
in an ATM scenario. First, with the likelhood of increasing ATM deployment to carry Internet traffic, 
this scenario is of practical interest. Secondly, the use of ATM links changes the average number of 
packets dam-aged per congestion episode and hence alters the efficiency- impairment characteristics of 
the TCP protocol. This alter-ation in the effects of feedback provides an opportunity to test our model 
with an adaptation different from that for packet networks and to substantiate the statement about the 
general applicability of the model to different feedback protocols and scenarios. The simulations were 
done with 25 and 50 homogeneous sources; drop-from-front and drop-from-tail were used in both cases. 
The user behaviour is the same as Group 2 above. The off-periods are always deterministic and the file 
sizes are either determinsitic or exponentially distributed. The results are shown in Table 6. In Table 
6 we see that the insensitivity property is present, and that the simulations and the model give very 
similar per- formance measures. The output rate from the multiplexer can be computed from the goodputs 
via (29). Moreover, the percentage difference between the output rates obtained from the simulations 
and the model is the same as the per- centage difference in the goodputs. The maximum difference between 
a simulation and the model is 7 per cent. For drop-from-front, the TCP degradation factor is 0.8049, 
and for drop-from-tail it is 0.7336. These are significantly smaller than one, so the TCP-modified Engset 
model should produce performance measures that differ noticably from those produced by the modified Engset 
model. This can be seen clearly in Table 6. Note once again that the TCP-modified Engset model accounts 
for the feedback induced rate reduction (as opposed to blocking in the pure Engset model) during congestion 
as well as protocol specific perfor- mance impairements due to the rate reduction. The mod-ified Engset 
model accounts only for the rate reduction. Clearly, the more poorly a feedback-based protocol tracks 
the available link rate during congested periods, the more important it becomes to incorporate the protocol 
specific performance impairment factors into the adaptations of the modified Engset model. 7 Comparison 
Between Simulated Traffic and Measured Traffic Willinger et al. [ll] measured external Ethernet traffic 
that is 85 per cent TCP. We compare our simulated traffic to the busiest hour of the measured traflic. 
Since we don t know the details (e.g. number of sources, round-trip times, buffer sizes, etc.) that might 
enable us to match the measure-ments, we just try to show that our simulated traffic and the measured 
traffic have qualitatively similar second order (autocorrelation) statistics. Willinger et al. [ll] show 
that the measured traffic has long-range dependence; i.e. for large lags k, the autocorrela- tion function 
is proportional to kZ(H- ) with l/2 < H < 1. H is called the Hurst parameter. Krishnan [6] has shown 
that the on-off processes we use as individual source models exhibit long-range dependence if, and only 
if, either the on or the off periods have an infinite variance. Consequently, we examine the traffic 
generated when the file lengths (i.e. on periods) have a Pareto distribution with infinite variance. 
To describe a way to estimate the Hurst parameter of a Figure 10: Variance-time plot for measured traffic 
Figure 12: autocorrelation functions for measured and sim- ulated traffic 0 4 2 3 4 L-own, Figure 11: 
Variance-time plot for simulated traflic (stationary) time series we need some notation. Let X,, be 
time plot. The variance-time plot asymptotically has slope 2(H -1) for processes with long-range dependence. 
Test statistics are obtained by averaging the original series over non-overlapping blocks of size m. 
The variance-time plots for the data trace and the simulation are shown in Figs. 10 and 11 respectively. 
These figures contain a curve fitted through the points (the solid line) and a straight line that is 
parallel to this curve for large m (the dotted line). The latter is used to estimate the slope. For the 
measured traffic we obtain H = 0.92 and for the simulated traffic we obtain H = 0.81, which matches the 
qualitative property of long- range dependence. Figure 12 shows the autocorrelation functions for small 
lags. Both the measured and the simulated traffic s autocor-relations look like damped periodic functions. 
The periodic- ities are caused by sources going on and off in the simulated traffic. One of the periodicities 
in the measured traffic may be caused by the 200 ms delayed-a&#38; timer. The difference in the periods 
may be due to differences in the means of the on and off periods. We have no convenient way of verify- 
ing these surmises because we cannot easily recover some of the details of the sources creating the measured 
traffic. The slow damping towards zero autocorrelation is a precursor of the long-range dependence at 
large lags; if a finite variance distribution is used in place of the infinite variance Pareto, the autocorrelation 
function reaches zero rather quickly. While the matches between the autocorrelation functions are far 
from perfect, we think they exhibit similar qualita-tive properties. The long-range dependence property 
in the simulated traffic is predictable; the matching of the short-range dependence is a better indication 
that the second or-der properties of the simulated traffic resembles those of the measured traffic. 8 
Concluding Remarks The main contributions of our paper are as follows: (1) We develop a general analytic 
method for characteriz- ing the performance of feedback-based flow and congestion control protocols using 
a Web-user-like and bursty traffic model It may be noted that most studies of these protocols in the 
literature have used persistent sources due to the dif- ficulties involved in evaluating these protocols 
using traffic similar to those generated by important applications. (2) Our method, firstly, involves 
modifying the Engset model to account for feedback. Feedback changes the blocking ex-perienced by sources 
in the Engset model to rate reductions. If the feedback protocol is good in the sense that sources controlled 
by this protocol are able to reasonably accurately track the congested link s rate, then our modified 
Engset model suffices to analyze the protocol. With the purported ability of some of the ATM ABR schemes 
to track avail-able link rates very well, our modified Engset model may  well be applicable to this 
class of protocols without further significant modification. Our second modification is a proto- col 
specific adaptation to the modified Engset model. This is necessary because congestion and the associated 
rate re- duction often triggers degradation factors in many pro-tocols. An example is timer expiry causing 
sources to go idle. Clearly, the more poorly a protocol tracks bottleneck link rates, the more important 
this second adaptation. Our method is general in the sense that for a specific feedback protocol if the 
modified Engset model does not produce suf-ficiently accurate results, we can adapt the model further 
by identifying some protocol specific performance impairing effects that may be triggered by rate reductions. 
 (3) We apply our method to a situation of much current interest: traffic engineering of Internet links 
or servers to achieve a specified quality of service. We show how the modified Engset model may be adapted 
to TCP, TCP over ATM, and to different buffer management schemes. The input traffic to TCP is picked 
to model what a Web user might do. We calculate perfomance measures such as good- puts for all the above 
cases using the mathematical model and compare them to detailed TCP simulations. The match in the results 
show that our model can be used to predict performance in a given situation or to engineer links for 
a specified quality of service. (4) An important result for tra&#38; engineering purposes is an insensitivity 
that arises from the Engset model. The goodputs and the fraction of time that the system has some given 
number of transferring sources are insensitive to the distributions of transferred file sizes and idle 
times except through the ratio of their means. Thus, apart from network round-trip times, only the ratio 
of average transfer sizes and think times of users need be known to size the network for achieving a 
specific quality of service. We show that our TCP-modified Engset model is insensitive as well. (5) 
Since our model permits arbitrary distributions for file sizes and idle times, we can choose at least 
one of these dis- tributions to have an infinite variance. Such a choice results in long-range dependent 
traffic. We generated simulated TCP traffic with user think times following a Pareto dis-tribution with 
infinite variance. Comparisons of this simu- lated trafhc to measured Ethernet traffic shows qualitatively 
similar second order autocorrelation. Also, both simulated and measured traffic have long-range dependence, 
though the Hurst parameters are different, possibly because of dif- ferences in round-trip times and 
numbers of sources. Our traffic model, nevertheless, agrees with measured long-range dependent tragic 
in this sense.  Appendix: Proof of insensitivity The task for this appendix is to establish the claims 
of sec- tion 5, particularly those of insensitivity. The claims concern the stationary distribution for 
a process f(t) recording the numbers of active users of various classes. The claimed in-sensitivity is 
to the distributions W, and F, affecting the times that user II remains in active and idle phases. In 
addi- tion, section 5 merely asserted that the stationary distribu-tion was given by the formula (24), 
and that this distribution satisfies the local-balance condition (26). Most of section 5 dealt with the 
case of just a single user class, for which the relevant process J(t) was a scalar, and similar claims 
were made for this process. The claims for this scalar process J(t) are just special cases of those for 
the vector process l(t). Accordingly, this appendix will establish 3 claims for the vector process: that 
its stationary distribution is insensi- tive, that it is given by the formula (24), and that it satisfies 
the local-balance condition (26). The technique empl?yed for recognizing stationarity is to embed the 
process J(t) in a Markov process and then identify the stationary distribution of the Markov process. 
Since a user u changes between active and idle when the current operation is complete, a feature describing 
u at time t is the residue X,,(t) of u s current operation. If u is active, in the midst of retrieving 
a Web page, Xu(t) is the size of the remainder of the page still to be retrieved. Conversely, if 11 is 
idle, X%(t) is the time remaining in the idle period. It is important to identify the set J(t) of users 
that are active at time t. Accordingly, a Markov process that formalizes the dynamics of the system is 
(J(t),J?(t)), where J?(t) is the list of residues X,,(t) for the various users u. Letting f.4 be the 
set of all users, and &#38; be the set of users of class k, notice that Jk(t) = #(J(t) fl&#38;), the 
number of elements in the Set J(t) fl&#38;. The Markov process (J(t), Y?(t)) evolves as follows. For 
an idle user u $! .7(t), X,(t) decreases at rate 1. For an ac- tive user u E J(t), X,,(t) decreases at 
whatever rate r( J(t)) the network can provide service, where J(t) = #g(t) and where r(j) is given by 
(19). When some residue X,, hits 0 at some time t, the user u switches activity, so J gains or loses 
u, and the new residue X,,(t+) is chosen randomly according to the appropriate distribution W, or F, 
(WV if u is becoming active at t, F, if II becomes idle at t). Having identified the evolution of the 
Markov process (J(t), X(t)) along a sample path, the next step is to trans- late this evolution to distributions. 
Let pt(j, 5) be the den- sity at time t (not necessarily stationary) of the distribution of (J(t),r?(t)). 
Its time derivative can be expressed as a sum of terms corresponding to the kinds of changes that can 
occur along a sample path, as follows. atpt (j, ?I = c &#38;,pt(j, q + c d#ALP&#38; 3) UC uU + c r(#(j 
+ u))pt(j + u, Z-4 . F;(G)  u!Zj +Cpt(j -u, 5-u) . w;(&#38;), (30) uEj at least to the extent that 
the distributions W,, and F, have densities, where j + u and j -u are used to abbreviate the sets j U 
{r~} and j -{u}, and where 2-u denotes the vector a whose u component vanishes, i.e. y,, = 0, but whose 
other components match those of Z, i.e. y, = zv for v # u. For nondifferentiable distribution functions 
W,, and F,, this equation still holds in the sense of Schwartz distributions. A stationary density is 
one for which the right-hand side of this equation vanishes. An exponential version (Jezp(t),r?,,,(t)) 
may be con- structed through the replacement of W,, and F, by ex- 36 ponential distributions with the 
same means wu and X; . The memoryless property of exponential distributions im-plies that A=,,(t) is 
Markovian. As this finite-state Markov chain Jezp(t) evolves, the only events that occur are busy users 
becoming idle and idle users becoming busy. Accord-ingly, the condition for stationarity of a distribution 
z for J &#38;,(t) is that for any j c U, T(#(j+ u)) 0 = n(j + u) - 4j)X (31)  CL WU I vZj r(#j) + 
c 4j -zl)X, -,uT(j) [ 1 uCj Each summand is the difference between two terms a(jr)&#38; and 7f#~zM~2Vwat 
the rates at which transitions occur between neighboring states jr and jz with j2 = jr + u for some u 
# jr. Thus, a sufficient condition for a probability distribution x to be the stationary distribution 
of LZp(t) is for it to satisfy the local-balance condition ~(j _ u)xu = QM$j) forallj~UandalluEj. Consider 
now the distribution defined by (33) To show that this distribution satisfies local balance, it helps 
to rewrite the condition (32) as =r(j-u)A 4j) = r(j -u)$$ (34) r(#d ForanyjCUandanyuEj, (35) 7r(0) 
Hvcj 7 = l Jtil r(k)  = djh which is just the desired relation (34). Thus, this probability distribution 
z defined by the formula (33) is the stationary distribution for ,j&#38;(t) and satisfies local balance. 
The main task is to i_dentify the stationary density of the Markov process (,7(t), X(t)). Theorem 1 The 
density (36) is stationary for the Markov process (J(t), d(t)), where the complement jc = U -j of the 
set j C U is with respect to the full set U of users, and where L@v and Fv give the tail probabilities 
of the distributions W, and F,: l@,(z) = 1 -W (z) and &#38;) = 1 -F,(z). 37 Proof. A factor independent 
of a particular user u is e- (z) = n %$J n qi. (37) EjC-u  vEj-u Specifically, aeU(Z) is independent 
of zU. Next, evaluating the partial derivatives of p(j,iT), one finds &#38;,p(j,Z) = *(j)-w~Jz )O-,(iT) 
if u E j, and (38)  &#38;,p(j, 2) = r(j)*@-u(i!) if u$ j, (39) at least if W,, and F,, are differentiable 
at x,,. For nondiier- entiable distribution functions W, and F,, these equations still hold in the sense 
of Schwartz distributions. The next step is to calculate some offset evaluations p(j f u,z u). Noting 
that FU(0) = J% %(O)= 1, one finds p(j - u,Z-u) = (j+a),_U(iT) if u E j, and (40)  p(j +u,Eu) = v+-,(Z) 
if u $Z j. (41) Now one can proceed to check on the stationarity con-dition. Thus, for the density p(j, 
23, the right-hand side of (30) becomes C [h,~(i 4 + r(#(j + 4)pb + 21, W GW] UC + C [r(#iPd(i 4 +P(j 
-u, z-u) . WA(G)] uEj where the final equation holds because the local-balance con-ditions (32) for 
?r imply the vanishing of each quantity in square brackets. The result of this calculation is that the 
density p(j, Z) defined by (36) realiy is that of a stationary distribution of the process (J (t), X(t)). 
I The claimed insensitivity is now easily verified. Note that for any distribution function G with mean 
g, &#38;O G(z)&#38; = g. Hence, from the fact that (36) gives the joint stationary distribution of .7(t) 
and X(t), it follows that in equilibrium,  Pr(J(t) = j) = p(j, Z)dZ = r(j). (42) J c4  1O.m) Recall 
now that z was defined by (33), in which the distribu- tions W, and F, entered only through the ratios 
-yu = XUwU of their means wU and XL . Thus, (42) shows the distribu- tion of J(t) to be insensitive to 
the distributions W, and F, , except through these ratios. The remaining claims of section 5 concerned 
the station- ary distribution Pf of the process f(t) that merely counts busy users. Since T(t) is a function 
of J(t), its stationary distribution is also insensitive, and so may be calculated as if users acted 
exponenti$y. The formula (24) for the sta- tionary distribution of J(t) follows from the formula (33) 
simply by addition. As for local balance, note that with PJ given by  l-IL ( y; ) * Pj. = Pa (43)S(C,d 
where S(n) = nz, r(l), it follows that pfm,, Wk -jk + 1)7k (44)jkr(C, A) = PG SE, 5 -1) * (Nk -jk + 
l)Yk jJJ(Cl jd = p (;)*nm,k( ;;)yim 6 SE, 3) This equation implies the desired local-balance condition 
(26). Thus, all the claims of section 5 have been established. References <RefA>[l] J. W. Cohen, The Generalized 
Engset Formula , Phillips Telecommunications Review 18, pp. 158-170, 1957. 121 Robert B. Cooper, Introduction 
to Queueing Theory, 2nd ed. North Holland, New York, 1981. [3] Daniel P. Heyman and Matthew J. Sobel, 
Stochastic Models in Operations Research, vol. I, McGraw-Hill, New York, 1982. (41 V. Jacobson, Congestion 
Avoidance and Control, Proc. ACM SIGCOMM 88, pp, 314329. [5] V. Jacobson, Berkeley TCP Evolution from 
4.3-Tahoe to 4.3-Rena, Proc. of the 18th Internet Engineering Task Force, Vancouver, August 1990. [6] 
K. R. Krishnan, The Hurst Parameter of Non-Markovian On-Off Traffic Sources , Internal Bellcore Report, 
1995. [7] T. V. Lakshman and U. Madhow, Performance Anal-ysis of Window-Based Flow Control using TCP/IP: 
the Effect of High Bandwidth-Delay Products and Random Loss IFIP tinsactions C-26, High Performance Net-working 
V, pp. 135-150, North-Holland, 1994. [8] T. V. Lakshman, A. Neidhardt and T. J. Ott, The Drop from Front 
Strategy in TCP over ATM and its Interworking with other Control Features , Proceeding of IEEE Injowm 
1996, pp. 1242-1250, April 1996. [9] A. Romanow and S. Floyd, Dynamics of TCP Traffic over ATM Networks 
, Proceedings of ACM SIGCOMM 94 pp.79-88, August 1994. [lo] S. Shenker, L. Zhang, and D. D. Clark, Some 
Obser- vations on the Dynamics of a Congestion Control Algo- rithm, Computer Communication Review, pp. 
30-39, October 1990. [ll] W. Willinger, M.S. Taqqu, W. E. Leland and D. V. Wilson, On the Self-Similar 
Nature of Ethernet Traf-fic , IEEE l+ansactions on Networking , vol. 2, No. 1, pp. 1-15, 1994. [12] G. 
R. Wright and W. R. Stevens, TCP/IP Illus-trated, Volume 2, The Implementation , Addison Wes-ley, 1995. 
[13) L. Zhang, S. Shenker, and D. D. Clark, Observations on the dynamics of a congestion control algorithm: 
the effects of two-way traflic, Proc. ACM SIGCOMM 91, pp. 133-147. </RefA> 
			
