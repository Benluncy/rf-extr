
 ALGEBRA AND MODELS C.A.R. Hoare Oxford University Computing Laboratory Wolfson Building Parks Road, 
Oxford. 0X13QD SUMMARY 1. INTRODUCTION Science makes progress by constructing mathematical mod­els, 
deducing their observable consequences, and testing them by experiment. Successful theoretical models 
are later taken as the basis for engineering methods and codes of prac­ticefor design ofreliable and 
useful products. Models can play a similar central role in the progress and practical appli­cation of 
Computing Science. A model of a computational paradigm starts with choice of a carrier set of potential 
director indirect observations that can be made of a computational process. A particular pro­cess is 
modelled as the subset of observations to which it can give rise. Process composition is modelled by 
relating ob­servations of a composite process to those of its components. Indirect observations play 
an essential role in such composi­tions. Algebraic properties of the composition operators are derived 
with the aid of the simple theory of sets and relations. Feasibility is checked by a mapping from a more 
operational model. A model constructed as a family of sets is easily adapted as a calculus of design 
for total correctness. A specification is given by an arbitrary set containing all observations permit­ted 
in the required product. It should be expressed as clearly as possible with the aid of the full power 
of mathematics and logic. A product meets a specification if its potential observa­tions form a subset 
of its permitted observations. This princi­ple requires that all envisaged failure modes of a product 
are modelled as indirect observations, so that their avoidance can be proved. Specifications of components 
can be composed mathematically by the same operators as the components themselves. This permits top-down 
proof of correctness of designs even before their implementation begins. Algebraic properties and reasoning 
are helpful throughout development. Non-determinism is seen as no problem, but rather as a part of the 
solution. Permission to copy without fee all or part of this material is granted provided that the copies 
are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of 
the publication and its date appear, and notice is given that copying is by permission of the Association 
for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. 
SIGSOFT 931121931CA, USA g 1993 ACM 0-89791 -625 -5/93 /0012 . ..$1 .50 A scientific theory is formalised 
as a mathematical model of realit y, from which can be deduced or calculated the ob­servable properties 
and behaviour of a well-defined class of processes in the physical world. It is the task of theoretical 
scientists to develop a wide range of plausible but competing theories; experimental scientists will 
then refute or confirm the theories by observation and experiment. The engineer then applies a confirmed 
theory in the reverse direction: the starting point is a specification of the observable properties and 
behaviour of some system that does not yet exist in the physical world; and the goal is to design and 
implement a product which can be predicted by the theory to exhibit the specified properties. Mathematical 
methods of calculation and proof are used throughout the design task. This paper suggests a similar fruitful 
division of labour between theoretical and experimental Computing Science, leading to eventual application 
in the engineering of computer software and hardware. Theoretical computing scientists de­velop a wide 
range of plausible theories covering a variety of computational paradigms. The mathematical consequences 
of each theory are explored, and also relationships with other competing or complementary theories. The 
experimental computing scientist can then select some combination of re­lated theories as the basis for 
the design of a software system or language, or an architecture for a computing device. The efficiency 
and effectiveness of this design is then tested by simulation or experimental implementation and application 
in representative case studies. Reliable use of the system or device or language will be further assisted 
by mathematical theorems, methods and heuristics derived from the original theoretical model. Computing 
Science is primarily concerned with discrete phenomena; and therefore cannot take advantage of the large 
body of established knowledge of continuous mathematics, developed by applied mathematicians for the 
enormous ben­efit of physical science and engineering. It is rather pure mathematics that supplies the 
concepts, notations, methods, theorems and proofs that are most relevant for computing. But in contrast 
to pure mathematics, potential relevance to computing is taken as a goal and a guide in the selection 
of di­rections for our research. Its achievement depends on a good general (but informal) understanding 
of practical Computing Science. This should cover 1 a range of problems which maybe solved by application 
of some computing device, and the terminology in which they are described. 2 the methods by which solutions 
to complex problems can be found by decomposition into simpler subprob­lems, so that these can then be 
solved by similar or even simpler methods. 3 the methods by which complex systems can be con­structed 
by connecting subassemblies and components implemented in a similar or lower level technology. 4. the 
comparative cost and efficiency of alternative meth­ods of design and implementation in hardware or in 
software. Understanding of this wide range of topics should relate not just to the current computing 
scene (for which any new theory will come too late) but to some possible future evo­lution of it. As 
in all branches of physical science, success depends on a large element of intuition, insight, guess-work 
and just plain luck. That is why the community of theoret­ical scientists must be prepared to develop 
a large number of alternative theories, most of which will never achieve ex­perimental confirmation or 
find practical application. This apparent profligacy is justified by simple economics. It is less expensive 
and risky to invent and develop ten new theories than it is to make even a prototype implementation of 
just one of them. And it is less risky and less onerous to design and implement ten prototypes than it 
is to invest in development of a new market for a genuinely innovative product. And not all new products 
that come to market will remain there. So theories are as numerous as the seeds scattered by the winds; 
only very few will settle and germinate and take root and reach maturity and propagate more seed to populate 
the forests of the future. So let us postulate the wisdom or courage to select some general line of enquiry 
for a new theory. In its detailed development, the researcher would be well advised to lay aside all 
hope of future relevance, and adopt the attitude of pure mathematicians, engaged in the pursuit of truth 
wherever their curiosity may lead them. Avoid competitive promotion of one line of enquiry against another. 
Otherwise you lose the spirit of dispassionate scientific objectivity, so necessary for the health of 
science. Whenever choice arises between directions of pursuit, choose first the path of greater simplicity 
and of greater elegance. And that should be your second and third choice too, and especially your last 
one. An elegant theory will attract attention of other theorists, and a simple one will attract the interest 
of teachers and students. This is the only way to reduce the risk that the theory will be forgotten before 
the time is ripe for its development and practical application. And finally, when the theory achieves 
widespread use or standardisation, the quality of elegance is the only hope we have of rescue from the 
quagmire of arbitrary complexity which is so pervasive, particulady in software of the present day. And 
elegance is a property that is needed not only for a mathematical model but also for the theorems and 
algebraic laws derivable from it. It is the purpose of this paper to encourage the develop­ment of new 
and simple theories with the aid of set-theoretic models. Such models permit easy derivation of algebraic 
laws, which in turn assist in derivation of efficient solutions to practical problems. Models also readily 
support a general method for deriving designs from specifications by top-down decomposition; in crossing 
levels of abstraction, we exploit a helpful correspondence between generality of specification at higher 
levels, and non-determinism at lower levels of de­sign and implementation. 2. OBSERVATIONS The first 
task of the theoretician is to decide on what kind of system to explore, and to characterise which of 
its properties are to be regarded as observable or controllable or otherwise relevant to the description 
and understanding of system be­haviour. For each property, an appropriate name is chosen: for example 
in a mechanical assembly the name z may de­note the distance of a joint along one axis, and x may denote 
its velocity. In mechanics, the observed values vary contin­uously with time, and they are often called 
measurements. In computing, observations usually yield discrete values, and they are made only at discrete 
points of time. For example, in the case of a fragment of program, z may denote the initial value of 
an integer variable before execution starts, and d might denote its final value on termination. The fact 
that these observations are not continuous measurements in no way detracts from the mathematical and 
scientific quality of the theories which describe them. Once the relevant observations have been named, 
the be­haviour and properties of a general system or a particular one can be described or specified by 
mathematical formu­lae, equations, inequations or other predicates which contain these names as free 
variables. Each predicate describes those systems in which the observed values of all its variables make 
the predicate true. In science, a general class of system is often described by differential equations; 
and a specific mem­ber of the class by adding particular boundary conditions. For example, the predicate 
z < Ikil describes the behaviour of any joint which moves sufficiently slowly in the vicinity of the 
origin of the z axis. Similarly in the case of programs, Xt>x describes the behaviour of any piece of 
code which does not decrease the value of z. Such predicates may serve either as scientific predictions 
about the behaviour of known systems, or as engineering specifications of systems yet to be designed 
and implemented. Sometimes the validity of a prediction R depends on va­lidity of some other condition 
P. This condition usually mentions variables whose values can be controlled by the experimenter, or the 
user, or in general by the environment within which the described system is embedded; and so it is often 
called a precondition. If the environment fails to make P true, no prediction at all can be made of the 
behaviour of the system; and, in the case of a specification, no constraint whatsoever is placed on the 
design of the product. Observations which are described in user specifications are usually those which 
can be made directly, as it were with the naked eye. But in a mature branch of science the most important 
observations are those which can be made only indirectly by some more or less elaborate experiment. An 
experiment involves connection of its subject in some well understood manner with other processes whose 
behaviour is well understood, so that a more direct observation can be made of the behaviour of the combined 
system. Very often, the presumed understanding of the experimental apparatus itself depends on the very 
theory that is being tested. There is clearly a danger of circularity, a risk which attends research 
in all branches of science. But a successful choice of the right kind of indirect observation (for example, 
energy in physics) can provide a remarkably coherent and general explanation of wide range of diverse 
phenomena. Such an indirect obser­vation is often accepted as if it were a direct observation in some 
theory at a lower level of detail. A familiar example of the distinction between direct and indirect 
observations can be drawn from models of sequential and parallel programming. For sequential programming 
it is adequate to make direct observations of the values of vari­ables before and after execution; and 
specification of a com­plete program can be formulated entirely in terms of initial and final values. 
But suppose in the interests of efficiency we wish to implement the specification with the aid of component 
programs executing concurrently in a multiprocessor with a single shared store. These processes can interact 
during ex­ecution in ways that cannot be understood in terms of direct observations before and after 
execution. So the presumed ob­servations of each process must include complete sequences (called trajectories) 
of state pairs, representing changes due to atomic actions of that process. These sequences are in practice 
not directly observable, except by running the pro­cess concurrently with some other process designed 
to test it. Indirect observations are usually more complex and ab­struse (and even controversial) than 
direct observations, and they are not intended to appear in the user s specification of the completed 
product; but they are vital to the engineer­ing soundness of the design, because they permit accurate 
specifications of interfaces and components that may then be designed and implemented separately by separate 
teams of engineers at separate times. In a theory intended for engineering design, it is also im­portant 
to include among the potential indirect observations all the possible ways in which a physical implementation 
may break or fail. It is only in a theory which includes such failures that it is possible to prove that 
a particular design or product will avoid them. Since all kinds of failure are to be avoided, there is 
no need to make fine distinctions between them, or to give accurate predictions of the behaviour after 
failure. For the same reason, there is no need to mention avoidance of failure explicitly in a user specification 
of a complete product. Let us describe all such universally undesirable observations by a predicate named 
FAIL. A familiar example of failure in software is non­termination of a sequential program due to infinite 
iteration or recursion. This can be represented by introducing a spe­cial variable terminated, which 
is true when the program has terminated, and remains false if it never terminates. It is understood that 
the final values of the variables z , y , . . . are observable only when terminated is true; this understanding 
can be coded in the mathematical theory by allowing these variables to take arbitrary values when terminated 
is false. A specification never needs to talk about termination: one can take for granted that it is 
desirable. But implementations need to avoid it. So the first step in moving from specification to design 
notation is to introduce this extra variable. Of course, in practice we can never wait the infinite time 
required to make an observation of a false value of the variable terminated. This leads to philosophical 
objections against introducing a value which is so essentially unobservable; but they are the same kind 
of objection that can be made to zero as a number or empty as a set. Projective geometers never expect 
to observe their line at infinity, but their theory would not work without it. And in our case, the explicit 
introduction of non-termination and similar failures gives a similar advantage: it enables us to deal 
automatically with failure to meet liveness conditions in the same simple way as we deal with safety 
properties. To deal with fairness conditions one must accept an even greater variety of indirect observations, 
which would take an infinite time to observe. In summary, a theory intended for engineering design works 
with observations at two (or more) levels of abstrac­tion. The direct observations are those which are 
described 3 in a user specification S, and in a precondition P, placing constraints on the method and 
circumstances of use. The indirect observations are those mentioned in a description D of the actual 
behaviour of a delivered product, and in the description FAIL of all the undesirable ways in which a 
 product may fail if the precondition is violated. The fact that the product meets its specification 
in now encapsulated in a single mathematical theorem D+ (P+ -TFAILAS ). This means that if the precondition 
P is satisfied, then ev­ery observation of the behaviour of the delivered product D will be a non-failing 
observation, and will also satisfy the specification S. The last important message of this section is 
that an en­gineer never just delivers a product, but rather a product together with its specification, 
including operating instruc­tions and preconditions for safe and successful use. Clarity and precision 
of specification are included among the most important qualities of a product; and mathematics provides 
excellent assistance in achieving them. Failure to realise this in software specification is notorious; 
and leads to many of the other problems encountered in current software engineer­ing practice. 3. IMPLEMENTABLE 
PROCESSES. The implication displayed at the end of the last section for­malises a proof obligation which 
may be discharged after the design is complete. But it is far better to regard it as a mathematical statement 
of the designers task, namely to find some design D which satisfies the implication and so meets the 
specification. Eventually, the whole design D must be expressed wholly within some limited set of design 
notations, which are known to be directly implementable in the avail­able technology. The task is in 
principle no different from that of solving any other engineering problem which has been precisely formulated 
in mathematics. An essential quality of the solution of a mathematical prob­lem (for example a differential 
equation) is that it is expressed in more restricted notations than those used to formulate the original 
problem; otherwise the solution could be just a triv­ial restatement of the problem. It is the notational 
restriction that makes the solution useful. In Computing Science, such notations can be designed to be 
translated automatically for direct implementation either in hardware or in the machine code of a computer. 
And, as in mathematics, it is very impor­tant that the notation of the solution should just be a subset 
of the notations used for specification. So the theorist must undertake to select from the class of all 
specifications those which are more or less directly implementable. In the previous section we have assumed 
that specifications are written as mathematical predicates with free variables standing for observable 
values in a fashion generally under­stood by the educated professional. This is a style preferred by 
practicing engineers and scientists, who tend to manipu­late, differentiate or integrate the text of 
formulae rather than abstract functions: it is also the style adopted by the Z school of specification, 
and in the specification-oriented semantics of programming languages. Pure mathematicians, on the other 
hand, tend to prefer closed mathematical abstractions like sets and functions and (more occasionally) 
relations. This is evident in the study of Analysis and even more in Topology. It is the style preferred 
in the denotational semantics of pro­gramming languages. Each style is more appropriate for the use to 
which it is put, and there is no conflict between them. Every predicate can be identified with a set, 
namely the set of those assignments of values to its free variables which makes the predicate true. And 
the sets and functions of the pure mathematician can and should be translated into predicates and formulae 
before use by engineers and programmers. The important relation of set inclusion then translates to logical 
implication, defining precisely the designer s proof obliga­tion. In the remainder of this talk, it is 
more convenient to adopt the style of pure mathematics, dealing with sets and relations rather than variables 
and predicates. Let us give the name OBS to the set containing math­ematical representatives for all 
possible direct and indirect observations of all possible processes of interest. We can now represent 
a particular process P as that subset of OBS which contains all observations which could in any circum­stances 
be made of that process. The set of all such processes, implementable in a particular envisaged language 
or technol­ogy, constitutes a family of subsets of OBS, to which we give the name of PROC. So the first 
two components of our model are similar to those of a topology a carrier set OBS and a particukw family 
PROC of its subsets. The conditions defining membership of PROC are intended to ensure physical implementability. 
Like the laws of physics, they describe general properties such as conservation of en­ ergy that must 
be preserved in any physical system. So it is not surprising that their discovery and formalisation is 
the first and most serious difficulty in the construction of realistic models; what is worse, their 
sufficiency and validity can be established only at the very last step in the evaluation of the model 
by practical use. That is why Dana Scott once char­acterised formalisation as an experimental science. 
So when the experiment succeeds, when all aspects of the theory link together harmoniously, then great 
satisfaction can be derived from the achievement, in addition to the possibility of more practical benefits. 
The sets in PROC are intended to represent exactly the implementable processes of the theory. But a specification 
of such a process does not have to be a member of PROC. Any other subset S, defined by any desired combination 
of mathematical predicates, can serve as a specification of re­ quirements placed on a particular member 
P from PROC, which is yet to be designed. The design will be correct if and only if the eventually delivered 
P is a subset of S, i.e., all possible observations of the process (including even the undesirable ones) 
are permitted by the specification. So the subset relation between a process and a specification cap­ 
tures exactly the concept of satisfaction, as described in the previous section. Of course, it may be 
that there does not exist any P in PROC which satisfies the specification. It is then logically impossible 
to meet the specification within the given technology. The theory may help the engineer in avoiding the 
danger of promising to deliver such a product. Consider a specification T, and let S be subset of T. 
Then S is a stronger specification than T: it places more con­ straints on the product and may therefore 
be more difficult to implement. Indeed, because set inclusion is transitive, every product that meets 
specification S will serve also as an im­ plementation of T, so implementation of T cannot possibly be 
more difficult than S. The subset relation may also be used to define an ordering among the members of 
PROC. By transitivity of inclusion, P ~ Q means that P satisfies every specification satisfied by Q, 
and maybe more. Consequently for all relevant pur­poses and in all relevant respects P is better than 
Q (or at least as good). Thus if Q is a simple design or prototype which clearly meets its specification, 
then Q can be validly transformed to (or replaced by) P, without jeopardizing cor­rectness; the motive 
for doing so may be a reduction in cost or increase in efficiency. One of the main objectives of a math­ematical 
theory is to provide a comprehensive collection of such correctness-preserving, but efficiency-increasing 
trans­formations. Notice that the interpretation of the relation ~ as better-than depends on the fact 
that OBS contains all relevant ways in which a process may fail. It is this that ensures that the better 
process is the one that fails less often; and further­more, because it gives rise to fewer non-failing 
observations, it is easier to predict and control what it is going to do. In this way better also implies 
more deterministic. It is unusual for a general theory to include cost or speed among its observable, 
because these factors are highly vari­able between one project and another. However if they are included 
in a more specific theory, it is important to ensure that the observations take the form it costs less 
than n or it goes faster than m. Then P ~ Q means that Q can cost more and go slower, so the interpretation 
of inclusion as a merit ordering can be maintained. But such a theory can deal only with uniform improvement 
with respect to all criteria simultaneously; it becomes wholly inapplicable in the more frequent case 
when one criterion must be traded against the other. That is another reason why these considerations 
are usually omitted from a general theory, and left to the good judgement of the engineer. No amount 
of mathematical the­orizing can ever replace that ! 4. NON-DETERMINISM The method of modelling a computational 
process as a set of observations is intended to deal in a uniform fashion with both deterministic and 
non-deterministic processes. It is pos­ sible to single out deterministic processes as a special case; 
it is possible to note informally when a combinator may fail to preserve determinism of its operands. 
But once non­ determinism is accepted and taken for granted, there is no necessity to make these distinctions; 
and the mathematical theory develops most smoothly without them. Many practicing engineers are very reluctant 
to accept non­determinism, and rightly so. The only way that they have been taught to assess reliability 
of a product is to test it. But a non-deterministic product may very well pass every test, yet later 
fail in practical use, just when most reliance is placed upon it. The only known solution to this problem 
lies in mathematical design methods that inhibit the intrusion of error. Indeed, this is already coming 
to be accepted by some engineers as more effective than testing, even for determinate products. Hence 
the slogan Design right -First time. Many mathematical computing scientists are also reluctant to accept 
non-determinism, and rightly so. They have been educated in a tradition that mathematics is about functions, 
and that its concepts are expressed primarily in functional notation. The use of functional notation 
for non-deterministic operations leads to immediate confusion: for example, one has to question the validity 
of the absolutely fundamental equation of mathematics, namely fz= j-x Similar difficulties arise for 
partial functions; and the solution is the same: go back to the foundation offered by set theory, and 
use relational notations wherever they are appropriate. This is a solution which is already being applied 
by abstract functional programmers, of the schools of FP and squiggol; they have found it more effective 
to calculate with function composition rather than function application. There is reason to suppose that 
non-determinism will come to play an increasing role in Computing Science, both prac­tical and theoretical. 
In practice, continuing miniaturisation of circuits will continue to favour highly parallel hardware, 
and provide increasing incentives to use it efficiently. But if parallel processes are to cooperate on 
the solution of a single problem, possibly sharing mechanical resources such as disc storage, they will 
certainly need on occasion to synchronise with each other. Each synchronisation will in principle delay 
at least one of the processes involved. Examples of the most significant delays are those arising from 
paging faults in a virtual memory, or from scheduling of arithmetic units in a 5 data flow architecture. 
Increase of processing speed can only increase the significance of these delays. The only solution is 
to allow the existence and duration of the delay to influence the course of the computation. For example 
the programmer can use the [ combinator of CSP to allow the earliest possi­ble selection of the first 
possible event that can occur. Since delays are essentially unpredictable, we are almost forced to accept 
non-determinism as an inherent property of programs and algorithms, and even of computer hardware. Learning 
how to cope with non-determinism is one of the most signif­icant challenges and achievements of theoretical 
computing science, and still offers exciting challenges for the future. 5. PROCESS ALGEBRA. The practical 
use of a model to assist in engineering design requires a significant use of mathematical reasoning of 
one kind or another. In principle, this reasoning can be based on the raw definitions of the operators 
involved; but the labour involved would be totally unacceptable like solving partial differential equations 
by expanding the primitive definitions in terms of the epsilons and deltas of analysis. The only way 
that a branch of mathematics can be applied by engineers in practice is when it offers a range of useful 
theorems, sym­bolic manipulations and calculations, together with heuristics for their application. Jt 
is reasonable to expect a modeller to formulate and prove an initial collection of such theorems, because 
their proof may require changes in some aspect of the model, its operators, its processes, or even its 
observations. The easiest kind of theorem to use is one that is expressed as a general equation, which 
is true of all values of the vari­ables it contains. An equation which is needed in a particular application 
can often be deduced by a process of calcula­tion: starting at either side of the desired equation, a 
series of substitutions is made until the other side is reached; each substitution is justified by a 
known equation of theory. Each step is relatively easy to check, even with the assistance of a computer; 
and long sequences of steps may be carried out almost automatically by a term rewriting system. Such 
trans­formations are most frequently required to increase the effi­ciency of implementation by breaking 
down the more elab­orate structure resulting from the top-down development. A very similar advantage 
can be taken of theorems (inequalities) using inclusion in place of equality, since all the operators 
in­volved in our theories are monotonic, and an engineer needs to exercise freedom to take design decisions 
which reduce non-determinism. Of course, there is no limit to the number of theorems that may be derived 
from a model, and the mathematician needs good judgement in selecting the ones which are likely to be 
useful. Equally important, the chosen theorems should be reasonably memorable; for this, brevity and 
elegance are an important aid, as well as self-evidence to the operational understanding of engineers. 
Again, it is helpful if the the­orems express familiar algebraic properties of combinators, for example, 
associativity, commutivity, idempotence, or dis­tribution of one operator through another. In fact the 
best possible way of educating an engineer in a new computa­tional paradigm is by an elegant collection 
of algebraic laws, together with examples and exercises combining theory with practice. This is the way 
in which pupils at school are taught to reason about the various kinds of number integers, fraction$, 
reals, complex numbers. The study of the sophisti­cated and widely differing models for these number 
systems is more the province of theoretical pure mathematics, and is a topic of specialist study in Universities. 
An important goal in the derivation of algebraic properties of a model is to find enough laws to decide 
whether one finite process (defined without recursion) is equal to another, or below it in the relevant 
ordering. In some cases there is a decision procedure which applies the laws in a particular di­rection 
to eliminate the more complex operators, and produce a simple normal form. This procedure is applied 
to both sides of an equation or inequation, and a simple comparison is then made of the two normal forms. 
The symbolic calculations are easily mechanised by a term rewriting system, though in many cases the 
normal form (or some intermediate expres­sion) is so much larger than the original formula that it may 
exhaust the available resources of a machine, or at least the patience of its user. Except in the case 
of rather small finite universes, there is rarely any hope of an effective decision procedure for processes 
defined by recursion: in general, an inductive proof is necessary to reason about them. The practical 
benefit of deriving laws strong enough for a decision procedure is that thenceforth it is known that 
all necessary equations (or inequations) can be proved from the laws alone, without expanding any of 
the definitions or even thinking about any of the observations. This is so valuable that it does not 
matter if the normal form contains notations which are not in fact implemented, or perhaps not even im­plementable. 
But even then the task of the mathematical modeller is far from over. In each particuhw application area 
for a computational paradigm, there are likely to be more specialised theorems, which can help in reliable 
use of the paradigm; and sometimes the theorems will be of more gen­eral utility, and so deserve a place 
in the central core of the theory. The constant illumination of practice by theory, and the constant 
enrichment of theory by practice over many years and centuries has led to the current maturity of modern 
math­ematics and its applications in science and engineering; and it shows the direction of future advance 
for the comparatively immature discipline of theoretical Computing Science. Algebraic laws have proved 
their value particularly in the design and implementation of general-purpose programming languages. 
They are most valuable in transforming a program from a structure which clearly mirrors that of its specification 
to one which most efficiently matches the architecture of the machine which will execute it. Such transformations 
may be carried out automatically, by an optimizing compiler. Sometimes the motive is to transform a program 
into some smaller subset of a language, so that it may be implemented in some more restricted technology, 
for example by silicon compilation. Finally, algebraic transformations seem quite effective in verifying 
aspects of the design of the compiler itself. The value of algebraic laws and equations is so great that 
there is a great temptation to avoid the laborious task of modelling, and simply to postulate them without 
proof. As Bertrand Russell has remarked: The method of postulation has many advantages: they are the 
same as the advantages of theft over honest toil. In the case of a computational paradigm, the honest 
toil of linking algebra with a more abstract observational theory of specifications is essential for 
an effective calculus of design. But of course, in spite of Russell s remark, the study of abstract algebra, 
independent of all its models and applica­tions, has a most important role. A complete and attractive 
algebra can stimulate the search for applications and models to match it. A cramped and awkward algebra 
can give warn­ings about problems that are best avoided. When two models obey exactly the same complete 
set of algebraic laws, there is no need to choose between them; each can be used for the purpose it suits 
best. But the most important role of algebra is to organise our understanding of a range of different 
mod­els, capturing clearly those properties which they share, and those which distinguish between them. 
The various number systems share many familiar algebraic properties a useful fact that is totally concealed 
by the radical differences in the structure of their standard models. The variety of program­ming languages 
is subject to a similar algebraic classification. 6. OUTLOOK I have described the ways in which both 
models and algebras can contribute to the solution of practical design problems in computing; and I have 
illustrated my points by examples which may have given the impression it is easy. This is not so. The 
construction of a single mathematical model obeying an elegant set of algebraic laws is a significant 
intellectual achievement; so is the formulation of a set of algebraic laws characterizing an interesting 
and useful set of models. But neither of these individual achievements is enough. We need to build up 
a large collection of models and al­gebras, covering a wide range of computational paradigms, appropriate 
for implementation either in hardware or in soft­ware, either of the present day or of some possible 
future. But even this is not enough. What is needed is a deep understand­ing of the relationships between 
all the different models and theories, and a sound judgement of the most appropriate area of application 
of each of them. Of particular importance are the methods by which one abstract theory may be embedded 
by translation or interpretation in another theory at a lower level of abstraction. In traditional mathematics, 
the relations between the various branches of the subject are well under­stood, and the division of the 
subject into its branches is based on the depth of this understanding. When the mathematics of computation 
is equally well understood, it is very unlikely that its branches will have the same labels that they 
have to­day. The investigations by various schools, now labelled as CSP, CCS, ACP, Petri Nets, etc., 
will have contributed to the understanding which leads to their own demise. The establishment of a proper 
structure of branches and sub-branches is essential to the progress of science. Firstly, it is essential 
to the efficient education of a new generation of scientists, who will push forward the frontiers in 
new directions with new methods unimagined by those who taught them. Secondly, it enables individual 
scientists to select a narrow specialisation for intensive study in a manner which assists the work of 
other scientists in related branches, rather than just competing with them. It is only the small but 
complementary contributions made by many thousands of scientists that has led to the achievements of 
the established branches of modern science. But until the framework of complementarily is well understood, 
it is impossible to avoid gaps and duplication, and achieve rational collaboration in place of unscientific 
competition and strife. The advantages to practical engineering are equally im­portant. In most branches 
of engineering, product design involves mixture of a number of differing materials and tech­nologies. 
Each separate technology must be well under­stood; but most of the difficulties and misunderstandings 
and unpleasant surprises occur at the interfaces between the technologies. And the same is true in computing, 
when attempting to put together a system from programs written, perhaps for a good reason, in different 
languages, with equip­ment of differing architectures, and perhaps increasingly in the future, with highly 
parallel application-specific integrated circuits. An appropriate theory can help in each individual 
aspect of the design; but only an understanding of the re­lationships between the theories, as branches 
of some more abstract theory, can help to solve the really pressing problems of overall system integration. 
SOME RELATED PUBLICATIONS The Varieties of Programming Language, <RefA>C.A.R. Hoare, Tap­soft 89, Lecture Notes 
in Computer Science, 351, Springer-Verlag pp 1-18. 7 A Theory of Conduction and Concurrency, C.A.R. 
Hoare, in Parallel Architectures ciety Press 1991, pp (cd. 18-27. Rishe et al) IEEE Computer So- Programs 
are Predicates, FGCS 92,ICOT,pp211-218. C.A.R. Hoare, Proceedings of Mathematics of Programming, C.A.R. 
Hoare, in Program Verification, (cd. CoIburn et al), Kluwer 1993, pp 135-154. Laws of Programming, C.A.R. 
Hoare et al, Communications of the ACM, 30, no 8, pp 672-687.</RefA>  
			
