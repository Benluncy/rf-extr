
 A Linear Time Algorithm for Placing #-Nodes Vugranam C. Sreedhar Guang R. Gao School of Computer Science 
McGill University Montreal H3A 2A7 Canada {sreedhar,gao}@acaps.cs.mcgill.ca Dataflow analysis framework 
based on Static Single As­signment (SSA) form and Sparse Evaluation Graphs (SEGS) demand fast computation 
of program points where data flow information must be merged, the so-called #-nodes. In this paper, we 
present a surprisingly simple algorithm for com­puting ~-nodes for arbitrary flowgraphs (reducible or 
irre­ducible) that runs in linear time. We employ a novel program representation the DJ graph by augmenting 
the dom­inator tree of a flowgraph with edges which may lead to a potential merge of dataflow information. 
In searching for ~-nodes we never visit an edge in the DJ-graph more than once by guiding the search 
of nodes by their levels in the dominator tree. The algorithm has been implemented and the results are 
compared with the well known algorithm due to Cytron et al. [CFR+ 91]. A consistent and significant speedup 
has been observed over a range of 46 Fortran procedures taken from a number of benchmark programs. We 
also ran experiments on increasingly taller ladder graphs and confirmed the linear time complexity of 
our algorithm.  Introduction Static Single Assignment (SSA) form [CFR+ 91], Sparse Eval­uation Graphs 
(SEGS) [CCF91 ], and other related intermedi­ate representations have been successfully used for efficient 
data flow analysis and program transformations [RWZ88, AWZ88, WZ85, W0192, WCES94]. The algorithms for 
com­puting these intermediate representations have one common step-computing program points where data 
flow informa­tion must be merged , the so called q$-nodes. Given a flow­graph, the original algorithm 
for computing ~-nodes for an SEG consists of the following steps [CFR+ 91, CCF91]: 1. Precompute the 
dominance frontier DF(z) for each node z. A node v is in DF(z) if x dominates a pre­decessor of y without 
strictly dominating y. Permission to copy without fee all or part of this matertal is granted provided 
that the copies are not made or distributed for direct commercial advanta~e, the ACM copyright notice 
and the title of the publication artd Its date appear, and notice is given that copyi is by permission 
of the Association of Computing Machinery.Yo copy otherwise, or to republish, requires a fee ancf/orspecific 
permission. POPL 951/95 San Francisco CA USA @ 1995 ACM 0-89791-692-1/95/0001....$3.50 2. Determine the 
initial set of sparse nodes N. that rep­resent non-identity transference in a data flow frame­work. For 
SSA, such nodes contain definitions of vari­ables [CFR+ 91]. 3. Compute the iterated dominance frontier 
IDF(N~) for the initial set N.. Cytron et al. have shown that the desired set of ~-nodes for an SEG is 
same as the iterated dominance frontier lDF(N.) of the initial set [CFR+ 91].  The time complexity of 
the original algorithm depends on the size of the dominance frontier. Although the size of the dominance 
frontier is linear for many programs (as was noted by Cytron et al.), there are cases in which the size 
of the dominance frontier is quadratic in terms of the number of nodes in a flowgraph. This is true even 
for some cases of structured programs, for example, nested repeat unt i 1 loops [CFR+ 91]. Note that, 
even though the size of the dom­inance frontier may be quadratic in terms of the number of nodes in the 
flowgraph, the number of #-nodes needed re­mains linear (for a particular SEG) [CFR+ 91]. As Cytron and 
Ferrante pointed out Since one reason for introducing ~-nodes is to eliminate potentially quadratic behavior 
when solving actual data flow problems, such worst case behav­ior during SEG or SSA construction could 
be problematic. Clearly, avoiding such behavior necessitates placing ~-nodes without computing or using 
dominance frontiers [CF93]. To overcome the potential quadratic behavior of computing @­nodes using dominance 
frontiers, Cytron and Ferrante pro­posed a 0(/3x a(~)) algorithm that does not use dominance frontiers.* 
To the best of our knowledge, the problem of find­ing an algorithm for computing @nodes for an arbitrarySEG 
in linear time remains open.z In this paper, we present a linear time algorithm for com­ puting the desired 
set of @-nodes for N. without precom­puting the domtnance frontiers for all the nodes. One key feature 
of our linear time algorithm is to order the nodes in the dominator tree in such a way that when the 
computation of dominance frontier DF (y) is performed, the dominance frontier DF(z) of any its descendant 
node x, if it is essential for computing the desired set of #-nodes for No, has already 1CI() is the 
slowly-growing inverse-Ackerrnann function. 2MOE recently, Johnson and Pingali ~3] have given a linear 
time algorithm for constructing an SSA like graph, called the Depen­dence Flow Graph. We compare our 
work with theirs in Section 7. been computed and is so marked. As a result for any such z, the computation 
of DF(v) does not require the traversal of the dominator sub-tree rooted at z. To perform the proper 
node ordering and marking, our algorithm uses a novel program representation, called the DJ-graph (Section 
3). The skeleton of the DJ-graph of a program is the dominator tree of its flowgraph (whose edges are 
called D-edges in this paper see Section 3). The tree skeleton is augmented with join edges (called J-edges 
in this paper see Section 3) from the original flowgraph which potentially lead to join nodes where data 
flow information are merged. The levels of the nodes in the dominator tree are used to order the computation 
of dominance frontiers of those nodes which are essential to compute the final set of @-nodes in a bottom-up 
fashion. We show that our algorithm visits each edge in the DJ-graph at most once, and therefore the 
complexity is linear in the size of the input flowgraph? The algorithm has been implemented on the top 
of Parafrase2 compiler [Har85b]. To compare our results, we also implemented the original algorithm based 
on iterating through dominance frontiers [CFR+ 91]. We experimented on a number of FORTRAN procedures 
taken from Perfect, Eispack, and other programs. With our algorithm we were able to obtain, on average, 
more than five-fold speedup over the original algorithm. We also tested our algorithm against the standard 
ladder graph example [CF93]. Again our algo­rithm exhibited a linear behavior compared to the quadratic 
behavior of the original algorithm: The significance of the algorithm presented in this pa­ per goes 
beyond to merely computing @nodes for SEGS or SSA form. Our framework can be used for the computa­tion 
of guards [Wei92]. More recently we have used iter­ated dominance frontiers to incrementally update domina­tor 
trees [SGL94b]. Finally, our framework is robust enough to support incremental computation of ~-nodes 
[SGL94b]. Organization. In the next section, we introduce some stan­dard notation and definitions that 
we will use in the rest of the paper. In Section 3, we introduce the DJ-graph. We also discuss some of 
the properties of this graph that are relevant to our discussion. In Section 4, we give a simple linear 
time algorithm for computing #-nodes. We show the correctness and the complexity of our algorithm in 
Section 5. In Sec­tion 6, we present an implementation of our algorithm and report results for a number 
of programs. We also report re­sults for the ladder graph example. In Section 7, we compare our work 
with related work and finally, in Section 8, we give our conclusion. 2 Background and Notation A flowgraph 
is a connected directed graph G = (N, E, START, END), where N is the set of nodes, E is the set of edges, 
START c N is a distinguished start node, and END g N is a distinguished end node. Figure l(a) shows an 
sAs we will show later in the paper, the number of edges in the DJ-graph is no more than IN I + IJ91, 
where IN I is the number of nodes in the flowgraph and IE I is the number of edges. 4Due to the complex 
nature and partial description of @mn and Ferrante s almost linear time algorithm we did not implement 
that algorithm. example of a flowgraph. If x + y E E, then z is called the source node and y is called 
the destination node of the edge. We will assume that every node in N is on some path from START to END. 
If S is a set, we will use the notation ]Sl to represent the number of elements in the set. 1 Legend 
13 6 + flowgfe~edge -.-+ kminetwedge (a) (b) 14 I Iw A Figure 1: Flowgraph and its dominator tree In 
a flowgraph, a node x dominates another node y iff all paths from START toy pass through z. We write 
m dom y to indicate that x dominates y, and write x !dom y if x does not dominate y. If z dom g and z 
# y, then z strictly dominates y. We write z sdom y to indicate z strictly dominates y, and write z !sdom 
g if x does not strictly dominate y. The dominance relation is reflexive and transitive, and can be represented 
by a tree, called the dominator tree. If x is a parent node of v in the dominator tree, then z immediately 
dominates y, and we write adorn(y) to denote the immediate dominator of y. Given a node x in the dominator 
tree, we define SubTree(z) to be the dominator sub-tree rooted at z. Note that the nodes in SuKFree(z 
) is simply the set of all nodes dominated by x. Figure 1(b) shows the dominator tree for the flowgraph 
shown in Figure 1(a).  For each node in the dominator tree we associate a level number that is the depth 
of the node from the root of the tree. We write z .levei to indicate the level number of a node z. For 
example, for the dominator tree shown in Figure l(b), START.level = O, 8.level = 2, 9.level = 3, etc. 
The dominance frontier DF(z) of a node x is the set of all Y such that z dominates a predecessor of y, 
but z does not strictly dominate y [CFR+ 91]. We can extend the definition of dominance frontier DF(S) 
to a set of nodes S: DF(S) = u DF(Z) (1) Zes We define the iterated dominance frontier lDF(S) for a set 
of nodes S as the limit of the increasing sequence: IDFI(S) = DF(S), (2) IDF,+I(S) = DF(S u IDFt(s)) 
(3) Let N= ~ N be the initial set of sparse nodes [CF93]. Cytron et al. have shown that the desired set 
of ~-nodes for N. is exactly same as lDF(N~) [CFR+ 91]. Therefore in the rest this paper we present a 
linear time algorithm for computing the iterated dominance frontiers for N. g N.  3 DJ-Graphs and Their 
Properties In this section we briefly introduce DJ-graphs and state some of the properties of DJ-graphs 
relevant to our discussion. We also give a simple algorithm for computing the dominance frontiers for 
a node using DJ-graphs. We will then show how to compute the dominance frontiers for a set of nodes without 
precomputing the dominance frontiers for all nodes. In the next section we will show how to extend this 
simple algorithm to compute the relevant set of @nodes in linear time. A DJ-graph has the same set of 
nodes as in the flowgraph, and two types of edges called D-edges and J-edges. D-edges are dominator tree 
edges, and we define J-edges as follows: Definition 3.1 (J-edge) An edgez -yin ajowgraph is named a join 
edge (or J-edge) if z !sdom y, Furthermore, y is named a join node. Therefore, in order to construct 
the DJ-graph of a flow­ graph, we first construct the dominator tree of the given flowgraph. 5 Then, 
we insert the J-edges into the dominator tree as follows: For each join node yin the dominator tree connect 
z to y (in the dominator tree) iff z -Y is a join edge in the original flowgraph. Figure 2 shows the 
DJ-graph for the flowgraph of Fig­ure 1(a). To see how a J-edge is inserted in the dominator tree, consider 
join node 2 shown in the flowgraph of Fig­ure 1(a). This node consists of 1 --+ 2 and 6 a 2 as its two 
incoming edges. Of these two incoming edges, node 1 strictly dominates join node 2, and so we do not 
insert an edge from 1 to 2 in the corresponding dominator tree; how­ever 6 does not dominate 2, therefore 
we insert an edge from 6 to 2 in the dominator tree. We can easily see that the time complexity for inserting 
all the J-edges in the dominator tree is O ( ~). Therefore the time complexity of constructing a DJ-graph 
is linear with respect to the size of the flowgraph.G In the rest of the section we discuss some of the 
proper­ties of DJ-graphs. Due to space reason we only outline the basic properties and direct interested 
readers to the full pa­per [SG94] for a more thorough discussion on this. 5Another ieW of D-edges and 
J-edges may give a better inti­ition We mark each edge z + y in the flowgraph as an immediate dominance 
edge if x = don(y). The edges that are not marked are join edges. 6Note that we can construct the dominator 
tree of a flowgraph in linear time [Har85a]. Level O Level 1 Level 2 Level 3 Level 4 Level 5 Level 6 
14 ~ join edge % Figure 2: The DJ-graph of the example flowgraph 1.The number of edges in a DJ-graph 
is less than the sum of the number of nodes and the number of edges in the corresponding flowgraph [SG94]. 
We will use this re­sult in the complexity analysis of our ~-node placement algorithm (Section 5.2). 
2. Let g G DF(z] (and thus is also in lDFI(z)). Then the level number of Y is always less than or equal 
to the level number of z [SG94]. This result is one of the key points for obtaining our linear time algorithm 
for placing @­functions. Intuitively what this result says is that if we want to find what nodes in a 
flowgraph could be in the dominance frontier (or the iterated dominance frontier) of node x, we only 
need to look at those nodes whose level number is no greater than that of z. Other nodes (whose level 
number is strictly greater than the level number of x ) can never be in the dominance frontier of x. 
3. A node y is in DF(z) iff there is a node z c SubTree(z) and a J-edge z + y such that the level of 
v is less than or equal to the level of x (Lemma 3.1). Using this property we next give a simple algorithm 
(Algorithm 3.1) for computing dominance frontiers. Computing the Dominance Frontier for a Single Node 
Our algorithm for computing the dominance frontier of a node is based on Lemma 3.1 which establishes 
a relation between a node z c DF(x ), and the nodes in the dominator sub-tree rooted at z. This relation 
is captured by a J-edge y + z, Where y is a node in the SubTree(x). Lemma 3.1 A node z c DF(x) if there 
exists a y c SubTree(x) with y -+ z as a J-edge and z.level < x.level. Using Lemma 3.1 we can easily 
devise a simple algorithm for computing the dominance frontier of a node as follows: Algorithm 3.1 Thefollowing 
algorithm computes the dominance frontier DF(x) of a node x using D}-graphs. DomFrontier(z) { o DFZ =@ 
1: foreach y G SubTree(z) do 2 if((y + z == Jedge) and 3: (z.level < z.ievel)) 4 DFZ = DF= UZ } For example, 
consider the DJ-graph shown in Figure 2. Suppose we want to determine the dominance frontier of node 
3. The SubTree(3) = {3,9, 10,11,12,13, 14}. At step @wefindth e f o 11owing J-edges: {10 + 12,11 + 12,13 
+ 3,13 + 15,14 + 12}. Of these J-edges, we see that only nodes 3 and 15 satisfy the condition at step 
~ Therefore DF(3) = {3, 15}. Computing the Dominance Frontiers for a Set of Nodes We can easily use the 
Algorithm 3.1 to compute the domi­nance frontier for a set of nodes S, by first pre-computing the dominance 
frontiers for all the nodes, and then using Equa­tion 1 to proceed with the computation To illustrate 
this consider the computation of DF( {9, 12}). By Equation 1, we know DF({9, 12}) = DF(9) U DF(12). Let 
us therefore precompute DF(9) and DF (12). Using Algorithm 3.1 we get DF(9) = {3, 15}, and DF(12) == 
{3, 12, 15}. Therefore DF({9, 12}) = {3, 12, 15}. Notice in the above example that we visit the nodes 
in the SubTree(12) twice-once during the computation of DI (9) and once again during the computation 
of DF(12). How can we avoid this redundant visitation of the nodes in the SuWree(12)? We can avoid this 
by first computing DF(12) and marking the node 12 as being processed. Now during the computation of DF(9) 
we avoid visiting any nodes in the SubTree(12) (since node 12 is already processed, and is so marked) 
thereby avoiding redundant visitation. Notice here that we never need to precompute DF(9) and DF(12) 
in order to compute DF({9, 12}). Therefore in order to compute DF({9, 12}), we first compute the DF(12) 
using Algorithm 3.1, and also mark node 12 as being processed. Any candidate nodes that is generated 
on-the-fly is then added to the set DF( {9, 12}). Now during the computation of DF(9) we avoid visiting 
the nodes in the SubTree(12). Again we add any candidate nodes that is generates on-the­fly to DF({9, 
12}). Based on this observation we can see that the ordering of the nodes in the dominator tree is im­portant 
to avoid redundant visitation of nodes during the computation of dominance frontiers. In the next section, 
we will show how to extend the above key observation to compute the relevant set of ~-nodes in linear 
time without pre-computing the dominance frontier for all the nodes in the flowgraph. Notice that one 
can still use Algorithm 3.1 for computing the relevant set of @-nodes by precomputing the dominance 
frontiers for all the nodes. But the time complexity of the resulting algorithm will be quadratic?  
4 Algorithm for Placing #-Nodes In this section, we give a simple linear time algorithm for computing 
@-nodes using DJ-graphs (Algorithm 4.1). Given a set of initial nodes N., the algorithm computes the 
rel­evant set of @-nodes by computing the set I.DF(N.), the iterated dominance frontier of N.. As we 
indicated in Sec­tion 3, a direct application of Algorithm 3.1 based on the inductive definition of iterated 
dominance frontier can lead to quadratic behavior. Instead, our linear time algorithm is based on two 
key observations: 1. Let y be an ancestor node of a node x on the dominator tree. If DF(z) has already 
been computed before the computation of DF(y), DF(x) need not be recomputed when computing D F (y). However, 
the reverse may not be true; therefore the order of the computation is crucial. In Algorithm 4.1 the 
computation of relevant set of nodes is ordered in such a way that at the time when the computation of 
DF(Y) is performed, DF(z) of any node z within the dominator sub-tree rooted at y has already been computed 
and is so marked, if DF(z) is essential for computing the set of desired #-nodes for N.. As a result, 
the computation of DF(y) need not traverse the dominator sub-tree of x for all such z. 2. When computing 
DF(z) we only need to examine the J-edges y ~ z, where y is a node-in the dominator sub­tree rooted at 
x and z is a node whose level is no greater than the level of z. Recall that we have previously made 
this observation in Lemma 3.1.  We employ a data structure called the PiggyBank to keep the candidate 
nodes in the order of their respective levels. Based on the above observations, levels of the nodes in 
the dominator tree will be used in a bottom-up fashion to or­der the computation of dominance frontiers 
of those nodes, z, which are essential to compute the final set of ~-nodes. Meanwhile, during each computation 
of DF(x), the descen­dant nodes of z in the dominator sub-tree rooted at x are visited in a top-down 
fashion guided by the D-edges, while avoiding nodes which have already been marked. During this top-down 
visit, the J-edges are used to identify the can­didate nodes which should be added into the IDF the set 
of final ~-nodes and those to be recursively explored further. Note that each new candidate generated 
on-the-fly always has a level number no greater than that of the node currently being processed, and 
we assure that no nodes are inserted into the PiggyBank more than once. This, and the structure of the 
F iggyl?ank, are the basis of the time linearity of our algorithm as will be demonstrated later in Section 
5. TAn example of a flowg.aph which exl-dbitquadratic behavior is the ladder graph. We will discuss more 
on this later in Section 6. The Piggy Bank is like a piggy-bank, where nodes are temporarily deposited 
for later withdrawal (See Figure 3). The Piggy Bank is an array of list of nodes, with index i stor­ing 
nodes of level t. Associated with the Pigg yBan k are two procedures: InsertNodeo and GetNodeo. InsertNodeo 
in­serts a node in the PiggyBank at the index corresponding to the level number of the node. GetNodeo 
returns the node whose level number is the maximum of all nodes currently stored in the Piggy Bank. We 
first insert the initial set of nodes N. into the Piggy Bank. Then, we iteratively com­pute the dominance 
frontiers of the nodes in the PiggyBank in the order that GetNodeo returns to obtain the iterated dominance 
frontier of the initial set of nodes N.. (A node is inserted into the PiggyBank if it is either in N~ 
or in the iterated dominance frontier of some node in N..) We for­mally prove the correctness the algorithm 
in Section 5.1, and analyze its complexity in Section 5.2. To simplify the presentation of the algorithm, 
we use the following notation and data structures: N umLeve/ is the total number of levels in the domina­tor 
tree embedded in the DJ-graph.  Each node z c N has the following attributes:  struct Node Structure{ 
visited = {Visited, NotVisited} alpha = {Alpha, NotAlpha} inphi = {InPhi, NotInPhi} level = {O... NumLevel 
 1} } c Each edge x + y c E has an attribute that specifies the type of the edge: {Dedge, Jedge}. c 
The Pigg yBan k is an array of pointers to nodes. Its structure is defined as follows: struct Piggy BankStructure{ 
Node Structure *node Piggy BankStructure *next } Piggy Bank[NumLevel] o CurrentLevel is initially NumLevel 
 1,and subse­quently has a value that corresponds to the level num­ber of the node that GetNodeo returns. 
current R.o ot always points to the node that GetN­ odeo returns. CurrentRoot is equivalent to root of 
the Sv,bTree () whose dominance frontier is currently being computed. The first step in the algorithm 
is to insert all the nodes in N. into the Piggy Bank structure. This is shown below in the Main procedure 
as steps @to @. We mark the nodes that are initially inserted into the PiggyB a n k as Alpha to indicate 
that they belong to the initial set N.. This is needed to avoid re-inserting these nodes into the Piggy 
Ban k again in the future (a condition that we check in the procedure Visitor at step @ ). We then iteratively 
invoke the procedure Visito on the nodes that GetNode( ) returns to compute the iter­ ated dominance 
frontier. At step ~ we assign the variable CurrentRoot to point to the node x that GetNodeo returns in 
order to keep track of the current root of SubTree(z ). Be­ fore Visit(z) is invoked at step ~ the node 
z is marked Visit ed at step @ This marking is crucial because we never visit a node that has been marked 
Visited. We check for this condition in the procedure Visito at step @ . Algorithm 4.1 The following 
algorithm computes N4 = IDF(Na). ~ Inpuk A DJ graph DJ = (N, E), and the initial set N. g N of sparse 
nodes. 4 Output The set IDF = N4 = DF+(Na). ~ Initialization c IDF = {} s Vx E N (x. visited = NotVisited 
; x.inphi = NotInPhi ; x.alpha = Not Alpha ; / . Compute the level numbers. / x.level = Level(z)) ~urrentLevel 
= Num Level ~ ~ The Algorithm Maino { / Insert N. into the PiggyBank * / 1: foreach x G No do 2 x.alpha 
= Alpha 3: InsertNode(z) 4 endfor / * repeat until no more nodes in the PiggyBank * / 5: while((z = 
GetNodeo) ! = NULL) 6: CurrentRoot = z 2 x.vzstted = Visited 8: Visit(z) 9: endwhile } / EndMain / The 
procedure Visito, called with the current root CurrentRoot, essentially traverses the dominator sub-tree 
SubTree(CurrentRoot) in a top-down fashion marking all nodes in the sub-tree as Visited if the nodes 
are not al­ready marked Visit ed (a condition checked at step ~. Notice that the nodes in the dominator 
sub-tree are con­nected through D-edges. As it walks down the sub-tree, the procedure Visito also peeks 
at all nodes that are con­nected through J-edges, but does not mark them as Vistted. It only checks the 
level number of these nodes, and when­ever it notices that the level number of a node (that it peeked 
through a J-edge) is less than or equal to the level number of CurrentRoot, it adds the node into the 
set IDF, if the node is not already in the set (a condition checked at step ~. It also marks the node 
as InPhi whenever the node is added to the set IDF. This marking is necessary to avoid adding the node 
again into IDF whenever it may peek at this node through some other J-edge in the future. It also inserts 
the node into the PiggyBank if the node is not in the set N. (a condition checked at step ~. Procedure 
Visit(x) { 10 foreach y ~ Succ(z) 11: if(s + y == Jedge) 12 if(y.ievei < currentRoot.ieve~) / Check 
if y already in N+ / 13 if(y.2nphi ! = InPhi) 14 y.inphi =InPhi /* y in N+ */ /* Compute the set N+ / 
15 IDF = IDF U{y} 16 if(y.alpha ! = Alpha) / Do not reinsert if y s N. / lZ InsertNode(y) 1s endif 19 
endif 20 endif 21: else/* z + y is Dedge */ I* Avoid redundant visit */ 22 if(y.visited ! = Visited) 
23: y.visited = Visited 24 Visit(y) 25 endif 26 endif 2R end for } / EndVisit / InsertNodeo inserts a 
node into the PiggyBank at an index equal to the level number of the node. Procedure InsertNode(~) { 
2&#38; z.ned = ~iggyBank[x.level] 29 Piggy Bank[z.levei] = x } / EndInsertNode / GetNodeo returns a 
node whose level number is the max­imum of all the nodes currently in the Piggy Bank. GetN­odeo also 
removes this node from the Piggy Bank, and ad­justs the (?urrentLevel accordingly. CurrentLevel keeps 
track of the level number of the node that GetNodeo re­turns. Note that anode will never be inserted 
in PiggyBank at a level number greater than CurrentLevel. As a result, CurrentLevel monotonically decreases 
through the level numbers. That is, the calls to Visit(z) at step @is per­formed in a bottom-up fashion, 
in contrast, with each such call, the traversal of the dominator sub-tree rooted at x is performed in 
a top-down fashion. The marking of the nodes prevents any nodes from being processed more than once in 
the algorithm. This is essential to ensure the time linearity of the algorithm. Function GetNodeo { /* 
More nodes in the current level */ 30 if(pzggy~ank[cumentLevei] ! = NULL) 31: x = Piggy Bank[CurrentLevei] 
/ delete x from PiggyBank */ 32: .PiggyBank[CurrentLevel] = x.ned 33 return x 34 endif 35 for i = CurrentLeve~ 
downto 1 do 36 if(PiggyBank[i] ! = NULL) / * Update the current level */ 32 CurrentLevel = i 3s z = Piggy 
Bank[i] / Delete x from PiggyBank / 39 Piggy Bank[i] = z.nezt 40 return x 4k endif 42 endfor / No more 
nodes in PiggyBank */ 43: return NULL } /* EndGetNode */ Example Next we illustrate Algorithm 4.1 through 
an ex­ample, Consider the DJ-graph shown in Figure 2. Let N. = {5, 13}. The first step is to deposit 
the nodes 5 and 13 into the Piggy Bank, and also mark them as Alpha. After the for loop at step ~ the 
PiggyBank would look like Fig­ ure 3(a). At step ~ the function GetNodeo returns node 13. (GetNodeo also 
removes 13 from the PiggyBank.) At step ~ CurrentRoot is set to node 13. To find the dominance frontier 
of node 13 we call Visit(13) at step ~ Prior to this, we also mark node 13 as Visited at step @. In the 
procedure Visito, at step @ we find that the suc­cessor nodes of 13 to be nodes 3, 15, and 14. Of these, 
13+ 15and 13+ 3areJ-edges, and 13+ 14isaD­ edge. Since 15.level = 2 and 3.level = 2 are less than CurrentRoot.~evel 
= 13.level = 5,nodes3and15areadded to IDF (since they are not already in ID F). Also, neither 3 nor 15 
is marked Alpha (and hence not in N.), both the nodes are inserted into the PiggyBank (Step ~. Fig­ure 
3(b) shows the new state of the Piggy Bank. Next, since the edge 13 + 14 is a D-edge, and node 14 is 
not yet visited, we call Visit(14) at step @ . Again, before ca11ingVisit(14), we mark node 14 as Visited 
(step ~. The only successcr of 14 is node 12, and 12./eve/ = 4 is less than CurrentRoot.ievei = 13.level 
= 5. Ak+o, node 12 is neither in IDF nor in Nat and so is added to IDF and inserted into the PiggyBank 
(step @ and @ respectively). The call to Visit(13) terminates and returns at step @. Now the function 
GetNodeo is executed at step @and it returns node 12. Visit(12) is called at step ~ and CurrentRoot 
is set to node 12. The only successor of 12 is node 13, and 12 + 13 is a D-edge. Since node 13 is already 
marked Visited, the call to Visit(12) terminates and returns at step H. GetNodeo is called again, and 
this time it returns node 5. Visit(5) is called at step ~ and the process continues. Figure 3 shows 
a partial trace of the PiggyBank for the possible because of the PiggyBank structure and we for­ example. 
malize this in Lemma 5.1 and Lemma 5.2. We then make an inductive argument on the decreasing level of 
the nodes to demonstrate that all nodes in DF (v ) should already be inserted into IDF by this time. 
The node z should also be 0 1 2 5 3 4 5 13 6 CurrentLevel = 5 N+= {] &#38; (a) L-l H 5 CurrentLevel 
= 4 No= {15,3,12) 6 (c) 0 1 2 3 4 5 c, rrr.nfl . .1 = 5 . .. . n N+= {15,3) 6 (b) 0 in IDF according 
to Lemma 3.1. From Theorem 5.1 is established. In our chain of proofs, we begin with states that a node 
can never be inserted at an index greater than the level number this the validity of Lemma 5.1, which 
in the PiggyBank of the current root node CurrentLevel. We use this fact to prove Lemma 5.2. Lemma 5.1 
A node is never inserted in the PiggyBank at an index that is greater than cuTTentLevei. Lemma 5,2 gives 
an order (based on the level number of nodes) in which calls to Visitor at step ~ can be performed. 1 
The ordering of nodes is controlled by calls to GetNodeo at 2 815 3 23 step @. Recall that GetNodeo 
always returns a node whose CurrentLevel = 2 level number is the maximum of all nodes currently stored 
4 N+= {15,3,12,8,2) 5 in the PiggyBank structure.  a---­ (d) Figure 3: Partial trace of #-node placement 
algorithm 5 Correctness and Complexity In this section we establish the correctness of Algorithm 4.1 
and analyze its complexity. Due to space reasons, we state the main theorems (Theorem 5.1 and Theorem 
5.2) and only give intuitive sketch of their proof structures. We also state all the supporting lemmas 
needed and state their intuitive meaning. All proofs can be found in [SG94]. 5.1 Correctness The main 
theorem which establishes the correctness of Al­gorithm 4.1 is Theorem 5.1. The theorem states that the 
algo­rithm computes the iterated dominance frontiers of the set N.. The inductive proof of the theorem 
is based on a major lemma (Lemma 5.4), which establishes the fact that when the algorithm calls Visit(z) 
at step @and the call terminates, all nodes in the dominance frontiers DF(x) are already added into the 
set IDF (a fact used both in the induction basis and induction steps). Let z be the current root of a 
dominator SubTree(z) visited by Visit(z) at step @. Let z be in DF(z). Lemma 3.1, introduced earlier 
in Section 3, guarantees that there must exist a node g in SubTree(z) such that y ~ z is a J-edge and 
level.z ~ level.z. states that y will already have Visit(z) returns. There are two a node can be marked 
Visited: step @. The validity of Lemma Another lemma, Lemma 5.3, been marked Vzsited when cases in the 
algorithm where (1) at step ~ and (2) at 5.4 for case 1 is straightfor­ ward. For case 2, y must be 
marked Vis ite~ by an earlier call of Visit(v) for a node v in SubTree(z). This fact is made Lemma 5.2 
Let x and y be any two nodes that are inserted in the Piggy Bank and later removed (and returned) from 
the PtggyBank by GetNodeo at step @. lf y.level > z.level, then Visit(y) will be called earlier than 
Visit(x) at step @ The next lemma establishes an important fact that when a node z is visited by a call 
of Visit(z) from step @and returned, that all nodes in the dominator SabT~ee (z) have been marked Visited. 
Intuitively this means that when such a visit returns, none of the nodes in the SubTree (z ) have been 
overlooked. Lemma 5.3 When Visit(z) returns at step ~ all nodes in subTTee(x) are marked Visited. It 
is easy to see from Lemma 5.2 and Lemma 5.3, that calls to Visito at step @are made in a bottom-up fashion 
and while each recursive call at step @ the recursive procedure Visito visits the nodes in the dominator 
tree fashion. Lemma 5.4 is the main lemma which shows cedure Visito captures the dominance frontier the 
set IDF. Intuitively the lemma states that in a top-down how the pro­of anode in when Visit(z) is called 
and terminated at step S all the nodes in the dom­ @ inance frontier of x are added to the set IDF. 
Lemma 5.4 When Visit(x) is called with x as the current Root and returned at step@ all the nodes in DP 
(x ) are also in the set IDF. Notice that the above lemma only says that Visit(z), when it returns at 
step ~ will have added the entire dominance frontier of x to IDF. It does not specify which of the nodes 
in the set IDF belong to DF(x). Notice that the set IDF can contain nodes that are not in the set DF(x 
). Theorem 5.1 Algorithm 4.1 correctly computes the set of ~-nodes N+ = IDF(Na). The proof of the theorem 
easily follows from the above lemmas. 68  5.2 Complexity Next we will show that the time complexity 
of Algorithm 4.1 is O(I El). Recall that the number of edges in the DJ-graph is less than INf I + IEf 
1. Therefore, the time complexity of Algorithm 4.1 is O(lNf I + lEjl). Since lEf I > lNf I 1, the time 
complexity of the algorithm is 0( IEf I), which is linear with respect to the number of edges in the 
flowgraph. From Algorithm 4.1, readers may have already observed that for any node x in the DJ-graph, 
the node maybe pro­cessed by a call of Visit(z) (which may happen at step @ or D24 at most once. This 
observation is a key to the proof of linearity of the algorithm, and is stated as the following lemma. 
Lemma 5.5 When Algorithm 4.1 terminates, a node x c N may beprocessedby a call to Visit(x) at most once. 
From the above lemma, one can see that a node can never be marked Visited more than once, and there can 
be at most INI calls to Visito. Recall that at each node in the procedure Visito, we either visit (through 
a D-edge) or peek (through a J-edge) all the successor nodes (step ~ only once. This means that we have 
effectively probed all the edges in the DJ­graph at most once. Hence one can see that the complexity 
of the algorithm is 0(1131). Theorem 5.2 The time complexity ofAlgorithm 4.1 is 0(1111). An acute reader 
may ask the following question: What about the complexity of inserting and deleting nodes into/from the 
Piggy Bank? It is easy to see that the com­plexity of inserting a node in the PiggyBank is O(l). As for 
the complexity of getting a node from the Piggy Bank, it is again easy to see that a node will never 
be inserted in the PiggyBank at the index greater than the CumentLevei (from Lemma 5.1). Each call of 
GetNodeo will execute the for loop with a monotonically decreasing Cuneni!Level from NumLevel 1 down-to 
1during successive calls for the entire duration of the algorithm (follows from Lemma 5.1). Hence the 
overall complexity of deleting nodes from the PiggyBank is, in the worst case, O(INI). 5.3 Discussion 
Recall that one of the key point that makes our algorithm linear is the PiggyBank structure. If one were 
to use other structures such as a linked-list, a stack or a queue, either the proof of correctness would 
fail (if we still wish to continue to mark the nodes as Visited using one color), or the complexity of 
the algorithm would not be linear (we will need to mark the nodes as Visited using more than one color). 
The second situation is similar to finding the iterated dominance frontier by iteratively applying Algorithm 
3.1. We can easily show that the complexity of this method will be quadratic. To fully understand the 
above discussion, the readers are encouraged to apply the algorithm, with No = {O, 2}, to the ladder 
graph example shown in Figure 4(a) whose DJ-graph is shown in Figure 4(c). Try to use a linked-list structure 
to replace the PiggyBank structure, and assume node O is visited before node 2. o 1 x DF(x) o {) 23 
1 {3] 2 {3,5,7) 3 {5]  45 4 {5,71 5 {7] 6 {7) 7 () 67 J i F@qh CommmeFmitm DJ.gwh (a) (b) (c) Figure 
4: A ladder flowgraph  6 Implementation and Experimen­tal Results In this section we present our experimental 
results and their analysis. We implemented our linear time algorithm on top of the Parafrase compiler 
[Har85b] and executed on 46 FOR-TRAN routines taken from Perfect, Eispack, Lapack, Ode, Opt, and Gator!. 
We particular chose those procedures from these suites that are large and have unstructured con­trol 
flows. We carried out our experiments on a SPARC-10 workstation. To compare the performance of our algorithm 
with the original algorithm, we also implemented the @ node placement algorithm based on iterating through 
the dominance frontiers [CFR+ 91]. This implementation also allowed us to doubly verify the correctness 
of our algorithm by matching the results of ~-nodes of the two algorithms. To be fair, the time measurement 
shown for the original al­gorithm does not include the time for pre-computing the dominance frontiers. 
For convenience, we will denote lDF(d~) for iterated dominance frontier algorithm based on dominance 
frontiers, and .IDF(new) for our new linear time algorithm. Figures 5 and 6 shows the results for some 
typical procedures taken from Perfect and Lapack programs, respectively. The X-axis gives the names of 
the procedure we experimented on. In Figure 7 we give the performance for all the 46 FORTRAN procedures 
we tested. The second column in the table gives the number of flowgraph nodes for the corresponding pro­cedure. 
The time measurements (averaged over 25 runs) shown for 1.DF(d~) and lDF(new) are for computing ~­nodes 
for a single SEG. We randomly chose 15 to so~o of the nodes to be N., the initial set of sparse nodes; 
and we chose the same N. for both .IDF(new) and IDF(df) algorithms. Originally we implemented the lDF(d~) 
algorithm using bit-vectors for encoding the dominance frontiers for each node. With this implementation 
our algorithm exhibited a 8E@ck, LaPack, ode, and Opt are available from netlib.att.com. Gator is a Gas, 
Aerosol, Transport and Radiation model, and is available from ftp.cs.berkeley.edu ~erfect ime (miuis~) 
r­ d.dcm c,wd re.dm d.t.. +rn, e-rchk mod.h nmseq rn.sfe LY -IDF (new) ~ lDF(df) Figure 5: Performance 
of IllF(new) and lDF(d~) al­gorithms on Perfect LaPack rm, WUS+ 8 ..,...., ,, 1 I cbdq .ges d .hg.qz 
cMbs .{.,-dbdw ds% ds=.d dh.wv d%.. d,-.. -lDF (new) ~ !DF(d9 Figure 6: Performance of lDF(new) and lDF(&#38;) 
al­gorithms on Lapack speedup ranging from 4.2 to 19.8 over the lDF(d~) algo­rithm, with average speedup 
being around 9.0. We then translated the bit-vector representation to a linked-list rep­resentation. 
With this representation our algorithm shows a speedup ranging from 2.2 to 8.9 over the lDF (d$) algorithm, 
with average speedup being around 5.5. This suggest that bit-vectors may not be the best representation 
for encoding dominance frontiers. Figure 7 shows the performance of our algorithm over the lDF(dj) algorithm 
(using linked-list structure for repre­senting the dominance frontiers). In particular, the speedup for 
routines that we tested from the Perfect suite is ranging from 2.2 to 8.0, with average speedup being 
5.0. For rou­tines from Lapack speedup range is from 3.9 to 8.1, with the average speedup being 5.7. 
For smaller programs (less than 75 nodes) we found that both the algorithms take scant time. Clearly 
from these plots, we can see that our algorithm performs consistently and significantly faster even for 
real programs we tested. Our observation here is somewhat dif­ferent from [CFR+ 91]: the linear complexity 
of the algorithm has demonstrated significant benefit in terms of speedup on real programs. We also measured 
the execution time for both the al­ gorithms on increasingly taller ladder graphs of the form shown 
in Figure 4. Recall that for this graph, previous alg~ rithms exhibit non-linear running time because 
the size of the dominance frontiers of the left-spine increases quadratically as the size of the ladder 
is increased. For the ladder graphs, we tested our implementation on a SPARC 20 workstation. We measured 
the running time of lDI (d~) and lDF (new) algorithms as the size of the graph is increased. Figure 8 
show the performance curve for both the algorithms on increasingly taller ladder graph. As expected, 
IDF(df ) exhibit quadratic running time, while our new algorithm shows a linear behavior. Notice that 
the measurement shown for the IDF (df ) algorithm is in seconds, while for our algorithm it is in milliseconds. 
This shows that our algorithm is not only linear, but is also significantly faster even for increasingly 
taller ladder graphs. We observed sim­ilar trend for the nested repeat unt i 1 flowgraph. 7 Related 
Work The sparse evaluation technique is becoming popular, es­pecially for analyzing large programs. To 
this end, many intermediate representations have been proposed in the lit­erature for performing sparse 
evaluation [CFR+ 91, CCF91, JP93, WCES94]. The algorithms for constructing these in­termediate representations 
have one common step-deter­ mining program points where data flow information must be merged (the so 
called #-nodes). The notion of d-nodes dates back to the work of Shapiro and Saint [SS70] (as noted in 
[CFR+ 91]). Subsequently, others have proposed sparse evaluation in one form or another that is related 
to work of Shapiro and Saint [RT82, CF87]. Cytron et al. [CFR+ 89] gave the first algorithm for computing 
d-nodes for arbitrary flow­graphs. Tie time comple~ity of the algorithm depended on the size of the dominance 
frontier, which is O(N2). Recently C@on and Ferrante improved the quadratic behavior of computing ~-nodes 
to be almost linear time. The time com­ 70 Proc INI IDF(new)t Perfect dcdcmp 138 0.71 card 151 0.75 readin 
407 1.18 bjt 136 0.22 dctran 310 1.23 elprnt 163 0.91 errchk 347 1.05 modchk 307 1.67 moseq2 162 0.92 
mosfet 215 1.17 Eispack bandv 126 0.15 hqr2 177 0.27 invit 189 0.66 minfit 120 0.11 qzit 125 0.17 svd 
139 0.49 tsturm 151 0.25 Lapack cbdsqr 238 1.33 cgesvd 314 1.07 chgeqz 183 0.44 clatbs 228 1.07 clatrs 
218 0.65 dbdsqr 238 1.28 dgegv 173 0.48 dgesvd 324 0.84 dhgeqz 295 1.5 dtgevc 332 1.8 dtrevc 250 0.44 
Ode ddassi 235 1.07 svodpk 245 0.85 cntrl 228 0.38 ddastp 184 1.0 newmsh 158 1.14 * 245 1.05 dbocls 
169 0.16 dbols 129 0.4 dbolsm 318 1.81 Gator aerset 353 1.73 aqset 207 0.67 chemset 242 1.27 equilset 
350 2.06 initgas 203 1.07 jsparse 289 1.36 out 405 1.54 reader 217 0.93 smvgear 224 0.77 in milliseconds 
Figure 7 Performance of so-me typical programs. S 1.74 1.66 8.47 1.76 8.49 2.56 8.32 6.49 3.47 3.46 
0.94 1.67 2.76 1.06 1.34 2.39 2.04 5.1.5 5.78 3.17 5.17 4.88 5.08 3.46 6.81 5.82 7.02 5.18 3.50 4.82 
3.13 2.60 2.88 4.34 1.43 1.19 7.82 8.73 3.79 4.71 11.37 4.86 8.84 9.48 5.08 4.88 and Speedup 2.4 2.2 
7.2 8.0 6.9 2.8 7.9 3.9 3.8 2.8 6.3 3.6 4.1 9.6 7.9 4.9 8.2 3.9 5.4 7.2 4.8 7.5 3.9 7.2 8.1 3.8 3.9 6.5 
3.3 5.7 8.2 2.6 2.5 4.1 8.9 2.9 4.3 5.0 5.7 3.9 5.5 4.5 5.6 6.2 5.5 6.3 lDl?(d~) on lDF(new) lima (milisec) 
,23 -.. . <m m ea ,0 20 i o 0 ,C.w w Nu.bs r%? NcA.s­ - ,Cca Sc.w E iDF(new) JF(df) rime pns) XcO- 25a­ 
zW ­ ,Sm ) ~ IIKxl a xc­0 /2 1 5c.m5x07w00m  0 ,CC.2CCX.W Nun&#38;% NOdes = IDF(07J Figure 8: Performance 
of both the algorithms on ladder graph plexity of the new algorithm is O(E x a(E)), where ao is the inverse-Ackermann 
function [CF93]. It seems that the algorithm has not been implemented [CF93], and since the algorithm 
is not exactly linear, more experimental studies are needed to evaluate the performance of that algorithm 
when applied to real programs. Compared to any of the previous work, our algorithm reduces the time complexity 
of constructing a single SEG to O(E). Also, we can use our algorithm to construct SSA form or DFG in 
time O(,E x V), where V is the number of variables. Johnson and Pingali recently proposed an algorithm 
for constructing SSA-like representation called the Dependence Flow Graph (DFG) UP93]. To construct DFG 
they first com­pute regions of control dependence. Using this information they determine single-entry-single-exit 
regions. Then they perform, for each variable, an inside-out traversal of these regions, computing dependence 
information and inserting switch and merge nodes, whenever dependence cross re­gions of control dependence. 
The authors have shown that the running time of the algorithm for constructing DFG is O(E). One can easily 
construct the SSA form from the DFG by simply eliminating switch nodes in the DFG. Although, the method 
of Johnson and Pingali can be used for construct­ing the SSA form in time O (E x V) (where V is the number 
of program variables) UP93], it has the same problem as the SSA form, i.e. the DFG and the SSA form cannot 
be used for solving arbitrary data flow problems (for example, liverress analysis), as noted in [CF93]. 
Also, their algorithm can not be used for computing the iterated dominance frontiers for a 71 lDF(new) 
 set of nodes. Iterated dominance frontiers can also used for applications other than for placing ~-nodes 
[SGL94b, Wei92]. Recently Johnson et al. use Quick Propagation Graphs (QPGs) for performing sparse evaluation 
for arbitrary data flow problem UPP94]. They give an algorithm for construct­ing QPGs that runs in linear 
time. Construction of QPG is based on first constructing regions of control dependence. A disadvantage 
of QPGs is that it is more denser than SEGS. This means that solving data flow analysis may take more 
time on QPGs than on SEGS. We are not aware of any other algorithm for comput­ing ~-nodes. There is much 
related work that uses SSA like representation, for example, the Program Dependence Web [BM090] and the 
Value Dependence Graph [WCES94], and our algorithm could improve the complexity of con­structing these 
related intermediate representations. Also there are many optimizations that use SSA form for efficient 
implementation, for example, constant propagation [WZ85], value numbering [RWZ88], register allocation 
[Bri92], code motion [CLZ86], induction variable recognition [W0192], etc. Our algorithm could improve 
the overall running time of these optimization. In this paper we have employed a new program representation 
the DJ-graph. Derived from a flowgraph, the DJ-graph can be viewed as a refinement representing ex­plicitly 
and precisely both the dominator relation between nodes (via D-edges) and the potential program points 
where the dataflow information maybe merged (via J-edges). Pre­viously DJ-graphs have been used indirectly 
for capturing control flow properties of a flowgraph. DFt~~~L relation of Cytron et al. [CFR+ 91] are 
equivalent to J-edges. An edge z --A ~ is a join edge iff ~ e DFlo.az(z). In the DJ-graph we explicitly 
represent the DFlacal relation with join edges. CD.START and CD. END relations in [CFS90] are again re­lated 
to J-edges. The Algorithm 3.1 for computing domi­nance frontier is similar to the algorithm given by 
Cytron et al., with one difference, we use level information for captur­ing the dominance frontiers of 
a node, while Cytron et al. use CD.START and CD. END relations to do the job. As demonstrated in this 
paper, DJ-graphs have facilitated the development of our algorithm. Furthermore, some prop­erties of 
DJ-graphs make much easier the proofs of the cor­rectness and linearity of our algorithm. The DJ-graphs 
can also be applied to program analysis other than computing $-nodes, but that is beyond the scope of 
the present paper. In a recent work, we have used the algorithmic frame­work described in this paper 
to solve the problem of in­crementally maintaining dominator trees for an arbitrary flowgraphs [SGL94b]. 
In the same paper we also propose a method to incrementally update the set of @-nodes of a SEG when the 
flowgraph is subjected to incremental changes. We believe the framework presented here is robust to accommo­date 
incremental program analysis based on SEGS, We will further explore on this in a future paper. Conclusion 
 In this paper, we have provided a positive answer to the open problem posted in the introduction: it 
is indeed possible to design an algorithm for computing &#38;nodes in linear time. This is a good news 
for work which depends on efficient dataflow analysis as computing ~-nodes is a key step in constructing 
a sparse dataflow evaluation framework. Fur­thermore, the algorithm presented in this paper is very sim­ple. 
Our algorithm uses the properties of a new program repre­sentation called the DJ-graph which facilities 
its design and analysis. We also benefit from the simplicity of the algorithm in its implementation. 
We have constructed a prototype implementation of the algorithm on the top of Parafrase2 compiler. Our 
experimental results indicate consistent and significant speedup even on real benchmark programs. On 
increasingly taller ladder graphs our algorithm exhibit linear behavior. We have been using DJ-graphs 
and the algorithmic frame­work presented here to solve a number of other related flow­graph problems. 
We direct interested readers to our compan­ion papers [SGL94a, SGL94b]. Acknowledgement We would like 
to thank many people for their support and encouragement during the course of our work. We owe much thanks 
to Yong-fong Lee for his insightful technical discussions and also for critically commenting on drafts 
of the paper. Erik Altrnan simplified one of the proofs and also suggested many improvements. The comments 
of Russell Olsen, Bjarne Steengard, Berry Rosen, and the POPL review­ers were very useful and improved 
the quality of the presen­tation. Rajiv Gupta and Ron Cytron s constant encourage­ments and support are 
greatly appreciated. We thank the Na­tional Sciences and Engineering Research Council (NSERC) and the 
Canadian Centers of Excellence (IRIS) for their con­tinued support of this research. Lastly the first 
author would like to dedicate this paper in memory of his late father. References <RefA>[AWZ88] Bowen Alpern, 
Mark N. Wegman, and F. Ken­neth Zadeck. Detecting equality of variables in programs. In Conference Record 
of the 15th An­nual ACM Symposium on Principles of Programming Languages, pages 1 11, 1988. [BM090] Robert 
A. Ballance, Arthur B. Maccabe, and Karl J. Ottenstein. The program dependence web: A representation 
supporting control-, and demand-driven interpretation of imperative lan­guages. In Proceedings of the 
SIGPLAN 90 Confer­ence on Programming Language Design and Imple­mentation, pages 257 271, 1990. [Bri92] 
Preston Briggs. Register Allocation ing. PhD thesis, Rice University, April 1992. via Graph Color-Houston, 
Texas, [CCF91] Jong-Deok Choir Ron Cytron, and Jeanne Fer­rante. Automatic construction of sparse data 
flow evaluation graphs. In Conference Record of the 18th Annual ACM Symposium on Principles of Program­ming 
Languages, 1991. [CF87] [CF93] [CFR+89] [CFR+91] [CFS90] [CLZ86] [Har85a] [Har85b] UP93] UPP94] [RT82] 
Ron Cytron and Jeanne Ferrante. What s in a name? or the value of renaming for parallelism detection 
and storage allocation. In Proceedings of the 1987 International Conference on Parallel Pro­cessing, 
pages 19-27, St. Charles, Illinois, August 17-21,1987. Ron Cytron and Jeanne Ferrante. Efficiently com­puting 
~-nodes on-the-fly. In Languages and Com­pilers for Parallel Computing, 1993. Ron Cytron, Jeanne Ferrante, 
Barry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. An efficient method for computing static single­assignment 
form. In Conference Record of the Sixteenth Annual ACM Symposium on Principles of Programming Languages, 
pages 25-35, Austin, Texas, January 11-13, 1989. ACM SIGACT and SIGPLAN. Ron Cytron, Jeanne Ferrante, 
Barry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. Effi­ciently computing static single assignemnt 
form and control dependence graph. ACM Transactions on Programming Languages and Systems, 13(4):452­490, 
October 1991. Ron Cytron, Jeame Ferrante, and Vivek Sarkar. Compact representations for control depen­dence. 
In Proceedings of the SIGPLAN 90 Con­ference on Programming Language Design and lm­plementationr pages 
337 351, White Plains, New York, June 20 22, 1990. ACM SIGPLAN. Also in SIGPLAN Notices, 25(6), June 
1990. Ron Cytron, Andy Lowry and Kenneth Zadeck. Code motion of control structures in high-level languages. 
In Conference Record of the Thir­teenth Annual ACM Symposium on Principles of Programming Languages, 
pages 70-85, St. Peters­burg Beach, Florida, January 13-15, 1986. ACM SIGACT and SIGPLAN. Dov Harel. 
A linear time algorithm for finding dominators inflow graphs and related problems. In Symposium on Theoyof 
Computing. ACM, May 1985. William Harrison. An overview of the struc­ ture of Parafrase. Technical Report 
501, PR-85-2 UILU-ENG-85-8002, UIUC, July 1985. R. Johnson and K. Pingali. Dependence-based program ananlysis. 
In Proceedings of the SIG-PLAN93 Conference on Programming Language De­sign and Implementation, pages 
78-89,1993. R. Johnson, D. Pearson, and K. Pingali. The pro­gram tree structure: Computing control regions 
in linear time. In Proceedings of the SIGPLAN94 Conference on Programming Language Design and Implementation, 
pages 171-185,1994. J. H. Reif and Robert Tarjan. Symbolic program analysis in almost linear time. SIAM 
Journal of Computing, 11(1):81-93, February 1982. [RWZ88] [SG94] [SGL94a] [SGL94b] [ss70] [WCES94] [Wei92] 
[W0192] [WZ85] Barry K. Rosen, Mark N. Wegman, and F. Ken­neth Zadeck. Global value numbers and redun­dant 
computations. In Conference Record of the Fifeenth Annual ACM Symposium on Principles of Programming 
Languages, pages 12-27, San Diego, California, January 13-15, 1988. ACM SIGACT and SIGPLAN. Vugranam 
C. Sreedhar and Guang R. Gao. Com­puting ~-nodes in linear time using DJ-graphs. Technical Report ACAPS 
Memo 75, School of Computer Science, McGill University, January 1994. Submitted for publication. VugranamC. 
Sreedhar, Guang R. Gao, and Yong­fong Lee. DJ-graphs and their applications to flowgraph analyses. Technical 
Report ACAPS Memo 70, McGill University, May 1994. Submit­ted for publication. VugranamC. Sreedhar, GuangR. 
Gao, and Yong­fong Lee. An efficient incremental algorithm for maintaining dominator trees and its application 
to @nodes update. Technical Report ACAPS Memo 77, McGill University, July 1994. Submit­ ted for publication. 
R. M. Shapiro and H. Saint. The representation of algorithm. Technical Report CA-7002-1432, MCA, 1970. 
Daniel Weise, Roger F. Crew, Michael Ernst, and Bjarne Steensgaard. Value dependence graphs: Representation 
without taxation. In Conference Record of the 21st Annual ACM Symposium on Prin­ciples of Programming 
Languages, 1994. Michael Weiss. The transitive closure of control dependence: the iterated join. ACM 
Letters on Programming Languages and Systems, 1(2), June 1992. Michael Wolfe. Beyond induction variables. 
In Proceedings of the SIGPLAN 92 Conference on Pro­gramming Language Design and Implementation, pages 
161-174,1992. Mark Wegman and Ken Zadeck. Constant prop­agation with conditional branches. In Conference 
Record of the Twelfih Annual ACM Symposium on Principles of Programming Languages, pages 291 299. ACM 
SIGACT and SIGPLAN,January 1985.</RefA>  
			
