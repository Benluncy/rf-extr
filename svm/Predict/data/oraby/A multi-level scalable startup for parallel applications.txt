
 A Multi-level Scalable Startup for Parallel Applications Abhishek Gupta Gengbin Zheng Laxmikant V. 
Kalé gupta59@illinois.edu gzheng@illinois.edu kale@illinois.edu Department of Computer Science University 
of Illinois at Urbana-Champaign Urbana, IL 61801, USA ABSTRACT High performance parallel machines with 
hundreds of thou­sands of processors andpetascaleperformance are already in use, and even larger Exa.ops 
scale computing systems which may have hundreds of millions of cores are planned. To run parallel applications 
on machines of such massive scale, one of the biggest challenges is the parallel startup process. This 
task involves two components: (1) parallel launching of appropriate processes on the given set of pro­cessors 
and (2) setting up communication channels to enable theprocesses tocommunicatewith eachotherafterprocess 
launching has completed. Most current startup mechanisms focus on either using special purpose daemons 
which waste system resources or usinga startupmanager whichbecomes a scalability bottleneck. In this 
paper, we investigate the design and scalability of a SMP-aware, multi-level startup scheme with batching 
of remote shell sessions, which pro­vides a complete solution to startup of a parallel application and 
facilitates its management during execution. It mon­itors process health and can be used to support recovery 
from failures and provide scalable interaction with the ap­plication. We demonstrate the performance 
and scalability of this scheme by applying it to startup Charm++ appli­cations. In particular, starting 
up a Charm++ program on 16,384 cores ofRanger(atTACC) withEthernet as the un­derlying communication layer 
takes only 25 seconds and at­tains a speedup of over 400% compared to MPICH2 startup (using hydra as 
process manager) and over 800% compared to Open MPI startup on Ranger. 1. INTRODUCTION Highperformance 
parallelmachineswithhundredsof thou­sands of processors andpetascaleperformance are alreadyin use, which 
provide unprecedented computingpowerto solve scienti.c and engineering problems. Even larger Exa.op/s 
scale computing systems which may have hundreds of mil­lions of cores are planned. To run parallel applications 
on machines of such massive scale, one of the biggest challenges Permission to make digital or hard copies 
of all or part of this work for personal or classroom use is granted without fee provided that copies 
are not made or distributed for pro.t or commercial advantage and that copies bear this notice and the 
full citation on the .rst page. To copyotherwise, to republish, to post on servers or to redistribute 
to lists, requires prior speci.c permission and/or a fee. ROSS 11, May 31, 2011,Tucson, Arizona, USA 
Copyright 2011ACM 978-1-4503-0761-1/11/05 ...$10.00. is the parallel startup process, i.e. how to start 
the appli­cation on all the computation nodes (as an example, this is what mpirun does to start MPI [6] 
applications). On a large machine, there may be a signi.cant delay between job allocation and application 
execution. Furthermore, as another important part of the startup process, all the processes on the computation 
nodes need to exchange information with each other to set up communica­tionchannels forinter-process 
communication during execu­tion. This inter-process communication requires that each process knows about 
the existence of other processes and also where to send a message if it needs to communicate witha particular 
process. Thisinformation canbeinthe form of a socket address (in case of using TCP/UDP) con­sisting of 
IP address and aport. In general, each process canpotentially communicatewithanyof the processes and 
hence shouldhaveinformation which enablesitto sendmes­sages to them. Hence, the task of parallel startup 
involves two compo­nents: (1) parallel launching of appropriate processes on the given set of processors 
and(2) setting up communication channels to enable the processes to communicate with each other after 
startup has completed. In case of many MPI implementations, the startup time would be from the time mpirun 
starts launching processes on computation nodes to the time MPI_Init .nishes, at which point communication 
channels are setup and MPI processes are ready to communi­cate with each other. Note that our de.nition 
of the startup process is di.erent with the ones that only consider paral­lellaunching, e.g. remote executiontools 
such asGXP[24] and TakTuk [11], which are typically exploited for admin­istrative purposes such as running 
updates and setting up con.guration on all computation nodes. This paper focuses on the startup process 
of parallel applications and scalable techniques tospeedupbothcomponentsinthe process. The absence of 
fast startup mechanisms presents a ma­jorobstacleto the full utilizationofhighperformance com­puting 
power by the research community. Users of super­computers are charged in Service Units (SU) to run their 
experiments. One SU is equal to one core-hour of computa­tions. Also, the typical allocation size for 
research groups is a few tens of thousands of SUs. Existing parallel startup mechanism such as those 
used by Charm++ and Open MPI[13]take2to4minutes for startupon8Kprocessors on Ranger[3](atTACC) andperform 
evenworseforhigher core counts. This results in tremendous SU usage just to startup the application. 
As an example, startup time of 4 minutes for 16K processorswould mean thatasingle experi­ment on 16K 
processors resultsin consumptionofmorethan 1K SUs for application startup. This results in limiting the 
number of experimentsa researcher canperform given the .xed allocation size.  There aretwotypesofapproaches 
thathavebeen adopted by researchers to address the problem of parallel startup. The .rst one assumes 
the presence of special purpose dae­mons running on compute nodes to facilitate the startup process [9, 
18, 21]. An example of thisis a system called Multi-purpose Daemons (MPD) [9] used for MPICH [16] jobs. 
Here, when an application starts, the launcher con­tactsthese daemons to start the processes oneach compute 
node. The drawback of this approach is that these daemons keep running even when no MPI application is 
running and hence waste system resources. The second type of approach isto usealauncherwhich starts processes 
on compute nodes using existing daemons such as rsh or ssh and then sets up communication channels among 
them. However, as we go up to high core counts, the centralized launcher becomes a bot­tleneckandimposes 
scalabilitylimitations (demonstratedby Figure 4). In this paper, we present a multi-level scalable startup 
method which is generic and can be applied to most parallel programming environments, including MPI and 
Charm++ [20]. The fundamental idea is to use multiple launchers which form a startup tree and reside 
on di.erent proces­sors. This makes the process of parallel application startup decentralized and hence 
it scales well with increasing num­ber of processors. We also incorporate SMP-awareness in our approach 
to achieve faster startup. In addition, we in­troduce the concept of batching of remote shell sessions 
to makethe parallelstartup process fast ona consistentbasis and discuss the trade-o.s involved in parallel 
startup using a theoretical model. Moreover, our approach does not re­quire presence of any special daemons 
(except rsh or ssh daemons) on parallel machines to startup the application. However, it can still be 
used to monitor process health and provide scalable interaction with parallel application after startup 
is complete. We demonstratetheperformance and scalability ofmulti­level startup method by applying it 
to Charm++ run-time system. Charm++ is a widely used programming model for large scale scienti.c and 
engineering applications includ­ing NAnoscale Molecular Dynamics(NAMD)[7], whichisa highly scalable molecular 
dynamics code used ubiquitously on the TeraGrid and other HPC systems. Starting up a Charm++ program 
on 16,384 cores of Ranger with Ether­net as the underlying communication layer now takes only 25 seconds. 
This results in the SU consumption getting re­duced by an order of magnitude compared with the central­ized 
startup. Moreover, our scheme outperforms Open MPI startup by a factor of over 8 and MPICH2 startup (using 
Hydra) by a factor of 4 for 16K cores on Ranger. The remainder of the paper is organized as follows: 
Re­lated work is discussed in section 2. Section 3 describes the process of startup of parallel application. 
Section4 dis­cusses themulti-level approach to parallel startup. Section5 presents performance evaluation 
of our multi-level startup scheme. Finally, conclusions and future work are left for the .nal section. 
 2. RELATED WORK The problem of scalable startup for parallel application hasbeen studiedby many researchers. 
Butler et al.[9] pre­sented a scalable process management system called MPD (for Multi-purpose Daemon) 
for parallel programs such as those written using MPI. The main idea is the presence of special purpose 
persistent daemons, typically one instance per host in a TCP-connected network. The daemons are connected 
in a ring. Manager processes are started by the daemons to control the application processes (clients) 
of a single parallel program and provide most of the MPD fea­tures. To run an MPI program, mpirun .rst 
connects to the daemon ring in order to start the parallel program and then switches to manager ring 
in order to control the program. Our approach does not assume presence of anydaemons and provides fast 
startup and application management capabili­tieswithout needingany daemons. Yuetal.[25] havedone research 
on startup of MPI programs on In.niBand clusters. They use MPD for process spawn and focus on reducing 
the data volume exchanged during information exchange. SLURM[18]isa fault-tolerant andhighly scalablecluster 
management and job scheduling system for large and small Linux clusters. It allocates resources (compute 
nodes) to users for some duration of time and also provides a frame­work for starting, executing, and 
monitoring work (normally a parallel job) on the set of allocated nodes. ALPS[21] is another similar 
application placement and launch system. STORM [12] is another resource management framework designed 
for scalability and performance and provides job­launch mechanisms. Similar to MPD, SLURM, ALPS and STORM 
use special purpose daemons running on each com­pute node for startup and monitoring. Hydra [5] is the 
default process management framework forstarting MPI processes forMPICH2-1.3 onwards. It uses existing 
daemons such as ssh, rsh, pbs, slurm and sge to start MPI processes. ScELA[23]isa joblaunch mechanismwhich 
targets multi-core clusters. It decouples the two phases in a parallel application launch -spawning of 
processes and information exchangebetween processes to completeinitial­ization. It comprises a spawning 
agent which starts executa­bles on target processors and the communication primitives are used within 
the executables to communicate necessary initialization information. Our SMP-aware startup (section 4.1) 
is similar to ScELA process launch since ScELA has a Node Level Agent(NLA) for every node. An NLAis used 
tolaunchall processes ona node. NLAs are active only for the duration of launch, hence the framework 
is daemonless. However, since there is an NLA per node, there is an extra process per node consuming 
processor cycles. Our proposed approach (discussed in section 4) has child charmruns but v they are only 
a few (e.g. N for 2-levelstartup on N nodes). Moreover, they are necessary because that provides I/O 
ca­pabilities and scalable interaction with parallel application. Brightwell etal.[8] presentthe componentsof 
the runtime system for parallel application launch on Cplant project. They do not assume that the executable 
to be launched is available on a global .le system. Our proposed approach makes that assumption; since 
in our experience, that is the commoncasein highperformance systems. Research has been done on concurrent 
launching strate­giesincludingtree-basedlaunching.Claudeletal.[11] study theperformanceof standard remoteexecution 
protocols and explore various concurrent launching strategies. Also, they propose work-stealing method 
to balance the tasks of de­ployment to child nodes. They present TakTuk, a remote  Remote Charmrun Client 
Charmrun starts a remote session with client Time Client Process starts and sends I-tuple Client connects 
back with its I-tuple Charmrun receives I-tuple from client Charmrun sends global node table Figure 
1: Basic process of parallel startup execution deployment system which canbe used for fast and scalable 
distributed machine administration and parallel ap­plication development. Their work focuses on the execution 
of same process on a set of nodes. Another such parallel shelltoolisGXP[24] which facilitates running 
anidentical or a similar command line to many machines in parallel and getting results backinteractively. 
InbothTakTuk and GXP, the processes do not need to communicate with each other and hence the second phase 
of parallel startup -setting up communication channels is not needed.  3. LINEAR STARTUP Linear startupisa 
simple startupscheme that usesa single launcher to start all processes, as illustrated in Figure 1. Although 
the technique is generic and can be applied to most parallel programming environments such as MPI, we 
present the scheme in the context of Charm++. In this paper, we will refer to the launcher as charmrun 
and the processes which constitute the parallel applicationas clients. For simplicity, we assume processes 
on remote processors communicate via UDP sockets. Charmrun needs to know thesetof processors where the 
parallel application willbe run. One way of providing this information is using a ma­chine .le, which 
we call the nodelist .le. Charmrun starts a remote shell session with each processor where the applica­tion 
will be run. Standard remote execution shell facilities (such as rsh or ssh)can be used for this purpose. 
We chose to use ssh sinceit providesstrong authenticationandis more secure compared to rsh. After starting 
a remote shell ses­sion, charmrun sets up some environment variables, creates a process on the remote 
processor using the fork() system call and loads the application executable using the exec() systemcall. 
This taskisperformed for each processorin the nodelist .le. We refer to this scheme as linear startup 
or centralized startup. During the execution of a parallel application, the remote clients need tobe 
able tocommunicatewitheachother. This inter-process communication requires establishment of com­munication 
channels among clients. The second component of startup performs information collection and dissemina­tion 
to enable the clients to communicate with each other after startup has completed. Each client sends some 
infor­mation to charmrun which is used to set up communication channels between the clients. We refer 
to this information as an I-tuple. In the case of TCP or UDP as the under­lying communication layer, 
an I-tuple consists of a socket address comprising IP address and dataport. The dataport is the port 
where a client will listen for any incoming mes­sage from other clients during execution of the application. 
Charmrun receives I-tuples from allclients and collects them to form a table of I-tuples which we call 
the node-table. The node-tableis sent to every process. After receivingthe node­table, clients can communicate 
with each other without any need of charmrun and startup is complete. As another ex­ample,this componentofstartupwouldinvolve 
the process of establishing queue pairs on an In.niband based commu­nication network[17]. Charmrun is 
needed even after startup has completed since it acts as an interface between parallel application and 
the external world during execution. All input output and some additional features (discussed in section 
4.5) such as parallel debugging can utilize charmrun.  4. MULTI-LEVEL STARTUP In this section, we discuss 
the multi-level startup tech­nique.Thereis an optimization which canleadto signi.cant improvement in 
the performance of centralized startup. We discuss that before describing the multi-level approach. 4.1 
SMP-Aware Startup The scheme discussed in section 3 requires charmrun to perform a remote shell login 
to each processor. Most super­computers and even desktop systems today have multi-core chips where each 
node has many processor cores. 8-core, 16­core and 32-core nodes are not uncommon. Consider a node with 
16 cores; charmrun would create a ssh session with each of the 16 cores. An optimization to this is to 
create only one ssh sessionpernodeandspawnallclients fromthe same ssh session. We call this SMP-aware 
startup. With the trend towards clusters with increasing number of cores per node, this optimization 
is extremely useful. In addition, this is useful when multiple processes need to be launched on a single 
processor, such as in parallel application testing and debugging. The second phase of startup remains 
the same as the linear startup. Each client sends an I-tuple to charmrun, which collects them and sends 
the node-table to every client.  4.2 Multi-level Startup Even with the optimization discussed in previous 
section, the startup is inherently serial. Charmrun starts ssh ses­sions sequentially and waits for all 
the clients to connect back. Charmrun becomes a bottleneck in a few ways: 1. Charmrun has to start an 
ssh session with each node. 2. Charmrun has to receive a message containing an I­tuple from each of 
the clients.  In todays supercomputers, with hundred of thousands of nodes, centralized startup becomes 
a bottleneck and the startup performance degrades signi.cantly with increase in number of cores (See 
section 5.1). We can conclude that we  Root Charmrun Child Charmruns  Parallel Processes  Figure 2: 
Multi-level startup scheme need to explore a distributed startup scheme to prevent the centralcharmrunprocess 
frombecomingabottleneck. We propose a multi-level startup to overcome the prob­lemsdiscussedsofar.Fig2illustrates2-level 
startupscheme. Here we have a master charmrun process, which we call the root charmrun, and secondlevel 
charmrun processes, which we will refer to as child charmruns. Child charmruns reside on di.erent processors. 
Each child charmrun is assigned a subset of unique nodes for which it acts as a manager. The root charmrun 
acts as a top-level manager that coordinates the startup process between child charmruns. It decides 
the branching factor(number of child charmrun) which we call k. For simplicity, we keep the branching 
factor as the square root of number of unique nodes where the application will be run. (Section 4.4 provides 
an analysis of the e.ect of branching factor and number of levels on performance.) Each child charmrun 
gets approximately k nodes assigned to it and acts in a similar manner to the charmrun of central­ized 
startupmethod. It starts processes onits nodeset and waits for clients to connect back. After receiving 
I-tuple, it forwards that to root charmrun. Root charmrun receives all the I-tuples and disseminates 
node-table to child charmruns, whichinturn, forwardthatto their respective setofclients. We note that 
multi-level startup uses SMP-aware startup at leaf charmrun level. Child charmrunsmust continuetoexist 
afterthe startupis complete. A client process is aware of only its manager and is oblivious to the presence 
of other child charmruns and the root charmrun. All input-output and any communication for additional 
supported features must go throughit. The decentralized existence of charmruns makes parallel input output 
and user interaction with parallel application more scalable. The multi-level startup technique distributes 
the task of performing remote shell sessions and receiving I-tuples to a set of processors and is intuitively 
more scalable. How­ever, while evaluating startup performance we discovered that there was a huge variation 
in the startup time between di.erent runs. As an example, for 4096 cores on TACC s Ranger cluster with 
16 cores per node, startup time using multi-level startup varied from 10 to 60 seconds. To dis­cover 
the cause of this unexpected behavior, an analysis of the breakdown of time taken by startup was done. 
Fig­ure 3(a) shows the breakdown of the time spent by di.erent child charmruns in startup. With 4096 
processors and 16 cores per node, there are 256 unique nodes and hence the branching factor is kept 16 
for 2-level startup. So, there are 16 child charmrun. Init time is the initialization time taken by each 
child charmrun. It comprises the time taken by a child charmrun to connect to root charmrun and receive 
its set of nodes. SSH time is the time spent in creating remote login sessions and launching clients 
on remote processors. Wait time is the time spent in waiting for their set of clients to connect back. 
It can be observed that Init time and SSH time is small and does not vary a lot across charmruns. The 
main component where alarge fraction of time is spentis Wait time. Moreover, Wait time varies from 2 
seconds to 49 seconds acrosscharmruns resultingin inconsistentperfor­mance. If there is even a single 
outlier, it delays the whole startup process.  4.3 Multi-level Startup with Batching It was observed 
that the variation in startup time is small forless number of cores andbecomes worse as weincrease number 
of cores. Although multi-level startup makes the startupprocessdistributed,itdoesnot reducethe total 
num­ber of I-tuple messages that can be present in the network at any time. This is equal to the number 
of clients. If there are tens of thousands of clients, this can lead to congestion in network. To overcome 
this problem, we introduce the concept of batching of remote shell sessions. In this strat­egy, the nodes 
assigned to a leaf charmrun are divided into setsof .xedsize. Each child charmrun performs ssh to the 
nodesin the current set,waits forthe clientstoconnect back and thenperforms ssh on the next set. We call 
the num­ber of nodes in one ssh set as batch size. Batching reduces the total number of messages at any 
time and hence leads to better scalability. Figure 3(b) shows the breakdown of the time spent by di.erent 
child charmruns in multi-level startupwithbatchsizeof8 for4096 processors onRanger. Comparing with Figure 
3(a), we observe that the wait time is consistent across all charmruns and is small. This leads to a 
faster and more scalable startup process. However, batching introduces some serialization; only af­ter 
the clients in the .rst set are launched and I-tuples are received from those, next set of clients can 
be launched. We discuss the e.ectof batch sizeonperformancein section 5.2  4.4 Analysis In this section, 
we present a theoretical analysis of the di.erent startup schemes discussed in this paper. Consider a 
supercomputer with P processor cores and N nodes. Let c = P/N be the number of cores per node. Parallel 
startup time for ourlinear startup scheme(Tlinear)can be modeled as: Tlinear = Tinit + P × Tssh + Tclient 
(1) +Tsend + Tnw + P × Trecv where Tinit is the charmrun initialization time (which in­cludes getting 
the list of nodes to start e.g. reading nodelist .le, starting a server port where clients can send I-tuples 
etc), Tssh is thetime takenbycharmruntostarta rsh or ssh session with a remote node, Tclient is the time 
taken by the remote shell to create a new process at the remote proces­sor and load the program executable, 
Tsend is the processor sending overhead at a client, Tnw is the network latency for a message, Trecv 
is the message receiving overhead incurred by charmrun. We consider the total overhead due to Tinit, 
Tclient, Tsend and Tnw as constant and represent that by Tc to keep the  1 2 3 4 5 6 7 8 9 10 11 12 
13 14 15 16 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Child charmrun Child charmrun (a) Without batching 
(b) With batching using batch size of 8 Figure 3: Breakup of time spent in parallel startup using multi-level 
scheme for 4K processors analysis readable.1 Hence, we have Tlinear = Tc + P × (Tssh + Trecv) (2) where 
Tc = Tinit + Tclient + Tsend + Tnw (3) SMP-aware startup starts only one ssh process per node and hence 
it reduces the total ssh time. However, it still incurs the receive overhead for all clients. so, the 
time taken by SMP-aware startup(TSMP )can be modeled as: TSMP = Tc + N × (Tssh + c × Trecv) (4) Hence, 
both Tlinear and TSMP grow as .(P)since charm­run has to start a ssh process for each node and incur 
a receivingoverhead for each client. Thisbecomes toa scala­bility bottleneck. Now consider our multi-level 
startup. Letk be the branch­ing factor andd be the depth of the startup tree. We assume thebranchingfactoriskeptsame 
acrossthelevelsofthe tree. In a k-ary, d-level startup tree, a charmrun at level d is = responsible for 
acting as a manager for its k child charm­runs and a charmrun at level = d acts as a manager for k nodes 
(k × c clients). This scheme emulates the linear startup scheme at each level of the startup tree. Hence, 
the startup time for a d-level SMP-aware startup(Td-level)can be modeled as: Td-level = d× (Tc + k× (Tssh 
+ Trecv)) (5) +k× (c -1)× Trecv The crucial parameters here are k and d, which related by: d =logk(N) 
(6) Multi-level startup increases the overhead due to Tc by a factor of d. However, it reduces the e.ect 
of ssh time 1 Tnw is the maximum network latency of a short message between any two processors of the 
supercomputer. Note that with increase in the number of processors, Tnw (and hence Tc) could increase 
due to network contention. This would depend on the network architecture and should be considered for 
a more accurate analysis. and receive overhead by a factor of P/(ck logk(P/c)). We note that Td-level 
grows asymptotically as .(klogk(P/c)) instead of .(P) for linear startup. An optimal value of the number 
of levels can be obtained by minimizing Td-level using equation 5, with k given by equation 6. Determining 
the exact expression for this optimal value is beyond the scope of this paper. A simpler method of obtaining 
d is to bound k so that charmrun can handle the given number of clients without signi.cantperformance 
degradation. Past research[23] sug­gests that a process can handle 128 simultaneous connec­tions with 
acceptable performance. Hence, d = 2 should be adequate to handleup to 16384 processes. Fora2-level 
startup(d =2), k =P/c. There is one factor which we have ignored so far. As we scale to high core counts, 
the number of messages in the network sent by clients to connect back to the respective charmruns increase. 
This can make the network congested and degrade performance signi.cantly. Batching overcomes this problem 
and makes startup time consistent. However, batching comes with the cost of increasing the best case 
(no congestion) startup time. Startup time for SMP-aware multi-level scheme with batching can be modeled 
as: Tbatched d-level =(d-1)× (Tc + k× (Tssh + Trecv)) (7) +(k/b)× Tc + k× (Tssh + c × Trecv) where b 
is the batch size. Since batching is only done at thelastlevelof the tree,itdoes notdegradetheperformance 
of starting charmrun tree itself. However, it a.ects the time taken by last level charmruns to startup 
the clients. The overhead Tc is now incurred for every batch phase of a last­level charmrun.  4.5 Runtime 
Capabilities The startup system presented in this paper can also be used to interact with a parallel 
application after startup is .nished. After startup of the application is complete, each charmrunacts 
asamanager foritssetofclients. Wediscuss two aspects of this: 4.5.1 Process Health and Recovery from 
Failures After startup of the application is complete, each charm­run is accountable for monitoring the 
status of its clients.  1k 2K4K 8k Number of cores Figure 4: Startup time: Comparison among 3 startup 
schemes Each charmrun entersintoapolling modewhereit monitors process health. Ifa process fails,charmrunis 
responsible to terminate the whole application if there is no support for fault tolerance. Charmrun system 
has also been used to fa­cilitatethedesignof fault tolerance protocols for Charm++ applications [10]. 
If a process fails, appropriate charmrun restarts the failed process using restart protocol. In this 
protocol, execution of the parallel application is suspended by thecharmruns till the failed process 
is restarted. A new process is launched and node-table is modi.ed to use the I-Tuple received from the 
restarted process. This new node­table is communicated to all the clients and execution can resume. Multi-level 
charmrun system makes process health monitoring and failure recovery a decentralized process and hence 
more scalable.  4.5.2 Support for Scalable Interaction with Parallel Application The multi-level charmrun 
system can be used to provide scalable interaction with the parallel application. Such in­teraction can 
be extremely useful for providing parallel de­bugging, online performance analysis and simulation visu­alization[15]. 
Developers and end-users of parallel applica­tion can greatly bene.t from these capabilities provided 
by a runtime system. We have used Charmdebug[19] with our multi-level charmrun system to debug parallel 
application. Charmdebugisagraphicaltool thatallowsprogrammersto debug large scale parallel programs. 
It uses Converse Client Server (CCS)[22] whichis a communication protocol that allows parallel applications 
to act as remote server that re­ceives and serves requests from remote clients. Multi-level charmrun 
system makes this process more scalableby dis­tributing the task of managing the client processes among 
multiple charmruns and hence avoiding the single charmrun frombecomingabottleneck. CCScanalsobe used 
to pro­vide simulation visualization using tools such as liveViz [1]. During execution charmrun also 
acts as a means of any com­municationbetweentheclientprocessandthe outsideworld. An example of this is 
console input output.  5. PERFORMANCE RESULTS We ranperformance tests onTACC sRangerCluster which is 
one of the largest computational resource in the world (Ranked 15 in the November, 2010 top500 list [4] 
of su­percomputers). The Ranger system has 3,936 16-way SMP compute nodes providing 15,744 AMDOpteron 
processors for a total of 62,976 compute cores [3]. It has a theoretical peak performance of 579 TFLOPS. 
All nodes are intercon­nected using In.niBand technology in a full-CLOS topology and provide a 1GB/sec 
point-to-point bandwidth. For core counts above 4K, the executable was cached in each node s memory immediately 
before launching the parallel applica­tion to avoid any inconsistencies caused by disk to memory transfers. 
The following subsections discuss various perfor­mance resultswe got onthissupercomputer. 5.1 Performance 
of Different Schemes Inthis section,wediscuss theperformanceofour startup schemes. We ran all our experiments 
using all the 16 cores per node and used Ethernet as the underlying communi­cation network. Figure 4 
compares three schemes -linear startup, SMP-aware startup and multi-level startup with­out batching. 
The e.ect of batching is discussed in sec­tion 5.2. We can observe that linear startup does not scale 
beyond 4K processors. For 8K cores, we waited8 minutes for startup to .nish using linear startup; our 
allocation on Ranger did not allow us to wait inde.nitely. For 4K pro­cessor cores (256 nodes),linear 
startup takes 237 seconds to .nish startup, SMP-aware startup takes 51 seconds whereas multi-level startup 
without batching takes 37 seconds on av­erage giving a speedup of 6.4X over linear startup and 1.4X over 
SMP-aware startup. Moreover, the slopes of the three linesin the .gure show thatmulti-level startupscalesbetter 
withincreasingnumberof processors ascompared to other v schemes. With a branching factor equal to N where 
N is the number of nodes, we expect the time taken by ssh v phase of startup to grow as .( N) instead 
of .(P) in the baseschemewherePisthenumberof processors. Ifwe use more levels of charmrun, we get a tree 
startup scheme that can scale as .(log(N)). However, in our experiments, 2-level scheme was su.cient 
since Ssh time is not the main bottle­neck as discussed in section 4.3. The main bottleneck is the Wait 
time which is reduced further by using batching. We discuss improvement in performance and scalability 
due to batching in section 5.2.  5.2 Effect of Batching We experimented with di.erent batch sizes to 
.nd out the optimal batch size. Figure 5 shows the e.ect of batch size on startup time for 4K processors 
(256 nodes) on Ranger. For 256 nodes, branching factor is kept 16. So, batch size of 16isidentical to 
startingall nodes(no batching).For each batch size on the x-axis, circles represent the startup time 
of a particular run. Each point on the lower line shows the % di.erencebetween themaximum andminimum 
startup time for a .xed batch size. The graph also shows average startup time for di.erent batch sizes. 
We can maketwoimportant observations from this .gure. First, we see that the variation in startup time 
increases with batch size. The variation is as high as 600% for no batching. Second, average startup 
timeinitially decreases with batch size,becomelowest for batch size of 8 and increases again. The reason 
for that is the trade-o. between the slowdown caused due to inherent serializationintroducedbybatching 
andthespeedup caused due to avoiding congestion by reducing the total number of   2 4 816 Batch size 
Figure 5: Variation in startup time with batch size for 4K processors 4K 8k 16K Number of cores Figure 
6: Startup time vs number of cores for dif­ferent batch sizes message in the network at any time. The 
results for scaling of startup using multi-level startup with batching with di.erent batch sizes are 
shown in Fig­ure 6. We see that the batch size of 8 performs the best asweincrease thenumberof processors. 
Smallerbatch size does not scale well with the number of processors due to the serialization introduced. 
A batchsize of16does notper­formwell for4K and8K processorsbecauseitiscloseto no batching for these number 
of processors. Itperformswell for 16K processors(1024 nodes). We expect a batch size of 16 to be good 
for even higher number of cores.  5.3 Comparison with Open MPI and MPICH2 (Hydra) startup Open MPI and 
MPICH2 are two of the most prominent freeimplementations of MPI standard. Open MPI[13]is an open source 
MPI-2 implementation that is developed and maintained by a consortium of academic, research, and in­dustry 
partners. It is used by many TOP500 supercom­putersincluding Ranger. Ranger uses SGE(SunGrid En­gine)[14] 
parallelenvironmenttolaunch andmanageOpen MPI processes. MPICH2 [2] is a high performance and widely 
portable implementation of MPI standard. We in­ 1k 2K 4K 8k16K Number of cores Figure 7: Startup time 
on Ranger: Open MPI vs MPICH2 (Hydra) vs Multi-level Startup stalled MPICH2-1.3 on Ranger to compare 
our approach with hydra [5] which is the default process management framework for starting MPI processes 
for MPICH2-1.3 on­wards. Hydra uses existing daemons such as ssh, rsh, pbs, slurm and sge to start MPI 
processes. We compared the startup time taken by our multi-level startup with batching with Open MPI 
and MPICH2 startup time on Ranger. MPI startup time was measured by calculating the di.erence be­tween 
the time measured using Linux date command from the job script just before starting the parallel program 
and a timer call after MPI initialization. For our scheme, the startup time includes the time of the 
two phases of startup -parallel processlaunch and establishment of communica­tionchannelsbetween parallel 
processes. Figure7 shows the comparison between the startup time with varying number of processors. We 
can note that our startup scheme out­performs Open MPI startup by a factor of 8 and MPICH2 by a factor 
of 4 for 16K processors. Also, we see that our scheme scales very well with the number of cores. Startup 
for 16K cores on Ranger using multi-level startup takes only 25 seconds.  6. CONCLUSIONS AND FUTURE 
WORK This paper presents a scalable multi-level approach for startup of parallel applications on large 
systems. Parallel startup consists of two phases -parallel launching of appro­priate processes onthegiven 
setof processors and settingup communication channels to enable the processes to commu­nicate with each 
other after startup has completed. We ex­plored techniquestospeedupbothof these components. We also introduced 
the concept of batching of remote shell ses­sions and incorporated SMP-awareness to further improve scalability. 
We analyzedtheperformanceofdi.erentstartup techniques presented in this paper using a theoretical model 
and also evaluated their performance on TACC s Ranger cluster using Charm++. Our scheme was able to startup 
a Charm++ program on 16K cores of Ranger[3] withEth­ernet as the underlying communication layer in only 
25 sec­onds. We also compared theperformance withOpen MPI and MPICH2(withhydra as the processmanager) 
startup and our scheme outperformed Open MPI startup by a fac­tor of over 8 and MPICH2 startup by a factor 
of 4 for 16K cores.  The multi-level startup system presented in this paper is a complete solution to 
the startup of a parallel application anditsmanagement during execution. It monitors process healthand 
canbe used to support recovery from failures and provide scalable interaction with the application. In 
future, we plan to extend this work to startup of paral­lel application using underlying high-performance 
intercon­nects such as In.niband. Also, we plan to explore parallel startup techniques that involve lazy 
establishment of com­munication channels in the second phase of startup, which requires on-demand connection 
establishment during execu­tion. Acknowledgments Thisworkwas supportedin partbyNSF grantOCI-0725070 for 
Blue Waters deployment, by the Institute for Advanced Computing Applications and Technologies (IACAT) 
at the University of Illinois atUrbana-Champaign, andby Depart­ment of Energy grant DE-SC0001845. We 
used machine resources on the Ranger cluster (TACC), under TeraGrid allocation grant TG-ASC050039N supported 
by NSF.  7. REFERENCES <RefA>[1] LiveViz Library. http://charm.cs.uiuc.edu/ manuals/html/libraries/6.html. 
 [2] MPICH2:High-Performance and Widely Portable MPI. http://www.mcs.anl.gov/mpi/mpich. [3] Ranger User 
Guide.http://services.tacc.utexas. edu/index.php/ranger-user-guide. [4] Top500 supercomputing sites. 
http://top500.org. [5] UsingtheHydra Process Manager. http://wiki.mcs.anl.gov/mpich2/index.php/ Using_the_Hydra_Process_Manager. 
[6] Mpi: Amessage passing interface standard. In M. P. I. Forum (1994). [7] Bhatele, A., Kumar, S., Mei, 
C., Phillips, J. C., Zheng, G., and Kale, L. V. Overcoming scaling challenges in biomolecular simulations 
across multiple platforms. In Proceedings of IEEE International Parallel and Distributed Processing Symposium 
2008 (April 2008), pp. 1 12. [8] Brightwell, R., and Fisk, L. A. Scalable Parallel Application Launch 
on Cplant TM. In In Proceedings of the SC2001 Conference on High Performance Networking and Computing 
(2001). [9] Butler, R., Gropp, W., and Lusk, E. A Scalable Process-Management Environment for Parallel 
Programs. In Recent Advances in Parallel Virtual Machine and Message Passing Interface, vol. 1908 of 
Lecture Notes in Computer Science. Springer Berlin/ Heidelberg, 2000, pp. 168 175. 10.1007/3-540-45255-9 
25. [10] Chakravorty, S. A Fault Tolerance Protocol for Fast Recovery. PhD thesis, Dept. of Computer 
Science, University of Illinois, 2008. [11] Claudel, B., Huard, G., and Richard, O. TakTuk, adaptive 
deployment of remote executions. In Proceedings of the 18th ACM international symposium on High performance 
distributed computing (New York, NY, USA, 2009), HPDC 09, ACM, pp. 91 100. [12] Frachtenberg, E., Petrini, 
F., Fernandez, J., Pakin, S., and Coll, S. Storm: Lightning-fast resource management. In In Supercomputing 
2002 (2002). [13] Gabriel, E., Fagg, G. E., Bosilca, G., Angskun, T., Dongarra, J. J., Squyres, J. M., 
Sahay, V., Kambadur, P., Barrett, B., Lumsdaine, A., Castain, R. H., Daniel, D. J., Graham, R. L., and 
Woodall, T. S. Open MPI: Goals, concept, and design of a next generation MPI implementation. In Proc. 
of 11th European PVM/MPI Users Group Meeting (Budapest, Hungary, 2004). [14] Gentzsch, W. Sun Grid Engine: 
towards creating a compute power grid. In Cluster Computing and the Grid, 2001. Proceedings. First IEEE/ACM 
International Symposium on (2001), pp. 35 36. [15] Gioachin, F., Lee, C. W., and Kal´e,L. V. Scalable 
Interaction with Parallel Applications. In Proceedings of TeraGrid 09 (Arlington, VA, USA, June 2009). 
[16] Gropp, W., Lusk, E., Doss, N., and Skjellum, A. MPICH: A High-Performance, Portable Implementation 
of the MPI Message Passing Interface Standard. Parallel Computing 22, 6 (September 1996), 789 828. [17] 
Infiniband Trade Association. In.niband Architecture Speci.cation, Release 1.0. Tech. Rep. RC23077, October 
(2004). [18] Jette, M. A., Yoo, A. B., and Grondona, M. SLURM: Simple Linux Utility for Resource Management. 
In In Lecture Notes in Computer Science: Proceedings of Job Scheduling Strategies for Parallel Processing 
(JSSPP) 2003 (2002), Springer-Verlag, pp. 44 60. [19] Jyothi, R., Lawlor, O. S., and Kale, L. V. Debugging 
support for Charm++. In PADTAD Workshop for IPDPS 2004 (2004), IEEE Press, p. 294. [20] Kal´e, L., and 
Krishnan, S. Charm++ : A portable concurrent object oriented system based on C++. In Proceedings of the 
Conference on Object Oriented Programmi ng Systems, Languages and Applications (September 1993). [21] 
Karo, M., Lagerstrom, R., Kohnke, M., and Albing, C. The Application Level Placement Scheduler. In Cray 
User Group May 2006. [22] Laboratory, P. P. Converse Programming Manual. Department of Computer Science, 
University of Illinois at Urbana-Champaign, Urbana, IL. http://charm. cs.uiuc.edu/manuals/html/converse/manual.html. 
[23] Sridhar, J. K., Koop, M. J., Perkins, J. L., and Panda, D. K. ScELA: scalable and extensible launching 
architecture for clusters. In Proceedings of the 15th international conference on High performance computing 
(Berlin, Heidelberg, 2008), HiPC 08, Springer-Verlag, pp. 323 335. [24] Taura, K. GXP : An Interactive 
Shell for the Grid Environment. Innovative Architecture for Future Generation High-Performance Processors 
and Systems, International Workshop on 0 (2004), 59 67. [25] Yu, W., Wu, J., and P, D. K. Fast and Scalable 
Startup of MPI Programs in In.niBand Clusters</RefA>.   
			
