
 A Hypergraph-Partitioning Approach for Coarse-Grain Decomposition* ¨ UmitV.C¸ataly¨CevdetAykanat urek 
Department of Biomedical Informatics Computer Engineering Department The Ohio State University Bilkent 
University Columbus, OH 43210 Ankara, 06533 Turkey umit@cs.umd.edu aykanat@cs.bilkent.edu.tr ABSTRACT 
We propose a new two-phase method for the coarse-grain decompo­sition of irregular computational domains. 
This work focuses on the 2D partitioning of sparse matrices for parallel matrix-vector multi­plication. 
However, the proposed model can also be used to decom­pose computational domains of other parallel reduction 
problems. This work also introduces the use of multi-constraint hypergraph partitioning, for solving 
the decomposition problem. The proposed method explicitly models the minimization of communication vol­ume 
while enforcing the upper bound of p+q;2on the maxi­mum number of messages handled by a single processor, 
for a par­allel system with P=pxqprocessors. Experimental results on a wide range of realistic sparse 
matrices con.rm the validity of the proposed methods, by achieving up to 25 percent better partitions 
than the standard graph model, in terms of total communication vol­ume, and 59 percent better partitions 
in terms of number of mes­sages, on the overall average. 1. INTRODUCTION The standard graph partitioning 
approachhas been widely used to decompose irregular domains for the sake of ef.cient parallel exe­cution 
[2, 3, 16, 17, 19, 22, 23, 25, 30, 34]. In our previous works [6, 7, 9], we introduced two computational 
hypergraph models for the decomposition problems. In the mentioned works, the decomposi­tion problem 
is modeled as a mincut graph/hypergraph partitioning problem, such that minimization of cut-cost targeted 
to the mini­mization of total communication volume. However, for architec­tures with high message latency, 
minization of number of messages is also important. In this work, we propose a two-phase method which 
employs multi-constraint hypergraph partitioning. The pro­posed method naturally maintains an upper bound 
for the maximum number of messages handled by a single processor while minimiz­ingthetotalcommunicationvolume. 
Thisupperboundis p+q;2 messages per processor for a parallel system with P=pxqpro­cessors. *This work 
is partially supported by Turkish Science and Research Council under grant EEEAG-199E013. Permission 
to make digital or hard copies of all or part of this work for personal or classroom use is granted without 
fee provided that copies are not made or distributed for pro.t or commercial advantage, and that copies 
bear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on 
servers or to redistribute to lists, requires prior speci.c permission and/or a fee. SC2001 Nov 2001, 
Denver (c) 2001 ACM 1-58113-293-X/01/0011 $5.00. Partitioning of irregularly sparse matrices for repeated 
sparse-matrix vector multiplication (SpMxV) is an important appli­cation for the decomposition methods. 
Repeated SpMxV y=Ax that involves large, sparse, structurally symmetric or nonsymmetric square matrix 
Ais the kernel operation in iterative solvers. These algorithms also involve linear operations on dense 
vectors. For ef­.cient parallelization of these iterative algorithms, nonzeros of ma­trix Ashould be 
partitioned among processors in such a way that communication overhead is kept low while maintaining 
computa­tional load balance. In order to avoid the communication of vector components during linear vector 
operations, a symmetric partition­ing scheme is adopted. That is, all vectors (including xand yvec­tors) 
used in the solver are divided conformally. Parallel SpMxV is one of the basic parallel reduction algorithms. 
Elements of xvector are the inputs of the reduction and elements of yvector are the outputs of the reduction. 
Matrix Acorresponds to the mapping matrix from input elements to output elements. Hence, any technique 
used in sparse matrix partitioning is also applicable to other reduction problems. For example, analysis 
of large scienti.c datasets involves reduction operations. Vast amounts of datasets are being continuously 
produced by advanced sensors attached to in­struments; such as earth-orbiting satellites [31, 32, 37] 
and medical instruments [1]. In addition, long running scienti.c simulations that periodically generate 
snapshots of their time dependent states [11, 29, 36] are generating unprecedently large volumes of data 
for sci­entists. Typical analysis of such datasets usually involves subseting (to extract the data of 
interest from all the data available), and pro­cessingandtransformingitintoanewdataproduct. Thetypeofdata 
processing usually results in a signi.cant reduction in data volume. Both the existing graph models and 
our hypergraph models can be used to model those reduction operations as it has been used in Ac­tive 
Data Repository (ADR) [12, 24]. ADR is a framework devel­oped at University of Maryland that provides 
support for integrat­ing storage, retrieval and processing of multi-dimensional datasets on distributed 
memory machines. A variant of our .ne-grain hyper­graph model [5, 9] has been used in the query scheduling 
phase of ADR to solve decomposition (workload distribution) problem [13]. Hence the proposed two-phase 
coarse-grain decomposition method is also applicable to the reduction problems together with its nice 
number of message feature. The application of the proposed method to the sparse-matrix par­titioning 
problem results in 2D checkerboard partitioning. The nice property of this partitioning schemeis that 
it bounds the the maximum number of messageshandledby a processorto p+q;2: The parallel SpMxV algorithms 
proposedby Hendricksonet al. [18] and Lewis and van de Geijn [28] for 2D checkerboard partitioning are 
typically suitable for densematrices or sparsematrices with struc­tured nonzero patterns that are dif.cult 
to exploit. The 2D checker­board partitioning schemes proposed in the literature [18, 27, 28, 33] do 
not exploit sparsity for reducing communication volume, and hence, the total communication volume may 
be as high as (p+q;2)mfor an mxmmatrix. In this work, we propose a two-phase checkerboard partitioning 
method based on multi-constraint hyper­graph partitioning for minimizing communication volume while maintaining 
computational load balance. 2. PRELIMINARIES A hypergraph H=(V,N)is de.ned as a set of vertices Vand 
a set of nets (hyperedges) Namong those vertices. Every net nj2 Nis a subset of vertices, i.e., njV. 
The vertices in a net nj arecalledits pins. Weightscanbeassociatedwiththeverticesofa hypergraph. Let 
widenote the weight of vertex vi2V. A K-way vertex partition I=fV1,V2,:::,VKgof His said to be balanced 
if each part Vksatis.es the balance criterion Wk.Wavg(1+"),for k=1,2,:::,K:(1) In (1), weight Wkof a 
part Vkis de.ned as the sum of the weights P of the vertices in that part (i.e. Wk = wi), P vi2Vk Wavg 
=(vi2Vwi)/Kdenotes the weight of each part under the perfect load balance condition, and "represents 
the predetermined maximum imbalance ratio allowed. In a partition Iof H, a net that has at least one 
pin (vertex) in a part is said to connect that part. Connectivity set Ajof a net njis de.nedasthesetof 
partsconnectedby nj. Connectivity .j=jAjjof anet njdenotes the number of parts connected by nj. A net 
njissaidtobe cut (external)if itconnectsmorethanonepart(i.e. .j>1), and uncut (internal) otherwise (i.e. 
.j=1). The set of external nets of a partition Iis denoted as NE. There are various cutsize de.nitions 
for representing the cost of a partition I.Two relevant de.nitions are: cutsize(I)=jNEj (2) X cutsize(I)= 
(.j;1): (3) nj2NE Eq. 2, the cutsize is equal to the number of cut nets. In Eq. 3, each cut net njcontributes 
.j;1to the cutsize. Hence, the hyper­graph partitioning problem can be de.ned as the task of dividing 
a hypergraph into two or more parts such that the cutsize is mini­mized, while a given balance criterion 
(1) among the part weights is maintained. The hypergraph partitioning problem is known to be NP-hard 
[26]. 3. COARSE-GRAIN DECOMPOSITION Row and column coherences are important factors in sparse ma­trix 
partitioningforparallelSpMxV.Columncoherencereferstothe fact that nonzeroes on the same column incur 
the need for the same x-vector entry. Row coherence refer to the fact that nonzeroes on the same row 
incur the contribution of scalar multiplication results to the same y-vector entry. In a partitioning, 
disturbing column co­herence incurs expand communication of x-vector entries, whereas disturbing row 
coherency incurs fold communication of partial y­vector results. Here, expand communication of an x-vector 
entry xjrefers to the multicast operation where a processor, the one that owns the updated xjvalue, sends 
(i.e., expands) the same xjvalue to all processors that have nonzeros at column j. Fold communi­cation 
of partial y-vector results refers to the multinode accumula­tion operation on local, sparse y vectors 
computed by the proces­sors. That is, every processor having at least one nonezero at row i generates 
a partial yiresult after local SpMxV computations, and these processors send their partial yiresults 
to the processor that owns yifor a local accumulation operation to obtain the .nal yire­sult. Note that 
1D rowwise partitioning incurs only expand commu­nication,becauseit respectsrowcoherencebyassigningentirerows 
to processors while disturbing column coherence. In a dual man­ner, 1D columnwise partitioning incurs 
only fold communication, because it respects column coherence by assigning entire columns to processors 
while disturbing row coherence. So in 1D matrix par­titioning, the number of messages handled by a processor 
may be as high as P;1, for a parallel system with Pprocessors. The worst­case total communication volume 
is (P;1)mwords for an mxm matrix, and this worst-case occurs when each row (column) stripe has at least 
one nonzero in each column (row) in rowwise (column­wise) partitioning. The standard graph-partitioning 
approach used for 1D matrix partitioning has some .aws such that the cutsize met­ric doesn t re.ect the 
actual communication volume and it is restricted to symmetric matrices. Our previous hypergraph parti­tioning 
models [6, 7] correctly minimizes the communication vol­ume in 1D matrix partitioning. Other recently 
proposed alternative models for 1D matrix partitioning were discussed in the excellent survey by Hendrickson 
and Kolda [17]. In a recent work [5, 9], we have investigated 2D .ne-grain sparse­matrix partitioning 
which is overlooked in the literature. In this scheme, nonzeroes are allowed to be assigned individually 
to pro­cessors. Since neither row coherence nor column coherence is en­forced, this scheme may incur 
both expand and fold operations so that the number of messages handled by a processor may be as high 
as 2(P;1). The worst-case communication volume is 2(P;1)m words in total. We proposed a .ne-grain hypergraph-partitioning 
model [5, 9] which correctly minimizes total communication vol­ume in this highly scalable 2D partitioning 
scheme. Experimental results showed that this 2D .ne-grain partitioning model achieves drastic reduction 
in communication volume compared to 1D partitioning at the expense of increase in the number of total 
messages. This experimental .nding is expected, because the 2D .ne-grain model has a higher degree of 
freedom than the 1D mod­els in minimizing communication volume since it does not enforce any coherence. 
The 2D checkerboard partitioning scheme widely used for paral­lel SpMxV can be considered as a trade-off 
between 1D partition­ing and 2D .ne-grain partitioning schemes. This scheme respects both row and column 
coherences in a coarse level assuming a 2D processor mesh organization. It respects row coherence by 
assign­ing entire matrix rows to the processors in the same row of the pro­cessor mesh. It respects column 
coherence by assigning entire ma­trix columns to the processors in the same column of the processor mesh. 
Although both coherences are disturbed in processor level, the disturbances are con.ned to the processors 
of the same rows and columns. So, this scheme con.nes the expand operations to the columns and fold operations 
to the rows of the processor mesh. In this way, it reduces the maximum number of messages handled by 
a processor to p+q;2for a pxqmesh of processors. The total communication volume may be as high as (p+q;2)m, 
and this worst-case occurs when each row and column of each submatrix has at least one nonzero. The 2D 
checkerboard partitioning schemes proposed in the literature [18, 27, 28, 33] mainly aim at load bal­ancing 
and they do not exploit sparsity for reducing communication volume. Here, we propose a novel hypergraph-partitioning 
approach for minimizing total communication volume while maintaining com­putational load balance in checkerboard 
partitioning. The proposed method is a two-phase method, in which each phase models either the expand-communicationvolume 
or fold-communication volume. Therefore, we have two alternative schemes for the proposed ap­proach. 
For the sake of simplicity in the presentation we will dis­cuss only one scheme, the one which models 
the volume of expand communication in the .rst phase and then the volume of fold com­munication in the 
second phase. A dual discussion holds for the other scheme. We will discuss the checkerboard partitioning 
of an mxmsquare matrix Aon a pxqprocessor mesh. In the .rst phase, we perform a p-way 1D rowwise partitioning 
of matrix Ausing the column-net hypergraph-partitioning model proposed in our previous works [6, 7]. 
In the column-net hy­pergraph representation HR=(VR,NC)of matrix A, there exist one vertex vi2VRand one 
net nj2NCfor each row riand column cj, respectively. Net njVRcontains the vertices cor­responding to 
the rows which have a nonzero entry in column cj. That is, vi2njif and only if aij6=0. Weight wiof a 
vertex vi2VRis set equal to the total number of nonzeroes in row ri.As we discussed in [6, 7], by partitioning 
the column-net hypergraph into equally weighted vertex parts (maintaining balance condition in Eq. 1) 
so that nets are split among as few parts as possible (min­imizing cutsize metric in Eq. 3), the model 
correctly minimizes to­tal volume of expand communication while maintaining computa­tional load balance 
in rowwise partitioning. A p-way partition IR=fV1,:::,Vpgof HRinduces a p­way rowwise partitioning fR1,:::,Rpgof 
matrix A, where Ra denotes the set of rows corresponding to Vafor a=1,2,:::,p. Without loss of generality 
we assumethat the set Raof matrix rows is assigned to row aof processor mesh for a=1,2,:::,p. This rowwise 
partitioning also corresponds to assigning a maxmrow­stripe Arato row aof processor mesh, where ma =jVaj. 
In a row­wise partitioning, a column is said to be internal if all of its nonze­roes are con.ned to a 
single row stripe. A column is said to be ex­ternal if it has nonzeroes in the rows of at least two row 
stripes. An external column cjwhich has nonzeroes in .jrow stripes will in­cur the expand communication 
of xjto .j;1processors along a column of the processor mesh if all nonzeroes of cjare assigned to the 
same processor-column. Columnwise partitioning to be per­formed in the second phase satis.es this coherence. 
In the second phase, we perform a q-way multi-constraint hy­pergraph partitioning on the row-net representation 
of matrix Afor columnwise partitioning. In the row-net hypergraph representation HC=(VC,NR), there exist 
one vertex vi2VCand one net nj2 NRfor each column ciand row rj, respectively. Net njVC contains the vertices 
corresponding to the columns which have a nonzero entry in row rj. That is, vi2njif and only if aji=60. 
As we discussed in [6, 7], minimizing the cutsize according to Eq. 3 corresponds to minimizing the total 
volume of fold communication of partial yjresults that need to be accumulated. A q-way partition ICof 
HCinduces a q-way columnwise partitioning fC1,:::,Cqgof matrix A, where C(denotes the set of columns 
assigned to pro­cessor column f, for f=1,2,:::,q. So, (IR,IC)determines a checkerboard partition, where 
Raand C(denote the sets of rows and columns assigned to processor Pa(at row aand column f of the processor 
mesh. That is, processor Pa(will own nonzero aij6=0of matrix Aif ri2Raand cj2C(. For computational load 
balancing processors should be assigned roughly equal number of nonzeroes. We introduce the multi-constraint 
hypergraph-partitioning con­cept for handling the load-balancing problem in our two-phase ap­proach. 
The notion of multi-constraint and multi-objective parti­tioning has recently become popular in graph 
partitioning [21, 35] for the parallelization of multi-physics and multi-phase applications. In these 
applications, each constraint effectively corresponds to the computational load of the vertex in a different 
phase of the target parallel algorithm. Hence, maintaining balance on each constraint corresponds to 
maintaining load balance in each phase of the paral­lel algorithm. For our speci.c application, multiple 
weights of the vertices do not correspond to the weights of different phases. In fact they represent 
the computational loads that will be executed concur­rently. In our model, the rowwise partitioning IR=fR1,:::,Rpgperformed 
in the .rst phase already produces row stripes with roughly equal number of nonzeroes. In the second 
phase, each ver­tex viof HCis assigned pweights: wi(a), for a=1,2,:::,p. Here, wi(a)is set equal to the 
number of nonzeroes of column ci in row-stripe Ar a. Note that all internal columns in the rowwise par­titioning 
will have only one nonzero weight, whereas only external columns will have multiple nonzero weights. 
In any case, the sum of the pweights of each vertex will be equal to the total number of nonzeroes on 
the respective column. So, the balance constraint given in Eq. 1 is replaced with pbalance constraints 
that should be maintained simultaneously, i.e., W((a)Wavg(1+"),f=1,2,:::,q (4) for each a=1,2,:::,p: 
P Here, W((a)= wi(a)is the weight of part V(of ICon vi2V. the a-th constraint. Note that W((a)effectively 
denotes the num­ber of nonzeroes assigned to processor Pa(in (IR,IC). Hence, in the second phase, maintaining 
the balance on each weight con­straint during partitioning HCcorresponds to maintaining compu­tational 
load balance on the processors of each row of the 2D pro­cessor mesh. Figures 1 3 illustrate the basic 
steps of our method for 2x2 checkerboard partitioning of a sample matrix. We labeled the ver­tices and 
nets of hypergraphs with letters r and c to denote row and column of a matrix, for simplicity in the 
presentation. In the hypergraph drawings, circles represent vertices, whereas dots repre­sent nets. Figure 
1(b) displays the column-net hypergraph represen­tation HRof a sample matrix Agiven in Figure 1(a). It 
also shows a 2-way partition IRof HR. Figure 2(b) shows the 2-way row­wise partitioning of sample matrix 
Ainduced by IR. Figure 2(b) displays the row-net hypergraph representation HCof matrix A.It also shows 
a 2-way multi-constraint partition ICof HR. In Fig. 2, w9(1)=4and w9(2)=0for internal column c9 of row 
stripe R1, whereas w5(1)=4and w5(2)=2for external column c5. Figure 3 displays the 2x2checkerboard partitioning 
induced by (IR,IC).  4. EXPERIMENTAL RESULTS We havetested the validity of the proposed2D partitioning 
method on various realistic sparse test matrices arising in different applica­tion domains [4, 10, 14, 
15]. Table 1 illustrates the properties of the test matrices listed in the order of increasing number 
of nonzeros. The 2D partitioning results were obtained by running our multilevel hypergraph partitioning 
tool PaToH [8] on the hypergraphs. The 2D partitioning results were compared with the 1D partitionings 
ob­tained by running MeTiS [20] using the standard graph models, and PaToH using the 1D column/row-net 
hypergraph model presented in [6, 7]. For a speci.c Pvalue, P-way partitioning of a test matrix constitutes 
a partitioning instance. For 2D partitioning instances, p without loss of generality we selected p=q=P. 
MeTiS and PaToH were run 50 times starting from different random seeds for each partitioning instance 
and average performance results are dis­played in Figure 4. Communication volume values (in terms of 
the 1111111 123456789 0123456 1 2 3 4 5 6 7 8 9 10 11 12 13  c12 14 15 16  (a) (b) Figure 1: (a) A 
16x16sample nonsymmetric matrix A. (b) Phase 1: 2-way partitioning IRof column-net hypergraphrepresentation 
HRof A. 11 1 1111 512796 38 4 03 5 4162 10 13 5 R1 1 2 15 7  &#38; P12 P11 9 6 14 &#38; P22 P21 11 
3 R2 8 16 12 W1(1) = 12 W2(1) = 11 4 W1(2) = 12 W2(2) = 12 (a) (b) Figure 2: (a) Phase 1: 2-way rowwise 
partitioning of matrix Ainduced by IR. (b) Phase 2: 2-way multi-constraint partitioning IC of row-net 
hypergraph representation HCof A. 11 111 11 516 32798 4 Figure 3: Coarse-grain 4-way partitioning of 
matrix Ainduced by (IR,IC). Table 1: Properties of test matrices number of nonzeros number of name rows/cols 
sherman3 5005 bcspwr10 5300 ken-11 14694 nl 7039 ken-13 28632 cq9 9278 co9 10789 pltexpA4-6 26894 vibrobox 
12328 cre-d 8926 cre-b 9648 world 34506 mod2 34774 .nan512 74752 total 20033 21842 82454 105089 161804 
221590 249205 269736 342828 372266 398806 582064 604910 615774 per row/col min max avg 1 7 4.00 2 14 
4.12 2 243 5.61 1 361 14.93 2 339 5.65 1 702 23.88 1 707 23.10 5 204 10.03 9 121 27.81 1 845 41.71 1 
904 41.34 1 972 16.87 1 941 17.40 3 1449 8.24 number of words transmitted) are scaled by the number 
of rows/columns of the respective test matrices. Average and maxi­mum number of messages handled by a 
single processor are also displayed in this .gure. The percent load imbalance values are be­low 3% for 
all partitioning results displayed in these .gures, where percent imbalance ratio is de.nedas 100x(Wmax;Wavg)/Wavg. 
Figure 4(d) displays the partitioning times on a workstation equipped with a 133 MHz PowerPC processor 
with 64 Mbytes of memory. In terms of total communication volume, Figure 4(a), the pro­posed method produces 
better partitions than the standard graph model on 29 instances out of 42 instances. On the average, 
the pro­posed method produces 23%, 25% and 27% better partitions than the standard graph model on 16, 
32 and 64 processors. The pro­posed method produces comparable results with the 1D hypergraph model in 
terms of communication volume. In the proposed method, the upper bound on the maximum num­berofmessageshandledbyaprocessoris6,10,and14for 
aparallel system with 16, 32, and 64 processors, respectively. As seen in Fig­ure 4(b), these upper bound 
values are reached on 31 instances out of 42 instances. In terms of maximum number of messages handled 
by a single processor, the proposedmethod producesdrastically bet­ter partitions than the 1D graph and 
hypergraph models on every in­stance. On the average, it produces 56%, 62% and 72% better par­titions 
than the 1D graph model on 16, 32 and 64 processors. These relative performance .gures slightly reduce 
to 55%, 60% and 69%, respectively, for the 1D hypergraph model. As expected the perfor­mance gap increases 
rapidly with increasing number of processors in favor of the proposed method. In terms of average number 
of messages handled by a single pro­cessor, Figure 4(c), the proposed method produces better partitions 
than the 1D graph model on every instance except 32-and 64-way partitioningsof BCSPWR10matrix. Itproducesbetterpartitionsthan 
the 1D hypergraph model on 33 instances out of 42 instances. On theaverage,theproposedcoarse-grainmethodproduces54%, 
57%, and 62% better partitions than the 1D graph model on 16, 32, and 64 processors. These relative performance 
.gures considerably reduce to 44%, 42% and 46%, respectively, for the 1D hypergraph model. Note that 
these relative performance .gures also show the relative .gures on the total number of messages. The 
proposed method is approximately 3 times slower than the standard graph model, on the overall average 
(Figure 4(d)). The ex­ecution times of 1D hypergraph model and coarse-grain method are almost the same. 
For P-way partitioning, 1D hypergraph model necessitates P-way partitioning of the hypergraph representation 
of the sparse matrix. However, the proposed method requires two p P-way partitionings of the hypergraph 
representations of the sparse matrix. Since the current PaToH implementations achieve p multi-way partitioning 
by recursive partitioning, two P-way par­titionings is usually faster than one P-way partitioning, even 
though p one of the two P-way partitionings is a multi-constraint partition­ing.  5. CONCLUSION A coarse-grain 
decomposition method was proposed for 2D par­titioning of sparse matrices which has a better upper bound 
on the maximum number of messages handled by a single processor than the existing methods. The proposed 
model reduces the 2D matrix partitioning problem to a multi-constraint hypergraph partitioning problem 
so that partitioning objectives correspond to minimizing communication volume while maintaining load 
balance during re­peated matrix-vector multiplication. The performance of the pro­posed 2D partitioning 
model was tested against 1D partitioning through graph and hypergraph models on a wide range of realis­tic 
sparse matrices. On the average, the 2D partitionings achieved about 59 and 44 percent better decompositions 
than 1D graph and hypergraph models, respectively, in the number of communications required for a single 
parallel matrix-vector multiplication.  6. REFERENCES <RefA>[1] A. Afework, M. D. Beynon, F. Bustamante, A. 
Demarzo, R. Ferreira, R. Miller, M. Silberman, J. Saltz, A. Sussman, and H. Tsang. Digital dynamic telepathology 
-the Virtual Microscope. In Proceedings of the 1998 AMIA Annual Fall Symposium. American Medical Informatics 
Association, Nov. 1998. [2] T. Bultan and C. Aykanat. A new mapping heuristic based on mean .eld annealing. 
Journal of Parallel and Distributed Computing, 16:292 305, 1992. [3] W. Camp, S. J. Plimpton, B. Hendrickson, 
and R. W. Leland. Massively parallel methods for engineering and science problems. Communication of ACM, 
37(4):31 41, April 1994. [4] W. J. Carolan, J. E. Hill, J. L. Kennington, S. Niemi, and S. J. Wichmann. 
An empirical evaluation of the korbx algorithms for military airlift applications. Operations Research, 
38(2):240 248, 1990. [5] U. V. C¸ urek. Hypergraph Models for Sparse Matrix ataly¨ Partitioning and Reordering. 
PhD thesis, Bilkent University, Computer Engineering and Information Science, Nov 1999. [6] U. V. C¸ 
urek and C. Aykanat. Decomposing irregularly ataly¨ sparse matrices for parallel matrix-vector multiplications. 
Lecture Notes in Computer Science, 1117:75 86, 1996. [7] U. V. C¸ urek and C. Aykanat. Hypergraph-partitioning 
ataly¨ based decomposition for parallel sparse-matrix vector multiplication. IEEE Transactions on Parallel 
and Distributed Systems, 10(7):673 693, 1999. [8] U. V. C¸ urek and C. Aykanat. PaToH: A Multilevel ataly¨ 
Hypergraph Partitioning Tool, Version 3.0. Bilkent University, Department of Computer Engineering, Ankara, 
06533 Turkey, 1999. [9] U. V. C¸ urek and C. Aykanat. A .ne-grain hypergraph ataly¨ model for 2D decomposition 
of sparse matrices. In Proceedings of 15th International Parallel and Distributed Processing Symposium 
(IPDPS), San Francisco, CA, April 2001. [10] I. O. Center. Linear programming problems. ftp://col.biz.uiowa.edu:pub/testprob/lp/gondzio. 
  Total Comm. Vol. (words/#rows)  (c) Average number of messages handled by a processor  16-way -Execution 
Time 32-way -Execution Time  25.0 Execution Time (seconds) Execution Time (seconds) 25.0 20.0 10.0 
10.0 5.0 (d) Execution times of decomposition heuristics Figure 4: Average communication requirements 
of the proposed coarse-grain method for 2D partitioning and the existing 1D parti­tioning models. [11] 
C. F. Cerco and T. Cole. User s guide to the CE-QUAL-ICM three-dimensional eutrophication model, release 
version 1.0. Technical Report EL-95-15, US Army Corps of Engineers Water Experiment Station, Vicksburg, 
MS, 1995. [12] C. Chang, A. Acharya, A. Sussman, and J. Saltz. T2: A customizable parallel database for 
multi-dimensional data. ACM SIGMOD Record, 27(1):58 66, Mar. 1998. [13] C.Chang,T.Kurc,A.Sussman,U.V.C¸ataly¨urek, 
and J. Saltz. A hypergraph-based workload partitioning strategy for parallel data aggregation. In Proceedings 
of the Eleventh SIAM Conference on Parallel Processing for Scienti.c Computing. SIAM, Mar. 2001. [14] 
T. Davis. University of .orida sparse matrix collection: http://www.cise.u..edu/ davis/sparse/. NA Digest, 
92/96/97(42/28/23), 1994/1996/1997. [15] I. S. Duff, R. Grimes, and J. Lewis. Sparse matrix test problems. 
ACM Transactions on Mathematical Software, 15(1):1 14, march 1989. [16] B. Hendrickson. Graph partitioning 
and parallel solvers: has the emperor no clothes? Lecture Notes in Computer Science, 1457:218 225, 1998. 
[17] B. Hendrickson and T. G. Kolda. Graph partitioning models for parallel computing. Parallel Computing, 
26:1519 1534, 2000. [18] B. Hendrickson, R. Leland, and S. Plimpton. An ef.cient parallel algorithm for 
matrix-vector multiplication. Int. J. High Speed Computing, 7(1):73 88, 1995. [19] M. Kaddoura, C. W. 
Qu, and S. Ranka. Partitioning unstructured computational graphs for nonuniform and adaptive environments. 
IEEE Parallel and Distributed Technology, 3(3):63 69, 1995. [20] G. Karypis and V. Kumar. MeTiS A Software 
Package for Partitioning Unstructured Graphs, Partitioning Meshes, and Computing Fill-Reducing Orderings 
of Sparse Matrices Version 4.0. University of Minnesota, Department of Comp. Sci. and Eng., Army HPC 
Research Center, Minneapolis, 1998. [21] G. Karypis and V. Kumar. Multilevel algorithms for multi-constraint 
graph partitioning. Technical Report 98-019, University of Minnesota, Department of Computer Science/Army 
HPC Research Center, Minneapolis, MN 55455, May 1998. [22] G. Karypis and V. Kumar. A fast and high quality 
multilevel scheme for partitioning irregular graphs. SIAM Journal on Scienti.c Computing, to appear. 
[23] V. Kumar, A. Grama, A. Gupta, and G. Karypis. Introduction to Parallel Computing: Design and Analysis 
of Algorithms. Benjamin/Cummings Publishing Company, Redwood City, CA, 1994. [24] T. Kurc, C. Chang, 
R. Ferreira, A. Sussman, and J. Saltz. Querying very large multi-dimensional datasets in ADR. In Proceedings 
of the 1999 ACM/IEEE SC99 Conference. ACM Press, Nov. 1999. [25] V. Lakamsani, L. N. Bhuyan, and D. S. 
Linthicum. Mapping molecular dynamics computations on to hypercubes. Parallel Computing, 21:993 1013, 
1995. [26] T. Lengauer. Combinatorial Algorithms for Integrated Circuit Layout. Willey Teubner, Chichester, 
U.K., 199. [27] J. G. Lewis, D. G. Payne, and R. A. van de Geijn. Matrix-vector multiplication and conjugate 
gradient algorithms on distributed memory computers. In Proceedings of the Scalable High Performance 
Computing Conference, 1994. [28] J. G. Lewis and R. A. van de Geijn. Distributed memory matrix-vector 
multiplication and conjugate gradient algorithms. In Proceedings of Supercomputing 93, pages 15 19, Portland, 
OR, November 1993. [29] R. A. Luettich, J. J. Westerink, and N. W. Scheffner. ADCIRC: An advanced three-dimensional 
circulation model for shelves, coasts, and estuaries. Technical Report 1, Department of the Army, U.S. 
Army Corps of Engineers, Washington, D.C. 20314-1000, December 1991. [30] O. C. Martin and S. W. Otto. 
Partitioning of unstructured meshes for load balancing. Concurrency: Practice and Experience, 7(4):303 
314, 1995. [31] The Moderate Resolution Imaging Spectrometer. http://ltpwww.gsfc.nasa.gov/MODIS/MODIS.html. 
[32] NASA Goddard Distributed Active Archive Center (DAAC). Advanced Very High Resolution Radiometer 
Global Area Coverage (AVHRR GAC) data. http://daac.gsfc.nasa.gov/CAMPAIGN DOCS/LAND BIO/ origins.html. 
[33] A. T. Ogielski and W. Aielo. Sparse matrix computations on parallel processor arrays. SIAM Journal 
on Numerical Analysis, 1993. [34] C.-W. Qu and S. Ranka. Parallel incremental graph partitioning. IEEE 
Transactions on Parallel and Distributed Systems, 8(8):884 896, 1997. [35] K. Schloegel, G. Karypis, 
and V. Kumar. A new algorithm for multi-objective graph partitioning. Technical Report 99-003, University 
of Minnesota, Department of Computer Science/Army HPC Research Center, Minneapolis, MN 55455, Sep 1999. 
[36] T. Tanaka. Con.gurations of the solar wind .ow and magnetic .eld around the planets with no magnetic 
.eld: calculation by a new MHD. Jounal of Geophysical Research, 98(A10):17251 62, Oct 1993. [37] U.S. 
Geological Survey. Land satellite (LANDSAT) thematic mapper (TM). http://edcwww.cr.usgs.gov/nsdi/html/landsat 
tm/landsat tm.</RefA>    
			
