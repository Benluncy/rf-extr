
 A Multiresolution Control Method Using View Directional Feature HyungSeok Kim SoonKi Jung KwangYun 
Wohn Department of Computer Science, Department of Computer Engineering, Department of Computer Science, 
KAIST Kyungpook National University KAIST YooSungGoo, Taejon, Korea PookGoo, Taegu, Korea YooSungGoo,Taejon 
+82-42-8693572 +82-53-950-5114 +82-42-869-3532 likidas@vr.kaist.ac.kr skjung@vr. 1. ABSTRACT The technique 
using multiresolution models is one of the research efforts to render complex virtual world in real-time. 
It is desirable for a multiresolution model to preserve the prominent visual features. In this paper, 
we present a mechanism that generates the multiresolution model in real-time. Our model incorporates 
the so-called view dependent features directly into the model generation process. One of the consequences 
is that object silhouette, that is regarded as the primary feature for visual perception, is preserved 
in low-resolution representations. To facilitate the real time generation of multiresolution model, a 
new data structure called multiresolution view sphere is devised. The view sphere maps the directional 
relationship between object surface and the viewpoint. Using the view sphere, coherence of surface orientation 
in the object space is mapped into spatial coherence in the view sphere. In this paper we present fast 
algorithm to construct multiresolution view sphere and rendering method that preserves object silhouette. 
1.1 Keywords Level-of-Detail, Multiresolution model, Realtime rendering, Virtual Reality Permission to 
make digital or hard copies of all or part of this work for personal or classroom use is granted without 
fee provided that eopies are not made or distributed for profit or commercial advantage, and that copies 
bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on 
servers or to redistribute to lists, requires prior specific permission and/or a fee. 01998 ACM l-58113-019-8/ 
98/ 0011/%5.00 kaist.ac. kr wohn@acm.org 2. INTRODUCTION Applications of interactive 3-D graphics including 
virtual reality (VR) and interactive 3-D visualization need instant feedback to the user s intention. 
It requires high speed rendering independent to the complexity of the virtual world they present. With 
the advances in computer technologies, especially graphics the processing time involved in further reduced. 
However, as complex and more realistic simultaneously increased, the hardware, it is certain that image 
generation will be the demand for more virtual environments are efforts to reduce the computing cost 
will be steadily needed. The technique of multiresolution model is one of the research efforts to generate 
the rendered images for very complex virtual worlds, while maintaining real-time display rates[l]. A 
multiresolution model represents the objects at multiple levels of details. The level of detail (LOD) 
of an object reduces the rendering cost by preserving dominant visual features of the original object 
while the perceptual quality of the rendered images is maintained. Since the rendering cost is usually 
proportional to the number of polygons constructing the surface of the model, the multiresolution model 
typically adopts a set of successive coarse-to-detail representation of the original 3D model. Rendered 
shape of a 3D object is determined by the major 3 components of the scene, the object itself, the viewer 
and lights. The object has the geometric shape and the innate color as the visual feature to determine 
rendered shape. These features are presented in the rendered image with the effects of the view and lights. 
Lights determine colors of the rendered image with the innate color material of the object. The view 
determines the visible part of an object and the silhouette. We call these each of these features as 
object-centered feature, light-centered feature and view-centered feature. Traditionally, object-centered 
features have been used as the criteria for generating multiresolution model. Commonly used features 
are the distance between vertices, the face normal, and the curvature[2][3][4]. These features are defined 
on the object space and do not change by the view movement. Notice that most of previous work has dealt 
with the object-centered features. The view-centered features which vary on the fly are paper is a new 
data structure -the multiresolution view related to the view movements, object movements and object deformation. 
The relative direction and position of the object to the view consists the view-centered feature. In 
most of the existing LOD algorithms these features are taken into account by selecting one of the pre-defined 
detail level in the run time. If we select a coarse detail for an object, it means that we choose not 
to preserve the view centered features of the object, while the detail model preserves the features. 
This approach does not well fit into the case of large objects. A large object may occupy a large portion 
of the virtual space in such a way that it should reveal different details for different parts. For example, 
if you stand in front of the Great Wall of China, some part of the wall is close enough to be rendered 
in full detail, but other parts are not. Furthermore, even for a small object, if we put more detail 
on its silhouette than other parts, the resulting image gets closer to the original image with the same 
rendering complexity. Note that, no matter how good an approximation an interpolated shading model offers 
to the actual shading of a curved surface, the silhouette edge of the mesh is still clearly polygonal 
[S]. There are several approaches to apply different criteria to a single object in order to preserve 
important visual feature of the object [6][7]. Suppose an object is comprised with multiple parts. Some 
may be maintained in the high detail level to preserve view-centered features, while the others are assigned 
to lower detailed versions. These approaches give good result for the objects with predefined sets, but 
in general are not extendible to arbitrary objects. To apply these approaches for the view centered features, 
subdivide an object into a large set of small parts, and apply the LOD algorithm independently, finally 
put the parts together seamlessly. Furthermore, such method requires a large storage space. To accommodate 
the view-centered feature in the LOD generation, a method of real-time multiresolution model generation 
is required. A real-time generation method is a process of simplifying a set of specific parts on the 
fly, so that it is possible to generate the appropriate detail representation for time varying view parameters. 
Simplifying the selected polygons is an expensive operation to execute on the fly. Several methods using 
vertex tree have been proposed to make a view centered mesh reconstruction on the fly [8][9][ lo]. The 
vertex tree keeps the record of reducing operation (e.g., edge collapsing) inside the edge of the tree. 
In so doing, a reducing sequence is easily identified. By specifying the parts to preserve, one can build 
a simplified representation preserving those parts. Furthermore, the data structure is simple to construct. 
On the other hand, it has its drawback in regard to the data size of the tree; the tree may have as many 
nodes of the total number of vertices. For a complex object, the size of the vertex tree gets a large 
depth. TO handle view centered features, we should traverse the entire visible nodes in vertex tree that 
is at least as large as the number of visible vertices. One of the contributions of this sphere -to handle 
the mentioned difficulty. Section 3 introduces multiresolution view sphere that is the basic data structure 
of our algorithm. We describe the selection method of detail levels in section 4. In section 5, we deal 
with the re-triangulation to simplify the object on real-time. 3. Multiresolution View sphere When we 
consider the resulting rendered image, object is represented as the color and the silhouette of the visible 
part. From this observation we consider the silhouette is the one of the most important visual feature 
of the rendered image. Silhouette of an object is determined by the relative position between the object 
and the viewer. Silhouette of an object is usually defined as the set of polygons that are orthogonal 
to the viewing direction [lo]. The viewing direction and its directional relationship to the polygons 
are changmg as the view or object moves. Either object or view moves, basically we need to reconsider 
whole parts of the object into the candidates of silhouettes. In other words, without any directional 
relationships to the polygons or vertices, most part of the polygons of the object should be calculated. 
In this paper, we propose a structure of directional relationship on polygons. Using the structure, we 
describe a method to preserve directional feature on the flight. We discuss our method with the silhouette, 
but the proposed method can be applied to any directional relationships. To preserve object silhouette, 
we should preserve the details of polygons with view orthogonal direction and surface boundary of the 
silhouette polygons. Our approach is to simplify the polygons that have similar normal with view direction 
and refine the polygons on the silhouette. We propose view sphere that organizes directional relationship 
of polygons. View sphere captures directional distances between two surface parts as a distance of two 
points in sphere. Surface parts with similar directions are placed m neighbors on view sphere. Silhouette 
preserving simplification sequences on the view sphere are characterized as merging polygons with neighbor 
parts of the view sphere that facing the view, while refining other parts. In this section, we describe 
multiresolution view sphere structure. Figure 1 Mapping polygons onto the view sphere We assume that 
an object is represented as set of polygons. View sphere is a Gaussian sphere that maps the normal direction 
of polygons to the point in the view sphere. Points on the sphere (x,y,z) maps the unit direction vector 
(x,y,z). Polygons that have the direction vector (x,y,z) are mapped into the point (x,y,z) on the view 
sphere. Each point in the view sphere represents the set of polygons that have the direction of the point 
represents. Figure 1 illustrates this mapping. Distance of two direction vectors on the object surface 
is represented as distance of two points in the sphere. Using view sphere, polygons that have similar 
directions with the view are easily identified. If view direction is given as v, visible polygons with 
similar directions are located at the neighbor part of the point v on the surface of the view sphere. 
Simplification using directional feature is merging polygons with neighbors on the view sphere. To give 
efficient access to the view sphere, we have partitioned the sphere into discrete view cells. Each cell 
occupies part of the view sphere. Each cell contains a set of regions, which represents simplified representation 
of polygons mapped into that part of the sphere. In Figure 2, neighboring polygons in the same view cell 
are simplified as a few regions. Original \ \ / Merging result A %?ew Cell Polygon Mesh \__ _/ Figure 
2 Merging polygons in a view cell (Filled polygons in right figure are two regions in the view cell) 
From the view cell structure of view sphere, simple representation of the object is obtained through 
large view cell. If a view cell gets larger, polygons with wide range of directions are merged as a small 
number of regions. Each regions are represented by half-edge structure. Merging two regions is accomplished 
on real-time through the half edge traverse of regions. After merge, resulting region is a single polygon 
with vertices of original boundary of polygon set. Conversely, refined representation of given object 
is generated using small size view cells. We propose multiresolution view sphere to manipulate view cells 
in multiresolution manner. Multiresolution view sphere is defined by successive refinement process on 
the view cells of the view sphere. A refined view cell is generated from the coarse representation of 
view cell by successive subdivision. We call most coarse one as level 0 view cell. Level i-l view cell 
is subdivided into 4 level i view cells (figure 3). Level i view sphere is defined by the level i view 
cells. We define level 0 view sphere by subdividing the sphere into 8 view cells. For easy access of 
the multiresolution view sphere, we construct view quad tree (Figure 4). Each node in the quadtree is 
a view cell, and children of a node are the subdivided cells (Figure 4). We also keep neighbor cells 
for every view cell. Figure 3 Successive subdivision of view cell Figure 4 Quadtree representation of 
multiresolution view sphere In the next section, we describe silhouette preserving snnplification method 
using multiresolution view sphere. 4. Selecting Detail Level Selection of detail level is accomplished 
in two steps. At the first step, detail level for an object is selected using features as distance with 
the view, focus of the view, relative importance of the objects, or etc. We just use the distance with 
the view as the only criteria for selecting object detail level. More sophisticated selection mechanism 
is given in several studies [4,12]. After detail level of an object is determined, detail level of each 
object parts IS adjusted using view direction. In the example of the silhouettes, object silhouette parts 
have more detail. Conversely, parts that have similar normal directions with view direction and back 
faces have less detail. We define a view cell that covers the view direction as a view directional cell 
and view cells with orthogonal nomlal as view orthogonal cells. Our algorithm chooses a simplified cell 
for a view directional one and uses refined cell for view orthogonal ones. We assumed that object simplification 
is occurs when the object is far enough to the viewer. So, polygons in the view orthogonal cells form 
a silhouette of the object to the view. Simplified cell representation for a view directional cell is 
constructed through the merging process. A view directional cell is merged with neighboring cells. In 
the multiresolution view sphere structure, most of view cell forms subdivision connectivity with neighbors. 
So, merging with the three neighbors gives a larger triangular cell. Merged cell forms a coarser detail 
representation of a view directional cell. View orthogonal cells are subdivided into more detail level. 
Remainder view cells retains current level, As a result, a view quad tree is reconstructed as aligned 
to the view directional cell. Figure 5 shows the example of view cell selection. Figure 5 Multiresolution 
view cell selection A view directional cell is selected through traversing the view quad tree structure. 
For a efficient selection of neighboring cells, we keep the neighbor view cells in a counter clockwise 
order. Using the neighbor view cell pointer, reconstructing view quad tree process is represented as 
traversing neighbor pointer from the view directional cell. After simplifying the view directional cell, 
we traverse to next view cells. At first, we choose view cell labeled as 1,2,3 on the figure 6 to visit 
other 12 cells. View cell 1,2,3 is reached by move to the next neighbor of the incoming view cell. As 
mentioned before neighbor cells of a view cell are ordered in counter clockwise order. Repeating twice 
of this following, view cells 1,2,3 can be reached from the center view directional cell. Once the view 
cell 1,2,3 is selected, all the neighbors of the cell 1,2,3 are visited. Through the process, view cell 
1,2,3 are the new parents of the their neighbors in the reconstructed view quad tree. Repeating this 
process in coarser levels we can visit all the nodes in hierarchical order. Figure 6 Selecting neighbor 
view cells The only exception on this process is on the singular points which do not satisfies the subdivision 
connectivity for a view cell. Singular point is one of the vertices on the base view sphere, which does 
not have 6 degrees in a whole level of view cell. This case breaks subdivision connectivity of the view 
cells. View cells with singular point have deficiency for merging and finding its neighbor using described 
procedure. To overcome this deficiency, we choose a neighbor cell with subdivision connectivity as a 
new view directional cell. If the view directional cell is not the cell on the base sphere there exist 
a neighboring view cell with no singular point. For a view cell on the base sphere, we have no need to 
merge to simplify. Because of the simple structure of view sphere, we choose the first neighbor with 
no singular point as new view cell. As a result of detail selection, view cells have wide range of detail 
differences. Most detail part of the object is polygons with orthogonal direction with the view that 
keeps silhouettes of an object. Every simplified polygonal area preserves the boundary of the original. 
So, the overall shape of object silhouettes is preserved, even for sharp edges on the silhouette. In 
this section, 3 level wide detail differences are presented but reconstructed view quad tree gives easy 
selection of more or less detail level for each cell. 5. Triangulation Mechanism Our basic assumption 
on the rendering is there is a hardware that can render concave polygons in real-time. If this hardware 
is given, rendering using the hierarchical view sphere is simple as render each region in the current 
view cells. Unfortunately, most of the currently available hardware is not able to render the concave 
polygons properly. So, we need to retriangulate the polygon regions in the view cells. We propose simple 
retriangulation algorithms to render the view cell in real-time. As mentioned before, polygons in a view 
cell are merged to a few simplified regions. In general, these regions are formed as general concave 
non-planar polygons. At first, we made simplification for these regions. Connected boundary parts of 
the region are simplified as a single edge if the part is neighboring with same region. Resulting regions 
are used in rendering. Given a polygon region, retriangulation method constructs triangle mesh to fulfill 
real-time rendering and close representation with original mesh. To fulfill these requirements, the retriangulation 
process should be fast enough for real-time rendering, and the process should preserve sharp edges of 
the original mesh. (a) case of edge removing    F3 -c$ Edges of candidate bounda y (b) Case of bertex 
removing Figure 7 Cases of retriangulation (dotted lines are virtual edges) There are two cases on the 
merge process when retriangulation is required. The first one is the process of merging two polygons 
by removing single boundary edge. If we assume that all of the polygons are properly triangulated, we 
just put a new virtual edge instead of removed boundary edge. A virtual edge is an edge that constructs 
triangle mesh, but not forms boundary of the region. The second case is the case of a vertex is removed. 
Using boundary and virtual edge, we select vertices neighboring the removed one. Let s call the boundary 
constructed by these neighboring vertices as candidate boundary. All the vertices on the candidate boundary 
have a virtual or boundary edge to the vertex removed. Our algorithm is based on the strategy that constructs 
the triangle using two edges of the candidate boundary (Figure 8). All the vertices on the candidate 
boundary are ordered in a direction of the half edge structure. The basic operation is cut off a set 
of selected boundary vertex by making a set of triangles following the candidate boundary. New triangleCandidate 
boundary Figure 8. Generating new triangles from the candidate boundary For each vertex on the candidate 
boundary, we check if this vertex forms proper triangle. Proper triangle is a triangle that is contained 
in the boundary and does not intersect with other triangles. Let s say current vertex is v,, and the 
previous and next one is v,_~ and v,,]. If v,_], v,, v,+~ forms proper triangle, makes a virtual edge 
between v,_! and v,+~. This action forms a new triangle of vi+ v, and vi+,. After that, remove v, from 
the candidate boundary. This means make vi+, as next vertex of vi_] and vice versa. Verifying the proper 
triangle is simple. We first choose triangle with convex point in the candidate boundary, and test the 
edge intersections of new virtual edge with other edges. We can check convexity of the point using surface 
normal and the cross product of edge vectors. We use normal of the view cell which contains the boundary, 
as approximated value of the plane normal We first project all points to the plane that has normal as 
direction of the view cell. Because all the points have edges with the removed one , triangles that does 
not contain the vertex on the projected plane guarantees that it does not intersect with other one. Following 
above process, we can remove all of the convex points by single scan. If we can remove half of the boundary 
vertices, we can retriangulate the boundary in O(n) traverse time, where n is the number of rendered 
vertices on the given polygon region in the previous frame. The worst case of the proposed algorithm 
is that the convex points are less than other concave points. To resolve these cases we developed two 
pass algorithm. Once a new triangle is formed, we make v,., as current vertex, and repeat it until no 
new triangles is appears. For the concave vertex, we reverse test directions start with vi+]. If we repeat 
this process, we can remove all of the concave points in 2n order time (Figure 9). So, all the retriangulation 
process is done in O(n) to remove single vertex. Figure 9 Retriangulation 6. Experimatal Results (4 
@I cc> Figure 10 Silhouette preserving multiresolution model Figure 10 shows three different details 
on the apple model. (a) is full detail model with 1,704 triangles. (b) is low detail model imposing same 
detail level for whole part of the object with 856 triangles. (c) is silhouette preserving model using 
proposed algorithms with 897 triangles. For a visual impact, (c) gives less visual difference with (a) 
than (b). It means that preserving object silhouette is more effective than ordinary low detail level. 
 (a) Side view of a teapot (b) Teapot from different view  Figure 11 Rendering results for different 
view position Figure 11 illustrates effect of silhouette preserving simphfication results with different 
view. (a) and (b) have 1,357 and 1,478 polygons with the 4,000 polygon of the origmal. The part of the 
body that is simplified in (a), gets more detail when the view move to the position of(b). Figure 12 
shows experimental results for the bunny model with 69,451 polygons. Image (a)-(e) are rendering results 
of detail level 1 to level 5. Image (f)-(j) are rendering results from different view position. Each 
figure illustrates rendering result of silhouette preserving simplification on specified level. As the 
model gets simplified, surfaces that facing the view are flattened by successive merging process while 
silhouette parts are preserved. Average rendering rate are, 3.5 frames/set for level 1 representation 
(17,141 regions) and 12.6 frames/set for level 5 representation (3,392 regions) while 2.5 frames/set 
for the original model. The result shows that simplification cost is small enough to processing for the 
real-time rendering. 7. Conclusion We have proposed an algorithm for multiresolution model generation 
using the view parameter. Proposed algorithm uses the view sphere to organize view directional feature. 
We developed a simple, but effective mesh merging algorithm for real-time mesh simplification using quadtree 
and half-edge structure. We also developed a fast retriangulation algorithm for mesh simplification. 
Our algorithm uses relatively small memory for keeping the object structure. Proposed algorithm is simple 
enough to implement on the high performance hardware. Proposed algorithm assumes that the target surface 
is piecewise-smooth. When the surface is not piecewise-smooth like edges of a saw, simplification of 
polygons using only the directional relationship does not giving sufficient simplification results. Further 
more, when ___._.________.l__l__ ,.,_ _.....^. _... .,._,.,.^^............ ,^____ .._..._.... .. _.. 
simplifying surfaces relatively close to the viewer, the regions on the silhouette of the view sphere 
does not exactly form the silhouette of the object. To solve this problem, we can modify the algorithm 
to reassign regions to appropriate view cells when the view moves and when the other positional relationship 
is under the consideration. Our algorithm is fast enough to deal the region splitting and mergmg in real-time, 
we can replace regions to other proper view cells. For the next work, an efficient replacing mechanism 
for regions as view moves using frame coherence and the combining criteria of positional relationship 
between regions into the structure is required. Basically, the multiresolution view sphere can manipulate 
any directional relationship for the polygons. Simplification method preserving silhouettes or highlights 
generated by light can use the view sphere. Further more, by combining two criteria into one view sphere, 
it is possible to preserve view-centered feature and light centered feature simultaneously. View sphere 
based algorithm has many strong points in dealing directional features, but it does not guarantee the 
number of polygons in simplified model. In the future, the method to guarantee the number of reduced 
polygons should be developed. 8. ACKNOWLEDGMENTS This work is partially supported by STEP 2000 CG/VR 
project and ACRC. (a) Ll -17,141 regions (b) L2 -11,382 regions (c) L3 -6,816 regions (d) L4 -5,036 
regions (e) L5 -3,392 regions .-_I __ . _ ---_ (f) Ll -18,484 regions (g) L2 -12,563 regions (h) L3 
-5,720 regions (i) L4 -4,078 regions (j) L5 -2,820 regions Figure 12 Experimental results 9. REFERENCES 
<RefA>[I] P.S. Heckbert and M. Garland, Multiresolution modeling for fast rendering, Proceedings of Graphics 
Interface 94, pp. 43-50, 1994 PI G. Turk, Re-tiling polygonal surfaces, SIGGRAPH 92, pp. 55-64, 1992 
[3] W. Schroeder, J. Zarge, and W. Lorensen, Decimation of triangle meshes, SIGGRAPH 92, pp. 65-70, 1992 
[4] P. Astheimer and M. Poche, Level-of-detail generation and its application in virtual reality, VRAT 
94, pp. 299-309, 1992 [5] J.D. Foley, A. van Dam, S.K. Feiner and J.F. Hughes, Computer Graphics : Principles 
and Practice, Addison-Wesley Publishing Company, 2 ed., 1990 [6] M. Eck, T. DeRose, T. Duchamp, H. Hoppe, 
M. Lounsbery, and W. Stuetzle, Multiresolution analysis of arbitrary meshes, SIGGRAPH 95, 173-182, 1995 
[7] J. Cohen, M. Vashney, D. Manocha, G. Turk, H. Weber, P. Agarwal, F. Brooks, and W. Wright, Simplification 
envelopes, SIGGRAPH 96, 119-128, 1996 [8] J.C. Xia and A. Varshney, Dynamic view-dependent simplification 
for polygonal models, Visualization 96, pp.327-342, 1996 [9] H. Hoppe, View-dependent refinement of progressive 
meshes, SIGGRAPH 97, pp. 189-198, 1997 [lo] D. Luebke and C. Erikson, View-dependent simplification of 
arbitrary polygonal environments, SIGGRAPH 97, pp. 199-208, 1997 [I I] T.A. Funkhouser and C.H. Sequin, 
Adaptive display algorithm for interactive frame rates during visualization of complex virtual environments, 
SIGGRAPH 93, pp. 247-254 </RefA>
			
