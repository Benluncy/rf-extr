
 A NEGOTIATION PROTOCOL FOR BATCH TASK ASSIGNMENTS IN DYNAMIC LOAD DISTRIBUTION Chin LU and Sau-Ming 
LAU Department of Computer Science and Engineering The Chinese University of Hong Kong Shatin, Hong Kong 
E-mail: {clu, smlau} @cs.cuhk.edu.hk Keywords Dynamic load distribution, Protocol design, Distributed 
resource scheduling, Performance modeling Abstract A dynamic load distribution (LD) algorithm improves 
the per- formance of a distributed system by using current system load information to relocate application 
tasks among processing nodes so that workload imbalance in the system can be smoothed out. In this paper, 
we propose a Guarantee and Reservation Protocol (GR Protocol) which can be used by dynamic LD algorithms. 
The protocol enables batch task assignments, which mean that multiple tasks can be selected for remote 
execution during each single sender-receiver negotiation session. This is in contrast to the commonly 
used single task assignments. Principally, the GR Protocol is used for obtaining the mutual agreement 
between a task sender and a receiver on the appropriate task batch size. The key in this protocol is 
the virtual workload model which ensures coordinated task dispersements within the distributed systems, 
and at the same time avoids processor thrashing. That is, to avoid situations when a receiver node is 
flooded with excessive incoming tasks and thus becomes overloaded. 1 INTRODUCTION In a distributed computing 
system, it is highly likely for some processing nodes to have higher workload than others [6, 9]. It 
is desirable for such workload imbalance to be smoothed out by making use of the spare processing capacity 
of those idle or lightly loaded nodes, so that the average task response time can be minimized. A dynamic 
load distribution (LD) algorithm accomplishes this objective by using the current system load information 
to relocate application tasks among the nodes. Most existing LD algorithms allow only single task assign- 
mcttts, i.e. at most one task can be transferred from a sender to a receiver during each negotiation 
session [I, 2, 3, 8, 101. Obvi- ously, for a highly imbalanced system, many negotiation sessions "'Permission 
to make digital or hard copies of part or all of this work for personal or classroom use is granted without 
tee provided that copies are not made or distributed for profit or cotnmercial advantage and that copies 
bear this notice and the full citation on the first page. Copyrights for components of this work owned 
by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, 
to post on servers or to redistribute to lists, requires prior specific permission and/or a lee.'" (: 
1997 ACM 0-89791-850-9 97 0002 3.50 are needed before the workload imbalance can be smoothed out. There 
are two immediate performance penalties: (1) unneces- sarily high CPU and communication overhead can 
be incurred; and (2) tasks cannot be dispersed fast enough, resulting in large task response times. A 
new LD algorithm has been proposed recently using batch task assignments, which mean that multiple tasks 
can be selected for remote execution during each single sender-receiver negotiation session [5]. This 
approach requires that the amount of workload contained in a task batch, referred to as the task batch 
size, must be determined dynamically during negotiations based on the load states of the negotiating 
nodes. In this paper, we present the Guarantee and Reservation Protocol (GR Protocol), which can be used 
by dynamic LD algorithms employing batch task assignments. Principally, the GR Protocol is used for obtaining 
the mutual agreement between a task sender and a receiver on the appropriate task batch size. The key 
in this protocol is the virtual workload model which ensures coordinated task dispersements within the 
distributed systems, while at the same time avoids processor thrashing [4]. That is, to avoid situations 
when a receiver node is flooded with excessive incoming tasks from multiple sender nodes and. thus becomes 
overloaded. The principle idea of the GR Protocol is that at the initia- tion of the protocol, the sender 
declares the desired amount of workload that it wants to offload, and tries to locate a potential receiver. 
Having received the polling message from the sender, the intended receiver then calculates the amount 
of workload that it can accept from that particular sender, based on its cur- rent workload and the desired 
amount as declared by the sender in the polling message. When a sender node has promised to of road certain 
amount of workload, its workload is reduced ar- tificially. Similarly, when a receiver node has agreed 
to accept certain workload from a sender, its workload is elevated artifi- cially. This is to avoid the 
receiver from being a contender again, unless it still has extra spare capacity. We use the GR Protocol 
to derive a LD algorithm. Perlormance of the algorithm is eval- uated by simulation experiments. In particular, 
the use of batch assignments in reducing processor thrashing is studied. The rest of tile paper is organized 
as tbllows. Section 2 presents the virtual workload model and some basic assumptions. Section 3 describes 
the GR Protocol in detail. Section 4 evaluates the performance of the LD algorithm. Section 5 is the 
conclusion. 2 BASICASSUMPTIONS AND THE VIRTUAL WORKLOAD MODEL We assume an asynchronous distributed 
system with no shared memory. Message passing is the enly means of communication among processing nodes 
Pl, P2, ..., P,n. We also assume that the network is fault free, strongly connected, and messages are 
received in the order sent. The processing speed of a node Pi, denoted as ~,,is expressed in terms of 
the number of unit service demand(SD) that P~ can finish per unit time. One SD equals to the amount of 
computation that requires one second to complete in a particular processing node selected for calibration 
purpose. As the GR Protocol is used for distributed dynamic LD algo- rithms, the workload of each node 
must be made known to other nodes in the system. We assume that each node maintains a local table which 
stores the load states of all other nodes in the system. Maintaining the local load table is not the 
responsibility of the GR Protocol itself, though a particular LD algorithm using the GR Protocol may 
select to piggyback load state information on the negotiation messages of the protocol. Each invocation 
of the GR Protocol is associated with two variables gur and res, where gur stores tile amount of task 
service demands the sender is willing to offload; and res stores tile amount of task service demands 
the receiver is willing to accept. Details of how to obtain and update these two variables will be discussed 
in Section 3. In order to disperse the workload in a heavily loaded sender node as quickly as possible 
and to fully utilize the spare ca- pacity in those lightly loaded receiver nodes, we allow multiple sessions 
of the GR Protocol to proceed simultaneously within a single node. A sender may be negotiating with different 
re- ceivers at one time. Similarly, a receiver may simultaneously negotiate with different senders. Therefore, 
we need to define two variables that maintain the aggregate values ofgur and res inside each node. We 
define the accumulated reservation value of P,, denoted by Ri, as the total service demands that P, has 
currently reserved on behalf of its negotiating senders. In other words, it is the sum of P,'s outstanding 
res values. We also deline tile accumulated guarantee value of Pi, denoted by G,, as the total service 
demands that Pi has currently guaranteed to transfer to its negotiating receivers. In other words, it 
is the sum of Pi's outstanding 9ur values. We define the workload of Pi, denoted by H.',, as the total 
service demands of the tasks residing in it. I'V, represents the time needed lbr Pi to complete all its 
residence tasks. When a task arrives to P,, the service demands of the task is added to IV,. Conversely, 
when the task is assigned remotely, or when the task has completed its execution, that same amount is 
deducted from H",. In order to avoid processor thrashing, if a session has promised to send/receive a 
task batch, further negotiations should take the promised value of the session into consideration. Oth- 
erwise, senders will neglect other potential receivers, but keep approaching the most lightly loaded 
node which may then be flooded with excessive inconfing tasks. For this reason, we de- lille the virtttal 
workload, denoted by I"IV,, as the workload of 1 ), plus its accumulated reservation value subtr;|cted 
by its accumulated guarantee value. That is, VWi = Wi + Ri -Gi. (1) In the GR Protocol, tile workload 
of each node is measured by the virtual workload. For notation purpose, we use a super- script to distinguish 
a sender variable from a receiver variable in the protocol. For instance, res (r) and res (') represent 
the reservation values as measured with reference to receiver P,-'s and sender P~,'s processing speeds, 
respectively. We use virtual workload to measure how busy a node is. According to [1, 7], we can define 
three different load states for the purpose of de- termining whether load distribution effort is needed 
in a node. Table I gives the definitions of the load states in terms of virtual workload. Tup and 7]o~ 
are called upper and lower threshoM, respectively. They are design parameters for the particular LD algorithm 
used. Table I: Load states defined in terms of virtual workload. Load State Meaning Criteria L-load Lightly 
loaded VW~ < Tto,, N-load Normally loaded 7]0,.0 < VWi < Tuv H-load Heavily loaded VWI > Tup The GR Protocol 
can be started either by a sender in an attempt to search for a lightly-loaded (L-load) receiver node, 
or by a receiver in an attempt to search for a heavily-loaded (H- load) sender. The former is said to 
be sender-initiated and the latter receiver-bzitiated [2, I0]. 3 THE GR PROTOCOL We first describe how 
two negotiating nodes can reach a mutual agreement on the batch size. 3.1 Regulation~on Batch Size A 
batch size is derived dynamically according to the guaran- tee value 9ur, the reservation value res, 
and the current virtual workload of the negotiating sender-receiver partners. This sub- section describes 
the procedures for deriving these values. In subsequentdiscussions, P~ and Pr denote the negotiating 
sender and receiver nodes respectively. Deriving the Guarantee Value: The strategy for deriving the guarantee 
value gut has a profound effect on the performance of the GR Protocol. An exceedingly large 9ur prevents 
a sender from sending its surplus workload to other potential receivers, except the one currently in 
negotiation. Consequently, the amount of workload distribution would be limited. In contrast, a small 
gut limits the amount of workload transferred in a single batch, thus deprives tile advantage of using 
batch assignment. In the extreme case, the GR Protocol may degenerate into single task assignments. The 
value of gut is determined as tollows:   { (T,,~, - T~o~). t,~/i,~ gur I'') = rain ii.. + (2) where 
l V + denotes the total service demands of the tasks residing in P, that are eligible for remote execution. 
(The inclusion of the set of tasks that are eligible tor remote execution depends on the particular strategy 
used by the LD algorithm and is not the responsibility of the GR Protocol.) To justify the validity of 
(2), we introduce the first batch size regulation rule below. Rule I: The arrival of a task batch having 
a total service demand a ir) should not boost up the receiver P,. to the H-load state. That is, -~ (VW,. 
+ oe (~) ::~ H-load). r'q According to the load state definitions in Table 1, Rule 1 gives: VW, + ot 
(~) < T~v cr (') < T~ v -VW,.. (3) Again from Table 1, the virtual workload of a receiver node must 
be in the closed interval [0, Tto~]. Thus, the two bounding values of VW,. are: Minimum: VW~ = 0 Maximum: 
VW~ = Tto~o. We then have the two bounding conditions of (3). T~,p if VW~ = 0 c~('+) ~ T,q, -Tto~ if 
VW~ = Tt ..... (4) Since the second case in (4) gives a tighter bound, to take the less optimistic approach, 
sender P~ can expect that the amount of workload that receiver P,- will agree to receive is at most (T~v 
-Tto~), measured in Pr's processing speed. Therefore, 9ur should not be greater than ( T,,p -Tto~ ) 
H~ / t~ ~ , measured in Ps's processing speed. This explains the first criterion in (2). The second criterion 
is obvious as sender P, can guarantee nothing more than it can assign remotely. Deriving the Reservation 
Value: After the polling message from the sender has been received, receiver P,. has to derive the corresponding 
reservation value res based on the piggybackedgur. P,- affirms (3) by assuming that local task arrivals 
before the actual task batch is received can be ignored. This is a reasonable assumption as local task 
arrivals should indeed be rare in a lightly-loaded node. From (3), taking the largest possible value 
of o~ It) , we have a i') = T,,p -V14~. (5) Obviously, P,. should not reserve its processing capacity 
by an amount greater than the sender has guaranteed. We therefore have ,.i ( c~ ('l = T,,r, -VW,. ,'c.,i 
= nlill grit, t, ) . IGIIi,. (6) Note however from (2) that. the maxinmm value of gut is) is (E,p --"['tow) 
" lt,-/It.~, or (T~p --'Flow) when measured with reference to P~'s speed. On the otherhand, since the 
maximum valuc of VII,',. for receiver 1~,- is 7) ..... the minimum value of cv I'') is z, lso (T,,p -7) 
.... ). Thus. gut ('q . I~-~/l~ is always at least ;is stringent as ,~t,-l. Thcrelorc, (6) can be simplified 
as res (') = gut (') ~.//~,-. For receiver-initiated pollings, we have res (~) = T,p -VW~ as 9ur (') 
is not available. Thus, t.es/r) = f gtir (~') /J~/Dr if sender-initiated T~ v -VW~ if receiver-initiated. 
(7) t Deriving the Maximally Allowed Batch Size: The next step in batch size regulation is for P, to 
determine the largest amount of task service demands that are eligible to be transferred to the receiver 
P,-, under the constraint imposed by res (~) . This amount is called the maximally allowed batch size 
and is denoted as 0. To derive 0, we introduce other batch size regulation rules below. Rule 2: The removal 
of a task batch having a total service demands 0(') should not change P~, to the L-load state. That is, 
--, (VW~ -0 (s) =~ L-load). [] Again f(om Table 1, Rule 2 gives: VW,-0 (h > T~o~, 0 (~) < VW.-T,~. (8) 
 Rule 3: The relocation of a task batch having a total serAce demand 0 (') from P, to P~ should not result 
in P, having a lower virtual workload than P~. That is, VW~ -O (~) >_ VW'~ + O (0  o (~)+o (').t,,lu~ 
< vw~-vw~ t 0(, ) < VW, -VWr (9)  -(l +.,/.0 where VI, V~ is the virtual workload of P, as piggybackedon 
the reply message (R.Ack, see the next subsection) from P,- to P~, in reply to P,'s polling, or piggybacked 
on the polling message (R.Polling) from P,- to P,. [] By relaxing inequality (8) and taking the largest 
possible value satisfying (9), we have VIV° -Ttoo, nlin ! (I0) b (V't,_, -VW,)/(I + m/~,). Since P< 
shou Id not transfer more workload than Pr has reserved, we therelbre have 0(,) < res(-I . t]_L (11) 
/J., 0 I~<l is taken to be the largest integer value satisfying (10) and (I I). From (7), the largest 
possible value of res (r) is T<, r, (receiver-initiated negotiation with VI-V,. = 0). It follows from 
(11) that thc largest value of 0/<) is T,,p. t4r/lJs. In other words, -l',, I, it, is tim tipper bound 
on the batch size when measured in SD units. The higher a recewer's processing speed, the larger tile 
task batch possible for it. 3.2 Message Types The seven message types used in the GR Protocol are defined 
below. Messages sent by senders and receivers are prefixed with "S" and "R", respectively. Fields after 
the mark "--" are message contents. The negotiating sender and receiver nodes are denoted as Ps and P,- 
respectively. S.Polling --gur 0) The (sender-initiated) polling message a sender sends to an intended 
receiver to ask for its task transfer consent. The data field 9ur (~) is the guarantee value promised 
by the sender. R.Ack =-gur (~) + res(r) -t-VW~ The acknowledgment message the receiver replies to the 
sender to confirm reservation of processing capacity and acceptance of task transfer request. The first 
data field is the guarantee value 9ur (~'). The second field is the reservation value res (~) . The third 
field is the virtual / workload of the receiver, VW~. [ R.Nack-- gurO) + LS~ [ The negative acknowledgment 
message the receiver replies to the sender for refusing the sender's transfer request. The first field 
returns 9ur (~) to the sender. The second field, LS~, is the piggybacked load state (H-load / N-load 
/ L-load) of the receiver. IS.Transfer-- f2 + res(r) + LS~ The task transfer message the sender sends 
to the receiver. It contains the task batch ~, the reservation value res ('), and the new load state 
LS~, of the sender after the removal of the task batch. [ S.Release ---,'es (~) + LS~ ] The message sent 
by the sender to request the negotiating receiver to release the reservation value res("). In some occasions, 
the sender may fail to compose a task batch for remote transfer to the receiver, even after the consent 
from the receiver has been received. This message is then needed so that the reserved capacity on the 
receiver side can be released. The first field is the reservation value and the second field is the piggybacked 
current load state of the sender. R.Polling _= res (r) + VII,'~ The (r_eceiver-initiated) polling message 
a receiver sends to an intended sender to request for surplus workload from the sender. The two data 
fields are the reservation value and the virtual workload of the receiver. S.Nack --res (~l + LSs ] The 
negative acknowledgment mes- sage the sender replies to the receiver so as to reject a receiver- initiated 
polling request. The two data fields are the reservation value and the current load state of the sender. 
3.3 Details of the GR Protocol As both senders and receivers can initiate a negotiation session, the 
GR Protocol actually consists of two component protocols. The sender-initiated protocol is initiated 
by a node in the H- load state, being triggered by a h)c,iI task arrival. The receiver- initiated protocol 
is initiated by a node in the L-h)ad state, being triggered by a task completion. Figure I shows the 
state di:lgram H-Larad + h~c'al task arrivals eived ~R.Nack received Figure 1: State diagram of sender-initiated 
negotiation protocol I sender side. of the sender-initiated protocol on the sender side. A sender starts 
the protocol by calling the event handler Sender-Poll shown below. Event-Handler: Sender-Poll Triggering 
Event: H-Load state + local task arrivals Current State: SENDER_INITIATION Executor: P,  Begin Select 
target node P~; 9ur (') ~ min W+ Send S.Polling = 9ur (') [ to P,.; G~ ~-- G~ + gut (~) ; VW~ ~ W, + 
Rs-G~; End Sender-Poll first selects a target node which is likely to be a receiver by looking up the 
local load table. (Polling target selection is conducted by the location policy of the particular LD 
algorithm used and is not the responsibility of the GR Proto- col. This ensures flexibility of the GR 
Protocol so that it can be used with different LD algori_thms.) It then derives the guarantee value and 
composes the S.Polling message, which is sent to the intended target node. Included in the message is 
the guarantee value. The accumulated guarantee value and the virtual work- load are then updated accordingly. 
Afterwards, the sender enters the SENDER_INITIATION state to wait for the polled node's reply. The polled 
node either replies the sender's polling with R.Ack or R.Nack. The former indicates that the node agrees 
to accept some tasks from the sender, and that it has reserved some processing capacity on behalf of 
the sender. The latter means that the node rejects the sender's polling request. If R.Ack is received, 
the sender switches to the SENDER_ACTIVE state by calling the event handler Sender-Receive-Ack shown 
below. Event-Handler: Sender-Receive-Ack Triggering Event: R.Ack received Current State: SENDER_ACTIVE 
Executor: P~ Begin G,~ 6.-- Gs --gur (0 ; VW, ~ W, + R~ -G.,; (VW,-~o~) 0 (') <-- min (VW'~ -vw~)l(1+ 
~,,1~,.) res (0 " IJrllJs Select tasks to be included in task batch f2; if ~ is empty i i Send I S.Release 
= res (~) + LS, I to P~; else W~ 6-- W~ - batch size ; VW~ +-- W, + R~, - G,; Transfer ~ to P~: S.Transfer 
_~ f2 + res(~) + LS~ ] endif I End The sender releases the guarantee value, updates its virtual workload, 
and determines the maximally allowed batch size. The sender then tries to compose the task batch. If 
it fails, it sends the S.Release message to the polled node to request it to release the reservation 
value. Otherwise, the sender sends the S.'IYansfer message which contains the task batch, piggybacked 
with the reservation value and the new load state of the sender, LS~. The reservation value is needed 
by the receiver node to update its accumulated reservation value and virtual workload. LS~ helps to update 
the receiver's local load table. After sending S.Transfer or S.Release, the sender switches to the CLOSE 
state and the sender side of the sender-initiated pre'ocol completes. On the other hand, if R.Nack is 
receivc-I from the polled node, the sender switches to the SENDER_ABORT state by calling the event handler 
Sender-Receive-Nack, which updates the accumulated guarantee value and virtual workload. The sender then 
enters the CLOSE state as shown below. Event-Handler: Sender-Receive-Nack "l¥iggering Event: R.Nack received 
Current State: SENDER_ABORT Executor: P, Begin G~ 6- Cl~ -gurtS); I/IV, ~ I,~/'~ + R~ -G,; End Figure 
2 shows the state diagram of the sender-initiated proto- col on the polled node side. After the polled 
node has received the S.Polling message, it switches to the RECEIVER_ACTIVE state by calling the event 
handier Receiver-Receive-Polling shown below.  Event-Handler: Receiver-Receive-Polling Triggering Event: 
S.Polling received Current State: RECEIVER_ACTIVE Executor: P,. Begin if receiver not in L-load Send 
[R.Nack~gu, "l~' +L.b', ItoP, ; else res (r) 6-- gut (0 r; q I  Send R~ +-- R~ + ,'es (~) ; VW,. ~-W~ 
+ R~ -G~;  endif End  ~eceived~ i . , N~.Transferreceived t -.-, Nack ~nt Figure 2: State diagram 
of sender-initiated negotiation protocol --polled node side. The event handler first checks the load 
state of the polled node. If the node is in the L-load state, it regards itself as a receiver and derives 
the reservation value. It then replies the polling sender with the R.Ack message and updates its accumu- 
lated reservation value and virtual workload accordingly. The receiver then enters the RECEIVER_ACTIVE 
state. On the other hand, if the node is not in the L-load state, the message R.Nack will be sent to 
the polling sender to reject its transfer request. The polled node then enters the CLOSE state. When 
S.Transfer is received, the receiver switches to the TRANSFER_COMPLETE state by calling the event handler 
Receiver-Receive-Transfer shown below. Event-Handler: Receiver-Receive-Transfer Triggering Event: S.Transfer 
received Current State: TRANSFER_COMPLETE Executor: P,. Begin H~ +- R,. --r~:s(~); Put the task batch 
~ to task queue; Her ~-l V,. + Workload of [2; VIV,. {- IV,. + li,, -G~; End Upon receiving S.'lYansfer, 
the receiver releases the reserva- tion value by updating the accumulated reservation value and the virtual 
workload. The tasks contained in the task batch are put in its task queue lot processing. The receiver 
ends the protocol by entering the CLOSE state. O11 the other hand, if S.Release is received, the receiver 
en- ters the TRANSFER_ABORT state by calling the event handler Receiver-Receive-Releasein which the receiver 
simply releases tile reservation value, updates the virtual workload, and com- pletes tile protocol by 
entering tile CLOSE state as shown below. Event-Handler: Receiver-Receive-Release Triggering Event: S.Release 
received Current State: TRANSFER_ABORT Executor: P~ Begin R~ +-- R~ -res (r) ; I/'W~ +-- PV~ + R~-G~; 
End The receiver-initiated protocol is very similar to the sender- initiated protocol described above. 
Due to space limit, we do not give the details. The basic idea is that when a node finds itself in the 
L-Load state, it starts the protocol by sending the R.Polling message to the sender node selected (by 
the location policy of the LD algorithm used). Contained in the message is the reservation value and 
the current virtual workload of the receiver. The former is needed by the sender to determine the maximally 
allowed batch size. The latter allows the sender to estimate the load state of the receiver after transferring 
the task batch, and updates the local load table accordingly. The polled node, after receiving the polling 
message, checks its load state. If the node is in the H-load state, the node either composes a task batch 
and thus replies with the S.Transfer message, or replies with the S.Release message. Otherwise, the node 
replies with S.Nack. 4 PERFORMANCE EVALUATIONS We u.." the GR Protocol to derive a LD algorithm called 
GR.bo~.ch. It employs Shivaratri and Kmeger's [8] symmet- rically initiated location policy to maintain 
the local load table and to identify polling targets. We evaluate the perlormance of GR.bateh by simulation 
experiments. Another algorithm, labeled as SI(.single, is also studied so as to provide compara- tive 
results. This algorithm is based on single task assignments. It also adopts Shivaratri and Kmeger's location 
policy. This algorithm has proved to be highly successful [3, 8]. 4.1 Simulation Model We assume that 
task arrivals in a node Pi have an independent Poisson distribution with mean arrival rate A,. The mean 
arrival rates of the m nodes in the system (i.e. Aa,A_,,...,A,,,) are themselves having a log normal 
distribution specilied by ( A, 6), where ), and 6 are the mean and standard deviation respectively. The 
distribution characterizes the workload imbalance in the system. The standard deviation. 6", is called 
the imbalance factor. We define thrashing coefficient, ~, for quantitative measure- ment of the level 
of processor thrashing exhibited in a system: " ~( 1¢,)2_,n( R): _ = ~ (12) = 17," A" where /(, is the 
total number of times that node P, has been polled;/~" is the mean of A',, for 1 < i < ,,,. i.e. ~-~',"=~ 
I,',/,,,. Note that o-~,- is the standard deviation of the variable Ix',. Thus c is the coeflicient oi" 
variation of A',. Intuitively, c measures the degree of dispersion of polling activities. A large ~ signilies 
that most pollings were received by a li:w nodes. This itnplies a high level of processor thrashing. 
 4.2 Simulation Results and Analysis Figure 3 shows the performance comparisons between the two algorithms. 
Observation 1: GR.batch and SK.single have identical per- formance at low system workload. Figure 3(a) 
shows that GR.batch and St(.single have identical performance in terms of mean task response time up 
to system load 0.63. From Figure 3(c), the average batch size of GR.batch at low system load is very 
close to 1. This means that GR.batch at low system load is roughly identical to single task assignment. 
  Observation 2: GR.batch provides a higher performance saturation point than that of SK.single. Figure 
3(a) shows that beyond system load 0.8, St(.single be-comes saturated and its performance degrades exponentially. 
GR.batch, however, is not saturated until 0.9. As Fig-ure 3(b) shows, GR.batch has a higher net CPU utilization 
than SK.sirzgle at high system load. The difference in net CPU utilization means that with SK.single, 
the CPU capacity of some lightly loaded nodes is not as fully used as GR.batch. Since GR.batch can utilize 
the system capacity more fully than that of SN.single, the better average response time is easy to understand. 
+Observation 3: GR.batch exhibits a lower level of thrashing throughout the whole range of system load. 
Figure 3(d) shows the variation of thrashing coefficient. As sys- tem load increases, SK.single exhibits 
a decreasing thrashing coefficient until system load 0.8, beyond which the thrashing coefficient remains 
relatively stable (~ ~ 1). GR.batch also ex- hibits a decreasing thrashing coefficient at low system 
load until the trough at around 0.62 is reached. After the trough, thrashing coefficient increases gradually 
until it becomes rather stable at system load 1.1. GR.batch always exhibits a lower thrashing coefficient 
than SN.single. At high system load, the thrashing coefficients of the two algorithms converge. At low 
system load, most nodes are lightly loaded and thus are candidates of being a receiver. There are only 
a few sender nodes. Tile probability that a sender node being bound to a particular receiver node is 
intrinsically high. This is because the receiver has enough spare capacity to serve the sender. Therefore, 
at low system load, there are only a few actual receiver nodes to which sender-initiated pollings are 
targeted. This explains tile high thrashing coeflicient exhibited by both algorithms at low system load. 
If a sender still has some surplus workload after oflloading a task batch, it starts another sender-initiated 
polling. In the case of S N.single, it is very likely Ibr the sender to try the original receiver because 
the receiver is expected to possess some more spare capacity as only a single task has been transferred. 
In contrast, a sender node using GR.botch is more likely to search lot a different receiver after a batch 
transfer to a receiver. This is because the task batch may be large enough to consume all tile spare 
capacity of the receiver node and the GR Protocol allows the sender to estimate tile new load state of 
the receiver. Consequently, the new polling session will be directed to a different node. Thcrelorc. 
scnder-initLated pollings in (,'H.lu~lch are more distributed and thus tile lower thrashing coclticicnt 
compared with .b'A'. ~mgl,.. 100 I00 ' ' T 1 , NO ¢J5 ....... ~--61} S K.single ~...... i / t)O 85 40 
J,,t,:h i 20 171 ............ ! O O.5 0.6 0.7 ().8 0.9 System Load (at Response Time I0 : GR,balch 
~ I SK.single ....... i ............. I I. I =~ .4 ..................÷.........................................................i 
........................................ ~ ............................................i/7 iii . . . 
. . . . . . . . . . . .... i o 0 0.2 0.4 0.6 O.N 1.2 1,4 (c) Batch Size SysIcIT1 Load / SK's'"g'~ i 
...... i 8(1   [J i 75 70 / ...............................................................................................i 
................ 0.7 O.g O. t) 1 I. 1 1.2 1,3 1.4 Sys.t enl Lo~.td (b) Net CPU Utilization 3.5 ,,. 
: '~" "~.~ 2.5 '~', t.= 7 k,, i [ GR bate l i SK.single! ....... i L ~~ 1.5 " "~"-. ...... i 0.5  0.2 
0.4 0.6 0.8 1 1,2 1,4 Syslcnl Load (d) Thrashing Coefficient ,~ Figure 3: Pertbrmance comparisons between 
GR.batch and SN.single. The decreasing thrashing coefficient by both algorithms can be attributed to 
two reasons. (1) As system load increases, the number of sender nodes in the system increases. More receiver 
nodes are needed to serve the sender nodes. By its definition, thrashing coefficient obviously decreases. 
(2) As system load increases, the spare capacity of those potential receivers dimin- ishes, while the 
arnount of surplus tasks in senders increases. The probability that a receiver becomes saturated and 
thus a sender node has to search for another transfer partner grows. Sender-initiated pollings are more 
distributed and hence the decreasing thrashing coefficient. CONCLUSION We proposed the Guarantee and 
Reservation Protocol (GR Proto- col) which can be used by dynamic load distribution algorithms based 
on batch task assignments. The GR Protocol conducts mutual negotiations in a sender-receiver negotiation 
session to obtain the appropriate task batch size. This is achieved by using sender guarantee and receiver 
reservation. Multiple negotiation sessions are allowed to conduct in a node simultaneot,sly and the necessary 
coordination is ensured by the use of 111(2 vintml workload model. The GR Protocol yields fast dispersements 
of congested workload, and at the same time avoids processor thrashing. Based on the GR Protocol, a dynamic 
load distribu- tion algorithm is derived. Simulations show tlmt this algorithm provides signilicant perlormancc 
improvement over single task assignments in terms of mean task response time and the level of processor 
thrashing across a wide range of s','stem workload. REFERENCES <RefA>[11 D. L. Eagcrand E. D. Lazowska. Adaptive 
h),id r,l~anng in honu)gcncous distributed systems. II'~I'JE Tron.L .%'{)lilt" Eng., 12(5), May 1986. 
[2] D. L. Eagerand E. D. Lazowska. A comparisonof receiver- initiated and sender-initiated adaptive load 
sharing. Per-formance Evaluation, 6:53-68, 1986. [3] C. Lu and S.-M. Lau. A performance study on load 
bal- ancing algorithms with process migration. In Proc. IEEE TENCON 1994, pages 357-364, Aug. 1994. [4] 
C. Lu and S.-M. Lau. An adaptive algorithm for resolv- ing processor thrashing in load distribution. 
Concurrency: Practice and Experience, 7(7):653-70, Oct. 1995. [5] C. Lu and S.-M. Lau. An adaptive load 
balancing algorithm for heterogeneous distributed systems with multiple task classes. In Proc. 16th hzt. 
Conf Distributed Computing Systems. Hong Kong, May 1996. [6] J. R. Lyle and C. Lu. Load balancing from 
a unix shell. In Proc. 13th C(mf Local Computer Networks, pages 181- 183. Oct. 1988. [71 L. M. Ni, C. 
W. Xu, andT. B. Gendreau. Drafting algorithm -a dynamic process migration protocol for distributed sys- 
tems. In Proc. 5th hzt. Conf Distributed Computing Sys- tems. pages 539-546. IEEE, 1985. I,~1 N. G. Shivaratri 
and P. Kmeger. Two adaptive location policies tot gh)bal scheduling algorithms. In Proc. lOth Int. Con[. 
Distributed Computing Systems, pages 502-509, May 199(l. [t)] M. M. Thcimer and K. A. Lantz. Finding 
idle machines in a workstation-based distributed systems. IEEE Trans. Sqftm Enx., 15( I I ), Nov. 1989. 
[10l Y.T. \V, lng alld R. J. T. Morris. Load sharing in distributed %,.,,Ictus. II'El- Trans. Coml,Ut., 
340), Mar. 1985.  </RefA>
			
