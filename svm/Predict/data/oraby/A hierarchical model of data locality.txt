
 A Hierarchical Model of Data Locality * Chengliang Zhang Chen Ding Mitsunori Ogihara Computer Science 
Department University of Rochester 734 Computer Studies Bldg Rochester, NY 14627 {zhangchl,cding,ogihara}@cs.rochester.edu 
 Yutao Zhong Computer Science Department George Mason University MSN 4A5 4400 University Drive Fairfax, 
VA 22030 yzhong@cs.gmu.edu Youfeng Wu Programming Systems Research Lab Intel labs 2200 Mission College 
Blvd Santa Clara, CA 95052 Youfeng.wu@intel.com Abstract In POPL 2002, Petrank and Rawitz showed a 
universal result .nding optimal data placement is not only NP-hard but also impos­sible to approximate 
within a constant factor if P = NP. Here we study a recently published concept called reference af.nity, 
which characterizes a group of data that are always accessed together in computation. On the theoretical 
side, we give the complexity for .nding reference af.nity in program traces, using a novel reduction 
that converts the notion of distance into satis.ability. We also prove that reference af.nity automatically 
captures the hierarchical lo­cality in divide-and-conquer computations including matrix solvers and N-body 
simulation. The proof establishes formal links between computation patterns in time and locality relations 
in space. On the practical side, we show that ef.cient heuristics exist. In particular, we present a 
sampling method and show that it is more effective than the previously published technique, especially 
for data that are often but not always accessed together. We show the effect on generated and real traces. 
These theoretical and empirical results demonstrate that effective data placement is still attainable 
in general-purpose programs because common (albeit not all) lo­cality patterns can be precisely modeled 
and ef.ciently analyzed. Categories and Subject Descriptors D.3.4 [Programming Lan­guages]: Processors 
optimization, compilers General Terms Theory, Algorithms, Performance Keywords Hierarchical data placement, 
program locality, refer­ence af.nity, volume distance, NP-complete, N-body simulation 1. Introduction 
Data placement becomes increasingly important to programming language design as programmers routinely 
improve performance or energy ef.ciency by better utilizing the cache memory in proces­sors, disks, and 
networks. Different memory levels come with dif­ferent sizes and con.gurations. The hardware con.guration, 
how­ * This research has been supported by DOE (DE-FG02-02ER25525) and NSF (CCR-0238176) Permission to 
make digital or hard copies of all or part of this work for personal or classroom use is granted without 
fee provided that copies are not made or distributed for pro.t or commercial advantage and that copies 
bear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on 
servers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL 06 January 
11 13, 2006, Charleston, South Carolina, USA. Copyright c &#38;#169; 2006 ACM 1-59593-027-2/06/0001...$5.00. 
 ever, may not be fully visible to a user. In addition, a program may run on machines with different 
con.gurations. As the program­ming for speci.c memory levels becomes increasingly untenable, solutions 
for hierarchical data placement are developed in separate application domains including matrix solvers 
[16], wavelet trans­form [7], N-body simulation [30], and search trees [3], where the program data are 
recursively decomposed into smaller blocks. By exploiting the inherent locality in computation, hierarchical 
data placement optimizes for any and all cache sizes. While most stud­ies examined speci.c computation 
tasks, in this work we show that a general model can be used to derive the hierarchical data place­ment 
from program traces without user s knowledge of the meaning of the program. While the data placement 
is sensitive to a machine, it is .rst and foremost driven by the computation order. In fact, any layout 
is perfect if the program traverses the data contiguously. Given an arbitrary data access trace, we say 
a group of data have reference af.nity if they are always accessed together in the trace [44]. The closeness 
is parameterized by the volume distance (denoted by k), which is the volume of data between two accesses 
in a trace. We also call it the reuse distance if the two accesses are to the same datum. Changing k, 
reference af.nity gives a hierarchical partition of program data. We show an example here and give the 
formal de.nitions in the next section. Figure 1 (a) shows a trace, where different letters represent 
accesses to different data, and ... means accesses to data other than those shown in the trace. w x w 
x u y z . . . z y y z v x w x w . . . (a) Example data access seqeuence over time k  8 3 2 1 0  wxyzu 
v (b) The reference affinity gives a hierarchical relation in data Figure 1. An example reference af.nity 
hierarchy Reference af.nity gives a hierarchical relation shown as a den­drogram in Figure 1(b). The 
top of the hierarchy (k =8) is the set of all data {u, v, w, x, y, z}, which have the weakest af.nity. 
The group {w, x, y, z} have stronger af.nity than they do with u and v (when k =3). Inside this group, 
{w, x} have closer af.nity (k =2), so do {y, z}. At the bottom of the hierarchy (k =0), each data element 
becomes an af.nity group. The af.nity hierar­chy enables the hierarchical data placement, which is simply 
the order (or its reverse) of the leaves in the dendrogram. The hierar­chical placement improves the 
spatial locality for all cache con.gu­rations. When a data element is loaded, the following computation 
accesses more likely the neighboring data than the distant data. As shown by this example, reference 
af.nity converts a computation trace to a hierarchical data layout. In this paper we .rst present two 
theoretical results on reference af.nity. The .rst is the complexity of .nding reference af.nity. We 
give polynomial-time algorithms for cases k =1and k =2.We prove that the problems are either NPC or NP-hard 
when k = 3. Second, we prove that reference af.nity automatically captures the hierarchical locality 
in divide-and-conquer type computations including blocked matrix solvers and N-body simulation. The proof 
holds even when data are divided into non-uniform sections and traversed in any order. Despite of the 
theoretical complexity, ef.cient heuristics exist. We present a new analysis method based on sampling. 
We show through experiments that the new technique is more accurate than the previously published approximation 
method [44], especially for partial reference af.nity where a group of data is often but not always accessed 
together. We show two new uses of reference af.nity. The .rst is .nding hierarchical data layout in recursive 
matrix multiplication, and the second is improving the code layout of seven SPEC 2000 applications. The 
volume distance has been dif.cult for theoretical analy­sis because data may appear in different orders 
with different fre­quencies while still yielding the same volume distance. It raises interesting problems 
different from those in traditional graph and streaming domains. In this work, we present two new proof 
tech­niques that link between the volume distance of memory references and the af.nity of data groups. 
The .rst contains a reduction that converts the problem of data volume into formal logic. The sec­ond 
contains a construction that connects the recursive structure of computation and the hierarchical relation 
of data. In POPL 2002, Petrank and Rawitz showed a universal result .nding optimal data placement is 
not only NP-hard but also im­possible to approximate within a constant factor if P =NP.This work shows 
a .ner partition. On the one hand, good data placement is possible because reference af.nity exists in 
most programs. This explains the effective heuristics developed by many earlier studies. On the other 
hand, the optimal placement is still unreachable for ar­bitrary access patterns. The paper shows a division 
between a few solvable or approximable sub-cases and the general case governed by the Petrank-Rawitz 
theorems.  2. Reference Af.nity An address trace or reference string is a sequence of accesses to a 
set of data elements. If we assign a logical time to each access, the address trace is a vector indexed 
by the logical time. We use letters such as x, y, z to represent data elements, subscripted symbols such 
as ax,ax . to represent accesses to a particular data element x,and the array index T[ax]to represent 
the logical time of the access ax on a trace T. We use sequence and trace interchangeably. DEFINITION 
1. Volume distance. The volume distance between two accesses, ax and ay, at times T[ax]and T[ay])inatrace 
T,is one less than the number of distinct data elements accessed in times between (and including) T[ax]and 
T[ay]. We write it as dis(ax,ay). According to the de.nition, dis(ax,ax)=0and dis(ax,ay)= dis(ay,ax). 
In addition, the triangle inequality holds dis(ax,ay)+dis(ay,az)= dis(ax,az), because the cardinality 
of the union of two sets is no greater than the sum of the cardinality of each set. For example, in the 
trace abbbc, the volume distance from the .rst a and to the last c is 2 and vice versa. The symmetry 
is important because the closeness is the same no matter which access happens .rst. Next we de.ne the 
condition that a group of data elements are accessed close together. DEFINITION 2. Linked path. A linked 
path in a trace is param­eterized by the volume distance k. There is a linked path from ax to ay (x = 
y) if and only if there exist t accesses, ax1 , ax2 , ..., axt , such that (1) dis(ax,ax1 )= k. dis(ax1 
,ax2 )= k . ... . dis(axt ,ay)= k and (2) x1,x2,...,xt, x and y are different (pairwise distinct) data 
elements. In words, a linked path has a sequence of hops, each hop lands on a different data element 
and has a volume distance no greater than k.Wecall k the link length. We will later restrict the hops, 
x1,x2,...,xt, to be members of some set S and say that there is a k-linked path from ax to ay with respect 
to set S. For example consider the .rst part of the trace in Figure 1(a), wxwxuyz. The closeness between 
the .rst w and the last z is de.ned by the linked path with a minimal k, which is the path that jumps 
to the second x and then steps through each one in uyz. Each hop has a volume distance of 1 so is the 
link length. If we restrict the path to the set {w, x, y, z}, the link length becomes 2 since any path 
has to jump over u. DEFINITION 3. Reference af.nity group. Given an address trace, a set G of data elements 
is a reference af.nity group (i.e. they have the reference af.nity) with the link length k if and only 
if 1. for any x . G, all its accesses ax must have a linked path from ax to some ay for each other member 
y . G, that is, there exist different elements x1,x2,...,xt . G such that dis(ax,ax1 )= k.dis(ax1 ,ax2 
)= k.....dis(axt ,ay)= k 2. adding any other element to G will make Condition (1) impos­sible to hold 
  Reference af.nity is a communal bond. All members of an af.n­ity group must be accessed in the trace 
wherever one member is accessed. Each access is linked to some access of every other mem­ber in the group, 
and the linked path can go through only members of the group. We can now explain the hierarchy in Figure 
1 fully. When k =8, any access in the trace is linked to any other access, so all data belong to one 
group. When k =0, no two accesses can be linked, so each data element is a group. Now consider the group 
{w, x, y, z}, which are access in both parts of the trace. Its link length is 3 because in trace zyyzvxwxw, 
no linked path can wade from the .rst z to an access of x without hopping through four dif­ferent data 
elements around v. The path cannot land on the second z because it starts from z. Neither can it land 
on v because it is not a member of the group. When we reduce k, the group {w, x, y, z}is partitioned 
into two sub-groups with closer af.nity. The initial purpose of this complex de.nition is for reference 
af.nity to give a unique and hierarchical partition of data, as shown in the following three properties 
[44]. 1. Unique partition Given an address trace and a .xed link length k, the af.nity groups form a 
unique partition of program data. 2. Hierarchical structure Given an address trace and two dis­tances 
k and k. (k<k), the af.nity groups at k form a .ner partition of the af.nity groups at k. . 3. Bounded 
access range Given an address trace with an af.nity group G at the link length k, any time an element 
x of G is accessed at ax, there exists a section of the trace that includes ax and at least one access 
to all other members of G. The volume distance between the two sides of the section is no greater than 
2k|G| +1, where |G| is the number of elements in the af.nity group.  Having the de.nition of reference 
af.nity, the problems of checking and .nding reference af.nity groups can be formulated as the following: 
DEFINITION 4. Checking reference af.nity groups Given an ad­dress trace and k, check if a given group 
of data elements belongs to the same reference af.nity group with link length k. DEFINITION 5. Finding 
reference af.nity groups Given an ad­dress trace and k, .nd all reference af.nity groups with link length 
k. A related decision problem with checking reference af.nity groups is to test if two accesses is k-linked 
with each other: DEFINITION 6. Given an address trace, a volume distance k = 0 and two data accesses 
ax and ay, the Point-wise k-Linked Af.nity Problem (Pw-k-A., for short) is the problem of testing whether 
ax and ay are k-linked in the trace.  3. Hardness of Finding Reference Af.nity The following theorems 
give the complexity of the linking, check­ing, and .nding problems for different k. We include the basic 
ideas of the proofs and leave the full version in the appendix. THEOREM 1. For each k = 3, Pw-k-A. is 
NP-complete. We prove it by making a polynomial-time many-one reduction from a variant of 3-SAT problem, 
where every variable appears at most three times (an NP-complete problem, see, e.g., [31]) to the linking 
problem. The proof constructs a three-part reference trace. The .rst part forces a linked path to go 
through a set of elements we call separators , which cannot be used as hops in the next two parts. The 
second part prepares a set of data triples to model the truth values of the logical variables in a 3-SAT 
expression. Since two data elements may represent opposite values of a logical variable, the construction 
ensures that the elements cannot be both included in a possible linked path. A linked path can land on 
different places and can even go backwards the only constraint is that the volume distance of the longest 
hop. The critical moment of the construction is when the freedom of the linked path is contained within 
seven cases, and each is shown to have the needed property. The third part of the sequence models all 
3-SAT expressions. A linked path exists if and only if there is a truth value assignment to satisfy all 
expressions. To design a trace that enforces the logical consistency, we learned that we need to use 
multiple data accesses to represent logical variables instead of using data to represent them. The full 
proof is more than a page long and given in the appendix. From Theorem 1, we can easily prove two corollaries. 
COROLLARY 1. For k = 3, the problem of checking reference af.nity groups is NP-complete. COROLLARY 2. 
For k = 3, the problem of .nding reference af.n­ity groups is NP-hard. THEOREM 2. Pw-2 -A. is NL-complete. 
 Using the same polynomial-time reduction from Theorem 1, we can show that 2 -CNF-SAT can be reduced 
to Pw-2 -A.. The ex­act proof is in the appendix. This theorem shows that a polynomial algorithm exists 
for Pw-2 -A.. Then we have the following result, proved by the algorithm that follows. THEOREM 3. For 
k =2, the problem of .nding reference af.nity groups is in P. ALGORITHM 1. Finding reference af.nity 
groups when k=2 procedure FindReferenceAf.nityGroup 2(T) 1: {T is the trace, the link length k =2} 2: 
initially no af.nity groups 3: while there exist elements not yet grouped do 4: put all such elements 
into a set G and pick one x randomly from this set; 5: repeat 6: if there is an element z not 2-linked 
to x with respect to G then 7: remove z from G; 8: else 9: if there exist two elements y, z . G such 
that an access of y is not 2-linked to any access of z with respect to G then 10: remove z from group 
G. 11: end if 12: end if 13: until G is unchanged 14: output reference af.nity group G.  15: end while 
 endFindReferenceAf.nityGroup 2 Algorithm 1 is polynomial time. From Theorem 2, the linking problem, 
that is, testing whether a 2-linked path exists between two data accesses, can be solved in polynomial 
time. This algorithm needs a polynomial number of such tests. The algorithm gives cor­rect reference 
af.nity groups. First, it is easy to see that the groups found by this algorithm satisfy the .rst condition 
of reference af.n­ity. To show every group is the largest possible, we show that the algorithm removes 
z correctly, so that G still includes only the ref­erence af.nity group that x belongs to. Removing z 
at step 7 is straightforward. The correctness of the removal of z at step 10 can be proved by contradiction. 
Suppose z belongs to the same group as x and should not be removed, we can construct a 2-linked path 
from every access of y to an access of z. This contradicts with the test at line 9. The full proof is 
given in the appendix. From Theorem 3, we can get the following corollary. COROLLARY 3. For k =2, the 
problem of checking reference af.nity groups is in P. The complexity for k =1 is as follows. THEOREM 
4. Pw-1 -A. can be solved in linear time. THEOREM 5. For k =1, there is a polynomial-time solution for 
.nding reference af.nity groups. Here we give a naive method. Since k =1, all of the groups appear in 
the sequence continuously, and two groups do not over­lap. We sort the data elements according to their 
order of appear­ance in the trace. Then for every t (from the number of data ele­ments to 1) consecutive 
data elements starting from the .rst data element, we check if it is a reference af.nity group. Similarly, 
we .nd other af.nity groups. The algorithm is given in the appendix. Finally, from Theorem 5, we have 
COROLLARY 4. For k =1, the problem of checking reference af.nity groups can be solved in polynomial time. 
Compute(D1, D2, ..., Dn) begin if the input data is above a threshold size divide D1, D2, ..., Dn into 
sub-blocks for some set of sub-block combinations Compute(subblocki(D1), subblockj (D2), ..., subblockk(Dn)) 
end for else process without sub-division end if end Figure 2. The general form of the divide-and-conquer 
algorithm  4. Reference Af.nity in Divide-and-Conquer Computations The divide-and-conquer type of computations 
we consider are blocked and recursive algorithms for dense matrix operations, N­body and mesh simulation, 
and wavelet transform. The general form is given in Figure 2. The procedure takes a set of data such 
as matrices. It then divides the input data into smaller blocks and processes all or subsets of their 
combinations. For each subset, if the blocks are still large, it makes a recursive call to itself. The 
computation is hierarchical, so is its locality. We show that reference af.nity can reconstruct the hierarchical 
data locality from an execution trace, if the following two require­ments are met by the hierarchical 
computation. First, once a block of Datai is accessed, all its sub-blocks are accessed before moving 
to the next block of Datai. Second, the access order of sub-blocks is the same for the same block. For 
example, consider the multi­plication of two matrices A and B. The computation, if starting from the 
left sub-matrix of A, must access all elements of the left sub-matrix of A at least once before accessing 
any element from the right sub-matrix of A. Still, it is free to access B or other data at the same time. 
The traversal order within A is the same, for ex­ample, Morton order. The traversal order in B can be 
different. In addition, non-nesting blocks in the same matrix can have different traversal orders. We 
use N-body simulation as an example, which calculates how particles interact and move in space. Most 
computation is spent on computing the direct interaction between each particle and its neighbors within 
a speci.c radius. The typical implementation m. We call it an m-block of computation. In a divide-and-conquer 
computation, each m-block contains a contiguous sequence of the computation trace. We will prove the 
exact structure of the reference af.nity hier­archy for divide-and-conquer computations. As a shorter 
exercise, we .rst show that the reference-af.nity hierarchy has more than a constant number of levels 
when the size of data n is arbitrarily large. THEOREM 6. For one-dimensional N-body simulation in the 
Mor­ton order, the reference af.nity has more than a constant number of levels when n is arbitrarily 
large. Proof Given a k that is a power of 2, we show that (a) every k 2 ­section of data belongs to a 
k-af.nity group but (b) some m-section of data does not all belong to a k-af.nity group. We prove part 
(a) .rst. Every use of a k 2 -section data is contained in a k 2 -block of computation, which contains 
at most k distinct data. It is obvious that a k-linked path exists from any access to any other access 
in the k 2 -block, therefore a k 2 -section belongs to a k-af.nity group. We prove part(b) by contradiction. 
Suppose for any m,an m­section of data belongs to a k-af.nity group. We denote the .rst and the last 
data elements of the m-section as d1 and dm. According to the de.nition of reference af.nity, there must 
be a k-linked path from the .rst access of d1 to some access of dm, the path has at most m - 1links, 
and the volume distance of each link is no more than k. The trace from the .rst access of d1 to the .rst 
access of dm 2 includes at least a m -block, which has a length m . Hence the path 2 4 2 of m -1links 
spans at least m 4 data accesses, and there must exist m a link that spans at least 4 accesses. However, 
when m is large enough, it is impossible to bound the number of distinct data in m 4 contiguous accesses 
on the trace. The volume distance of the link must be greater than k. A contradiction. Therefore, the 
reference­af.nity hierarchy has more than a constant number of levels when n can be arbitrarily large. 
Next we prove the exact structure of the reference af.nity hier­archy. First, we give a key lemma needed 
by the .nal theorem. We call it the insertion lemma. It shows that the insertion of a new data access 
converts a link of length k into two shorter links. LEMMA 1. Insertion Lemma Given two different data 
elements u and v; their accesses au and av where the volume distance from au to av is exactly k; and 
a third access ax, which happens between divides the space into basic units. For ease of presentation, 
we au and av in the trace; then there exists an access a x between au assume each unit contains the 
same number of particles, and the and av such that the volume distance from au to a and the volume x 
 program computes the interaction between all unit pairs. Our main distance from a x to av are both less 
than k. result, Theorem 7, holds when units contain a different number of particles, and when interactions 
are limited within a radius. In the following analysis, we assume a one-dimensional space. Higher dimensions 
can be linearized by using a space-.tting curve [30]. For simplicity, we assume that the space has n 
=2t units, where integer t is non-negative. The N-body simulation trace is then of size 22t+1. As an 
example, we give the trace that follows the Morton space .lling curve when computing the interactions 
between all pairs of four molecule data, shown in Figure 3 (a). The locality is in the recursive structure 
of computation. The data access trace is given in Figure 3 (b). We show that reference af.nity can identify 
the locality structure by examining only the data access trace. We call each data a unit and divide units 
into sections. We call the set of units i * m +1to i * m +m an m-section of data, where m is a power 
of 2and i is a non-negative integer. As the example in Figure 3 (a) shows, the rows and columns of the 
matrix are data units, and the Morton space .lling curve gives the execution order. The interaction between 
an m-section and another m-section is computed at their product area in the graph, a block of size m 
by The insertion lemma states that a link from au to av of length k can be divided into two shorter 
links. In particular, for any data element x accessed along the path, there exists an access of x such 
that it breaks the link into two shorter links. Not all accesses to x can be the breaking point. The 
proof considers all possible con.gurations of u, v, x, au,av and shows the placement of x in each case. 
The proof is mechanical and long due to the number of cases. We include a sketch of the proof in the 
appendix. The following theorem gives the exact structure of the reference­af.nity hierarchy. It is the 
most important theoretical result, estab­lishing the link between the linear, .exible concept of linked 
paths in a computation trace and the hierarchical structure of locality in space. THEOREM 7. Given N-body 
simulation of 2s particles imple­mented using the divide-and-conquer technique or a space-.tting curve, 
the reference af.nity hierarchy contains s +1levels, where each 2i-section belongs to an i-level af.nity 
group. The proof is straightforward after proving the following lemma. 1 2 3 4 1 1 2 5 6 3 4 7 82 9 10 
13 143 11 12 15 164 (a) The Morton order for com­puting all interactions between all pairs of four molecule 
data (1,1) (1,2) (2,1) (2,2) (1,3) (1,4) (2,3) (2,4) (3,1) (3,2) (4,1) (4,2) (3,3) (3,4) (4,3) (4,4) 
(b) The trace of the access to data Figure 3. An example 4-body simulation LEMMA 2. Separation Lemma 
For any m-section, there exists a k, such that the m-section is a k-af.nity group, but the 2m-section 
does not all belong to the k-af.nity group. Proof Let k be the smallest reuse distance such that the 
m-section belongs to an af.nity group. Without loss of generality, we assume the m-section and 2m-section 
are the .rst such sections in the data space, as shown in Figure 4(a). Suppose the 2m-section also be­longs 
to the k-af.nity group, we derive a contradiction by showing that the m-section belongs to a k -1-af.nity 
group, for which it suf.ces to show that there is a k-1-linked path from the .rst access of 1to the .rst 
access of m. Because the 2m-section is in a k-af.nity group, there is a k­linked path from the .rst access 
of element 1 to some access of element m+1, as shown in Figure 4(a). The path is linked by at most one 
access of elements 2 to m. Now consider the two m­blocks of computation in the .gure marked with U and 
V.They divide the path into two parts. By adding an ending point at the .rst access of min the U block, 
and a starting point at the .rst access of 1in the V block, we cut the k-linked path from 1to m+1into 
two k-linked paths from an access of 1to an access of m. The two paths are shown at the bottom of Figure 
4(a). The intermediate links in the two paths are u1,...,us and v1,...,vt. We map the vi path in the 
V block to the U block. We now have two k-linked paths from the .rst access of 1to the .rst accesses 
of m. The links are accesses to different data elements. We construct a k -1linked path from the .rst 
access of 1to the .rst access of min U block in Figure 4(a). Consider each link on the ui path, say from 
ui to ui+1. If the link length is not exactly k, then we are done. If the length is k, and some vj happens 
in between, then from the insertion lemma, the k-link can be divided into two shorter links by moving 
vj .Ifno vj happens between ui and ui+1, there must exist vj and vj+1 that include ui and ui+1 in between. 
Since the volume distance from ui to ui+1 is k, vj and vj+1 must appear between ui-1 and ui+2, forming 
the sequence shown in Figure 4(c). If the volume distance from ui-1 to vj is smaller than k, then using 
the insertion lemma, the link from vj to ui+1 can be divided into two smaller links by moving ui, and 
the link from ui+1 to ui+2 can be divided into two smaller links by moving vj+1. Similarly we can construct 
smaller links when the volume distance between vj+1 and ui+2 is less than k. Otherwise, ui-1,ui,ui+1,ui+2, 
are exactly k-linked. We continue to consider elements of vj-1,vj-2,...,v1 and vj+2,vj+3,...,vt through 
similar steps. If we can not get a k -1linked path after examining all elements, it means that the path 
1,u1,··· ,us,mare exactly k-linked. This is impossible, since the original linked path 1 m 2m3m4m 1 m 
2m (a) Given that a k-link path exists from 1to m +1, we want to show that a (k­1)-link path exists 
from 1 to m.The k­link path is broken into two and placed in parallel at the bottom. If the top 3 are 
exactly k-linked u i-1 vj ui ui+1 vj+1 ui+2 Then the bottom 2 are exactly k-linked (b) When no links 
of length k can be broken into smaller links, all links must have exactly the length k, and the last 
link must be k +1, which contradicts to the assumption. Figure 4. An illustration for the proof of Separation 
Lemma goes from the .rst access of 1to an access of m+1. The last link connecting to the access of m+1must 
have a length greater than k. A contradiction. We make two observations. First, the proofs do not assume 
what, when, and whether a section of data is used. It requires only that a section is used together as 
a block. In N-body simulation, the interactions are calculated within a radius. In this case, an af.nity 
group cannot cross the boundary of a radius. The proofs assume the Morton order for the convenience of 
illustration, but it remains valid as long as all data are traversed in some order. The order may change 
when the same block is accessed at a different time. Hence the theorems can be extended to general divide-and-conquer 
algorithms. Second, the proofs are for the existence of k. The exact size of the data sections may change 
the value of k but not the existence of k. Therefore, the af.nity hierarchy exists when data sections 
are divided into non-uniform sections. Given Theorem 1, a natural question is whether the reference af.nity 
in divide-and-conquer algorithms can be ef.ciently discov­ered. While the answer requires a systematic 
study that is beyond the scope of this paper, we note that our initial experiments in Sec­tion 6 show 
good results for recursive matrix multiplication. One reason is that in divide-and-conquer algorithms, 
the elements of the same af.nity group are accessed in a similar order, while the reduction in the NP-complete 
proof requires data be accessed in all Ignore those data elements x with S[x] <. Construct a graph with 
the data elements not ignored as vertices. for two vertices x, y do if (confidence(x, y)= P [x, y]/min(S[x],S[y])) 
>. then Add an edge between x and y end if end for Output every connected subgraph as a group. Suppose 
M is the number of distinct data elements, L be the length of the trace. The time complexity of this 
algorithm is O(Ldw2). The space complexity is O(M2). THEOREM 8. For any data element x in the reference 
af.nity group, there exists a y in the same group and their expected con.­ dence (de.ned in the algorithm 
2) is greater than 1 2 . possible orders.  5. Af.nity Analysis Through Sampling Given a trace, an af.nity 
group G,and x, y . G, if there is a window of the trace covering at least an access for ax and an access 
for ay, and the length of the section is no greater than k(|G|-1)+1, then we call it a witness window 
for x and y. We consider that af.nity holds for ax and ay if there a witness window 1. A sampling window 
of size w is a window of the trace where the volume distance between the two boundaries is w. We estimate 
Proof Suppose the upper bound for the af.nity group size is g. Consider reference af.nity group G. Clearly, 
|G|= g. For any data access to x . G, from the de.nition of reference af.nity, we can .nd an access to 
y . G, where their volume distance is within k(|G|-1)+1. The sample size is 2kg. Hence the sampling window 
has k(|G|- 1) + 1 - 1 k|G| kg 1 1 -= 1 -= 1 - = 2kg 2kg 2kg 2 probability of covering x and y, given 
it covers x. Since the windows are sampled independent of x and y, the af.nity groups by .nding elements 
that are frequently accessed con.dence value for x and y is at leasttogether in sampling windows. 1 2 
By Theorem 8, we know that if we set the threshold . to be 1 2 , The sampling method is given by Algorithm 
5. It .rst estimates we can ensure the data elements in the same group remain in the the upper bound 
for the size of af.nity groups. Suppose it is g. The size of the sampling window is set accordingly to 
l =2gk. For a pair of data elements, x, y, the sampling method measures the af.nity by what percent of 
the sampling windows have both x and y. Since they may appear in a different number of windows, the smaller 
number is used as the denominator. If the value is bigger than some threshold ., then we consider x and 
y have af.nity. The pairwise relation is extended into a group relation by taking the transitive closure. 
To reduce the number of data elements considered in the analysis, we may exclude infrequently accessed 
data element, i.e. the number of appearances fewer than ,ina similar fashion as association rule mining 
[21]. ALGORITHM 2. Sampling method for reference af.nity analysis Input: A trace; window size w; sample 
rate d; threshold ; Af.nity threshold .. Output: the reference af.nity groups. Method: Let S be the number 
of sampled windows every single data element appears; Let P be the number of sampled windows where each 
pair of data elements appear; Sample windows of size w from the given trace, according to the sampling 
rate d. for each window do for each distinct data element x do increase S[x] by 1. end for for every 
distinct pair of data element x, y do Increase P [x, y] by 1 end for end for 1 This is an approximation 
because the bounded appearance is a necessary but not suf.cient condition for reference af.nity. same 
group found by Algorithm 2. Notice that in the algorithm, the window size, instead of the con.dence threshold, 
is dependent of the reuse distance k. The previous algorithm .nds one level of reference af.nity. For 
practice use, we can vary the sampling window size w in a logarithmic scale to .nd af.nity groups at 
different levels. Pairwise af.nity has been used to reduce cache con.icts for code and data. Thabit [38] 
modeled the pairwise af.nity of arrays in a proximity graph and showed that optimal packing is NP­hard. 
Gloy and Smith [19] studied procedure placement and used pro.ling to .nd the frequency a pair of procedures 
are called within a distance smaller than the cache size 2. Similar pro.ling methods are used for placing 
dynamic program data by Calder et al. [5] and Chilimibi et al [9]. The goal of the sampling method is 
to .nd reference af.nity rather than to minimize cache con.icts. It is unique in at least three aspects. 
First, the pairwise frequency is the percentage of accesses. Consider the access sequence abcabc..abc. 
The percentage weight between all three data is 100%, so they belong to an af.nity group. Now consider 
the sequence abab..ab ... bcbc..bc ... acac..ac.The percentage frequency is no more than 0.5 on average, 
so they do not have af.nity, despite that access frequency of data pairs can be arbitrarily high. Petrank 
and Rawitz showed that for any placement method, a trace exists that has the same pairwise frequency 
but the data layout given by the method is at least a factor of k - 3 away from the optimal layout, where 
k is the cache size. The sampling method alleviates this problem to a degree because the high per­centage 
from the .rst example implies the af.nity pattern in the ac­cess sequence, while the frequency from the 
second example does not contain enough information to ensure a good data layout. This shows the gist 
of the theory of reference af.nity it identi.es a 2 If a procedure p is called and the reuse distance 
after the previous call pis no greater than twice of the cache size, the frequency is incremented for 
all pairs between p and all procedures called between p and p. speci.c access pattern and provides a 
better solution to the limited P,P,...,P12n andscatteredintogroups G,G,...,G12n found problem. by an 
analysis method. We de.ne the accuracy for this af.nity Two other differences are also signi.cant. The 
size of the sam-group as pling window is proportional to the group size rather than the cache |P . i 
|G. size. Since the useful group size is the size of cache blocks, it is | P n i=1 much smaller than 
the cache size, and the smaller window allows more ef.cient analysis and in turn larger traces and .ner 
granular­ i accuracy(G)= | . n2 ity. Finally, the data transformation is simple, which is to group members 
of the same af.nity group. It is no problem if group members have different sizes. In contrast, previous 
methods solve weighted data packing problems and must reconcile between differ­ent size data. The difference 
is due to the fact that reference af.n­ity is transitive but the pairwise frequency is not. The placement 
between af.nity groups is still a problem of packing, and the gen­eral complexity is given by Petrank 
and Rawitz. Still, the solution within an af.nity group is clear. As a heuristic, the sampling method 
is not accurate for several reasons. First, it may miss an af.nity group when not enough witness widows 
are sampled. Second, it may .nd false groups that are not accessed together as often as they do in sampling 
windows. While in general one cannot guarantee without a priori knowledge about the distribution of data 
accesses in a trace, we can use a higher sampling rate to improve the statistical coverage and accuracy. 
In fact, we can sample every window in pro.ling analysis. Finally, it needs an upper bound for the group 
size. This prevents the method from .nding large af.nity groups. However, if we target speci.c hardware, 
the size of the storage unit, for example, the size of cache blocks, can be used as the upper bound for 
the group size.  6. Evaluations of The Sampling Method 6.1 Comparisons with K-distance Analysis We 
.rst review the k-distance analysis based on reuse signa­ture [44]. K-distance analysis targets only 
groups of data that are always accessed together. It .rst measures the reuse signature of ev­ery data 
element, which is the histogram of the reuse distance of all accesses of the element. Then it computes 
the Manhattan distance between the reuse signature of every data pair x, y as follows. The more pieces 
a group is separated into, the lower the accuracy is. Same is true when a small group is clustered into 
a bigger group. We use n 2 instead of n as the denominator. Consider the following situation: if G is 
scattered into exactly |G| trivial groups, then the accuracy would be 1 if we used n as the denominator. 
The overall accuracy is the average accuracy of all af.nity groups. We use both the match rate and the 
accuracy in the following comparison. When the af.nity groups are correctly recognized, both the match 
rate and the accuracy are 1. Otherwise the accuracy is in general higher than the match rate because 
the former includes partially recognized groups. The results presented here are an average of simulating 
20 traces. The variances of the accuracy and the miss rate are very small and we don t report them here. 
The size of the trace is 200,000 by default or a length in which every af.nity group occurs roughly 400 
times. In every .gure except Figure 7, the link length k is set to 1, which is the best case for k-distance 
analysis. Fig­ure 7 will show that when k increases, the accuracy of k-distance drops much faster than 
the sampling method does. For every exper­iment, the sample rate is set to be 0.001. For lack of space, 
we omit parameters of the trace generation that are not essential to the pre­sentation. A more detailed 
description can be found in a technical report [42]. We .rst compare sampling and k-distance analysis 
on 50 groups whose members are often but not always accessed together. The frequency of the af.nity is 
the weakness. A weakness of 0.7 means that when one member is accessed, 70% of other members will be 
accessed. Figure 5 shows that when the weakness is 0.5, the match rate and the accuracy are over 96% 
for the sampling method but below 73% for k-distance analysis (the accuracy is under 65%). As the weakness 
changes from 0.5 to 1, both methods improve. The match rate and accuracy are close to perfect for the 
sampling method but still below 86% for k-distance analysis. B X i =1 is the average distance for bin 
i, B is the number of xiyi (1) |Avg- Avg | d =  Weakness of affinity groups where Avg xi xi - bins considered. 
If their access pattern is near identical, |Avg | is smaller than k in every bin. Hence if d = k * B, 
then x, y are in the same af.nity group. yi Avg K-distance builds the af.nity hierarchy incrementally. 
Initially every data element is a group. Then it merges two groups if the distance between a member in 
one group and a member in the other group is the smallest among all cross-group distances. Compared with 
the sampling method, k-distance tends to cluster irrelevant data elements into one group, especially 
for those single data elements which occur randomly. The second problem is that the vector may not be 
the same for data elements in the same strict reference af.nity group because of the partition boundaries 
when collecting the histogram. Finally, it does not work well for partial reference af.nity where groups 
of data are often but not always accessed together. Their reuse signatures may differ by an average of 
more than k. We .rst compare the two methods using generated traces. A di­rect measure is the number 
of perfect matches between the af.nity groups we use to generate the trace and the af.nity groups found 
by an analysis method. We call it the match rate of the analysis. A more complex metric, accuracy, measures 
the quality of im­perfect matches. Let an af.nity group G be separated into pieces Figure 5. Comparison 
of performance when groups of data are often but not always accessed together Figure 6 shows the comparison 
when the number of af.nity groups varies. For strict af.nity groups (weakness is 1), when the number 
of groups change from .ve to .fty, the match rate and the accuracy are stable (around 85%) for the sampling 
method but drop signi.cantly for k-distance, from 86% to 67% for the match rate and from 78% to 50% for 
the accuracy. The match rate of the sampling method is not only much higher but also much closer to the 
accuracy, compared with k-distance. The upper two curves differ by no more than 5%, showing that most 
af.nity groups are detected in whole by the sampling method. g =20. Since accesses are randomly scattered, 
the average dis­tance is about half of kg. Thus using only kg, we can get enough con.dence. The performance 
varies by less than 2% for window sizes between 10 and 30 and less than 8% between 5 and 50. The match 
rate and the accuracy are always higher than 92%. There­fore, the sampling method tolerates a wide range 
of window sizes when targeting groups of speci.c sizes. 5 101520253035404550 Number of non-naive affinity 
groups Figure 6. Comparison of performance when the number of af.nity groups varies We now compare the 
effect of k, which is the closeness between accesses to data of the same af.nity group. As k increases, 
the complexity of .nding af.nity groups increases, from polynomial time to NP-hard as shown in theory 
in earlier sections. Figures 7 shows the result for 200 strict af.nity groups. The match rate of the 
sampling method is perfect when k is 1 and 2 and drops to around 80% when k increases from 3 to 10. K-distance 
analysis has much worse performance from the beginning (around 85%) followed by a steep drop (to below 
20%). The sampling method detects most af.nity groups in whole but k-distance does not. 1 2 3 4 5 6 
7 8 910 K Figure 7. Comparison of performance when k (the closeness) of the af.nity group varies Last 
we test different sizes of the sampling window using af.n­ity groups of size 20. Figure 8 shows that 
the performance is best when the window size is 20, comparable to kg, where k =1 and  Sample size Figure 
8. The performance of the sampling method for different window sizes The two methods can be combined. 
We .rst run K-distance analysis to .nd approximate af.nity groups, estimate the size of the af.nity groups, 
and then use the sampling method to re.ne and improve the results. The estimate from K-distance analysis 
also reduces the memory requirement of the analysis. We have left out results from other experiments 
based on generated traces because of the limited space. In all cases, the sampling method gives ef.cient 
and accurate analysis for a wide range of af.nity groups. Next we look at real program traces. 6.2 Recursive 
Matrix Multiplication We test af.nity analysis on recursive matrix multiplication. Given two square matrices, 
each has 256 elements (16*16), the program recursively divides the matrices into four parts and calculates 
their product. The length of data access trace is 12288 (3 * 163). Ac­cording to Theorem 7, there are 
.ve levels of reference af.nity in 16*16, 8*8, 4*4, 2*2, and 1*1 sub-matrices. When we set the sam­pling 
window to be volume distance 12 and the af.nity threshold to be 0.6, the sampling method correctly identi.es 
af.nity groups in 2*2 blocks. Figure 9 shows one of the input matrix, with af.nity groups drawn in different 
shades of gray. Figure 10 shows that k­distance analysis identi.es half of the 2*2 blocks but groups 
others into 4*4 blocks, which is not as accurate as the sampling method. This is a simple experiment, 
but it demonstrates that sampling analysis can uncover the high-level locality structure despite that 
it examines only the data access trace of the complex computation, and that the general problem is NP-hard. 
The af.nity analysis can be used in a pro.ling tool for a user to see the locality structure in program 
data. Some of the data transformations can be automated, for example, structure splitting and array regrouping. 
Some cannot, for example, the recursive data layout in complex computations, so user support is currently 
needed. It may be possible for dynamic data transformation. A program can often analyzes its run-time 
data access and reorganizes the data for better locality, as demonstrated by many studies for array data 
in scienti.c programs or objects managed by a garbage 2 4 6 8 10 12 14 16 Figure 9. All 2*2 af.nity 
groups are identi.ed by the sampling method 2 4 6 8 10 12 14 16 Figure 10. Half of the 2*2 af.nity groups 
are identi.ed by k­distance analysis collector. Reference af.nity should be pro.table if its analysis 
can be made ef.cient enough.  6.3 Af.nity-based Code Layout We use pro.ling to collect the trace of 
basic block references, apply reference af.nity analysis, and then reorganize code layout by placing 
basic blocks of the same group sequentially in memory. The sim-cache module of SimpleScalar is modi.ed 
to simulate the instruction cache and measure the cache miss rate. We compare these af.nity groups with 
the original code layout and the code regions formed by checking transition frequencies between basic 
blocks [22], a common method in pro.ling-based code layout. For lack of space, we brie.y describe only 
the main setup and the result. The purpose is to demonstrate the applicability of reference af.nity and 
the sampling method. We do not intend in this work to design a complete compiler technique nor evaluate 
it against the existing literature. We test seven integer programs from SPEC 2000 3. We use com­plete 
traces, which contain up to one billion basic blocks and 10 bil­lion references. K-distance method is 
based on the reuse signature 3 Bzip2, Crafty, Gap, Mcf, Perlbmk, Twolf, and Vpr from long-distance reuses, 
so it ignores many basic blocks, which are only accessed once with a short reuse distance. It suffers 
more from false positives because the reuse signature lacks the timing in­formation. In experiments, 
k-distance tends to produce one or two very large groups, containing up to 50% of all basic blocks. The 
sampling method overcomes these drawbacks. For complete cover­age in pro.ling, the sampling rate is 100%. 
The exact comparison depends on the thresholds and parameters for both sampling and the frequency-based 
method. A detailed comparison is too long to include. However, the best af.nity-based layout always has 
better locality than the best frequency-based layout, as shown in Figure 11 for two different cache con.gurations. 
Compared with the unopti­mized code layout on 8KB direct-mapped cache, the af.nity-based layout reduces 
the cache miss rate for six of the seven programs by up to a factor of more than 3. When 16KB 2-way set 
associative cache is used, the miss rate of the three of the seven programs drops to near zero. The af.nity-based 
layout improves the locality in all other four programs. The largest relative improvement is on Twolf, 
a circuit placement and global routing program using simulated an­nealing. The miss rate drops from 0.4% 
by more than a factor of 10 by the af.nity-based layout. In comparison, the frequency-based layout reduces 
the miss rate by a factor of about 2.  7. Related Work Thabit showed that data packing for a given block 
size using pair­wise frequency is NP-hard [38]. Kennedy and Kremer gave a gen­eral model that includes 
run-time data transformation (among other techniques) and showed that the problem is NP-hard [23]. Pe­trank 
and Rawitz showed the strongest theoretical result to date if P = NP, no polynomial time method can guarantee 
a data layout whose number of cache misses is within O(n 1-E) of that of the optimal data layout, where 
n is the length of the trace. In addition, if only pair-wise information is used, no algorithm can guarantee 
a data layout whose number of cache misses is within O(k - 3) of that of the optimal data layout, where 
k is the size of cache. The results hold even when the computation sequence is completely known, objects 
have the same size, and the cache is set associa­tive [32]. These general results, however, do not preclude 
effective optimization targeting speci.c (rather than all) data access patterns. In fact, one can easily 
construct traces for which it is trivial to .nd the optimal data layout. Zhong et al. de.ned reference 
af.nity and used a heuristic called k-distance analysis in structure splitting and array regrouping [44]. 
The earlier work does not give the computational complexity of reference af.nity, nor is it used in hierarchical 
data placement. In addition, as shown in Section 6, k-distance analysis does not work well for partial 
reference af.nity. Hierarchical data layout is .rst used for matrix multiplication and QR factorization 
by Frens and Wise [16, 17], Cholesky fac­torization (based on data shackling [25]) and wavelet transform 
by Chatterjee et al. [7], and N-body and mesh simulation by Mellor-Crummey et al [30]. We show that reference 
af.nity can help a programmer to analyze data locality in these and other programs (without knowing the 
structure of the computation). Bender et al. used the recursive van Emde Boas layout for dynamic search 
trees and proved the asymptotic optimality if the input consists of random searches [3]. Reference af.nity 
cannot yet give the van Emde Boas layout because the af.nity hierarchy is .at (two levels) for any constant 
k. The extension for variable­distance af.nity groups is a subject of future work. On the other hand, 
if the input is not random search and has reference af.nity, for example, a group of tree nodes are often 
searched together, then reference af.nity is directly applicable and in principle may yield better locality 
than the van Emde Boas layout does. (a) 8KB direct-mapped cache, cache block (b) 16KB 2-way set associative 
cache, cache size 64 bytes block size 32 bytes Figure 11. Comparison of cache performance of the original 
and the best of frequency-and af.nity-based code layout for seven SPEC2K integer benchmarks Wolf and 
Lam [40] and McKinley et al. [29] used compiler analysis to identify groups of memory references in loop 
nests. Our work is related to trace-based pairwise af.nity models, as dis­cussed in Section 5. Hang and 
Tseng used pairwise (connection) models for scienti.c data and found that hierarchical clustering was 
more cost-effective than general graph partitioning [20]. Chilimbi de.ned hot data streams, which are 
sequences of repeated data ac­cesses and are not limited by pairwise af.nity [8]. When a pro­gram has 
good computation locality, simple data packing (.rst­touch ordering) improves the data layout, as shown 
by several stud­ies [12, 30, 36, 39]. Recently, Strout and Hovland introduced mod­els based on hyper-graphs 
[37]. Most of these techniques are in­tended for on-line adaptation and do not model hierarchical local­ity. 
Earlier studies have established the locality upper bound (the best possible locality) for speci.c computing 
problems. Hong and Kung used a graph model to prove the tight lower bound of I/O for FFT, matrix multiply, 
and sorting. Aggarwal et al. gave a hierarchi­cal memory model (HMM), where the cost for access location 
x was .logx. [1]. They showed that the maximal slowdown factor (O(logn)) could be avoided for some computations 
(FFT, matrix multiply, and sorting) but not for others (list search and circuit sim­ulation). They also 
studied other convex cost functions. Alpern et al. extended the hierarchical model with explicit transfer 
cost and gave a set of threshold functions when computations (FFT, matrix multiply, matrix transpose, 
and parallel matrix multiply) became communication bound [2]. Frigo et al. re.ned the cache model to 
consider cache size Zand block size L. They gave recursive al­gorithms that yielded the best asymptotic 
locality for FFT, matrix multiply, and sorting for any Zand L(Z=.(L2)) [18]. The al­gorithms are cache 
oblivious because they do not depend on Zand L. Yi et al. showed that a compiler can automatically convert 
loops into recursive subroutines to exploit hierarchical locality [41]. The ordering of general computations 
has been studied as a clustering problem in loop fusion and computation regrouping. Kennedy and McKinley 
[24] showed that fusion for locality was NP-hard because it could be reduced to the problem of k-way 
cut [10]. Ding and Kennedy proved a similar result for a re.ned model, where hyper graphs were used to 
represent data reuses among multiple computations [13]. Darte gave complexity results for a larger class 
of fusion problems [11]. These results suggest that ef.cient, hierarchical models are unlikely for general-purpose 
computations. In 1970, Mattson et al. .rst de.ned reuse distance (named LRU­stack distance) [28]. The 
distribution of all reuse distances (which we call the reuse signature) gives the temporal locality of 
an execu­tion. Snir and Yu recently disproved the possibility of a more com­pact metric by showing that 
temporal locality could not be char­acterized by one or few parameters [35]. Indeed, reuse signature 
has been widely used in virtual memory and cache design. Build­ing on decades of development by others, 
our earlier work reduced the measurement cost to near linear time [14] and used reuse dis­tance patterns 
for program locality prediction [33, 43], data struc­ture transformation [44], and locality phase prediction 
[34]. Reuse­distance based models are found useful for program analysis by an increasing number of studies 
(three appeared in 2005), includ­ing cache miss rate prediction across inputs by Marin and Mellor-Crummey 
[26, 27], per-reference miss rate prediction by Fang et al. [15] and Beyls and D Hollander [4], and cache 
interference prediction for parallel processes by Chandra et al [6]. This paper presents a formal theory 
for reuse distance. The proof for Theo­rem 1 gives a reduction between reuse distance and formal logic. 
The proof of Theorem 7 connects the structure in computation with the locality in data. These formal 
connections serve as a theoret­ical basis for current and future reuse-distance based models and techniques. 
 8. Summary In this paper we have given a complete characterization of the com­plexity of reference af.nity. 
We have proved that when k=1or k=2, .nding reference af.nity grooups can be done within poly­nomial time 
and when k=3, .nding reference af.nity groups is a NP-hard problem. We have extended the hierarchical 
locality to divide-and-conquer computations such as matrix solvers, factoriza­tion, wavelet transform, 
N-body and mesh simulation, where the results con.rms the previous empirical solutions. The theoretical 
results have established formal links between the computation, the data reuse, and the locality. In practical 
use side, We don t know a real use for cases k =1or k =2. The hardness of .nding the reference af.nity 
groups when k>2implies that it is hard to .nd any polynomial algorithms. Instead, we have presented a 
sam­pling method and shown through experiments that it is more accu­rate than the previously published 
technique, especially for groups greater in number and complexity and weaker in their af.nity. We have 
shown two new uses of reference af.nity. The .rst is .nding hierarchical data layout in a recursive program, 
and the second is improving the code layout of seven SPEC 2K applications. In POPL 2002, Petrank and 
Rawitz precisely characterized the theoretical dif.culty of the general data placement. Reference af.n­ity 
side steps this limitation by targeting a common pattern rather than all patterns of data access. Since 
the volume distance is widely used in experimental algorithms, the theoretical .ndings in this pa­per 
may help the development of other distance-based locality the­ories.  A. Proofs Theorem 1 For each k 
=3, Pw-k-A. is NP-complete. Proof It is obvious that the problem is in NP. We will prove its NP­hardness 
by constructing a polynomial-time many-one reduction to Pw-k-A. from 3-SAT, which is the problem of testing, 
given a formula of conjunctive normal form in which each clause has at most 3 literals, whether the formula 
is satis.able. We consider the variant of this problem in which each variable appears as a literal (positively 
or negatively) at most three times. This problem is also known to be NP-complete (see, e.g., [31]). Without 
loss of generality, we can assume that all variables appear both positively and negatively in the formula. 
If a variable appears only positively (respectively, negatively) then we can create a simpler, equivalent 
formula by setting the value of the variable to true (respectively, false). Let .be a CNF formula of 
N variables and M clauses in which each clause has at most 3 literals and each variable appears at most 
three times. Let x1,...,xN be the variables of .and C1,...,CM be the clauses of .. Let .in and .out be 
two distinct labels. We will de.ne a se­quence T whose .rst label is .in and whose last label is .out. 
.out appears nowhere else in the sequence. We will consider the prob­lem of creating a k-linked path 
between the two. The sequence is of the form .inSG1 ···GN T1 ···TM .out. The sequence S is the k repetitions 
of .1 ···.k separated by k-1 .in s. where .1,...,.k are kpairwise distinct labels. Recall that for a 
pair of positions to be k-linked there must be a set of intermediate points with pairwise distinct labels 
in which the reuse distance between each neighboring intermediate points is at most k. To create such 
a path between our two end points, the subsequence S must be traversed without visiting a same label 
more than once so that the distance between the two neighboring visited points have reuse distance at 
most k. The only way to construct such a path is to visit every (k+1)st element of S besides the .rst 
.in, exiting at the .rst element after S. This path visits .1,...,.k exactly once. This means that any 
k-link path between our two endpoints should not visit any one of .1,...,.k+1 again. For each xi appeared 
in the formula, 1 =i =N, Gi if of form ai,1.i,1.i,2.i,3.1 ···.k-2.i,1ai,2.1 ···.k-1. The a s here appear 
nowhere else in the sequence. Each . appears at most once elsewhere. If it does indeed, it appears in 
one of the T s. Suppose that a k-linked path between the two endpoints lands on ai,1. Then the path can 
only be threaded in Gi using one of the following paths: 1. [.i,3,ai,2], 2. [.i,3,.i,1,ai,2], 3. [.i,2,.i,3,ai,2], 
 4. [.i,2,.i,3,.i,1,ai,2], 5. [.i,2,.i,1,ai,2],  6. [.i,1,.i,2,.i,3,ai,2],and 7. [.i,1,.i,3,ai,2]. 
  Consider the set of all . s that has not been visited yet. The set is 1. {.i,1,.i,2}, 2. {.i,2}, 
3. {.i,1}, 4. Ø, 5. {.i,3}, 6. Ø, 7. {.i,2}.  Two crucial observations here are that (a) there 
is no set that contains .i,3 and one extra element and (b) that the .rst set has both .i,1 and .i,2. 
Suppose that xi appears three times in the formula, twice as xi and once as xi. Then we use .i,1 to denote 
the .rst occurrence of xi, .i,2 to denote the second occurrence of xi,and .i,3 to denote xi. In the case 
when xi appears twice and xi appears once, we use .i,1 to denote the .rst occurrence of xi, .i,2 to denote 
the second occurrence of xi,and .i,3 to denote xi. In the case when both xi and xi appear only once, 
we use .i,1 to denote the unique occurrence of xi and .i,3 to denote the unique occurrence of xi. Note 
that all of these possible paths must land the .rst element after Gi. For each i, 1 = i = M, such that 
Ci has exactly two literals, Ti is of the form ßi,1.i,1.i,2.3 ···.k.ßi,2.2 ···.k, and for each i, 1 
= i = M, such that Ci has exactly three literals, we construct Ti as ßi,1.i,1.i,2.i,3.4 ···.k.ßi,2.2 
···.k, where .i,l is the lth literal of Ci. Note here that the literals in the clause are replaced using 
. s in the sequence according to the rules in the construction of G s. Suppose that the k-linked path 
between our two endpoints land on ßi,1. Since there are k labels between ßi,1 and ßi,2 and none of the 
. s can be visited again, the k-linked path can only be extended if one of the . literals is visited. 
The segment after ßi,2 forces the path to land on the element right after Ti. we can see that S is of 
length k(k +1) - 1, for each i, 1 = i = N, Gi has length 2k+3, and for each i, 1 = i = M, Ti has length 
2k +1. So, the total length of the sequence, including the two endpoints, is 2+ k(k+1) -1+ N(2k+3)+ M(2k+1), 
 which is equal to k(2N +2M + k+1)+3N + M +1, which is polynomial of the size of the CNF formula. So 
the construction can be done in polynomial time. We view the literals that are visited in Ti as those 
satis.ed by the assignment represented by the path. For such a path to be valid, the selections in the 
T sections have to be made so that the literals satisfying the clauses are still available. Suppose that 
. is satis.able. Let A be a satisfying assignment of .. Construct the path within T s so that the those 
that are visited are precisely those that are satis.ed by A. Then it is possible to select the paths 
in G so that none of those visited in T are visited in G. So, the two endpoints are k-lined. On the other 
hand, suppose that . is not satis.able. Take any potentially k-linked path pin the T s. There exist at 
least one vari­able, xi for which both one occurrence of xi and one occurrence of xi is selected. Then 
it is not possible to construct a k-linked path within Gi, so there is no k-linked path between the two 
endpoints. We note here that the set of labels, ., which is the part of the instance is the set of all 
labels that we ve de.ned. By now, we have constructed a polynomial-time many-one reduction from 3-SAT 
to Pw-k-A.. Since Pw-k-A. apparently belongs to NP,weprove that Pw-k-A. is a NP-complete problem. Corollary 
1 For k = 3, the problem of checking reference af.nity groups is NP-complete. Proof Suppose the group 
of data elements is G. First, let s show that this problem belongs to NP. This can be done by .rst guessing 
the possible supersets of G,say G. For every two different data elements x, y . G, for every ax, we guess 
it can be connected to the nearest ay located left-side or right-side, and then we guess a link-path 
between them and then verify if this is a link path of link­length k. If it is, then continue to check 
other ax s and then other pairs of data elements. But if not, it will just refuse to accept. We can check 
for all of the pairs and all accesses of x in a sequential way. If every pairs and every accesses are 
checked to be linked successfully, then accept. By the de.nition of reference af.nity group, for any 
x, y . G, for all ax, we need to check if there exists an ay, such that ax and ay are k-linked. The only 
way is to check if there is a k-linked path from ax to the left-side or right-side nearest ay.Sowecan 
see that if there is a polynomial-time algorithm for checking reference af.nity problem, then there is 
a polynomial-time algorithm for Pw-k-A. problem. Thus we have proved that for k = 3,checking reference 
af.nity group problem is NP-complete problem. Corollary 2 For k = 3, the problem of .nding reference 
af.nity groups is NP-hard. Proof The proof is quite straightforward. If there is a polynomial­time solution 
that can .nd out the reference af.nity groups, then we can solve the problem of checking reference af.nity 
groups in polynomial time. This contradicts with Corollary 1. Theorem 2 For k =2, Pw-k-A. is NL-complete. 
Proof 2 -CNF-SAT is the problem of testing whether a given conjunctive normal form formula with two literals 
per clause is satis.able. This problem is the standard NL-complete problem. By following the proof of 
Theorem 1 with k =2, we can show that the 2 -CNF-SAT is reducible to Pw-k-A. for k =2. To prove that 
PWkAff belongs to NL for k =2, suppose that a set of labels ., a sequence S= {si}M i=1 over ., an integer 
k = 0, and two integers I and J, 1 = I = J = M are given as an instance to the problem. We wish to test 
whether I and J are k-linked. Since the elements before the Ith entry and those after the Jth are irrelevant 
to the problem at hand, we may assume, Without loss of generality, that I =1 and J = M. Also, if the 
ith entry and the (i +1)st entry are the same, at most one of the two can be visited, and if one is visited 
at all which one doesn t matter. So, one of them can be safely removed. This means that, for all i, 2 
= i = M - 2, si = si+1. For each i, 2 = i = M -1,let yi be the variable that represents whether the ith 
element is visited. We construct a formula . by joining the following size-two clauses: for each i, 
2 = i = M - 2, (yi . yi+1),and  for all . . S and for all i and j such that 2 = i<j = M - 1 and si = 
sj = ., (yi . yj ).  Suppose that this formula is satis.able. Let A be a satisfying assignment of the 
formula. Then A clearly de.nes a k-linked path, since only those belonging to S are visited, no element 
in S is visited more than once, and there is at most one entry between any two neighbors on the path. 
Similarly, if there is a k-linked path, then by setting the truth value of each variable according to 
whether the node is included in the path, we can satisfy the formula. So, the satis.ability of the formula 
is equivalent to the existence of a k­ linked path. Theorem 3 For k =2, the problem of .nding reference 
af.nity groups is in P . Algorithm 1 can be found in Section 3. Here we present the detailed proof. Proof 
First let us show this is a polynomial-time algorithm. By Theorem 2, we need polynomial time to test 
whether two data accesses are 2-linked. Hence, testing if two data elements is 2­linked with respect 
to a given group can be done in polynomial time. Constructing the graph G needs only polynomial time. 
For the reference af.nity group that x belongs to, we remove at most m data elements from the group, 
where m is the number of data elements in the trace. There are at most m reference af.nity groups. Therefore, 
the algorithm takes polynomial time. Next we prove the correctness. First, it is easy to see that the 
groups found by this algorithm satisfy the .rst condition of refer­ence af.nity. Second, let us show 
every group is the maximal size possible. We show that the algorithm removes z correctly. Remov­ing z 
at step 7 is straightforward. The correctness of the removal of z at step 10 can be proved by contradiction. 
Suppose z and x be­long to the same group G1.Wehave y/. G1. From the algorithm, an access ay cannot be 
2-linked to any access of z. Since x and y are 2-linked, there are some accesses of x that is 2-linked 
to ay.We pick the nearest one as ax. Without loss of generality, we assume ax appears at the right side 
of ay. Similarly, we choose az, which is 2-linked to and nearest to the ax. This az can not appear on 
the left side of ax. Otherwise, we have two cases. First, if az appears between ay and ax, then the path 
from ay to ax must pass the very data element at the right side of az, since k =2.Thenthe ay can be 2-linked 
to this az by replacing the very data element with az , which is a contradiction. Second, if az appears 
on the left side of ay, since x and z are in the same group, a path exists from ax to az without passing 
ay. This path must land on the very data element at the right side of ay, since k =2. Then we can replace 
the very data element with ay and get a new path from ay to az, which is also a contradiction. Now let 
s select the leftmost data element in G1 that appears on the section of trace between the ay and az. 
Suppose it is al. This is shown in Sequence (2). ...y...l...x...z... (2) We .rst show that a path exists 
from ay to al with respect to S (G - G1) {l}.Since ay is 2-linked to ax with respect to group G, there 
is a path p connecting them. If p does not pass al, it must pass the very data element at the left side 
of al, since k =2.Anew path p1can be generated from ay to al by .rst reaching the very data element and 
then one step further to al.If p passes al, then we pick the segment from ay to al as p1. All of the 
data elements on the path p1 is in G - G1 except for l. Since l is in the same group with z, there is 
a path p2 from al to az with respect to G1. We get a new path p. by merging paths p1 and p2.Now p. is 
a 2-linked path without duplicated data elements from ay to az, which is a contradiction with step 9. 
Theorem 5 For k =1, there is a polynomial-time solution for .nding reference af.nity groups. ALGORITHM 
3. Finding reference af.nity groups when k=1 procedure FindReferenceAf.nityGroup 1(T) 1: {T is the trace, 
k =1} 2: encode the data elements according to the order of appearance in the trace. Suppose there are 
mdistinct data elements. case 1 3: while there exist data not yet grouped do 4: pick the smallest not 
yet grouped datum s. case 2 5: for t=mto sstep -1 do 6: if IsAGroup(T,s,t) then case 3 7: break; 8: 
end if ax 9: end for case 4 10: output elements in {s,...,t} as a group. { 11: end while case 5 endFindReferenceAf.nityGroup 
1 case 6 procedure IsAGroup(T,s,t) 1: for ifrom 1 to |T| do 2: if T[i] is within sand tthen a x a x a 
x (au) au a x a x (av) av a x a x a x  3: if The elements T[i] can be 1-linked to with respect 
to (a) {s,...,t} can not cover set {s,...,t} then 4: return false; 5: end if a x  6: end if case 1 
7: end for 8: return true; case 2 { endIsAGroup It is straightforward to show that the algorithm is 
polynomial ax time and can output the correct reference af.nity groups. Lemma 1 Given two different data 
elements u and v;their accesses au and av where the volume distance from au to av is exactly k; and a 
third access ax, which happens between au and case 3 x in the trace; then there exists an access a between 
au and a v . x such that the volume distance from a u and the volume to a a v distance from a to av are 
both less than k. x Proof The element u is either earlier or later than v in the data (b) space. Because 
a link and a path are not directed, the two cases are symmetrical. Without loss of generality, we assume 
uis before Figure 12. Cases in proving the insertion lemma. au and av show v. Consider the smallest m-block 
that contains u,v,au,av. The the possible positions in the trace. a x , and a x show the possible targets 
of moving x to split trace between au and av. The arrows show the destination of moving x. For every 
case, there is a solution for xto break the k-link in the old path. element x must be in the data section 
of the block; otherwise the path from au to av does not go through x. There are two cases shown by the 
two graphs in Figure 12, each has six sub-cases. The location of ax is given for each sub-case in the 
.gure. In most cases, ax splits the k-link from au to av into two shorter links of less than k. The .rst 
case is when au and av are in upper and lower half blocks. In the .rst sub-case of the .rst case, we 
need to use one [4] K. Beyls and E. D Hollander. Reuse distance-based cache hint of the two locations, 
marked by a and a . Then we use the au x x selection. In Proceedings of the 8th International Euro-Par 
Conference, Paderborn, Germany, August 2002. to break the link from the access of xto av and treat the 
access to x as au. The last sub-case of the .rst case is similar. The second [5] B. Calder, C. Krintz, 
S. John, and T. Austin. Cache-conscious data case happens when au and av are both in the upper or lower 
half placement. In Proceedings of the Eighth International Conference block. Figure 12 shows the three 
out of the six sub-cases when both on Architectural Support for Programming Languages and Operating accesses 
are in the upper half block. The other three sub-cases are Systems (ASPLOS-VIII), San Jose, Oct 1998. 
symmetrical. In sub-case 1 and 3, we pick a to be in the middle x on the same side of ax. In sub-case 
2, we use one of the two middle points depending on the position of ax. References <RefA>[1] A. Aggarwal, B. 
Alpern, A. Chandra, and M. Snir. A model for hierarchical memory. In Proceedings of the ACM Conference 
on Theory of Computing, New York, NY, 1987. [2] B. Alpern, L. Carter, E. Feig, and T. Selker. The uniform 
memory hierarchy model of computation. Algorithmica, 12(2/3):72 109, 1994. [3] M. A. Bender, E. D. Demaine, 
and M. Farach-Colton. Cache­oblivious b-trees. In Proceedings of Symposium on Foundations of Computer 
Science, November 2000. [6] D. Chandra, F. Guo, S. Kim, and Y. Solihin. Predicting inter­thread cache 
contention on a chip multi-processor architecture. In Proceedings of the International Symposium on High 
Performance Computer Architecture (HPCA), 2005. [7] S. Chatterjee, V. V. Jain, A. R. Lebeck, S. Mundhra, 
and M. Thot­tethodi. Nonlinear array layouts for hierarchical memory systems. In Proceedings of International 
Conference on Supercomputing, 1999. [8] T. M. Chilimbi. Ef.cient representations and abstractions for 
quantifying and exploiting data reference locality. In Proceedings of ACM SIGPLAN Conference on Programming 
Language Design and Implementation, Snowbird, Utah, June 2001. [9] T. M. Chilimbi, M. D. Hill, and J. 
R. Larus. Cache-conscious structure layout. In Proceedings of ACM SIGPLAN Conference on Programming Language 
Design and Implementation, Atlanta, Georgia, May 1999. [10] E. Dahlhaus, D. S. Johnson, C. H. Papadimitriou, 
P. D. Seymour, and M. Yannakakis. The complexity of multiway cuts. In Proceedings of the 24th Annual 
ACM Symposium on the Theory of Computing,May 1992. [11] A. Darte. On the complexity of loop fusion. Parallel 
Computing, 26(9):1175 1193, 2000. [12] C. Ding and K. Kennedy. Improving cache performance in dynamic 
applications through data and computation reorganization at run time. In Proceedings of the SIGPLAN 99 
Conference on Programming Language Design and Implementation, Atlanta, GA, May 1999. [13] C. Ding and 
K. Kennedy. Improving effective bandwidth through compiler enhancement of global cache reuse. Journal 
of Parallel and Distributed Computing, 64(1):108 134, 2004. [14] C. Ding and Y. Zhong. Predicting whole-program 
locality with reuse distance analysis. In Proceedings of ACM SIGPLAN Conference on Programming Language 
Design and Implementation, San Diego, CA, June 2003. [15] C. Fang, S. Carr, S. Onder, and Z. Wang. Instruction 
based memory distance analysis and its application to optimization. In Proceedings of International Conference 
on Parallel Architectures and Compilation Techniques, St. Louis, MO, 2005. [16] J. D. Frens and D. S. 
Wise. Auto-blocking matrix-multiplication or tracking BLAS3 performance with source code. In Proceedings 
of the ACM SIGPLAN Symposium on Principles Practice of Parallel Programming, Las Vegas, NV, 1997. [17] 
J. D. Frens and D. S. Wise. QR factorization with Morton-ordered quadtree matrices for memory re-use 
and parallelism. In Proceedings of the ACM SIGPLAN Symposium on Principles Practice of Parallel Programming, 
San Diego, CA, 2003. [18] M. Frigo, C. E. Leiserson, H. Prokop, and S. Ramachandran. Cache­oblivious 
algorithms. In Proceedings of Symposium on Foundations of Computer Science, October 1999. [19] N. Gloy 
and M. D. Smith. Procedure placement using temporal­ordering information. ACM Transactions on Programming 
Languages and Systems, 21(5), September 1999. [20] H. Han and C. W. Tseng. Locality optimizations for 
adaptive irregular scienti.c codes. Technical report, Department of Computer Science, University of Maryland, 
College Park, 2000. [21] J. Han and M. Kamber. Data Mining: Concepts and Techniques. Morgan Kaufmann 
Publishers, 2000. [22] R. E. Hank, W. W. Hwu, and B. R. Rau. Region-based compilation: An introduction 
and motivation. In Proceedings of the Annual International Symposium on Microarchitecture, 1995. [23] 
K. Kennedy and U. Kremer. Automatic data layout for distributed memory machines. ACM Transactions on 
Programming Languages and Systems, 20(4), 1998. [24] K. Kennedy and K. S. McKinley. Typed fusion with 
applications to parallel and sequential code generation. Technical Report TR93­208, Dept. of Computer 
Science, Rice University, Aug. 1993. (also available as CRPC-TR94370). [25] I. Kodukula, N. Ahmed, and 
K. Pingali. Data-centric multi-level blocking. In Proceedings of the SIGPLAN 97 Conference on Programming 
Language Design and Implementation, Las Vegas, NV, June 1997. [26] G. Marin and J. Mellor-Crummey. Cross 
architecture performance predictions for scienti.c applications using parameterized models. In Proceedings 
of Joint International Conference on Measurement and Modeling of Computer Systems, New York City, NY, 
June 2004. [27] G. Marin and J. Mellor-Crummey. Scalable cross-architecture predictions of memory hierarchy 
response for scienti.c applications. In Proceedings of the Symposium of the Las Alamo s Computer Science 
Institute, Sante Fe, New Mexico, 2005. [28] R. L. Mattson, J. Gecsei, D. Slutz, and I. L. Traiger. Evaluation 
techniques for storage hierarchies. IBM System Journal, 9(2):78 117, 1970. [29] K. S. McKinley, S. Carr, 
and C.-W. Tseng. Improving data locality with loop transformations. ACM Transactions on Programming Languages 
and Systems, 18(4):424 453, July 1996. [30] J. Mellor-Crummey, D. Whalley, and K. Kennedy. Improving 
memory hierarchy performance for irregular applications. International Journal of Parallel Programming, 
29(3), June 2001. [31] C. H. Papadimitriou. Computational Complexity. Addison Wesley, 1994. [32] E. Petrank 
and D. Rawitz. The hardness of cache conscious data placement. In Proceedings of ACM Symposium on Principles 
of Programming Languages, Portland, Oregon, January 2002. [33] X. Shen, Y. Zhong, and C. Ding. Regression-based 
multi-model prediction of data reuse signature. In Proceedings of the 4th Annual Symposium of the Las 
Alamos Computer Science Institute, Sante Fe, New Mexico, November 2003. [34] X. Shen, Y. Zhong, and C. 
Ding. Locality phase prediction. In Proceedings of the Eleventh International Conference on Architect 
ural Support for Programming Languages and Operating Systems (ASPLOS XI), Boston, MA, 2004. [35] M. Snir 
and J. Yu. On the theory of spatial and temporal locality. Technical Report DCS-R-2005-2564, Computer 
Science Dept., Univ. of Illinois at Urbana-Champaign, July 2005. [36] M. M. Strout, L. Carter, and J. 
Ferrante. Compile-time composition of run-time data and iteration reorderings. In Proceedings of ACM 
SIGPLAN Conference on Programming Language Design and Implementation, San Diego, CA, June 2003. [37] 
M. M. Strout and P. Hovland. Metrics and models for reordering transformations. In Proceedings of the 
2nd ACM SIGPLAN Workshop on Memory System Performance, Washington DC, June 2004. [38] K. O. Thabit. Cache 
Management by the Compiler. PhD thesis, Dept. of Computer Science, Rice University, 1981. [39] B. S. 
White, S. A. McKee, B. R. de Supinski, B. Miller, D. Quinlan, and M. Schulz. Improving the computational 
intensity of unstructured mesh applications. In Proceedings of the 19th ACM International Conference 
on Supercomputing, Cambridge, MA, June 2005. [40] M. E. Wolf and M. Lam. A data locality optimizing algorithm. 
In Proceedings of the SIGPLAN 91 Conference on Programming Language Design and Implementation, Toronto, 
Canada, June 1991. [41] Q. Yi, V. Adve, and K. Kennedy. Transforming loops to recursion for multi-level 
memory hierarchies. In Proceedings of ACM SIGPLAN Conference on Programming Language Design and Implementation, 
Vancouver, Canada, June 2000. [42] C. Zhang, Y. Zhong, C. Ding, and M. Ogihara. Finding reference af.nity 
groups in trace using sampling method. Technical Report TR 842, Department of Computer Science, University 
of Rochester, July 2004. presented at the 3rd Workshop on Mining Temporal and Sequential Data, in conjunction 
with ACM SIGKDD 2004. [43] Y. Zhong, S. G. Dropsho, and C. Ding. Miss rate prediction across all program 
inputs. In Proceedings of the 12th International Conference on Parallel Architectures and Compilation 
Techniques, New Orleans, Louisiana, September 2003. [44] Y. Zhong, M. Orlovich, X. Shen, and C. Ding. 
Array regrouping and structure splitting using whole-program reference af.nity. In Proceedings of ACM 
SIGPLAN Conference on Programming Language Design and Implementation, June 2004.</RefA>  
			
