
 A Parallel Algorithm For Linear Programming in Fixed Dimension Martin Dyer* School of Computer Studies 
University of Leeds, Leeds, UK. email: dyerOscs.leeds. ac.uk Abstract We describe a parallel algorithm 
for linear programming in fixed dimension which runs in deterministic time O(log n(log log n)~-l ) on 
an EREW PRAM. This improves a previous bound of of O((log n)2) for this problem. Introduction Deng [9] 
gave a parallel algorithm for linear programming in two dimensions which runs in O(log n) time on a CREW 
PRAM, using O(n/ log n) processors. The method does not generalise to higher dimensions, but he asked 
the question as to whether an algorithm with similar running time can be found. In this paper, we go 
part of the way to answering this question in the affirmative by describ­ ing a parallel algorithm for 
linear program­ ming in fixed dimension which runs in time O(log n(log log n)~-l) on an EREW PRAM. The 
previous best (deterministic) time bound for this problem in. general fixed dimension *Supported in part 
by SERC Grant GR/H 77811 Permission to copy without fee all or part of this material is granted provided 
that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice 
and the title of the publication and its date appear, and notice is given that copying is by permission 
of the Association of Computing Machinery.To copy otherwise, or to republish, requires a fee and/or specific 
permission. 1Ith Computational Geometry, Vancouver, B.C. Canada @ 1995 ACM 0-89791 -724 -3/95/0006 ...$3.50 
is, as far as we know, an O((log n)2) algo­rithm obtained by derandomiz;ng Clarkson s al­gorithm [6] 
by the method of Chazelle and Ma­touiek [5], but using the more explicit and sim­plified treatment of 
Goodrich [11]. We note in passing that Alon and Megiddo [3] gave a randomized parallel implementation 
of Clark­son s algorithm which runs in constant time on a CREW PRAM with high probability in fixed dimension. 
Ajtai and Megiddo [2] have given an algo­rithm which runs in O((log log n)~) time in fixed dimension 
on a particular model of parallel computation which is discussed in Section 2 be­low. Their algorithm 
is a clever parallel imple­mentation of the idea of Megiddo [13], The al­gorithm described here is also 
a parallel imple­mentation of Megiddo s algorithm which, while being more straightforward, appears to 
possess most of.the power of that of Ajt ai and Megidclo. 2 Background As mentioned above, the algorithm 
developed here is essentially a parallel implementation of the idea of Megiddo [13]. His method is based 
on rapidly eliminating a proportion of the constraints using few hyperplane queries . Megiddo s hyperplane 
query problem has the following form. We have some unknown point x* (which we choose to be the linear 
program optimum), and a set of n hyperplanes, lYi={z< lR:~.xi=/3i} (ill,..., n) (which we choose to 
be the linear constraints). We have to determine sign (~~.x b;), for each i = 1,..., n, where sign: 
IR 4 { 1, O, +1}. (Clearly such information is ad­ equate to solve a linear program. ) There is an oracle 
which, given an arbitrary hyperplane H = {x : ci.z = ~}, will return sign (a.z b). Megiddo s result 
is that in fixed dimension we can determine the signs of a constant propor­tion of the hyperplanes using 
only a (carefully selected) constant number of queries to the ora­cle. In linear programming a hyperplane 
query is a linear program in smaller dimension. The method is recursive on dimension. For simplic­ity 
let us ignore possible degeneracies. Then we may assume without loss that ail = 1 for i=l ,. ... n. The 
dimensional recursion results from pairing hyperplanes so that each pair has one member with ai2 >0 and 
one with a,2 <0. In order to ensure that about ~n pairs can be formed, a rotation on the first two coordinates 
is performed after a median calculation. There is a straightforward parallel implemen­tation of Megiddo 
s algorithm [13] which runs in O((log n)~) time on a EREW PRAM. This algorithm is rather inefficient 
in terms of pro­cessor utilisation, since at the later stages, when there are few constraints remaining, 
most pro­cessors are idle. In order to improve mat­ters, it is necessary to remedy this defect, Ajtai 
and Megiddo [2] gave an algorithm based on the use of an expander graph to select more non-disjoint pairs 
to utilise the idle processors and obtain more rapid elimination. Their al­gorithm runs in Cl((log log 
n)~) time in a non­standard model of parallel computation, Their model allows O(log log n) time median 
selection from n numbers using n processors. This is known to be possible [1] in Valiant s compar­ison 
model [14], but is also known to require Q(log n/ log log n) even on a CRCW PRAM [4]. The Ajtai-Megiddo 
model also uses a non­uniform scheme for O(log log n) time processor allocation, again based on the use 
of expander graphs. Here we work within the standard PRAM model, and show that rapid elimination can 
be achieved without the use of expander graphs. The method is based on using groupings larger than pairs 
in order to achieve full processor util­isation. 3 The Search Problem We will consider the hyperplane 
search prob­lem. Suppose we have n hyperplanes and m z n processors. Let us say a hyperplane is lo­cated 
if sign (a~. z 13;) has been determined. We wish to locate a proportion of the given hy­perplanes in 
a constant number of rounds. Each repetition of the basic recursion step in the loca­tion process will 
be called a round. Each round contains some non-trivial computations, but we will ignore this initially. 
However, we will en­sure that there are sufficient processors avail­able for all the computations associated 
with any given round. Let us again assume, for ease of exposition, that degeneracies do not occur, and 
so ail = O foralli=l,..., n. (For a more careful treat­ment of the degeneracy issue in the context of 
this search problem, see [10].) The method is then as follows. Choose a suitable group size r = max{2, 
l(rn/n)l/~j }, and let s = n/r. For sim­plicity let us assume s is an integer. For k = 1 , . . . . r, 
determine the ksth largest value of ai2 (i=l,... , n), Ok say. Let O. = GO. These val­ues induce a partition 
of the set of hyperplanes Hi into r sets, according as ok-l < a~z < ok (k= l,..., r). Now form s groups 
of size r so that each group has exactly one H; from each of these r sets. (This is analogous to the 
pair­ing in Megiddo s sequential algorithm. ) In each group, consider each pair of hyperplanes Hi, Hj 
say, where a;z s aj2. There will exist ok, ~e such that ak C [ai2, aj2 ] and al @ [aiz, aj2] (1 S k, 
1< r). Make a change of variables so that If we now eliminate each of z!, zj in turn, we produce hyperplanes 
Hij, Hj; such that if we can locate both Hij and Hji, we can locate at least one of Hi, Hj. (See [13]. 
) But locating the Hij is essentially a (d I)-dimensional problem, since all such hyperplanes have a~2 
= O. Note that there are only r different variables x! or X; in this process. Thus there are r collections 
of (d l)-dimensional hyperplanes. We recursively locate a proportion ~f the hyperplanes in each of these 
collections. We will repeat the location process at the (d 1)-dimensional level eight times, each time 
removing from consideration those hyperplanes already located. This is simply to boost the proportion 
sufficiently, and the number eight has no significance other than it is large enough for this purpose. 
This idea of increasing the proportion was used in [7, 10], in relation to Megiddo s algorithm,, There 
is a difference here, however. We cannot wait for the location on each collection before we start on 
the next as is done in the sequential algorithm, since this uses the accumulated information to guide 
the query process. This is no longer feasible be­cause the number of collections, r, is not con­stant 
if m >> n. Thus we have to operate on all collections in parallel. At the bottom level, we have zero­dimensional 
hyperplanes, i.e. points. We can now locate with respect to a proportion (r 1)/r of these points in one 
round. We do this by selecting the [jn/rj th largest points for j = 1,2 ,. ... r 1. The resulting queries 
will in fact require solving up to three linear programs in (d 1) dimensions. Note that solving a one­dimensional 
linear program reduces to a simple maximum or minimum computation. 4 Analysis of the search algorithm 
Let the number of rounds for location in di­mension d be Td, and let pd denote the pro­portion of d-dimensional 
hyperplanes which are located. Each group of size r gives rise to r(r 1) hyperplanes in (d 1) dimensions. 
These are partitioned into r collecticms, accord­ing to the selection computations. Location at the (d 
 I )-dimensional level implies that all T(T- 1) hyperplanes from the same group will have been located 
for a fraction of at least 1 r(r 1)(1 p&#38;.~ )8 of the groups, since there are eight repetitions 
of the (d 1)-dimensional process. Now, we will have located at least (r 1) d-dimensional hyperplanes 
in each of these groups. (If this were not the case, there would be some pair of hyperplanes for which 
we had not located either, a contradiction. ) Hence we have the recurrence We also have, by assumption, 
pl = (r 1)/r and T1= 1. It is easy to check, by induction, that for dz 2, Also, clearly, Td = 8Td_l 
for d > 2, and since T1 = 1 this gives Td = 8d-1 rounds. Thus, in fixed dimension, only a constant number 
of rounds are required for the search procedure we have described. We have only to check that there are 
suffi­cient processors for this search procedure. Now the tot al number of (d 1)-dimensional hyper­planes 
generated simultaneously is n/r x r(r 1) = n(r 1). Thus the total number of hy­perplanes simultaneously 
generated in one di­mension is at most n(r 1)~ 1 ~ m. We there­fore have enough processors to allocate 
one to each hyperplane simultaneously in each dimen­sion. The total number of queries generated at the 
bottom of the recursion is r~ l (?--1), since r collections are generated at each inter­mediate level, 
and (r 1) queries are generated at the bottom level. We need n processors for each of these queries, 
since they are linear pro­grams with n constraints. So we require that n(r l)rd l < m, which is clearly 
true by choice of r. We have merely ensured that we have suffi­cient processors, but we may note that 
the to­tal storage requirement is only a constant factor larger (in fixed dimension). 5 The linear programming 
algorithm To solve linear programs, we simply iterate the above search procedure, starting with the n 
hy ­perplanes assigned one to each of n processors. Let ni be the number of d-dimensional hy­perplanes 
remaining at the start of iteration i. Thus nl = n, and the number of proces­sors m is equal to n throughout. 
Thus we choose the group size r; at iteration i so that ri = [(n/ni)l/~j. Then we will have It follows 
that ni+l < rein{ ~ni, 3ni d+l)ld/n ld}. Letting ~, = ni/n, we have from which it follows easily that 
~i ~ l/n (i.e. ni < 1) after O(log log n) iterations. However, each iteration involves solving (in parallel) 
(d 1)-dimensional linear programs. Thus the total number of (nested) iterations is O((log log n)d-l 
). The bottom (one dimen­sional) level requires a maximum or minimum, which is a selection, which we 
have disregarded so far. Thus only O((log log n)d-l ) rounds are re­quired in total (in fixed dimension). 
So far we have ignored the work per round. We must now account for this. Each round requires a selec­tion 
operation. This can be done in O(log n) time on an EREW PRAM by, for example, sorting on the value of 
ai2 using Cole)s algo­rithm [8]. The group formation can then also be accomplished easily. Constructing 
the new collections by pair formation requires broad­casting a hyperplane known by one processor to at 
most r others. This can be achieved in O(log r) = O(log n) time. Finally, deleting lo­cated hyperplanes, 
and re-organizing the unlo­cated hyperplanes for the next iteration can be achieved by prefix sum computation 
in O(log n) time. Thus the hidden work per round can be carried out in O(log n) time. The total time 
bound of O(log n(log log n)d-l ) for the entire al­gorithm now follows. 6 Remarks While we have described 
the algorithm for an EREW PRAM, we may note that the algo­rithm could be run in O((log log n)d) time 
on the parallel computation model of Ajt ai and Megiddo [2]. (Their processor allocation tech­nique would 
also be required, however. ) Observe that there is a O(log n) time lower bound on finding the maximum 
of n numbers on an EREW PRAM [12, p. 889]. Since one­dimensional linear programming amounts to computing 
a maximum, it follows that there is a O(log n) tifie lower bound for fixed-dimensional linear programming. 
Hence the O((log n)l+ (ll) upper bound for running time on an EREW PRAM achieved here is not too far 
from opti­mal. References M. Dyer, On a multidimensional search <RefA>[1]M. Ajtai, J. Kom16s, W. Steiger and 
E. Szemer&#38;di, I)eterministic selection in O(log log n) time, Proc. 18th Ann. Symp. on Theory of Computing 
(1986), 188-195. [2] M. Ajtai and N. Megiddo, A deterministic poly(log log n)-time n-processor algorithm 
for linear programming in fixed dimension, Proc. 2dth Ann. Symp. on Theory of Com­puting (1992), 327-338. 
[3] N. Alon and N. Megiddo, Parallel lin­ear programming almost surely in constant time, Proc. 31st IEEE 
Symp. on Foun­dations of Computer Science (1990), 574­ 582. [4] P. Beame and J. Hastad, Optimal bounds 
for decision problems on the CRCW PRAM, Proc. 19th Ann. Symp. on Theory of Computing (1987), 83 93. [5] 
B. Chazelle and J. Matou5ek, On linear­time deterministic algorithms for optimiza­tion problems in fixed 
dimension, Proc. dth ACM-SIAM Symp. on Discrete Algorithms (1993), 281-290. [6] K. Clarkson, A Las Vegas 
algorithm for linear programming when the dimension is small, Proc. 29st IEEE Symp. on Foun­dations of 
Computer Science (1988), 452­ 456. [7] K. Clarkson, Linear programming in O(n x 3d2) time, Information 
Processing Letters 22 (1986), 21-27. [8] R. Cole, Parallel merge sort, SIAM J. on Computing 17 (1988), 
770-785. [9] X. Deng, An optimal parallel algorithm for linear programming in the plane, Informa­tion 
Processing Letters 35 (1990), 213-217. problem and its application to the Eu­clidean one-centre problem, 
SIAM J. on Computing 15 (1986), 725-738, 11] M. Goodrich, Geometric partitioning made easier, even in 
parallel, Proc. 9th ACM Symp. on Computational Geometry (1993), 73-82. 12] R. Karp and V. Ramachandran, 
Parallel algorithms for shared-memory machines, in Handbook of Theoretical Computer Sci­ence Vol. A: 
Algorithms and Complexity, Ed. J. van Leeuwen, Elsevier, Amsterdam, 1990, pp. 869-941. [13] N. Megiddo, 
Linear programming in linear time when the dimension is fixed, J. of the ACM31 (1984), 114-127. [14] 
L. Valiant, Parallelism in comparison prob­lems, SIAM J. on Computing 4 (1975), 348-355.</RefA> 
			
