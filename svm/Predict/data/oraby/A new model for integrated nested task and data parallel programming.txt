
 A New Model for Integrated Nested Task and Data Parallel Programming Jaspal Subhlok and Bwolen Yang 
School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 {jass ,bwolen}Ocs. cmu.edu 
 Abstract High Performance Fortran (HPF) has emerged as a stan­dardlanguage fordata parallel computing. 
However, awide variety of scientific applications are best programmed by a combination of task and data 
parallelism. Therefore, a good model of taak parallelism is important for continued success of HPF for 
parallel progr amrning. This paper presents a taak parallelism model that is simple, elegant, and relatively 
easy to implement in an HPF environment. Task parallelism is exploited by mechanisms for dividing processors 
into sub­groups and mapping computations and data onto processor subgroups. This model of taak parallelism 
has been im­plemented in the Fx compiler at Carnegie Mellon Univer­sity. The paper addresses the main 
issues in compiling inte­grated task and data parallel programs and reports on the use of this model 
for programming various flat and nested task structures. Performance results are presented for a set 
of programs spanning signal processing, image processing, computer vision and environment modeling. A 
vaxiant of this task model is a new approved extension of HPF and this paper offers insight into the 
power of expression and ease of implementation of this extension. Introduction Use of high level languages 
is gaining popularity as a means to portable and efficient parallel programming. Research and development 
in parallel languages has primarily focused on data parallelism [4, 11, 18] and High Performance For­tran 
[12] has emerged as a promising standard language for parallel programming. A disadvantage of using a 
parallel Effort sponsored by the Advanced Research Projects Agency and Rome Laboratory, Air Force Materiel 
Command, USAF, under agree­ ment number F30602-96-1-0287. The U.S. Government is autho­ rized to reproduce 
and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. 
The views and conclusions contained herein are those of the authors and should not be interpreted aa 
necessarily representing the official policies or endorsements, either expressed or implied, of the Advanced 
Research Projects Agency, Rome Laboratory, or the U.S. Govern­ment. Permission to make digital/hard copy 
of par~ or all this work for personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advan­tage, the copyright notice, the title of the publication 
and its date appear. and notice is given ~hat copying is by permission of ACM, Inc. To copy otherwise, 
to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or 
a fee PPoPP 97 Las Vegss. NV m 1997 ACM 0-89791 -906-8/97/0006.. $3.50 language like HPF, aa compared 
to explicit parallel program­ming using, e.g., a message passing library like MPI [8], is that the ueer 
is constrained by the models of parallelism supported by the language. Application developers are of­ten 
reluctant to use HPF because many applications do not completely fit the HPF data parallelism model. 
This paper presents a simple model of task parallelism which signifi­cantly extends the domain of applications 
that can be effi­ciently programmed in a language like HPF. A variation of this model is now an approved 
extension of HPF [10]. Most parallel applications contain data pmallelisrn, but a large number of such 
applications need a combination of task and data parallelism to represent the natural computa­tion structure 
or to enhance performance. Pipelines of data parallel tasks [5], a common computation structure in im­age 
processing, signal processing, and computer vision, re­quire a combination of task and data parallelism 
to achieve good performance, The reason is that individual data sets for these applications are often 
small, and therefore, a pure data parallel implementation may not scale well. Multiblock codes containing 
irregularly structured reguku meshes [1] are more naturally programmed as interacting tasks with each 
task representing a regular mesh, rather than as a single large irregular application. Tree structured 
algorithms like Barnes-Hut for N-body problems [2] need task parallelism to recursively divide the available 
processors to solve the recursively generated subproblems. Multidisciplinary appli­cations like air quality 
modeling [13] are often modeled as sets of interacting modules, where each module represents a different 
scientific discipline. Finally, applications that in­teract with external devices like cameras and display 
devices need special tasks for such activities. Integration of task and data parallelism is currently 
an active area of resewch and several compiler based and run­time system baaed approaches have been proposed 
[3, 6, 7, 9]. Compiler baaed solutions presented in the literature are either limited in terms of the 
forms of tack parallelism struc­tures they can support or have not demonstrated that they can be compiled 
efficiently. Runtime solutions have the ad­vantage that they do not require new compiler technology, 
but they require that the programmer manage task paral­lelism at a lower level than data parallelism, 
and lack the advantages and flexibility of a single programming and com­pilation framework. This paper 
presents a new model for integrated task and data parallelism. The central idea is to support the ability 
to divide the executing processors into processor subgroups and map data and computation onto these subgroups. 
A proces­sor subgroup can be further divided into smaller subgroups allowing nested parallelism. The 
language extensions pro­posed are simple and minimal yet powerful enough for the expression of most common 
forms of mixed task and data parallelism including nested data parallelism. We show that this model can 
be compiled efficiently in an HPF framework with relative ease and present results from a dkerse set 
of applications that use task and data parallelism. The paper is organized as follows. Section 2 introduces 
our model of task parallelism and Section 3 shows how it is used to program difYerenttask structures. 
Section 4 outlines some of the issues in implementing this model efficiently. Section 5 describes the 
implementation and performance re­sults from a set of applications. Section 6 discusses related work 
and Section 7 contains conclusions. 2 Model We present our model of integrated task and data paral­lelism 
which has been implemented in the Fx parallelizing compiler. Fx provides dhectives to guide the layout 
of data arrays onto groups of processors to help exploit data par­allelism [18]. Supported data distributions 
include block, cyclic and block-cyclic. Loop parallelism is expressed by a special parallel loop construct 
that combines loop and re­duction parallelism [24]. Since Fx is conceptually similim to HPF in terms 
of data parallelism, and the details are not directly relevant to this research, we will not discuss 
them any further. Task parallelism in our model is based on the concept of a processor subgroup, which 
is a collection of processors ex­ecuting in data parallel manner. An instance of a program statement 
is executed by a processor subgroup, which may include all processors executing the program, or only 
a sub­set of them. The processor subgroup executing a statement is the current processor subgroup, and 
the corresponding processors are the current processors. Task parallelism is obtained by dividing the 
current processors into processor subgroups and performing independent data parallel com­putations on 
disjoint processor subgroups. In this section we describe the directives for task paral­lelism. These 
directives are used to divide the current pro­cessors into subgroups, map data structures onto processor 
sub~oups, and specify independent task parallel computa­tions. ITrealso describe the task parallel execution 
model obtained by using the directives. 2.1 Task parallelism direct ives Task parallelism is expressed 
using declaration directives that appem in the declarative section of a procedure, and ezecution directives 
that appear in the executable section of a procedure. The declaration directives are TASKJARTITIDN directive, 
which defines a template for dhiding processors into subgroups, and SUBGROUPwhich maps vari­ directive, 
ables onto processor subgroups. The execution directives include TASK-REGION directives, which partition 
current pro­cessors and define the task parallelism region, and ON SUB-GROUPdirectives which specify 
the execution of a code block on a processor subgroup. 1A similar concept is described with active processors 
in HPF ter­minology [10] Declaration directives A TASKIARTITION directive provides a template for parti­tioning 
the current group of processors into subgroups. As an example: TASK-PARTITIONmyPart: : some(5) , meny(n 
-5) declares a partition named myPert (partition name is op­tional if there is only one task partition 
in a procedure) by dividing the current processors into two named subgroups: subgroup some is assigned 
5 processors and subgroup msny is assigned (n-5) processors. An important special case is ob­tained by 
replacing variable n with NUMBSR.OFJROCESSORS ( ), a function that returns the number of processors in 
the cur­rent group: TASK-PARTITIONmyPart: : some(5) , my (NWBER-OF-PROCESSORS­ () 5) This directive 
assigns 5 processors to subgroup some and the remaining processors to subgroup msny. The expressions 
in a task partition directive can use for­mal procedure parameters, and hence the partitioning can be 
different on different invocations of a procedure, depend­ing on the values of the actual parameters. 
A subprogram unit can have multiple task partition directives to declare multiple templates for partitioning 
the current processor group. A SUBGROUP directive is used to map (or attach) variables to a named processor 
subgroup formed with a task partition directive. These mapped variables will be referred to as subgroup 
variables. As an example: SUBGROUP(some) :: some.low, some~igh maps two variables some-low and some-high 
to the proces­sor subgroup some. Each variable can be mapped to at most one processor subgroup. Variables 
that are not explicitly mapped to a processor subgroup will be mapped to all pro­cessors in the current 
processor group. Alignment directives can be used only among variables mapped to the same subgroup and 
distribution directives are with respect to their corresponding processor subgroup. In the above example: 
DISTRIBUTEsome-low (BLOCK) will distribute the elements of array some-low in block fash­ion only among 
the processors sssigned to subgroup some. Execution directives TASKREGIONdirectives are used to activate 
and deactivate a partition of current processors into subgroups. A block of code delimited by BEGINTASKREGIONand 
ENDTASK-REGION directives is a task region. A task region is a single-entry single+xit block of code 
that can include re- ON SUBGROUPgions. The BEGINTASKREGIONpert it iordkune directive ac­tivates the partitioning 
of the current processors into sub­groups declared by the corresponding TASKYARTITIONper­tit ionNsme 
statement and an ENDTASKREGION directive deactivates this partition. The partition name is optional if 
there is only one TASK-PARTITIONdeclaration, and hence only one processor partitioning template, in a 
procedure. Variables mapped to a subgroup in a partitioning template can be accessed only inside a corresponding 
task region, An ON SUBGROUPblock region is a single-entry singl~exit of code delimited by ON SUBGROUPsubgroupNsme 
and END ON, and is used to advise the compiler that code in this block should be executed on the processors 
of the named subgroup. The code region can only inside an ON SUBGROUPaccess variables mapped to that 
subgroup ezcept for repli­cated variables that areread-only fortheduration of the ON region. Note that 
there are no special restrictions on vari­able access inside a task region but outside of ONSUBGROUP 
blocks; i.e., all variables in the scope of the current pro­cedure and all variables mapped to the subgroups 
of the current partition can be accessed. Further, code in two ON SUBGROUPdirected on different subgroups 
blocks to execute cannot have interfering 1/0, which intuitively means that there should be no concurrent 
accesses to the same files. Lexical nesting is not permitted in task regions or ON SUBGROUPregions. However, 
a procedure called from an ON SUBGROUPcan its processors with another region partition task region directive. 
Thus, dynamic nested partitioning of processors is allowed. The following code illustrates simple usage 
of task re­gions. We assume that there is only one TASKYARTITION statement and hence there is no explicit 
partition name. The start of a task region with statement BEGINTASKREGIONac­tivates the partitioning 
of current processors into subgroups some and many. The corresponding subgroup variables are also activated. 
The assignment some-lou = . .. will be executed on processors of subgroup some and assignment many-high 
= f (msny.lou) will be executed on processors of subgroup many. The assignment many-low = some-low is 
not in an ON SUBGROUPblock and will be executed on processors assigned to both subgroups, which is all 
the pro­cessors executing the task region. TASK4ARTITION:: come(5), many(NIMBELOF-PROCESSORS5) () SUBGROUP(some) 
:: some-lou, someltigh SUBGROUP(many) :: many-low, many_high ..... BEGINTASK_REGION ON SUBGROUPcome 
 some.lou = . . . ENDON many-low = some_lom ON SUBGROUP many many~igh =f (many-low) ENDON ... ENDTASK-REGION 
  2.2 Execution Model The task mrallelism directives are in the form of assertions about the code and 
hints to the compiler, and hence do not introduce any new semantics. The compiler uses these di­rectives 
and program analysis to exploit task parallelism. The user asserts that the code in ON SUBGROUPsc­ regions 
cesses only local subgroup variables (with the exception of read only scalars) and that code in dfierent 
ON SUBGROUP regions cannot have 1/0 related dependence. The compiler can therefore safely schedule code 
in different ON SUBGROUP regions in parallel. Further, whenever the compiler can de­termine that only 
a subset of current processors are required to execute a code block outside ON SUBGROUP blocks, it uses 
that information to allow non-participating processors to skip that code safely. To explain the execution 
model, we classify the code in­side a task region into two scopes: the subgroup scope and the parent 
scope. All the code blocks that are enclosed in DN SUBGROUPto subgroup scope. Code regions belong the 
blocks that do not belong to any ON SUBGROUP region are in the parent scope. The execution of a subprogram 
with a task regjon pro­ceeds as follows. The code is executed normally in data parallel mode on current 
processors until a task region is en­countered. Inside the task region, code blocks in subgroup scope 
are executed only by the processor subgroup speci­fied in the corresponding ON SUBGROUPProcessors directive. 
not belonging to the named subgroup of an ON SUBGROUP region can skip past the region. The code in the 
parent scope is executed by all current processors, which includes the processors in all the subgroups 
of the task region, in normal data parallel mode. It is important to note that normal mechanisms to ensure 
legal data parallel execution still have to be used but their scope is constrained to the current subgroup 
scope or parent scope. For example, at the start of the execution of the task region in the program in 
Section 2.1, the many processors will immediately skip to the statement many-low = some-lou since they 
are not re­quired for the execution of the ONSUBGROUPsome region. However, the statement many-low = some_lou 
itself will not be executed until some processors also reach there, as is re­quired for any legal execution 
that respects dependence. The actual synchronization mechanism is implementation dependent. It may be 
a barrier, a subset barrier, or baaed on message passing, and would typically be the same as that used 
for normal data parallel execution. In the subgroup scope, the executing processor subgroup of an ON 
SUBGROUPhas read and write to vari­ block access ables mapped onto that subgroup, and read-only access 
to replicated variables of its parent processor group. All un­mapped scalar variables are automatically 
replicated by the compiler. All subgroup and parent variables are accessible in the parent scope of a 
task region. Computations only involving replicated scalar variables are automatically replicated on 
all executing processors, and are therefore performed asynchronously on all processors without synchronization 
or communication. If the compiler can determine that certain subgroups do not need to partic­ipate in 
the execution of a block of code in the parent scope, they can skip that code and continue to execute 
other parts of the program. An important special case is a simple ar­ray assignment statement where the 
compiler can trivially determine the participating and non-participating processor subgroups. This case 
is important for exploiting pipelined task parallelism and will be illustrated in later sections. 3 Usage 
Task region and associated directives discussed in the pre­vious section present a simple yet powerful 
model to write integrated task and data parallel programs. We illustrate how this model can be used to 
exploit task parallelism in four basic computation structures: parallel sections, data parallel pipeline, 
replicated data parallelism, and divide­and-conquer style nested parallelism. We omit declarations of 
arrays and their alignments and distributions for concise­ness when they are not relevant to the discussion, 
 P! TASK-PARTITION:: Agroup (nA) , Bgroup (nB) SUBGROUP(Agroup) :: A proca SUBGROUP(Bgroup) :: B A BEGINTASK-REGION 
doi =l,m doi =l,m CALLproca(A) ONSUBGROUPAgroup CALLprocb(B) procb CALL trsnsfer(A,B) enddo (a) transf 
(b) Figure 1: Parallel section 3.1 Parallel Sections A task region can beusedto divide the processors 
into disjoint subgroups for performing independent computations, simulating what isoften referred toasparaUel 
sections. This form of task parallelism is relatively straightforward and use­fulinmany application scenarios, 
anexample being multi­block applications. We present an example in Figure 1 where two procedures can 
execute in parallel on disjoint data sets but need to ex­change data between invocations. This structure 
is common in multiblock applications where parts of the computation domain can be processed independently, 
but boundary ele­ments have to be exchanged periodically. Figure 1(a) shows the high level code for this 
example and Figure 1(b) shows the data dependence between the procedures for one com­putation step. The 
procedures proca and procb execute one computation step in each invocation and their data sets are updated 
through procedure transfer between invocations. An abbreviated task parallel program that expresses this 
computation is shown in Figure 1(c). The executing pro­cessors are divided into two subgroups Agroup 
and Bgroup, and variables A and B are mapped to these processor sub­groups, respectively. When the execution 
of the task re­gion starts, subroutines proca and procb start processing independently on processor subgroups 
Agroup and Bgroup, respectively. All processors combine to execute subroutine transfer after proca and 
procb complete execution. This sequence is repeated for m iterations. 3.2 Data Parallel Pipeline Task 
regions can be used to implement pipelined data par­allel commutations where a stream of irmut data sets 
is rmo- A .. cessed by a series of functions. This is a common compu­tation structure in signal processing, 
image processing and computer vision and it is often important to exploit task parallelism to achieve 
acceptable performance. We use FFT-Hist, an image processing kernel, to illus­trate how a data parallel 
pipeline is programmed with task regions. This program takes a stream of complex arrays as input. For 
each array, a two dimensional fast Fourier transform (FFT) is performed, followed by computation of CALL 
proca(A) ENDON ON SUBGROUP Bgroup CALLprocb (B) ENOON CALL transfer (A, B) enddo ENDTASKREGION (c) and 
join computation histograms. The main loop of the program is shown in Fig­ ure 2(a) and the corresponding 
task graph is show-n in Fig­ ure 2(b). For each iteration of the loop, the cf f ts function inputs the 
array A and performs lD FFTs on the columns, the rffts function performs ID FFTs on the rows, and the 
hist function computes histograms and outputs the results. A high level task and data parallel implementation 
of this pipeline is shown in Figure 2(c). The TASK-PARTITION statement is used to divide the processors 
into three named subgroups, Gl, G2 and G3 that are assigned Pc,Pr and Ph processors, respectively, corresponding 
to the 3 computa­ tion stages for column and row FFTs and histograms. A se quence of SUBGROUPis used 
to map the variables statements Al, A2 and A3 onto subgroups G1, G2 and G3, respectively, Along with 
a TASK-REGIONdirectives directive, ON SUBGROUPare used to map the computations, i.e., the three subroutines 
Cffts , rf f ts and hi st, onto the three processor subgroups. Data is exchanged between subgroups by 
assignments of the form A2=A1 which are not region, and in any ON SUBGROUP therefore, can access all 
data. Once the pipeline is started up, the processors in each subgroup repeatedly perform computations 
and data ex­ changes, with each processor subgroup working on a differ­ ent data set. To exploit maximal 
parallelism, it is important that assignment statements are executed only by necessary processors. e.g., 
processors in subgroups G2 and G3 should synchronize and participate in the execution of the assign­ 
ment A3=A2, but processors in subgroup PI should skip past this assignment. Fortunately, this is a relatively 
simple op­ timization for a compiler. 3.3 Replicated data parallel computations If a data parallel computation 
is repeated for a sequence of data sets, it is also possible to divide the processors into subgroups 
and assign the entire processing of each data set to one of the processor subgroups. We consider the 
example FFT-Hist computation in Figure 2 again. If each of the m data sets can be processed independently, 
then we can di­vide the processors into two subgroups and assign alternate data sets to each subgroup. 
The code for such an implemen­tation is shown in Figure 3. The processors are divided into Inputstream 
ofdata sets TASK-PARTITION:: G1(Pc), G2(Pr), G3(Ph) SUBGROUP(G1):: Al 1 SUBGROUP(G2):: A2 SUBGROUP(G3):: 
A3 Cffts BEGIN TASKREGION do i = l,m do i = call call call enddo l,m cffts(A) rffts(A) hist(A) rffts 
Q4 ON SUBGROUPG1 cffts(Al) ENDON A2 = Al ON SUBGROUPG2 rffts(A2) I ENDON A3 = A2 (a) ON SUBGROUPG3 hist 
hist(A3) ENDON enddo ENDTASK-REGION 1 Outputstream of data sets (c] (b) Figure 2: FFT-Hist as a 3 stage 
data parallel pipeline 3.4 Nested Parallelism TASK-PARTITION:: Gl(N/2), G2(N/2) SUBGROUP(G1):: Al Tsskregionscanbe 
dynamicallynested, i.e., aprocedurecsll SUBGROUPA2 made from an ON SUBGROUPcan fur­ (G2) :: region in 
ataskregion BEGINTASKREGION ther subdivide the executing processors using another task doi= 1,)4/2 region 
directive. This allows the exploitation of nested par- ON SUBGROUPG1 allelism, one form of it being dynamic 
tree structured divide­cffts(Al) and-conquer computations. As a simple concrete example, rffts(Al) we 
show how quicksort can be implemented by recursively hist(Al) partitioning the input array of keys around 
a pivot and as- ENDON signing proportionate number of processors to the two new ON SUBGROUPG2 arrays 
obtained es a result of partitioning. cffts(A2) Figure 4 shows quicksort implemented using task par­rffts(A2) 
allelism directives. Subroutine qsort picks a pivot key us­hist(A2) ing the function pick.pivot, and 
determines the number ENDON of elements less than the pivot element using the function enddo count_less-than-pivot. 
A suitable partitioning of proces- ENDTASKREGION sors to sort these two sets of elements is determined 
with the procedure compute-subgroup_sizes. Then the proce­dure qsort -helper is called with this information. 
Subroutine qsort-helper declares a partition qsortpart Figure 3: FFT-Hist as a replicated data parallel 
computa-of current processors with subgroups great erEqG and lessG, tion and declares the appropriate 
size arrays aGreaterEq and dess to be mapped onto these processor subgroups. In the corresponding task 
region, procedure pick-less -then-pivot equal size subgroups GI and G2, and each one performs the is 
called to copy elements smaller than the pivot to the sub­complete computation with calls to cf f ts, 
rf f ts and hi st group array aLess. Similarly, pick-greater-equal-t o-pivot for half of the data sets. 
copies elements greater than or equal to the pivot to the ar-Replication is a special case of parallel 
sections and is ray aGreaterEq. Then the subroutine qsort is invoked sep­used to improve scalability 
of stream based computations, in arately on these two arrays in the two ON SUBGROUP regions. amanner 
similar to data parallel pipelines. Replication can Finally, the results are merged with subroutine merge-result. 
be used in combination with pipelining, and we will present When the program executes, the array to be 
sorted is re­ the results of this combination for example applications. cursively partitioned around 
pivots, and the processors are subdivided proportionately to perform sorting on the par­titioned subarrays. 
In the end, the subgroups and the cor­responding array sections are recursively merged to obtain the 
final result. SUBROUTINEqsort (a, n) INTEGERn, a(n) INTEGERpivot, nLess, pl, p2 if (n .eq. 1) return 
if (NUMBER.OFX ROCESSORS()1) .eq. then call qsort-sequential(a, n) return  endif pivot = pick.pivot(a, 
n) nLess = count_less_than_pivot(a, n) call compute_subgroup_sizes(n, nLess, pl, p2) call qsort-helper(a, 
n, nLess, n-nLess, pl, p2, pivot) END SUBROUTINEqsort-belper(a, n, nLess, nGreaterEq, pl, p2, pivot) 
INTEGERn, a(n), nLess, nGreaterEq, pl, p2, pivot TASK-PARTITIONqsortPart:: lessG(pl), greaterEqG(p2) 
INTEGERaLess(nLess), aGreaterEq(nGreaterEq) SUBGROUP(lessG):: aLess SUBGROUP(greaterEqG):: aGreaterEq 
BEGINTASK-REGIONqsortPart CALL pick_lees__than_pivot(aLess, nLess, a, n, pivot) CALLpick_greater_equal_to_pivot 
(aGreaterEq, nGreaterEq, a, n, pivot) ON SUBGROUPlessG call qsort(aLess, nLess) ENDON SUBGROUP ON SUBGROUPgreaterEqG 
 call qsort(aGreaterEq, nGreaterEq) ENDON SUBGROUP call mergexesult(a, aLese, aGreaterEq) END TASKREGION 
END Figure4: Quick Sort using dynamically nested task parallelism Implementation The integrated task 
and data parallelism model presented in this paper has been implemented in the Fx compiler at Carnegie 
Mellon. The implementation of data parallelism in Fx is discussed in [18, 24]. An earlier version of 
task parallelism isdiscussedin[!l] andrelated work in automatic mapping in [21, 22]. In this section, 
we outline the major issues in implementing our new task parallelism model in Fx. Our model of task parallelism 
allows the programmer to assert that a computation can be efficiently and correctly performed by asubgroup 
ofprocessors. The main challenge in implementing taak parallelism is to develop a simple and efficient 
mechanism to perform computations independently on processor subgroups, thereby -allowing the exploitation 
of data parallelism inside subgroups and task parallelism between subgroups. Thespecific design goals 
of the Fxim­plementation were as follows: . Compilation oftesk parallelism should be orthogonal to compilation 
of data parallelism, i.e., the data paral­lel compiler should not, be significantly afTected by the addition 
oftaak parallelism. . Computation and communication inside a subgroup  should only use the processors 
assigned to the sub­group, i.e., the compiler should be ableto exploit the assertion that a subgroup 
computation can be local­ized to the subgroup processors. . Computation and communication outside of 
a sub­group (in parent scope) should only involve the nec­essary processors, i.e., the compiler and the 
runtime system should infer the set of processors required to execute a code block and allow the remaining 
proces­sors to skip past that code block. In the rest of this section we outline the techniques used 
to achieve these goals and discuss other relevant issues for implementing task parallelism. Processor 
mappings Weuse the concept of processor mappings tolocalizecom­putation and communication to processor 
subgroups, and to insulate the data parallel compilation module from the im­plementation oftask parallelism. 
Aprocessor mapping maps virtual processors to physical processors. All data parallel compilation is done 
for asubgroup ofvirtual processors and virtual processors are mapped to physical processors during execution. 
The act uaf set of physical processors for a com­putation is identified only at runtime. For computations 
in the parent scope, the implementation conservatively identi­fies the participating processor subgroup, 
while in the sub­group scope, this information is explicitly provided by the corresponding ON SUBGROUP 
statement. We outline the implementation of task parallelism us­ing processor mappings. At startup, 
the processor mapping (which represents the group of all processors) is an iden­tity mapping, i.e., every 
virtual processor ID corresponds to the physical processor ID. Each TASKIARTITION template defines a 
new processor mapping for the executing proces­sors. The BEGINTASKREGIONdirective activates the proces­sor 
mappings of the corresponding task partition template. The ON SUBGROUPinstantiates map­ directive the 
processor ping for the specified processor subgroup. In the instantia­tion process, each processor of 
the subgroup changes its vir­tual processor ID baaed on the new processor mapping. To support nested 
parallelism, each processor maintains a st ack of virtual to physical processor mappings. This stack 
tracks all processor groups that a processor belongs to, with the top of the stack representing the current 
processor group. When a processor enters an region, the pro- ONSUBGROUPcessor mapping for the new subgroup 
that it belongs to is pushed onto this stack. When a processor leaves a task re­gion, the corresponding 
processor mapping is popped off the stack. This concept of processor mapping allows compilation, management 
of data distributions, computation of collec­tive communication patterns, and barrier synchronizations 
to be all done in terms of a group of virtual processors. Hence implementation of data parallelism is 
done in terms of virtual processors and is not directly related to task par­allelism. At runtime, computation 
and communication are automatically mapped to the relevant executing processors. A task partition specifies 
the number of processors to be as­signed to each subgroup and the implementation is free to choose any 
such legal assignment. The Fx implementation attempts to choose a mapping that minimizes communica­tion 
and synchronization overheads. The layer of indirection offered by processor mappings makes integration 
of this task parallelism model to an etisting HPF compiler relatively easy. Identification of minimal 
processor subsets A computation in the parent scope of a task region can use data from any processor. 
However, it may only need data horn a subset of processors. To exploit mwrimaf task par­allelism, it 
is important for an implementation to identify the set of processors required to execute a computation 
in the parent scope and allow the remaining processors to by­pass the computation. Note that for computations 
inside the subgroup scope of a task region, the set of procesors that contain the relevant data is explicitly 
specified, and therefore no compiler anakysis is required. The example in Figure 2 illustrates the importance 
of recognizing the processors required to execute an array as­signment. In Figure 2(c), the array assignment 
statement A2=A1 only requirea processors in subgroups G1 and G2 for execution. If the compiler correctly 
recognizes this, then the subgroup G3processors can continue past this statement and execute other code 
before the statement is executed. This is essential for full exploitation of task parallelism in this 
example. In general, it is not possible to identify the processors that may have data required for a 
general computation like the execution of a procedure call. However, for simple state ments like a full 
array assignment, the set of processors r~ quired to execute it can be determined easily by the com­piler. 
In our experience, the ability to handle simple state ments is sufficient to exploit maximal task parallelism 
in most cases. Localization Localization of computation, communication, and synchro­nization to a subgroup 
of processors is critical for exploiting task parallelism. The basic communication model in Fx is direct 
deposit of data by a sender to a receiver s memory space with the use of global barriers for synchronization 
[19]. To support task parallelism, the barriers are replaced by subset barriers on subgroups of virtual 
processors. At run­time, the physical processors are identified and only those processors participate 
in a barrier. We should point out that the details of localization are strongly related to the underlying 
communication and syn­chronizat ion framework. For example, if conventional mes­sage passing was used, 
the actuaf communication will implic­itly involve only the necessary processors. However, such an implementation 
must ensure that empty messages are not exchanged with processors that are not participating in a communication 
step. Replicated Computations For effectively exploiting task parallelism in th~ model, the Fx compiler 
replicates all unmapped scalars and replicates all computations exclusively involving replicated variables. 
Replication of scalar computations is legaf since it simply involves execution of the originaf sequential 
code on all pro­cessors, and preferable since it eliminates synchronization and communication between 
processors. A simple alterna­tive is that one processor performs the computations and broadcasts the 
results to all processors. This approach is not recommended since it incurs extra synchronization and 
communicantion overhead and may prevent full exploit ation of task parallelism due to unnecessary synchronization. 
Replication is particularly important for loop induction and loop bound variables that are active inside 
task regions and we will illustrate this with the FFT-Hist example in Fig­ure 2. The loop induction variable 
i in the program in Fig­ure 2(c) must be replicated for exploiting task parallelism. If i is replicated, 
the loop control statements (check and in­crement for i) are executed on all processors subgroups in­dependently, 
while the relevant processor subgroups partici­pate in the execution of ON SUBGROUP and other code blocks 
inside the loop. However, if i was mapped to a single proces­sor, then the value of i must be computed 
and broadcast at the beginning of every iteration by the processor that owns it. This leads to unnecessary 
synchronization that prevents pipelined task parallelism between loop iterations to be ex­ploited. Implication 
for 1/0 In some parallel system implementations, 1/0 is performed through a single processor of the system. 
Task parallelism in the presence of 1/0 assumes that all processors can perform 1/0 independently and 
this paradigm has to be supported, although it is not necessary that each processor be able to physically 
perform all 1/0 operations independently. One simple solution is to have a single designated 1/0 processor 
that performs all 1/0 but is not considered an executing processor and hence does not have any execution 
related dependence. SPMD or MIMD code generation Another issue for the compiler is whether or not the 
same code image should execute on all processors. Since differ­ent processor groups may need different 
variables, a naive SPMD implementation is likely to be wasteful of memory since it must allocate all 
variables on all processors. The Fx compiler generates SPMD code and uses dynamic mem­ory allocation 
to reduce the memory overhead at the cost of some added complexity. An alternative is to use differ­ent 
code images for different processor subsets. This will have better memory efficiency but add more implementa­tion 
complexity. 5 Results The model of task parallelism presented in this paper has been used to develop 
a variety of integrated task and data parallel programs using the Fx compiler. We now discuss the implementation 
and results from a set of programs. All performance results are presented for an Intel Paragon ma­chine. 
5.1 Sensor based programs We present results for the FFT-Hist program discussed in Section 3 and two 
other realistic task parallel programs: narrowband tracking radar and multibaseline stereo. The narrowbaud 
tracking radar benchmark was developed by researchers at MIT Lincoln Labs [17]. The processing of an 
input data set by the radar program consists of four steps: a corner turnto form a transposed matrix, 
indepen­dent row-FFTs, scaling, and thresholding. The multibase­line stereo uses an algorithm developed 
at Carnegie Mellon to measure depth in a scene accurately by using multiple cameras [15, 23]. Input consists 
of three (or more) images acquired from the cameras. The major computation steps are the following: finding 
a set of difference images by com­puting the sum of squared differences between correspond­ing pixels 
of match images, finding error images formed by computing the sum of a surrounding window of pixels for 
each pixel, and finding the depth image by computing the minimum element for each corresponding pixel 
for a set of images. We will skip further details of these applications, which are described in [5] and 
corresponding results from an earlier Fx implementation of task pamdlelism are discussed in [20]. The 
structure of each of these three programs is a pipeline of data parallel tasks that processes streams 
of independent input data sets. The usage models for data parallel pipelines and replicated computations 
described in section 3.2 and section 3.3, respectively, can be used to build various task and data parallel 
implementations of these programs. For processing of streams of input data sets, there are two performance 
criteria 1) Throughput, which is the rate of processing data sets, and 2) Latency, which is the time 
taken to process an individual data set. Use of task paral­lelism in the form of pipelining or replication 
improves the throughput, but can increase the latency. Our program­ming model allows us to target the 
development of such applications to specific performance goals. Figure 5 shows that the best way to map 
the FFT-Hist example depends on the throughput requirement of the application. If there is no throughput 
requirement (i.e. minimizing latency is the only goal) then the simple data parallel mapping on the left 
is optimal, but with different throughput requirements, the best mapping changes. This model, along with 
the use of mapping algorithms presented in [21, 22], allows us to au­tomatically determine the best mapping 
for a program for different performance goals. Table 1 shows the results ob­tained for this set of applications 
by setting a throughput goal and optimizing for minimum latency. We observe from Table 1 that a factor 
of 3 improvement in throughput was achieved for the 256x256 FFT-Hist kernel with less than 20% increase 
in latency, but a 25% improve­ment in throughput for the 512x512 version of the program increased the 
latency by over 60%. These results illustrate that the benefits of task parallelism in this form vary 
widely and are higher for smaller data sets. A significant improve­ment in throughput was achieved for 
both the radar and the stereo program, but an increase in latency was observed only for the stereo program. 
The reason is that the improvement in throughput of the radar program was achieved by using processors 
that could not be used in the data parallel pro­gram because of the structure of parallelization. The 
main point is that integrated task and data parallelism offers a variety of ways in which such applications 
can be mapped, and facilitates the development of automatic tools to achieve different performance goals. 
 5.2 Airshed simulation Airshed air quality simulation model developed by McRae and Russell [13] models 
the formation, reaction, and trans­port of atmospheric pollutants and related chemical species. The main 
data structure used in airshed simulation is a con­centration matrix whose dimensions (with typical values) 
are number of atmospheric layers (5), number of grid points (500-5000) and number of chemical species 
(35). The simu­lation proceeds as follows. Every hour, a new set of initial conditions are input and 
a preprocessing phase is executed. This is followed by the main computation phase which con­sists of 
iteration over three simulation steps. The tirst step simulates particle transport, the second simulates 
chemical calculations, and the third also simulates particle transport. The number of iterations is determined 
at runtime depend­ing on the hourly input and this phase is followed by the hourly output phase. High 
level structure of this applica­tion is captured by the following code: DO i = l,nhrs CALL inputhour 
(A) CALLpretrans (A) DDi = l,nsteps CALL transport (A) CALL chemistry (A) CALLtransport (A) ENDDO CALL 
outputhour (A) ENDDO   fo-4E&#38;-o) Cdtfts rowifts hist 1 { corns rowffts hist ) P- $f$- Mcdule 
1 Module 2 pi:profcssorspcr instance 64 32 8 14 n: Wllm:;of 1 2 2 3 Ia QUU12UEIEIU: E.iEIDnnDn FiammmEianFan 
 m m : n13rJElLlf3E3 : mnm&#38;lBaEaD EluunEIQnLl Ei?mmEaE?3Fi4E3n : uunnnncl : c3nDn1312Ll E3EafflE3Ei3E?EJn 
: lclnnnnnn : UCJUD13C3U E3fflE3E!il E4EmFkln IUUEIC2UCIUC31 : lnclDnElnn E3MMME4E3BD tlataParallel Mapping 
Latency Optimization with Latency Optimization with Minimum Throughput = 2 Minimum Throughput = 4 Figure 
5: Mappinga of a 512x512 FFT-Hist program on an Intel Paragon Problem Description Data Parallel Best 
Task-Data Parallel Name Size Throughput Actual Beat of of Throughput Latency Constraint Throughput Latency 
Program dataset datasets/sec seconds dataaets/sec datasets/sec seconds FFT-Hist 256x256 3.90 .256 8 13.3 
.293 FFT-Hist 512x512 1.99 .502 2 2.48 .807 Radar 512x1OX4 23.4 .043 50 70.2 .043 Stereo 256x240 3.64 
.275 10 11.67 .514 Table 1: Performance results on 64 nodes of an Intel Paragon application for integrated 
task and data parallelism. We expect to report on it in more detail in the future. A data parallel version 
of the airshed model has been de-Figure 7 shows how a variation of Barnes-Hut algorithm veloped with 
Fx and used over a period of time. One of the can be implemented recursively with task parallelism. The 
reasons thw application did not scale well for larger num-function build.bh.tree builds a balanced binary 
tree of cells berofproceasors waathat the input and output processing by evenly partitioning the particles 
along each axis, one at phases are mainly sequential. These phases consume well a time. This partitioning 
step is very similar to the parti­under 2% of the total time in sequential execution, but be-tioning 
in quicksort discussed in section 3, so further details come the main bottleneck once rest of the computation 
is are omitted. speeded up by more than a factor of 10 by exploiting data The force computation is done 
by recursively dividing parallelism. We used task parallelism to alleviate the ef-particles into subgroups, 
with each subgroup owning one half fects of this performance problem. The input and output of the particles 
and a partial tree which covers the particles phases were separated intoaeparatetasks andeachonewas plus 
a copy of the top k level of the Barnes-Hut tree, where placed on a separate processor subgroup. The 
input sub-k is a fsxed constant. The force vector for each paxticle is group would preprocess the next 
input data set and transfer calculated at the lowest level of the recursion tree, where it when the main 
computation neededit. Similarly, the main each subgroup has only one processor. If the force vector computation 
would transfer raw output data to the output computation of a particle requires traversal of the missing 
task andcontinue. Theoutput taskwould process andphys-(non-local) part of the Barnes-Hut tree, the particle 
is placed ically output the data. The impact of using data and task in a worklist of particles to be 
processed later by the parent parallelism in this fashion on the airshed application is plot-subgroup. 
When the children complete, the parent subgroup ted in Figure 6. The figure shows a significant improvement 
will try to compute the force vector for the particles in the in scalability with the use of task parallelism. 
In particular, worklist baaed on its own more complete version of the tree, the execution time on 64 
processors was reduced by around and paas a potentially reduced worklist to its parent. 25%. The force 
computation for a particle can potentially prop­ agate all the way to the root of the tree, but in general, 
the size of the worklist that is passed from a child to a parent 5.3 Barnes-Hut will be small. In particular, 
for n particles uniformly dis-We present an abbreviated discussion of our ongoing imple-tributed in a 
3-dimensional cube, the total size of the work­mentation of Barnes-Hut algorithm, since it is an important 
. DalaParallel I Taskand DataParallel 4 8 163264 Number of Processors Figure 6: Speedup of Airshed application 
on an Intel Paragon list is expected to be O(n~ ). Furthermore, the size of the worklist can be reduced 
by controlling the number of repli­cated layers k. For p processors, k should be at least log(p) to avoid 
excessive communication. And since 2k nodes will be replicated, k should be within a small constant multiple 
of log@) to avoid excessive space overhead. The expected running time of this algorithm is O ( ~ log(n)) 
for a uniform distribution, and the memory usage is bounded by O(n). Related Work In recent years, several 
ideas and proposals have addressed integration of task and data parallelism. We shall state just a few 
of them and discuss the relative merits of the model presented in this paper. In Opus [3, 14], task parallelism 
is obtained by invocation of task programs (like subroutines) onto a specified set of processor resources. 
This is broadly similar to our approach but the task parallelism can only be exploited at subroutine 
call level. Opus also uses shared data abstraction to exchange data between executing tassks. While some 
form of sharing between tasks is indeed desirable, it makea the language and its implementation more 
complex and there is not enough evidence yet to justify the added complexity. Earlier work on Fx [9] 
was based on exploiting task par­allelism by specifying task subroutines and their input and output parameters, 
but leaving all the details of task man­agement to the compiler. While this model is somewhat higher 
level than the model presented in this paper, it makes it more difficult to program complex forms of 
task paral­lelism (e.g., nested parallelism) as the user has less direct control over the resources. 
Recently task parallelism has been added to High Perfor­mance Fortran as an approved extension. Both 
HPF task parallelism and our model are baaed on the basic idea of providing mechanisms for mapping data 
and computations onto processor subgroups. One of the goals of this paper is to contribute to the development 
of compilers that support HPF task parallelism by illustrating the implementation and results from our 
task parallelism model. ZThiSis &#38;Ca ~eof the ~t~o~ginteraction between the twO design efforts. The 
differences between HPF and Fx task parallelism partially emanate from the somewhat difFerentdesign philoso­phies 
of a language standard and a research project. HPF haa a generzd ON construct which can be used in different 
contexts including task regions. HPF task subgroups do not have to be declared explicitly, and execution 
on a subset of processors is specified by describing the processor subset in the relevant ON clause. 
In Fx, ON can be used only in the context of a task region, and the processor subgroup corresponding 
to an ON clause must be explicitly declared. HPF task parallelism requires minimal new syntax es no new 
declarative syntax is needed. It offers more flexibility aa it allows the programmer to specify execution 
on a sub­set of processors that may have been computed during the execution of the current procedure, 
but it also offers leas flex­ibility since only rectilinear subsets of the current processor arrangement 
can be processor subgroups. A more detailed comparison of the two approaches is be­yond the scope of 
this paper. As stated earlier, HPF requires a general implementation of ON constructs and it does not 
give the compiler writer some of the declarative informa­tion that we have used to help build a simple 
yet efficient implementation. However, we do believe that HPF task par­allelism can also be implemented 
efficiently, at least for most common patterns of task parallelism. We expect commercial implementations 
of HPF task parallelism to become avail­able in the future and shed more light on the HPF model and the 
significance of the differences between Fx and HPF task parallelism models. An alternate approach to 
integrated task and data par­allelism is use of a coordination language to communicate between tasks. 
An example of such an approach is pre­sented in [7] where a subset of MPI is used to coordinate HPF tasks. 
This approach has the obvious and important advantage that the data parallel language does not have to 
be modified at all, but consequently, it does not offer a single clear programming model or semantic 
notion. For a task and data parallel program developed using this approach, there is no semantically 
equivalent sequential or data parallel pro­gram; while in our model, such a program is obtained simply 
by ignoring the task parallelism directives. Further, a coor­dination language does not allow compiler 
optimizations on the whole program, or allow dynamic load management by reassigning processors to different 
tasks within a program. c c c c c c c c c SUBROUTINEbh(particles, nPartlcles) call build.bh.tree(particles, 
n, bhTree) call compute_force(particles, n, bhTree, nullWorkList) for each particle, update position 
based on force vector END SUBROUTINEbuild_bh_tree(particles, n, bhTree) Build a binary tree by repeatedly 
partitioning the particles along one axis, i.e., first partition along the x axis, then y, then z, and 
repeat. The particles uill be sorted based on the ordering of the leaves of the bhTree. END SUBROUTINEcompute.force(particles, 
n, bhTree, workList) if (NWBER.OF-PROCESSORS().eq. 1) then for each particle p, call sequential_force_computation(p, 
bhTree, workList) return endif call compute-numbers-of-eubgroup-processors(n, pl, p2) call compute-force-interactiomhelper(particles, 
n, bhTree, pl, p2) END SUBROUTINEcompute-force_helper(particles, n, bhTree, pl, p2, vorkLiet) TASKYARTITIONbhpart:: 
subTreeGl(pl), subTreeG2(p2) SUBGROUP(subTreeGl):: bhTreel, particles, uorkListl SUBGROUP(subTreeG2):: 
bhTree2, particles, uorkList2 BEGINTASKREGIONbhpart partition-bhXree(bhTree, n, bhTreel, bhTree2) perticlesl 
= particles(l:n/2) particles2 = particles(n/2+l:n) ONSUBGROUP bhTreeGl call compute-force(particlesl, 
n/2, bhTreel, uorkListl) ENDON SUBGROUP ONSUBGROUPbhTreeG2 call compute_force(particles2, n-n/2, bhTree2, 
workList2) ENDON SUBGROUP for each particle p in vorkListl and uorkList2 call sequential_force-computation(p, 
bhTree, WorkList) ENDTASKJLEGION END SUBROUTINEn, bhTree2) partition-bh-tree(bhTree, bhTreel, Replicate 
top k levels of bhTree onto bhTreel and bhTree2 (uhere k is a fixed constant constant ~ log(Total # of 
processors)). Place the 1st half of the leaves and all of their ancestors onto bhTreel end similarly 
for the 2nd half. Mark all branches leading to missing subtrees in bhTreel and bhTree2 as remote. END 
 SUBROUTINEsequential-force-computation(p, bhTree, workList) Perform the sequential force computation 
with p on tree bhTree. If p needs to access a tree branch that is marked remote, then place p on workList 
and return. END Figure7: Barnes-Hut algorithm with dynamic nested task parallelism Conclusions We have 
presented anew model for writing integrated task and data parallel programe and show how it can beuaedto 
program different forms oftask parallelism, including nested parallelism. This model is relatively simple 
and intuitive, and is designed as an extension to data parallel languages like High Performance Fortran. 
A related model is now an approved extension of HPF 2.0. Our model has been im­plemented in the Fx compiler. 
We argue that this model is relatively easy to implement and we have outlined some of the important implementation 
issues. We demonstrate how this model of task parallelism is usedtoimprove the performance ofavarietyof 
parallel pro­grams. Data parallel pipelining and replication with task parallelism carI be used to improve 
the performance and scalability ofsensor baeed applications, andtotunesuch ap­plications to specific 
performance goals. We present results from aFFT-Hist kernel, anarrowband tracking radar pro­gram, and 
a multibaeeline stereo program, to illustrate the value oftask parallelism for sensor based applications. 
Task parallelism was used to improve the performance of Airshed application by separating the input and 
output stages into independent tasks. Finally, we have demonstrated how this model of task parallelism 
is used to build tree structured applications like N-body problems. Taak parallelism can extend the power 
ofa data parallel language like HPF and make it significantly more useful. We have demonstrated the advantage 
of integrated task and data parallelism and presented a simple and powerful way to achieve them. Acknowledgements 
 We would like to thank the current and former members of the Fx compiler tearn at Carnegie Mellon, in 
particular Peter Dinda, Thomas Gross, Peter Lieu, David O Hallaron, James Stichnoth, Tom Stricker and 
Gary Vondran, for their many contributions to the research presented in this paper. This work also benefited 
significantly from discussions in the control parallelism subgroup of the HPFF forum which was led by 
Dr. Rob Schreiber. References <RefA>[1] AGRAWAL, G., SUSS~AN, A., AND SALTZ, J. An in­tegrated runtime and 
compile-time approach for par­allelizing structured and block structured applications. IEEE Transactions 
on Parallel and Distributed Systems 6, 7 (July 1995), 747-754. [2] BARNES, J., AND P.HuT. A hierarchical 
O(N log N) force calculation algorithm. Nature #, 324 (1986), 446­ 449. [3] CHAPMAN, B., MEHROTRA, P., 
VAN ROSENDALE,J., AND ZIMA, H. A software architecture for mult idisci­plinary applications: Integrating 
task and data paral­lelism. Tech. Rep. 9418, ICASE, NASA Langley Re­search Center, Hampton, VA, Mar. 
1994. [4] CHAPMAN, B., MEHROTRA, P., AND ZIMA, H. Pro­gramming in Vienna Fortran. Scientific Programming 
1, 1 (Aug. 1992), 31--50. [5] DINDA, P., GROSS, T., O HALLARON, D., SEGALL, E., STtCHNOTH, J., SUBHLOK, 
J., WEBB, J., AND YANG, B. The CMU task parallel program suite. Tech. Rep. CMU-CS-94-131, School of Computer 
Science, Carnegie Mellon University, Mar. 1994. [6] FOSTER, I., AVALAN], B., CHOUDHARY,A., AND Xu, M. 
A compilation system that integrates High Per­formance Fortran and Fortran M. In Proceeding of 1994 Scalable 
High Performance Computing Conference (Knoxville, TN, October 1994), pp. 293-300. [7] FOSTER, I., KOHR, 
D., KRISHNAIYER, R., AND CHOUDHARY,A. Double standards: Bringing task par­allelism to HPF via the Message 
Passing Interface. In Supercomput;ng 96 (Pittsburgh, PA, November 1996). [8] GROPP, W., LUSK, 13., ANDSKJELLUM,A. 
Using MPI: Portable parallel processing with the Message Passing Interface. The MIT Press, Cambridge, 
MA, 1994. [9] GROSS, T., O HALLARON, D., AND SUBHLOK,J. Task parallelism in a High Performance Fortran 
framework. IEEE Parallel @ Distributed Technology, 3 (1994), 16­ 26. [10] HIGH PERFORMANCEFORTRANFORUM. 
High Perfor­mance Fortran Language Specification, Draft Version 2.0, Dec. 1996. [11] HIRANANDANI, S., 
KENNEDY, K,, AND TSENG, C, Compiling fortran I) for MIMD distributed-memory machines. Communications 
of the ACM 35, 8 (August 1992), 66-80. [12] KOELBEL,C., LOVEMAN,D., STEELE,G., ANDZOSEL, M. The High 
Performance Fortran Handbook. The MIT Press, Cambridge, MA, 1994. [13] MCRAE, G., RUSSELL,A., ANDHARLEY, 
R. CIT Pho­tochemical Airshed Model -Systems Manual. Carnegie Mellon University, Pittsburgh, PA, and 
California In­stitute of Technology, Pasadena, CA, Feb. 1992. [14] MEHROTRA, P., AND HAINES, M. An overview 
of the Opus language and runtime system. Tech. Rep. 94 39, ICASE, NASA Langley Research Center, Hampton, 
VA, May 1994. [15] OKUTOMI, M., AND KANADE, T. A multiple-baseline stereo. IEEE lkansactions on Pattern 
Analysis and Machine Intelligence 15, 4 (1993), 353-363. [16] RAMASWAMY, S., SAPATNEKAR,S., AND BANERJEE, 
P. A convex programming approach for exploiting data and functional parallelism. In Proceedings of the 
1994 International Conference on Parallel Processing (StCharles, IL, August 1994), vol. 2, pp. 116-125. 
 [17] SHAW, G., GABEL, R., MARTINEZ, D., Rocco, A., POHLIG, S., GERBER, A., NOONAN, J., AND TEITEL-BAUM, 
K. Multiprocessors for radar signal processing, Tech. Rep. 961, MIT Lincoln Laboratory, Nov. 1992. [18] 
STICHNOTH, J., O HALLARON, D., AND GROSS, T, Generating communication for array statements: De­sign, 
implementation, and evaluation. Journal of Par­allel and Distributed Computing 21, 1 (1994), 150 159. 
[19] STRICKER, T., STICHNOTH, J., O HALLARON, D., HINRICHS, S., AND GROSS, T. Decoupling synchro­nization 
and data transfer in message passing systems of parallel computers. In Proceedings of the 1995 In­ternational 
Conference on SuperComputing (Barcelona, July 1995), ACM, pp. 1-10. [20] SUBHLOK,J., O HALLARON, D., 
GROSS, T., DINDA, P., AND WEBB, J. Communication and memory re­quirements as the basis for mapping task 
and data par­allel programs. In Supercomputing 91 (Washington, DC, November 1994), pp. 33W339. [21] SUBHLOK,J., 
AND VONDRAN, G. Optimal mapping of sequences of data parallel tasks. In Proceedings of the Fiflh ACM 
SIGPLAN Symposium on Principles and Practice of Parallel Programming (Sauta Barbara, CA, July 1995) , 
pp. 134-143. [22] SUBHLOK, J., AND VONDRAN, G. Optimal latency­throughput tradeoffs for data parallel 
pipelines. In Eighth Annual ACM Symposium on Parallel Algorithms and Architectures (Paclua, Italy, June 
1996), pp. 62 71. [23] WEBB, J. Latency and bandwidth consideration in par­allel robotics image processing. 
In SuperComputing 93 (Portland, OR, Nov. 1993), pp. 23&#38;-239. [24] YANG, B., WEBB, J., STICHNOTH,J., 
O HALLARON, D., AND GROSS, T. Do&#38;merge: Integrating paral­lel loops and reductions. In Sizth Annual 
Workshop on Languages and Compilers for Parallel Computing (Portland, Oregon, Aug 1993)</RefA>.  
			
