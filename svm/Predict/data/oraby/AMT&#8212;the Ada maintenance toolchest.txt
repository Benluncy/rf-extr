
 AMT -The Ada Maintenance Toolchest A. von Mayrhauser Illinois Institute of Technology Department of 
Computer Science 10, W. 31rst Street Chicago, IL 60616 I.Introduction The role of the Ada Maintenance 
Toolchest (AMT) is to help the Ada maintenance programmer to understand code, to make appropriate changes, 
to analyze the effect of these changes, to regression test changes, and to do so while preserving the 
quality of the code (function and structure). At this point, the AMT is primarily based on static analysis. 
thus it could be considered a further development and extension of [COLL83], [COLLSS], [RAJL90] for Ada. 
Like Pecan and Balsa [REIS85], it uses basic analysis information in the form of symbol table, control 
flow graph, etc. It also relies on window technology. Unlike Pecan and Balsa, the AMT provides incremental 
parsing and analysis for all its tools, it relies heavily on the concept of code Processing QVONM90B]), 
it provides capabilities such as data flow analysis (similar to the Expert Dataflow tool and the Surgeon 
s Assistant regression testing support, metrics, and various forms of program slicing that go beyond 
existing tools, chunking, etc. Other approaches to chunking include [CHO190] on a file basis and Seela 
for blocks. For descriptions on Expert Dataflow, Surgeon s Assktant and Seela see [OMAN90B]. AMT also 
emphasizes browsing capabilities similar to [OBR187], but it includes non-object oriented browsing capabilities 
as well. Users of Battlemap/ACT and Grasp/Ada ([OMAN90B]) would find much of the same information as 
part of the AMT results. AMT differs from all of these tools in its approach to the user interface and 
its incremental analysis capabilities. It also provides techniques that combine the results of individual 
tools to enhance understanding. Further, AMT has a design structure which allows new tools to be plugged 
in easily. Since much of its capability is Clerivect rrom structures mat are built by me parser or later, 
it is fairly language independent as well. The use of a compiler generator makes switching to different 
languages easier and provides a standard interface. This would allow not just addition of new languages, 
but also analysis of mixed language code. This paper presents the design structure of a maintenance toolchest 
for Ada, the AMT, and reports on progress and experience with the tool chest to date. Section 2 details 
the objectives for the AMT. Section III explains the approach to AMT design and the impact of this approach 
on the design structure and the algorithm design for all tools in the tool chest. Section IV describes 
the contents of the tool chest, its user interface and all its took, their current state and planned 
future extensions. Section V summarizes AMT s contributions to tool technology. 11.AMT Objectives AMT 
s primary objectives are to support the maintenance of Ada code in the field. The tool chest must be 
able to deal with large systems as well as small programs. It must facilitate the understanding of code. 
This is a primary need for maintenance programmers since most must change code they did not develop. 
AMT also must help to make appropriate changes consistently. It must help to analyze the effect of these 
changes and support regression testing the changes. Some changes may compromise code quality. AMT should 
collect information that guides assessment of the effect of the code change on quality of the code. Ultimately, 
AMT wiO make change as easy to deal with as using word processing in a document. This leads to the concept 
of code processing ([VONM90B]). The maintenanceprogrammer makeschanges to the code and has functions 
available that help to analyze what he or she just did (as a writer would asking for a spell check or 
for an analysis of the grammar or writing style), For the AMT this includes using/showing existing information 
about the code (dataflow or control flow information for example) with the code (as opposed to separate 
tables and graphs) while hiding detailed analysis and all of its results (as opposed to the specific 
information the programmer requested. This circumvents a common problem with code tools flooding the 
user with too much information). AMT also must provide abstraction facilities to facilitate understanding 
the code. Bottom-up understanding @1991ACM O-89791-445-7 /91/100o-0294 $1.50 works via chunking programmers 
associate higher level abstractions with chunks of code and build up their understanding of the code 
via successive levels of chunks. The size of the chunk varies by programmer experience. Thus a novice 
may chunk a loop in a program line by line while a more experienced programmer may immediately look upon 
the same loop as sort array in ascending order . Top down understanding uses domain experience which 
the programmer then tests. As with most standards, rules of programming discourse define expectations 
and, when met, can speed up understanding of code. For more detail on various forms of program understanding 
see [ADEL85], [CORB89], [EHRL84], and [MAYE81]. AMT must support these means of understanding. Besides 
providing bottom-up abstraction through a chunker, AMT should also let the user add domain experience 
and/or state and check rules of programming discourse. AMT should also facilitate association of information 
through bringing analysis results and abstracted information close to the code. This supports the making 
of code changes and analyxing their effect. AMT must also help to determine needs of regression testing 
and the extent to which new test caseshave to be designed. 111.Approach AMT is designed around incremental 
analysis. This includes all tools and all analysis, such as the parser, control flow and data flow analysis, 
path analysis, concurrency analysis, object analysis, syntactic and dynamic structure analysis, etc. 
Since trial changes should be easy to roll back, we are using the concept of forward deltas. A delta 
consists of the changes to be made to the current object. Internally, we use two types of operations, 
add and delete (a modi~ consists of a delete followed by an insert). For some of the tools, we are still 
experimenting with the optimal internal representation for a move operation, since a delete/insert sequence 
is less efficient than a direct move. For the structure tree (one of the outputs of the parser), a delta 
would consist of one or more subtrees together with information where and how to connect them with the 
current structure tree. Note that in the case of a delete operation, the subtree in the delta is empty 
and the connector information specifies the root of the subtree to be deleted. In the case of a move 
operation, we also only need connector information (old and new). The situation gets more complex with 
some of the more involved analysis algorithms and objects. Additionally, our aim is to devise incremental 
analysis algorithms that take their input via the deltas of the object that provides information and 
do not require building a full current object. For example, the control flow graph is derived from the 
structure tree. After changes, we have the baseline structure tree and deltas for it. The control flow 
graph for the changed code is determined using delta information for the structure tree. It does not 
require building the full new structure tree. Rather, deltas for the control flow graph are determined 
via deltas for the structure tree and a minimum of context information to the baseline structure tree. 
This savesspace (no duplication of major parts of objects that remain the same) and time (incremental 
analysis instead of full reanalysis no need to construct a full current object). This incremental approach 
via deltas also facilitates version control and lets the user understand by doing . This is of special 
help to kinesthetic learners who need to see things in action to learn most effectively. For the AMT 
this requires to develop or adapt existing incremental algorithms for parsing ~ONM90A], data flow analysis 
[KEABS8], Control flow and path anatysis ~ONM89], etc. These algorithms take the (new) deltas of the 
prerequisite information (e.g. the code deltas from the editor) and produce their level of delta (e. 
g. the deltas that are parser output for the structure tree, the parse tree, the symbol and use table 
etc.). Some of this is challenging to do. The basic principle is that it is in most cases more efficient 
to do an incremental rather than a full re-analysis. Numerous checks against maintenance changes of existing 
production code have contirmed this. To fulfill our objective of Code Processing and bringing the code 
close to the abstractions and analysis results derived from it, we provide a commonly used editor and 
a display manager as a front end and add code processing functions as tools (e. g. browse, slice, chunk, 
check). The user need not see how the analysis is done (e.g. paths are slices, not a sequence of nodes 
in a control flow graph). We distinguish between user views such as a slice and internal views such as 
a path through a control flow graph. In addition, the tool chest s user interface can switch between 
presentation levels (e. g. nesting levels, calling hierarchy levels, levels of abstraction, module level, 
summary vs. detail level) and types (e. g. variables/data/objects, paths, unmodified, in/direct impact, 
test cases). A notepad function helps programmers to take notes on anything related to the software and 
their strategy of understanding, modi~lng, or regression testing it. It can also be used locally to attach 
personal comments to (parts of) the code or its analysis. These are personal to the individual programmer 
and do not become part of the software nor visible to other programmers who look at the same code. Understanding 
becomes more immediate when the programmer does not have to translate analysis information and relate 
it back to the code. Therefore, our approach is to present multiple analysis results as close to the 
code as possible. We may, for example want to know what happens to the variable SYMITIL in a chunk that 
was just modified. This triggers constrained (local) data flow analysis (that needs path information) 
as influenced by the change. Highlighting and multiple windows are tools to make this information visible. 
Since analyxing the effects of changes is a major objective of this tool chest, support to identify the 
effects of changes, test them, and update/add test cases based on path and/or data flow driven testing 
strategies is part of this tool chest. Its major purpose is to facilitate regression test at the unit 
and interface level. Other efforts by the author are looking at 295 the support that a tool can give 
for black box regression testing (~ONM91]). Like the AMT, the Requirements driven Regression Testing 
Tool (RTT) works via standard change operations (add, delete, modiij) for requirements and helps to update 
the regression test suite accordingly. RTT is a further development of RASE (~ONM88]), adding regression 
test analysis capabilities to RASE. IV. Contents 1. User Interface The user interface uses separation 
of issues to distinguish between editor, display manager, browsing capabilities (actually a separate 
tool) to implement the idea of code processing explained earlier. It makes heavy use of XWhdow technology, 
side-by-side views, highlighting. It can distinguish between structure and content views. The structure 
editor is used like any other editor. It is a modified version of Xedit. It serves as a switchboard to 
analysisfiiewing capabilities. The editor itself is simple to keep the toolchest extensible. Therefore, 
we removed the display manager (DM) (the editor only knows how to edit, not how to display. So if the 
DM is currently suppressing lines or highlighting others, only those that are visible on the screen can 
be edited, because those are the only ones the user can see and operate on. When the user edits them, 
the DM hands that work off to the editor). The editor also has no knowledge of analysis results, nor 
does it need to. Atl tools, even the browser, are separate tools. The design of the user interface centers 
around the concept of code processing, i. e. editing plus function keys/mouseclicks to activate analysis 
and show answers to user questions (~ONM90B]). A maintenance tool needs to work with several levels of 
abstraction, but where we can, we fold this information back into the code, since maintenance programmers 
work with code. Browsing (like Pecan) is a crucial element in understanding code. AMT s chunker is probably 
the most useful tool for bottom-up understanding and reverse engineering. It uses other tools to help 
answer questions about chunk behavior (e.g. Data Flow, Control Flow, test paths, interfaces, variable 
declarations). AMT can highlight chunks, replace them with their abstraction, show how they fit into 
the overall structure, analyze and show their interface behavior, etc. Judicious highlighting can speed 
up understanding ([F ISH81]). Visual tools with primary connections to the user interface are zoom in/out/information 
suppression, highlighting, graphical information overlaid over code, side-by-side views (e. g. variables 
in code highlighted, their declarations in separate viewbox). Besides browsing and viewing capabilities 
of the code as is , the user interface needs to show the programmer the effect of changes ( What am I 
about to do to myself and the code? ). All analysis teds contribute such information, the user interface 
makes it visible. Currently we have a prototype of the User Interface for a subset of the took. We are 
in the process of designing experiments to quantitatively justify our design decisions for the user interface 
before building the full user interface. 2. Chunker The Chunker provides the abstraction mechanisms necessary 
to understand and identify levels of abstraction, restructuring and reverse engineering that is based 
on programmer understanding as opposed to solely on syntactic and some derived semantic information. 
The Chunker combines a syntactic structure view (blocks, modules) with a dynamically created structure 
based on the user s understanding. The first step is to provide a blocking feature similar to WordPerfect, 
that lets the user identify chunks of understanding. These are made into an abstraction through a userdefined 
description. Currently this description is an English narrative. In the future we would like to also 
associate a chunk with a formal specification to enable cut-and-paste support and more powerful reverse 
engineering capabilities. The structure created by the chunker defines the user s understanding of the 
code. It parallels the existing results of cognitive psychology about how programmers (novices, experts) 
understand programs through chunking. Ilis way the tool supports bottom-up understanding and k ilding 
of levels of abstraction that are influenced by the code s; yntactic structure as well as its understanding 
(dynamic structure) by the programmer. This combined with XWindows viewing capabilities for elements/levels 
of this structure provides visual aid for program understanding (see also [OMAN90A] which motivated some 
of this work). Beyond visual aids the AMT chunker interfaces with some of the other tools via their results 
to provide support that can guide and support reverse engineering and restructuring. The Incremental 
Parser (via its symbol table, use table etc.), and the Control Flow and Data Flow tools can provide information 
about syntactic and semantic connections behveen user defined chunks (the elements of the code s dynamic 
structure). This helps answer questions like * which variables are local to this chunk, which are global? 
* what are their types? * could some chunks be reorganized to increase cohesivenesswithin a module and 
to decrease coupling between modules? E. g. 3 chunks are identified sequentially. Chunk 1 and 3 operate 
on the same data, Chunk 2 does not. Could we/should we combine Chunks 1 and 2 into one module, and make 
Chunk 2 a separate syntactic entity? * if I wanted to make a chunk into a procedure or function, what 
would its interface (control/data) be? Such a capability would also help the user to avoid introducing 
procedure generation errors when variable types are incorrectly stated and variables are (erroneously) 
assumed to be local or global [ADAM89]. * if I were indeed to make such changes as the above, what would 
I have to retest? This tool feature cooperates heavily with the regression testing tool, applied to chunks 
instead of syntactic units. * what does the current structure (syntactic and dynamic) look like? The 
advantage of such a chunker tool is that we can augment reverse engineering and restructuring algorithms 
to make them work with understanding as defined by the user. This way, the resulting structure of the 
software will parallel how the user understands the software and work with user understandable chunks. 
Currently work is underway at IIT to extend the existing chunker to enable user defined and AMT guided 
cut­and-paste capabilities. This will provide AMT assisted restructuring capabilities. 3. Incremental 
Parser The incremental parser uses editor information (code plus changes) and generates deltas for parse 
tree, symbol table, usetable, syntactically given structure, etc. The editor provides source and changes 
to parser. The Parser parses changes and determines connectors where these changes fit into its derived 
objects (structure/parse tree, various tables like symbol table, use table, etc.). The parser is based 
on an LALR(l) parser generator (Yacc). It uses Bottom-up (shift/reduce) parsing. Incremental parsing 
is based on using two grammars, a concrete grammar for the source language (Ada) and an abstract grammar 
based on the structure tree to determine when to start and stop reparsing and where the parsed string 
fits into the existing structure tree. Our method (~ONM90A]) is a variation of Ghezzi/Mandrioli ([GHEZ80]) 
and Ballance/Butcher/Graham ([BALL88]). It needs less of the parser state history around. It also needs 
a lot less data than other incremental parsers. This parsing approach is useful for any tool that needs 
parsed information whether that be a static or dynamic analysis tool. AMT uses it as one of the core 
services and a prerequisite to practically all other tools. The user sees user level views based on user 
level questions (e. g. what is the type of this variable? Which variables are used in this change?), 
not on internally available table information. we are currently working on extending the incremental 
parser to understand C++. This will enable us to show in practice how easily the AMT can be adapted to 
new languages. 4.Control Flow/Data F1OWAnalysis The user seesslices through the code (paths) either as 
highlighted statements with the surrounding code or with the surrounding information suppressed. He or 
she may ask for declarations of variables involved along a path. Internally, paths are patched after 
changes, new ones are determined via path patching and delta analysis of the Control flow graph (~ONM89]). 
Incremental data flow analysis ([KEABS8]) determines potentially affected and affected variables for 
a change. One may also ask for all variables defined/used in a chunk or path of code. Most information 
is shown with highlighting of relevant code segments. Data flow analysis is done inter and intra module. 
This tool supports analysis of impact of change. It also is a prerequisite to regression testing support. 
5.Regression Testing Support Regression test support must identi~ test cases to throw out test cases 
to modify test cases to add test cases to rerun as is. In the AMT, test case attributes include: Input 
(maybe data or ranges/class definition), expected output (data or ranges/class definition), path traversed, 
assertions along path. Not all attributes of a test case need to be present. The tool uses the relationship 
between test case attributes, derives changes with the aid of other tools of the AMT (e. g. control tlow/data 
flow analysis and path patching algorithm) and with testing rules/criteria. Test criteria can be pure 
path oriented (e. g. minimally thorough, see [VONM90C]), data flow oriented (e.g. Weyuker D-U-A criteria). 
We also allow for some user definable testing rules. Test support is on the module and interface level. 
We are using a frame based system and an assertion based truth maintenance system to guide the identification 
of necessary changes after code has been altered ([JEON9 l]). This regression test support tool has been 
prototype in prolog. 6.Object Support Most of the existing work is browsing (e.g. [OBR187]). Besides 
such aids for object comprehension, AMT will provide analysis of structure and inheritance properties. 
It will also identify possibilities for object slicing and object reorganization when changes to the 
code make this advisable. Work on this tcml is in the beginning stages. 7.Metrics These include impact 
and change metrics. They also allow trend analysis. Emphasis is on metrics relevant for estimating effort 
of proposed change and quality of resulting software. Many are tools driven and collected while the tool 
does its analysis. Metrics come in three categories * traditional change metrics (cf. [COLL83], [YAUSS]), 
e. g. Yau/Collofello ripple -Logical Stability Metrics McClure maintenance metric, a simplified version 
of Yau/Collofello s LSM Information Flow Metric (Kafura, Henry) McCabe s v(G) etc. These metrics can 
be used in a snapshot fashion, but we can also collect them over a period of time and determine trends 
(deltas make that very easy). * Delta Metrics, e. g. Size of change in structure tree, DFA tables, Control 
flow graph, paths, etc. These metrics show a measure of change at various levels of analysis. They can 
be used as independent variables for effort prediction models. We view these single issue delta metrics 
as more useful to this end than composite metrics like Yau/Collofello s LSM. Composite metrics have had 
difficulties with statistical model validation (cf. [CONT86]). * Tool related Metrics, e. g. Chunker 
lines of code defined by chunk, average chunk size, % of code defined by chunk (i. e. understood and 
abstracted by programmer), #chunks (high number relates to complexity of understanding for person doing 
the chunking), # chunks affected by changes in another chunk (interface complexity of a chunk). Test 
Support YO paths with defined tests % paths tested % invalid assertions after change % paths that must 
be redone and/or retested after a change % input descriptions that need changes. These tool related metrics 
measure the amount of effort involved in understanding code, indicate the amount of change in the software 
product after a code modhication, and indicate whether modifications are complex or not. All this affects 
effort to regression test and debug changes and the resulting stability and reliability of the modified 
software product. Once the AMT is implemented, experiments will have to be run to build the metrical 
models and validate all the metrics. 8.Database An object oriented, semantic data base has been primarily 
useful for applications like a tool chest. Some tools use their database as the bottom level , others 
use a smart database that controls the tools which then do not have to know as much about each other. 
We are using the latter, especially since we are working on knowledge based regression testing and need 
to use a database that allows for frames and knowledge representation. 9.Configuration Management and 
Configuration Control A project selection utility up front controls basic access to code files. A browsing 
mechanism helps to identify the proper files to work on. Access may be only to learn through use without 
being able to save changes, or it may allow establishing a new baseline. The Delta mechanism allows for 
that easily. Work on this part of the AMT is ongoing. V.Conclusion Maintenance need not be the stepchild 
of CASE tools. While some of the AMT s tools exist elsewhere as single purpose utilities or tools, The 
AMT shows that together can support more efilcient maintenance. Needs that existing single purpose tools 
do not support are * efficient delta mechanism (don t duplicate too much data or analysis). * incremental 
analysis for all tools. * aggregation of analysis information. The AMT provides for these needs. Maintenance 
tools are about learning (about the code) and applying that knowledge (to make the right changes without 
destroying the product). As in a class room, hands-on experiences support and enhance the knowledge acquisition 
process. AMT is built around the concept of Use-to-learn . Many programmers have a deeply rooted skepticism 
about using design information to guide their change decisions. They want to stay close to the code Only 
code really tells me what is going on. I need to make changes to the code, not to the design . This motivated 
AMT s approach to code processing. We still give a variety of sophisticated analysis tools including 
reverse engineering, but the code stays at center stage. Last, but not least, the user interface should 
answer questions about the code, not inundate with massive amounts of derived information. This motivated 
our What you want is what you get philosophy. We have excellent technology for individual tools to support 
aspects of maintenance. What we need is to put them together into an effective package. AMT shows how 
this can be done. Like any other tool chest, its success will depend on validation through experiments. 
In spite of single issue experiments (e. g. [SCAN90]), we still have much to learn about putting effective 
toolchests together. Lherature <RefA>ADAM89 Adamoy A Naive Approach to Software Structure Validation , Microprogramming 
and Microcomputers 25, 1989, North Holland, pp.361-366. ADEL85 Adelson, Solowa~ The Role of Domain Experience 
in Software Design , TSE SE-11, 11(1985), pp.1351-1360. BALL88 Ballance, Butcher, Graham; Grammatical 
Abstraction and Incremental Syntax Analysis in a Language Based Editor , Prom. ACM SIGPLAN Conf. on Programming 
Language Design and Implementation, Atlanta, GA June 1980, pp. 1S5-18S. CHO190 Choi, Scacchi; Extracting 
and Restructuring the Design of Large Systems , IEEE Software, Jan. 1990, pp.66-71. COLL83 Collofello, 
Woodfield; A Proposed Software Maintenance Environment , Software Maintenance Wshop, Dec. 6-8, 1983, 
pp.1 18-119. COLL88 Collofello, Orn; A Practical Software CONT86 CORB89 EHRL134 FISH81 GHEZ80 JEON91 
KEAB88 MAYE81 OBR187 0MAN90A OW90B RAJL90 REIS85 SCAN90 VONM88 VONM89 VONM90A VONM90B Maintenance Environment 
, Procs. Conf. on Software Maintenance 1988, Oct. 24-27, 1988, pp.45-51. Conte, Dunsmore, She% Software 
Metrics and Models , Benjamin Cummings, 1986. Corm Program Understanding Challenge for the 1990s , IBM 
Systems Journal 28, 2(1989), pp.294-306. Ehrlich, Solowa~ Empirical Studies of Programming Knowledge 
, TSE SE-10, 5(1984), pp.595-609. Fisher, Coury, Tengs, Du~ Minimizing the Time to Search Vkual Displays 
the Role of Highlighting , Human Factors 32, 2(1981), pp.167-182. Ghezzi, Mandrioli; Augmenting Parsers 
to Support Incrementality , JACM 27, 3(July 1980), pp.564-579. Jeoq Knowledge Based Regression Testing 
in the AMT , PhD Dissertation, Illinois Institute of Technology, 1991. Keables, Roberson, von Mayrhauseq 
Data Flow Analysis and its Application to Software Maintenance , Procs. Conference on Software Maintenance, 
1988, pp.46-58. Mayer The Psychology of How Novices Learn Computer Programming , CACM Computing Surveys, 
13(1981), pp.121-141. O Brien, Habert, Kilia~ The Trellis Programming Environment , 00PSLA 87 Procs. 
October 4-8, 1987, pp.91-102. Oman, Coolq The Book Paradigm for Improved Maintenance , IEEE Software, 
January 1990, pp.39-45. Oman; Maintenance Tools , IEEE Software, May 1990, p. 59-66. Rajlich, Damaskinos, 
Lines, Korshi@ VIFOR: A Tool for Software Maintenance , Software -Practice and Experience 20, l(Jan 1990), 
pp.67-77. Reisq Pecan: Program Development Systems tat Support Multiple Views , TSE SE-1 1, 3(March 1985), 
pp.276-285. Scanla~ Structured Flowcharts outperform Pseudocode: an Experimental Comparison , Software, 
Jan. 1990, pp.28-36. von Mayrhauser, Johnsory RASE -Requirements Analysis for Software Evolution , IIT 
Technical Report AvM-3­ 1988. von Mayrhauser, Jeory Path Patching for a Maintenance Tool , IIT Technical 
Report, AvM-1-1989. von Mayrhauser, Kent, Weber; Efficient Incremental Parsing for Software Took , submitted 
for publication. von Mayrhause~ We need Code Processing or Should CASE care about Maintenance? , Procs. 
CASE 90, Dec. 1990, Irvine, CA. VONM90C von Mayrhausev Software Engineering Methods and Management 
, Academic Press, 1990. VONM91 von Mayrhauseq Regression Testing Support for Evolving Software , IIT 
Technical Report, AvM-2-1991. YAU88 Yau, Chang A Metric for Software Maintenance ; Procs. Conf. on Software 
Maintenance 1988, Oct. 24-27, 1988, pp.374­ 381</RefA>. Bibliography Dr. Anneliese von Mayrhauser is an Associate 
Professor at Illinois Institute of Technology. She received her PhD in 1979 from Duke University. She 
also holds a Dipl. Inf. degree from the Technische Universitaet Karlsruhe, West Germany. Her research 
interests include tools for software development and maintenance, regression testing and software reliability 
models. She is a member of the Board of Governors of the IEEE Computer Society and Chair of the Technical 
Committee on Software Engineering of the IEEE Computer Society. 299 
			
