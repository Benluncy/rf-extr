
 A HIERARCHICAL STRUCTURE FOR CONCURRENCY CONTROL IN A DISTRIBUTED DATABASE SYSTEM H. Yamazaki, S. Hikita, 
I. Yoshida, S. Kawakami and Y. Matsushita Oki Electric Industry Co., Ltd. Minato-ku, Tokyo, 108 Japan 
 ABSTRACT This paper focuses on the concurrency control problem for a distributed database system. A 
new control philosophy called hierarchical processing structure is proposed. Two different types of the 
consistency are clearly defined, and the hierarchi- cal processing structure is derived fromthese consistency 
types. This structure provides the fol- lowing features; i) The centralization of processing particular 
site can be avoided. load on a 2) Two distinct types of updating mdefined according to two aspects consistency. 
echanism of data are 3) A comprehensible philosophy for the concurrency control is established. i. 
Introduction In the current computer network environment, a distributed database may be an attractive 
approach for designing a database system. Rothnie et al. point out that a distributed database system 
has the following advantages over a centralized database systeml: a) insusceptible to total failure 
 b) faster data access and reduced communication cost c) Upward scaling of database capacity. However, 
there are many technical problems to be overcome in distributed database systems Such as scheduling and 
distribution of queries, preservation of data consistency, optimal file allocation, etc.. One of the 
major problems is how to preserve the database consistency in a distributed database. However, the consistency 
problem has two aspects; syntactic consistency and semantic consistency. Although many papers have been 
written about the former kind of consistency, few papers have discus- sed the latter kind2,3,4,5,7, 
8. In this paper, a hierarchical processing struc- ture for both kinds of consistency is proposed. 
In section 2, the terminology used to discuss con- sistency problems in this paper is defined. In section 
3, the hierarchical processing structure is proposed. In section 4, the algorithm based on the hierarchical 
processing structure is described and, also, a brief description for handling crash detection and recovery 
is presented. 2. Consistency Problems in Concurrency Control In this paper, the relational model is 
assumed for discussing consistency problems. A relation consists of a set of "fragments" each of which 
is a set of tuples. In the proposed distributed database system, the fragments of a relation could be 
distri- buted over the network. In this environment, each database site has a directory in which the 
fragment identifier is mapped to the site identifier(s) and vice versa. A database site is a logically 
defined node, and is equivalent to a database management system itself. The distributed database system 
is defined by a set of database sites connected by com- munication channels. Fragments are redundantly 
stored among sites and the fragment is updated by the "fragment process" which is generated by the distributed 
database management system (DDMS) within the site. Therefore, a fragment process is identi- fied by a 
pair (fragment id, site id). A set of fragment processes for the same redundant fragments is called a 
"closed update group". Generally, an update query is decomposed into more than one transaction, each 
of which is proces- sed concurrently within each closed update group. At this time, the redundant fragments 
within a closed update group have to be syntactically consis- tent and all fragments in the closed update 
groups, each of which processes a transaction for a given query, must be semantically consistent. Here, 
the former is called "syntactic consistency" and the latter is called "semantic consistency". An example 
of these two types of the consis- tency is shown in Fig. 2.1. Suppose that an employee relation is composed 
of three fragments, each of which is stored redun- dantly. Therefore the fragment-processes for these 
fragments form three closed update groups as shown in Fig. 2.1. Furthermore, suppose that a query which 
adds ten dollars to all employees' salaries is 35 input from the source site B soon after a query from 
source site A which raises all salaries by ten percent. In this case, let us assume that each transaction 
from the site A is processed in the closed update group a and b earlier than that from site B, and that 
the query from site B is proces- sed in the closed update group c earlier than that from site A. Here, 
there is a contradiction between the closed update group a, b J update source site A A2 ~/ QA: Rl0a~sSealary 
~TA3~ ~ QB: $RlaOisSealary and group c since in the former groups the employees' salaries are raised 
by ten percent and then are in- creased by ten dollars. On the other hand, in the latter group the salaries 
are increased by ten dollars first and then are raised by ten percent. Thus, it can not be said that 
semantic consistency is preserved. However, all redundant fragments within each closed update group have 
the same value. Thus syntactic consistency is preserved. QA, QB: queries TAi , TBi: transactions fragment 
processes for fragment 'a' I c~osed gr~up~a' .....update ~ closed update group 'b' fragment processes 
for fragment 'c' Fig. 2.1 Two types of consistency In a distributed database system, those two kinds 
of consistency must be preserved whenever fragments are updated. SDD-i 3,4 proposes several protocols 
to preserve semantic consistency. Particularly, in protocol 4 all sites in a network are locked simultaneously. 
This locking mechanism leads to the centralization of all updating controls on the source site, which 
may cause a performance bottleneck in a distributed database system. INGRESs proposes two protocols for 
database up- date, one to preserve syntactic consistency only, the other to preserve both syntactic 
and semantic consistency. However, the protocol for the latter is almost the same as that of SDD-i, 
which may also cause the performance bottleneck. 3. Hierarchical Processing Structure In this section, 
a hierarchical processing struc- ture to solve the performance bottleneck problem described in section 
2, is proposed. Generally, to process a given update query, processing of more than one closed update 
group is required. This general query processing is described in Fig.3.1. Here, the process accepting 
the user's update re- quest is called the "source". On the other hand one of the fragment processes 
within the closed update group, which is eonmnanded by the source to process the transaction is called 
the "master" and all the remaining processes within the closed update group are called the "slaves". 
Furthermore, the group formed by the source and the masters for a given query is called the "related 
update group". A query from a user is accepted by the source, then it is decomposed into transactions, 
each of which is sent to the appropriate master. The master processes the given transaction. Then, the 
update of the fragments within the closed update group in- cluding the master is performed, based on 
the direction by the source. Thus, the processing load is shared by the source and the master, resulting 
in the improvement of the performance bottleneck. related update group f~------ I I I I i closed 
update group \ user queryi O O O O  : source ( ) : closed update group [] : master +~...... ) : 
related update group O : slave  Fig. 3.1 Relationship between a closed update group and a related update 
group The interactions between the source and a mas- Q = [update R ] ter, between the master and a slave 
are based on the two-phase-commit scheme 9 described in section 4. LR. SAL: = R. SAL *i.i Fig. 3.2 shows 
an example of the formation of  l update Fi ] a related update group preserving semantic consis- T 
i = tency by using the same example as shown in section LFi. SAL: = F i. SAL *i.i 2. Here, the employee 
relation R is divided into the fragments F i (i=l, 2, 3) each of which is up- dated by master i. The 
query Q which gives a ten Single percent salary raise (SAL) to all employees is related given from a 
user to source: update  group update R ] Q = I ~ r 1 L_R. SAL: = R. SAL *i.i ~ t e The source decomposes 
Q into three transac- tions (Ti, T2, T3) forming a single rather than  T~er 2 several related update 
groups. This formation of a single related update group guarantees semantic consistency. Fig.3Â°2 Formation 
of a single related update group On the other hand, Fig.3.3 shows the case when each of the transactions 
is processed by independ- ent related update groups, with the result semantic consistency is not always 
guaranteed. related update group 1 related update group 2 related update group 3  Fig. 3.3 Formation 
of multiple related update groups Fig. 3.4 shows the conflict between two relat- ed update groups, each 
of which is formed as a single related update group. Here, the conflict is the situation where a master 
receives more than one transaction simultaneously from more than one source. In Fig. 3.4, masters i, 
2, 3 are placed in conflict by the sources A and B. ~olua~edA update~~ ~ ~ua~edB update Fig. 3.4 The 
conflict between two related update groups  A query from a user has its own priority and this priority 
is also attached to the transactions when they are issued by the source. Here, the priority of a query 
is determined by loosely syn- chronized time stamp and site identifier where the query is given. Each 
source can compare the priority of its own user's query with the priority of another query which is reported 
by the master where the conflict occurs. Then, the source with a lower priority query must give up the 
formation of a related up- date group if the other source has a query with higher priority (see section 
4). Thus, the source which accepts the query with higher priority can obtain all required masters, resulting 
in the removal of the conflict condition. Generally, to guarantee both syntactic and semantic consistency, 
the formation of a single re- lated update group must be applied for query pro- cessing. However, higher 
system throughput is obtained by the formation of multiple related up- date groups, since the parallel 
processing of many independent queries is possible. Therefore, the formation of multiple related update 
groups is pre- ferable, if the both consistencies can be still preserved by such a formation. After 
the processing of a related update group, all the fragments within a closed update group must be updated 
consistently. Each of these groups is also a processing group called a commitment group 6, and is processed 
by the hierarchical processing scheme described in the following section. Furthermore, the hierarchy 
of these commitment groups can be further nested. Note that any fragment process within a closed update 
group can be a master. The master is select- ed by the source according to the master-selection list, 
which is different in each site. This master- selection list consists of the identifiers for the sites, 
the correspondence tables between sites and fragments. Ordered sequence of the site identi- fiers is 
used as criteria for the master selection by the source. In the following section, the hierarchical 
schemes which are executed between the source and the master, and the master and the slave are dis- cussed. 
 4. The Hierarchical Processing Scheme  The sequence of the proposed schemes is shown in Fig. 4.1. The 
sub-scheme processed between the source and the master is called the "related update control" sub-scheme. 
On the other hand, the sub- scheme processed between the master and the slave is called the "closed update 
control" sub-scheme. These sub-schemes are based on the "two-phase-commit" scheme. related control update 
sub-scheme closed update control sub-scheme Source ~f Master Slave secure (transaction) lock (update 
list) secure phase secured/reject (priority) ACK/NAK (priority) lock phase conmnit phase commit/backward 
committed recover update/recover ACK I update phase Fig. 4.1 Sequence of processing  In the related 
update control sub-scheme, the two phases are called the "secure" phase and "commit" phase, respectively. 
On the other hand, in the closed update control sub-scheme, the phases are called the "lock" phase and 
"update" phase for facilitating understanding although the same names used in the related update control 
sub-scheme could be applied. The details of our scheme are described below. The Source's Operation 
 (i) The source allocates the transaction to each master in the related update group by sending the 
"secure" message.  (2) The source sends the "commit" message to each master if and only if "secured" 
messages from all the masters are received.  (3) If at least one "reject" message is received, the source 
compares its own priority with the priority received as a parameter of "reject" message. If the latter 
priority is higher than the former, the source gives up the formation of related update group and sends 
the "backward recover" messages to the masters which returned "secured". Otherwise, the source repeatedly 
sends the "secure" message to the masters returned "reject" until they return "secured". Here it is guaranteed 
that this scheme works successfully since other sources having lower priority withdraw the formation 
of related update group by the same priority comparison mechanism.  The Master's Operation  (i) The 
master, upon receiving the "secure" mes- sage, makes the update list (the list of modified tuples), whenever 
the master has no other transaction to deal with. If the mas- ter is processing other transaction, it 
sends back the "reject" message to the source, with  the hierarchical scheme the priority of the transaction 
being processed.  (2) The master sends the "lock" message including the update list to its all slaves 
in the closed update group.  (3) The master can send back the "secured" message to the source if, and 
only if, it receives ACKs from all the slaves. If the master receives at least one NAK from the slave(s), 
the priority carried by the NAK message is compared with the priority of the transaction being processed 
by the master. If the latter priority is higher than the former, the master sends the "lock" message 
again to the slave returned NAK. This step is repeated until ACK is returned from the slave. Otherwise, 
the master sends back the "reject" message as a response of the "secure" message to the source. Furthermore, 
the "recover" messages, by which the lock con- dition is released and the received update list is discarded, 
are sent to the slaves returned ACK.  (4) When the master receives the "commit" from the source, it 
actually proceeds to update the re- quested fragment in a secondary storage, resulting in the situation 
where backward re- covery is impossible. Then the master sends the "update" message to all the slaves. 
 (5) When the master receives the "backward recover" from the source, it stops updating its fragment 
and directs all the slaves to stop updating their fragments by sending the "recover" mes- sages. No responses 
must be returned to the source for the "backward recover".  (6) When the master receives ACKs from all 
the slaves in the closed update group, it returns the "committed" message to the source. Thus, all of 
the update processing at the master is completed.  group or not. The manager broadcasts the diagnostic 
 The Slave's Operation message specifying the error site identifier to all (1) When the "lock" message 
is received~ the re- quested fragment is locked and the update list is put into the temporary storage 
and ACK message is sent back to the master, if the slave has no other "lock ''~ message to deal with. 
Otherwise the slave sends back the NAK message to the master. (2) When the "update" message is received, 
the slave proceeds to update the requested frag- ment, resulting in the situation where back- ward recovery 
is impossible. Then, the slave sends the ACK to the master.  (3) When the "recover" message is received, 
the slave stops updating its fragment and locked fragment is released. In this case no res- ponses must 
be returned to the master.  If error conditions such as time-out or illegal sequence are detected in 
any site, the processing of the scheme described above is aborted, and the crash detection and recovery 
processing is started. In our sites. The manager receiving this diagnostic message registers the specified 
site identifier into the error-site-list and also sends back an acknowledge- ment to the broadcasting 
source manager. The broad- casting source manager determines whetlher itself is in majority or not by 
the number of received acknow- ledgements. If it is in majority, the sites which did not send back acknowledgement 
are registered to the error-site-list and it broadcasts the diagnostic message again. This step is repeated 
until the broadcasting source manager receives acknowledge- ments from all the normal operating sites. 
If the manager itself is in minority, it sends the message "we are down" to all the sites sending back 
acknow- ledgements. Then, all the fragments within the minority are blocked. After the partitioning state 
of the network is recovered, the transactions al- ready processed in the majority are also processed. 
Details of the crash detection and recovery process- ing will be discussed in the near future. 5. Conclusion 
 system, the crash detection and recovery processing is executed by the diagnostic managers each of 
which resides in the site where special logical connec- tions, called diagnostic connections, between 
any pairs of diagnostic managers, are established at the system generation time. So far, in most of the 
proposed distributed database systems, the schemes to synchronize the update transactions and the schemes 
for crash detection and recovery are not clearly separated. Furthermore, these two types of schemes are 
mixed on the single logical connection. Thus, these schemes have the following dis- advantages; i) 
The schemes are too complex. 2) The broadcast of emergency messages or high priority messages is not 
easily handled. 3) Useless messages may be transmitted at the crash detection and recovery. In our 
system, these problems are solved by introducing the diagnostic manager and diagnostic connection and 
by separating clearly the crash detection and recovery scheme from the ordinary concurrent update scheme. 
This makes both of the schemes concise and simple. Furthermore, the amount of data transferred at crash 
detection/recovery and the time expended can be considerably decreased, since the scheme is executed 
by the uniqu e diagnos- tic manager in the site instead of by fragment processes. The outline of the 
crash detection and recovery is as follows. The diagnostic manager which is notified about the error 
condition from a fragment process tries to communicate with the other manager .which resides in the possibly 
erroneous site in or- der to determine whether the network is partitioned or not. If the network is partitioned, 
the manager must try to ascertain whether it is in the majority It is suggested in this paper that concurrency 
control in a distributeddatabase system may be de- signed to use a hierarchical processing structure 
in which the related update group and the closed update group are the basic components. The follow- ing 
features are given in this hierarchical struc- ture. l) The centralization of the processing load on 
a particular site can be avoided (distribution of the processing load to each site). 2) The mechanism 
to preserve data consistency becomes simple and clear since two types of data consistency are clearly 
distinguished and two distinct update groups are defined accord- ing to each type of consistency. 3) 
A comprehensible philosophy for concurrency control is established. In order to further improve the 
proposed con- currency control scheme, its performance must be analyzed quantitatively. The positioning 
of the distributed database system in the layered computer network architecture, especially the relationship 
between the crash detection and recovery processing and the overall network management processing need 
 further discussion. REFERENCES  <RefA>[1] J.B. Rothnie, N. Goodman, "A Survey of Research and Development 
in Distributed Database Management", Proc. Int. Conf. VLDB, 1977, pp.48-62.  [2] R. H. Thomas, "A Solution 
to the Concurrency Control Problem for Multiple Copy Data Base", Compcon 78, 1978, pp.56-62. [3] P.A. 
Bernstein, J.B. Rothnie, N. Goodman, 40 C.A. Papademitriou, "The Concurrency Control Mechanism of SDD-i. 
A System for Distributed DataBase (The Fully Redundant Case)", IEEE, Trans. Software, 1978, Vol. SE-4, 
no. 3, pp.154-168.  [4] P.A. Bernstein, D.W. Shipman, J.B. Rothnie, N. Goodman, "The Concurrency Control 
Mechanism of SDD-i: A System for Distributed Database (The General Case)" Technical Report, CCA-77-09, 
1977.  [5] M. Stonebraker, "Concurrency Control and Consistency of Multiple Copies of Data in Distributed 
INGRES:, Proc. of the THIRD BERKELEY WORKSHOP, 1978, pp.235-258. [6] J.B. Brenner, "Preliminary View 
on Administra- tion and Use of Logically Related Sessions", Contribution to ISO/TC97/SCi6/WG2 N60. [7] 
C.A. Ellis "A Robust Algorithm for Updating Duplicate Databases", Proc. of the SECOND BERKELEY WORKSHOP, 
1977, pp.146-158. [8] C.H. Lee, R. Shastri, "Distributed Control Schemes for Multiple-Copied File Access 
in a Network Environment" Compsac 77, 1977, pp. 722-728.  [9] J.N. Gray, "Notes on Database Operating 
Sys- tems", Operating Systems and Advanced Course. Berlin, Heidelberg: Springer-Verlag, 1978, pp.394-481.</RefA> 
 
			
