
 A Genetic Approach to Color Image Compression Harald Feiel Sub Ramakrishnan Bowling Green State University 
Department of Computer Science Bowling Green, Ohio 43403 Phone: (419) 372 2337 email: hfeiel @ cosy.sbg.ac.at 
 Bowling Green State University Department of Computer Science Bowling Green, Ohio 43403 Phone: (419) 
372 2337 fax: 419 372 8061 email: rama@cs.bgsu.edu  Harald Feiel is now with Institut fiir Computerwissenschaften, 
Universit/it Salzburg, A-5020 Salzburg, Austria ABSTRACT Advances in compression technique will have 
to keep pace with the exponential increase in the need for storage and transport of bulky multimedia 
images. This paper concerns the use of vector quantization for processing color images. We propose a 
new, genetic algorithm based, scheme for code book design in the quantization process, and compare its 
performance with the widely used Linde- Buzo-Gray algorithm [8]. We also introduce a new genetic operator 
known as synchronization that works especially well in this problem domain. The quality of the resulting 
code book is used as a performance criteria for LBG and for the proposed approach. Our observations reveal 
that the proposed scheme is better than the LBG algorithm by a factor of between 5% to 25%. The performance 
gain is especially significant for large code books. 1 Introduction The use and growth of multimedia 
technologies require that storage and retrieval of documents and images be expedited to meet response 
time constraints of end-users. Commercial full length movies on a CD ROM could not exist without data 
compression. However, advances in compression techniques will have to keep pace with the exponential 
increase in the need for storage and transport of bulky multimedia images. This paper concerns one approach 
to data compression, known as vector quantization [3, 5]. Common input data for vector quantizers are 
segments of speech, sound wave forms and segments of images. Vector quantization provides a reduction 
in the volume of data to be handled thereby simplifying certain digital signal processing tasks such 
as classification and linear transformation. In many instances, these tasks can be replaced by simple 
table lookup procedures [3]. "'Permission to make digital or hard copies of part or all of this work 
tbr personal or classroom use is granted without fee provided that copies are not made or distributed 
for profit or commercial advantage and that copies bear this notice and the 1i111 citation on the first 
page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with 
credit is permitted. To copy otherwise, to republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee.'" .c': 1997 AC..X..I ()-897t)1-850-9 97 0002 350 Vector 
quantization is a pattern recognition scheme where an input pattern known as training sequence, is broken 
up into small segments called training vectors. Each training vector is then mapped to a vector from 
among a predefined set of code book vectors. It is a lossy compression scheme, since the original pattern 
is approximated by code book vectors with appropriate references to the original image. Good compression 
can be achieved by transmitting only a reference to the training vector and not the actual training vector. 
The code book has to be chosen carefully to achieve good compression quality and image reproduction at 
the sink. The mean squared error of the sum of the difference between each of the training vectors and 
its corresponding code book vector is often used as a performance measure of vector quantization algorithms. 
Linde, Buzo, and Gray have proposed a well known, LBG, algorithm for image compression [8]. Since then 
a number of researchers have proposed extensions to speed up LBG (see [2] for references). However, approaches 
for improving the quality of the compressed image are less well understood. Thede [ 10] proposed the 
use of simulated annealing for improving the quality of LBG. Although the paper claims that the simulated 
annealing approach is an improvement in quality over LBG the details of the implementation are unclear 
from the paper and we have not been able to reproduce their results. Recently, there has been a growing 
interest in algorithms which rely on analogies to natural processes. Genetic algorithms belong to these 
classes of algorithms and have been used to solve complex optimization problems [!, 7, 9]. The traditional 
methods of optimization do not fare well over a broad spectrum of problem domains [4]. Some are limited 
in their scope because they employ local search techniques (e.g. calculus based methods). Others, such 
as enumerative schemes, are not efficient when the practical search space is too large. A basic advantage 
of GA is the implicit parallelism which makes these algorithms very fast and efficient. GAs maintain 
a population of potential solutions, have a certain selection process based on fitness of individuals, 
and some 252 recombination operators. GA is an exploratory procedure that can locate high performance 
structures in complex problem domains. In this paper, we propose a new algorithm (GA VQ) -a genetic 
algorithm based approach for vector quantization of color images. We compare the performance of GAVQ 
with LBG for varying values of the number of code book vectors, n. Based on extensive numerical observations, 
we conclude that the proposed algorithm outperforms LBG by a factor of between 5% to 25%. The performance 
is more pronounced with an increase in the number of code book vectors. A brief overview of the LBG algorithm 
is given in Section 2. Details of the implementation of the GAVQ algorithm are outlined in Section 3. 
Performance evaluation of the algorithm and concluding remarks appear in Section 4. LBG Algorithm The 
LBG approach starts of by generating an initial code book and refines the code book in phases. The process 
terminates when the refinement is less than a user specified threshold. The process consists of four 
basic steps [8]: (i) Initialization The performance of the LBG algorithm is critically dependent on the 
choice of the initial code book. Poor choice of the initial code book can result in a final code book 
trapped in a local optimum. The splitting technique [5] , discussed below, works in stages and provides 
a good choice for generating the initial code book. Splitting: In stage 1, the code book has 1 vector 
(whose value is the average of all the training vectors). At each stage, the training vectors are mapped 
to code book vectors using a partition scheme (see item (ii) below for a discussion of the mapping strategy) 
after which the code book vectors are replaced by the centroid (or average) of its corresponding training 
vectors. In stage 2 we double the number of code book vectors. Formally, the number of vectors in stage 
i, i > 2, is double that of the number in stage i-l; and includes all vectors of stage i-1 plus a set 
of new vectors obtained by adding a small perturbance vector (for example, a vector of all Is) to each 
of the code book vectors of stage i-l. The process is complete when the number of code book vectors is 
equal to n. (ii) Partition Each vector, tv , in the training sequence is mapped to some code book vector, 
Ck, with which it has the least mean square error. Formally, (tv-Ck) 2 < {(tv-Cj) 2,Vjecj} (1) (iii) 
Update Once all the training vectors have been mapped, each code book vector is replaced by a centroid 
value which is the average of its corresponding (training sequence) vectors. (iv) Termination The quality 
of the code book is evaluated by computing the overall error (or quantization error), Ei at phase i, 
that would be introduced by vector quantizing the training set with the code book. Suppose C is the set 
of code book vectors and M(ck) is the set of all training vectors mapped to Ck Then, E/=c~ c ~ ~/(tv-ck)2 
(2) tv~ M(c k) The algorithm terminates when (Ei_ 1 - E i ) / E i < s, where s is a user specified parameter; 
otherwise, proceed to the next phase and repeat steps (ii) to (iv). 3 Vector Quantization Using GA (GAVQ) 
In GA the search space is composed of possible solutions to the problem. The algorithm begins with an 
initial generation of a uniformly random population of solution strings known as to as a chromosome. 
The objective function associated with each chromosome is called the fitness value. The fitness value 
of a chromosome represents the strength or quality of the solution. A set of chromosomes and associated 
fitness values is called the population. This population at a given stage of GA is referred to as a generation. 
New generations are evolved from the current generation by applying genetic operators like selection, 
crossover and mutation operators [4]. The genetic algorithm in the most general form is given in Figure 
1. We now discuss the details of GA as they relate to the problem, as defined in Section 2, of compressing 
color images using vector quantization. (i) Chromosome Representation One approach is to use a coding 
scheme that describes directly the mapping of the training vectors to code book vectors. For m training 
sequence vectors there will be m fields in the representation. Each field will be log2m bits long, assuming 
integer representation. In addition, the representation maintains a tag for each code book vector in 
order to interpret the mapping information. Consider the following representation for m = 8 and n = 4: 
t2 t6 t3 t4 tl t5 t7 t8 {tag:2024}. This implies that code book vector Cl has 2 training vectors, t2 
and t6; no training vector is mapped to code book vector c 2, and so on. However, this representation 
results in a fairly long string and is dependent on the length of the image being processed (for example, 
an image of size 512 x 512 with a 4 x 4 training sequence vector will need room for m = (512 * 512 ) 
/ (4 * 4) training vectors). In addition, this method poses some technical difficulties (for brevity, 
we do not discuss these difficulties here) in applying GA operators such as crossover and mutation. An 
alternate and more elegant approach is to use a chromosomal representation with only n fields (or genes), 
one for each of the code book vectors. The value of gene k is chosen to be the centroid (or average) 
of all training vectors mapped to the code book vector Ck. In a sense, these n genes carry almost the 
same information as the first approach but the representation is simpler and more efficient to handle 
than the first approach. Further, note that the average is in fact the code book vector Ck. Thus, for 
the example in the previous paragraph, the chromosomal representation using this alternate approach has 
four genes and is given by the four code vectors: {Cl, c2, c3, c4}. In the rest of this paper, we only 
use this chromosomal representation. (ii) Objective function Since each chromosome represents a code 
book and contains all the information required to build the mapping, the fitness value of a chromosome 
is readily computed as the quantization error given by equation (2) of Section 2. (iii) Inigializ~ati0n 
The initial population is determined by evaluating the mean square error (equation (1) of Section 2) 
of a randomly generated code book. A randomly generated code book is the assignment of randomly chosen 
training vectors as code vectors. (iv) Synchronization and Crossover The basic idea of the crossover 
operator is to select two chromosomes (or two code books) and generate their offsprings. Genetic algorithms 
are randomized search techniques. In optimization problems it is important to ensure that the search 
escapes local optimums. Crossover is one approach to help avoid premature convergence of the algorithm. 
Population diversity is created by combining two chromosomes at random to generate two offspring. Each 
offspring carries genetic traits from both parents and brings in diversity to the search space. Crossover 
can be illustrated by an example. Suppose the two chromosomes chosen for crossover are C. = {c 1, c 2, 
c 3 ..... Cn} and C. = l j {dl, d2, d3 ..... dn}. With a random crossover position of 2, the two offspring 
are readily seen to be {Cl, c2, d3 ..... dn} and C. = {dl, d2, c3 ..... Cn}. J Our studies reveal that 
the solution quality can be improved by the use of a new operator known as synchronization. This operator 
is applied after a pair of chromosomes is chosen for crossover but before they actually generate their 
offspring (or before crossover is applied). Synchronization attempts to fine tune the parents prior to 
crossover. The need for synchronization is motivated by the following observation. Suppose that the first 
chromosome chosen for mating has w light gray and n - w dark gray code vectors, while the second chromosome 
has w dark gray andn - w light gray code vectors. With the crossover point at w, one offspring will be 
all light gray while the other one will be all dark gray. Since images usually contain a good blend of 
light and dark grays, such pure offspring are not desirable. Synchronization is a mechanism that alters 
the arrangement of the second chromosome in order to reduce the possibility of generating such pure offspring. 
The exact synchronization mechanism is illustrated in Figure 2. Assume that the two chromosomes chosen 
for crossover are C. = {Cl, c2 ..... Cn} and C. = {dl, d2 ..... i j dk, dk+l ..... dn}. Synchronization 
is an attempt to reorder the code vectors of chromosome of the second chromosome in a way that the revised 
ordering is roughly compatible (see step 1 in the box below) to corresponding code vectors of the first 
chromosome. First time through the loop, code vector dk is compatible with Cnext at the end of Step 1. 
In Step 2, C. is reordered, C i = {dk, d2, dnext, dk+l ..... J °.., dn}. (v) Mutation Without mutation 
we can sometimes lose potentially useful information from a chromosome which can in turn result in a 
loss of population diversity. Mutation introduces random variation in the chromosomal patterns. For each 
chromosome, with certain probability mutation site is selected randomly and the value at that site is 
altered, again randomly. Viewing the code book as linear sequence of pixel values, for n = 4, and a code 
vector of size 4 x 4, there are 64 integer values representing the color information of a pixel. A genetic 
mutation for random site changes its old (color) pixel value to a random new value. (vi) Reolacement 
Strategy A number of strategies can be used to create a new generation from the previous generation [4]. 
We have used the elitist strategy by  selecting the best performing chromosome from the previous and 
current generation, thereby ensuring that the chromosome with the best objective function value always 
survives to the next generation. This strategy works very well for the problem domain addressed in this 
paper. (vii) GAVQ Terminal~i0n The execution of the algorithm can be terminated when the solution does 
not improve after the fixed number of generations specified the user. The upper bound on the number of 
iterations GA must be specified carefully [ I, 6, 7] to ensure that best objective function value is 
obtained within the specified number of iterations. 4 Performance Evaluation and Conclusions There are 
three parameters that affect the solution GAVQ: probability of crossover (Pcross)' probability mutation 
(Pmut), and population size (pOPsize). We have conducted extensive numerical evaluation of our implementation, 
across a number of these parameter values, and have arrived at the following parameter values for the 
results reported in this paper: poPsiz e = 50, Pcross = 0.6, Pmut = 0.001. Small variations to these 
parameter values did not produce appreciable change in the quality of the final solution. For brevity, 
we do not discuss some these sensitivity aspects in this paper. For all runs GAVQ is terminated after 
50 generations. Consistent with [8], the parameter, s, for termination of LBG is chosen to be 0.001. 
 Table 1 below is organized to permit evaluation of GAVQ and LBG for a set of 10 typical color images 
and the number of code vectors, n, in a code book is studied for three different values: 32, 64, and 
128. The images are obtained from a well known public domain ftp site*. The images are reduced to 96 
by 64 pixels and the size of each training sequence and code book vector is chosen to be a 4 by 4 matrix, 
i.e. each as 16 pixels. As noted earlier, the quality of image (quantization error, see equation (2) 
of Section 2) is used to compare the two algorithms. As can be seen from Table 1, our approach performs 
better than LBG for all data sets. The improvement ranges from about 5% for n = 32 to about 25% for n 
= 128. For example, note that for n = 32, the application of LBG to the beach image results in a quantization 
error of 18.97 while the proposed GAVQ algorithm has a better quantization error, 17.78, yielding an 
improvement of 6.27% over LBG. However, the performance improvement for the same image is about 19.56% 
for n = 128. It needs to be pointed out that GAVQ outperformed LBG in a fairly consistent manner across 
each of the three color values; however for simplicity, we only report in Table 1 the average quantization 
error of the three color values. Table 1 includes the effect of synchronization. Although not shown in 
Table 1, we note that GAVQ performed better than LBG even when synchronization is not applied. Synchronization 
typically improves performance from about 5% to about 10%. The primary focus in this paper was on developing 
an algorithm whose performance quality is better than that of a well known algorithm, LBG. Our implementation 
of GAVQ is a lot slower than LBG. Our future research will investigate techniques for improving the execution 
efficiency.. In addition, we will explore means for improving the quality even further without a significant 
increase in execution time. We would like to explore alternate synchronization strategies, and techniques 
for incremental mapping of training vectors to code book. Acknowledgment: The first author would like 
to thank Professor Horst Clausen, University of Salzburg, Austria for his guidance and support. Sub Ramakrishnan's 
research was supported in part by a National Science Foundation Grant # CCR CCR - 9111 292  References 
<RefA>[1] Davis, L., Steenstrup, M. (1987). Genetic Algorithms and Simulated Annealing: An Overview. Genetic 
Algorithms and Simulated Annealing (Edited by: L. Davis). Los Altos, CA: Morgan Kaufman Publishers. 
* ftp:llipl.rpi.edu/pub/imagelstilllKodaklmageslcolor [2] Equitz, W. H. (1989). A new Vector Quantization 
Clustering Algorithm. IEEE Transactions on Acoustics, Speech, and Signal Processing, 37, 1568-1575. 
[3] Gersho, A., Gray, R. M. (1992). Vector Quantization and Signal Compression. Boston, Dortrecht, London: 
Kluwer Academic Publishers. [4] Goldberg, D. E. (1989). Genetic Algorithms in Search, Optimization and 
Machine Learning. Addison-Wesley Publishing Company. [5] Gray, R. M. (1984). Vector Quantization. IEEE 
ASSP Magazine, 1, 4-29. [6] Grefenstette, J.J. (1986) Optimization of Control Parameters for Genetic 
Algorithms. IEEE Trans. on Systems Man and Cybernetics, 16, 122-128. [7] Holland, J. H. (1973) Genetic 
Algorithm and the Optimal Allocation of Trials. SIAM Journal of Computing, 2, 88-105. [8] Linde, Y., 
Buzo, A., Gray, R. (1980). An Algorithm for Vector Quantizer Design. IEEE Transactions on Communications, 
COM-28, 84-95. [9] Liepins, G. E., Hilliard, M. R. (1989). Genetic Algorithms: Foundations and Applications. 
Annals of Operations Research, 21, 31-58. [10] Thede, L., Kwatra, S. (1989). Image Sequence Coding by 
vector Quantization. Proceedings ICASSP, 10, 1909-1012.</RefA> Figure 1: Main Steps of GA Main genetic algorithm 
{ evaluate objective function; while not termination-condition do { create a new generation; } output 
best among current generation; } Procedure to create new generation { while new generation size < Population 
size do { select parent strings (from previous generation); apply crossover operator; apply mutation 
operator; evaluate objective function, add offsprings to new generation as determined by acceptance 
} Figure 2: Chromosome Synchronization. for (next = 1; next < n; next = next + 1) { 1. find a k, next 
< k < n such that dk has least mean square error with Cnext [this error is similar to Equation (1), 
Section 2 - omitted here for clarity] 2. swap dnext with dk in Cj. Table 1: Com " of LBG and GAVQ LBG 
" GAVQ change in % 32 64 128 32 64 128 32 64 128 beach 18.97 15.11 11.30 17.78 13.54 9.09 6.27 10.39 
19.56 :aps 23.24 18.92 13.67 22.34 17.09 11.40 3.87 9.67 16.61 doorknob 19.18 15.81 12.22 17.84 14.19 
9.76 6.99 10.25 20.13 face 26.91 21.51 15.93 25.68 19.18 12.68 4.57 10.83 20.40 farm 28.62 24.23 18.64 
27.33 22.13 15.58 4.51 8.67 16.42 house 32.53 28.11 22.70 31.24 26.31 19.17 3.97 6.40 15.55 lighthouse 
29.74 24.85 17.62 27.92 22.10 14.70 6.12 11.07 16.57 ~arrots 28.01 22.74 16.46 26.59 20.53 13.63 5.07 
9.72 17.19 i flane 24.47 20.14 14.52 22.22 16.50 10.51 9.19 18.07 27.62 window 30.11 25.19 18.72 28.58 
23.15 15.54 5.08 8.113 16.99  
			
