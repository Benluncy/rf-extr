
 A Parallel Application Interface Or. James A. Brown IBM Santa Teresa Lab M46/D12 P.O Box 49023 San Jose, 
Calif. 95161-9023 USA  Abstract This paper dcscribes the use of APL2 to implement parallel algorithms. 
API,2 includes high level con-structions for support of coarse grain parallel com-puting. It is possible 
to move paris of applications between machines of different architecture without compilation or linking. 
This allows the partitioning of an algorithm among available processors at run time instead of design 
time. The API,2 coopcrative processing interface allows the programmer to make use of any processor on 
a TCP/IP connected network. The possibilities for imbedded fine grain parallel support are discussed. 
 Introduction Discussion of parallel languages and algorithms has been a subject of research for many 
years. Typically, efforts are made to parailelize the largest most numer-ically intense problems which 
are targeted to run on the largest super-computers. Today, however, parallel processors are becoming 
widely available with low enough cost that they can be applied to smaller prob- lems of interest to the 
business community as well as the scientific community. Examples are the IBM ES9000 vector facility which 
provides parallelism on mainframes; the IBM 9076 Scalable Parallel System (SPI) which provides parallelism 
on AIX RISC workstations (lbl) the Everex STEP Dual Processor that provides parallelism on OS/2 personal 
computers (Evl); and many others. Parallel computers are becoming common and will become the everyday 
computing platforms in the near Permission to copy without fee all or past of this material is granted 
provided that the copies arc not made or disaibuted for direct commercial advantage, the ACM copyright 
notice and the title of the publication and its date appear, and notice is given that copying is by permission 
of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or 
specific permission. O 1994 ACM 089791..6,47-.6/ 94/0003 $3.50 future. ]'his make the question of software 
for par- allel-systems urgent. This paper discusses the styles of parallel processing and shows how the 
existing API,2 language family exploits parallel on today's machines. Directions for future deeply imbedded 
parallel support on API,2 is discussed  Classifications of Machine Architectures The literature usually 
identifies three architectural classes based on the number of data items processed by a single instruction 
and the number of instruction streams active at one time. The reader is refercd to (All) for an in depth 
discussion of parallel architec-tures. Scalar Machines When the first computers were built, they were 
very expensive and unreliable. It is not surprising that these machines processed a single instruction 
at a time and that the instruction cffccted a single primitive opcration on a single item of data and 
produced a single result. Such an architecture is classified Single Instrnetion Single Data (SISD) In 
concept, an SISD machine is like a relay race where each runner is an instruction. ()ne runner locates 
single arguments, delivers a singlc result and then returns to pass Ihe baton to the next runncr (next 
instruction). (See SISI) Compuling on next page') Vector and Array Parallel Machines A second clas~ 
of machine is realized by allowing one instruction to operate on more than on piece of dala and possibly 
produce mord than one result. This architecture is classified Single Instruction Multiple l)ata (SIMD). 
S1MI) machines are sometimes called vector processors because they process ordered col-lections of data 
in one instruction. This might be pic- tured as a relay racc whcrc each runner picks up 588 S! M I) 
Computin.g - resul t [] ............. ° , ." ( "%. "°.,. .. .,.,...'" ,,..,.."" do taE] ........... 
... ........ .' "'"'"*., ...................................... ...... . ....... ...... SISD Computing 
data [] result [] result [] ....................... " f~ ..................................................... 
 doto[] ........ ".... ""'"" ",.. ........ ,.,..,,..'" ...... ....-.-'"'"""" ~s~tt ~ d..o..t.a~ .......................... 
 SIMD Computing -Vector  ~sult . d o to...tL t ....... ~ .................~E3 .................................. 
 r~ i l do ......... ~s~t ~ .........~to~ ................... 589 several sets of single arguments 
and deposits several results before returning to the starting point. From the users point of view, the 
result of vector processing looks as if there were a vector of processors each applied to a single items 
from the argument vectors. Implementations where there actu- ally are arrays of processors, such as the 
Connection Machine, are called array processors. This might bc pictured as a relay race where each runner 
generates clones that each visit one set of arguments and deposit one result. Independent Parallel Machines 
Finally and most generally, a third class of machine is realized by applying more than one instruction 
each potentially to different data. This architecture is clas- sified Multiple Instruction Multiple Data 
(MIMI)). For example, two independent programs may run at the same time. This might be pictured as a 
relay race where all the runners run at the same time. Clearly, if this behavior is permilted by the 
rules, it will lcad to the fastest conclusion of the race. reGult :" ~ "'. ""'". ""...... data 12Z].......................... 
 '-. ......... . ................... ...... ........ result [] ....................................... 
 ~J~-~" MIMD Computing  APL2 System Structure IBM API,2 products are provided on mainframe CMS and 
TSO systems, on RISC System/6000 and Sun Solaris workstations, and on Personal System OS/2 systems. Essentially 
the same system structure is provided on all platforms despite the fact that oper- ating systems themselves 
provide very different facili- ties. (;MS is a multi-uscr system where each user gets a single-task (multi-tasking 
(,'MS systems are just appearing); OS/2 is a single-user system where the user gets multiple tasks; and 
RISC System 6000 and Sun S;olaris arc multi-user multi-tasking systems. The API,2 family of products 
has been structured to take advantage of MIMD computing. An API,2 session consists of a set of independent 
processes each of which serves a different purpose. On a machine which can run differcnt tasks on different 
processors, the API,2 system itsclf will exhibit a degree of parallelism. The processors that make up 
an API, session are idcntified in the API,2 language by number. A user interacts with tile session manager 
causing computa- tion requests to bc sent to the AP12 language processor, As the AI'I,2 program needs 
various system services, it communicates with various othcr proccssors. result ........~ ......................................... 
 :: ....... ".... "... data []. ..... ..... result [] .................................... ... "-. 
"'... "..... "... data [~]' ..................................................  590 "I'he following 
picture shows some of the processors Coarse Grain Parallel that may be active during a typical APL2 session: 
 One significant feature of the diagram has 120 not been discussed. Notice that the API,2 language processor 
is iisted as one. of the  1001 Session manager tasks that the language processor can use. APL2 Thus 
one thing an APL2 program can do is share a variable with another language processor and assign to this 
variable an  Language Relational data expression to be computed. A subsequent Processor rcfcrencc 
to this same variable givcs the result of the computation (along with some control informat;on). Object 
files On an IBM SPI, for example, you could run an API,2 language processor on each 207 of the up to 
64 physical processors. You Graphics could have a controlling program send computations to each of the 
processors so that ttmy compute in parallel. API,2 con- 210 tains all the mechanisms to synchronize 
System files and direct multiple shared variables. This samc style of computing could be mapped to any 
set of machines supported bv 1002 API,2 that arc on a TCP/IP connected  APL2 Language network.  Processor 
 Thus, while the API,2 system itself does not detect and automatically schedule The communication between 
processors is via shared independent coarse grain parallel tasks, a variables. A shared variable is a 
name which can be knowledgeable user could. accessed by more than one processor. For example, the APt,2 
processor may set the value of a vanabte Dynamic Transmission of programs shared with the database 
processor to a character / string that represents a selection (Ib2): An important feature of AI'L2 is 
that it is interprctivc X+'SELECT * FROM D ~¢HERE CT > 9' and machine independent. There is very little 
infor- mation about machine architecture in an API,2 When the program subsequently references this same 
program, l:or example, there are no distinctions variable, the value has been replace with a matrix between 
imegcrs and real numbers. Because it is representing the resulls of the selection. interpretive, tile 
source program and the executable program are identical. A program can be moved from The Cooperative 
processing interface of the API,2" any supported platform to any other supported plat- system permits 
you to run these diffcrent software form wilho,I the need to compile or link. Programs processors on 
different physical proccssors. I:or example, you could run the Session manager on an can even be passed 
as the value of shared variables. "l'his means thal it is possible to write an application OS/2 system 
to take advantage of the windowed user that takes advantage of parallel processing by making interface, 
run the APL2 language processor on a decisions at run time instead of at program design RISC System 6000 
to take advantage of the speed of time. An application could decide to run a program that machine, and 
run the graphics processor on a on small data locally but ship the program to a Sun workstation so that 
the pictures are displayed on a separate screen. remote machine if more processing power is required 
or if that is cheaper than shipping the data. 591 -: -,-~<~:,?j . .... -  Fine Grain Parallel APL2 
takes advantage of fine grain parallel when running on the ES9000 with Vector Facility which is a SIMD 
machine. In this case, the user does nothing to enable vector processing. The APL2 system itself detects 
the presence of the vectorfacility and the pres- ence of a computation that would benefit from the vector 
facility. Again, because no compiler is ever involvcd, a program written on a machine without a vector 
facility will run faster on a machine with a vector facility with no programmer intervention. ]'his style 
of computing can be extended to vector processing on other machines should they become common. Imbedded 
Parallelism The AI'I,2 language involves parallel operations at a fundamental level. To date, implementations 
have not taken advantage of this inherit parallel content. The arrival of inexpensive parallel machines 
raises the interest in automatically exploiting the parallel content of API,2 programs. Suppose you have 
four large collections of data named B, C, D and E. The following expression is a legal API,2 statement: 
A+B+C÷DxE  Conceptually, each arithmetic operation can'procecd in parallel. In a non parallel architecture, 
the API,2 language processor does the work of mapping these operations to loops if vector computing is 
not avail-able on the machine. APL2 has perfect knowledge of the sizes and types of arguments at execution 
time (recall there is no compilation time) and can choose one of several execution algorithms to guarantee 
optimal performance. Thus, the user gets the best performance while the hardware itself remains invis-ible. 
lJse of parallel hardware would share these same properties. The above example is a clearly superior 
way t(1 write this kind of expression, when compared to some kind of explicit programmer written loop. 
This has been recognized by the creators of the FORTRAN O0 standard which includes many of the array 
operations that API,2 has ahvays had. A second form of parallelism is possible at the expression level. 
Consider this expression: ( A+B )x( C-D ) Each name could represent a megabyte of floating point numbers 
or more. If so, it becomes worth con-sidering the assignment of the two independent expressions (A+B) 
and (C-D) to separate processors. Once the results become available, the multiplication of' the intermediate 
values could be started on a third processor. The feasibility of actu- ally doing a computation like 
this in parallel dcpcnds on whether the the processors on the machine have shared memory or must transmit 
tim array arguments and results on a network or a switch. API,2 has a construction that allows any arbitrary 
program to be applied to items of arrays. Suppose that A, 8 and C are arbitrary arrays of data and that 
F is a user wrilten program. Then the expression: F A B C  uses the each operator (") to request application 
of the program F to each item of the three-item vector - A B C. /\]most by definilion, the above expression 
means; (F.A) (F B) (F C) Each is Al'l,2's iterate control structure. This opera-tion coukl be evahmted 
by three independent processors executing in parallel. Since AH.,2 allows subroutines to be written in 
other languages (like FORTRAN, ASSF, MI"/I,Y, and C), F could be a FOR'I'RAN program and the APl,2 control 
structure "'each" coukt slill be used to apply the program in par- allel to different sets of data. APl,2"s 
parallel control structures arc not limited to /kPl,2 programs. All forms of/\I'I,2 parallelism discussed 
so far arc at the cxpresskm level and can be detected and acth'atcd automatically. The use of shared 
variables has been discussed before as a way to achieve coarse grain par- allel computations, l!sing 
shared variables, you can write a progra:n that simulates a parallel each and receives ahnost all the 
benefits of a real implementa- lion. l)i~cussion of the parallel each program and a listing of the code 
can be found in (Brl). Conclusion Parallel machines have arrh'cd. More than cvcr before, the co~t of 
developing the software to exploit new machh~e architectures is more than the machines themselves. [Jsing 
a programming language that inherinflv conhfins vect<)r and parallel content leads to applications that 
do not need to be rewritten when architectures change.- 59'2., APL2 can take advantage of existing parallel 
machines today. API,2 contains parallel content that coukl easily by identified to provide automatic 
parallelization of applications without any need to rewrite or even recompile. Finally, explicit parallel 
constructions could give the programmer explicit control over parallel execution.  Acknowledgements 
IBM, APL2, OS/2, APL2/6000, AIX/6000, and RISC System/6000 are trademarksof the International Busi- ness 
Machines Corporation. Sun and Solaris arc trademarks of Sun Microsystems, Inc. The author would like 
rccognize the contribution of Manuel Alfonseca to this paper.  References <RefA>(All) Almasi, George S. and 
Gottlieb, Allan, "llighly Parallel Computing", The Beniamin/Cummings Pub- lishing Co.. Inc., Redwood 
City, Ca.. ISBN 0-8053-0177-1, 1989 (Brl) Brown, .lames and Van Der Meulen, Mike, " API, Support of Parallel 
and Ciu:.ter Architectures," Proceedings of S I IA RE Europe, ltamburg, Germany, Aor 1993. (Evl), "F, 
VEREX STEP Dual Processor Desktop Personal Computer," Evercx Systems, Inc., Fremont, Ca. (lbl) , "IBM 
9076 scalable POWER Parallel 1 General Information," IBM Corporation, GI I76-7219, Feb 1993. (Ib2) ,'AI'I,2 
Programming: Using Structurcd Query l,anguage," S!I20-9217. (Ib3) ,"AP1.2 for OS/2: User's Guide," $1121- 
10q I. (lb4) ,"AI'I,2 for Sun Solaris: User's Guide," S1121-1092. (lb5) ,"API 2 for AIX/6000: [Jser's 
(;uidc," SC23-3051</RefA>. $93  
			
