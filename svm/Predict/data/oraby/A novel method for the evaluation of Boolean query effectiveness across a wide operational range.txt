
 A NOVEL METHOD FOR THE EVALUATION OF BOOLEAN QUERY EFFECTIVENESS ACROSS A WIDE OPERATIONAL RANGE Eero 
Sormunen Department of Information Studies, University of Tampere P.O. Box 607, FIN 33101 Tampere, Finland 
Tel. +358-3-2156972 Mail eero.sormunen @ uta.fi ABSTRACT Traditional methods for the system-oriented 
evaluation of Boolean IR systems suffer from validity and reliability problems. Laboratory-based research 
neglects the searcher and studies suboptimal queries. Research on operational systems falls to make a 
distinction between searcher performance and system performance. This approach is neither capable of 
measuring performance at standard points of operation (e.g. across R0.0-RI.0). A new laboratory-based 
evaluation method for Boolean IR systems is proposed. It is based on a controlled formulation of inclusive 
query plans, on an automatic conversion of query plans into elementary queries, and on combining elementary 
queries into optimal queries at standard points of operation. Major results of a large case experiment 
are reported. The validity, reliability, and efficiency of the method are considered in the light of 
empirical and analytical test data. Keywords evaluation (general), structured queries, testing methodology, 
test collections 1. INTRODUCTION The mainstream of the evaluative IR research has followed the Cranfield 
paradigm. The major focus has been on the best match IR models, see e.g. [2, 23]. The low interest in 
studying the Boolean IR model can be seen in the low volume of research output (see e.g. [8] and other 
TREC reports), and also in the slow development of system-oriented evaluation methods for the Boolean 
IR model. Research on operational systems has focused on Boolean IR systems but the contribution on the 
development of methods has been very slight [3, 28]. Research within the Cranfield paradigm has shared 
a very critical attitude towards the Boolean IR model [7]. The studies of Salton [21] and Turtle [30] 
are examples of attempts to show empirically the overall superiority of the best match IR models over 
the Boolean IR model. The results of some recent Permismon to make digital or hard copies of all or part 
of this work for personal or classroom use is granted without fee provtdod that copies are not made or 
distributed for profit or commercial advan- tage and that copies beer this notice and the full citation 
on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires 
pdor specific permmsion and/or a fee. SIGIR 2000 7 00 Athens, Greece &#38;#169; 2000 ACM 1-58113-226-310010007.- 
$ 5.00 comparisons, have suggested that studying the overall superiority of one model over the other 
may be a naive approach [11, 20]. Boolean queries seem to perform better in some situations, and best 
match queries in other situations. It may be more reasonable to focus on studying performance of different 
IR models under changing operational constraints. New methods are needed to draw a more detailed picture 
of query effectiveness in different IR models. 1.1 Methodological Problems in Boolean IR Experiments 
The Boolean IR model has three features that cause methodological problems for experimental research 
[ 13 ]: 1. The formulation of Boolean queries requires a trained person to translate the user request 
into a query. 2. The searcher has very little control over the size of the output produced by a particular 
query. 3. The Boolean IR model does not support ranking of documents in order of decreasing probability 
of relevance.  The necessity to use a human expert in query formulation is a potential source of validity 
and reliability problems. It is very difficult to separate the effects of a technical IR system from 
those of a human searcher. For instance, in the well known STAIRS study, the searchers had a predefined 
goal to locate at least 75 per cent of all relevant documents. It turned out that only less than 20 per 
cent of relevant documents were found. On the other hand, the average precision of the test queries was 
as high as 79 per cent [3]. The searchers were obviously formulating high-precision queries although 
they were asked to work towards high recall. The latter two features (no ranking, little control over 
the output size) of the Boolean IR model cause problems in measuring the performance at the standard 
point of operation (SPO, e.g. at fixed recall levels or document cut-off values). Typically, only one 
query (from an arbitrary operational level) is formulated per search request. Performance is measured 
using single recall/precision values. Recall and precision are averaged separately over all requests. 
As Lancaster has shown [15], the distribution of recall and precision values for a large set of requests 
is very wide. It is very difficult see how the averaged recall and precision values should or could be 
interpreted, since averaging mixes queries from different operational levels. The coordination level 
method developed for the Cranfield 2 project, is a traditional approach to omit the trained searcher 
from the query formulation, to rank output, and to measure the wide range performance of a Boolean system 
[5]. Unfortunately, replacing the cognitive effort of a searcher by a mechanical query term selection 
procedure leads to a Facet A [Information (information retrieval OR retrieval] online systems OR online( 
w )search D AND (tactic ? OR Facet B heuristic? OR [Search process] trial(1 w)error OR expert systems 
OR artificial intelligence OR attitudes/DE OR behavior ?/DE, ID, Tl OR cognitive~de) Figure 1. An example 
of a high recall Oriented query used by Harter [10] to illustrate the facet  based query planning approach. 
 fundamental validity problem. Queries exploit the Boolean IR model in a suboptimal way. 1.2 Harter's 
Idea: the Most Rational Path Harter [10] introduced an idea for an evaluation method based on the notion 
of elementary queries (EQ)) Harter used a single search topic to illustrate how the method could be applied. 
He designed a high recall oriented query plan (see Fig 1). Hatter applied the building block search strategy 
which quite commonly used by professional searchers [6, 9, 12, 16]. The major steps of the building blocks 
strategy are 1) Identify major facets and their logical relationships with one another. 2) Identify query 
terms that represent each facet: words, phrases, etc. 3) Combine the query terms of a facet by disjunction 
(OR operation). 4) Combine the facets by conjunction or negation (AND or ANDNOT operation) [9]. The notion 
of facet is important in query planning. It is a concept that is identified from, and defines one exclusive 
aspect of a search topic. In step 2, a typical goal is to discover all plausible query terms appropriate 
in representing the selected facet. Next, Harter retrieved all documents matching the conjunction of 
facets A and B represented by the disjunction of all selected query terms, and assessed the relevance 
of resulting 371 documents. In addition, all conjunctions of two query terms (called elementary queries) 
from the query plan representing facets A and B in Fig. 1 were composed and executed. A sample from the 
24 elementary queries and the summary of their retrieval results are presented in Table 1. Hatter [10] 
demonstrated the procedure of constructing optimal queries (called the most rational path). An estimate 
for maximum precision across the whole relative recall range was determined by applying a simple incremental 
algorithm: 1. To create the initial optimal query, choose the EQ that achieves the highest precision. 
t Actually Hatter talked about elementary postmgs sets. This is very confusing since tt applies set-based 
terminology to address quenes as logtcal statements. Eq # Elementary queries # of # of Rel Pre-Recall 
Does Does F cision sl information retrieval 0.50 AND tactic? 8 4 0.04 s2 information retrieval AND heuristic? 
17 4 0.24 0.04 s3 information retrieval AND trial(l w)error 2 2 1.00 0.02 s22 online(w)search? AND attitudes/DE 
9 1 .0.11 0.01 s23 online(w)search? AND behavior?/DE,lD,TI 18 10 0.56 0.11 s24 online(w)search? AND cognitive/de 
10 7 0.70 0.08 s25 sl-s24/OR 371 90 0.24 1.00 Table 1. Retrieval results for the 24 elementary queries 
in the case search by Hatter (1990).    1.00 '1 I I I s3 orsl8 I I I 0.80 I s3ors18ors24 n. ,~ I 
I I 0,70 . o,~ g "2 ~gllNÂ¢ s / t,---....___._ 11.411 u ~ i O [] ~ W"-"--O.._.,I 0.30i~ u ~, O.II) .,.2 
L 0.{Xl T , , t , O,Ix) 0.I0 11.211 11,311 0,40 I}.511 0.611 11.7711 0,811 0.90 1.00 Recisll Most rational 
path o Elementary queries [ Figure 2. Recall and precision of the 24 elementary queries and the most 
rational path in the case search presented by Hatter [10]. 2. Create in turn the disjunction of each 
of the remaining EQs with the current optimal query. Select the disjunction with the EQ that maximizes 
precision. The disjunction of the current optimal query and the selected EQ creates a new optimal query. 
 3. Repeat step 2 until all elementary queries have been exhausted.  Precision and recall values for 
the 24 elementary queries and the respective curve for the optimal queries at~ presented in . Harter 
never reported full-scale evaluation results based on the idea of the most rational path except this 
single example. He did neither develop operational guidelines for a fluent use of the method in practice. 
 1.3 Research Goals The main goal of the study was to create an evaluation method for measuring performance 
of Boolean queries across a wide operational range by elaborating the ideas introduced by Harter [10]. 
The method is presented and argned using the framework suggested by Newell M=[domain, procedure, justification} 
[19]: 1. The domain of the method specifies the appropriate application area for the method. 2. The 
procedure of the method consists of the ordered set of operations required in the proper use of the method. 
Especially, two major operations unique to the procedure need to be elaborated: a) Query formulation. 
How the set of elementary queries is composed from a search topic? b) Query optimization. What algorithm 
should he used for combining the elementary queries to find the optimal query for different operational 
levels? 3. The justification of the method. The appropriateness, validity, reliability and efficiency 
of the method within the specified domain must be justified.  The structure of this paper is the following: 
First, some basic concepts and the procedure of the method are introduced. Second, a case experiment 
is briefly reported to illustrate the domain and the use of the proposed method in a concrete experimental 
setting. Third, the other justification issues of the method: validity, reliability and efficiency are 
discussed. Several empirical tests were carried out to assess the potential validity and reliability 
problems in applying the method. 2. OUTLINE FOR THE METHOD The aim of this section is to introduce a 
sound theoretical framework for the procedure of the method and to formulate operanonal guidelines for 
exercising it. 2.1 Query Structures and Query Tuning Spaces IR models address the issue of comparing 
a query as a representation of a request for information with representations of texts. The Boolean IR 
model supports rich query structures, a (simple) binary representation of texts, and an exact match technique 
for comparing queries and text representations [2]. A Boolean query consists of query terms and operators. 
Query terms are usually words, phrases, or other character strings typical of natural language texts. 
The Boolean query structures are based on three logic connectives conjunction (A), disjunction ( v), 
negation ('), and on the use of parentheses. A query expresses the combination of terms that retrieved 
documents have to contain. If we want to generate all possible Boolean queries for a particular request, 
we have to identify all query terms that might be useful, and to generate all logically reasonable query 
structures. Facet, as defined in section 1.2, is a very useful notion in representing relationships between 
Boolean query structures and the search topic. Terms within a facet are naturally combined by disjunctions. 
Facets themselves present the exclusive aspects of desired documents, and are naturally combined by Boolean 
conjunction or negation. [9]. Expert searchers tend to formulate query plans applying the notion of facet 
[9, 16]. Resulting query plans are usually, in a standard form, the conjunctive normal form (CNF) (for 
a formal definition, see [1]). The structure of a Boolean query can be easily characterized in CNF queries: 
Query exhaustivity (Exh) is the number of facets that are exploited. Query extent (QE) characterizes 
the broadness of a query, and can be measured, e.g. as the average number of query terms per facet. 
For instance, in the query plan designed by Harter Exh=2 and QE=5.5 (see Fig. 1). The changes made in 
query exhaustivity and extent to achieve appropriate retrieval goals are called here query tuning. The 
range within which query exhaustivity and query extent can change sets the boundaries for query tuning. 
The set of all elementary queries and their feasible combinations composed at all available exhaustivity 
and extent levels form the query tuning space. In the example by Harter (Fig 1), seven different disjunctions 
of query terms can be generated from facet A (=23-1) and 255 from facet B (=2~-1). The total number of 
possible EQ combinations is then 7 x 255 =1,785 at Exh= 2. In addition, 7 and 255 EQ combinations can 
be formed at Exh=l from facets A and B, respectively. Thus, the total number of EQ combinations creating 
the query tuning space across exhaustivity levels 1 and 2 for the sample query plan is 2,047. 2.2 The 
Procedure of the Method The procedure of the proposed method consists of eight operations at three stages: 
STAGE I. INCLUSIVE QUERY PLANNING 1. Design inclusive query plans. Experienced searchers formulate inclusive 
query plans for each given search topic. It yields a comprehensive representation of the query tuning 
space available for a search topic.  2. Execute extensive queries. The goal of extensive queries is 
to gain reliable recall base estimates. 3. Determine the order of facets. The facet order of inclusive 
query plans is determined by ranking the facets according to their measured recall power, i.e. their 
capability to retrieve relevant documents. STAGE II. QUERY OPTIMISATION 4. Generate the set of elementary 
queries (EQ). Inclusive query plans in the conjunctive normal form (CNF) at different exhaustivlty levels 
are transformed into the disjunctive normal form (DNF) where the elementary conjunctions create the set 
of elementary queries. All elementary queries are executed to find the set of relevant and non-relevant 
documents associated with each EQ. 5. Select standard points of operation (SPO). Both fixed recall levels 
R0.1 ..... R1.0 and fixed document cut-off values, e.g. DCV2, DCV5 ..... DCV500 may be used as SPOs. 
 6. Optimization of queries. An optimisation algorithm is used to compose the combinations of EQs performing 
optimally at each selected SPO.  STAGE III. EVALUATION OF RESULTS 7. Measure precision at each SPO. 
Precision can be used as a performance measure. Precision is averaged over all search topics at each 
SPO. 8. Analyse the characteristics of optimal queries. The optimal queries are analysed to explain the 
changes in the performance of an IR system. The above steps describe the ordered set of operations constituting 
the procedure of the proposed method. Inclusive query planning (steps 1-3) and the search for the optimal 
set of elementary queries (steps 4-6), are in the focus of this study.  2.3 Inclusive Query Planning 
The techniques of query planning are routinely taught to novice searchers [9, 16]. A common feature in 
different query planning techniques is that they emphasize the analysis and identification of searchable 
facets, and the representation of each facet as an exhaustive disjunction of query terms. The goal of 
inclusive query planning is similar, but the thoroughness of identification task is stressed even more. 
In inclusive query planning, the goal is to identify 1. al.._l searchable facets of a search topic, and 
 2. al_! plausible query terms for each facet.  A major doubt in using human experts to design queries 
ts probably associated with the reliability of experimental designs. For instance, the average inter-searcher 
overlap in selection of query terms (measured character-by-character) is usually around 30 per cent [25]. 
Fortunately, the situation is not so bad when facets are considered. For instance, in a study by Iivonen 
[12], the average concept-consistency rose up to 88 per cent, and experienced searchers were even more 
consistent. This indicates that expert searchers are able to identify the facets of a topic consistently 
although the overlap of queries at string level may be low. The identification of all plausible query 
terms for each identified facet is another task requiring searching expertise. Basically, the comprehensiveness 
of facet representations is mostly a question of how much effort are used to tdentify potential query 
terms. The query designer i~ freed from the needs to make compromised query term selections typical of 
practical search situations. The optimization operation will automatically reject "ill-behaving" query 
terms. The process can be improved by appropriate tools (dictionaries, thesauri, browsing tools for database 
indexes, etc.). The final step is to decide the order of facets in the query plan. In the case of a laboratory 
test collection, full relevance data (or at least its justified estimate) is available. The facets of 
an inclusive query plan can be ranked in the descending order of recall. The disjunction of all query 
terms identified for a facet is used to measure recall values.  2.4 Search for the Optimal Set of EQs 
The szze of the query tuning space mcreases exponentmlly as a function of the number of EQs. We are obviously 
facing the risk of combinatorial explosion since we do not know the upper limit of query exhaustivity 
and, especially, query extent in inclusive query plans. Solving the optimization problem by blind search 
algorithms could lead to unmanageably long running times. The search for the optimal set of EQs is a 
NP- hard problem. Harter [10] introduced a simple heuristic algorithm but he did not define it formally. 
Query optimization resembles a traditional integer programming case called the Knapsack Problem. The 
problem is to fill a container with a set of items so that the value of the cargo is maximized, and the 
weight limit for the cargo is not exceeded [4]. The special case where each item is selected once only 
(like EQs), is called the 0-1 Knapsack Problem. Efficient approximation algorithms have been developed 
to find a feasible lower bound for the optimum [ 17]. The problem of finding the optimal query from 
the query tuning space can be formally defined by applying the definitions of the 0-1 Knapsack Problem 
as follows: Select a set of EQs so as to n maximise z = ~ r~a i=1 n subject to ~ nixi _< DCV j ~=1 (1, 
if eqiis selected where xi = i O, otherwise ri = no of relevant documents retrieved by eq, ni = no of 
documents retrieved by eqi and DCVj = selected document cut - off value The above definition of the 
optimization problem is in its maximization version. The number of relevant documents is maximized while 
the total number of retrieved documents is restricted by the given DCV r In the minimization version 
of the problem, the goal is to minimize the total number of documents while requiting that the number 
of relevant documents exceeds some minimum value (a fixed recall level). Unfortunately, standard algorithms 
designed for physical objects would not work properly with EQs. Different EQs tend to overlap and retrieve 
at least some joint documents. This means that, in a disjunction of elementary queries, the "profit" 
r, and the "weight" n, of the elementary query eq, have dynamically changing effective values that depend 
on the EQs selected earlier. The effect of overlap in a combination of several query sets is hard to 
predict. A simple heuristic procedure for an incremental construction of the optimal queries was designed 
applying the notion of efficiency list [17]. The maximization version of the algorithm contains seven 
steps: Remove all elementary queries eqi a) retrieving more documents than the upper limit for the number 
of documents (i.e. n, > residual document cut- off value DCV', starting from DCV' = DCV) or b) retrieving 
no relevant documents (r~=0). 2. Stop, if no elementary queries eq, are available. 3. Calculate the 
efficiency list using precision values r/ni for remaining m elementary queries and sort elementary queries 
in order of descending efficiency. In the case of equal values, use the number of relevant documents 
(r,) retrieved as the second sorting criterion. 4. Move eqx at the top of the efficiency list to the 
optimal query.  5. Remove all documents retrieved by eql from the result sets of remaining elementary 
queries eq2, .... eq,. 6. Calculate the new value for free space DCV'. 7. Continue from step one. 
 The basic algorithm favors narrowly formulated EQs retrieving a few relevant documents with high precision 
at the expense of broader queries retrieving many relevant documents with medium precision. The problem 
can be reduced by running the optimization in an alternative mode differing only in step four of the 
first iteration round: eqi retrieving the largest set of relevant documents is selected from the efficiency 
list instead of eql. The alternative mode is called the largest first optimization and the basic mode 
the precision first optimization. 3. A CASE EXPERIMENT The goal of the case experiment was to elucidate 
the potential uses of the proposed method, to clarify the types of research questions that can be effectively 
solved by the method, and to explicate the operational pragmatics of the method. 3.1 Research Questions 
The case experiment focused on the mechanism of falhng effectiveness of Boolean queries in free-text 
searching of large- full-text databases. The work was inspired by the debate concerning the results of 
the STAIRS study [3, 22]. The goal was to draw a more detailed picture of system performance and optimal 
query structures in search situations typical of large databases. Assuming an ideally performing searcher, 
the main question was: What is the difference in maximum performance of Boolean queries between a small 
database and two types of large databases? The large &#38; dense database contained a larger volume of 
documents than the small database but the density of relevant documents (generality) was the same. In 
the large &#38; sparse database, both the volume of documents was higher and the density of relevant 
documents was lower than in the small database. Twelve hypotheses were formulated concerning effectiveness, 
exhaustivity and proportional query extent of queries in large databases. For details, see [26].  3.2 
Data and Methods 3.2.1 Optimization Algorithm The optimization algorithm described in Section 2.5 was 
programmed in C for Unix. Both a maximization version exploiting a standard set of document cut-off values 
(DCV2, DCVs, .... DCV~x~) and a minimization version exploiting fixed recall levels (Ro~...Rt o) were 
implemented. At each SPO, the iteration round (called optinuzation lap) was executed ten times starting 
each round by selecting a different top EQ from the efficiency list: five laps in the largest first mode, 
and five in the precision first mode. The alternative results at a particular SPO achieved by the algorithm 
in different optimizatmn laps were sorted to find the most optimal queries for further analysis.  3.2.2 
Test Collection The Finnish Full-Text Test Collection developed at the University of Tampere was used 
in the case experiment [14]. The test database contains about 54,000 newspaper articles from three Finnish 
newspapers. A set of 35 search topics are available including verbal topic descriptions and relevance 
assessments. The test database is implemented for the TRIP retrieval systems 2. The test database played 
the role of the large &#38; dense database. Other databases, the small database and the large &#38; sparse 
database, were created through sampling from EQ result sets. The large &#38; sparse database was created 
by deleting about 80 % of the relevant documents, and the small database by deleting about 80 % of all 
documents of the EQ result sets. Thus, the EQ result sets for the small database contained the same relevant 
documents as those for the large &#38; sparse database. Query optimization was done separately on these 
three EQ data sets. 3.2.2.1 Inclusive Query Plans The initial versions of inclusive query plans were 
designed by an experienced search analyst working for three months on the project. Query planning was 
an interactive process based on thorough test queries and on the use of vocabulary sources. Later parallel 
experiments (probabilistic queries) revealed that the initial query plans failed to retrieve some relevant 
documents. These documents were analyzed, and some new query terms were added to represent the facets 
comprehensively. The final inclusive query plans were capable to retrieve 1270 (99,3 %) out of the 1278 
known relevant documents at exhaustivity level one. In total, inclusive query plans contained 134 facets. 
The average exhaustivity of query plans was 3.8 ranging from 2 to 5. The total number of query terms 
identified was 2,330 (67 per query plan and 18 per facet). The number of terms ranged from 23 to 169 
per query plan, and from I to 74 per facet. The wide variation in the number of query terms per facet 
characterizes the difference between specific concepts (e.g. named persons or organizations) and general 
concepts (e.g., domains or processes). 3.2.2.2 Data Collection and Analysis Precision, query exhaustivity 
and query extent data were collected for the optimal queries at SPOs. The sensitivity of results to changes 
in search topic characteristics like the size of a recall base, the number of facets identified, etc. 
were analyzed. Also the searchable expressions referring to query plan facets were identified in all 
relevant documents of a sample of 18 test topics to find explanations for the observed performance differences. 
Statistical tests were applied to all major results. 3.3 Sample Results Figures 3-5 summarize the comparisons 
between the small, large &#38; dense, and large &#38; sparse databases: average precision, exhaustivity 
and proportional extent of optimal queries at recall levels Ro t-R10. 3 The case experiment could reveal 
interesting performance characteristics of Boolean queries in large databases. The average precision 
across Ro i-Rj o was about 13 % lower in the 2 TRIP by TietoEnator, Inc. 3 Proportional query extent 
(PQE) was measured only for high recall and high precision searching because of research economical reasons. 
PQE is the share of query terms actually used of the available terms in inclusive query plans (average 
over facets). 1,00 0,90 0,80 0,70 0 0,60 e Small db [ ~ 0,50 -- 41-- L&#38;d db I a. 0,40 0,30 mL ! 'A 
~ X '~---L&#38;s db J 0,20 0,10 0.00 0,00 0,20 0,40 0,60 0,80 1,00 Recall Figure 3. Average precision 
at fixed recall levels in optimal queries for small, large&#38;dense and large&#38;sparse databases. 
5,0 4,0 $ Smalldb ~"~ '~.~ b --.--L&#38;ddb 2,0 " I "" ~''L&#38;sdb I,LI 1,0 0,0 0,00 0,20 0,40 0,60 
0,80 1,00 Recall Figure 4. Exhaustlvity of high recall queries optlmised for small, large&#38;dense and 
large&#38;sparse databases. 0,8 [ 0,7 = 0,6 --~ 0,5 i ~ 0,4 USmalldb Â¢ ~ 0,3 1L&#38;d db Â¢~ 0,2 [] L&#38;s 
db Â£ o,1 0,0 0,10 0,20 0,30 0,80 0.90 1,00 Recall Figure 5. Proportional query extent (PQE) of optimal 
queries in the small, large&#38;dense, and large&#38;sparse databases. large &#38; dense database (database 
size effect), and about 40 % lower in the large &#38; sparse database (database size + density effect) 
than in the small database (see Fig 3). The average exhaustivity of optimal queries was higher in the 
large databases than in the small one, but the level of precision could not be mamtained. Proportional 
query extent was highest m the large &#38; dense database suggesting that more query terms are needed 
per facet when a larger number of documents have to be retrieved. 18 17 17 16 o~ 14 12 10 ~6 c D-2 0 
1 (18 topics) 2 (18) 3 (17) 4 (12) 5 (5) Query exhaustlvlty Figure 6. The number of search topics where 
full recall can be achieved as a function of query exhaastivtty in the small and large recall bases 
(18 topl~ in total). A very interesting deviation was identified in the precision and exhaustivity curves 
at the highest recall levels. In the large &#38; dense database, the precision and exhaustivity of optimal 
queries fell dramatically between Ro. 9 and Rjo. The results of the facet analysis of all relevant documents 
in a sample of 18 test topics clarified the role of the recall base size in falling effectiveness at 
Rio. The more documents need to be retrieved to achieve full recall, the more there occur relevant documents 
where some query plan facets are expressed implicitly. The results are presented in Fig 6. For Exh=l 
full recall was possible in all but one test topic for both recall bases. At higher exhanstivity levels, 
the number of test topics where full recall is possible fell much faster in the large recall base. Above 
results are just examples from the case study findings to illustrate the potential uses of the proposed 
method. High precision searching was also studied by applying DCVs as standard points of operation. It 
turned out, for instance, that the database size alone does not induce efficiency problems at low DCVs. 
On the contrary, highest precision was achieved in the large &#38; dense database. It was also shown 
that earlier results indicating the superiority of proximity operators over the AND operator in high 
precision searching are invalid. Queries optimized separately for both operators show similar average 
performance. For details, see [26]. 4. JUSTIFICATION OF THE METHOD Evaluation methods should themselves 
be evaluated in regard to appropriateness, validity, reliability, and efficiency [24, 29]. The appropriateness 
of a method was verified in the case study by showing that new results could be gained. Validity, reliability, 
and efficiency are more complex issues to evaluate. The main concerns were directed at the unique operations: 
inclusive query planning and query optimization. 4.1 Facet Selection Test Three subjects having good 
knowledge of text retrieval and indexing were asked to make a facet identification test using a sample 
of 14 test topics. The results showed that the exhaustivity of inclusive query plans used in the case 
experiment were not biased downwards (enough exhaustivity tuning space). The test also verified earlier 
results that the consistency in the selection of query facets is high between search experts.  4.2 Facet 
Representation Test The facet analysis of all relevant documents in the sample of 18 search topics showed 
that the original query designer had missed or neglected about one third of the available expressions 
in the relevant documents. However, the effect of missed query terms was regarded as marginal since their 
occurrences in documents mostly overlapped with other expressions already covered by the query plan. 
The effect was shown to be much smaller than the effect of implicit expressions. In the interactive query 
optimization test (see next section), precision was observed to drop less than 4 %.   4.3 Interactive 
Query Optimization Test The idea of the interactive query optimization test was to replace the automatic 
optimization operation by an expert searcher, and compare the achieved performance levels as well as 
query structures. A special WWW-based tool, the IR Game [27], designed for rapid analysis of query results 
was used in this test. When interfaced to a laboratory test collection, the tool offers immediate performance 
feedback at the level of individual queries in the form of recall-precision curves, and a visualization 
of actual query results. The searcher is able to study, in a convenient and effortless way, the effects 
of query changes. An experienced searcher was recruited to run the interactive query optimization test. 
A group of three control searchers were used to test the overall capability of the test searcher. The 
test searcher was working for a period of 1.5 months trying to find optimal queries for the sample of 
18 test topics for which the full data of facet analysis was available. In practice, the test searcher 
did not face any time constraints. The results showed that the algorithm was performing better than or 
equally with the test searcher in 98 % out of the 198 test cases. This can be regarded as an advantageous 
result for a first version of a heuristic algorithm.  4.4 Efficiency of the Method The investment in 
inclusive query planning was justified to be reasonable in the context of a test collection. It was also 
shown that the growth of running time of the optimization algorithm can be characterized by O(n log n), 
and that it is manageable for all EQ sets of finite size. 5. CONCLUSIONS AND DISCUSSION The main goal 
of this study was to design, demonstrate and evaluate a new evaluation method for measuring the performance 
of Boolean queries across a wide operational range. Three unique characteristics of the method help to 
comprehend its potential: 1. Performance can be measured at any selected pomt across the whole operational 
range, and different standard points of operation (SPO) may be applied. 2. Queries under consideration 
estimate optimal performance at each SPO, and query structures are free to change within the defined 
query tuning space in search of the optimum. 3. The expertise of professional searchers could be brought 
into a system-oriented evaluation framework in a controlled way.  The domain of the method can be characterized 
by illustrating the kinds of research variables that can be appropriately studied by applying the method. 
Query precision, exhaustivity and extent are used as dependent variables, and the standard points of 
operation as the control variable. Independent variables may relate to: 1. documents (e.g. type, length, 
degree of relevance) 2. databases (e.g. size, density) 3. database indexes (e.g. type of indexing, 
linguistic normalization of words) 4. search topics (e.g. complexity, broadness, type) 5. matching 
operations (e.g. different operators).  The proposed method offers clear advantages over traditional 
evaluation methods. It helps to acquire new information about the phenomena observed and challenge present 
findings because it is more accurate (averaging at defined SPOs). The method is also economical in experiments 
where a complex query tuning space is studied. The query tuning space contains all potential candidates 
for optimal queries, but data are collected only on those queries that turn out to be optimal at a particular 
SPO. The proposed method yielded two major innovations: inclusive query planning, and query optimization. 
The former innovation is more universal since it can be used both in Boolean as well as in best match 
experiments, see [14]. The query optimization operation in the proposed form is restricted to the Boolean 
IR model since it presumes that the query results are distinct sets. The inclusive query planning idea 
is easier to exploit since its outcome, the representation of the available query tuning space, can also 
be exploited in experiments on best-match IR systems. Traditional test collections were provided with 
"complete" relevance data. Inclusive query plans are a similar data set that can be used in measuring 
ultimate performance limits of different matching algorithms. Inclusive query plans help also in categorizing 
test topics according to their properties, e.g. complex vs. simple (exhaustivity tuning dimension), and 
broad vs. narrow (extent tuning dimension). This opens a way to create experimental settings that are 
more sensitive to situational factors, the issue that has been raised in the Boolean/best-match comparisons 
[ 11, 20]. 6. ACKNOWLEDGMENTS I am grateful to my supervisor Kalervo J~velin, and to the FIRE group: 
Heikki Keskustalo, Jaana Kekiilainen, and others. 7. REFERENCES <RefA>[1] Arnold, B.H. (1962). Logic and Boolean 
algebra. Eaglewood Cliffs: Prentice-Hall. [2] Belkin, N.J. &#38; Croft, W.B. (1987). Retrieval Techniques. 
In: Williams, M.E., Annual Review of Information Science and Technology 220, 109-145, New York: Elsevier 
&#38; ASIS. [3] Blair, D.C. &#38; Maron, M.E. (1985). An evaluation of retrieval effectiveness for a 
full-text document retrieval system. Comm. of the ACM (28)3, 289-299. [4] Chv~tal, V. (1983). Linear 
Programming. New York: W.H. Freeman. [5] Cleverdon, C.W. (1967). The Cranfield tests on index language 
devices. Aslib Proceedings 19(6), 173-193. [6] Fidel, R. (1991). Searcher's Selection of Search Keys. 
Journal of the American Society for Information Science 42(7), 490-500, 501-514, 515-527. [7] Frants, 
V.I., Shapiro, J., et al. (1999). Boolean Search: Current State and Perspectives. Journal of the American 
Society for Information Science 50(1), 86-95. [8] Harman, D. (1993). The First Text Retrieval Conference 
(TREC-1). Gaithersburg: National Institute of Standards and Technology. (NIST Spec. Publ. 500-207). [9] 
Hatter, S.P. (1986). Online Information retrieval. Orlando: Academic Press. [10] Harter, S.P. (1990). 
Search Term Combinations and Retrieval Overlap: A Proposed Methodology and Case Study. Journal of the 
American Society for Information Science 41(2), 132-146. [11] Hersh, W.R. &#38; Hickam, D.H. (1995). 
An Evaluation of Interactive Boolean and Natural Language Searching with Online Medical Textbook. Journal 
of the American Society for Information Science 48(7), 478-489. [12] Iivonen, M. (1995). Consistency 
in the selection of search concepts and search terms. Information Processing &#38; Management 31 (2), 
173-190. [13] Ingwersen, P. &#38; Willett, P. (1995). An Introduction to Algorithmic and Cognitive Approaches 
for Information Retrieval. Libri 450, 160-177. [14] J~velin, K., Kristensen, J., et al. (1996). A Deductive 
Data Model for Query Expansion. In: Proceedings of the 19th International ACM SIGIR Conference, Ziidch, 
Switzerland, August 18-22, 1996. [15] Lancaster, F.W. (1968). Information Retrieval Systems: Characteristics, 
Testing, and Evaluation. New York: John Wiley. [16] Lancaster, F.W. &#38; Warner, A.J. (1993). Information 
Retrieval Today. Arlington: Information Resources Press. [17] Martello, S. &#38; Toth, P. (1990). Knapsack 
Problems. Algorithms and Computer Implementations. Guildford: John Wiley &#38; Sons. [18] McKinin, E.J., 
Sievert, M.E., et al. (1991). The Medline Full-Text Project. Journal of the American Society for Information 
Science 42(4), 297-307. [19] Newell, A. (1968). Heuristic programming: Ill-structured problems. In: Arofonsky, 
J. (Ed.). Progress in Operations Research, Vol III, 360-414. New York. [20] Paris, L.A.H. &#38; Tibbo, 
H.R. (1998). Freestyle vs. Boolean: A comparison of partial and exact match retrieval systems. Information 
Processing &#38; Management 34(2/3), 175-190. [21] Salton, G. (1972). A new comparison between conventional 
indexing (MEDLARS) and automatic text processing (SMART). Journal of the American Society for Information 
Science 23(March-April), 75-84. [22] Salton, G. (1986). Another look at automatic text-retrieval systems. 
Communications of the ACM 29(7), 648-656. [23] Saiton, G. &#38; McGilI, M.J. (1983). Introduction to 
Modern Information Retrieval. Singapore: McGraw-Hill. [24] Saracevic, T. (1995). Evaluation of evaluation 
in information retrieval. In: Fox, E.A. et al. (Eds.), SIGIR "95 - Proceedings of the 18th Annual International 
ACM SIGIR Conference. Washington July 9- 13, 1995, p. 138- 146. [25] Saracevic, T., Kantor. P. et al. 
(1988). A Study of Information Seeking and Retrieving. Journal of the American Society for Information 
Science 39(3), pp. 161- 176, 177-196, and 197-216. [26] Sormunen, E. (2000). A Method for measuring Wide 
Range Performance of Boolean Queries in Full-Text Databases. Doctoral Thesis. Tampere: University of 
Tampere. Acta Electronica Universitatis Tamperensis, ISBN: 951-44-4732-8, 231 p. URL: http://granum.uta.fi/pdf/951-44-4732-8.pdf. 
[27] Sormunen, E., Laaksonen, J., et al. (1998). The IR Game -A Tool for Rapid Query Analysis in Cross-Language 
IR Experiments. PRICAI '98 Workshop on Cross Language Issues in Artificial Intelligence. Singapore, Nov 
22-24, 1998, p. 22-32. [28] Sparck-Jones, K. (1981). Information retrieval experiment. London: Butterworths. 
[29] Tague-Sutcliffe, J. (1992). The pragmatics of information retrieval experimentation, revisited. 
Information Processing &#38; Management 28(4), 467-49.0. [30] Turtle, H. (1994). Natural Language vs. 
Boolean Query Evaluation: A Comparison of Retrieval Performance. In: Proceedings of the Seventeenth Annual 
International ACM-SIGIR Conference. London: Springer-Verlag. p. 212-220. </RefA>  
			
