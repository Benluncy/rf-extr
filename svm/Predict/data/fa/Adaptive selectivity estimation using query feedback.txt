
 Adaptive Selectivity Estimation Using Query Feedback* Chungmin Melvin Chen Department of Computer Science 
University of Maryland College Park, MD 20742 min(lcs.umd.edu Abstract In this paper, we propose a novel 
approach for estimating the record selectivities of database queries. The real attribute value distribution 
is adaptively approximated by a curve­fitting function using a query feedback mechanism. This approach 
has the advantages of requiring no extra database access overhead for gathering statistics and of being 
able to continuously adapt the value distribution through queries and updates. Experimental results show 
that the estimation accuracy of this approach is comparable to traditional methods based on statistics 
gathering. Introduction In most database systems, the task of query optimiza­tion is to choose an efficient 
execution plan, Best plan selection requires accurate estimates of the costs of al­ternative plans. One 
of the most important factors that affects plan cost is selectivity, which is the number of tuples satisfying 
a given predicate. Therefore, in most cases, the accuracy of selectivity estimates directly af­fects 
the choice of best plan. A study on error propagation [IC91] revealed that selectivity estimation errors 
can increase exponentially with the number of joins and thus affect the decisions in query optimization. 
Accurate selectivity estimation has become even more important in today s systems of much larger database 
sizes, possibly distributed over a LAN or a WAN. In such systems, the query plans are expected to diverge 
much more in cost due to the database size and the volume of data transmission. *This research was partially 
sponsored by the National Science Foundation under grants IRI-9057573 and GDR-85-OO1O8, by NASA/USRA 
under contract 5555-o9, and by the University of Maryland Institute for Advanced Computer Studies (UMIACS). 
Permission to co y without fee all or part of this material is granted prowd J that the copies are not 
made or distributed for direct commercial advantage, the ACM copyright notice and the titie of the publication 
and its date appear, and notice is given that copyin is by permission of the Association of Computing 
Machinery. 7 o copy otherwise, or to republish, requires a fee ancf/or specific permission. Nick Roussopoulos 
Department of Computer Science University of Maryland College Park, MD 20742 nick Clcs.umd.edu Therefore, 
accurate selectivity estimation is even more crucial. The issue of selectivity estimation has attracted 
pop­ular interest, and different methods have been pro­posed [M079, Chr83b, Chr83a, PSC84, KK85, HOT88, 
Lyn88, MD88, LN90, SLRD93, Ioa93]. They can be cat­egorized into four classes: the non-parametric method, 
the parametric method, sampling, and curve fitting. In the following paragraphs, we review the essential 
ap­proaches for each of these four classes. A detailed survey of the first two classes can be found in 
[MCS88]. Non-Parametric Method Methods in this class maintain attribute value distributions using ad 
hoc data structures and algorithms. The most common method is the histogram, which divides an attribute 
domain into intervals and counts the number of tuples holding values which fall into each of the intervals. 
Variations of the histogram method can be found in [M079, PSC84, MD88, Lyn88, Ioa93]. The histogram is 
simple, but tradeoff between the computation/storage overhead and the estimation accuracy must be considered. 
Satisfactory accuracy will not be reached until the domain is divided into a sufficient large number 
of small intervals. In addition to the histogram, a pattern recognition technique was used by [KK85] 
to construct discrete cells of distribution table, and [Lyn88] used a keyterm-oriented approach to keep 
counts of the most frequently queried attribute values. Parametric Method Parametric methods approxi­mate 
the actual distribution with a mathematical distribution function of a certain number of free sta­tistical 
parameter(s) to be estimated (we call such a function a model function). Examples of the model function 
incIude the uniform, normal, Pearson fam­ily and Zipf distributions. In these methods, statis­tics must 
be collected, either by scanning through or by sampling from the relation, in order to esti­mate the 
free parameter(s). These methods usually SiGMOD 94-5194 Minneapolis, Minnesota, USA @ 1994 ACM 0-89791 
-639-5/94/0005...$3.50 require less storage overhead and provide more ac­curate estimation than non-parametric 
methods (if the model function fits the actual distribution). The disadvantage of this method is that 
the shape of the actual distribution must be known a priori in or­der to choose a suitable model function. 
Moreover, when the actual distribution is not shaped like any of the known model functions, any attempt 
to approxi­mate the distribution by this method will be in vain. Contributions to research of parametric 
methods can be found in [S+79, SB83, Fed84, Chr83b, Chr83a]. Curve Fitting In order to overcome the inflexibility 
of the parametric method, [LST83] and [SLRD93] used a general polynomial function and applied the criterion 
of least-square-error to approximate at­tribute value distribution. First, the relation is ex­haustively 
scanned, and the number of occurrences of each attribute value is counted. These numbers are then used 
to compute the coefficients of the optimal polynomial that minimizes the sum of the squares of the estimation 
errors over all distinct at­tribute values. Polynomial approximation has been widely used in data analysis; 
however, care must be taken here to avoid the problem of oscillation (which may lead to negative values) 
and rounding errorl (which may propagate and result in poor estimation when the degree of the polynomial 
is high, say, more than 10). Sampling The sampling method has recently been investigated for estimating 
the resulting sizes of queries. Sample tuples are taken from the relations, and queries are performed 
against these samples to collect the statistics. Sufficient samples must be examined before desired accuracy 
can be achieved. Variations of this method have been proposed in [HOT88, LN90, HS92]. Though the sampling 
method usually gives more accurate estimation than all other methods (suppose sufficient samples are 
taken), it is primarily used in answering statistical queries (such as COUNT (. . .)). In the context 
of query optimization where selectivity estimation is much more frequent, the cost of the sampling method 
is prohibitive and has essentially prevented its practical use. Although accuracy is very important for 
selectivity estimates, the cost of obtaining such estimates must be confined if they are to be cost effective. 
In all the above methods, however, extra 1/0 accesses to the 1The problem caused by rouuding errors is 
usually termed a case of being ill-conditioned. This can always be avoided by representing the approximating 
polynomial with a more numerically stable basis. For example, the Legendre polynomials are used as the 
basis in [LST83]. database are required for the very purpose of collecting statistics. This procedure 
might be expensive and, as suggested, should be done off-line or when the system is light-loaded. In 
a static database where updates are rare, this overhead is acceptable. However, in the presence of updates, 
the procedure must be re-run either periodically or whenever the updates exceed a given threshold. This 
process not only incurs more overhead, but also degrades the query optimizer before the out­dated statistics 
are refreshed. In the following, we present a novel approach which approximates the attribute value distribution 
using query feedbacks and totally avoids the overhead of statistics collection, The idea is to use subsequent 
query feedbacks to regress the distribution gradually, in the hope that as queries proceed, the approximation 
will become more and more accurate. We say that the adap­tive approximation learns from the query executions 
in the sense that it not only remembers and recalls the selectivities of repeating query predicates, 
but can also infer (predict) the selectivities of new query pred­icates. This approach is advantageous 
in the following respects: Efficiency Unlike the previous methods, no off­line database scans or on-line 
sampling are needed to form the value distribution. Also, unlike all the other methods (except sampling 
[Wi191] ), where the statistics collection and computation overhead is proportional to the relation size, 
the overhead of our method has a negligible cost in constant time for each query feedback, regardless 
of the relation size. Adaptation The technique we use here adapts the approximating value dis~ribution 
to queries and updates. None of the previous methods achieve this. They neither take into account query 
information when approximating the value distribution (only relations are scanned), nor continuously 
adjust the distribution to updates (re-computation is invoked only after the updates exceed a threshold). 
The rest of this paper is organized as follows: Section 2 describes the adaptive selectivity estimator 
in detail. Section 3 presents some of our experimental results. Finally, conclusions are given in Section 
4. 2 Adaptive Selectivity Estimation In this section, we describe the implementation of an Adaptive 
Selectivity Estimator (ASE). At the heart of our approach is a technique called recursive least­square-error 
(RLSE), which is adopted to adjust the approximating distribution according to subsequent feedbacks. 
Before exploring the details, we first define some notations used throughout this paper. Let A be an 
attribute of relation R, and let range D = [d~~~, d~az] be the domain of A. In this study, we consider 
only numerical domains (either discrete or continuous).2 Let D be the collection of all sub-ranges of 
D, and define -fA : D -N as the actual distribution of A, i.e., for each sub-range d ~ D, ~~(d) = I{t 
G R : t .A c d} I is the number of tuples in R whose values of attribute A belong to range d. Notice 
that the above notation is well-defined for both discrete and continuous cases. We denote a seiection 
query ul<R,A<h(R), where 1< h, as q = (1, h). The selectivity o? que~y q, defined as s = $A ([1, h]), 
is the number of tuples in the query result. The query feedback from query q is then defined as<= (l, 
h,s). 2.1 Custornizkg RLSE for Query Feedback The goal of our approach is to approximate f~ by an easily 
evaluated function f which is able to self­adjust from subsequent query feedbacks. Thus, given a sequence 
of queries ql, qz, . . .. we can view f as a sequence fo, fl, fz, . . . where ft _ 1 is used to estimate 
the selectivity of q! ~ and, after qi is optimized and executed, fi-1 is further adjusted into fi using 
feedback <~ (which c.oniains the actual selectivity, Sz, of query qt obtained after the execution). We 
use a general form f(r) = ~~=o az~$(z) as the underlying approximating function, where #i(z), i = o , 
. . . . n, are n + 1 pre-chosen functions (called model functions), and a~ are coefficients to be adjusted 
from the query feedbacks. The corresponding cumulative distribution of f(z) is given as ~(x) = ~~=o ai@i(x), 
where @i(~) is the indefinite integral of &#38;(a). Using this form of approximation, the estimated selectivity 
of query q = (/, h), denoted by 3, is computed as: ;= h+l Jf(z)ch = F(h+l)-F(i) = -&#38;[oj(h+l)-o,(l)]. 
1 J=o Now suppose a sequence of query feedbacks <1, . . .,&#38;, where m ~ n, have been collected, A 
reasonable crite­rion for tuning f(z) is to find the optimal coefficients ai that minimize the sum of 
the squares of the estimation errors (thus referred to as /east-square-error (LSE)): The above problem 
can be reformulated in linear algebra form as: minimize 11X*A Y//2, (2) 2 Non-numerical domains can 
be mapped into numericaf ones using certain mapping techniques. The mapping functions should be provided 
by the database creators who know the semantic meaning of the at tribut es. where I[. [12denotes the 
sum of the squares of all elements in the vector, and %(hl + 1) @o(ll) . . . @n(h, +1) %(11) @o(h, + 
1) 00((,) . . . @n(h2 +1) @n(12) x= ... ... ... %(hm + 1) Oo(lm) . . . @n(hm + 1) @n(lm) [I =[!IIA=[N 
3) Let Xt be the transpose of X; the solution to Eq. 2 is obtained as A* = (X X)-l X Y. (4) The above 
computation has one drawback; the space requirement of X and Y increases in proportion to the number 
of query feedbacks m, and each time a new query feedback is added, the whole thing must be re-commted. 
This concern can be relieve~ with some rearrangement of the above computation. Let P = XtX and N = XtY. 
It is not hard to see that P is a n x n matrix and N is a n x 1vector both of whose dimensions are independent 
from the number of feedbacks m. A more careful look into P and N shows that 1=1 ,=1 where Xi is the 
ith row of X, and X: its transpose. Now, let <l, <z, . . ..<~. . . . be a sequence of query feedbacks, 
and A: be the optimal coefficients off(z) corresponding to the first i feedbacks. According to Eqs. 4 
and 5 we have A~=P, l N,, fori=n+l, n+2,.. ., where (6) P, = P%-l +x:x,, N, =N,-l +X~s,, for i= 1,2,...,(7) 
with initial condition P. = No = O. Note that for i ~ n, Pi-1 dose not exist and thus a default distribution 
(e.g., uniformity) must be used temporarily. Later in this context, we will relax this restriction. Also 
notice that by using Eqs. 6 and 7, only two constant size arrays, P and N, need to be maintained. The 
above equations can be further transformed into another form where the expensive matrix inversion Pi-1 
need not be explicitly computed. [You84] derived the following recursive formulas, referred to as Recursive 
Least-Square-Error (RLSE), from Eqs. 6 and 7: A; = A;_l G, X;(X, A;_l s,), (8) G, = G,_l (G,-IX;) 
(1+ X, G,-l X;)- (X, G,-,), (9) for i = 1,2, ..., while A. and Go can be of any arbitrary values. In 
this expression, no explicit matrix inverse operation is needed, and only an n x n matrix G (called a 
gam matriz) needs to be maintained (actually, G = P-l). The computation complexity is in the ai. Experiments 
with different values of ~, are given in order of 0(n2 ). Since n is a pre-chosen small integer, the 
computation overhead per query feedback is small and is considered constant, regardless of the relation 
size. The initial values Go and A. may affect the convergence rate of A; and, thus, the rate at which 
ji converges to ~A. We describe later in this section how to initialize Go and A. with appropriate values. 
It is interesting to see that the computation of At resembles the technique of siochasttc approximation 
[AG67], in the sense that A: is adjusted from A~_ ~ by subtracting a correction term which is the product 
of the estimation error (X~A~_l si) and the gain value G~X~. Because of their simplicity and efficiency 
in both space requirement and computation, Eqs. 8 and 9 were adopted in the ASE. Accommodating Update 
Adaptiveness (with Weighted LSE) The RLSE can be further generalized to accommodate adaptability to updates. 
We accomplish this by associating different wetghts with the query feedbacks so that the outdated feedbacks 
can be suppressed by assigning smaller weights to them. In Eq. 1, we now associate an importance weight 
pi to the estimation error of the ith query, and a fading weight ~i to the estimation errors of all the 
preceding queries. That is, instead of minimizing Eq. 1, we now want to minimize: mm (lo) The recursive 
solution to the above is similar to Eqs. 8 and 9 (derivation details can be found in [CR94]): G, = (:) 
G,_, ­ (:) ( Gt-lX:)(a; +P:XtGt-lX:)- ( XtGt_1)(12) fori=l,2, ... Intuitively, ~~s determine the impor­tance 
of individual feedbacks; ~is determine the for­getting rate of previous feedbacks. Note that Eqs. 8 and 
9 offer a special case of Eqs. 11 and 12 with ~i = /3i = 1, for all i. Apparently, different weights 
af­fect the adaptation behavior of the approximating func­t ion. As an innovation, we consider only fixed-value 
weights. We set /?i = ai = 1 for all i, except that al is assigned another positive number less than 
1 if <, is the first feedback after update. The smaller the ai, the more the knowledge from previous 
feedbacks is to be forgotten. Note that we cannot set ~i = O, because it appears as a denominator in 
Eq. 12. Nonetheless, the same effect (of discarding all previous knowledge) can be achieved by assigning 
an extremely small number to the next section. Initializing A. and Go The initial values of Go and A. 
must be determined before the recursive formulas in Eqs. 11 and 12 can be used. Theoretically, arbitrary 
initial values can be used for Go and A. [You84], though they differ greatly in convergence rates. To 
speed up convergence, we compute A; and Go(= PO I) using Eqs. 4 and 5 by substituting the following (71+ 
1) manual feedbacks into Eq. 3: l,=h, =d~,n+-*A, s,=~, forz=l... n (13) where A = dma$ dm,n, lR\ denotes 
the number of tuples in relation R. The intention here is to force ASE to begin with a uniform distribution 
(enforced by Eq. 13), and to keep knowledge of the relation cardinality in the gain matrix (enforced 
by Eq. 14). Choosing the Model Functions The remaining problem now is to choose the model func­tions 
~i (~). The polynomial function is a good candi­date due to its generality and simplicity and has been 
used in [LST83] and [S LRD93]. We adopted polynomi­als of degree 6 throughout our experiments, i.e. ~ 
the ap­proximating function is of the form f(z) = ~i=o a,%i. Whereas polynomials of higher degrees have 
the pot­ential problem of being ill-conditioned, polynomials of lower degrees might not be flexible enough 
to fit the variety of actual distributions. Therefore, our choice of degree 6 is a compromise between 
these concerns 3. Another interesting class of functions is the splme func­tions [dB78], which are piecewise 
polynomial functions. Splines have many advantages over polynomials in the aspects of adaptability and 
numerical stability. How­ever, they are more complex in computation and partic­ularly in representation. 
We are currently investigating this approach and will not discuss it here. A practical problem of polynomials 
is the negative values which are undesired in distribution approxima­tions. This poses no problem so 
long as the negative values occur only outside the attribute domain, or so long as the resulting estimated 
selectivity of the query of interest is still positive (even if some negative values do occur within 
the domain). If a negative selectivity is ever estimated for a query, we simply use zero instead (and 
note that if the error is large, it will be tuned 3In our experiments using degree 6, the ill-conditioned 
problem did not arise. However, for higher degrees we might need to use another basis (such as Legeudre 
polynomials or B-splines) since the basis of z , i = I , . . . . n is in general ill-conditioned for 
large values of n. through feedback). Finally, we summarize ASE in the following description. Variables 
A: the (adaptable) coefficients of a polynomial f of degree 6; let F be the indefinite integral of ~; 
G: the gain matrix; Initialization Use the manual feedbacks listed in Eqs. 13 and 14 to compute the initial 
values for A and G from Eqs. 3, 4 and 5. Selectivity Estimation The selectivity of query q, = (L, h, 
) is estimated as I (h, + 1) F(Z, ); if it is negative, simply return o. Feedback and Adaptation After 
the execution of g,, get feedback ~, = (L, h,, s,) where s, is the actual selectivity of q, obtained 
from execution. If q, is the first query after the latest update, set the fading weight a, to a positive 
number less than 1. Use <, to adjust A and G, as shown in Eqs. 11 and 12. Comparison with [SLRD93] Sun, 
Ling, Rishe, and Deng proposed in [SLRD93] a method of approximating the attribute distribution using 
a polynomial with the criterion of least-square­error. While both their method and ours use polynomial 
approximations, there are several differences between the two methods. First, their approach is static 
in the sense it is necessary to scan the database and count the frequencies of distinct attribute values, 
and, once computed, the approximating distribution remains unchanged until the next re-computation. Our 
method is dynamzc and depends only on query feedbacks, with no access to the database. For a relation 
which is large and/or is updated regularly, the overhead of collecting or refreshing the statistics can 
be very expensive. Our approach totally avoids such overhead. Besides, in an environment where queries 
exhibit highly temporal or spatial locality on certain attribute ranges, ASE S dynamic adaptation to 
queries will perhaps be of greater benefit. Finally, ASE S adaptiveness to updates not only eliminates 
the overhead of statistics re-collection, but also provides a more graceful performance degradation for 
selectivity estimations through a query session interleaved with updates. An Example We use an example 
to demonstrate how the ASE works by using successive query feedbacks to approximate the data distribution. 
The experimental data is from a queries 1 ~ 3 4 [1,, h,] [1935)1966] [1925,1950] [1904,1939] [1890,1923] 
a , 1073 1138 1248 567 s, 1872 1399 890 136 5 6 7 8 9 [1908,1913] [1948,1989] [1957,1980] [1964,1989] 
[1916,1981] 2 1956 1103 1041 3173 14 2033 1130 1134 3045  I;,&#38; \ll ;9: ,&#38;. , l:ly. 1960 1980 
1923 1940 1960 1980 (3) aftsi 3 query feedbacks (4) .Itef 9 query fesdbadw 104 80 :~1  so ..... . .. 
,,, .,. 40 ... .. .. fc? . al ., .. . .. . .. b 19C4 1920 1940 f960 1980 Figure 1: Adaptation Dynamics 
of ASE -an Example movie database, courtesy of Dr. Wiederhold of Stanford University, which records 
3424 movies produced during the years 1890 1989. Figure 1 snapshots the evolution of the approximating 
distribution for a sequence of query feedbacks. The queries are listed in the table, where [ii, hi] denotes 
the selected range of the ith query, $ denotes the selectivity estimated (by ASE) before the query execution, 
and st denotes the actual selectivity obtained after the query execution. In each frame, the curve of 
the approximating distribution fi, drawn in solid line, is compared to the real distribution, drawn in 
discrete points. In frame 1, uniform distribution is assumed at the very beginning, as no queries have 
been issued. Note that knowledge of the relation cardinality (3424 tuples) has been implicitly enforced 
in the initial approximating distribution jo, using the initialization scheme explained in the previous 
section. After the execution of two queries, as shown in frame 2, the approximating curve becomes closer 
to the actual distribution. However, jz is relatively inaccurate for attribute ranges outside [1925, 
1966] which have not been queried yet (and, thus, no distribution information is yet known). The third 
query and its feedback C3 = (1901, 1939, 890) tunes fz into f3 with better accuracy for range [1904, 
1939]. It is worth mentioning that at the same time, ~3 improves the distribution of years greater than 
1966, though no queries against this range have ever been posed. This is attributed to ASE S ability 
to infer and properly shape the unknown ranges using knowledge about the relation cardinality and distribution 
information obtained from queries on other attribute ranges, Subsequently, frame 4 shows the curve after 
nine query feedbacks, by which time the approximation has become even closer to the real distribution. 
3 Experimental Results A comprehensive set of experiments was performed to evaluate the ASE. We ran the 
experiments using the mathematics package MAPLE, developed by the Symbolic Computation Group of the University 
of Waterloo; MAPLE was chosen for its provision of immediate access to matrix operations and random number 
generators. We experimented also with the method proposed in [SLRD93] (referred to as SLR in what follows) 
for comparisons whenever appropriate. The selectivity estimation errors and the adaptation dynamics of 
ASE were observed and graphed for demonstration. However, to interpret and compare the estimation errors 
correctly, both absolute error and re~ative error are presented; they are calculated as: 1;-SI~loo abs. 
err. = ~ x100, rlt. err. = , s where ; and s are the estimated and actual query result sizes, respectively; 
IRI is the cardinality of the queried relation. Our reason for using both is that neither one alone can 
provide evidence of good or poor estimation in all cases. For example, a 200 %0 relative error for a 
query of selectivity of 1 tuple by no means represents a poor estimate; in fact, it is the stringent 
selectivity (of 1 tuple) that causes such an exaggerated relative error. It must be pointed out that 
we do not compare the computation overhead since our method, which costs only negligible CPU time for 
query feedback computation, is definitely superior to all other methods which require extra database 
accesses (either off-line or on-line) for statistics gathering or sampling. Both real and synthetic data 
were used in the exper­iments. The use of real data validates the usefulness of our method in practice 
(as has been demonstrated in the example); the use of synthetic data allows systematic evaluation of 
ASE under diverse data and query distri­butions. Throughout the experimentation, only selec­tion queries 
were considered. Each query is represented as a range [x 6/2, z + 6/2], where d~in < z < dmaz, O < 6 
< dmaz dm,n. In this paper, we report only results from those experiments where x and 6 are gen­erated 
randomly from their respective domains using Notattons Meaning N(P> a) normal distribution with mean 
K, standard ch]-square distribution w]th n degrees of freedom F distribution with m and n degrees of 
freedom for numerator and denominator, respectively a hi-modal distribution which is an overlap of ~ 
Table 1: Distribution Notations ~ Table 2: Customized Experiment Parameters average error of 1st -50th 
queries ASE SLR I abs. err. I rlt err I abs. err. I rlt err average error of 10th -50th queries ASN SLR 
abs err rlt err. abs err rlt. err. N O 16 366 016 273 0.33 836 040 8.93 F 110 153 176 301 B o so 511 
0.60 3.13 X2 Table 3: Average Errors in Various Data Distributions a random number generator. Experimental 
results re­garding the impacts of different distributions of x and 6 on the convergence rate of ASE are 
prepared in a more detailed version of this paper. Three sets of experimental results are presented here. 
The first set shows the adaptability of ASE to various data distributions. The second set shows how ASE 
adapts to query locahty, in the sense that it provides more accurate selectivity estimates for the attribute 
sub-ranges which are queried most. In the last set, we demonstrate ASE S elegant adaptation through database 
updates which require no overhead for database re-scan and statistics re-computation. 3.1 Adaptation 
to Various Distributions To observe ASE S adaptability to various data distribu­tions, synthetic data 
generated from each of the follow­ing four customized distributions were tested: normal distribution, 
chi-square distribution, the F distribution, and a hi-modal distribution.4 The notations and cus­tomized 
parameters of each distribution are described in Tables 1 and 2. For each data distribution, three ran­dom 
query streams (each of which contains 50 queries) 4 We do not present the results of uniform distribution 
since the ASE assumes uniform distribution from the very beginning. were run for both ASE and SLR. Table 
3 lists the average error per query of ASE and SLR under each data distribution. In order to achieve 
a fair comparison between ASE and SLR, the average errors, which exclude the first 10 queries of each 
query stream (during which ASE is still in its learning stage), are also calculated for comparison. The 
first table shows that ASE is slightly inferior to SLR in estimation accuracy; however, the second table 
shows that after ASE converges (after 10 queries), its accuracy is very comparable to that of SLR. Figures 
2 through 5 depict the corresponding dynamics of ASE and SLR for one of the query streams under each 
data set. In the figures to the left marked (a), curve g corresponds to the approximating distribution 
computed from SLR; curve ~$ denotes the adaptive approximating distribution of ASE after i query feedbacks. 
Figures (b) compare the estimation errors of ASE and SLR by plotting them along with the query streams. 
The adaptiveness of ASE can be clearly observed from the decreasing trend of errors as queries proceed. 
The occasionally high relative errors of ASE are either caused by stringently small selectivities (as 
evidenced by the high relative errors of SLR for the same queries), or are indications of the moments 
where feedbacks take place for the first time on the queried ranges. However, as can be seen from all 
the figures, after sufficient query feedbacks have covered the whole attribute domain, ASE converges 
the approximating distribution to a stable curve and provides estimations with constantly small errors. 
3.2 Adaptiveness of ASE to Query Locality No matter what method is used to estimate the data distribution, 
the computation capacity of the method is always limited (e.g., the number of intervals in a histogram, 
the degree of a polynomial). It is not uncommon for the distribution to be approximated to be too detailed 
to be modeled by the limited capacity. Therefore, we believe that instead of approximating the overall 
distribution evenly, the limited capacity should be used to approximate more accurately the local distribution 
of a rather narrow attribute sub­range which imposes either a temporal or spatial query locality, ASE 
inherits this merit: the more query feedbacks obtained from a local area, the more accurate the resulting 
approximating distribution for this area. An event database which contains 431,258 records of events 
during 1948-1978 was used in this experiment. Three levels of query localities, aa outlined in Table 
4, were designed to compare ASE and SLR. For each level of locality, three random query streams (each 
of which contains 50 queries) were tested for both ASE and SLR. Table 5 summarizes the average errors 
for the 10th to 50th queries (we excluded the first 10 queries during Queried Range Local]ty LowQL Jan. 
1948 Dec 1978 Low MedQL Jan 1948 June 1960 Medial HighQL Jan. 1948 Jan. 1953 High Table 4: Three 
Levels of Query Localities ASE SLR abs. err. rlt. err. abs err rlt. err LowQL 11 5.6 0.93 50 MedQL 033 
6.3 0.66 10.6 HlghQL 0.086 12.8 014 21.3 Table 5: Average Errors in Different Q.uerv Localities which 
ASE has not yet converged). The curves of the approximating functions and the estimation errors of ASE 
and SLR are graphed for comparison, according to the three levels of localities, in Figures 6 , 7, and 
8. It can be seen both from the tables and figures that ASE and SLR behave almost the same for low locality, 
but that as locality increases, ASE turns out to be better. This is because ASE is computed dynamically 
according to the query feedbacks and thus implicitly takes into account the query locality; in contrast, 
SLR is statically computed from the underlying data. 3.3 Adaptiveness to Updates (ASE Performance under 
Updates) In this section, we show the elegant adaptation of ASE to updates. The normal distribution data 
from Sec­tion 3.1 is used again. Table 6 briefs the character­istics of three different update workloads 
to be inter­leaved with the query streams (more details about the update workloads are given in the appendix). 
Orthog­onal to the update loads are three versions of ASE, namely, ASEO 01, ASEO.1, and ASE0,5, with 
different fading weights (as indicated in the subscripts). For each update workload, three random query 
streams (each of which contains 40 selection queries interleaved with up­dates) are generated, and each 
of them is tested with all three fading weights. Table 7 tabulates the average errors; Figures 9, 10, 
and 11 correspond to the adap­tation dynamics of ASE in the three different update loads. The corresponding 
curves for the three fading weights are grouped and graphed in each figure. It can be seen from the figures 
that ASE adapts ele­ gantly to all update loads. For example, in Figure 9.b, the errors go up over a 
few queries after the 10th query where update occursj an d then decline back to a sta­ ble low level. 
This adaptation can also be observed in Figure 9a, where frames 2 through 4 show the adap­tation of the 
approximating curves to the local distri­bution change at interval [ 50, 250]. It is interesting to note 
from Table 7 that ASEO, O1, ASEO.1, and ASE0,5 are respectively the best in update loads LOAD 1, LOAD3, 
 update occurrences no. of total tuples updated change of distribution shape update transition LOAD1 
at 11 4,500 local, blg Increase In batch LOAD2 at 11, 17, 23, 29 9,000 global, slightly increase gradual 
LOAD3 at 11, 17, 23, 29 9,000 global, drastic gradual Table 6: Characteristics of Three Update Workloads 
Update ASEm=o.oI ASE.=0 I ASE.=O s Workload abs. err. rlt. err abs err rlt. err. abs. err. rlt err. LOAD1 
3.38 16.7 3.58 25.7 4.71 30.0 LOAD2 3.35 22.2 2.66 17.2 259 ~ 15.9 LOAD3 5.58 31.0 4.19 21.3 424 21.6 
 Table 7: Ave. Errors in Different UDdate Workloads and LOAD2. This is no surprise since in LOAD1, a 
vast amount of update is done at once and thus it is advan­tageous to forget previous feedbacks and rely 
mainly on new ones. Therefore, the smallest fading weight ASEO,O1 (which forgets previous feedbacks to 
the greatest extent) outperforms the other two in this case. Similarly, in LOAD2, the shape of the distribution 
does not change too much during successive updates, and thus ASEO s benefits the most by using old knowledge 
during transi­tion. Finally, in LOAD3, where the distribution shape changes greatly through gradual updates, 
the use of ASEO, 1 offers a compromise between the two extremes. Conclusions In this paper, we have presented 
a new approach for se­lectivity estimation. Capitalizing on the technique of re­cursive weighted least-square-error, 
we devised an adap­tive selectivity estimator which uses query feedbacks to approximate the actual attribute 
distribution and to provide efficient and accurate estimations, The most significant advantage of this 
approach over traditional methods is that it incurs no extra cost for gathering database statistics. 
Furthermore, it adapts better to updates and query localities. We hope this study will inspire a new 
direction for data knowledge acquisition, especially in systems where statistics gathering is cost prohibitive 
because of large data sizes (such as tertiary databases). The adaptive selectivity estimator can be further 
improved in several ways and explored in several directions. First, we will refine the feedback mechanism 
so that adaptation will stop after the approximating distribution converges and will be triggered after 
updates. We would also like to extend this work to complex queries which involve compound predicates 
or joins. Lastly, mathematical analysis of ASE is desired in order to give deeper insight into its performance 
behavior under diverse query distributions and into its theoretical limits. Acknowledgements The authors 
would like to thank Dr. Gio Wiederhold for providing us with a copy of the movie database. Special thanks 
go also to the anonymous referees for their valuable comments which have helped us improve the quality 
and readability of this paper. References <RefA>[AG67] <SinRef><author>A.E. Albert </author>and <author>L.A. Gardner</author>. <title>Stochastic Approximation 
and Nonlinear Regression</title>. <publisher>M.I.T. Press</publisher>, <location>Cambridge, Massachusetts</location>, <date>1967</date></SinRef>. [Chr83a] <SinRef><author>S. Christodoulakis</author>. 
<title>Estimating block trans­fers and join sizes</title>. In <booktitle>Proceeding of 1983 ACM-SIGMOD Intl. Conf. on Management 
of Data</booktitle>, pages <pages>40 54</pages>, <location>New York</location>, <date>1983</date></SinRef>. [Chr83b] <SinRef><author>S. Christodoulakis</author>. <title>Estimating record selec­tivities. 
lnf. Syst</title>., <volume>8(2):</volume><pages>105-115</pages>, <date>1983</date></SinRef>. [CR94] <SinRef><author>C.M. Chen </author>and <author>N. Roussopoulos</author>. <title>Adaptive selectivity estimation 
using query feedback I: polynomial approach</title>. <tech>Technical Report CS­TR-3197</tech>, <tech>Dept. of Comp. Sci., University 
of Maryland</tech>, <location>College Park</location>, <date>1994</date></SinRef>. [dB78] <SinRef><author>C. de Boor</author>. <title>A practzcal guzde to splines</title>. <publisher>Springer-Verlag</publisher>, <location>New 
York</location>, <date>1978</date></SinRef>. [Fed84] <SinRef><author>J. Fedorowicz</author>. <title>Database evaluation using multiple regression techniques</title>. In <booktitle>Proceed­ings 
of the A CM-SIGMOD Inil, Conf. on Management of Data</booktitle>, pages <pages>70 76</pages>, <location>Boston, MA</location>, <date>1984</date>. </SinRef>[HOT88] <SinRef><author>W. Hou</author>, 
<author>G. Ozsoyoglu</author>, and <author>B. K. Taneja</author>. <title>Statistical estimators for relational algebra expressions</title>. In <booktitle>Proceedings 
of the ACM SIGA CT-SIGMOD Sympostum on Princi­ples of Database Systems</booktitle>, pages <pages>276-287</pages>, <date>1988</date></SinRef>. [HS92] 
[IC91] [Ioa93] [KK85] [LN90] [LST83] [Lyr188] [MCS88] [MD88] [M079] [PSC84] <SinRef><author>P. Haas </author>and <author>A. Swami</author>. <title>Sequential 
sampling procedures for query size estimation</title>. In <booktitle>Pro­ceedings of the A CM-SIGMOD Int 1. Conf. on Management 
of Data</booktitle>, pages <pages>341 350</pages>, <location>San Diego, CA</location>, <date>1992</date></SinRef>. <SinRef><author>Y.E. Ioannidid </author>and <author>S. Christodoulakis</author>. <title>On the propagation 
of errors in the size of join results</title>. In <booktitle>Proceedings of the A CM-SIGMOD Intl. Conf. on Management of 
Datal</booktitle> pages <pages>268-277</pages>, <location>Denver, Colorado</location>, <date>1991</date></SinRef>. <SinRef><author>Y.E. Ioannidis</author>. <title>Universality of serial his­tograms</title>. In 
<booktitle>Proceedings of the l~th VLDB Conference</booktitle>, <location>Dublin, Ireland</location>, <date>1993</date></SinRef>. <SinRef><author>N. Kamel </author>and <author>R. King</author>. <title>A method of data 
distribution based on texture analysis</title>. In <booktitle>Proceedings of the A CM-SIGMOD Intl. Conf. on Management of 
Data</booktitle>, pages <pages>319­325</pages>, <location>Austin, Texas</location>, <date>1985</date></SinRef>. <SinRef><author>R. J. Lipton </author>and <author>J. F. Naughton</author>. <title>Practical selectivity estimation 
through adaptive sam­pling</title>. In <booktitle>Proceedings of the ACM SIGMOD Inil. Conj on Management of Data</booktitle>, pages 
<pages>1-11</pages>, <location>Atlantic City, NJ</location>, <date>1990</date></SinRef>. <SinRef><author>E. Lefons</author>,<author> A. Silvestri</author>, and <author>F. Tangorra</author>. <title>An analytic approach to statistical 
databases</title>. In <booktitle>Proceedings of the 9th VLDB Conference</booktitle>, <date>1983</date></SinRef>. <SinRef><author>C. A. Lynch</author>. <title>Selectivity estimation and 
 query optimization in large databases with highly skewed distributions of column values</title>. In<booktitle> Proceedings 
of the l~th VLDB Conference</booktitle>, pages <pages>240-251</pages>,<location> Los Angeles, CA, </location><date>1988</date></SinRef>. <SinRef><author>M.V. Mannino</author>, <author>P. Chu</author>, and <author>T. Sager</author>. 
<title>Statis­tical profile estimation in database systems</title>. <journal>ACM Computing Surveys</journal>, <volume>20(3), </volume><date>1988</date></SinRef>. <SinRef><author>M. Muralikrishma </author>
and <author>D. DeWitt</author>. <title>Equi­depth histograms for estimating selectivity factors for multi-dimensional queries</title>. 
In <booktitle>Proceedings of the A CM-SIGMOD Conf. on Management of Data</booktitle>, pages <pages>28 36</pages>, <location>Chicago, Illinois</location>, <date>1988</date>.</SinRef> 
 <SinRef><author>T.H. Merrett </author>and <author>E. Otoo</author>. <title>Distribution models of relations</title>. In <booktitle>Proceedings of the 5th VLDB Conference</booktitle>, 
pages <pages>418-425</pages>, <location>Rio De Janero, Brazil</location>, <date>1979</date></SinRef>. <SinRef><author>G. Piatetsky-Shapiro </author>and <author>C. Connell</author>. <title>Ac­curate estimation 
of the number of tuples satisfying a condition</title>. In <booktitle>Proceedings of the ACM-SIGMOD Intl. Conf. on Management 
of Data</booktitle>, pages <pages>256 275</pages>, <location>Boston, MA</location>, <date>1984</date></SinRef>.  [s+79] <SinRef><author>P. G. Selinger </author><author>et al. </author><title>Access path selection in a relational 
database management system</title>. In <booktitle>Procs. of A CM-SIGMOD</booktitle>, pages <pages>23-34</pages>, <date>1979</date></SinRef>. [SB83] <SinRef><author>W. Samson </author>and <author>A. Bendell</author>. 
<title>Rank order distributions and secondary key indexing</title>. In <booktitle>Proceedings of the 2nd Intl. Conf. on Databases</booktitle>, 
<location>Cambridge, England</location>, <date>1983</date></SinRef>. [SLRD93] <SinRef><author>W. Sun</author>, <author>Y. Ling</author>, <author>N. Rishe</author>, and <author>Y. Deng</author>. <title>An instant and accurate size 
estimation method for joins and selection in a retrieval-intensive environment</title>. In <booktitle>Proceedings of the 
ACM-SIGMOD Intl. Con~ on Management of Data</booktitle>, pages <pages>79-88</pages>, <location>Washington, DC</location>, <date>1993</date>.</SinRef> [Wi191] <SinRef><author>D.E. Willard</author>. 
<title>Optimal sample cost residues for differential database batch query prob­lems</title>. <journal>Journal of the ACM</journal>, <volume>38(1):</volume><pages>104-119</pages>, 
<date>1991</date></SinRef>. [You84] <SinRef><author>P. Young</author>. <title>Recursive estimation and time­series analysis</title>. <publisher>Springer-Verlag</publisher>, <location>New York</location>, <date>1984</date></SinRef>.</RefA> 
 Appendix Specifications of Update Workloads In our experiments, an update query is simulated by its 
effect on the value distribution of the attribute of interest. An update query is specified by five parameters: 
(i, fV, D, [rein, rnaz], problNs), where i means this update takes place immediately before the ith 
query in the query stream. N is the number of tuples updated (either inserted or deleted). Each tuple 
s attribute value is randomly generted from range [rein, maz] according to a distribution D. A tuple 
is inserted with probability problNs or deleted with probability y 1 problNs. Three different update 
workloads are tested, each of which is interleaved with another 40 random selection queries. The three 
update workloads are specified in the following, with i7(z, y) denotes the uniform distribution among 
range [z, y], iV(~, a) the normal distribution with mean p and standard deviation u. LOAD1: (11, 4500, 
U( 50, 250), [ 50, 250], 1.0) LOAD2: (11, 2250, U( 150, 550), [ 150, 550], 0.75), (17, 2250, U(-150, 
550), [-150, 550], 0.75), (23, 2250, U(-150, 550), [-150, 550], 0.75), (29, 2250, U(-150, 550), [-150, 
550], 0.75). LOAD3: (11, 3000, N( 63, 50), [ 150,25],0.9), (17,1500, N(112,40), [25,200],0.1), (23, 2250, 
N(290, 60), [200, 375], 1.0), (29, 2250, N(455, 50), [375, 550], 0.4). (a) approximating distribution 
curves 35 30 ~25 * . :20 g,5 : 10 5 0 -100 0 100 200 300 400 Figure (a) approximating distribution curves 
70 60 ,: .... ... .,. 50 ­$ ,. z ,? Ob o 200 400 600 800 arlibute values Figure 3: (a) approximating 
d!strbution curves atbibute values Figure [a) atmroximatina,,, distribution curvas 60 50 - (b) selectivity 
e$timstion errors [c o 10 20304050 queries 100, , I I   ~ :k 500 o 10 20304050 queries 2: Normal 
Distribution (b) selecwty aatimstion errors 2030 4050 queries IL10 150 T ~ ~ 100 ? G .: 50­g PY 1000o 
1020304050 Chi-Square Distribution fb) selactivitv stimation errors 1P o 1020304050 queries 4: the F 
Distribution (b) selectivity estimation errors 20, . .. .. [L o 1020304050 queries Figure 5: a hi-modal 
Distribution (a) approximating distribution curves 3 ~ z * al 3~~ % m I z~ i 4000 ­ .. E 2 c 3000 -.,. 
... ... . . .. ~ 2000 ­ . ... 1000;. J o~ 1950 1955 1960 1965 1970 1975 attribute values (monthfyear) 
 Figure 6: Adaptation in LowQL (a) approximating distribution curves 3000 g ,-1. 0 2500-k al $J = 032000 
-; 0. ~ u! ~ ~ 1500 -. g .. .. E 40 3 .. :. ~ . ~ 1!46 1950 1952 1954 1956 1958 1960 attribute values 
(monthfyear) Figure 7: Adaptation in MedQL (a) approximating distribution cuwes 900 800 -. 700­ 500 
... . 400-8 . 300 ... . . . 200­ ;~1 1949 1950 1951 1952 1953 attribute values (month/year) Figure 8: 
Adaptation in HighQL (b) selectivity estimation errors queries (Low queries Query Locality) (b) selectivity 
estimation errors * (Medial queries ., f 35 Query Locality) (b) selectivity estimation ,, 40 errors 45 
, 50 queries (High Query Locality) 172 (1) afkr (a) Animation of DWibution Curves 9 queries, O update 
(2) aner 13 warms, 1 ur3date :m (3) after 20 queries, 1 update (4) aiter 35 queries, 1 update ~ ~ Figure 
9: (a) Animation of Distribution Curves (1) aner 9 qwmes, 0 update (2)attQr 13querles, ~ (3) atter 19 
queries, 2 updates (4) after 35 queries, 63 Adaptation l update 4 updates 1 in Update 30 q @ o Y [L0 
5 150, LOAD1 (b) Selectivity ,Wx 10 15 queries Estimation Errors ,x &#38;*+ %:;3.%: :.%..Xx 20 25 30 
S5 queries I ) I ~ (1) after Figure 10: Adaptation (a) Animation of Distribution Curves 9 queries, O 
update (2) after 13 queries, 1 update 60, 1 in 50, Update LOAD2 (b) Selectivity Estimation Errors ~~ 
(3) after 24 queries, 3 updates ~~ (4) after 35 queries, 4 update$ queries Figure 11: Adaptation in Update 
LOAD3  
			
