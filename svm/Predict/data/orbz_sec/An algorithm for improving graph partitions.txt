
 An Algorithm for Improving Graph Partitions Reid Andersen * Kevin J. Lang Abstract We present an algorithm 
called Improve that im­proves a proposed partition of a graph, taking as in­put a subset of vertices 
and returning a new subset of vertices with a smaller quotient cut score. The most powerful previously 
known method for improv­ing quotient cuts, which is based on parametric .ow, returns a partition whose 
quotient cut score is at least as small as any set contained within the pro­posed set. For our algorithm, 
we can prove a stronger guarantee: the quotient score of the set returned is nearly as small as any set 
in the graph with which the proposed set has a larger-than-expected intersec­tion. The algorithm .nds 
such a set by solving a sequence of polynomially many s - t minimum cut problems, a sequence that cannot 
be cast as a single parametric .ow problem. We demonstrate empiri­cally that applying Improve to the 
output of various graph partitioning algorithms greatly improves the quality of cuts produced without 
signi.cantly impact­ing the running time. 1 Introduction The minimum quotient cut problem is a fundamental 
and well-studied graph partitioning problem. The goal is to .nd, within an input graph with weighted 
vertices, a subset of vertices whose weight is at most half the total weight of the graph, and whose 
quotient score is as small as possible. The quotient score measures the quality of the set as a partition 
of the graph, and is de.ned to be the number of edges between the set and its complement, divided by 
the weight of vertices in the set. The problem is NP­hard [16], and includes as special cases the minimum 
ratio cut problem and minimum conductance cut problem. *Microsoft Research (reidan@microsoft.com) Yahoo! 
Research (langk@yahoo.com) Approximation algorithms for the minimum quo­tient cut problem have numerous 
practical and theo­retical applications; they have been used to develop divide-and-conquer algorithms 
[31], to solve multi­way partitioning problems [32], for VLSI layout [4], and for clustering [6, 9, 21]. 
There are several fami­lies of algorithms for .nding quotient cuts, including local search heuristics 
[23, 14], spectral partitioning and clustering algorithms [10, 29, 21], the multicom­modity .ow algorithm 
of Leighton and Rao [25], and the multilevel heuristic implemented in METIS [22]. The best approximation 
ratio known for the mini­ v mum quotient cut problem is O( log n), which is at­tained by the semide.nite 
programming algorithm of Arora, Rao, and Vazirani [2]. We consider the complementary task of taking a 
proposed set of vertices and improving its quotient score. There is a surprisingly powerful known algo­rithm 
for this task. Given a set A of vertices whose weight is at most half the total weight of the graph, 
the subset of A with the smallest quotient score can be found in polynomial time by solving a single 
para­metric maximum .ow problem. This algorithm was described in the paper of Gallo, Grigoriadis, and 
Tar­jan [15], as an application of their fast parametric .ow algorithm. This .ow-based improvement tech­nique 
was a key tool in several theoretical results, most notably in Feige and Krauthgamer s approxi­mation 
algorithm for the minimum bisection prob­lem [13], and in .nding hierarchical oblivious rout­ing schemes 
[18]. In addition to its theoretical im­portance, the .ow-based improvement technique is a useful heuristic 
for improving the results of other par­titioning algorithms. This was demonstrated by Lang and Rao [24], 
who showed that applying .ow-based improvement to the output of existing partitioning al­gorithms, including 
spectral partitioning and METIS, produced the best known partitions for a variety of benchmark graphs. 
In this paper, we describe and analyze an algo­rithm called Improve that is strictly more powerful than 
the improvement method described in the pre­vious paragraph. Like the existing method, Improve produces 
a set with quotient score at least as small as the best subset of the proposed set A. In addi­tion, we 
prove the following stronger result, which we state now for the special case where each vertex has unit 
weight. Let C be a given set with small quotient score, let F = |AnC|/|C| be the fraction of C that is 
contained in the proposed set A, and let E = |A|/|V |, which is the expected fraction of C contained 
in a ran­dom set of the same size as A. If F = E + E, meaning the fraction of C contained in A is better 
than ran­dom, then the Improve algorithm will produce a set whose quotient score is at within a 1/E factor 
of the quotient score of C. As an immediate consequence, we prove that to .nd a cut whose quotient score 
is within a constant-factor of optimal, it su.ces to .nd a set of vertices whose intersection with the 
optimal quotient cut is a small constant fraction better than one would expect from a randomly chosen 
set. The Improve algorithm solves a sequence of min­imum cut problems to .nd the optimal way to simul­taneously 
add and remove vertices from the proposed cut, subject to a carefully designed scoring system that imposes 
a penalty for each vertex added from outside of the proposed cut. The two key steps in our analysis are 
the introduction of a modi.ed quo­tient score that enforces this system of penalties, and a proof that 
the Improve algorithm produces in poly­nomial time a set that minimizes this modi.ed quo­tient score. 
Previously known improvement methods based on parametric .ow can .nd the optimal way to remove vertices, 
or the optimal way to add vertices, but do not simultaneously add and remove vertices from the proposed 
set. The Improve algorithm solves a di.erent sequence of s - t minimum cut problems than existing .ow-improvement 
methods, and the se­quence of problems it solves cannot be cast as a single parametric .ow problem. We 
show that the Improve algorithm is a power­ful and e.cient tool for improving graph partitions in practice. 
We demonstrate empirically that the Improve algorithm outperforms existing .ow-based improvement methods. 
Our experiments indicate that the results of most partitioning algorithms are signi.cantly improved by 
applying Improve as a post­processing step, and that applying Improve to the output of spectral partitioning 
and METIS produces excellent partitions. 1.1 Related Work Here we survey previous algo­rithms for improving 
graph partitions. The basic greedy method for improving a quo­tient cut is to swap vertices from one 
side of the cut to the other when this improves the quotient score. Variations of this swapping procedure 
are used in the local search heuristics of Fiduccia-Mattheyses [14] and Kernighan-Lin [23], and within 
the multilevel al­gorithm METIS [22]. Vertex-swapping procedures have been used in various algorithms 
for .nding the optimal bisection in random graphs with planted bisections, including Jerrum and Sorkin 
s algorithm based on simulated annealing [19], Boppana s algorithm based on eigen­vector computation 
[5], and the local search heuris­tic Go with the winners analyzed by Impagliazzo and Dimitriou [12]. 
The algorithm of Dasgupta et al. [11] combines recursive spectral partitioning with greedy vertex-swapping 
to .nd hierarchical planted partitions. Bui et al. [8] .nd planted bisections using an algorithm based 
on network .ow. Cut improvement procedures based on network .ow are often stronger than greedy methods. 
Alon and Milman used a .ow-based algorithm for .nding a vertex separator from an eigenvector [1]. Boykov 
et al. [7] used parametric minimum cut computations to minimize energy functions that have applications 
to computer vision and image analysis. The algorithm of Patkar and Narayanan [27] improves the quality 
of a proposed cut by computing the principal partition of a submodular function. In the special case 
of quotient cuts, their method reduces to the .ow­based algorithm for .nding the optimal subset of a 
proposed cut that we described earlier, but their method applies to more general objective functions. 
Narasimhan and Bilmes [26] introduced an algorithm that uses submodular function optimization to .nd 
the optimal set of vertices to add to a proposed cut. The main computation performed by the Improve algorithm 
of this paper is to optimize a certain objective function (the modi.ed quotient cut score) by constructing 
and solving a sequence of min­imum cut problems. It is known that many objective functions can be optimized 
by constructing similar cut problems. Examples include solving the dens­est subgraph problem [17], solving 
the more gen­eral selection problem introduced by Balinski [3] and Rhys [30], and solving fractional 
programming prob­lems in the framework of Picard and Queyranne [28]. Numerous applications of parametric 
.ow are given in the paper of Gallo et al. [15]. 1.2 Preliminaries Let G =(V, E) be an undi­rected graph 
with an edge weight function w(u, v). We de.ne the edge border .(S) of a set S . V to be the sum of the 
weights of the edges with one endpoint ¯ in S and one endpoint in the complement S. Let p(v) be a function 
that assigns a positive weight to each vertex in V . We assign a weight to each subset S . V by letting 
p(S)= Lv.S p(v). The minimum quotient cut problem is to .nd a set S . V that minimizes the quotient score 
Q(S), .(S) Q(S)= . min(p(S),p(V \ S)) We will assume that the weights p(v) are integers, and that the 
total weight p(V ) is bounded by a polynomial in the number of vertices in the graph. Two useful weight 
functions are p(v) = 1 and p(v)= degree(v), which turn the minimum quotient cut problem into the minimum 
ratio cut problem and the minimum conductance cut problem. 2 The quotient cut improvement algorithm In 
this main section we state and analyze the Improve algorithm. The Improve algorithm takes as input a 
set A and outputs a new set S. Our main theorem, stated below, gives upper bounds on the quotient score 
Q(S) of the improved cut. The .rst part of the theorem shows that Q(S) is at least as small as the smallest 
quotient score of any subset of A. This implies that Improve is at least as powerful as the existing 
.ow-based improvement algorithm. The second part shows that for any set C, if the fraction of C contained 
in A is larger by an additive term E than one would expect for a randomly chosen set, then Q(S) is within 
a 1/E factor of Q(C). Theorem 2.1. The algorithm Improve runs in poly­nomial time. Let A be a set satisfying 
p(A) = p(A¯), and let S = Improve(A). 1. For any set C . A, we have Q(S) = Q(C). 2. If C is a set for 
which the intersection of A with C satis.es p(A n C) p(A) p(A¯) = + E, p(C) p(V ) p(V ) for some E> 0, 
then the set S output by the algorithm satis.es 1 Q(S) = Q(C). E The proof of this theorem, and the description 
of the Improve algorithm, are given later in this section. 2.1 Overview of the proof of the main theo­rem 
In this section we prove the main theorem, af­ter stating the key de.nitions and lemmas needed for the 
proof. The cornerstone of the analysis is the def­ inition of a modi.ed quotient score QA, which we 
call the quotient score relative to the proposed cut A. This modi.ed quotient score has a denominator 
DA(S) that is always smaller than the denominator p(S) of the original quotient score Q(S). In e.ect, 
this new denominator penalizes S for containing ver­tices outside of A. Definition 2.1. Given a set A 
. V that satis.es p(A) = p(A¯), we de.ne ¯ DA(S)= p(S n A) - p(S n A) p(A)/p(A¯) . We remark that DA(S) 
may be zero or may be negative. In particular, DA(S) is zero when S = Ø and when S = V , and it may be 
zero in other cases as ¯ well. DA(S) is negative when S = A. Keeping this in mind, we de.ne with caution 
the ratio between .(S) and DA(S). Definition 2.2. Given a set A . V that satis.es p(A) = p(A¯), we de.ne 
.(S)/DA(S) if DA(S) > 0. QA(S)=+8 if DA(S) = 0. We refer to Q A as the quotient score relative to A. 
We prove that a set S achieving the minimum value of QA over all subsets of V can be found in polynomial 
time by solving a sequence of minimum cut problems. This is the task performed by the Improve algorithm. 
The proof of the following lemma, and the description of the Improve algorithm, are given in section 
2.2. Lemma 2.1. The algorithm Improve(A) outputs a set S that minimizes QA(S). If the vertex weights 
p(v) are integers, the algorithm halts after at most p(V )2 iterations. If the edges of the graph are 
unweighted, the algorithm halts after at most m iterations, where m is the number of edges in the graph. 
We prove a pair of crucial facts relating the modi.ed quotient score QA to the actual quotient score 
Q. The proof of the following lemma is given in section 2.2. Lemma 2.2. Assume that p(A) = p(A¯), Then, 
1. For any set S . V , we have Q A(S) = Q(S). 2. If C is a set for which the intersection of A with 
C satis.es  p(A n C) p(A) p(A¯) = + E, p(C) p(V ) p(V ) for some E> 0, then Q A(C) = .1Q(C). We can 
now prove the main theorem by combin­ing the two previous lemmas. Proof. [Proof of Theorem 2.1] Lemma 
2.1 shows that the Improve algorithm runs in polynomial time and produces a set S minimizing Q A. We 
use Lemma 2.2 to show that S satis.es the two assertions of the main theorem. If C . A, then Q A(C)= 
Q(C), and so Q(S) = Q A(S) = Q A(C)= Q(C). If C is a set for which the intersection of A with C satis.es 
p(A n C) p(A) p(A¯) = + E, p(C) p(V ) p(V ) for some E> 0, then Q A(C) = .1Q(C), and so Q(S) = Q A(S) 
= Q A(C) = 1 Q(C). E 2.2 The Improve algorithm In this section, we state the Improve algorithm and show 
that it outputs in polynomial time a set minimizing the relative quo­tient score Q A. The algorithm works 
by constructing and solving a sequence of s - t minimum cut prob­lems. We now describe the s - t minimum 
cut prob­lems the algorithm will construct. We construct an augmented graph GA(a) that depends on the 
input graph G, on the proposed set A and on a parameter a . [0, 8) to be determined later. The vertex 
set of GA(a) contains the vertex set of G, plus two new nodes s and t that serve as the source and sink 
of the minimum cut problem. The edge set of GA(a) con­tains the edges of G, with their original weights, 
plus the following additional edges from the source and sink: the source node s is connected to each 
node v in A by an edge of weight ap(v), and the sink node t is connected to each node v in V \ A by an 
edge of weight ap(v)f(A), where f(A)= p(A)/p(A¯) = 1. The list below summarizes these edge weights. w(s,v)= 
a · 1v.Ap(v). w(v, t)= a · 1v .Ap(v)f(A). w(u, v)= wG(u, v). Notice that f(A) has been chosen so that 
the total weight of edges connected to s is equal to the total weight of edges connected to t. There 
is a simple bijective correspondence be­tween subsets S . V and cuts separating s and t in the augmented 
graph GA(a). Speci.cally, we as­sociate the subset S . V with the cut (s.S, t.V \S). We de.ne the cost 
of a set S, written costA,a(S), to be the cost of the corresponding cut. By the con­struction of the 
augmented graph, the cost of a cut is given by the following equation. ¯ costA,a(S)= .(S)+ ap(A n S¯) 
+ ap(S n A)f(A). The cost can be rewritten in terms of DA(S), as follows. costA,a(S)= .(S)+ ap(A n S¯) 
+ ap(S n A¯)f(A) = .(S)+ ap(A) ¯ - ap(S n A)+ ap(S n A)f(A) = ap(A)+(.(S) - aDA(S)) . We can now state 
the Improve algorithm. Improve(A) Input and Output. The input is a set A . V satisfying p(A) = p(A¯), 
and the output is a new set S . V . Initialization. Let S0 = A, let i = 0, and let a0 = QA(A)= Q(A) < 
8. Main loop. 1. Compute the minimum s - t cut in the graph GA(ai), and let Si+1 be the subset of V corresponding 
to this cut.  2. Let ai+1 = QA(Si+1). 3. If ai+1 <ai, let i . i + 1 and repeat the loop. Otherwise 
halt and output Si.  We can now prove Lemma 2.1 and Lemma 2.2. Proof. [Proof of Lemma 2.1] Given the 
proposed set A, let a* = min QA(X). X.V We will prove the set S = Improve(A) satis.es QA(S)= a*. We 
.rst show that if there exists a set of vertices X that satis.es Q A(X) <a, then we can .nd such a set 
by solving an s - t minimum cut problem in GA(a). Fix a value of a . (0, 8), and let Sa be a .xed set 
that minimizes costA,a, satisfying costA,a(Sa) = min costA,a(X). X.V We will show that if any set X . 
V satis.es Q A(X) < a, then Sa also satis.es QA(Sa) <a. To see this, observe that for any set X, we have 
QA(X) <a if and only if .(X) - aDA(X) < 0. If some set X . V satis.es Q A(X) <a, then costA,a(Sa) = costA,a(X) 
= ap(A)+(.(X) - aDA(X)) < ap(A). Then, we have ap(A)+(.(Sa) - aDA(Sa)) = costA,a(Sa) < ap(A). The inequality 
above implies .(Sa) - aDA(Sa) < 0, and hence Q A(Sa) <a. The Improve algorithm repeatedly applies the 
procedure described above to produce a set that satis.es Q A(S)= a*. The algorithm maintains the following 
loop invariant: At the beginning of the ith step of the main loop, we have QA(Si)= ai < 8. The initialization 
of the algorithm ensures this is true for step 0. During step i, the algorithm performs a minimum cut 
computation in GA(ai) to .nd a set Si+1 that minimizes costA,ai (S), and then sets ai+i = QA(Si+1). 
The previous paragraph tells us that either ai+1 <ai, or else ai = a*. In the case where ai+1 <ai, the 
algorithm proceeds to step i + 1 with a set Si+1 and value ai+1 that satisfy the loop invariant. In the 
case where ai = a*, the algorithm halts and outputs the set Si, which satis.es QA(Si)= ai = a*. We have 
shown that if the algorithm halts it produces the correct output, but we still need to show that the 
algorithm halts after a reasonable number of steps. We will do this by showing that the quantity DA(Si) 
strictly decreases during any step in which the algorithm does not halt. For any step i> 0 during which 
the algorithm does not halt, we know ai+1 and ai are not equal to 8, and so both DA(Si+1) and DA(Si) 
are strictly greater than zero. Then, we may write costA,ai-1 (Si)= ai-1p(A)+(.(Si) - ai-1DA(Si)) = ai-1p(A) 
+ DA(Si)(Q A(Si) - ai-1) = ai-1p(A)+ DA(Si)(ai - ai-1) . Similarly, costA,ai-1 (Si+1)= ai-1p(A) + DA(Si+1)(ai+1 
- ai-1) . Because Si minimizes costA,ai-1 , we have costA,ai-1 (Si) = costA,ai-1 (Si+1), which implies 
DA(Si)(ai - ai-1) = DA(Si+1)(ai+1 - ai-1) . Since ai+1 <ai <ai-1 < 8, we have DA(Si+1) = DA(Si), as desired. 
If the vertex weights p(v) are positive integers, the quantities p(S n A) and p(S n A¯) can each take 
at most p(V ) possible values. Therefore, the quantity DA(S)= p(S n A) - p(S n A¯)f(A) can take at most 
p(V )2 values for a .xed value of A. Since DA(S) strictly decreases at each step, the algorithm halts 
after at most p(V )2 steps. Since both Q A(Si) and DA(Si) strictly decrease at each step, .(Si) also 
strictly decreases at each step. If the edges of the graph are unweighted, then .(Si) can take on at 
most m possible values, so the algorithm halts after at most m steps. Although our upper bound on the 
number of steps required for Improve to converge is large, in practice the algorithm often converges 
after a much smaller number of steps. In our preliminary experiments, the algorithm typically halted 
after fewer than 10 steps on graphs with a few hundred thousand nodes. We remark that is it also possible 
to perform binary search over a to compute a*, and then produce a set S attaining Q A(S)= a*. Proof. 
[Proof of Lemma 2.2] To prove assertion 1, it su.ces to show that DA(S) = min(p(S),p(V \ S)) whenever 
p(A) = p(A¯). It is easy to see that DA(S) = p(S), since we have DA(S)= p(S n A) - p(S n A¯)f(A) = p(S), 
so it remains to prove that DA(S) = p(V \ S). To prove this, notice that DA(S)+ DA(V \ S) = 0, because 
DA(S)+DA(V \S)= DA(V )= p(A)-p(A¯)f(A)=0. We now perform the following calculation, in which we use the 
fact that f(A)= p(A)/p(A¯) = 1. DA(S)= -DA(V \ S) = -p((V \ S) n A)+ p((V \ S) n A¯)f(A) = p(V \ S)f(A) 
= p(V \ S). This completes the proof of Assertion 1. We remark that equality holds, with Q A(S)= Q(S), 
if and only if S . A. To prove assertion 2, it su.ces to show that DA(C) = Ep(C) when A and C satisfy 
the assump­tion of the lemma. DA(C) p(C n A) p(C n A¯) = - f(A) p(C) p(C) p(C) p(A) p(A¯) =+ E p(V ) 
p(V ) p(A) p(A¯) - f(A)1 -- E p(V ) p(V ) p(A¯) p(A¯) = E+ f(A) p(V ) p(V ) p(A) p(A) p(A) p(A) +- + 
p(V ) p(A¯) p(A¯) p(V ) p(A) p(V ) p(A) = E · 1+ 1 - + p(V )p(A¯) p(A¯)= E. 3 Experiments Improve is 
provably at least as strong as the MQI al­gorithm (MQI is the parametric .ow-based quotient cut improvement 
method discussed earlier [24]), and empirically it costs only a small constant factor more to run. For 
example, in the hard graphs experiment below, Improve required solving an average of four or­dinary .ow 
problems on the whole graph rather than the two parametric .ow problems on half the graph which MQI requires. 
In the following experiments we focus on graphs whose best quotient cut has close to 50:50 balance, because 
that is the situation where MQI is weakest since it cannot move nodes in both directions, so paying the 
extra constant factor would be most justi.ed. We study 3 classes of graphs. First there are seven classic 
meshlike graphs from the Graph Parti­tioning Archive, so we can compare with the best so­lutions generated 
by many researchers. Second, there is a set of 1000 modi.ed random geometric graphs that are hard enough 
to produce a wide spread be­tween the performance of di.erent algorithms. Fi­nally, we study a small 
set of random graphs with planted cuts, a class which has been heavily studied by theorists. 3.1 Meshlike 
benchmark graphs Here we re­port results on seven meshlike benchmark graphs Figure 1: Results for real-world 
meshlike graphs. Improve always beats or ties MQI, and the resulting qcuts are very competitive with 
the best nearly balanced cuts from the graph partitioning archive. name n nodes Improve vs MQI wins 
ties losses wing 62032 3914 2378 0 fe tooth 78136 1970 625 0 fe rotor 99617 1794 241 0 598a 110971 1263 
181 0 144 144649 999 14 0 wave 156317 1132 0 0 m14b 214765 605 27 0 name best qcut found with at least 
475:525 balance compared to archive cut wing fe tooth fe rotor 598a 144 wave m14b 0.025504 = 791 / 31015 
0.097809 = 3821 / 39066 0.041964 = 2004 / 47755 0.043219 = 2398 / 55485 0.089733 = 6488 / 72303 0.111400 
= 8702 / 78115 0.035723 = 3836 / 107382 1.000032 0.992518 1.036868 1.000000 0.999982 1.001702 1.000000 
 downloaded from the Graph Partitioning Archive,1 a web site that was set up by Chris Walshaw as a followup 
to the paper [33]. This web site keeps track of the best nearly-balanced cuts ever found by any algorithm 
for these graphs and others. By now at least 22 algorithms are represented on the site. These benchmark 
graphs were actually one of the original motivations for the present work; in 2004 we used Metis+MQI 
plus some additional hacks to .nd some new record cuts to post on the web site. The addi­tional hacks 
were needed because MQI isn t naturally good at .nding cuts with nearly perfect balance. The Improve 
algorithm in this paper makes it possible to .nd cuts of a similar quality in a much more princi­pled 
way. The seven graphs are listed in Figure 1. For each graph we ran a randomized version of Metis roughly 
a few thousand times to generate a set of initial balanced cuts. Then Improve and MQI were both run on 
each of these cuts. Figure 1(top) contains tabulations of the win/tie/loss scores of the two improvement 
methods on the various cuts for each of seven graphs. Improve always beats or ties MQI, 1http://staffweb.cms.gre.ac.uk/~wc06/partition 
which is consistent with Theorem 2.1, so both the theorem and our code pass this sanity check. Figure 
1(bottom) lists for each graph the best quotient cut found by any of these runs of Metis+Improve, subject 
to the constraint that the small side must contain at least 47.5 percent of the nodes. For comparison, 
we computed the quotient cut score of each of the four cuts posted on the web site (which are the smallest 
known cuts with at least 47.5, 48.5, 49.5, and 50 percent of the nodes on the small side), then divided 
the qcut score of our solu­tion by the best of those four qcut scores to obtain the ratio listed in the 
.nal column of the table. Clearly, all of these ratios are very close to 1.0. 3.2 Graphs designed to 
be hard for all algo­rithms In this section we present results for seven di.erent methods on a set of 
1000 graphs drawn from a class of graphs that was speci.cally designed to be di.cult in order to produce 
a wide spread of outcomes. Our graph generator produces a modi­.ed variant of the random geometric graphs 
which have been very popular in empirical graph parti­tioning experiments. Early papers like [20], which 
were studying local improvement methods, consid­ered these graphs to be challenging. However, we have 
found that Leighton-Rao and Spectral with .ow­based rounding can easily produce solutions that are probably 
within a few percent of optimal. Therefore we add some additional random edges to make the problem harder. 
Speci.cally, each of our graphs was generated as follows. First we choose ten thousand points from a 
uniform distribution on the unit disk (not the unit square). We generate a set of candi­date edges consisting 
of each point and its 50 nearest neighbors. These candidate edges are then added to the graph in order 
from shortest to longest, stopping when the graph becomes connected. Finally, we add 2000 additional 
completely random edges. The re­sulting graphs contain 10000 nodes and an average of 58913 edges, for 
an average degree of 11.7826. We point out that Improve can be viewed as a rounding method for extracting 
cuts from spec­tral embeddings. In this experiment four of the seven studied methods fall into the Spectral 
Embed­ding + Rounding Method category. The spectral embedding is speci.cally the ordering of the nodes 
induced by the second smallest eigenvector of the  Figure 2: Solution quality distributions, for 7 
algorithms on a set of 1000 hard graphs (random geometric plus random edges). The distributions for the 
four spectral rounding methods are strikingly di.erent, with the new Improve algorithm working the best. 
graph s (un-normalized) Laplacian matrix, aka the graph s Fiedler vector. The four rounding meth­ods, 
in order of strictly increasing power, are Me­dianCut, SweepCut, MedianCut+MQI, and Median­Cut+Improve. 
The remaining three methods in this experiment are, in order of strictly increasing power, Metis, Metis+MQI, 
and Metis+Improve. The previ­ous two sentences contain 5 provable claims that one algorithm is strictly 
more powerful than another. The win-tie-loss tabulations in Figure 3 provide a sanity check on those 
claims and on our code. On these graphs, Improve required an average of 4 .ow prob­lems to be solved. 
The histograms in Figure 2 show the distribution of solutions produced by each of the algorithms on our 
sample of 1000 graphs. Instead of plotting raw qcut scores, we divide the score of each solution by the 
best known score for each graph, which we found by running 3 additional, more expensive algorithms on 
each of the graphs.2 Evidently, there is a big di.erence in the power of the four rounding methods. Median 
cuts can 2The additional algorithms included the implementation of Leighton-Rao described in [24], and 
the two heuristic methods 1) multi-try Metis+MQI+Mid.ow and 2) SDP embedding rounded with multi-try Mid.ow+MQI, 
that we previously used to .nd record cuts for the Graph Partitioning Archive. The SDP is the hypersphere 
embedding used by [2], but without the triangle inequalities on squared distances. algorithm A B algorithm 
A wins ties wins B SpecSwp 997 3 0 SpecMid SpecMid+MQI 934 66 0 SpecSwp SpecMid+Improve 928 72 0 SpecMid+MQI 
Metis+MQI 1000 0 0 Metis Metis+Improve 899 101 0 Metis+MQI Figure 3: Win-tie-loss counts, for 7 algorithms 
on a set of 1000 hard graphs (random geometric plus random edges). be more than 4 times worse than the 
best known cuts, while Sweep cuts (to which most theoretical approximation guarantees apply) range from 
about 1 to 3 times worse than the best known. The new rounding method, Median+Improve, produces cuts 
ranging from about 1 to 1.5 times worse than the best known solutions. 3.3 Random graphs with planted 
bisections There are numerous theoretical results for random graphs with planted bisections [8, 5, 19, 
12, 11]. On the whole, they indicate that spectral methods and local improvement methods should work 
well. In the following experiment, we tried several algorithms on a small collection of graphs with planted 
bisections of various sizes. The graphs were produced as follows. First specify a cutsize c< 50000. Then 
generate two Figure 4: Results for random graphs with planted bisections. Here the new algorithm Improve 
is second best after Fidduccia-Mattheyses (local search is known to work well on random graphs). 50000-node 
random degree-4 graphs A and B, each by choosing 4 random matchings containing no duplicate edges. Then 
randomly choose a set of c nodes from graph A and c nodes from graph B, and connect them randomly. The 
resulting graph contains 100000 nodes and 200000 + c edges. We generated 16 graphs with c ranging from 
20000 to 35000 in steps of 1000. In Figure 4 we plot the qcut score of the cut found by Metis, and of 
the Spectral Median Cut. Also plotted are the Spectral Median Cut improved by MQI, by Improve, and by 
Fidduccia-Mattheyses [14], a standard local improvement algorithm. The best results are SpecMid+FM, with 
SpecMid+Improve a close second. This outcome seems consistent with the body of theoretical work mentioned 
above. References <RefA>[1] N. Alon and V. Milman. Isoperimetric inequalities for graphs and superconcentrators. 
J. Combin. Theory B, 38:73 88, 1985. [2] S. Arora, S. Rao, and U. Vazirani. Expander .ows, geometric 
embeddings and graph partitioning. In STOC 04: Proceedings of the thirty-sixth annual ACM symposium on 
Theory of computing, pages 222 231, New York, NY, USA, 2004. ACM Press. [3] M. Balinski. On a selection 
problem. Management Science, (17):230 231, 1970. [4] S. Bhatt and F. Leighton. A framework for solving 
vlsi graph layout problems. J. Computer Systems Sciences, 28:300 343, 1984. [5] R. B. Boppana. Eigenvalues 
and graph bisection: An average-case analysis (extended abstract). In FOCS, pages 280 285, 1987. [6] 
C. Borgs, J. T. Chayes, M. Mahdian, and A. Saberi. Exploring the community structure of newsgroups. In 
KDD, pages 783 787, 2004. [7] Y. Boykov, O. Veksler, and R. Zabih. Fast approxi­mate energy minimization 
via graph cuts. In ICCV (1), pages 377 384, 1999. [8] T. Bui, S. Chaudhuri, F. T. Leighton, and M. Sipser. 
Graph bisection algorithms with good average case behavior. In FOCS, pages 181 192, 1984. [9] J. Carrasco, 
D. Fain, K. Lang, and L. Zhukov. Clustering of bipartite advertiser-keyword graph, 2003. [10] F. Chung. 
Spectral graph theory, volume Number 92 in CBMS Regional Conference Series in Mathemat­ics. American 
Mathematical Society, 1997. [11] A. Dasgupta, J. E. Hopcroft, R. Kannan, and P. P. Mitra. Spectral clustering 
by recursive partitioning. In ESA, pages 256 267, 2006. [12] T. Dimitriou and R. Impagliazzo. Go with 
the winners for graph bisection. In SODA: ACM-SIAM Symposium on Discrete Algorithms, 1998. [13] U. Feige, 
D. Peleg, and G. Kortsarz. The dense k-subgraph problem. Algorithmica, 29(3):410 421, 2001. [14] C. M. 
Fiduccia and R. M. Mattheyses. A linear-time heuristic for improving network partitions. In DAC 82: Proceedings 
of the 19th conference on Design automation, pages 175 181, Piscataway, NJ, USA, 1982. IEEE Press. [15] 
G. Gallo, M. D. Grigoriadis, and R. E. Tarjan. A fast parametric maximum .ow algorithm and applications. 
SIAM J. Comput., 18(1):30 55, 1989. [16] M. R. Garey and D. S. Johnson. Computers and Intractability, 
A Guide to the Theory of NP-Completeness. W.H. Freeman and Company, New York, 1979. [17] A. Goldberg. 
Finding a maximum density subgraph. Technical Report UCB CSD 84/71, University of California, Berkeley, 
1984. [18] C. Harrelson, K. Hildrum, and S. Rao. A polynomial-time tree decomposition to minimize congestion. 
In SPAA 03: Proceedings of the .f­teenth annual ACM symposium on Parallel algo­rithms and architectures, 
pages 34 43, New York, NY, USA, 2003. ACM Press. [19] M. Jerrum and G. B. Sorkin. Simulated annealing 
 for graph bisection. In IEEE Symposium on Foun­dations of Computer Science, pages 94 103, 1993. [20] 
D. S. Johnson, C. R. Aragon, L. A. McGeoch, and C. Shevon. Optimization by simulated annealing: An experimental 
evaluation; part i, graph partition­ing. Operations Research, 37(6):865 892, 1989. [21] R. Kannan, S. 
Vempala, and A. Vetta. On cluster­ings: Good, bad and spectral. J. ACM, 51(3):497 515, 2004. [22] G. 
Karypis and V. Kumar. A fast and high quality multilevel scheme for partitioning irregular graphs. SIAM 
J. Sci. Comput., 20(1):359 392, 1998. [23] B. Kernighan and S. Lin. An e.ective heuristic procedure for 
partitioning graphs. The Bell System Technical Journal, pages 291 308, 1970. [24] K. Lang and S. Rao. 
A .ow-based method for improving the expansion or conductance of graph cuts. In IPCO, pages 325 337, 
2004. [25] F. T. Leighton and S. Rao. An approximate max­.ow min-cut theorem for uniform multicommodity 
.ow problems with applications to approximation algorithms. In FOCS, pages 422 431, 1988. [26] M. Narasimhan 
and J. Bilmes. Local search for balanced submodular clusterings. In IJCAI, pages 981 986, 2007. [27] 
S. B. Patkar and H. Narayanan. Improving graph partitions using submodular functions. Discrete Appl. 
Math., 131(2):535 553, 2003. [28] J.-C. Picard and M. Queyranne. Selected appli­cations of minimum cuts 
in networks. INFORM, 20(4):394 422, 1982. [29] A. Pothen, H. Simon, and K. Liou. Partition­ing sparse 
matrices with eigenvectors of graphs. SIAM Journal of Matrix Analysis and Applications, 11:430 452, 1990. 
[30] J. Rhys. A selection problem of shared .xed costs and network .ows. Management Science, (17):200 
207, 1970. [31] D. Shmoys. Cut problems and their applications to divide-and-conquer, 1996. [32] H. D. 
Simon and S.-H. Teng. How good is recursive bisection? SIAM Journal on Scienti.c Computing, 18(5):1436 
1445, 1997. [33] A. J. Soper, C. Walshaw, and M. Cross. A combined evolutionary search and multilevel 
ap­proach to graph partitioning. In Proceedings of the Genetic and Evolutionary Computation Conference 
(GECCO-2000), pages 674 681, Las Vegas, Nevada, USA, 10-12 2000. Morgan Kaufmann.  
			</RefA>
