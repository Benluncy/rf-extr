
 ANIMA II: A 3-D COLOR ANIMATION SYSTEM Ronald J. Hackathorn COMPUTER GRAPHICS RESEARCH GROUP THE OHIO 
STATE UNIVERSITY ABSTRACT An animation software system has been developed at The Computer Graphics 
Research Group which allows a person with no computer backgroundto develop an animation idea into a finished 
color video product which may be seen and recorded in real time. The animationmay include complex polyhedra 
forming words, sentences, plants, animals and other crea­ tures. The animation system, called Anima II, 
has as its three basic parts: a data generation rou­ tine used to make colored, three-dimensional objects, 
an animationlanguage with a simple script-like syntax used to describe parallel mo­tion and display 
transformations in a flexible, scheduled environment,the Myers algorithmused in the visible surface 
and raster scan calculations for the color display. This paper discusses the requirements,the problems, 
and the trade-offs of such a system. An overview of research in the area is given as well as the design 
and implemen­tation highlights of the Anima II system. 1. Introduction During the past several years, 
films from the Universityof Utah (16), General Electric Corp. (17) and by N. Max (18), illustrate that 
the abil­ ity to produce 3-D shaded object animationhas been a significant addition to the field of com­puter 
animation. Max's comment about his film, "Sphere Eversion," describes the basic feeling to­wards this 
type of animation: "The film produces a visualizationwhich could not have been achieved in any other 
medium, and could never have been animated by hand." (26) A 3-D animation system which uses a visible 
sur­ face algorithm to calculate the final displayed image must deal with severe time-space considera­ 
tions resulting from the increased complexity of both the data and the data handling algorithms, through 
all phases of the system. Traditionally, shaded object animation while producing high qual­ ity has 
been a difficult, slow and expensive proc­ ess as a result of implementationaltrade-offs among these 
various considerations. An animation software system has been developed by the Computer Graphics Research 
Group as an Permission to make digital or hard copies of part or all of this work or personal or classroom 
use is granted without fee provided that copies are not made or distributed for profit or commercial 
advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, 
to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or 
a fee. Siggraph 77, July 20-22 San Jose, California attempt to maximize the trade-offs involved in 3-D 
color animation. The goal has been to achieve the capability and image quality necessaryfor complex animationand, 
yet, maintainthe total system efficiencynecessary for a production ani­mation environment. Anima II 
is a computer animation system designed for the production of color, three-dimensional video tapes. It 
is aimed at the animator, educa­tor and artist who requires anything from a high volume of short color 
sequences for teaching pur­poses, to realistic key frame animation involving complex color objects and 
precisely timed life­like movements. The Anima II system provides an efficient environment for the creation, 
animation and real-time playback display of color-shaded polyhedra. The video output is directly connected 
to video recording equipment and a standard color television set. 2. Background Before discussing previous 
research in the area of 3-D shaded animation systems it is important to briefly discuss the requirements, 
the problems and the trade-offs which accompany the design and im­plementation of such a system. 2.1 
Requirements System and user requirements for 3-D shaded anima­tion can be classified into three factors 
deter­mining overall system performance. 2.1.1 Capabilities -A shaded animation system can be viewed 
as having three separate capabili­ ties. Each has a unique functionwithin the sys­tem and each has different 
problems.  Data Generation is the process of constructing a computer model representing the three-dimen­sional 
object or form that is to be animated. The type of data to be generated is determined by the type of 
visible surface algorithmused. Essentially,polygon-basedalgorithms need planar polygons while parametric 
surface algo­rithms need high-order patch equations. There have been many approaches to inputting of 
3-D polyhedra. These include dual data tablet dig­itizing (32), single data tablet (23,27), and geometric 
modeling (4,8,13,29).  Animation is the process of "giving life" to  the generatedobjects by specifying 
motions animation a visible surface algorithm using the which imitate the actions of the physical world. 
second method is capable of sufficient quality. These motions involve changes to an object's  position, 
orientation (rotation), size and shape. Also the concepts of acceleration, de­celerationand "path-following"are 
included as motion descriptions. The animator controls the motions of an object through a program or 
 "script" written in the syntax of the system's language. Key frame techniques implemented in several 
2-D/3-D line animation systems (3,6,19, 36), notablyby the Film Board of Canada (34, 35), have proven 
a most effectivemeans for specifying the motion dynamics (movement through time and space) of complex 
animation. The centralnotion of key frame animation is that an action of an object will change "from" 
some spatial state "to" a new state and that this action will range "from" some time frame within the 
sequence "to" a later frame. In this manner the user need only specify the spatial-temporalextremes 
and the in-between frames are calculatedby the language. As well as other responsibilities,the animation 
lan­guage must also control a visible surface algo­rithm in one manner or another.  Display and record 
techniques give the animation system the capabilityof viewing the animation during development and documentingthe 
final animation sequence. The most common method of dealing with the output from a visible surface algorithm 
is to photograph it frame by frame. The image output may be a sweepinghorizontal scan line on a refresh 
CRT, or may be buffered in a 2-D matrix memory with raster video dis­play. Anothermethod is to encode 
the visible surface algorithm's results and store this in­formation on an analogue (30), or digital (5, 
25) disk. The video sequence may then be read from the disk, decodedthrough a scan-line de­coder, and 
displayed in real time on a video monitor. A refresh CRT needs filters to pro­ duce color while a raster-scan 
displaywill typicallyhave color output.  2.1.2 Image Quality -The type of visible sur­face algorithmused 
by the system determines to a large degree the single (still) frame image qual­ ity produced. There has 
been much work in the area of visible surface algorithms, and no at­tempt is made here to present all 
the factors in­ volved. Basically, however, there are two types of algorithms, the first type of which 
calculate the intensities of a curvedvisible surface to the resolution of a single picture element. This 
type includes the "reflected radiation" algorithm of Magi (20), the recursivebivariate surface patch 
algorithm by Catmull (9), and (10,28). The second type, polygon-based,simply colors or shades in the 
faces of 3-D polyhedra. Here the Watkins (31, 33) and Myers (24)algorithms serve for examples. The first 
method inherently produces a smoother surface than the other and lends itself to the cal­culation of 
texture, patterns and reflections (7). Thus, for still frame images one typically insists on a visible 
surface algorithmusing the first method. However, for multiple frame (moving) images, motion contributes 
significantlyto qual­ ity. Indeed it may be argued that the quality of motion is the most significant 
contributor to the qualityof the animation. In any case, for 2.1.3 Efficiency -A general definition 
of effi­ciency is "the ability to produce without waste." In a computer environment,the most valuable 
re­source to prevent from wasting is computer time. In an animation environment the resource is people 
time. Efficiency in an animation system implies that all capabilitieswithin the system are easy to use 
and produce their desired results quickly. For example, the system is inefficient if people who have 
been trained in animationhave to be re­trained in mathematics and/or computer programming just so they 
may apply their previous knowledge in areas of color, form, composition, rhythm, flow and motion. Further, 
it is inefficient if an ani­ mator is forced to stop repeatedlyduring the pro­duction process, because 
of a slow turn-around time to see the results. To gauge efficiency at the system level (i.e. sys­tem 
responsivenessand system throughput) an ani­mator must question all phases of the system: How long will 
it take to make the data? How hard is it to describe the animation? How long will it take to calculate 
(turn-aroundtime)? How compli­cated is the final recording process and how long will it be to see the 
results? System efficiency in an animation environment can only be measured in terms of how long it 
takes and how hard it is for the animator to get an animation idea off of a storyboard and onto film 
or video tape. A system which provides direct interactionand fast feed­back gives an animator the freedom 
to experiment with the system and get a feeling for what kind of animation can be done.  2.2 Problems 
and Tradeoffs Fitting the algorithms used for producing,handling and displaying 3-D color data together 
into a uni­fied animation system causes problems which effect the system's total performance. The problems 
are due to fixed limits within the system determined by how much time, money and memorywas available. 
 Trade-offs occur as some features must be lost in order for others to be implemented. For example the 
amount of directly addressable mem­ory available determines how much data memory and instructionmemory 
can coexist. The size of the data space limits the complexity of the objects while instruction space 
can decide capability and faster response times since program overlay and task switch techniques can 
be avoided if all the programs are in main memory together. Another example is image quality and its 
relation­ ship to capability and efficiency. A high-order parametric surface equation realisticallydescribes 
a smooth curved surface and has an increase in image quality over shaded polyhedra. While it may not 
be difficult to generate, the data for this type of algorithmwithout a control language, pre­sents difficulties 
for the animator. The algorithm can also take a considerable amount of calculation time to generate 
the final pictures. For instance, there are some excellent results with Catmull's method that took 25 
minutes on a PDP 11/45 for a single picture (7). Calculationtime becomes im­portant in an animation sequence 
where one minute takes 1440 or 1800 frames (depending on film/video "encoding manually when additional 
artistic free­ recording). If polygon based shading algorithms are used, image quality drops but capability 
and efficiency increase (especially if the algorithm is efficient). Another trade-off in an animation 
system is the means of displaying the data and recording the fi­nal sequence. Film offers higher quality 
(resolu­tion and contrast ratio) compared to video, but must be chemically processed before the results 
can be seen. Video however, has the advantage that it can be immediately seen as it is being re­ corded 
and the video tape can be reused. Also color is a natural component in a video system whereas it must 
be added through filters for the film process.  It is often said that standard TV display of com­puter 
pictures is of low resolution because one sees the jaggies. This assumption is quite mis­leading and 
one should make a distinction between the inherent resolution of TV and computer gener­ated pictures. 
For instance, if a color TV camera is recording a rotating 3-D color cube (a real­world object) and it 
is displayed on the monitor viewed at a distance 5 times the height of the screen, then there will be 
no apparent jaggies. On the other hand a computed animation sequence of a similar colored cube rotating 
on the monitor also viewed from the same distance will usually have jaggies. The visible surface algorithm 
must com­pute the 3-D position, intensity, hue and satura­tion for each point generating the scan lines 
to display the picture. Typicallythere is a certain percentage of error in these calculations and the 
computational time required to overcome these er­rors can be lengthy. What one must consider are the 
trade-offs. While high picture quality is im­portant and desirable,what does it mean in the context of 
moving images and the bandwidth limita­tions of an NTSC signal? Vision research suggests that less picture 
resolution is necessaryfor mov­ ing images than static images.  2.3 Other Systems  Based on the literature 
to date, there have been many computer graphics facilities which have imple­mented either a technique 
for generating 3-D ob­jects, an animation language, or a visible surface algorithm. Two examples would 
be the Universityof Utah which produced the Watkins Algorithm (31,33), and Archuleta's work at Lawrence 
Livemore Labora­ tory (2)where he implemented a fast version of the Watkins Algorithm on a CDC 7600. 
However, only a few facilities have attempted to integrate these fundamental capabilities into one 
complete system for the expressedpurpose of animation.  2.3.1 An experimental3-D animation systemwas 
de­veloped at the IBM Watson Research Center by Appel et al. (1). This system produced output to a high 
 resolution microfilm recorder in the form of hidden line or shaded objects. A special "movie specifi­cation 
language" was used to control motion, chang­ing viewpoints of perspective and a remote light source capable 
of casting shadows. Efficiency in the system was increased by sharing program tasks among an IBM 360/67, 
a 360/91, and a 1130. 3-D data was entered into the system either by inter­actively picking points with 
an IBM 2250 or by dom is required." 2.3.2 Case Western Reserve University has a com­ puter system which 
can generate shaded perspective pictures in real time. This "Shaded Graphic Sys­tem" was developed for 
Case by Evans and Sutherland Corporation at a cost $400,000. It consists of a graphics processor driving 
a pipe­line of special purpose hardware for matrix multi­plication and shading. Sharing memory with the 
graphics processor is a PDP-11 with a 10 megabyte disk and an assortment of I/O devices. 3-D data is 
processed on a scan line by scan line basis by a hardware implementation of Watkin's hidden sur­face 
algorithm and sent to a shader which uses the Gouraud shading technique (21). The resulting im­age is 
displayed on a raster scan CRT for real­time display. A camera unit with color filters under computer 
control is used to produce computer animated films. Jones (22) describes a high level programming lan­guage 
he implemented for the Case system. It con­sists of a complete implementation, for the PDP of Algol-60with 
the addition of string vari­ ables, I/O facilities, and extensions for handling graphic shaded images. 
The primary purpose of this work was to facilitate the use of a custom­built system which can produce 
shaded images in real time. According to Jones one important ad­vantage of Algol was its block structure 
which Jones decided would lend itself quite nicely to the description of graphical structures. The con­sequence 
of this approach is that just as Algol itself is a way of talking about algorithms, the graphic-extendedAlgol 
is a way of talking about graphical data structures. Currently, the system requires 3-D data to be en­tered 
through a dual data tablet arrangement which means the animator must provide detailed drawings from several 
viewpoints (something most animators with their "sketchy" storyboards don't have readi­ ly available). 
But besides this and the lack of color in the system, the combination of Jones' ex­tended Algol-60 language 
and the powerful graphic display processor presents a good example of a general purpose 3-D real-time 
animation system. Most of the film "Sphere Eversion" was calculated with this system. 2.3.3 Credit should 
be given to Goldstein (20), Nagel, et al. (13) and Elin (14) for their pio­neering work in the area of 
3-D shaded animation with the Magi-Synthavision system. The unique visible surface algorithm uses curved 
patched sur­faces, but its approach is fundamentally different from others. "Rays" are fired from some 
point in space and traced to the first visible point on a 3-D object. The advantage of this technique 
is that since the rays are stopped at the first sur­ face encountered,no time is spent examining the 
parts of the model which would be normally hidden. The system is capable of generating data with a sophisticated 
"combinatorial geometry" technique (thus preventing the decrease in data generation capability, typically 
associated with parametric surface algorithms). Here, "the user specifies the geometry by establishing 
two tables. The first table contains the type and location of the bodies used in the geometric description 
(there are nine 56 component in the calculations for the final basic shapes). The second describes 
the physical region in terms of the bodies in table 1 and the display output. three Boolean operators, 
'+', '-', and 'or'. Each region has a unique region number and the bodies o Displaying and directly recording 
in real time, are numbered in the order of their occurence. The the color video sequence that was calculated 
terms of its re-and stored (in binary) on the system disk by model is completely described in gion number." 
the animation language. The input to the system also includes "the loca­ tion and characteristics of 
a camera (focal length and size of image plane), the direction from which the light is coming and a set 
of instructions called "Director's Language," which tells the com­ puter how to treat the objects (animatethe 
ac­tors) in the film." The calculated visible surface output is stored on magnetic tape. Using this tape 
as input a second pass through the computer is made to convert the region-intensitydata into color-intensity. 
The film process (based on color addition) requires that the output tape be made with three weighted 
red, green and blue frames for every one frame from the input tape. The tapes, in this form, are fed 
through a Data General Minicomputerto a pre­ cision CRT. The images are filmed through a com­ puter controlled 
color wheel (triple exposure-­ once for red, green and blue). The Magi-Synthavisionsystem has taken 
an excel­ lent approach to 3-D shaded animation with the use of "Combinatorial Geometry" and a "Directors 
Lan­guage" to control their calculated visible surface output. Unfortunately the system suffers from 
a lack of interaction, because to use these powerful facilities the animator must keypunch in the com­ 
mands to control both the data generation and ani­mation process. Also, calculation time is slow, ranging 
from 30 seconds a frame for extremely sim­ ple data, up to around 20 minutes a frame (15). 3. Anima II 
The animation software system has been implemented in a standard minicomputer environment (Diagram 1) 
with a PDP as the central processing unit. The CPU has 64 K of core memory (32 K of which contains the 
RSX ll-D operating system) and 32 K of MOS memory. In addition, the peripherals in­ clude: a 4096 x 
4096 Vector General refresh CRT with joystick, buttons and dials; a 44 mega-word (16 bit), "3330 type" 
moving head disk used as the system disk; a special purpose color, raster-scan decoder which serves 
as our real-time video inter­ face. The software in the animation system was written in assembly language 
to increase efficien­ cy (Diagram 2). The system, Anima II, supports an environment in which a user trained 
in areas other than computer programming, is capable of: Creating complex color polyhedra with a real­ 
time interactive geometric "modeling" routine. Writing an animation script describing parallel, keyframe 
motion dynamics controlling multiple objects.  Animating the script using a specially written animation 
language processor in which the Myers visible surface algorithm is the kernal While each of these areas 
have noteworthytheoret­ical and implementational features in and of them­ selves, what is significant 
about the Anima II system is the integration of these separate, com­plex processes into a complete system, 
which is both easy to use and efficient. Currently the Anima II system is supporting anima­tion projects 
in the areas of education, telecom­munication and art as well as research projects for astronomy, statistics 
and computer-aided design. 3.1 Data Generation The objects in the animation sequence are created with 
Parent's (29) interactive data generation pro­gram. The user views and interacts with the ob­jects in 
real time on a random scan CRT. Concave polyhedra are joined and intersectedto form com­plex shapes. 
The object can be bent or warped into  ANIMA II SOFTWARE  multiple shapes for animating later. Transparent 
 to the user is the data structure of the objects which consists of closed polygons forming closed convex 
or concave surfaces. The user is only aware of positioning two objects in some relation to each other, 
pushing a button, and either joining the two together or cutting one into the other. The proc­ess is 
accumulativeand can be repeated as often as necessaryto build the final object. The color of the objects 
can be specifiedwhen the user chooses his primitive objects (a green ball can cut a green hollow in a 
red cube) or individual faces may be selected and "painted." The data generation routine uses 32 K of 
MOS memo­ry for instruction and data space and uses 20 K of core memory for a device handler which buffers 
the display lists and refreshes the Vector General. The routine can handle up to 2500 unique edges. A 
user accustomed to the "sculpting and building" ap­proach of the routine can make an object in a very 
short time. This can range from 5-15 minutes for a simple shape such as a block letter, 2-3 hours for 
the frog and duck in Figures 1-12, and up to five hours for complex data like the "Jack-in-the- Box" 
shown in the video tape accompanyingthis presentation. These times also include the bending and warping 
process to make multiple shapes for in­terpolation (blending)in the language. A detailed presentationof 
the intersectionalgorithm used in the data generationroutine is being given at this conference by its 
author. 3.2 Script Once the 3-D objects have been created,the user controls the rest of the animation 
process through his script. The script is a story-boarded anima­tion idea, transcribed into a list of 
instructions written in the special descriptive syntax of the language. The language of the Anima II 
system of­fers a means of imitatingthe complex motions of "real" world objects by breaking each motion 
into simple, but preciselycontrolled changes through space and time. Language instructions are indi­viduallyscheduled 
to be active over a range of time during the animation sequence. When the in­structionhas reached its 
time limit, it can be re­scheduled to be active later in the sequence, or it can be removed from further 
consideration. An in­struction specifies key frame time parameters and it also describes key frame spatial 
transform para­meters. However only the extreme parameter which the instruction is changing to need be 
given, be­cause the language keeps track of where each object currently is. This saves animators from 
having to keep records on their own of what they have done so far in the script. They specify where they 
want to go "to," and the language calculateswhat the vec­tor should be to get there. This applies whether 
the transform affects position, rotation, size, shape, or path. Using this format, a combination of 
a "set" and a "change" scene directive can com­pletely control one simple motion as in the exam­ple: 
 position of some object to the point specified by X, Y, Z on the first frame of the sequence (coor­dinate 
and frame values can be given as numbers or symbolic variables)." "At the same time, change the position 
from the point where it was set, to a new point given by different X, Y, Z values and be there by frame 
number 100." The ability to schedule the language instructions allows the user to animate multiple objects 
with­out being concernedwith looping or programmatic flow control. This notion of parallel commands is 
quite different from the typical approach found in other graphics languages in which the animation is 
controlled by guiding an internal program address counter or pointer, into, through and out of a se­ries 
of transformation control loops. The photographs in Figures 1 through 12 are sev­eral of the extreme 
positions taken from an anima­tion sequence involving a duck, a frog and the meeting of the two. The 
first four stills show the duck in a head-down, head-up position as it takes a drink of water. What the 
photos can't show is the duck wadling, wagging his tail, flap­ ping his beak, as well as changing his 
orientation (turning to one side then the other) and moving through space--all at the same time. The 
sched­ uled commands in the script can be given quite di­ rectly to control the transformations needed 
for this type of animation. The animator works on mo­ tions independently,component by component. In 
 the case of Figures 5 to 9, the animator created the frog and then intuitively "bent" the legs and 
 arms into the extreme shapes that make up jumping and swimming. Then, in the script, the animator 
decides what the timing will be to get the frog to change realisticallyfrom one shape to another. When 
this is settled, the animator may decide on when to turn the frog during the sequence. After that, 
how should the frog be moved to give the ef­ fect it is swimming. Here, acceleration and de­ celeration 
can be controlledby the animator to improve the quality of motion. The introduction of the duck, as 
seen in Figures 10 to 12 presents no difficulty to the user. Commands animating the duck and frog are 
given directly, in parallel and with no regard to mutual interference. When an animator is satisfiedwith 
the actions of the objects, he has the option of controllingthe whole scene. Commands are used similar 
to conven­ tional animation terms such as pan, tilt, zoom and field, which change the relationship of 
the ob­ server to the objects. Other features of the ani­ lighting-control in the form of independent 
posi­ mation language include color, brightness, fades, tion and rotation of multiple light sources 
and the ability to calculate a single frame or short animation segment within the script.  3.3 Animation 
Language When the objects have been made and the motion de­scribed, the animator need only evoke the 
language to calculate the final video sequence. The lan­guage processor, designed and implementedby Hackathorn 
(12), follows the user written script. It compiles an animation file which contains all the object and 
color parameters needed by the vis­ible surface routines next in the production proc­ess. If, for example, 
the script describes a sequence animating thirty multi-coloredblock let­ters and lasting for twenty 
seconds (600 frames), then the compiled animation file will look like a sequential list of six hundred 
dynamicallychanging data structures, each defining the spatial and dis­ play parameters of a collection 
of colored surfaces for one frame. The program tasks for the animation language is di­vided into four 
routines: preprocessor, scheduler, interpreter,and compiler.  3.3.1 Preprocessor The preprocessingroutines 
are concerned primarily with building the data structure, but also with the keyword parsing of the script 
syntax. This routine is controlled first by the prescene directives then by the scene directives of 
the user written script. The prescene directives instruct the preprocessor in the building of the data 
structure for the en­tire animation sequence. The data structure in­ cludes:  Face and vertice information 
describing the three-dimensionalpolygonal surface of each ob­ject in the sequence. The different possible 
shapes that any object can change into. Group pointers for each object. Sub-group pointers within each 
object (object parts). Multiple "floating" light sources.  Multiple, three-dimensionalpaths through 
space, sharable by all objects. A separate color for the inside and outside of every face for every 
object. The animation language currently can control up to 128 objects, groups of objects, or possible 
object shapes, however, the real limiting factor in the maximumcomplexity of the system is the 32 K ad­dress 
space of the PDP-11/45 CPU. The animation language uses 32 K of MOS memory and 32 K of core memory. This 
allows 20 K of object data space (4000 to 5000 unique edges) and elsewhere a 16 K section for buffering 
shape vertices and path ver­tices (about 5300 points at 3 words each). If there is room in memory for 
the data, the language can control 128 objects, 128 shapes, 64 groups, 32 paths and over 500 command 
instructions. The preprocessor routine parses, interprets, and executes each of the prescene directives 
until it  comes to a SCENE START directive in the script. For the remainder of the script the data structure 
is fixed, no new objects can be added, and the rou­ tine parses nothing, but scene directives. Each scene 
directive gets parsed and converted into a "command block," kept as part of a data list in  memory. 
A command block has all the parametric and key frame (schedule) information in it that was given in the 
directive line. It also contains pointers into the data structure, plus a workspace area big enough 
to hold the unique motion values which will change from frame to frame.  3.3.2 Scheduler The scheduler 
is the first of three routines which are evoked for each frame. The scheduling routine is event driven 
by the start of each new frame. Every frame it:  Sequences through each command block in the list, compiled 
by the parser. Judges whether the command block is flagged ac­tive or inactive after a comparison of 
the key frame informationin the command block and the current system frame counter.  Updates the motion 
parameters in the workspace area if the command is active this frame.  The scheduler routine works 
double duty by both scheduling the command blocks and updating the unique motion information that each 
block carries. It is at this state that the concepts of "set" and "change" become important. A "set" 
command block holds its initial parameters through its time range within the animation process. However, 
a "change" block has a direction initially calculated as spe­ cified by the animator with a "change 
to" direc­tive. From the first frame of activity,the direc­tion (an increment in X, Y,' and Z) of the 
"change" block will be added to the block's own internal workspace memory. These incrementing(positive 
or negative) parameters get interpretedand executed as if they belonged to a "set" commandblock. This 
 information is used, with no further modifications, by the interpreterroutines in doing the actual transformations 
to the data structure.  Interpreter After the command blocks have been scheduled for the current frame, 
the interpreter finds each ac­tive command,determines the parameter type (rota­tion, position, size, 
color, shape, path, etc.), and performs the necessarymotion or display trans­ formations to the data 
structures. The key to the interpreter is that for each new frame, all command blocks scheduled active 
will start their transfor­mation on the original data. In this manner, both the order of the commands 
and the range of their schedule determines what transformationswill be done to the data on any given 
frame.  3.3.4 Compiler The compiler routine compiles a data file as op­posed to executable code. The 
routine calculates the color of each face, does perspectivetransfor­ mations, clips all faces not seen 
by the observer and builds an animation file containing a complete scene descriptionof every frame in 
the script. The color of a face is a product of the relation­ship between the current positions of the 
light sources and the plane of the face. The system has three light sources which it keeps as X, Y, 
Z points in space and allows them to be translated and rotated just like objects. The distance each 
of the light sources is from the face, decides a weighted brightness. From this relationship a value 
between 1 and 224 is determined. This value corresponds to a color palette made up of 224 en­tries, each 
entry describing a fifteen bit red,  green, blue hue combination. The color palette is logically organized 
into eight intensity-chroma sections with 28 entries (the first entry is the darkest color and the last 
entry is the brightest). When the object is created in the data generation stage, it is "colored" by 
assigning one of the eight intensity-chroma sections to each face. With the information supplied by 
the light source calcu­ lations the final offset into the color palette is produced. The scene has 
a user-specified observer position. Every object has its own "pictureplane." With this information,the 
compiler routine calculates perspective. Each frame, the vertices after being transformedby the interpreter 
are projected onto a picture plane. The 'Z' axis coordinates are un­ affected by the perspective so 
that depth compari­ sons may be done later by the objects in memory as one object. It checks which faces 
can still be seen and appends the animation file with:  The faces in the object that are displayable. 
The colors of the displayable faces. The transformedvertices for the current frame. Miscellaneous displayparameters 
i.e., z-clip­ ping plane position, and background color. When the last frame of the script has been 
com­ piled, what is left is a data file on the system disk ready to be turned into the final color video 
 sequence by the visible surface algorithm and a raster-scan conversionroutine. Up to this point the 
calculation time has been relatively short. The only major calculations in the language are the dot products 
and face normals needed for the light source equations. As a result, the language typi­cally calculates 
a 300 frame (10 second) sequence in under 5 minutes. Through the script the animator may request that 
the animation file on the disk be played back (in real-time) to the Vector General. Since the transformations 
of the objects are already com­ pletely defined for every frame in the sequence, the V.G. playback routine 
has no computationre­quirements. This makes for an excellent way of previewing the animation sequence 
to get an idea about the motions, but of course no color or lighting information can be displayed. If 
V.G. output is not specifiedin the script, the lan­ guage automaticallyevokes the visible surface. 
 3.4 Visible Surface Algorithm The visible surface routine of the Anima II system is a version of the 
Myers Algorithm. Full imple­mentation details of the original algorithm may be found (24), but for completeness, 
a brief descrip­tion of the algorithm, as it affects the animation process will be discussed. The program 
uses 32 K of MOS memory, containing a data space of 20 K. At the beginning of each new sequence, the 
program reads a list of faces from the animation file left by the language. Here we note that the language 
has described all the ob­jects in the animation sequence to appear as one to the visible surface routine, 
also that all polygons createdwith the data generationroutines have been reduced to triangles. For 
each frame and starting with the first, the procedure is as follows. The face's information is read in. 
This contains faces clipped out of view, backfaces removed optionallyby the animator, and color for each 
displayed face. Next the list of unique vertice is read in as well as miscella­neous information such 
as background color. The program checks each face against the face file for this frame and if it is 
to be displayed (not clipped or "back faces" removed) the face is added to a list of faces whose highest 
'Y' value is iden­ tical to that of the current face. When all faces have been checked for displayability,the 
algorithm begins producing the visible surface output. As is typical of linear to raster conversion 
and visible surface algorithms, a scan line at a time is proc­ essed. Starting at the highest of the 
512 scan lines, lines are processed one line at a time until all lines are processed. Each line is 
processed as follows. If the line contains no active faces (i.e., no face starts, crosses or ends on 
the line) it is ignored. If the list of faces starting on the line is not null then all of the faces 
on the list undergo a format conversion and are added to a list of active faces. If the list of active 
 faces is not null then each face on the list is processed, one face at a time, in whatever order the 
faces on the list are in, until each active face has been processed. Processing a face means processinga 
segment of a face, since one scan line at a time is processed. Thus the list of active faces can be 
thought of as a list of segments to process on a scan line. The first segment of the list is scanned 
(i.e., con­verted to points). The 'Z' (distance from the ob­server) and intensity values for each point 
are stored in the appropriate places in the ZVSLS (Z values scan line structure) and IVSLS (intensity 
values scan line structure) respectively. Both the ZVSLS and IVSLS consist of 512 locations, each lo­cation 
of which corresponds to a horizontalposi­tion on the output raster. At each horizontal po­sition the 
'Z'value of the new point is compared with the 'Z'value of the point in the ZVSLS. If the new 'Z' is 
closer to the observer then both the ZVSLS and IVSLS values at the current horizontal position are updated 
with the values from the new segment. If the new 'Z' is farther or equal then no updating occurs. After 
processinga segment the corresponding active face is updated for the next scan line. If the lowest point 
of the face has been passed then the face is removed from the list of active faces. After processing 
all faces on the list of active faces for a scan line the scan line is converted into run length encoded 
binary data and stored a scan line at a time on the system disk. Given a typical animation sequence 
which contains polyhedra of around 1000 edges and covering an area of about one quarter of our TV monitor, 
the visible surface and raster scan conversion calcula­tion of a 300 frame (10 seconds) sequence takes 
be­tween 5 to 10 minutes. If the complexity doubles, but the area remains the same then the same se­quence 
will take 7 to 12 minutes. However, if the area doubles and the complexity remains the same, the sequence 
will take 10 to 20 minutes.  3.5 Display and Record Currentlywe are using a standard broadcast televi­ 
sion as the viewing mechanism,a large capacity di­gital disk for image storage and broadcast video for 
the raster-scan format image representation. The broadcast video is not stored in compositeNTSC format, 
but rather is stored as run-lengths of par­ticular intensity-chromacombinationswhich are converted (in 
real-time) to compositeNTSC format for display. The use of run-length encoding is our response to the 
insufficiency of current computer technology to easily handle the large quantities of information implied 
by raster-format representation of dynamic images. For example, a raster-format dynamic image of 512 
by 512 resolution, 8 bits per resolvable element information content, 30 frames per second display rate 
and 30 seconds duration re­quires over 235 million bytes of storage. The im­plied data transfer rate 
(8 million bytes per sec­ ond) is prohibitivewithin our general purpose design strategy. This is due 
to the fact that although disks of over 200 million byte capacity are available, the transfer rate available 
is less than 2 million bytes per second. The run-length decoding and analog systems were constructed 
by Dr. John Staudhammer and his asso­ ciates (DIGITEC,Inc.; Box 5486; Raleigh, N. C. 27607). The analog 
system and rearend of the run­length decoding system are similar to an earlier system built under Staudhammer's 
direction. The decoding system converts our run-length format to that used in Staudhammer's earlier system. 
(30) A dynamic sequence is transferred from the disk to the TV according to the following scheme. A 32 
KB run-length buffer is divided into two 16 KB buffers for double buffering. A buffer is filled from 
the disk. While this buffer is being filled, informa­ tion to/from the disk controller from/to the CPU 
must be multiplexed with the data from the disk.  This multiplexing is automaticallyhandled by the UNIBUS 
priority arbitration unit. Fortunately, the quantity of control informationnecessary to run the disk 
is a small percentage of the quantityof  data being transferred. Also fortunate is the fact that the 
dual ported MOS main memory permits the instructions and associated data of the control program to 
be fetched simultaneouslywith the data being stored from the disk. Thus, there are vir­tually no memory 
cycles lost directly to the con­ trol program.  Information flowing into the run-length decoding system 
is buffered in an internal 32 KB MOS buffer before it is decoded. This is the reason that in­formation 
may be transferred from the MOS main mem­ory buffer into the decoding system with no concern for field 
or frame boundaries. More explicitly, since field and frame boundary informationis con­tained in the 
data, putting off decoding the data until after information transfer permits the data to be treated as 
a uniform stream. The calculations below are intended to give a quan­ titative indication of the capabilities 
of the sys­ tem. It should be noted that in order to provide the clearest calculations, minor overheads 
such as start of field instructions are ignored. The following calculations assume an average of one 
byte per run. This case is approachedfor images with (typically) fewer than 33 intensity-chroma combinationswithin 
a scan line and fewer than 25 within a field. The disk specifications are those of the manufacturer. 
Since the RJPO4 disk system is (relatively)the slowest part of the system, it determines the maximum 
performance level. For con­tiguously stored files (as video files have to be in this system) the disk 
can be read continuously at maximum possible speed with the exception that some time (7 milliseconds) 
is lost when changing cylinders. Since there are 19 tracks per cylinder and the disk requires 16.7 
milliseconds for one revolution,one cylinder can be ready every 317 milliseconds. Since 214,016 bytes 
are stored per cylinder, the average data transfer rate is 675 bytes per millisecond. Allowing 10 milliseconds 
for change of cylinder, 207,266 (214,016 minus 6,750) bytes can be obtained for every cylinder read. 
Note that the storage space "passed over" for change of cylinder is best wasted as an extra revolutionwould 
be required to retrieve it. Thus, the average data transfer rate is 654 (207,266di­vided by 317) bytes 
per millisecond. Since each TV frame lasts about 33 milliseconds,this is 21,582 bytes per frame. At one 
byte per run, this is 21,582 runs per frame. Since the disk has 411 cy­linders and the system is retrieving 
207,266 bytes per cylinder, there are 85,186,362 retrievable bytes. At a maximum of 21,582 runs per frame, 
this represents 3,947 frames. Since the data is contig­uous, any reduction in runs per frame directly 
translates into more frames. Thus, at 2,158 runs per frame there are 39,470 frames. (25) 4. Conclusion 
 The development of computer generated 'solid' ob­ject animation is changing the way an animator ap­proaches 
the documentation of an idea. Convention­al animation involves drawing and redrawing planar images on 
each frame throughout the entire se­quence. Image creation and image animation are very often the same 
process. But in a 3-D computer animation environment,the user first builds a col­ored object then animates 
it and these processes are separate. The approach of 3-D color animation is similar to that found in 
other disciplines such as Cinematography,Theatre and Choreography. Here actors or dancers are chosen 
and given their roles by a director who is responsible for the whole show. The approach is closer still 
to that of pup­pet animation in which the work if Jiri Trinka, Willis O'Brien (King Kong) and Jim Hensen 
with his Muppets serves as excellent examples. The implementationof such an animation system re­ quires 
balancing system requirements against the available resources, while at the same time keeping some 
notion of efficiencyin mind. Anima II, has been developed and implemented as one solution to the production 
of 3-D color animation. Each of the subsystems in Anima II have been especially design­ ed to both interact 
freely with a user and inte­ grate transparentlyinto a unified system. While sitting at one work station, 
a user of Anima II can create, animate and display 3-D colored objects, then directly record the animation 
onto standard video cassette tape. The system has limitations in  the areas of data complexity: 5000 
unique edges ACKNOWLEDGEMENTS per scene; and data transfer: limited mainly by the system disk which can 
transfer about 20,000 The Anima II system was designed and implemented bytes (1-3bytes per run-length) 
each video frame. by the Computer Graphics Research Group at The Currentlymethods are being explored 
to improve Ohio State University. Work on the project was these areas and the areas of image quality 
and to-performed by Charles Csuri (Director), Allan Myers, tal system throughput. Richard Parent, Timothy 
VanHook, Diana Rainwater, and the author. Funding for this project was pro­vided by NSF Grant DCR 74-00768. 
 REFERENCES <RefA>1. Appel, A., Stein, A., Landstein, J. (1970). The Interactive Design of Three-Dimensional 
Animation,Proceedings of the Ninth Annual UAIDE Meeting.  2. Archuleta, Personal Communication with 
CGRG.  3. Baecker, R. M. (1969). Interactive Computer MediatedAnimation. Dissertation,Massa­chusetts 
Institute of Technology.  4. Baumgart, B. G. (1974). Geometric Modeling for Computer Vision. Dissertation, 
Stan­ford University. NTIS Report Number AD/A­002261.  5. Belady, L. (1970). TV Plus Computer Equals 
Videographics. Proceedings of the Ninth Annual UAIDE Meeting.  6. Blasgen, M. W., Gracer, F. (1970). 
KARMA: A System for Storyboard Animation. Proceed­ings of the Ninth Annual UAIDE Meeting.  7. Blinn, 
J. F., Newell, M. E. (1976). Texture and Reflection in Computer Generated Im­ages. Communications of 
the ACM, Vol. 19, No. 10.  8. Braid, I. C. (1975). The Synthesis of Solids Bounded by Many Faces. Communications 
of the ACM, Vol. 18, No. 4.  9. Catmull, E. (1974). A Subdivision Algorithm for Computer Display of 
Curved Surfaces. Tech. Report UTEC-CSC-74-133,University of Utah.  10. Clark, J. (1976). HierarchicalGeometric 
Mod­els for Visible Surface Algorithms. Com­munications of the ACM, Vol. 19, No. 10.  11. Csuri, Charles 
(1975). Computer Animation, Proceedings of the Second Annual Conference on Computer Graphics and Interactive 
Tech- niques--SIGGRAPH '75.  12. Csuri, Charles A. (1977). 3-D Computer Ani­mation. Advances in Computers. 
Academic Press, Inc., New York.  13. Davis, J. R. (1968). A Model Making and Dis­play Technique for 
3-D Pictures, Pro­ceedings of the Seventh Annual UAIDE Meet­ing.  14. Elin, L. (1975). Synthevision: 
Serendipity from the Nuclear Age, Artist and Computer, edited by R. Leavitt, HarmonyPress.  63  15. 
Elin, L. (1977). Presented at National Con­ference and Workshopon Electronic Music and Art, University 
of Buffalo, Suny.  16. Film -"WalkingMan," Universityof Utah.  17. Film -"NASA Space Shuttle" General 
Electric.  18. Film -"Sphere Eversion" N. Max.  19. Gattis, W., Watson, (1971). An Input Transla­tor 
for Animation and Its Relationship to Key Position Character Animation. Proceed­ings of the Tenth Annual 
UAIDE Meeting.  20. Goldstein, R. (1971). A System for Computer Animationof 3-D Objects, Proceedings 
of the Tenth Annual UAIDE Meeting.  21. Gouraud, H. (1971). Computer Display of Curved Surfaces. IEEE 
Transaction on Computers.  22. Jones, B. (1976). An Extended ALGOL-60 for Shaded Computer Graphics. 
Proceedings ACM Symposium on Graphics Languages.  23. Lafue, G. (1975). Computer Recognition of 3- 
 Dimensional Objects from Orthogonal Views. Research Report No. 56, Institute of Phys­ical Planning, 
Carnegie-MellonUniversity.  24. Myers, A. J. (1975). An Efficient Visible Surface Program. Technical 
Report to the National Science Foundation,Grant Number DCR 74-00768AO1.  25. Myers, A. J. (1976). A 
Digital Video Infor­mation Storage and Retrieval System. Pro­ceedings of the Third Annual Conference 
on Computer Graphics and Interactive Tech-  26. Max, N., (1975). Computer Animation of the "Sphere 
Eversion," Proceedings of the Sec­ond Annual Conference on Computer Graphics --SIGGRAPH.  27. Negroponte,N. 
(1973). Recent Advances in Sketch Recognition. Proceedings of the National Computer Conference.  28. 
Newell, M. (1975). The Utilization of Proce­dure Models in Digital Image Synthesis, Ph.D. Dissertation,Universityof 
Utah.  29. Parent, R. E., Chandrasekaran,B. (1976).  Moulding Computer Clay. Pattern Recogni­tion 
and Artificial Intelligence. (C. H. Chen, Ed.) Academic Press, Inc., New York. 30. Staudhammer, J., 
Eastman, J. F. (1975). Com­ puter Display on Colored Three-Dimensional Object Images. Proceedings of 
the Second Annual Symposium on Computer Architecture, pp. 23-27. 31. Sutherland, I. E., Sproull, R. 
F., Schumacker, R. A. "A Characterizationof Ten Hidden- Surface Algorithms," ACM Computing Surveys, 
Vol. 6, No. 1, pp. 1-55, 1974. 32. Sutherland, I. E. (1974). Three-Dimensional Data Input by Tablet. 
Proceedings of the IEEE. Vol. 62, No. 4, pp. 453-462. 33. Watkins, G. S. (1970). A Real-TimeVisible Surface 
Algorithm. University of Utah Technical Report UTEC-CSC-70-101.  34. Wein,M., Burtnyk,N. (1971). A Computer 
Ani­mation System for the Animator. Proceed­ings of the Tenth Annual UAIDE Meeting.  35. Wein, M., Burtynk,N. 
(1975). Computer Anima­  tion of Free Form Images. Proceedings of the Second Annual Conference on Computer 
 Graphics and Interactive Techniques-- SIGGRAPH '75. 36. Whitney,John, Citron, J. (1968). Camp-Com­puter 
Assisted Movie Production,Proceed­ ings of the AFIPS Fall Joint Computer Conference.  </RefA>
			
