
 Analysis of the in Distributed Parallel peris@src.umd.edu Abstract We consider an important tradeoff 
between prcJces­sor and memory allocation in distributed parallel pro­cessing systems. To study this 
tradeoff, we formulate stochastic models of parallel program behavior, distrib­uted parallel processing 
environments and memory cwer­heads incurred by parallel programs as a function of their processor allocation. 
A mathematical analysis of the models is developed, which includes the effects of contention for shared 
resources caused by paging activ­ity. We conduct a detailed analysis of real large-scale scientific applications 
and use these results to parame­trize our models. Our results show that memory over­head resulting from 
processor allocation decisions can have a significant effect on system performance in dis­tributed parallel 
environments, strongly suggesting that memory considerations must be incorporated in the re­source allocation 
policies for parallel systems. We also demonstrate the importance of the inter-locality ]miss ratio, 
which is introduced in this paper and analyzed for the first time. 1 Introduction Parallel processing 
systems comprise an important class of computing environments that make it possible to solve large, complex 
problems which are otherwise in­ *The work of this author was partially supported by NASA Grant #NAGW-2777S. 
Permission to copy without fee all or part of this material is granted provided that the copies are not 
made or distributed for direct commercial advantaqe, the ACM copyright notice and the title of the publication 
and Its date appear, and notice is given that copying is by permission of the Association of Computing 
Machinery. To copy otherwke, or to republish, requires a fee and/or specific permission. SIGMETRICS 94-5/94 
Santa Clara, CA. USA @ 1994 ACM 0-89791 -659-xKWO005..$3.5O  Impact of Memory Vinod G. J. Peris* Institute 
for Systems Research University of Maryland College Park, MD 20742 Processing Systems . Mark S. Squillante 
and Vijay K. Naik IBM Research Division T. J. Watson Research Center Yorktown Heights, NY 10598 {mss,vkn}@watson. 
ibm.com tractable. The general problem of scheduling, and par­ticularly resource allocation, in these 
systems is a key factor in realizing the potential benefits of parallel pro­cessing. The design and analysis 
of policies that define the manner in which processors are allocated among the parallel programs submitted 
for execution has been an active area of research over the past several years. Ma­jumdar, Eager and Bunt 
[11], Sevcik [21], Leutenegger and Vernon [9], and Zahorjan and McCann [27] con­ducted (primarily) simulation-based 
studies of differ­ent aspects of processor allocation in uniform-access, shared-memory (UMA) environments 
under a wide va­riety of workload assumptions. Similar issues were addressed by Tucker and Gupta [25] 
and McCann, Vaswani and Zahorjan [14] via implementations of dif­ferent parallel applications running 
under various sched­uling policies on actual UMA systems. Makowski and Nelson [12, 13] used techniques 
of stochastic ordering and stochastic majorization to derive the structure of the individually optimal 
policy in a class of distributed parallel processing systems under general assumptions for task service 
times and the number of tasks per job. Naik, Setia and Squillante [17, 16] conducted studies, based on 
simulation and analysis, of different aspects of processor allocation in another class of distributed 
par­allel systems under a workload representative of large computational fluid dynamics applications. 
One of the many important results from this body of research is that the optimal processor allocation 
size (i.e., the number of processors allocated to jobs that yields the best system performance) tends 
to decrease with increasing system loads, assuming each processor has infinite memory resources. This 
is shown explicitly in [21, 12, 13, 17, 16], and it can be inferred from the re­ sults presented in [25, 
9, 27, 14]. The (negative) impact of program inefficiencies (due to factors such as synchro­nization, 
communication, data ftask dependencies and load balancing) on overall system performance tends to rise 
with increasing traffic intensities. As the system utilization rises queueing delays represent increasingly 
larger fractions of a job s response time, all else being equal. Therefore to achieve the objective of 
minimiz­ing mean response time it-becomes increasingly more important to maximize the efficient use of 
processors (thus reducing delays incurred by waiting jobs) than to complete the execution of a job somewhat 
sooner (and not sufficiently utilize the processors) with increasing system loads. In distributed parallel 
processing systems, each processor has its own local memory with finite capacity. The working set size 
of a job on each of the proces­sors allocated to it increases with reductions in the size of this allocation, 
since the job s memory requirements are spread over fewer processing nodes. Reducing the number of processors 
allocated to a sufficiently large job below a certain point can cause excessive memory over­head as the 
job s working set on each node no longer fits in the local memory, thus increasing the number of page 
faults incurred by the job. While previous scheduling work for parallel systems suggests decreasing processor 
allocations with increasing system loads, memory con­siderations in distributed environments tend to 
argue in the opposite direction. Hence, there is a fundamen­tal tradeoff between the impact of program 
efficiency toward allocating fewer processors to jobs at heavier loads and the performance impact of 
memory require­ments toward larger allocations. It is this important tradeoff that motivates our study. 
Our primary objective in this paper is to develop an­alytic modeling methods that support the study of 
the above scheduling tradeoff. This consists of the stochas­tic analysis of processor and memory allocation 
in dis­tributed parallel systems. We derive expressions for the memory overhead incurred by a program 
as a function of its processor allocation, the mean job response time under these allocation decisions, 
and the effects of con­tention for shared resources caused by paging activity. To exercise these methods 
and demonstrate their utility, we also conduct a detailed analysis of actual large-scale scientific applications 
and use these results to parame­trize the models developed herein. This includes the analysis and simulation 
of well-known scientific bench­mark programs to obtain their memory and processing requirements. Our 
results show that memory overhead resulting from processor allocation decisions can have a significant 
effect on system performance in distributed parallel environments, strongly suggesting that mem­ory considerations 
must be incorporated in the resource allocation policies for parallel systems. We also demon­strate the 
importance of the inter-locality miss ratio, which is introduced in this paper and analyzed for the first 
time. In Section 2 we further motivate and define the problems considered. Our mathematical models and 
analysis are presented in Section 3. We then use these methods in Section 4 to study an important class 
of parallel applications. Finally, we conclude in Section 5. 2 Problem Motivation Our work is motivated 
by the scheduling trade­off described in the introduction as it arises in high-performance, distributed-memory 
parallel pro­cessing systems executing large-scale scientific appli­cations. These environments are characterized 
by computationally-intensive parallel applications, with relatively large data sets, that arrive at various 
inter­vals to systems consisting of a relatively large number of processors. Such parallel processing 
workloads are typically comprised of a mix of different jobs with sig­nificant variations in resource 
requirements, which are often unpredictable [20]. The numerical computations performed by these applications 
tend to be iterative in nature. In addition to the large and variable process­ing demands of these parallel 
jobs, the large amounts of data on which the computations are performed result in large (and variable) 
memory requirements. Parallel processing workloads of this type place sig­nificant demands on both the 
processing and memory resources of the computer system. The resource allo­cation decisions made by the 
system scheduling policy can have a major impact on these demands, which in turn can have a significant 
effect on system performance. While considerable progress has been made in obtaining a better understanding 
of different aspects of processor allocation, much less is known about the interaction be­tween processor 
and memory allocation in parallel pro­cessing systems. Our goal in this paper is to develop formal methods 
that support the analysis of processor and memory allocation in distributed parallel systems. The memory 
reference behavior of programs is an important aspect of this research. In the 1970s sev­eral empirical 
studies independently revealed important properties of program behavior [23, 7, 10, 8]. These studies 
showed that programs tend to consist of phases which clearly dominate the fraction of total memory ref­erences 
and transitions which account for a consider­able fraction of the program s page faults. Phase peri­ods 
tend to be of much longer duration than transition periods, but both phases and transitions are of equal 
importance in the behavior of programs. Subsequent studies confirmed and refined the properties established 
for program behavior. The research literature in this area is quite rich, and we refer the interested 
reader to [22, 8] for more details. We note that these established properties appear to also hold for 
an important class of parallel applications (see Section 4). Focusing on the importance of locality in 
programs, Courtois and Vantilborgh [7] proposed a nearly com­pletely decomposable model of program behavior 
that consists solely of disjoint locality phases. An approxi­mate solution of the model is obtained using 
the meth­ods of decomposition and aggregation [6]. This solution is then used to estimate the miss ratio 
of a program for a given page replacement policy and set of memory para­meters. As noted above, however, 
transitions between different phases are just as important as the concept of locality and thus must be 
included in the model of program behavior. Moreover, programs can have over­lapping localities [22, 15] 
and thus a model with disjoint locality phases can be too restrictive. More recently, Aven, Coffman and 
Kogan [1] de­scribe a method to obtain a nearly completely decom­posable program model that can include 
sets of transi­tion page references not belonging to any locality phase as well as overlapping localities. 
The model is solved via extensions to the Simon-Ando approximations [6] for the stationary probabilities 
of the corresponding Markov chain. This solution is then used to obtain pro­gram miss ratios, which are 
computed as weighted sums of the miss ratios for the individual locality phases where the weights correspond 
to the long-run proportion of time the program spends in the respective localities. The iterative nature 
of the parallel applications of interest cause program localities to be visited multiple times in some, 
potentially non-deterministic, sequence. This aspect of parallel programs is not taken into ac­count 
in any of the previous models of program behav­ior. However, the impact of this behavior on system performance 
can be quite significant, as we will demon­strate in Section 4. The reason for this is easily ex­plained 
with a simple example. Consider a program consisting of multiple localities whose sizes are individ­ually 
less than, but cumulatively greater than, the sys­tem memory capacity. Previous modeling apprc+aches, 
including those in [7, 1], will yield a miss ratio of zero in this case because the miss ratio for each 
locality is zero and thus the weighted sum (for any set of weights) is zero. However, if the program 
iterates between the different locality phases then it can experience a signif­icant memory overhead 
as potentially large portions of each locality are reloaded upon return to the locality. We develop a 
reahstic and tractable, nearly com­pletely decomposable model of parallel program behav­ior that includes 
overlapping localities, transitions be­tween localities and multiple visits to localities. Our ap­proach 
introduces the notion of the inter-locality miss ratio which includes the memory overhead caused by the 
reloading of pages upon the return to each locality, as well as a more accurate representation of the 
page faults due to transitions between localities. We develop efficient techniques for approximating 
the total memory overhead incurred by a parallel program as a function of its processor allocation, for 
which known error bounds can be obtained [24]. We then continue our development of analytic modeling 
methods to support the study of the scheduling tradeoff described in the introduction, and investigate 
this tradeoff for an important class of parallel applications. 3 Models and Analysis In this section 
we focus on the stochastic analysis of processor and memory allocation in distributed paral­lel processing 
systems. We describe our memory, sys­tem and workload models in turn. We do not present our approach 
for modeling the impact of increased con­tention for shared resources caused by paging activity due to 
space considerations, but instead refer the reader to [18]. While our approach is motivated by different 
aspects of the distributed parallel environments of in­terest, it is important to note that our methods 
apply more generally. 3.1 Memory Models The memory models developed in this section are used to approximate 
the paging overhead incurred by a pro­gram on a single processor. This consists of modeling the memory 
reference behavior of the program and de­riving approximations for the miss ratio realized by the program 
when executed with a finite capacity memory of M pages. To help clarify the presentation, we di­vide 
the page faults incurred by a program into two different classes. The first consists of faults that occur 
within each locality, whereas the second class of page faults consists of those resulting from transitions 
be­tween localities and from the iterative nature of parallel programs in which localities are visited 
multiple times in some (non-deterministic) fashion. We refer to the miss ratios due to these different 
types of page faults as the intra-localzty and inter-locality miss ratios, respec­tively, and we address 
each type of miss ratio separately. These methods are then used toapproximate the total memory overhead 
incurred by a parallel application as afunction of its processor allocation, which is described in Section 
3.2. The page replacement policy defines which page cur­rently residing in memory will be replaced upon 
the ref­erence to a page that is not in memory, and thus it deter­mines the set of pages that are in 
memory at any given time. Computer systems typically use (or approximate) the least recently used (LRU) 
replacement algorithm in which the page chosen for replacement is the one that has not been referenced 
for the longest time. Several recent distributed parallel processing systems support an LRU page replacement 
scheme and it is expected that this trend will continue. One example is the IBM SP-1 machine where each 
node runs the AIX* operating system. The analysis presented in this paper therefore assumes an LRU page 
replacement algorithm. 3.1.1 Program Behavior As noted in Section 2, there has been a significant amount 
of research concerning the stochastic modeling of the page reference behavior of programs [22, 8]. The 
independent reference model (IRM) has many advan­tages from the mathematical modeling viewpoint, but 
this model can produce very poor approximations in practice because it does not capture any concept of 
lo­cality. In fact, IRM goes to the other extreme by as­suming that the probability of the next page 
reference is independent of the page currently being referenced. The LR U stack model (LRUSM), on the 
other hand, specifies the distribution for the inter-stack distances in an LRU stack. While this model 
can be parameterized to give a high probability for referencing the pages at (or near) the top of the 
stack, it still does not capture the idea that there could be sudden jumps between disjoint localities, 
which has been empirically shown to occur in real programs [10], nor does it capture changing locality 
sizes. In our study, we use the itfarkov Teference model (MRM) of program behavior because it captures 
quite well the concepts of program locality and dynamic tran­sitions between localities during program 
execution. The page reference string generated by a program, denoted by a sequence X(n), n >0, is assumed 
to be a realization of a time-homogeneous Markov process with irreducible page transition matrix (Pij 
) where Pzj=p[x(n+ l)=j]x(n)=i]. * AIX is a registered trademark of the IBM Corporation. The miss ratio 
is defined as the ratio of the total number of page faults to the total number of references, If we define 
LRU(n) to be the set of all pages in the LRU stack after the n+ reference, then we can calculate the 
page fault probability as J~mm P [X(n + 1) @ LRU(n)] . Assuming the measurement interval to be sufficiently 
long, the miss ratio converges to the page fault proba­bility. Computing the page fault probability directly 
from the probability transition matrix can be prohibitively expensive both in terms of time and space. 
This is due to the fact that the computation involves a sum over each of the states of the LRU stack 
[1], which grows ex­ponentially. This direct approach, however, ignores the important aspects of program 
behavior noted in Sec­tion 2. The structural properties of this behavior result in a type of system that 
has been termed neady com­pletely decomposable by Simon and Ando [6]. We reduce the complexity of the 
problem at hand by exploiting these structural properties to facilitate our analysis of the dynamic behavior 
of parallel programs. Assume for the moment that the program localities are disjoint and that no additional 
pages are referenced in transition from one locality to the next. We first arrange the rows and columns 
of the probability tran­sition matrix P = (P,j ) such that the pages of each locality are grouped together 
consecutively. We then block partition P according to these locality sets, which results in the following 
structure for the matrix P where PIJ describes transitions from pages in locality It to pages in locality 
J, 1 <1, ,7 < L, and L denotes the number of program localities. It follows from the local­ity property 
of program behavior that the submatrices along the main diagonal (i.e., P1l, . . . . PLL) consist of 
relatively large probability measures, while the elements of the other submatrices are very small in 
comparison (i.e., PIJ % [0], J # 1). Matrices of this type are called nearly completely where Q* = diag(Q~, 
Q;, . . . . Q;), the matrices Q; are decomposable [6]. In general, the matrix P can be writ­ ten in 
the form P= Q*+cC (1) tA capital letter is used for a single locality while lower-case letters are used 
for individual pages. stochastic, 1 < 1 < L, e is small compared to the el­ements of Q , an d Icij I 
< 1. The matrix Q is said to be completely decomposable because all elements off the main diagonal of 
submatrices are zero. We refer the interested reader to [6] for a comprehensive treatment of completely 
and nearly completely decomposable ma­trices and their solutions. Intuitively, we decompose the probability 
transition matrix P into a macro model and L individual micro models. This notation is consistent with 
the use of macro and micro models in [8]. The key idea is that the macro model characterizes the transitions 
between localities, while each micro model characterizes the ref­erences inside a particular locality, 
From this perspec­tive, the matrices Q;, 1 < 1 < L, are the probability transition matrices for each 
of the individual micro mod­els. The macro model is defined by the matrix (~IJ ) of dimension L x L whose 
elements represent the proba­bility of referencing some page in locality J on the next memory reference, 
given that the current reference is to some page in locality 1. More formally, we have   PIJ=p[x(~+ 
l) EJlx(n)EI]. Note that the subscripts are capitalized to emphasize that they refer to the macro model. 
Letting @l denote the invariant distribution vector of the matrix Q;, 1< 1< L, the elements ~IJ are then 
computed as In the interest of space, we do not describe here our approaches for adding within the above 
framework the case of overlapping localities and the set of pages that are referenced in transitions 
between localities. We note that the page faults due to transitions are included in our analysis of the 
macro model to approximate the inter-locality miss ratio (see Section 3.1.3). These and other technical 
details can be found in [18]. As a consequence of the Simon-Ando theorems [6], we can make the following 
important observations re­garding the model of program behavior derived above. In the short-term period, 
equilibrium is (approximately) reached separately by each locality Q;. These equi­librium states are 
(approximately) preserved over the long-term period such that the distribution of references among the 
different localities approaches the distribu­tion fl = (T1, Tz,. . .,~r, . . ., fiL), where TI is the 
steady-state probability that a memory reference is di­rected to locality 1. 3.1.2 Intra-Locality Miss 
Ratio Given a nearly completely decomposable model of pro­gram behavior with irreducible page transition 
matrix P = (Pij ) of the form presented in equation (l), we compute the intra-localit y miss ratio Rwt 
(i.e., the miss ratio within a locality) under the LRU page replacement algorithm as follows. Let F(P, 
ikf) denote the station­ary page fault probability for a program model P and a finite memory of M pages. 
More formally, F(P, ikf)=J~mm P[X(n+ 1)= i I i @ LRU(n)] . It follows from the properties of the nearly 
completely decomposable matrix P that the page fault probability can be approximated as L F(P, M) w 
~7TrF(Q;, M), (3) 1=1 the accuracy of which is known to be (at least) within O(c) [6]. Due to the locality 
property exploited in the construction of the matrix P, such errors are expected to be very small. The 
intra-locality miss ratio is then ob­tained from equation (3) by recalling that RW1 converges to 3 (P, 
M) for sufficiently long measurement intervals. The intra-locality miss ratio of the program model P 
is therefore obtained by solving L much smaller mi­cro models to calculate their corresponding miss ratios 
(i.e., ~(Qf, ~), 1 < I < L) and combining these re­ sults with the solution to the macro model (i. e., 
T). If the matrices Q; are fairly small, then the intra-locality page fault probability can be computed 
by analyzing the Markov chain over the state space of all possible LRU stacks. While this Markov chain 
is also irreducible, since the Q; are irreducible, and a closed-form solution exists for the page fault 
probability, the state space can become prohibitively large even for moderate sized Q; matrices. It is 
important to note, however, that there is significant evidence [1, 8] suggesting that in the short­term 
(micro model) the IRM and LRUSM assumptions may be satisfied. We note that closed-form expressions exist 
for # (Q, M) when Q corresponds to the IRM or LRUSM [1]. Equation (3) is a very valuable result in that 
it greatly simplifies the computation involved in calculat­ing Rut. This approximation, however, does 
not ac­count for the page faults generated during repeated vis­its to each locality. The importance of 
these effects were mentioned in Section 2, but now the cause is even more clear. Consider a program whose 
every locality fits in memory, however the total number of pages far exceeds the capacity of memory. 
From equation (3) the miss ratio works out to be 0, which is clearly not the case. We capture this important 
aspect of program behavior in the inter-locality miss ratio.  3.1.3 Inter-Locality Miss Ratio We develop 
an approximation to the inter-locality miss ratio Rat (i.e., the miss ratio across localities) under 
LRU page replacement based on the macro model. Since the program reference behavior is assumed to be 
Mar­kovian, the process repeats itself (i.e., regenerates) on each return to locality 1. Thus, the question 
that we need to consider is: On Tetum to locality I, what is the numbeT of pages in locality I that must 
be Teloaded? We define for 1~ .l ~ L # of pages in locality 1 that have CI = E to be reloaded on return 
to I 1 elapsed time between first entry  T;= [ E [ into 1 and the next return to 1 1 The inter-locality 
miss ratio can then be computed as ~~=1 C1 x number of returns to locality 1 Rat = total number of references 
total number of references = ~;=, CI( . Tr ) total number of references = g~ Multiplying this miss ratio 
by the total number of refer­ences yields the expected number of pages in locality 1 that must be reloaded 
on return to 1. All that remains is to compute the values of T; and CI, 1 ~ 1 ~ L. 3.1.4 Computation 
of 7} We can calculate T; from the classical definition of mean recurrence time. Let Y(n), n ~ O, be 
a Markov chain with one-step transition matrix corresponding to the macro model of Section 3.1.1, defined 
over the state space S$ = {1, 2, . . . L} of localities. The mean recur­rence time to state I, TX, is 
then defined as CO TI=~nP IY(n)=I, Y(v)#I, o<v< nlY(o)= I]. ?3=1 (4) i A bold capital letter is used 
for a set of localities while normal capital letters are used for individual localities. Note that TI 
is not the quantity we are inter­ested in because it includes events of the form [Y(1) = 1 I Y(0) = 1] 
which clearly do not imply any change of locality. What we actually need is T~ which is defined by r; 
= ~nPIY(n)=I,Y(v) #I,, <v<nl n=l Y(1) # I, Y(o) = 1]. We can rewrite equation (4) as 71 = PIY(l) =1 
I Y(o) =1] + ~nPIY(n) =I,Y(v) #I, o<v<nl Y(o)=I] n=2 m .x  FII+~{n p[y(l)#IIY(o)=I] ?3=2 P[Y(n) = 
I,Y(v) # 1,1< v < n I Y(1) # I,Y(o) = 1]} = ~11 +~;(l pH), from which it follows that ~~ I = TI jlI 
(5) l ?I1 For a recurrent Markov chain, TI = &#38; where ~1 is the invariant distribution for the Markov 
chain Y(n). Substituting for Tr in equation (5) yields ~q = l 7i_I~II I 7TI 7TI~11  3.1.5 Computation 
of Cl The computation of CI is slightly more complicated. In what follows we compute an upper and lower 
es­timate of CI. To illustrate our approach, consider a particular sample path that takes us through 
the local­ities K1,Kz, . . . . K,, in any order and any number of times, before returning to locality 
1. Since P is an ir­reducible and nearly completely decomposable Markov reference matrix, it is reasonable 
to assume that the program spends enough time in each of the localities K1, K2, ..., K., to have referenced 
all of their pages at least once. Let G = {K1, K2, . . .KT}, and define \lG\l to be the number of distinct 
pages in all of the localities comprising G. Note that if all of the localities in G were referenced 
at least once before a return to locality 1, then \IGil pages would have been loaded in memory. If ]IGII 
+ 11{1}11 s M, no pages need to be reloaded on n 2 return to 1, whereas if Ill 11{1}11 < /lGll < M, 
then the number of pages to be reloaded on return to locality 1 is between l/Gl[ + 11{1}11 M and 11{1}11. 
Finally, if IIGII z M, then all 11{1}11 pages have to be reloaded on return to 1. More precisely, we 
define a lower estimate of the cost aj(G) as o IIGII+ 11{~}11  s J f (Y:(G) = IIGII+ Il{l}lt M M 11{1}11 
< IIGII < M llG1l > M { IHI}II Similarly, we define an upper estimate a~(G) as o IIGII + 11{1}1[ < M 
CX;(G) = 11{1}11 otherwise { Now let &#38;I denote the probability that upon leaving locality 1, all 
(and only) the localities in G are visited in any order and any number of times before returning to locality 
1. More formally, J:) = PIY(n)= l,{ Y(v) :l<v<n}=Gl Y(1) #I,Y(o) =1] = ~ ~c:) . G(II n>l With this formulation 
we can compute an upper and lower estimate for CI as follows Note that the summation in equation (6) 
can be reduced to only those G for which aU(G) >0. To calculate the &#38;l we need to define a quantity 
similar to taboo probabilities [5], Consider a set H of localities (we are actually interested in H = 
G ). For all J c S\H$ and n ~ 1, define ~p$) = PIY(n)= l, Y(v)@ H,l<v <n, Y(1) # J IY(0) := J] and ~pJI 
= (7) E ) - HPJI n~l We then can decompose the HP$) by the method of jirst entrance (see [5]) as follows 
K~H =0 K#J $A\B is the set of elements that belong to A and not to B. E FJK (lo) HPKI . KCH 1 j~~ K#J 
 Rewriting the left hand side of equation (10) in terms of equation (7) yields a system of II S\Hl I 
equations which we can solve for the same number of unknowns. That is, for each J c S\H To compute ~&#38;I 
we actually need ~PIY(n)=l,Y(v) @H,l<v<nl Y(l)#l, n~2 Y(o) = 1] #11  PIY(l)#I I Y(0)=I] HPII (11) ~ 
 Substituting H = G in equation (11), we can calculate G(II via the following recursion Hence, we compute 
the upper and lower estimate for C r by substituting equation (12) in equation (6). Note that in (12) 
we only need to compute GfZI for those G for which au(G) > 0. We also note that one can often obtain 
a more accurate approximation for C1 by exploiting properties of the matrix Q; in the above analysis. 
In fact, exact values for CI are obtained for the parallel applications in Section 4. 3.2 System and 
Workload Models We consider a distributed parallel processing system that consists of P identical processors 
each having its own local memory. It is assumed that the system proces­sors can be partitioned arbitrarily, 
and that a general interconnection network supports information sharing among the processors. Arrivals 
to the system are rep­resentative of large-scale scientific applications, and are assumed to come from 
a Poisson source with mean ~. The processing and memory requirements of jobs depend upon the number of 
processors allocated to them by the system scheduling policy. We restrict our atten­tion to static processor 
allocation policies under which the system processors are divided into a fixed number of disjoint sets, 
or partitions, of equal size. This ap­proach allows a tractable model formulation of the par­allel system 
and allows us to focus on the issues at hand. Moreover, recent studies [17, 16] of scheduling in parallel 
systems under workloads typical of large-scale scientific computing suggest that adaptive partitioning 
performs as well as the optimal static policy for a given value of A, and thus our results can be applicable 
to adap­tive as well as static partitioning. These studies also show that distributed parallel systems 
can achieve bet­ter performance when long-running jobs are scheduled according to a particular class 
of dynamic partitioning policies. However, even in these cases, our analysis of processor and memory 
allocation under static partition­ing can yield insights into appropriate allocation deci­sions for dynamic 
partitioning. Finally, our decision to consider equal-sized partitions is motivated by the re­ sults 
of recent studies [19] showing that adaptive par­ titioning strategies in which the system is divided 
into equal-sized partitions outperform other adaptive poli­ cies. We denote by Z the number of processors 
in each partition, and we use K to refer to the number of system partitions. Thus, K ~ P/Z where we only 
consider values of Z that evenly divide P. Upon job arrival, the scheduler allocates a processor partition 
to the job if one of the K partitions is available. Otherwise, the job waits in a system queue until 
a partition becomes free. Each parallel job is executed to completion without interruption and the entire 
partition is reserved by the application throughout the duration of this time. Upon job departure, the 
available partition is allocated to the job at the head of the system queue. A first-come first­ served 
queueing discipline is assumed. Each job is assumed to belong to a particular job class i, 1 s i < IVJC 
< m, where IVJC denotes the total number of job classes. Let p: be the probability that a job arrival 
belongs to class i, independent of all else. The processing requirements of class i jobs at any of the 
K partitions (each of size Z) are modeled as inde­pendent and identically distributed (iid) non-negative 
random variables, with a general probability distribu­tion F,z whose mean, Di (Z), and standard deviation, 
These times include all aspects of the job s execution, with the exception of memory usage. Let ~i (Z) 
be a non-negative integer-valued ran­dom variable that represents the total number of mem­ ory references 
for class i jobs on a partition of size Z. Note that Jf ($ (Z) is in general dependent on the process­ing 
requirement and that multiplying the miss ratio by the total number of references yields the expected 
num­ber of page faults incurred by the program. Let CPg be the cost to load a page in memory. We can 
then com­pute the memory overhead incurred by a class i job on a partition of size Z as qi(z), are both 
finite. Mi(z) = (R:l(Z)+@(Z)) .AL(.Z) Cpg, 1< i < JJ.Tc, (13) where R~t(Z) and R~l (Z) are the program 
s intra­locality and inter-locality miss ratios when executed on a partition of size Z, respectively, 
both of which are obtained as described in Section 3.1. The total service times of class i jobs at a 
partition are modeled as iid ran­dom variables, equal to the sum of their processing re­quirements and 
memory overhead, with a general prob­ability distribution B%z whose mean, &#38;(Z), and stan­dard deviation, 
al(Z), are both finite. We also define s(z) = ~fl: p:.%(z). It follows from the above assumptions that 
our par­allel system model is equivalent to an M/G/K queue­ing system, for which there are no known exact 
steady­state solutions. There are, however, a large num­ber of approximate solutions. In our study we 
use an approach due to Yao [26] based on numerous sim­ulation experiments where we found this approxima­tion 
to yield very accurate steady-state response time results across a wide range of values for c,(Z) = (s(z) 
)- Jz&#38; P:((~i(z))2 + ($%-a) )]-(s(z)) (coefficient of variation), which is important given the large 
variation in resource requirements expected for parallel processing workloads [20]. Using this approx­imation, 
we can calculate the mean stationary job re­ sponse time in our parallel system with K partitions each 
of size Z as K P/(1 P) T(Z) = yK()+ s(z) (14) 1 ~z-1 where 1 ~ (Kp)k (Kp)K 7r = +~c k! + K!(l P) 
{ k= } p = /)/(Kp) < 1, < = exp(r/2) exp( r/2) r, = 2(A p)/(A + ~c~(Z)), p = (S(2))- 1 and ; = (J 
-Kp)/(A + Kpc:(z)). We now can make an important observation about distributed parallel processing systems 
within this mod­ eling framework. Consider the case where El,z (t) equals B~(Zt) for each job class Z, 
with P of the form 21, 1>0, K s P/Z, Z = 1,2,4,..., P/2,P,l~i~NJC,t>0 and CS < 1 (note that CS is not 
dependent upon Z in this case); i.e., the sequence of service times (includ­ing memory overheads) for 
the Z = P (K = 1) sys­ tem is I/K h that for the general Z system. When these conditions hold, it can 
be shown that Z = P is the optimal processor allocation. This follows from a result due to Brumelle [4] 
that ~(Z) ~ ~(P), Z = P, P/2, P/4, .... 2, 1,under the conditions stated above. 4 Case Study In this 
section we apply the models and analysis of the previous sections within the context of real large­scale 
scientific parallel applications. We have chosen for our case study the solution of a discrete 3-D Pois­son 
problem using the multigrid algorithm. Note that many physical phenomenon can be represented by the 3-D 
Poisson equations, and thus their solution is often sought in many scientific and engineering applications. 
Moreover, the same problem has been included in the NAS Parallel Benchmark Suite [2], which is considered 
to be representative of the computational fluid dynam­ics applications that are typically executed in 
the aero­space industry. In our study we use the multigrid kernel (MG) from these benchmarks, which requires 
discrete solutions of a 3-D Poisson problem using the multigrid algorithm on a 256 x 256 x 256 grid. 
The MG prob­lem is a computationally intensive problem with con­siderable memory requirements, as evidenced 
by recent measurements showing that an efficient single processor implementation requires 3.9 x 109 floating 
point oper­at ions and 453. 6M B of memory on a single processor Cray Y-MP [3]. In this study we consider 
three grid sizes in addition to the 256 x 256 x 256 grid problem: 32x32x 32,64x64x 64and 128x128x128. 
Werefer the interested reader to [18] and the references therein for more details on the multigrid algorithms, 
the MG programs and the data parallelization methods used to parallelize the MG problems considered. 
4.1 Application Analysis A detailed analysis of the processing and memory refer­ence patterns for the 
above applications was conducted assuming the following system characteristics. Jobs ar­rive from a Poisson 
source to a multiprocessor with P = 64. Each processor has its own local memory and delivers sustained 
execution rates of 50 MFLOPS in the computationally intensive sections of an applica­tion. Each local 
memory hafi a capacity of 8K pages of size 4KB, lK of which is used by the operating sys­tem; thus &#38;f 
= 7K pages are available to user pro­grams on each node. We assume that the cost to load a page in memory 
is 0.78ms. The network fabric is assumed to be such that every processor-pair is equidis­tant in the 
sense that, at the application level, the same amount of time is required to transfer information be­tween 
any pair of processors. This assumption is reason­able for multi-stage interconnection networks or when 
the software overhead for packing and unpacking mes­sages dominates the cost of interprocessor communica­tion. 
The latency per message, as experienced at the application level, is assumed to be 10 psec and the av­erage 
achievable message transmission speed is assumed to be 20 MB/see. The MG programs were carefully analyzed 
for their processing properties, including computation, commu­nication and data dependencies. The processing 
de­mands of each application represent the actual process­ing requirements of a complete solution to 
a 3-D Poisson problem using the multigrid method. The correspond­ing execution times, excluding memory 
overhead, were obtained by a detailed analysis and simulation of the programs on each processor partition 
size considered, i.e., Z= 1,2,4, 8,16,32and 64. References to data (as opposed to instructions) clearly 
dominate the memory reference behavior of the parallel applications considered, which is contrary to 
what has been shown for more general workloads [15]. Moreover, the processor allocation policy tends 
to have a limited effect on the instruction references issued by these programs. We therefore focus on 
data references in our study. The memory requirements for each appli­cation were obtained by developing 
instrumented traces of the execution of the application as follows. We for­mulated a simple mapping from 
each data reference to its corresponding (virtual) page in memory. We then constructed versions of the 
parallel programs that were modified to record the page number of every memory reference and were stripped 
of the actual numerical com­putations. For the applications considered and a given value of Z, the memory 
reference behavior on each of the allocated processors is quite similar, with the main exception being 
that the processor executing the critical path references a few additional localities which are very 
small. Thus, the modified versions of the programs were run to generate memory reference trace~ of the 
proces­ sor executing the critical path of an application when given a particular processor partition 
size. The program behavior model for each of the cases considered was developed from the corresponding 
mem­ory reference trace as follows. The traces were parti­tioned into locality sets as described in Section 
3.1 and a micro model was constructed for each locality based on the LRUSM. Values for the intra-locality 
miss ratios (i.e., R~l(Z)) were calculated via the closed-form ex­pressions that exist for F(Q~, iM) 
when Q; corresponds to the LRUSM [1]. A macro model was constructed for each memory reference trace as 
described in Sec­tion 3.1.1 and the corresponding inter-localit y miss ra­ tio (i.e., R~t (Z)) was calculated 
by applying the analytic methods of Sections 3.1.3-3.1.5. Our analysis suggests that the properties previously 
established for program behavior (see Section 2) appear to hold for the class of parallel applications 
considered. We also note that ex­act values for C1 were obtained by exploiting properties of the matrices 
Q; in this analysis. 4.2 Quantitative Results Our first set of results consists of the execution times 
and memory overheads obtained from the above applica­tion analysis. In Table 1 we summarize our findings 
for each application and every possible processor partition size on which the parallel job could be executed 
under our model. These results clearly demonstrate the im­portance of the inter-locality memory overhead, 
which dominates the total memory costs incurred by the pro­grams when they experience paging activity. 
For these cases, previous memory modeling methods would have predicted paging overheads that are orders 
of magni­tude smaller than the paging costs that will be realized in practice, as can be seen from a 
comparison with the intra-locality results. We further observe that the mag­nitude of the total memory 
overhead relative to the ex­ecution time is quite significant when the applications are allocated pa1tition5 
in which the program working sets do not fit entirely in memory. These factors domi­nate the total service 
times of the larger applications in these cases, and will have a significant effect on system performance 
in distributed parallel environments. The mix of parallel applications comprising the sys­tem workload 
can also have a significant impact on per­formance and thus is an important issue. Studies of Exec. Intra-Loc. 
Inter-Lot. Prob. Part. Time Mere. ovhd. Mere. ovhd. Size Size (sees) (sees) (Sees) 323 1 0.371 0.000 
0.000 323 2 0.225 0.000 0.000 323 4 0.119 0.000 0.000 323 8 0.064 0.000 0.000 323 16 0.037 0.000 0.000 
323 32 0.022 0.000 0.000 323 64 0.014 0.000 0.000  643 1 2.972 0.000 0.000 643 2 1.632 0.000 0.000 
643 4 0.836 0.000 0!000 643 8 0.430 0.000 0.000 643 16 0.226 0.000 0.000 643 32 0.121 0.000 0.000 643 
64 0.065 0.000 0.000 1283 1 23.776 5.400 61.865 1283 2 12.454 0.000 29.187 1283 4 6.300 0.000 0.000 1283 
8 3.188 0.000 0.000 1283 16 1.633 0.000 0.000 1283 32 0.837 0.000 0.000 1283 64 0.431 0.000 0.000 2563 
1 190.206 114.566 500.834 2563 2 97.347 89.569 252.086 2563 4 48.957 26.687 118.658 2563 8 24.622 5.400 
61.874 2563 16 12.455 0.000 29.196 2563 32 6.301 0.000 0.000 2563 64 3.189 0.000 0.000 Table 1: Summary 
of parallel application and modeling analysis expected parallel processing applications suggest that 
parallel processing workloads will consist of a mix of job. with wide variations in resource requirements 
[20]. Experience with actual parallel systems further sug­gests that the job stream will consist of a 
sequence of problem sizes wherein smaller problems appear more frequently than larger problem sizes. 
Values of p:, 1 < i ~ 4, were chosen based on these expectations to achieve reasonable coefficients of 
variation in the work­load. The results presented in this section are for two 20 ­ ... .... 15 -.., 
 %.., ..,. [L ......... --.,­10 -.., -..... .... .... -.. -%.. . ..... ..\ ~~%.-.......... .... 
... ---% ..,%.,-, -....... .3............ --%.* *..._ -------. ...-,-,. --,-.4 ---.--.k _,, =............= 
--+--%%+,+ )1---... ....... . -.-~~~~::::~:..::-:*-~.::*-~-=-*-aM - - -- - - - --.-* -----. --. --. 
--*-. ---. -*-------., -.*.*-**** . 0~ 0 0,1 0.2 0,3 0.4 0.708 09 1 0.10,20,30,4 0,708 09 $ S)vem Utilii%. 
S&#38;m UMLXn Figure 1: Response time ratios for workload WA with Figure 3: Response time ratios for 
workload WA with no memory overhead, as a function of Z and p memory overhead due to intra-locality and 
inter-locality page faults, as a function of Z and p e4* opbmd parbbon, inkad.c + intwkc t 0P m ar 
0n n e40cno mem ovhd - oPbma Pw800n, a.. 32 \ t 8 40~ 1 System Utilii. bon Figure 2: Response time 
ratios for workload WA with memory overhead due to intra-locality page faults, as a Figure 4: Optimal 
partition sizes for workload WA, as function of Z and p a function of p such workloads, denoted by WA 
and WB, in which the context of our models for both workloads. In Figures 1, squared coefficients of 
variation for the execution times 2, and 3 we plot the ratio of mean response time for of the workloads 
fall within the ranges [4.7, 5.06] and general Z to that for Z = 64 as a function of p under [11.77, 
14.4], respectively. It is important to note that WA when memory overheads are ignored (i.e., the job 
the cause of such high variability is not due to the in-service demands are as given in column 3 of Table 
1), dividual service times, which are deterministic (see Ta-when memory overhead consists solely of intra-locality 
ble 1), but rather due to the variance in the job mix. page faults (i.e., column 4 of Table 1), and when 
mem-Since the multigrid (and other similar) algorithms are ory overhead consists of both intra-locality 
and inter­ commonly used as solvers in large-scale scientific appli-locality page faults (i.e., the sum 
of columns 4 and 5 of cations and form the bulk of the computation in these Table 1), respectively. The 
optimal partition size (i.e., applications, it is reasonable to assume that our results the value of 
Z that provides the best system perfor­ under the above workloads are representative of a more mance) 
as a function of p for each of these cases is given heterogeneous application workload. in Figure 4. 
Figures 5 -8 illustrate the corresponding results for workload WB. We refer the interested reader to 
[18] for additional results. Our next set of results, a portion of which follows, considers system performance 
under different processor partition sizes (i.e., Z = 1,2,4, .... P/2, P) within the We first ignore memory 
overhead and observe that 30 Pm PA64 -+-PA4 PAE4 -a--PA8 Ph.% -* PA16 PAM ~­25 20 ­ $. 15 - ..% ,.. ...$ 
... % 10 ,... ,, ...% ..... 5 ,G=:--:---:: : =&#38;==== ~-=---­0 010203040,5 07080,9 1 system Ublu;;on 
 Figure 5: Response time ratios for workload WB with no memory overhead, as a function of Z and p 30 
PAZ PAM +­?M, PA64 -B-PA8 PAM -!4-- PA16 PA64 ~ ­25 ..<, ..$ 1... 20-.. \,, .. -$., 15 \<\ ., m +..,. 
10 - .... .... -.... . ..... -. .,, ... x.. .... ..... %.. 5 ,C %....... .--....  *-­ ------­ 01 1 01 
02 03 0.4 05 06 07 00 09 1 system Um!mbon Figure 6: Response time ratios for workload WB with memory 
overhead due to intra-locality page faults, as a function of Z and p both workloads exhibit the important 
processor allo­cation property noted in the introduction (albeit for somewhat different reasons), i.e., 
the optimal processor allocation size tends to decrease with increasing system loads for sufficiently 
variable workloads under the as­sumption of infinite memory resources (see Figures 4 and 8). Moreover, 
the differences in mean response times among the various processor allocation sizes are quite significant 
(see Figures 1 and 5). When load is light, job response times are essentially dominated by their processing 
requirements given the number of al­located processors and thus the benefits of maximizing the parallel 
execution of jobs outweigh its associated costs. As the system utilization rises, however, queue­ing 
delays have an increasingly larger relative impact on job response times. This together with the non-linear 
speedups of the applications comprising the workload 30 i, P%?PAE4 -+-PA4 PAM -n- PM PAM if i PA16 PAM 
~­25 \\$ ..,. ii., , *O ..... \ It. \ ...< ,,, x 15 ... I,. ... ... \ 10~ \ h.,,. k . ..... .... 
..... -. ... \ $... -.... \ 5 ..-,. -,-,-. = .......... x --------­ .------~ . ..-. ..-T.:I:*Y=-- +-----­ 
---.............*..---.-*.-.-.-.-.-.-.*-.... .---------.-.-.*-..... 0 01020304 070809 1 S:stm w%. Figure 
7: Response time ratios for workload WB with memory overhead due to intra-locality and inter-locality 
page faults, as a function of Z and p wm. ] parman,+ lnt.r40c mua.loc+­ ~m Figure 8: Optimal partition 
sizes for workload WB, as a function of p (see Table 1), due to factors such as synchronization, communication, 
data/task dependencies and load bal­ancing, tend to cause the optimal partition size to de­crease with 
increasing system loads. As expected, in­creasing the workload variance magnifies these effects and results 
in an increased and more rapid trend toward smaller optimal partition sizes with increasing traffic in­tensities 
(compare Figures 4 and 8). Reducing the vari­ance in the workload has the opposite effect. The only exception 
to these trends occurs in Fig­ure 8 where the optimal partition size jumps from Z = 4 to Z = 64 for p 
close to 1. This is due to an interest­ing aspect of our workloads where, for a fixed set of p?, 1 s 
i < NJC, the squared coefficient of variation for the processing demands of the workload increases with 
de­creasing values of Z. Thus, the optimal partition size is Z = 64 for p close to 1 because the service 
requirements of the workload are the least variable for the -Z = 64 case and this outweighs the benefits 
of smaller parti­tions at very heavy loads under workload W&#38;?. These results suggest that assuming 
c: to be fixed across each of the possible partition sizes, as has been done in some studies, is not 
always appropriate and that there is an interesting interaction between the workload variance and previously 
observed processor allocation properties when this is not the case. We next consider the performance 
impact of the constraints imposed by finite capacity local memories on the parallel applications of interest. 
Our results clearly show the significant effects that these memory overheads can have on system performance, 
where the benefits of allocating fewer processors to jobs at suffi­ciently high utilizations have been 
reduced considerably. The working set size of a job on each of the processors allocated to it increases 
with reductions in the partition size, since the job s memory requirements are spread over fewer processing 
nodes. Reducing the number of processors allocated to a sufficiently large job below a certain point 
results in excessive memory overhead that can dominate the service requirements of the job (see Table 
1), which in turn decreases system performance. This tends to push the optimal partition size toward 
larger values with increasing system loads (see Figures 4 and 8), which runs counter to the above processor 
al­ location property observed in the absence of memory overhead. The larger the memory costs incurred 
by the system, the sooner these memory overheads outweigh the factors that argue for smaller partition 
sizes with increasing p. This explains the shift toward larger par­tition sizes with increasing traffic 
intensities as seen in comparing the different curves in Figure 8, as well as Figure 4. Increasing the 
workload variance magnifies these ef­fects and results in an increased and more rapid trend toward larger 
optimal partition sizes with increasing traffic intensities (compare Figures 4 and 8), i.e., in­creasing 
c~ and increasing p push the system in the same direction viz smaller partition sizes. Reducing the vari­ance 
in the workload has the opposite effect. It is also important to note that one must be careful in making 
comparisons across the workloads WA and WB because the differences in c; are due to changes in both the 
vari­ance and the mean. 5 Conclusion In this paper we analyzed processor and memory allo­cation in distributed 
parallel processing systems. The primary objective of our study was to develop formal modeling methods 
that support the stochastic analysis of an important scheduling tradeoff between the impact of program 
inefficiencies toward reducing the number of processors allocated to parallel applications with in­creasing 
system loads and the performance impact of memory requirements toward larger allocations. We de­rived 
expressions for the memory overhead incurred by a program as a function of its processor allocation, 
the mean job response time under these allocation decisions, and the effects of contention for shared 
resources caused by paging activity. We also conducted a detailed analy­sis of actual large-scale scientific 
applications to para­metrize these models. Our results show that memory overhead resulting from processor 
allocation decisions can have a significant effect on system performance in distributed parallel environments, 
strongly suggesting that memory considerations must be incorporated in the resource allocation policies 
for parallel systems. We also demonstrate the importance of the inter-locality miss ratio component of 
the memory overhead incurred by a program. Acknowledgements We thank Henry Chang, Scott Leutenegger, 
Armand Makowski, Ajay Mohindra and Sanjeev Setia for fruit­ful discussions regarding the research described 
in this paper. We also thank Scott Leutenegger, Armand Makowski and the anonymous referees for helpful 
com­ments on earlier drafts of the paper. References <RefA>[1]O. I. Aven, Jr. E. G. Coffman, and Y. A. Kogan. 
Stochastic Analysis of Computer Storage. D. Rei­del, 1987. [2] D. Bailey, J. Barton, T. Lasinski, and 
H. Simon. The NAS parallel benchmarks. Technical Report RNR-91-002 Revision 2, NASA Ames Research Center, 
August 1991. [3] D. H. Bailey, E. Barszcz, L. Dagum, and H. Simon. NAS parallel benchmark results. IEEE 
Parallel and Distributed Technology, 1:43-51, February 1993. [4] S. L. Brumelle. Some inequalities for 
parallel-server queues. operations Research, 19:402-413, 1971. [5] K. L. Chung. Markov Chains with Stationary 
Tran­sition Probabilities. Springer-Verlag, 1960. [6] P. J. Courtois. Decomposability. Academic Press, 
1977. [7] P. J. Courtois and H. Vantilborgh. A decomposable model of program paging behavior. Acts Informat­ica, 
6:251 275, 1976. [8] P. J. Denning. Working sets past and present. IEEE Transactions on Sofiware Engineering, 
SE­6(1):64-84, January 1980. [9] S. Leutenegger and M. Vernon. The performance of multiprogrammed multiprocessor 
scheduling poli­cies. In proceedings of the ACM SIGMETRICS Conference, pages 226-236, May 1990. [10] 
A. W. Madison and A. P. Batson. Characteristics of program locality. Communications of the ACM, 19:285-294, 
1976. [11] S. Majumdar, D. L. Eager, and R. B. Bunt. Sched­uling in multiprogrammed parallel systems. 
In Pro­ceedings of the ACM SIGMETRICS Conference, pages 104-113, May 1988. [12] A. M. Makowski and R. 
D. Nelson. Distributed parallelism considered harmful. Technical Report RC 17448, IBM Research Division, 
December 1991. [13] A.M. Makowski and R. D. Nelson. Optimal sched­uling for a distributed parallel processing 
model. Technical Report RC 17449, IBM Research Divi­sion, February 1992. [14] C. McCann, R. Vaswani, 
and J. Zahorjan. A dynamic processor allocation policy for multipro­grammed shared-memory multiprocessors. 
A CM Transactions on ComputeT Systems, 11(2):146­178, May 1993. [15] J. M. Murphy and R.. B. Bunt. Characterizing 
program behaviour with phases and transitions. In Proceedings of the ACM SIGMETRICS Confer­ence, pages 
226-234, May 1988. [16] V. K. Naik, S. K. Setia, and M. S. Squillante. Per­formance analysis of job scheduling 
policies in par­allel supercomputing environments. In Proceedings of Supercomputing 93, November 1993. 
[17] V. K. Naik, S. K. Setia, and M. S. Squillante. Scheduling of large scientific applications on dis­tributed 
memory multiprocessor systems. In Pro­ceedings 6th SIAM Conference on Parallel Process­ing for Scientific 
Computing, pages 9 13 922, March 1993. [18] V. G. Peris, M. S. Squillante, and V. K. Naik. Analysis of 
the impact of memory in distributed parallel processing systems. Technical Report RC 19336, IBM Research 
Division, October 1993. [19] E. Rosti, E. Smirni, G. Serazzi, L. Dowdy, and B. Carlson. Robust partitioning 
policies of mul­tiprocessor systems, Technical report, Department of Computer Science, Vanderbilt University, 
1992. [20] R. Schreiber and H. D. Simon. Towards the ter­aflops capability for CFD. In Parallel CFD -Imple­mentations 
and Results Using Parallel Computers, 1992. [21] K. C. Sevcik. Characterizations of parallelism in applications 
and their use in scheduling. In Pro­ ceedings of the ACM SIGMETRICS Conference, pages 171-180, May 1989. 
[22] J. R. Spirn. Program Behavior: Models and Mea­surements. Elsevier, 1977. [23] J. R. Spirn and P. 
J. Denning. Experiments with program locality. In Proceedings of the AFIPS FJCC, volume 41, pages 611-621, 
1972. [24] G. W. Stewart. Computable error bounds for ag­gregated Markov chains. Journal of the ACM, 
30:271-285, 1983. [25] A. Tucker and A. Gupta. Process control and scheduling issues for multiprogrammed 
shared­memory multiprocessors. In Proceedings of the l,%h ACM Symposium on Operating Systems Principles, 
pages 159-166, December 1989. [26] D. D. Yao. Refining the diffusion approxima­tion for the M/G/m queue. 
Operations Research, 33(6):1266-1277, 1985. [27] J. Zahorjan and C. McCann. Processor scheduling in shared 
memory multiprocessors. In Proceedings of the ACM SIGMETRICS Conference, pages 214 225, May 1990.  
		</RefA>	
