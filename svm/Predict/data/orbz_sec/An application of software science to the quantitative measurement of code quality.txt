
 AN APPLICATION OF SOFTWARE SCIENCE TO THE QUANTITATIVE MEASUREMENT OF CODE QUALITY Farid A. Naib Information 
Systems Group International Business Machines Corp. March 1982 Permi~on to copy without fee all or 
part of thb materiaJ is granted provided that the copies are not made or distributed for direct commercial 
advantage, the ACM copyright notice and the title of the publk:ation and its de= appear, and notice is 
given that copying is by permission of the A~tion for Computing lVlachinery. To copy otherwise, or to 
republish, requires a fee and/or specit'w permi~ion. 0"1982 ACH 0-8979"1-086-9/82/0300-010"1 $00.75 AN 
APPLICATION OF SOFTWARE SCIENCE TO THE QUANTITATIVE MEASUREMENT OF CODE QUALITY Farid A. Naib IBM 20P/43L 
 P. O. Box 2150 Atlanta, GA 30055 The error rate of a software application may function as a measure 
of code quality. A methodology has been developed which allows for the accurate prediction of the error 
rate and hence code quality prior to an application's release. Many factors were considered which could 
conceivably be related to the error rate. These factors were divided into two categories: those factors 
which vary with time, and those factors which do not vary with time. Factors which vary with time were 
termed environmental factors and included such items as: number of users, errors submitted to date, etc. 
Factors which do not vary with time were termed internal factors and included Halstead metrics, McCabe 
metrics and lines of code. Because the error rate of a software package fluctuates greatly with time, 
environmental factors rather than internal factors were first studied. A combination of environmental 
factors were found to be highly correlated to the fluctuations of the error rate. With historical data, 
regression analysis may be run to determine the coefficients for each environmental factor. The coefficient 
may be thought of as a measure of the effect each environmental factor has on the error rate. Every application 
has its own unique coefficient for each environmental factor. These coefficients are constant throughout 
the period studied. Software science measures were generated for each application. These measures were 
then studied to determine any relationships between an application's coefficients and its software science 
metrics. The differences in certain of the applications metrics were found to be highly correlated to 
the differences in the applications coefficient's. By utilizing the methodology outlined in this paper, 
it is possible to accurately estimate the error rate of a software package prior to its release through 
the utilization of software science metrics. I. INTRODUCTION The measurement of code quality is becoming 
increasingly important to the computer industry. While several definitions of code quality exist, perhaps 
the most useful are expressed in terms of the number of errors submitted 1 against a piece of code. 
This definition is particularly meaningful because of the rising costs of software maintenance. Almost 
half of the software dollars now spent in the United States are devoted to maintenance. With maintenance 
costs being such a large part of total costs, an accurate estimate of the maintenance costs is vital 
in determining the profitability of 2,3,4 developing and supporting a software package. A quantitative 
method for determining the quality of code, and hence, the expected error rate, prior to an application 
release, would be of great benefit 4 to the computer industry. The error rate may function as a quantitative 
measure of code quality. In order to compare the quality of various pieces of code, their error rates 
from historical data can be studied. However, the need exists for an accurate quantitative error estimate 
prior the availability of historical data. 5 Currently, code quality can only be subjectively evaluated 
at this point. Software science may provide an quantitive measure of code quality determined as a function 
of the expected error rate prior to an application's release. This paper details results of research 
which relates software science measures to an error prediction model. The research was conducted over 
two large RPG software packages. This paper is organized into three parts: The first part summarizes 
the establishment of a predictor for trends and fluctuations in the error rate is discussed. The next 
part describes techniques and tools used in the research. The remaining portion of this paper deals with 
the correlation of internal factors (software science measures) to the error prediction model. II. ENVIRONMENTAL 
vs INTERNAL FACTORS In order to better plan maintenance activity, a function providing the estimated 
number of errors versus time is needed. An accurate estimate would allow for the more efficient utilization 
of manpower and machine resources, resulting in a cost savings. The time unit chosen for this study 
was the month. Random fluctuations in the number of errors received in a month (hereafter referred to 
as the error rate) were removed by applying a moving average. 1 These random fluctuations can include 
such things as: when the error was logged, postal strikes, vacations, and so on. After the moving average 
was taken, there still existed 6 a wide monthly fluctuation in the error rate. (Fig. I) Many factors 
can conceivably be correlated to the error rate of a software package. These factors were divided into 
two categories: those factors which vary with time, and those factors which do not vary with time. In 
the context of this paper, I shall define those factors which vary with time as environmental factors 
and those factors which do not vary with time as internal factors. Internal factors include such measures 
as lines of code, Halstead metrics and McCabe metrics. As these factors do not vary greatly with time, 
they can be assumed not to be the cause of trends of fluctuations in the errors rate. Instead, we must 
direct out attention to environmental factors which do vary with time. Environmental factors include 
total users, errors submitted to date, and so on. Previous research has proved successful in correlating 
a combination of 7 these variables to the trends and fluctuations in the error rate. Some of the internal 
factors may prove to be a measure of the effect environmental factors have on the trends and fluctuations 
of the error rate. This paper deals with the investigation of this relationship. l) Moving Average is 
a device for describing the decline or growth in a time series. It is particularly useful if random 
flucuation exist in the time series. GRAPH OF ERRORS RECEIVED BY MONTH ADJUSTED BY MOVING AVERAGE 45 
35 30; 25 20 15 :k 10 FIGURE 1 1 3 5 7 9 11 13 15 17 MONTHS SINCE RELEASE 19 21 23 25 27 III. AN ERROR 
PREDICTION MODEL FOR TRENDS AND FLUCTUATIONS IN THE ERROR RATE Several criteria were used in determining 
the best combination of environmental variables. The two most important criteria were that the combination 
of variables have a high correlation coefficient 2 for each ap- plication studied and that each variable 
in the model should be statistically accurate from application to application. A large number of environmental 
factors were considered. Utilizing the above criteria, the group of factors was reduced in the following 
way: A maximum R 2 improvement technique (a variation of stepwise regression) 3 was run on the variables 
by application. The fifteen variables most often used by the application were kept for further analysis, 
while the other variables were dis- carded. The next step involved calculating the correlation coefficient 
by application for every combination of one, two, three and four variables. In each category, the top 
ten combinations were listed for each application and awarded points. The ten combinations with the highest 
point total over all the applications in each category were retained. This step resulted in forty different 
combinations of environmental factors. The criterion that each var- iable in the model be statistically 
accurate removed all combinations of four or more variables from consideration. The criterion of high 
correlation coefficients removed all of the one variable models and most of the two variable models from 
consideration. The study then concentrated on models containing three variables. By applying the various 
criteria, the best combination was chosen. The final combination consisted of: (I) number of recently 
installed users, (2) number of total users, (3) number of installed users with error history. Each of 
these variables are adjusted to some degree by the elapsed time since the package was issued. The average 
correlation coefficient over the seventeen applications was 95%. Figure 2 is a graph of the predicted 
error rate versus the actual error rate. 2) See any basic statistics text for a description of stepwise 
regression. 3) The correlation coefficient is a mesure of how well the model fits the actual data. 
P~OT OF PREDICTED ~n LF, muR RATE VS ACTUAL ERROR RATE * _ = ACTUAl_ -= PREDICTED ERROR RATE 37.5 35.0 
32.5 30.0 27.5 25.0 )k )k 22.5 20.0 17.5 15.0 12.5 I0.0! 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 MONTHS 
SINCE APPLICATION RELEASE FIGURE 2 109 The final equation takes the form: # OF ERRORS = B 1 (RECENTLY 
INSTALLED USERS) + B 2 (USERS W/APAR HISTORY) + B 3 (TOTAL USERS) Note the use of coefficients (B I,B2,B3,) 
in the model. These coefficients vary to some extent from application to application The proper coefficients 
for each application can be generated from running regresion analysis on historical data for each appplication. 
However, Before a package is shipped, no history exists and hence it is not possible to generate the 
coefficients of the environmental factors by regression analysis. A way of predicting the coefficients 
before historical data exists is needed. The stage is now set for bringing internal factors into the 
error predict- ion model. The remainder of this paper deals with the correlation of internal factors 
to the coefficients of each variable. IV. DATA COLLECTION The basis of this study consisted of two 
large RPG packages. These packages consisted of over 720,000 lines of code and 1,000 modules. Each package 
was divided into several applications. The number of modules and lines of code vary greatly from application 
to application. There was an average of 45,000 lines of code and 70 modules in each application. Each 
application could be installed independently. Sixteen of these applications formed the basis for this 
study. The software science meterics were generated by a counting tool developed by Sandra Hartman of 
IBM. This tool scans RPG source code and generates metrics for each module in an application. These metrics 
include the measures defined by Halstead and McCabe and fall into the field known as software science. 
 All Halstead metrics are a function of four variables: total operands, unique operands, total operators, 
and unique operators. An operand is defined as a symbol or group of symbols that may take a value, i.e., 
variables and constants. An operator is defined as a symbol or group of symbols which af- 9 fect the 
value of some operand. From these four measures, Halstead derives certain other metrics. The equations 
used to generate these metrics are illustrated in Chart I. Care must be taken not to take the naming 
conventions that Halstead used literally. The measure of difficulty, for example, would be hard to correlate 
to how hard it is to code the algorithm. I0 The McCabe metric included in this study is the cyclomatic 
complexity. The equation for this metric is also listed in Chart I. EQUATIONS AND DEFINITIONS OF SOFTWARE 
SCIENCE METRICS HALSTEAD MEASURES (1) n I = UNIQUE OPERATORS  (2) n 2 = UNIQUE OPERANDS  (3) VOCABULARY 
= n = n I + n 2  (4) N 1 = TOTAL OPERATORS  (5) N 2 = TOTAL OPERANDS  (6) LENGTH = N = N 1 + N 2 
 (7) ESTIMATED LENGTH = [n I x log2(nl)] + [n 2 x log2(n2)]  (8) VOLUME = N x log2(n)  (9) DIFFICULTY 
= (nl/2) x (N2/n/2)  (10) ETA1/2 = nl/2 (11) N2/ETA 2 = N2/n/2  (12) LANGUAGE LEVEL = VOLUME/(DIFFICULTY) 
2  (13) EFFORT = VOLUME x DIFFICULTY  McCABE MEASURES  (14) LOGIC (CYCLOMATIC COMPLEXITY) = e - n 
+ 2p  e = number of edges of arcs in the flow graph n = number of nodes p = number of connected components 
 CItARTI The output of the counting tool is generated by module and then later summarized into descriptive 
statistics for the application as illustrated by Fig. 3. Note that the descriptive statistics generated 
for each application includes the range, average, weighted average, standard deviation and where applicable, 
the total value over the entire application for a metric in question. Once the data was collected~ 
the first step taken was to ascertain that the average values for an application were different than 
those of other appli- cations. An ANOVA 4 test for unbalanced data was run indicating that the appli- 
cations were indeed different. This step was vital in allowing use of these meterics in further analysis, 
as without it, it would be impossible to attribute variations between applications to anything but chance. 
 4) Analysis of Variance Analysis - test to determine how much of the difference between applications 
can be attributed to chance. FIGURE 3 OUTPUT OF RPG COUNTING TOOL MODLL~ ~.I~I~UE GPE~ATC;,S L~,ICU=-. 
CP'ERA~C5 VLCAL!LLA~Y TCTAL GPE~ATC~ TCTAL ~PE~ANOS LE"GTM ,_3TI'a,~T~ LC,',,GTr-VCLL-~ ............................................................................................................................... 
o 3] 20~ 2.~2 606 5~d 1194 1777 (~:-, 31 187 ~1=' 59Z b25 122~ ~&#38;~ ~: ~ ~Z" 2 | 133 I 5~ 355 3d2 
63 tJ 2J ~C~ ! 5 I 293 24~, 170~ I e'~ 3]. B9 Z~gt) 2o;'/'~ ,=e. 33~ 3;4 LOZd llE~ 2~02 2=,CJ L 3"-)'~5 
54 ~20 37~ 13Jl 1.337 2bb~ 2973 2C~-.3 35 2L(S 2¢.L 560 b~.2 i].72 I~-5:, ",d.! 40 15Q l~O 595 55~ I 
1 ~g I 2~7 ~c,;r 2.7 173 2t~C 44~ 5.39 9P9 I:,1~ ;';-'3 34 3L~ 3=-3 lOIZ 12~1 22;.8 282C IdtZ-~ 2~ 27? 
2CE 726 937 1663 23~2 1~72"~ 50 2{J~ Z.=2 10bt~ 89d ig5d ibEg l~l~ ,3~ 295 321 eOe 980 17~ 25 LO [-~d 
_-~' 9 J0 ? 415 1849 19~4, 3033 -;/~.~ I 333~~ 55 25~. -~12 1~ 1302 2750 ~37b ~JZ~Z ~I 301 402 1210 14,22 
2bJ2 -~2~b =~.~ 7 ~g 3~. 2~8 302 bSb 87g 1575 233 ~. ]. Z c; 75 ~t~ 3~1 ,]95 l~e 1756 J33~ ZI~; 2~l-_-~ 
4~ 32~; 375 l~gq 14, E~ ~682 ~0~5 2C~Z2 3~ 237 271 ~46 O~EI 12~'-~ 20~-2 I JJ .~ 0 I 4. [4 475 2=- l 
4 2U9~) 5614 3S 5U :'';~ I ~. 95 4157 =-42 4~32 ~2r~ 9106 &#38;.5~=2 82732 56 ~3C 2¢3E l~b~ 1321 27~0 
21:9 22~ ~ 2J5 256 1209 10~ 2295 ldE~ ]`J)¢) 3=-2:)5 243 762 82g 15~ I 1773 I J~ 3~ 191 22*; ~1~1 87~, 
175T le=O 137T} ~4 15e I~2 52b ~32 105~ 132b 3~2~ t 15 e5 100 I17 lg,~ 3L3 003 £0:'~ I .................. 
--2-' ............... !2 ............. !~-~ ............. !J2 ............. -~!. ~ .......... --~-~-~ 
........... -~2~ ........... ~):2- ~' ,5b94~1 U~IGUE CPE~AICR$ LNICbE CPER~C5 ~='C~ELLagY TCTAL CPE=ATCRS 
TCTAL QPERANOS L=-NGTh ESTI.~ATEO L=,.NGTH ~CLL W~ i ................................ L_.................................................................................. 
.~_ = TCTAL 72538 T L9 e-U 0]`3~0 I HIGH d5 ~57 5/42 /~32 ~271~ 9l~b ~5~2 ~-7C2 I LOl 1'3. 79 1C0 1L7 
Lg~ 313 5~g 2~7~ ! AVcN*GE 43 257 JCC l. lbv 118~) 236~ ~321 1;7~2 5TD £~.V 15 ~5 lOT ~24. ~25 [74~ 979 
]. O.] 7 c ,GHT AVE 5~ 3~.2 37~ IgOd l~'~'~ 375b 3C22 ~2 -~49 I .................... --: ............................ 
=---...... -; ............. -........................ _ ............. -%--- MOUU-E 0iFFICL~LTY ¢TA1/2 
K2/ETA2 LCG|C L~hGUAGE LEVEL FFC~T CGMMENT5 NCN CGNMENT5 CALC 3T~T5 TCT,~L ~t IS i............. ....... 
iiii ...... ilii ..... !ii .......... iiiii ....... iii?iiii ..... ii .......... iii .......... !ii ........ 
:iii ...... i ;!! iili iiil ili iiili iili! ili ,ili ,i!i , ii! jL,~a~-y CIFPIC~Lr¥ ETa|/2 P,~/ETa2 
LCC|C ~,t~G~aGE L~V=,L ~FFI~:~T C=,W~ENT% ~.CN CC~MEhT5 CALC ST=T5 TC*~IL ~T3 ...................................................................................................................... 
 V. STATISTICAL ANALYSIS No assumptions were made about relationships between internal and external 
factors. Instead, relationships were determined by statistical methods in which every internal factor 
was equally tested to see if any correlation existed between it and a dependant variable (environmental 
factor's coefficient). Two criteria were used to determine the optimum relationships. One cri- terion 
was that the model have a high correlation coefficient (R2). The other criterion was that the model be 
statistically accurate. This criterion was measured by the null hypothesis. Type I and IV tests where 
also performed. 5 The first step in analyzing the data involved determining the correlation coefficient 
of all combinations of one and two internal factors against each environmental variable. Because of the 
low number of points, models with more than two variables had large F-values. Therefore, combinations 
larger than two internal variables were not considered due to the criterion of statistical accuracy. 
The best one and two variable models were then subjected to regression analysis. Figure 5 is an illustration 
of the output of one such run. 5) See any basic statistics text. OUTPUT OF REGRESSION ANAL": '~=~ 
RUN DEPENDENT VARIABLE: B2 SOURCE DF MODEL 1 ERROR 12 UNCORRECTED TOTAL 13 R-SQUARE C.V. 0.857278 
53.7307 SOURCE DF MOD I SOURCE DF MOD 1 PARAMETER ESTIMATE MOD 0.13792320 SUM OF SQUARES 1137.44989665 
254.30490335 1391.75480000 STD DEV 4.60348512 MEAN SQUARE 1137.44989665 21.19207528 TYPE I SS F VALUE 
1137.44989665 53.67 TYPE IV SS F VALUE 1137.44989665 53.67 B2 MEAN 8.56759231 PR > F 0.0001 PR > F 0.0001 
F VALUE PR > F 0.0001 T FOR HO: PR > ITI PARAMETER=O 7.33 0.0001 STD ERROR OF ESTIMATE 0.01882600 
FIGURE 5 Vl. RESULTS .... i, Statistical analysis yielded high correlations between the dependent variables 
and some of the internal factors. VI.I RECENTLY INSTALLED USERS The coefficients for recently installed 
users were found to be correlated to the average of volume divided by the difficulty squared. In addition, 
the model for category one applications (the more technical applications) includes total effort while 
the model for category two included total volume. The effect that recently installed users have on the 
error rate is thought to be due to simple errors that escape the testing cycle. Volume divided by difficulty 
squared, therefore, may be a measure of how hard it is to test the code for these errors. If you assume 
that the difficulty remains constant, the coefficient for recently installed users increases with the 
volume. It has been observed that motivation to perform careful reviews decreases as time already spent 
 8 inspecting code increases. As the volume increases, test time should also increase. Therefore, the 
larger a program, the harder it is to test. For example, consider a module in which the majority of 
executable code is taken up by a subroutine. The modules associated with this module come primarily from 
the subroutine. They are: a 1 = unique operators A 1 = total operators a 2 = unique operands A 2 = total 
operands Assume that when a programmer codes this module. Instead of utilizing a subroutine, he includes 
the code associated with the subroutine twice in the main routine. The number of unique operators would 
not increase as the addition of the code containing the second version of the subroutine would perform 
the same function as the first version. The number of total operators would double, however; as would 
the number of total operands. The number of unique operands would also double due to the programmer's 
utilization of different variable names in each version of the subroutine. The metrics now associated 
with the code are: a 1 = unique operators = total operators 2A1 2a 2 = unique operands 2A 2 = total 
operands Halstead's equation for volume is: Vol=(total operators+total operands) x log 2 (unique operators+unique 
operands) Halstead's equation for difficulty is: Diff = (unique operators) x (total operands) 2 (unique 
operands) Calculating the measure Vol/Diff 2 for each version of the module yields the following: SUBROUTINE 
NON-SUBROUTINE (A I + A 2) x logz(a I + a 2) (2A l + 2A 2 ) + log 2 (a I + 2a 2) [(a 1) x (A2)]2 [(a 
I x 2A2)]2 2 a 2 2 2a2 Comparing the two versions, we find that the difficulty measure is the same. Factoring 
the difficulty out, we are left with: (A l + A 2) x log2(a I + a2) (2) x (A 1 + A2) x log2(a I + 2a 
2) The value for the non-subroutine code is obviously greater than the value for the subroutine code. 
The effort involved in inspecting the non-subroutine code would also be greater than the effort involved 
in inspecting the subroutine code. Both versions of the module perform the same function, yet the non-subroutine 
version is twice as long as the subroutine version. Thus we see that it is possible to assume that the 
difficulty in testing a module can be correlated to the volume over difficulty squared. VI.II USERS 
WITH APAR HISTORY The range of coefficients for users with error history is correlated to the number 
of modules an application contains. The number of modules is thought to be a measure of interface problems, 
as well as the complexity of the application's structure. This relationship may hold true only for applications 
coded in RPG. If this is the case, then the difficulty in modularizing RPG applications would be illustrated. 
 The environmental factor of users with error history effect on the error rate relative to the effort 
of recently installed users factor increases with time. This symptom results in increasing complexity 
of software errors as time progresses. Vl. III. TOTAL USERS The coefficients for total users are negative. 
The larger the absolute value of the coefficients, the greater the affect total users has on the error 
rate. The range of coefficients for total users was found to be correlated to the average difficulty. 
Difficulty is defined as: D = unique operators x (# Av. # of times each unique operand is used) At 
a very low difficulty level, the coefficient approaches -.25. The higher the difficulty level, the closer 
this coefficient approaches zero. Therefore, at a large average difficulty level, total users would not 
exert as large a negative affect on the error rate and the number of errors received in a month will 
not diminish as rapidly. The effect total users has on the error rate is thought to be a measure of 
how well the code has been exercised. Therefore, the higher the average difficulty over the application, 
the more difficult it is to exercise the code. II Difficulty is affected by program structure and code 
impurities. As the program structure becomes more complex the difficulty value goes up. This relationship 
is consistent with the relation of the difficulty value to the coefficient for total users. VI.IV SUMMARY: 
It is possible to utilize internal factors to measure the affect the environmental factors will have 
on the error rate. A final equation for predicting errors is shown in Fig. 6. 0 Z 0 "!" -I'- .....1 
13.. 13_ I..- E3-- ILl '-r-" LLI"-r-I-" 0 LL ILl _.1 r,_;, ¢Y 0 I-- L.LJ z E3:E LLJ t.LI LLI -r-I"-- 
"1- + -t-O LL 0 I_1._ L.~ (,,"3 LLJ L/3 ILl U..IC~ I._l.J g, rJ~ = ILl Z LLI U3 ILl ._1 <Z: __1 n LL_ 
1.1.. 0LI- .._.1 M I_LJ L.~ I'-'-ZUJ V V ~ LU ,--J II LL LL 0 0 0 Z I L"ZE U.J L.LJ LLI --r- t-'-H LI... 
v VII. SUMMARY OF ERROR PREDICT MODEL With the discovery of correlations between the environmental variables' 
coefficients and internal factors, a very powerful methodology for predicting error rates is available. 
The life cycle of an application error prediction has two main stages. The first stage of the error 
predictor model exists in the time frame after the code is compiled and before historical error data 
is available. The compiled application can be run through a counting tool and software science metrics 
generated. These metrics are then plugged into an equation in order to generate the coefficients for 
environmental variables. When enough history is available from the field, it becomes possible to directly 
generate the coefficients for the environmental variables. At this point, the internal factors are not 
needed. The error predictor life cycle for an application is illustrated in Fig. 7. The question may 
be raised about applying the model to code in the design stage. Halstead has said that all other software 
science metrics can be cal- culated from language level and unique operands. Therefore, it becomes neces- 
sary to estimate language level and unique operands and modules. The remaining software science measures 
can then be calculated. However, the author feels that such an application is overextending the capabilities 
of the error .prediction model. F IGU P<Z 7 ERROR PREDICTION CYCLE PRE - PRODUCT RELEASE COMPILED/~ 
INPUT CODE "PREDICTED/ NUMBER OF I INPUT SERS / GENERATE SOFTWARE  ~ SS SCIENCE MEASURES ESTIMATE 
~~ VARIABLE COEFFICIENTS GENERATE ERROR PREDICTION POST -RELEASE -- HISTORICAL DATA AVAILABLE DATA 
< "N~UT INkiER OF I I I I RUN REGRESSION, ~ROCESS ANALYS I S CALCULATE k ROCESS VARIABLE I COEF~IC~ENTSI 
~,~ GENERATE ERROR. PREDICTION Vlll. CONCLUSIONS Variables which affect the error rate can be separated 
into two catego- ries, environmental factors and external factors. Environmental factors include those 
variables which fluctuate with time, such as total users, total errors to date, and so on. Internal factors 
are those variables which do not fluctuate to a large extent with time. Internal factors include lines 
of code, Halstead metrics, and McCabe metrics. The volume of errors received by a maintenance center 
fluctuates greatly with time. By using a combination of environmental factors, it is possible to accurately 
predict these fluctuations. The environmental factors which are correlated to the error rate are known. 
With historical data, a regression analysis can be run to discover the coefficients for each environmental 
factor. The coefficient may be thought of as a measure of the affect the environmental factor has on 
the error rate. Each application has its own unique coefficient for each environmental factor. In order 
to determine an environmental factor's coefficient for an application prior to availability of historical 
data, internal factors must be considered. Certain internal factors were found to be correlated to the 
coefficients of the environmental variables. The benefits of this methodology are numerous. The error 
rate and hence the maintenance costs can be accurately correlated for any volume of users, prior to an 
application's release. Better cost/benefit analysis can be per-formed. The break even volume and unit 
contributions can be calculated more accurately. IX. DISCUSSION This study has illustrated a practical 
application of Software Science metrics. By demonstrating that the metrics of a software package can 
be used to estimate the effect an environmental variable has on the error rate, we have shown that there 
exists some definable relationship between software science measures and code quality as determined by 
the error rate. To further illustrate this relationship, consider the plot of language versus the average 
number of errors per user. This relationship has a correlation coefficient of 92%. Many people might 
argue that the length or volume would have a higher correlation to the average number of errors per user. 
Rowever, the correlation coefficients for these relationships was under 80%. The main benefit to be 
derived from this paper is not the results but the methodology and ideas utilized. While the relationships 
discovered hold true for software written in RPG, these relationships may vary from language to language. 
There exist many unanswered questions concerning the utilization of software science metrics. More research 
needs to be done in this very promising field. X. REFERENCES <RefA>(1) F. Tsui and L. Priven, "Implementaion 
of Quality Control in Software Development," AFIPS Proceedings of 1976 National Computer Conference New 
York, pp. 443-449. (2) B. W. Boehm "Sofware Engineering: Rod Trends and Defense Needs," in Research 
Direction in Software Technology, Wergner, Ed. Cambridge, MA: MIT Press, 1979 pp. 44-86. (3) R. E. Gunderman, 
"A Glimpse into Program Maintneance," in Techniques of Program and System Maintenance, Garish Parike, 
Ed. Lincoln, NE: Etno-teck, Inc. 1980, pp. 23-27. (4) H. D. Mills, "Software Development," in Research 
Directions in Software Technology, Wergner Ed. Cambridge MA: MIT Press 1979, pp. 87-105.  (5) G. P. 
Fitosis, Vocabulary Effects in Software Science, Technical Report TR 03.082, IBM Santa Teresa Laboratory, 
California, 1980.  (8) S. K. Stearns, "Experience with Centralized Maintenance of a Large Appli- cation 
System," in Techniques of Program and System Maintenance, Garish Parika Ed. Lincoln, hiE: Etnotheck, 
Inc. 1980, pp. 143-149. (7) F. A. Naib~ A Study of the Trends and Fluctuations of a Large Centrally 
Maintained Software Package, Unpublished Work, IBM Atlanta Development Center, Georgia. (8) S. D. Hartman, 
An Analysis of Commencial RPG Programs Using Halstead and McCabe Metrics, Unpublished Work, IBM Menlo 
Park, California  (9) M. H. Halstead, Elements of Software Science. New York: North Holland 1977. 
 (I0) T. J. McCabe, "A Complexity Measure," IEEE Transactions of Software Engineering, 1976. (II) C. 
P. Smith, A Software Science Analysis of IBM Programming Products, Technical Report TR03.081, IBM Santa 
Teresa Laboratory, California, 1980. XI.</RefA> ACKNOWLEDGEMENTS No part of this research would have been 
possible without the support of Dr. Gerry Howe of IBM's Industry Application Development Center in Atlanta. 
Dr. Howe was responsible for initiating the steps which led to the existance of this research. In addition, 
he provided me with a tremendous amount of guidance as well as a substantial contribution in the development 
of the research methodology. His creative support spawned many of the ideas used in this work. The author 
also wishes to thank and acknowledge Sandra Hartman of IBM's Palo Alto location. Ms. Hartman has developed 
a very proficient tool which allowed this research to jump many steps. Her knowledge about software science 
metrics proved to be an invaluable asset to the progress of this research. The author also acknowledges 
the contributions that the following people or organizations made to this research: Emory University, 
Bill Frey, Allen Gresham, Bonnie Johnson, A. R. Jones, IBM's Atlanta Application Development Center, 
Dianne Quammen, Richard Quammen, Dean Rau, Lynn Silverman and Dr. Frank Tsui.  
			
