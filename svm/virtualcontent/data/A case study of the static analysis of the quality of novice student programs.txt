
   An alternative to quality estimation is to use the grade for the program. The grade, however, might 
not be a good indicator if the static characteristics of the program are not graded in detail whether 
or not the grade includes appropriate dynamic analysis of the program. Linear regression for each of 
the selected metrics as independent variables was carried out using Microsoft Excel 97 SR-1 to see which 
of the metrics had a significant relation with the dependent variable of quality. The relation was determined 
from the graphs between the input variable and the output variable values resulting from the regression 
analysis. If in the graph, the output values of the regression analysis closely followed the estimated 
values, it was inferred that there was a direct relation. Having determined, which metrics had a relation 
to quality, a multivariate analysis was performed with all the qualityÂ­related metrics as input variables 
and the estimated values of quality as output values. The result of the multivariate regression analysis 
showed how the selected metrics combined to predict a program s estimated quality. If the output of the 
multivariate analysis did not match the estimated output values, then it would be inferred that the selected 
metrics were not enough to give a significant correlation with quality, and more metrics would have to 
be analyzed. Figures 1 through 5 show the relationship that each of the metrics has with quality for 
Assignment One. For example, in Figure 1, QUAL represents the estimated quality values of the programs 
in Assignment One. L_Q_LV is the plot of the line equation resulting from linear regression with the 
independent variable, number of nested levels (which did not vary greatly in Assignment #l due to the 
simplicity of the assignment), and the dependent variable, quality. The X-axis represents the programs 
and the Y-axis, quality. The only related metrics are the average complexity (inverse relation), number 
of statements (inverse relation), comment characters per statement, and number of functions. The strongest 
correlation is with average complexity and the next is comment characters per statement. Number of functions 
and number of statements correlate less since the program is relatively short not requiring a large number 
of functions. The number of nested levels is not a factor due to the simple nature of the program. Using 
these metrics in a multivariate analysis yields a fairly close equation as to the estimated quality of 
a program as shown in Figure 6 where the multivariate regression equation is plotted against the estimated 
quality of the programs. For Assignments Two and Three, number of nested levels did correlate with quality 
and was used in addition to the other metrics for the multivariate analysis. As shown in Figures 7 and 
8, the metrics can give a fairly good sense of the quality of a program. 80 Trends in the metrics are 
shown in Figures 9 through 13. The metrics are sorted in nondecreasing order to show more clearly how 
the metrics varied in each program. The number of functions has risen from Assignment #1 to Assignment 
#3 which is a favorable trend in the student programs. More comments were added to the Assignment Two 
programs, but the Assignment Three programs were inadequately commented due to the complexity of the 
assignment since it was larger than Assignments One and Two. In retrospect, Assignment Three should have 
been made smaller, but the students did attempt to break the program up into modules. Number of nested 
levels and number of statements were similar for Assignments One and Two, but much higher for Assignment 
Three. The average cyclomatic complexity for Assignment Two was lower than Assignment One and the complexity 
for Assignment Three was much higher. 81  Figure 13. Average Complexity The trends in the metrics are 
encouraging for Assignments One and Two. The students tended to improve in their ability to control complexity 
and place documentation into the programs. Assignment Three is clearly shown to be much larger in scope 
than the first two assignments. It was difficult for the students to scale up to the program as they 
sacrificed documentation and complexity due to time constraints, but they used modularity to their advantage. 
This assignment should be scaled back in its scope to allow the students an opportunity to continue to 
improve their skills, but it was good to see them applying what they learned in class to tackle so difficult 
a program. 4 Benefits The primary benefit of this study is that it gives a procedure by which instructors 
can increase the granularity of grading student programs using a commercially available tool. The cost 
of the procedure is low since the process can be automated with Verilog Logiscope which can not only 
collect desired metrics, but also combine them to show an estimated value of program quality. The procedure 
does not obviate the need to review student programs manually, but it does reduce the need to inspect 
them tediously. By studying trends in the metrics over the course of a semester, an instructor can determine 
if the scale factor from one assignment to another is too large. In short, is the assignment of such 
significant difficulty over the previous ones that the students would have difficulty improving their 
skills in desired areas, such as documentation, modularization, and reduced complexity? The trends in 
the metrics can help instructors to choose programming assignments more appropriately and help them to 
understand why students may perform more poorly from one assignment to another. The metrics collected 
throughout a semester can form a baseline from which to judge student programs and for students to compare 
themselves to their peers. The metrics can help students consider why their programs may have higher 
complexity values than other correctly executing programs. They can serve to motivate the student in 
the direction of improving his/her program quality. 5 Future Work The study presented in this paper 
is small and is meant as a pilot study for further investigation. The results are encouraging and it 
is planned to collect more metrics and find better ways to use an expert s judgement to assess the quality 
of the student programs. Also, it is planned to give a more formal analysis of the results. Other possible 
future directions of research are given below: . Standardization of the proposed metrics: the task that 
the research has accomplished is finding metrics which seem to have a significant relation with the quality 
of the programs. Before the metrics can be used for any evaluation purposes, one needs to know the numerical 
values for which the program can be considered good or poor. These values may vary with the application 
domain, therefore, the values found in one domain should be scaled and adjusted appropriately for other 
domains. Use of the metrics in automated grading of the student programs: this can be accomplished after 
finding quality, and deciding upon the boundaries for quality as it pertains to grading purposes. 6 
Acknowledgements All terms known to be trademarks or registered trademarks have been capitalized. The 
authors would like to thank the CS 1462 faculty, Tom English, Susan Mengel, and Nancy Van Cleave, and 
Muralidhar, Kirk Watson, and Pramob Kumar Yenmanpra, for their help and cooperation in gathering the 
programs. The authors would like to thank Verilog for the use of Logiscope at Texas Tech University. 
 7 References <RefA>[l] <SinRef><author>R.E. Berry </author>and <author>B.A.E. Meekings</author>. <title>A Style Analysis Communications of the ACM</title>, <volume>Volume 
28, Issue 1</volume>,<date> January 1985</date>, pp. <pages>80 - 88</pages></SinRef>. [2] <SinRef><author>W. Harrison </author>and <author>C. Cook</author>. <title>A Note on the Berry-Meekings Style 
Metric</title>. <journal>Communications of the ACM</journal>, <volume>Volume 29, Issue 2</volume>, <date>February 1986</date>, pp. <pages>123-125</pages>. </SinRef>[3] <SinRef><author>S. Hung</author>, <author>L. Kwok</author>, 
and <author>R. Chan</author>. <title>Automatic Program Assessment. Computers and Education</title>, <volume>Volume 20, Issue 2</volume>, <date>1993</date>, pp. <pages>183-190</pages>. </SinRef>
[4] <SinRef><author>D. Jackson</author>. <title>A Software System For Grading Student Computer Programs</title>. <journal>Computers and Education</journal>, <volume>Volume 
27, Issue 314</volume>, <date>1996</date>, pp. <pages>171 - 180</pages>. </SinRef>[5] <SinRef><author>R.J. Leach</author>. <title>Using. Metrics To Evaluate Student Programs</title>. <journal>SIGCSE 
Bulletin</journal>, <volume>Volume 27, Issue 2</volume>, <date>June 1, 1995</date>, pp. <pages>41-43</pages>. </SinRef>[6] <SinRef><author>V. Yerramilli</author>. <title>Static Analysis of Novice Student 
C++ Programs. Master s Thesis</title>, <journal>Computer Science</journal>, <publisher>Texas Tech University</publisher>, <date>May 1998</date></SinRef>. </RefA>82 
			
