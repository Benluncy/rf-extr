
 Permission to make digital or hard copies of part or all of this work or personal or classroom use is 
granted without fee provided that copies are not made or distributed for profit or commercial advantage 
and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, 
to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee.Siggraph 
76, July 14-16 Philadelphia, Pennsylvania AN INTERACTIVE COMPUTER GRAPHICSAPPROACH FOR DISSECTING A MIXTURE 
OF NORMAL (OR LOGNORMAL) DISTRIBUTIONS Richard B. McCammon U.S. Geological Survey National Center 
952 Reston, Va. 22092 An interactive computer graphics program has been developed to dissect mixtures 
of normal (or lognormal) distributions. The program incorporates both graphical and analytical techniques 
to obtain a more satisfactory solution totheproblem of dissection. Within a matter of minutes, a mixed 
frequency curve can be decomposed into its normal (or lognormal) components. A statistical summary following 
dissection makes it possible to evaluate the goodness-of-fitand the separability of the inferred subpopulations. 
Individual components can be added or subtracted and adjustments can be made to individualparameters 
of components. An example of dissection is given in geology and in sports. 1. BACKGROUND Most natural 
phenomena are observed as statistical in which components are first localized by Fourier distributions 
and can be represented by a transforms and then the parameters estimated frequency curve. The parameters 
of these distri­ sequentiallythrough a series of iterations. butions are estimated usually from the 
observa- Although he obtained excellent results in tions. Natural events however are the product dissecting 
a bimodal normal distribution, severe of compound effects resulting from multiple computational difficulties 
arose when the number causes and the observed distributions are not of componentsto be estimated increased 
 or else always representative of homogeneous populations the subpopulations to be separated were close 
 but rather a mixture of two or more sub­ together. populations each of which possesses unique parameters. 
An observation therefore can Recently, another computer-basedsolution was represent a sample from one 
of several described / 5/ in which the parameters of populations. heterogeneouslymixed normal populations 
were estimated by curve fitting regression techniques. The statistical problem that arises is the Included 
was a statistical test to decide whether dissection of a mixed frequency distribution the addition 
of a subpopulation contributed into its individual components. This is significantly to the overall 
fit. This was a classic problem which can be tracedback intended to eliminate an earlier problem, namely 
to the early statistical literature /1_/.  prior knowledge of the number of subpopulations However 
as recently as 1967 C.G. Bhattacharya present. Even for a large-scale computer however / 2/ wrote: 
"A satisfactory practical this approach proved unworkable for complex data. solution of the problem 
under investiga- The failure was due to the inherent difficulty tion continues to elude mathematical 
 in selecting initial estimates of the parameters statisticians." close enough to the true values in 
order to offset the large amounts of computertime required to The approaches to the problem have been 
both performthe calculations. The selection of the numerical and graphical. The early work initial 
estimates of the parameters therefore is involved an attempt to estimate the a critical factor. parameters 
of a bimodal normal distribution by setting the first five moments equal to Graphical methods have been 
used for many years the values calculated from the observations to obtain initial estimates of parameters 
for and solving for the five unknown parameters. mixed distributions, or for that matter, to Even though 
this technique was later modified obtain "best" estimates in cases where the and further simplified, 
A. Hald /3/ was forced subpopulations are clearly separated. Perhaps to write: "The purely formal dissection 
of the most successful approach in obtaining a heterogeneous distributionsby numerical methods graphical 
solution was developed by Harding is very complicated." which involved dissection of the cumulative 
frequency curve on probability-graphpaper. For Trial-and-errormethods with a computer have this type 
of graph paper the cumulative frequency largely replaced the more formal mathematical distribution for 
a normal distribution appears approach. Gregor has proposed an algorithm as a straight line. The mean 
and standard deviation of the distribution can be estimated from the position and slope of this line. 
In the case of a bimodal normal distribution,the cumulative frequency curve can be decomposed into two 
lines from which the mean and standard deviation can be estimated separately. The inflection point of 
the curve can be used to estimate the proportion of each population. This is illustrated in fig. 1. 
For mixed distributions involvinga small number of components, graphical techniques like those of Harding 
and of others /2, 3/ have been mining L/_/ and petroleum /10/. effective in separating subpopulations. 
In the field of geology these graphical techniques have been appliedto the dissection of mixed frequency 
distributions exploration geochemistry The disadvantage of graphical solutions is that it is difficultto 
compare easily and quickly the theoretical distributions derived from different estimates of the parameters 
because of the considerable calculations required. Similarly it is difficult to evaluate goodness-of-fittests 
 quantitatively. Because of this, there is always uncertaintyassociated with results obtained graphically. 
 The difficulties described above in dissecting mixed distributions can be overcome by combining a graphical 
with an analytic approach. This can be accomplishedby interactive computer graphics and it is this 
approach which is described in this paper.  2. A COMBINED GRAPHICAL-ANALYTICAPPROACH In order to obtain 
a more satisfactory solution to the problem of dissectinga mixture of normal (or lognormal) distributions, 
a computer program was developed which combined the elements of graphics, an interactive environment, 
and high-speedcomputation. The program has been implemented on a storage tube type CRT terminal connected 
to a time share computer. It satifies the following ten requirements: 1) graphic display of observed 
 cumulative frequency curve plotted on arith­ metic (or logarithmic)probability scale, 2) graphic estimation 
of initial values of parameters, 3) selective display, erasure or deletion of graphic estimates, 4) 
selection of arbitrary number of normal (or lognormal) components up to some fixed number, 5) graphic 
 display of theoretical compound distribution for a specified number of components and given values 
of parameters, 6) computing algorithm for least squares solution which finds an acceptable model nearest 
to a set of initial estimates of the parameters, 7) graphic display of component probability density 
curves, 8) graphic input of threshold values for determining separability of inferred sub­ populations, 
9) statistical summary which displays final values of the parameters, results of goodness-of-fittest, 
and quantitative measures of the separability of the sub­ populations, 10) ability to restart, advance 
to next set of data or end program. By implementing the program on a terminal, a high  degree of user-directedcontrol 
is possible. A menu-selectiontype dialogue was created to allow the operator to exercise this control. 
Work space was reservedon the viewing screen to allow message printouts to assist the operator. The 
program logic follows as near as possible the logic one would use to dissect a curve manually. As a result, 
intermediate fits to the original curve can be checked, estimates of parameter values can be adjusted, 
the number of components can be changed, results for different model can be compared, and copies of 
all results can be obtained. Once data are entered and the operator has indicatedwhether to dissect 
a mixture of normal or lognormal components, a display panel shown in fig. 2 appears on the viewing 
 screen. The main work space is used to display  the observed cumulative frequency which is approximated 
by percentiles specified for the data. In the graph, the percentiles are plotted along the abscissa 
and the probabilityvalues (probits) are plotted along the ordinate. The size of the graph is kept fixed 
so that the values along each axis are scaled according to the maximum and minimum values. The axes 
are unmarked so that the operator will not be influencedby actual values. In the upper right hand corner 
of the panel, the name assignedto the data set is shown together with the designation of whether the 
data are considered to be derived from a mixed normal (N) or lognormal (LN) population. For the latter, 
the data are transformedby taking the logarithm. In the top leftmost part of the panel, space is reserved 
for messages intended to guide the operator through the dissection. In most cases, the message will either 
verify an action taken by the user in selecting an item from the menu or direct the operator to proceed. 
The menu occupies the right-hand portion of the panel and contains the identifiers of the componentsand 
parameters to choose from the types of actions to be taken. The following is a brief explanation of the 
menu: components  Fig. 2-Mixed   distribution Fig. 1-Graphic dissection a d each component is identified 
by an alphabetic character. b e Components may be selected in any order but must be c f arranged in sequence 
for dissection. parameters slope parameters associated with each component. Estimates inflectionpt entered 
as graphic input either as a scalar or vector threshold quantity. display display, erase, or delete graphic 
estimates of erase parameters for specified components. delete grafit fit theoretical curve based on 
current set of graphic estimates. besfit fit theoretical curve using non-linear least-squares with current 
graphic estimates. pdf display probabilitydensity curves of components. statistics clear work space 
and provide quantitative summary of results. restart restart, ask for next set of data or end. next 
 end The limited experience with the program to date has confirmed the importance of being able to switch 
back and forth between grafit and besfit,  making adjustments to the parameters and comparing results 
obtained by statistics. We turn now to consider two examples.  3. EXAMPLE IN GEOLOGY A large amount 
of data related to oil occurrence is available to those responsible for evaluating the present and future 
supply of petroleum. One of the responsibilities of geologists is to relate the occurrence of oil to 
the detailed stratigraphy and structure of the sedimentary basins where oil accumulates and is trapped 
in reservoirs. By studying the size distribution and number of oil deposits in an area, it is possibleto 
arrive at estimates as regards the size and number of undiscovered deposits. As an illustration, data 
have been collected for all oil pools in Cretaceous and Jurassic (65 to 190 million years before present) 
 reservoir rocks with more than 500,000 barrels of oil in place in Alberta, Canada /10/. The Cretaceousand 
Jurassic pools exhibit a variety of reservoir types and pose some interesting geologic problems. The 
data file contains information on the size, geometry, lithology, stratigraphy and production of over 
1100 oil and gas pools. Fig. 2 represents the cumulative frequency curve of in-place oil reserves for 
28 oil pools in the Viking formation of Late Cretaceous age. The data here are plotted on a log-probabilityscale. 
 Past experiencehas shown that the size of oil pools measured by reserves particularlywithin a single 
formationtends to follow a lognormal distribution. If that be the case, the data points in fig. 2 should 
fall more or less along a straight line. This is clearly not the case however and therefore some explanation 
is requiredto account for this deviation. A closer look at fig. 2 reveals there may be in fact two 
lognormal populations instead of one and thus we have just cause to try and dissect  the curve. Since 
the data are entered and the CRT is active, let us proceed.  The first step involves the graphic estimation 
 of the parameters for the two distributions.  It requires that we draw two lines and choose an inflectionpoint 
for the data. Fig. 3 shows the result. The slope of each line provides an estimate of the standard 
deviation, and the inflection point provides an estimate of mean and proportion for each subpopulation. 
The lines may be drawn any length anywhere on the screen and only the ordinate of the inflection point 
is needed. The theoretical curve based on the graphic estimates is shown in fig. 4. This is not bad 
for a first try.  Fig. 4-Grafit Fig. 3-Graphic   estimation By adjusting the slopes of our lines, 
we can do even better as shown in fig. 5. The curve now matches the data reasonably close. We are 
in a position to attempt a least squares best fit. This is shown in fig. 6. While the curve does not 
appear to be much different, the values of parameters have changed appreciably. In order to inspect 
the probability density functions which produced this curve, we select pdf on the menu. The result is 
shown in fig. 7. We see that the two subpopulationshave contributed Fig. 6-Besfit Fig. 5-Adjust grafit 
unequally to the total distribution. This indicates there are fewer larger oil pools in terms of reserves 
than smaller ones. To distinguish individual pools for the two populations, a threshold value can be 
selected. This is shown by the upper-case letter A in fig. 7. It allows us also to estimate the degree 
of separability of the two populations. By selecting the statistics item on the menu, we obtain quantitative 
results of dissection. This is shown in fig. 8. Because the data were  presumed to be lognormal, the 
mode, median, and mean of each component distribution are different. In this example, the values are 
expressed in millions (x 106 )of barrels of in-place reserves. Thus the smaller-sized pools which constitute 
nearly three-fourths the total number of pools have mean in-place reserves of 2.85 x 106 barrels. How 
good is the fit we have obtained? The chi­square value based on 5 class intervals (expectedclass frequency 
> 3 for each interval) is 0.15 which is low. Because there were five parameters to estimate plus one 
extra for the sum of the probabilities, there are negative degrees of freedom however for the chi-squaretest. 
Although the chi­square test cannot legitimatelybe applied, it can be noted that for 1 degree of freedomat 
the 95 confidence level, the critical value is 3.84 which is considerably greater than the value we observed. 
To apply the test on a more strict basis, we would need a larger sample of oil pools. The threshold value 
to distinguish the two populations is estimated as 11.59 x 10- barrels of oils. For this value, we would 
misclassify smaller­sized pools 1.2 percent of the time and larger sized pools 1.8 percent of the time. 
This amounts to a total 3.1 percent error. With such a small error, there is little doubt the two populations 
are distinct. In their original study, Dickie and Williams /10/ investigatedthe reasons why the two 
populations should differ. They found no single pool measurement such as porosity, permeability,water 
saturationor combination of pool measurements which uniquely classified the larger pools. The distinguishingfeature 
was that most of the larger pools occurred in the "main" Viking sand whereas the smaller pools occurred 
in restricted sandstone lenses above, below and laterally equivalent to the main  Fig. 8-Statistics 
Fig. 7-PDF sandstone. It suggests there may be genetic differences between the two types of oil accumulation. 
Clearly this requires a more detailed geologic investigation. In this instance dissection has proved 
a valuable tool for the analysis.  4. AN EXAMPLE IN SPORTS In golf as with any sport players take 
their chances. In 1975, 296 professional golfers divided up a total of slightly over $7,000,000 in prize 
money during the Professional_Golfer's Association of America (PGA) tour /11/. As might be expected, 
the money was unevenly divided among the competing golfers. As most people know, Jack Nicklaus was the 
top money winner with his share of the total being close to $300,000. What most people do not know however 
is that the golfer with the least share earned exactly $63. Moreover, the top 30 money winners won over 
half of the--total/amount and of their half, the top 15 took nearly two thirds. These statistics are 
characteristicof sporting activities in which a group of individuals are competing over a regular season 
for prize money which goes mainly to the winner and runner-up of each event and much lesser amounts 
to the rest. It is of interest to consider the nature of the distribution of prize winnings among professional 
 golfers. Through the courtesy of the Tournament Players Division of the PGA who provided the data, 
it was possible to construct a cumulative frequency curve of prize winnings for the PGA golfers who 
competed in 1975. The curve plotted on a log-probabilityscale is shown in fig. 9. If this represented 
a single lornormal distribution, the points would lie more or less along a single straight line. Clearly 
they do not however. This suggests either the data follow some other type of distribution or else the 
data represent a mixture of lognormal distributions. Suppose we choose to pursue the latter possibility. 
We can proceed by dissectingthe curve into lognormal components. The graphic estimates  Fig. 10-Graphic 
   estimation Fig. 9-Mixed   distribution for two components are shown in fig. 10. From these estimates, 
a least-squares best fit of the data is obtained and is shown in fig. 11. The probabilitydensity curves 
of the components are shown also. The theoretical curve departs somewhat from the observed cumulative 
frequency at each tail of the distribution and could be interpretedas an indication of a departure 
 from lognormality. We can examine this more carefully in the goodness-of-fit test. An upper case A 
has been positioned in fig. 11 to indicate a threshold which separates the two subpopulations. We 
consider the quantitative results of dissection given by the statistical summary shown in fig. 12. 
Based on our dissection, there are two groups of golfers divided about equally among the 296 professionals, 
one group with low winnings having a mean of around $3,500 and a second group with high winnings having 
a mean of $57,000. A chi-square test at the 95 confidence level does not reject the hypothesis that 
the data represent a mixture of two lognormal distributions. The two populations, if they are real, 
are reasonably distinct since there is less than a ten percent chance of misclassifyinga golfer into 
one of the two groups. The threshold income which divides the golfers into two groups is around $7,300. 
Obviously, more data about each golfer would be requiredbefore the analysis could proceed further. 
The distribution of prize winnings suggests perhaps that the golfers who participated in the 1975 
PGA tour did not constitute a homogeneous population.  5. CONCLUSIONS A more satisfactory solution 
to the problem of dissecting a mixed frequency distribution has been found through the application of 
inter­active computer graphics. The solution has come about mainly as the result of combining the more 
robust graphical methods with the more precise analytic techniques of regression. The graphical approach 
provides good initial estimates of the parameters to be fitted in the models whereas the analytical approach 
of regression provides final best estimates of the parameters. In an interactive environment, it is possible 
to alternate back and forth between these two methods until an acceptable solution has been obtained 
for a particular set of data. The combined approach to the dissectionof mixed frequency curves has also 
resulted in greater speed at arriving at a solution. Within a matter of minutes, for instance, it is 
possible for someone seated before a terminal to dissect a curve and examine the results. Such an increase 
in speed provides more time for an investigator to think about the results rather than on how to obtain 
them. Finally there is an element of personal satisfaction in carrying on a meaningful dialogue with 
the computer at a level in which both man and machine can understand. To have the ability to interact 
in a dynamic fashion where form and function are closely intermingled brings about a new sense of 
 discovery in data analysis. This level of communication should make for even greater progress in the 
problem of dissection in the future.  REFERENCES <RefA>Karl Pearson, Contributions to the mathematical theory 
of evolution, Phil. Trans, Royal Soc. London, vol. 185A, 1894, 71-110.  C.G. Bhattacharya, A simple 
method of resolution of a distribution into Gaussian components, Biometrics, vol. 23, 1967, 115-135. 
 A. Hald, Statistical theory with engineering applications, John Wiley, New York, 1952. J. Gregor, 
An algorithm for the decompositionof a distribution into Gaussian components, Biometrics, vol. 25, 1969, 
79-93.  Stephen V. Balint, Identifying parameters of hetergeneouslymixed normal populations using 
curve fitting techniques, Intern Training Center Report No. USAMC-ITC-2-71­ 25, NTIS AD 761021, U.S. 
Dept. of Commerce, July, 1971.  J.P. Harding, The use of probability paper for the graphical analysis 
of polymodal frequency distributions,Jour. Mar. Biol. Assoc., vol. 28, 1949, 141-153.  Derek W. Spencer, 
The interpretation of grain size distribution curves of clastic sediments, Jour. Sediment. Petrol., vol. 
33, 1963, 180-190.  A.J. Sinclair, Selection of threshold values in geochemical data using probability 
graphs, Jour. Explor. Geochem., vol. 3, 1974, 129-149. /9 / George S. Koch, Jr. and Richard F. Link, 
Statistical analysis of geological data, John Wiley, New York, 1971.  G.J. Dickie and G.D. Williams, 
Geologic features of oil and gas pools, Twenty Fourth Internat. Geol. Congress, Sec. 16 1972, 103-111. 
 PGA Tour, 1975 Statistics, Tournament Players Division, PGA, Wash., D.C.  
			</RefA>
