
 An optimal randomized logarithmic time connectivity algorithm for the EREW PRAM (Extended Abstract) 
Shay Hcdperin * Uri Department of Computer Tel Aviv University Tel Aviv, 69978 Israel Zwick * Science 
 Abstract Improving a long chain of works we obtain a randomized EREW PRAM algorithm for finding the 
connected compo­ nents of a graph G = (V, E) with n vertices and m edges in O(log n) time using an optimal 
number of O((rrz + n)/ log n) processors. The result returned by the algorithm is always correct. The 
probability y that the algorithm will not com­ plete in O(log n) time is at most n-c for any desired 
c >0. The best deterministic EREW PRAM connectivity algo­ rithm, obtained by Chong and Lam, runs in 
O(log n log log n) time using m + n processors. 1 Introduction Finding the connected components of an 
undirected graph is perhaps the most basic algorithmic graph problem. While the problem is trivial in 
the sequential setting, it seems that elaborate methods should be used to solve the problem effi­ ciently 
in the parallel setting. A considerable number of re­ searchers investigated the complexity of the problem 
in var­ ious parallel models including, in particular, various mem­ bers of the PRAM family. In this 
work we consider the EREW PRAM model, the weakest member of this family, and obtain, for the first time, 
a parallel connectivity algo­ rithm that achieves the minimal possible running time using the minimal 
possible number of processors. The algorithm we obtain is randomized. Relatively simple CRCW PRAM algorithms 
that find the connected components of a graph G = (V, E) deter­ ministica.lly in O(log n) time using 
m + n processcms were obtained by Shiloach and Vishkin [SV83] and by Awerbuch and Shiloch [AS87]. More 
complicated deterministic CRC W PRAM connectivity algorithms that run in O(log n) time us­ ing O((m + 
n)a(m, n)/ log n) processors, where a(rn, n) is a functional inverse of the Ackermann function, were 
ob­ tained by Cole and Vishkin [CV91] and Iwama and Kam­ bayashi [IK94]. The number of processors can 
be reduced * This work forms a part of an M. SC. thems wr]tten by the first author. E-mail addresses 
of authors {hshay,zwick}@math tau ac il. Permission to copy without fee all or part of this material 
is granted provided that the copies are not made or distributed for direct commercial advantage, the 
ACM copyright notice and the title of the publication and its date appear, and notice is given that copying 
is by permission of the Association of Computing Machinery. To copy otherwise, or to republish, requires 
a fee and/or specific permission. SPAA 94-6/94 Cape May, N.J, USA 0 1994 ACM 0-89791-671 -9/94/0006..$3.50 
to the optimal number of O((m + n)/ log n) if randomiza­tion is allowed (Gazit [Gaz91]). Until not long 
ago, the best CREW PRAM connectivity algorithm used 0(log2 n) time (Hirschberg, Chandra and Sarwate [HCD79]) 
(Chin, Lam and Chen [CLC82]). The problem of designing an o(logz n) time CREW PRAM algorithm was open 
for over a decade (cf. [KR90]) until a deterministic 0(log 5 n) time CREW PRAM algorithm that uses m 
+ n processors was obtained by Johnson and Metaxas [JM91]. Johnson and Metaxas have later shown [JM92] 
that their algorithm can also be implemented in the EREW PRAM model. At about the same time, Karger, 
Nisan and Parnas [K NP92] used the in­teresting technique of short random walks on graphs, devel­oped 
initially by Aleliunas, Karp, Lipton, Lovasz and Rack­off [AKL+79], to develop a randomized EREW PRAM 
al­gorithm that runs in either O(log n) time using O((nl+ + m)/ log n) processors, for any e >0, or in 
O(log n log log n) time using O((m + n)/ log n) processors. Their algorithm can also be derandomized, 
using universal sequences, yield­ing a deterministic algorithm that matches the performance of the algorithm 
of Johnson and Metaxas. Finally, Chong and Lam [CL93] obtained a deterministic EREW PRAM algorithm that 
runs in O(log n log log n) time using m + n processors. Concurrently and independently of our work, Radzik 
 [Rad941 had recently obtained a randomized EREW PRAM ;onnec~ivity algorithm that runs in O(log n) time 
using m + n processors. In this work we combhe methods from many of the pre­ vious works, including 
in particular the method of short ran­dom walks used by Karger, Nisan and Parnas [KNP92], to obtain a 
randomized EREW PRAM algorithm that runs in O(log n) time using O((m + n)/ log n) processors. A running 
time of O(log n) is best possible in the EREW PRAM model (Cook, Dwork and Reischuk [CDR86]) and f2((m+n)/log 
n) processors are clearly necessary to obtain a running time of O(log n). Our result is therefore optimaJ. 
Sparse graphs usually pose the greatest difficulty to al­gorithms for finding connected components. Note, 
for ex­ample, that the algorithm of Cole and Vishkin [CV91] is op­timal if m = Q(n log* n) and that the 
algorithm of Karger, Nisan and Parnas [KNP92] is optimal if m = fl(nl+ ) for some ~ > 0. Our ~gorithrn 
is unusual in the sense that it reduces the problem of finding the connected components of a graph G 
= (V, E) to the problem of finding the connected components of a sparse graph G = (V , E ) with O(m + 
n) vertices and edges in which each vertex has a degree of at most three. By carefully controlling the 
growth of the max­imal degree as this graph is contracted during the various stages of the algorithm, 
we are able to obtain our optimal algorithm. Fast and efficient parallel algorithms for special classes 
of sparse graphs, such as planar graphs, were developed by Hagerup [Hag90]. Hagerup obtained in particular 
a deter­ministic O(log n log * n) time optim.d speedup EREW PRAM algorithm for finding the connected 
components of planar graphs. An overview of our algorithm is given in the next section. One simple concept 
that should be defined beforehand is the following: A graph G = (V , E ) is an image of a graph G = (V, 
E) if there exists a mapping h : V ~ V such that for every u, v c V, u and u are in the same connected 
component of G if and only if h(u) and h(v) are in the same connected component of G . The problem of 
finding the connected components of a graph G = (V, E) is equivalent to the problem of finding an image 
G = (V , ~) of G = (~ E) in which every vertex is isolated, together of course with the mapping h : V 
-i V . A graph G = (V , E ) is said to be a partial image of a graph G = (V, E) if there exists a mapping 
h : V ~ V such that for every u, v E V, if h(u) and h(v) are in the same connected component of G then 
u and v are in the same connected component of G. 2 An overview of the algorithm The input to the connectivity 
algorithm is a graph G = (~ E) with n vertices and m edges. The graph is specified using its adjacency 
lists. Our algorithm is composed of two main stages. The first stage takes the input graph G = (V, E) 
and produces an image G = (V , E ) of it in which IE I = O((nz + n)/ log2 n). Isolated vertices in G 
represent complete connected components of G and can be removed. The graph G will now contain at most 
O((m + n)/ log n) vertices. The second stage takes the reduced graph and finds its connected components. 
Both stages take O(log n) time using O((m + n)/ log n) processors. The failure probability of both stages 
is polynomially small in n (i.e., n c for any desired c > O). This first stage (the size reduction) uses 
an adaptation of a method developed by Gazit [Gaz91] to reduce the num­ber of non-isolated vertices, 
followed by an application of a method of Karger, Nisan and Parnas [KNP92] to reduce the number of edges. 
The method of Gazit was developed for the CRCW model and adapting it to run in the EREW model requires 
subtle changes. The second stage (finding the connected components) uses an adaptation of the random 
walk method of Karger, Nisan and Parnas [K NP92]. Their algorithm finds the con­nected components of 
a graph G = (V, E) in O(log n) time using O((nl+ + m)/ log n) processors, for any fixed c >0. Our version 
of their algorithm runs in O(log n) time using only 0( (m + n) log n) processors (the number of processors 
may be easily reduced to m + n). This enables us to find the connected components of the graph G = (V 
, E ), in which IV 1, IE I = O((m+ rz)/ log n), in O(log n) time using O((m+n)/ log n) processors, as 
required. To improve the al­gorithm of Karger, Nisan and Parnas [KNP92] we use some ideas inspired by 
the growth control techniques of Johnson and Metaxas [JM91] and Chong and Lam [CL93]. The algorithm of 
Radzik [Rad94] also uses the random waJk method of Karger, Nisan and Parnas [KNP92] and there are several 
similarities between it and the second stage of our aJgorithm. Our implementation of Gazit s size reduction 
algorithm [Gaz91] and our improvement of the algorithm of Karger, Nisan and Parnas [K NP92] are made 
possible by carefully controlling the degrees of the vertices in the various stages of the algorithm. 
The first stage of our algorithm starts by obtaining an image of G = (V, E) in which the maximaJ degree 
is at most three. Similarly, the second stage starts by obtaining an image of G = (V , E ) in which the 
maxi­mal degree is again at most three. This gives us favorable starting points. We then make sure that 
the degrees of the vertices of the intermediate graphs constructed by our algo­rithm do not grow too 
fast. A graph G = (V, E) with n vertices and m edges in which no vertex is isolated is easily transformed 
into a graph G = (V , E ) with IV I <2_ m vertices and IE I < 3m edges in which the degree of each vertex 
is at most three. This is done by replacing each vertex v with degree k, where k > 3, by a cycle (or 
a path) of k new vertices and by connecting each edge of v to one of the vertices of v s cycle. The obtained 
graph G = (V , E ) is clearly an image of G. Such a transformation can be easily done in O(log n) time 
using O((m + n)/ log n) processors. 3 A load balancing scheme To solve the processor allocation problem 
in the size reduc­tion stage of our algorithm we use a simple load balancing scheme. As this scheme may 
also be useful in other situa­tions, we describe this scheme here in a fairly general setting. Other 
load balancing schem&#38; were obtained by Cole and Vishkin [CV88], for the EREW PRAM, and by Gil, Ma­ 
tias and Vishkin [GMV91], Goodrich [Go091] and Hagerup [Hag92],[Hag93], for the CRCW PRAM. These balancing 
 schemes are much more sophisticated than our balancing scheme. Yet, neither one of them suits our purposes. 
Suppose we are given a (possibly randomized) loosely specified parallel algorithm AL G that uses m virt 
uid pro­cessors. The execution of ALG is composed of K phases. The i-th phase takes t, ~ 1 time units 
and at most m, of the virtual processors are active in it. Assume, at first, that the set of active processors 
at the i + l-st phase is a subset of the set of processors active at the i-th phase. At the end of each 
phase, each virtual processor that participated in that phase knows whether it should stay active during 
the next phase or whether it should become and stay idle during all subsequent phases. Apart from this 
assumption, we do not assume anything else about the sets of active processors. The total time required 
by the loosely specified algorithm isclearly T = ~ fi~ ti.The total amount of useful work performed by 
the active processors is W = ~~1 m;t; . We would like to obtain an EREW PRAM implementation of this algorithm 
using P processors whose total running time is as close as possible to O(W/P + T). We assume that P,T 
s m and that W/P = Q(log m), We divide the actual processors into P/b groups of size b = log m (we assume 
for simplicity that b is an integer divid­ing P). Each such group is initially assigned with rob/P of 
the virtual processors. To record the allocation of the virtual processors to the actual processors, 
we use a P/b x rob/P matrix iWAT. The virtual processors allocated to the i-th group are listed in the 
i-th row of this matrix. Before we start the simulation of the loosely specified algorithm, we apply 
m independent random cyclic rotation on each one of thernb/P columns of MAT. The amount bywhicheachcol­ 
umn is rotated is chosen uniformly among {O, 1, . . . , P/b 1}. This can be easily implemented on an 
EREW PRAM in O(m/P + log m) time using P processors. Perhaps surpris­ingly, this simple mean guarantees, 
as we shall see, a rela­tively balanced partition of the work among the processors throughout the course 
of the algorithm. Each phase of ALG is simulated in the following way. Each group of b actual processors 
equally divides the virtual processors allocated to the group among its b members. Let L, be an upper 
bound on the number of virtual processors allocated to a single actual processor before starting to sim­ulate 
the i-th phase of ALG. The i-th phase of ALG can be easily simulated then in 0( Li t, ) time. After the 
simulation of the i-th phase is completed, each group of b processors compresses the list of virtual 
processors allocated to it and removes from it all the virturd processors that become idle. Once the 
active processors are compressed, it is easy to di­vide them equally among the processors of the group. 
Such a compression of a list whose size is at most L, b using b processors can be easily implemented 
on an EREW PRAM in O(L; + log(Lib)) = O(Li + log log m) time. The total amount of time required for the 
simulation of the i-th phase is therefore O(L, t, + log log m). Let A > 3 be a fixed constant. We claim 
that with a probabfity of at least 1 rn-nfAJ, the number of vir­tual processors allocated to each actual 
processor during the simulation of the i-th phase of ALG is at most L, = A . max{ ~, 1}. Equivalently, 
the number of virtual proces­sors allocated to each group of b = log m processors is at most ~, = A . 
max{ ~, log m}, again with a probability of at least 1-m-Q(A). As ALG may be a randomized algorithm, 
the exact set of active processors at the i-th phase may be a random vari­able. We show that for every 
possible set S, of at most m, active processors at the i-th phase, the probabfity that a certain group 
of b actual processors is assigned with more than ~, processors at the i-th phase is at most m-n(A). 
Multiplying this bound by PK/b, we get a bound on the probabfity that any group of processors will get 
too much work during any phase of the algorithm. As PK/b = mOfl), the obtained bound is still of the 
form m-n(A). To obtain this probability bound we use one of the stan­dard Chernoff bounds (see Alon and 
Spencer [AS92], p. 237). Let X1, XZ,.. ., X. be mutually independent random variables, not necessarily 
identically distributed, that take only the values O and 1. Let X = ~~=1 Xl and let p := .E[X]. Then, 
for every /3 >1 we have Pr[X z ~p] s (e@-l/~~)~. A simple manipula~on shows that for any M z p we have 
Pr[X ~ lkf] ~ (ep/lf) . Suppose that S, is the set of active virtual processors at the i-th phase of 
ALG. Consider a specific group of b ac­tual processors. Let X, be a random variable which is 1 if the 
j-th virtual processor originally allocated to the group is active in the i-th phase of the algorithm, 
and O otherwise. Then, X = ~~ fp Xj is the number of active processors al­located to the group at the 
i-th phase. As a random rotation was applied on the j-th column of MAT, the j-th proces­sor allocated 
to the group was chosen uniformly at random among the processors of the j-th column of MAT. Thus E[X,] 
= Pr[X3 = 1] = m,j /(P/b), where m,j is the number of processors in the j-th column of MAT that are active 
at the i-th phase, (i.e., members of S;). So, v = -E[X] = Xj mt~/(P/b) < mib/P. The random rotations 
performed on the columns of MAT are mutually independent. The Xj s :re therefore also mutually independent. 
As ~i ~ AK and L$ z A log m, we may apply the Chernoff bound and obtsin that Pr[X > Z:] < (ep/L;)- ; 
< (e/A) AIOgm. AS A > 3, this is indeed of the form m Q(3 as promised (it is even of the form m-n(A]O 
 )). The simulation of the i-th phase takes therefore, with very high probability y, at most 0(( ~ + 
l)t, + log log m) time. The total time required for the simulation of ALG is therefore O(~fil(~ + l)t, 
+ Kloglog m) = O(W/P + T+ K log log m). To this we should add the O(log m) time re­quired for the initial 
random rotations. This term is gobbled up by the 0( W/ P) term. Comparing this to the expression we hoped 
for, we see that we got an extra O(K log log m) term. In the situation we encounter in the first phase 
of our con­nectivity algorithm we have W = O(m), P = O(m/ log m), K = O((loglog m)z) and T = O((log log 
m)3). The ex­tra O(K log log m) term is therefore negligible and the total running time will be O(log 
m). To allow the specification of the size reduction algorithm of the next section in the framework set 
in th~ section, we have to allow the loosely specified algorithm a bit more free­dom. The loosely specified 
algorithm is allowed to keep a stack to which the configurations of the active processors in each phase 
are pushed. The algorithm is then allowed to re­turn to a previously used configuration, reviving some 
of the virtual processors if necessary. It is easy to check that the simulation result obtained holds 
also for this more general class of loosely specified algorithms. 4 Reducing the size of the graph In 
this section we describe the first stage of our algorithm. This stage receives the input graph G = (V, 
E) with n ver­tices and m edges and produces an image G = (V , E ) of it with at most O((m + n)/ logz 
n) edges and non-isolated vertices. The input graph is transformed, in the beginning of the first stage, 
into a graph with O(m + n) vertices and edges with maximal degree at most three. We may therefore as­sume, 
for simplicity, that the input graph G = (V, E) is already of that form, i.e., that the maximal degree 
of G is at most three and that therefore m < 1. 5n. The size reduction is performed in two substages. 
In the first we obtain an image G = (V , E ) of G with at most O(n/ log5 n) non-isolated vertices. This 
is done using an adaptation of Gazit s method [Gaz91]. We then obtain an image G = (V , E ) of G with 
at most O(n/ log5 n) non­isolated vertices and O(n/ log2 n) edges. This is done using a simple idea of 
Karger, Nisan and Parnas [KNP92]. In the next subsection we give a high level description of the procedure 
RED UCE that performs the size reduction. We then describe, briefly, each of the procedures used by REDUCE. 
4.1 High level description The size of the graph G = (V, E) is reduced by running the procedure RED UCE 
whose description is given in Fig­ure 1. The procedure REDUCE calls the procedures CO U­ REDUCE(GO) input: 
A graph GO = (V, E) with IVl = n vertices, 1111< 1.5n edges and maximal degree 3. output: An image G 
= (V , E ) of Go with at most O(n/ logz n) edges and non-isolated vertices. complexity: O(log n) time 
using O(n/ log n) processors. for i + Oto k 1 (where k = O(loglog n)) do begin G; t COUPLE(G,) G,+, 
+ DLLUTE(G;) end G w IMAGE(Go , Gk~ G t-SPARSIFY(G ) return(G )  Figure 1: The procedure REDUCE. PLE, 
DILUTE, IMAGE and SPARSIFY. The specifications of these procedures are given in Figure 2. The procedure 
REDUCE employs the general strategy used by Gazit [Gaz9 1]. The implementation details however are completely 
different. REDUCE begins by performing O(log log n) phases. In each phase a call is made to the pro­cedure 
COUPLE that reduces, with a very high probably, the number of non-isolated vertices in the graph by a 
factor of at least /3, where @ < 1 is a fixed constant. The proce­dure COUPLE achieves this by first 
finding a large enough matching in the graph. It then contracts each edge of this mat thing. The words 
mat thing and edges are quoted as the matching found is actually a matching in the square of the graph. 
Each edge in this matching may actually be a path of length two in the current graph. The reduction of 
the number of non-isolated vertices by a constant factor achieved by COUPLE does not imply a similar 
reduction in the number of edges. To obtain a linear work algorithm we have to dilute the edges of the 
graph at each phase. This is done by the procedure DILUTE that makes sure that the number of edges in 
the graph is at most a constant factor times the number of non-isolated vertices in the graph. To achieve 
this, DILUTE deletes certain edges from the graph. This may cause certain connected components of the 
graph to break into several smaller connected components. The number of such new connected components 
formed by DI- LUTE is relatively small however. The graph obtained after these O(log log n) phases has 
a small enough number of non-isolated vertices but is only a partial image of G. The procedure IMAGE 
is then ap­plied to produce an image of the original graph with the same set of vertices. This may slightly 
increase the number of non-isolated vertices and drastically increase the num­ber of edges. Finally, 
a call to the procedure SPARSIFY is made. This call reduces the number of edges by the required amount. 
Before describing the procedures COUPLE, DILUTE, IMAGE and SPAIK5TFY in more detail, we show that RE-D 
UCE does fulfill its task, assuming that these four proce­dures meet the specifications given in Figure 
2. The proce­dures COUPLE, DILUTE and SPARSIFY are randomized and they may fail to achieve their goals 
but this will only happen with a very small probability (n-c for any c > O). Let n, be the number of 
non-isolated vertices in G,, let m: be the number of edges in G, and let di be the maximal degree of 
G, (refer to the pseudo-code given in Figure 1). At the start we have no ~ n, mo ~ 1.5n and do <3. As 
each call to COUPLE reduces the number of non-;solated vertices by a factor of /3, we get that n, < ~ 
n. A call to COUPLE at most doubles the maximal degree. Thus d, s 3.2 . The call to DILUTE ensures that 
m, < An, and that G, cent ains at most O(n i / log5 n) new connected components. The number of non-isolated 
vertices in Gk is at most O(n/ log5 n), provided that k ~ 5 logll@ log n. As the transition from G, to 
G,+l creates at most O(n, / log5 n) new connected components, the number of connected components in Gk 
that do not correspond to original connected components of GO is 0(~~~~ n,/ log5 n), which is O(n/ log5 
n), as the n, s form a decreasing geomet­ric sequence. Each isolated vertex in Gk that is not an isolated 
vertex in G is such a new connected component. The number of non-isolated vertices in G is therefore 
also O(n/ log5 n). The call to SPARSIFY now generates an im­age G of GO with at most O(n/ logz n) edges, 
as required. The adjacency lists of the graph GI are given in an n x d, array N,. Each vertex v of G, 
is a vertex of GO and the row that corresponds to it in N, gives all the edges incident to v in G, (there 
may be up to d, such edges). The edges incident to v occupy consecutive entries at the start of v s row. 
The entry in v s row cent aining the edge (v, u) also cent ains a pointer to the entry containing the 
opposite edge (u, v) in u s row. The rows of Ni that correspond to vertices of Go that are not vertices 
of G, are not in use. The description of REDUCE, CO UP.LE and DILUTE is greatly simplied by assuming 
that each edge and each non­isolated vertex of G, has a virtual processor allocated to it. As a vertex 
becomes isolated and as an edge is deleted from the graph, the virtual processor associated with it be­comes 
idle, This fits the general framework of the previ­ous section. The execution of DILUTE involves a certain 
backtracking process that uses the stack mechanism. The complexity bounds of COUPLE and DILUTE specified 
in Figure 2 are given in this loosely specified setting. We now bound the complexity of REDUCE. Consider 
at first the complexity of the for loop in the idealized setting, assuming that a virtual processor is 
initially allocated to every vertex and edge of GO. The total time taken by the COUPLE(G:) input: A 
graph G, = (Vl, El with ni non-isolated vertices, ml edges and maximal degree at most di. ) output: 
An image G; = (~ , Ej) with at most @n, non-isolated vertices, where /3 < 1, and maximal degree at most 
2d,. complexity: O(log d~) time and O(mi log d;) work using mi + ni virtual processors. DILUTE(G,) input: 
A graph G; = (Vi, Ei ) with ni non-isolated vertices, mi edges and maximal degree at most di. output: 
A subgraph G: = (Vi, E;) of Gi such that IE{l < A. ni and such that G; cent sins at most O(ni / log5 
n) new connected components. complexity: O((log di). log log n + (log log n)2 ) time (in O(log log n) 
phases) and O(mi log di ) work using mg + n~ virtual processors. IMAGE(G, G ) input: A graph G = (V, 
E), with IVl = n and IEI = m, and a partial image G = (V , E ) of it. output: An image G = (V , E ) of 
G, i.e., an image of G with vertex set V . complexity: O(log n) time using O((m + n)/ log n) processors. 
 SPARS1l Y(G) input: A graph G = (V, E) with IVI = n vertices and IEI = m edges. output: An image G = 
(V , E ) of G with at most O(n log n) edges. complexity: O(log n) time using O((m + n)/ log n) processors. 
Figure 2: The specifications of COUPLE, DILUTE, IMAGE and SPARSIFY, for loop is then k-l O(~[(log di).log 
log n + (log log n)2]) = O((log log n)3) i=O (in O((log log n) ) phases), while the total work is k-l 
k-1 O(Z mi log di) = O(n . ~ i/3i) = O(n). ,=0 1=0 Using the load balancing scheme of the previous setction 
the for loop can be simulated, with very high probability, in O(log n) time using O(n/ log n) processors. 
The procedure IMAGE again requires O(log n) time us­ing O(ra/ log n) processors. Finally, the call to 
SPARSIFY also takes only O(log n) time using O(n/ log n) processors se it is applied to a graph with 
O(n/ log5 n) vertices and O(n) edges.  4.2 The procedure COUPLE Let G = (V, E) be a graph with n non-isolated 
vertices and maximal degree at most d. To reduce the number of non­isolated vertices in G by a const 
ant fraction, we find a match­ing in the square of G whose size is, with very high probabil­ity, at least 
n/13. We then contract the edges of thw match­ing and obtain, with very high probabfity, a graph with 
at most 12n/13 vertices and maximal degree at most, 2d. We may therefore take $ to be 12/13. Assume, 
for simplicity, that no vertex in V is isolated. For every v E V we let ~(v) be a random neighbor of 
v, The graph Gf = (V, Ef), where .Ef = {(v, ~(v)) : v c V} is a pseudo-forest, i.e., a directed graph 
in which the outdegree of each vertex is one. Let S c V be the set of vertices v for which the indegree 
of $(v) in Gf is at least 2. The vertices of S are divided into disjoint sets of siblings, where u and 
v are siblings if and only if $(u) = ~(v). The size of each such set is at least 2 and at most d. Within 
a set of k z 2 siblings, we can define a matching of size at least (k 1)/2 z k/3. This gives us a matching 
whose size is at least ISI /3 between the elements of S. We now remove the elements of S from G~. The 
graph Gf is now composed of isolated vertices, paths and cycles. Let 1 be the set of isolated vertices, 
let P be the set of vertices contained in paths (of length at lead 2) and let C be the set of vertices 
contained in cycles (of length at least 2). We now apply the random mating method of Reif [Rei84] (see 
also [MR89]). Each vertex v G P U C chooses, with equal probabilities, a random color color(v) from {O, 
1}. If coior(v) = O and color(~(v)) = 1, assuming that ~(v) exists, we add (v, ~(u)) to the matching. 
On each path or cycle of size k ~ 2 there are at least (k 1)/2 ~ k/3 disjoint pairs. Each such pair 
joins the matching, independently, with a probability of 1/4. Finally, note that III s ISI, as each element 
in 1 points to an element in S and no two elements in 1 point to the same element in S. The expected 
size of matching that will be obtained is therefore at least n/12. Using the Chernoff bounds it is easy 
to show that the probability that the size of the matching will be less than, say, n/13 is exponentially 
small. The exact computation can be found in [MR89] and is omitted from t KM extended abstract. We also 
omit, due to lack of space, the implementation details of this procedure. 4.3 The procedure DILUTE Let 
G, = (w, E,) be the graph handed to DILUTE. Let ni be the number of vertices in G,. The procedure DILUTE 
should delete edges from Gi so that the number of edges will be at most A. n,, for some A >0. Yet, the 
number of new connected components formed by these deletions should be at most O(ni / 10g5 n). The procedure 
DILUTE attempts to identify a set of extrovert vertices. This should be a relatively small set of vertices 
which is responsible for a substantial part of the 4.5 The procedure SPARSIFY edges in the graph. Edges 
connecting two extrovert vertices are then removed from the graph. The idea and the term ext revert are 
taken from Gazit [Gaz91]. The procedure DILUTE performs k = O(log log n) iter­ations. Denote by G,,l 
the graph obtained after j of these iterations. An edge of G,,j is either liue or dead. At the start, 
all the edges of G,,o = G, are live. At each iteration some of the live edges become dead. At the j-th 
iteration a virtual processor is allocated to each live edge of G,j. Let a < 1 be a constant such that 
~ < cr4, where P(= 12/13) is the constant used in COUPLE. Let d = [a/(l ~)1. An mtrouert vertex of G,,J 
is a vertex adjacent to less than d live edges. An introvert edge is an edge adjacent to an introvert 
vertex. Note that live edges may become introvert. In each one of its k = O(log log n) iterations, the 
procedure DILUTE performs the following operations: 1. Each live edge dies with a probability of 1 a. 
 2. Introvert vertices and edges are removed 3. The procedure CO UP.LE is called to reduce the num­ber 
of non-isolated vertices by a factor of at least /3.  The vertices of the graph G,,k obtained after 
these k = O(log log n) iterations are taken to be the extrovert vertices. Their number is at most O(n, 
/ log5 n). Each edge in G,yk corresponds to an edge of Gi,o. An edge of G,,o may not be present in G,,k 
for two reasons. It was either removed in one of the iterations because it became introvert, or it was 
removed in one of the iterations by the call to COUPLE as it became an internal edge. When called from 
DILUTE, the procedure COUPLE does not remove parallel edges. Each edge of G, = Gt,o can therefore be 
classified as either an ext revert, inrovert or an internal edge. The extrovert edges are deleted from 
Gi. It is easy to see that this increases the number of connected components of G, by at most O(n, / 
log5 n). It can be shown that the number of edges that become introvert at the j-th iteration is, with 
very high probabil­ity, at most O(cr~ n, ). The proof is a generalization of the proof of Lemma 5.13 
of Gazit [Gaz91]. The total number of intovert edges in G, is therefore, with high probability, at most 
A . n,, for some suitably chosen A >0. The graph G, may cent sin a large number of edges clas­sified 
as internal edges. We would like to remove most of the internal edges from the graph without affecting 
its connec­tivity. For that purpose, we identify the set of edges nsed by COUPLE to form its matchings. 
These edges are classified as connectors. There are at most 2n, such edges. By re­taining all these edges 
we may remove all the other internal edges from the graph without creating any new connected components. 
The number of edges in the graph is now at most An, (where A = A + 2) as required. 4.4 The procedure 
IMAGE The implementation of IMAGE is straightforward. Let h : V ~ V be the mapping from the vertices 
of G to those of G . We assume that each vertex v E V has a list con­taining the vertices of V mapped 
to it. The procedure lM- AGE return the image G = (V , E ) of G, where E = {(h(u), h(v)) : (u, v) E E, 
h(u) # h(v)}. The image G is easily constructed in O(log n) time using O((m + n)/ log n) processors. 
Let G = (V, E) be a graph with n vertices and m edges. Let G = (V , E ) be a random subgraph of G obtained 
by selecting each edge of E with probabdity q. Karger et al. [KNP92] (see also Karger [Kar93]) show that, 
with very high probability, the number of edges from E that connect dif­ferent connected components in 
G is at most O(n log n/g). The procedure SPARSIFY generates a random subgraph G = (V, E ) of G by choosing 
each edge of E with proba­bility 1/ logz n. The graph G will have, with very high probability y, at most 
m = O(m/ logz n) edges and n = O(m/ logz n) non-isolated vertices. It then finds the con­nected components 
of G using the algorithm that consti­tutes the second stage of our algorithm. This algorithm uses O((n 
+ m ) log n) = O(m/ log n) processors. Let G = (V , ~) be the image of G returned by this algorithm. 
The procedure SPARSIFYterminates by calling IMAGE(G, G ). The image produced will contain, with very 
high probability, at most O(n log3 n) edges as required. The call to IMAGE takes again O(log n) time 
using O((m+n)/ log n) processors. 5 Finding the connected components using random walks In this section 
we describe an algorithm for finding the con­nected components of a graph G = (V, E) with n vertices 
and m edges in O(log n) time using O((rrz + n) log n) pro­cessors. This algorithm constitutes the second 
stage of our optimal O(log n) time algorithm. The number of processors used by the algorithm de­scribed 
in this section can be easily reduced to O(n + m). This however is not of much importance as we can, 
anyway, use the algorithm of the previous section to reduce the size of the input graph by any desired 
poly-logarithmic factor. The algorithm we describe uses short random walks, as suggested by Karger, Nisan 
and Parnas [KNP92]. Before describing our algorithm, we shortly review the algorithm of Karger et al. 
and sketch the changes that we make to it. 5.1 The algorithm of Karger, Nisan and Parnas Barnes and Feige 
[BF93] showed that a random walk of length sin a connected graph is likely to visit at least 0(s /3) 
vertices (or all the vertices of the graph). More precisely, for some constant co > 0, the probability 
that such a random walk visits at least co .S113 vertices is at least 3/4. By per­forming c1 log n independent 
random walks of length s, for some large enough c1 > 0, the probability y that we visit at least c0.s113 
vertices is at least 1 n c, for any desired c >0. The algorithm of Karger et al. [KNP92] initiates @(log 
n) independent random walks of length @(n ), for some e >0, from each of the n vertices of the graph 
G. Two vertices u and v are then said to be virtual neighbors if a random walk started at u passes at 
v or if a random walk started at v passes at u. This defines for every vertex v in G a virtual neighborhood 
C(v) that, wit h very high probability y, cent ains at least Ct(n C/3) vertices, or all the vertices 
in the connect ed component of w. Each vertex v then looks at all vertices within (virtual) distance 
two of itself (i.e., at all the vertices of C(C(V))) and hooks, by setting its parent pointer, to the 
vertex of this set that has the largest serial number (this vertex may be v itself). It is easy to see 
that at most ~1 cls trees that do not correspond to complete connected components are formed in this 
way. Each such tree is then contracted and a graph with at most O(nl /3) non-isolated vertices is obtained. 
Repeating this process 0(1/c:) times yields the connected components of the graph. The algorithm of Karger 
et al. [KNP92] uses only O(1) phases of random walks, but each such phase takes (>(log n) time using 
O(nl+c + m) processors. A sli htly improved f version of their algorithm uses only O((n] + m)/ log n) 
mocessors. Our algorithm performs about O(log log n) phases of ran­dom walks. The first random walks 
performed are extremely short and require substantially less than O(log n) time. The random walks become 
longer and longer as the number of non-isolated vertices in the graph becomes smaller and the number 
of processors allocated to each such vertex becomes larger. The lengths of the random walks performed 
form a doubly exponential sequence. We use a simple scheme to allocate more and more pro­cessors to the 
surviving vertices of the graph. To implement the short random walks quickly we again find it helpful 
to control the growth of the degrees of the vertices of the graph. We show that random walks of length 
s from each vertex of a graph with n vertices, m edges and maximal degree d can be simulated in O(log 
s + log d) time using sn + m proces­sors. Our algorithm begins again by finding an image of the input 
graph whose maximal degree is at most 3. The data structures that are used by the algorithm and that 
allow us to allocate more and more processors to the surviving vertices of the graph are described in 
the next subsection.  5.2 Graph organization We assume again that the input graph G = (V, E) has a maximal 
degree of at most 3. Let n be the number of vertices in G and let m be the number of edges in G. It follows 
that m s 1.5n. We have O(n log n) processors at our disposal. We alIocate O(log n) processors to each 
vertex of G and a single processor to each edge of G. Each processor allocated to a vertex knows it serial 
number among the processors allocated to that vertex. The vertices of G are organized in stars. The leaves 
of these stars get the attribute DONE. The roots of these stars get the attribute STAR. Each DONE vertex 
v has a pointer root(v) to the root of its star. If u is a star vertex then root(u) = a. Each STAR vertex 
u has a list vertices(u) of all the vertices in its star (including itself) and a field size(u) that 
holds the length of thk list (i.e.; the size of the star). Each vertex v in vertices(u) knows it serial 
number in the list. In this way a STAR vertex v can be easily assigned with size (v) olog n processors. 
At the beginning of the algorithm all vertices get the attribute STAR. When a star is formed, the adjacency 
lists of all the ver­tices participating in it are amalgamated into a single adja­cency list. An edge 
(a, ~) in the original graph is converted into the edge (root(z), root(y)). A STAR vertex v lhas a list 
N(v) of the edges that connect the STAR of v to the other STARS. There is a pointer from the edge (u, 
v) in N(a) to the edge (v, u) in N(v) and vice versa. The degree of v is the length of the list fV(v). 
As the original graph has maxi­mal degree 3, the degree of a STAR v is at most 3. size(v). The processor 
allocated to the edge (u, v) has a pointer to that edge in the list N(u). When the adjacency list of 
a star is formed, internal and parallel edges are removed. The processors allocated to these edges become 
inactive. The STAR vertices of the graph are organized in trees. Each STAR vertex v has a pointer parent(v) 
to another STAR vertex of the graph. If v is a root of a tree in the forest thus formed then parent(v) 
= v. Each STAR ver­ tex v has a list children(v) of all the STAR vertices that point to it. Each STAR 
vertex u in the list chikh-en(v) has a pointer to its place in the list. The size size(T) of a tree 
T is defined as the sum of the sizes of all the STARS in that tree. Clearly, the STARS in a tree T have 
together at most 3.size(Z ) neighbors. At the beginning of the algorithm each STAR vertex v forms a tree 
of its own and parent(v) = v.  5.3 High level description To get the algorithm started we perform a 
single step in which every vertex hooks on the largest vertex at a distance at most two from itself. 
This can be easily implemented in O(1) time as the degree of each vertex is at most 3. Every non-isolated 
vertex in G is now contained in a tree whose size is at least 2. Define the sequence so = 2 and s,+] 
= s~ 3. Surely, s, = 2(4/3) . The algorithm performs k = [log413 log nl = O(log log n) iterations. Note 
that Sk ~ n and that (4/3)~ = O(log n). The i-th iteration includes a contraction phase in which trees 
whose sizes are at most sit 1 are contracted, followed by a hooking phase that uses random walks of length 
si. An outline of the algorithm is given in Figure 3. Before iteration O $ z < k of the algorithm is 
executed, the size of each tree m G, that does not correspond to a complete connected component of G, 
is at least s,. The size of each STAR vertex, however, is at most s,. The i-th iteration starts by a 
call to CONTRACT(si+l ). This call contracts each tree in G whose size is at most s,+] into a single 
STAR vertex. The STAR vertices that corre­spond to trees that were contracted during this iteration get 
the attribute ACT (active). All STAR vertices contained in trees that were not contracted get the attribute 
INA (inac­tive). ACT vertices that correspond to complete connected components are removed from the graph. 
The size of each remaining ACT vertex is at least s,, as we assumed that the size of each tree in G (that 
does not correspond to a com­plete connected component of G) at the entrance to the i-th iteration is 
at least s,. The execution of these operations takes O(log S,+l + log log n) time. A call is next made 
to HOOK(S,). This call starts @(log n) independent random walks of length @(si) from each ACT vertex 
of G. Each such random walk is stopped if it reaches an INA vertex. As the size of each ACT vertex is 
at least s,, each ACT vertex has at least s, *log n processors allocated to it. As the size of each ACT 
vertex is at most S,+l, its degree is at most 3. Si+l. These random walks can therefore be implemented 
in O(log s,+ 1) time. Using these random walks we obtain, with very high probability, for every ACT vertex, 
a virtual neighborhood that is either of size at least s~ 3, or contains a INA vertex, or contains all 
the vertices of the corresponding connected component. Using these virtual neighborhoods, we hook ACT 
ver­tices on either INA vertices or other ACT vertices so that the size of each tree that does not correspond 
to a com­plete connected component is, with very high probability, at least S,+I. Our hooking method 
is slightly different from that used by Karger et al. as we want to ensure not only that the number of 
trees formed is small enough but also to Convect input: A graph G= (V, E) with n vertices and maximal 
degree at most 3. output: The connected components of G. complexity: O(log n) time using O(n log n) processors. 
 SO+2 for i t O to k 1 (where k = O(loglogn)) do begin 4/3 S1+1 ~ Si C70NTRACT(S,+1 ) HOOK(S,) end 
 Figure 3: The procedure CONNECT. ensure that each tree formed is large enough. The hooking process takes 
O(log s;+l + log log n) time, The O(log log n) term accounts for the fact that we have to consider the 
re­sults of ~(log n) independent random walks from each ver­tex. When the hooking process is done we 
ignore the ACT and INA attributes. This completes the i-th iteration. The graph now sat­isfies the conditions 
that should be met before the i + l-st iteration. The i-th iteration takes O(log s;+l + log log n) time, 
us­ing O(rz log n) processors. The total running time of second stage is therefore k O(~(lOg Si+l + 10g 
10g n)) = O(lOg n). i=O We now describe in some more detail the contractions, the random walks and the 
bookings phases. 5,4 Contraction The input to the contraction phase is a graph in which each tree of 
STARS is of size at least si (ignoring trees that corre­spond to complete connected components) and each 
STAR is of size at most s,. The degree of each STAR is therefore at most d = 3. s:. We are supposed to 
contract each tree T whose size is at most s = si+l and replace it by a single STAR vertex. If r is the 
root of a tree T of size at most s, then r will stay a STAR vertex. All the other STAR vertices of T, 
including the DONE vertices of these stars will now point to r. The adjacency lists of all the STAR vertices 
in T will be amalgamated into a single adjacency list iV(r). Dur­ing this amalgamation, duplicate and 
parallel edges will be removed. The lists vertices(o) for every STAR vertex v in T will also be amalgamated 
into the single list vertices(r). The list children(r) will be reset to contain only r itself and parent(r) 
will be set to r. We shortly sketch the implementation details of the con­traction phase. Using the children 
arrays of the STAR ver­tices we can easily define an Euler tour in each tree. This takes only O(1) time. 
We then perform the first O(logs) steps of the naive list ranking algorithm in an attempt to sum the 
sizes of the STAR vertices in each tree. We can then tell which trees are of size at most s (note that 
a tree of size at most s contains in particular at most s STAR vertices). We then use the list ranking 
algorithm to com­pute the prejh sums of the sizes of STAR vertices contained in each small enough tree. 
This again takes only O(logs) time. It is then easy to amalgamate the lists vertices(v) associated with 
the STAR vertices in each tree. The amal­gamation takes O(log d) time as O(log d) time is needed in the 
EREW model to distribute the corresponding prefix sum to the at most s, ~ d DONE vertices associated 
with each STAR vertex. In a similar manner we amalgamate the lists N(v) associated with the STAR vertices 
in each tree. This amalgamation again takes O(log s + log d) time. If v is a STAR vertex in a tree T 
that was contracted to its root r, then every edge (u, v) connected to v should now be re­placed by an 
edge (u, r). This is done using the pointer from (v, u) in N(o) to (u, v) in N(u). Internal edges, i.e., 
edges between two STAR vertices contained in a contracted tree, are removed. All the adjacency lists 
are now sorted and parallel edges are removed. As the length of each adja­cency list is at most 3s, the 
sorting takes only O(logs) time. The total time required by the contraction stage is therefore O(logs 
+ log d) = O(log s,+ 1). For all these operations we use only one processor for each vertex of the graph 
and at most one processor for each edge. 5.5 Hooking The input to the hooking phase is a graph produced 
by the contraction phase. The forest defined by the parent pointers is composed of trees whose size is 
at least si+l (ig­noring again trees that correspond to complete connected components) and STAR vertices 
that result from trees con­tracted during the contraction phase. Each such STAR ver­tex is of size at 
least si. These STAR vertices have the attribute ACT. The STAR vertices contained in trees all have the 
attribute INA. Each ACT vertex v has at least size(v) log n ~ s~*log n processors allocated to it. Using 
@(log n) independent random walks of length @(s,), we obtain for every ACT vertex w a virtual neighborhood 
that, with very high probability, either includes at least s: 3 vertices, or includes an INA vertex, 
or includes all the ver­ tices in the connected component of v. We aleo make sure that the virtual neighborhood 
of v wiJl not be too big, so that the vertex with the largest serial number in this neigh­borhood could 
be found in O(log S,+l ) time. The way these neighborhoods are obtained is described in the next subsec­tion. 
Their construction takes O(log s,+l + log log n) time. Each STAR vertex in G has an associated serial 
number. We want the serial numbers of the INA vertices to be larger than those of the ACT vertices. To 
achieve this we add n to the serial number of each INA vertex. Each ACT vertex v scans its neighborhood 
C(v). If C (v) contains a vertex v with a serial number larger than v then v hooks to u, i.e., parent(v) 
-u. If such a vertex is not found, then v hooks to the vertex with the largest serial number in C(C(V)). 
This vertex may be v itself. This again takes O(log S;+l + log log n) time. Each ACT vertex hooks either 
to itself or to a vertex with a larger serial number. No cycles are thus formed. Suppose now that an 
ACT vertex v hooked to itself, i.e., parent(v) = v, and became the root a tree. We claim that all the 
vertices in C(v) hooked to v. Let u c C(v). As u < v and v c C(u), the vertex rL hooks on some vertex 
w c C(u). As w c C(u) ~ C(C(V)), we get that w s v and u does indeed hook to v. We now show that the 
size of each tree (which does not correspond to a complete connected component) in the graph after the 
hooking process is at least Si+l. Each tree of INA vertices was of size s,+l to begin with. Some ACT 
ver­tices may have hooked to such trees. This will only increase their size. Consider now a tree T formed 
entirely from ACT vertices. Let r be the root of this tree. We have shown that all vertices of C(r) will 
hook on r and thus belong to T. The 1/3 set C(r) contains at least s, vertices the size of each is at 
least S;. The size of T is therefore at least s: 3.s, := S,+l as required,  5.6 Random walks To implement 
random walks of length s starting flrom each ACT vertex we use an n x s array called walk. ,4 similar 
array was used by Karger et sJ. [KN P92]. We actually use L = @(log n) such arrays to implement @(log 
n) random walks of length s from each vertex. The entry walk[v, t], for an ACT vertex v and 1 s t < s, 
is set to a randomly chosen vertex from v s neighbors-fist N(v). Each entry is the walk array is referred 
to as a cell. The value walk [v, t]represents a pointer from the cell [v, t] to the cell [walk [v, t], 
$ + 1]. These pointers define trees whose roots are either cells that correspond to INA vertices or cells 
at time s. If c = [v, t], we say that v is the vertex that corresponds to the cell c. The matrix walk 
defines a random walk for each ACT vertex v as follows: U1 = v and ut = walk[tit-1, t 1], for 1< t s 
s. Random walks defined by independent] y chosen walk arrays are clearly independent. Let T~v,,l be the 
tree containing the cell [v, t]of the i-th walk array. We would like to define the virtual neighbor­ 
hood C(v) of an ACT vertex v as the union of the vertices contained in the trees T(fl,tl for 1 < t < 
s and 1 S z < L. This includes much more than L = ~(log n) independent random walks of length s starting 
at each ACT vertex and thus all the virtual neighborhoods will be large eno,ugh with very high probability. 
This definition would alscl satisfies the requirement that v c C(u) if and only if u c C(v). The neighborhoods 
defined in this way may, however, be too big and finding the largest vertex in each neighborhood would 
then consume too much time. Luckily, most of the trees created will not be too big. Lemma 5.1 Z%eexpected 
size oi a tree T/ ,01 co~ta:~iw the cell [v, 0] is at moat sz.d, where d is a bound on the maximal degree 
of the STAR graph. The simple proof of this Lemma will be given in the full version of the paper. It 
immediately follows, using Markov s inequality, that the probability that the size of T~v,ol is greater 
than 4s2. d is at most 1/4. The probabfity that the random walk starting at v will visit less than S1 
3 vertices is also at most 1/4. The probability y that either one of these bad events will happen is 
therefore at most 1/2. Thus, with very high probability, at least one of @(log n) tress Tfv ,01 that 
correspond to an ACT vertex v will con­ t ain cells corresponding to at least S1 3 different vertices, 
yet its total size will be at most 4s2d. Trees whose size is at most 4s2d are said to be small enough. 
The largest vertex contained in a small enough tree can be found in O(logs + log d) time. The virtual 
neighborhood of an ACT vertex v is defined to be the union of all the small enough trees that contain 
a cell that corresponds to v. 5.7 Implementation details The exact implementation details of the contractions, 
ran­dom walks and the hooking phases are quite involved. Due to space limitations they are not described 
here. They use however only standard PRAM techniques such as list rank­ing, prefix sums, Euler tours 
and sorting.  5.8 Reducing the number of processors To reduce the number of processors required by the 
algo­rithm described in this section from O((m + n) log ~) to O((rn + n)), we start the algorithm with 
k = O(log log n) iterations in which we use the actual neighborhoods of the STAR vertices of the graph 
and do not perform any ran­dom walks. Note that the size of the actual neighborhood of a STAR vertex 
that does not correspond to a complete connect ed component is at least 2. During these iterations we 
use the exponential sequence so = 2 and s~+l = 2s, instead of the doubly exponential sequence described 
ear­lier. After these k iterations sk, = Q((log n)2 ). In the i-th iteration we then perform @(log n) 
random walks of length @(s,/ log n) z @(s~ 2) and continue with the se­7/s quence s,+l = s, . 6 concluding 
remarks We presented a randomized O(log n) time and O(m + n) work EREW PRAM algorithm for computing the 
connected components of an undirected graph with n vertices and m edges. Does a deterministic O(log n) 
time and O(m + n) work aJgorithm exist? <RefA>Currently no such deterministic al­gorithm is known even in the 
much stronger CRCW PRAM model. References [AKL+79] R. Aleliuna+ R.M. Karp, R.J. Lipton, L. Lovasz, and 
C. Rackoff. Random walks, universal se­quences and the complexity of maze problems. In Proceedings of 
the 20th Annual IEEE Sympo­sium on Foundations of Computer Science, San Juan, Puerto Rico, pages 218-223, 
1979. [AS92] N. Alon and J.H. Spencer. The probabilistic method. Wiley, 1992. [BF93] G. Barnes and U. 
Feige. Short random walks on graphs. In Proceedings of the 25rd Annual A CM Symposium on Theory of Computing, 
San Diego, California, pages 728-737, 1993. [CDR86] S. Cook, C. Dwork, and R. Reischuk. Upper and lower 
bounds for parallel random access ma­chines wit bout simultaneous writes. SIAM Jour­nal on Computing, 
15:87 97, 1986. [CL93] K.W. Chong and T.W. Lam. Finding con­nected components in O(log n log log n) time 
on the EREW PRAM. In Proceedings of the Jth Annual ACM-SIAM Symposium on Discrete Al­gorithms, Austin, 
Tezas, pages 11 20, 1993. [CLC82] F.Y. Chin, J. Lam, and I.N. Chen. Efficient par­allel algorithms for 
some graph problems. Com­munications of the ACM, 25(9):659 665, 1982. [CW88] R. Cole and U. Vishkin. 
Approximate paral­lel scheduling. Part I: The basic technique with applications to optimal parallel list 
ranking in logarithmic time. SIAM Journal on Computing, 17(1):128 142, 1988. [CV91] R. Cole and U. Vishkin. 
Approximate parallel scheduling, Part II: Applications to logarithmic­time optimal graph algorithms. 
Information and Computation, 92:1-47, 1991. [Gaz91] H. Gazit. An optimal randomized paral­lel algorithm 
for finding connected components in a graph. SIAM Journal on Computing, 20(6):1046-1067, 1991. [GMV91] 
J. Gil, Y. Matias, and U. Vishkin. Towards a the­ory of nearly constant time parallel algorithms. In 
Proceedings of the Wnd A nnual IEEE Sympo­sium on Foundations of Computer Science, San Juan, Puerto Rico, 
pages 698-710, 1991. [Go091] M.T. Goodrich. Using approximation algorithms to design parallel algorithms 
that may ignore processor allocation. In Proceedings of the 3.2nd Annual IEEE Symposium on Foundations 
of Computer Science, San Juan, Puerto Rico, pages 711 722, 1991. [Hag90] T. Hagerup. Optimal parallel 
algorithms on planar graphs. Information and Computation, 84:71 96, 1990. [Hag92] T. Hagerup. The log-star 
revolution. In Proceed­ings of the 9th Annual Symposium on Theoreti­cal Aspects of Computer Science, 
Lecture Notes in Computer Science, Vol. 577, pages 259 278. Springer, 1992. [AS87] B. Awerbuch and Y. 
Shiloach. New connectiv­ity and MSF algorithms for shuffle-exchange net­work and PRAM. IEEE Transactions 
on Com­puters, C-36:1258 1263, 1987. [Hag93] T. Hagerup. Fast deterministic processor al­location. In 
Proceedings of the ~th Annual ACM-SIAM Symposium on Discrete Algorithms, Austin, Tezas, pages 1 10, 1993. 
[HCD79] D.S. Hirschberg, A.K. Chandra, and D,V. Sar­wate. Computing connected components on par­allel 
computers. Communications of the ACM, 22(8):461 464, 1979. [IK94] K. Iwama and Y. Kambayashi. A simpler 
paral­lel algorithm for graph connectivity. Journal of Algorithms, 16:190-217, 1994. [JM91] D. B. Johnson 
and P. Met axas. Connected com­ ponents in 0(log3i2 [Vi) parallel time for the CREW PRAM. In Proceedings 
of the W?nd An­nual IEEE Symposium on Foundations of Com­puter Science, San Juan, Puerto Rico, pages 
688 697, 1991. [JM92] D.B. Johnson and P. Metaxas. A parallel aJgo­rithm for computing minimum spanning 
trees. In Proceedings of the Jth Annual ACM Sympo­sium on Parallel algorithms and architectures, San 
Diego, California, pages 363-372, 1992. [Kar93] D.R. Karger. Global rein-cuts in RNC, and other ramifications 
of a simple rein-cut algorithm. In Proceedings of the Jth Annual A CM-SIAM Sym­posium on Discrete Algorithms, 
Austin, Texasl pages 21-30, 1993. [KNP92] D.R. Karger, N. Nisan, and M. Parnaa. Fast connected components 
algorithms for the EREW PRAM. In Proceedings of the Jth Annual A CM Symposium on Parallel algorithms 
and archi­tectures, San Diego, California, pages 373-381, 1992. [KR90] R. M. Karp and V. Ramachandran. 
Parallel al­gorithms for shared-memory machines. In J. van Leeuwen, editor, Handbook of Theoretical Com­puter 
Sciencel Volume A, Algorithms and Com­pie$it y, chapter 17, pages 869 932. Elsevier and The MIT Press, 
1990. [MR89] G. L. Miller and J. H. Reif. Parallel tree contrac­tion, Part 1: Fundamentals. In S. Micali, 
ed­itor, Advances in Computing Research, Volume 5, Randomness and Computation, pages 47 72. JAI Press, 
1989. [Rad94] T. Radzik. Computing connected EREW PRAM. Technical Report College London, 1994. componnets 
on 94/02, King s [Rei84] J. H. Reif. Optimal parallel algorithms for graph connectivity. Technical Report 
TR-08-84, Cen­ter for Computing Research, Harvard University, 1984. [SV83] Y. Shiloach connectivity 3(1):57 
67, and U. Vishkin. An o(log n) parallel algorithm. .lournai o~ Algorithms, 1983.  
</RefA>			
