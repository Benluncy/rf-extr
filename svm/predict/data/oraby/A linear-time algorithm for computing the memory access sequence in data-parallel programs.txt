
 A Linear-Time Algorithm for Computing the Memory Access Sequence in Data-Parallel Programs* Ken Kennedy 
Nenad Nedeljkovit Ajay Sethi kentlcs rice. edu nenad@cs. rice. edu sethi@cs.rice.edu Center for Research 
on Parallel Computation Department of Computer Science, Rice University Abstract Data-parallel languages, 
such as High Performance For­tran, are widely regarded as a promising means for writing portable programs 
for distributed-memory machines. Novel features of these languages call for the development of new techniques 
in both compilers and run-time systems. In this paper, we present unimproved algorithm for finding the 
lo­cal memory access sequence in computations involving reg­ular sections of arrays with cyclic(k) distributions. 
After establishing the fact that regular section indices correspond to elements of an integer lattice, 
we show how to find a lattice basis that allows for simple and fast enumeration of memory accesses. The 
complexity of our algorithm is shown to be lower than that of the previous solution for the same problem. 
In addition, the experimental results demonstrate the efficiency of our method in practice. 1 Introduction 
 High Performance Fortran (HPF) [8, 14] incorporates a set of Fortran extensions for portable data-parallel 
program­ming on distributed-memory machines. The most impor­tant of these extensions are the altgn and 
distribute di­rectives, which are used to describe how data should be distributed across processors in 
a parallel computer. Ar­ray elements are first aligned to templates (abstract spaces of indexed positions), 
and templates are then distributed onto a processor grid. Using this data mapping specifica­tion, the 
compiler must partition the arrays and generate SPMD (Single Program Multiple Data) code which will be 
executed on each processor. Several variants of data-parallel Fortran that preceded HPF, such as Fortran 
D [10] and Vienna Fortran [2], also provided ways for the programmer to specify the mapping This work 
was supported ]n part by ARPA contract DABT63­92-C-0038 and NSF Cooperative Agreement Number CCR-9120008 
The content of thw paper does not necessarily reflect the pos)t]on or the policy of the Government and 
no official endorsement should be Inferred. Permission to make digital~ard copies of all or part of 
this material with­out fee is granted provided that the copies are not made or distributed forprofitor 
commercial advantage, the ACMcopyrighVserver notice, the title of the publication and its date appear, 
and notice is given that copyright is by permission of the Association for Computing Machiner Inc. (ACM). 
To copy otherwise: to republish, to post on servers or to redistribute to lists, requires prior specific 
permission and/or a fee. PPOPP 95 Santa Clara, CA USA 01995 ACM 0-89791-701 -6195/0007.. $3.50 of array 
data onto processors. Implementations of these lan­guages included support for Hock and cycltc distributions. 
Both of these are just special cases of the cyclic(k) distribu­tion in H PF, which first divides a template 
into contiguous blocks of size k and then assigns these blocks to processors in a round-robin fashion. 
Obviously, CUCIZC is equivalent to cyclic(l), and Mock is equivalent to cyc~ic( [nlpl), where n is the 
template size, and p is the number of processors Since there are no commercial HPF compilers that pro­vide 
efficient support for the cyclic(k) distribution, pro­gramming examples of its use are somewhat scarce. 
Never­theless, Dongarra, van de Geijn, and Walker emphasize the importance of this, in their terms block 
scattered, dis­tribution in the design of scalable libraries for dense linear algebra computations [6]. 
An array A distributed with a cyclic(k) distribution is effectively split into p subarrays, each being 
local to one processor. For a program that requires a traversal of the ar­ray section A(Z: a:s) with 
the lower bound 1, upper bound U, and stride s, the compiler or run-time system must deter­mine the sequence 
of local memory addresses that a given processor must access when performing its share of compu­tation 
over the array section. This problem and its solu­tion were first described by Chatterjee et al. [4]. 
They give an algorithm for solving the memory address problem in O(k log k + min(log s, logp)) time, 
and show that any algo­rithm for this problem takes fl(k) time. Hiranandani et al. present an algorithm 
that works in O(k) time, but only if some special conditions are satisfied (s mod pk < k) [9]. In this 
paper, we describe an improved algorithm that com­putes the memory address sequence for the general case 
in O(k+min (logs, log p)) time. Experimental comparison with the method presented by Chatterjee et al. 
shows that the theoretically proven lower complexity of our algorithm is mirrored by its superior performance 
in practice. The pro­posed algorithm allows for simple and efficient implementa­tion and. as such. is 
suitable for inclusion in comDilers and . run-time systems for H P F-like languages. The remainder of 
this paper is organized as follows. In Section 2 we describe the problem and the solution proposed by 
Chatterjee et al. in some detail. In Section 3 we show how regular section accesses fit in the framework 
of integer lattice theory. This is further explored in Section 4, where we lay the theoretical foundation 
for our algorithm. The al­ gorithm itself and analysis of its running time are presented in Section 5. 
We describe our implementation experience in Section 6, discuss related work in Section 7, and conclude 
inY, Section 8 by summarizing our contributions and indicating directions for future research. 102 Processor 
O processor 1 Processor 2 Processor 3 @ ,234567 8~1011 12 13 14 15 16 17 @ 1920 21 22 23 24 25 26 @ 28 
29 3031 ---i 32333435~ 3738394041424344 @ 46 47 48 49 50 51 52 53 @ 55 56 57 58 59 60 61 62 @ 64 65 66 
67 68 69 70 71\72~73747576777879 80 @ 82 83 84 85 86 87 8889 ~ 9192939495 u I I 96 97 98 ~ 100 101 
102 103 104 105 106 107 ~ 109 110 Ill 112 113 114 115 11611171 118 1191120 121 122 123 124 125 112611271 
 u l LJI 128 129 130 131 132 133 134 @ 136 137 138 139 140 141 142 143 11441145 146 147 148 149 150 151115211531154 
155 156 157 158 1591 u IuI 160 161 n 163 164 165 166 167 168 169 l 70 @ 172 173 174 175 176 177 178 179 
@ 181 182 183 184 185 186 187 188 l @ 190 191 192 193 194 195 196 197 @ 199 200 201 202 203 204 205 206 
h 208 209 210 211 212 213 214 2151~ 217 218 219 220 221 222 2231 L 1-1  224 12251226 227 228 229 230 
231 232 233 12341235 236 237 238 239 240 241 242 @ 244 245 246 247 248 249 250 251 @ 253 254 255 uu 
1I 256 257 258 259 260 ~ 262 263 264 265 266 267 268 269 i2701 271 272 273 274 275 276 277 278 1279\\280 
281 282 283 284 285 286 2871 u UII @ 289290291292293294295 296 @ 298 299 300 301 302 303 304 305 @ 307 
308 309 310 311 312 313 314 @ 316 317 318 319 Figure 1 Layout of array elements distributed with cyciic(8) 
distribution over 4 processors. Rectangles indicate elements of the array section with lower bound 1== 
O and stride s = 9. 2 Problem statement of k linear Diophantine equations {sj-pkq= ilkm-l<i<km-l+k-1} 
 Given an array A distributed across p processors using a cyclic(k) distribution, we can visualize the 
layout of its in variables j and g. Each individual equation has solutions elements in local processor 
memories as a two-dimensional if and only if GCD(S, pk) divides i. Furthermore, all solutions matrix, 
with each row divided into p blocks [4]. The location for each equation can be found by using the extended 
Eu­of array element A(z) is determined by the processor hold­clid s algorithm for computing GCD. For 
each solvable equa­ing A(i), the block within this processor containing A(z), tion, Chatterjee et al. 
find the solution having the smallest and the offset of A(t) within the block. For example, in nonnegative 
j and use the minimum of these solutions to get Figure 1 array element A(1 08) has offset 4 in block 
3 of the starting array section element A(l + js). The last array processor 1 (we assume that array elements, 
offsets, blocks, section element can be found in a similar way using the up­and processors are numbered 
starting from zero). per bound u. Since our focus is on finding the memory gapThe sequence of local memory 
locations that a given pro­sequence, and since the sequence itself is independent of u, cessor m must 
access when performing its share of compu­we restrict our treatment of the problem to array sections 
tation over the array section A(Z: u:s) can be described by for which only the lower bound i and stride 
s are specified. the starting location and the distances between memory lo- Furthermore, we assume that 
s >0, since the case when s cations of every two successive elements of the array section is negative 
can be treated analogously. that belong to processor m. Given the number of processors While we follow 
the approach described above for find­ p, the array distribution cyclic(k), and the regular section 
ing the starting location, our method differs in the way we stride s, the offset of an array section 
element determines find the offset and memory gap sequences. After finding the the offset of the next 
array section element on the same set of smallest positive solutions as described above, Chat­ processor, 
and also determines the local memory gap be­terjee et al. sort this set to produce the sequence of array 
tween those two elements. Since the offsets range from O section indices that will be successively accessed 
by the pro­to k 1, the sequence of offsets, as well as the sequence of cessor. The memory gap sequence 
can then be found by memory gaps, must have a cycle whose length is at most a simple linear scan through 
the sorted sequence of arrayk. Chatterjee et al. visualize the table containing the offset indices. Since 
sorting the sequence requires O(k log k) time, and memory gap sequences as the transition diagram of 
a it represents the dominating term in the overall complex­finite state machine [4]. State transitions 
depend only on ity of the algorithm. In order to reduce the complexity of p, k, and s, whereas a processor 
s start state in the tran­finding the memory gap sequence to O(k), we show how ar­sition table also deDends 
on the lower bound of the arrav , ray indices can be enumerated in increasing order without section 
2 and that processor s number m. The upper boun~ actually sorting the sequence. u does not play any role 
in finding the transition table and HPF allows affine alignments between arrays and tem­ the starting 
state, and is only used to find the last location plates. In other words, array element A(i) can be aligned 
for each processor. to a template cell aZ + b, for arbitrary a and b. IdentityThe starting location for 
a given processor m is deter­alignment of an array to a template is given by a = 1, b = O. mined by the 
first element of the array section A(l: u:s) that Chatterjee et al. show that the memory access problem 
for belongs to the processor. Since array element A(z) belongs any affine alignment can be solved by 
two applications of to processor m if and only if its offset relative to the begin­the access sequence 
computation algorithm for the identityning of its row (i mod pk) lies in the range [km, k(m + l)), alignment. 
Therefore, we present our algorithm only for the finding the starting location reduces to finding the 
smallest csse of identity alignment without loss of generality. nonnegative integer j such that In multidimensional 
arrays, alignments and distributions of each dimension are independent of one another. If akm<(l+sj) 
modpk<k(m-tl) l. multidimensional array section can be described using For-Chatterjee et al. show that 
this is equivalent to solving a set tran 90 subscript triplet notation, i.e., if array subscripts 103 
in different dimensions are also independent, then memory access problem simply reduces to multiple applications 
of the algorithm for one-dimensional case. Consequently, we describe our algorithm only for one-dimensional 
array sec­tions. Extensions necessary to handle coupled subscripts and subscripts containing multiple 
index variables are de­scribed in our related work [12]. 3 Integer lattice Our approach is based on treating 
each array element as a point in RZ space with the origin corresponding to the array element with index 
0, positive y-axis in the direction of increasing row numbers, and positive z-axis in the di­rection 
of increasing offsets. For example, in Figure 1 the coordinates of the array element with index 108 are 
(12, 3). The first coordinate of an array element is equal to the number of the row to which that element 
belongs, and the second coordinate is equal to its offset within that row. Since each row has pk elements, 
the ith element of the reg­ular section with the lower bound 1 and stride s, and its coordinates (z, 
y) in RZ satisfy the relationship: pky+z = 1+1s,162. Each of these points has a unique corresponding 
point (bta) = (z 1mod pk, y 1div ~k) for which ­ pka+b = pk(y ldiv pk)+(z lmodpk) = (pky+ z) -(pk(l 
div pk) + lmod pk) = (2+2s) 1 = 2s. The following theorem gives a useful characterization of the set 
of all points (b, a) that are in correspondence with regular section indices. Theorem 1 ,9ef A == {(bja) 
E 22 ] pka+b = ts, z c 2} is an tnteger lattice]. Proofi Every discrete subset of Xln closed under subtrac­ 
tion is a lattice [II]. Let (b], al ) : pk al + bl = il.. and (bz, a,) : pk az + bz = zzs (il , zz c 
2) be two arbitrary points in A. Since pk (al az) +(bl bz) = (z1 iz)s, point (bl b2, al az) is also 
in A. By construction, set A is discrete, and therefore it is an integer lattice. 1 By construction, 
the lattice A is independent of the start­ing location 1; in other words, for a given array distribution, 
any regular section with stride s will have the same corre­sponding lattice A. Therefore, when exploring 
the prop­erties of this lattice, we can safely assume that the lower bound 1 = O, in which case the set 
of regular section ele­ments is identical to A. Our goal is to find a basis for A, i.e., the maximal 
set of linearly independent vectors in A from which we can gener­ate all lattice points using integer 
linear combinations. Since A c 22, any basis for A can have at most two vectors. If A can be generated 
using only a single vector, it is easy to see that pk must divide s. This special case can be trivially 
handled in the algorithm, and therefore we will assume that any basis for A contains exactly two vectors. 
1~ Iattlce In R! m is the set of all Integer linear combinations of a set of Ilnearly Independent vectors 
In Rn In order for vectors (bl, al) : pk al +bl = ils and (b2, a2) : pk a2 + b2 = izs (Z1, t2 G 2) to 
form a basis, for every lattice point (b, a) : pk a + b = ts in A, there have to exist integers a and 
,L9, such that (b, a)=a(bl, al)+,6(b2, a2). The solution of the system of equations b1a+b2/3 = b ala+az~ 
= a is given by -( az-2 azi ali atl  altz azt~ aliz azi~ ) It is not hard to show that a and ~ will 
be integers for every i c 2, if and only if aliz aztl = +1. In other words, the necessary and sufficient 
condition for vectors (bl, al ) and (bz, a2 ) to form a basis is given by Ializ aztll= 1. In the example 
in Figure 2 line segments between lattice points correspond to the vectors (3, 3) :3 x 32 + 3 = 11 x 
9 and( l, 2):2x 32 l =7x 9. Since 3x7 2xll= 1, these vectors form a lattice basis. Although we now have 
a simple test for deciding whether two given vectors form a basis, we still need a constructive method 
for finding a basis of the regular section lattice. If vector (6, a) : pk a + b = is belongs to a basis 
for A, then the segment joining (O, O) and (6, a) does not contain any other points from A. It is easy 
to show that (b, a) satisfies this condition, if and only if GCD(a, t) = 1. We can now use this fact 
to find a basis of the lattice A. Let (6] , al) be an arbitrary lattice point such that there are no 
other lattice points between (O, O) and (bl , al); for example, this can be achieved by choosing II = 
1, al = s div pk, and hl = s mod pk. Since GCD(al, i]) = 1, we can use the extended Euclid s algorithm 
to find a2 and iz, such that aliz aztl = 1. BY defining bz = 12s pk az, we compIete the construction 
of (bz, az), which together with (b], al ), forms a basis for the lattice A. Having a lattice basis allows 
us to construct all lattice points as integer linear combinations of basis vectors. How­ever, our goal 
is to enumerate regular section indices within a given processor s range in increasing order, and this 
can not be achieved with an arbitrary basis. We now describe the construction of the basis for A that 
makes this enumer­ation possible. 4 Basis selection We choose the first vector to correspond to the smallest 
regular section access on processor O (not counting index O itself). More precisely, we define (br, ar) 
: pk a,+b, = irs to be the lattice point with the smallest positive i,, such that O ~ b, < k. The complementary 
lattice point is defined by (b,, al) : pk a, + bt = ZLS to correspond to the regular section index with 
the largest negative il, such that O < b~ < k. It is clear that ar z O and al < 0. Furthermore, the case 
when 104 Processor O Processor 1 Processor 2 Processor 3 * @ ,0 .. ~ , 2 3 4/ ~ ~ ~ 11 1.2 13./ 14 
is.., 16 17 @ 19 20 21..,2:: 23 ~,. 25 26 @ 28 29 30,, 31 ,. .. ..... . . ,. .. >. ,.. S3Q,,40~ 
41 - @L43 56 b~ 32 k, 34 35 @ 37 38  44 @ 46 47 ~~, 49;: 50  $1.,52 53 @ 55 58/ 59  6Q,.61 62 
@ .. ., ... .. ,.. ..t ,,., . .... 64 65 36, 67! 68  O!.,70 71 ~ 73 74 ?$.,76! 77 >8., 79 80 @ 82 
83  84,,85;: 86  b~,.88 89 @ 91 92 93., 94! 95 . . ... .. . .. . . ., .J %.,, 97 98 ~ 100 101 h~% 107 
104 illj,, 106 107 @ 109 110 h~ l]y:~ 113 i ~~.,115 116 ~ 118 119 k? 12}: 122 i23,,124 125 ~ 127 
. .. .. . .. ... . ,, 128 iy,,,130J131 - ix,., 133 134 ~ 136 137 ht 139:140 fiJ 142 143 ~ 145 146 
h? 14&#38;y149 kQ.. 151 152 @ 154 155 tSf 15# 1S8 ]S~, ... ... ,.. ., ... ... ... . . . : . . .. ,.? 
160 161 ~ 163 164 ~6j, 166 167 k~ 169 170 @ 172 173 h~. 1~~ 176 h~ 17S 179 @ 1S1 182 j S~, 184~ls5 i~. 
187 188 ~ 190 191 , .. . ~9719?{194 i%,,19619719S199200 h@J,.20~ 203 ~,, 205206~ 208209 Q,,,,21~::212 
 h?,214215 @ 217 218 h9 22$7;221 tiq,, 223 cl224 225 226 227 h{., 22?; 230 %~. 232 233 ~ 235 236 237..23~: 
239 k? 241 242 @ 244 245 ?4$, 24? 248 ~~~, 250 251 ~ 253 254 ti~ :.d   ,  . :: :  ,: . .. 25$ 
257 ht, 259 260~ 262 263 >6$ 26Sf266 ?6~,26S 269@ 271 272 h.~,.27$;275 he,,277 278@ 280 281 y8~28# 
284 >~,.,286 287 ,.: .. . ... .. .. . . . .. .. ,.. ... @ 2S9 290 ~J, 29?; 293 ~ 295 296 ~ 298 299 
 h! 30~; 302 b? 304 305 @ 307 30s 309 3NJ 311 %+ 313 314 ~ 316 317 &#38;,, 319 . .  Figure 2 Line 
segments corresponding to basis vectors (3,3) and ( 1, 2). b, = O or bl = O is easily detected in the 
algorithm, and 1. If O < {cY} + {/3} ~ 1, then (bl, al) lies within the therefore we can assume that 
b, >0 and b[ >0. triangle with vertices (O, O), (br, a,), and (lx, at), which The two points are constructed 
so that there can be no contradicts the construction of R and L. lattice point with the first coordinate 
in the range [0, k) whose corresponding regular section index is smaller than 2. If1< {a} +{~} < 2,then 
welook atthe point (b2, a2) =R+L (bl, al). Since (bl, al) is in A, so is z,s or greater than ZIS. In 
other words, the triangle with (b2, a2). From the definition of (b2, a2) we have vertices (O, O), (b., 
a~), and (bl, W) contains no other lattice points. (bz,az)= (1 -{a})R+(l -{~})L.In our example, vector 
R defined by the point (b,, G.) is equal to (4, 1) and corresponds to the regular section index Using 
the fact that 1 < {a} + {/3} < 2 we get O < 1 x 32+4 = 36. Vector L given by the point (h, W) is equal 
(1 -{~})+ (1 -{/3}) <1, which means that (bz,az) to (5, l), and its corresponding index is 1 x 32+5 = 
27. lies within the triangle with vertices (0, O), (b,, ar), and This is illustrated in Figure 3. (bl, 
al), which is again in contradiction with the way R and L were constructed. Theorem 2 Vectors R = (b,, 
G.) and L = (b~, CU) /orrn a basis for the lattice A. In this way we conclude that every (b, a) G A must 
be an integer linear combination of R and L. @ Proofi Suppose (b, a) : pk a + b = is is a point in A 
that Vectors R and L can be used to enumerate the regular cannot be represented as an integer linear 
combination of R section indices for a given processor m in increasing order. and L. This means that 
in the equaiity (b, a) = a R+@ L at Given (bl, al) : pk al + b] = i]s as an arbitrary regular least one 
of a, /3 is not an integer. In this case, we construct section index that he. within the offset range 
of processor the point (bl, al) = {cr} l?+ {,B}L, where {x} = z [zJ is m, i.e., mk < bl < (m+ l)k, we 
need to find the next lattice the fractional part of z, Since point on the same processor. We first want 
to show that points (h, a,) = (b, a) -(lcY]R+ l.Pj~) (b~,az) = (bl,al)+lt (1)and (b, a) G A, we conclude 
that (bl, al ) also belongs to the lattice A. and By definition of the fractional part, we have O s 
{z} <1, for any z c R. Using the fact that at least one of a, ~ is (b,, as)= (b], a~) L (2) different 
from O, we get O < {cr} + {/3} <2. We now consider cannot both belong to processor m. Suppose they did, 
and the following two cases: consider the vector (bz, az) (bs, aS)=R+L =(b, +b,, a,+ai). We assume that 
br and bi are both positive, and thus b? + b! >0. Furthermore, using bz = bl + br < (m+ I)k, and bz= 
bl-bl~ mk, weget b.+bl< k, which means that vector R + L 1s within the offset range of processor O. Since 
irs >0 and its <0, the regular section index (z, + Zt)s corresponding to the vector R + L must satisfy 
one of the inequalities: O < (2, +il)s < i,s, or Figure 3 Vectors R = (4,1) and L = (5, l). its <(i, 
+i~)s< 0. 105 Processor O @.....~. 2 345 67. ................ ,............- 32 33 34 ..S.;:@5?~.38 
39 ,.,.........-- :%~ 65 66 67 68 69 70 ;Zi­ -%.. ... ........-.- 96 .9~.,;;~:~:~QO.!O,! 102 103 ........­ 
............ 128 129 130 131 132 133 .B4-.a .......... ........ ..----­ ......... 160 M.-~~fi3 164 
165 166 167 ....----............ 192 193 194 195 196 l;%.~z~i ....---­ ..----­;i%@&#38;6;22~, 228 229 
230 231 ......... ....---­256 257 258 ,2~9,.,.@@i2.263 ~%9 290 2,12,2293 29429i- Figure 4 3 Processor 
1 Processor 2 Processor ...s ....~ ....ul.. 11 12 13 14 15, ,;b. .....12. ...~. ...l2. 20 21 22 23 24 
.2S...,~..~.,29,,29,, 30 31 --......... ...... .........­ ...........  ........... 40 41 42 43 ,..4&#38;@4~jj~ 
48 49 50 51 S2..;ii.:@-~;~ 56 57 58 59 60 61 ....62-.~ .........  ..........75 76 77 78 79 .......- 
........... ,,., .....---  ~ii::.<:... :xi.:~:ii:.. ,.,. 84 85 86 87 88 .89...~::!X:x?.x? 93 94 95 
............­ ,...............- ........... 104 105 lo6...ln&#38;iQ;&#38;iQ; 110 111 112 113 114 115...lM..~..Ll8::l;9 
120 121 122 123 124 M-@ii7 .......... ......... ............. ,..............  .....--- -  ......- 
- - ....... :i6,!,31 138 139 140 141 142...3.3. ~.i~f 146 147 148 149 1513 151-@@ii4 155 156 157 158 
159 ............  ...........-  -.-... ., ...........-  ......... %.. ..,.. 168 169 ..JIQ-~i~i,,!,~.~. 
174 175 176 177 178~~~~.~~Li:l.;.S:,. 183 184 185 186 187;~&#38;@ii:-.?-9! .........­ ...........-... 
 .....--- - ­ 200 201 202 203 204 ,,,2~5.~~&#38;.:@ @&#38;i12!8 219 220 221 222 223 .%............ 
.......... ......... .......... ,, .........- - .2~2,...2~ti ~, 23~2,~., 237 238 239 24;24]~~z.:~zz~,:45-246 
247 248 249 250...2L@~ti2~2~4~4 255 ........... :5Qi:.~f9 7-IO 21 ~ 212 213.,.2!4.:23 ,.........--  
......... -----­264 265 266 267,,.268,.,.26Y@%( 212 273 274 215 276 ,.221,..2lX-~ ~ia6 2gI 282 283 284 
285 Xb ....2&#38;7 ............  ~$~~~98 ~~~ 30030~ 302303 3;4&#38;@-G 3;8 3093103,1 312 313 ;4@% 
 3;73l8 319 Line segments corresponding to basis vectors R = (4,1) and L = (5, l). The first inequality 
violates the construction of R, and the second vioIates the construction of L, which proves that at most 
one of the points (bz, az) and (bs, as) can be within the offset range of processor m. In our example 
in Figure 4, (IM, az ) will belong to pro­cessor m if and only if bl + 4 < 8(rn + 1), which reduces to 
bl < 8m + 4; (bs, as) will belong to m if and only if /J1 5 ~ 8m, which is equivalent to bl ~ 8m + 5. 
Obviously, the two conditions cannot be satisfied at the same time. If (bz, az) is on processor m, i.e., 
if bl+b~ < (m+l)k, then by construction of R, index zz .s, corresponding to (bz, az ), is the smallest 
regular section index in the processor m s offset range that is larger than Z1s. If bl + 1+2 (m + l)k 
and bl b~~ mk, which means that (bz, az) does not belong to processor m, but (bs, as) does, then by 
construction of L, index iss, corresponding to (b,, as), is the smallest regular section index on processor 
m that is larger than ZIs. The only remaining possibility is that neither (bz, az ) nor (b,, as) belong 
to processor m, i.e., bl + b, z (m + l)k and bl bI < mk. (In our example, this happens when bl = 8m 
+ 4.) In this case, the point (b4, a4)=(bl, al)+R L (3) must lie on processor m, because bA = (bl + br) 
 bl z (m+l)k k=mk, and bA=(bl bl)+br <mk+k= (m + l)k. The regular section index i,s = (il + i, il).s 
corresponding to (bA, aA ) is obviously larger than ils, and since bz and bs are outside processor m 
s range, this index comes immediately after (bl, al ) on processor m. Thus, we have effectively proven 
the following: Theorem 3 The distance between a reguiar section indez (bl , al ) that belongs to processor 
m and the next regu­lar sectzon index on the same processor must have one of three possible values: R 
(if bl + br < (m + l)k), L (if bl +6, z (m+l)k andbl bl z ink), orR L ( otherwise). Vector R can be found 
in much the same way as the start­ing location for processor m. We simply find the minimum of the smallest 
positive regular section indices over all off­sets in the range (O, k) that have at least one such index. 
Vector L is computed by finding the maximum of these in­dices, and taking the coordinates of this point 
relative to the point that starts the next cycle, i.e., the first positive index whose offset is equal 
to O. In the example in Figure 4 the smallest positive index on processor O is 36 and therefore R = (4, 
1). The largest index in the first cycle is 261, and since the point that starts the next cycle is 288, 
we have L = (5,8) (0,9) = (5, l). 5 The linear-time algorithm The linear-time algorithm for computing 
the local mem­ory access sequence, which is based on Theorem 3 from the previous section, is given in 
Figure 5. As mentioned in Sec­tion 2, we use the approach described by Chatterjee et al. [4] to find 
the starting location for a given processor m. The ex­tended Euclid s algorithm (line 3) computes d = 
GCD(S, pk), and z and g, such that s x +pk g = d. These values are then used to solve linear Diophantine 
equations sj pkq = i, for each i in the range [km 1, km 1+ k). Solutions for each of the equations 
exist if and only if d divides Z. The smallest regular section index for each i is computed in line 7, 
and the minimum of these indices gives the starting location for processor m (lines 4 11 ). While the 
algorithm by Chatter­jee et al. stores all these indices into an array and later sorts them, our method 
does not have this space overhead since we compute only the first location for the processor. If the 
sequence length is greater than 1 (special cases are handled in lines 12 1 8), we compute the basis vectors 
R and L. Since the vectors are independent of m and 1, we find them by considering processor O s memory 
access pattern when the regular section starts with index O. As described in Section 4, R can be computed 
by finding the smallest positive regular section index that lies in the range (O, k) (the lower bound 
of the loop in lines 19-26 is 1 because i = O corresponds to index O itself). The largest index in the 
initial cycle of processor O is used to find vector L. The coordinates of the two vectors are computed 
in lines 28 30: b, and a, are equal to the row number and the offset of the smallest location, respectively, 
while bl and at are found using the coordinates of the largest index (maz) relative to the point that 
starts the next cycle (index pks div d, whose coordinates are (O, s div d)). Chatterjee et al. note that 
the loop in lines 4-II (and similarly the loop in lines 19 26) can be simplified by iden­tifying the 
first solvable equation (based on the value of (km /) mod d) and recognizing that successive solvable 
equations are d offsets apart. In the actual implementa­ 106 Input: Distribution parameters (p, k), regular 
section tion, we exploit these facts to remove the conditionals from parameters (1, s), and processor 
number (m). Output: The AM table and its length. Method: 1 start = co; length = O 2 min =m; max =O 3 
(d, z, y) + EXTENDED-EUCLID(S, pk) 4 // Find the starting location for processor m. 5 fori=km l, km l+k 
ldo 6 if (i mod d = O) then // Solutions exist. 7 10C= 2+ ;(iz+pk[ ~1) 8 start = min(start, 10C) 9 length 
= length+ 1 10 endif 11 endfor 12 // Handle special cases. 13 if (length = O) then 14 return 0, length 
15 else if (length = 1 ) then 16 AMIO] =ksdiv d 17 return AM, iength 18 endif 19 // Find minimum and 
maximum of the initial cycle. 20 fori=l, k ldo ?1 if (i mod d = O) then // Solutions exist. 22 10C = 
:(ZX +pk( ~1) 23 min = min(min, 10C) 24 max = max(max, ioc) 25 endif 26 endfor 28 // Compute basis 
vectors R and L. 29 (b,, a,) = (rein mod pk, min div pk) 30 (bl, al)= (maz mod pk, max div pk -s div 
d) 31 // Compute the table of local memory gaps. 32 offset = start mod pk 33 2=0 34 while (i < Jength) 
do 35 while (i < length and oflset+ &#38; < k(m + ])) do 36 AM[i] = a,k +b~ // Equation 1. 37 ofiset 
= oflset+ b~ 38 ;= 2+1 39 endwhile 40 // Stop if the whole sequence has been found. 41 if (i = length) 
break 42 AM[i] = (a[k + b[) // Equation 2. 43 oflset = ofiset bl 44 if (oflset < km) then 45 AM[z] = 
AM[z] + ark+ b, // Equation 3. 46 o~set = ofiset+ L? 47 endif 48 i=i+l 49 endwhile 50 return AM, length 
both loops (lines 6 and 21). Once we have vectors R and L, we begin at the offset corresponding to the 
starting location (line 32) and ap­ply Equation 1 from Section 4 until the range of offsets for processor 
m is exceeded (lines 35 39). The distance R = (b,, a,) between two consecutive regular section indices 
results in the local memory gap of aTk + b,. If the range is exceeded we move to the next point using 
Equation 2 (lines 42-43) and compute the corresponding memory gap. However, it is possible that this 
point is out­side the processor m s offset range (line 44), in which case we apply Equation 3 and adjust 
the memory gap accordingly (lines 45-46). We illustrate the algorithm by showing how it computes the 
table of local memory gaps for processor 1 for the ex­ample in Figure 6. The input parameters are p = 
4, k = 8, 1 = 4, s = 9, and m = 1. The lower bound of the regular section is enclosed in the circle, 
while the rectangles mark exactly those points that are visited in the algorithm. Values returned by 
EXTENDED-EUCLID in line 3 are d = 1, z = -7, and y = 2. Lines 4-11 compute start = 13 and set Zertgth 
= 8. Lines 19 26 find mirz = 36 and max = 261; thus, (b,, a,) = (4, 1) and (bl, al) = (5, 1) (lines 28-30). 
As pointed out earlier, vectors R and L depend only on the distribution parameters and the regular section 
stride. For this reason, they are the same as in the previous example, although the lower bound has changed. 
The offset of the first location on processor 1 is oflset = 13 (line 32). The outer while loop (lines 
34-49) is executed five times. In the first iteration we skip the inner while loop (lines 35-39), because 
the point (13, 0)+(4, 1) = (17,1) (cor­responding to index 49) exceeds the offset range of proces­sor 
1. Instead, we visit the point (13, O) (5, 1) = (8, 1), which corresponds to index 40 (line 42 43), 
and compute AMIo] = ( 1 x 8 + 5) = 3. Since this point belongs to processor 1, no adjustment to AM IO] 
is necessary. In the next iteration of the outer loop, we visit index 76 in the inner loop, setting AM[l] 
= 12 (lines 36-37). After terminating the inner loop, the next index visited is 103, which does not belong 
to processor 1, and therefore we move to the point 139, setting AM[2] = 15 (lines 45 46). The process 
is con­tinued until we reach the first point of the next cycle, index 301, and at the end, AM = [3, 12, 
15, 12, 3, 12, 3, 12]. 100 101 102 @:@ 105 Processor 1 @567 8 9 1011.....l2...~ 14 15 16 17 18 19 .....- 
36 37 38 39 ~.~i~ 42 43 44 45 46 47 4s 49 50 51 ....................... 68 69 70 71 72 73 74.....l5.@ 
.....-. 77 7s 79 80 81 82 83 106 107 108 109 110 111 112 113 114 115 ............ 132 133 134 135 136 
137 138 139...l~O 141 142 143 144 145 146 147 -  Q .......... 164 165 166 167 168 169 170 171 172 173..li~.@ 
176 177 178 179 ...........-  196 197 198 199 200 201 @=iQ3:.204 205 206 20 7 208 209 210 211 228 229 
230 231 232 233 234 235 236..,.@@ 239 240 241 242 243 260 261 262 263 264 @2;6 267 268 269 270 271 272 
273 274 275 292 293 294 295 296 297 298 299 300 @ 302 303 304 305 306 307  __-_k- Figure 5 Algorithm 
for computing the local memory ac-Figure 6 Points visited in the algorithm when p = 4, cess sequence. 
k=8,1=4, s=9, andm=l. 107 5.1 Complexity The running time of the extended Euclid s algorithm is o(log 
min(s, pk)) [5]. The loops in lines 4-11 and 19-26 of Figure 5 are both O(k). Finally, the doubly nested 
loop in lines 34 49 does only O(k) work; in fact, we show that, in the worst case, at most 2k + 1 points 
are examined. The algorithm visits all regular section indices belonging to the initial cycle, plus the 
first point in the next cycle. ,Since the length of the cycle is s k, this means that at most k + 1 points 
belonging to processor m are visited. In addi­tion, the algorithm could also visit some extra points, 
i.e., some regular section indices that are outside the processor m s range. This happens when after 
applying Equation 2, the resulting point does not belong to processor m, and therefore we have to apply 
Equation 3 (lines 44 47), In the worst case, the inner while loop could always be empty, and all points 
in the initial cycle could require the applica­tion of Equation 3. This results in visiting k extra points, 
which together with k + 1 regular section indices that lie on processor m brings the total number of 
points examined to 2k+l. This proves that the complexity of the presented algo­rithm is O(log min(s, 
pk)) + O(k), which can be reduced to O(IJ + min(log s, log p)). Since Chatterjee et al. show that the 
problem is Q(k), our algorithm is O(min(log s, log p)) away from being theoretically optimal. This term 
in the complexity equation comes from the use of the extended Euclid s algorithm to find the starting 
memory location for a given processor~ and we believe that, in order to solve this problem for the general 
case, at least one GCD mu st be computed. 6 Experimental results Despite its theoretical advantage, 
the algorithm described in Section 5 cannot be a method of choice for solving the memory access problem 
in compilers and run-time systems for H PF-like languages, unless it allows for an efficient im. plementation. 
We now describe our implementation expe­rience, which shows that our algorithm is more efficient in practice 
than the method developed by Chatterjee et al. [4]. We also illustrate the impact that the shape of the 
node code can have on the overall performance of programs that require regular section traversal. 6.1 
Table construction The description in Figure 5 provides low-level details that are sufficient to directly 
convert our algorithm into working code. In order to perform a correct comparison with the al­ gorithm 
by Chat terjee et al., we modified the code provided to us by S. Chatterjee [3] so that the segments 
common to both methods (lines 3 1 1 in Figure 5) were coded identi­ cally. Moreover, since their method 
requires sorting of the initial cycle of memory accesses, we tried to use the most efficient sorting 
routines available to us, so as not to obtain an unfair advantage. If input parameters p, k, 1, and s 
for our algorithm are compile-time constants, then the compiler could compute the table of memory gaps 
(AM) for each processor. In that case the code that computes the basis vectors R and L (lines 19 30 in 
Figure 5) would have to be executed only once, and values of (br, a,) and (bl, at) could be reused. Furthermore, 
as Chatterjee et al. note, if GCD(S, pk) = 1, then the local AM sequences are cyclic shifts of one another, 
and after computing the table once, only the starting locations for all the processors need to be found. 
If values of some of the input parameters are not known at compile time, then the table of local memory 
gaps must be computed at run time. In other words, every processor would run the algorithm from Figure 
5 by supplying its processor number m. Our experiments were designed to compare the performance of the 
two methods in this case, when every processor must execute the complete version of either algorithm. 
The lower bound of the regular section has almost no in­fluence on the running time of the algorithm, 
and therefore all our experiments were performed with 1 = O. Similarly, the effects of varying the number 
of processors are only mi­nor, and therefore we always used p = 32. The two varying parameters were block 
size k and stride s. We used powers of 2 for the block size, since these are the most likely values to 
be encountered in practice (cases when k = 1 or k = 2 are not reported because the amount of work done 
by ei­ther algorithm is negligible). We experimented with several values of stride s, and took into consideration 
two perhaps unusual cases: s = pk ], and s = pk + I. The reason for this was that these cases result 
in reversely (s = pk 1 ) and properly (S = pk + 1) sorted access sequences and as such were interesting 
to test the behavior of the sorting routine. Although Chatterjee et al. describe several special cases 
that can be handled more efficiently, we do not report results for these cases, since we are interested 
in comparing two general methods, and the special cases could be detected in our implementation in the 
same way as in theirs. Table 1 contains the execution times that our algorithm and the method by Chatterjee 
et al. take to find the memory gap sequence (AM table in Figure 5) for different values of k and s. In 
Figure 7 we plot the execution times for the case when s = 7. All measurements were performed on an Block 
S=7 .s =99 s=k+l S=pk 1 s=pk+l size Lattice Sorting Lattice Sorting Lattice Sorting Lattice Sorting 
Lattice Sorting k=4 48 56 60 68 52 65 44 53 40 48 k=8 58 82 70 94 53 78 49 75 44 70 k=16 60 138 76 145 
65 134 60 140 54 133 k=32 83 286 95 295 81 287 81 288 72 276 k=64 122 775 140 749 132 747 124 735 109 
727 k = 128 183 1384 232 1451 201 1453 203 1385 181 1367 k = 256 332 2708 394 2814 340 2730 368 2713 
325 2776 k = 512 614 5550 679 5281 618 5328 698 5312 617 5262 Table 1 Execution times in microseconds 
for our algorithm (Lattice) and the algorithm by Chat terjee et al. (Sorting). 108 @s - o Sorting @s 
-- o Sorting .* e  a_ Lattice , Lattice . 5000 . 300 - I .* ., .. , .. .. ,. .. 200 ­,. .. .. .. , 
.* .. 100 -,.,.. .* ,.. 111 I 48 16 32k Figure 7 Performance comparison of the two Intel iPSC/860 hypercube, 
using the zcc compiler with -04 optimization level and dclock timer. Reported times are maximums over 
all 32 processors given in microseconds. While the difference in performance of the two algorithms is 
not significant for small values of k (k = 4, k = 8), as k increases the algorithm from Figure 5 clearly 
outperforms the algorithm based on sorting the initial cycle of memory accesses. It should be noted that 
the implementation of the latter method uses the linear-time radix sort when k ~ 64, which causes the 
relative performance gain achieved by our algorithm to be constant. However, if a sorting method that 
sorts the sequence in place were used, for larger values of k relative performance improvement would 
also increase.  6.2 Code generation After the table of local memory gaps is constructed, each processor 
uses its table to access the array section elements that it owns. In order to achieve good performance 
great care has to be taken when generating the node code. In Figure 8 we show four different ways to 
generate the node code based on the memory sequence table. The C code fragments correspond to the simple 
array assignment statement A(I : u : s) = 100.0. The code in Figure 8(a) is identical to that proposed 
by Chatterjee et al. [4]. Ex­pensive mod operations can be replaced by simple tests, as in Figure 8(b).2 
A slight modification of the same idea is shown in Figure 8(c). While these three code templates can 
directly use the table of local memory gaps as computed by the algorithm (AM in Figure 5, deltaM in Figure 
8), this is not the case with code shown in Figure 8(d). During the execution of the algorithm for table 
construction, local memory gaps are computed in the order seen by processor m. For example, AMIO] does 
not necessarily correspond to offset O, but to the offset of the starting location for processor m. However, 
deltaM table in Figure 8(d) must be indexed by local offsets. The local offset of the starting location 
(startoffset) is equal to start mod k. In order to index AM table using the local offsets and to compute 
NextOflset table used in Figure 8(d), lines 36 38 of the algorithm in Figure 5 should be changed as follows: 
2S Chatterjee pointed out that the code template from Figure 8(a) was g!ven ]n reference [4] only for 
conceptual reasons. The lmplemen­tatton actually used the code from Figure S(b) . .. . .$ 3000-. . . 
.0 .. .. .. .. ,* 1000 -,.  64 128 2.56 512 k table constriction algorithms for the case s = 7. AM[offset 
km] = ark + b, NextO@et[ofiset km] = oflset km + b, offset = offset+ b,. Similar changes are required 
in lines 42 43 and 45 46, where distances based on equations 2 and 3 are computed. Experiments with different 
versions of the node code were performed in the same environment as described in the pre­vious section. 
Lower bound 1 was always O, while the upper bound was scaled in proportion to stride s, in order to keep 
the number of memory accesses constant. The execution times reported in Table 2 are for the case when 
each pro­cessor performed assignments to 10,000 array elements. The most notable was the very poor performance 
of the code that uses mod operations compared to the other three versions. The node code template in 
Figure 8(c) ran some­what faster than that in Figure 8(b), with the difference increasing with larger 
block sizes. The main reason for this was better instruction scheduling by the zcc compiler for the code 
fragment in Figure 8(c). The best performance was achieved using the code version in Figure 8(d). Although 
this code requires two table lookups per array access, its simple structure makes it more efficient than 
the others, es­pecially for smaller values of k. Knies, O Keefe, and MacDonald point out that the ad­dress 
generation scheme based on table lookup makes a time versus space tradeoff [13]. This is particularly 
true for the code in Figure 8(d), which while being the fastest, requires two tables to be stored. An 
important feature of our method is that the algorithm can be modified to return only vectors R = (br, 
a,) and L = (lu, at), without storing any tables. Based on these values, every processor can generate 
its lo­cal addresses as needed, using simple tests similar to those in lines 35 and 44 of Figure 5. The 
details of this tech­nique, which eliminates memory overhead with only a small penalty in the execution 
time, are described in our related work [12]. 7 Related work The work by Chatterjee et al. [4], which 
has been exten­sively cited throughout this paper, describes a method for enumeration of local regular 
section indices in increasing or­der. Hiranandani et al. present a linear-time algorithm for computing 
the memory gap table when s mod pk < k [9]; the simplicity of the resulting access pattern allows them 
to 109 base = startmem; i = O; base = startmem; i = O; while (base t= lastmem) { while (base <= lastmem) 
{ *base = 100.0; *base = 100.0; base += deltaM[i]; base += deltalf[i.++]; i = (i+l) X length; if (i == 
length) i = O; 1 J } (a) (b) base = startmem; i = O; base = startmem; while (TRUE) { = startoffset; for 
(i = O; i < length; i++) { ~hile (base <= lastmem) { *base = 100.0; *base = 100.0; base += deltall[i]; 
base += deltaM[i]; if (base > lastmem) goto done; i = nextoffset[il; } } } done: (c) (d) Figure 8 Four 
different versions of the node code. generate the correct sequence of local indices without ac­tually 
sorting it. While other researchers have also dealt with issues of compiling programs that contain arrays 
with cgclzc(k) distributions, their techniques provide efficient so­lutions only in the case when array 
elements can be accessed in any order, which is not true for arbitrary loops. Ancourt et al. describe 
a linear algebra framework for compiling independent loops in HPF [1]. The assumption of independent 
parallelism allows them to enumerate loop iterations in any order. Although their method can handle arbitrary 
affine subscripts and alignments, generated loop bounds and local array subscripts can be quite complex, 
and thus introduce a significant overhead. Gupta et al. address the problem of array statements (A(l~:u~:s~ 
) = ~(~(lb : Ub : .Sb))) invoking C!dk(k) distributions using the virtual processor approach [7]. In 
their virtual-cyclic scheme, only array elements that have the same offset are accessed in increasing 
order, while the order of accesses for elements with different offsets is de­termined by the values of 
the offsets, and not by the array indices. In the virtual-block scheme, array elements are ac­cessed 
in the order of increasing indices, but if the array section stride is larger than the block size, this 
method ef­fectively reduces to the run-time address resolution. Finally, Gupta et al. assume that arrays 
are directly distributed onto Code shape 8(a) 8 (b) 8(c) 8(d) g=3 18086 3219 3096 2291 k=4 .9=15 18111 
3447 3325 2506 S=99 18573 3920 3811 3028 S=3 18072 3277 2606 2295 k=32 S=15 18101 3488 2833 2526 S=99 
18567 3965 3321 3050 S=3 18079 3317 2574 2341 k = 256 S=15 18130 3470 2731 2550 5-=99 18585 3942 3271 
3028 Table 2 Execution times in microseconds for different node code versions shown in Figure 8. processors, 
and they do not describe how their techniques could be extended in the presence of arbitrary affine align­ments 
between arrays and distributed templates. The method described by Stichnoth, O Hallaron, and Gross [16] 
is similar to the virtual-cyclic scheme mentioned above. They use intersections of array slices to generate 
communication for array statements, but support only a re­stricted class of alignments. Reeuwijk et al. 
present methods for rowwise and column­wise scanning of arrays in forall loops [15]. The two orders of 
traversal are derived from different decompositions of the positton equation, which specifies the relation 
between array indices and alignment and distribution parameters. Row­wise and columnwise enumeration 
correspond to the virtual­cyclic and virtual-block scheme, respectively, but they are also applicable 
in the presence ofnon-identity alignments. 8 Conclusions Widespread use of data-parallel languages, such 
as High Performance Fortran, will not come about until fast com­pilers and efficient run-time systems 
are developed. In this paper, we have presented a new algorithm for generating the local memory access 
sequence for computations over regular sections of arrays that are distributed using HPF cyciic(k) distributions. 
When evaluated based on its theoretical com­plexity, our method proves to be superior to the previously 
known solution for the same problem. Furthermore, our implementation experience indicates that the proposed 
al­gorithm is also more efficient in practice, which makes it a preferred choice for compilers and run-time 
systems for H PF-like languages. Although we know how to efficiently generate local ad­dresses in programs 
traversing regular sections of arrays with c~clic(k) distributions, many questions in HPF compi­lation 
remain unresolved. Some of the problems that require investigation are compiling programs that access 
diagonal or trapezoidal array sections and optimizing communication resulting from non-local accesses 
of array elements, both in the presence of cyclic(k) distributions. Only after these problems are fully 
and efficiently solved, can the use of such novel features of HPF become commonplace. 110 Acknowledgments 
We would like to thank Siddhartha Chatterjee for pro­viding us the code that implements the table construction 
algorithm based on sorting, and for his insightful comments on an early draft of this paper. Comments 
by the refer­ees have also helped improve the present ation. Finally, we thank Debbie Campbell and Lani 
Granston for their help in proofreading various drafts of the paper. References <RefA>[1] C. Aucourt, F. Goelho, 
F. Irigoin, and R. Keryell. A linear algebra framework for static HPF code distribution. In Pro­ceedings 
of the Fourth Workshop on Compilem foT Parallel Computers, Delft, The Netherlands, December 1993. [ ] 
B. Chapman, P. Mehrotra, and H. Zima. Programming in Vienna Fortran. Scientific P~ogramming, I (1 ):31-50, 
Fall 1992. [3] S. Chatterjee. Private communication, October 1994. [4] S. Cbatterjee, J. Gilbert, F. 
Long, R. Schreiber, and S. Teng. Generating local addresses and communication sets for data­parallel 
programs. In P~oceedings of the Fourth ACM SJG-PLAN Symposium on Principles and Practice of Parallel 
Programming, San Diego, CA, May 1993. [5] T.H. Cormen, C.E. Leiserson, and R.L. Rivest. Introduction 
to Algorithms. The MIT Press, Cambridge, MA, 1990. [6] J. Dougarra, R. van de Geijn, and D. Walker. 
A look at scal­able dense linear algebra libraries. In Proceedings of th e 1992 Scalable High Performance 
Computing Conference, pages 372-379, Williamsburg, VA, April 1992. [7] S.K.S. Gupta, S.D. Kaushik, C.-H. 
Huang, and P. Sadayap­pan. On compiling array expressions for efficient execution on distributed-memory 
machhles. Technical Report OSE­ CISRC-4/94-TRl 9, Department of Computer and Informa­ tion Science, The 
Ohio State University, April 1994. [8] High Performance Fortran Forum. High Performance For­tran language 
specification. Scientific Programming, 2(1­2):1 170, 1993. [9] S. Hiranandani, K. Kennedy, J. Mellor-Crummey, 
and A. Sethi. Compilation techniques for block-cyclic distri­  butions. In Proceedings of the 1994 
ACM Inteswaiional Conference on SupeTcomputing, Manchester, England, July 1994. [10] S. Hiranandani, 
K. Kennedy, and C.-W. Tseng. Compiling Fortran D for MIMD distributed-memory machines. Com­munications 
of the A CM, 35(8):66 80, August 1992. [11] R. Kannan. Algorithmic geometry of numbers. In J. Traub, 
editor, Annual Review o.f Compute7 Science. Annual Re­views Inc., Palo Alto, CA, 1987. [12] K. Kennedy, 
N. Nedeljkovit, and A. Sethi. Efficient address generation for block-cyclic distributions. In Proceedings 
oj the 1995 ACM International Con.feTence on Ssspercomput­ing, Barcelona, Spain, July 1995. [13] A. Knies, 
M. O Keefe, and T. MacDonald. High Perfor­mance Fort ran: A practical analysis. Scientific P~og~am­ming, 
3(3):187 199, Fall 1994. [14] C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. The 
High Performance FoTtTan Handbook. The MIT Press, Cambridge, MA, 1994.  [15] C. van Reeuwijk, H. J. 
Sips, W. Denissen, and E. M. Paal­vast. Implementing HPF distributed arrays on a mesage­passing parallel 
computer. Technical report, Advanced School of Computing and hnaging, Delft University of Tech­nology, 
February 1995. [16] J. Stichnoth, D. O Hallaron, and T. Gross. Generating com­munication for array statements: 
Design, implementation, and evaluation. JouTnal of PaTallel and Distributed Com­puting, 21(1 ):1 50 159, 
April 1994.</RefA> 111  
			
