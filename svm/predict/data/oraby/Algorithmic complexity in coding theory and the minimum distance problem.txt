
 Algorithmic Complexity in Coding Theory and the Minimum Distance Problem Alexander Vardy Coordinated 
Science Laboratory University of Illinois 1308 W. Main Street, Urbana, IL 61801 vardy@golay. csl .uiuc 
.edu Abstract. We startwithan overviewof algorithmiccomplexity problemsin coding theory We then show 
that the problemof com­puting the minimumdiktanceof a binaryIinwr code is NP-hard,and the correspondingdeci~ 
onproblemis W-complete. This constitutes a proofof the conjectureBedekamp,McEliece,vanTilborg,dating 
back to 1978. Extensionsand applicationsof this result to other problemsin codingtheqv are discussed. 
1. Introduction. This paper is organized in a manner contrapositive to the talk. In the talk, we give 
a detailed overview of algorithmic complexity issueain codhg theory. Herein,westart withan overviewof 
the prominentrolethat computational complexity plays in coding theory: we briefly surveythenumerousalgorithmiccomplexityissuesthat 
arise in code construction and decoding. Subsequently,we focus on one specific problem. Namely, we consider 
the problem of computing the minimum distance of a linear code, which isalsoequivalenttotheproblemof 
findingtheshortestcycle in a linear matroid over a finite field [90]. A long-standing conjecture [12] 
says that this problem is NP-hard, and we will settle this conjecture in the ai%rmative. We provide a 
detailed proof. We hope that our proof illustrates some of the elegant techniques used in coding theory 
today, such as the construction of MDS codes via Vandermondematri­ces[15,59,72]andconcatenatedcoding 
[23,31],forinstance. It is interesting that algebraic techniques of this kind cart be employed to answer 
an essentially combinatorial ques­tion. The complexity of computing the minimum distance and the proof 
of the conjecture of Berlekamp,McEliece, and vanTilborg [12] will be only briefly mentioned in the talk. 
 2. Complexity in coding theory In discussing the close ties between coding theory and com­plexity theory, 
there are two general categories of natural questions: one involving the application of codes to compu­tational 
complexity and the other focusing on the computa­tional complexity of coding itself. In the first category,codes 
havebeen usedextensivelyboth to deviseefficientalgorithms in a varietyof contexts and to prove that no 
such algorithms This workwassupportedby the Packard Foundation, the Na­tional Science Foundation, and 
the JSEP grant NOOO14-961O129. Permission 10 nmkc digiull/ll:lrd ct~pics OFall or p;lll ol lhis maleri:ll 
lilr pemomd or clmwoonl we is grantixi $vitllwl (LCprovided IIml the coplcs ,arenot madeor di.slnlw[cd 
I orprolil ial or cwnmerc;idvoi~[ogc.[he cop},­ right noliw. lhe title ot Illc p(llll]c:llitm find 11sd:llc 
nppcar, atld oo[ice is exist. For example, the elegant theory of probabiliiticrdly checkableproofs [6]usesalgebraiccodes 
in an essentialway to resolve Iong-staudIngopen questions about the hardness of approximation for such 
basic optimization problems as graph-coloring and clique-size. This mea of research has by now accumulated 
a sizable body of interestingresults. How­ever, we will not even attempt to discuss the use of codes 
in computational complexity. We refer the reader to [4, 6, 79], and especially [27], for a detailed overview 
of this subject. On theotherhand, inwhat follows,webrieflysurveysomeof the fascinating computational 
problems that arise in coding theory itself. We will try to make this overview accessible to as broad 
an audience as pcsssible,and will not assume any prior knowledge in coding theory. Our overview is by 
no means comprehensive;when [10] is finally ready, a more detailed survey would hopefully become available. 
The bkth of the subject of codkg for data transmission oc­curredatthetimeoftheannouncementof Shannon 
scoding theorems [73], which not only established the limits of the gains possible with codktg, but also 
proved the existence of codesthatcouldeffectivelyreachtheselimits. Shannon[73] showed that for every 
communication channel, there is a constant C, called the copocity of the channel, which has the followingfundamentalsignificance: 
if one wishesto commu­nicateoverthechannelatarateR (inbitsperchanneluse), then one can do so as reliably 
as desired, if and only if R < C. Specifically, for every &#38; >0 there exists a sufficiently long error-correctingcode 
C of rate R, such that the probability of error in maximum-likelihood decoding of C is at most e. It 
was recognized early on that the trouble with Shannon s codingtheorems,fromapracticalpointof view,isessentially 
computationalin nature. AlthoughShannon[73]settledthe question Dogoodcodesexist? intheatlirmative,hiswork 
led to two other questions How can we find such codes? and How can we decode them? . In a sense, coding 
the ory is all about these two questions, and both questions are fundamentallycomputational. It is trivial 
to find the codes promised by Shannon using a superexponentialsearch, but in practice we would like to 
construct a code in polynomial time. This leads to the prob­lem of code construction, which is d~cumed 
in the next sutr­section. Furthermore, Shannon used maximum-likelihood givw thal wp~iglll is h! permission 
ol llle ,-\~h[. Im. 1 0topy olhcrw isc. tO republish, 10 posl011scllws or (0dislnhulc [0 Iisls. t cqllires 
Spccilic permissionand/or Lx .W()(  97 El Paso, 1-cxnsLis<\ Copyrighl 1997 ACM 0-8979 I -XXX-(W71(J5 
..$3.50 decoding in the proof of his theorems [73], which can be trivially accomplished by an exponential 
brute-force search, but in practice we would like to decode more efficiently. This leads to the questions 
of complexity of decoding, which we discuss in more detail in a later subsection. 2.1. Complexity of 
code construction A code C of length n over the finite field IFq is a subset of IF;, the vector space 
of n-tuples over lF~. The primary example of a finite field, in theory aa well aa in practice, is IFz 
= {O, 1}, and codes over IFz are called binary. A code is said to be linear if it is a subspace of IF:, 
and we will be concerned with linear codes unless stated otherwise. The rate of a linear code C ~ lF~nis 
defined as R= k/n, where k = dim C Given a code C of length n and dimension k over IFg, we can encode 
an arbitrary information word u 6 IF} into a codeword c E C via a one-to-one mapping S : IF) ~ C called 
an encoder for C. Thus the rate R is the number of information symbols per code (channel) symbol, and 
we would like the rate to be high. A linear code C can be compactly specified as the row-space of a kx 
n generator matrix G or as the kernel of a (n k) x n parity-check matrix H. Both G and H are usually 
assumed to have full row-rank, and can be computed from each other in time 0(n3 ) using standard linear 
algebra techniques. No­tice that a generator matrix G also specifies an encoder for C via &#38;(u) = 
UG. Thus encoding is easy, it can be always im­plemented in time 0(n2) as a vector-matrix product. The 
error-correction capability of a code has to do with its minimum Hamming distance where the second equality 
is true for linear codes. Here, the Hamming distance d(z, y) is the number of positions where z and y 
differ, while the Hamming weight wt(z) = d(z, O) is the number of nonzero positions in z. Thus an error­correcting 
code can be viewed m a packing of disjoint spheres of radius t= l(d 1)/2j in the space IF; endowed with 
the Hamming metric. If codewords of C are transmitted over a noisY channel, then errors in any < tpositions 
may be cor­rected at the receiver end by identifying the unique sphere to which the error-corrupted channel 
output belongs. Ev­idently, we would like t, and hence also d, to be large. It should be obvious that 
attaining a high rate R= log~ 1~/n and a large distance d are conflicting goals. Hence the codes of interest 
for communications are those that achieve a good tradeoff between these two parameters. Reed-Solomon 
codes. Here is an example of a simple polynomial-time construction that produces excellent codes. As 
is often done in coding theory, we will specify a code C by describing an encoder for C. For each information 
word u = (Uo,ul,. ... Uk_~) c F;, we first consider the polyno­ k 1 mid ~.(Z) = Uk-1$ + ~~. + UIZ + 
UO. Then the code­word c=(cl, cz, ..., ~) E C of length n = g 1 correspond­ing to u is the evaluation 
of /U(z) at all the nonzero ele­ments crl,az, .... crg-..lof the field lF~,that is c~= fw(ai). Since 
a nonzero polynomial of degree s k 1 vanishes for at most k 1 elements of JFg, we immediately obtain 
that d z n (k 1) in view of (l). On the other hand, it is not difficult to show that d < n k + 1 for 
any linear code. This result is known as the Singleton bound [59, p.33], and codes that attain this bound 
with equality are called maximum­distance separable (MDS). MDS codes are related in many ways to various 
problems in combinatorics and linear alge­bra (cf. [59, Chapter 11]), and the codes we have just con­structed, 
known as Reed-Solomon codes [71], constitute the primary example of MDS codes. This is also an example 
of the general class of polynomial codes, used extensively in the theory of probabilistically checkable 
proofs [4, 27, 79]. Reed-Solomon codes can be found today in almost every household (compact disk players 
use these codes) and on the outskirts of the solar system (in the hardware of the Voyager probe). A major 
problem with Reed-Solomon codes is that they are short. The one hint given to us by Shannon [73], with 
regard to code construction, is that achieving channel capacity cafls for long codes, namely n should 
be large. On the other hand, for practical considerations, we would like to keep the size q of the code 
alphabet fixed and small, preferably q = 2. Yet, for Reed-Solomon codes n ~ q. There are several ways 
of modifying the original Reed-Solomon construction [71] to alleviate this problem, as discussed in what 
follows. Algebraic-geometry codes. One option is to consider poly­nomials in more than one variable. 
For example, a poly­nomial ~(z, y) over lF~ can be evaluated at up to q2 dis­tinct points, leading to 
a code of length n s q2 rather than n~ q. The problem here is that the known bounds on the number of 
points in the plane lt?~x IFq at which such a polynomial might vanish lead to rather weak codes. These 
bounds become much stronger if we evaluate polynomials along an algebraic curve. For example, if q = 
T2, we can eval­uate f(z, y) at the rational points of the Hermitian curve, namely at all the solutions 
(z, y) c Fq x Fq to the equation ~T+l y y= O. It is easy to see that there are exactly r3 distinct solutions, 
and this procedure produces the Hermi­tian code of length n = r3 > g which is nearly MDS. Namely d = 
(n k+l) g, where g = l/2r(r 1) is the genus of the curve. This is a simple example of a generaf type 
of codes, known aa algebraic-geometry codes [83, 84]. Algebraic-geometry codes are arguably the most 
powerful codes known today. These codes are constructible in polyno­mial time, but the complexity of 
constructing the best codes in this class tends to be too high for practiczd purposea: 0(n30) was originally 
reported in [60], which was recently improved to 0(n17 ) in [56]. A construction that would pro­duce 
a generator matrix for such codes in time 0(n3 ), say, would be a significant achievement, and there 
is reason to believe that a result of this nature is forthcoming [70]. Another drawback of algebraic-geometry 
codes is that their symbol alphabet is still quite large. Algebraic-geometry constructions are successful 
(in particular, better than the Gilbert-Varshamov bound discussed below) only for q >49. BCH codes. On 
the other hand, here is how one can get binary codes from Reed-Solomon codes: given an (n, k, d) Reed-Solomon 
code C over I13m, let C be the (n, k*, d*) binary subfield subcode of C, that is C = Cfl IF;. This pro­duces 
the important class of binary BCH codes [59, Chap­ter 9]. These codes are easily constructible in polynomial 
time, as we can compute a parity-check matrix for the BCH code O in time O(n log n) from a parity-check 
matrix for the Reed-Solomon code C. It is obvious that d* ~ d but k < k, and the question is whether 
a code with param­eters (n, k , d*) is still a good code. It turns out that for short lengths, say up 
to n < 128, BCH codes are among the best binary codes known. However, as n -+ cc, either d*/n or k* /n 
tend to zero. The latter result, due to Lin and Weldon [54], shows that BCH codes are asymptotically 
bad. Asymptotically good codes. This brings us to the subject of asymptotic properties of codes. For 
i = 1, 2, ..., let G be an (~i, ki, di) linear code over a fixed field IFq. The infinite se­quence of 
codes Cl, C2,. . . is said to be asymptotically good if ni + cm with i, while kl/n~ + R and d~/n~ ~ b 
for some nonzero R and 6. Thus all we require is that the rate and the relative distance are both asymptotically 
non-vanishing. This defining property being so generous, it is rather sur­prising that it took nearly 
25 years to come up with the first example of an asymptotically good sequence of binary codes that is 
constructible in polynomial time [46]. The ba­sic idea of Justesen [46] was a clever use of a concatenated 
coding technique [31], which is described in detail in the next section. Today, many such constructions 
are known: Zyablov codes [94], codes constructed by Shen [74] from Hermitian curves using a variation 
of Justesen s concatena­tion, and codes constructed using expander graphs by Alon et al. in [3] are just 
a few examples. Asymptotically good bi­nary codes with polynomial-time complexity of construction and 
the best known parameters were obtained by Vl&#38;du~, Katsman, and Tsfasman in [89]. The parameters 
of these codes are depicted in Figure 1. The approach of [89] com­bines powerful algebraic-geometry codes, 
constructed from Drinfeld s modular curves, with concatenated coding. 0.5 04 JPL upparbound Gilbott.Varehamov 
lowerbound (expopantial-tkna cqratruclion) 0.2 Waduta-KatamawTafaanuncodaa (polvmmlal-tkna construction)0 
0 0.25 0.5 0.75 R Figure 1: Parameters of mymptotically good binary codes The Gilbert-Varshamov bound. 
Not surprisingly, if we aflow exponential-time complexity of construction, things become much easier: 
a simple greedy algorithm works and produces parameters R and J much better than those of [89]. Since 
a linear code C is the kernel of its parity-check matrix H by definition, the minimum distance of C is 
just the minimum number of linearly dependent columns of If, in view of (1). We construct an (n k) x 
n binary parity-check matrix H with the property that every d 1 columns of H are linearly independent, 
column-by-column, using greedy search. After i columns of H have been already chosen, there are at most 
  (:)+o+ +( a distinct linear combinations of these i columns, taken d 2 or fewer at a time. If this 
number is less than 2n-k 1 we can always find another nonzero column, different from these linear combinations, 
and append it to H. We can keep doing this, and complete the construction of all the n columns in H, 
provided 1+C1)+(3+ +(0  <2 - 2) Thus if (2) holds, we can construct an (n, k, d) binary code in exponential 
time. This simple result is known as the Gilbert-Varshamov bound. Further evaluating this bound for n 
+ co produces asymptotically good binary codes whose parameters R and d lie on the curve R = 1 H2(8), 
where H2(z) = zlog2 z-1+ (1 z)log2(l x)-l is the binary entropy function. A long-standing conjecture 
says that this curve is the best possible for binary codes. Proof of this con­jecture is one of the major 
open problems in coding theory today. Such a proof would establish a rather remarkable fact that the 
simple greedy algorithm described above is (asymp­totically) the best possible construction method for 
binary codes, regardless of the available computational resources. An even more significant achievement, 
at least for practical purposes, would be a polynomial-time construction of bi­nary codes whose parameters 
lie on the Gilbert-Varshamov curve R= 1 Hz(6). A considerable amount of effort has been devoted to this 
problem, which nevertheless remains open. The lack of such construction is particularly surpris­ing in 
view of the fact that almost all linear codes attain the Gilbert-Varshamov bound [83, p.77]. Thus it 
is easy to devise randomized algorithms that with high probability produce codes lying on the curve R 
= 1 H2 (6), Such algo­rithms could indeed be used for code construction, if there were a polynomial-time 
procedure for computing the mini­mum distance of a general linear code. However, as we will show in the 
next section, computing the minimum distance of linear codes is NP-hard. In this subsection, we discussed 
the complexity of code con­struction, while completely ignoring the complexity of de­coding the resulting 
codes. Although, historically, this has afso been the prevailing approach in coding theory for some time, 
it is worth emphasizing that the two problems code construction and decoding are closely related to 
each other: we would generally like to have a good code which can be also efficiently decoded. In this 
context, it might be appro­priate to mention the recent work of Spielman [76, 77] who constructs asymptotically 
good codes that can be decoded in linear time. The approach of [76, 77] is based on certain results from 
the theory of expander graphs, and it would be desirable to resolve the random nature of these results. 
We defer a more detailed discussion of the expander codes of Spielman [76, 77] to the following subsection. 
 2.2. Complexity of decoding Suppose that a codeword c of a linear code C c lJ?~is trans­mitted over 
a noisy communication channel with output al­phabet A, and a vector y E An is observed at the channel 
output. Decoding is the task of trying to determine what c was, given y, ~ and a probabilistic model 
for the channel. There is a large number of various approaches to this task, which can be roughly classified 
into four categories, accord­ing to whether they are maximum-likelihood or bounded­distance, hard-decision 
or soft-decision. Of course, many useful decoding algorithms do not fall squarely into either category, 
as we discuss later in this subsection. Hard-decision vs. soft-decision decoding. The terms hard and 
soft decision are largely historical; in our context these terms have to do with the channel model assumed 
for decod­ing purposes. In both cases, the channel is usually assumed to be memoryless. This means that 
the noise is an i.i.d. random process: the probability of error is the same at all times (codeword positions), 
and what happens at one time is independent of what happens at all other times. To illustrate the difference 
between hard and soft decision decoding we will, for the sake of simplicity, restrict our at­tention 
to bhmry codes. That is, we consider the case where the channel input is {O, 1}. In hard-decision decoding, 
the channel output A is also {O, 1}. The most useful model in this case is the binary symmetric channel: 
a bit at the channel input is either transmitted as is, or inverted with a fixed probability p. In soft-decision 
decoding, the channel output A is a large set, most often A is the real line. The channel is characterized 
by two known probability-density functions fo(.) and fl (), where f,(a) is the probability of receiving 
a E A given that z ~ {O,1} was transmitted. This is a very general channel model. The most useful special 
case, known as the additive white Gaussian noise (AWGN) channel, is when fO(.) and fl (.) are the Gaussian 
dktribu­tions IV(+1, a2) and N( -1, Oz), respectively. In this case, it is convenient to think of codewords 
as embedded in the Eu­clidean space R via the mapping {O,1} ~ {+1, l}. Under this mapping, a binary code 
C of length n becomes a subset of the 2 vertices of the hypercube [+1, 1] . Furthermore, the logarithm 
of the probability (log-likelihood) of receiving y E R becomes proportional to the squared Euclidean 
dis­tance from y to the transmitted codeword c E C c [+1, 1] . Usuafly, soft-decision decoding is a much 
more challenging task than hard-decision decoding, but the potential rewards are great. Loosely speaking, 
for a given code, maximum­likelihood soft-decision decoding requires twice less energy per bit than maximum-likelihood 
hard-decision decoding, to achieve the same probability of decoding error [17]. Maximum-likelihood vs. 
bounded-distance decoding. Given the channel output y E An, the optimal decoding strategy is to find 
the codeword c E C that maximizes the probability Pr{c]y} that c was transmitted given that y was received. 
We may usually assume, without loss of generality, that the codewords of C are transmitted with equal 
a priori probabil­ity l/l Cl. In this case, by a simple application of the Bayes rule, the optimal decoding 
strategy is equivalent to finding the most likely codeword c E C that maximizes the probabil­ity Pr{ylc} 
that y would be received if c was transmitted. A decoder for C that afways finds the most likely codeword 
(or one of the most likely codewords, if there are ties) is said to be a maximum-likelihood decoder. 
As discussed above, maximum-likelihood decoding is a com­putational task that means different things 
for different channels. On a binary symmetric channel, the most likely codeword is obviously the one 
closest to y G IF; in the Hamming metric. In fact, the same conclusion easily ex­tend to the more general 
case of g-ary symmetric channels. Thus hard-decision maximum-likelihood decoding is a near­est neighbor 
search in the space IF; endowed with the Ham­ming metric. On the other hand, it is not difficult to show 
that soft-decision maximum-likelihood decoding (for binary codes) is equivalent to finding the codeword 
c c C which maximizes the log-likelihood sum M(c), given by n M(c) = ~(-l)c pi (3) i=l where pi = log 
~O(yi) log ~1(yi) is the log-likelihood ra­tio for the i-th position. A similar expression exists for 
maximum-likelihood soft-decision decoding of non-binary codes. In the important special case of AWGN 
channels, maximizing (3) again reduces to the nearest neighbor search, except that now C is viewed as 
a subset of [+1, l] and the search is in the space R endowed with the Euclidean met­ric. Thus in most 
(but not all) cases, maximum-likelihood decoding is equivalent to nearest neighbor decoding. Although 
maximum-likelihood decoding is the optimal de­coding strategy, it is NP-hard for the general class of 
linear codes [12]. Moreover, polynomial-time maximum-likelihood decoding algorithms are not known today 
for any specific family of useful codes, such as the binary BCH codes, for instance. Thus we are often 
interested in decoding strate­gies that are sub-optimal, but permit more efficient decod­ing algorithms. 
The major example of such decoding strat­egy is bounded-distance decoding. A decoder is said to be bounded-distance 
if there exists a constant t >0 such that the decoder always finds the closest codeword to a channel 
output y, provided the distance from y to that codeword is at most t. There is no guarantee on what a 
bounded-distance decoding algorithm does if the distance from y to the closest codeword c exceeds t:it 
may still find c, or it may output another codeword, or it may simply halt indicating failure. In hard-decision 
bounded-distance decoding, we usually have t = l(d 1)/2], while in soft-decision bounded-distance de­coding 
(of binary codes on the AWGN channel) we usually have t= W. In both cases, t is equal to half the minimum 
distance between distinct codewords, in the corresponding metric. This means that t is the largest constant 
for which we can guarantee the correction of all error patterns of weight (norm) at most t, even if we 
were to use maximum-Euclidean space !Rn. The increase in complexity from hard­likelihood decoding. Bounded-distance 
decoders of this kind decision to soft-decision decoding is by a factor of O(d). are said to achieve 
the error-correction radius of the code. Algebraic bounded-distance decoding. For many important families 
of codes, in particular BCH codes, Reed-Solomon codes, and most algebraic-geometry codes, we now have 
polynomial-time hard-decision bounded-distance decoding algorithms that achieve the error-correction 
radius of the code. The availability of such bounded-distance decoders is the result of a considerable 
body of work in coding theory, see [11, 42, 83] and references therein. The most notable contribution 
to the development of these decoders is arguably the Berlekamp-Massey [11, 61] algo­rithm which found 
applications also outside of coding the­ory, in such fields as cryptography and dynamic control. We will 
illustrate the idea for Reed-Solomon codes. Suppose that c= (co, cl, . . ..cl l ) is a codeword of a 
Reed-Solomon code C over lF~ = IF~+l, and let e = (eo, el, . .,e~ 1) be the channel error-vector, so 
that y = c + e. Consider the 1z n-1 associated polynomials C(Z) = C. + ..+ CIZ+CO and e(z) = en lzn 
l +.. + elz + eo. Notice that the sum y(z) = c(z) + e(z) is known to the decoder. Furthermore, it is 
not difficult to see that the (n, k, d) Reed-Solomon code C, defined in the previous subsection, is precisely 
the set of all c EIF:, such that the associated polynomial C(X) satis­fies c(crl) = C(crz) = . ~. = C(ad- 
) = O, where a is a fixed element of order n in lF~. The decoder can thus compute S, ~f e(ai) = e(ai) 
+ C(CY;) = y(ai) (4) for all i = 1,2, . . . . d 1. Moreover, observe that the entire error-vector e 
can be reconstructed from its n syndromes SI, S2,.. ., S., defined by (4). The problem is that only the 
first d 1 of these n syndromes are known to the de­coder. Fortunately, it can be shown that if wt(e) 
~ t then the syndrome sequence S1, S2,. . ., S~ satisfies a linear re­currence of order ~ t. This recurrence, 
and with it the en­tire sequence S1, S2, . . . . S., can be recovered from the first d 1 syndromes S1, 
S2,... , S,j-1, provided d 1 ~ 2t. This is precisely what the Berlekamp-Massey algorithm does: it is 
a general purpose procedure for efficiently determining the linear recurrence of lowest order that produces 
a given sequence (over any field). If the sequence S1, %, ..., Sd-1 of known syndromes satisfies a linear 
recurrence of order t s L(d 1)/2], then the Berlekamp-Massey algorithm finds this recurrence in time 
0(t2 ). The complexity of the syn­drome computation in (4) is O(nt), which is also the com­plexity of 
reconstructing the error-vector e given the recur­rence generating S1, S2, ....Sn. It is obvious that 
the Berlekamp-Massey algorithm is in­herently a hard-decision decoding algorithm. However, For­ney and 
Vardy [36] have recently shown how this algorithm can be employed for polynomial-time bounded-distance 
de­coding in Euclidean space. The results of [36] are actually more general. Forney and Vardy [36] show 
that whenever an (n, k,d) code C has a hard-decision bounded-distance de­coder which achieves the error-correction 
radius [(d 1)/2j, it can be used to design a soft-decision bounded-distance de­coder which achieves the 
error-correction radius W in the It is also clear from our discussion of the BerIekarnp-Massey algorithm 
that it may fail whenever the weight of the er­ror vector exceeds [(d 1)/2]. A number of papers [30, 
40] have been devoted to developing polynomial-time algorithms that guarantee decoding to the closest 
codeword beyond this bound. For Reed-Solomon codes, the best results have been recently obtained by Sudan 
[80], using a new approach. The algorithm of [80] decodes to the closest codeword whenever the weight 
tof the error vector is bounded by t < =a,b:(a+l)(b+l )+~b(.t-l)(b+l)>n {n-a-b(k-l)-l} This may be well 
approximated as t~ n (2nk)112. The decoding complexity is still polynomial, and the foregoing upper 
bound on t becomes far better than the conventional ts l(d 1)/2j for Reed-Solomon codes of rate R < ~. 
Finally, our discussion of the Berlekamp-Massey afgorithm illustrates the idea of reconstructing the 
unknown syn­dromes tkom the known ones. Once all the syndromes have been computed, the entire error vector 
can be recovered. Loosely speaking, this general idea underlies most of the known bounded-distance decoding 
algorithms for algebraic­geometry codes. These decoding algorithms involve deep r~ suits from algebraic 
geometry, and we will not even attempt to describe them here. We refer the reader to H@holdt and Pellikaan 
[42] for an excellent recent survey of this subject. The hardness of maximum-likelihood decoding. Maximum­likelihood 
decoding of a binary linear (n, k, d) code C, both hard-decision and soft-decision, can be trivially 
ac­complished in time 0(2~ ) by simply comparing the chan­nel output y with all the 2k codewords of C. 
It is not difficult to show that this task can be also accomplished in time 0(2 k). The following result, 
due to Berlekarnp, McEliece, and van Tilborg [12], shows that for the general class of binary linear 
codes, polynomial-time algorithms are unlikely to exist (unless P = NP), even for the simpler case of 
hard-decision maximum-likelihood decoding. To describe this result, we first need to express the maximum-likelihood 
decoding problem in slightly different terms. Given the chan­nel output y E IF; and an (n k) x n parity-check 
matrix H for a binary code ~ we define the syndrome of y as s = Hyt. Then the most likely codeword, that 
is the one closest to y in the Hamming metric, is given by c = y + e, where Hei = s and e c IF; is the 
minimum-weight vector with this prop­erty. Hence maximum-likelihood decoding is equivalent to finding 
e, given H and s. The corresponding decision prob­lem can be stated w follows: Problem: MAXIMUM-LIKELIHOODDECODING 
Instance: A binary m x n matrix H, a vector s E lF~, and an integer w >0. Question: Is there a vector 
z E IF; of weight <w, such that Hzt = s? Berlekamp, McEliece, and van Tilborg [12] show that this problem 
is NP-complete using a reduction horn 3-DIMEN-SIONALMATCHING,a well-known NP-complete problem [39]. 
The complexity of MAXtkNJM-LikelihOOd DECODING is fun­damental in coding theory, and it was analyzed 
in several contexts by several authors [5, 9, 16, 20, 25, 78]. Fang, Cohen, Godlewski, and Battail [25] 
used the results of Berlekamp, McEliece, and van Tilborg [12] to establish the rather obvious fact that 
soft-decision maximum-likelihood decoding of linear codes is afso NP-hard. Barg [9] showed that MAXIMUM-LIKELIHOODDECODINGis 
NP-complete for codes over an arbitrary alphabet of fixed size q, not only for the case q = 2. Stern 
[78] and Arora, Babai, Stern, Sweedyk [5] proved that approximating the solution to MAXIMUM-LIKELIHOODDECODINGwithin 
a constant factor is NP-hard. Bruck and Naor [16] considered a slightly different context of maximum-likelihood 
decoding with pre-processing. They assume that the parity-check matrix H is not part of the instance; 
rather it is fixed and available to the decoder in advance (as is the case in practice). The decoder 
can thus perform computations on the matrix H, prior to being pre­sented the instance consisting ofs 
c lF~-k and w >0. Bruck and Naor [16] prove that MAXIMUM-LIKELIHOODDECODING remains hard even with unlimited 
amount of pre-processing on H, in the following sense: the existence of a polynomial­time afgorithm for 
this version of MAXIMUM-LIKELIHOOD DECODING implies that the polynomial hierarchy collapses at the second 
level, namely U~l X: = X;. The proof of [16] is baaed on the theorem of Karp and Lipton [47] which shows 
that the polynomial hierarchy collapses in this way if an NP­complete problem can be solved in polynomial 
time with polynomial advice (belongs to the class P/poly). The result of the pre-processing on H constitutes 
the polynomial advice in the argument of [16]. Downey, Fellows, Vardy, and Whittle [20] recently proved 
that MAXIMUM-LIKELIHOODDECODING is hard for the parametrized complexity class W[l]. Namely, it is un­likely 
that there exists an algorithm which solves MAXIMUM-LIKELIHOODDECODINGin time /(w)nc, where c is a con­stant 
independent of w and f(. ) is an arbitrary function. Many NP-complete problems are fixed-parameter tractable. 
For example VERTEX COVER, a well-known NP-complete problem [39] which asks whether a graph G on n vertices 
has a vertex cover of size at most k, can be solved [8] in time O(kn + (4/3) kk2). Loosely speaking, 
the parametrized complexity hierarchy FPT = WIO] ~ WII] c W[2] c . . . in­troduced by Downey and Fellows 
[18, 19] distinguishes be­tween those problems that are fixed-parameter tractable and those that are 
not. The result of [20] shows that MAXIMUM-LIKELIHOODDECODINGis not likely to be fixed-parameter tractable. 
Furthermore, this result implies that bounded­distance decoding for the class of binary linear codes 
is hard in the following sense: if a polynomial-time algorithm for bounded-distance decoding exists then 
the parametrized hi­erarchy collapses with IV[l] = FPT. Although we have, by now, accumulated a considerable 
amount of results on the hardness of MAXIMUM-LIKELIHOOD DECODING,the broad worst-case nature of these 
results is still somewhat unsatisfactory. For example, as discussed in the previous subsection, it is 
known [83, p.77] that al­most alI linear codes attain the Gilbert-Varshamov bound. However, there is 
no proof so far that the hard instances of MAXIMUM-LIKELIHOODDECODINGdo not belong to the vanishing fraction 
of codes that do not meet the Gilbert-Varshamov bound. It is very unlikely that this is so, but we would 
like to have a proof. Thus, it would be worth­while to establish the hardness of MAXIMUM-LIKELIHOOD DECODINGin 
the average sense [53], or for more narrow classes of codes. A first step along these lines was taken 
by Barg [9], who showed that MAXIMUM-LIKELIHOODDECOD-INGis NP-hard for multilevel (or generalized concatenated) 
linear codes. However, the class of multilevel codes consid­ered in [9] is still rather broad, and further 
results of this nature would be desirable. Trellis decoding and its complexity. As discussed in the fore­going 
paragraph, it is unlikely that there exist polynomial­time maximum-likelihood decoding algorithms for 
Iinear codes, either soft-decision or hard-decision. Nevertheless, we can do substantially better than 
(9(2k ) or 0(2n k). Namely, it is possible to achieve full maximum-likelihood soft-decision decoding 
of certain linear codes in exponential time, but with complexity exponent much less than n min{R, 1 R}. 
Despite their exponential nature, such algorithms are of con­siderable interest in coding theory, due 
to the significant gap in performance between hard-decision bounded-distance de­coding and soft-decision 
maximum-likelihood decoding: on many useful channels, a maximum-likelihood soft-decision decoder for 
a short code might achieve a lower probability of error than a hard-decision bounded-distance decoder 
for a much longer code. Trellis decoding is a primary example of exponential-time maximum-likelihood 
decoding algorithms of this type. Trel­lises can be also used to implement randomized sequential decoding 
algorithms [26, 33], whose performance is close to maximum-likelihood and whose running time is polynomial 
with appreciably high probability. A trellis T is an edge-labeled directed graph T = (V, E, A) with the 
following property: the set of vertices V can be partitioned into disjoint subsets Vo, VI,..., Vn, such 
that ev­ery edge e E E begins at a vertex v E Vi and ends at a vertex v E V,+l for some i = O,1,....n 
l. It is usually assumed that the subsets VO,V. C V each consist of a single vertex, called the root 
and the toor, respectively. Clearly, the se­quence of edge labels along each path from the root to the 
toor in T defines an n-tuple (al, az, . . . an) over the label al­phabet A. We say that T represents 
a code C over A if the set of afl such rwtuples is equal to the set of codewords of C. Given a code C 
over Il?gand a trellis T = (V, E, IFq) that rep­resents ~ maximum-likelihood soft-decision decoding of 
C can be accomplished with exactly 21El IVI+ 1operations. The decoding procedure that accomplishes this 
is known as the Viterbi algorithm [32]. Basically, the Viterbi afgorithm is a simple application of dynamic 
programming [68]. We refer the reader to [63, 86] for a detailed description of the Viterbi algorithm. 
Here, we are more concerned with the properties of the trellis itself. It is obvious that every trellis 
T represents a unique code, which can be determined by readng the edge labels of each path in T. However, 
we usually need to solve the converse problem: namely, given a code C we wish to construct a trellis 
T which represents C. It is easy to see that there are always many different trellises representing the 
same code. Hence, we would generally like to construct the best trellis representing a given code. This 
problem has two important aspects, discussed in what follows. If we think of a code C as a fixed set 
of codewords, then constructing the best trellis for C is relatively straightfor­ward. It is shown in 
[65, 49] that for every linear code ~ there exists a unique, up to graph isomorphism, minimal trellis 
representing C. It is now known [63, 65, 87] that the minimal trellis simultaneously minimizes all the 
conceivable trellis complexity measures, such as 21EI IVI + 1, IVl, I.El, maxi lV~l,aad lVil for each 
i = O,1, ..., n, among all possible trellis representations for C. Here is one way to construct the minimal 
trellis, due to Babl, Cocke, Jelinek, and Raviv [7]. Let H=[hl, h2,..., hn] bean (n k) x n parity-check 
matrix for C. Then the set of vertices Vi is given by ~ = {Clhl+... +C{h; :( Cl, Ci, ci+l,i,Cn)EC, Cn)EC 
fOr SOmeci+l, . . ..~CIFq} fori=l,2,..., n, with the convention VO= {O}. There is a edge fkom a vertex 
v c Vi to a vertex v 6 Vi+l if and only if there exists a codeword (cl, cz,. ... c~) c C, such that Clhl 
+-. +&#38;-1hi-1 +Ciha = V Clhl +... + Cihi + Ci+lhi+l = v The label of this edge is ci G lF~. Today, 
at leaat three other constructions of the minimal trellis are known [62, 34, 49]. In general, it is fair 
to say that minimal trellises for linear codes are by now well understood, and all the questions pertaining 
to the minimal trellis have been already answered. Surprisingly, however, the innocuous operation of 
permut­ing the symbols in each codeword of C according to a fixed permutation n seems to assume a fundamental 
significance in the context of trellis complexity, and leads to a number of challenging problems. Two 
codes C and T(C) that dif­fer by a permutation of coordinates are called equivalent in coding theory. 
Until recently, such codes were considered aa essentially same. They certainly have the same parame­ters 
(n, k, d) and share all other important properties, ezcept trellis complexity. It turns out that a permutation 
of coor­dinates can drastically change the number of vertices in the minimal trellis representation of 
a given code ~ often by an exponential factor. The problem of minimizing the trellis complexity of a 
code via coordinate permutations, termed the art of trellis decoding by Massey [62] has attracted a lot 
of interest recently (for example, seven papers in [28] are devoted to this problem). Nevertheless, the 
problem remains essentially unsolved. We now describe the present state of knowledge on the permutation 
problem, with an emphasis on the asymptotic results. To do so, we need to introduce a precise mea­sure 
of the complexity of a trellis. The most widely ac­cepted [34, 51, 52, 65] trellis complexity measure 
may be de fined ass = maxi=o,l,...,n logq IUI, for a trellis T = (V, ~, ~g) with V = V. U V1 U ...U Vn. 
We can also define the relative trellis complexity as s = sfn. A binary code represented by a trellis 
with relative complexity s can be decoded in time 0(2nf), using the Viterbi algorithm. A simple upper 
bound, due to Wolf [93] and Massey [62], is given by f ~ min{R, 1 R} (5) This bound is true for the 
minimal trellis of any linear code, for any permutation of the code coordinates. The best known lower 
bound is due to Lafourcade and Vardy [51, 52]. For n + co this bound takes the following form: for my 
asymptotically good sequence of codes with rate fixed at R and relative distance d/n iixed at b, we have 
(6) where Rm= (.) is the JPL bound (see Figure 1). The bound in (6) holds for all linear codes, and for 
all possible coor­dinate permutations. It is depicted in Figure 2 for the case R= 1 Hz (6), that is for 
binary codes meeting the Gilbert-Varshamov bound. The best known upper bound in this < 0.5 0,45 0.4 0.35 
0.3 0.Z5 0,2 0,15 0.1 o.r6 o0 0.1 0.2 0,3 0.4 0.5 0.6 0.7 0,6 0.9 1 R Figure 2: Asymptotic bounds on 
trellis complexity context was obtained by Dumer [22] and Kudryashov-Zakha­rova [50]. They show that 
there exist bhry linear codes on the Gilbert-Varshamov curve R= 1 Hz(6), whose trellis complexity satisfies 
1 Hz(c$) O ~ R~ l Hz(l/d) <~ Hz(2J) If2(6) l Hz(l/d) ~ R ~ 1 ( for some coordinate permutation. This 
bound is also pitted in Figure 2. We observe that both bounds are signif­icantly below the triangle ~ 
= min{R, 1 R}, which corre­sponds to the Wolf-Massey bound of (5) and to straightfor­ward maximum-likelihood 
decoding. Tanner graphs and iterative decoding. There is a different way of representing a code by a 
graph, that has a polynomial rather than exponent ial complexity. That is, the number of vertices and 
edges in this graph is 0(rz2) for afl linear codes, and O(n) for a certain important sub-class of low­density 
codes [38, 76]. This idea datea back to the work of Tanner [81] in 1981, but has recently attracted a 
considerable renewed interest [20, 35, 37, 57, 58, 76, 77, 91, 92]. Staying true to the original source 
[81], we will refer to such graphs as Tanner graphs in what follows. Let H bean (n k) x n parity-check 
matrix for a binary linear code C. Then the corresponding Tanner graph for C is the bipartite graph on 
a set of n k red vertices 7? and a set of n blue vertices B, having H as its red-blue adjacency matrix. 
Thus a blue vertex /3 G L?corresponds to a code coordinate, while a red vertex p c 73 corresponds to 
a parity-check on its blue neighbors. If X ~ 23is the support of a vector z c IF;, then x G C if and 
only if afl the checks are satisfied: every red vertex has an even number of neighbors in X. It is clear 
that every bipartite graph G is a Tanner graph for some code, that is the binary linear code defined 
as the kernel of the red-blue adjacency matrix of G. Thus one can construct codes from bipartite graphs 
with certain proper­ties. Spielman [76, 77] uses this idea to construct a fam­ily of asymptotically good 
binary linear codes, for which bounded-distance decoding can be accomplished in linear time. The underlying 
Tanner graph is (h, 1)-regular, namely every blue vertex is adjacent to h red vertices (checks), and 
every red vertex is adjacent to 1 = hn/(n k) blue vertices (coordinates). The decoding algorithm, originally 
due to Gallager [38], is surprisingly simple. Given the assignment of coordinate values corresponding 
to the channel output y 6 IF;, some of the n k checks are satisfied while some oth­ers are unsatisfied. 
Examine the coordinates in some fixed order, and invert the first coordinate that appears in more than 
h/2 unsatisfied checks; recompute the checks and re­peat. It is obvious that for (h, 1)-regular graphs, 
with h fixed, this algorithm runs in time O(n). Spielman [76, 77] shows that if the underlying Tanner 
graph is a sufficiently good expander, then the algorithm is guaranteed to correct an errors, where a 
is a constant fraction depending on h, n, k and the expansion properties of the graph. This means that 
we have a linear-time bounded-d~tance decoder for the cor­responding code C, although this decoder does 
not usually achieve the error-correction radius of C. It is interesting that the Tanner graph for a code 
C can be afso used for soft-decision decoding of C, using a variant of the Viterbi algorithm known as 
the sum-product algo­rithm [35, 81, 91]. It has been recently shown by severaf au­thors [37, 57, 58] 
that the sum-product algorithm is actually a version of probability propagation or belief propaga­tion 
in Bayesian networks, a topic well-studied in the field of expert systems and artificial intelligence 
[69]. If the under­lying Tanner graph is cycle-free, then the sum-product algo­rithm converges to the 
optimal maximum-likelihood solution in time 0(n2). Unfortunately, Trachtenberg and Vardy [82] have recently 
shown that the question Which codes have cycle-free Tanner graphs? has a disappointing answer: es­sentially, 
only trivial ones. If the Tanner graph has cycles, the sum-product algorithm generally regresses into 
itera­tions. It is presently not clear under which conditions these iterations converge, and if they 
do, whether they converge to the maximum-likelihood solution. In fact, it is easy to construct pathological 
counter-examples [64] where no con­vergence is possible. Nevertheless, iterative decoding tech­niques 
based on Tanner graphs and similar graph represen­tations of a code (cf. Wiberg [91]) seem to work very 
well in practice. Of particular interest in this context are graphs of large girth. For such graphs, 
iterative decoding techniques appear to perform reasonably close to maximum-likelihood soft-decision 
decoding, with polynomial-time complexity. In fact, the turbo decoding algorithm of [13], considered 
by many a major breakthrough in coding theory, is a special case of iterative decoders of this kind [37, 
91]. Although these recent iterative decoding techniques consti­tute what is arguably the most promising 
type of decoders available today, a major concern is that we do not yet un­derstand why they work as 
well as they do. A first attempt to analyze such decoders was made by Wiberg [91]. Further investigation 
of iterative decoding on graphs with cycles is certainly an important project for future research. 3. 
The Minimum Distance problem Berlekamp, McEliece, and van Tilborg [12] showed in 1978 that two fundamental 
problems in coding theory, namely maximum-likelihood decoding and computation of the (non­zero terms 
in the) weight distribution, are NP-hard for the class of binary linear codes. They conjectured, but 
were unable to prove, that the following decision problem: Problem: MINIMUMDISTANCE Instance: A binary 
m x n matrix H and an inte­ger w >0. Question: Is there a nonzero vector z E IF; of weight ~ w, such 
that Hzt = O? is also NP-complete. It is easy to see that the NP-comple­ teness of this problem would 
imply that computing the min­ imum distance of a binary linear code is NP-hard. Indeed, let C be a linear 
code defined by the parity-check matrix H, and let d denote the minimum distance of C. If d is known, 
then one can answer the question of MINIMUMDISTANCEby simply comparing d and w. On the other hand, if 
one can solve MINIMUMDISTANCEin polynomial time, then one can also find d in polynomial time by successively 
running an algorithm for MINIMUMDISTANCEwith w = 1,2,..., until the first affirmative answer is obtained. 
The MINIMUMDISTANCEproblem has a long and convo­luted history. To the best of our knowledge, it was first 
mentioned by Welsh [90] at an Oxford Conference on Com­binatorial Mathematics in 1969. In the printed 
version [90] of his paper, Welsh calls for an efficient algorithm to find the shortest cycle in a linear 
matroid over a field IF. It is easy to see that for IF = IFq this is equivalent to find­ing the minimum 
weight codeword in a linear code over IF~. Hence the NP-completeness of MINIMUMDISTANCEimplies that a 
polynomial-time algorithm for the problem posed by Welsh [90] is unlikely to exist. Following the publication 
by Berlekamp, McEliece, and van Tilborg [12] of their con­jecture, the MINIMUMDISTANCEproblem was mentioned 
as open in the well-known book by Garey and Johnson [39, p. 280]. Three years later, it was posed by 
Johnson [44] as an open problem of the month in his ongoing guide to NP­completeness column. The problem 
remained open despite repeated calls for its resolution by Johnson [45] and others. Determining whether 
computation of the minimum Ham­ming distance of a linear code is NP-hard is important not only because 
this is a long-standing open problem. There are several more compelling reasons. First, for a host of 
problems in coding theory there is an easy reduction from MINIMUMDISTANCE. A few examples of such problems 
are presented in the next section. Hence if MINIMUMDISTANCE is computationally intractable, then all 
these problems are intractable as well. Secondly, as mentioned in the previous section, a polynomial-time 
procedure for computing the min­imum distance of a linear code would imply the existence of polynomial-time 
randomized algorithms for constructing bi­nary codes that attain the Gilbert-Varshamov bound. Thus it 
is important to know that such a polynomial-time proce­dure is unlikely to exist. Due to these and other 
reasons, the conjecture of Berlekamp, McEliece, and van Tilborg [12] sparked a remarkable amount of work, 
most of it unpublished. In particular, in an attempt to establish the NP-completeness of MINIMUMDISTANCE,a 
great number of closely related problems were proved to be NP-complete. For example, the problems of 
finding the max­imum weight of a codeword, finding a codeword of minimum weight which is not a multiple 
of a given integer, determining the existence of a codeword of weight in/2j, and computing the minimum 
weight of a codeword which is non-zero in a specified position, were shown to be NP-hard by Ntafos and 
Hak~mi [67], Calderbank and Shor (see [21]), and Lobstein and Cohen [55], respectively. At least eight 
different prob­lems of this kind are presently known to be NP-complete. It is pointed out in [55] that 
all the eight problems are tan­talizingly close to MINIMUMDISTANCE,and hence provide further evidence 
to support the conjecture of [12] that MIN-IMUMDISTANCEis NP-complete. The ensemble of all these problems, 
however, does not suffice to prove this conjecture. Our main goal in this paper is to prove that MINIMUMDIS- 
TANCEis NP-complete. To this end, we exhibit a polynomial transformation to MINIMUMDISTANCEfrom MAX]MUM-LIK-ELIHOODDECODING. 
Thus we settle the conjecture of Ber­lekamp, McEliece, and van Tilborg [12] in the affirmative, using 
a reduction from the main result of [12]. 3.1. Preliminaryobservations The transformation from MAXIMUM-LIKELIHOODDECODING 
to MINIMUMDISTANCEthat we will use places certain minor restrictions on MAXIMUM-LIKELIHOODDECODING. Hence, 
our goal herein is to observe that MAXIMUM-LIKELIHOOD DECODINGremains NP-complete under these restrictions. 
First, we slightly modify the question of MAXIMUM-LIKELI-HOODDECODINGby requiring that the solution to 
Hxt = s is nofzzero. This restriction makes a difference only for the special case s = O, for if s # 
O then obviously any solution x to Hxi = s is nonzero. We therefore observe that the proof in [12] of 
the NP-completeness of MAXIMUM-LIKELIHOODDECODING,based on the transformation from 3-DIMENSIONALMATCHING,uses 
only the special case where S=(ll. .l)t. Hence the same proof establishes that the minor variation of 
MAXIMUM-LIKELIHOODDECODING&#38;j­ cussed above is also NP-complete. Next, as pointed out in [12], one 
may assume w.1.o.g. that the m x n matrix H at the input to MAXIMUM-LIKELIHOOD DECODINGis full-rank. 
This implies that the columns of H contain a basis for IFzm,and we may further assume w.1.o.g. that w 
~ m 1. Indeed, if H is full-rank and w ~ m, then the answer to the question of MAXIMUM-LIKELIHOOD DECODINGis 
trivially Yes. We also assume w.1.o.g. that the columns of H are distinct. If this is not so, then we 
can form in polynomial time an m x n matrix H by retaining a single representative from each set of equal 
columns of H. It is easy to see that Hzt =s hm a solution of weight at most w, if and only if so does 
H x~ = s, providing s # O. But the case s = O may be safely excluded from the input, as discussed above. 
The assumption that all the columns of H are distinct further implies that n ~ 2-. These are all the 
assumptions that we will need. The key idea in the transformation from MAXIMUM-LIKE-LIHOODDECODINGto 
MINIMUMDISTANCEis to regard the columns of the m x n parity-check matrix H aa elements al, az, ..., an 
in the finite field ll?z~. The syndrome vector s E IFzmmay be also regarded as an element ~ in lF2~. 
With this notation, taking into account the restrictions discussed in the foregoing paragraphs, we may 
re-phrase MAXIMUM-LIKELIHOODDECODINGas the finite-field version of SUBSET SUM,a well-known NP-complete 
problem [39, p.233]. Specif­ically, consider the following problem: Problem: FINITE-FIELDSUBSETSUM Instance: 
An integer m ~ 2, a set of n s 2m distinct elements 0s, crz, . . ., an C IFw, a nonzero element @ c IFZWI,and 
a positive integer w S m 1. Question: Is there a subset {~,1 , cri,, . . . . ~,$} of the set{crl, a2, 
..., cYn}, such that CYil+.. .+cu$ = @ and b<w? According to the discussion in this subsection, we con­clude 
that the NP-completeness of MAXIMUM-LIKELIHOOD DECODING, established in [12], immediately implies that 
FINITE-FIELDSUBSETSUM is NP-complete. 3.2. NP-completenessfor codesof characteristictwo Given the input 
m, CM,CY2,..., cr~, ~, and w to FINITE-FIELD SUBSETSUM, we first construct a series of matrices Al, Az, 
. . . . AW, which may be thought of aa paxity-check matrices for the codes Cl, Cz, . . . . ~ over IFzm. 
These matrices are constructed in such a way (see Lemma 1 be­low) that the minimum distance of ~ is equal 
to 6 + 1 if fl~l+~:~+.. . + ~i~ = /3 for some il,iz, . . . ,i~. Otherwise the minimum distance of ~ is 
equal to 6 + 2, and o is an MDS code [59, p.317]. The matrix AI is given by AI= 1 l ~n; (7) 01 CY2... 
[ 1 and it is easy to see that the minimum distance of Cl is either2or3,according as~=criforsome i=1,2,....nor 
not. In general, for J = 2,3, . . . . w, the matrix A6 is given by Aa= (8) Notice that for all 6 = 1,2,. 
. . ,w, the matrix Ah has n + 1 columns and d + 1 linearly independent rows. Hence the dimension of ~ 
is n 6, and its minimum distance is at most (n+ 1) -dim ~ + 1 = d+ 2 by the Singleton bound [59, p.33], 
mentioned in the previous section. Lemma 1. Let dh denote the minimum distance of G. Then dh= 6+ 1,if 
and dh = C$+ 2 otherwise. Proof Let M be a (6+1) x (6+1) square matrix consisting of some 6 + 1 columns 
of A6. If the last column of Ah, namely (O. . . Ol@t, is not among the columns of M, then M is a Vandermonde 
matrix [59, p.1 16]. Since al, cw, . . . . an are all distinct, M is non-singular in this case. Otherwise, 
assuming w.1.o.g. that (O... 01~)~ is the last column of M, we expand along this column to obtain: where 
[ -I with respect to a matrix denotes the determinant. The first determinant on the right-hand side of 
the above expression is again a Vandermonde determinant, while the second one is a simple first-order 
alternant [14, 66]. Alter­ants were studied by Muir [66] and many others. In general, it is well-known 
that 1 1 .. 1 Xl XZ .Xk = ~k.j(x) ~ (Xiz Xil ) A proof of the above expression may be found in Muir 
[66, VO1.111,Chapter 5], for instance. The elementary symmetric function .S,(.) is defined by Sr(X) gf~~<ll<;z<...<;r<kxilxiz 
  i. and in particular S1(X) = Xl + X2 + ..-+ Xk. In our case, we indeed have r = k j = 6 (6 1) = 1, 
and the foregoing expression for det M reduces to det M = (ail +~i2 + . . -+aiJ p) ~ (@ib c%) (9) l~a<b~6 
Since al, cr2,. . . , an are distinct, the Vandermonde factor in (9) is non-zero, which implies that 
det M = O if and only if ail + ~iz + ... + ~a~ = /3. Thus if no subset of exactly 6 elements of {al, 
az, . . . . cr~} sums up to ~, then every 6 + 1 or less columns of A,5 are linearly independent. In this 
case dh = J + 2 by the Singleton bound, and ~ is MDS. On the other hand, if ail + craz+ . . . + ~it = 
/3 for some il, iz, . . . , ah, then obviously dh< d+ 1. Now, deleting the last row of Ah, we obtain 
the parity-check matrix A; which defines the code ~ that contains ~ as a subcode. It is easy to verify 
(cf. [59, p.323]) that ~ is an MDS code, andhence dh~d~=(n+l)-dim~6 +1=6+1. I We observe that the MDS 
codes discussed in Lemma 1 are of independent interest; they were studied by Roth and Lempel in [72]. 
We also point out that the counterpart of Lemma 1 over the positive integers was proved by Khachiyan 
in [48]. In our context, it follows immediately from Lemma 1 that if we could find the minimum distance 
of a linear code over a field of characteristic 2 in polynomial time, we could solve FINITE-FIELD SUBSET 
SUM in polynomial time. Formally, consider the following problem: Problem: MINIMUMDISTANCEOVER lF2-Instance: 
A positive integer m, an r x n matrix % over lFzI., an integer u >0. Question: Is there a nonzero vector 
x of length n over IF2M,such that Hzt = O and wt(x) < w? One might argue that the operations in MINIMUMDISTANCE 
OVER lFzm, in short MDv, are over the finite field IFw, whereas the operations in MAXIMUM-LIKELIHOODDECOD-INGare 
over IFz. If one were to implement the operations in lFz~ using a table of the field, for example, then 
this would require exponential memory. However, if we implement the operations in 11% as polynomial addition 
and multiplication modulo an irreducible polynomial g(z) of degree m, then only linear memory is required, 
and each operation in IFzm can be carried out in polynomial time using operations in IFZ. Proposition 
2. Existence of a polynomial-time algorithm for MDWI implies the existence of a polynomial-time algo­rithm 
for FINITE-FIELDSUBSETSUM. .. .. Prooj. Suppose that @ is a polynomial-time algorithm for MDz.m. Then, 
given the input to FINITE-FIELD SUB­ x;   x; ... x; SETSUM,we construct the matrices Al, Az, ...,AWas 
in (7) forj=l,2,. ... k 1, where ~,(. ) is the r-th elementary sym-and (8). We then run @ with N = A6 
~d w => + 1, for metric function in the indeterminate X = Xl, X2, . . . . xk. 15=1,2 ,. ... w. It follows 
from Lemma 1 that if @ returns Yes in at least one of these queries, then the answer to the question 
of FINITE-FIELD SUBSETSUM is (Yes, otherwise the answer is [No. It is also easy to see that in each of 
the w queries, the length of the input to MDz~ is bounded by a polynomial in the length of the input 
to FINITE-FIELDSUBSETSUM. If the in­putcll, a2, ..., G and @ to FINITE-FIELDSUBSETSUMtakes m(n+l) = O(n2) 
bits, then the number of bits required to specify each matrix A6 is O(rz3), and the number of bits required 
to specify all of them is at most 0(n4 ). Further­more, each of these matrices can be obviously constructed 
in polynomial time from al, az, . . . . ci~ and /3, using oper­ations in IFVI. The only thing that is 
not entirely obvious is that IFzrnitself, namely an irreducible polynomial g(z) of degree m that defines 
IFz~, can be constructed in deter­ministic polynomial time. However, Shoup [75] provides a deterministic 
algorithm for this purpose, whose complexity is strictly less than 0(rn5 ) operations in IFz. I The procedure 
used in the proof of Proposition 2 is called Turing reduction in Garey and Johnson [39]. It uses a polynomial 
number (namely w, in our case) of queries to an oracle @ for MDWI. This shows that MDzm is NP­hard, but 
not necessarily NP-complete, at leaat according to how this terminology is used in Garey and Johnson 
[39]. There are at least two alternative ways to establish the NP-completeness of MDWI. One way is to 
reduce directly from 3-DIMENSIONALMATCHING. The key observation here is that the reduction from 3-DIMENSIONALMATCHINGto 
MAXIMU?..I-LIKELIHOOD DECODINGin Berlekamp, McEliece, and van Tllborg [12] holds without change if we 
replace the phrase of weight ~ w with the phrase of weight ex­actly w in the question of MAXIMUM-LIKELIHOODDECOD-ING. 
This eliminates the need for multiple queries to @ in the proof of Proposition 2, and establishes a polynomial 
trans­formation from 3-DIMENSIONALMATCHINGto MDVI. How­ever, we find some intrinsic merit in reducing 
to MDz~, and hence also to MINIMUM DISTANCE, from MAXIMUM-LIKELIHOODDECODING rather than from 3-DIMENSIONAL 
MATCHING. Therefore, in what follows, we describe a sim­ple construction which shows that a single query 
to @ would suffice to solve FINITE-FIELD SUBSETSUM, and hence also MAXIMUM-LIKELIHOODDECODING. As before, 
given al, cm,. . . . an, ~, and w, we first construct the matrices Al, Az,. ... Aw, given by (7) and 
(8), which define thecodes Cl, Cz,..., G. Next, forb = 1,2,..., w, we let ~6 denote the linear code obtained 
by repeating each codeword of ~ exactly 16times. Thus a parity-check matrix for CTdis given by A6 I Zn+l 
In+l H; = l.+l In+l (lo) . In+l In+l I mension is n 6, and its minimum distance is dj = l~d~ which is 
equal to either lJ(J + 1) or la(6 + 2) by Lemma 1. The integers 11,12,. ... lW are defined, recursively, 
as follows: w(w+l)_l lW = 2+3+.. +w = 2 (11) and for d = w l, w 2,. ..,1 (12) = r6+$:~2)l Finally, we 
define the code @ over lFWI as the direct sum of the codes C?l,~, ..., ~. A parity-check matrix for @ 
is given by H; Hi H# = (13) . [ .1 1 where H{j H~, ..., H; are given by (10), and blanks again denote 
zeros. That is, a typical codeword of@ is obtained by independently selecting one codeword each from 
the codes CI, G,..., ~, and then concatenating them all together. Clearly, the length of@ is n#= (11+12+ 
. ..+lw)(n+l) its dimension is (n 1) +.. ~+ (n w) = O(n2), and its min­imum dktance is given by dx 
= rein{ fldl, 12d2,. . ., lwdw}. We now show that the number of bits required to specify H# is bounded 
by a polynomial in n. It is easy to see from (12) that ll>lz >.. > lU, and therefore Using the relation 
(J+1)16 < (3+2)lt+l +(6+1) which follows from (12), it can be readily verified by (reverse) induction 
that the following holds for all 6 = w l, w 2,. ..,1 (6+1)13 < (w+l)lw +(d+l)+(c$+2)+. +w Substituting 
6 = 1 in this expression yields 211 < (W+l)lW +(2+.. + w) = (w+ 2)/w < W3 (14) where the last two inequalities 
follow from (11 ). Hence, we have n# < lln2 < w3n2 = 0(n5), and the number of bits required t: specif~ 
H# isatmostn#(n# k#)m=O(nll ). Since the expressions in (10) and (13) are straightforward, this argument 
is all we need to prove that H# can be con­structed from the input al, az, ..., an, p, and w to FINITE-FIELD 
SUBSETSUM in polynomial time. We are now ready to prove that FINITE-FIELDSUBSETSUM can be solved with 
a single query to an oracle for MDz-. Theorem 3. MDv is NP-compiete. Proof. Clearly, MDv is in NP, since 
given a putative so­lution z, we can verify Hzt = O and wt(z) < win polynomial time. We exhibit a polynomial 
transformation from FINITE-FIELDSUBSETSUMto MDWI as follows. Given the input to where l.+l is the (n+l) 
x (n+l) identity matrix and blanks FINITE-FIELDSUBSETSUM,we construct in polynomial time denote zeros. 
Clearly, the length of ~J is 1~(n + 1), its di-the matrix H# in (13), and then run the oracle @ for MD2m 
with ?-f = H# and u = 211. By the definition of the integers ll,lz,. ..,lU in (12), we have (c$+ 1)16 
~ (d+ 2)1J+I for all J = 1,2,. . . ,w 1. This implies 211 ~ 312 ~ . . Z (~+l)lw (15) 311 ~ 412 > . . 
~ (W+2)1W (16) Now, suppose that the answer to the question of FINITE-FIELD SUBSETSUM is Yes. Then it 
follows from Lemma 1 that da = d + 1 for at least one rS= 1,2,..., w. Therefore dx = min{lldl,lzdzl . 
. ..l~d~} < max{21~,31z,. ... (w+ l)lW} (17) = 211 in view of (15), and @ will necessarily return Yes. 
On the other hand, suppose that the answer to the question of FINITE-FIELD SUBSET SUM is No. Then, by 
Lemma 1, we have d8=6+2for a116=l,2,..., w, and dx = min{ildl,. ..,l~d~} = min{311,. ... (w+2)lW} (18) 
= (w+ 2)1W >21, where the third equality follows from (16), and the last in­equality is precisely (14). 
Hence, in this case, @ will neces­sarily return No. I Obviously, the NP-completeness of MDWI is a weaker 
result than the NP-completeness of MINIMUMDISTANCE,since the set of inputs to MINIMUMDISTANCEis a special 
case of the set of inputs to MDY-. However, Theorem 3 is a useful step­ping stone in the proof of the 
NP-completeness of MINIMUM DISTANCE,which is the subject of the next subsection. 3.3. NP-completenessfor 
binarycodes Given the transformation from FINITE-FIELD SUBSETSUM to MDzm in Theorem 3, the NP-completeness 
of MINIMUM DISTANCEwould follow if we could map, in polynomial time, the code @ constructed in (13) onto 
a binary linear code C, in such a way that the minimum distance d# of @ can be determined from the minimum 
distance d of C. A mapping of this kind is exhibited in this section. Certain simple mappings from codes 
over IFv to binary codes are well-known [59, pp.207 209]; however none of these mappings is adequate 
for our purposes. For example, we could let C be the binary subfield subcode of @, as is commonly done 
in obtaining BCH codes from Reed-Solomon codes (see the discussion in Section 2.1). In this cased ~ d#. 
Alternatively, one could let C be the trace code (cf. [59, p.208]) of @, in which case d < dx. Yet another 
option is to represent each element of fl?zmas a binary rn-tuple (cf. [59, p.298]), using a fixed basis 
for lF2m over IFz. In this case, we again have d ~ dx. All these mappings establish bounds on d#. Furthermore, 
it can be shown that these bounds are reasonably tight [59]. However, such mappings are not suf­ficient 
to determine the value of d# exactly, which is what we need in the present context. Instead, we will 
employ a concatenated coding scheme [23, 31], using I@ as the outer code. We let 0 denote the (n*, k*, 
d*) binary linear code used as the inner code in the concatenation: namely, we require that k = m and 
repre­sent each element of IFzmby a codeword of C . Specifically, fix a basis /31,~2,. . . . /3m for 
IFw1 over IF2 and a generator matrix G* for C. Then a one-to-one mapping q : IF.w ~ U7 takes each element 
~ = bl ~1 + bz~z +.. ~+ b~@~ of 11% into w(~) = (bl, bz, . . .. bm)G* (19) which is a binary n -tuple. 
When this mapping is applied to@, the result is a binary linear code C of length n n# and dimension mkx. 
If @ = [~i,j] is the TX x n# parity-check matrix for @ in (13), and the code C is systematic (as we may 
assume without loss of generality), then a parity-check matrix for C is given by P 1 P* I . P* I H= (20) 
... [? 1,11 [W] [-fl,?#l 1-1 1: ... [-m] [-ml [72,n#l ... [7r#,J [ Yr#,21 [-rr#,n#r where blanks denote 
zeros, If = [P* 11] is a parity-check matrix for C in a systematic form, and [-y] standa for the m x 
m companion matrix (cf. [59, p.106]) of -yc lF.z~ with respect to the baais /31,~Z, ..., ~~. Henceforth, 
we let d denote the minimum distance of the code C defined by (20). The following lemma provides an upper 
bound on d, in terms of d#, n*, and k = m. Lemma 4. d ~ ~*d# 2m 1 Zm 1 Proof. Since @ is a linear code 
over IFIYII,if it contains a codeword c of weight d#, then it contains 2m 1 such codewords, namely all 
the multiples of c by the nonzero ele­mentsoflFzm. Letcl, cz, ..., CWI I E@ denote these 2m 1 codewords, 
and consider the (2m 1) x n# matrix M having C1,C2,..., c2m_1asitsrows. Itisobviousthateachofthe d# 
non-zero columns of M contains each of the 2m 1 non-zero elements of IPv exactly once. Now let c{, c;, 
. . . ,cjm-l Ec be the images of c1, cz, . . . , C2WI 1 under the mapping p(.) in (19), and consider 
the (2m 1) x n nx matrix M having C;, cj, . . . ~c~m 1 as its rows-If some n* columns of M corre­spond 
to a non-zero position of c E @, then every non-zero codeword of C appears exactly once in these n columns. 
It follows that the weight of each non-zero column of M is precisely 2m-1, and there are at most n*d# 
such columns. Thus the total weight of M is at most n*d#2 -1. The lemma now follows by observing that 
M has 2m 1 rows. 1 We note that Lemma 4 is just a variation of the well-known Plotkin bound [59, p.41]. 
Yet, it provides exactly the kind of instrument we need for our purposes. Indeedj suppose that d# <21, 
as in (17), where 11 is defined by (11) and (12). The; Lemma 4 implies that yn 1 d < 211n* (21) Zm 1 
 On the other hand, suppose that d# ~ 211+ 1asin (18). Then, since d ~ d*d# by construction, we obviously 
have (22) In the present context, one is more interested in the vice­versa interpretation of the bounds 
in (21) and (22). Namely, given d (say, by an oracle for MINIMUMDISTANCE),we would like to distinguish 
between the two possibilities for d#. For­tunately, if 2m-1 211 d*>n  (23) 2/1+12~ l then the right-hand 
side of (22) is strictly greater than the right-hand side of (21). Thus our goal can be achieved, pro­vided 
the minimum distance of V is sufficiently large. We now observe that 211< U13in view of (14), and w < 
m 1 as discussed in Section 3.1. Thus, in order to satisfy (23), it would certainly suffice to require 
that * ~ Zm-l L> m -= 0.5 ~1 2(2m 1) (24) 2m - 3 m3 Zm 1 n These considerations may be translated 
into a specific set of conditions relating to the code C used as the inner code in our construction: 
PI. The length of C is bounded by a polynomial in n, and a generator matrix for C can be constructed 
in polynomial time. P2. The dimension of C is at least m (if dim ~ is strictly greater than m, then any 
subcode of CY will suffice for our purposes). P3. The ratio of the minimum distance of C to its length 
satisfies (24). Less formally, what we need is a sequence of binary lin­ear codes, whose relative distance 
approaches the Plotkin bound d*/n* ~ 0.5, and whose rate tends to zero only polynomially fast aa function 
of their dimension. Further­more, we should be able to construct each code in the se­quence in polynomial 
time. This rules out codes that at­tain the Gilbert-Varshamov bound [59, p.557], as well as Zyablov codes 
[94], since the complexity of Zyablov s con­struction [94] becomes exponential at low rates. Neverthe­less, 
many other known constructions of asymptotically good families of low-rate codes have the desired properties. 
For example, all the polynomial-t ime constructions discussed in Section 2.1, with the exception of Justesen 
[46] concatena­tion, would suffice for our purposes. In what follows, how­ever, we shall use a simple 
construction due to Noga Alon [2], which is concise enough to be described in one paragraph. Alon s construction: 
Given an integer v ~ 2 and a non-negative integer s ~ 2V 2, consider a con­ catenation of the (2 , s 
+ 1, 2 s) Reed-Solomon 104 code over IFz. with the (2 1, u, 2 1) binary sim­plex code [59, p.30]. The 
result is a binary linear code C (v,s) with the following parameters n = 2 (2 1) (25) k = U(S+ 1) 
(26) = 2 -1(2 s) d (27) Noga Alon [2] observes that a generator matrix G* for C (v,s) maybe specified 
directly as follows. The columns of this matrix are indexed by pairs (z, y), where z, y c IF w and y 
# O, while its rows are in­dexed by integer pairs (i, j), where i = 0,1,... ,s andj=l,2, . . ..u. Letal, 
as, . . ..a beabaais for IF2. over IF2. Then the entry in row (i, j) and column (z, y) is defined as 
(~j~i, y), where ~jxi is computed in IFw, and (., .) denotes the inner product of ~jx and y as binary 
v-tuples. We takes = m and u = [5 log2 ml in the foregoing construc­tion. Then C = C (v,s) trivially 
satisfies property P2, since k = v(nz + 1) ~ m. Furthermore 2U(2V_ 1) < 22t5}0g2~+lJ = 4m1° = O(nl ) 
n = so that C also satisfies property PI. Thus the length n n# of the concatenated code C is upper bounded 
by n nx < l/.7n10(m-l)4(n+l) = 0(n15)  and the matrix H in (20) can be specified in polynomial time 
using at most 0(n30) bits. Now, it follows from (25) and (27), along with our choice of s and v, that 
d _ 2 -1(2 s) ~ 2 (2 1) > o.5 &#38; >0.5 $ 1 2m m3 ~ 0.5 ~ Z( p 1) where the last inequality holds 
for all m > 10 (and follows straightforwardly from the fact that 2m ~ m3 + mz + m + 1 for such m). Thus 
C also satisfies property P3. With both &#38; and (7 at hand, we are ready to prove our main result. 
Theorem 5. MINIMUMDISTANCEis NP-compIete, Proof. Clearly, MINIMUMDISTANCEis in NP. A poly­nomial transformation 
from FINITE-FIELD SUBSET SUM to MINIMUMDISTANCEcan be described as follows. Given the input al, crz,..., 
cr~,~ EIFz-and w to FINITE-FIELDSUB-SET SUM, we answer the question of FINITE-FIELD SUB-SET SUM by exhaustive 
search if m < 10. Otherwise, we construct in polynomial time the matrix H in (20), with H* = [F II ] 
specified by Alon s construction. We then query an oracle for MINIMUMDISTANCEfor the existence of a codeword 
of weight at most Zll ~. 2m 1 = 1,2 (2 l)&#38; Zm 1 where 11is defined by (11) and (12), and v = [5 
logz ml. By the foregoing discussion, the oracle for MINIMUMDISTANCE will return Yes if and only if the 
answer to the question of FINITE-FIELDSUBSETSUMis Yes. I This concludes the proof of the conjecture of 
Berlekamp, McEliece, and van Tilborg [12]. In the next section, we dis­cuss certain extensions and consequences 
of this result.  4. Further results and concluding remarks We note that our proof of Theorem 5 can 
be immediately extended to codes over an arbitrary, fixed, finite field IFq. This is based on the observation 
[9] that the transformation from 3-DIMENSIONALMATCHINGto MAXIMUM-LIKELIHOOD DECODING in [12] holds without 
change if the input to MAXIMUM-LIKELIHOODDECODINGis an m x n matrix H over IFQ,rather than a binary matrix. 
Given the NP-comp-Ieteness of MAXIMUM-LIKELIHOODDECODINGover IFg, one can essentially go through the 
proof in the previous section, replacing each instance of 2 by q. There are a few intricate points along 
the way, that require some explanation. First, in rephrasing MAXIMUM-LIKELIHOODDECODINGas FINITE-FIELD 
SUBSET SUM, one should leave the expres­sion ~i~ + Ct i~+ ... + ai~ =/3 in the question of FINITE-FIELD 
SUBSET SUM as is, rather than ask whether @ is a linear combination of cql, ~i2, ....ai$. This is certainly 
not the question that one would be concerned with for de­coding purposes, but it is legitimate in an 
NP-completeness proof given the specific transformation horn 3-DIMENSIONAL MATCHINGto MAXIMUM-LIKELIHOODDECODINGin 
[12]. (It is easy to see that a vector z 6 IFq of weight s m/3 satisfies Hzf = (11 . . . I)t for the 
m x n incidence matrix H con­structed in [12] only if all the m/3 nonzero positions in z are equal to 
1.) Secondly, the bound in Lemma4 becomes m 1 qd < n*d# m l+qm 2. ..+q+l 9 and one has to modify equation 
(24) accordingly. Fortu­nately, Alon s construction [2] works in this case as well. Here, the columns 
of G* would be indexed by x, y E lF~., so that equation (26) remains without change, equation (25) becomes 
n = q (q 1), and equation (27) becomes d* > (q I)q -l(q S) (28) The key observation in the proof of 
(28) is as follows: if <, y c IFg. and ~ # O, then as y ranges over all the elements of IFQU,the inner 
product (~, y) takes each value in lF~ ex­actly q 1 times. (Alternatively, this can be viewed as a concatenation 
of the (q , s + 1, q s) Reed-Solomon code over lF~uwith the (q 1, v, (q l)qv L) first-order general­ized 
Reed-Muller code over lF~,see [11, p.362]. ) To complete the proof, one can again take s = m and v = 
[5 log~ ml in this construction. The complexity of approximation algorithms for NP-hard problems haa 
been extensively investigated recently [41], and it is natural to ask whether approximating the minimum 
dk­tance of a linear code is still hard. Since our proof of the NP-completeness of MINIMUMDISTANCEis 
based on a trans­formation from MAXIMUM-LIKELIHOODDECODINGand it is known [5, 78] that MAXIMUM-LIKELIHOODDECODINGre­mains 
RTP-complete under approximation within a constant factor, it is plausible that the same should be true 
for M] N-IMUMDISTANCE.We leave a more rigorous investigation of this question as an open problem. Another 
immediate consequence of our proof is that certain useful computational tasks in coding theory are NP-hard, 
aa there is an easy reduction from MINIMUMDISTANCEto each of these tasks. There is a large number of 
computational problems of this kind; we will give just three examples here. First, we note that determining 
whether a given linear code is MDS is NP-hard: namely, the following decision problem Problem: MDS CODE 
Instance: A prime p z 2, positive integers m and r, andan Txprn matrix H over lFP_. Question: Is there 
a nonzero vector x of length pm over lFPm,such that Hxt = O and wt(x) < T? is NP-complete. The fact that 
MDS CODE is NP-hard, even for p= 2, follows immediately from Lemma 1. The NP­completeness of MDS CODE 
then follows from the obser­vation that the phrase of weight < w in the question of MAXIMUM-LIKELIHOODDECODINGcan 
be changed to the phrase of weight exactly w, as discussed in Section 3.2. As another example, consider 
the problem of determining the trellis complexity of a linear code. More precisely, the computational 
task is to find a coordinate permutation that minimizes the number of vertices at a given time z in the 
minimal trellis for a binary linear code. For more details on this problem, and on trellises in general, 
see Section 2.2. It is not difficult to see that the corresponding decision prob­lem [43] may be posed 
as follows: Problem: PARTITIONRANK Instance: A binary k x n matrix H, and positive in­tegers i and w. 
Question: Is there a column permutation that takes H into a matrix H = [Ai 113m_a],such that Aa is an 
k x i matrix and rarrk(Ai) + rank(Bn-i) < w? Horn and Kschischang [43] recently proved that this problem 
is NP-complete, using an ingenious and elaborate transfor­mation from SIMPLEMAX CUT [39, p.210] which 
spans over five pages. On the other hand, given the NP-completeness of MINIMUMDISTANCE,this result can 
be established in a few lines as follows. First, we observe that the least integer i for which rank(Ai) 
+ rank(Bn_i) < rank(H) + z is equal to min{d, d~ }, where d, d~ denote, respectively, the distance and 
the dual distance of the code defined by H. Notice that it does not matter whether H is viewed as a parity-check 
or as a generator matrix in this problem, Now, suppose that C is an (n, k, d) binary linear code whose 
mini­mum distance we would like to determine, and let d~ denote the dual distance of C. Given C, we 
first construct a binary linear Reed-Muller code ~ of length 2m and order r, where ~= +1~d T= [10g2~].~ 
is an (n , k , d ) 2[log2~]Then self-dual code, where 22(1OK2nl+l < 8n2 n = k = n /2 ~4n2 Zrlwz ~1+1 
> Zn d = 2m- = We then use the well-known Kronecker product construc­tion [59, p. 568] to obtain a generator 
matrix for the product code G = CL @ ~, where CL is the dual code of C. Evi­dently, the length of C7 
is n* = nn < 8n3, and its minimum distance is d*=dAd ~2ndL~n>d On the other hand, it is easy to see that 
the dual distance of C is the minimum of the dual distances of Cl and ~, namely min{d, d } = d. Hence, 
running a polynomial-time algorithm for PARTITION RANK with the input H being a generator matrix for 
C , we can determine d in polynomial time. The foregoing firing reduction from MINIMUMD]s-TANCEshows 
that, given a linear code C, computing either the minimum dktance d or the minimum dual distance d~ is 
NP-hard. This furthermore proves that PARTITIONRANK remains NP-hard, even if the input is restricted 
to w = rank(H) +i 1 In other words, even if all we would like to know is whether there exists a permutation 
of a linear code ~ such that ]Og2lU1# z in the minimal trellis T = (V, E, F2) for C, the computational 
task of determining this is still NP-hard. This is a somewhat stronger result than the one reported in 
[43]. Moreover, we believe that the NP-completeness of MINIMUMDISTANCEcan be now used to show that deter­mining 
the mtimum trellis complexity of a code, namely s = maxi logz IVaI as defined in Section 2.2, is also 
NP-hard. As a third example, we mention the problem of finding the largest subcode with a prescribed 
contraction index [88]. Namely, given a k x n generator matrix for a binary linear code C, and a positive 
integer A, we wish to find the largest subcode ~ ~ C which has a generator matrix with at most A + dlm 
C distinct columns. This problem is of importance in soft-decision and majority-logic decoding (see [88] 
for an extensive treatment ), and it is possible to show that it is NP-hard using a transformation from 
MINIMUMDISTANCE. The proof of this is a bit tedious, and we omit the details. Finally, we would like 
to mention two important problems in coding theory, for which we do not have a polynomial transformation 
from MINIMUMDISTANCE,but believe that it should be possible to find one. The first problem is that of 
bounded-distance decoding of binary linear codes. While the intractability of maximum­likelihood decoding 
has been thoroughly studied (see, for ex­ample [5, 9, 12, 16, 20, 78], and the discussion in Section 
2.2), most of the decoders used in practice are bounded-distance decoders. It is still not known whether 
bounded-distance decoding is NP-hard for the general class of binary linear codes. For bounded-distance 
decoding that achieves the error-correction radius of the code, as defined in Section 2.2, the corresponding 
decision problem can be formulated as Problem: BOUNDED-DISTANCEDECODING Instance: A positive integer 
d, a binary m x n ma­trix H, such that every set of d 1 columns of H is linearly independent, a vector 
s E lF2~, and a positive integer w s l(d 1)/2]. Question: Is there a vector z E IFn of weight ~ w, such 
that Hxt = s? Notice that BOUNDED-DISTANCEDECODINGis not likely to be in NP, since in view of our main 
result in this paper, verifying that every d 1 columns of H are linearly inde pendent is NP-hard. Thus 
BOUNDED-DISTANCE DECODING is an example of a promise problem (cf. [24]). Neverthe­less, we could ask 
whether BOUNDED-DISTANCEDECODING is NP-hard. We concur with the remark of Barg [9], and conjecture that 
this is so. Furthermore, we believe that the NP-completeness of MINIMUMDISTANCEshould be instru­mental 
in trying to prove this conjecture. We note that a hardness result in a somewhat different context was 
recently established in [20]; see the discussion in Section 2.2. How­ever, the problem as posed above, 
is still open. The second problem we would like to mention is that of find­ing the shortest vector (in 
the Euclidean norm) in a sublat­tice of 2 . As already observed by Johnson in [45], the over­all status 
of computational problems for lattices is remark­ably similar to the situation with linear codes. P. 
van Erode Boas [85] proved in 1980 that finding the nearest vector (which is equivalent to maximum-likelihood 
decoding) in a sublattice of 23 is NP-hard, and conjectured that finding the shortest vector should be 
hard aa well. Formally, van Erode Boas [85] conjectured that the following problem: Problem: SHORTESTVECTOR 
Instance: A basis VI, VZ,. ... v~ E Z for a lattice A, and an integer w >0 Question: Is there a nonzero 
vector z in A, such that IIz112< VJ? is NP-complete. In spite of a considerable amount of work (see [5] 
for a recent survey), the proof of this conjecture re­mains elusive. Arora, Babai, Stern, and Sweedyk 
[5] classify this as a major open problem. Moreover, this conjecture becomes particularly significant 
in view of the celebrated recent result of Ajt ai [1], who showed that hard instantes of the SHORTESTVECTOR 
problem can be efficiently gen­erated, provided that the problem itself is hard. Thus the NP-completeness 
of SHORTESTVECTOR is of utmost impor­tance in cryptographic applications. We notice that if the phrase 
]IxI12< w in the question of SHORTESTVECTOR is replaced with IIzI12= w, the prob­lem becomes NP-complete. 
This result, which was recently established by Fellows and Vardy [29], shows that the coun­terpart of 
the WEIGHT DISTRIBUTIONproblem, which was proved to be NP-complete for binary linear codes in [12], is 
NP-complete for lattices. It is, therefore, all the more plau­sible that the counterpart of MINIMUMDISTANCEshould 
be also NP-complete for lattices. Intuitively, finding the shortest vector in a lattice is at least 
as difficult as finding the minimum-weight vector in a bi­nary linear code. Thus it is reasonable to 
suggest that there would be a polynomial transformation from MINIMUMDIS-TANCEto the SHORTESTVECTOR. Specifically, 
we pose the following problem: given a binary linear code C construct, in polynomial time, a lattice 
A ~ Z so that the minimum dktance of C can be determined from the minimum norm of A. In view of our main 
result, solving this problem would amount to proving that SHORTESTVECTOR is NP-complete. Acknowledgement. 
I would like to acknowledge helpful dkicusaions with Noga Alon, Alexander Barg, Yoram Bresler, Shuki 
Bruck, Ilya Dumer, Herbert Edelsbrunner, Mike Fel­lows, Nabil Kahale, Moni Naor, Ronny Roth, Dilip Sarwate, 
Leonard Schulman, and Vijay Vazirani. I am especially in­debted to Noga Alon for referring me to his 
construction, which is used in Section 3. Finally, I would like to thank Hagit Itzkowitz for her invaluable 
help.  References <RefA>p] M. AJTAI, Generating hard instances of lattice prob­lems, in Proc. 28-th Annual 
ACM Symp. Theory of Computing, pp. 99-108, Philadelphia, PA, May 1996. [2] N. ALON, Packings with large 
minimum kissing num­bers, preprint, 1996. [31 N. ALON, J. BRUCK, J. NAOR, M. NAOR, and R. ROTH, Construction 
of asymptotically good low-rate error­correcting codes through pseudo-random graphs, IEEE Trans. Inform. 
Theory, vol. 38, pp. 509-516, 1992. [4] S. ARORA, Probabilistic checking of proofs and hard­ness of approximation 
problems, Ph.D. thesis, Univer­sity of California, Berkeley, CA., 1994. [5] S. ARORA, L. BABA1, J. STERN, 
and Z. SWEEDYK,The hardness of approximate optima in lattices, codes, and systems of linear equations, 
in Proc. 34-th AnnuaJ Symp. Found. Computer Science, pp. 724-733, Palo Alto, CA, 1993. [6] S. ARORA and 
C. LUND, Hardness of approximations, in Approximation Algorithms for NP-Hard Problems, D.S. Hochbaum 
(Ed.), Boston: PWS Publishing Co., pp. 399-446, 1997. {7] L.R. BAHL, J. COCKE, F. JELtNEK,and J. RAVtV, 
Opti­mal decodhg of linear codes for minimizing symbol er­ror rate, IEEE Trans. Inform. Theory, vol. 
20, pp. 284 287, 1974. [8] R. BALASUBRAMANIAN,M. FELLOWS, and V. RAMAN, An improved fixed-parameter algorithm 
for vertex co­ver, to appear, 1997. [9] A. BARG, Some new NP-complete coding problems, Prob~. Peredachi 
Informatsii, vol. 30, pp. 23-28, 1994, (in Russian). [10] A. BARG, Complexity issues in coding, survey 
chapter to appear in Handbook of Coding Theo~, R.A. Brualdi, C. Huffman, and V. Pless (Ed.), Amsterdam: 
Elsevier. 107 [11]E.R. BERLEKAMP,Algebmic Coding Theory, New York: McGraw-Hill, 1968. [12] E.R. BERLEKAMP, 
R.J. MCELIECE, and H.C. A. VAN TILBORG,On the inherent intractability of certain cod­ing problems, IEEE 
!lkans. Inform. Theory, vol. 24, pp. 384-386, 1978. [13] C. BERROU,A. GLAVIEUX,and P. THITIMAJSH]MA,N­ar 
Shannon limit error-correcting coding and decoding: turbo codes, in Proc. IEEE Int. C onf. on Communica­tions, 
pp. 1064-1070, Geneva, Switzerland, 1993. [14] M. BLAUM, J. BRUCK, and A. VARDY, On MDS codes and alterants 
over certain rings, in Proc. 900-th Meet­ing, American Math. Sot., Chicago, IL., March 1995. [15] M. 
BLAUM,J. BRUCK,and A. VAR~Y,MDS array codes with independent parity symbols, IEEE Trans. Inform. Theory, 
vol. 42, pp. 529-542, 1996. [16] J. BRUCKand M. NAOR, The hardness of decoding lin­ear codes with preprocessing, 
IEEE Ihns. Inform. The­ory, vol. 36, pp. 381 385, 1990. [17] G.C. CLARK and J.B. CAIN, Error-Correction 
Coding for Digital Communications, New York: Plenum Press, 1981. [18] R.G. DOWNEY and M.R. FELLOWS, Fixed 
parameter tractability and completeness I: b~ic theory, SIAM J. Comput., vol. 24, pp. 873-921, 1995. 
[19] R.G. DOWNEY and M.R. FELLOWS, Fixed parameter tractability and completeness II: completeness for 
W[l], Theoret. Computer Sci. A vol. 141 pp. 109-131, 1995. [20] R.G. DOWNEY,M. R. FELLOWS,A. VARDY, and 
G. WHI-TTLE, On the parametrized complexity of certain fun­damental problems in coding theory, preprint 
in prepa­ration, 1997. [21] P. DIACONK and R.L. GRAHAM, The Radon transform on 73$,Pacific J. Math., 
vol. 118, pp. 176 185, 1985. [22] 1.1. DUMER, On complexity of maximum-likelihood de­coding of the best 
concatenated codes, in Proc. 8-th AJ1-Union Conf. on Coding Theory and Information The­ory, Moscow-Kuibishev, 
pp. 66-69, 1981, (in Russian). [23] 1.1. DUMER, Concatenated codes and their general­izations, to appear 
in Handbook of Coding Theory, R.A. Brualdi, W.C. Huffman, V. Plesa (Ed.), Amster­dam: Elaevier. [24] 
S. EVEN and Y. YACOBI, Cryptography and NP-comp­leteness, Lect. Notes Comp. Science, vol. 85, pp. 195­207, 
Springer-Verlag, 1982. [25] J. FANG, G.D. COHEN, PH. GODLEWSKI, and G. BAT-TAIL, On the inherent intractability 
of soft decision decoding of linear codes, Lect. Notea Comp. Science, vol. 311, pp. 141 149, Springer-Verlag, 
1988. [26] R.M. FANO, A heuristic discussion of probabilistic de­coding, IEEE Tkans. Inform. Theory, 
vol. 9, pp. 64 73, 1963. [27] J. FEIGENBAUM,The use of codkig theory in com­putational complexity, in 
Proc. Symp. Appl. Math, A.R. Cafderbank (Ed.), Providence, RI: AMS Press, pp. 203-229, 1995. [28] J, 
FmGmw3Auhi, G.D. FORNEY, B. MARCUS, R.J. Mc ELIECE, and A. VARDY, Special issue on Codes and Complexity, 
IEEE Trans. Inform. Theory, vol. 42, November 1996, [29] M.R. FELLOWSand A. VARDY, The hardness of theta 
series in lattices, preprint in preparation, 1997. [30] G.-L. FENG and K.K. TZENG, A new procedure for 
de­coding cyclic and BCH codes up to actual minimum dis­tance, IEEE Trans. Inform. Theory, vol. 40, pp. 
1364 1374, 1994. [31] G .D. FORNEY,JR., Concatenated Codes, Cambridge, MA: M.I.T. Press, 1966. [32] G.D. 
FORNEY,JR., The Viterbi algorithm, Proc. IEEE, VO].61, pp. 268 278, 1973. [33] G.D. FORNEY,JR., Convolutional 
codes III: sequential decoding, Inform. Control, vol. 25, pp. 267-297, 1974. [34] G.D. FORNEY,JR., Coset 
codes II: Binary lattices and related codes, IEEE Ikms. Inform. Theory, vol. 34, pp. 1152 1187, 1988. 
[35] G.D. FORNEY,JR., The forward-backward algorithm, in Proc. 34-th Allerton Conference on Comm., Con­trol, 
and Computing, Monticello, IL., pp. 432-446, Oc­tober 1996. [36] G.D. FORNEY,JR. and A. VARDY, Generalized 
mini­mum distance decoding of Euclidean-space codes and lattices, IEEE Trans. Inform. Theory, vol. 42, 
pp. 1992­2026, 1996. [37] B.J. FREY and F.R. KSCHtSCHANG,Probability propa­gation and iterative decoding, 
in Proc. 34-th Allerton Conference on Comm., Control, and Computing, Mon­ticello, IL., pp. 482 493, October 
1996. [38] R.G. GALLAGER,Low Density Parity-Check Codes, Ca­mbridge: M.I.T. Pressl 1962. [39] M.R. GAREY 
and D.S. JOHNSON,Computers and htra­ctability: A Guide to the Theoy of NP-Completeness, San l+ancisco: 
Freeman, 1979. [40] C.R.P. HARTMANand K.K. TZENG, Decoding beyond the BC!H bound using multiple sets 
of syndrome se­quences, IEEE Tkns. Inform. Theory, vol. 20, pp. 292 295, 1974. [41] D.S. HOCHBAUM,(Editor), 
Approximation Algorithms for NP-Hard Problems, Boston: PWS Publishing Co., 1997. [42] T. H@HOLDTand R. 
PELLIKAAN,On the decoding of algebraic-geometric codes, IEEE Tkans. Inform. The­ory, vol. 41, pp. 1589 
1614, 1995. [43] G.B. HORN and F.R. KSCHISCHANG,On the intractabi­lity of permuting a block code to minimize 
trellis com­plexity, IEEE Zkns. Inform. Theory, vol. 42, pp. 2042 2048, 1996. [44] D.S. JOHNSON,The NP-completeness 
column: An on­going guide, J. Algorithms, vol. 3, pp. 182 195, 1982. [45] D.S. JOHNSON,The NP-completeness 
column: An on­going guide, J. Algorithms, vol. 7, pp. 584-601, 1986. [46] J. JUSTESEN, A class of constructive 
asymptotically good algebraic codes, IEEE Thans. Inform. Theory, vol. 18, pp. 652 656, 1972. [47] R. 
KARP and R. LIPTON, Some connections between nonuniform and uniform complexity classes, in Proc. 12-th 
Annual Symp. Theory of Computing, pp. 302­309, 1980. [48] L. KHACHIYAN,On the complexity of approximating 
ex­tremal determinants in matrices, J. Complexity, vol. 11, pp. 138-153, 1995. [49] F.R. KSCHISCHANGand 
V. SOROKINE, On the trellis structure of block codes, IEEE Tkans. Inform. Theory, vol. 41, pp. 1924 1937, 
1995. [50] B.D. KUDRYASHOVand T.G. ZAKHAROVA,Block codes from convolutional codes, Problemy Peredachi 
Infor­matsii, vol. 25, pp. 98 102, 1989, (in Russian). [51] A. LAFOURCADEand A. VARDY, Asymptotically 
good codes have infinite trellis complexity, IEEE Trans. In­form. Theory, vol. 41, pp. 555-559, 1995. 
[52] A. LAFOURCADEand A. VARDY, Lower bounds on trel­lis complexity of block codes, IEEE 2hns. Inform. 
The­ory, vol. 41, pp. 1938 1954, 1995. [53] L.A. LEWN, Average case completeness, SIAM J. Com­put., vol. 
15, pp. 285-286, 1986. [54] S. LIN and E.J. WELDON, Long BCH codes are bad, Inform. and Control, vol. 
11, pp. 445-451, 1967. [55] A. LOBSTEINand G.D. COHEN, Sur la complexit6 d un prob16me de codage, Theoretical 
Informatics AppL, VO1.21, pp. 25 32, 1987. [56] B. L6PEZ JIMENEZ, Plane models of Drinfeld modu­lar curves, 
Ph.D. thesis, University of Complutense, Madrid, March 1996. [57] D.J.C. MACKAY and R.M. NEAL, Near Shannon 
limit performance of low-density parity-check codes, Elect. Lett., to appear, 1997. [58] D.J.C. MACKAY 
and R.M. NEAL, Good error-corre­cting codes based on very sparse matrices, IEEE Tkans. Inform. Theory, 
submitted for publication, 1996. [59] F.J. MACWILLIAMS and N.J.A. SLOANE,The Theory of Error Correcting 
Codes, Amsterdam: North-Holland, 1977. [60] Yu.I. MANIN and S.G. VL.kDUTS, Linear codes and modular curves, 
J. Soviet Math., vol. 30, pp. 2611 2643, 1985, (in Russian). [61] J.L. MASSEY, Shift-register synthesis 
and BCH decod­ing, IEEE fians. Inform. Theory, vol. 15, pp. 122 127, 1969. [62] J.L. MASSEY, Foundation 
and methods of channel en­codhg, Proc. ht. Conf information Theory and Sys­tems, NTG-Fachberichte, Berlin, 
1978. [63] R.J. MCELIECE, On the BCJR trellis for linear block codes, IEEE Tkans. Inform. Theory, vol. 
42, pp. 1072 1092, 1996. [64] R.J. MCELtECE, E. R. RoDEM]cH, and J.-F. CHENG, The turbo decision algorithm, 
in Proc. 33-rd Allerton Conference on Comm., Control, and Computing, Mon­ticello, IL., pp. 366-379, October 
1995. [65] D.J. MUDER, Minimal trellises for block codes, IEEE Tkans. Inform. Theory, vol. 34, pp. 1049-1053, 
1988. [66] T. MUIR, Tkentise on the Theory of Determinants, New York: Dover, 1960. [67] S.C. NTAFOS and 
S.L. HAKIMI, On the complexity of some coding problems, IEEE Trans. Inform. Theory, vol. 27, pp. 794 
796, 1981. [68] J.K. OMURA, On the Viterbi decoding algorithm, IEEE ?kans. Inform. Theory, vol. 15, pp. 
177-179, 1969. [69] J. PEARL, Probabilistic Reasoning in Intelligent Sys­tems: Networks of Plausible 
Inference, San Mateo, CA: Kaufmann, 1988. [70] R. PELLIKAAN, Asymptotically good sequences of cur­ves 
and codes, in Proc. 34-th Allerton Conference on Comm., Control, and Computing, Monticello, IL., pp. 
276 285, October 1996. [71] 1.S. REED and G. SOLOMON, Polynomial codes over cer­tain finite fields, SIAM 
J. Appl. Math., vol. 8, pp. 300­304, 1960. [72] R.M. ROTH and A. LEMPEL, A construction of non­Reed-Solomon 
type MDS codes, IEEE Tkans. Inform. Theory, vol. 35, pp. 655-657, 1989. [73] C.E. SEIANNON,A mathematical 
theory of communi­cation, Bell Syst. Tech. J., vol. 27, pp. 379-423 and pp. 623-656, 1948. [74] B.-Z. 
SHEN, A Justesen construction of binary con­catenated codes that asymptotically meet the Zyablov bound 
for low rate, IEEE Tkans. Inform. Theory, VOI.39, pp. 239 242, 1993. [75] V. SHOUP,New algorithms for 
finding irreducible poly­nomials over finite fields, Math. Computation, vol. 54, pp. 435-447, 1990. [76] 
M. SIPSERand D.A. SPIELMAN,Expander codes, IEEE Tkans. Inform. Theory, vol. 42, pp. 1710 1722, 1996. 
[77] D.A. SPIELMAN,Linear-time encodable and decodable codes, IEEE Ihms. Inform. Theory, vol. 42, pp. 
1723 1731, 1996. [78] J. STERN, Approximating the number of error locations within a constant ratio is 
NP-complete, Lect. Notes Comp. Science, vol. 673, pp. 325-331, Springer, 1993. [79] M. SUDAN,Efficient 
checking of polynomials and proofs and the hardness of approximation problems, Ph.D. thesis, University 
of California, Berkeley, CA., 1992. [80] M. SUDAN, Decoding of Reed-Solomon codes beyond the error-correction 
bound, J. Complexity, vol. 1, 1997, to appear. [81] R.M. TANNER,A recursive approach to low-complexity 
codes, IEEE Trans. Inform. Theory, vol. 27, pp. 533 547, 1981. [82] A. TRACHTENBERGand A. VARDY, Which 
codes have cycle-free Tanner graphs?, preprint, 1997. [83] M.A. TSFASMAN and S.G. VLADUTS, Algebmic Geome­try 
Codes, Dodrecht: Kluwer Academic, 1991. [84] M.A. TSFASMAN, S.G. VLADUTS, and T. ZINK, Mod­ular curves, 
Shimura curves, and Goppa codes bet­ter than the Varshamov-Gilbert bound, Math. IVach­richten, vol. 104, 
pp. 13 28, 1982. [85] P. VAN EMDE BOAS, Another NP-complete partition problem and the complexity of computing 
short vectors in a lattice, Tech. Report 81-04, Dept. of Mathematics, Univ. of Amsterdam, 1980. [86] 
A. VARDY, Tkellis structure of block codes, to appear in Handbook of Coding Theory, R. Brualdi, W.C. 
Huifman, and V. Pless (Ed.), Amsterdam: Elsevier. [87] A. VARDY and F.R. KSCHISCHANG, Proof of a conjec­ture 
of McEliece regarding the expansion index of the minimal trellis, IEEE Tkans. Inform. Theory, vol. 42, 
pp. 2027-2033, 1996. [88] A. VARIX, J. SNYDERS, and Y. BE ERY, Bounds on the dimension of codes and subcodes 
with prescribed con­traction index, Linear Algebra Appl., vol. 142, pp. 237 261, 1990. [89] S.G. VLADUTS, 
G.L. KATSMAN, and M.A. TSFASMAN, Modular curves and codes with polynomial complex­ity of construction, 
Problemy Peredachi In formatsii, vol. 20, pp. 47 55, 1984, (in Russian). [90] D.J.A. WELSH, Combinatorial 
problems in matroid theory, pp. 291 307 in Combinatorial Mathematics and its Applications, D.J.A. Welsh 
(Ed.), London: Aca­demic Press, 1971. [91] N. WIBERG, Codes and decoding on general graphs, Ph.D. thesis, 
University of Linkoping, Sweden, 1996. [92] N. WIBERG, H.-A. LOELIGER, and R. KOTTER, Codes and iterative 
decoding on general graphs, Euro. Trans. Telecommun., vol. 6, pp. 513-526, 1995. [93] J.K. WOLF, Efficient 
maximum-likelihood decoding of linear block codes using a trellis, IEEE Trans. Inform. Theory, vol. 24, 
pp. 76-80, 1978. [94] V.V. ZYABLOV, An estimate of the complexity of con­structing binary linear concatenated 
codes, Problemy Peredachi Informatsii, vol. 7, pp. 5-13, 1971.</RefA>  
			
