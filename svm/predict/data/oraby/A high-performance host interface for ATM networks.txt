
 A High-Performance Host Interface for ATM Networks C. Brendan S, l kaw (traw@dsl. cis . upenn. edu) 
Jonathan M. Smi/h (jms@dsl. cis. upenn. edu) Distributed Systems Laboratory University of Pennsylvania 
200 South 33rd St., Philadelphia% PA 19104-6389 ABSTRACT The advent of high speed networks has increased 
demands on processor architectures. These architec­tural demands are due to the increase in network bandwidth 
relative to the speeds of processor com­ponents. One impottant component for a balanced system is the 
workstation-to-network ho,u inlerjface. Our solution migrates a carefully selected set of protocol processing 
functions into hardware. It connects an IBM RS/6000 workstation to a Synchronous Optical Network (SONET) 
STS-3c~ line carrying fmed­size Asynchronous Transfer Mode (ATM) cells. The host interface described 
in this paper is highly parallel and a pure hardware solution to maximize performance. The~ is a clean 
separation between the interface functions, such as segmentation and reassembly, and the interfacelhost 
communication. This separation eases porting the interface to other workstation plafforms. We intend 
to use this interface (and its successors) as a component of the AURORA gigabit per second (Gbps) testbed. 
1. Introduction connects the node to the network medium. The intervening layers add functions, such as 
reliable delivery, connection Processor speeds have been increasing rapidly, but network management, 
etc., as required by the application. speeds have been increasing more rapidly. In particular, When implemented, 
the protocol layers need notoffered bandwidths have recently leaped towards a billion observe the separation 
of the logical model, The physicalbits per second. One severe bottleneck for moving data at layer must 
consist of hardware by definition, but the imple­these speeds is the host in~e?jizce. A host interface 
resolves mentor can make hardware versus software implementationmismatches between data on a network 
medium and data decisions for each succeeding layer. usable internally by a comp~ter, e.g. SONET frames 
and pages of main memory. Protocol architectures can be viewed as a slack of Application Layer HI layers. 
The ISO 0S1 mo&#38;l, for example, consists of seven presentation Layerlayers, which are illustrated 
in Figure 1. The highest layer, the Application Layer, is the application s interface Session Layer to 
the network. The Physical Layer is the ha~ware which Transport Layer t STS-n and OC-n refer to multiples 
of a base rate of about 52 ] Network Layer I megabits per second, thus tbe STS-3 bandwidtb is 155 kfbps. 
Permission to copy without fee all or part of this material is granted provided that the copies are not 
made or distributed for direct commercial advantage, the ACM copyright notice and the w title of the 
publication and its date appear, and notice is given that copying is by permission of the Association 
for Computing Figure 1: Illustrationof1S00S1 layers Machinery. To copy otherwise, or to republish, requires 
a fee and/or specific permission. 01991 ACM 0-89791 -444-9 /91/0008 /0317 . ..$1 .50 Software is often 
used when flexibility or tuning are required. As the behavior of a layer becomes better under­stood, 
functionality can be migrated from software to hardware, The benefit is twofold. First, protocol process­ing 
ovexhead is offloaded from the host. This frees the host to address applications workioad, and provides 
concurrent processing. If the computers am high-performance works­tations, and not supercomputers, this 
is a sign~lcant attrac­tion. Secon4 the specialized hardware can often perform functions faster than 
the host, thus increasing the bandwidth available to applications. Typically, host interfaces for high 
speed data con­nections must be placed architecturally close to the pro­cessor memory bus to achieve 
high performance. This is due both to latency and to the delay imposed by multiple stages of processing. 
These stages can be implemented in a variety of ways, as hardware or software. We have con­sistently 
chosen to implement freed decisions in the inter­face hardware and leave unmade decisions to host Sofhvare, 
The result of our research is embodied in our host University of Pennsylvania s Distributed Systems 
Laboratory in Philadelphia, PA Research areas in AURORA include alternative switch tech­nologies (specifically, 
IBM s Paris [4] and Bellcore s Sunshine [8] switches), new protocol designs, paradigms for application 
service interfaces, and demonstration of selected applications. me research methodology for AURORA is 
the actuai development and deployment of a long-haul experimental network, which will provide practi­cal 
proof of concept for the key ideas, and permit the sup­port of applications for actuai users at remote 
sites. The geographical topology of AURORA is illustrated in Figure 2. The logicai topology is illustrated 
in Figure 3. ) 5< interface prototype. It consists of two wire-wrapped Micro Channel cards (which can 
be reduced to one if double­sided surface-mount fabrication techniques are used) and assumes framers 
a connection to an ATM switch through SONET [10]. The host interface performs the following functions: 
1. Physicai Layer Interface 2. Segmentation and Reassembly 3. Virtual Circuit Support 4. Provides 
a degree of buffering for the host  The remainder of the paper is organized as foiiows: Section 2 describes 
the setting for our interface, the AURORA tm.tbcd. Section 3 argues our design philosophy, and Sec­tion 
4 discusses related work. Section 5 provides back­ground information on the IBM RS/6000 Micro Channel 
architecture. Sections 6 and 7 describe the host interface design and comprise the technical body of 
the pape~ 6 discusses the Segmenter/Transmitter, and 7 the Receiver/Reassembler. Section 8 sets forth 
our assump­ tions about host software support for the interface, and Sec­ tion 9 pnxents current status 
and future plans as of early 1991. Section 10 concludes the paper. 2. Setting Wkstn AURORA is an experimental 
wide area networktestbed[11] whose main objective is the exploration and evaluation of technologies. 
The Gbps network wiil link four sites Bellcore s Morristown Research and Engineering Laboratory in Morristown, 
NJ  IBM Research s Computer Science Laboratory in Hawthorne, NY  MIT s Laboratory for Computer Science 
in Cambridge,  >$ n Penn . Figure 2: AURORA Geography c @alVideo  t O;fice I Wkstn ] 3090/600 Central 
Video IBM I office Wkstn OC-12 T 4 1[1 Central Vidw office I Ill Cen Penn office II I Wkstn I Figure 
3: Intended AURORA Logicai Topology As illustrated, only one location (IBM Research) ha! supercomputing 
capability, thus we will not Ix interconnecting supercomputers. Rather, the connections are * supercomputer 
to workstation, remote display to workstation, and workstation to workstation. Our focus on workstation 
technology is driven by two factors: 1. Supammputers are expensive, and networking research does not 
motivate their purchase. 2. Interesting networks are large in both bandwidth and scale. The rarity of 
supercomputers limits the ability of testing scalability with interconnected supemomputers.  Each of 
the three lines fkom sites to central offices represents a logical OC-12. The point of this contlgnration 
is to 6rst build independent Paris and Sunshine networks. These independent netsvorks will be interconnected 
in order to understand interoperability of the technologies. Our host interface is intended for the Sunshine-ATM 
logi­ cal topology. One of the pticipal research interests of the AURORA participants is the use of 
Gbps wide-area com­munications between computers, specilleally the construc­tion of distributed systems. 
These systems are used for manipulating data in processing tasks, and manipulating various types of information 
in distributed multimedia sys­tems. We expect such multimedia systems will place huge demands on both 
network and computer resources. 3. Design Philosophy As with any piece of engineering design, there am 
a number of design goals which are tugging in different and sometimes conflicting directions. Our design 
philoso­phy resulted from an attempt to reconcile conflicts in our goals. We saw our goals ax 1. Developing 
a high-performance interface to support workstation hosts in AURORA. The current host technol­ogy at 
Penn is IBM RS/6000 workstations. 2. Building the interface with extremely constrained time and budget. 
 3. Allowing experiments with portions of the protocol stack e.g., congestion contro-and error correction 
stra­tegies.  The resulting philosophy behind our interface is to provide a common denominator set of 
services which can be used to support a complete range of higher level protocols. By reducing the scope 
of the project to a collection of base services, our effort could be focused towards implementing this 
feature set inexpensively and effkiently. After defi­ningthis common denominator, it was clear that it 
could be implemented entirely in high performance dedicated hardware, which offered several cost and 
performance advantages over a general-purpose microcontroller archi­tecture. This approach, however, 
incurs a cost in reduced host interface flexibility, as programmability is lost By only supporting a 
subset of the services which may be required for a particular appliMion, the host inter­face is not capable 
of completely relieving the host of all protocol operations. We feel that this is reasonable since we 
are able to maintain flexibility in protocol implementa­tion in exchange for some overhead incurred by 
host soflware. Protocol flexibility is important for the following reasons 1. Not all protocols and 
applications are defined yet. 2. Services are extmmly varied. It would be difficult to provide support 
for all possible adaptation layers in dedicated hardware.  4. Related Work Several research pro@cts 
are targeted towards high­pexformance host interfaces. The major difference between these implementations 
is the number of protocol processing functions which the host interface perfoxms. One important focus 
has been interfaces which accelerate transport protocol processing [13]. For example, Kanakia and Cheriton 
s [9] VMP Network Adapter Board serves as a hardware implementation of Chenton s Versatile Mes­sage Transaction 
Protocol (VMTP). Abu-Amara et al. [1], can target any set of protocol layecs (to the degree that they 
can be precisely specitled) with their PSi silicon compiler approach. With this method, the protocol 
is specified using a symbolic programming language, and mask descriptions for fabrication process layers 
are generated as output of the compiler. These masks are then used to create custom hardware. The Nectar 
Communications Accelerator Board (CAB) [2] can be programmed with various protocols. The CAB communicates 
with the host memory directly, and the prog rammability can conceivably be used by applications to customize 
protocol processing. Cooper, el al [51. report that TCP~ and a number of Nectar-specific protocols have 
been implemented on the CAB connected to Sun-4 processors. Our host interface is not the only one being 
designed for the AURORA Testbed environment Davie of Bellcore [6, 7] reports on a host interface design 
for the TnrboChan­nel bus of the Dec Station 5000 workstation. His design relies on an Intel 80960 RISC 
microcontroller to perform the protocol processing and flow control for a trunk group of four STS-3C 
lines (622 MbPs). We see our implementation as providing much of the same functionality as the interface 
being developed by Davie, while at the same time exploring an alternative host interface architecture 
and providing netsvork connectivity to another class of workstations. In the next section, we discuss 
the relevant features of the IBM RS/6000 s Micro Channel bus [3], as a prelude to the discussion of actual 
host interface design. IBM RS/60001 Workstation ----------------------Other Hosts --14-, Running UPWARDS 
@: ,Q : Host Interface ! Other Switches I!!* ,C: :m .---, -: -------­-------­--.--­ 4 Electrical InterconnectIon 
STS-S c I Optical InterconnectIon OC-3 or OG12 # Figure 4: Operating Environment for Host Interface ----­* 
, I , ;= I . 0 1­:% : ga * I , s l , m 1 c , C/a I : L---- Figure 5. Micro Channel on the RS/6000 The 
IBM RS/6000 has a 32-bit Micro Channel bus for I/O interconnections. Transfers can be either 8, 16, or 
32 bits wide. The basic cycle time for the bus is 200 ns, but with data streaming, a single 100ns setup 
time can be amortized over many transfers. The streaming operation starts out as a basic transfer cycle. 
After the transfer has been setup, the slave acknowledges that it can support a streaming transac­tion 
which enables the master to then start the 10 Mhz streaming strobe. A data transfer can then be made 
every IN ns. No addmsing information is generated by the mas­ter while the stream is in progress. Streaming 
can be ter­minated or paused by either the slave or the master. Them are several simple steps by which 
the perfor­mance of this bus can be increased to the desired bandwidth for AURORA. First, the width of 
the streaming mode transfers can be increased to the 64 bits spectiled in the Micro Channel architecture 
description. Second, the 5: Segmenter clock rate can be increased from 10Mhz to 20Mhz. As this is a 
natural growth path for a high performance worksta­tion, we anticipate such steps. Note that the performance 
reaIized in these two steps is sufficient for a factor of four increase in performance. Thus an STS-12 
(the full AURORA Channel bandwidth) Amhitecture. can be accommodated by the Micro Figure 4 interface. 
shows the ope rating environment of this hosl ~. ___ , I tus a , j= ~ ~ 1 s 0 0 Command SONET Interface 
8 Linked List * STS-30and * :9 g Manager VCI Lookup :m r -----------­, smtus 1 Pointer Memory * VCI 
Reference Controller s a --------------,1D _ : CAM 256 by 48 :% ----------­ 1 % Reassembly/Removal , 
pGI t Cn $ Dual Port Control m m Reassembly Buffer cell Bodies I I Dud Pori Reassembly eutrer: 1-..--­ 
, ,----------..----_-, Figure 6: Receiver/Reassembler , t It , I . 13vte dock 1 20 VCI Lookup , 1 
GontroWGl to be Ree~ Controller , / ,: a 10 : Linked List ~ : Manager y Vcl / A A , : Reference I 
Hen, I Pen, CAM L----------- Ten250 by 48 % Am92C10 cAM g Framer Figure 7: SONET interface and VCI 
lookup controller 6. SegmenterlI ransmitter Design 7. Receiver/Reassembler Design Operation of the Segmenter/Transmitter 
portion of the host Referring to Figure 6, the three separated boxes interface shown in Figure 5 is as 
follows The fmt four (labeled SONET Interface and VCI Lookup , Linked List bytes of the A IM header am 
composed by the host proces-Manager , and Dual Port Reassembly ) can operate con­sor for the particular 
virtual circuit identifier (VCI) which currently once they me initialized and configured. This is to 
be transmitted. This header is then written into the concurrent operation can be exploited to allow reassembly 
header registers on the interface. A checksum is calculated of an ATM cell in much less than the 2.7 
microsecond cell and written into the register for the fti header byte. Data transmission thne. We will 
expand on each of these sec­for that VCI is then streamed into the FIFO buffer on the tions below. interface. 
As 48 bytes (one ATM cell body) arrive, the seg­menter controller concatenates the header and cell body, 
7.1. SONET interface and VCl lookup controller passing the completed cell to the SONET framer. The host 
Figure 7 shows the f~st of the subsections. This subsection is responsible for alternating VCIS. Analytic 
work has sug­operates as follow.w gested that alternating VCIS may avoid congestion of the The cell manager 
splits the header from the cell body Sunshine switching fabric. This alternation may however 1. place 
a considerable load on the host. and validates the checksum of the header to detect header corruption. 
2. The 16-bit VCI (as specitled in the ATM Standard [12] Am92G10 GAM Vcl #1 . Vcl #2:­~ .­ l\ Pointer 
Memory . . Pointer to Lasf Cell Pointer to First Cell Add,ess of Length of Llsf I Next Not Azwigned Pohmer 
to Last Cell -Null I Not Ae.lgned 1 I%&#38;aH -  Cell Cell Body Address of Cell &#38;c&#38; Address 
of Cell Body Null Polnter fo Last cell Pointer to First cell i Not Asskmed 4K H Address of Cell eody 
Nexf Cell . . . Address of Cell eody Next Cell SK . : 2K Pointer to Fir&#38; Free Pointer to LE!@t Frea 
: I Address of Cell Addreso of Cell T IThis potilon of memory represented above. _L Bcdy Null Body . 
.. is used for the Ilnked hsts Note: All cell body addresses dual Porf reassembly buffer. Address of 
Cell Next Cell refer to locations Body * In the Null Figure 8: Linked List Data Structures ~----, I ---­ 
, I ,----­ ----­ . [ = renster St.9rte F!dy 11,t-----­%-p:t-­-----­, Dual Port 1 Reassembly Buffer* --------­--------­-----J 
Address -T : Figure 9: Linked List Manager 322 ) is extracted from the header and loaded into a register 
while the body is loaded into the body storage FIFO. 3. The VCI lookup content addnxsable memory (CAM) 
locates the VCI and determines its linked list refe~nce location. 4. Once the reassembly linked list 
reference has &#38;n determined from the-VCI speeitiui by either the host interface or an incoming ATM 
cell, the reference can be passed to the linked list manager along with which link list operation is 
to be performed.  The CAM is a 256 by 48 device, thus 256 VCIS can be reassembled simultaneously. We 
believe that 256 virtual circuits should be sufficient for a personal workstation, and we can scale this 
number up (with improved CAM density) if applications prove to be more demanding. If the VCI has not 
been used before, the VCI lookup controller will add the VCI to the CAM, provided that space is available. 
Another function of the VCI lookup controller is to locate VCIS which the host is interested in moving 
from the reassembly buffer into mairi memory. The CAM controller is also capable of removing unwanted 
VCIS, from the CAM, under the dhection of the host. 7.2. Linked List Manager The linked list manager 
can perform a number of opera­tions on the reassembly linked lists data structures shown in Figure 8 
to effect reassembly. Note that the linked list structure consists solely of pointers and cell counts. 
The cell bodies being reassembled am stored separately in the dual reassembly buffer which will be described 
later. The data is separated from the data structures to minimize the amount of data movement. The sepamtion 
also allows for parallelization of the linked list management and data movement. Figm 9 shows the hardware 
used to support these data stmctums. We believe that the separation of the linked list structure and 
data will become more important as the network speed is increased, because it limits the degree to which 
memory bandwidth can throttle the inter­face. The fmt portion of the linked list manager memo~ consists 
of a series of locations for each of the 256 possible VCIS. For each VCI, the pointer to the newest cell 
received is stored in the fmt pointer, the pointer to the oldest dl is stord in the second pointer, and 
the current count of cells stord in the reassembly buffer for the particular V(3 is stored in the third 
location. It is useful to have access to a count of the number of cells available for a particular VCI 
in order to gauge the volume of txaffic on that VCI. The second portion of the linked list memory is 
com­posed of sets of pointers used for the nodes of the reassem­bly linked lists. Each cell body unit 
of reassembly memory has two painters associated with it. The fwst pointer is the address of the unit 
in the reassembly buffer and the second is the address of the next node in the linked list. Upon initialization, 
all of these units m assembled into a linked list known as the free list. AU pointers and countm used 
to keep track of the reassembly linked lists are zeroed. During initialization, operation of the host 
interface is suspended. Following initialization, four operations can be performed by the linked list 
managen 1. Cells can be added to the linked list for a particular VCI. 2. Cells can be removed from 
VCI S linked list 3. A VCI can be cleared, returning all of its nodes to the free list. 4. A cell count 
for a VCI can be obtained and returned to the host.  73. Dual Port Reassembly Buffer The f~st two operations 
pass the address of the cell body which is to be affected in the reassembly buffer to the dual port reassembly 
buffer controller described in the next section. The count and linked list associated with that VCI are 
also updated. The fourth operation reads the count for a particular VCI and returns it to the host. The 
final section of the design, Figure 10, is the dual port reassembly buffer. This buffer is used to reassemble 
the cell bodies arriving from the network and stem them until the host is ready to process them. when 
the linked list manager has determined the location in the reassembly buffer for a new cell, that location 
is written into the regis­ter associated with the assembly counter. Under the control of the dual port 
controller, the cell body is moved from the body FIFO where it had been temporarily stored in the reassembly 
buffer at addresses generated by the assembly counter. Another purpose of this section is to provide 
a means of transferring reassembled data to the host. The linked list manager provides the address in 
the reassembly buffer which contains the cell body which is to be transferal to the host When the reassembly 
buffer address of the is loaded into the retrieval counter, the data stream transfer can be initiated. 
As soon as the streaming strobe has been activated, (1) the retrieval counter inc~ments on the rising 
edge of the streaming strobe, (2) the dual port RAM con­troller reads the appropriate word from memory 
and (3) places it on the Micro Channel bus. ~-----------------------------------­ , 1 Linked List s ------­ 
-----­ -­ --------­ output RdY ddcess x Resister I r -8 ~; II ! m .. , : * 1 I I , m # 1-----i 
Figure 10: Dual 8. Host Software Support A number of assumptions have been made about the host software, 
particularly the host operating system s active management of the host interface. Active management is 
assumed due to the following observations: 1. Unlike mainframes, supercomputers or minicomputers, workstations 
are rarely a shared resource 2. Egalitarian scheduling policies have made real-time awkward 3. Interrupt-handling 
overhead is large~, and effects a sig­nillcant reduction in cache effectiveness. Full interrupt service 
per ATM cell would severely limit the workstation s network bandwidth. 4. The general solution to this 
problem is to use more aggressive I/O device ~agement policies and scheduling strategies. An example 
would using an interrupt only as an event indicator. Actual transfer of bnrsts$ of ATM cells would be 
accomplished in a polled manner.  While we are not certain that polled I/O is the proper solu­tion, 
the assumption allows us a simpler design, and the operating system research findings can be incorporated 
in succeeding host interface designs. ~ For exsmple, a savekestore of the RS/600!Ys registers is 256 
bytes versus the 48 byte ATM payload. * Bursis of ATM cells will arise as a conseq.eoce of the mismatch 
between some computer data units such as pagea, sixed as multiples of 1024 bytes, and the ATM payload 
of 48 bytes. 324 Manager Transfer Started -----, 1 -----1 In-&#38;l R dy 1...... Ed I Port Reassembly 
Buffer 9. Current Status and Future Plans As of May 1991, the design of the interface is complete. The 
segmenter (with the exception of the SONET framer) has been prototype and tested. A Micro Channel interface 
 has been developed which currently supports a sustained bandwidth of 110 Mbps. A minor modification 
which has not been fully tested should raise this rate to about 200 Mbps. Work has begun on prototyping 
the reassembler. The completion of the entire system is expected during the Fall of 1991. After testing 
and evaluation of this prototype has been completeil we plan to exploit the feahnes of this mchitechue 
to design a more powerful, higher bandwidth host interface. This interface should deliver the prophesied 
1 Gbps to the workstation. 10. Conckions We have described an instance of a high performance host interface 
used to connect powerful workstations to a high­bandwidth network fabric. Several factors influenced 
our design ckxisionx 1. Budgetary and time constraints have forced us to choose a simple implementation 
with limited func­tionality. 2. Uncertainty abut the protccol architecture used by the workstations 
suggested relegating most functions to software. This retains flexibility and allows us to iden­tify 
bottlenecks which can be addressed with hardware support in our next design. The criterion for using 
hardware in the current implementation was quite sim­pkx if the protocol behavior was fixed by external 
requirements, we put the function in our host interface. This was the ease for SONET/ATM on the network 
side, and the Micro Channel on the host side of the interface.  3. Host computer bus performance limitations 
currently limit bandwidth to a single STS-3C pair. We designed for scalability so we could adapt to incmsed 
host bus speeds by managing an entire trunk group of four STS­3C pairs. Our new host interface amhitecture 
has high performance, low cost, and a reasonable expected time to completion, 11. Acknowledgments Bell 
Communications research and IBM Research have been extremely supportive, supplying intimation, advice, 
and hardware. Bruce Davie of Bellcore has contributed to our thinking about ATM cell management and host 
inter­face technology as well as carefully reviewing earlier ver­sions of this work. Al Broscius and 
David Farber have have contributed design advice and constmctive critism throughout the design process. 
Marc Kaplan of IBM Research has been particularity helpful in unraveling many of the peculiar features 
of the RS/6000 MCA. Anew is a joint research effort undertaken by Bell Atlantic, Bell Communications 
Research, Inc., IBM T.J. Watson Research Center, Massachusetts Institute of Tech­nology, MCI, NYNEX, 
and the University of Pennsyl­vania. Anew is sponsored as pal of the NSF/ DARPA Sponsored Gigabit Testbed 
Initiative through the Corpora­tion for National Research Initiatives. NSF and DARPA provide funds to 
the University participants in Awom. In addition, Bellcore is providing support to MIT and U. Penn through 
the DAWN project. RS/6000 and Micro Channel are trademarks of IBM. DECStation, DECStation 5000, and TurboChannel 
are trademarks of Digital Equipment Corporation. 12. References <RefA>[1] H. Abu-_ T. Bahaj, T. Barzilai, and 
Y. Yem­ini, PW A Silicon Compiler for Very Fast Protocol Processing, in Protocols for High Speed Networks, 
ed. R. C. Williamson, North-Holland (1989). [2] Emmanuel A. Amonlg Francois J. Bitz, Eric C. Cooper, 
Robert D. Sansoq and Peter A. Steenkiste, The design of Nectm A network backplane for heterogeneous mnlticomputers, 
in Proceedings, ASPLOS-HI (Apti 1989), Pp. 205-216. [3] H. B. Bakoglu, G. F. Grohoski, and R. K. Montoye, 
The IBM RISC System/6000 processor Hardware overview, IBM Journal of Research and Develop­ment 34(l), 
pp. 12-22 (January, 1990). [4] Israel Cidon and Index S. Gopal, PARIS: An Approach to Integrated High-Speed 
Private Net­works, Inlemalional Journal of Digital and Analog Cabled Systems 1, pp. 77-85 (1988). [5] 
Eric C. Cooper, Peter A. Steenkiste, Robert D. Sansom, and Brian D. ZiR, Protocol Implementa­ tion on 
the Nectar Communication Processor, in Proceedings, SIGCOMM 90, Philadelphia, PA (September24-27, 1990),pp. 
135-144. [6] Bruce S. Davie, Host Interface Design for Experi­mental, Very High Speed Networks, in Proc. 
Compcon Spring 90, San Francisco, CA (February 1990),pp. 102-106. VI Bruce S. Davie, A Host-Network Interface 
Archi­tecture for ATM, in Proceedings, SIGCOMM 1991, Zurich, SW~ (September 4-6, 1991). [8] J. Giacopelli, 
M. Littlewood, and W. D. Sincoskie, Sunshinti A broadband packet switch architec­ture, in Proceedings, 
International Switching Sym­posium, Stockholm (1990). [9] H. Kanakia and D. Cheriton, The VMP Network 
Adapter Board (NAB): High Performance Network Communication for Multiprocessors, in Proceed­ings, SIGMETRKX 
88 (1988). [10] Thomas J. Robe and Kenneth A. Walsh, A SONET STS-3C User-Network Interface IC, in Proceed­ings, 
Custom Integrated Circuits Conference, San Diego, CA (May, 1991). [11] Computer StafT, Gigabit Network 
Testbeds, IEEE Computer 23(9) (September, 1990). [12] CCITT Study Group XVIII, Meeting Report of sub­working 
party 8/1 ATM, Document 14-E, June 1989. [13] Martina Zitterbart, High-Speed Transport Com­ponents, IEEE 
Network, pp. 54-63 (January 1991).</RefA> 
			
