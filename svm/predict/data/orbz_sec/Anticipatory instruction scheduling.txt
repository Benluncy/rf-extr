
 Anticipatory Instruction Scheduling Vivek Sarkar Barbara Simons Application Development Technology Institute 
IBM Software Solutions Division 555 Bailey Avenue, San Jose, California 95141 {vivek.sarkar, simons}Qvnet 
.ibm.com Abstract works well in conjunction with hardware branch prediction which enables the lookahead 
window to be filled with instruc­tions from the basic blockl that is predicted to be executed Modern 
processors have many levels of parallelism arising next. If the branch direction is incorrectly predicted, 
these from multiple functional units and pipeline stages. In this hardware mechanisms provide a safe 
rollback of instructions paper, we consider the interplay between instruction schedul­from the next 
basic block that may have been eagerly exe­ ing performed by a compiler and instruction lookahead per­cuted. 
 formed by hardware. Anticipatory instruction scheduling is the process of rearranging instructions within 
each basic For instructions in a single basic block, hardware does not block so as to minimize the overall 
completion time of a set offer any lookahead advantage over software; in this case, the of basic blocks 
in the presence of hardware instruction looka­ compiler can essentially schedule instructions with complete 
head, while preserving safety by not moving any instructions lookahead at compile-time. However, hardware 
instruction beyond basic block boundaries. Anticipatory instruction lookahead does have an advantage 
in safety and in perfor­scheduling delivers many of the benefits of global instruction mance when scheduling 
instructions in the presence of (stat­scheduling by accounting for instruction overlap across basic ically 
unpredictable) branches. Out-of-order instruction ex­block boundaries arising from hardware lookahead, 
without ecution in the hardware can also adapt to non-deterministic compromising safety (as in some speculative 
scheduling tech­ execution times and dynamic memory disambiguation. niques) or servicabilit y of the 
compiled program. Antzc$patory instruction scheclulirzgis the process of rearranging We present the first 
provably optimal algorithm for a special instructions within each basic block so as to minimize the case 
of anticipatory instruction scheduling for a trace of basic overall completion time of a set of basic 
blocks in the pres­blocks on a machine with arbitrary size lookahead windows. ence of hardware instruction 
lookahead, while preserving We extend this result for the version of the problem in safety by not moving 
any instructions beyond basic block which a trace of basic blocks is cent ained wit hin a loop. boundaries. 
Anticipatory instruction scheduling delivers many In addition, we discuss how to modify these special-case 
of the benefits of global instruction scheduling by accounting optimal algorithms to obtain heuristics 
for the more general for instruction overlap across basic block boundaries arising (but NP-hard) problems 
that occur in practice, from hardware lookahead, without compromising safety (as in some speculative 
scheduling techniques) or serviceability of the compiled program. Safety is assured by the hard- Introduction 
ware s ability to provide a safe rollback of instructions from the next basic block that may have been 
eagerly executed. All modern processor architectures include some degree of Serviceability is obtained 
by ensuring that instructions are instruction lookaheadto support out-of-order instruction dis­ not rearranged 
beyond basic-block boundaries, thus making patch, issue and execution. Hardware instruction lookahead 
it easier to map from an instruction location to the source code location. Permission to make digitsl/hard 
copies of all or part of thk material for personal or classroom use is granted without fee provided that 
the copies We present the first provably optimal algorithm for a special are not made or distributed 
for profit or commercial advantage, the copy­right notice, the title of the publication and its date 
appear, and notice is 1A bas%c block is a single-entry single-exit sequence of instructions given that 
cop yright ia by permission of the ACM, lnc. To copy otherwise, to republish,to post on servers or to 
redkatribute to Iiata, requires specific with no intervening control flow [1]. permission and/or fee. 
 SPAA 96, Padua, Italy @ 1996 ACM 0-89791-809-6/96/06 .,$3 .50 case of anticipatory instruction scheduling 
for a tracez of basic blocks on a machine with arbitrary size lookahead win­dows. We extend this result 
for the version of the problem in which a trace of basic blocks is contained within a loop. For the case 
of a loop containing a single basic block, we observe that anticipatory instruction scheduling can be 
used as a post-pass to software pipelining [6] (the two techniques are complementary). In addition, we 
discuss how to modify these special-case optimal algorithms to obtain heuristics for the more general 
(but NP-hard) problems that occur in practice. One of the key ideas in our solution is that of moving 
idle slots as late as possible in a given basic block. This is a useful step because it offers more opportunity 
for overlap with instructions at the start of the next basic block. We present an extension to the Rank 
Algorithm from [10] for moving idle slots as late as possible by using the Rank Al­gorithm s ability 
to handle deadlines. The extension is a heuristic solution in general, and provably optimal for the case 
in which the Rank Algorithm is optimal (0/1 latencies, unit execution times, and a single functional 
unit). Our Anticipatory Instruction Scheduling algorithms use this idea when scheduling instructions 
in a trace of basic blocks that may or may not have an enclosing loop. If the compiler has no trace or 
loop information, a simple application of this idea is to move idle slots as late as possible independently 
in each basic block. The rest of the paper is organized as follows. Section 2 illustrates the benefits 
of anticipatory instruction scheduling through examples. Section 3 describes our extension to the Rank 
Algorithm for moving idle slots as late as pos­sible. Sections 4 and 5 present our Anticipatory Instruction 
Scheduling algorithms respectively for the case of a trace of basic blocks, and the case of a single 
loop cent aining a trace of basic blocks. Section 6 discusses related work, and Section 7 cent sins our 
conclusions and an outline of future work. Examples In this section, we illustrate the benefits of anticipatory 
instruction scheduling through examples. As a prelude, we provide examples of the Rank Algorithm, and 
our modifica­tion to the Rank Algorithm for moving idle slots as late as possible. 2 A trace is a sequence 
of basic blocks obtained by following a simple path in the program s control flow graph. The subsections 
are organized as follows. Subsection 2.1 contains a brief overview of the Rank Algorithm from [10] with 
an example. Subsection 2.2 illustrates our extension to the Rank Algorithm for moving idle slots as 
late as possi­ ble. Subsection 2.3 illustrates our Anticipatory Instruction Scheduling Algorithm for 
a sequence of two basic blocks. Fi­ nally, subsection 2.4 illustrates our Anticipatory Instruction Scheduling 
Algorithm for a loop containing a single basic block. 2.1 Rank Algorithm and Example We now give an overview 
of the Rank Algorithm; the reader is referred to [10] for a more detailed description. The dead­ltne 
of instruction a, written d(z), is the latest time at which z can be completed in any feasible schedule. 
The rank of z, rank(z), is an upper bound on the completion time of x if z and all of the descendants 
of z are to complete by their deadlines. The Rank Algorithm executes the following steps: 1) Compute 
the ranks of all the nodes, 2) Construct list, an ordered list of nodes in nondecreaszng order of their 
ranks, 3) Apply a greedy scheduling algorithm to list. All nodes are given the same very large number 
as an artzji­ czal deadline3 The deadline is sufficiently large that it will not introduce any additional 
constraints into the problem. The Rank Algorithm then computes the rank of every node. Consider the dependence 
graph shown in Figure 1 for a single basic block, B.B1. A node represents an instruction in the basic 
block with unit execution time. The weights on an edge, say (z, ~), represents the latency, i.e. the 
fact that ~ cannot start until at least one time unit after z completes. Each node is given an artificial 
deadline of 100. Since this implies that every node must be completed by time 100, we give all nodes 
an initial rank of 100. As we obtain more information about a node, we decrease the value of its rank 
accordingly. The example schedules in subsections 2,1, 2.2, and 2.3 are for a single processor (functional 
unit). If the schedule for Figure 1 is to be completed by 100, in­structions a and r must complete no 
later than 100 (rank(a) = ~ank(r) = 100), and instructions w and b must complete no later than 98 (rank(w) 
= rank(b) = 98). Instruction x has nodes w, b, a, and r as descendants. Therefore, z must complete sufficiently 
early to allow all of its descendants to complete by their ranks. The same is true for e. The rank computations 
yield ~ank(z) = ~ank(e) = 95. 31n our algorithm, deadlines are assigned to nodes when idles slots are 
moved and when basic blocks are merged. We assume that the initial basic block instructions have no pre-assigned 
deadlines. 120 Dependence graph for basic block BB1: Schedule obtained by Rank Algorithm: Jelxl vvlrlal 
 01234567 Schedule after Delaying Idle Slot: Jxlelrlblvvl I 01234567 Figure 1: A Basic Block Example 
 The algorithm then constructs a totally ordered list in order of rank, going from small to large. Suppose 
the ordering we choose is: e, z, b, w, a, ~. The greedy algorithm will then use this ordering to obtain 
the schedule shown in the middle of Figure 1. 2.2 Example of Rank Algorithm Extension to Move Idle Slots 
as Late as Possible Continuing with the example of Figure 1, we observe that the minimum completion 
time schedule given in Figure 1 has a makespan4 of 7. Therefore, if we reduce the deadlines and ranks 
of all the nodes of the basic block by 100 7 = 93, we get reduced ranks as follows, mznk(z) = rank(e) 
= 2, rank(w) = Tank(b) = 5, rank(a) = mznk(~) = 7. Notice that in the first schedule in Figure 1 all 
nodes complete by their reduced ranks, the last node, a, completes precisely at its reduced rank, and 
x, the node immediately preceding the idle slot, also completes at its reduced rank. Since x is an ancestor 
of all the nodes that follow in the schedule and since there are latency 1 edges from z to all AThe ~ahe*P=n 
of ~ Schedule is the cOmp]e*km time of the last instruction in the schedule, assuming that the first 
instruction starts at time O. its immediate successors, the idle slot could be move to a later time only 
if z is started earlier. So we set its deadline, d(z) = 1. The new schedule that is obtained is shown 
at the bottom of Figure 1. This schedule also has a makespan of 7, but the idle slot occurs at a later 
time. 2.3 Example of Anticipatory Scheduling for a Sequence of Two Basic Blocks We now illustrate how 
moving idle slots later can reduce the makespan of two adjacent basic blocks in a trace. Our model for 
instruction lookahead is as follows. Let W be the size of the lookahead window. At any given instant, 
the window contains a sequence of W instructions, say in, . . . . i~+w 1 that occur contiguously in the 
program s dynamic instruction stream. The processor hardware is capable of issuing and executing any 
of these W instructions in the window that is ready for execution. The window moves ahead only when the 
first instruction in the window (in) has been issued. We observe that W, is usually very small (typically, 
<10 instructions) in practice. Larger values of W are harder to implement in hardware because of the 
quadratic growth in the number of instruction-pairs that need to be examined by the hardware to check 
for data dependence. Definition 2.1 Let S be a schedule for a trace oj basic blocks BBL, ..., BB~. Let 
P be a permutation (total ordering) con­ sistent with S. The subpermutation of P for baszc block BB~, 
P~, is defined to be the permutation of the instructions of BB~ as given by their relatwe ordering in 
P. Definition 2.2 (i, j) IS sad to be an inversion in permu­tation P if i < j and the ith instruction 
in P belongs to a later basic block than the jth instruction in P. Definition 2.3 Schedule S with permutation 
P assaid to be legal ij itsatisfies all data dependence and the followzng two constraints: Window Constraint: 
for everg inuerszon (i, j) in P, j i + 1 < W, the size of the Zookahead window. Ordering Constraint: 
S is obtainable as a greedy schedule from przority list L = PI o Pz 0 ...0 Pm, (where o denotes list 
concatenation). The Window Constraint enforces a bounded lookahead win­ dow size. The Ordering Constraint 
models the hardware property of not issuing a later instruction in the lookahead window that s ready 
to execute before an earlier instruction Dependence graph for basic block BBz: We first schedule both 
basic blocks together with no con­ %3 1 1 1 z1 1 v A schedule for basic blocks BB1  ]xlelrlblvvl 
zlalaldvla( 01234567 89 10 11 An edge from BB1 to BB2: A schedule for BBI to BBz with extra edge:  Jxlelrhdblzlalalpl 
VICJ 01234567 89 10 11 Figure 2: Example with Second Basic Block that s ready to execute. Consider basic 
block BB1 from Figure 1 in conjunction with basic block BB2 from Figure 2, with a hardware lookahead 
window size of W = 2. If there are no edges from the nodes of BB1 to those of BB2, then we can construct 
the first schedule in Figure 2 for two basic blocks by giving all BBI nodes (except for z, which has 
d(z) = ~unk(z) = 1) deadlines of 7 and all BB2 nodes a very large deadline. Since the schedule is for 
a single processor, it directly corresponds to a permutation from which we can obtain subpermutations 
P1=xerb waand P2=zqpvg for basic blocks BBl and BBz as specified in Definition 2.1. Now suppose there 
is a latency 1 edge from instruction w in BB1 to instruction z in BBz, as shown in Figure 2. In this 
case we cannot start z one unit after w has been started. When there are edges that run from one basic 
block to the next, the nodes in the second basic block might impact the ordering of the nodes in the 
first basic block in an optimal schedule, straints on the deadlines of the BB1 nodes so as to obtain 
a lower bound on the completion time of a legal schedule for BB1 U BBz. Suppose for this example that 
we again give every node in BBI U BB2 a deadline of 100. The rank computation gives the following values: 
rank (g) = rank(v) = rank(a) = ~unk(r) = 100, rank(p) = rank(b) = 98, rank(g) = 97, Tank(z) = 95, ~ank(w) 
= 93, Tank(e) = 91, rank(z) = 90. Notice that the ranks of w and b are now significantly different. The 
following is a list ordered by the new ranks: z, e, w, z,q, p, b, v, a, ~,g. The schedule that will be 
constructed from the ordered list is given at the end of Figure 2. This is also a legal schedule (see 
Definition 2.3). However, in general we cannot guarantee that a legal sched­ule will always be constructed 
by this step. If, for example, the latency on the edge from z to g had been O instead of 1, the algorithm 
might have scheduled q immediately after z, instead of scheduling a at that time. The resulting schedule 
with permutation P= xerwbzqapvgwould not satisfy the Window Constraint for a window of size W = 2. But 
even if W is large enough for the Window Constraint to be satisfied, the schedule would not be legal 
because it still would not satisfy the Ordering Constraint since the subpermutations ofParePI =xerwbaand 
P2=zqp vg and the schedule is not obtainable as a greedy schedule from priority list L=xerwbazqpvg. The 
Ordering Constraint forces a to be scheduled before q in this case. Therefore, after first determining 
a lower bound on the com­pletion time of a legal schedule for BB1 U BB2, which in this case is 11, we 
give all BBI nodes deadlines that correspond to the minimum completion time for BBI alone, which in this 
case is 7. The only exception is if the algorithm has already determined that some node must have a smaller 
deadline, as is the case for z, which has d(z) = 1. The smaller deadlines are retained. All BB2 nodes 
are given the deadline that is equal to the completion time of the merged schedule, in this case 11. 
The corresponding schedule S is computed. If S is not feasible, we increase the deadlines assigned to 
the BBz nodes until we find the minimum value for which we can construct a feasible schedule. 2.4 Example 
of Anticipatory Scheduling for a Loop containing a Single Basic Block Figure 3 contains a simple C code 
fragment for computing partial products of a zero-terminated sequent: of integers stored in array x. 
The output zero-terminated sequence of partial products is stored in array y.  C code fragment: { int 
*x, *y, i; YIOI = xIO]; for(i=l ;x[i]!=O; i++ ) y[il = ytl-11 * x[il; y[il = O; ) Target instructions: 
,.. CL.18: // Load x[i] into gr6, update index gfi L4AU gz6, gr7=x(gr7,4) // Store grO into y[i-1], 
update index gr5 ST4U gr5,y(gr5,4)=gr0 // Compare gr6 (x[i]) with O, store result in crl C4 crl=gr6,0 
// Multiplygr6 (x[i]) &#38; grO (y[i-lI), store ingrO M grO=gr6,gr0,mq // Exit if gr6 (x[i]) is = O BT 
CL.l,crl,0x4/eq // Loop back B CL,18 CL.1: ... Dependence graph: <(),0> _. , .< \ (-jj!!i-&#38;e + 
 crl BT / $ M <%,0> / @!j!+\ // <0,0> <4,1 ; 0 Datadependence Control dependence * > Schedulel: L4 ST 
C4 M BT L4 ST ,.. Schedule2: L4 ST M C4 BT L4 ST . . . Figure 3: Loop example: C code, instruction sequence, 
depefidence graph, and schedules After various back-end optimization, the set of target in­structions 
that can be selected for the for loop is shown in Figure 3 starting at label CL. 18. For concreteness, 
we used a specific architecture (the IBM RS/6000) to represent the target instructions. grO, gr5, gr6, 
gr7 denote general­purpose (fixed-point) registers, and crl denotes a condition register. Note that the 
optimizations performed include software pipelin­ing; the store instruction belongs to the iteration 
of the for loop that is previous to the iteration that the remaining instructions belong to. The prolog 
and epilog of the soft­ware pipeline are not shown, for the sake of brevity. This example illustrates 
how anticipatory instruction scheduling can be used as a post-pass to software pipelining (the two techniques 
are complementary). Figure 3 also shows the dependence graph for the instruc­tions in the for loop. Each 
node represents an instruction with unit execution time. Each edge is labeled with a < latency, distance> 
pair as de finedin Section 5. distance= 0 identifies a loop-independent dependence and didance >0 identifies 
a loop-camied dependence [13]. In this example, we assume that the LOAD and COMPARE instructions have 
a latency of 1 cycle, and the MULTIPLY instruction has a latency of 4cycles6. The only loop-carried 
data dependencein Figure 3 that is not a self-dependence is from the MULTIPLY instruction to the STORE 
instruction with a distance of 1 iteration. All other dependence edges are either self loop-carried dependence 
with distance = 1 or loop-independent dependence with distance = O. The control dependence edges ensure 
that all other instructions precede the branch instruction (BT) in the instruction schedule output by 
the compiler. There are two possible schedules shown at the bottom of Figure 3forthe instructions inthe 
CL.18100p. The first is an optimal schedule for the basic block i.e. for a single iteration of the loop. 
It yields a completion time of 5 cycles for a single basic block. However, in steady-state this schedule 
executes one iteration every 7 cycles. Two extra idle slots are forced to appear at the end of each iteration 
to satisfy the MULTIPLY instruction s latency of 4 cycles. In contrast, the second schedule has a completion 
time of6 cycles for a single iteration, but it also executes one iteration every 6 cycles in steady-state. 
In general, a schedule which is optimal for a single basic block can be suboptimal in steady-state, and 
a schedule which is suboptimal for a single basic block can be optimal in steady-state. 5These latenciesdo 
not correspond to any specific implementation of the RS/6000 instruction set architecture. Procedure 
Move-Idle-Slot (S, t,) /* t, is the idle slot currently being processed in schedule S. Procedure Move-Idle-Slot(ti) 
returns t, ifit fails to increase the start time of the ith Idle slot or it returns tj> t;if it succeeds 
in delaying the start time of the ith idle slot to t:. S is the most recently constructed (input) schedule. 
a, is the node scheduled at time t, 1inS. */ begznMove_Idle_ Slot; t:= t,;Oldsch = ~ / this step insures 
that idle slots don t move earlier */ zf there is any node y c ~; with rank(y) > ti, mnk(y) = t,; whale 
t: = t, do d(ai) = rank(a; ) = ti 1; if there is no node in a, with rank at least t, goto failure; / 
mmk.afg computes new schedule using modified deadlines 1 S =rank-alg(Ns, Es, d~); if S = 0then / rank.alg 
cannot meet all deadlines *I goto failure; s = s ; tj = start time in S of ith idle slot; end whzle / 
t, has been delayed / jinalize all deadline modifications; retzma(S , tj); Failure: Undo all deadlines 
that were reduced; retzwn.(Oldsch, t,); end Move_Iclle_Slot Figure 4: Procedure Move_Idle_Slot 3 Extension 
to Rank Algorithm to Move Idle Slots as Late as Possible A schedule S assigns each instruction z a start 
hme S(a) and functional unit on which to run. We assume that each start time has an integer value. A 
functional unit has an idle slot at some integer value t, where tis less than the completion time of 
S, if that functional unit is not starting or running any instruction at time t. Procedure Move-Idle_Slot 
in Figure 4 contains our algorithm for delaying an idle slot in a schedule, based on the Rank Algorithm 
from [10]. our algorithm provides a heuristic solution to the general problem. We also have proved for 
a restricted case that, given an optimal (minimum makespan) schedule S as input, Procedure Move-Idle-Slot 
schedules the specified idle slot at the latest possible time over all optimal schedules [11]. Therefore, 
repeated application of Procedure Move_Idle_Slot constructs a minimum makespan schedule in which each 
idle slot occurs as late as possible. The restricted case for which the optimality result holds is the 
case in which the Rank Algorithm is optimal i.e. 0/1 latencies, unit execution times, and a single functional 
unit. For convenience, the definitions in this section assume the restricted case of 0/1 latencies, unit 
execution times, and a single functional unit. However, it is straightforward to gen­ eralize these definitions. 
For example, we can accommodate non-unit execution times by using the start time to identify nodes that 
are scheduled prior to an idle slot, and we can accommodate multiple functional units by defining a set 
of tail nodes (defined below) rather than a single tad node. Partitioning a schedule. Suppose a schedule 
S for a single functional unit has j idle slots at times tl,t2,.. ,tj, with tl< tz< < tj.We partition 
S into j + 1 u sets, U1, U2, . . . , ~j+l, where al consists of all the nodes scheduled in S prior to 
tl,and u,, 1 < i s j, consists of all the nodes scheduled in S between t;_land t?; rr7+I is all the nodes 
scheduled after tj.Each u set except for ~j+l terminates at an idle slot. For u,, i s j, define tat~to 
be the node scheduled in S at time t, 1, i.e. just prior to the idle slot. Moving the idle slots. The 
Rank Algorithm com­putes ranks of nodes by initially assigning all nodes some deadline D, where D is 
sufficiently large that it s guaranteed not to interfere with the ordering determined by the Rank Algorithm. 
Let BB be a basic block. We first construct schedule SO with makespan of T by run­ning the Rank Algorithm 
on input BB. Next, we modify the ranks assigned to the nodes in BB by decrementing every deadline, and 
consequently every rank, by D T. One effect of this modification is to give all sink nodes a rank of 
T. If SO has no idle slots, we then process BB together with the next basic block in the trace. If, however, 
So has one or more idle slots, we process the nodes in BB as follows. Let S be a schedule with makespan 
T constructed by the Rank Algorithm. We define Ns to be the nodes in S, Es to be the edges between nodes 
in NS and ds to be the deadlines of the nodes in S. The first time the Procedure Move-Idle-Slot is called, 
all deadlines in d~ have value T. However, each iteration of Procedure Move_Idle_Slot decre­ments the 
deadline (and rank) of the tail node from the last iteration. If at some iteration the idle slot being 
processed 124 (t,) is delayed, then all deadline modification that have been made since the processing 
of ti began are made permanent. Otherwise, they are undone, Assume that there is at least one idle slot 
in SO, and let the start times of the idle slots be tl, t2,....tk,with tl< tz< ...< t~.Procedure Move-Idle_Slot 
is called to try to move the ith idle slot of schedule S so that it starts at a later time. If it is 
not possible to move the ith idle slot to a later time, Procedure Move-Idle310t returns the input schedule 
S and the input time t;. If it is possible to move the ith idle slot later, then Procedure Move.Idle_Slot 
returns a modified schedule S and tj,the new start time of the ith idle slot, with t: > t;. 4 Anticipatory 
Instruction Scheduling for a Trace. of Basic Blocks Subsection 4.1 presents a general algorithm (called 
Algo­rithm Lookahead) for Anticipatory Instruction Scheduling for a trace of basic blocks. We have proved 
that this algo­rithm is optimal for the case in which the Rank Algorithm is optimal i.e. for the case 
of 0/1 latencies, unit execution times, and a single functional unit [11]. In subsection 4,2 we elaborate 
on the issues involved in using the algorithm as a heuristic for general machine models. 4.1 The General 
Algorithm Let BBI, BB2,..., BB~ be the ordered list of basic blocks in the input trace, and let W be 
the size of the window. Figure 5 shows the overall structure of Algorithm Looka­head. The for loop iterates 
over the basic blocks and has the following invariants: sched is the partial schedule (permutation) 
built so far. old is the set of instructions from previous basic blocks that still need to be scheduled. 
 new is the set of new instructions from the current basic block, BBi.  The first major step in the 
for loop is to call procedure Merge (Figure 7), which returns a schedule S for instruc­tions in old U 
new. Procedure Merge assigns deadlines to instructions in okl to ensure that instructions from new do 
not displace instructions in o[d, but only fill idle slots that may be present among instructions in 
old. The next step is to /* BB,, BBz , . . . . BB~ are the input basic blocks in the trace, and W is 
the size of the lookahead window. N = BB1 U BBz, . . . U BB~ is the set of all instructions and E G N 
x N is the set of all dependence edges. For any subgraph G = (N , E ) with node set N ~ N, wedefine E 
= E n (N X N ). */ Algorithnr Lookahead(BBl, BBz, . . . . BBn, W) begin main sched := empty list; oki 
:= 0; dold := a; fori:=l tom new:=BB; ; / Schedule merged instructions in old U new and return new schedule 
and deadlines in S, ds. */ (S, ds) = rnerge(old, new, dw, W); /* Delay all idle slots in S*/ (S, ds) 
:= Delay-Idle-Slots(S, ds~ /* chop S into prefix S and suffix S+ */ (S-, S+ , d~+) := chop(S, d,y); / 
Append S-to current sched / sched := concat(sched, S ); / Replace old by S+ / old := S+; dold := d~+; 
end for /* Append final S+ to sched / sched := concat(sched, S+); output(sched); end main; Figure 5: 
Algorithm Lookahead delay all idle slots in S as far as possible by calling procedure Delay-Idle-Slots 
(Figure 6), which repeatedly calls procedure Move-Idle_Slot (Figure 4). The final major step in the for 
loop is to chop schedule S into S-and S+, where S-is the prefix of S up to the occurrence of the last 
idle slot prior to the last W nodes in S, and S+ is the rest of S (see procedure Chop in Figure 6)6. 
If there is no idle slot in S, then since there dependence edges with non-zero latencies between old 
and new nodes that could create unnecessary idle slots between the last old node and the first new node, 
we retain all of old. In this caseS-=oandS+=S.WealsosetS-=0andS+=S1 if S has fewer than W nodes. Since 
deadlines are used to move the idle slots as late as possible, if S-# O, we next decrement all deadlines 
and ranks in S+ by tj+ 1,which is Because no modifications to S-can improve the final schedule, we can 
output S-at this point, thereby reducing the complexity of scheduling the remaining input. Since ultimately 
we will output a schedule for the entire trace of basic blocks, this short-cut does not impact our assumption 
that a schedule can be modelled as a permut at ion. /* Let tk be the start times of the idle slots in 
S. tl,tz,..., *I Delay -Idle.Slots(S, ds) begzn zfS has no idle slot return(S, ds); /* process idle 
slots from earliest to latest */ fori:=ltok (S, ds, temp):= Move-Idle.Slot( S,d.s, t,); /* keep trying 
to move ith idle slot */ whale temp > t, t,:= temp; (S, temp) := Move_ Idle_Slot(S, t,); end whale end 
for ~eturn(S, d5); end schedule /* Let tl, ta,....tkbe the start times of the idle slots in S. *I chop 
(s, ds) begin chop; zfS has no idle slots or if INSI < W ds+ =ds; else let j be the largest value so 
that at least W 1 nodes follow tjin S; S-:= the prefix of S up to tj; S+ := the suffix of S after tj; 
decrement all deadlines and ranks in S+ by tj +1; end af r-etrmn(S-, S+, d~+); end chop Figure 6: Procedures 
Delay _Idle.Slots and Chop equal to the length of S-plus one for the idle slot Procedure merge (Figure 
7) works as follows. If there are dependence from old to new, and if the number of nodes in old U new 
is greater than W, it may be necessary to reorder some of the old nodes in order to allow the new nodes 
to fill idle slots. Therefore, we initially compute a minimum makespan schedule, S, for cur = old U new 
with makespan T in which we give all nodes in cur the same large deadline D. T is a lower bound on the 
makespan of a schedule for cur that satisfies the window constraint. For each node w in new we set d(w) 
= T; for each node w in old we set 7The output schedule keeps old separate from new; the overlap among 
their instructions occurs only in the lookahead window at runtime. /* The inputs for Procedure merge 
are two sets of instruc­tions, old and new, and the outputs are schedule S and deadlines d .Ur obtained 
by scheduling instructions in the union, old U new. *I rnerge(old, new, ddd, W) begmrnerge; cur := old 
U new; I D is an artificially large deadline / set all deadlines in d~~. to D; S := rank_ alg(cur, E 
fl (CUT x CUT), d,~,); /* TOM is the makespan of the schedule for old / for w E old set d(w) = min{d(w), 
TOM]; / T is the makespan of schedule S / for w Enew set d(w) = T; S := rank. alg(cuT, E (? (cur x CUT), 
dc~, ); whale S is infeasible zncrease all deadlines in dn.~ by 1; compute S using the new values of 
d~e~ end whale; d cur = dold U dnetu; S := rank.-alg(cur, E n (CUT x cur), dcti, ); return (S, d=w, ); 
end merge Figure 7: Procedure Merge d(w) = rnin{d(w), Told}, where d(w) is the old deadline of w and 
Told is the makespan for the nodes in old. The algorithm next computes the ranks using the newly assigned 
deadlines. If the rank of some node is determined to be less than one, we increase the deadline of all 
new nodes by the minimum amount that will result in the construction of a feasible schedule. We know 
that there is a feasible, if not necessarily minimum makespan, schedule that can be obtained by first 
scheduling all of the otd nodes followed by all of the new nodes, with possibly a unit of idle time8 
between the two. Therefore, the number of iterations of the rank computation cannot exceed 2W (or log(W) 
if binary search is used). Since by assumption the ~umber of nodes in old U new is at least W, these 
iterations do not increase the complexity of the total algorithm. 4.2 General Machine Models The instruction 
scheduling problem that we are likely to encounter in practice can involve multiple functional units 
SIn the ~a~e of latencies larger than 1, the amount of idle time between the two schedules cau be as 
much as the value of the largest latency. that perform specific functions, instructions that can require 
more than a single unit of time, and latencies that can exceed 1. For these cases there is no hope of 
obtaining an optimal polynomial time algorithm, even for a single basic block. We recommend using the 
Anticipatory Instruction Scheduling algorithm described in subsection 4.1 as a heuristic. In­cluded below 
are brief discussions of issues that can arise. Multiple Functional Units. There may be multi­ple idle 
slot candidates for Procedure Move.Idle.Slot, and so a heuristic choice has to be made as to which idle 
slot to attempt to move. For example, suppose that some type of functional unit(s) is in great demand. 
We can try to minimize the number of idle slots on that type of functional unit by reducing the deadlines 
of nodes only on the specific type of functional unit. The goal is either to move an idle slot later 
or to eliminate it alt oget her. (Eliminating an idle slot is a possibility y since we are now talking 
heuristics, not optimal algorithms). Non-unit execution times. All processors have in­structions (e.g. 
divide) that take more than one cycle. There are several variants that can be used to compute ranks for 
such instructions . The simplest approach is to insert each instruction whole into the backward schedule 
so that it com­pletes at the latest possible time no later than its rank. An alternative approach that 
maintains the upper bound condition on the ranks in the multiple functional unit case is to break up 
longer instructions into single units. Each unit is then started at the latest possible time no greater 
than its rank on a functional unit of the correct type. The piece of the instruction that has the earliest 
start time assigned to it in the backward schedule is used for the rank computation. Longer latencies. 
Latencies larger than one can also be found in many modern processors. Since a schedule can cent ain 
multiple adj scent idle slots in this case, several instructions may be candidates for deadline modification. 
There are a variety of heuristic modifications that can be made to Algorithm Lookahead, depending on 
what type of optimization is most desired. An obvious choice is to force earlier only nodes that start 
shortly before some heuristi­cally determined set of idle slots. Since latency values are limited by 
pipeline depths, the maximum latency value (e.g. 4 cycles for a 5-stage pipeline) is usually much smaller 
than the maximum instruction length (e.g. 20 cycles for a divide instruction), This makes the algorithm 
more tractable be­cause the maximum latency value limits the number of nodes that we need to examine 
just before an idle slot. 5 A Loop containing a Trace of Basic Blocks We extend the dependence graph 
to represent loops by in­cluding loop-carried dependence edges. Now each edge is labeled with a < kztency, 
distance > pair specifying the latency (O or 1) and the distance (in iterations) for the dependence. 
distance = O identifies a loop-independent de­pendence and distance > 0 identifies a loop-carmed depen­dence 
[13]. Let BB1, BBz, ..., BB~ be the trace of basic blocks en­closed within a loop. The completion time 
of n iterations of the loop on a machine with hardware lookahead equals the completion time that would 
be obtained if the loop was completed unrolled (ignoring the cost of the loop-back branches) i.e. the 
completion time of the following sequence BB, [l],..., BBm[l], BBl[2], BBm[2], [,],..., BB1[n], . . . 
. BB~[n] where BB2 [k] denotes the kth iteration instance of the jth basic block. The key extension 
is that loop-carried depen­ dence edges can now place extra scheduling constraints across basic blocks 
from different iterations. 5.1 Loops with Two or More Basic Blocks We first consider the case when a 
trace of m > 1 basic blocks is enclosed within a single loop. The scheduling prob­lem for an n-iteration 
loop containing a trace of m basic blocks is very different from the scheduling problem for a trace containing 
n x m basic blocks because the instruction sequence for a given basic block needs to be the same for 
all n iterations in the former case. Our solution is to simply use Algorithm Lookahead from Section 4, 
and add an extra step in which BB~ is sched­uled with the BB~ as a successor using the loop-carried data 
dependence to establish the dependence constraints between the two sets. This yields an Anticipatory 
Instruc­tion Scheduling algorithm that is provably optimal for the case of 0/1 lat encies, unit execution 
times, and a single functional unit, and can be used as a heuristic for more general cases. The optimal 
proof for the special case assumes that Bl% and BBI-together contain at least W instructions. This is 
not a major restriction in practice because the lookahead window size, W, is usually very small. 5.2 
Loops with a Single Basic Block We now consider Anticipatory Instruction Scheduling for a loop cent aining 
a single basic block. This is harder than Anticipatory Instruction Scheduling for a loop containing multiple 
basic blocks because we now have to consider the overlap among instructions in BB1 and BB~ which belong 
to the same basic block, BBI. Let G be the data dependence graph for loop L, and G~; be the loop-independent 
subgraph of G that contains all the nodes of G but only the loop-independent dependence edges of G. In 
the remainder of this subsection, we present solutions for cases that are progressively more general 
based on the structure of Gli. Although the final case subsumes the previous cases, we discuss each case 
separately, for the sake of clarity. 5.2.1 G1, has a single source node Let y be the single source node 
(the ancestor of all other nodes in Gl; and the target of all loop-carried edges in G). We solve this 
case by modifying a copy of G to obtain an acyclic graph G that can be used to produce a relative minimum 
completion time schedule for G as follows: 1. Add dummy sink node z to N. z represents the next Iteration 
instance of source node y. 2. Add a zero-latency, zero-distance edge from every other node to z. 3. 
Remove each loop-carried edge (z, V) in G and replace it with edge (z, z) with distance zero and the 
same latency as edge (a, y),  G is then scheduled as a single basic block using the Rank Algorithm, 
followed by repeated applications of d Procedure Move_IdleSlot. We have proved that this algorithm con­structs 
a minimum completion time schedule for loop L, for this restricted case of a single source node and the 
conditions in which the Rank Algorithm is optimal [1 I]. <1,0> Edge label = <latency, iteration distance> 
EQUIVALENT ACYCLIC GRAPH <0.0> <0,0> Schedule Sl =12 3 12 3 ... (wmpletion tinle = 5n-1 cycles) Schedule 
S2=2 1 3 2 1 3 ... (conlpletion tinle = 411 cycles) Figure 8: Counter-example to using single-source 
solution for multiple sources 5.2.2 Gz, has a single sink node Figure 8 shows that we cannot solve the 
multiple sources problem using the technique of subsection 5.2.1. The equiv­alent acyclic graph is completely 
symmetric with respect to nodes 1 and 2, but it is clear than node 2 should be scheduled first to hide 
the latency of the loop-carried dependence (see schedules S1 and S2 at the bottom of Figure 8). However, 
we can exploit duality to solve the single sink node version. Let y be the single sink node (the descendant 
of all other nodes in Gli and the source of all loop-carried edges in G). We solve this case by modifying 
a copy of G to obtain an acyclic graph G that can be used to produce a relatlve minimum completion time 
schedule for G as follows: 1. Add dummy source node z to N. z represents the previous iteration instance 
of sink node y. 2. Add a zero-latency, zero-distance edge from z to every other node.  3. Remove each 
loop-carried edge (y, z) in G and replace it with edge (z, z) with distance zero and the same latency 
as edge (y, z), G is then scheduled as a single basic block using the Rank Algorithm, followed by repeated 
applications of d Procedure MoveJdle510t. We have proved that this algorithm con­st ructs a minimum completion 
time schedule for loop L, for this restricted case of a single sink node and the conditions in which 
the Rank Algorithm is optimal [11]. 5.2.3 General Case our solution for the generid case is to apply 
the techniques from subsections 5.2.1 and 5.2.2 with different source/sink nodes and select the best 
of the following candidate sched­ ules: 1. For each node that is a target of a loop-carried depen­dence 
edge, apply the algorithm in subsection 5.2.1 using this node as the source node and by creating a dummy 
sink node. 2. For each node that is a source of a loop-carried depen­dence edge, apply the algorithm 
in subsection 5.2.2 using this node as the sink node and by creating a dummy source node.  This solution 
increases the worst-case execution-time com­plexity of the scheduling algorithm by a factor equal to 
the number of instructions in the basic block. This increase may be tolerable since it is required only 
in the case of a loop containing a single basic block, and basic blocks are usually small. For the example 
in Figure 3, the edge from the MULTIPLY instruction to the STORE instruction is the only loop-carried 
data dependence that is not a self-dependence. Schedule 2 at the bottom of Figure 3 is obtained when 
the MULTIPLY instruction is selected as a candidate for the source node in step 1 above. We have proved 
that this algorithm constructs a minimum completion time schedule for loop L, for the restricted case 
in which the Rank Algorithm is optimal [1 I]. For 0/1 latencies, we can reduce the compile-time of this 
optimal solution by observing that only instructions with no predecessors in G[, need to be considered 
as candidate source nodes in step 1, and only instructions with no successors in G~i need to be considered 
m candidate sink nodca in step 2. 6 Related Work Due to space limitations, we only provide a brief summary 
of related work in this section. More details on related work are provided in [1 I]. Scheduling wit h 
0/1 lat encies. Bernstein and Gert­ ner [3] developed an algorithm for constructing an optimal schedule 
for a problem with an arbitrary DAG, unit pro­ cessing time, and latencies of O and 1. Their algorithm 
is a generalization of the approach used by Coffman and Graham for two processor scheduling [5]. The 
Rank Algo­rithm, developed by Palem and Simons [10], also constructs an optimal schedule for scheduling 
an arbitrary DAG with unit processing times and latencies of O and 1 on a single processor. In addition, 
the Rank Algorithm constructs a minimum tardiness schedule if the problem input has dead­lines. The assigned 
processor model. The assigned pro­cessor model most closely approximates the multiple func­tional unit 
processors that are being produced today. In the assigned processor model an instruction must be scheduled 
on a particular functioned unit, or perhaps on one of several functional units of a particular type. 
A VLIW machine can be considered to be a special case of an assigned processor (see for example [6]). 
Heuristic algorithms. Hennessy and Gross [9] have an 0(n4 ) heuristic for instruction scheduling in which 
haz­ards can be introduced by the register allocator and by mem­ory access. The 0(n2 ) heuristic of Gibbons 
and Muchnick [8] deals with register allocation by introducing an edge in the dependence graph to prevent 
instructions that share regis­ ters from overlapping. The approaches of [8] and [2] obviate the need 
for the scheduler to explicitly deal with constraints introduced by register allocation, other than those 
encoded in the dependence graph. Warren s algorithm [12], was implemented in the product compiler for 
the IBM RISC System/6000. It does greedy scheduling on a prioritized list. Since this algorithm targets 
both fixed and floating point processors, the algorithm is designed to construct a schedule for a version 
of the assigned machine scheduling problem. Beyond basic blocks. An approach to increasing the size of 
the basic blocks is to utilize what would otherwise be idle cycles to precompute results. This can be 
done by <RefA>[4] D. Bernstein and M. Rodeh. Global Instruction moving an instruction from later in the schedule 
into an idle Scheduling for Superscalar Machines. SIGPLA Ngl, cycle, so long as executing that instruction 
earlier has no side pages 241-255, 1991. effects. Alternatively, either because it s a problem to keep 
 [5] E. G. Coffman and R. L. Graham. Optimal Schedulingtrack of instructions from both branches or because 
there for Two-Processor Systems. Acts Informatica, (1):200­ are not a lot of idle cycles, the compiler 
can guess which 213, 1972. branch of the conditional will be taken. It can then move instructions from 
that branch that have no side effect into [6] K. Ebcioglu. A Compilation Technique for Software idle 
cycles that precede the conditional. Such a speculative of Conditional of Pipelining Loops with Jumps. 
Proc. move is discussed in [4]. If the compiler guesses correctly, the I?Oth Annual ACM Workshop on Microprocessing, 
then the idle cycles will have been put to good use. pages 69-79, December 1987. A different strategy 
for scheduling across basic blocks in­ [7] J. A. Fisher. Trace scheduling: A technique for global volves 
trace schedrdmg [7]. Trace scheduling uses profiling microcode compaction. IEEE Trans. on Computers, 
C­to compute the path through the program graph that is 30(7):478-490, July 1981. most likely to be taken, 
and then optimizing that path. The [8] P. B. Gibbons and S. S. Muchnick. Efficient Instruction anticipatory 
instruction scheduling algorithms presented in Scheduling for a Pipelined Architecture. Proc. SIG­ this 
paper also work with traces but do not allow instructions PLAN 86 Symp. on Cornpder Construction, pages 
11­ to move beyond basic block boundaries in the generated 16, June 1986. Published as SIGPLAN Notices 
Vol. 21, code. No. 7. [9] J. Hennessy and T. Gross. Postpass Code Optimization 7 Conclusions and Future 
Work of Pipeline Constraints. ACM Trans. on Programming Languages and Systems, 5(3):422-448, July 1983. 
We have presented polynomial-time algorithms for anticipa­ [10] K.V. Palem and B. Simons. Scheduling 
Time-Critical tory instruction scheduling of a trace of basic blocks, with Instructions on RISC Machines. 
Transactions on Pro­ extensions for the case when the trace is enclosed in a sur­ gramming Languages 
(TOPLAS), 15(4):632-658, 1993. rounding loop. The algorithms are shown to be optimal for the Rank Algorithm 
is optimal (0/1 latencies, unit execution [II] Vivek Sarkar and Barbara Simons. Anticipatory In­times, 
and a single functional unit), and can be used as st ruction Scheduling. Technical report, IBM Applica­heuristics 
for larger lat encies and multiple functional units, tion Development Institute (ADTI), 1996. Extended 
version of this conference paper (in preparation). Send As future work, we will do a prototype implementation 
of requests for copies to the authors. these algorithms to compare their effectiveness with known local 
and global scheduling algorithms. [12] H. Warren. Instruction Scheduling for the IBM RISC System/6000 
Processor. IBM J. Research and Develop­ment, pages 85-92, 1990. References [13] Michael J. Wolfe. Optimizing 
Supercompilers for S uper­ [I] A.V. Aho, R. Sethi, and J.D. IJllman. Compilers: cornputem. Pitman, London 
and The MIT Press, Cam-Prtnczples, Techruques, and Tools. Addison-WesleY, bridge, Massachusetts, 1989. 
In the series, Research 1986. Monographs in Parallel and Distributed Computing. [2] M. Auslander and 
M. Hopkins. An overview of the PL.8 Compiler. Proc. SIGPLA N 82 Symp. on Compiler Construction, pages 
22-31, June 1982. Published as SIGPLAN Notices Vol. 17, No. 6. [3] D. Bernstein and I. Gertner. Scheduling 
Expressions on a Pipelined Processor with a Maximal Delay of One Cycle. ACM Trans. on Programming Languages 
and S~stems, 11(1):57-66, January.  130  </RefA>
			
