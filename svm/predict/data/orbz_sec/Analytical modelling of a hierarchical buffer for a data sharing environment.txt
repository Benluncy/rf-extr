
 Analytical for Modelling of a Data Sharing a Hierarchical Environment Buffer IBM Asit Dan Daniel Research 
Division, Yorktown M. Dias and Philip S. Yu T. J. Watson Research Center Heights, NY 10598 Abstract 
In a data sharing environment, where a number of loosely coupled computing nodes share a common stor­age 
subsystem, the effectiveness of a private buffer at each node is limited due to the multi-system invalida­tion 
effect, particularly under a non-uniform data access pattern. A global shared buffer can be introduced 
to al­leviate this problem either as a disk cache or shared memory. In this paper we developed an approximate 
analytic model to evaluate different shared buffer man­agement policies (SBMPS) which differ in their 
choice of data granules to be put into the shared buffer. The analytic model can be used to study the 
trade-offs of different SBMPS and the impact of different buffer al­locations between shared and private 
buffers. The ef­fects of various parameters, such as, the probability of update, the number of nodes, 
the sizes of private and shared buffer, etc., on the performance of SBMPS are captured in the analytic 
model. A detailed simulation model is also developed to validate the analytic model. We show that dependency 
between the contents of the private and shared buffers can play an important role in determining the 
effectiveness of the shared buffer par­ ticularly for a small number of nodes. 1 Introduction Coupling 
multiple systems has become increas­ingly popular for reasons of capacity, availability and cost . One 
method of coupling multiple systems is the data sharing approach [YU87, STR182, DAN90d], also referred 
to as closely coupled clustering [KRON86], where all coupled computer systems have access to shared data 
stored on disks or file servers. Each com­puting system, which we will refer to as node, also has a local 
buffer for recently accessed data. When a node lA~it Ila IL was ~IIppor(, rd in pnlt hy NSF II IIdC-r 
GTcInt, lRi. 8W81393 for thi< work Permission to copy without fee all or part of this material is granted 
provided that the copies are not made or distributed for direct commercial advantage, the ACM Ocopyright 
notice and the title of the publication and its date appear, and notice is given that copying is by permission 
of the Association for Computing Machinery. To copy otherwise, or to republish, requirea a fee and/or 
specific permission. G 1991 ACM 089791-392-2/91/0005/01 56...$1.50 updates a block, any copies of this 
block in the lo­cal buffers of other nodes become invalid. As shown in [DAN90b], this buffer invalidation 
phenomenon can substantially reduce the amount of memory that can be effectively used for local buffering. 
The invalidation rate increases with the local buffer size, the degree of data access skew (i. e., where 
some granules are more likely to be accessed than others), and the number of nodes in the coupled system. 
A global shared buffer can be introduced to alleviate not only the buffer invalidation problem but also 
the proliferation of local copies of the same data. Depending upon the system organization, the shared 
buffer may be physically located at the disk, file server or an intermediate shared memory, In this paper, 
we develop an analytic modeling framework for a hierarchical buffer in such a data shar­ing environment, 
with private buffers at each node and a global shared buffer. While an LRU replacement policy is used 
for the buffer replacement scheme, various shared buffer management policies (SBMP) are then proposed 
and analyzed that differ in their choice of data granules (i.e., pages or units of transfer between disk 
and mem­ory) to be placed in the shared buffer. The effect of skewed data access patterns on the buffer 
hit ratio, and its implications to the SBMPS are also examined. The major complexity encountered in analyzing 
these poli­cies comes from the fact that the same data may appear in both the private and shared buffers. 
This correlation in the buffer contents depends on the shared buffer poli­cies and other system parameters. 
For example, placing a granule that caused a shared buffer miss in both the local and shared buffers 
results in a positive dependency between the contents of the two buffers. Thus, a miss in the local buffer 
is more likely to also be a miss in the shared buffer as compared to the case where the local and shared 
buffer contents are uncorrelated. As another example, consider the case of an update at a remote node 
that is also propagated to the shared buffer. This results in a negative dependency between the contents 
of the local and shared buffers, since the copy of the updated granule is invalidated at the local buffer 
and is propagated to the shared buffer. In general, a policy may introduce both positive and negative 
dependencies 1.56 between the contents of the local and shared buffers, and the shared buffer hit probability 
will depend on the re­sultant effect. The analysis captures these dependencies and the intricacies of 
the different SBMPS. A detailed simulation model is also developed to validate th,e anal­ysis. Although, 
the analysis is in some sense simple, what is striking is the close match between the analysis and simulation 
for such a complex environment. our model is most applicable to the transaction processing environment, 
where the data reference string can more or less be considered independent (IRM), i.e., each ac­cess 
to the database is independent of all other previous accesses [DAN90a, DAN90b]. The model has been aP­plied 
and validated in a study of the proper allocation of buffer between the private and shared buffers for 
a transaction processing environment, where the trade­offs include the delay and overhead of accessing 
shared buffer versus the buffer hit probability [DAN90c]. There are few existing analytical buffer models 
particularly for a multi-system environment. This is especially the case for hierarchical buffers with 
buffer invalidation and in the presence of data skew. In [DAN90b] the buffer hit probability and response 
time for a data sharing environment is examined in the pres­ence of skewed data access when there is 
no shared global buffer. Previous models of the database buffer for multi-system data sharing [YU87, 
DIAS88] have been empirical, based on trace driven simulations. Ncj stud­ies of the performance of the 
shared buffer manage­ment policies currently exist. In [PRZY89] a l~ierar­chical cache organization is 
studied where optimal al­location of primary and secondary caches to minimize the overall response time 
is studied. The model is ex­tremely simple as the environment it assumes has none of the complexities 
of the data sharing environment. In [CHER89] a trace driven simulation results are reported for a multi-level 
shared cache organization where the number of access to the shared cache is reduced by in­troducing a 
local shared cache which is shared only by a few processors. The model for the shared buffer for the 
present environment needs to capture not only the ef­fect of skewed access, LRU replacement poiicy, effect 
of invalidation but also the effect of dependence between the contents of the private and shared buffers. 
The paper is organized as follows. Section 2 out­lines the data sharing environment and various shared 
buffer management policies. In Section 3, an analyti­cal modeling framework is developed to analyze various 
buffer management policies. Model validation, and pro­jections are described in Section 4. A summary 
and concluding remarks appear in Section 5. Data Sharing Environment The data sharing environment consists 
of multi­ple loosely coupled nodes sharing a common database at the disk level. To improve the data access 
time and to reduce the number of disk 1/() operations re­ quired, each node maintains a private buffer 
to cache some of the data granules. Hence, copies of the same granule may be present at more than one 
node. Due to the multi-system invalidation effect, the maximum amount of private buffer that can be effectively 
utilized is limited [DAN90b]. Hence, in some system architec­tures an additional global buffer is provided 
that may be shared by all nodes through an interconnection net­work. Depending on the the system organization, 
the shared buffer may be physically located at the disk, file server [CHER89] or intermediate shared 
memory [DIAS89]. The network connecting the shared buffer to the processors may be the same as the 1/0 
network as in the case of the disk cache or a special purpose inter­connection network as proposed in 
[DIAS89]. The delay and overhead of accessing the shared buffer will depend on the specific physical 
organization. Our focus here is not to study various architectural trade-offs, but to de­velop a modeling 
methodology to predict the buffer hit probability under various buffer management policies. We assume 
that the LRU (Least Recently Used) buffer replacement scheme is used for managing both private and global 
buffers. However, what is put into the shared buffer will depend on the SBMP detailed later. 2.1 Private 
Buffer Management Policy To access a data granule, a request is made to the private buffer manager. The 
private buffer manager returns a copy of the granule if the granule is present in the buffer. Otherwise, 
a copy of that granule is brought in from the shared buffer or the shared disk to the pri­vate buffer. 
In either case, the newly accessed granule is placed at the top location of the private LRU stack. In 
the case where a new granule is brought in from out­side the node (disk or shared buffer), if there is 
no free buffer available, then the granule at the bottom of the LRU stack is pushed out of the stack. 
For updates, the identities of the updated granules are broadcast to all remote nodes so that each remote 
node can invali­date the old copies of the updated granules if present in their private buffers. (We 
do not detail here the inter­action of concurrency and coherency protocols, except that we assume page 
level locking. The details of the interaction of concurrency and coherency protocols can be found in 
[DIAS89].) At the remote nodes, the inval­idated buffer locations are placed at the bottom of the LRU 
stack and are made available for bringing in new granules. Depending on the SBMP the updated gran­ ules 
either replace any copy that may be present in the shared buffer, or are purged from the shared buffer. 
2.2 Shared Buffer Management Policy For the policies examined here, a granule may be placed into the 
shared buffer following one or more of the events described below: Database Update (U): Updated granules 
are good candidates to be put into the shared buffer for the fol­lowing reason. old copies of the updated 
granules are deleted from the remote buffers and any subsequent ac­cess to that granule at the remote 
nodes will result in a buffer miss. When access pattern to granules is skewed, the high invalidation 
rate of the hot granules results in a low probability of private buffer hit. With the addi­ tion of the 
shared buffer, these updated granules will be found in the shared buffer with a high probability. Note 
that if the updates are not propagated to the shared buffer, then the old copies of the updated granules 
in the shared buffer need to be invalidated since they are in the private buffers that are remote to 
the updating node. Shared Buffer Miss (M): If a requested granule is not found in either the private 
buffer of its local node or the shared buffer, then the granule is fetched from the disk to the private 
buffer of the requesting node. Under this policy, the granule is also placed in the shared buffer. In 
the pure version of this policy, where the updates are not propagated to the shared buffer, the old copies 
of the updated granules in the shared buffer need to be invalidated. Private Buffer Replacement (R): 
Here, the clean (i.e., not updated) granules that are replaced from a private buffer are placed into 
the shared buffer, if it is not already present in the shared buffer. Hence, from the point of view of 
a node, the shared buffer can be thought of as a continuation of the LRU chain. How­ever, the replacements 
of remote buffers are not related from the point of view of any particular node. With this policy, the 
less frequently accessed granules are pushed out of each private buffers and are placed in the shared 
buffer. The advantage of this policy is not clear. In the pure version of this policy, the old copies 
of the updated granules in the shared buffer need to be invalidated as in the pure miss policy. A SBMP 
may choose to propagate a granule fol­lowing one or more of these events. This results in 7 dif­ferent 
shared buffer policies, namely Pure Update (U), Pure Miss (M), Pure Replacement (R), Update + Miss (UM), 
Update + Replacement (UR), Miss + Replace­ment (h4R) and Update + Miss + Replacement (UMR). 3 Modeling 
Framework for SBMPS To analyze various SBMPS, we have to determine the rates at which the hot and cold 
granules are placed into the shared buffer by these policies as well as the in­validation rates to hot 
and cold granules in the shared buffer. As discussed earlier, the analysis is complicated by the fact 
that there may be significant (positive and negative) dependencies between the contents of a pri­vate 
and the shared buffers for some SBMPS, particu­larly if the number of nodes in the system is small. In 
this section, we first develop a modeling framework to analyze these policies ignoring dependencies. 
Usiong this simple analysis as the base, we then extend the mod­elling framework to handle both positive 
and negative dependencies between the contents of the private and shared buffers. The analysis is presented 
from the view point of a particular private buffer. For example, the number of replicated granules in 
the shared buffer is in relation to only a particular private buffer, not to all or any private buffers. 
The policies that do not include updates (M and R) are harder to analyze, since they introduce ad­ditional 
dependencies than those that will be captured in the modeling framework. As will be seen from the graphs, 
these two policies also perform poorly. Hence, we will not attempt any further finer analysis of these 
two policies. Even though the modeling methodology can be used to analyze all composite policies, for 
rea­sons that will be clear later, we will focus our attention only on the pure policies (U, M and R) 
and then on two composite policies that include updates (UM and UR). 3.1 Analysis of the Policies Ignoring 
Dependency Assume that the data sharing system consists of IV loosely coupled nodes and the total number 
of data granules is D. To model skewed access, we further as­sume that the data granules are grouped 
into M parti­tions, such that the probability of accessing any granule within a partition is uniform. 
Let ~, denote the frac­tional size of partition z} i.e. the size of partition z is /3LD. Let a, denote 
the probability that any data ac­cess lies in partition i. The probability that a granule access from 
partition z is also updated is denoted as pti, . Each SBMP has a corresponding stream of refer­ences 
to be considered for placement into the shared buffer. For example, for the U policy the reference stream 
is formed by the updates only while for the M policy, it is formed by all the misses from the private 
buffers. For each reference in the reference stream un­der a given SBMP, the shared buffer is checked 
for the existence of that granule and the granule is added to the shared buffer if not already in the 
shared buffer. In either case, it is placed at the top of the LRU stack associated with the shared buffer. 
This stream of ref­erences corresponding to each SBMP is used to provide an estimate of the buffer hit 
ratio for each partition (or the mean number of granules for each partition in the shared buffer). From 
the reference stream, the ref­erence rate for the granules of each partition, i.e., the rate at which 
the granules of each partition appear in the reference stream, can be derived. These rates along with 
the invalidation rate to each partition can be used to estimate the buffer hit probability for each partition 
using the model of [DAN90b]. (Note that for SBMPS excluding the M policy, the private buffer miss causes 
another access stream to the shared buffer, For each private buffer miss, the shared buffer is checked 
for the requested granule. If it is present in the shared buffer, it is forwarded to the requesting node. 
However, a miss in the shared buffer does not cause the granule to be brought into the shared buffer. 
Since this stream does not change the shared buffer content it will be ignored in the subsequent discussion. 
) Let # denote the rate at which granules of the Zth partition are referenced at the shared buffer, and 
let ~~ denote the rate at which granules of Jh partition are invalidated from the shared buffer. The 
overall rate of granule access is assumed to be one reference per unit of time per node. Define Y, to 
be the average number of granules of partition i present in the shared buffer. Sim­ilarly, let X! be 
the average number of granules of parti­tion i present in the private buffer. We define the shared buffer 
hit probability, H?, as the conditional probabil­ity of finding a granule in the shared buffer given 
that it is not found in the private buffer of the requesting node. Ignoring the dependency between the 
contents of the private and shared buffers, the buffer hit H,S for each partition in the shared buffer 
can be expressed as T, (1) Similarly, buffer hit ratio H,p for each partition i in a private buffer can 
be expressed as H: . ~, i= M. 1,...,(2) ~,D The overall shared buffer hit probability y, Hs, is approx- 
Here a,(l H,P) is the rate of accesses to the ith par­tition of the shared buffer. X, can be derived 
using the LRU model in [DAN90b] based on the access rate, a,, and the invalidation rate, Qipu, (A 1), 
to partition i. Similarly, X can be derived based on the rates, y: and ~f. The rates, -y,s and ~~, for 
all policies are given as follows. U Policy: ?: = a,pu, N; -y: = O. Updates of all nodes are kept in 
the shared buffer. The shared buffer does not suffer from invalidation since updates are clirectly kept 
in the shared buffer. I_ M Policy: V: = a~(l H8P)(1 pu,)~; 7, Q tPIL,N. Only the granules that are 
not found in the private buffers are brought to the shared buffer from the disk. However, out of those 
granules, the ones that will be updated will be erased from the shared buffer shortlY by the invalidation, 
and hence, are not included in ~, . Note that if the shared buffer size is very small, the above approximation 
will cause an error. R Policy: y? = T~(l P~, )N; -Y/ = cY,P~, N, where r: is defined as the replacement 
rate from the private buffer for the ith partition. Here, ~~ is determined dur­ing the solution of the 
LRU model for the private buffer. We observe that ~~ is a function of the private buffer size 13p. If 
a granule to be replaced out of a private buffer is not an updated granule, then it is propagated to 
the shared buffer. The rationale here, is that, in the case of a composite policy that includes updates, 
the updated granules are already propagated to the shared buffer. UM Policy: V: = a,((l H,p) + H,PP., 
)N; ? ,I=tl. Here, the invalidation rate is zero since updates are di­ rectly propagated to the shared 
buffer. The granules that are brought into the shared buffer include both the private buffer misses and 
all updates. UR Policy: y? = Q,[(PU, +T:(l -PU, )]N; T: = O. Up­dates are directly propagated and only 
the non-updated replacement pages are propagated to the shared buffer. 3.2 Analysis Capturing Dependency 
From the point of view of any particular pri­vate buffer, the data streams to the private and shared 
buffers can be decomposed into four kinds of indepen­dent data streams that need to be considered to 
deter­mine the dependency between granules in the private buffer and those in the shared buffer. Data 
stream P goes to both the private and shared buffers. This introduces a positive dependency be­tween 
the contents of the private and shared buffers, and hence reduces the shared buffer hit probability (con­ditional). 
Hence, this data stream is denoted as data stream P. Data stream N goes to the shared buffer and any 
data corresponding to this stream is erased from the private buffer, if present. This stream introduces 
a neg­ative dependency between the contents of the private and shared buffers. Whereas the P stream reduces 
the shared buffer hit probability, the N stream on the other hand increases it. This is because the contents 
of the private and shared buffers are negatively related and in the event that a granule is not found 
in a private buffer, it is more likely to be found in the shared buffer. We will denote this data stream 
as N. Data stream IS goes to the shared buffer only. Data brought into the shared buffer by this stream 
is not de­pendent on the contents of the private buffer. IS stands for an Independent stream to Shared 
buffer with neither positive nor negative dependency to the contents of the private buffer. Data stream 
1P goes to the private buffer only. Data brought into the private buffer by this stream is not de­pendent 
on the contents of the shared buffer. 1P stands for an Independent data stream to Private buffer with 
neither positive nor negative dependency to the contents of the shared buffer. Note that the data stream 
to the shared buffer can be decomposed into three component streams P, N and IS while the data stream 
to the private buffer can be decomposed into two component streams 1P and P. We define a replicated granule 
as a shared buffer gran­ule that is also present in a particular private buffer. Let G, be the average 
number of replicated granules of partition i. Assuming that an access does not lie in the set X, (i.e., 
a private buffer miss), the replicated granules corresponding to this set that are present in the shared 
buffer need to be subtracted, in order to determine the conditional shared buffer hit probability. Hence, 
the conditional shared buffer hit probability, 11~, is exDressed as . K G, H: . 2= 1,..., M. (4) D.D 
X. To derive G,, we Wiil determine the average number of replicated granules in the shared buffer corresponding 
to the granules brought in by each data stream. Let G~, G! and G~S be the average number of replicated 
granules of partition i due to streams P, N and IS re­spectively. Then, G, = G? +G~ +G:s, i = I,...JM. 
(5) (Note that stream 1P does not reference the shared buffer.) Let vT, -y,N, ~~s and -y/P be the reference 
rates of the four data streams to partition t respectively. These reference rates for the five policies 
are derived in Section 3.2.1. We then classify the granules in each par­tition i into three categories 
based on the data stream which has brought them in. In Section 3.2.2, the av­erage number of partition 
i granules in each category is estimated based on the reference rates of these data streams. Finally, 
we derive approximations for G?, G: and G~s in Section 3.2.3, based on the results in Sec­tions 3.2.1 
and 3.2.2. 3.2.1 Reference Rates of the Data Streams under Various SBMPS In this subsection, we describe 
the component rates, 7ZP, 7,N , 7tIs and --y, p for the five selected poli­ties. Note that -y,S, defined 
in the previous section, is the sum of ~,p, ~,N and -y~s. U Policy: ~~ = a;pw,; --y,N = a,pU1(IV 1); 
and 1s = O Local updates cause positive dependencies, Y, . and the remote updates cause negative dependencies 
be­tween the contents of the shared and private buffers. M Policy: ?: = Q,(1 J7,p)(l p,,); ~tN = O; 
and 1S = Q,(I H,p)(l ~W,)(IV 1). Local non-update ? , private buffer misses cause positive dependencies, 
and remote non-update private buffer misses cause neither positive nor negative dependencies, referred 
to hereon as random dependencies, between the contents of the  shared and private buffers. R Policy: 
-y,p=o;~, N = ~~(1 p.,); and -y~s = ~~(1 PU, )(N I). Local buffer replacements cause neg­ ative dependencies 
and similar items from remote nodes cause only random dependencies between the contents of the shared 
and private buffers.  UM Policy: T: = at[(l Htp)(l P., ) +Pu,l; T,N = a,pz, (iV -1); and -y/s = Q,(1 
H,p)(l pW, )(N 1). Local updates and private buffer misses cause positive dependencies, remote updates 
cause negative dependen­cies, and remote non-update private buffer misses cause only random dependencies 
between the contents of the shared and private buffers. UR Policy: -y: = a,pu,; y: = a,pul(lV 1)+~~(1 
 P~,); and ~, s = Cp(l %<)(N 1) Local updates cause positive dependencies, remote updates and local 
replacements cause negative dependencies, and remote replacements cause only random dependencies between 
the contents of the shared and private buffers. Furthermore, under all policies, ~~p = a, -y:. 3.2.2 
Composition of the Shared and Private Buffers Let us take a closer look at the contents of the shared 
buffer. We classify the granules in the shared buffer into three categories: stream P, stream N, and 
stream IS granules. Assume that the following gran­ule labelling mechanism is used at the time a granule 
is referenced. If a granule is referenced by data stream P (respectively, data stream N), it is labelled 
as a P granule (respectively, N granule). A granule referenced by data stream IS is labelled as an IS 
granule if it is not already present in the shared buffer; otherwise, the reference is ignored and the 
original label is kept. As­suming that the granules are not flushed out due to the limited buffer size, 
the P granules contain all granules that at one point are brought into both the shared and the private 
buffers and are not yet accessed by a stream N reference. (These granules may subsequently be ref­erenced 
by data stream lS. ) Under a similar labelling definition, the category of stream N granules contains 
all granules that at one point are explicitly deleted from the private buffer and placed only in the 
shared buffer, and have not been brought back to the private buffer by subsequent references in data 
stream P. The quantity ~ is the sum of three components, i.e., ~ = ~p +~N +Y~s, where ~p, XN and YIS 
are the expected number ~f granules of partition i in the three categories, respectively. Based on the 
labelling mechanism described above, the rate at which a gran­ule is labelled P (respectively, N) is 
yip (respectively ~,N). The rate at which a granule is labelled IS is not ~~s} since it is contingent 
on the granules referenced by data stream IS not being present in the shared buffer. Since the probability 
of referencing an existing granule of partition i in the shared buffer is ~ the effective ~,D> rate that 
a granule in partition i is labelled as a stream IS granule is y~s(l ~). We now examine the relationship 
between the ef­fective reference rates from the three streams and the shared buffer composition. We show 
that under the above labelling mechanism, the shared buffer composi­tion of P, N and IS granules of partition 
i is proportional to the effective reference rates of the three data streams to partition z granules. 
The explanation is based on the assumption of the conservation of flow stated below. Conservation of 
Flow Assumption: Under 160 steady state the new granule (i.e. a granule not currently in the shared 
buffer) input rate to each category (P, N, or IS) is equal to its replacement (i.e. flushed out) rate 
and the composition o} the shared buffer at jiush out should resemble its composition at the introduction 
of a granule. Note that a reference from the three streams does not necessarily bring in a new granule 
to the shared buffer. For a stream P or N reference, either (1) an ex­isting granule in the shared buffer 
is accessed ancl may be relabeled} or (2) a new granule is brought in and an existing granule is flushed 
out. In the first case, the distribution of the original classification or label of the referenced granule 
resembles the composition of parti­tion i in the shared buffer. (This is due to the fact that the granule 
reference within each partition is gen­erated independently based on an uniform distribution. Hence, 
the probability that a P (respectively, N or IS) granule is referenced is proportional to the number 
of P (respectively, N or IS) granules of partition i in the shared buffer. ) In the second case, over 
time the num­ber of misses in partition i will roughly be equal to the number of flushed out granules 
in partition z. The dis­tribution of the classification or label of the flushed out granules in partition 
i should also resemble the cc,mpo­sition of partition i in the shared buffer. (This is due to the conservation 
of flow assumption mentioned above.) For stream IS references, only new granule references are considered. 
The effective rates at which granules are labelled into the three categories (P, N and IS) are proportional 
to the composition of partition i in the shared buffer, Therefore, yp=+f -  XN = S::ed, K fha ed  (1-$)7: 
, and Y 1 =y (6) 7:ha ed where -y, ha ed = ? : + ? tN + (1 2?%)7:s. Next we consider the composition 
of the contents of the private buffer. The private buffer has two refer­ence streams: data stream 1P 
and data stream P. We can classify the private buffer content into two cate­gories: 1P granules and P 
granules, using a similar la­belling mechanism. If a granule is referenced by data stream P, it is labelled 
as a stream P granule. For a granule referenced by data stream 1P, it is labelled as an 1P granule if 
it is not already present in the private buffeq otherwise, the reference is ignored and the origi­nal 
label is kept. Thus the P (respectively, 1P) granules in the private buffer are similar to the stream 
P (respec­tively, IS) granules in the shared buffer. Then, X, can be considered to consist of two components 
X~p, X: corresponding to the number of 1P and P granules of partition i, respectively. Therefore, p,,v.te 
= f + (1 *)7:5where -y, 3.2.3 Estimation of the Average Number of Replicated Granules We next estimate 
the average number of repli­cated granules present in both a particular private buffer and the shared 
buffer, i.e., the number of shared buffer granules also appearing in a particular private buffer. The 
stream IS and 1P granules (in shared and private buffers respectively) are generated independently; the 
probability that a stream IS granule also appears in a particular private buffer is denoted as p[r. Note 
that a stream N granule that is deleted from the private buffer may subsequently be brought back into 
the pri­vate buffer by the data stream 1P. The probability that a stream N granule also appears in the 
private buffer depends upon when the granule was deleted from the private buffer. Asymptotically, this 
conditional proba­bility will approach the unconditional probability, p~N (different from the replication 
probability of the inde­pendent stream IS granules since the N granules can be relabeled as P granules). 
Alt bough stream P granules at one point in time appear in both the private and shared buffers, they 
may be flushed out from the two buffers independently at some later time. These fac­tors complicate the 
estimation of the average number of replicated granules from streams P and N granules. Stream P Granules: 
G~ is the average num­ber of replicated stream P granules. Although stream P granules are placed in both 
the private and the shared buffers initially, they can independently be flushed out from either buffer. 
(A stream N reference to a replicated stream P granule changes its labelling in the shared buffer and 
deletes it from the private buffer, and is therefore irrelevant to this discussion. ) We will assume 
for the purposes of the following analysis that the rela­tive order of the P granules in the respective 
LRU stacks of the private and the shared buffers are the same. We then divide the P granules in the shared 
buffer into two sets: 1) PP granules that are retained in both the pri­vate and the shared buffers, and 
2) PN granules that are present in the shared buffer (at the bottom of the P stream) but are flushed 
out of the private buffer. The PN granules are negatively dependent on the con­tents of the private buffer 
and these granules may later be brought back by the data stream 1P to the private buffer. Let the average 
sizes of the above two sets be G~P and G/ N respectively. Therefore, G: = G:p i-G:N, (8) where by definition, 
G;p = MIN(X~, ~p). (9) Note that PN granules exist in the shared buffer only if Y,p > G~p. The estimation 
of G~N will be given following the estimation of G?. Stream IS Granules: G!s is the average num­ber of 
replicated granules corresponding to the granules Y%Is. By definition, p~I is the probability that a 
stream 1S granule is present in the private buffer. Hence we estimate, G~s as, Let us first consider 
the case that G~p = X,p. Since both data streams P and N are mutually disjoint from the data streams 
IS and 1P, IS granules can appear only in the stream 1P. As the data streams IS and 1P are independent, 
p~I can be estimated as, (11) Here, the denominator /?iD -~p -~N simply repre­sents the mean number 
of granules not in P or N streams and the numerator represents the average number of 1P granules in a 
private buffer. The formula also holds for the case X,p > G, p, but the interpretation is slightly different 
since the potential candidate granules for a match in the private buffer also include the unmatched granuIes 
at the bottom of stream P (implied by the dif­ference between X,p and G~p ). Stream N Granules: G: includes 
the set of replicated granules corresponding to the set Y,N. Recall that the data stream N, retains a 
granule in the shared buffer while simultaneously deleting a copy of that gran­ule from the private buffer 
if present. Hence, very few granules corresponding to this set will be present in the private buffer, 
Note that the longer a stream N granule is in the shared buffer, the higher the probability that it is 
brought back to the private buffer. Our strategy here is to determine for each location in the shared 
buffer, the probability that it contains a granule of partition z and was labelled a stream N granule, 
and the probabil­ity that such a granule is brought back into the private buffer. Let p:(j) be the probability 
that location ) in the shared buffer contains a granule of partition i. Here P?(i) is obtained as a byproduct 
of the LRU analysis of the shared buffer. Therefore, the probability that lo­cation j in the shared buffer 
contains a N granule of partition i, p~(j), is estimated as, Iv (12) P:(j) = 2i17)*. Let z denote a particula~ 
N granule of partition z residing in location y of the shared buffer. The mean time taken by z to reach 
location j of the shared buffer since it was first brought to the top of the the LRU stack, t(~), is 
given by ~ t(j)=~ /~ . (13) ,=,z,=,T, (~) where T?(k) is the push down rate of the ith partition at 
location k in the LRU stack, and A/f is the number of data granule partitions. Thus, the summation term 
in the denominator is the total push down rate at location k of the shared buffer and its reciprocal 
is the average time a granule spends at that location. Let p~ c~(j) be the probability that a copy of 
granule z will be brought back into a private buffer during the time duration t(j) by data stream 1P. 
We estimate p~ack(j) as follows. 1P be the rate at which new granules are Let q, brought to a private 
buffer by the data stream 1P. It is given by the expression % p= (1 -~)~:p. (14) , Let Z be the random 
variable denoting the number of granules brought by the 1P stream to a private buffer during the time 
t(j). The expected value of Z is t(~ )~~p. By definition, granule z was deleted from the private buffer 
when it was first brought to the shared buffer. Therefore, z lies in the set of granules of partition 
that are not present in the private buffer, and the size of this set is given by (~,11 X,). The probability 
that a new granule brought by the 1P stream is a copy of z is l/(~$D X$). Therefore, the probability 
that z lies in the set of the new granules brought by the 1P stream during the time duration t(j) is 
approximated as p: (j) = E 1 -(I ­ [ m+-dz] % 1 (1 &#38;-#zl. The above approximation holds since the 
probability mass of Z is concentrated around its average. A sim­ilar approximation technique is used 
to estimate the abort probability under OCC protocol in [YU90] and was found to match well with simulation 
results. As the expected number of new granules brought to the private buffer, ~~pt(j), increases, the 
probability that z lies in this set approaches unity. However, due to the limited size of the private 
buffer, only a subset of the set q,~pt(j) can be retained, i.e., the granules that are brought in later 
replaces the granules brought in earlier. Hence, there is an upper limit on the probability that a copy 
of z is brought back and retained in the private buffer. Let p~N be the upper limit on the probability 
that c is replicated in the private buffer. As z cannot lie in the set G/ p, p~N is given by the expression 
.7 ,-YP7J A, -(J; TN = P, (15) ( /3,D G~p Therefore, )t(j)n:= , P:N p: (j) = MIN 1 -(1 -1 ( /3,D-Xi 
) Summing up the replication probability of all N granules in the shared buffer, we get (16) PN Granules<=~e 
now estimate the average number of replicated granules in the private buffer cor­responding to the PN 
granules in the shared buffer. The analysis is very similar to that for the N granules except for the 
estimation of the time duration since a PP gran­ule is relabeled as a PN granule. Let p:(j) be the probability 
that location j in the shared buffer contains a granule of partition i and is labelled P. by P:(o= Tw-P 
Let w be the minimum i.e., when a P granule buffer, its counterpart out and the P granule as PN. Let 
y be such mean time taken by w. It is given by the k such t~at ~~=1 reaches location w in the private 
buffer in the shared buffer a PN granule. Let y to reach location j expression This is given (17) p:(j) 
> G~p, in the shared gets flushed is relabeled t N(j) be the from location (18) Therefore, the probability 
that granule y is brought back and retained in the imated by backF IV(j) =MIN P, Finally, G;N Validation 
In this SBMPS using simulates the for both the then explore The simulation data collection private buffer, 
1 -(1 ­ ~ ( B. p~ac~PN(j), is approx­ section we will validate our analysis of a detailed simulation 
model that explicitly buffer placement and replacement policies private and the shared buffers. We will 
various results using the analytical model. consists phase. phase was chosen such any buffer is at least 
an the buffer size. Since t (20) = ~P:(,)Pb kpN(,) j=w and Results (lQ) ,.-, of two The that order 
only phases: initializaticln and duration of initialization the number of accesses to of magnitude higher 
than a fraction of the data ac­ cesses may result in shared of private buffer miss) this long for certain 
parameter tively, we estimate the hot rates to the shared buffer buffer accesses (in the event duration 
may be excessively range and policies. Alterna­and cold granule placement using the analytical approxi­ 
mation, and then fill up the shared buffer according to that ratio. This is followed by the regular initialization 
process, whose duration was chosen such that the num­ber of accesses to each of the private buffer is 
20 times the size of the private buffer. In the graphs of the shared and local buffer hit probabilities 
shown in this section, confidence intervals were estimated using the method of batch means [LAVE83], 
and the 95% confidence interval was estimated to be within 370 of the mean. We will first focus our attention 
on the proper se-Iection of SBMPS that perform well and are also robust for a wide range of parameters, 
namely the number of nodes in the system, the probability of update (of the hot and cold granules, the 
sizes of private and shared buffers, and the skewness sume that the size of the the probability of update 
and cold granules is 0.3. 4.1 Selection of the in the database for each SBMP We first compare the three 
the U, M and R (see, Section 2 for to understand the effect of keeping ules in the shared buffer. We 
will access pattern. We as­as 20000 granules and access to the both hot pure policies, namely description), 
in order various types of gran­show that any SBMP that does not include updates will perform poorly. 
We then narrow our attention. to only three SBMPS namely the U, UM and UR policies. (Note that the MR 
policy does not include updates, and hence performs poorly. The UMR policy has a similar performance 
to the UM policy while incurring more traffic to the shared buffer,) The performance of the three selected 
SBMPS are very close if the number of nodes in the system is large, the probabilities of update of the 
hot and cold granules are a private ~~ ~ p:N number of nodes, the effect of update  ) pN(J)qf , t 1 
) the relative sizes of private and shared the same and the private buffer size is the effect of data 
dependency between and the shared buffers in the these policies apart, and the selection of an SBMP will 
depend on the criteria of robustness of performance for an wide range of parameters or the best for a 
specific range of parameters. 4.1.1 Comparison of Figure 1 examines probability to the shared cies for 
the 16 node case. of all the private buffers Pure Policies the sensitivity of buffer size under The total 
buffer and system is kept constant (1OK shared buffer size is increased, each of the nodes is decreased. 
bination of private and shared buffer hit probability increases the shared granules). the private This 
covers small. However, the contents of case of a small probability and buffer will set performance the 
buffer hit the pure poli­size (i.e., sum buffer) Hence, buffer a large buffer sizes. The with the shared 
size for all the policies, and the U policy performs in the as the size in com­shared buffer bet­ ter 
than the other two policies. Also shown in Figure are the private buffer hit probabilities. Comparing 
the analytical estimates of the buffer hit probabilities with the simulation results shows a good match 
for both the U and M policies, and some discrepancy for the R policy for intermediate shared buffer sizes. 
The reason is that the R policy has other types of dependencies between the contents of the private and 
shared buffers than have been accounted for in our model. Under the R policy, an updated granule is not 
propagated to the shared buffer until it is replaced from the private buffer. Hence, all the updated 
granules present in the private buffers will continue to be missed by the remote nodes, and this reduces 
the shared buffer hit probability. The effect is more pronounced at the higher invalidation rate of the 
16 node case. However, as the above graph indicate, the M and R policies are worse than the U policy, 
and it is clear that any policy that does not include updates will perform poorly. The exception is the 
situation when there are very little invalidations (updates) in the sys­tem. However, in this situation 
the UM and UR policies will be equivalent to the M and R policies, respectively. Therefore, we do not 
attempt to further improve the ac­curacy of the model for the M and R policies. Further, as we will see 
later, the performance of the U policy is very sensitive to the probability of updating a granule, and 
its performance is poor for small update probabili­ties. In order to exhibit a reasonable performance 
for a wide range of parameters the SBMP must be a compos­ite policy. Therefore, in the remainder of this 
paper we will focus our attention on the following two composite policies, UM and UR, as well as the 
U policy. 4.1.2 Comparison of U, UM, UR SBMPS Figures 2 and 3 compare the buffer hit proba­bilities of 
the three selected policies in terms of their sensitivity to buffer allocation between the shared and 
private buffers for systems with the number of nodes as 16 and 2 respectively. The update probabilities 
for the hot and cold granules are taken to be equal (0.3). Figure 2 is for the case of 16 nodes. In contrast 
to the M and R policies in Figure 1, the UM and UR policies have performance very similar to the U pol­icy. 
However, as we shall see, the dependency in the buffer contents between private and shared buffers and 
other system parameters will set these policies apart. For the case of two nodes (Figure 3) the graph 
shows that the buffer hit probability for the U policy is no worse than that for the selected composite 
policies. The curves have two points of inflection (i. e., are S shaped) rather than exhibiting the concave 
behavior of perfor­mance curves corresponding to a system where a re­source saturates. The reason for 
this is the dependency between the granules in the private and shared buffers that depresses the shared 
buffer hit probability for in­termediate sizes of the shared buffer. For 16 nodes (Fig­ure 2) the shared 
buffer hit probability has a more typi­cal saturation curve because the dependency between the contents 
of the private and shared buffers decreases considerably. Note that if the dependency is ignored as in 
Equation 1, the shared buffer hit probability curves in Figure 3 would not have the S shape but would 
look like those in Figure 2 instead. Further, the match be­tween the analysis and the simulation is quite 
good for all three policies. Except for the end-points of this fig­ure, the shared buffer hit probability 
of the UM policy is lower than that for other two policies. This is due to the large positive dependency 
between the granules in the private and shared buffers for the UM policy, be­cause the missed granules 
are placed in both the private and shared buffers. This effect is explored further under various skewed 
access workloads for the UM policy. The effect of dependencies and the probability of update are further 
explored in the remainder of this subsection. Effect of Dependency: Specifically, we exam­ine four cases: 
(i) 50-50, i.e. uniform access (5070 of the accesses goes to 50% of the granules), (ii) 50-20, i.e. 5070 
of the accesses goes to 20~o of the granules, (iii) 50-5, i.e. 50% of the accesses goes to 5% of the 
database, and (iv) 80-20, i.e. 80~o of the accesses goes to 20% of the database. The first three cases 
differ in the size of the hot-set. Cases (ii) and (iv) differ in the access rate to the hot-set. In Figure 
4, the UM pol­icy is examined for the four cases. The curves with a positive slope represent the buffer 
hit curves for the shared buffer, while those with the negative slope are for the private buffers. Once 
again, we note that the match between analysis and simulation is excellent. The S-shaped characteristics 
is again due to the positive dependency in the contents of the private and shared buffers. As the shared 
buffer size is increased, beyond a certain point (Yip > G:, i.e., when the granules at the bottom of 
the Stream P granules in the shared buffer are not replicated in the private buffer), the increase in 
the shared buffer hit probability improves dramatically since most of the additional shared buffer locations 
go toward holding N and IS granules. Note that this effect is observed even for the uniform access pattern 
(50-50 workload) and is most pronounced for a highly skewed access pattern (80-20 workload). Figures 
5 and 6 examine the effect of dependency between the contents of private and shared buffers vary­ing 
the number of nodes in the system. In Figure 5 the three selected policies are compared for shared buffer 
sizes of 2000 (lower set of curves in each figure) and 5000 (upper set of curves) in the lower and upper 
sets of curves, respectively, and the workload access pat­tern of 80-20. The private buffer size per 
node is kept constant (1000) for all cases. The same set of curves is shown for the uniform access pattern 
in Figure 6. In all cases, the UM policy suffers the most from the positive dependency effect for a small 
number of nodes. As the number of nodes increases this effect becomes less sig­nificant. The dependency 
effect is also more significant when private and shared buffer sizes are comparable; the effect is more 
pronounced for shared buffer size of 2000 than for the size of 5000. Next consider the UR policy. As 
the number of nodes increases the invalida­ tion rate in the private buffer also increases and this in 
turn reduces the replacement rate for the hot granules from the private buffer. That is to say the replacement 
granules from the private buffer are mainly cold gran­ules, thus reducing the shared buffer hit ratio. 
Hence, the shared buffer hit probability under the UR policy decreases with an increase in the number 
of nodes. For the uniform case (Figure 6), on the contrary, the perfor­mance of the UR policy is comparable 
to that of the U policy since all granules are of equal importance. The UM policy, similar to the skewed 
case, exhibits the poor­est performance of the three polices for a smaller number of nodes due to the 
dependency effect. The simulation results that are also shown in these graphs compare well with the analytical 
predictions, at least preserving the relative orders in terms of their performance. The ana­lytical predictions 
for the U and the UR policies are a little pessimistic. Effect of Update Probability: Earlier in this 
subsection, we noted that the shared buffer hit proba­bility of the U policy can be close to or better 
than that of the UM and UR policies for a large number of nodes. However, this is only true if the probability 
of update for the hot granules is equal to or larger than that of the COM granules. Since this may not 
be the case in general, we explore the effect of update probabilit~es on the shared buffer hit probability 
in this subsection. Figure 7 shows the effect of the ratio of the up­date probabilities of the hot and 
cold granules on the buffer hit probabilities of the three selected policies for a shared buffer size 
of 21< and 5A granules. Hers, the update probability for the cold granules is kept con­stant(O.3) and 
the update probability of the hot gran­ules is varied. The figure shows that the performance of the U 
policy depends critically on the ratio of up­date probabilities for the hot and cold granules. This is 
because under the U policy the granules are brought into the shared buffer only as a result of update 
cjpera­tions and the relative number of hot and cold grmules brought into the shared buffer determines 
the composi­tion of the shared buffer. Both the skew in the workload and the update probabilities determine 
the relative rate at which hot and cold granules are brought to the shared buffer. The difference among 
these policies is magnified as the shared buffer size is increased. Both the UM and UR policies are less 
sensitive to the ratio of the update probabilities. As a general policy decision, however, this poses 
a dilemma: on the one hand the U policy ma,y per­form better than the other two selected policies if 
the ratio is higher than unity; on the other hand, the per­formance of the U policy is sensitive to the 
value of the ratio of the update probabilities, and hence, it should be selected with some caution. 5 
Summary and Conclusions h a data sharing environment, a global shared buffer cart be introduced to alleviate 
not only the buffer invalidation problem but also the proliferation of local copies of the same data. 
In this paper, we developed an analytic model to examine trade-offs between policies for managing the 
shared buffer, and the effectiveness of the shared buffer in various system configurations. Var­ious 
SBMPS are considered that differ in their choice of data granules to be placed in the shared buffer. 
Policies are anaIyzed where the granules are propagated into the shared buffer after one or more of the 
following events: update (U), shared buffer miss (M) and pri­ vate buffer replacement (R). Based on the 
granules put into the shared buffer, seven different policies are con­sidered, namely U, M, R, UM, UR, 
MR, and UMR. The methodology analyzes all policies using a uniform framework by decomposing the reference 
string to the shared buffer into multiple (three) sub-streams based on their effects on the dependency 
between the private and shared buffer contents. This approach simplifies the problem of analyzing different 
SBMPS into (1) estimat­ing the reference rate of each sub-stream under the var­ious policies and (2) 
evaluating the impact of each sub­stream on the dependency effect and hence the shared buffer hit ratio. 
The analytic model captures the effect of skewed data access patterns on the buffer hit ratio, and its 
implications for the shared buffer policies. The performance of an SBMP depends on the ratio of hot and 
cold granules put into the shared buffer by that pol­icy as well as the invalidation rate to the shared 
buffer. The analysis can be used to study the appropriate division of a given amount of buffer between 
the shared and private buffers under various system architectures. The positive dependency between the 
buffer contents re­duces the effectiveness of shared buffer and distorts the curve of the shared buffer 
hit versus the shared buffer size into an S shape. That is to say that the effec­tiveness of shared buffer 
allocation can be divided into regions. In the first region, strong dependency exists be­tween the buffer 
contents, and the buffer hit improves slowly with shared buffer size. (This region shrinks as the number 
of nodes increases. The dependency effect is most noticeable for the case with a few nodes. ) Af­ter 
the shared buffer size goes beyond a certain thresh­old, dependency in the buffer contents decreases 
and a significant improvement in the buffer hit ratio can be observed. Eventually, after the hot set 
is buffered in the shared buffer, the buffer hit improvement again slows down. This S-shape behavior 
of the buffer hit ratio curve can be accurately predicted by the analytic model which also captures the 
intricacies of the different policies. Finally, we summarize the observations on the proper selection 
of an SBMP: 1. The performance of the U policy is sensitive to the update ratio of the hot and cold granules 
and, hence, should be chosen only if the update proba­ bilities of the hot and cold granules do not 
change dynamically. For a small number of nodes, even if the above condition is violated, the U policy 
may still be a contender with the UM and UR policies. 2. The UM policy is not a good choice for a small 
 number of nodes since it suffers the most from the dependency effect. However, it is the policy of 
choice for a larger number of nodes if the crite­rion for selection is the robustness of performance 
for a wide range of parameters such as the ratio of update probabilities and the relative sizes of the 
private and shared buffers. It may also outperform the U policy for a certain range of update proba­bilities 
for the hot and cold granules. 3. The UR policy is a contender only for a small num­ber of nodes. However, 
it is ruled out for a larger number of nodes because it is sensitive to the rel­ative sizes of the private 
and shared buffers and, more importantly, it incurs a significant overhead in propagating the large number 
of private buffer replacements to the shared buffer. Acknowledgements: It is a pleasure to thank Don 
Towsley and Stephen Lavenberg for their helpful sug­gestions that resulted in improved presentation. 
References <RefA>[CHER89] Cheriton, D., H. A. Goosen, and P. D. Boyle, Multi-Level Shared Caching Techniques 
for Scalability in VMP-MC , 16th Annual Inter­national fymposzrcm on ComptiteT ATchztectu Te, Vol. 17, 
pp. 16-24, 1989. [DAN90a] Dan, A., and D. Towsley, (An Approximate Analysis of the LRU and FIFO Buffer 
Replace­ment Schemes, ACM SIGMETRIGS, Denver, CO, May 1990, pp. 143-152. [DAN90b] Dan, A., D. M. Dias, 
and P. S. Yu, The Ef­fect of Skewed Data Access on Buffer Hits and Data ContentIon in a Data Sharing 
Environ­ment , 16th International Conference on VeTy La7ge Databases, Brisbane, Australia, Aug. 1990, 
[DAN90C] Dan, A., D. M. Dias, and P. S. Yu, (Modeling a Hierarchical Buffer for the Data Sharing En­vironment}) 
~ IBM ReseaTch Report RC 15707, 1990. [DAN90d] Dan, A., Performance Analysis of Data Sharing Environments 
, PhD DtsseTtatzon, University of Massachusetts, Amherst, September 1990. [DIAS88] Dias, D. M., B. R. 
Iyer, and P. S, Yu, (Trade­offs Between Coupling Small and Large Proces­sors for Transaction Process 
ing, IEEE Trans. Computers, Vol. C-37, No. 3, pp. 310-320, March 1988. [DIAS89] Dias, D. M., B. R. Iyer, 
J. T. Robinson, and P. s. Yu, (Integrated Concurrency-Coherency Controls for Multisystem Data Sharing) 
IEEE Trans. So}truare Engtneemng, Vol. 15, No. 4, PP. 437-448, April 1989, [KRON86] Kronenberg, N., H. 
Levy and W. D. Strecker, VAXcluster: a Closely-Coupled Distributed System, ACM Tmns. Computer System, 
Vol. 4, PP. 130-146, May 1986. [LAVE83] Lavenbere, S.S. (Editor), [Computer r: Perfor­mance Modeling 
Handbook, Academtc P7ess, New York, NY, 1983. [PRZY89] Przybylski, S., M. Horowitz, and J, Hennessy, 
[Characteristics of Performance-Optimal Multi-Level Cache Hierarchies]], 16th Annual Inter­national Symposium 
on Compute T Architecture) Vol. 17, pp. 114-121, 1989, [STR182] Strickland, V. L. Watts, IBM Systems 
1982. J. P., IMS/VS: Journal, P. P. Uhrowczik and An Evolving System) Vol. 21, pp. 490-510, [YU87] Yu, 
Iyerl P. B. S., R. Dias, D. and Cornell, M., D. Robinson, W.l On J, T., Coupling Multi-Systems ings of 
the pp. 573-587. Through IEEE, Vol. Data Sharing , 75, No. 5, Proceed-May 1987, [YU90] Yu, P. S. and 
D. M. Dias, Impact of Large Memory on the Performance of Optimistic Con­currency Control Schemes , in 
PA RABASE-90: Inte? natzonal Conference on Database, PaTal­/e/ Archztectu;es, and thezr Applications, 
Miami Beach, FL, March 1990, pp. 86-90</RefA>, =L=u~ T 9 . 0 n  4000 woo Sooo 100CQ SHAREDBUFFERSIZE Figure 
1: Comparison of pure policies (Validation: NBp + BS = 10K,N = 16) g L n n I 11 1 it 2000 4000 acoo 
Booo Im SHAREDBUFFERSIZE Figure 2: Comparison of U, UM and UR policies (v.lidation:NBp + l?. = 10K,N 
= 16) t 0 1 Figure 5: (validation:BP 1 4 Effect = ! 1 1 I t ! s 12 16 NUM5EROF NOOES of dependency on 
the shared buffer hit lK, BS = 2K, 5K, access pattern: 80-20) g g + + --R­ fmww SM UM PCWXSM URPOUIM9U 
[~k -+.0 ,.. .... .. m w m n o 2000 4000 Oooo SHAREDBuFFERSIZE Figure 3: Comparison of U, (Validation: 
NBP + Bs UM and = 10K,N UR = policies 2) 0 1 Figure 6: (validation:l?p I 1 1 1 1 1 I 4 s 12 18 NUMBEROF 
NODES Effect of dependency on the shared buffer hit = IK, B.s = 2K, 5k, access pattern: uniform) g ~ 
* -0 + --u­~ ... . . u POI.)XSB-SKUu PcuYss-5K UR FWJG$SB-SK s3-2K : ,U,..J u SHAREDBUFFERSIZE Figure 
4: Effect of skewed access on the shared buffer (Validation: NBP + B.s = lo~, N = 2 M PO1icJ ) hit = 
r I 1 ,! I 0.4 0.8 1.2 UPWE wno Figure 7: Effect of the ratio (Analysis: BP= 125,Bs , 1 1.6 of update 
= 2K,5K,N , , 20 probability = 16) 167  
			
