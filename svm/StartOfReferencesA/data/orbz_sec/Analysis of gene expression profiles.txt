
 Analysis of gene expression profiles: class discovery and leaf ordering Chris H.Q. Ding NERSC Division, 
Lawrence Berkeley National Laboratory University of California, Berkeley, CA 94720 chqding@lbl.gov (Extended 
Abstract) Abstract We approach the class discovery and leaf ordering problems using spectral graph partitioning 
method­ologies. For class discovery or clustering, we present a min-max cut hierarchical clustering method 
and show it produces subtypes quite close to human expert labeling on the lymphoma dataset with 6 classes. 
On optimal leaf ordering for displaying the gene expression data, we present a sequential or­dering method 
that can be computed in O(n2) time which also preserves the cluster structure. We also show that the 
well known statistic methods such as F-statistic test and the principal component anal­ysis are very 
useful in gene expression analysis. 1 Introduction Clustering analysis of DNA microarray gene ex­pression 
proflies is among the first steps in under­standing the activities of genes during biological process 
and their responses to certain desease con­ditions. By grouping tissue samples into homoge­neous groups 
that correlates highly to particular macroscopic phenotypes such as different cancer types or other clinical 
syndromes [17, 11, more sys­tematic characterization can be developed and new subtypes discovered. Copyright 
2002 Association for Computing Machinery, ACM acknowledges that this contribution was authored or co-authored 
by a contractor or affiliate of the U.S. Government. As such, the Government retains a nonexclusive, 
royalty-free right to publish or reproduce this article, or to allow others to do so, for Government 
purposes only. RECOMB 02, April 18-21,2002, Washington, D.C., USA Copyright 2002 ACM l-58113-498-3-02/04...$5.00 
 Sever1 clustering methods applied to gene ex­pressions data has been studied, the hierarchical agglomerative 
methods [15], self-organized maps [24, 171, simulated annealing [a], graph partion­ ing methods [6], 
[22] [25]. However, many of these methods are more focused on clustering genes; when clustering tissue 
samples, many of them applied only to small number of phenotypes, typically 2-3. In this paper we present 
a min-max cut hierar­chical clustering algorithm and applied it to several publically available gene 
expression datasets. The effectiveness of our algorithm is demonstrated on the lymphoma dataset [l] where 
our algorithm can correctly identify 6 phenotypes based on standard correlation alone. We also found 
out that several samples of DLBCL type have high correlation with T-cell lines type, differ from the 
original study [l]. (see section 6). As the second main contribution of the paper, we present an optimal 
leaf node ordering algorithm. In both hierarchical agglomerative clustering and divisive clustering, 
the clusters and the members of clusters contained in the leaf nodes of the binary hierarchical tree 
are often displayed in linear order. Biological and clinic studies are often performed in the context 
of this leaf node linear ordering, mak­ing it significant part of the clustering analysis. In Eisen et 
a1.[15], the leaf nodes are ordered based on the average expression levels and patches of visible structures. 
In self-organizing maps [24], clusters are organized as a 2D topological mesh which does not always match 
those of hierarchical clustering method. Alon et al [2] used similarity between nodes and their parent 
s siblings to order the leaf nodes. Most recently, an optimal ordering method based on similarity of 
adjacent nodes is proposed by Bat-Joseph, et a1.[4]. Here we propose a new optimal ordering objec­tive 
function that both maximizes the similarities on adjacent nodes, but also minimizes similarities on large 
distance pairs of nodes. We then present an efficient algorithm to compute an approximate optimal ordering 
based on this ordering objective. This algorithm can also compute an optimal or­dering that preserves 
the clustering structure. We apply this algorithm on the lymphoma datasets to illustrate the usefulness 
of our approach (see sec­tion 7). The min-max cut algorithm follows a min-max clustering principle tissue 
samples are grouped into clusters such that the similarity between clus­ters are minimized while similarities 
within each clusters are maximized. It is a new development in spectral graph partition [12, 16, 201 
that makes use of the eigenvectors of Laplace matrix of a graph. It is more effective in finding balanced 
clusters than earlier algorithms. The optimal leaf node ordering algorithm uses a spectral formulation 
that is closely related to the spectral graph partitioning. Due to widely available software for efficient 
computation of eigenvectors (LAPACK, ARPACK, etc.), these methods can be efficiently implemented on a 
vari­ety of computer architectures. In this work, we use F-statistic for gene se­lection and show it 
is effective method. We also use principal component analysis for preliminary understanding of the data 
(see Figures 3 and 1). These well established statistical methods are quite useful in gene expression 
profiles analysis.  2 Gene selection Of the thousands of genes measured in a microar­ ray experiment, 
many of them show little varia­tions across the tissue samples. and therefore are not useful in distinguishing 
different phenotypes. Furthermore, many genes are highly correlated, ex­hibit a large degree of redundancy. 
Selection of those informative genes [17] which show large vari­ance among the targeted phenotypes is 
an impor­tant part of clustering analysis. There exist sev­eral methods for gene selection, from the 
simple t-statistic like tests [2, 171 to more sophisticated ones, such as information gain and Markov 
blan­ket. In this paper, we emphasizes the multi-cluster nature of the problem and use the F-statistic 
test which is a generalization of t-statistic for two class. Given a gene expression across n tissue 
samples g = (!Ilig2,-, gn), the F-statistic is defined as F = c n&#38;&#38; - ij)2/(11 - 1)/a2, [ k 
1 (1)  where s is the average expression across all sam­ples, Sk is the average within class c,+, and 
0 is the pooled variance: (T2 = c(nk 1) 0; /(n Ii) 1 k where nk and Crk are the size and variance 
of gene expression within class ck. For K = 2, nln2 91-92 F=t2, t= (2) J-- + 0 nl n2  F-statistic 
reduces to t-statistic. We pick genes with large F-values or t-values. (If gene expres­sions follow Gaussian 
distribution, F-value of genes follow F(K-1, n--K) distribution and we can com­pute p-values and confidence 
levels.) F-statistic for gene selection is used a classification study [14].  3 Similarity metric For 
automatic class discovery, the association or similarity between tissue samples are the main fac­tors. 
We wish to group tissue samples into clusters such that similarities between clusters are mini­mized 
and similarities within each clusters are max­imized. There are a number of ways to define the similarity. 
A popular method is to measure the Pearson correlation [15, 21 c(i,j) between two tis­sue samples i, 
j, and define the similarity as sij = exr+&#38;Mc)), where (c) is an average correlation between nearest 
k neighbors. Another method is to measure the Euclidean distance d(i,j) and define sij = exp(-d(i,.?)l(d)), 
 where (d) is some average distance between nearest k neighbors. These similarity metrics are generic 
and are used in wide areas of applications. There are, however, more detailed modeling of similar­ities 
or weights based on statistical properties of the underlying populations [22].  4 Hierarchical divisive 
clustering Many current research on gene expression cluster analysis uses hierarchical agglomerative 
clustering methods [15, l] which builds clusters from bottom up, gradually merging clusters into bigger 
and big­ger clusters [13]. Hierarchical divisive clustering follows a top­ down approach. It first partitions 
the samples into two clusters, and then recursively partition each leaf clusters into more clusters. 
This approach naturally uses a graph partitioning method. The similarities between all pairs of samples 
are first computed and stored in a matrix W = (u);j). W then defines a weight matrix, or the adjacency 
ma­trix of an undirected graph with each node as a tissue sample. (Here we focus on clustering tis­sue 
samples. One can equivalently consider clus­tering genes according their responses to all tissue samples 
or other experiment conditions). Cluster­ing becomes partitioning the graph into subgraphs based on certain 
objective or cost criteria. Cluster­ing gene expression data using graph partitioning approach has also 
been studied in [22, 251.  4.1 Min-max cut We briefly introduce the min-max cut graph par­tition and 
clustering method very recently devel­oped for internet newsgroup clustering [ll]. Given a weighted graph 
G with weight matrix W, we wish to partition it into two subgraphs A, B using the above mentioned min-max 
clustering principle. The similarity or association between A, B is the sum of weights between the two 
clusters, sim(A, B) = s(A, B) = c w;j, (3) iEA,jEB The similarity or association within a cluster is 
the sum of all edge weights within A or B: sim(A, A) = s(A, A), sim(B,B) = s(B,B). (4) The clustering 
principle requires minimizing s(A, B) while maximizing s(A, A) and s(B, B) independently at the same 
time. These requirements are simulta­neously satisfied by the objective function, 44 B) 44 B) J ~ (5) 
 Mcut = s(A, A) + s(B,B). JMcut is called min-max cut (Mcut) objective[ll]. The solution of partition 
problem can be repre­sented by an indicator vector q, where the element of q on node i is if iEA 4; 
= (6) - h if iEB {  where a and b (0 < a,b < 1) are two constants. Finding the optimal partition is 
NP-complete. An effective solution is the following. First, one can show that min JMcut (A, B) + q n 
qT?T;%)q, (7) 9  subject to qTWe = qTDe = 0, where D = (d;) is a diagonal matrix and d; = Cj w;j is 
the degree of node i and e = (1,. . ., l)T. Second, we relax Q; from discrete indicators a and -b to 
real number in (-l,l). The solution of q for minimizing the Rayleigh quotient of Eq.(7) is given by (D 
 W)q = (Dq. (8) The solution to this generalized eigenvalue prob­ lem is the second eigenvector q2, 
called the Fiedler vector. Third, we sort the Fiedler vector q to es­ tablish a linear search index order. 
Fourth, using the linear index order, given any cutpoint icut, we partition the graph into two subgraphs 
(clusters): A = (4; I i 5 Gut), B = (4; i > icut}. A contains all nodes left of the cutpoint iCUt and 
B contains all nodes on the right. We search for the cutpoint icut such that JM~~~(A, B) is minimized. 
The corresponding A and B are the final clusters of the Mcut algorithm. This Mcut algorithm is the latest 
development along the line of spectral graph partitioning that is based on the properties of eigenvectors 
of the Laplacian matrix L = D W [12, 16, 201. Besides the min-max cut objective function, the ratio 
cut objective, Jncut = 44 B)/IAI t s(A,B)/IBI is proposed earlier [7, 181 to balance the sizes of the 
partitions. The normalized cut objective, JNcut=S(A, B)/s(A, G)ts(A, B)/s(B, G),proposed in [23] attempts 
to balance the volumes of the par­titions (s(A, G) is the volume of subgraph A [S]). In Contrast, &#38;cut 
balances within-cluster similar­ity. Both theoretical analysis and experiments on internet newsgroup 
data sets indicate [ll] JM,-ut gives more balanced clusters while JRcut and J&#38;-t sometimes cut a 
small subgraph away from a large graph resulting in unbalanced clusters. Note that although &#38;cut, 
&#38;cut and &#38;cut objective fUnCtiOnS are first proposed based on appropriate intuitions, they can 
also be obtained automatically as the eigenvalues of the Fiedler vector in perturbation analysis [lo]. 
This further justifies using the Fiedler vector for finding the (approximate) optimal par­ titions based 
on these objective functions.  4.2 Recursive Clustering Once a cluster is partitioned into two clusters, 
we can further partition each of them using the same method. This process is repeated several times and 
a binary partition tree is established where the each node contains a cluster during the process. A stopping 
criteria is necessary to stop the di­visive process. The Mcut objective provides such a criteria. For 
a cluster Gk on the leaf node, we compute the Fiedler vector q, find the optimal cut, and obtained J, 
= minJMcUt(q) value. If .J, is large, the overlap between two resulting subclus­ ters is large in comparison 
to the within-subcluster similarity, hence cluster GI, should not be further partitioned. We set J stop 
= 1.0, as the threshold for J, in our experiments. The complete clustering algorithm is: 1. For the current 
leaf node Gk, solve Eq.(5) for the second lowest eigenvector q. 2. Sort q. Find the cutpoint iCUt with 
min(JMcut).  3. If min(JM,Ut) < Jstop, cut Gk into two chil­dren clusters &#38;, Bk. Ak and Bk become 
 new active leaf nodes on the binary tree. If min(Jbht) > Jststop, Gk becomes a dead-end leaf node. 
4. Examine all active leaf nodes until none of them can be further partitioned. (A) 06 + 04 All 7070 
genes :+ 4 ++v 0 0 v $ , v+vv , v  -0.4 015 0.155 0.16 0.165 0.17 0.175 0.16 (6) 0.3 0.2 -0.1 -0.2 
-03. -040 0.05 0 1 015 0.2 0.25 03 0.35 0.4 0.45 Figure 1: Leukemia dataset as shown in 2D space of 
the first two principal components. (A) All genes are used, i.e., we used PCA to reduce the data from 
the original 7070 dimensions to 2 dimensions. (B) Only 50 selected genes are used in PCA.  5 Analysis 
of Leukemia subtypes The leukemia dataset of Golub et al [17] is well studied. Here we study the training 
dataset: 7070 gene expressions of 38 tumor tissue samples. The goal here is to see if we can automatically 
detect the two phenotypes of the cancer: acute lymphoblas. tic leukemia (ALL) and acute myeloid leukemia 
(AML). To gain insight, we perform the princi­pal component analysis (PCA) and show results on the first 
two principal components in Fig.la. One can see the structure of two phenotypes. The two classes overlap 
substantially when all 7070 genes are used. We used the t-test statistic criteria to  Figure 2: The 
-t-values (Eq.2) for all 7070 genes in the Leukemia dataset of Golub et al [17]. Gene indices are reordered 
according to t-values. We se­lect 25 genes with largest positive t-values and 25 genes with largest negative 
t-values. select 50 genes (shown in Fig.2). Using 50 selected genes, the two classes separate clearly 
(Fig.lb). We perform the cluster algorithm on the dataset using Pearson correlation. The cluster result 
using all 7070 genes is shown in the two-way contingency table (Table 1). We use the simple Q-accuracy 
[21, 91 (sum of the diagonal elements divided by the total number of samples). ALL 9 18 Table 1: A 
contingence table summarizes the dis­covered clusters Cr, C2 using all 7070 genes. The accuracy is Q 
= (10+18)/(10+1+9+18) = 0.737. Using the 50 selected genes, the clustering re­sults (contingency table 
2 ) and the accuracy are Q = 0.947. Only two samples of the ALL class (the two points with the n symbol 
in Fig.lb instead of the v sym­bol for the rest of ALL samples) are incorrectly clustered into the AML 
class. Clearly, these two samples are on the boundary between the clusters. We note that if we use the 
Euclidean distance op­tion to define the similarity metric, these two sam­ples will be correctly clustered, 
while one sample from AML class (the point nearest to ALL samples in Figure lb) is mis-clustered into 
ALL class. The accuracy will be Q = 0.974. Thus our clustering al­gorithm performs well, and from the 
PCA analysis we understand the origin of clustering errors. This dataset is studied in [25]. The CLIFF 
al­gorithm begins with 360 genes to perform iterative feature selection and clustering, to graduately 
re­duce the number of genes. We perform the cluster­ing using 360 genes selected by the t-statistic and 
the results are identical to that using 50 genes, al­though in 2D PCA space, the two classes mix more 
than-the case of 50 genes (not shown). This indi­ cate the effectiveness of the t-statistic n gene se­ 
lection.  6 Analysis of Lymphoma classes This dataset contains 4029 gene expression of 96 tissue samples 
from Alizadeh et al.[l]. Using bi­ological and clinic expertise, Alizadeh et al clas­sify the tissue 
samples into 9 classes as shown in Figure 3. Because of the large number of classes and also highly uneven 
number of samples in each classes (46, 2, 2, 10, 6, 6, 9, 4, ll), simply using all genes would not show 
clear class distinctions. We use F-statistic to select 200 genes for this study as shown in Figure 4. 
We also ignore 8 tissue sam­ples belonging to classes C2, C3, and C8 because (i) the number of samples 
in these classes are too small. (ii) as discussed in [l], C2 (germinal center B), C3 (lymph node/tonsil) 
are very close to Cl (DLBCL) - in fact, they are clustered together in [l]. Therefore, we focus on 6 
largest classes of 88 samples. Using PCA, we first examine the samples in the first two principal components 
as in Fig.3. The structure of 6 classes are visible in Fig.3. This motivate us to further study the automatic 
class discovery using the clustering algorithm. We perform the clustering algorithm this dataset. The 
partition tree is shown in Figure 5. The clus­tering results (contingency table) and accuracy are listed 
below: -39 . 1 . . 6 . 10 . . . . . . g . . . . . . 11 . * . . . . 6 . . . . . . 6   -0.2 -0.15 -0.1 
-0.05 0 0.05 0.1 0.15 0.2 Figure 3: The lymphoma dataset of [l] in PCA space, using 200 genes selected 
based on F-statistic (class labels are according to Fig. 1 of [l]). &#38; = 0.921. These results are 
quite reasonable for this rela­tively difficult problem with such a large number of classes and varied 
sizes of each class. We independently verified these results by check­ ing the sample-sample correlations. 
Samples OCI- Ly3, OCI-LylO, DLCL-0042, DLCL-0017 of Cl class have high correlations with samples in C6 
class and have low correlations with the rest of samples in Cl. These samples should belong to C6 if 
the sample-sample correlation is the only factor in de­termining the class information. These results 
do not change if we use 100 genes, and therefore re­flect the inherent structure of the gene expression 
data. Further studies are necessary to understand why they differ from the expectations of human expertise. 
One of the main results of [l] is using the gene expression profiles to further detect two subtypes of 
DLBCL which are previous unknown and are more subtle to detect. Indeed, using our algorithm, we can further 
split DLBCL samples into the Germi­  )o  Figure 4: The F-value (Eq.1) for the 4029 genes in the lymphoma 
dataset of Alizadeh et al. [l]. We select 200 genes with largest F-values. nal center B-like DLBCL and 
the Activated B-like DLBCL, although the &#38;cut value are larger, in­dicating these two subtypes mix 
more than other phenotypes (see Figure 5).  7 Ordering tissue samples Once the cluster structures are 
discovered (and also before that), very often we need to order the genes or tissue samples in a linear 
order such that ad­jacent tissue samples are similar and samples far away along this sequential order 
are diflerent. This is quite useful both for displaying results and for further inspection and study[l5, 
2, 41. Here we present a new ordering objective function and an efficient algorithm to compute an approximate 
op­timal solution. This optimal leaf ordering also pre­serve cluster structure, i.e., all nodes within 
a clus­ter should be adjacent to one another. 7.1 Leaf ordering objective function In [4], the objective 
of leaf node ordering (defined by index permutation 7r = (~1, . . . , K,) ) is to in­sure that adjacent 
nodes are similar. This is achieved by maximizing the sum of similarity between adja­cent nodes: However, 
this objective ignore the similarity be­tween larger distance nodes. To see this point, we 46Cl+lOC4+9C7+11C9+6C5+6C6 
 ordering is the following 0 106 J 145c1+6c61 /(I o,,\  /39c:1/ jbc6+6clj / I I ~~~~  ,/ , lCl+lOC4+9C7+11C9+6C5 
 c?!Li> i.!yl / 026.i\  Figure 5: The binary partition tree outlines the divisive clustering process. 
Each node contains a current cluster, whose content, the number of sam­ples in each class are given, 
e.g., 46Cl means 46 samples of class Cl. The six leaf nodes contain the final 6 clusters discovered by 
the Mcut algorithm. The min( JMcut) value for each divisive partitioning is also shown. We also attempted 
to further parti­tion the DLBCL cluster (39Cl) into two subtypes, 2lCl and 18Cl. The min(JMcut) is 1.147, 
slightly bigger than Jstop = 1. We verified that the 21Cl cluster corresponds to GC B-like DLBCL and 
the 18Cl cluster corresponds to Activated B-like DL- BCL (see Figure 3 in [l]). list the different distances 
of a 5-node graph below: 0 1 2 3 4 0 1 2 3 0 1 2 0 1 1 1 0 There are four d = 1 pairs, and their similarity 
is contained in J&#38;r. There are also three d = 2 pairs. We believe the similarities on these pairs 
should be smaller than on those d = 1 pairs, if the final leaf order is meaningful. Further more, there 
are two d = 3 pairs and the similarities on these pairs should be smaller than both d = 1 and d = 2 pairs. 
This consideration goes all the way to d = n 1 pair. All these considerations (except for d = 1 pairs) 
are not taken into account in the Jdd=r objective. For this reason, we believe a more appropriate, distance-sensitive, 
objective function in leaf node larger weights to ensure that the larger the distance between a pair 
of nodes is , the less similar this two nodes are. We may rewrite the distance-sensitive objective function 
Jd as min Jd(7T), Jd(7f) n-1 = x @J e=1 &#38;e(T) (10) where (11) Here we penalize large distance 
similarities with J&#38;r) = 4s + c( - 4).&#38;&#38;) - 3&#38;=&#38;r). n-l 1 [ e=3  where s = Cij 
s;j is the total weight of the graph which is a constant. Therefore, minimizing Jd is equivalent to simultaneously 
maximizing adjacent similarities Jd=r and minimizing large distance sim­ilarities. 7.2 Approximation 
algorithm An exact algorithm to find the optimal solution is NP-complete and is beyond the scope of this 
work. However, an efficient O(n2) algorithm exist to compute an approximate solution to minimizing Jd. 
First, we note that Jd can be written as  We can rewrite Jd(? r) as  For simplicity, we define 7ri~; 
- n/2 2-n 4-n P,, = . . ..l}. (12) UT7 n 7 42  and replace ri by i in the above summation (the result 
remains identical). Note that z Sij(Pi - Pj)2 = C sij(Pf + p2 - 2p p ) 3 a 3 ij ij   = C 2pi(diiSij 
- Sij)pj = 2pT(D S)p (13) ij  where 6;j = 1 if i = j; 6, = 0 otherwise. There fore, the index permutation 
?r is obtained by min­imizing pT(D  S)p for pi taking those discrete values of Eq.(12) in (-l,l]. So 
far everything is exact. The critical approx­imation step here is that we relnzpi from these dis­crete 
values to continuous values in (-1, 11. With this, pT(D  S)p can be minimized by solving an eigenvalue 
problem. Since s;j 1 0, from Eq.(13), pT(D  S)p 2 0 and trivial solutions such as po = 0 or po = e 
will minimize pl (D  S)p: pF(D  S)po = 0. Therefore we need to impose a constraint on the normalization 
of p and other constraint so that p f e. These two constraints are pTDp = const, pTDe = 0. (14) These 
constraints can be simultaneously satisfied with the scaled ordering objective  The above approximation 
by relaxation of discrete permutation indicators for computing the optimal solution was first proposed 
in slightly different form in [19] (see also [5]). One can see it has a close connection to spectral 
graph partition (Eq.7). Clearly, the solution to the minimization prob­lem is the eigenvector of (D S)p 
= XDp. The lowest eigenvector is trivial p1 = e with X1 = 0, which should be discarded. The correct solution 
is the second lowest eigenvector pz, which automati­cally satisfies the constraints of Eq.(14). Once 
pz is computed, we sort pz to increasing order. This sorting induces an index permutation ?r which is 
the desired solution. To measure the quality of leaf ordering, we de­fine the large-distance similarity 
ratio Ed = Jd(r)/Jd(random),  and the adjacent pair similarity ratio 7-1 = Jd,l(~)/Jd,l(randOm),  where 
and Jd=l(random) = (&#38;j)(n - l), where 1~ - 1 ac­counts for the number of adjacent pairs on the or­dering. 
Although Td includes all distances, there are 1) 1 adjacent pairs and (n - l)(n - 2)/2 pairs with d 
> 1. Thus for n > 4, Td is dominated by large distance pairs (thus the name large-distance similarity 
ratio). If the dataset is randomly per­muted, we expect Td N 1 and y1 N 1, which can be easily verified. 
As the leaf ordering is improved, the large-distance similarity ratio T* will decrease while the adjacent 
pair similarity ratio ~1 will in­crease. For the 88 tissue samples in the lymphoma dataset, we obtained 
Td = 0.18, 7.1 = 3.39. Thus the large-distance similarities are reduced about a factor of l/O.18 = 5.6 
from random ordering and the adjacent pair similarities increase by 239%. The results of optimal leaf 
ordering on the lym­ phoma dataset is shown in Figure 6. Note that we can also reorder genes by first 
computing gene­ gene similarity using Pearson correlation (see 53) and then ordering them using the same 
method. This is done in Figure 6.  Figure 6: Optimal leaf node ordering of the lym­ phoma dataset: 
88 tissue samples with 200 genes. (A) Data are displayed as the original order from Alizadeh et al [I]. 
(B) Both tissue samples and genes are ordered according to Jd objective corn puted from Eq.(15).  7.3 
Preserving cluster structure In the above distance-sensitive ordering heuristic, our main goals are (i) 
to maximize the similari­ties on adjacent pairs of nodes and (ii) to minimize similarities on large distance 
pairs. However, these considerations do not take into account the clus­ter structure - it sometimes occurs 
that nodes of it cluster are not consecutively ordered, and nodes from another cluster could mix in between. 
Here we propose a method to take this into ac­count in leaf ordering. Our approach is not to mod­ify 
the J,+ objective. Instead, we modify the simi­larity matrix with the following considerations: (1) preserve 
local ordering within each cluster, while (2) enforce nodes within a cluster to stay together. The first 
consideration implies that similarity be­tween nodes of a cluster, relatively, should remain unchanged. 
The second consideration suggest we reduce the similarities between different clusters (or equivalently, 
increase within-cluster similarities uniformly). The following re-weighting achieves both goals:  where 
e; is the cluster id of node i. a > 0 is a parameter that adjusts how much we increase the within-cluster 
similarity. If (Y >> 1, clusters will become well separated. Thus (Y N 1 is good choice. In Fig.7, we 
show the effects on modifying the leaf order that preserves the cluster structure for the 88 sample dataset. 
We set cy = 1 in Eq.(16). The cluster structure is 6.class structure discovered in Figure 5. One can 
see that the cluster structure is preserved in the leaf ordering.   8 Discussions The main contributions 
of this work are two fold: (1) we introduce the min.max cut hierarchical di­visive clustering algorithm 
and show it produces good cluster results on the gene expression dataset with large number of classes. 
(2) we introduce a fast and effective leaf nodes ordering method for tissue samples and genes that maximize 
similar­ities on adjacent nodes and minimize similarities on large distance pairs of nodes. A simple 
modi­  1 2 3 4 s b 7 8 9 Figure 7: Leaf node ordering that preserves clus­ ter structure. The cluster 
ids (Cl, C4, C5, C6, C7, C9) for each sample are color coded. (A) Original ordering computed from Eq.(15). 
There are sev­eral samples from different clusters mix together. (B) The modified ordering computed with 
the sim­ilarity matrix modified according to Eq.(16). The cluster structure is preserved. fication of 
the method leads to leaf ordering that also preserves cluster structure. This work also demonstrate that 
the well-known statistic methods such as F-statistic test and PCA are quite useful in gene expression 
analysis. The F-statistic is effective in gene selection. PCA is effective in gaining initial knowledge 
of the cluster structure of the dataset. PCA has been used in [3] for different goals, and is recently 
criticized [26] for not being effective in cluster analysis. Our clustering results on lymphoma dataset 
also reveals some difference in class labeling of several tissue samples. This needs to be more carefully 
studied. More details and analysis results on the lymphoma dataset will be collected in a website (www.nersc.gov/~cding/lymphoma). 
Acknowledgements. This work is supported by U.S. Department of Energy (Office of Science, Of­fice of 
Advanced Scientific Research, Division of Mathematical, Information, and Computational Sci­ences) under 
contract DE-AC03-76SF00098.  References <RefA>[l] A.A. Alizadeh, M.B. Eisen, et al. Distinct types of diffuse 
large B-cell lymphoma iden­tified by gene expression profiling. Nature, 403:503-511,200o. 135 [2] U. 
Alon, N. Barkai, D.A. Notterman, K. Gish, S. Ybarra, D. Mack, and A.J. Levine. Broad patterns of gene 
expression revealed by clus­tering analysis of tumor and normal colon tis­sues probed by oligonucleotide 
arrays. Proc. Nat 1 Acad Sci USA, 966745-6750, 1999. [3] 0. Alter, P. 0. Brown, and D. Botstein. Singu­lar 
value decomposition for genome-wide ex­pression data processing and modeling. Proc. Nut 1 Acad Sci USA, 
97:10101-10106,200O. [4] Z. Bar-Joseph, D.K. Gifford, and T.S. Jaakkola. Fast optimal leaf ordering 
for hi­erarchical clustering. Bioinformatics, 17:S22­ s29, 2001. [5] S. T. Barnard, A. Pothen, and H. 
D. Simon. A spectral algorithm for envelope reduction of sparse matrices. Proc. Supercomputing 93, IEEE, 
pages 493-502, 1993. [6] A. Ben-Dor, R. Shamir, and Z. Yakhini. Clus­tering gene expression patterns. 
J. Computa­tional Biology, 6:281-297, 1999. [7] C.-K. Cheng and Y.A. Wei. An improved two­ way partitioning 
algorithm with stable per­formance. IEEE. Trans. on Computed Aided Desgin, 10:1502-1511, 1991. [8] F.R.K. 
Chung. Spectral Graph Theory. Amer. Math. Society, 1997. [9] C. Ding and I. Dubchak. Multi-class protein 
fold recognition using support vector machines and neural networks. Bioinformatics, 17:349­358, 2001. 
 [lo] C. Ding, X. He, and H. Zha. A spectral method to separate disconnected and nearly­ disconnected 
web graph components. Proc. 7th ACM Int l Conf Knowledge Discovery and Data Mining (KDD 2001), pages 
275-280, August 2001. [ll] C. Ding, X. He, H. Zha, M. Gu, and H. Simon. A min-max cut algorithm for graph 
partition­ing and data clustering. Proc. 1st IEEE Int l Conf. Data Mining, pages 107-114, November 2001. 
[12] W.E. Donath and A. J. Hoffman. Lower bounds for partitioning of graphs. IBM J. Res. Develop., 17:420-425, 
1973. [13] R. 0. Duda, P. E. Hart, and D. G. Stork. Pat­tern Classification, 2nd ed. Wiley, 2000. 
[14] S. Dudoit, J. Fridyand, and T.P. Speed. Com­parison of discrimination methods for the clas­sification 
of tumor using gene expression data.  Univ. of California, Dept of Statistiscs, Tech Report 576, 2000. 
[15] M.B. Eisen, P.T. Spellman, P.O. Brown, and D. Botstein. Cluster analysis and display of genome-wide 
expression patterns. Proc. Nat 1 Acad Sci USA, 95:14863-14868, 1998. [16] M. Fiedler. Algebraic connectivity 
of graphs. Czech. Math. J., 23:298-305, 1973.  [17] T.R. Golub, D.K. Slonim, P. Tamayo, et al. Molecular 
classification of cancer: class dis­covery and class prediction by gene expression monitoring. Science, 
286:531-537, 1999. [18] L. Hagen and A.B. Kahng. New spectral  methods for ratio cut partitioning 
and cluster­ing. IEEE. Trans. on Computed Aided Desgin, 11:1074-1085, 1992. [19] K. M. Hall. r-dimensional 
quadratic place­ment algorithm. Management Science, 17:219-229, 1971. [20] A. Pothen, H. D. Simon, and 
K. P. Liou. Par­titioning sparse matrices with egenvectors of graph. SIAM Journal of Matrix Anal. Appl., 
11:430-452, 1990. [al] B. Rost and C. Sander. Prediction of protein secondary structure at better than 
70% accu­racy. J. Mol. Bio., 232:584-599, 1993. [22] R. Sharan and R. Shamir. CLICK: A clus­tering algorithm 
with applications to gene ex­pression analysis. Proc. ISMB 2000, pages 307-316,200O. [23] J. Shi and 
J. Malik. Normalized cuts and im­age segmentation. IEEE. Trans. on Pattern Analysis and Machine Intelligence, 
22:888­905, 2000. [24] P. Tamayo, D. Slonim, J. Mesirov, Q. Zhu, S. Kitareewan, E. Dmitrovsky, E.S. Lander, 
and T.R. Golub. Interpreting patterns of gene expression with self-organizing maps: Meth­ods and application 
to hematopoietic differen­tiation. Proc. Nat 1 Acad Sci USA, 96:2907­2912, 1999. [25] E.P. Xing and 
R.M. Karp. CLIFF: clustering of high-dimensional microarray data via iter­ative feature filtering using 
normalized cuts. Bioinformatics, 17:S306-S315, 2001. [26] K. Y. Yeung and W. L. Ruzzo. Principal com­ponent 
analysis for clustering gene expression data. Bioinformatics, 17:763-774, 2001.   </RefA>
			
