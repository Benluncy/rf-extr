
 Another nail to the coffin of faceted controlled-vocabulary component classification and retrieval 
Hafedh Mili, Estelle Ah-Ki, Robert Godin, and Hamid Mcheick D4partement d htformatique Universit6 du 
@e&#38; h Montr~al Case Postale 8888 (A) Montr6al, PQ H3C 3P8, CANADA Abstract Our research centers 
around exploring methodologies for developing reusable software, and developing methods and toofs for 
building with reusable software. In this paper, we focus on reusable software component retrieval methods 
that were developed and teated in the context of ClassServer, an experimental library tool developed 
at the University of Qu6bec at Montr6al to explore fssues in software reuse [15]. The methods dfscusaed 
in tbfa paper fall into two categori~ 1) string search-based retrieval metbod$ and 2) keyword-based retrieval 
methods. Both kinds of methods have been implemented and tested by researchers, both in the context of 
software repositories (see e.g. [6,9]) and in the context of more traditional docu­ment tibrarles (see 
e.g. [2,25]). Experiments have shown that keyword-based methods, which require some manual, labor­intensive 
pre-proceashrg, performed only marginally better than the entfrely mechanical strhtgsearch methods (see 
e.g.[6, 2S]), raising the issue of cost-effectivene= of keyword-based methods as compared to string search 
baaed methods. In this paper, we describe an implementation and experiments which attempt to brfng the 
two khtds of methods to a level-playing field by: 1) automating as much of the pre-processing involved 
hr controlled vocabulary-based methods as possible to address the crds issue, and 2) using a realistic 
experimental setting in which queries consist of problem statemenlx rather than component specifications, 
in whjch query results are aggregated over several trials, and in which recaU measures take into account 
overlap­ping components. Our experiments showed that string search based methods performed better than 
semi-controlled vocabulary-based method$ which goes further in the direction of more recent component 
retrfeval experiments which chal­lenged the superiority of controlled vocabulary based clarification 
and retrieval of components (see e.g. [61). Further, reuse being essentially a cost issue, comparing 
 1. Introduction classification and retrieval methods based on retzievaf perfor­ mance measures alone 
is of little use for methods whose set-up The problem of compcment retrieval has been widely and use 
costs are significantly different. Finally, for a reuse addressed in the software reuse literature. A 
wide range of com-library tool to be successful, the cost of reusing has to be per­ ponent categorization 
and searching methods have been pro-ceived by potential reusers as being significantly less than that 
pose~ from the simple string search (see e.g. [15]), to faceted of developing from scratch [26], and 
the cost of performing classification and retrieval (e.g. [21]) to signature matching (see searches is 
only one of several costs associated with an instance e.g. [27]) to behavioral matching (see e.g. [10, 
28] or even [8]). of reuse [17]. These issues have been a major concern of our Different methods rely 
on more or less complex descriptions for research, both in the design of retrieval algorithms, and in 
both software componertts and search queries, and strike dif-evaluating them. ferent trade~ffs between 
performance and cost of implementa- Within the context of our research, we have developed tion [17]. 
The cost of implementing a retrieval method involves four classes of retrieval algorithms: 1) retrieval 
using full-text both initial set-up costs, and the cost associated with formulat­ search on software 
documents and program files, 2) navigation ing, executing and refining queries. In this paper, we describe 
through the structure of components, 3) multi-faceted the implementation and experimental comparison 
of two such classification and retrieval of components, and 4) signature methods, free-text based retrieval 
and multi-faceted matching. We are es much interested in pwformance-based classification and retrieval 
of reusable code components. comparisons between the different methods, as we are con- Typically, retrieval 
experiments focus on abstract perfor-cerned about getting the tools and methods adopted by organiza­ 
mance measures such es recall and precision, which are used tions end used by their developers in a realistic 
development lwth as absolute measures, or as a way of comparing methods environment. Issues of set-up 
costs for the various methods (see e.g. [6]). Recall and precision, which have traditionally have been 
a major cmtce~ and we tried to automate as much been used to measure the performance of bibliographic 
retrieval of the pre-processing steps as possible, lest we degrade slightly systems, have been criticized 
because they view relevance as a retrieval performances. The experiments described in this paper yes/no 
property, and because they don t take into account the compiwe full-text retrieval with a variant of 
multi-faceted specific goal that the searcher is trying to achieve. classification in which we attempted 
to automate some of the Iabr-irrtensive pre-processing steps. The experiment design Permission to make 
digitaUhard copy of part or all this work for was airned towards simulating real-life situations, and 
the personal or clessroom use is grented without fee provided that evaluation of retrieval performance 
was goal-oriented, rather copies are not made or distributed for profit or commercial advan­ than a simple 
count of returned and potentially useful com­ tage, the copyright notice, the title of the publication 
and its date ponents. The experiments showed that our efforts at automating appear. and notice is given 
that copying is by permission of ACM, multi-faceted classification of compnents were not very ffrtit- 
Inc. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior 
specific parmisaion and/or a fee. ful. They also showed that full-text retrieval of components was SSR 
97 MA, USA superior to multi-faceted retrieval, which contradicts the result @ 1997 ACM 0-89791 -945 
-9/97 /0005 ...$3.50 of document re~ieval experiments in the information retrieval literature. In the 
next section, we describe the representation of software compvmts used by our library tool. In section 
3, we describe the multi-faceted retrieval of software components. The retrievaf exue.riments are described 
in section 4. We conchtde in section 5. ­ 2. Representing reusable components 2.1. Overview Thii work 
is part of ongoing research at the University of Qu6bec at Montr&#38;l aiming at developing methods and 
tools for developing reusable software (see e.g. [7, 14, 18]), and for developing with reusable software 
(see e.g. [15, 16, 19]). Our work on supporting development with reusable components centers around a 
tool kit called ClassServer that consists of vari­ous tools for classifying, retrieving, navigating, 
and presenting reusable components (see Figure 1). Reusable components con­sist essentially of object-oriented 
source code components, occa­sionally with the accompanying textual documentation. The internal representation 
of reusable components supports the four reuse functionalities mentioned above, namely, classification 
retrieval, navigation, and presentation. Raw input source files are put through various tools called 
extractors which extract the relevant pieces of information and package them for the purposes of the 
four reuse functionalities. The information extracted by these tools includes language-defined structures, 
such as classes, methods,parameters. To variables, and methodthese, we added views, which are client-or 
application-specific interfaces that classea may support, and the notion of object jhrmeworks, which 
are class-like object aggregates that are used to represent application frameworks and design patterns 
[20]. Each kind of component is defined by a descriptive template which includes: 1) structural information 
describing the kind of subcomponents a component can or must have (e.g. a class has views, a view has 
variables snd methods), 2) code, which is a string containing the definition or declaration of the component 
in the implementing language, and 3) descriptive attributes, which are used fol search purposes, as in 
a claw has an author and an application domai~ method has a purpose, etc. The tool set may be seen as 
consisting of three subsys­tems. The first subsystem supports the required functionalities for full-text 
retrieval of source code files. Simply puL words from user queries are matched against the contents of 
source code files (or other kinds of textual information). Before this matching is done, a number of 
pre-processing steps are per­formed with two goals in mind: 1) speeding up searck and 2) maximizing the 
chimces for matchmg by removing inessential lexical variations. To speed up the search an inverted list 
is created once and for all, which is a table whose keys are unique meaningful * words and whose values, 
for each word, are the documents in which that word occurred. In order to remove unessential lexical 
variations, the words of the table are first put through a word stemmer which reduces a number of word 
forms to the same stem (as in facility and facilities reduc­ing to facilit ). The component browser and 
the keyword retrieval sub­systems use the structured representation of the components extracted using 
the tool referred to as semantic/structured 1. Commonweds such as the however , etc. are not taken into 
account in either the inverted list or the queries, parser in the Figure. The parser for C++ was developed 
using Lcx and Yacc, and a public domain C++ grammar [23], which we augmented to handle templates. The 
Smalltalk parser was written directly in Smalltafk. For both parsers, the parsing pro­duces a trace of 
the traversal of the abstract syntax stree. The trace consists of a batch of component creation commands, 
which are executed when we load the trace; that is the struc­ tured component loader.  2.2. A Multi-Faceted 
Classification of Components Attributes are used in ClassServer to represent categorization/classification 
facets, as in Rieto-Diaz s multi­faceted categorization of components [21]. Auributes are them­selves 
objects with two properties of their owrx 1) te.xf, which is a (natural language) textual description, 
and 2) values, which is a collection of key words or phrases, taken from a predestined set referred to 
as the vocabulary of the attribute. The text is used mainly for human consumption and for documentation 
generation [13]. Filling in the values property is referred to as categorization or indexing. Typically, 
human experts read about the software componen~ and chose key words or phrases from a predetirted lis~ 
this is referred to in the information retrievaf literature as munual controlled-vocabulary intkcing 
[24]. In some cases, we used automatic controlled-vocabulary indexing whereby a key word or phrase is 
assigned to an attribute if it occurs within the tatfield. More on this in section 4.2.2. For a given 
attribute, e.g. Purpose , multiple values are considered to be alternative values , rather than partial 
values. For a given vocabulary, the terms of the vocabulary (key words and phrases) may be organized 
along a conceptual hierarchy. Figure 2 shows excerpts of the conceptual hierarchies of key phrases for 
the attributes Application Domain (Figure 2a) and Purpose (Figure 2.b). Typically, for a given attributq 
the keywords are organized in a single hierarchy whose root is the name of attribute itself. Notice that 
the Application Ekmain hierwchy of key phrases is inspired from the (ACM) Computing Reviews s classification 
structure [4]. The hierarchical relation­ship between key phrases is a loose form of generalization, 
commonly referred to in information retrievaf as Broader-Term [24]. Attribute values (key words and phrases) 
me used in boolean retrieval whereby component attribute vrdues are matched against required attribute 
values (queries); more on this in $ 3. The hierarchical relationships within an indexing voca­bulary 
are used to extend the basic retrieval algorithms by adding different degrees of matching (instead of 
true or fake) between two key words, which now depend of the iength of the path separating them in the 
hierarchy (see [16]).  3. Muti-faceted Retrieval Of Reusable Components ClassServer includes a search 
tool--called ChrsSeardter--[hat enables developers to retrieve software components based on the keyword 
values of their attributes. We implemented three matching algorithms: 1) a weighted boolean retrieval 
(see e.g. [24]), 2) conceptual distance measures, and 3) classification. Both conceptual distance measurements 
and classification use semantic relationships between keywords­hierarchical relationships for the case 
of classification [16]. For the purposes of tits paper, we limit our discussion to boolean and weighed 
boolean retrieval. We begin by discussing the structure of queries and the operations that can be prformed 
on them. - .­ 1 r Full-text retrieval Keyword-retrieval T I~ I FGiii=id T -1 / 1k%$h:bab] / #LJlgl&#38;> 
 II rhis class:: ~ikc, Peraqn(mtage,...) // rfrlsConstnsc[o .,. } classWoman: Perso ) Data flow Figure 
1. Overall architectu~ of ClassSetver z AdministrativeDataProwssing ComputerAidedEngineesing Application 
airs Filtering---\\ LifeAndMedicalSciences Samptirlg-- DaraAcquld . ! ~ \ SocialAndBehavioralScisme.s 
EncodingDecoding- Communica< MessagePassin&#38; -­ , (a) DataConvemiew -- DataProcess< , DataBasefnIerrogation 
T QuestionAnswering~cisionSu~ foma*Re . .Forccastrqy - DocusnentRelricval CommandAndCknrtrol (b) Figure 
2. Hierarchies of key phrases for the attributes Application Domain and Purpose . 3.1. Queries measures, 
respectively. Symbolically: * Our choice for the representation of queries involved a Query ::= AQT I 
AQT AND Query trade-off between flexibility and expressiveness, on the one * AQT ::= Attribute Weight 
CutOff ListOfKeyPhraseshand, and allowing users to specify the most common queries * ListOfKeyPhrases 
::= Key Phrase I Key Phrase OR Lis­most easily and most efficiently, on the other. As a guiding tOfKeyPhrases 
 principle, we liened the specification of a query to the process of specifying a prototypical component. 
Accordingly, the simp­lest form of a quety is a list of so called atwibtie query terms A single AQT retrieves 
the cxsmponenta whose attribute <Attri­(AQTs), considered to be ANDed. h its simplest form, an bute> 
has at least one value in common with <LutOfKey -AQT consists of an attribute, and a list of key phrases, 
con-Phraaes>. Viewing attributes aa functions, an AQT denoted by sidered to be ORed. In the actual implementation, 
each AQT is the four-tuple cAttribute, Weight, Cut Off, LktOfKeyPhrase~ assigned a weight and cut-off 
poin~ used for weighted boolean retrieves the components C such that Attribute n Lis­retrieval, discussed 
further below, and conceptual distance tOfKeyPhrases #0. The query denoted by the tuple (AQTt,,.., ,4 
QTk), returns the intersection of sets of components that would have been returned by the individual 
AQTs. With weighted boolean retrieval, components are assigned numerical scores that measure the extent 
to which they satisfy the query, instead of being either in or out , Let Q be a query with terms (AQT1,...,AQTJ, 
where AQT, = <Attibut~,Weigh~, CutOffi,ListOfKey Pftrmesi>. The score of a component C is computed as 
follows: ~Weigh@core(AQTi,C) Score(Q,C) = 1 (1) ~Weigh~ j=) where Score(AQTi,C) equals 1.0 if ListOfKeyPhrasesi 
n Attribute,(C) # 0, and O otherwise. Notice that only some combinations of AQTs make sense since each 
kind of component has a different set of applicable attributes. This, plus our goal of making the specification 
of queries easy led us to support two ways of ini­tializing queries: 1) by specifying the kind of component 
we wouId like to search on, or 2) by specifying a component. In the fist case, the query is initialized 
to an initial list of AQTs corresponding to the default set of atrnbutes for the component category with 
empty lists of key phrases; the user has then the option of adding and/or removing AQTs, and must specify 
lists of key phrases for the AQTs. When a query is initialized from a componen~ in addition to using 
the attributes of the com­ponent to select the initiaf list of AQTs, we use the values of those atrnbutes 
to initialize the list of key phrases for those AQTs. We believe that this will actually be the most 
natural way of submitting a query, in terms of integrating in the worktlow of developers: typically, 
a developer would start entering, explicitly or implicitly, the specifications of a com­ponent to be 
built depending on the problem at hand, and on the component s interactions with other components; those 
par­tial specifications may serve as a way of searching, in the library of components, for that that 
satisfies those require­ments. 3.2. Operations On Queries Queries in ClassSearcher may be combirre~ once 
pro­ cessed, using the usual set-theoretic operators (union, intersec­ tio~ and set difference) directly 
on the answer lists. This enables developers to formulate arbitrarily complex queries involving negation, 
ORed AQTs, ANDed values for the same AQT, or combinations thereof. Note, however, that within a single 
query, the AQTs are always ANDed, and multiple vafues are always ORd. This forces us to write queries 
such as FIND COMPONENTS C SUCH THAT [Purpose(C) = Rename A Copy A ~(Move)] OR [IntendedUsers(C) = UnixHacker] 
 as a boolean expression of four separate queries. Namely, let QI = ((purpoSe,{Rename})).Q:; ((Purpose, 
$,opy})), @ = ((Purpose,{Move})), ((IntendedUsers, {UnixHacker})), the above request can ; satisfied 
using the boolean expression Q = (QIA CLA -@i) V Q,), or the equivalentexpressionusing set operations: 
(Q1n(&#38;Q.JwQ4. We chose not to generalize the format of queries for the purposes of accommodating 
the above query more readily, for a number of practical and theoretical reasons, FirsL we believe that 
such queries are relatively uncommon so as not to warrant compl icsting the format of queries, or offering 
two different formats. Second, from a theoretical point of view, negation is difficult co express mathematically 
in the context of graph­based conceptual distance measurements (see [22]). Finally class ifrcation is 
hard to interpret semantically if a query con­tains two ORed AQTs that correspond to different attributes 
[12]. 3.3. ClassSearcher Figure 3 shows the graphical interface to ClassSearcher. The upper third of 
the window contains two identical lists of queries, identified by names. The selection in the left list 
determines the state of the rest of the interface. The right list ia used simply to specify (i.e. select 
with the mouse) the second operand to tie set/logical operations. The pane with heading Query Transcription 
is used for output only, and shows m SQL-like tmnscription of the currently selected query in the left 
lis~ notice that weights and cut-off values are not represented in the transcription. Figure 3. Graphical 
Interface of ClassSearcher. The pane labehd Search Types is used to set the parameters (cut-off, to 
the right) and perform (button SEARCH) the currently selected search type BooleanSearch in this case. 
Result List(score) shows the answer list for the currently selected search type, ordered by decreasing 
order of score (shown between parentheses). The button Add Term is used to add an attribute query term 
(AQT) to the currently selected query. The list of AQTs for the currently selected query, with their 
weight and cut-offs shown between parentheses, is shown in the leftmost bottom pane labeled Query Terms(weight,cut 
off). The buttons Remove , weight , Cut Off , and Add Value are relative to the currently selected query 
term (AQT), with self-explanatory names. Add Value enables developers to navigate through the hierarchy 
of keywords of the corresponding attribute (see e.g. Figure 2a) to select a value. The button Rem. Value 
removes the currently selected value in the list above it (Term Values). 4. Retrieval experiments 4.1. 
Experimental design In this first set of experiments, we were more concerned with establishing the usefulness 
of the library tool in a produc­tion setting than we were with performing comparisons between the various 
retrieval methods. It is our belief that such comparisons do not mean anything if a developer won t use 
ANY of the methods in a real production setting. The decision for a developer to use or not use a tooi 
has to do with: 1) his/her estimate of the effort it takes to build the components from scratch [26], 
2) the cost of using the library tool, inchsd­ing formulating the queries and looking at the results, 
and 3) the perceived track record of the tool and the library in terms of either finding the right components, 
or quickly convinc­ ing the developer that none could be found that satisfy the query. Typically, comparative 
studies between the retrieval methods focus on the retrieval performance, regardless of the cost factors. 
Second, to obtain a fair and finely detailed com­parison the format of the queries is often restricted 
in those experiments to reduce the number of variables, to the point that they no longer reflect normal 
usage of the library. With these considerations in mind we made the follow ­ing choices: 1) we only cxmttrolled 
the search method that the users could use to answer each of the queries, without giving a time limit 
on each query, or a limit on the number of trials made for each query; we assumed that users will stop 
when they are convinced tiat they have found all that is relevanh 2) we spied on the subjects interactions 
witlr the tool by recording a trace of the actions performed. This allowed us to obtain finer experimental 
data without interfering with the subjects workflow. Note that the trace &#38;ta may not always be of 
sufficient quality to make reliable statistical inferences. For example, with boolean retrieval, subjec~ 
could search on two search atrn­butes, separately or in combination. One attribute, Applica­tion Domain 
, was indexed manually with a manually-built vocabukuy, while the other, Description , was indexed automatically 
with the automatically generated hierarchy (see $ 4.2.1 ). We didn t ask the subjects to use one or the 
other, or both in combmation. When we studied the tnces, it turned out that the Description attribute 
was used in only two of the 43 keyword queries performed by the different subjects, and neither query 
returned a relevant document. Accordingly, we have no basis for comparing the quality of the two attributes. 
However the fact that the Description attribute was used only twice tells us that subjects didn t feel 
it provided useful information, and tha~ in and of itself, is a valuable data. The experimental data 
set consisted of aboul 200 classes and 2000 methods from the OSE library (see $ 4.2). We used 11 queries, 
whose format is discussed in section 4.3. Seven subjects participated in the experiment. All subjects 
were experienced C++ programmers. They included two professors, three graduate students, and two professional 
developers work­ing for the industrial partners of the project. Two subjects (a professor and a graduate 
student) did not complete the experi­ment, and we had to discmd the results of the few queries they DID 
complete. The subjects were given a questiomtaire which included the statements of the queries, and blank 
spaces to enter the answer as a 1ist of compmrent names, much like an exam book. For each of the initial 
77 (subjec~query) pairs, we randomly assigned a search method (keyword-based versus plain text). For 
each (subject,query, search method) triple~ the subject could issue as many search statements as s/he 
wishes using the designated search, with no limitation on the time or on the number of search statements. 
The experiment started with a general presentation of the ftsnctionafity of the tool set (about 45 inn), 
followed by a hands-on tutoriaf with the tool set (about 1 hour), providing the subjects with art tmderstand­ing 
of the theoretical underpinnings of the functionalities, as well as some practical know-how. Before leaving, 
the subjects were asked to fill out a questionnaire to collect their qualitative appreciation of the 
toolset. In order to analyze the results, we used the query ques­tionnaires to compare the subjects 
answers to ours, which were based on a thorough study of the library s user manual and some code inspection 
where warranted. For this first experiment, the traces generated for the subjects were analyzed only 
to determine which attributes were used for boolean key­word retrieval. The traces included enough details, 
however, to support more and finer analyses. 4.2. The library As mentioned earlier, the data set consisted 
of the entire OSE library [5], which contained some 200 classes and 2000 methods distributed across some 
230 files. A shell script put the files through a C++ pre-processor (to process the #include directives, 
among others) before they were input into the C++ extractor, which generated a file of Smalltafk commands 
to construct the reusable C++ components. Because of the good quality and format consistency of the in-line 
documentation, we were able to assign C++ comments as text values for the Description attribute of various 
components (classes, methods, variables). For the purposes of this experiment, we classified com­ponents 
using two attributes ApplicationDomain , and Description . Classification according to Application-Domain 
was done manually, although fairly systematically. For this attribute, the classification hierarchy followed 
closely the table of contents of the textual documentation, whose organization is based on the application 
areas covered by the library. Generally speaking, if a component is dkcussed in some paragraph, we assign 
it the title of the smallest section that contains that paragraph. A consequence of this indexing scheme 
is that not all methods had values for the attribute Application Domain . For example, constructor methods 
were rarely discussed explicitly, and ye~ are always present. However, we can be sure that all the classes, 
or some of their superclasses had values for Application Domain . For the Description attribute, all 
the steps were automated, from the construction of the vocabulary, to the actual indexing of reus­able 
COItIpOIteIttS. Both steps are briefly discussed below. While the quality of the indexing vocabulary 
and of the index­ing method have a significant impact on the quality of retrieval, because the experimental 
subjects seldom used the attribute Description in their queries, we shall keep the descriptions very 
brief (see [19] for a thorough discussion); on the other hand, this may very well be the ultimate measure 
of the quality of that attribute, i.e. whether it was deemed useful or not by the subjects. 4.2.1. Building 
a hierarchy of domain concepts These are two aspects to building a conceptual hierar­chy. FWSL there 
is the problem of finding a set of terms (key­words or key phrases) that desaibe the important concepts 
withii a domain of dmcourae, and that use the most widely accepted terminology. To this end, we used 
a variant of statist­ical indexing which, given a document collection, it considers ss valid cament indicators 
those terms which occur neither too often, nor too rarely, and which occur unevenly in the docu­ment 
collection i.e. are concentrated in a small subset of the document collection [24]. However, instead 
of using single words as potential content indicators (or index terms), we used noun phraaesz , which 
we extracted from the OSE software documentation using Xerox s Parts of Speech Tagger (XPost) [3]. Having 
identified the set of important domain concepts it is important to organize them in a conceptual hierarchy, 
both for navigation purposes (e.g. to find the proper term to use in a query) and to support the extensions 
to boolean retrieval mentioned earlier (see also [19]). We adapted an algorithm that we had developed 
to organize a set of index terms into a graph-more specifically, a hierarchy based on their usage profiles 
within a document collection [11]. The basis of that algorithm was the hypothesis that index terms that 
often characterize the same documents (i.e. co-occur in the docu­ments indices) tend to be related, and 
should be connected; additional heuristics were used to refine the relationships [11]. The new a&#38;ptation 
of the algorithm performed rather poorly compared to the experiment described in [11], and this for a 
variety of reasons, including the more problematic nature of the da~ and the much smaller size of the 
data set [19]. Note however that the quality of the relationships within the gen­erated graph has no 
impact on indexing (discussed next), and that in the end, it had no measurable impact on rerneval per­formance 
because the attribute Description was seldom used. 4.2.2. Automatic indexing from a controlled vocabulary 
The information retrieval literature makes the distinction between manual controlled vocabulary indexing, 
and automatic unconmolled vocabulary indexing [24]. With manual con­trolled vocabulary indexing, subject 
experts read documents and assign to them an index consisting of several terms, meant to be content descriptors, 
taken from the predeflrted set of key terms, referred to as the controlled vocabulary. By contrast, the 
basic premise behind automatic uncontrolled-vocabulary index­ing is that the words that occur in the 
document with a certain statistical profile are good content indicators. The relative mer­its of the 
two approaches have been thoroughly debated in the literature (see e.g. [2, 2S], and we won t indulge 
into the debate for the purposes of this paper. Suffice it to say that we used a mix of the two methods: 
we did use a controlled voca­ 2. Curnputerscience bcins a relatively new field, most concepts am stilt 
described by noun phmsses, as in Software Engineering Bubble Smt , Riming Monitm , ere., radur than 
singte words as is the essc fu mom maure fietds such as mcdicinq see [23] for a fasciristing trestmcs!t 
of lhe cvotution of languages and rerrninology. bulary for indexing and retrieval, but the indexing was 
done automatically [19]. At first glance, this approach shares simi­lar problems with automatic plain-text 
indexing and retrieval to the extent that classification (and retrieval) depends on authors having used 
the same words as the controlled vocabulary, and on the fact that matches are done in context. We argued 
in [19] why this is not a big problem here, and that we are able to achieve the advantages of controlled 
vocabulary indexing at a fraction of the cost. Simply pu~ automatic indexing with a controlled voca­bulary 
works as follows: a document D is assigned a term T=wlwz . ~. Wn if it contains (most of) its component 
worda, consecutively ( ... w]wz ~ w.... ), or in close proximity ( ...wlnlnzwzwJ . . . wn... ). In our 
implementatio~ we reduced the words of both the terms of the vocabulary and the docu­ments to their word 
stem by removing suffixes and word end­ings. Also, we used two tunable parameters for indexing: 1) proximity, 
and 2) threshold of number of words found in a document, to the total number of words of a term, before 
the term is assigned to the document; we refer to it as the purtial mulch rhreshofd. The proximity parameter 
indicates how many words apart should words appear to be considered part of the same noun phrase (term). 
Maarek et al. had found that 5 worked well for two-word phrases in english [9]. It has been our experience 
that indexing works best when both parameters depend on the size of the term. This method was applied 
to the Description attribute using the vocabulary produced by the algorithm described in # 4.2.1. As 
it turned OUL the quality of indexing for the Description attributes did not really matter &#38;cause 
the attri­bute was used only twice (see section 4.1). 43. Queries Information retrieval systems suffer 
from the difficulty users have in translating their needs into searchable queries. The issue is one of 
translating the description of a problem ~theirneeds) into a &#38;scription of the solution (relevant 
docu­ ments). With document re~ievrd systems, problems may be stated as I need to know more about ~> 
, and solutions as A document that tafks about <Y> . For a given problem, the challenge is one of making 
sure that cX> and <Y> are the same, and in systems that use con?solled vocabulary in&#38;xing, traird 
librarians interact with naive users to help them use the proper search terms, With sojlware componenl 
retrieval, the gap between problem statement (a requirement) and solution &#38;scription (a specification) 
is not only terminological, but also conceptual. In an effort to minimize the effect of the expertise 
of subjects in art application, and their familiarity with a given fibrary, component retrieval controlled 
experiments usually use queries that correspond closely to component specifications. This does not reflect 
normal usage for a reusable components library tool because. For instance, users typically do not know 
how the solution to their problem is structured, and for the case of a Ctt component library, e.g., the 
answer could be a class, a method, a function, or any combination thereof. It has gen­erally been observed 
that developers need to know the underly­ing structure or architecture of a library to search for com­ponents 
effectively [17]. Accordingly, in an effort to get a realistic experimen~ we formulated our queries as 
problems to be solved. Each query was preceded by a problem description setting up the contex~ followed 
by a statement Fred a wuy of qxrfonning a given tasb . The problem description is also used to familiarize 
the subjects with the terminology of the application domain using textbook-like language3 4.4. Component 
relevance The difference between traditional bibliographic docu­ment retrieval and reusable component 
retrieval manifests itself in the retrieval evaluation process as well. The concept of relevance, which 
serves as the basis for recall and precision measures, is notoriously difficult to define. Within the 
context of bibliographic document retrieval, a search query for a con­cept X is understood as meaning 
I want documents that talk about X , and hence a document is relevant if it tafks about X. This definition 
is different from pertinence which reflects a document s usefulness to the user [24], which depends, 
among other things, on the users prior knowledge, or on the per­tinence of the other documents shown 
to them. Recall, which measures the number of relevant documents returned by a query to the total number 
of relevant documents in the docu­ment se~ implicitly assumes that sfl the relevant documents are equally 
pertinent and irreplaceable: the user needs all of them. In other words, assuming that a query Q hm N 
relevant docu­ments, and retrieved a set of documents S = {D1,..., D~}, we can define pertinence or usefulness, 
and recall as follows: , 1. , If Di is relevant N PJ3RT(Di)= 0, if Di is not relevant and { PERT(S) = 
RECALL(S)= ~PERT(Dj) j=] With software component retrieval, the notion of usefulness and substitutability 
are much easier to define as both relate to a developer s ability to solve a problem with the components 
at hand. Symtmlicsfly, we view query as a requirement Q, which may be satisfied by several, possibly 
overlapping, sets of com­ponents Sl,...,S~, where Si =. {(Di,,Di,, ....Di.}. For each i=l ,...,k, we 
have, as a first approximation: PERT(Si) = PERT(Di,,Di,,... $Di,) = ~PERT(D~i) = 1 (A) j=l whine pERT(D/fji) 
is the usefulness or pertinence of the cOm­ ponent D in the context of the solution set Si. This illustrates 
the fact that a retrieved component D is useful only i~ the other components required to build a solution 
are retrieved with it. Further, this definition of PERT means that total user satisfaction can be achieved 
with a subset of the set of relevant dccuments, which is not the case for recall. We ilhsstrate the properties 
of PERT through an example. Consider two solutions sets S, = {Dl, D2} and S2 = {D1,D3,Dd}, and assume 
that D,, D2, and Dg have the sizes 30, 20, 40, and 30, respectively, giving S1 and S2 the sizes 50, and 
100, respectively. We can use the relative sizes of the com­ponents with respct to the enclosing solution 
as @eir size(Di) contextualhxmditional pertinence, i.e. PERT(DJSj) = ~ SIZt?(S1) In this case PERT(D1/SL) 
= 0.6, PERT(DJSI) = 0.4, PERT(D1/SJ = 0.3, PERT(Ds/S~ = 0.4, and PERT(D~S2) = 0.3, Assume that a query 
retrieves the component Dl. In this case, PERT(D1) = Max(PERT(D1/S1), PERT(Dl/S~) = 0.6. If the query 
retrieved DI and ~, instead, PERT({D1,D3}) = Max 3, The subjects were mostty frwrch-spukingwhile he tibrary 
s d-eatstim md ccistrottadvoeabutsrks wem in Engtish. (PERT( { D), D~}/Sl ), PERT({D:,Dj}/S2)) = Max 
(PERT(D1/Sl) + PERT(DJS1 ), PERT(D1/SJ + PERT(D~SJ) = Max (0.6 + 0.0, 0.3 + 0.4) = 0,7. This illustrates 
the fact that when several partial solutions are returned by the system, we take into account the one 
that is most complete, and the vafue of individual components is relative to that solution. Symboli ­cally, 
given the solution sets Sl,...,S~, a query that returns a set of components S has the pertinence: PERT(S) 
= jM~LPERT(SmS~j) @) Finally, we add another refinement which takes into account the overlap of two 
mmponents within the same sohr­tion set. Consider the solution S, above, and assume that the system retrieves 
D, and D2, where Dz is u superck.ars of D2 that implements only part of the functionality required of 
D2. In this case, we could take PERT(Dl,D~ = 0.6 + 0.3 = 0.9. If the query retrieved D 2 AND D2, then 
we discard the weaker compmwnt. This is similar to viewing solutions sets as role fillers and for each 
role, take the component that most closely matches the role. Within the context of reusable 00 com­ponents, 
roks may be seen as class interfaces, and role fillers as class implementations. For our experiments, 
some of the 11 queries were straightforward in the sense that there was a single component (a method 
or a class) tha[ answered the query, and both com­ponent relevance and recall were straightforward to 
compute. Queries whose answered involved several classes collaborating together (e.g. an object framework) 
were more mmplex to evaluate and involved all of the refinements discussed above. For the case of precision, 
we used the traditional meas­ure, i.e. the ratio of the retrieved components that were relevant (i.e. 
had a non-zero PERT(.)) to the total number of retrieved documents. We can also imagine refining the 
definition of precision to take into account the effective useful­ness of the individual components, 
and factor that in with the cost of retrieving and examining a useless component. The cost of examining 
a useless component is a function of its complexity, and size could be used as a very first approxima­tion 
of that complexity. 4S. Performance results Table 1 shows recall and precision for the 11 queries. Initially, 
with the initial 7 subjects, for each query, we selected 3 subjects at random to perform the query using 
full-text retrieval, and 4 subjec~ to perform keyword retrieval, or vice versa, while making sure that 
each subject had a balanced load of full-text and keyword queries (6 and 5, respectively, or vice-versa), 
Because the results of two subjects could not k used, we ended up with some queries answered by 4 subjects 
using full -text retrieval, say, and only once using keyword retrieval (see e.g. query 2). The 11th query 
was rejected because the three keyword-based answers were all rejected for one reason or another. Hence, 
comparisons between the two methods for the individual queries are not reliable. Intuitively, it appears 
that plain-text retrievaf yielded signiticantfy better recall and somewhat better precision. It also 
appears that it has done consistently so for the 10 queries, with a couple of exceptions. In order to 
validate these two results statistically, we have to ascertain that none of this happened by chance. 
We performed a number of ANOVA tests, to check whether recall and precision were random variables of 
the pair (query, search method), and both tests were rejected [1]. Next, we isolated the effect of the 
search type to see if the difference in recall and precision performance is significant. The results 
collection of componerm with complex interactions, often a are shown in Table 2. mix of classes and methods. 
With full-text search queries retrieve indiscriminantly methods and classes. With Full-text retrieval 
Keyword retrieval Q. Sub] Rec Prec Sub] Rec Prec 13 100 88.66 2 50 50 24 50 100 150 100 3 1 100 100 
4 100 100 4 1100 80 4 50 100 , III , 8, Is 1 4125 1 2.5 1 00 6 3 33.33 33.33 2 12.5 25 7 2 65 7.5 3 
66.33 50 82 30 7.5 3 30 83.33 9 3 53.33 100 2 30 78 10 II 3 78.33 80.33 1 35 100 I Avg ][ (26) 63.49 
74.47 (23) 42.41 68.33 Table 1. Summary of retrieval results. Effect of search method Recall Precision 
F Value 4.1 0.93 Pr>F 0.0500 0.3404 Table 2. Significance of differences between plain-text retrieval 
and keyword retrieval. The Pr > F shows the probability that such a difference in performance could have 
been obtained by chance. It is gen­erally accepted that a threshold of 5 ~rcent is required to affirm 
that the differences are significant. Thus, we conclude that Full-text retrieval yields provablykignificantly 
better recall than controlled vocabulary-based retrieval Full-text retrieval vieids com~arable rmecision 
~er­formance to that of contr&#38;led voc~bulary-b&#38;ed retrieval Our resuits seem to run counter 
to the available experimental evidence. Document retrieval experiments have consistently shown that controlled 
vocabulary-based indexing and retrieval yielded better recall and precision than plain-text search [2, 
24, 2S], although the difference hardiy justifies the extra costs involved in controlled vocabulary-based 
indexing and retrieval [25]. Similarly, a comparative retnevai experiment for reusable components conducted 
by Frakes and Pole at the SPC showed that recall values were comparable, and a supe­rior precision for 
controlled vocabulary-based retrieval [6]. Most surprising in our resui~ is the significant difference 
is recall performance. We sought to explain the courtter-intuitive/evidence difference in recall performance. 
We first note that out of the 11 queries, some were supposed to retrieve single components (often methods), 
as in Query 7, formulated as getting the length of a string , and the others were supposed to retrieve 
a 3. Fraku and Pole compared 4 methods, and tkii test of stausucal significance was based on variance 
analysis of dse precision averages for the four methods, which was inconclusive [6]. However, we are 
quasi-cenam thaL by p.domnng pairwise comparison between plain-mxt search (50%) and comrotted vocabulary 
search (what appears to be lCKl% m tic plot [6]), hey would have esmbtished, swtisticatty, be superiority 
of controlled­ vccabulary rauieval controlled-vocabulary search, users have to specify the kind of components 
they are seeking (a class or a method), since the two types do not support the same set of attributes/facets; 
this makes the search more tedious and users may give up easily. For this explanation to hold, there 
has to be a marked differ­ence bet ween the performance for the singie-component queries (queries 1,7,8,9) 
and the queries whose answers con­sisted of collections of components (queries 2,3,4,5,6,10). Tabie 3 
compares the two kinds of queries. Table 3. Comparing the two sets of queries our hypthesis that plain-text 
retrieval favors component col­lection queries is not validated. Along the same lines, we hypothesized 
that plain-text retrieval favored queries whose answers involved a mix of methods and classes, or just 
methods, since the same query would retrieve both kinds of components. Table 4 shows recall and precision 
values for the two methods, separated into the two kinds of queries. Table 4. Comparing the two sets 
of queries depending on whether they remieve methods or not. Interestingly, there is a marked difference 
in performance between the two groups of queries. However, in both cases, plain-text retrieval is mmkedly 
superior to controlled­voeabulsry retrieval with reg srd to retail (and precision, for the case of queries 
retrieving methods). bother possible explanation for the lower performance of controlled-vocabulary based 
retrieval is reiated to the quality of indexing, but the results didn t seem consistent with the hypothesized 
effect on retnevai performance [19]. 5. Discussion We set out to deveiop, evaluate, and compare two classes 
of component retrieval methods which, supposedly, strike different balances along the costs/benefits 
spectmu-n, namely, the (quasi-) zero-investment free text classification and retrieval versus the uplYont 
investment-laden but presumably superior controlled vocabulary faceted indexing and retrieval. Recent 
experiments with software component repositories have put into question the cost-effectiveness of the 
controlled voca­bulary approach, but not its superior or at least us good retrieval performance [6]. 
We attempted to bring the two kinds of methods to a level-playing field by: 1) automating as much of 
the pre-processing involved in controlled vocabulary ­based methods as possible to address the costs 
issue, and 2) using a realistic experimental setting artd realistic evahtation measures. Our experiments 
showed that: 1) those aspects of the pre-processing invoived in controlled vocabulary methods that we 
automated were of poor enough quality that they were not used (the Description attribute; see section 
4.2), and 2) the fully automatic free text search performed better than the fully manurd controlled-vocabuhry 
based indexing and retieval of components. Because this result is somewhat counter-intuitive, we continue 
to analyze the results, which suggest, in some cases, a tnisunderstandmg of the semantics of multiple-attribute 
queries, or multi-valued attributes, leading to what seemed to be a number of aimless queries with no 
clear search strategy. It thus appeared that the two-hour tutorial was not sufficient and users could 
have used some further experience with the toolset4. However, whichever additional effort we can put 
into, either the construction of the vocabulary, or the indexing of com­ponents, or the training of users 
of the tool, will only add to the costs of controlled vocabulary multi-faceted classification and retrieval, 
and we are not guaranteed, by any means, to achieve bettes results than with plain text search. Perhaps 
more irrtportarrtly, four subjects out of five preferred plain-text search. We hypothesize that multi-faceted 
classification and retrieval of reusable components to be at the wrong level of formality for the typicaf 
worktlow of developers using a library of reusable components. We identify two very distinct search stages. 
The first stage is fairly exploratory, as developers do not yet know which form the solution to their 
problem will take, and a free-format search technique such as plain-text search is appropriate. Multi-faceted 
search may be too rigid and constraining for this early search step. This is even more so, considering 
that one might be searching components in several sites, each with its own representation conventions. 
The second search stage aims at selecting, among an initial set of potentially useful components, ones 
that will effectively solve the problem at hand. At this second stage, we need a far more detailed description 
of components and their inter-relationships than that provided by multi-faceted classification. Acknowledgements: 
This work was supported by grants from Canada s Natttraf Sciences and Engineering Research Council (NSERC), 
TANDEM Computers, Qu&#38;rec s Fends pour la Crdiztion et l Ai&#38; h la Recherche (FCAR), and Qur%ec 
s Minis;tire de i Errseignement Supt?rieur et de la Science (MESS) under the IGLOO project organized 
by the Centre de Recherche Irtforma­ tique de Montr6al. Bertrand Foumier, a statistician with the Service 
&#38; Consultation en Anaiyse de Donne es (SCAD, http://www.scad. uqam.ca) provided us with invaluable 
assistance in measuring and interpreting the results. References <RefA>1. Estelle Ah-Ki, in Reutilisation 
de Cornposantm Logi­cieiks Orientees-Objet, Department of Computer Sci­ence, Universiv of Quebec at Montreal, 
Montreaf, Canada, July 1996. 150 pages  2. David Blair and M E Maron, An Evaluation of Retrieval Effectiveness 
for a Full-Text Document-Retrievaf System, Commum c&#38;ions of the Associatwn  4, The smitx AIM of 
this paper, who did his Doctoral research cm intsttigcnt retrkvd systmrs, stitt can t find a bock using 
the Univasity of Quebec s tibrary bibliographic re&#38;ieval systsm, after S years an the facutty. for 
Computing Machinery, 28, 3, pp. 289-299, March 1985. 3. Doug Cutting, Julian Kupiec, Jan Pedemen, and 
Penelope Sibun, A Practical Part-of-Speech Tagger, in Proceedings of the Applied Natural .hnguage Pro­cessing 
Co#erence, 1992. 4. D Denning, J Minker, A Parker, A Ralston, E Reilly, A Rosenberg, C Walston, T Willoughby, 
J Strtnme~ and A Blum, The Proposed New Conqmting Reviews Classification Scheme, Conmunic atwnr of the 
Asscxi­ation of Computing Machinery, vol. 24(7), pp. 419434,  Ju]y 1981. 5. Graham Dumpletom in OSE 
-C++ Library User Guide, Dumpleton Software Consulting Pty Limited, Par­ramatta, 2124, New South Wales, 
Australia 1994. 124 pages 6. William B. Frakes and Thomas POIG AII Ernpiric~ Study of Representation 
Methods for Reusable Software Components, IEEE Transactions on Software engineering, pp. 1-23, August 
1994. 7. Robert Godin and Hafedh Mili, Building and Main­taining Analysis-Level Class Hierarchies Using 
Galois Lattices, ACM SIGPLAN Notices, vol. 28, no. 10, pp. 394-410, Washington, D.C., 26 Sept -1 OCL 
1993. 00PSLA 93 Proceedings  8. Robert J. Hall, Generalized Behavior-based Retrieval, in Proceedings 
c# the 15th Internatwnal Conference on Software Engineering, pp. 371-380, ACM press, Baftimore, Maryland 
May 17-21, 1993. 9. Yoelle S. Maiwek, Daniel M. Berry, and Gail E. Kaiser, An Information Retrieval Approach 
for Automatically Constructing Software Libraries, IEEE Transactions on Sojiware Engineering, vol. 17 
(8), pp. 800-813, August 1991. 10. Ali Mili, Rym Mili, and Roland Mittermeir, Storing and Retrieving 
Software Components: A Refinement-Based Approach, in Proceedings of the SL@zrmth International Conference 
on Software Engineering, Sorrento, Italy, May 1994.  11. Hafedh Mili and Roy RadA Building a Knowledge 
Base for Information Retrieval, Proceedings of the Third Aruwal Expert Systems in Government Confer­ence, 
pp. 12-18, October 22-25, 1987. 12. Hafedh Mili, in Building and Maintaining Hierarchical Semantic Nets, 
The George Washington University, August 1988. Doctoral Dissertation 13. Hafedh Mili and Manon Grenier, 
Managing Documen­tation for Software Reuse, Information and Deciswn Technologies, vol. 18, pp. 115-134, 
1992. 14. Hafedft Mili and Haitao Li, Data Abstraction in SoftClass, an 00 CASE Tool for Software Reuse, 
 in Proceedings of TOOLS 93, ed. by Bertrand Meyer, pp. 133-149, Prentice-Hall, Santa-Barbara, CA, August 
2-5, 1993. 15. Hafedh Mili, Roy Rad&#38; Weigang Wang, Karl Strick­land, Comelia Boldyreff, Lene Olsen, 
Jan Wi~ Jurgen Heger, Wolfgang Scherr, and Peter Elzer, Practitioner and SoftClass: A Comparative Study 
of Two Software Reuse Research Projects, Journal of System and Sojhwrre, vol. 27, May 1994. 16. Hafedh 
Mili, Odile Marcotte, and Anas Kabbaj, intel­ligent Component Retrieval for Software Reuse, in  17. 
18. 19. 20. 21. 22.  23. 24. 25. 26. 27. 28.  Proceedings of the Third Maghrebian Conference on 
Artifiial Intelligence and Sojtware Engineering, pp. 101-114, Rabat, Morocco, April 11-14, 1994. Hafedh 
Mili, Famta Mili, and Ali Mili, Reusing Software: Issues artd Research Duections, IEEE Tran­sactwns on 
Sof!ware Engineering, vol. 21, no. 6, pp. 528-562, June 1995. Hafedh Mili, WiJliarn HarrisorL and Harold 
Ossher, supporting Subject-Oriented Programming in Smalltalk, in Proceedings of TOOLS USA 96, Santa-BarbarLCA 
July29-August2,1996. Hafedh Mili, Estelle Ah-Ki, Robert Godin, and Harnid Mcheick Representing and retrieving 
reusable com­ponents, IEEE Transactions on Knowledge and Data engineering, October 1996. Submitted (revised 
version of 1994 draft). Hafedh Mili, Houari Sahraoui, and Ilharn Benytila, Representing and Querying 
Object Frameworks, Tectilcal report, Dept of Computer Science, University of Quebec at Mormeal, May 1996. 
Rubesr Prieto-Dkw and Peter Freeman, Claasiljing Software for Reusability, IEEE Software, pp. 6-16, 
Janumy 1987. Roy Rad~ Hafedh Mili, Ellen BickneIl, and Maria Bletmer, Development and Application of 
a Metric on Semantic Nets, IEEE Transactions on Systems, Man, and Cybernetics, vol. 19(1 ), pp. 17-30, 
January/February 1989. James Roskind, The C+ + Grammar, July 1991. Gerard Salton and Michael McGill, 
Introduction to Modern Ir@ortnatwn Retrieval, McGraw-Hill, New York, 1983. Gerard Sahon, Another Look 
at Automatic Text- Retrievrd Systems, Cornmunic atwns of the Associatwn of Computing Machitwy, vol. 
29(7), pp. 648-656, July 1986. Scott N. Woodfield, David W. Embley, and Del T. Scott! Can programmersReuse 
Software, IEEE Sojiware, pp. 52-59, July 1987. Amy Moormarm Zaremski and Jeannette M. Wing, Signature 
Matching: A Key to Reuse, Software Engineering Notes, vol. 18, no. 5, pp. 182-190, 1993. First ACM SIGSOFT 
Symposium on the Foundations of Software Engineering Amy Moorrnann Zaremski and Jeannette M. Wing, Specification 
Matching: A Key to Reuse, Sofhvare Engineering Notes, vol. 21, no. 5, 1995. Third ACM SIGSOFT Symposium 
on the Foundations of Software Engineering Object Oriented Reuse &#38; Reuse on the Internet  
			</RefA>
