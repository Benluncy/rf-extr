
 ANALYSIS OF ALGORITHMS, A CASE STUDY: DETERMINANTS OF POLYNOMIALS by W. M. Gentleman S. C. ~ohnson University 
of Waterloo Bell Laboratories Waterloo, New Jersey Murray Hill, New Jersey ABSTRACT We consider the 
problem of computing the determinant of a matrix of polynomials; we compare two algorithms (expansion 
by minors and Gaussian elimination), examin- ing each under two models for polynomial computation (dense 
univariate and totally sparse). The results, while interesting in themselves, also serve to display two 
points: i. Asymptotic results are sometimes misleading for noninfinite (e.g.~ practical) problems. 
 2. Models of computation are by definition simplifications of reality: Algorithmic analysis should be 
carried out under several distinct computational models, and should be supported by empirical data. 
I. INTRODUCTION Some algorithms which are effective for integer computations are distinctly suboptimal 
for the analogous polynomial computations. As an example, Gentleman (i) studied powering of polynomials, 
and showed that, even in the computation of p , it can be faster to compute this as ((p2)p)p than as 
(p2)2(as one would for flxed-length integers). Heindel (2) and Fateman (3) have also studied this problem, 
using more than one model. We examine the computation of deter- University of Waterloo minants of n 
by n matrices with polynomial entries. At the outset we shall restrict our problem still further by ignoring 
the important case where the matrix has many zero entries, and assuming that the entries in the matrix 
are all the same size. Our analysis will focus on two algorithms (Gaussian elimination and expan- sion 
by minors) and two computational models (dense univariate and totally sparse); the next section will 
describe the algorithms, the following section the models, and Section IV specializes the analysis to 
these models. Finally, the last several sections will discuss the results of the theoretical analysis 
and give some empirical data in support of these results. II. The Determinant Problem Suppose we are 
given an n by n matrix of polynomials. We shall consider two ways of computing the determinant of this 
matrix, Gaussian elimination and expansion by minors. We shall assume that all the entries in the matrix 
are of equal size. We first describe the Gaussian elimination method, using exact division, which is 
appropriate for computations over the integers; an ALTRAN program is given in Appendix A. If we ignore 
pivoting, the algorithm consists of n-i steps, in- dexed by a variable k running from 1 to n-l. The kth 
step involves the computa- tion of an n-k+l by n-k+l matrix, which we shall call A[k+l) the entries will 
 ;" be (k+l) with k < i,j < n. The denoted aij , _ _ original matrix is identified with A (I). a(k+l) 
 For each k, the entry ij is computed by the formula (k+l) a(k) (k) a(k) a(k) aij = k,k ai,,~,-irk k~ 
(k-l) ak-l,k-i for k+l < i,j ~ n where (0) is taken to --ao, 0 be i. The division is always exact, 
so that (k) # % is a polynomial and a in) is the each aij A(n) " n,n determinant of An analysis of 
this algorithm (see Bareiss (4), Lipson (5)) shows that each (k) a.. is a determinant [minor) of some 
k by 10 k submatrix of the original matrix. Since we assumed all entries in the original matrix to be 
of the same size, we may ex- pect that all elements in A [k)" ", for a given k, are the same size. 
To compute . takes two a (k). multiplica- ij tions, a subtraction, and a division. In general, the 
cost of multiplying two polynomials will depend on their size; in our situation the size is assumed 
to depend only on the order of the minor making up the element. Thus, we shall use numbers C rs to 
compute the cost of our algorithm; C is the cost of multiplying a minor of rs order r by a minor of 
order s. Notice that Ci, I is the cost of multiplying two ele- ments from the original matrix. We assume 
also that an exact division of A by B to yield C has the same cost as a multiplication of B by C, and 
we ignore the costs of addition and subtraction. We can now write down the cost for Gaussian elimination 
in terms of the C rs" To compute a~k+ijt\., requires two multiplica- i] tions of cost Ckk , a division 
of cost Ck_l,k+l, and a subtraction whose cost we do not count. There are, for a given k, (n-k) 2 elements 
a (k+l) so the total cost 1j for the Gaussian elimination is n-I G = ~ (n-k)2(2Ckk + Ck_l,k+l). k=l 
 Note that, when C = i for all r and s, rE representing the familiar floating point or fixed-length 
integer case, the cost be- comes n-i G = 3 ~ (n-k)2 k=l 3 3 n 2 1 = n ~ + ~ n. We now turn our attention 
to the expansion by minors algorithm; an ALTRAN program for this is given in Appendix B. We again have 
n-l steps, indexed by k from 2 to n. At each step, we compute all of the k by k minors from the first 
k columns (there are (~) of them), using the k-I by k-i minors from the first k-i columns, computed 
in the previous step. We ignore the bookkeeping, and concentrate on the cost of the poly- nomial operations. 
Computing each k by k minor involves k multiplications, and k-l additions and subtractions, which we 
ignore: Thus, using the C , each new minor has a rs cost of kCk_l, I. The total cost is thus n M = 
~ (~)kCk_ I k=2 (n-l) '. = n~ (k-l)'.(n-k): Ck-l,l k=2 n n-i = n ~ (k_l)Ck_l, i k=2 n-i n-i = n ~ 
( k )Ck,l. k=l Once again, we examine the cost when each C is i; we have r,s n-i M ~ n-1 =n (k) k=l 
 = n(2 n-I -i). We now consider the models of polynomial computation. 136 III. ModEls of Computation 
can frequently be described best by de- We shall consider two models of poly-scribing what they ignore, 
not what they nomial computation in this section. The contain. Both of these models are extreme; two 
models will be extremes, in the sense we suggest that most practical problems that other models will 
tend to lie between will show aspects of both models. In the them. The models share many similarities, 
most radical departure from reality, both chiefly in the simplifying assumptions. models ignore coefficient 
growth. Speak- We llst the major ones here for future ing roughly, models that treat coefficient reference. 
growth behave as though another (dense) i) We assume that the algorithm polynomial variable were added 
to the mo- which multiplies a polynomial with n terms del; for example, univariate polynomials by one 
with m terms has a cost proportional whose coefficients grow behave very to mn. similarly to two variable 
polynomials. 2) We assume that the cost of co-Thus, the addition of variable length efficient operations 
is constant, irres-coefficients to the dense model makes the pective of the size of the polynomial. model 
"less dense", since intermediate (This is frequently false; we discuss our results grow in size more 
rapidly than reasons for making this assumption later.) the univariate model we consider. Simi- 3) Finally, 
we assume that, in larly, putting a dense dimension into the multiplying two polynomials, terms expli-sparse 
model makes the model "more dense". citly in the product never end up with zero We choose to analyze 
the sparse and dense coefficients ('cancel out'). models in their purest form, and then back up our 
conclusions with empirical data. Suppose that we multiply two poly- nomials, with m and n terms respectively. 
The other assumption which deserves Under Assumption i, the only difference mention is that multiplications 
of m between models is the size of the result; terms by n terms takes time mn. In the the multiplication 
cost is always mn. In dense case, this is immediate by using the sparse model, we assume no combina-the 
classical multiplication: we ignore tion of terms at all; the resulting poly-the fast Fourier techniques. 
In the nomial has mn terms, the maximum possible sparse case, one usually wishes to sort the number. 
Similarly, an addition of n and m terms in the product into some kind of term polynomials yields m+n 
terms. In canonical ordering. The most naive algor- the dense model, we assume that, subject ithm would 
take a time proportional to to Assumption 3, the product of m term and n mn log(mn); the algorithm used 
in ALTRAN term polynomials has the least possible takes a time proportional to mn log(min(m,n)). number 
of terms: m+n-l. This bound is This appears to be the fastest known algor- attained when the polynomials 
are uni-ithm; we know of none which actually runs variate with no nonzero coefficients. in time mn. Nevertheless, 
we shall neglect Similarly, addition of these polynomials the effect of the log term; in practice, yields 
max(m,n) terms. the bookkeeping and coefficient operations run as mn, and dominate the sorting term 
 In the remainder of this section, over a wide range of practical problems. we examine our assumptions, 
to better place these models into context; the next section Section IV. specializes the formulae to the 
two deter-In this section we apply the results minant algorithms. of Section III to the algorithms of 
 Section II. Recall the cost formulae The aim of every model is to describe for Gaussian and Minor 
expansion: and focus on some aspect of reality; models 137 n-i G ~ (n-k) 2 = (2Ck, k + Ck+l,k_ 1) 
k=l M = n (n~l) Ck,l k:l By assumption i in the last section, each C , the cost of multiplying an rth 
order rs minor by an sth order minor, is given by C = S S rs r s where S is the number of terms in 
an rth r order minor, and similarly for S s. We may thus write n-i G = (n-k)2(2S~ + Sk+iSk_ I) k=l 
 n-i M nS I ~ n-i = ( k ) Sk k=l The problem is now reduced to computing a value for S k, for each 
model, and then doing some summation. We shall, in the following, use the superscripts d and s to refer 
to the dense and sparse models, respectively. We assume that, in each model, the initial entries of the 
matrix have t terms. Computing S~ d) is very simple: a dense polynomial with t terms has degree t-l, 
so a k by k determinant of such poly- nomials has degree kit-l), and thus kit-l) + i terms. We compute 
 n-i M(d) nt ~ n-i = ( k ) (kit-l) + i) k=l n-i = nt (n-l~ (n-l)+(t-l)k~n-lkk ) k=l n-i k) = st 
2n-l-l+(t-1) ~ (n~l) k=l Moreover, n-i ~(n-l) = n-i k k (n-l) ~ in-2) : k= I k=l (n-k-l) : (k-l) : 
 n-i n-2  = (n-l) ~ (k_l) k=l n-2  = (n-l) ~ (n~2) k=0 = (n-l) 2 n-2 Thus M(d) = nt (2n-l-i 
+ (n-l)(t-l)2 n-2) Thus M(d) depends exponentially on n, and quadratically on t. G (d) is conceptually 
easier, but comp- utationally harder, to compute. We have G (d) = ~'(n-k)2( 2 kit-l)+ k+l) (t-l)+ k=l 
 n-i =k~= ~n-k) 2<3(k (t-l) +i~ -(t-l~ 2 n(n-l)( = 30 (3n3+3n2-7n+S) (t-l)2+15n(n+l) (t-l) +15(2n-i~ 
 Since this depends quintieally on n, and quadratically on t, M(d) is asymptotically greater than G (d) 
as n goes to infinity. Table I gives the values of G Id)" " and MId)"" for small values of n and t. Notice 
that M(d)"" is smaller than G (d)- -for n < 5, for n=6 and t > l, and for n=7, t > 3. In all other cases, 
G (d)" " is smaller. Thus, in spite of the evidently poor asymptotic growth of MId)" ", it appears that 
(at least counting number of multiplications) expan- sion by minors may be faster in many practical dense 
situations. The analysis in the sparse case is considerably clearer. We must first compute the sizes 
k , a k by k determinant is the S(s)" sum of kl products, each of which is the product of k t-term polynomials. 
By the assumption of sparseness, each product has t k terms, and there is no cancellation in the addition; 
thus o~ o~ oo o~ o~ O~ oo o~ O~ v ~m ~o ~ oo ~m o~ o~ ~N o~ v oo~ ~n~ oo ~nea oh-~n~ oo c~O~ ~. ~ 
~ o~ ~-o ~ coo ~0 o~- ,-t c,~ u~c0 o:r =o m .... ~ o N .... ~ ~ o~ .... ~o 8 ~ ~O ~O O~ ~ o~ O~ 
~ O~ ,,"* r.t Sk(S) = k;t k We may now compute: n-i M (s) nt Z n-i = ( k )k:tk k=l n-i  = nt __(n-i); 
k:l (n-k-l) : n-I t-(n-k-l) = nit n (n-k-l)' k=l < n;a n el/t  The cost approaches this bound very 
 quickly as n increases. Notice that M (s) /S (s) < e I/t (n) --, so that, in general, expan- sion by 
minors does less than three multi- plications per term in the answer; 139  The Gaussian result is harder 
to write in closed form. We have G( s ) ~(n-k)2(2S~ + Sk+iSk_ I)  = ~ (n-k) 2 (k:)2t2k+(k-l): (k+l):t 
2 k=l n-i k+l. ~ (n-k)2(k:t k)2(2 + --~--) k=l n-i 1  = ~ (n-k)2(k:tk)2( 3 + ~) k=l Clearly, the 
terms grow very quickly with increasing k. In fact, G (s) is certainly larger than its last term (k=n-l) 
: G(S) > 3((n_l),tn-l) 2 __ , SO  G(S)/s (s) > 3((n-l):)2t 2n-2= 3(n-l):tn-2 n --n. n,t n  Thus 
the cost per term grows exponentially in both n and t. Further analysis shows that G (s) is always greater 
than M (s) To summarize the results of this section: in the dense case, for small matrices expansion 
by minors takes fewer multiplications while for large problems Gaussian elimination takes fewer; in the 
sparse case, expansion by minors always takes fewer multiplications. The next section discusses these 
findings in a bit more detail. V. Discussion Recall that the sparse and dense poly- nomial models 
represented two extremes, where the results of the computation grew as fast or as slowly as possible 
without cancellation. Inspection of the formulae for M and G shows clearly what is happening: when S 
grows slowly (e.g. polynomially) r  in k the (n-l) ' k term in the formula for M causes an exponential 
dependence of M on n, while G has only a polynomial dependence on n. Thus, for large enough n, G is 
 smaller. On the other hand, when S grows r rapidly (e.g., exponentially) the S k terms in the formula 
for G come to dom- n-i inate the ( k )Sr term in the formula for M, and M is smaller. The question 
of which algorithm to use in practice thus seems to depend on how rapidly we expect S k to grow with 
re- spect to k. If we believe that our problems include considerable combination of terms, and small 
answers, we tend to favor Gaussian elimination, while if we believe in large answers, as caused by 
 variable length coefficients and multi- variate problems, we tend to favor expan- sion by minors. 
 Another striking feature of the above results is the extent to which an 'exponent- ial' formula for 
M is better than a 'polynomial' formula for G over a consider- able range of practical problems, even 
in the dense case. This serves to show the pitfalls of asymptotic analysis when applied to non-infinite 
problems. VI. Some Empirical Results Since the analyses in the previous sections make many simplifying 
assumptions (although the typical asymptotic analysis makes more:), the concluded superiority of minor 
expansions may still not appear in practice. To investigate how well our assumptions are borne out, 
we made a number of tests with the ALTRAN programs in Appendices A and B which tended to support the 
above analysis. As an example, we attempted to compute the determinants of n by n Symmetric Toeplitz 
matrices for orders through 8, using Gaussian Elimination and expansion by minors. The nth order matrix 
 was defined in terms of n+l variables, Xo,Xl, . . ,X by n' aij = Xli_j I  The results are given 
in Table II, be- low. These represent CPU time, in seconds, for the ALTRAN programs running with a 20000 
word workspace in an IBM 360/75 at the University of Waterloo. TABLE II  Order Minors Gauss 4 14.5 
 5 36.5  6 97.3  10.4 47.2  247.6 7 252.43 > 1475.2 out of time 8 > 588 (out of space) - Thus, 
even with a highly structured matrix in 7 or 8 variables, the minor method is a clear winner. VII. Conclusion 
 We hope to have made two larger points, over and above the direct results of this paper. We have seen 
that an asymptotically inferior method cannot always be dismissed when dealing with practical problems. 
In effect, the algebraic manipulations which are currently practical frequently lie much closer to zero 
than infinity. This is not to say that asymptotic analysis is not useful, but just that it must be kept 
in its placel Our second goal has been to indicate that rival models, or extreme models, can and should 
be used to gain insight into the algorithms being studied. In this sense, a continuum of crude approx- 
imations seems to us less revealing than a couple of well chosen extreme points. References<RefA> i. Gentleman, 
W.M., "Optimal Multiplication Chains for Computing a Power of a Symbolic Polynomial" SIGSAM Bulletin 
No. 18, April 1971, which appears in Math. of Comp. Voi.26, No. 120, Oct.'72. 2. Heindel, L.E., "Computation 
of Powers of Multivariate Polynomials over the Integers", Journal of Computer and System Science, vol. 
No. i, Feb. 1972.  3. Fateman, Richard J., "On the Compu- tational Powers of Polynomials", De-  14o 
  partment of Mathematics Report, M.I.T. Cambridge, Mass.  4.   Bareiss, E.H., "Silvester's Identity 
and Multistep Integer-Preserving Gaussian Elimination", Math. Comp., 22 (1968), pp. 565-578~  5.  Lipson, 
J.D., "Symbolic Methods for the Computer Solution of Linear Equations with Applications to Flow- graphs", 
in P.G. Tobey, (Ed.), Proceed- ings of the 1968 Summer Institute on Symbolic Mathematical Computation, 
  I.B.M. Programming Laboratory Report FSC69-0312, June 1969. </RefA>APPENDIX A  PROCEDURE BET (A,N) ALGEBRAIC 
ARRAY A; INTEGER N; VALUE A,N #THIS PROCEDURE CALCULATES THE DETERMINANT OF TEE #NTH ORDER MATRIX A 
BY INTEGER PRESERVING DAUSSIAN ELIMINATION INTEGER ARRAY (I:N} REORDER INTEGER I, II,J,E, KK,SIGN=i 
 ALGEBRAIC DIVISOR DO K"I,N; ~tEORDER (K}=K;' DOEND OD K=I,N-I DO I=K,N; IF (A(REORDER(I),K}.NE.O) 
GO TO PIVOT;DOEND RETURN (0) PIVOTIIF (I.EQ.E) KK=REORDER(K) ELSE DO; KK=REORDER(I); REORDER(I)=REORDER(K); 
REORDER (K) =KK; SIGN=-SIGN ; DOEND DB I=K+I,N; II=REORDER(I). DO J=K+i, N IY(R .NE. I)A (II, J)'A(II 
t J)/DIVISOR A(I I ' J) =A(I I, J) *A(KK, K)-A (II, K) *A (KK, j)  DOEND A(II,K)-. NULL. DOEND DIVISOR-A(KR,K) 
 DO J=R,N; A(KK.J)-.NULL. ;DOEND DOEND RETURN (SIGN*A (REORDER(N) , N) ) END APPENDIX B PROCEDURE 
DET(A,N) ALGEBRAIC ARRAY A INTEGER |THIS PROCEDURE CALCULATES THE DETERMINANT OF THE N ORDER #MATRIX 
A BY EXPANDING BY MINORS INTEGER ARRAY (0:N,O:N) BIHOM ALTRAN INTEGER PASCAL ALGEBRAIC ARRAY (0:i,0:PASCAL(N,BINOM)) 
MINOR=0 #PASCAL INITIALIZES THE BINOMIAL COEFFICIENT TABLE, AND # RETURNS AS ITS VALUE N CHOSE N/2 
LESS 1 IJE KEEP TRACK OF OUR MINORS USING THE METHOD OF INDEXING DESCRIBED IN CACM ~ (1960), p. 235 (R.M. 
Brown) INTEGER LuC,ADDR,SIGN,M, ~K,OLD=0,NEW=i INTEGER ARRAY (0:N+I) I=-1 I (0 } =N DO LOC=i,N; MINOR(OLD,LOC-I)=A(I,LOC);DOEND 
DO M=i,N-I # COMPUTE M+i DOEND MINORS FROM M ORDER LOC=O; DO J=I,M; I (J)=M-J; DOEND NEXT MINOR:IF (MINOR(OLD,LOC).NE.0) 
DO; K=N-i; J=0; ADDR=LOC+BINOM(K,M+I); SIGN=I NEXT USE:IF (K.EQ.I(J+I)) DO; J=J+l; SIGN=-SIGN; DOEND 
ELSE MINOR(NEW,ADDR)=MINOR(NEW,ADDR)+SIGN~A (M+i,K+i)_ #NINOR(OLD,LOC) IF (K.GT.0) DO; K=K-i; ADDR=ADDR-BINOM 
(K,M-J); GO TO NEXT USE DOEND # DISPOSE OF U~ECESSARY MINOR AND INCREMENT INDICES MINOR(OLD,DOC)=0 DOEND 
LOC=LOC+I DO J=M,i,-i; I(J)=I(J)+l; IF (I(J).LT.I(J-I)) GO TO NEXTMINOR ELSE I(J)=M-J~ DOEND #ALL M+i 
ORDER MINORS ARE NOWCALCULATED OLD=i-OLD; NEW=I-NEW DOEND RETURN (MINOR (OLD, 0 ) ) END PROCEDURE PASCAL(N,BINOM) 
INTEGER N,BINOM ARRAY BINOM #INITIALIZES TABLE OF BINOMIAL COEFFICIENTS, AND RETURNS THE #MAXIMUM NUMBER 
OF MINORS,LESS 1 INTEGER I,J ALTRAN INTEGER FLOOR=S9IFLR BINOM(0,0)=I DO I=I,N BINOM(I,0)=i; BINOM(I-I,I)=0 
 DO J=l,I BINOM(I,J)=BINOM(I-I,J-i)+BINOM(I-i,J) DOEND DOEND RETURN {BINOM(N,FLGOR(N/2))-I) END 141 
  
			
