
 An Iterative Semi-supervised Approach to Software Fault Prediction Huihua Lu Lane Department of Computer 
Science and Electrical Engineering West Virginia University Morgantown, WV 26506-6109  hlu3@mix.wvu.edu 
 Bojan Cukic Lane Department of Computer Science and Electrical Engineering West Virginia University 
Morgantown, WV 26506-6109 bojan.cukic@mail.wvu.edu Mark Culp Department of Statistics West Virginia 
University Morgantown, WV 26506-6330 mculp@stat.wvu.edu ABSTRACT Background: Many statistical and machine 
learning techniques have been implemented to build predictive fault models. Tradi­tional methods are 
based on supervised learning. Software metrics for a module and corresponding fault information, available 
from previous projects, are used to train a fault prediction model. This approach calls for a large size 
of training data set and enables the development of effective fault prediction models. In practice, data 
collection costs, the lack of data from earlier projects or product versions may make large fault prediction 
training data set unattain­able. Small size of the training set that may be available from the current 
project is known to deteriorate the performance of the fault predictive model. In semi-supervised learning 
approaches, soft­ware modules with known or unknown fault content can be used for training. Aims: To 
implement and evaluate a semi-supervised learning ap­proach in software fault prediction. Methods: We 
investigate an iterative semi-supervised approach to software quality prediction in which a base supervised 
learner is used within a semi-supervised application. Results: We varied the size of labeled software 
modules from 2% to 50% of all the modules in the project. After tracking the per­formance of each iteration 
in the semi-supervised algorithm, we observe that semi-supervised learning improves fault prediction 
if the number of initially labeled software modules exceeds 5%. Conclusion: The semi-supervised approach 
outperforms the cor­responding supervised learning approach when both use random forest as base classi.cation 
algorithm. Categories and Subject Descriptors D.2.8 [Software Engineering]: Metrics complexity measures, 
per­formance measures  General Terms Design, Experimentation, Performance Permission to make digital 
or hard copies of all or part of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice 
and the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute 
to lists, requires prior speci.c permission and/or a fee. PROMISE 11, September 20 21, 2011, Banff, Canada 
Copyright 2011 ACM 978-1-4503-0709-3/11/09 ...$10.00.  Keywords Fault prediction, Semi-supervised Learning 
 1. INTRODUCTION Software Quality (SQ) is one of the most common topics dis­cussed in the development 
of software products. An important and challenging part of software quality involves automatically detect­ing 
modules which contain faults. Fault proneness detection can be conducted throughout the development life 
cycle [8, 23]. Given the complexity of modern software systems, accurate prediction of where faults hide 
enables effective guidance for monitoring of the potentially problematic components under development. 
The well accepted method for software fault prediction is to learn from past projects or earlier versions 
of the same product. Software measurements collected from a previous system release have often been used 
to predict software quality. Size-related metrics are some of the most important measures used to detect 
the fault prone mod­ules. Replicated studies on size-defect relationship for software modules argue that 
in large scale software systems, smaller mod­ules will be proportionally more defect prone compared to 
larger ones [21]. In addition to size metrics, software complexity met­rics quantitatively describe the 
characteristics of software structure. The more complex the module is, the more likely it will contain 
faults. Therefore, it is natural to use software complexity metrics to predict the fault proneness of 
software modules. Detection of fault prone modules is typically approached as a binary classi.ca­tion 
problem, where a particular module is labeled as either fault prone (fp) or not fault prone (nfp) [17]. 
The well known software complexity metrics include code size [21], Halstead s complex­ity [15] and McCabe 
s cyclomatic complexity [24]. In recent years, many statistical and machine learning techniques have 
been used to build fault prediction models [20]. Most of these techniques are based on supervised learning. 
Software met­rics are used as independent variables to predict the occurrence of one or more faults in 
the module, the dependent variable. Typi­cally, only software modules with known association between 
in­dependent and dependent variables (labeled data) are used to train a fault prediction model. The model, 
subsequently, predicts the fault proneness of other modules with unknown fault content (unlabeled data). 
In this setting, the size of training data set may dramati­cally impact the model s stability. It is 
generally agreed that the smaller of the training size, the poorer the performance of a fault prediction 
model. Therefore, one challenge of such techniques is that the size of training sets should be as large 
as possible so that true associations are revealed by fault prediction models. In prac­tice, data collection 
costs, the lack of data from earlier projects or classification using labeled data only classification 
using labeled and unlabeled data 02468 02468 x1 x1 Figure 1: A simple example for Semi-Supervised Learning. 
product releases may make large fault prediction training data set unattainable. In a domain like space 
exploration, the source of the fault prediction data used in this study, most projects are one-of­a-kind 
development and earlier releases do not exist. In spite of differences, the following characteristic 
of the software fault pre­diction data sets are common, regardless of the size: (i) most soft­ware modules 
are not faulty (unbalanced classes), (ii) we know or can measure the metrics of all software modules 
but may know the actual fault status of a few, and (iii) human labeling for unlabeled modules is costly 
and time consuming, and may not be perfectly accurate. These characteristics limit the accuracy of fault 
predic­tion models, but research has demonstrated their utility and cost effectiveness. An intuitive 
alternative to supervised learning approach are semi­supervised learning algorithms. In them, both labeled 
and unla­beled data are considered for model training so that the size of train­ing set is augmented 
as information becomes available. A simple abstract example of semi-supervised learning for a binary 
classi.­cation problem is shown in Figure 1. The size of initial labeled data is rather small. In the 
left plot of Figure 1, the decision boundary is learned from the labeled data only. In the plot on the 
right side, both labeled and unlabeled data (little circles) are used to generate the decision boundary. 
From the plots, a small set of labeled data cannot accurately depict the overall distribution of the 
data. On the other hand, the unlabeled data sets are typically abundant and de­scribe data structure 
more appropriately with its natural clustering. Incorporating the information from unlabeled data into 
the learn­ing process may result in a better classi.cation, as shown in Figure. The question to answer 
here is whether software quality data sets, fault proneness prediction more speci.cally, can bene.t from 
this approach. In this paper, we investigate the application of an existing semi­supervised algorithm 
to fault prediction problem. The algorithm, called Fitting The Fits (FTF) [6], is a variant of traditional 
self­training algorithms. In the FTF algorithm, a base supervised learner is iteratively trained from 
both labeled and unlabeled data. The status of unlabeled software modules is repeatedly updated until 
some stopping criterion is met. In other words, the algorithm is repeatedly .tting the .xed status (.ts) 
of unlabeled data. There are several reasons we are interested in the implementation of FTF in software 
fault prediction problem: As motivated previously, semi-supervised learning holds promise for advancing 
fault proneness classi.cation in situations where labeled data (modules with known fault content) are 
scarce and unlabeled data are abundant; FTF is technically simple and less restrictive compared to other 
semi-supervised learning algorithms such as EM-based method or graph-based methods;  It is interesting 
to apply FTF on the highly imbalanced data set, where the number of software modules that contain faults 
is much smaller than the number of fault-free modules;  FTF can accommodate many (supervised) classi.cation 
tech­niques as the base learner. We want to investigate the behav­ior of FTF when Random Forest (RF) 
is used as the base supervised learner, because RF has shown consistently good performance in related 
studies [13].  We use NASA MDP software project repository in our experi­ments. Although the repository 
offers fault content labels for all modules, we will assume the size of labeled data set to range from 
2% to 50% of original data. This assumption mimics the practi­cal development scenario in which only 
a subset of software mod­ules (in this case functions or methods) has undergone veri.cation and validation 
activities revealing the existence of faults. We ob­serve that for the same proportion of labeled data, 
proposed semi­supervised learning approach tends to perform better than the corre­sponding supervised 
algorithm. The new approach performs partic­ularly well when the non-fault prone (nfp) class represents 
a strong majority of overall observations and the number of labeled modules is small relative to the 
unlabeled ones. The paper is organized as follows. In Section 2, we provide a general discussion of supervised 
learning and semi-supervised learning. Section 3 presents methodology for semi-supervised clas­si.cation. 
In Section 4, we describe the software data sets used for our experiments. In Section 5, we present the 
results of our exper­iments. A discussion is offered in Section 6. Finally, we provide a summary in Section 
7.  2. SUPERVISED AND SEMI-SUPERVISED LEARNING The most popular approach to predict fault prone software 
com­ponents is classi.cation based on supervised learning [19, 12, 9]. In this setting the objective 
is to develop a model through train­ing from software metrics of modules for which we know the fault 
status (labeled modules). This model classi.es the remaining mod­ules for which we do not know the software 
fault status (unlabeled modules). Techniques vary depending on the machine learning al­gorithms used. 
The literature indicates that logistic regression [18, 27], discriminate analysis [20], tree-based classi.cation 
[10] and random forests [13], amongst others, work well. Random forests are particularly attractive since 
they tend to work well on unbal­anced data sets with noisy responses [3]. One important premise of supervised 
learning approaches is that the amount of fault data in the training set should be suf.ciently large 
to ensure effective fault prediction. Semi-supervised learn­ing addresses this problem by augmentation 
of the training set with unlabeled data. An intuitive assumption for semi-supervised learn­ing approaches 
is that the knowledge stored in unlabeled data may provide useful predictive information and assist in 
achieving better model performance. In the past decade, semi-supervised learning has attracted con­siderable 
attention and many semi-supervised learning approaches have been proposed for classi.cation that can 
outperform corre­sponding supervised learning approaches [4, 31]. The main re­search directions include 
co-training approaches [2], self-training approaches [7], generative semi-supervised approaches using 
Ex­pectation Maximization algorithm (EM) [26], and manifold ap­proaches [11]. Self-training is computationally 
simple and requires few restriction on data compared to the others. For example, gen­erative semi-supervised 
learning approaches need the calculation of estimated parameters before learning; co-training assumes 
the training data follows nature separation" assumption, etc. Contrary to their complexity, self-training 
simply extends the supervised ap­proach with a known base learner into semi-supervised learning through 
an iterative procedure. Different way of implementing the iterative procedure results in different variants 
of self-training. Fig­ure 2 depicts a classic self-training approach. Labeled (L) and unla­beled (U) 
data items form the training set, although in the .rst itera­tion only the labeled data would be used 
in training. Model learned from the current training set (L+U) is used to classify unlabeled instances. 
From the diagram, we can see that without the aid from unlabeled data this is supervised learning (the 
dashed frame). The classic semi-supervised approach takes the originally unlabeled in­stances, for which 
predicted labels have high con.dence scores (compared to a user de.ned threshold) and incorporate them 
into the training set. The labels in the extended training set are repeat­edly updated until some stop 
criterion is met. This is a straightforward procedure to model training assisted by unlabeled data instances. 
The earliest version of this approach was proposed as Yarowsky s algorithm [30], later studied in [1, 
14]. Our approach in this study is a recently proposed variant of Yarowsky s algorithm. Unlike Yarowsky 
s algorithm where the in­stances with high con.dent scores are always considered for aug­menting the 
training set, Fitting-the-Fits (FTF) learns the labels from the entire data set (L+U), where the labels 
for unlabeled data are predictions from the previous iteration. A detailed description of the algorithm 
is offered in the next section. To the best of our knowledge, semi-supervised learning algo­rithms have 
been marginally considered in the .eld of software fault prediction. Khoshgoftaar [28] proposed an EM-based 
semi­supervised learning approach [4] to the data sets from NASA MDP repository. The EM algorithm .ts 
this problem well since one could consider unlabeled modules as missing the label and thus semi-supervised 
learning can be reduced to be missing data prob­lem. Their work showed that EM-based semi-supervised 
software Figure 2: Diagram of classic self-training. quality modeling provides better performance compared 
to stan­dard tree-based supervised learning approach -C4.5. Unfortu­nately, since C4.5 is not one of 
the best performing classi.ers in this domain, Khostgoftaar s results are not conclusive. Another interesting 
approach proposes semi-supervised cluster­ing [29]. Unlike in self-training, which extends supervised 
into semi-supervised learning, the semi-supervised clustering extends traditional unsupervised learning 
(clustering algorithm) into semi supervised context. Better partitioning, fault-prone (nfp) and not­fault-prone 
(fp), is achieved with the incorporation of unlabeled data. However, this is not an entirely automated 
approach, because it utilizes software engineering experts. Domain experts would work on the labeling 
of clusters. The procedure cycles until the size of labeled set and unlabeled set remain unchanged. The 
results of such a semi-supervised clustering were shown to improve the performance compared to the corresponding 
unsupervised learn­ing approaches which, again, lags signi.cantly behind supervised learning. Both semi-supervised 
approaches above have implemented as inductive learning strategies, that is, a model is built given a 
set of training data (labeled and unlabeled) and then used to predict unlabeled instances (test data 
set). The goal of inductive semi­supervised learning is to .nd a model for entire data space. In our 
study, we implement a transductive learning strategy, in which label predictions are only required for 
a given test set (unlabeled data). We refer the reader to [4] for detailed discussion of differences 
between inductive and transductive semi-supervised learning.  3. METHODOLOGY Let X be the (n + m) × 
p matrix that denotes the software fault prediction data set. Certain number of software modules in the 
data set have associated labels (faulty, not faulty), others do not. n is the size of the labeled set 
l and m is the size of unla­beled set u.Rows in X are p-dimensional vectors denoted as x, with x .1p, 
where p denotes the number of independent vari­ables, i.e., software metrics used to predict the response 
variable Y. Speci.cally, X = {Xl,Xu}, where Xl = {x1,x2,··· ,xn} and Xu = {xn+1,xn+2,··· ,xn+m}. Let 
Y = {Yl,Yu}be response variable (or labels) where Yl = {y1,y2,··· ,yn} are known and Yu = {yn+1,yn+2,··· 
,yn+m}are missing or unknown. The ob­served labels are binary variables, yi .{0,1}, where 0 denotes non-fault 
prone (nfp) module and 1 denotes fault prone (fp) mod­ule. Our goal is to extend supervised learning 
based fault prediction into a semi-supervised setting. Let us de.ne f(.) as any given su­ Algorithm1: 
Fitting The Fits (FTF) 1: Initialization: Y0 l =Yl, Y0 u =fD(0) l (Xu), k =0; 2: loop until stop criteria: 
3: Yk l =Yl 4: Fit Y(k+1) =fD(k) (X), where D(k) =(X, Y(k)) 5: k =k+1 6: End loop Figure 3: FTF algorithm. 
pervised learner (base learner). Given a set of input-output pairs Dl =(Xl,Yl), the notation fDl (Xu)indicates 
that the classi.er trained from Dl is used to predict (assign labels) on unlabeled data set Xu. The probability 
class estimates (PCEs) for fault prone class, p =P(Y =1|Xu), are returned. PCEs are a speci.c form of 
con.dence scores generally used in the literature. We consider a module as fault prone when p>t and non 
fault prone other­wise, where p . [0,1]and t is a threshold speci.ed for making the decision. 3.1 Semi-supervised 
learning-FTF Fitting-the Fits (FTF) is a variant of the original Yarowsky s al­gorithm. In Yarowsky s 
algorithm a supervised learner is trained from the labeled set, which consists of the initially labeled 
data and selected labeled instances from the unlabeled data. Note that the unlabeled instance will be 
selected and marked as labeled only if the con.dence of prediction is above some threshold. Additionally, 
the label status of an instance from unlabeled data may change. For example, an instance currently labeled 
as fp may become unlabeled and be labeled as nfp in future iterations, depending on the con.­dence of 
current prediction. FTF provides an interesting variant of traditional Yarowsky algorithm. It initially 
sets up the labels for all unlabeled instances, based on the model fDl (Xu). This ensures that both the 
labeled and unlabeled instances are labeled. Next, a classic supervised procedure is implemented on the 
entire data set (as the entire data set is labeled now). The labels for the unlabeled data are gradually 
updated until a convergence or stopping criterion is met. This is different from Yarowsky s algorithm 
in which only a subset of unlabeled data are used to train a new learner at each iter­ation. Also, FTF 
can be shown to globally converge for some base learners, a property that can not be guaranteed with 
Yarowsky s algorithm. Figure 3 provides a pseudo code of the FTF algorithm. The pro­cedure of FTF starts 
with setting the initial labels for the unlabeled data at the 0th iteration. Speci.cally, the learner 
is trained from la­beled data D(0) l =(Xl,Yl). It is then used to predict the labels for unlabeled data 
Y u (0) =fD(0) (Xu). In the loop, labels for initially l labeled instances are always reset to the original 
values Y lk =Yl, because this is the ground truth (step 3). The base learner which is built based on 
the current status of entire data set D(k) =(X,Y (k)) Y(k+1) is used to predict new labels = fD(k) (X)(step 
4). This cycle continues until the stop criteria is met. In the original paper on FTF, this iterative 
cycle stops when the labels converge. How­ever, the convergence property is sensitive to the use of base 
learner which will be discussed later. In our study, we found that the con­vergence of the FTF algorithm 
which uses Random Forest as the base learner cannot be guaranteed. Therefore, our stop criterion is static, 
de.ned as the speci.ed number of iterations. Note that in this iterative procedure soft labels (PCEs) 
are assigned to unlabeled modules as Y u (k). In other words class labels represent probabilities for 
the purpose of this algorithm.  3.2 The base learner -Random Forest In FTF, a supervised learner is 
trained repeatedly. The supervised learner plays three important roles: 1. It initializes the labels 
for unlabeled data as shown in Step 1. Note that a base learner trained from initial labeled data pro­vides 
prediction for unlabeled data so that the iterative pro­cedure can be triggered . Apparently, a well-chosen 
base learner can provide effective prediction for initially unlabeled portion of the data set and ensure 
a good starting point for tracking better learners.  2. It improves" the labels of unlabeled data repeatedly. 
The base learner is repeatedly trained to generate Probability Class Estimates (PCEs) for unlabeled data 
based on the model learned in the previous iteration. The accuracy of PCEs generated by the base learner 
is critical as it determines the informa­tion extracted from unlabeled data, and eventually decides the 
behavior of entire algorithm. 3. It may lead to global convergence. The convergency property of FTF 
is base-learner sensitive. In [6], various statistical base learners are discussed. It is shown that 
some base learn­ers cause the algorithm to converge but cannot provide bet­ter performance compared to 
the corresponding supervised learning approach, such as linear regression and logistic re­gression. Some 
base learners, such as Graph-based smooth­ing and Kernel smoothing, show convergence property and present 
improved performance as well. Some others, such as Partial Least Square regression (PLS), never make 
FTF converge.  Based on the discussion above, we have to carefully choose the base learner. In supervised 
learning literature, there are lots of choices. We have two constraints on the choice of base learner. 
First, the learner should have competitive performance in the fault prediction domain and its implementation 
should be available off­the-shelf. Second, it should produce well-calibrated probabilities (PCEs). [25] 
examines the relationship between the predictions made by different supervised algorithms and true posterior 
proba­bilities. The authors showed that some of the supervised algorithm, such as boosted tree and boosted 
stumps, tend to push predicted probabilities away from 0and 1. Other algorithms, such as Naive Bayes, 
have the opposite bias and tend to push predictions closer to 0and 1. Both of these trends hurt the quality 
of probability distribu­tions and do not represent good choices for our study. Other learn­ing algorithms 
analyzed in [25], such as random forest and logistic regression, showed little or no bias and offered 
well-calibrated pre­diction probabilities. On the other hand, compared to ensemble methods such as Adaboost, 
random forest provides robust perfor­mance when the data used for model training is noisy [5]. Given 
previous experience with random forest in fault prediction [13], we decided to use random forest as the 
base learning algorithm. Random forest has been successfully applied as a major data analysis tool since 
its introduction in 2001 [3]. Its popularity is helped by the fact that it can be applied to a wide range 
of pre­diction problems, including high dimensionality and complex in­teraction effects. Random Forest 
is an ensemble of individual tree predictors, such that each tree is randomly generated based on the 
values of a random sampled vector. The outputs of random forest are obtained by the majority classi.cation 
votes of individual trees. Random forest exhibits improved performance compared to single Table 1: Characteristics 
of data sets from NASA MDP repository Data Size# % faulty project description language JM1 10,878 19.3% 
Real time predictive ground system C KC1 2109 13.9% Storage management for ground data C++. PC3 1563 
10.43% Flight software for earth orbiting satellite C PC4 1458 12.24% Flight software for earth orbiting 
satellite C PC1 1109 6.59% Flight software from an earth orbiting satellite C trees due to its ability 
of utilizing redundant features and the inde­pendence of different trees. In random forest, each tree 
is grown on a bootstrap sample of the training data set which ensures the inde­pendence of different 
trees. In this study, we utilize Random Forest implementation from statistical computing package R [22]. 
  4. SOFTWARE FAULT PREDICTION DATA SETS The NASA Software Metrics Data Program (MDP) repository provides 
metrics that describe the software artifacts from 13 NASA projects. Five projects that have more than 
1,000 modules are used in our experiments. Table 1 provides a brief description of these .ve projects. 
Each module in these project is measured in terms of the same set of software product metrics. A label 
associated with each module indicates if the module has been found to contain one or more faults (fault 
prone, fp) or no faults have been detected (not fault prone, nfp). In order to provide a better understanding 
on our experiments, we brie.y describe each project: JM1: JM1 project is a real-time ground system that 
uses simulations to generate certain predictions for the space mis­sion. It is coded in C language. There 
are eight years of fault data associated with the modules and their metrics. Modules in JM1 were characterized 
by 21 software measurement at­tributes. The data set contains 10,878 modules, of which 2,102 have one 
or more faults and 8,776 have zero faults, the rate of 19.3%. The maximum number of faults in a mod­ule 
is 26.  PC1: The PC1 project is .ight software from an earth-orbiting satellite that is no longer operational. 
There are eight years of fault data associated with the metrics. It consists of more than 40,000 lines 
of source code written in C. The software measurement data set contains 1,107 modules characterized by 
41 attributes / metrics. Only 76 modules have one or more faults and 1,031 have zero faults, fault rate 
of 6.59%. The maximum number of faults in a module is 9.  PC3: The PC3 project is also a .ight software 
from an earth­orbiting satellite, but the mission is currently operational. It consists of approximately 
40,000 lines of source code writ­ten in C. The data set describes 1,563 modules characterized by 41 attributes, 
of which 160 have one or more faults and 1,403 have zero faults, the fault rate of 10.43%. The maxi­mum 
number of faults in a module is 9.  PC4: The PC4 project is .ight software for an Earth orbiting satellite 
that is currently operational. It consists of approx­imately 36,000 lines of source code written in C. 
The soft­ware measurement data set contains 1,458 modules charac­terized by 41 attributes, of which 178 
have one or more faults and 1,280 have zero faults, with the fault rate of 12.24%. The maximum number 
of faults in a module is 25.  KC1: The KC1 project is a computer software con.guration item within a 
large ground system and consists of approx­imately 43,000 lines of source code written in C++. The data 
set contains 2,107 modules, of which 325 have one or more faults and 1,782 have zero faults with the 
fault rate of 13.9%. The maximum number of faults in a module is 7. It is important to note that these 
projects did not share a devel­opment process, they come from different government contracting organizations 
and, generally, at the time of their development, it would not have been possible to use one of them 
as a training data set for the fault prediction modeling on the other one. In situa­tions like this, 
if an organization wants to deploy a fault prediction model, it needs to develop it using the metrics 
and fault labels as­sociated with the modules emerging from the development early.  5. EXPERIMENTS To 
depict the performance of fault prediction experiments we will provide the Probability of Detection (PD) 
and the Area Under Receiver Operating Characteristic Curve (AUC) as the measures of binary classi.cation. 
(PD), also called recall or speci.city, is de.ned as the probability of correctly classifying a module 
as fault­prone, that is P(C = fp|C = fp). It is one of the most commonly used measures in this .eld. 
It is clearly suitable for representing the ability of an algorithm to correctly classify the instances 
of a mi­nority class (fp). The PD value is obtained based on a speci.ed threshold, which can be adjustable 
by user as needed. Receiver Operating Characteristic (ROC) curve is a plot of probability of de­tection 
(PD)as a function of the probability of false alarm (PF, the probability of misclassifying a fault free 
module as faulty) across all thresholds setting. AUC is the area under the ROC curve, an­other frequently 
used measure for performance evaluation in soft­ware fault prediction [16]. As discussed in previous 
sections, semi supervised learning should improve software fault prediction models in situations where 
the number of modules with known fault content (labels) is limited. So, the goal is to improve the performance 
of prediction on unla­beled data (test data) using augmented training data set, referred as transductive 
semi-supervised learning. Our study aims to answer the following questions: 1. Does FTF with Random Forest 
as base learner outperform supervised learning with Random Forest? 2. How small can the size of the 
labeled data set be for the FTF to outperform supervised learning? 3. Are the behavior and performance 
of FTF consistent over different data sets?  With these questions in mind, we developed experiments 
that compare the FTF algorithm and the corresponding supervised ap­proach. Both the supervised and semi 
supervised approaches use the same base learner, Random Forest. Additionally, we will vary the sizes 
of labeled data instances: 2%, 5%, 10%, 20%, 50%, of the threshold=0.75 threshold=0.5  0 10 20 30 
iteration 40 50 0 10 20 30 iteration 40 50 threshold=0.1  iteration iteration Figure 4: Fault prediction 
results of FTF algorithm on project PC3. size of the .ve MDP projects. To track performance trends, we 
will present above described performance measures(PD and AUC)at each iteration of the FTF, so that a 
performance curve can be de­rived. The results of our experimental study con.rm that the Ran­dom Forest 
based FTF algorithm never converges. This could be understandable because random forest is a non-parametric 
method. The outputs of random forest are vote-based, so that it would be mathematically hard to track 
the estimates of outputs when it is im­plemented in FTF algorithm. On the other hand, the randomness 
of tree building in random forest also breaks the possibility of esti­mates tracking. However, we stop 
the iterative procedure in FTF at 50th iteration as the range spanned from 0th to 50th iteration pro­vides 
an overall changing trends in our experiments. A total of 20 experiments were performed on each experimental 
setting. The in­stances of labeled data are randomly selected from each set for the .rst iteration of 
the FTF algorithm, with the remaining software modules being used for prediction as unlabeled data. Ideally, 
in the context of software development scenario, the components that .rst come out of development would 
be utilized for model building. Unfortunately, this information is not available for MDP projects. This 
is the reason for the 20 experiments over the same data set (and the proportion of labeled data). The 
repetition indicates our attempt to make the order of component delivery to the project less important. 
We understand this is one of the validity threats. Nev­ertheless, within the MDP data repository, there 
is no remedy that would offer its reduction or elimination. Figure 4 shows the results of the application 
of FTF algorithm on PC3 data set. Recall that FTF assigns the label to each module based on the value 
of PCEs(p ). When p>t the module is clas­si.ed as fp and as nfp otherwise. We compare the values of PD 
using three different threshold set ups (t): 0.1, 0.5 and 0.75. The values of PD are shown along the 
Y axis of the .rst three charts. The fourth plot represents the comparison of the corresponding areas 
under the ROC curves. To better compare the outcomes, we chart the values of PD for the various sizes 
of labeled data: 2%, 5%, 10%, 20% and 50%. The curved lines connecting the points represent the results 
of semi-supervised learning after each of the 50 iterations of the FTF algorithm. Note that each PD per­formance 
curve starts in 0th iteration at the initial point of a straight line. These straight lines represent 
the results of supervised learn­ing using the same size of the labeled data set as the corresponding 
semi supervised algorithm. Since supervised learning uses only one pass through the training data, there 
are no iterations and no improvements, thus the straight PD performance lines. Each per­formance point 
in these charts represents the average of 20 experi­ments, as described in the description of the experimental 
design. Several trends are easy to observe from Figure 4. The higher the selected threshold, the lower 
is the corresponding PD. For exam­ple, threshold 0.75 implies that modules with 75 are classi­ p> 0..ed 
as fp and the modules with p = 0.75 are classi.ed as nfp. Such a high threshold implies that very few 
modules will be classi.ed as fault prone. Depending on the size of the labeled data set, PD rates vary 
between 3% and 15%. At this threshold, many faulty modules are misclassi.ed as not fault prone. Therefore, 
we can ex­ PC1 PC4  0 10 20 30 iteration 40 50 0 10 20 30 iteration 40 50 KC1 JM1  iteration iteration 
Figure 5: Results of FTF algorithm on four data sets with threshold=0.5. pect a signi.cant room for improvement 
that can be achieved with semi supervised learning. The performance curves for FTF signi.­cantly exceed 
the corresponding straight lines (the performance for supervised random forest) across all sizes of the 
labeled data set. For some sizes of labeled data, the increase in the PD does not appear to stop at the 
50th iteration (for example the curve for 10% labeled modules at 0.75 threshold). When the threshold 
moves to 0.5, the PD rates jump to the 13% to 40% range. FTF offers im­proved performance for experiments 
which started with 10%, 20% and 50% of the labeled modules, while the curves of 2% and 5% labeled training 
set size decline below the performance results of the corresponding supervised learning algorithm. We 
also observe that the performance changes between the successive iterations of the FTF algorithm are 
more modest with the changes, positive and negative, more stable. The PDrates tend to stabilize after 
the 35th iteration. NASA data sets describe safety critical applications, therefore the most interesting 
fault prediction results occur at low thresholds, where most software modules which are known to contain 
faults are correctly identi.ed as fault prone (with the side effect of increased FP rates, of course). 
At this threshold(t =0.1), the improvements that come as the result of semi supervised learning are modest. 
Most PDcurves for FTF outperform the corresponding supervised learning performance lines. However, prolonged 
iterations of semi supervised learning algorithm reverse any performance gains after the 10th iteration. 
In case of the fault prediction from only 2% of the data set, a very small size of the training set given 
the imbalance between fp and nfp, FTF offers no gains in PD. Nevertheless, the AUC values hold steady 
over all the iterations of FTF, indicating that the minor decrease in PD must result in a decrease of 
false alarms too. In general, for project PC3, semi supervised learning offers bet­ter probability of 
detecting fault prone modules especially when the number of labeled modules exceeds 10% of the project 
s .nal size. The rate of improvement differs over the threshold settings. For instance, the PD curve 
from 20% of labeled modules at the threshold of 0.75 increases fast from an initial value of 0.08 up 
to 0.13 (62.5% improvement). The performance increase tends to be smoother when the threshold is 0.5, 
from 0.24 to 0.26 an 8.3% improvement). When the threshold is set to be 0.1, the PD in­creases from 0.77 
to 0.78, a minimal 1.3% improvement. The logical next step is to evaluate whether the observed fault 
prediction trends using semi supervised learning are the same over all the MDP projects included in this 
study. Due to the paper size constraints, we cannot present all the plots that describe PC3 for other 
data set. We will limit our attention at the performance results obtained when the decision threshold 
is set to 0.5 and 0.1. The results for other four data sets(PC1, PC4, KC1, JM1) are shown in Figure 5 
and Figure 6. We can see that FTF algorithm outperforms the corresponding Random Forest supervised algorithm 
for most labeled data size settings. The exception is the 2% labeled set size, where semi supervised 
learning cannot improve through algorithm iterations. At the 5% setting, the PD trend is not consistent 
across the data sets. For example, in Figure 5 the performance curve of PC1 PC4 PD PD 0.55 0.60 0.65 
0.70 0.75 0.80 0.85 0.55 0.60 0.65 0.70 0.75 0.80 0.85  PD PD 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.55 
0.60 0.65 0.70 0.75 0.80 0.85  iteration iteration KC1 JM1  iteration iteration Figure 6: Results 
of FTF algorithm on four data sets with threshold=0.1. 5% labeled modules of PC1 starts to exceed the 
supervised learning after the 4th iteration. No such improvement exists in KC1 data set. In order to 
depict the number of modules in each project classi.ed into either fp or nfp, we also provide the confusion 
matrix for the four data sets with threshold 0.1 and the size of labeled data of 10% (Figure 7). Actual 
Actual Up until now, it seems that the condition for achieving the ben­ 0 1 0 629 19 1 303 46 Predict 
e.ts of semi supervised learning for software fault prediction with FTF algorithm is the size of the 
available labeled data set which 0 1 0 795 21 1 354 143 exceeds 5%. A general performance comparison 
is presented in Figure 8, where the PD values with thresholds 0.5 and 0.1 are Predict (a) PC1 (b) PC4 
Actual Actual shown as box plots. Each box plot is generated using the average performance over each 
data set. From the plot, we can observe a performance increase at both threshold settings. One obvious 
im­plication is the con.rmation of the known phenomenon that the size of the training data set (i.e., 
in our case the number of labeled software modules) impacts the performance on both FTF and the supervised 
fault prediction. The probability of detecting a fault prone software module with FTF is generally better 
than offered by supervised learning except for the cases when we attempted to 0 1 0 1124 96 1 469 208 
 Predict 0 1 0 4913 722 1 2884 1272 Predict (c) KC1 (d) JM1 Figure 7: Confusion Matrix(threshold=0.1), 
0 stands for nfp and 1 represents fp class build fault prediction models from very small subsets of software 
modules (the 2% setting).  6. DISCUSSION Our experiment offers new insight into the performance of semi­supervised 
approach to software fault prediction using FTF algo­ Labeled Data Figure 8: Comparison of supervised 
and semi supervised learning across .ve NASA MDP projects. rithm, compared to the corresponding supervised 
approach. We observe that there are apparent merits of semi supervised learning and FTF in particular. 
The advantage of FTF appears to be consis­tent on all the .ve data sets as we discussed. These observations 
offer answers to two of the questions posed in this study: 1. Does FTF with Random Forest as base learner 
outperform supervised learning with Random Forest? The answer is yes. 2. Is the behavior and performance 
of FTF consistent over dif­ferent data sets? Based on the results shown in Figures 4, 5, and 6, the results 
appear to be consistent.  The answer to the remaining question we posed, "how small can the size of 
the labeled data set be for the FTF to outperform su­pervised learning?" requires some elaboration. We 
refer the reader to Figure 8 which indicates that the differences between semi su­pervised and supervised 
approach to software fault prediction be­come more noticeable as the number of labeled modules increases. 
When only 2% or 5% of the modules are available for model de­velopment, there is simply too little useful 
information for any of the machine learning techniques. Empirically, it appears that fault prediction 
model development can start when 10% of the project modules are available and their fault content is 
known. In this sit­uation, semi supervised learning, however slightly, does appear to outperform supervised 
learning approach. Con.rming this obser­vation will require additional research. The bad news" from this 
study is that the difference between semi supervised and supervised learning is very modest and not sta­tistically 
signi.cant at the low threshold values (0.1). Given the na­ture of fault prediction especially for safety 
critical systems where identifying all faulty modules is very important, such low thresh­olds are of 
signi.cant interest. However, this is the type of the prob­lem where fault prediction becomes really 
dif.cult. Low thresh­olds imply large probability of detection (PD), i.e., more faulty modules are correctly 
identi.ed as fault prone. However, this also means that at low thresholds, a majority of the fault prone 
modules are being identi.ed and there are very few that remain undetected. For instance, project PC1 
contains 1,107 modules with the fault rate of 6.59%.If 50%,or 553 modules are labeled, amongst the remaining 
553 modules there are only 36 with true faults. After reaching the 82% probability of detection, observed 
for threshold 0.1 in Figure 8, there are only 6 undetected modules with faults. Finding these last few 
modules is a thorny issue for any learning technique, especially when we believe there is noise in data 
la­bels (i.e., some labels may have been assigned by mistake). All of these factors may prevent further 
improvements. However, ad­ditional work is needed to explore the impact of different factors. 7. CONCLUSIONS 
Semi-supervised approaches have been successfully applied to many machine learning problems. In this 
study, we investigated the application of an interactive self-training based semi-supervised approach 
(FTF) to the problem of software fault prediction. Semi supervised learning is a promising modeling technique 
because it may offer clearer guidance on the minimal size of data needed for training. Further, for software 
engineers, applying a more sophisti­cated modeling technique is straightforward because, at some point 
in time, the algorithms will be embedded into tool sets, making the complexity of the underlying approaches 
transparent and their ef­fects, the models, widely available. In FTF, the base supervised learner repeatedly 
updates by the labels of originally unlabeled software modules, those for which the fault content is 
unknown. We compared the semi-supervised approach and the corresponding supervised approach with random 
forest as the base learner. Our results show that the semi-supervised approach outperforms the corresponding 
supervised approach across the .ve projects from NASA MDP repository. We observed that semi supervised 
learning improves fault prediction only if the num­ber of initially labeled software modules exceeds 
5%. In other words, because there is a signi.cant imbalance between faulty and fault free modules in 
software projects (a large majority of com­ponents are fault free), very small number of modules available 
at the time of model development will not provide accurate models regardless of the modeling approach. 
In the future it might be worth to validate the performance of FTF algorithm in experiments with software 
projects of different sizes or from different domains than those used here. On the other hand, since 
we know that the choice of the base learner may have dramatic effects on the behavior of FTF algorithm 
experimenting with different base algorithms seems a good research direction too. 8. REFERENCES <RefA>[1] 
S. Abney. Understanding the yarowsky algorithm. Computational Linguistics, 30:2004, 2004. [2] A. Blum 
and T. Mitchell. Combining labeled and unlabeled data with co-training. pages 92 100. Morgan Kaufmann 
Publishers, 1998. [3] L. Breiman. Random forests. Machine Learning, 45:5 32, 2001. [4] O. Chapelle, 
B. Schölkopf, and A. Zien, editors. Semi-Supervised Learning. MIT Press, Cambridge, MA, 2006. [5] M. 
Culp, K. Johnson, and G. Michailidis. The ensemble bridge algorithm: a new modeling tool for drug discovery 
problems. Journal of Chemical Information and Modeling, 50(2):309 316, 2010. [6] M. Culp and G. Michailidis. 
An iterative algorithm for extending learners to a semisupervised setting. In The 2007 Joint Statistical 
Meetings (JSM, 2007. [7] R. M. Department and R. Mihalcea. Co-training and self-training for word sense 
disambiguation. In In CoNLL-2004, 2004. [8] N. Fenton, P. Krause, and M. Neil. Software measurement: 
Uncertainty and causal modeling. IEEE Softw., 19:116 122, July 2002. [9] N. Gayatri, S. Nickolas, A. 
Reddy, and R. Chitra. Performance analysis of datamining algorithms for software quality prediction. 
In Advances in Recent Technologies in Communication and Computing, 2009. ARTCom 09. International Conference 
on, pages 393 395, 2009. [10] S. Gokhale and M. R. Lyu. Regression tree modeling for the prediction of 
software quality. In In Proc. of ISSAT 97, pages 31 36, 1997. [11] A. B. Goldberg. Multi-manifold semi-supervised 
learning. [12] L. Guo, B. Cukic, and H. Singh. Predicting fault prone modules by the dempster-shafer 
belief networks. In Proc. 18th International Conference on Automated Software Engineering (ASE, 2003. 
[13] L. Guo, Y. Ma, B. Cukic, and H. Singh. Robust prediction of fault-proneness by random forests. In 
Software Reliability Engineering, 2004. ISSRE 2004. 15th International Symposium on, pages 417 428, 
2004. [14] G. Haffari and A. Sarkar. Analysis of semi-supervised learning with the yarowsky algorithm. 
In 23rd Conference on Uncertainty in Arti.cial Intelligence (UAI, 2007. [15] M. H. Halstead. Elements 
of Software Science. Elsevier, North-Holland, 1975. [16] Y. Jiang, B. Cukic, and Y. Ma. Techniques for 
evaluating fault prediction models. Empirical Software Engineering, 13:561 595, 2008. 10.1007/s10664-008-9079-3. 
[17] Y. Jiang, B. Cukic, T. Menzies, and N. Bartlow. Comparing design and code metrics for software quality 
prediction. In Proceedings of the 4th international workshop on Predictor models in software engineering, 
PROMISE 08, pages 11 18, New York, NY, USA, 2008. ACM. [18] T. Kamiya, S. Kusumoto, and K. Inoue. Prediction 
of fault-proneness at early phase in object-oriented development. In Object-Oriented Real-Time Distributed 
Computing, 1999. (ISORC 99) Proceedings. 2nd IEEE International Symposium on, 1999. [19] T. Khoshgoftaar, 
E. Allen, W. Jones, and J. Hudepohl. Classi.cation-tree models of software-quality over multiple releases. 
Reliability, IEEE Transactions on, 49(1):4 11, Mar. 2000. [20] T. Khoshgoftaar, E. Allen, K. Kalaichelvan, 
and N. Goel. Early quality prediction: a case study in telecommunications. Software, IEEE, 13(1):65 71, 
Jan. 1996. [21] A. Koru, K. Emam, D. Zhang, H. Liu, and D. Mathew. Theory of relative defect proneness. 
Empirical Software Engineering, 13:473 498, 2008. 10.1007/s10664-008-9080-x. [22] A. Liaw and M. Wiener. 
Classi.cation and regression by randomforest. RNews, 2(3):18 22, 2002. [23] A. Marchenko and P. Abrahamsson. 
Predicting software defect density: a case study on automated static code analysis. In Proceedings of 
the 8th international conference on Agile processes in software engineering and extreme programming, 
XP 07, pages 137 140, Berlin, Heidelberg, 2007. Springer-Verlag. [24] T. McCabe. A complexity measure. 
IEEE Transactions on Software Engineering, 2(4):308 320, Dec. 1976. [25] A. Niculescu-mizil and R. Caruana. 
Predicting good probabilities with supervised learning. In In Proc. Int. Conf. on Machine Learning (ICML, 
pages 625 632, 2005. [26] K. Nigam, A. K. Mccallum, S. Thrun, and T. Mitchell. Text classi.cation from 
labeled and unlabeled documents using em. In Machine Learning, pages 103 134, 1999. [27] N. F. Schneidewind. 
Investigation of logistic regression as a discriminant of software quality. In METRICS 01: Proceedings 
of the 7th International Symposium on Software Metrics, page 328, Washington, DC, USA, 2001. IEEE Computer 
Society. [28] N. Seliya and T. Khoshgoftaar. Software quality estimation with limited fault data: a semi-supervised 
learning perspective. Software Quality Journal, 15:327 344, 2007. 10.1007/s11219-007-9013-8. [29] N. 
Seliya and T. M. Khoshgoftaar. Software quality analysis of unlabeled program modules with semisupervised 
clustering. Systems, Man and Cybernetics, Part A: Systems and Humans, IEEE Transactions on, 37(2):201 
211, 2007. [30] D. Yarowsky. Unsupervised word sense disambiguation rivaling supervised methods. In IN 
PROCEEDINGS OF THE 33RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, pages 189 196, 
1995. [31] X. Zhu. Semi-supervised learning literature survey, 2006.   </RefA>
			
