
 A New Privacy Model for Hiding Group Interests while Accessing the Web Yuval Elovici Bracha Shapira 
Adlai Maschiach Ben-Gurion University Ben-Gurion University Ben-Gurion University P.O.B. 653, Beer-Sheva, 
P.O.B. 653, Beer-Sheva, P.O.B. 653, Beer-Sheva, Israel 84105 Israel 84105 Israel 84105 elovici@inter.net.il 
bshapira@bgumail.bgu.ac.il meshiach@bgumail.bgu.ac.il ABSTRACT This paper presents a new privacy model 
for hiding the information interests of a homogenous group of users who share a local area network and 
an access point to the Web. The suggested model is aimed at preventing eavesdroppers from using identifiable 
members' tracks to infer the group common interests (referred to as the group profile) while allowing 
members of the group to identify themselves to various services. The model consists of generating faked 
transactions in various fields of interest in order to confuse eavesdroppers by preventing them from 
accurate derivation of the group profile. A privacy measure is defined as one that reflects the degree 
of confusion a system can cause eavesdroppers. A prototype was developed implementing an HTTP proxy that 
derives the group profile and generates faked transaction in order to hide it. Experiments were conducted 
in order to examine the effectiveness of the model. Initial results are encouraging. Categories and 
Subject Descriptors K.4.1 [Computers and Society]: Public Policy Issues Â­ privacy  General Terms Measurement, 
Design, Experimentation, Security Keywords Privacy, User-Profile, User-Groups, Web -security 1. INTRODUCTION 
Privacy is becoming a serious challenge in computerized environments. Many companies on the Web are constantly 
attempting to violate the privacy of individual users as well as that of companies and organizations 
for their commercial benefit. Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are not made or distributed for 
profit or commercial advantage and that copies bear this notice and the full citation on the first page. 
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific 
permission and/or a fee. WPES'02, November 21, 2002, Washington, DC, USA. Copyright 2002 ACM 1-58113-633-1/02/011...$5.00. 
 Consequently, organizations that wish to preserve their privacy and thus not expose their professional 
interests to the outside world forego important Web services in order to prevent exposure. In general, 
Web users leave identifiable tracks at every surfed Web site [2]. The tracks can be differentiated between 
information rooted in the communication infrastructure involved and information rooted in the user s 
explicit or implicit information, such as his actions and behavior [5,7]. In the current study we deal 
solely with information directly received from the user. When a member of an organization requests any 
service on the Internet, eavesdroppers are able to reveal his identity, the Web site accessed (end server), 
and the link between them. Potential eavesdroppers are the end server, the organization s Internet Service 
Providers, or any entity that is able to listen to the communication between any of the organization's 
computers and the end server. From listening to the communication originated at the organization, eavesdroppers 
may collect contents of pages visited by organization members as well as other navigational behavior 
properties such as the time, frequency, and duration of exchanges. Eavesdroppers may employ machine learning 
techniques on the collected data to construct a group profile, i.e., to infer the organizational interests 
from the content of the accessed information. The group profile is constructed by eavesdroppers for different 
undesirable goals, such as industrial espionage, and without the authorization or awareness of the organization. 
While an organization might change its IP number, e-mail address and geographical location in an attempt 
to preserve its privacy, it is impossible to change the content of its information needs and interests 
if they are to be fulfilled, as they are like a fingerprint. The computer security community has concentrated 
on improving user anonymity on the Web by hiding identifiable tracks originating from the communication 
infrastructure involved, and by creating an anonymous channel between the user and the accessed site. 
However, hiding the identifiable tracks is not an adequate solution when the user is requested to be 
identified by the end server in order to receive their services. This identification is required for 
many information services for members, such as digital libraries, specialized databases and repositories, 
or when payment is requested and involves the use of credit cards. In the current study, a new non-anonymous 
privacy model is presented for privacy preservation while accessing public information repositories. 
The model provides a solution for hiding the accurate content of their information interests and surfing 
behavior of a homogenous group of users sharing a local area network and the access point to the Web. 
The group may for example be employees of an organization accessing the Web as part of their work, who 
want to hide their profile, i.e., information needs, interests and surfing behavior, in order to gain 
industrial confidentiality. The vector space model [15] which is a common and sound model to represent 
users profiles in the IR area was used to represent the group profile, as well as each group member profile. 
The privacy model suggested in this paper is aimed at hiding the group's profile while allowing its members 
to identify themselves to various services. The model prevents eavesdroppers from using identifiable 
members' tracks in order to derive the common group interests, i.e., its profile. The model is based 
on continuous tracking of information accessed by members of the group while surfing to enable derivation 
of an accurate group profile. In addition, continuous generation of faked transactions in various fields 
of interest is performed in order to disrupt the eavesdroppers from accurate inference of the group profile. 
As a result, the transactions log which eavesdroppers record consists of a mixture of the group's real 
and faked transactions, so that derivation of the group's correct profile is impossible. The faked transactions 
generated refer to information interests sufficiently different but still in the same general area of 
interest of the users, so that eavesdroppers are not able to distinguish between real and faked transactions, 
and thus cannot derive the users' and group profiles. To make the separation more difficult, faked transactions 
generation was designed to imitate user's navigation behavior, including following links. In addition, 
a new privacy measure is defined that allows an organization to calibrate a desired degree of privacy. 
The measure is based on the distance between the real users profile and the profile that the eavesdroppers 
observe. The remainder of the paper is organized as follows: section 2 reveals related background on 
Web anonymity and user profiling; section 3 presents the new model for hiding group interests; section 
4 introduces the implementation and evaluation of the model. Section 5 details possible attacks and cites 
the limitations of the model, and section 6 includes conclusive remarks and future research. 2. RELATED 
WORK The background relates to the following topics relevant to this study: Web privacy and anonymity 
 User and group profiling.  2.1 Web Privacy and Anonymity A number of tools have been developed to 
help Internet users surf the Web anonymously. These tools focus on ensuring that requests to Web sites 
cannot be linked to an IP address identifying the user. Some of the better-known tools are: Anonymizer 
- [3], (http://www.anonymizer.com), is a tool that submits HTTP requests to the Internet on behalf of 
its users so that the only IP address revealed to the Web sites is that of the Anonymizer. However, users 
have to trust the Anonymizer and their own ISPs who can still observe their activities. Crowds an anonymity 
agent developed at AT&#38;T Labs [12,13] based on the idea of blending into a crowd, i.e., concealing 
one's actions among the actions of others. To execute web transactions using this approach, a user first 
joins a crowd of users. The user's request to a Web server is passed to a random member of the crowd 
who can either submit it directly to the end server or forward it to another randomly chosen member. 
Neither the end server nor any other member of the crowd can determine where the requests originated. 
However, Crowds does not offer receiver anonymity. Any request includes the end server s address which 
can be viewed by any crowd member in order to enable submitting the request to the end server. Other 
tools include Onion-Router [5,6,18], in which users submit layered encrypted data that specifies the 
cryptographic algorithms and keys. At each pass through each onion-router on the way to the recipient, 
one layer of encryption is removed. All the above tools assume that users have no need or desire to be 
identified, which is a too strong assumption for the Web today. Many e-commerce transaction-based services 
on the Web are gaining popularity and require user's identification. The only type of solution suggested 
for private identified access is originated at the Web sites and attempts to build users trust in the 
Internet services. Some initiatives exist such as TRUSTe [1], that are dedicated to building consumers 
trust on the Internet by licensing Web sites to display a trust mark on their sites. However, for this 
type of solution, privacy depends on the good will of Web sites and limits the users access to licensed 
sites. Our privacy non-anonymous model originates at the user and does not rely on a third party nor 
limit access to specific sites. 2.2 User and Group Profiling While users browse, eavesdroppers may acquire 
knowledge about them and represent it in a user model (also referred to as user profile). The user profile 
is customarily used to represent users for various applications such as personalization [21,22], filtering 
services [23,24] . A profile consists of presentation of the user's information interests and behaviors 
so that prediction of relevancy of information items for him is made possible in many applications, by 
computation of the similarity between the user profile and the item. Various implicit acquisition methods 
exist and are employed for information retrieval (IR) and information filtering (IF) systems to derive 
accurate user and group profiles by recording and analyzing user behavior and activities [8,15,16,11,17]. 
Such activities include exploring sites (whose content is examined) spending time reading a data item, 
saving data items in the favorites list, printing and more [9,10]. Implicit user profile acquisition 
is transparent to the user and does not necessitate any active user involvement. Thus, profile derivation 
could be applied for the eavesdroppers commercial benefit without user awareness or authorization. In 
this study, the model tackles the eavesdroppers profile acquisition ability by the deterioration of its 
data, i.e., changing its log of user activities by faking activities. In our model we use the vector 
space model [14,15], which is commonly used in IR to represent user profiles. In the vector space model 
documents as well as profiles are represented each by a vector of significant weighted terms. The term 
weights reflect their importance in the document and their scarcity in the entire collection. A document 
d is represented by an n-dimensional vector space d=(w1,w2, ..wn) where wi represents the weight of terms 
i in document d. One common method for term weighting is TF*IDF [14], where TF is the term's frequency 
in the document (TF), and reflects the importance of the term in the document, and IDF is the inverse 
of the term frequency in the collection, (IDF), reflecting the extent to which this term differentiates 
between documents. The user profile part that represents his areas of interest consists of a vector of 
weighted terms reflecting the importance of the terms to his information needs. The profile can be derived 
(as done in our model) from a set of documents relevant to the user. In our application each site a user 
visits is transformed to a weighted term vector format, and the terms weights are added to the relevant 
entries of the user profile. Consider for example that a user visits a "security" related site. When 
transformed to the vector form, the term "security" was assigned the weight 30 reflecting its relative 
importance on the site. To reflect the fact that a user is interested in security, an entry for the term 
"security" in the user profile vector is added (if not already existing), and assigned the value 30. 
If the entry "security" already exists in the user's profile vector, its value is increased by 30. A 
new document is considered relevant to a user if the vector representing the document is sufficiently 
similar to the user profile. The similarity between a candidate document and a profile, or between two 
profiles, is defined by vector distance measuring methods, such as Euclidian distance or cosine (we use 
the cosine measure most commonly used in information retrieval [4]). The cosine measure represents the 
cosine of the angle between two vectors in the vector space. The similarity between the two vectors is 
in the interval of [0,1] where lower angles are closer to 1, and denote higher similarity.  . n (tu 
.tu ) ij ik i=1 S(uj ,uk ) = (1) nn  .tu2 ..tu2 ij ik i=1 i=1 where uj, uk = vectors tuij = the ith 
term in the vector uj. tuik the ith term in the vector uk. n = the number of unique terms in each vector. 
The group profile is represented as an accumulated weighted vector of its members. Each site visited 
by any group member is transformed to a weighted term vector and is added to the common group profile. 
 3. A NEW MODEL FOR HIDING GROUP INTERESTS Assume a group of users accessing the Web through an access 
point such as a cache proxy server. An eavesdropper located on the path between the access point and 
the Web can observe traffic between the group and the Web, and derive the group's common information 
interests. Of course, anonymous access might prevent this problem, but is not always desired. A basic 
assumption underlying the suggested privacy model is that preserving the group privacy can be partially 
accomplished by concealing the group's common information interests, and this is the goal of the suggested 
model. As mentioned in section 2, in our model the vector space model to represent the common information 
interests of the group is used. The common interests are derived from the content of the sites they visit. 
The vector space model implies that terms which relate to subjects included in the group's areas of interests 
are likely to appear on the visited site and therefore receive higher weight on the vector. In order 
to formulate the model the following definitions are used: A User Transaction is defined as an access 
to a Web page from the user computer. Internal Group Profile (IGP) is defined as the common group profile 
constructed inside the group s HTPP proxy. The IGP is based on the content of pages accessed by members 
of the group.  External Group Profile (EGP) is defined as the common group profile based on the information 
flowing from the Web to the group s proxy server. An eavesdropper is able to compute the EGP by listening 
in the  traffic between the group proxy server and the Web. We assume that eavesdroppers do not have 
any smart agents such as computer viruses inserted inside the proxy server or inside the group's computers, 
and are therefore not able to compute the IGP. However, the EGP and the IGP are usually identical since 
the pages accessed by the group members are the same pages flowing from the Web to the group HTTP proxy. 
The suggested model includes a component that monitors group transactions and accordingly generates faked 
transactions in related but different fields of interest. The faked transactions are aimed at creating 
a cloud of fussiness around the real group's exact interests to prevent the eavesdropper from accurate 
derivation of the IGP by confusing its automated programs with wrong data. The faked transactions are 
mistakenly considered by eavesdroppers as group members transactions. Therefore, the EGP, which is based 
on faked transaction, is different from the IGP which is based on real group transactions. The model 
architecture is shown in Figure 1. It consists of four main components: Browser Monitor, Transaction 
Generator, Group Profile Meter, and a proxy server. The Browser Monitor is located in each of the group 
member computers. The Transaction Generator and Group Profile Meter are located inside a dedicated server 
which we call "fake transaction server". The proxy server is a shelf product that is used among the other 
components to hide the original IP of the group computers so that an eavesdropper outside the organization 
will not be able to distinguish between the group member transactions and the fake transactions generated 
by the fake transactions generator. The Browser Monitor Input-Output: The Browser Monitor receives as 
input user transactions (from one of the group members), and produces as output a vector of weighted 
terms for each transaction result. The vector represents a page visited by a user. Each vector is sent 
to the Group Profile Meter. Functionality: While the user surfs the Web using a browser, the Browser 
Monitor analyzes the contents of the results of user transactions i.e., the Web pages that are presented 
to the user. For each user s transaction, the Browser Monitor generates a vector of weighted terms to 
represent the transaction. denotes the UVt U vector of weighted terms for a transaction at time stamp 
tU. The vector U is sent to the Group Profile Meter as input for the VtU process of the construction 
of IGP(tU) . IGP(tU) stands for the Internal Group Profile (IGP) at time stamp tU .  The Transaction 
Generator Input-Output: The Transaction Generator receives input from two sources. One source is an internal 
glossary DB, which includes a glossary of terms in the areas of interests generally related to those 
of the organization, that sends a set of related terms. The other input source is the Group Profile Meter 
that sends the Internal Group Profile. These sources of input are used by the Transaction-Generator to 
generate and send queries to a search engine resulting in a list of sites from which the system randomly 
activates pages to form the faked transactions. The Transaction Generator also sends the Group Profile 
Meter a vector of weighted terms for each faked transaction. Functionality: The Transaction Generator 
generates faked transactions. The exact number of faked transactions generated for each user transaction 
is controlled by a system parameter that symbolizes the average number of faked transactions per user 
transaction. This parameter (denoted as Tr) can be calibrated by the user. The Transaction Generator 
does not generate exactly Tr transactions for each user transaction, but rather generates Tr transactions 
on average for each user s transaction. This is done to prevent eavesdroppers from discovering the regularity 
of faked transactions among user s transactions. The faked transactions consist of Web pages in various 
fields of interest. In order to assure the confusion of eavesdroppers most of the faked transactions 
should be composed of pages not specifically related to the group interests, but rather to the same general 
idea. The faked transactions aim at blurring the exact group interests to make it difficult or even impossible 
to identify the group's specific interests. We believe that it is more effective to use the fuzziness 
method that generates transactions related to the same general topic than to generate totally unrelated 
transactions. Eavesdroppers might apply sophisticated classifying (categorization) techniques (such as 
cluster analysis) to distinguish between real and faked transactions. Group Member Computers  WEB Degree 
of Privacy Pr , Tr Figure 1. Privacy Model Architecture Sending a faked transaction string as a query 
to a search engine activates a faked transaction generation. The Transaction Generator randomly accesses 
selected pages from the set of results to the faked query, and follows links in depth. Each access to 
a page is a faked transaction. The effect of fuzzy faked transactions relating to the general group interests, 
is obtained by constructing the faked transaction query string from a mix of terms originating from the 
Internal Group Profile (IGP), along with random terms originating from an internal database of term glossary 
relating to the general topics of interest to the group. The heuristics for choice of terms for the faked 
transactions string is still one of our current research objectives. In addition, the Transaction Generator 
builds a vector of term weights for each of the faked transactions results generated. T Vt T denotes 
the vector for a faked transaction result at time stamp tT . The vectors are sent to the Group Profile 
Meter where they will be used to compute the EGP (External Group Profile).  The Group Profile Meter 
Input-Output: The Group Profile Meter receives a vector of term weights from the Browser Monitor for 
each user UVtU UVtU transaction. Based on the vectors it computes the IGP(tU) which is the Internal Group 
profile at time stamp tU . The IGP(tU) is sent to the Transaction Generator. The Group-Profile Meter 
also receives a vector of term weights T from the Transaction Vt T Generator for each faked transaction, 
and computes the FGP(tT ) which is the Faked Group Profile at time stamp tT. Another Input-Output interaction 
of the Group Profile Meter is with the organization. The Group Profile Meter sends the organization administrative 
information about its current degree of privacy (the privacy measure) so that it can set the Group Profile 
Meter parameters (Tr, Pr) in order to calibrate the desired degree of privacy. Functionality: The following 
section describes in detail the four tasks that the Group Profile Meter is responsible for. Figure 2 
is a graphical illustration of the tasks: 1. Generation of the IGP(tU) UVtU In the event of receiving 
a user transaction vector of term weights from the Browser Monitor, the Group Profile Meter constructs 
IGP(tU) -the Internal Group Profile at time stamp tU . This is done by combining vectors: UV tU with 
former Pr-1 transaction ( UIGP t ) = . -= -1Pr 0i U it V U (2) Pr is a system parameter that defines 
the number of previous transactions vectors to be included in the construction of a group profile. Pr 
can be calibrated by the organization. Transaction Generator Vector of Term Frequencies of real users 
transactions IGP ( tU ) n Browser .(tigpi . tegpi ) Monitor i=1 S(IGP, EGP) = nn Pr EGP(t) .tigpi 
2 ..tegpi 2 i =1 i =1 i=1 .Termi+t TrÂ·Pr-1 Transaction VtTT Generator .-i i=0 FGP(tT ) Vector of Term 
Frequencies of faked transactions Degree of Privacy at time t Figure 2. Group Profile Meter Internal 
Components 2. Generation of the FGP(tT) In the event of receiving a faked transaction vector of term 
weights T from the Transaction Generator, the Group Profile Vt T Meter constructs FGP(tT) -the Faked 
Group Profile at time stamp tT. This is done by combining T with former (Pr x Tr 1) faked Vt T transactions 
vectors: Pr Â·Tr-1 FGP(tT ) =.V TT (3) t -i i=0 Tr is a system parameter that defines the average number 
of faked transactions generated by the Transaction Generator for each user transaction. Tr can be calibrated 
by the organization. 3. Generation of the EGP(t) In the event of receiving a transaction vector of terms 
weight, either a real user transaction vector or a faked one, the Group Profile Meter constructs the 
EGP(t) that denotes the External Group Profile at time stamp t. To construct EGP(t), the Group Profile 
Meter combines IGP(tU) and FGP(tT) into one vector: EGP(t) = IGP(tU ) + FGP(tT )(4) EGP(t) is updated 
when either IGP(tU) or FGP(tT) changes. An eavesdropper that observes the group traffic will derive EGP(t) 
as the group common profile because EGP(t) reflects all transactions that occurred in the group s environment. 
In fact, EGP(t) is not the real group profile, rather it is a blend of the group profile and the faked 
transactions profile. 4. Computing the privacy measure Whenever the IGP(tU) or the EGP(t) changes, the 
Group Profile Meter computes the similarity between the profiles (IGP,EGP)) by finding the cosine of 
the angle between the vectors:   . n (tigpi . tegpi ) i=1 S(IGP, EGP) = nn (5) .tigpi 2 ..tegpi 2 
i=1 i=1 where: tigpi = the ith term in the vector IGP tegpi= the ith term in the vector EGP n = the number 
of unique terms in each vector. The similarity between the IGP(tU ) and the EGP(t) is used as a measure 
of privacy at time stamp t, and is sent to the group administrator, which might in return calibrate Tr 
and Pr in order to achieve the group degree of privacy desired. The Profile Meter tasks are described 
in Figure 2. 4. MODEL IMPLEMENTATION AND EVALUATION An initial prototype system implementing the model 
was developed in order to examine the feasibility of the model and to conduct experiments. The Transaction 
Generator and Group Profile Meter are implemented as one process, and is written in Borland C++ Builder, 
and activated in a dedicated server (faked transaction server). A plug-in was written to the Microsoft 
IE6 using MSDEV in order to implement the Browser Monitor and was installed in five computers of a group 
of programmers working on the same project (i.e., having the same information interests). During the 
experiment five users, group members of a programming team in a software company, generated approximately 
200 transactions while browsing pages related to their project (computer security). In order to create 
a fuzzy profile (EGP), to confuse eavesdroppers, the system created faked transactions using a glossary 
containing terms related to general computer programming subjects. The system s parameters were set to 
Tr=1,..,10, where Pr was set such that all the transactions were taken into account (Pr=125), thus, for 
each user s transaction, an average of Tr=1,..,10 faked transactions were generated (Tr). The graph presented 
in Figure 3, shows the similarity S(IGP,EGP) between IGP(tU) and EGP(t) at each event of a new transaction, 
whether it is a user or a faked transaction for Tr=1,..,10. The values of S(IGP,EGP) range from 0 to 
1. Higher values of S(IGS,EGS) suggest that EGP(t) is similar to IGP(tU) while lower values suggest that 
EGP(t) is less similar. In fact, the similarity reflects the degree of accuracy of the real group profile 
derivation that an eavesdropper could achieve and it could be used as a privacy measure. The preliminary 
runs presented in Figure 3 show low similarity between the real and fake transactions (ranges between 
0.42 for Tr=1 to 0.14 for Tr=10). As the number of fake transactions increases, similarity decreases. 
Further examination of results suggests that marginal decrease in similarity is lessens as Tr increases. 
We believe that the reason for this is the inclusion of terms taken from the group profile in the creation 
of faked transactions which stabilize the difference between the real and faked profile over time. The 
balance between the terms taken from the group profile and the glossary DB need further research and 
experiments. Figure 3. Results of Preliminary Runs 5. POSSIBLE ATTACKS AND LIMITATIONS This section 
details the possible attacks on a system implementing the privacy model. The main objective of the attacks 
is to separate between human and machine-generated transactions by identifying the different behavior 
of human and machine surfing, as is done in traffic analysis. It explains how the system based on the 
model can cope with such attacks, and discusses some of the model's limitations.  5.1 Possible Attacks 
The first transaction feature that attackers might look at is the duration of the transactions, assuming 
that human browsing takes longer than that of a machine since humans tend to look at and even read the 
content of the accessed page. In order to prevent attackers from identifying machine transactions by 
their duration, the Transaction Generator that activates several search transactions in parallel adjusts 
their duration. The average duration of the faked transactions is similar to the average human transactions, 
and depends on the length of the page. Thus, attackers observing several transactions will not find it 
easy to differentiate between human and machine navigation by their duration. In addition, since all 
the transactions pass to the Web through one proxy it is possible to add a mechanism to the proxy that 
changes the duration of the real transactions in order to prevent any "duration attack". An attacker 
might try to distinguish between the user and machine navigation by the navigational behavior. More precisely, 
in real transactions humans may follows links from page to page while faked machine transactions are 
a single level access to sites appearing on the search result page of the query generated by the Transaction 
Generator. In order to prevent this attack two opposite tactics were employed. The first consists of 
humanizing the faked machine transactions by learning and imitating the user's navigational behavior 
such as the number of links he follows and the depth of navigation. The second consists of dehumanizing 
the real human transactions by generating additional machine-like navigations to some of the links on 
the pages browsed by the user. Both tactics should be able to cope with other attacks trying to differentiate 
between human and machine navigations. Another possible method that might be employed to differentiate 
between user and faked transactions is to apply autoÂ­clustering algorithms on the transactions observed 
by eavesdroppers resulting with several clusters. Eavesdroppers may assume that one of the resulting 
clusters represents the users transactions as these transactions are probably more similar to each other 
than to the faked transactions. For example, assume a group of users browsing in the area of computer 
security and that the faked transactions are in the areas of computer hardware and computer software. 
An auto-clustering algorithm might create three clusters/groups of transactions, and accordingly create 
three groups profiles. Now, the probability of finding the group profile reduces to 1/3, as one of those 
three profiles is that of the user. In order to cope with such attacks, the Transaction Generator navigates 
in the general area of interest of the group (as explained in section 4). For example, if a group is 
navigating in computer security sites, the Transaction Generator generates transactions that are related 
to this general topic but not in sites concerning the exact sub topic that the group is interested in, 
so that to confuse the auto-clustering process. We are in the process of experimenting the system's ability 
to cope with these types of attacks, and for this we use cluster-analysis methods to cluster transactions 
data. Initial results are encouraging as on most runs the user real profile was not one of the clusters. 
 5.2 Limitations of the Model Some may argue that any transaction employing an interactive form or account 
login will be difficult to fake convincingly. Indeed a system based on the new model will not be limited 
in its navigation to the non-hidden Web only in those sites where human interaction is not needed. Actually, 
in many interactive sites the username and password are taken by default from the browser repository. 
However, in order to reduce this problem, the browser that is used for navigation (Microsoft Internet 
Explorer) was set to automatically identify itself as an anonymous user in any site requiring identification, 
and where the user name and password in the browser repository are not used as a default user. This setting 
prevents the display of annoying login screens to the user. Moreover, in our experience with the system, 
the number of transactions requiring user interaction is minor, so in our view this limitation is not 
crucial. There has been a relatively new effort to invent explicit tests aiming at distinguishing humans 
from bots [19,20]. Most of these tests are used to prevent a machine from subscribing to various services. 
For example, when someone wants to create new e-mail accounts in yahoo, the site activates a "Captcha" 
which is a check to identify if a subscriber is human by requesting him to solve a problem that only 
humans can solve. However, our system cannot access sites or pages that are protected by Captcha , and 
so will not be able to solve the problem. On the other hand, we believe that the reverse Turing test 
is a minor limitation to our system for the following two reasons: Captcha tests are not used to prevent 
machines from accessing sites, but rather from registrations to services. It is unlikely that sites which 
usually encourage as many accesses as possible for self publication will use this type of mechanism to 
prevent machines from accessing and analyzing of their content.  The Transaction Generator uses a search 
engine in order to retrieve links used to generate faked transactions. It is unlikely that a search engine 
crawler (which is itself a machine), is able to access links that are protected by such a mechanism. 
 There is another philosophical question that must be taken into account here. Since the number of transactions 
that emanate from the group proxy is high, it is reasonable to assume that the attackers will be using 
some computerized program. Any claim that our model might to some extent pass the Turing test is not 
relevant since the judge (or the actor that needs to differentiate between a machine and a human), in 
our case is not itself human , but another computer program. We believe that a model based on our program 
makes it difficult for another program to differentiate between human and machine-generated transactions. 
 6. SUMMARY AND FUTURE RESEARCH The new privacy model presented in this paper implies some important 
theoretical and practical significance. Non-anonymous privacy - The main advantage of the suggested 
model is the idea of privacy preservation. It enables a member of a group to identify himself while preserving 
the group privacy essential in the current international situation where new laws aimed at fighting international 
terrorism are introduced to compel users to identify themselves when accessing public information such 
as the Web. In addition, the urgency of our solution stems from the fact that the Internet is transforming 
into a nonÂ­free service where identifiable transactions will be required. Thus, anonymity cannot be a 
solution for privacy and our model seems to be a suitable solution.  Privacy measure A new privacy 
measure is suggested based on the similarity between the Internal Group and the External Group Profiles. 
This idea of quantifying privacy is significant as it can be used to evaluate the degree of privacy guaranteed 
by systems, sites, or any software to access a public information repository, and allows  organizations 
to define the degree of privacy desired to achieve while using a system. To evaluate the effectiveness 
of the model experiments are planned to test the system with a range of users and various group domains. 
We will also examine: 1. The effect of different values of Pr on similarity. It is obvious that lower 
values of the parameter Pr will increase the frequency of the group profile update, which is useful in 
the case where the user has several distinct group profiles, while higher values will decrease the frequency 
of group profile update, which is useful in the case where the user does not change his group frequently. 
We plan to empirically examine the effect of the frequency of group profile adaptation on similarity 
results, and to try to define an optimum. 2. The effect of different values of Tr on system performance. 
It is obvious that higher values of Tr will increase the communication bandwidth towards the Internet. 
We want to define the relation between bandwidth and similarity, and thus define the additional bandwidth 
needed in order to confuse the eavesdropper to a certain degree.  7. REFERENCES <RefA>[1] Benassi, P.: TRUSTe: 
An online privacy seal program. Communication of the ACM, 42,2 (1999) 56-59 [2] Brier, S. : How to keep 
your privacy: Battle lines get clearer. The New York Times, January 13, (1997) [3] Claessens, J., Preneel, 
B., Vandewalle, J.: Solutions for anonymous communication on the Internet. Proceedings of the 1999 IEEE 
International Carnahan Conference on Security Technology 487 (1999) 298-303 [4] Frakes, W. B., Baeza-Yates, 
R. (ed.): Information Retrieval, Data Structures and Algorithms, Pentice Hall, NJ, USA (1992). [5] Goldschlag, 
D. M., Reed, M. G., Syverson, P. F.: Hiding Routing Information, Information Hiding, R. Anderson (editor), 
Springer-Verlag LLNCS 1174 (1996) 137-150. [6] Goldschlag, D. M., Reed, M. G., Syverson, P. F.: Onion 
Routing for Anonymous and Private Internet Connections, Communications of the ACM 42 (2) (1999) 39-41. 
[7] Grabber, E., Gibbons, P.B., Matias, Y., Mayer, A.: How to Make Personalized Web Browsing Simple, 
Secure, and Anonymous. Proceedings of Financial Cryptography (1997). [8] Hanani, U., Shapira, B., Shoval, 
P.: Information Filtering: Overview of Issues, Research and Systems, User Modeling and User-Adapted Interaction 
11 (2001) 203-259. [9] Konstan, A., Bradley, N. M., Malts, D., Herlocker, J. L., Gordon, L. R. and Riedl, 
J.: GroupLens:., Applying collaborative filtering to usenet news. Communications of the ACM 40(3) (1997) 
77-87. [10] Morita, M., Shinoda, Y.: Information filtering based on user behavior analysis and best match 
retrieval. Proceedings of the 17th Annual Intl. ACM SIGIR Conference on Research and Development (1994) 
272-281. [11] Oard, D.: The State of the Art in Text Filtering, User Modeling and User Adapted Interaction, 
7(3) (1997) 141Â­ 178. [12] Reiter, M.K., Rubin, A.D.: Crowds: Anonymity for Web Transactions, ACM Transactions 
on Information and System Security, 1(1) (1998) 66-92. [13] Reiter, M.K., Rubin, A.D.: Anonymous Web 
Transactions with Crowds, Communications of the ACM 42(2) (1999) 66-92 [14] Salton, G., Buckley, C.: 
Term-Weighting Approaches in Automatic Text Retrieval, Information Processing and Management, 24( 5) 
(1998) 513-523. [15] Salton, G., McGill, W.J.(ed.): Introduction to Modern Information Retrieval. McGraw-Hill. 
New-York (1983). [16] Shapira, B, Shoval, P., Hanani,U.: Experimentation with an Information Filtering 
System that Combines Cognitive and Sociological Filtering Integrated with User Stereotypes, Journal: 
Decision Support Systems 27 (1999) 5-24. [17] Shapira B., Hanani U., Raveh A., Shoval P.: Information 
Filtering: A New Two-Phase Model Using Stereotypic User-Profiling. Journal of Intelligent Information 
Systems 8 (1997) 155-165. [18] Syverson, P. F., Goldschlag, D. M., Reed, M. G.: Anonymous Connections 
and Onion Routing, Proceedings of the 18th Annual Symposium on Security and Privacy, IEEE CS Press, Oakland, 
CA, (1997) 44-54. [19] Blum, M., Von Ahn L. A., Langford, J.: The CAPTCHA Project, Completely Automatic 
Public Turing Test to tell Computers and Humans Apart, www.captcha.net, Dept. of Computer Science, Carnegie-Mellon 
Univ., and personal communications, November 2000. [20] Coates, A. L., Baird, H. S., Fateman, R. J.: 
Pessimal Print: A Reverse Turing Test, Proceedings of the Sixth International Conference on Document 
Analysis and Recognition (ICDAR 2001); 2001 September 10-13; Seattle, WA. Los Alamitos, CA: IEEE Computer 
Society; 2001; 1154-1158. [21] Batista, P., Silva, M. J.: Mining Web Access Logs of an On-Line Newspaper, 
RPEC2-Workshop on recommendation and personalization on e-commerce, Spain 2002. [22] Loeber, S. G., Aroyo, 
L. M., Hardmannbsp, L.: An Explicit Model for Tailor-Made eCommerce Web Presentations, Workshop on recommendation 
and personalization on eÂ­commerce, Spain 2002. [23] Delgado, J.: Multi-Agent Learning in Recommender 
Systems for Information Filtering on the Internet, International Journal of Cooperative Information Systems 
Vol. 10, Nos. 1 &#38; 2 (2001) 81-100. [24] Claypool, M., Gokhale, A., Miranda, T., Murnikov, P., Netes, 
D., Sartin, M.: Combining Content-Based and Collaborative Filters in an Online Newspaper (1999), ACM 
SIGIR Workshop on Recommender Systems Berkeley, CA, 1999. </RefA> 
			
