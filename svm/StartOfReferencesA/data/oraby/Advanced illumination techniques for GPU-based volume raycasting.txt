
 Course Notes Advanced Illumination Techniques for GPU-Based Volume Raycasting Markus Hadwiger VRVis 
Research Center, Vienna, Austria Patric Ljung Siemens Corporate Research, Princeton, USA Christof Rezk 
Salama University of Siegen, Germany Timo Ropinski University of M¨unster, Germany  Advanced Illumination 
Techniques for GPU Volume Raycasting Abstract Volume raycasting techniques are important for both visual 
arts and visualization. They allow an e.cient generation of visual e.ects and the visualization of scienti.c 
data obtained by tomography or numeri­cal simulation. Thankstotheir .exibility,expertsagreethatGPU-based 
raycastingis the state-of-the art techniqueforinteractive volume render­ing. It will most likely replace 
existing slice-based techniques in the near future. Volume rendering techniques are also e.ective for 
the direct ren­dering ofimplicit surfaces used for softbody animation and constructive solid geometry. 
The lecture starts o. with an in-depth introduction to the concepts behind GPU-based ray-casting to provide 
a common base for the fol­lowing parts. The focus of this course is on advanced illumination tech­niques 
which approximate thephysically-based light transport more con­vincingly. Such techniques include interactive 
implementation of soft and hard shadows, ambient occlusion and simple Monte-Carlo based ap­proaches to 
global illumination including translucency and scattering. With the proposed techniques, users are able 
to interactively create convincing images from volumetric data whose visual quality goes far beyond traditional 
approaches. The optical properties in participating media arede.ned using thephasefunction. Many approximations 
to the physicallybasedlighttransportappliedfor rendering naturalphenomena such as clouds or smoke assume 
a rather homogenous phase function model. For rendering volumetric scans on the other handdi.erentphase 
function models are required to account for both surface-like structures and fuzzy boundaries in the 
data. Using volume rendering techniques, artists who create medical visualization for science magazines 
may now work ontomographicscansdirectly, withoutthenecessity tofallback to creating polygonal models 
of anatomical structures. Course participants shouldhave a workingknowledge in computer graph­ics, basic 
programming skills. They should be familiar with graphics hardware and shading languages. We will assume 
a basic knowledge re­garding volume data as well as interactive volume rendering techniques. Prerequisites 
Intermediate The course targets the steadily growing number of devel­opers who create specialized implementations 
of volume rendering tech­niques on state-of-the-art graphics hardware, regardless of whether they are 
working in visual arts or scienti.c visualization. Level of Di.culty Contact Christof Rezk Salama Timo 
Ropinski (course organizer) Visualization and Computer Computer Graphics Group Graphics Research Group, 
University of Siegen University of M¨unster H¨olderlinstr. 3 Einsteinstr. 62 57068 Siegen, Germany 48149 
M¨unster, Germany email: rezk@fb12.uni-siegen.de Markus Hadwiger Patric Ljung VRVis Research Center for 
Siemens Corporate Research Virtual Reality and Visualization Imaging &#38; Visualization Department Donau-City-Straße 
1 755 College Road East A-1220 Vienna, Austria Princeton, NJ 08540 email: msh@vrvis.at email: patric.ljung@siemens.com 
 Lecturers Markus Hadwiger VRVis Research Center for Virtual Reality and Visualization Donau-City-Strasse 
1, A-1220 Vienna, Austria email: msh@vrvis.at Markus Hadwiger is a senior researcher at the VRVis Research 
Center in Vienna, Austria. He received his Ph.D. in computer science from the Vienna University of Technology 
in 2004, and has been a researcher at VRVis since 2000, working in the areas of visualization, volumerendering, 
andgeneralGPU techniques. Hehasbeeninvolvedin several courses and tutorials about volume rendering and 
visualization atACMSIGGRAPH,IEEEVisualization, andEurographics. Heis a co­author ofthebook Real-TimeVolumeGraphicspublishedbyAKPeters. 
Patric Ljung Department of Imaging and Visualization Siemens Corporate Research 755 College Road East 
Princeton, NJ 08540, U.S.A. email: patric.ljung@siemens.com Patric Ljung joined in 2007Siemens CorporateResearch 
inPrince­ton, NJ, where he works as a Research Scientist in the Imaging Ar­chitectures group. He received 
2006 his PhD in Scienti.c Visualization from Link¨oping University, Sweden and graduated with honors 
in 2000 his MS in Information Technology from Link¨oping University. Between 1989and1995he worked as 
a software engineer with embedded and tele­com systems involving software architectures, graphical user 
interfaces, voice-mail systems, communication protocols, network and interprocess communication, compilers. 
Dr. Ljung has published several papers in international conferences and journals including IEEE Visualization, 
Eurographics conferences, IEEE TVCG and others, on volume rendering of large medical data sets, GPU-based 
raycasting of multiresolution data sets. One important focus areahasbeenVirtualAutopsiesforforensicpathology. 
His current research interest is in advanced illumination and shading techniques, software architectures 
for extensible graphics, and management and rendering of large medical data sets. Timo Ropinski Visualization 
andComputerGraphicsResearchGroup(VisCG) University of M¨unster Einsteinstr. 62 48149 M¨unster, Germany 
email: ropinski@math.uni-muenster.de Timo Ropinski is a postdoctoral researcher working in the .eld of 
medical volume visualization. After receiving his PhD in 2004 from the University of M¨unster, he became 
a project leader within the collabo­rative research center SFB 656, a cooperation between researchers 
from medicine, mathematics, chemistry, physics and computer science. His researchisfocused oninteractive 
aspectsin medical volume visualization with the goal to make these techniques more accessible. He is 
initiator of the Voreen open sourceproject(www.voreen.org), in whicha .exible volume rendering framework 
is developed. The results of his scienti.c work have been published in various international conferences 
including Eurographics, IEEE Visualization, IEEE VR, VMV and others. Christof Rezk Salama Computergraphik 
und Multimediasysteme, University of Siegen, H¨olderlinstr. 3, 57068 Siegen, Germany phone: +49 271-740-3315 
fax: +49 271-740-3337 email: rezk@fb12.uni-siegen.de Christof Rezk-Salama has received a PhD from the 
University of Erlangen-Nuremberg as a scholarship holder of the graduate college 3D Image Analysis and 
Synthesis. He has worked as a research engineer for the R&#38;D department of Siemens Medical Solutions. 
Since October 2003 heis working asanassistantprofessor attheComputerGraphicsGroup of the University of 
Siegen, Germany. The results of his research have been presented at international con­ferences, including 
ACM SIGGRAPH, IEEE Visualization, Eurograph­ics, MICCAI and Graphics Hardware. He is regularly holding 
lectures, courses and seminars on computergraphics, scienti.c visualization, char­acter animation and 
graphics programming. He has gained practical experience in applying computer graphics to several scienti.c 
projects in medicine, geology and archaeology. Christof Rezk-Salama has released the award winning open-source 
project OpenQVis and is a co-author of the book Real-Time Volume Graphics. Detailed information about 
this research projects are available at: http://www.cg.informatik.uni-siegen.de/People/Rezk http://www.real-time-volume-graphics.org 
http://openqvis.sourceforge.net  Course Syllabus The half-day course will consist of four di.erent 
blocks. From a didactic point of view, each block will loosely build upon the information pro­vided in 
previous blocks with growing complexity and increasing level of di.culty. The schedule is only tentative, 
since at the time of writing these notes, the .nal time slots have not yet been allocated by the organizers. 
MORNING Introduction and Basics [45 min] (M. Hadwiger) Introduction and Basics Application Areas for 
Volume Rendering Bene.ts and Drawbacks of Ray-Casting GPU-based Volume Ray-Casting Space Leaping and 
Early Ray Termination Memory Management Multiresolution LOD and Adaptive sampling 1:45pm 2.30pm Light 
Interaction [45 min] (T. Ropinski) Light Transport and Illumination Models Local Volume Illumination 
 Specular Re.ection through Ray-Tracing Soft vs. Hard Shadows Semi-Transparent Shadows with Deep Shadow 
Maps 2:30pm 3:15pm BREAK [15 min] 3:15pm 3:30pm 3:30pm 4:15pm Ambient Occlusion [45 min] (P. Ljung) 
 Ambient Occlusion for Isosurfaces Deep Shadow Maps Local Ambient Occlusion (DVR) Dynamic Ambient 
Occlusion (DVR) 4:15pm­5:15pm Scattering [60 min] (C. Rezk-Salama) Single and Multiple Scattering Transparency 
and Translucency Monte-Carlo integration GPU-Based Importance Sampling GPU-Based Monte-Carlo Volume 
Raycasting Scattering with Deep Shadow Maps 5:15pm­5:30pm Discussion, Questions and Answers [15min] 
(all speakers)  Contents I GPU-Based Ray Casting 1 1 Introduction 2 1.1 VolumeData ........................ 
3 1.2 DirectVolumeRendering .................. 4 1.2.1 OpticalModels ................... 5 1.2.2 The 
Volume Rendering Integral . . . . . . . . . . 6 1.2.3 Ray Casting ..................... 8 1.2.4 AlphaBlending 
................... 10 2 GPU-based Volume Ray Casting 12 2.1 BasicRay Casting ..................... 
13 2.2 Object-Order Empty Space Skipping . . . . . . . . . . . 15 2.3 AdvancedRay CastingPipeline . 
. . . . . . . . . . . . . 17 2.3.1 Culling and Brick Boundary Rasterization . . . . 20 2.3.2 Geometry 
Intersection ............... 23 2.4 IsosurfaceRay Casting ................... 24 2.4.1 AdaptiveSampling 
................. 25 2.4.2 IntersectionRe.nement . . . . . . . . . . . . . . 28 2.5 Memory Management 
.................... 28  2.6 Mixed-Resolution Volume Rendering . . . . . . . . . . . 30 2.6.1 Volume 
Subdivision for Texture Packing . . . . . 31 2.6.2 Mixed-Resolution Texture Packing . . . . . . . . 32 
2.6.3 AddressTranslation ................ 33  2.7 Multiresolution LOD and Adaptive sampling . . . . 
. . . 35 2.7.1 Octree-based Multiresolution Representation . . . 35 2.7.2 Block Properties and Acceleration 
Structures . . 36 2.7.3 Hierarchical Multiresolution Representations . . . 37 2.8 Level-of-DetailManagement 
................ 38  2.8.1 View-Dependent Approaches . . . . . . . . . . . . 39 2.8.2 Data Error Based 
Approaches . . . . . . . . . . . 39 2.8.3 Transfer Function Based Approaches . . . . . . . 40  2.9 
Encoding,Decoding andStorage . . . . . . . . . . . . . . 40 2.9.1 Transform and Compression Based Techniques 
. . 41 2.9.2 Out-of-Core Data Management Techniques . . . . 43 2.9.3 Flat blocking Multiresolution Representation 
. . . 44  2.10 Sampling of Multiresolution Volumes . . . . . . . . . . . 46 2.10.1 NearestBlockSampling 
. . . . . . . . . . . . . . 47 2.10.2 Interblock Interpolation Sampling . . . . . . . . . 48 2.10.3 Interblock 
Interpolation Results . . . . . . . . . . 50 2.11Raycasting ontheGPU .................. 51  2.11.1 Adaptive 
Object-Space Sampling . . . . . . . . . 51 2.11.2FlatBlocking Summary . . . . . . . . . . . . . . 53 
II Light Interaction 55 3 Light Transport and Illumination Models 56 3.1 Phong Illumination ..................... 
56 3.2 GradientComputation ................... 59 3.3 Specular Re.ections through Ray-Tracing . . . 
. . . . . 61 4 Shadows 68 4.1 Softvs. HardShadows ................... 68 4.2 Semi-Transparent Shadows 
with Deep Shadow Maps . . 70 III Ambient Occlusion 77 5 Ambient Occlusion for Isosurfaces 78 6 Ambient 
Occlusion for Direct Volume Rendering 80 6.1 LocalAmbientOcclusion.................. 80 6.1.1 Emissive 
Tissues and Local Ambient Occlusion . 82 6.1.2 Integrating Multiresolution Volumes . . . . . . . 82 6.1.3 
Adding Global Light Propagation . . . . . . . . . 84 6.2 DynamicAmbientOcclusion................ 85 
  6.2.1 Local Histogram Generation . . . . . . . . . . . . 87 IV Volume Scattering 99 7 Scattering 
E.ects 100 7.1 PhysicalBackground .................... 100 7.2 Scattering .......................... 
101 7.3 SingleScattering ...................... 101 7.4 Indirect Illumination and Multiple Scattering 
. . . . . . 103 7.4.1 IndirectLight .................... 103 7.4.2 Transparency and Translucency . . 
. . . . . . . . 104 7.4.3 PhaseFunctions. . . . . . . . . . . . . . . . . . . 105 7.4.4 Scattering at 
Transparent Surfaces . . . . . . . . 106  7.5 APracticalPhaseFunctionModel . . . . . . . . . . . . 108 
7.6 FurtherReading . . . . . . . . . . . . . . . . . . . . . . . 109 8 Monte-Carlo Intergration 110 8.1 
NumericalIntegration ................... 110 8.1.1 Blind Monte-Carlo Integration . . . . . . . . . . 
. 110 8.2 When Does Monte-Carlo Integration Make Sense? . . . . 112 8.3 ImportanceSampling . . . . . 
. . . . . . . . . . . . . . . 114 8.4 GPU-based Importance Sampling . . . . . . . . . . . . . 116 8.4.1 
Focussing of Uniform Distribution . . . . . . . . . 116 8.4.2 Sampling of Re.ection MIP-Maps . . . . 
. . . . . 118  8.5 FurtherReading . . . . . . . . . . . . . . . . . . . . . . . 121 9 GPU-Based Monte-Carlo 
Volume Raycasting 123 9.1 Monte-Carlo Techniques for Isosurfaces . . . . . . . . . . 123 9.2 Isosurfaces 
with Shift-Variant or Anisotropic BRDFs . . 125 9.2.1 FirstHitPass .................... 125 9.2.2 DeferredShading 
Pass ............... 128 9.2.3 Deferred Ambient Occlusion Pass . . . . . . . . . 130 9.3 VolumeScattering 
..................... 133    9.3.1 Heuristic Simpli.cations . . . . . . . . . . . . . . 137 10 Light 
Map Approaches 140 Course Notes Advanced Illumination Techniques for GPU Volume Raycasting  GPU-Based 
Ray Casting Markus Hadwiger VRVis Research Center, Vienna, Austria Patric Ljung Siemens Corporate Research, 
Princeton, USA Christof Rezk Salama University of Siegen, Germany Timo Ropinski University of M¨unster, 
Germany  Introduction In traditional modeling, 3D objects are created using surface representa­tionssuch 
aspolygonal meshes,NURBSpatchesorsubdivision surfaces. Inthetraditional modelingparadigm,visualproperties 
of surfaces, such as color, roughness and re.ectance, are modeled by means of a shading algorithm, which 
might be as simple as the Phong model or as complex as a fully-featured shift-variant anisotropic BRDF. 
Since light transport is evaluated only at points on the surface, these methods usually lack the ability 
to account for light interaction which is taking place in the atmosphere or in the interior of an object. 
Contrary to surface rendering, volume rendering[60,23] describes a wide range of techniques for generating 
images from three-dimensional scalar data. These techniques are originally motivated by scienti.c visu­alization, 
where volume data is acquired by measurement or numerical simulation of natural phenomena. Typical examples 
are medical data of the interior of the human body obtained by computed tomography (CT) or magnetic resonanceimaging(MRI). 
Other examples are com­putational .uid dynamics (CFD), geological and seismic data, as well as abstract 
mathematical data such as 3D probability distributions of pseudo random numbers. With the evolution of 
e.cient volume rendering techniques, volumet­ric data is becoming more and more important also for visual 
arts and computer games. Volume data is ideal to describe fuzzy objects, such as .uids, gases and natural 
phenomena like clouds, fog, and .re. Many artistsand researchershavegenerated volumedatasynthetically 
tosup­plement surface models,i.e.,procedurally[24], whichis especially useful for rendering high-quality 
special e.ects. Although volumetric data are more di.cult to visualize than sur­faces,itisboth worthwhile 
and rewarding torenderthemastruly three­dimensional entities without falling back to 2D subsets. 1.1 
Volume Data A discrete volume data set can be thought of as a simple three­dimensional array of cubic 
elements (voxels1)[49], each representing a unit of space(Figure1.1). Althoughimagining voxels as tiny 
cubesis easy and mighthelp to vi­sualize theimmediate vicinity ofindividual voxels, itis more appropriate 
to identify each voxel with a sample obtained at a single in.nitesimally small point from a continuous 
three-dimensional signal f(x). IR with x . IR3 . (1.1) Provided that the continuous signal is band-limited 
with a cut-o.­frequency .s, sampling theory allows the exact reconstruction, if the signal is evenly 
sampled at more than twice the cut-o.-frequency, i.e., theNyquist rate. However, there are two majorproblems 
whichprohibit the ideal reconstruction of sampled volume data in practice. Ideal reconstruction according 
to sampling theory requires the con­volution of the samplepoints with a sinc function(Figure 1.2a) in 
the spatialdomain. For the one-dimensional case, the sincfunction is: sin(px) sinc(x)= . (1.2) px The 
three-dimensional version of this function is simply obtained by tensor-product. Note that this function 
has in.nite extent. Thus, for an exact reconstruction of the original signal at an arbi­traryposition 
all thesamplingpointsmustbeconsidered, not only 1volume elements those in a local neighborhood. This 
turns out to be computation­ally intractable in practice. Real-life data in general does not represent 
a band-limited signal. Any sharp boundary between di.erent materials represents a step function which 
has in.nite extent in the frequency domain. Sam­pling and reconstruction of a signal which is not band-limited 
will produce aliasing artifacts. In order to reconstruct a continuous signal from an array of voxels 
in practice, the ideal 3D sinc .lter is usually replaced by either a box .l­ter(Figure1.2a) oratent .lter(Figure1.2b). 
Thebox .ltercalculates nearest-neighbor interpolation, which results in sharp discontinuities be­tween 
neighboring cells and a rather blocky appearance. Trilinear in­terpolation, which is achieved by convolution 
with a 3D tent .lter, rep­resents a good trade-o. between computational cost and smoothness of the output 
signal. 1.2 Direct Volume Rendering In comparison to the indirect methods, which try to extract a surface 
descriptionfrom the volumedatain apreprocessing step,direct methods displaythe voxeldatabyevaluating 
an optical model whichdescribeshow the volume emits, re.ects, scatters, absorbsand occludeslight[73]. 
The scalar value is virtually mapped to physical quantities which describe light interaction at the respective 
point in 3D space. This mapping is often called classi.cation andis usuallyperformedby means of a transfer 
function. The physical quantities are then used for images synthesis. Di.erent optical models for direct 
volume rendering are described in section 1.2.1. During image synthesis, the light propagation is computed 
by inte­grating light interaction e.ects along viewing rays based on the optical model. The corresponding 
integral is known as the volume rendering integral, which is described in section 1.2.2. Naturally, under 
real-world conditions this integral is solved numerically. Furthermore, the volume can be shaded according 
to the illumination from external light sources. 1.2.1 Optical Models Almost every direct volume rendering 
algorithm regards the volume as adistribution oflight-emittingparticles of a certaindensity. Theseden­sities 
are more or less directly mapped to RGBA quadruplets for com­positing along viewing rays. Thisprocedure,however,is 
motivatedby a physically-based optical model. The most important optical models for direct volume rendering 
are described in a survey paper by Nelson Max [73], and we only brie.y summarize these models here: 
Absorption only. The volume is assumed to consist of cold, perfectly black particles that absorb all 
the light that impinges on them. They do not emit, or scatter light.  Emission only. The volume is assumed 
to consist of particles that only emit light, but do not absorb any, since the absorption is negligible. 
 Absorption plus emission. This optical modelisthemost com­mon one in direct volume rendering. Particles 
emit light, and oc­clude, i.e., absorb, incoming light. However, there is no scattering or indirect illumination. 
 Scattering and shading/shadowing. This modelincludes scat­tering ofilluminationthatisexternal toavoxel. 
Lightthatisscat­tered can either be assumed to impinge unimpeded from a distant light source, or it can 
be shadowed by particles between the light and the voxel under consideration.  Multiple scattering. 
This sophisticated model includes support for incident light that has already been scattered by multiple 
par­ticles before it is scattered toward the eye.  The volume renderingintegral describedin thefollowing 
section assumes the simple emission-absorption optical model. More sophisticatedmodels including shadowing 
and self-shadowing, and single and multiple scat­tering are covered in later parts of these notes. Figure 
1.3 illustrates GPU-based ray casting with the emission­absorption model with and without shading, as 
well as a combination with semi-transparent isosurface rendering. Figure 1.4 illustrates the addition 
of shadows, i.e., the (partial) occlusion of impinging external light via the absorption occuring within 
the volume. 1.2.2 The Volume Rendering Integral Everyphysically-based volume rendering algorithm evaluates 
the volume rendering integral in one way or the other, even if viewing rays are not employed explicitly 
by the algorithm. The most basic, but also most .exbile, volume rendering algorithm is ray casting, which 
is introduced in Section 1.2.3. It might be considered as the most direct numerical method for evaluating 
this integral. More details are covered later on, but for this section it su.ces to view ray casting 
as a process that, for eachpixel in theimage to render, casts a single rayfrom the eye through the pixel 
s center into the volume, and integrates the optical properties obtained from the encountered volume 
densities along the ray. Note that this general description assumes both the volume and the mapping 
to optical properties to be continuous. In practice, of course, the volume data are discrete, and the 
evaluation of the integral is ap­proximated numerically. In combination with several additional simpli­.cations, 
the integral is usually substituted by a Riemann sum. We denote a ray cast into the volume by x(t), andparameterizeitby 
thedistance t from the eye. The scalar value corresponding to aposition () alongthe rayisdenotedby sx(t).Ifweemploytheemission-absorption 
model, the volume rendering equation integrates absorption coe.cients .(s) (accounting for the absorption 
of light), and emissive colors c(s) (accounting for radiant energy actively emitted) along a ray. To 
keep the equations simple, we denote emission c and absorption coe.cients . as function of the eye distance 
t instead of the scalar value s: (()) (()) c(t):= csx(t)and .(t):= .sx(t)(1.3) Figure 1.5 illustrates 
the idea of emission and absorption. An amount of radiant energy, which is emitted at a distance t = 
d along the viewing rayis continuously absorbed along thedistance d untilit reaches the eye. ' This means 
that only aportion cof the original radiant energy c emitted  Figure 1.5: An amount of radiant energy 
emitted at t = d is partially absorbed along the distance d. at t = d will eventually reach the eye. 
If there is a constant absorption . = const along the ray, c ' amounts to '-.d c = c · e. (1.4) However, 
if the absorption . is not constant along the ray, but itself dependent ontheposition,the amount of radiant 
energy c ' reaching the eye mustbe computedbyintegrating the absorption coe.cient along the distance 
d: c ' = c · e - d 0 .( t)d t . (1.5) The integral over the absorption coe.cients in the exponent, d2 
t(d1, d2)= .( t)d t (1.6) d1 is also calledthe opticaldepth. In this simple example,however,lightwas 
only emitted at a singlepoint along the ray. If we want to determine the total amount of radiant energy 
C reaching the eye from this direction, we must take into account the emitted radiant energy from all 
possible positions t along the ray: 8 -t(0,t) dt C =c(t)· e (1.7) 0 Inpractice, thisintegralis evaluated 
numerically through eitherfront-to­back orback-to-front compositing(i.e., alphablending) of samplesalong 
the ray, which is most easily illustrated in the method of ray casting. Ray casting usually employs front-to-back 
compositing. 1.2.3 Ray Casting Ray casting [60] is an image-order direct volume rendering algorithm, 
which uses straight-forward numerical evaluation of the volume render­ingintegral(Equation1.7). For eachpixel 
oftheimage, a single ray2 is cast into the scene. At equi-spaced intervals along the ray, the dis­crete 
volumedata are resampled, usually using tri-linearinterpolation as reconstruction .lter. Thatis,foreachresamplinglocation,thescalarval­ues 
of eight neighboring voxels are weighted according to their distance to the actuallocationfor which adata 
valueis needed. After resampling, the scalar data value is mapped to optical properties via a lookup 
ta­ble, whichyields anRGBAquadrupletthat subsumesthecorresponding emission and absorption coe.cients[60] 
forthislocation. The solution of the volume renderingintegralis then approximated via alphablending in 
either front-to-back or back-to-front order, where usually the former is used in ray casting. The optical 
depth t (Equation 1.6), which is the cumulative absorp­tion up to a certainposition x(t)along the ray, 
can be approximated by a Riemann sum lt/.tJ L t(0,t) t (0,t)= .(i · .t).t (1.8) i=0 with .t denoting 
the distance between successive resampling locations. The summation in the exponent can immediately be 
substituted by a multiplication of exponentiation terms: lt/.tJ I -t (0,t) -.(i·.t).t e = e (1.9) i=0 
Now, we can introduce the opacity A, which is well-known from tradi­tional alpha blending, by de.ning 
-.(i·.t).t Ai =1- e (1.10) and rewriting Equation 1.9 as: lt/dJ I -t (0,t) e = (1- Aj) (1.11) i=0 This 
allows the opacity Ai to be used as an approximation for the ab­sorption of the i-th ray segment,instead 
of absorption at a singlepoint. Similarly, the emitted color of the i-th ray segment can be approxi­mated 
by: Ci = c(i · .t).t (1.12) 2assuming super-sampling is not used for anti-aliasing Having approximated 
both the emissions and absorptions along a ray, we can now state the approximate evaluation of the volume 
rendering integral as: (denoting the number of samples by n = lT/dtJ) ni-1 LI C = Ci (1- Ai) (1.13) 
i=0 j=0 Equation1.13 canbe evaluatediteratively byperforming alpha blending in either front-to-back or 
back-to-front order. 1.2.4 Alpha Blending Equation1.13canbe computediterativelyinfront-to-back orderby 
step­ping i from 1 to n: Ci ' = Ci'-1 +(1 - Ai'-1)Ci (1.14) A ' = A ' +(1 - A ' )Ai (1.15) ii-1 i-1 New 
values Ci ' and A ' i are calculated from the color Ci and opacity Ai at the current location i, and 
the composited color Ci'-1 and opacity A ' i-1 from the previous location i - 1. The starting condition 
is C0 ' = 0 and A ' 0 =0. Note that in all blending equations, we are using opacity-weighted colors [110], 
which are also known as associated colors [7]. An opacity­weighted color is a color that has been pre-multiplied 
by its associated opacity. This is a very convenient notation, and especially important for interpolation 
purposes. It can be shown that interpolating color and opacity separately leads to artifacts, whereas 
interpolating opacity­weighted colors achieves correct results[110]. The following alternative iterative 
formulation evaluates Equa­tion 1.13 in back-to-front order by stepping i from n - 1 to 0: Ci ' = Ci 
+(1- Ai)Ci' +1 (1.16) A new value Ci ' is calculated from the color Ci and opacity Ai at the cur­rent 
location i, and the composite color Ci' +1 from the previous location i +1. The starting condition is 
Cn ' =0. Note that front-to-back compositing requires tracking alpha values, whereas back-to-front compositing 
does not. However, while this was a problem for hardware implementations several years ago, in current 
single-passimplementations ofGPU ray casting thisisnot aproblem at all. In multi-pass implementations, 
destination alpha mustbe supported by the frame bu.er for tracking the accumulation of opacity, i.e., 
an alpha value must be stored in the frame bu.er, and it must be possible to use it as a multiplication 
factor in blending operations. The major advantage offront-to-back compositing is an optimization called 
early ray termination, where the progression along a ray is termi­nated as soon as the cumulative alpha 
value reaches 1.0, or a su.ciently close value. In currentGPU architectures, thisis very easy toimplement 
by simply terminating the ray casting loop as soon as the accumulated alpha value exceeds a speci.ed 
threshold.  GPU-based Volume Ray Casting The basic idea of GPU-based ray casting is to store the entire 
vol­ume in a single 3D texture, and drive a fragment program that casts rays into the volume. Each pixel/fragment 
corresponds to a single ray x(t, x, y)= c+t d(x, y)in volume coordinates. Here, the normalized di­rection 
vector d(x, y)can either be computed from the camera position c and the screen space coordinates (x, 
y) of the pixel, or be obtained via rasterization [55]. In this section, we will use the approach build­ing 
on rasterization since it allows for very simple but e.cient empty space skipping, which is described 
in later sections. The range of depths [tstart(x, y),texit(x, y)]from where a rayenters the volume to 
where a ray exits the volumeis computedperframein a setup stagebefore the actual ray casting fragment 
program is executed. In the simplest case, tstart, or the corresponding 3D volume coordinates, are obtained 
by rasteriz­ing the front faces of the volume bounding box with the corresponding distance to the camera. 
Rendering the back faces of the bounding box yields the depths texit, or the corresponding 3D volume 
coordinates, of each ray exiting the volume.  Figure2.1illustrates this raysetup using rasterization. 
Asillustrated in Figure 2.2, ray entry positions are determined by the front faces of the volumeboundingbox(showninblue), 
and ray exitpositionsbyits backfaces(showningreen), respectively. Ray castingisperformedby sampling the 
space in-between, usually by using a constant sampling rate. On current GPUs, a single rendering pass 
and ray casting loop in the fragment program can be employed for casting through the volume in front-to-back 
order, building on the images illustrated in Figure 2.1 for ray setup, which yield exactly the setup 
positions needed by the ray caster(f0 -f4, and l0 -l4 in Figure 2.2). 2.1 Basic Ray Casting Figure2.3illustratesbasic 
raycasting with ray setup using rasterization. It consists of four principal stages: 1. Frontfacegeneration:Renderthefrontfacesof 
thevolumebound­ing boxto atexture(Figure2.1(left)). 2. Directiontexturegeneration:Renderthebackfaces 
of the volume bounding box (Figure 2.1 (center)), while subtracting the previ­ously generated coordinates 
of the front faces and storing the re­sulting ray vectors as normalized vectors in RGB, as well as their 
lengths in A, of a separate RGBA direction texture (Figure 2.1 (right)).  3. Ray casting: Get the starting 
position from the front face image and cast along the viewing vector until the rayhasleft the volume. 
Exiting the volume is determined by using the previously stored vector lengths. 4. Blending:Blendtheraycasting 
resulttothescreen,e.g.,composite it with the background.  Theonly expensive stageof thisalgorithmistheactual 
ray castingloop, which iteratively steps through the volume, sampling the 3D volume texture using tri-linear 
interpolation, applies the transfer function, and performs compositing. Ray setup via rasterization is 
several orders of magnitude faster with negligible performance impact, and thus no bot­tleneck. The .nalblending 
stageisnegligibleintermsofperformanceas well, or can even be skipped entirely if the ray casting pass 
is executed directly on the .nal output bu.er.  2.2 Object-Order Empty Space Skipping When we consider 
Figure 2.2, and imagine that the actually visible part of the volume does not .ll up the entire bounding 
box, we see that a lot of empty space will be sampled during ray casting if rays are started on the front 
faces of the volume bounding box. However, if we subdivide the volume into smaller blocks and determine 
for each of these blocks whether it is empty or not, we can rasterize the front faces of these smaller 
blocks instead of the entire bounding box. This can simply be achieved by rasterizing front and back 
faces of smaller blocks, resulting in ray setup images as shown in Figure 2.4, which already more closely 
resemblethevisiblepart ofthevolume(inthecaseofthis .gure,ahuman skull and spine). This is illustrated 
in Figure 2.5, where both the ray entry positions(f0 -f2)as well asthe ray exitpositions(l0 -l4)have 
been modi.ed via this rasterization of block bounding faces to be inside the volume bounding box and 
closer to the visible part of the volume. Figure 2.6 illustrates a potential performance problem of this 
ap­proach, whichoccurs when raysgrazethe volume early on,butdo nothit avisiblepart right away(right-hand 
sideof the .gure). Inthiscase,alot of empty space maybe traversed. However, this case usually occurs 
only for a smallnumber of rays, and canbehandledby combining object-order empty space skipping with regular(image-order) 
empty space skipping, i.e.,decidingin the raycastingfragmentprogram to skipindividual sam­ples or advancing 
the samplingposition along the ray by several samples at once. These two approaches for empty space skipping 
complement each other. Object-order empty space skippingisextremelyfast(with negligible overhead compared 
with no empty space skipping), it employs very simple andfast rasterization, and the ray castingfragmentprogram 
does not need to be modi.ed at all. It, however, in principle cannot skip all empty space. Image-order 
empty space skipping, on the other hand, either requires multiple ray castingpasses or mustperformchecks 
on essentially a per-sample basis, and thus is much more expensive. It, however, can skip additional 
empty space that would otherwise be sam­pled. Figure 2.7illustrates another example of object-order empty 
space skipping via rasterization of tight-.tting bounding geometry.   Figure 2.5 illustrates another 
important issue of ray setup, which is the handling of rays when the view point is inside the volume. 
Rays r3 and r4 in this .gure cannot be started on front faces of bounding geometry, because they have 
to start inside it, i.e., on the near plane of the viewfrustum(positions n3 and n4). The next section 
describes an advanced ray castingpipelinethat correctly handlesthis case, as well as the intersection 
of the volume with opaque geometry, e.g., navigational markers or tools in medical interventions. 2.3 
Advanced Ray Casting Pipeline This section describes an advanced ray casting pipeline that combines object-order 
and image-order stages in order to .nd a balance between thetwo, andleveragetheparallelprocessing of 
modernGPUs[90]. For culling of irrelevant subvolumes, a regular grid of min-max values for bricks of 
size 83 is stored along with the volume. Ray casting itself is performedin a single renderingpassin order 
to avoidthe setup overhead of casting eachbrick separately[44]. The .rst step of thealgorithmculls bricks 
on the CPU and generates a bit array that determines whether a brick is active or inactive. This bit 
array contains the state of bricks with respect to the active parts of the volume, where a brick is active 
whenit contains samplesthat aremapped toopacitiesgreaterthanzero by the transfer function and inactive 
otherwise. Intheobject-order stageontheGPU,thebit arrayisused toraster­ize brick boundary faces in several 
rendering passes. The result of these rendering passes are two images that drive the subsequent ray casting 
stage. The .rst image, the ray start position image, contains the volume coordinatepositions where ray 
casting should startfor eachpixel. Coor­dinates are storedintheRGBcomponents, andthe alpha(A) component 
is one when a ray should be started, and zero when no ray should be started. The second image, the ray 
length image contains the direction vectors for ray casting in the RGB components and the length of each 
rayinthealphacomponent. Notethatthedirectionvectors could easily be computed in the fragmentprogramfrom 
the cameraposition and the ray start positions as well. However, the ray length must be rendered into 
an image that is separate from the ray start positions due to read­writedependencies, which canthenalsobeusedforstoring 
thedirection vectors that are needed for ray length computation anyway. The main steps of our ray casting 
approach for each pixel are: 1. Compute the initial ray start position on the near clipping plane of 
the current viewport. When the start position is in an inactive brick, calculate the nearest intersection 
point with the boundary faces of active bricks, in order to skip empty space. The result is stored in 
the ray start position image. 2. Compute the ray length until the last intersection point with boundary 
faces of bricks that are active. The result is stored in the ray length image. 3. Optionally render 
opaque polygonal geometry and overwrite the raylength image where thedistancebetween the ray startposition 
and the geometry position is less than the stored ray length. 4. Cast from the start position stored 
in the ray start position image along the direction vector until the accumulated opacity reaches a speci.ed 
threshold(early ray termination)or the ray length given by the ray length image is exceeded. The result 
of ray casting is stored in a separate compositing bu.er. 5. Blend the ray casting compositing bu.er 
on top of the polygonal geometry.  The two main acceleration schemes exploited here are object-order 
empty space skipping and early ray termination. For the former, view­independent culling of bricks and 
rasterization of their boundary faces are employed, whereas the latter is handled during ray casting. 
2.3.1 Culling and Brick Boundary Rasterization Each brick in the subdivision of the volume is either 
inactive or active with respect to the transfer function. In order to determine ray start positionsand 
raylengths, weemploy rasterizationof theboundaryfaces between active and inactive bricks, which is illustrated 
in Figure 2.5. To handle brick culling e.ciently, the minimum and maximum voxel values of each brick 
are stored along with the volume, which are compared at run-time with the transfer function. A brick 
can be safely discarded when the opacity is always zero between those two values, which can be determined 
veryquickly using summed areatables[30]. Rasterizing the boundary faces between active and inactive bricks 
results in object-order empty space skipping. It prunes the rays used in the ray casting pass and implicitly 
excludes most inactive bricks. Note, however, that this approach does not exclude all empty space from 
ray casting, which can be seen for ray r3 in Figure 2.5(left). This is a trade­o. that enables ray casting 
without any per-brick setup overhead and works extremely well in practice. The border between active 
and inactive bricks de.nes a surface that can be rendered as standard OpenGL geometry with the corresponding 
positionin volume coordinates encodedintheRGBcolors. Allvertices of brick bounding geometry are constantly 
kept in video memory. Only an additional index array referencing the vertices of active boundary faces 
have to be updated every time the transfer function changes. As long as the near clipping plane does 
not intersect the bounding geometry, rays can always be started at the brick boundary front faces. However, 
if such anintersection occurs, it willproduceholesin thefront-facinggeometry, which resultsin some rays 
notbeing started at all, and others started atincorrectpositions. Figure2.9illustratesthisproblem. In 
an endoscopic view, we constantlyface this situation, so rays typically needtobe started atthe near clippingplane, 
whichis showninFigure2.5 in the case of points n2 -n4. To avoid casting through empty space, rays should 
not be started at the nearclippingplaneif thestartingpositionisinaninactivebrickbut at the next intersection 
with active boundary faces, such as rays r0 and r1 in Figure 2.5. These rays are started at f0 and f1, 
instead of being starting at n0 and n1. We achieve this by drawing the near clipping plane .rst and the 
front faces afterwards, which ensures that whenever there are no front faces to start from, the position 
of the near clipping plane will be taken. However, since the non-convex bounding geometry often leads 
to multiple front faces for a single pixel, the next front face is used when the .rst front face is clipped, 
which results in incorrect ray start positions. The solution is to detect when a ray intersects a back 
face before the .rst front face that is not clipped. The basic steps to obtain the ray start position 
image are as follows: 1. Disable depth bu.ering. Rasterize the entire near clipping plane into the color 
bu.er. Set the alpha channel to zero everywhere. 2. Enable depth bu.ering. Disable writing to the RGB 
components of the color bu.er. Rasterize the nearest back faces of all active bricks into the depth bu.er, 
e.g., by using a depth test of GL LESS. Setthe alpha channelto one wherefragments aregenerated. 3. Enable 
writing to the RGB components of the color bu.er. Ras­terize the nearest front faces of all activebricks, 
e.g.,by onceagain using a depth test of GL LESS. Set the alpha channel to one where fragments are generated. 
 This ensures that all possible combinations shown in Figure 2.5 (left) are handled correctly. Rasterizing 
the nearest front faces makes sure that all near plane positions in inactive bricks will be overwritten 
by startpositions on activebricksthat arefarther away(rays r0 and r1). Rasterizing the nearest back faces 
before the front faces ensures that nearplanepositionsinside activeblocks will notbeoverwrittenbyfront 
facesthat arefarther away(rays r2 and r3). Brick geometry that is nearer than the near clipping plane 
is auto­matically clipped by the graphics subsystem. After that, the ray length image can be computed, 
which .rst of all means .nding the last in­tersection points of rays with the bounding geometry. The 
basic steps are:  1. Rasterize the farthest back faces, e.g., by using a depth test of GL GREATER. 
 2. During this rasterization, sample the ray start position image and subtractitfromthebackpositions 
obtained via rasterization ofthe back faces. This yields the ray vectors and the ray lengths from start 
to end position. 3. Multiply all ray lengths with the alpha channel of the ray start positionimage(whichis 
either1 or0).  These steps can allbeperformedinthe samefragmentprogram. Drawing the back faces of the 
bounding geometry results in the last intersection points of rays and active brick geometry, which are 
denoted as li in Fig­ure 2.5. Subtracting end positions from start positions yields the ray vectors, 
which can then be normalized and stored in the RGB compo­nents of the ray length image together with 
the ray lengths in the alpha channel. Note that the alpha channel of the ray length image has con­sistently 
be set to zero where a ray should not be started at all, which is exploited in the ray casting pass. 
 2.3.2 Geometry Intersection Many applications, e.g., virtual endoscopy, require both volumetric and 
polygonal data to be present in the same scene. Naturally, intersections of the volume and geometry have 
to achieve a correct visibility order, and in many cases looking at the intersections of the geometry 
and the isosurface is the reason for rendering geometry in the .rst place. Also, partsthatdonotcontributetothe 
.nalimagebecausethey areoccluded by geometry should not perform ray casting at all. An easy way to achieve 
this is to terminate rays once they hit a polygonal object by modifying the raylength image accordingly. 
This isillustrated in Figure 2.11. Ofcourse, raylengths should onlybe modi.edif apolygonal object is 
closer to the view point than the initial ray length. This problem can  Figure 2.12: Modifying ray endpositionsprevents 
rendering occludedparts of the volume (left). Blending the result of ray casting on top of the opaque 
geometry thenyieldsthe correct result(right). again be solved by using the depth test. After rendering 
the back faces of active/inactive brick boundaries withtheirrespectivedepth values(anddepthtest setto 
GL GREATER), the intersecting geometry is rendered to the same bu.er, with the cor­responding volume 
coordinates encoded in the color channel. With the depth test reversed to GL LESS, only those parts will 
be drawn that are closer to the viewpoint than theinitial raylengths. This approach mod­i.es ray casting 
such that it results in an image that looks as if it was intersected with an invisible object. Blending 
this image on top of the actual geometry in the last pass of the algorithm results in a rendering with 
correct intersections and visibility order.  2.4 Isosurface Ray Casting This section describes a special 
case of volume ray casting for rendering isosurfaces, which is also known as .rst-hit ray casting. In 
order to fa­cilitateobject-order empty space skipping withoutper-sample overhead, we maintain min-max 
values of a regular subdivision of the volume into smallblocks, e.g.,with43 or83 voxelsperblock. These 
blocksdo not ac­tually re-arrange the volume. For each block, a min-max value is simply stored in an 
additional structure for culling. If the whole volume does not .t in GPU memory, however, a second level 
of coarser bricks can be maintained, whichisdescribedinlater sections on memory management. Whenever 
the isovalue changes, blocks are culled against it using their min-maxinformationand a rangequery[12], 
whichdeterminestheir ac­tive status. The view-independent geometry of active block bounding faces that 
are adjacent to inactive blocks is kept in GPU memory for fast rendering.  In order to obtain ray start 
depths tstart(x, y), the front faces of the blockboundinggeometry are rendered with their correspondingdistance 
to the camera. Thefront-mostpoints of rayintersections are retainedby enabling a correspondingdepthtest(e.g., 
GL LESS). For obtaining ray exit depths texit(x, y)we rasterize the back faces with an inverted depth 
test that keeps only the farthest points (e.g., GL GREATER). Figure 2.6 shows thatthis approachdoes not 
excludeinactiveblocksfromthe search range if they are enclosed by active blocks with respect to the current 
viewing direction. The corresponding samples are skipped on a per­sample basis early in the ray casting 
loop. However, most rays hit the isosurface soon after being started and are terminated quickly (yellow 
points in Figure 2.6, left). Only a small number of rays on the outer side of the isosurface silhouette 
are traced for a larger distance until they hit the exit position of the block bounding geometry (red 
points in Figure 2.6, left). The right side of Figure 2.6 illustrates the worst case scenario, where 
rays are started close to the view point, miss the corresponding part of the isosurface, and sample inactive 
blocks with image-order empty space skipping until they enter another part of the isosurface bounding 
geometry and are terminated or exit without any intersection. In order to minimize the performance impact 
when the distancefrom raystart to exit or terminationislarge, we use an adaptive strategy for adjusting 
the distance between successive samples along a ray. 2.4.1 Adaptive Sampling Inorderto .nd thepositionofintersectionforeach 
ray,thescalarfunc­tionis reconstructed atdiscrete samplingpositions xi(x, y)= c+tid(x, y) for increasing 
values of ti in [tstart,texit]. The intersection is detected when the .rst sample lies behind the isosurface, 
e.g., when the sample value is smaller than the isovalue. Note that in general the exact inter­section 
occurs somewhere between two successive samples. Due to this discrete sampling, it is possible that an 
intersection is missed entirely when the segment between two successive samples crosses the isosurface 
twice. This is mainly a problem for rays near the silhouette. Guaran­teedintersections evenforthin sheets 
arepossibleif thegradientlength isbounded by some value L [48]. Notethatfordistance .elds, L is equal 
to 1. For some sample value f, it is known that the intersection at iso­value . cannot occur for any 
point closer than h = |f - .|/L. Yet, h canbecome arbitrarily small neartheisosurface, which wouldlead 
toan in.nite number of samples for guaranteed intersections. We use adaptive sampling to improve intersection 
detection. The actual intersection position of an intersection that has been detected is then further 
re.ned using the approach described in Section 2.4.2. We have found that completely adaptive sampling 
rates are not well suited forimplementations ongraphicshardware. Thesearchitectures use mul­tiplepipelines 
where small tiles of neighboringpixels are scan-converted inparallel using the same texture cache. With 
completely adaptive sam­pling rate, the sampling positions of neighboring pixels diverge during parallel 
execution, leading to under-utilization of the cache. Therefore, we use only two di.erent discrete sampling 
rates. The base sampling rate r0 is speci.ed directly by the user where 1.0 corresponds to a single voxel. 
It is the main tradeo. between speed and minimal sheet thick­ness with guaranteed intersections. In order 
to improve the quality of silhouettes(seeFigure2.14), we use a second maximum sampling rate r1 as a constant 
multiple of r0: r1 = nr0. We are currently using n = 8 in our system. However, we are not detecting silhouettes 
explicitly at this stage,becauseit wouldbetoocostly. Instead, weautomaticallyincrease the sampling ratefrom 
r0 to r1 when the current sample s value is closer to the isovalue . by a small threshold d. In our current 
implementation, d is set by the user as a quality parameter, which is especially easy for distance .elds 
where the gradient magnitude is 1.0 everywhere. In this case, a constant d can be used for all data sets, 
whereas for CT scans it has to be set according to the data.   2.4.2 Intersection Re.nement Once a 
ray segment containing an intersection has been detected, the next stagedetermines an accurateintersectionposition 
using aniterative bisection procedure. In one iteration, we .rst compute an approximate intersection 
position assuming a linear .eld within the segment. Given the sample values f at positions x for the 
near and far ends of the segment, the new sample position is . - fnear xnew =(xfar - xnear)+ xnear (2.1) 
ffar - fnear Then the value fnew isfetched at thispoint and compared to theisovalue .. Depending on the 
result, we update the ray segment with either the front or the back sub-segment. If the new point lies 
in front of the isosurface(e.g. fnew >.), we setxnear to xnew, otherwise we set xfar to xnew and repeat. 
Oftena .xed numberofiterationsteps,e.g.,foursteps, is enough for obtaining high-quality intersection 
positions.  2.5 Memory Management Volume sizes are increasing rapidly, and can easily exceed the available 
amount of GPU on-board memory. However, large parts of many types of volumes are often mapped to optical 
properties such that they are completely transparent, e.g., the air around a medical or industrial CT 
scan. In order todecouple the amount of memory thatis actually needed to render a given volume, i.e., 
the working set required for rendering it, from the overall volume size, a variety of memory management 
schemes such as bricking and, additionally, multi-resolution schemes, can be em­ployed. We .rst consider 
the conceptually simple case of rendering iso­surfaces, which, however, almost directly extends to the 
case of direct volume rendering with arbitrary transfer functions. Foranypossibleisovalue, many of theblocksdonot 
containanypart of the isosurface. In addition to improving rendering performance by skipping emptyblocks, 
thisfact can alsobe usedfor reducingthe e.ective memoryfootprint of relevantparts ofthe volume signi.cantly. 
Whenever theisovalue changes,the corresponding rangequery alsodeterminesthe active status ofbricks of 
coarser resolution, e.g.,323 voxels. Thesebricks re-arrange the volume and include neighbor samples to 
allow .ltering without complicatedlook-ups attheboundaries,i.e., abrick of resolution n3 is storedwith 
size(n+1)3 [54]. This overheadisinverselyproportional to the brick size, which is the reason for using 
two levels of subdivision. Smallblocks .ttheisosurfacetightlyforemptyspaceskipping andlarger bricks avoid 
excessive storage overhead for memory management. In order to decouple the volume size from restrictions 
imposed by GPUs on volume resolution(e.g., 5123 onNVIDIAGeForce6) and avail­able video memory(e.g.,512MB), 
wecanperformray castingdirectly  Figure 2.17: Hierarchicalbricking(top row) vs. .atbricking(bottomrow). 
Culled bricks are marked in white. on a re-arranged brick structure. Similar to the idea of adaptive 
texture maps[54],wemaintainanadditionallow-resolution .oatingpointrefer­ence texture(e.g., 163 for a 
5123 volume with 323 bricks) storing texture coordinate o.sets of bricks in a single brick cache texture 
that is always residentinGPU memory(e.g., a512x512x256texture). However,both the reference and the brick 
cache texture are maintained dynamically and notgeneratedin apre-process[54]. Figure2.16illustrates the 
use of the reference and brick cache textures. Note that since no gradient re­construction or shading 
isperformedduring ray casting, no complicated neighborlook-ups are required at this stage. When theisovalue 
changes, bricks that potentially contain a part of the isosurface are downloaded into the brick cache 
texture. Inactive bricks are removed with a simple LRU(least recently used) strategy whentheir storage 
spaceis required for active bricks. Bricks that are currently not resident in the cache tex­ture are 
specially marked at the corresponding position in the reference texture (shown as white squares in Figure 
2.16). During ray casting, samples in such bricks are simply skipped. 2.6 Mixed-Resolution Volume Rendering 
Most multi-resolution volumerendering methodsarebased onhierarchi­cal bricking schemes where the brick 
size in voxels is kept constant from level to level, and the spatial extent of bricks increases from 
high to low resolution until a singlebrick coversthe entire volume(Figure2.17,top row). Conversely, .atbricking 
schemes(Figure2.17,bottomrow) keep the spatial extent of bricks constant and successively decrease the 
brick size in voxels. A major advantage of .at bricking schemes is that the culling rate is much higher, 
illustrated by the number of white bricks in Figure2.17,because thegranularityof culling stays constantirrespective 
of actual brick resolutions. This not only reduces the required texture memory, as more bricks can be 
culled, but also allows for a much more .ne-grainedLOD orfragmentprogram selectionperbrick[61]. However, 
.at multi-resolution techniques have a bigger memory overhead when samples are replicated at brick boundaries, 
because for decreasing brick sizes the overhead of duplicated voxels increases. This overhead can be 
removedby avoiding sampleduplication[65],trading o.runtime .lter­ing cost for memory savings. We employ 
.at multi-resolution bricking with sample duplication, but reduce the run-time overhead signi.cantly 
by using hardware .ltering and only warping the texture coordinates of samples where necessary[5]. 2.6.1 
Volume Subdivision for Texture Packing The original volume is subdivided into equally-sized bricks of 
size n3 in a pre-process, where n is a power of two, e.g., n = 32. During this sub­division, the minimum 
and maximum value in each brick are stored for culling later at run time, and lower-resolution versions 
of each brick are constructed. For the latter we compute the value of the new sample at the center of 
eight surrounding higher-resolution samples as their aver­age, but higher-order .lters could also be 
used. We limit the number of resolution levels to minimize the overhead of duplicated boundary vox­els, 
and also to allow tightpacking oflow-resolutionbricks in the storage space reservedforhigh-resolutionbricks(Section2.6.2). 
Bydefault we use only two resolution levels, e.g., 323 bricks with a downsampled reso­lution of 163 . 
For fast texture .ltering during rendering, voxels at brick boundaries are duplicated. In principle, 
duplication at one side su.ces for thispurpose[104], e.g., storing(32+1)3 bricks. However, in thehigh­resolutionlevel 
weduplicate atboth sides,becausethespaceforasingle (32+2)3 brickprovides storagefor eight(16 +1)3 bricks. 
Coinciden­tally, this often even does not impose additional memory overhead. The brick cache texture(Section2.6.2) 
alwayshaspower-of-twodimensions forperformance reasons, and a cache of size5123, for example, can hold 
the same number of 343 and 333 bricks. Although this approach is not fully scalable, it is very simple 
and a good trade-o. that is not as restrictive as it might seem. Because culling is very e.cient in a 
.at scheme, fewer bricks need to be resident in GPU memory. Even without culling, if the size of the 
brick cache textureis512x512x1024(256 megavoxels),forexample, and tworesolu­tionlevels are used(brick 
storage size343), 15x15x30 bricks .t into the cache. This yields a possible data set size of about 1.7 
giga voxels, e.g., 960x960x1920, if all bricks actually need to .t into the cache. Due to culling, the 
real data set size can typically be much larger. Additionally, for very large data three levels could 
be used. For example, increasing the allocated spacefor eachbrickfrom(32+2)3 to(32+4)3, both 163 and 
83 bricks can be packed tightly, including boundary duplication for .ltering. Using threelevels with 
storagefor(32+4)3 bricks, 14x14x28 brickswould .tintothecache,yielding adataset sizeof10.7gigavoxels, 
e.g., 1792x1792x3584, and more when bricks are culled. 2.6.2 Mixed-Resolution Texture Packing Forrendering, 
alist of activebricksisdetermined viaculling,using,e.g., the transfer function or iso value, and clipping 
plane positions to deter­mine non-transparent bricks that need to be resident in GPU memory. The goal 
is to pack all active bricks into a single 3D brick cache tex­ture(Figure2.18, right). Inthebeginning, 
all cache spaceis allocated for high-resolution bricks. If the number of active bricks after culling 
exceeds the allocated number, individual bricks are chosen to be repre­sented at lower resolution. In 
this case, the e.ective number of bricks in the cache is increased by successively mapping high-resolution 
bricks in the cache to eight low-resolution bricks each, until the required over­all number of bricks 
is available. This is possible because the storage allocation for bricks has been chosen in such a way 
that exactly eight low-resolution bricks .t into the storage space of a single high-resolution brick, 
including duplication of boundary voxels, as described in the pre­vious section. After thelistof activebricks 
along with the corresponding resolutions hasbeen computed, thelayout of the cache texture and mapping 
ofbrick storage space in the cache to actual volume bricks can be updated ac­cordingly, which results 
in an essentially arbitrary mixture of resolution levels in the cache. The actual brick data are then 
downloaded into their corresponding locations using, e.g., glTexSubImage3D(). During rendering, a small 
3D layout texture is used for address translation be­tween virtual volume space and physical cache texture 
coordinates (Figure 2.18, topleft), which is described in the next section. 2.6.3 Address Translation 
A major advantage of thetexturepacking schemedescribedhereisthat address translation can be done in an 
identical manner irrespective of whether di.erent resolution levels are mixed. Each brick in virtual 
vol­ume space always has constant spatial extent and maps to exactly one brick in physical cache space. 
Virtual addresses in volume space, in [0, 1], corresponding to the volume s bounding box, are translated 
to physical texturecoordinatesinthebrick cachetexture, alsoin[0, 1], corresponding to the full cache 
texture size, via a lookup in a small 3D layout texture with one texel per brick in the volume. This 
layout tex­ture encodes(x, y, z)address translation information in the RGB color channels, and a multi-resolution 
scale value in the A channel, respec­tively. A volume space coordinate xx,y,z . [0, 1]3 is translated 
to cache texture coordinates x ' . [0, 1]3 in the fragment program as: x,y,z ' x = xx,y,z · bscalex,y,z 
· tw (2.2) x,y,z + tx,y,z, where tx,y,z,w is the RGBA-tuple from the layout texture corresponding to 
volume coordinate xx,y,z, and bscale is a constant fragment program parameter containing a global scale 
factor for matching the di.erent coordinate spaces of the volume and the cache. When .lling the layout 
layout texture virtual volume cache texture Figure 2.18: Mixed-resolution texturepacking and address 
translationfrom virtual volume space to physical cache texture space via the layout texture. Resolution 
levels are mixed by packing low-res bricks tightly into high-res bricks. texture, the former is computed 
as: () tx,y,z = bx,y,z ' · bres x,y,z ' - ox,y,z + tw/csizex,y,z (2.3) tw =1.0, (2.4) for a high-resolution 
brick, where b ' is the position of the brick in the cache(0,1, ...), bres ' isthestorageresolutionof 
thebrick, e.g.,343, and csize is the cache texture size in texels to produce texture coordinates in the 
[0, 1] range. For a low-resolution brick, this is computed with tw =0.5. The o.set ox,y,z is computed 
as: ox,y,z = bx,y,z · bresx,y,z · tw, (2.5) where b isthepositionofthebrickinthevolume(0,1, ...),and 
bres is the brick resolution in the volume, e.g., 323 . The global scale factor bscale is computed as: 
bscalex,y,z = vsizex,y,z/csizex,y,z, (2.6) where vsize is the size of the volume in voxels.  2.7 Multiresolution 
LOD and Adaptive sampling 2.7.1 Octree-based Multiresolution Representation Thelinear storage scheme 
described abovehas apoordatalocalityprop­erty. The lookup of neighboring voxels is frequent and it is 
only along the x-axis that this translates to access of neighboring memorylocations. Theimpact of cache-misses 
in rendering andprocessingis signi.cant and often causes a scheme to ultimately fail if not well addressed. 
Blocking of the volume is therefore generally e.cient and signi.cantly improves the cache hit-rate. The 
size of a block is typically derived from the size of the level 1 and 2 caches. Grimm et al. [32, 31] 
.nds that a block size of 32, B = (32, 32, 32), is the most e.cient for their block-based raycaster. 
Parker et al.[78] useasmallerblock size, B =(4, 4, 4),for aparalleliso­surface1 renderer. This block 
size matches the size of the L1 cache line onSGI super-computers(SGIOnyx2&#38;SGIOrigin2000). In numerous 
publications it is indicated that blocking by 16 or 32 is an optimal size for many block related processing 
tasks.2 The addressing of blocks and samples within blocks is straightfor­ward, but introducing a block 
map structure allows for arbitrary place­ment ofblocks andpacking in memory with unused blocksbeingignored 
and thus saving memory space. The introduction of blocking results in an additionallevel of complexityforblockboundaryhandling, 
especially for the cases when a sample is requested in a neighboring block that has beenignored. Twostrategiescanbeexplored 
todeal with this. The .rst requires the access of neighboringblocks. Grimm et al.[32],for example, propose 
a scheme based on table lookups for neighboring samples that avoids conditionalbranchesin the code. The 
second strategyisbased on self-containedblocks and requires the replication of neighboring samples. The 
overhead for sample replication is less than 20% for block sizes of 16 and up. Additionalbasicdata conversions 
mayalsobe applied, such as remap­ping the value range and conversion to 8-or 16-bit integers, that is 
data types that directly map to native GPU types. Blocking improves mem­orylocalityfor software-based 
rendering andprocessing. Skipping empty 1An iso-surface, S, is an implicit surface de.ned as S = { p 
| s(p)= C }, where C is a constant. 2In two-dimensional blocking, or tiling, the equivalent size is 64× 
64, also being thedefaulttile sizeinJPEG-2000[1]. blocksusuallyhasasigni.cant e.ect onthedatasizeand 
renderingper­formance. The concept of anemptyblock,however, needstobeclari.ed and de.ned, which is an 
integral part of the next section. 2.7.2 Block Properties and Acceleration Structures In order to reveal 
any embedded entities within a volume it is obvious that some samples must be rendered transparent and 
other samples ren­dered semi-transparent or opaque. This is achieved through the use of a TransferFunction(TF).For 
ablocking scheme, asdescribed above,the meaning of an empty block is a block that has all its voxels 
classi.ed as completely transparent. Naturally, such a block could be discarded in the rendering process 
and thus improve the performance. Since the goal is to reduce the amount of data in the pipeline it is 
essential that empty blocks can be predicted without access to all samples in a block. Meta-data for 
such predictions is collected during preprocessing, and preferably without knowledge of speci.c TF settings. 
TheTFusuallyde.nes one or more regionsinthe scalar range as non­transparent and,forthe rendering ofiso-surfaces, 
either narrowpeaks are de.ned or special iso-surface renderers are used. It is therefore natural that 
ideas from iso-surface extraction acceleration schemes have been applied. Thegoalofthese schemes are 
to minimize theprocessing sothat only cellsintersecting theiso-surface areconsidered. Wilhelms&#38;Gelder 
[108] create a tree of min/max values. The tree is created bottom up and starts with the cells, cubes 
of8 voxels. Livnat et al.[62] extend this approach and introduce the span-space. For iso-surface rendering, 
a leaf in the tree is included if the iso-value is within the range spanned by the minimumand maximum 
valueof thecell. Parkeret al.[78] usealimited two-level tree and .nd that su.cient in their software 
implementation of an iso-surface raycaster. For arbitrary TF settings, the min/max scheme is generally 
overly conservative and may classify emptyblocks as non-empty. Summed-Area Tables[17] oftheTF opacity 
are usedbyScharsach[91] todetermine the blocks content by taking the di.erence of the table entries for 
the minimum and maximumblock values. Thelowgranularity of the min/­max approach is addressed by Grimm 
et al. [31] who, instead, use a binary vector to identify block content. The scalar range is quantized 
into32 uniformregionsand thebit-vectorindicatesthepresence of sam­ples within the correspondingrange. 
A similar approachis takenbyGao et al. [27] but they use a larger vector, matching the size of their 
TF table(256 entries). Figure 2.19: Hierarchical blocking with subsampling. Downsampling is achieved 
by removing every evensample[56] orby asymmetricodd-sized .lter[105]. 2.7.3 Hierarchical Multiresolution 
Representations Simply skipping empty blocks might not reduce the volume size su.­ciently, the total 
size of the remaining non-empty blocks may still be above the available memory size. A strategy is then 
to apply techniques that vary the resolution in di.erent parts of the volume, so di.erent blocks in the 
volume have di.erent resolutions. This Level-of-Detail (LOD)approach enables a more graceful adaptation 
to limited memory and processing resources. The most common scheme is to create a hierarchical representation 
of the volume by recursive downsampling of the original volume. Since each lower resolution level is 
1/8 the size of the previous, the additional amount of memory required for this pyramid is less than 
14.3%. The created hierarchies may di.er depending on the selected downsampling scheme. Figure 2.19 illustrates 
three levels of an hierarchy created using subsampling, every second samplebeing removed. This schemeis 
usedby LaMar et al.[56] andBoada et al.[8], amongst others. Weiler et al.[105] also use this placement 
of samples but employ a quadratic spline kernel in the downsampling .lter since they argue that subsampling 
is a poor approximation. The positions of the downsampled values, however, do require some attention. 
Thepositioningindicatedin .gure2.19 skewstherepresented domain. A more appropriate placing of a downsampled 
value is in the center of the higher resolution values it represents. This placement is illustrated in 
.gure 2.20 and is also a placement supported by average downsampling. Figure 2.20: Hierarchical blocking 
with average downsampling. In order to be able to select di.erent resolution levels in di.erent parts 
of the volume blocking is suitable for hierarchical representations as well. The block size, in terms 
of number of samples, is usually kept equal at each resolution level and the block grids are indicated 
by wide, blue lines in the .gures 2.19 and 2.20. Blocks at lower resolutions cover increasingly large 
spatial extents of the volume. These multiresolution hierarchies thus provide supporting data structures 
for LOD selection. Methods to determine an appropriate level of detail are discussed in the following 
section.  2.8 Level-of-Detail Management Itis not su.cient to onlydetermine if ablockis empty or not. 
The mul­tiresolution representations described above require additional and dif­ferent techniques that 
also candetermine resolutionlevels for theblocks. This section reviews techniques and approaches for 
LOD selection that have been suggested in the literature. These approaches can be classi­.ed into: view 
dependent and region-of-interest, data error, and transfer function based techniques. It is, furthermore, 
common to combine sev­eral of these measures in di.erent con.gurations. The following sections will, 
however, review them individually. The conceptualprincipleforhierarchicalLOD selectionissimilarfor all 
approaches. The selection starts by evaluating one or more measures for a root node. If the resolution 
of a block, a node in the hierarchy, is foundadequate then the traversal stops andthe selectionprocessisdone. 
If the resolution needs to be increased the block is either immediately replaced by all its children 
or a subset of the children is added. The latter approach will remove the parent node when all its children 
have been added. If the amount of data to use is limited, this constraint is checked at every step and 
the LOD selection is stopped when the limit is reached. 2.8.1 View-Dependent Approaches View-dependent 
techniques seek to determine the LOD selection based on measures like distance to viewer and projected 
screen-space size of voxels. Region-of-interest methods work similarly to distance to viewer measures. 
Using full resolution blocks when viewing entire volumes can be suboptimal. When a single pixel covers 
multiple voxels it may result in aliasing artefacts. Reducing the resolution of the underlying sampled 
data(pre.ltering) is,infact, standardingraphicsrenderinginstead of supersampling. It is referred to as 
mipmapping3 in the graphics litera­ture. Distanceto viewer approaches are usedin[56,105,35,6],forinstance. 
A block is re.ned if the projected voxel size is larger than one pixel on the screen, for example. The 
distance to viewer or region-of-interest can furthermore be used to weight some other measure, like a 
data error measure, by dividing that measure by the distance. 2.8.2 Data Error Based Approaches Representing 
a block in a volume with a lower resolution version may naturally introduce errors when the volume is 
sampled compared with using the full resolution. A measure of this error, for instance the Root-Mean-Square-Error(RMSE), 
expressesthe amount of errorintroduced. When selecting a LOD for the multiresolution hierarchy, the block 
with thehighestdata error shouldbe replaced with ahigher resolution version. Repeating this procedure 
until the memory budget is reached will then select alevel-of-detailfor the volume that minimizes thedata 
error. This measure onlydepends onthedataand canthereforebecomputedinthe preprocessing step. This approach 
is used in Boada et al. [8], who also take into account the e.ect of linear interpolation in the lower 
resolution version. In ad­dition, a user-de.ned minimum error threshold is used to certify that the represented 
data correspond to a certain quality. Guthe et al. [35] 3Mip is an abbreviation of the Latin multum in 
parvo many things in a small place. also take this approach, using the L2-norm, but combine it with 
view­dependent measures, namelydistance-to-viewer andprojected voxel size. 2.8.3 Transfer Function Based 
Approaches The shortcoming of data error approaches lies in the mapping of data samples through the TF. 
The content of the TF is arbitrary and conse­quently the data error is a poor measure if it is used for 
volume render­ing. Determining the content of a block in the TF domain has a higher relevance since this 
will a.ect thequality of the rendered image more di­rectly. The notion of a block s TF content is explored 
below and several schemes forTF contentprediction are reviewed. The challenge, however, istopredicttherequiredLODforeachblock 
without accessing thedata beforehand. The completedistribution of sample valueswithin ablockisahighly 
accurate description of the block content, losing only spatial distribu­tion. Such a description could, 
however, easily result in meta-data of signi.cant sizes, potentially larger than the block data itself. 
LaMar et al. [57] therefore introduce frequency tables to express the frequency of speci.cdata errors(di.erences) 
and compute anintensity errorfor a greyscaleTFas an approximationtothe currentTF.Guthe et al. [34]in­stead 
use a more compact representation of the maximum deviation in a small number ofbins,for which the maximum 
errorinRGB-channels are computed separately. A combined approach of these two, using smaller binned frequency 
tables, is presented by Gyulassy et al. [36]. Gao et al. [27]use abit-vectorto representthepresence of 
valuesin a block. The block vector is gated against RGB bit-vectors of the TF. If the di.erence of two 
such products, compared with a lower resolution blocklevel, isless than a userde.ned threshold then thelower 
resolution blockcanbe choseninstead. Asimilar approach using aquantizedbinary histogramispresentedin[31] 
butis not reportedtobe usedforLOD selection.  2.9 Encoding, Decoding and Storage In section 2.7.3 a 
conceptual view of multiresolution hierarchies was de­scribed. As mentioned, the amountofdatais not reducedbythisprocess, 
rather increased. When the amount of data in the hierarchy can not be handled in core memory, additional 
techniques are required. Data com­pression is one viable approach and, speci.cally, lossy compression 
can signi.cantly reduce the amount of data, at the cost of a loss of .delity. Another approach is to 
rely on out-of-core storage of the volume hierar­chy and selectively load requested portions of the data. 
A combination of these techniques is also possible. Some of the well-known approaches are described in 
the following sections. 2.9.1 Transform and Compression Based Tech­niques Following thesuccess ofimagecompressiontechniques,itisnatural 
that such techniques be transferred to volumetric data sets. Usually a trans­form is applied to the data 
and it is the coe.cients from the transform that are stored. The underlying idea for the transform is 
to make data compression more e.cient. Applying a compression technique on the coe.cients, such as entropy 
coding, then yields a higher degree of com­pression compared to compressing the original data. The following 
sec­tions review two transforms that are commonforimage compression and have been used for volume data. 
Basic concepts of compression are also presented. 2.9.1.1 Discrete Cosine Transform TheDiscreteCosineTransform(DCT) 
is well establishedinimage and video coding standards, such asJPEG andMPEG, andthere existhighly optimized 
algorithmsand codetoperformthistransform, with atypical block size of 8. Relatively few researchers have 
applied this transform to volumetricdatasets although someexamples exist[112,79,69]. Lum et al. [70]apply 
theDCT for time-resolved data. Instead of computing the inverseDCT,itis replacedbyatexturelookup sincetheDCT 
coe.cients are dynamically quantized and packed into a single byte. 2.9.1.2 Multiresolution Analysis 
 Wavelets The wavelet transform has gained a wide acceptance and has been em­bracedin many applicationdomains, 
speci.callyin theJPEG-2000 stan­dard,describedbyAdams[1]. A signi.cant amount of work on volume data 
compressionhas employed wavelet transforms. Being a multiresolu­tion analysis framework it is well suited 
for the multiresolution handling of volumedata. Several wavelets exist,buttheHaar andLeGall(a.k.a. 5/3) 
integer transforms, supporting lossless encoding [9, 100], and the Daubechies9/7[19] arethe most common 
and canbe e.cientlyimple­mented using thelifting scheme[99]. Conceptually, the transform appliedon a1D 
signalproduces two sub­band signals as output, one describing low frequency content and the other describing 
high frequency content. Recursive application of the transform on the low frequency output produces multiple 
sub-band de­scriptions until a lowest resolution level is reached. Once the multiband analysis is done, 
the inverse transform can be applied in an arbitrary number of steps until the original full resolution 
has been reconstructed. For every step, the resolution of the volume is increased, doubled along each 
dimension. It is furthermore possible to apply the inverse trans­form selectively and retrieve higher 
resolution in selected subregions of the volume. This approach is taken by Ihm &#38; Park [45], Nguyen 
&#38; Saupe[76] andBajaj et al.[2]. Their workisprimarily concerned with e.cient random access of compressed 
wavelet data and caching of fully reconstructed parts. Awavelettransform can alsobe applied onblocksindividually. 
Guthe et al. [35] collect eight adjacent blocks and apply the transform once on the combined block. The 
low frequency sub-band then represents a downsampled version. The high frequency sub-band, the detail, 
is com­pressedand stored separately. Theprocedureisthen repeatedrecursively on eachlevel until a singleblock 
remains. Ahierarchyofdetaildata,high frequency coe.cients, is thus constructed and can be selectively 
used to reconstruct a multiresolution level-of-detail selection. A hardware sup­ported application of 
this technique, using a dedicated FPGA-board, is presentedby Wetekam et al.[107]. 2.9.1.3 Data Compression 
Applying transforms to volume data does not reduce the amount of data. Instead, it frequently increases 
the size of the data. The Haar and LeGall integer wavelet bases, for example, require an additional bit 
perdimension andlevel. Nevertheless, astheentropy of thetransformed signal is generally reduced, compared 
with the original signal, a com­pression scheme yields a higher compression ratio for the transformed 
signal. Lossless compression is, however, quite limited in its ability to reduce thedata size. In manypractical 
situations with noisydata, ratios above 3:1 are rare. Even this small reduction canbe valuablebut should 
be considered against the increased computational demand of decoding the compressed data stream and applying 
the inverse transform. Signi.cant data reduction can be achieved, however, if lossy com­pression is allowed. 
Quantization of the coe.cients is commonly used and can be combined with thresholding to remove small 
coe.cients. Hopefully this results in many long sequences of zero values that can be compactly represented 
using run-length encoding. There exist a wide range of quantization and compression methods presented 
in the liter­ature and several of these are used in the context of volume data com­pression [106, 45, 
2, 33, 70]. Vector quantization of coe.cient vectors is also applied[92,69]. For reasonabledistortion, 
causing minor visual degradation, compression ratios of30:1 are achieved[35]. It is also reasonable to 
allow the encoding stage to take a signi.cant processing time if the results are improved, it is the 
performance of the decoding stage that is critical.  2.9.2 Out-of-Core Data Management Techniques Computer 
systems already employ multiple memory level systems, com­monlytwolevels of cache are employed tobu.erdatafrom 
main memory. Since the cache memoryisfaster, thishelps to reducedata accesslatency for portions of the 
data already in the cache. Extending caching tech­niques to include an additional layer, in the form 
of disk storage or on a network resource, is therefore quite natural and bene.cial. A data set can be 
signi.cantly larger than core memory but those subsets of the data to which frequent access is made can 
be held in core making these accesses much faster. Indeed, several techniques exist in operating systems 
that exploit this concept, for instance memory-mapped .les. The semantic di.erence between general purpose 
demand-paging is that an application may know signi.cantly more about data access pat­terns than a general 
low level scheme could. Cox &#38; Ellsworth [15] present application controlled demand-paging techniques 
and compare those with general operating system mechanisms, showing signi.cant improvements for application-controlled 
management. Their work is ap­pliedtoComputationalFluidDynamics(CFD) data. Another example is the data 
querying techniques for iso-surface extraction presented by Chiang et al. [13]. TheOpenGLVolumizertoolkit,presentedby 
Bhani­ramka&#38;Demange[6], also supports management oflarge volumes and volume roaming on SGI graphics 
systems. Volume roaming provides scanning through the volume, with a high-resolution region of interest, 
and large blocks, B = (64, 64, 64), are loaded from high-performance disk systems on demand. Distributed 
rendering approaches also make use of out-of-core data management ideas, the capacity of each rendering 
node is limited and Figure 2.21: Flat multiresolutionblocking. Spatialposition and sizeis constantfor 
allblocks(blue squares). The resolutionof eachblockisarbitrary andindependent of the resolution level 
of neighboring blocks. data transport is costly. Predictive measures are required to ensure that the 
renderingloadisbalancedbetween the nodes. Thisis another aspect where application controlled data management 
is preferred, since the access latency can be reduced or hidden. Examples of this approach are presentedin 
severalpapersby Gao et al.[28,29,27]. 2.9.3 Flat blocking Multiresolution Representation In .atblocking 
thesamplesontheuniformgrid arecentered onthegrid cells instead of on the cell vertices. This sample placement 
is shown in .gure2.21(gridinblack and samplesin red). This cell-centered sample placement is compatible 
with OpenGLs sampling location for the pixels in aframebu.er andtexelsinthetextures. Blockdatais also 
cellcentered on the block grid, as indicated by the blue block grid. Furthermore, a multiresolution representation 
is created individually foreachblock, eitherby awavelettransform[66] orby averagedown­sampling[65,63, 
67]. This scheme is referred to as a .at multiresolution blocking, or .at blocking, since no global hierarchy 
is created. The spa­tial extent of a block is constant and the blocks spatial extents do not growwithreduced 
resolutionlevel. Thekeyadvantagesofthe .at scheme can be summarized by: A uniform addressing scheme 
is supported.  Thegranularityofthelevel-of-detail selectionremains .ne-grained.   Hierarchical blocking 
Flat blocking  Level 0 Level 1 Level 2 Level 3 Arbitrary resolution di.erences between neighboring 
blocks can be supported since a block is independent of its neighbors.  The resolution of a block is 
not restricted to be in powers-of-two.  A heuristic analysis shows that .at blocking provides a higher 
memory e.ciency than a corresponding cut through a hierarchical scheme, see table 2.1.  The disadvantage 
with this .ne-grained .at blocking is that the number of blocks is constant. Hierarchical schemes scale 
in this respect with reduced memory budget. On the other hand, since there are no hierar­chical dependencies 
it is trivial to exploitparallelismin manyprocessing tasks for .at multiresolution data. Figure 2.22 
shows an illustrative comparison between hierarchical and .at multiresolution blocking. In this example 
the LOD is selected so that a block that intersects the boundary of the embedded object must have full 
resolution, while the blocks on the interior should be at the second lowest resolution, level 1. Blocks 
on the exterior are to be ignored, level 0. The LOD selection is indicated with level speci.c Table 2.1: 
Memory e.ciency for multiresolution volumes. The LOD selectioncriteriausedin .gure2.22 areapplied. Hierarchy1 
corresponds toaLOD selectionwhereall children alwaysreplaceaparent. Hierarchy 2 is more .exible and allows 
partial usage, as shown in .gure 2.22. coloring. Two strategies for block re.nement are used in the 
literature, as discussed in section 2.7.3. The .rst is to completely replace a block with allits children. 
The second allowspartial usage of ablock with only the requiredhigher resolutionblocksbeing added and 
apart of thelower resolution block can still be used. These are referred to as Hierarchy 1 and 2 in table 
2.1 which shows memory e.ciency for both hierarchical LOD selection schemes and the .at scheme. Full 
resolution blocks have 8× 8 =64 samplesand theoriginalimagesizeis64× 64. The achieved data reduction 
using .at blocking is 2.8:1, compared to 1.6:1 and 1.8:1 for the two hierarchical LOD selection approaches. 
 2.10 Sampling of Multiresolution Volumes The .at multiresolution structureprovides a uniform addressing 
scheme for access to blocks and samples. The volume range is de.ned by the number of blocks, Nx,Ny,Nz, 
along each dimension. The block index, .,isthen easily retrieved astheintegerpart of aposition, p, within 
the volume. The remainder of the position then de.nes the intrablock local coordinate, p . = frac(p). 
The block index map, holding the block size, s, and the location, q, of the block in the packed volume, 
is then used to compute the coordinate for the sample to take. The coordinate range for .at block addressing 
is de.ned in units of blocks. Taking the integer partof a coordinate thenyields theblockindex and the 
remainderyields the local, intrablock coordinate. This scheme is in e.ect virtualizing the volume address 
domain. The block index is used to lookup a block s d3 Block 3 *) y Size 2 Block 1 Size 1 ) d1 Figure 
2.23: Illustration ofblock neighborsin 2D.Theblocks are assigneddi.erent resolutions and the samples 
are indicated by the red dots. The distance between the sampleboundaries(dashedlinesin red) and theblocks 
spatialboundaries(blue lines) are denoted by di, for each block, i. The shaded area between block centers 
* indicates the domain for local interblock coordinate, p . Edge weights, ei,j , are de.ned along the 
edges between adjacent blocks. scale and location in the packed volume. Since blocks in the packed volume 
are rarely neighbors in the spatial domain, special care has to be taken in the sampling of a block. 
Furthermore, the scale of a block is arbitrary which alsohasimplicationsforblock sampling. Anillustration 
of a neighborhood of four blocks is shown in .gure 2.23. 2.10.1 Nearest Block Sampling The .rst approach 
toblock samplingistorestrictthesampling toaccess only data within the current block, suitably expressed 
in terms of oper­ations on the intrablock coordinate, p . . The valid coordinatedomain for intrablock 
samples is indicated by squares of red, dashed lines in .gure 2.23. The inset from theblocks spatialboundariesisindicated 
by di, for each block, i. The restricted sample location, p . C, is then de.ned as p = Cd (p .), (2.7) 
C1-d where Cßa(x)clamps the value, or element values for vectors, of x to the interval[a, ß]. This form 
of sampling has been relabeled Nearest Block (NB)samplinginpaper[67],but wasintroduced as no interblock 
inter­polation in paper [65]. The GPU-based raycaster introduced in paper [63] also used NB sampling 
only. It is evident that block artefacts will arise, but these are not always visible anditis notedbelow 
that they are most apparent for thin, iso-surface like TF settings. 2.10.2 Interblock Interpolation 
Sampling To overcome these block artefacts, an interblock interpolation technique wasdeveloped andintroducedinpaper[65]. 
Previoustechniques rely on sample replication andpaddingbetweentheblocks[56,105,35]. Repli­cation,however, 
counteractsthedata reductioninthepipeline and may also distort samples, where a block has higher resolution 
than its neigh­bor,in orderto reduceinterpolationdiscontinuities. Speci.callyforlower resolutionblocks, 
andimplicitlyforhigherdata reduction, thedata over­headbecomesincreasingly large,asshownin .gure[65]:1b. 
InterblockInterpolation(II) removesthe needfor sample replication andis a scheme fordirect interpolation 
between blocks of arbitrary reso­lution,including non-power-of-twosizes. TheprincipleforII samplingis 
to take a sample from each of the immediate closest neighboring blocks usingNB sampling and compute a 
sample valueby a normalized weighted sum. The domain for interblock interpolation in a neighborhood is 
indi­cated by the shaded area between the block centers in .gure 2.23. The interblock local coordinate, 
p * =frac(p +0.5)- 0.5,has its origin at the intersection of the adjacent blocks. Theblockweight, .b,for 
each oftheblocksis computed usingindivid­ual edge weights, ei,j, for the edges between two facing block 
neighbors centers, blocks i and j, as illustrated in 2D in .gure 2.23 by the grey dotted box edges between 
block centers. In .gure 2.24 four di.erent edge weight schemes are shown, including the NB sampling mode. 
The Maximum Distance scheme provides the smoothest interpolation, but is also the most sensitive to poor 
choice of level-of-detail since the .lter kernel has the widest weighted support. Consider, for instance, 
the case where an iso-surface intersects a block outside its sample boundary. If a neighboring block 
is chosen at a low level-of-detail, the surface would bendout towards thelow resolutionblock. Therefore 
theblock valuedis­tributions used for LOD selection includes one layer of samples around each block. 
The 2D version of the II sample scheme is then succinctly de.ned by the following normalized sum: 4 b=1 
.b.b . =, (2.8) 4 b=1 .b where .b is an NB sample from block b and the block weights, .b, are Block 
tance Split tance .E Figure 2.24: A comparison of the interpolation methods on a slightly rotated linear 
gradient. The original image was a 256×256 greyscale image (8×8 blocks) being reconstructed using random 
levels between 1×1and thefull resolution,32×32. Thebottom row shows color mapped images of the pixel-wise 
errors using the CIEL* u * v * .E color di.erence. The images have been cropped. de.ned as .1 = (1- e1,2)· 
(1- e1,3), .2 = e1,2 · (1- e2,4), .3 = (1- e3,4)· e1,3, .4 = e3,4 · e2,4. The corresponding 3D variant 
and de.nitions of the edge weight func­tions are presented in paper [65]. All edge weight functions described 
therein resultininterpolations equivalent totrilinearinterpolation when­ever the neighboring blocks are 
all of equal resolution. It is also shown that the method constitutes a C0 continuous function. 2.10.2.1 
Sample Placement Discussion The placement of sample points within a block depends on the align­ment of 
the superimposed block grid and on the scheme being used to derivelowerresolutionblocks. Theplacementusedin 
.gure2.23 canbe motivatedfor average valuedownsampling and some wavelet transforms, such as theHaar and 
theLeGall5/3 wavelets[9,100]. Other approaches for lower resolution representations might suggest di.erent 
placements. Sample replication techniques use a di.erent sample placement scheme a) SkinwithNBsampling(1.1 
b) SkinwithIIsampling(0.1/ fps) 0.4 fps)  Figure2.25: Comparison between Nearest Block andInterblock 
Interpo­lation sampling. Images are rendered with texture slicingin a1024x1024 viewport. The opaque surfaceinimage(a) 
clearly showsblock artefacts while these are di.cult to perceive for the softer TF setting in image (c). 
Interblock interpolation (b &#38; d) removes these artefacts. The II examples show single/dual pass framerates. 
that suggests downsampling by skipping every second sample, arguably a less suitable approach.  2.10.3 
Interblock Interpolation Results The quality of the interblock interpolation sampling is shown in .g­ure 
2.24. A slightly rotated gradient is used to evaluate the di.erent edge weight functions and the bottom 
row shows the pixel-wise error in CIEL* u * v * color space. A .nal comparison between interblock interpolation 
and nearest block sampling is presented in .gure 2.25. The top row, with a TF de.ning an opaque iso-surface, 
clearly shows block artefacts without II sampling. The softer TF setting for the bottom row shows less 
perceiv­able artefacts. The multiresolution interblock interpolation scheme was initially de­ployed in 
a renderer based on texture slicing. The increased compu­tational requirements for II sampling causes 
a signi.cant performance reduction, about a factor of 8 10. It is, however, possible to render each slice 
in two passes. The .rst pass samples the interior domain and dis­cards the fragment for samples outside 
the sample bounding box. The second pass then .lls in the samples outside sample boundaries by ex-ploitingearlyZ-termination4 
. The costofthe more expensiveII sampling shader is thus signi.cantly reduced and the performance hit 
is lowered to a factor of only 3 4, compared to NB sampling only. In this case it is important that the 
blocks at the lowest resolution are sampled entirely with NB sampling since theblocks with a single average 
value constitute a degenerate case with no interior intrablock sampling domain.  2.11 Raycasting on 
the GPU The sampling techniques for .at multiresolution representations pre­sentedinprevious sections 
aredescribed using texture slicing techniques, but a more direct raycasting approach can be implemented 
on modern GPUs. The workpresented theredoes notpresentincreasedperformance for GPU-based raycasting over 
texture slicing techniques, but the ren­dering quality is improved and more complex rendering techniques 
are supported. In this section several techniques are described that signif­icantly improve the rendering 
performance for GPU-based raycasting, asintroducedinpaper[63]. Exploiting the .exibility ofprogrammable 
GPUs, severaladvanced classi.cationtechniques combining multipleTFs into single-pass raycasting, are 
brie.y described. Full details are to be found in[67]. 2.11.1 Adaptive Object-Space Sampling As can be 
seen in images of volumetric data, there are often large areas of empty space and, for multiresolution 
volumes, blocks at less than full resolution. Obviously rendering performance could be increased if these 
parts were sampled more sparsely or skipped entirely. This has been the goal of many research e.orts 
and several schemes have been proposedinvolving frame-to-frame andspatial coherence as well as using 
acceleration data structures such as octrees. These approaches have, in general, been mostly binary decisions, 
either a region is skipped or it is sampled at full density. 4Early Z-termination prevents the execution 
of a fragment program if the depth test fails. Table2.2: Renderingperformance of the adaptive object-space 
sampling in a 1024×1024 viewport. Data reductions are given for each data set. TS refers to texture slicing 
and is comparable with full single-pass ray­casting. Numbers specify FPS and (speed-up vs. full sampling). 
The last column indicates the expected theoretical speed-up. Adaptive stepping along the ray wasintroducedinpaper[63] 
where theblock meta-data, the resolution level for eachblock, is used to adjust the density of samples. 
The LOD selection is, in fact, a .ne-grained acceleration data structure with a more continuous adaptationdirective, 
speci.cally when not rendering thin iso-surfaces only. The theoretical improvement through this scheme 
is a performance gain in proportion to the cube-root of the data reduction factor since the sampling 
is only reduced in one dimension, along the rays. Special care has to be taken when stepping across block 
boundaries. In a naive approach, stepping is dictated by the resolution of the cur­rent block being sampled. 
A sample close to the block boundary could thenpotentially advancethe ray deep into a neighboring blockhaving 
a signi.cantly higher resolution, resulting in an undersampled region. The scheme is therefore modi.ed 
to limit the stepping at block boundaries. Inpaper[67] the raycaster wasfurtherdevelopedto supportinterblock 
interpolation sampling, in this case the stepping is limited according to the minimum number of steps 
among all eight neighboring blocks. Adaptive object-space sampling signi.cantly improves the perfor­mance 
ofGPU-based raycasting while maintaining ahigh visualquality. From table 2.2 the speed-up is slightly 
surpassing the expected theoret­ical speed-up (last column). The naive stepping approach shows even higher 
gains, clearly indicating that undesired undersampling occurs. The images in .gure 2.26 show the result 
of full, limited and naive sam­plingdensity. The naive stepping shows obvious undersampling artefacts 
(.g. 2.26c). Applying II sampling reduces the performance and attempts have a)Full sampling(7.6 b) Limited 
sampling c)Naivesampling(29.8 fps) (19.2 fps) fps) been made to counter this e.ect by conditionally 
executing II sampling in the shader. Due to the SIMD nature of fragment processing units, processing 
many fragments in parallel, these e.orts are often counter­e.ective in thatfragmentsfrequently do notbehave 
uniformly over su.­ciently largetiles. Whilethisis anissueinthe currentGPUgeneration, the execution granularity 
is gradually being re.ned. 2.11.2 Flat Blocking Summary The images in .gure 2.27 capture several signi.cant 
contributions of the .atblocking scheme,includinginterblockinterpolationand adaptive sampling. They show 
the graceful degradation of reducing the volume data size to meet aprogressively decreased memorybudget. 
Image2.27c uses only 2.7 MB of a 864 MB data set. Image 2.27a renders all blocks which contribute to 
the image at full resolution, referred to as virtu­ally lossless. The .ne-grained blocking scheme e.ciently 
reduces the data size to 144 MB, a ratio of 6:1, without any loss of image quality. a)6:1,Virtuallyloss-b)80:1,Mediumreduc-c)315:1,Extremere­less 
tion duction Figure 2.27: Transfer function based data reduction. This data set is .rst used in paper[66] 
whichintroducedperceptualTF-basedlevel-of-detail selectionand .at multiresolution blocking. They are 
here re-rendered using the .nal version of my GPU-based raycasting renderer. Data reductions are maintained 
in all stages of the rendering pipeline, including texture memory on the GPU. Together with out-of-core 
data management this pipeline supports the rendering of data sets substan­tiallylarger than available 
core memory. Thisdata reductionis achieved withoutincluding compression techniques, that could reducedata 
sizein the .rst stagesof thepipeline. Maintaining accesstofull resolutiondata at originalprecision is 
essential in the medical domain and thepresented pipeline supports the use of full resolution data, given 
that the needed memory resources are available. Course Notes Advanced Illumination Techniques for GPU 
Volume Raycasting  Light Interaction Markus Hadwiger VRVis Research Center, Vienna, Austria Patric 
Ljung Siemens Corporate Research, Princeton, USA Christof Rezk Salama University of Siegen, Germany 
 Timo Ropinski University of M¨unster, Germany  Light Transport and Illumination Models In this chapter 
we discuss how light interactions can be computed for each voxel. Besides a brief overview of the Phong 
illumination with a focus on volume rendering, we will explain how to extend a raycaster to support shadowing 
as well as ray-traced re.ections. The way light qualities change, when travelingthroughparticipatingmediais 
explained in the last chapter of this course. 3.1 Phong Illumination The purpose of an illumination model, 
in both polygonal and volume rendering, is the simulation of real-world light interactions. During this 
simulation real-worldphenomena as re.ection and shadowing have tobe incorporated. To improve the performance, 
often the simpli.ed Phong illumination model is used, to reduce the O(n2)complexity, inherent to computing 
global illumination phenomena. By using this illumination model, only directilluminationis computed,i.e.,illumination 
notin.u­enced by otherparts of the scene. Thus not every otherpart of the scene has to be considered 
when computing the illumination for the current object and thus the complexity is reduced to O(n). Also 
within volume rendering the Phong illumination model is often used to simulate light re.ections. In the 
following we brie.y describe its usage. We are not giving an entire introduction to the Phong model, 
and refer to [26] for further reading. For simplicity we assume that a volumetric data set is represented 
by a scalar intensity function which assigns to each point x a scalar value f(x). In this context, the 
Phong computation is dependent of the following entities: Position of the current voxel x,  Gradient 
\t(f(x))at the current voxel position,  /** * Returns the diffuse term, considering the  * currently 
set OpenGL lighting parameters. *  * @param kd The diffuse color to be used.  * Usually this is fetched 
from the transfer  * function.  * @param G The computed gradient.  * @param L The normalized light 
vector. */  vec3 getDiffuseColor(in vec3 kd, in vec3 G, in vec3 L) { float GdotL = max(dot(G, L), 0.0); 
return kd * lightParams.diffuse.rgb * GdotL; } Listing 3.1: A simple GLSL shader, which computes the 
contribution of di.use lighting. Voxel color as assigned through the transfer function, and  Position 
of the light source.  To further simplify things, we assume for the following, that we have only one 
idealized point light source. We can compute the re.ection occurring at the current voxel as a sum of 
three di.erent illumination types: di.use re.ection, specular re.ection and ambient lighting. Di.use 
re.ections can be simulated by incorporating the Lambertian law, which states that the re.ected light 
intensity is relative to the an­gle of incidence. Thus we can compute the di.use re.ection Id for the 
current voxelby considering itsnormalizedgradient |. t(f(x))| and the normalized light vector L as follows: 
Id(x)= Ld,in · kd · max(|. t(f(x))|· L, 0). (3.1) Thus, we can modulate the incoming di.use lighting 
Ld,in based on its incident angle and the current voxel color kd. To achieve this e.ect within a volume 
raycastertheGLSLfragment shader showninListing3.1 can be used. In order to incorporate specular re.ections, 
e.g., when dealing with specular materials as for instance metal acquired with a micro CT scan­ner, also 
the specular colorfor a voxelhastobe computed. In contrast to di.use re.ections, specular re.ections 
also depend on the viewing angle. Therefore the normalized view vector V is implicitly used to modulate 
the incoming specular lighting Ls,in and the voxel s specular color ks: Is(x)= Ls,in · ks · max(|. t(f(x))|· 
H, 0)a . (3.2) /** * Returns the specular term, considering the  * currently set OpenGL lighting parameters. 
*  * @param ks The specular color to be used.  * @param G The computed gradient.  * @param L The normalized 
light vector.  * @param V The normalized view vector. */  vec3 getSpecularColor(in vec3 ks, in vec3 
N, in vec3 L, in vec3 V) { vec3 H = normalize(V + L); float GdotH = pow(max(dot(G, H), 0.0), matParams.shininess); 
return ks * lightParams.specular.rgb * GdotH; } Listing 3.2: A simple GLSL shader, which computes the 
contribution of specular lighting. a is used to in.uence the shape of the highlight seen on surface-like 
structures. A rather large a results in a small sharp highlight, while a smaller a results in a bigger 
smoother highlight. As it can be seen, V is not used directly, but the degree of re.ection depends on 
the normalized half-way vector H, which can be computed as follows: V + L H = . (3.3) 2 For adetailedderivation 
of this equation,please also refer to[26]. To integrate specular re.ections into a volume renderer, the 
GLSL shader shown in Listing 3.2 can be used. Please note, that since the half-way vector H is normalized, 
we can neglect the division by 2. As stated abovethePhongilluminationmodelisalocalillumination model, 
which considers onlydirectillumination,i.e.,illumination coming from the light source directly, without 
being in.uenced by other objects. However, to approximate also indirect illumination e.ects, usually 
an ambienttermis used. Thisproceedingensures, that voxels withgradients pointing away from the light 
source do not appear pitch black. Since this ambient term does not incorporate any spatial information, 
it is the easiest to compute: Ia(x)= La,in · ka, (3.4) by only considering the ambient light La,in and 
the voxels ambient color ka. The GLSL shader shown in Listing 3.3 can be used to incorpo­rate the e.ect 
within a GPU-based raycaster. Now, we know how to compute the three contributing illumination /** * 
Returns the ambient term, considering the  * currently set OpenGL lighting parameters. *  * @param 
ka The ambient color to be used.  * Usually this is fetched from the transfer  * function. */  vec3 
getAmbientColor(in vec3 ka) { return ka * lightParams.ambient.rgb; } Listing 3.3: A simple GLSL shader, 
which computes the contribution of ambient lighting. types: di.use, specular and ambient. Listing 3.4 
shows how to combine them and thus realize the Phong illumination model. To alsoincorporate attenuationbasedonthedistance 
d ofthe current voxel to the light source, the computed light intensity can be modulated before returning 
it as shown in Listing 3.5. It requires a function for computing the attenuation, which is shown in Listing 
3.6. However, thisdistancebased weightingdoes notincorporate any vox­els possibly occluding the way to 
the light source, which would result in shadowing. To solve this issue, shadowing techniques need to 
be incor­porated as described in Section 4 of this chapter. So far, we have only considered a single 
light source. To extend the Phong illumination model to multiple light sources we can simply compute 
the contribution of each light source and add them up. This summation becomes possible, due to the additive 
nature of light energy. 3.2 Gradient Computation As we have seen the gradient \t(f(x)) is important 
for both, di.use and specular re.ections. Thequality of the computedgradientsis crucialfor the visualquality 
of the rendering. Especially whendealing with ahighdegree of specular­ity, it is important that the gradients 
change smoothly along a surface. InFigure3.1 threegradient calculationmethods areshown side-by-side: 
forwarddi.erences, centraldi.erencesandSobelgradients[95]. Asex­pected, among these Sobel gradients result 
in the smoothest surfaces. This becomes directly visible when using the gradients for specular re-.ections(seeFigure3.6). 
While the visualdi.erenceisquitebig when rendering arti.cial data sets, it is not as prominent when visualizing 
 Course : Advanced Illumination Techniques for GPU Volume Raycasting Listing 3.4: This GLSL shader realizes 
Phong shading by exploiting the previously introduced shaders. shadedColor *= getAttenuation(d); Listing 
3.5: After computing the attenuation factor, it can be used to modulate the shaded color. Listing 3.6: 
Distance-based attenuation can be computed with a con­stant, a linear or a quadratic attenuation factor. 
 Figure 3.1: Visual comparison of di.erent gradient computation methods when rendering a synthetic volumedata 
set. Fromleft to right: forwarddi.erence, central di.erence and Sobel gradients. real-world data sets 
with smooth intensity changes. However, gradient .ltering can be applied(see Figure 3.2) tofurther improve 
the rendering quality. Due to its relatively high computing costs 26 additional voxel lookups are needed 
 the Sobel operator is often applied in a prepro­cessing phase. This has the drawback, that the currently 
set transfer functiondoesnothave any e.ect onthecomputedgradients. Therefore, often forward and central 
di.erences are used, when rendering perfor­mance is important. As shown in Listing 3.7 computing forward 
di.er­ence gradients requires only three extra voxel lookups. 3.3 Specular Re.ections through Ray-Tracing 
Now, that we know how to compute the gradients, we can use them as normals in various ways to improve 
the illumination. For instance, the gradients canbe usedto realize a volumetric raytracer. One approachfor 
GPU-based raycasting considering only .rst order rays to be traversed hasbeenproposedbyStegmaier et al.[97]. 
With their approach they are able to simulate mirror-like re.ections and refraction. By casting a ray 
and computing its de.ection, when a re.ection or refraction occurs. To simulatethe mirror-like re.ection,theyperformanenvironmenttexture 
lookup. An alternative approach to trace rays of higher order, would be to use additional entry and exit 
points. This can be achieved by exploiting /** * Calculate the gradient based on the A channel  * using 
forward differences. */  vec3 calcGradient(sampler3D volume, vec3 voxPos, float t, vec3 dir) { vec3 
gradient; float v = texture1D(transferFunc , textureLookup3D(volume, volumeParameters, voxPos).a); float 
v0 = texture1D(transferFunc , textureLookup3D(volume, volumeParameters, voxPos + vec3(offset.x, 0.0, 
0.0)).a); float v1 = texture1D(transferFunc , textureLookup3D(volume, volumeParameters, voxPos + vec3(0, 
offset.y, 0)).a); float v2 = texture1D(transferFunc , textureLookup3D(volume, volumeParameters, voxPos 
+ vec3(0, 0, offset.z)).a); gradient = vec3(v -v0, v -v1, v -v2); return gradient; } Listing3.7: For 
computingforwarddi.erencegradients only3 additional voxel lookups are required. By also fetching the 
transfer function, dy­namic gradient changes are considered. the work.ow shown in Figure 3.3. Inthe .rst 
stagetheinitial unmodi.ed entry and exitpointsarecom­puted as described in the .rst chapter of these 
course notes. By using thesepointsthe .rst orderrayscanbecasttogenerateoneintermediate imageaswell astoconstructa 
.rsthitpointtexture,i.e.,thepositions involumespacewherearay .rstencountersavisiblemedium(seeFig­ure3.4(right)). 
Whiletheintermediateimageis cachedin ordertobe abletoblenditinthelast stage,the .rsthitpointsareused 
astheentry points in the next recursion step. To increase the foot print of the voxels encountered at 
the .rst hit point,the .rsthitpointcomputationcanbealteredby sampling onestep further into the volume. 
This ensures that the encountered medium is su.ciently penetrated and thus more clearly visible in the 
intermediate image. The exit points for the next recursion step can be computed in the second stage, 
as shown in Figure 3.3. Based on the entry position p and the direction d of a higher order ray r, the 
intersection between r and the bounding box of the volume can be computed. This is done by .rstperforming 
an intersectionpoint test for r and the six sideplanes of the bounding box. When knowing the normal N 
and the distance to the origin D of such a side plane, we can express each point x on the plane by the 
equation: (N · x)- D = 0. By substituting the parameter form of the ray x = p +t · d into this equation, 
the following parameter value Figure 3.2: Gradient .ltering improves the visual quality especially for 
synthetic datasets: centraldi.erencegradients(left),Sobelgradients(right). for the intersection point 
can be obtained: D - (p · N) t ' = . (3.5) d · N This parameter value needs to be computed for each of 
the six side planes, before choosing the smallest positive value tmin among these. After this computation, 
the entry and exit points can be forwarded to the subsequent raycaster. When all raycasters have .nished 
rendering, the .nal image can be computed by blending all available intermediate images in stage 4 as 
shown in Figure 3.3. As mentioned above, to compute the ray direction for rays re.ected on a specular 
surface, the gradient \t(f(x)) of this surface is consid­ered. Similar to the computation of the specular 
re.ection in the Phong illumination model as described in Section 3.1, the outgoing ray can be computed 
by considering the incoming ray and \t(f(x)). In cases where a refraction occurs, the incoming ray is 
bent at the surface based on Snell s law. Snell s law is used in physics to describe the behavior of 
a ray hitting an refractive interface, e.g., the surface of a glass-like object. It expresses the relationship 
between the angle of incidence f and the angle of refraction . and says that the ratio of the sines of 
f and . is a constant depending on the present media: sinf n2 = (3.6) sin. n1 Thus, to compute the bending 
angle the refraction indices of the adjacent media, for which the refraction should be computed, has 
to be Figure 3.3: Ray tracing can be achieved by using four subsequent stages. Initially theunmodi.ed 
entry and exitpointsarecomputedforthe .rst orderrays(stage1). Afterwards,foreach recursivestep,theexitpointsarecalculatedbased 
onthe .rst hitpointsof thepreviousstep(stage2). Araycastercomputesintermediateimages based on the generated 
entry and exit points (stage 3). Finally all intermediate imagesareblendedintothe .nalimage(stage4). 
known. When assuming that these indices are n1 for the medium which is left by the ray, and n2 for the 
medium which is entered by the ray, we can compute the bending angle . based on the incoming angle f 
as follows: 2 cos(.)=1-n1 · (1- (cos(f))2). (3.7) n2 Thus, we can compute thedirection of theleaving 
ray Avec asfollows: n1 n1 Avec =Evec +| cos(f)|- cos(.)|\ t(f(x))|, (3.8) n2n2 where Evec is a vector 
representing the incoming ray. In general total re.ection has to be considered in cases the incoming 
rayhitstheobject underavery.at angle,i.e.,norefractionbut specular re.ection occurs. The critical angle 
for which total re.ection occurs can be computed as follows: fcrit = sin-1( n2). (3.9) n1 Figure 3.4: 
A synthetic Cornell box data set rendered with refraction and specular re.ection(left) andits .rsthitpoints(right). 
Thus, when the incident angle f exceeds fcrit, a specular re.ection ray has to be computed instead of 
a refractive one. The result of a ray tracer implemented using the concepts described aboveis showninFigure3.4(left). 
Asit canbe seeninFigure3.5 some scenes require only a rather low recursion depth to render them more 
realistically. Again, by using di.erent gradient calculation methods, di.erent im­age qualities can be 
achieved as shown in Figure 3.6. During theblending of theintermediateimagesin stage4, a mapping functioncanbeused, 
which expressesthespecularity of certainintensity ranges, andthusin.uencethe contribution ofhigher order 
raysto apixels color. Alternatively,byusing an automatic approach, the specularity can be set proportional 
to the length of \t(f(x)). It should be mentioned that this is not an exact measure, since the gradient 
length determines onlytheintensity changes of neighboring voxels. However, when ahigher intensity range 
occurs, one of the materials must have a higher intensity and may thus be assumed as being more specular. 
 Figure 3.6: Specular re.ections can be computed by incorporating forward di.er­encegradients(left) 
andSobelgradients(right).  Shadows Ithasbeen shown thatthe usedlightingmodelhas a majorimpact onthe 
spatial comprehension[58]. Forinstance, shadows serveasanimportant depth cue[89]. Inthis section we wouldliketodiscuss 
sometechniques for integrating shadows into a GPU-based volume raycaster. While we will focus on volume 
rendering, it should also be mentioned that some researchers address the combination of polygonal and 
volumetric data when consideringlightinteractions[113]. 4.1 Soft vs. Hard Shadows Variousshadowalgorithmshavebeendevelopedinthe 
.eld of computer graphics. Crowhasproposed a shadow volumetechniqueforgenerating shadowsforscenes containingpolygonaldata[16]. 
To compute shadows of an object, its silhouette is extracted and extruded in direction of the light rays 
in order to generate shadow polygons which form the shadow volume of an object. During rendering each 
object is tested whether it lies inside or outside a shadow volume, and thus it can be determined whether 
the object is shadowed. Due to the polygonal nature of this algorithm, it is not suited for volume rendering. 
Another common ap­proachforgenerating shadows when renderingpolygonaldatais shadow mapping whichhasbeenpresentedin1978byWilliams[109]. 
Shadow mapping is an image-based approach which exploits an additional ren­dering pass in which the scene 
is rendered from the light source s point of view in order to determine the structures closest to the 
light source (seeFigure4.1). With thisknowledge, afragment-based shadowtest can beintroducedinthemainrenderingpass,i.e., 
eachfragmentistested whether it is further away from the light source than the corresponding texel in 
the shadow map. In contrast to the shadowing approaches for slice-based volume ren­dering [4, 52], only 
little has been done in order to integrate shadows into GPU-based raycasting. However, due to the similar 
ray paradigm, it should be noted that shadows have been integrated in volume ray­tracing systems[103,102]. 
An overview of these techniques canbefound in[72]. In analogy to the polygonal shadow mapping approach, 
for volume rendering also a depth map is needed in order to store light source visi­bility. Therefore 
.rst hit positions as seen from the light source can be computed and used to compute thelight sourcedistance(seeFigure4.2). 
When rendering polygonal models a depth value is properly de.ned. In contrast, volume rendering does 
not provide depth values in general. Therefore a shadow threshold value has to be used. With this threshold 
the intensity values representing opaque geometry can be changed, and with it the content of the generated 
shadow map. The example shadow map shown in Figure 4.2 has the same resolution as the viewport, i.e., 
512× 512 pixel, which allows to generate shadows without introducing aliasing artifacts(seeFigure4.7). 
One bene.t of shadow mapping is that soft shadows can be approxi­matedby exploitingpercentagecloser .ltering[82]. 
Thisisdemonstrated in Figure 4.3, where the Visible Human torso data set is rendered with both,hard shadows(left) 
andfake soft shadows(right). Furthermore, when using shadow mapping the combination withpolygonal models 
can be supported quite easily, since the geometry can also be represented within the shadow map.  4.2 
Semi-Transparent Shadows with Deep Shadow Maps While shadow mappingallows very e.cient shadows on afragmentbasis, 
itdoes notsupport semi-transparent occluders often occurin volume ren­dering. In order to address semi-transparent 
structures, opacity shadow maps serve as a stack of shadow maps, which store alpha values instead ofdepth 
valuesin each shadow map[50]. Another more compact repre­sentationfor semi-transparent occluders aredeep 
shadow maps[68]. The used data structure consists also of a stack of textures, but in contrast to the 
opacity shadow maps, an approximation to the shadowfunctionis storedinthesetextures(seeFigure4.4). Thusitispossibleto 
approxi­mate shadowsby usingfewerhardware resources. Deep shadow mapping has .rstbeenappliedtovolumerendering 
by Hadwigeretal.[39]. Whilethe originaldeep shadow map approach[68] storesthe overall light intensity 
in each layer, in volume rendering it is advantageous to store the absorption given by the accumulated 
alpha value in analogy to the volume rendering integral. Thus, for each shadow ray, the alpha functionis 
analyzed,i.e.,thefunctiondescribingthe absorption, and approximate it by using linear functions. To have 
a termination criterion for the fragment shader which per­forms the processing, the depth interval covered 
by each layer can be restricted. However, when it is determined that the currently analyzed voxels cannot 
be approximated su.ciently by a linear function, smaller depth intervals are considered. Thus, the approximation 
works as fol­lows. Initially,the .rsthitpointforeachshadowrayiscomputed,similar as done for the shadow 
mapping described above. Next, the distance to thelight sourceof the .rsthitpoint and thealphavalueforthisposition 
are stored within the .rst layer of the deep shadow map. Since we are at the .rst hit position, the alpha 
value usually equals zero when the shadow threshold is set accordingly. Starting from this .rst hit point, 
we traverse each shadow ray and check iteratively wether the samples encountered so far can be approximated 
by a linear function. When processing a sample, where this approximation would not be su.cient, the distance 
of the previous sample to the light source as well as the accumulated alpha value at the previous sample 
are stored in the next layer of the deep shadow map. This is repeated until all eight layers of the deep 
shadow map have been created. To demonstrate the possibilities of deep shadow maps, we again use the 
simple volumetric Cornell box scene consisting of 128 × 128 × 128 voxel(see Figure4.5). Theblueball, 
which is set tobe semi-transparent by modifying the transfer function, casts a transparent shadow on 
the back wall. The di.erence to the shadow of the opaque purple box can be noticed, especially when looking 
at the shadow borders. For gener­ating the shown images, a deep shadow map consisting of eight layers 
has been used. A color coded version of the layers is also shown in Figure 4.5 (bottom). To generate 
the layers rapidly, current graphics hardware can be exploited. By using multiple render targets eight 
layers could be computed in a single rendering pass. However, using only four rendering targets and creating 
eight deep shadow layers by performing simple channel splitting allows a more e.cient storage. Therefore, 
the light source distance and the alpha value are stored in two successive channels, i.e., R and G as 
well as B and A. Thus, two shadowlayers can be represented by using only one RGBA texture. However, for 
illustra­tion purposes, we wrote these values into the R and G channels when generating thepictures showninFigure4.5(bottom). 
 Whengenerating thedeep shadowmaps, anerrorvalueisintroduced todetermine wether the currently analyzed 
samples canbe approximated by a linear function. In analogy to the original deep shadow mapping technique 
[68], this error value constrains the variance of the approxi­mation. Thiscanbedoneby adding(resp. subtracting) 
theerrorvalue at each sample s position. When the alpha function does not lie within the range given 
by the error value anymore, a new segment to be ap­proximated by a linear function is started. The e.ect 
of choosing a too small error value is shown in Figure 4.6. As it can be seen, a too small error value 
results in a too close approximation, and the eight layers are not su.cient anymore to represent shadow 
rays having a higher depth complexity. Thus especially in regions, were the two occluders both in­tersect 
a shadow ray, shadow artifacts appear. Obviously this drawback can be avoided by introducing additional 
shadow layers. This would allow a more precise approximation of the shadow function, but would also resultindecreased 
renderingperformance since additional rendering passes are required. A comparison of di.erent shadow 
computation methods is shown in Figure4.7andFigure4.8. Asitcanbe seendeep shadow mapsintroduce artifacts 
when thin occluder structures arepresent. The shadows ofthese structures show transparency e.ects although 
the objects are opaque. This results from the fact that an approximation of the alphafunction is exploited. 
Especially thin structures may diminish when approximating over too long depth intervals.  Course Notes 
Advanced Illumination Techniques for GPU Volume Raycasting  Ambient Occlusion Markus Hadwiger VRVis 
Research Center, Vienna, Austria Patric Ljung Siemens Corporate Research, Princeton, USA Christof Rezk 
Salama University of Siegen, Germany Timo Ropinski University of M¨unster, Germany  Ambient Occlusion 
for Isosurfaces Ambient Occlusion refers to techniques to simulate global lighting by estimating the 
visibility of a global ambient omnidirectional illuminant from a primitive, be it a polygon vertex, surface 
location or voxel in a volume. In this section we will outline various methods based on vol­umetric models 
or data sets. We have divided the algorithms into two classes; isosurface based volume rendering and 
full volume rendering, in­corporating semi-transparent samples and translucency. In both cases direct 
volume rendering is considered, that is, no intermediate geome­try is created for isosurfaces. The remainder 
of this chapter describes techniques for direct isosurface volume rendering. In the past many methods 
based on the theory of light transfer have beendeveloped to enableinteractivedi.useinterre.ectionsforpolygonal 
models[93,11]. Most ofthesephysically motivated approaches arebased onpre-computing illumination for 
all vertices and storingitin an appro­priate data structure which is accessed during rendering. Thus 
these algorithms support interactive modi.cation of light and camera param­eters as well as some material 
parameters. However, the application to deformablegeometryisconstrained and requiresanewpre-computation 
in most cases [94]. To address these limitations, approximations have been proposed, which are not physically 
motivated, but lead to visually convincing results. In contrast to polygonal models the structure repre­sented 
by a volumetric data set depends on the rendering parameters, which include the transfer function as 
well as thresholding parameters, since these renderingparameters canbe used to omit certain voxelsfrom 
being displayed. For instance, by only changing the transfer function or the thresholding, a medical 
volume data set can be visually mapped to manydistinct structures, e.g.,the skeleton only,the muscles 
only or the skeleton with surrounding tissue. Obviously this structural variance has a strong impact 
on the light interaction between the structures of such adata set, renderingthe currentlyknown surface-basedillumination 
techniques insu.cient for interactive volume illumination. Since trans­fer function as well as thresholding 
are altered frequently it should be possible toperform these changesinteractively[51]. Another challengeis 
the fact that volume rendering requires to compute light interactions for several samples along a viewing 
ray and thus introduces a higher level of complexity compared to computing light interactions only once 
for each fragment as is necessary when rendering opaque polygonal data. Vicinity Shading [98] simulates 
illumination of isosurfaces by tak­ing into account neighboring voxels. In a pre-computation the vicinity 
of each voxel is analyzed and the resulting value, which represents the occlusion of the voxel, is stored 
in a shading texture which can be ac­cessed during rendering. Vicinity Shading requires a new preprocessing 
when changing the rendering parameters, and it does not support color bleeding. Desgranges andEngeldescribe 
aless expensive approximation of am­bientlightthanVicinity Shading[20]. They combine ambient occlusion 
volumes from di.erent .ltered volumes into a composite occlusion vol­ume. While pre-processing time is 
greatly reduced the ambient occlu­sion volume still must be recomputed whenever the transfer function 
is changed. Wyman et al. have presented a technique to pre-compute or lazily compute global illumination 
for interactive rendering of isosurfaces ex­tracted from volumetric data sets [111]. They support the 
simulation of direct lighting, shadows and interre.ections by storing pre-computed global illumination 
in an additional volume to allow viewpoint, lighting and isovalue changes. Beason et al. present a method 
which addition­ally can represent translucency and caustics but supports static lighting only[3]. Forthatpurposethey 
extractdi.erentisosurfacesfrom a vol­umetric data set, illuminate them with a path tracer and store the 
re­sults in a new volume data set. During rendering they can interactively change the isovalue and access 
the pre-computed illumination. Penner andMitchell recentlyproposed a methodbased onhistograms to classify 
the visibility around a voxel [80]. It bears some similarities with the techniqueby Ropinski et al.[85],describedlater. 
All these surface illumination models are only applicable to isosur­facesrepresenting asingleintensity 
withinthedataset,butdonot allow to consider multiple surfaces correspondingtodi.erentintensities. Hence 
itis notpossible to represent the entire volumedata set, whereas varying intensities are one of the major 
advantages overpolygonal models, e.g., when representing di.erent types of tissue. Ambient Occlusion 
for Direct Volume Rendering Desgranges and Engel describe a less expensive approximation of ambi­ent 
light than Vicinity Shading [20]. They combine ambient occlusion volumes from di.erent .ltered volumes 
into a composite occlusion vol­ume. While pre-processing time is greatly reduced the ambient occlu­sion 
volume still must be recomputed whenever the transfer function is changed. 6.1 Local Ambient Occlusion 
Recently Hernell et al. have proposed a method for computing ambient and emissive tissueillumination 
e.ciently[42]. Thistechniqueisbased on shooting rays in several directions from each non-transparent 
voxel in the data set. The rays are terminated at predi.ned radial boundary making this technique a local 
approach. Since these rays are generated for each non-transparent voxel in the data set it bene.ts from 
multires­olution level-of-detail data reduction. Empty space is ignored and low resolution regions requirelessprocessing. 
This techniquehasbeenimple­mented using the multiresolutiontechniquesby Ljung et al.[63,65,67] In many 
applications of volume rendering itis notdesired to create a realisticglobalillumination where objects 
canbefully shadowed. Alocal modelisthereforeproposed, where alimited spherical neighborhood,O, around 
each processed voxel is evaluated. To consider semi-transparent materialsthelocal ambient occlusion model 
needstogobeyondthe all­or-nothing approach used in previous work. The proposed technique includessemi-transparent 
objectsand supportsasmoothfading of shad­ows as occluding objects occupy more or less of the local neighborhood. 
The scheme is based on sampling the locally occluding material along multiple rays originating from the 
voxel at the center of O. The incident light intensity, Ik(x), arriving at a voxel location, x, from 
one ray direction, k, is given by the equation: RO s wk Ik(x)= exp(- t(u)du)ds (6.1) a RO - a a where 
a is an initial o.set from the voxel along the ray and RO is the radius of the spherical support. A ray 
is also associated with a weight, wk, and thus enables directional weighting of the ambient light. The 
attenuation oflight contribution along the rayis estimated using the the opticaldepth, theintegral of 
transferfunctiondensities, t(s). Numerical evaluation of the integral in equation 6.1 is obtained as: 
Mm-1 LI wk Ik(x)= (1- ai) (6.2) M m=0 i=0 in afront-to-backcompositing scheme, where M isthe number ofsamples 
along the ray and ai is the sample s opacity according the the current transfer function. Lightiscontributed 
at each samplepoint, normalized tosumto one. This ensures that shadows from an occluding object increase 
smoothly as alargerfraction of the raypenetrates the occluding object. This e.ect canbeseenin .gure6.3(top-right). 
Thelocal ambientocclusionfora voxel, I(x), is then given by the sum of all incident light rays Ik 1 K 
L I(x) = K Ik(x). (6.3) k Figure6.3(bottom-rightimage), shows64 rays. Thedirectionsforthe rays are createdby 
subdividing atetrahedron,icosahedron or octahedron to di.erent levels, depending on how many rays are 
desired. Estimation of the volumetric LocalAmbient Occlusion(LAO)isper­formed as an initial step of the 
rendering pipeline. The resulting light intensity of each voxel is stored in an intensity map which is 
used in the .nal volume rendering to illuminate each sample. To further control the e.ect of the ambient 
occlusion the .nal value canbe adjusted according to agammafunction. In addition, an ambient biascanbespeci.edtoadjusttheamount 
of minimumlighting. The .nal luminance, I ' (x), is computed bythe following equation. I ' (x)=(Ibias 
+ I(x)). (6.4) 6.1.1 Emissive Tissues and Local Ambient Occlu­sion Having established a framework for 
volumetric local ambient occlusion we can proceed to incorporate emissive components from the transfer 
function. This simulates singlelightscattering ofluminous objects within the volume. Equation 6.1 is 
simply modi.ed to also incorporate colored light emission, cE. RO 1+cE(s) s Ik(x)= wk exp(- t(u)du)ds 
(6.5) a RO - a a The emittance of a sample point along a ray within O can therefore a.ect the intensity 
and color of point x. The emissive parameter is included during the .nal ray-casting as well, further 
enhancing the e.ect of luminosity. 6.1.2 Integrating Multiresolution Volumes Mapping from volume coordinates 
to packed coordinates is straightfor­ward usingtheforwardindextexture mentioned above,describedin more 
detailin[65]. The reverse mappingpresents aminor challenge, sincethe location ofa voxelinthe volume mustbedetermined. 
Potentiallyablock couldbe represented by a single voxel in thepacked texture which would require areverseindexmap 
of thesame size asthepacked textureitself. This is avoided by ignoring reverse mapping of blocks below 
a certain minimum resolution level. By ignoring blocks with the two lowest resolution levels, 13 and 
23 , the size of the reverse index map can be reduced by a factor of 64. All empty blocks are assigned 
the lowest resolution or ignored entirely and the remaining blocks are assigned higher resolutions, with 
a minimum of 43 voxels. The reverse map can then hold a block s location in volume coordinates(V) forall 
non-empty blocks. Figure6.1illustratesboth the forward and reverse mapping schemes. Neighboring blocks 
in the packed texture are not always located closely in the original volume. Since linear interpolation 
is used be­tween texels this implies that interpolation also occurs between block boundaries which then 
leads to artifacts. A distance d is therefore used to clamp the coordinates so that interpolation only 
occurs within each block [65]. In the local occlusion calculation interpolation of samples between blocks 
is not considered. 6.1.2.1 Multiresolution Ambient Occlusion Pipeline TheLAOcomputationisdrivenbytheprocessing 
of each voxelinthe vol­ume. Since a multiresolution volume management is applied the gained data reduction 
from the LOD selection also implies a speed-up of the LAO processing. With increased data reduction the 
LAO processing is also reduced, the LAO illumination is computed with the same data reduction as the 
LOD selection of the volume. The processing of each voxel can, in turn, be further accelerated by adapting 
the sampling rate depending on the voxel s block resolution level. This per-fragment pro­cessing pipeline 
is presented next. Thepipelineis executedforall voxelsin aslice of thepacked volume texture in parallel. 
A 3D texture is created to hold both the scalar values of the volume and the colored illumination of 
each voxel. The entire volume is then processed by mapping all the slices, one-by-one, as a rendering 
target of a framebu.er object. The fragment pipeline is initiated by rendering a large quad over the 
entire framebu.er, where each pixel maps to one voxel in the mapped 3D texture. This approach is e.cientbut 
requires that thehardware support the attaching of a slice of a3D texture to aframebu.er, as availablein 
thelatestGPUhardware generation, e.g. NVIDIA GF8800. For each voxel a fragment program is initiated. 
An outline of the per-fragment processing is shown in 6.2. The fragment location is used to lookup the 
corresponding voxel location in the volume through the reverse index texture, P.V. In addition a per-slice 
constant z-position is provided to .nd the correct 3D location. The pipeline then iterates over all the 
incident light directions in O. The number of rays, K, to sample can be dynamically con.gured and the 
ray directions are stored in a texture together with the directional weight, wk. The samplingalong a 
ray starts at a user-de.ned o.set, a. In addition the samplingdensity, d, and radius, RO, ofO canbe speci.ed. 
Theinitial o.set avoids unnecessary self-occlusion for voxels on the boundary of highly opaque regions. 
The sampling density is adjusted for voxels in lower resolution blocks. A voxel belonging to a block 
of resolution level, ., relative to the highest resolution level, increases the sample distance, the 
step length, by a factor of 2. . Thus, the sampling rate stays .xed relative to the sampling density 
of the underlying volume data. The immediate lower resolution will have twice the sampling distance. 
When the sampling rate changes the opacities and colors have to be corrected as well, this is done on 
the .y using the opacity correction formula: amod =1.0- (1.0- aorig)2.d . (6.6)  6.1.3 Adding Global 
Light Propagation Hernell et al. [43] recently extended the Local Ambient Occlusion ap­proach described 
above to include a Global Light propagation and .rst order scattering e.ects. Below we describe this 
approach. Figure 6.7 shows a schematic overview of the technique. The methodprogressesin several steps 
to simulatethelight transport inthe volume and, ateach step, capturesthe mainphysical contributions. 
In the .rst step opacities are composited into a global shadow volume foragivenlight sourceand transferfunction(TF) 
settings. Thecalcula­tionisbased onpiecewiseintegrationtechniques ontheGPU and sparse representations 
ofintermediate results. The method thenproceeds toin­clude .rst orderscattering of thegloballight arriving 
at each voxelbyin­tegration of scatteredlightin alocal spherical neighborhood around each voxel. This 
step is similartothelocal ambient occlusion(LAO) method describedin[42],which enhancesperception oflocal 
shapesandtissue properties. In the last step a single pass ray-casting approach is used to render the 
.nal image. The resulting application enables updates of lightposition and transferfunctions while maintaining 
interactiveframe rates yet simulating a realistic light model.  6.2 Dynamic Ambient Occlusion In this 
section we cover recent techniquesbased onRopinski et al.[85] which realizes dynamic ambient occlusion 
as well as an approximation to color bleeding when rendering volumetric data sets. The method is independent 
of the currently applied transfer function as well as the thresholding and therefore represents ambient 
occlusion as well as color bleeding like e.ects for all combinations of structures contained in a  Figure 
6.4: Emissive illumination e.ects. Left to right: Emissive light captured in the ray-tracing stage, emitted 
light illuminating the surrounding material, and both e.ects combined. Data reduction A 323 2563 8.9:1 
284 552 14.8:1 178 515 22.1:1 121 403 35.2:1 81 365 B 233 145 96 62 C 68 68 48 46 Piecewise segment length(voxels) 
4 8 16 32 323 261 284 331 436 643 267 297 339 439 A 1283 439 373 380 463 2563 1428 552 641 862 Table 
6.1: Performance, in milliseconds (ms), for di.erent levels of il­lumination updates((A),(B) and(C)) 
versus varyingdata reductions (lefttable). Thepiecewise segments are8 voxelslong. Inthe righttable the 
performance is shown for di.erent sizes of the SVR versus varying lengths ofthelocalpiecewise segmentsforlight 
update(A).Thedata reductionis8.9:1. Measurementsinboth tables areperformedfora vol­ume of 5123 voxels, 
rendered in a 1024x1024 window. The same volume, TF and rendering settings are used in .g. 6.10, using 
a step length of 16 voxels. volume data set, which can be extracted interactively by changing these renderingparameters. 
However, thepresented rendering algorithmis not basedonthephysics oflightpropagation,butprovides a visually 
convinc­ing approximation. It canbe applied todirect volume rendering(DVR) as well as isosurface shading 
techniques, and for the latter the isovalue can be changed interactively. Rendering time is kept low, 
since the pro­posed technique requires only little overhead compared to the solution of the standard 
volume rendering integral combined with the applica­tion of strictly local illumination. Besides the 
transfer function and the thresholding, lighting as well as the camera parameters can be changed interactively. 
Furthermore, in contrast to frequently used surface-based illumination models our techniquedoes notnecessarily 
require agradient calculation and is therefore also applicable to homogeneous regions. In the pre-computation 
we ensure that we analyze and store the en­vironment of each voxel x in such a way that we are able to 
compute an environmental color Eenv approximating the in.uence of the voxels neighborhood during rendering 
interactively. In order to support in­teractive modi.cation of the transfer function and the thresholding, 
the computation is performed independently of these rendering parameters. The consecutive steps needed 
are shown in Figure 6.12 and are further explained in the following subsections. 6.2.1 Local Histogram 
Generation To approximate the environmental color Eenv for a given voxel x, we exploit itslocalhistogram 
LH(x). Localhistogramshave alsobeen used in other areas of volume rendering[86,71]. For our approachlocalhis­tograms 
are adequate, because indirect illumination can be calculated properly for a given point by considering 
close objects only [18]. All voxels x lying in a sphere Sr(x) with radius r centered around x con­tribute 
to the local histogram LH(x), weighted basedon their distance to x (see step1 in Figure 6.12). Thus assuming 
that f(x). [0, 2b], with b .{8, 12, 16} being the bit depth of the data set, LH assigns to each x an 
n-tuple, with n =2b: LH(x)=(LH0(x), . . . , LHn-1(x)), with (6.7) L |x - x | LHk(x)= fdist · g(f( x),k). 
(6.8) dmin x .Sr(x) x . =x dmin denotes the minimal distance between any two di.erent voxels in the data 
set. fdist = d1 2 is used to achieve a distance based weighting and takes into account that energy falls 
o. as the inverse square of the distance. g(i, k)is used to group the intensity values appropriately: 
1, if i = k g(i, k)= (6.9) 0 , otherwise. LHk(x)representsthein.uence voxels ofintensity k in the neighbor­hood 
of x have on x. In contrast to other techniques we can not consider the attenuation of light which results 
from voxels lying between the cur­rent voxels and its neighbors, because we discard the spatial locations. 
Since we are only interested in the relative distribution of LH(x), we normalize the valuesin each LH(x)with 
respect to the number of voxels lying in the sphere Sr(x)with radius r centered around x. To capture 
the neighborhood of an object in scenes consisting of polygons, often ray casting is exploited which 
involves sampling that may in.uence the image quality. Since volume data sets are already a discretized 
representation, ray casting and thus possible sampling arti­facts should be avoided. We use a simple 
method which captures the vicinity of voxel x by iterating over all voxels x in its neighborhood and 
adding their contribution to LH(x) as de.ned by equation (6.8). By de.nition LH(x) does not contain any 
spatial information except the distance based weighting fdist inherently capturing the degree of in.u­ence 
of the voxels in the neighborhood. In order to capture also direc­tional information, we subdivide Sr(x) 
based on the gradient at x into two hemispheres. Instead of one local histogram for Sr(x), we compute 
two local histograms, one for the forward facing hemispherical region Hf(x)and one for the backward facing 
hemispherical region Hb(x)(see Figure 6.12). As already mentioned, instead of capturing the light interactions 
be­tween all voxels of adata set, we consider only the vicinityde.nedby the radius r. Obviously r is 
data set dependent, but cangenerally be chosen rather small in comparison to the number of voxels n. 
This reduces the complexity from O(n2)operations to O(r3 · n). a) Di.use Illumination b) Local Ambient 
Occlusion, 32 rays c) Close-up of vesselsin(a) d) Close-up of vesselsin(b) Figure6.5: Exampleimages 
showingthe enhanced3Dstructure made clearthrough the LAO method. (a)Di.use Illumination (b)LAO with 
illuminat-(c) LAO with emissive ing material ray-tracing Figure 6.6: Example images showing the enhanced 
information from the emissive materials. The bullet and fragments are clearly visible in the abdomen. 
The e.ect of the LAO in revealing the bone structure is also very clear. (a) (b) (c) (d) (e) Figure 
6.7: Opacityfor alocal segmentis integratedin(a) andglobal opacityisin­tegratedin(b). Asimplemethod tocompute 
Id is toperform adirectinterpolation from ag, asin(c),however, abetter approximationis obtained,in(d),by 
utilizing thepiecewiseintegrationas aninitial step. Thequality of nearby shadowcontribu­tions is thereby 
increased. In-scattering of the initial intensity approximation, Is, is illustrated in(e). (a)Opacity 
integration (b)Opacity computed (c)Pixel-wise error, for each point to the with piecewise .E light source 
(5110 ms). integrations (545 ms). Figure 6.8: A comparison of illumination integrated for each point 
to the light source, in (a), with intensities computed with our approximation using piecewise integration, 
in (b). The volume and the SVR are 5123 voxels, the view-port is 1024x1024pixels and no in-scatteringis 
considered. A color-coded errorimage that shows the pixel-wise error, .E, is provided in (c). The errors 
that appear are small(.ERMS = 0.19 and .E6=6%), and mainly appear at sharp edges and thin structures. 
 =0, RO =16 =0, RO =48 =0.2, RO =16 =0.2, RO = 48 (a)Ibias (b)Ibias (c)Ibias (d)Ibias Figure 6.9: Light 
is integrated, for a CT scan of a carp, with varying settings for Ibias and RO. Theshadowsbecomeslessdistinct 
with alargeradius(denotedin units of voxels) of the scattering sphere, O. Ibias adjusts the minimum intensity 
of each voxel. a) 163.1 b) 83.1 c) 43.1 d) 23.1 Figure 6.10: Block artifacts appear due to reduced size 
of the shadow volume representation. The original volume size is 5123 with a data reduction of 8.9:1 
and the SVR is computed for(a)323 (b)643 (c)1283 and(d) 2563 voxels. Jagged edges appearin(a) and(b), 
as canbe seeninthe close ups. Also,band artifacts arise at the side of thehead. Computationtimesfortheseimagesareasshownintable6.1. 
The length used for the piecewise segments is 16 voxels.  Figure 6.12: Work.ow: Inthe .rstpreprocessing 
stagealocalhistogramisgener­ated for each of the n voxels in order to capture the distribution of intensities 
in its environment(1). Then the n local histograms are sorted into m clusters(m<n) through a vectorquantization(vq). 
To acceleratethe vectorquantization,it oper­ates onpackedhistograms. Thepackingisbased on thehistogram 
of the volumetric data set. After the clustering is .nished the packed local histograms are replaced 
by their unpacked counterpartsduring the matching,before computing new cluster representatives(2). During 
rendering thelocalhistogramsrepresenting the clusters are modulated with the transfer function to determine 
an environmental color for each voxel(3). (a)Blinn-Phong (b)our technique Figure 6.13: Ahanddata set(244 
× 124 × 257 voxel) rendered using our surface shading technique(r=24, nc=2048)in comparison to Blinn-Phong 
shading. Notice the shading di.erences in the obscured areas. Figure 6.14: Allfourimagesshowthe sameCornellboxdataset(r=32, 
nc=1024). Forthe rightcolumnthetransferfunctionhasbeenchangedin such a way thatthe blue spheredisappears. 
Thelower row shows theimages withdi.useinterre.ections only(left) andmaterialparameterssettosimulatehighlydi.usesurfacesandan 
additional glow e.ect (right). The images have been rendered interactively by changing the transfer function 
resp. the glow mapping. Notice the color bleeding on the objects, whichdisappearsfor the blue sphere 
when removingit by modifying the transfer function. (a)Blinn-Phong (b)our technique Figure 6.15: Ourinteractive 
volume rendering method(r=20, nc=2048) applied to theVisibleHumanheaddata set(192 × 192 × 110 voxel). 
Both hemispheres, in direction ofthegradientandthe oppositedirection, are consideredduring rendering, 
and thus in contrast toBlinn-Phong subsurface scattering e.ects as well as ambient occlusion in the inner 
parts of the auricle. Course Notes Advanced Illumination Techniques for GPU Volume Raycasting  Volume 
Scattering Markus Hadwiger VRVis Research Center, Vienna, Austria Patric Ljung Siemens Corporate Research, 
Princeton, USA Christof Rezk Salama University of Siegen, Germany Timo Ropinski University of M¨unster, 
Germany  Scattering E.ects 7.1 Physical Background Before we examinehowto model realisticlightpropagationintranspar­ent 
and translucent media,let usstartby looking atthephysical theory of light transport. Inphysics, light 
has been described by three di.erent models: Geometric Optics: Geometric optics consider rays of light. 
Rays are perpendicular to the wavefronts of the actual optical waves. They canbe thought of asthepaths 
oflightparticles(photons). Photons will get re.ected or refracted at the interface between two media 
withdi.erent refractiveindex. Geometric opticsisthe usualwayto describe light transport in computer graphics. 
It fails, however, to accountfor some optical e.ects such asdi.raction andpolarization. Wave Optics: 
Wave optics consider light as electromagnetic radia­tion, as described by Maxwell s Equations. This model 
calculates the amplitude and phase of an electromagnetic wave as is passes through optical systems. It 
can account for di.raction, interfer­ence, andpolarization e.ects, as well as aberrations and other com­plexe.ects. 
Incomputergraphicsthismodelisrarely used mainly because thebene.tshardlyoutweighits computational complexity. 
Quantum Optics: Quantum optics is the fundamental theory of light transport, which accounts for both 
particle and wave characteris­tics of light. It is the only explanation for the photoelectic e.ect as 
noted by Albert Einstein. Although computer graphics mainly deals with geometric optics, there are manyin.uences 
from the other models. TheMonte-Carlointe­gration schemes described in this chapterfor example maybe 
considered a technique which accounts for the probabilistic characteristics of the motions of photons 
as derived by quantum mechanics. 7.2 Scattering For surface rendering, light transportis usually assumed 
to takeplace in the vacuum. The interaction between light and matter is restricted to the surfaces of 
objects. In the vacuum, light travels unimpededly along straightlines(rays). Photons are re.ected at 
surfaces, or refracted at the interface between dissimilar optical media. Scattering is thephysical process 
whichforceslighttodeviatefromitsstraighttrajectory. There­.ection of light at a surface point is thus 
a scattering event. Depending onthe materialproperties of the surface,incidentphotons are scattered in 
di.erent directions and thereby change their energy and frequency, which possibly causes a change in 
color. For rough surfaces the direc­tionin whichindividualphotons are scattered vary within a considerably 
higher range(di.use of Lambertian re.ection) compared with shiny sur­faces(specularre.ection). Thisisthereasonwhy 
we observe material types with a di.erent appearance. 7.3 Single Scattering To explain how scattering 
events are modelled in computer graphics, let s start with something familiar. Simple surface illumination 
may be achieved in computer graphics using single scattering events. Here, Figure 7.1: Simple single 
scattering as modelled by the Phong local illumination model with a single point light source. From left 
to right: In a perfect mirror, incidentlight(green arrow) is re.ected aboutthe surface normal(blue arrow) 
into thedirection ofperfect re.ection(red arrow). With Lambertian re.ection, incident light is scattered 
equally in all directions. With specular re.ection light is scattered inside a specular lobe around thedirection 
ofperfect re.ection. a photon coming from a light source is scattered only once before it reaches the 
eye. In terms of ray-casting we may say that the ray changes its direction only once. Single scattering 
is closely related to local illu­mination. The only di.erence is that local illumination does not include 
shadows, while single scattering may account for occluded light sources as well. In Figure 7.1, single 
scattering events are modelled using the well­known Phong illumination model. For the di.use illumination 
term in­cident light is scattered equally across the entire hemisphere centered about the surface normal. 
For the specular term, scattering is restricted to a specularlobe centered aboutthedirection ofperfect 
re.ection. More complex materialproperties may be modelled using the bidirectional re­.ectance distribution 
function(BRDF). The BRDF fr(x, ., .i,.o) at a single surface point x is a function that de.nes how light 
is re.ected at an opaque surface. The function takes an incoming light direction, .i, an outgoing direction, 
.o and a wavelength . and returns the ratio of re.ected radiance exiting along .o to the di.erential 
irradiance incident from direction .i. The wave­length parameter . is usually omitted. In this case, 
the BRDF returns a chromatic RGB value f r(x,.i,.o). In many applications, and especially real-time applications, 
the local illumination model is used in combination with point light sources, i.e. light is coming from 
exactly one or a few discrete directions. Local illu­mination with aBRDF and a singlepointlight source 
canbeformulated as a rendering equation: Li L(x,.o)= f r(x,.i,.o)cos .i , (7.1) pr2 where L(.o)is the 
radiance travelling from the surface point into direc­tion .o. Li is the emitted radiance of the light 
source, r is its distance from the surface point, and .i is the angle between the surface normal and 
the incoming direction .i. In a more realistic scenario, however, incident light may come from all directions 
on the positive hemisphere O+ centered about the surface normal. The observed radiance L(.o)must thus 
be calculated by inte­gratingtheincoming radiancefrom alldirections .i over thehemisphere: L(x,.o)= f 
r(x,.i,.o)cos .i L(x,.i)d.i. (7.2) O+ In mathematical terms, the rendering equation is a Fredholm equation 
of the second kind, which in general cannot be solved analytically. Nu­ merical solutions exist only 
if the BRDF is conservative, i.e. . .o, x, : | f r(x,.i,.o)| d.i = 1. (7.3) .+ To be physically plausible, 
BRDFs must also be positive de.nite and reciprocal. In chapter 8 we will see how the integral can be 
solved nu­merically usingMonte-Carlointegrationtechniques. TheBRDFnotation can also be modi.ed to account 
for transparent materials. 7.4 Indirect Illumination and Multiple Scattering In computer graphics, single 
scattering accounts for light emitted from a light source directly onto a surface and re.ected unimpededly 
into the observer s eye. Incident light L(.i)in Equation 7.3 thus comes directly from alight source. 
Togenerate more realisticimages, we must account for both indirect light and multiple scattering events. 
Both concepts are closely related, they only di.er in their scale. Indirect illumination means that light 
is re.ected multiple times by di.erent objects in the scene. Multiple scattering refers to the probabilistic 
characteristics of a scattering event causedby aphotonbeing re.ected multiple times within an object. 
7.4.1 Indirect Light The traditional (deterministic) ray tracing technique as shown in Fig­ure 7.2,left 
only accounts for a very special type of indirect illumination: Perfect mirror re.ections and perfect 
refractions in transparent objects. Ray tracing assumes that the path of a photon after emission from 
a light source is deterministic. The ray is traced by accounting for mirror re.ections and refractionuntilithitsapoint 
onasurfacewithdi.use or glossy characteristics. Atthispoint alocalBRDF model(such asBlinn, Phong, etc) 
is evaluated. In comparison, Figure 7.2, right shows a more realistic illumination scenario, which accounts 
for indirect illumination. In the left image the ceiling is completely black. If you look at the ceiling 
you will notice that the left part is rendered slightly red and the right part slightly blue. Thisisbecausephotonsthat 
reach theleftpart of theceiling were mostlikely re.ected from the red wall. This typical indirectillumination 
e.ect is called color bleeding. Another e.ect of indirect illumination is the caustic,thefocussedlightpattern 
causedby thetransparent sphere.  7.4.2 Transparency and Translucency Transparent materials are clear, 
so you can see through them. In con­trast,translucentmaterials(such asfrostedglass,clouds,milk etc.) 
al­lowlighttopassthrough them only di.usely. Since ray-tracing relies on deterministic scattering events, 
it cannot account for translucent media, which requires the consideration of the probabilistic path of 
photons. The ray tracing technique thus neglects many possible paths that light cantakeformthelight sourcetotheeye. 
Ingeneral,thesameistruefor GPU-raycasting of volume data. We assume that the light travels along straight 
lines. Figure 7.3 illustrates the di.erence between transparent and translu­cent material. The .gureshowsaCT 
scanof theUTCTGiantSalaman­der head rendered with di.erent tissue properties. The internal struc­tured 
of a transparent object are clearly visible,possiblydistorteddue to refraction (left). Depending on the 
opticalproperties(phasefunction), light is scattered in translucent materials (middle andright). Ingeneral, 
the exit point of a single photon is non-deterministic in translucent ob­jects. In Chapter 9, we will 
see how such images can generated using Monte-Carlo integration techniques which account for the probabilistic 
nature of light.  7.4.3 Phase Functions Up until now, we have only talked about scattering events at 
surfaces, which canbedescribedby aBRDF(Equation7.3). Inside oftranslu­cent objects andparticipating media, 
scattering eventsareconsidered to potentially happen at every point inside this object. In this case, 
the BRDF fr is replaced by the phase function h, and incoming radiance is integrated over the entire 
sphere O: Lo(x,.o)= h(x,.i,.o)Li(x,.i)d.i. (7.4) O Thephasefunctiondescribesthe scattering characteristics 
of thepartic­ipating medium. Note that the cosine termfromEquation7.3is omitted in Equation 7.4, since 
the phase function directly operates on radiance values rather than di.erential irradiance like the BRDF. 
The most popular phase function models are Henyey-Greenstein, Schlick,Mie andRayleigh(see[41,25,81]). 
The Henyey-Greenstein phase function is a simpli.ed model for ra­diation in the galaxy. It describes 
the probability of a scattering events which changes the direction of a photon by a given angle. Compared 
with other models, the Henyey-Greenstein model does not incorporate any wavelength dependency. Nevertheless, 
it may be used to realisti­cally model the optical properties of many natural phenomena, such as fog 
and clouds. The probability of a photon changing its direction of motion by an angle of . is approximated 
by the Henyey-Greenstein model as 1- g2 G(., g) (7.5) = . 32 (1 + g2 - 2g cos.) The parameter g 
. [-1, 1] describes the anisotropy of the scattering event. A value g = 0 denotes that light is scattered 
equally in all direc­tions. A positive value of g increases the probability of forward scatter­ing. Accordingly, 
with a negative value backward scattering will become more likely. If g = 1, a photon will always pass 
through the point unaf­fectedly. If g = -1it willdeterministically be re.ectedinto thedirection it came 
from. Figure 7.4 shows the Henyey Greenstein phase function for di.erent anisotropy parameters g. We 
can use the probability distribution from the Henyey-Greenstein model to construct a phase function according 
to Equation 7.4 1- g2 h HG(x,.i,.o) (7.6) = 4p (1 + g2 - 2g (.i · .o)) 32 with cos. =(.i · .o). 7.4.4 
Scattering at Transparent Surfaces Thephasefunction modelisidealfordescribing scattering events inpar­ticipating 
media, where the probability of a scattering events depends only on the angle between the incoming and 
outgoing direction, regard­less of their explicit orientation (rotation invariance). In contrast, the 
BRDF at agivenpointdepends onthe orientation of the normal vector of the surface element. If we want 
to render images from volume data obtained by tomographic scans, however, we are not only interested 
in the scattering events in the homogenous regions, but also in the surfaces contained in the volume 
data. In this case we need a phased function whichhas an orientation,justlikethe normal vectorintheBRDF. 
An alternative to using the phase function here is to supplement the BRDF by a transmissive term. For 
semi-transparent surfaces, scat­tering events are still considered to happen only at surface boundaries, 
but light can be transmitted through transparent or translucent mate­rials. The BRDF in Equation 7.3 
is supplemented by a bidirectional transmittance distribution function(BTDF) f t de.ned on the opposite 
hemisphere. Both the BRDF and the BTDF are often considered to­gether as a singlebidirectional scattering 
distributionfunction(BSDF). The BSDF f (x,.i,.o)leads to a rendering equation according to Lo(x,.o)= 
f (x,.i,.o)Li(x,.i)|(n · .i)| d.i, (7.7) O and can easily be convertedtothephasefunction notation(Eq.7.4) 
by incorporating the cosine term: h BSDF(x,.i,.o)= f (x,.i,.o)|(n · .i)|. (7.8) For volume rendering 
in practice, this means that we may account for surfaces inside the data, e.g. by looking at the gradient 
magnitude. If the gradient magnitude exceeds a given threshold ., we can use a BSDF to model the scattering 
events using the normal vector of the isosurface atthatpoint(which coincides withthe normalizedgradient 
vector). In rather homogeneous regions, where the gradient magnitude is small, we model rotation-invariant 
scattering events according to the Henyey-Greensteinphasefunction. Foragivenscalar .eld s(x), suchas 
a tomographic scan of an object, a suitable phase function would thus be Vs(x) h BSDF(x,.i,.o) with 
n = |Vs(x)|, if |Vs(x)| >. h(x,.i,.o)= h HG(x,.i,.o), otherwise Thefreeparameters of the model(e.g the 
anisotropy g in h HG)may be obtained as a function of the scalar value s(x) as well using a transfer 
function. Now we have seen the equations we need to solve to build scattering into our GPU-based volume 
ray-caster. In the following chapter, we will see how we can solve these integrals using Monte-Carlo 
techniques.  7.5 A Practical Phase Function Model In an example in Chapter 9.3 we will use a simple 
phenomenological phasefunction model, whichis equaltotheBSDF at speci.edisosurfaces and otherwise contains 
a simple forward peak or a Henyey-Greenstein term as described above. The parameters of this phase function 
model are derived from the underlying scalar .eld s(x). To keep the model controllable by the user, we 
restrict scattering events to happen at a .xed set of isosurfaces. Between these isosurfaces the ray 
direction does not change, but attenuation may still happen. At the speci.ed isosurfaces, thegradient 
magnitude Vs(x)isguaran­teedtobe non-zero. Thegradientvectoris normalized andits orientation is adjusted 
to match the viewingdirection. Asin mostillumination mod­els we assume the viewing vector v to point 
towards the eye position. Vs(x) if Vs(x)· v = 0 .Vs(x). g(x)= (7.9) Vs(x) - if Vs(x)· v < 0 .Vs(x). Our 
phenomonological BSDF is illustrated in Figure 7.5. The re.ec­tive part fr is equal to the specular and 
di.use term of the Phong local illumination model, fr = fdi. + fspec (7.10) fdi.(v . .i)= kd (n · .i) 
(7.11) fspec(v . .i)= ks (r · .i)s (7.12) with r =2n(n · v)- v. (7.13) The transmissive part scatters 
the transmitted light in an additional Phong lobe centered around the negative viewing vector -v in case 
of non-refractive transmission, ft(v . .i)= kt (-v · .i)q. (7.14) For refractive transmission, the Phong 
lobe is centered around the re­fractedraydirection t (Figure7.5, right). Inthis casethe refractedvector 
t is calculated according toSnell slaw and replaces -v inEquation7.14. Figure 7.3 shows the in.uence 
of the exponent q for the transmission lobe. The resulting BSDF modelhas5parameterstoadjustforeach spec­i.ed 
isosurface: the di.use, specular and transmissive material coe.­cients kd, ks and kt and the exponents 
s and q for the specular and transmissive lobe. 7.6 Further Reading Material properties of translucent 
surfaces, such as skin or paper, are often modeled using the bidirectional surface scattering re.ectance 
dis­tribution function (BSSRDF), which require two surface locations to be speci.ed. Apractical modelhasbeenproposedby 
Jensen et al.[46]. Donner etal.[22]have supplemented this modelfor multi-layered translu­cent materials. 
Interactive rendering technique for translucent surfaces havebeenpresentedby Lensch et al.[59] andCarr 
et al.[10].  Monte-Carlo Intergration Intheprevious section wehave seenthatin ordertogeneratephysically 
plausible images, we need to solve Equation 7.4. In order to obtain the radiance at a given point x in 
a given direction .o, we need to inte­grates the incident radiance over the sphere around the point, 
weighted by the phase function. I have also noted, that this integral cannot be solved analyticallyingeneral. 
We thus need to use numericalintegration schemes. 8.1 Numerical Integration The most popular numerical 
integration scheme is the Riemann sum. Let shave alook at a simple one-dimensional integral of afunction 
f(x), b I = f(x)dx. (8.1) a To solve thisintegral numerically using aRiemann sum, we approximate the 
function f(x) by a piecewise constant function and calculate the area of the rectangularblocks as shown 
inFigure8.1. Theheights of the blocks are obtained by sampling the function in n equidistant intervals 
.x = b-na . An approximation to the integral is thus obtained by b n-1 L b - a b - a I = f(x)dx  f(xi) 
with xi = i · (8.2) a i=0 n n The approximation error of this sum depends on the number of samples n. 
If n approaches in.nity, the approximation error will become zero. 8.1.1 Blind Monte-Carlo Integration 
What happens if we modify the Riemann sum, such that the function is sampled at randomized locations, 
as shown in Figure 8.1 right?. Let Figure 8.1: A function f(x) (left) may be integrated by a Riemann 
sum, which approximates the function using piecewise constant blocks (middle). Alternatively, blind Monte-Carlo 
integration will sampling (right) samples the volume at random positions. us assume that the samples 
are taken with a uniform probability distri­bution, which means that the probability of each sample position 
xi is constant. We will use the sameformula(Eq.8.2) to estimate theinte­gral, however, the sample position 
x . [a, b] now is a random variable with a uniform probability dendisty p(x): n-1 L b - a 1 (I) = f(x) 
with p(x)= (8.3) nb - a i=0 To prove that such the estimator (I) is useful, we must show that it is unbiased, 
i.e that the expectation value of (I) is equal to the exact solution of the integral: n-1 L b - a E[(I)]= 
E[ f(x) ]= n i=0 n-1 b - a L = E[f(x)]= n i=0 b - a n-1 L b = f(x)p(x)dx = n i=0 a b - a n-1 L b 1 = 
n i=0 a f(x)b - a dx = 1 n-1 L b = f(x)dx = n i=0 a b = f(x)dx = I (8.4) a We have now shown that the 
estimator (I) is unbiased and converges against the integral sought-after. This means that if the number 
of sam­ples n approaches in.nity, the estimation (I) will converge against the exact value of the integral, 
just like the Riemann sum. The practical meaning of this statement, however, is limited, because we have 
not yet investigated what happens if we use a .nite number of samples n. Ifyou compareFigure8.1middle 
and left,yourintuitionis right, that the stochastic estimatoris not asgood as theRiemann sum. Indeed, 
one canshowthatforagrowing number of samples n, the approximation er­ror of theRiemann sumdecreases muchfaster 
than the error of stochasic estimator (I). The Riemann sum converges faster to the exact solution. The 
estimator (I) we have just seen is called a blind Monte-Carlo estimator. In the following section we 
will see that there are advantages of the blind Monte-Carlo estimator compared to the Riemann sum. The 
estimatoris called blindbecauseitdoes not make any assumptions about the function f(x) to be integrated. 
In Section 8.3 we will also see how the convergence of the blind Monte-Carlo estimator can be improved 
by using a-priori information about the function f(x)  8.2 When Does Monte-Carlo Integration Make Sense? 
In the previous section we have seen, that the blind Monte-Carlo es­timator requires much more samples 
to achieve the same accuracy as the Riemann sum. Nevertheless, there are cases, especially in computer 
graphics, where the Monte-Carlo estimator is more e.cient than the Riemann sum. The blind Monte-Carlo 
estimator has two important ad­vantages: Noise instead of aliasing: If the function f(x) contains high 
frequencies, such as the pattern in Figure 8.2 top row, equidistant sampling will inevitably lead to 
aliasing artifacts as can be seen in the right column of the .gure. Even if the number of samples is 
increased by a factor of 16, aliasing will still be strongly visible (bottom left). Stochastic sampling 
may have a higher approxima­tion error, nevertheless theimages are visually morepleasing, since the human 
eye is less sensitive to noise than to aliasing artifacts.  Independence of any grid structure: The 
second bene.t of stochastic sampling in general is its independence of a grid struc­ture. This is best 
explained using an example: Let us assume we  are going to integrate a three-dimensional function instead 
of the one-dimensional one in our example. We may try to integrate the function using a Riemann sum with 
10 × 10× 10 =1000 samples. Now if we .nd out that the appoximation error is still too large, we need 
to increase the number of samples. The next possibility we have is to use 11 × 10× 10 samples, so we 
will have to evaluate 100 more samples in order to increase the accuracy. Since stochas­tic sampling 
does not require any sampling grid, we can increase the samples in steps of 1. Stochastic sampling is 
thus advanta­geous if the function to be integrated is high-dimensional and the evaluation of a sample 
is rather expensive. In Figure 8.2 we have already seen the visual bene.t of blind Monte-Carlo sampling. 
Stochasic sampling, however, may be further improved by integrating partial knowledge about the function 
to be integrated intothe estimationprocess. We willdiscussthisinthefollowing section which deals with 
importance sampling. 8.3 Importance Sampling In our simple exampleinSection8.1.1we used a uniformprobabilityden­sityfunction(PDF) 
fordetermining the samplepositions x (see Equa­tion 8.3). Monte-Carlo integration, however, may be performed 
with an arbitrary PDF p(x). The general Monte-Carlo integration formula is: L 1 n-1 f(x) (I) = (8.5) 
np(x) i=0 We can easily show that this is a useful estimator again by calculating its expectation value: 
L 1 n-1 f(x) E[(I)]= E[ ]= np(x) i=0 L 1 n-1 b f(x) = p(x)dx = n a p(x) i=0 n-1 L b 1 = f(x)dx = n i=0 
a b = f(x)dx = I (8.6) a Now, the question is: Can we .nd a PDF p(x) which increases the convergence 
of the estimator (I)? It turns out that the convergence of the Monte-Carlo estimator is optimal, if theprobability 
density function p(x)is a multiple ofthefunction f(x)itself. Inpractice this makes sense: We need to 
place more samples at positions x where the function f(x) is large, and less samples where the function 
is small. This way we will focus the computational load to where the largest contribution to the integral 
is expected. However, we cannot use the function f(x)directly as a PDF because it needs to be normalized: 
b f(x)dx p(x) = c · f(x) and p(x)dx = 1 (8.7) a This leads us to: 1 1 c = b = I (8.8) a You might notice 
the .aw: Calculating the constant c to normalize the ideal PDF would require us to know the sought-after 
integral I in ad­ Figure 8.3: A function f(x) with a sharp and narrow peak (left). Equidistant sampling 
of the Riemann sum might easily miss the peak. Based on a-priori infor­mation about the function f(x), 
Monte-Carlo integration will place manysamples at locations close to the peak. vance. Nevertheless, even 
if we have only partial knowledge of the func­tion to be integrated, we can signi.cantly improve the 
convergence of the estimator. Now, let us .nally go back to our rendering equation: Lo(x,.o)= h(x,.i,.o)Li(x,.i)d.i. 
O Although we do not know the integral completely, we have knowledge about the phase function h(x,.i,.o). 
We should thus shoot many rays into directions .i where the phase function h is high, and less samples 
where the phase function is low. Also, if we know anything about the lightingenvironment, we should shoot 
many raysintodirections .i where L(.i) is expected to be large. This concept is known as importance sampling 
and signi.cantly increases the convergence of the Monte-Carlo estimator. Now you may see another important 
bene.t of Monte-Carlo integra­tion over theRiemann sum: Try tointegrate afunction f(x)whichhas a known 
very sharp and narrowpeak(Figure8.3). SinceRiemannintegra­tion does not take into account a-priori information 
about the function f(x), its equidistant sampling might easily miss the narrow peak com­pletely. If the 
location of the peak is known in advance, Monte-Carlo integration may place many samples at location 
near the peak and ap­proximate the .at regions with fewer samples. With Riemann integra­tion, the boxes 
in Figure 8.3 all have equal width. With Monte-Carlo estimation (see Eq. 8.5), the width wi of a box 
in Figure 8.3, right, is depending on the probability of the sample: 1 wi = (8.9) n · p(xi) This means, 
that the more likely a samples position xi is, the more narrow is the box representing the sample. 8.4 
GPU-based Importance Sampling In this section we arehaving alook atdi.erentimplementation ofimpor­tance 
sampling on theGPU. In Chapter 9 we will seepractical examples which apply these strategies for GPU-based 
volume ray-casting. 8.4.1 Focussing of Uniform Distribution Afast and simpleGPU-based techniqueforimportance 
sampling, which is straight-forward to implement, has been used in [88]. Here, pre­computed random directions 
uniformly distributed on the unit sphere are used as basis. Simple but e.ective strategies to omit regions 
with only little contribution according to the phase function or BSDF are employed. To avoid the necessity 
to account for di.erent probability distribu­tions within a shader program, this approach restricts itself 
to uniform distributions. In Section 8.4.2 we will see that the solid angle .i of a sampledepends onitsprobability,justliketheboxesinFigure8.3 
right. Uniform samples are admittedly not the optimal sampling schemes, but they allow us to remove p(x)from 
the sum in Equation 8.5 and re­placethe weightedsumby a simple averagefor e.ciency. InSection8.4.2 we 
will see a GPU-based implementation of importance sampling which deals with di.erent pdfs in the shader. 
8.4.1.1 Rejection Sampling For afast access to randomized direction vectors from within afragment shader, 
a set of random value triplets representing points uniformly dis­tributed on the unit sphere is pre-computed. 
We generate such vectors by the use of rejection sampling: We obtain triplets rS of uniformly dis­tributed 
random valuesinthe range of[-1, 1]. We discard all vectors with a magnitude larger than 1 and normalize 
the remaining vectors to unit length. The pre-computed random vectors are stored in a 2D and 3D texture. 
By sampling the texture at runtime, we can generate samples uniformly distributed on the unit sphere. 
 8.4.1.2 Focussing The randomdirections obtainedfrom the texture candirectlybe used to sample the phase 
function. For di.use, surface-like re.ection, however, itis necessary to restrict the randomdirections 
to ahemisphere centered around a given unit vector n. We can easily generate such samples by negating 
all random vectors outside the given hemisphere, rH(n) = sgn(n · rS)rS, (8.10) with sgn being the signum 
function. For e.ciently sampling a specular Phong lobe, we need to focus the sampling directions to a 
narrow cone centered around a given direction of re.ection. A simple way of focussing ray directions 
is to compute a weighted sum of the hemispherical random samples rH and thedirection of perfect re.ection 
h: rP(h)= a · rH(h) +(1 - a)h. rP(h) rP(h) = (8.11) . rP(h). The scalar weight a determines the maximum 
cone angle of scattering around the direction h. A value a =1 means scattering in all directions onthehemisphere, 
while a value of a =0resultsinthe(non-randomized) ray direction perfectly focused into direction h. Todetermine 
an appropriate value of a for agiven specular exponent s, we calculate the maximum re.ection angle . 
max, at which the specular term falls below a user-speci.ed threshold T (say 0.1), . max(s) = max{. | 
cos(.)s > T }. (8.12) Solving this equation yields v . max = arccos(s T ). (8.13) Figure8.4illustrates 
the relationshipbetween thefocus weight a and the angle .. The maximum angle between a hemispherical 
sample rH and the re.ection direction h is p 2. The interpolation according toEqua­tion 8.11 moves the 
point along the dotted line and the normalization raises to interpolated point back to the hemisphere. 
From Figure 8.4, it is easy to derive a relationship between a and the maximum angle . max by - p)1+tan(. 
max a = 4(8.14) 2 The three sampling techniques outlined in this section should be suf­.cient to e.ectively 
increase the convergence of the Monte-Carlo esti­mator. Importance sampling requires knowledge about 
the scattering distribution at the surfaces. Which sampling strategy to use depends, of course, on the 
phase function model. The drawback of this technique is that the threshold introduced in Equation 8.12 
results in a Monte-Carlo estimator which is no longer un­biased. This means that the estimator will not 
converge against the correct solution of the integral. Instead a small error is introduced by truncating 
the specular lobes. Nevertheless, the described technique al­lows us to e.ciently generate images at 
high visual quality, as will be shown in Chapter 9. A mathematically more accurate solution will be discussed 
in the following section.  8.4.2 Sampling of Re.ection MIP-Maps An e.cient implementation of importance 
sampling has been published in volume3 oftheGPUGems[14]. Thisimplementation utilized area­preservingparameterizations 
ofdi.erentBRDF models[81]. 8.4.2.1 Area-Preserving Parameterizations The Phong specular lobe centered 
around the re.ection vector r, for ex­ample, maybe writteninpolar coordinates(f = azimuth, . = elevation): 
fspec(.i,fi)= cos n .i sin .i. (8.15) du = r d. dv = r sin. df dA = = du dv r 2 sin . d. df d. = dA r2 
= sin. d. df  The additional sine term is introduced by changing from solid angle parameterization of 
the hemisphere to polar angle parameterization as outlined in Figure 8.5: d. = sin. d. df (8.16) To calculate 
the probability density function for sampling the specular lobe, we need to normalize the lobe to integrate 
to one: cosn .i sin.i p(.i,fi)= 2p p/2 00 cosn .i sin .i d. df (n +1) = cos n .i sin.i (8.17) 2p Notethatthisfunctionisindependent 
of theazimuth angle f because of rotational symmetry. We can thus split the PDF into two independent 
equations for f and .: p(.i) = (n +1)cosn .x sin .i (8.18) 1 p(fi) = (8.19) 2p Inpractice, we caneasilygenerate 
randomvariables . . [0, 1] with a uniformdistribution(e.g. using the rand() function in C/C++). Note 
that Equation 8.19 is already a uniform distribution within the range [0, 2p]. Now we need amapping fromauniformdistributiontothePDF 
in Equation 8.18. We calculate the inverse of the cumulative PDF: .i P (.i)= p(.)d. 0 (n+1) .i = cos(8.20) 
 1 ) P -1(.) = arccos.(n+1(8.21) Now, if we have a pair of uniformly distributed random variables (.1,.2). 
[0, 1]× [0, 1], we can generate a random direction: 1 ( n+1) .i = arccos .1 (8.22) fi =2p.2 (8.23) We 
can then convert the polar coordinates back to direction vectors in cartesian coordinates. Since these 
samples are notuniformlydistributedlikein thefocussing approach, we need totakeinto account the solid 
angle .i for eachsample: 1 .i = . (8.24) n · p(.i,fi) Compared to thefocussing approach, we willhave 
to weight each sample byits corresponding solid angle: Wedivide each sample(.i,fi) by its probability 
p(.i,fi)before averaging the samples. 8.4.2.2 Re.ection MIP-Map Inthe approachdescribedin[14], a mip-mappedparabolic 
environment map is used to store the incident light averaged within di.erent solid angles. The MIP-levels 
of the re.ection map contain pre-.ltered envi­ronment maps[40]forapre-de.ned rangeof solid angles. TheMIP-level 
for a texture sample is then calculated as a function of its solid angle (Eq. 8.24). This restricts the 
technique to local illumination of surfaces or isosurfaces. Furthermore, .ltering of environment maps 
is restricted to BRDFs with a rotational symmetric specular lobe. 8.4.2.3 Filtering Environment Maps 
The .ltering of an environment map, however, can also be performed using Monte-Carlointegration. Wecaneasilygeneratea 
.ltered version Figure 8.6: Examples of .ltered environment maps: Original environment map (left). Filteredversion 
with a specular exponent s =100((middle)). Irradiance map created by .ltering over the entire hemisphere 
(right). (Shader code in Listing 8.1) of an environment map by rendering a screen-spaced quad (with the 
correct normal vectors speci.ed at the vertices) into the six faces of a cubemap(render-to-texture). 
The .lteringisperformedbythefragment shader shown in Listing 8.1. This shader uses the focussing technique 
from Section 8.4.1. For a given specular exponent s, the parameter focus must be calculated according 
to Equation 8.13. The shader reads from an arbitrary environment cube map and a pre-de.ned 3D texture 
containing unit random directions calculated using rejection sampling. Examples of .ltered environment 
maps are shown in Figure 8.6.  8.5 Further Reading In this chapter we have covered the theory of Monte-Carlo 
integration to an extent which is su.cient to understand the rest of the course. More information on 
Monte-Carlo integration can be found in the book by Pharr and Humphries[81], or in the SIGGRAPH 2002 
course notes on Advanced Global Illumination by Philip Dutr´e, Kavita Bala and Philippe Bekaert. 122 
Course : Advanced Illumination Techniques for GPU Volume Raycasting 1 #define NUMSAMPLES (200.0) 2 3 
float4 main( 4 float2 uv : TEXCOORD0, 5 float3 normalIn : NORMAL, 6 uniform float4 randSeed, 7 uniform 
float focus, 8 uniform sampler3D noiseTex, 9 uniform samplerCUBE envMap 10 ) : COLOR 11 { 12 float3 N 
= normalize(normalIn); 13 float4 sample = 0.0; 14 15 for(float i = 0; i < NUMSAMPLES; ++i) { 16 17 // 
calculate a randomized 3D texture coordinate 18 float offset = randSeed.a * i/NUMSAMPLES; 19 float3 randUV 
= (NormalIn + offset*randSeed.xyz); 20 21 // sample a precalculated 3D noise texture 22 // to obtain 
a randomized sampling direction 23 float3 randDir = expand(tex3D(noiseTex,randUV)); 24 randDir = normalize(randDir); 
25 26 // invert direction of back-facing 27 float cosTheta = dot(randDir,N); 28 if cosTheta < 0.0) { 
29 randDir = -randDir; 30 cosTheta = -cosTheta ; 31 } 32 33 // focus the direction to specular lobe 34 
randDir = normalize(lerp(randDir,N,focus)); 35 36 // sample the environment cube 37 sample += cosTheta 
* texCUBE(envMap,randDir); 38 } 39 // average all samples 40 sample /= NUMSAMPLES; 41 42 return sample; 
43 44 } Listing 8.1: Cg fragment shader for .ltering of environment maps. The shader is intended to 
read from an environment cube map and render into the faces of a re.ection cube map.  GPU-Based Monte-Carlo 
Volume Raycasting 9.1 Monte-Carlo Techniques for Isosur­faces In the .rst part of these course notes 
we have seen the basic implemen­tation of GPU-based ray-casting. For each pixel it casts a ray into the 
volume and samples the volume on a straightline. Ifthe volumeis stored in a single 3D texture, however, 
we have the freedom to modify the ray direction to accountfor multiple scattering events with respect 
to agiven phase function. We then of course need to cast multiple rays per pixel. As an initial implementation, 
we may modify the fragment program for GPU-raycasting to calculate the .rst directional derivative of 
the scalar .eld along the viewing ray using central di.erences. Let s(x)be the scalar .eld, and xi,i 
.{0, 1,...m} be the sample positions along the viewing ray(i =0 is closest to the eye): ds(x) s(xi+1)- 
s(xi-1) = Vs(x)· v (9.1) dv xi+1 - xi-1 If the magnitude of the .rst derivative is larger than a speci.ed 
threshold, we assume a surface scattering event according to the phase function model described inSection 
7.5. Weprocess the scattering event by calculating a randomized direction using importance sampling and 
scatter the ray into this direction. The user-speci.ed threshold restricts strongdirectional changes 
toisosurfaces with ahighgradient magnitude, while rays may pass forward through rather homogenous regions 
of the volume. We restart the fragment program for GPU-based raycasting multiple times with di.erent 
random values. The program samples the volume at equidistant positions along the ray and integrate the 
phase function while the ray travels through the volume. When the viewing ray leaves the volume s bounding 
box, the incident radiance is sampled from an environment cube map. The accumulated radiance RGB triplet 
is .nally written into the frame bu.er and averaged with the previous passes. This straight forward implementation 
has the problem that many calculations,such asdeterminationof the .rstscattering event,areper­formed 
again and again in successive passes. To improve this, we use a multi-pass rendering technique to reuse 
as much information aspossible. The di.erent rendering passes are described in the following sections. 
 First Hit Pass: The .rst pass calculates the intersection of the viewing rays with the .rst isosurface. 
It is reused in subsequent passes to start viewing rays directly at the relevant position.  Local Illumination 
Pass: The local illumination pass utilizes Monte-Carlo integration techniques to calculate surface shading 
with shift-variant BRDFs.  AmbientOcclusionPass: The ambientocclusionpass calculates self-shadowing 
of the isosurface.  Scattering Pass: The scattering pass accounts for translucency and transparency. 
  The .rstthreepassestogethermay beusedtogeneratehigh-quality renditionsofisosurfaces, such astheonedisplayedinFigure9.1 
atinter­active frame rates. Thesepasses will be explained indetailin thefollow­ing Section. The scattering 
pass is more computationally expensive and willbedescribedinSection9.3. It maybepreviewed atinteractive 
rates with a reduced quality (more noise due to fewer samples). Rendering the scattering pass in .nal 
quality will take up to a few seconds. 9.2 Isosurfaces with Shift-Variant or Anisotropic BRDFs The process 
of rendering high-quality shaded isosurfaces can be divided into three di.erent tasks: 1. Extraction 
of the isosurface. This includes the determination of position, normal direction and other surface properties 
obtained from the scalar .eld. 2. Localilluminationof thesurface. Thisincludesevaluationof shift­variant 
BRDFs at the surface. 3. Shadow calculation. We will use Monte-Carlo integration to calcu­late ambient 
occlusion.  Each task is performed in a separate rendering pass as outlined in the following sections: 
9.2.1 First Hit Pass In a .rst rendering pass the front faces of the bounding box are raster­ized. This 
.rst pass simultaneously renders into two .oating-point o.­screen bu.ers using multiple-render-targets. 
The basic fragment shader is shown in Listing 9.1. The shader contains a loop that successively samples 
the volume along the viewing ray. The user has speci.ed the scalar values of an isosurface. While sampling 
the volume along the ray, we continuously check if the speci.ed isosurfaces was intersected. (In the 
sample code, we assume that the scalar value of the .rst sample is below the isovalue) If the .rst isosurface 
is hit, the shader breaks out of the loop. The 3D texture coordinate of the intersection point is written 
tothe .rstrendertarget. Thegradientmagnitudeiscalculated,and the vector is then normalized. Gradient 
direction and magnitude are stored 126 Course : Advanced Illumination Techniques for GPU Volume Raycasting 
 1 uniform sampler3D VolumeTexture : TEXTURE0; 2 uniform float isoValue; 3 uniform float4x4 World2Tex; 
4 struct COLOROUT { 6 float4 value0 : COLOR0; 7 float4 value1 : COLOR1; 8 }; 9 COLOROUT main(float3 uvw 
: TEXCOORD0, 11 float3 vecCameraPos : TEXCOORD1, 12 float3 vecRayPos : TEXCOORD2) 13 { 14 COLOROUT retval; 
 16 float4 firstHit = 0..xxxx; 17 float4 gradient = 0..xxxx; 18 // ray direction 19 float3 dirRay = normalize(vecRayPos-vecCameraPos); 
 21 for(int j=0;j < MAX NUM SAMPLES; j++) { 22 // sample volume at ray position 23 float3 uvw = mul(World2Tex,float4(vecRayPos,1.0)); 
24 float sample = tex3D(VolumeTexture, uvw).r; // check for hit with isosurface 26 if (sample > isoValue) 
{ 27 // calculate gradient 28 float3 grad = getGradient(uvw); 29 gradient.a = length(grad); gradient.rgb 
= normalize(grad); 31 32 firstHit.rgb = uvw; 33 break; 34 } // check if ray exits the volume without 
intersection 36 if ((uvw.x < 0.0) || (uvw.y < 0.0) || (uvw.z < 0.0) || 37 (uvw.x > 1.0) || (uvw.y > 
1.0) || (uvw.z > 1.0)) { 38 break; 39 } // proceed one step along the ray 41 vecRayPos += STEPSIZE*dirRay; 
42 } 43 44 retval.value0 = firstHit; retval.value1 = gradient; 46 47 } Listing 9.1: A basic Cg fragment 
shader for determining the .rst hit with a speci.ed isosurface. A possible implementation of the function 
getGradient in line 28 can be found in Listing 9.2. 1 float3 getGradient(float3 rayPos) { 2 3 float3 
grad; 4 grad.x = tex3D(VolumeTexture,rayPos + float3(EPSILON,0.0,0.0)).r - 5 tex3D(VolumeTexture,rayPos 
-float3(EPSILON,0.0,0.0)).r; 6 grad.y = tex3D(VolumeTexture,rayPos + float3(0.0,EPSILON,0.0)).r - 7 tex3D(VolumeTexture,rayPos 
-float3(0.0,EPSILON,0.0)).r; 8 grad.z = tex3D(VolumeTexture,rayPos + float3(0.0,0.0,EPSILON)).r - 9 tex3D(VolumeTexture,rayPos 
-float3(0.0,0.0,EPSILON)).r; 10 frad /= (2.0*EPSILON); 11 return grad; 12 } Listing 9.2:Cg shaderfunctionfor 
estimating thegradient vector of the volume using central di.erences. 1 // interval bisection 2 float3 
start = vecRayPos-STEPSIZE*vecStep; 3 float3 end = vecRayPos; 4 5 for(int b = 0; b < 10; ++b) { 6 float3 
testPos = (start+end)/2.0; 7 float3 uvw = mul(World2Tex,float4(testPos,1.0)); 8 if (tex3D(VolumeTexture, 
uvw).a < isoValue) { 9 start = testPos; 10 } else { 11 end = testPos; 12 } 13 } 14 15 vecRayPos = (start+end)/2.0; 
16 uvw = mul(World2Tex,float4(vecRayPos,1.0)); 17 Listing9.3:Cg shaderfunctionforimproving theaccuracy 
of theisosur­face detection using interval bisection. as a .oating-point RGBA quadruplet in the second 
render target and the fragment program terminates. The gradient vector of the scalar .eld is always perpendicular 
to the isosurface. It may be estimated numerically using .nite di.erences scheme. Listing 9.2 shows a 
working implementation. The function getGradient obtains six additional texture samples around the inter­section 
point to estimate the gradient vector using central di.erences. Foradetailedderivation of thegradient 
estimation scheme, seeChapter 5.3.1 in[25]. To improve accuracy of the isosurface detection, a few iterations 
of interval bisection may be performed to come closer to the exact inter­section point, as suggested 
in [38]. If the .rst sample larger than the isovalue is found, we go half a stepsize back and see if 
this sample is already larger than the isovalue. We proceed this way several times, halving the stepsize 
with each iteration. A code sample for interval bi­section is shown in Listing 9.3. It may be inserted 
in Listing 9.1 right after line 21. Thecontentsof thetworendertargetsforthe .rst-hitpassareshown in 
Figure 9.2. Successive rendering passes start the ray integration di­rectly at the intersection point 
with the .rst isosurface by reading the 3D texture coordinate determined in the .rst pass. 9.2.2 Deferred 
Shading Pass Given the contents of the two rendering targets from the .rst hit pass, the surface shading 
for the isosurface may be calculated by reading the surface normal(the normalizedgradient vector) fromthe 
second render target and evaluating a local illumination model, such as Phong with point light sources. 
If we want to account for environment light (e.g. stored in a cube map) we need to di.erentiate between 
shift-invariant BRDFwhose re.ectancebehaviour can easilybeprecomputed, and shift­invariant BRDF which 
require Monte-Carlo integration to calculate the scattering events at run-time. A shift invariant BRDF 
is a function f (x,.i,.o)(see Equation 7.2) which does not depend on the position x, i.e. the re.ectance 
properties ACM SIGGRAPH 2009 1 #define NUMSAMPLES (50.0) 2 3 float4 main( 4 float2 screenPos : TEXCOORD0, 
5 uniform float3 cameraPos, 6 uniform float4 randSeed, 7 uniform sampler2D firstHitPos, 8 uniform sampler2D 
firstHitGrad, 9 uniform samplerCUBE irradianceMap, uniform samplerCUBE environmentMap, 11 uniform sampler3D 
noiseTex 12 ) : COLOR 13 { 14 // read the information from previous pass 15 float4 rayPos = tex2D(firstHitPos, 
screenPos); 16 float4 gradient = tex2D(firstHitGrad, screenPos); 17 // focus parameter is the gradient 
magnitude in this example 18 float focus = gradient.w; 19 float3 V = normalize(rayPos.xyz-cameraPos); 
21 float3 N = normalize(gradient); 22 float3 R = reflect(V,N); 23 24 float3 diffuse = texCUBE(irradianceMap, 
N); 25 float3 specular = 0.0; 26 27 for(float i = 0; i < NUMSAMPLES; ++i) { 28 29 // calculate a randomized 
3D texture coordinate float offset = randSeed.a * i/NUMSAMPLES; 31 float3 randUV = (rayPos + offset*randSeed.xyz); 
32 // sample a precalculated 3D noise texture 33 // to obtain a randomized sampling direction 34 float3 
randDir = expand(tex3D(noiseTex,randUV)); 35 randDir = normalize(randDir); 36 // invert direction of 
back-facing 37 float cosTheta = dot(randDir,N); 38 if cosTheta < 0.0) { 39 randDir = -randDir; cosTheta 
= -cosTheta ; 41 } 42 // focus the direction to specular lobe 43 randDir = normalize(lerp(randDir,R,focus)); 
44 // sample the environment cube 45 specular += cosTheta * texCUBE(environmentMap,randDir); 46 } 47 
// average all samples 48 specular /= NUMSAMPLES; 49 return diffuse + specular; 51 52 } Listing 9.4: 
Cg shader function fordeferred shading using a shift-variant Phong BRDF. The di.use term is sampled from 
apre-.ltered irradiance map. The specular term is calculated using stochastic sampling of an environment 
map are contantfor the entire surface. Toimplement a shift-invariantBRDF, we can use an irradiance map 
for the di.use term and a pre-.ltered re.ection map forthe specularterm(seeSection8.4.2.3), assuming 
that the specularlobehas rotational symmetry. For a shift-variantBRDF,the size of the specularlobe variesfordi.erent 
surfacepoints x. For a shift­invariant, but anisotropicBRDF, the shape of the specular lobedepends on 
the local orientation. In these cases, only the di.use term may be pre-.ltered, the specular term cannotbe 
obtainedbya singlepre-.ltered re.ection map. We therefore modify Listing 8.1 to perform the Monte-Carlo 
integration directly on the surface. An example implementation is shown in Listing 9.4. The di.use term 
is simply looked up in an irradiance cube map using the normal direction as texture coordinate. The specular 
exponent represented by thefocusparameter(seeEq.8.11) inthis exampleis settothegradient magnitude, which 
resultsin a shift variant specular term. TheMonte-Carlointegrationisbasicallythe same as in Listing 8.4.2.3. 
The only di.erence is that the specular lobe is focussed onto the re.ection vector R instead of the normal 
N. The result of the deferred shading pass is shown in Figure 9.3, left. 9.2.3 Deferred Ambient Occlusion 
Pass Using the results from the .rst hit pass, we can as well calculate a de­ferred ambient occlusion 
pass for the isosurface. The fragment shader is ACM SIGGRAPH 2009 1 #define NUM SAMPLES (32.0) 2 #define 
FACTOR RAYSTEP (3.0) 3 #define MAX NUM RAY STEPS (10.0) 4 5 float4 main( 6 float2 screenPos : TEXCOORD0, 
7 uniform float isoValue, 8 uniform float4 randSeed, 9 uniform sampler2D firstHitPos, uniform sampler2D 
firstHitGrad, 11 uniform sampler3D volumeTex, 12 uniform sampler3D noiseTex 13 ) : COLOR 14 { 15 // read 
the information from previous pass 16 float4 rayPos = tex2D(firstHitPos, screenPos); 17 float4 gradient 
= tex2D(firstHitGrad, screenPos); 18 19 if (gradient.w < EPSILON) return 0..xxxx; 21 float ambOcc = NUM 
SAMPLES; 22 23 float3 pos; 24 float3 dir; 25 float3 dirStep; 26 27 for(float i = 0; i < NUM SAMPLES; 
i+=1.0) { 28 29 // calculate a randomized sampling direction float offset = randSeed.a * i/NUMSAMPLES; 
31 float3 randUVW = (rayPos + offset*randSeed.xyz); 32 float3 randDir = expand(tex3D(noiseTex,randUVW)); 
33 if (dot(randDir,N) < 0.0) randDir = -randDir; 34 35 // we do not really need to sample the full 180 
degrees 36 randDir = normalize(randDir + 0.1*N); 37 // we sample with a larger stepsize for efficiency 
38 dirStep = randDir * FACTOR RAYSTEP * STEPSIZE; 39 // ensure the first sample point is outside 41 pos 
= rayPos + dirStep; 42 43 for(float s = 0; s < MAX NUM RAY STEPS; s+=1.0) { 44 float value = tex3D(volumeTexture, 
pos).r; 45 if (value > isoValue) { 46 ambOcc -= 1.0; 47 break; 48 } 49 pos += dirStep; } 51 } 52 53 return 
ambOcc.xxx; 54 } 55 Listing9.5:Cg shaderfunctionfordeferredisosurfaceambient occlusion with Monte-Carlo 
sampling shown in Listing 9.5. In this example ambient occlusion is calculated in the local environment 
of the isosurface point. Starting from the surface point, random rays are shot across the hemisphere. 
For each ray, we check if the isosurface is hit again. The intensity value is the fraction of rays which 
do not hit the isosurface again. The result of the deferred ambient occlusion pass is shown in Figure 
9.3, right. The deferred shading pass and the ambient occlusion pass may be multiplied together for each 
pixel, resulting in a high-quality rendition of the isosurface. In the example imagein Figure9.4 showing 
the skin of theUTCTVeiledChameleonCT scan, the shininess ofthe surfaceispro­portional to the gradient 
magnitude at each surface point. In this data set, high gradients of the isosurface are mainly caused 
by bone struc­turesbeing closetothe skin surface. Ifhighest renderingperformanceis required, the number 
of samples for both the deferred shading pass and the ambient occlusionpass mightbe reduced, which still 
resultsingood quality images as shown in Figure 9.5.  9.3 Volume Scattering In chapter7.4 wehave seendi.erentphasefunction 
models which canbe build into our GPU-based volume ray-caster. For volumetric scans, we want todi.erentiatebetween 
scattering at surfaces and scattering within rather homogeneous regions of the volume. This means that 
we approx­imate the directional derivative along the ray using .nite di.erences and change the behaviour 
of the phase function accordingly. While such an implementation is not very di.cult, in practice, the 
visual appearance of this technique is hard to control by the user. We therefore suggest to directly 
specify a set of isovalues representing the isosurfaces at which the BSDF should be evaluated. Inbetween 
these isosurfaces aHenyey-Greensteinphase function or a simple forwardpeek with attenuation will be used. 
Again,using theresultsof the .rsthitpassfromtheprevioussection, we now cast rays from the .rst isosurface 
into the interior of the object. Thefragment shaderfor such a scatteringpass willbebasically the same 
as Listing 9.1. The only di.erence is that we start the rays directly at the .rstisosurface(reading thestartingpointsfromthe 
.rstpass) and check for intersection of multiple iso surfaces simultaneously. The code sample in Listing 
9.6 shows a function which checks for multiple isosurface intersections. In this example we have speci.ed 
four di.erent isovalues si,i .{1, 2, 3, 4}. These values are written in ascend­ing order( si <s i+1)into 
the vector components of a float4 value. The function computeIsoIndex takes a sample s and returns a 
float4 value which contains a 1 in the respective vector component i, if the sample is inbetween si and 
si+1, and0 otherwise. Iftheisovalues si for example are isoValues =(0.1, 0.3, 0.5, 0.7) and the sample 
value s =0.4, this func­tion will return a vector isoIndex =(0.0, 1.0, 0.0, 0.0). To determine an isosurface 
intersection for successive samples si along the ray, we need to compare the isoIndex of the current 
sample to the previous sample. If the dot product between both isoIndices is zero, we have found an intersection 
with an isosurface. If there is no isosurface intersection we may proceed the sampling in one of the 
following ways: ACM SIGGRAPH 2009 1 float4 computeIsoIndex(float sample) { 2 3 float4 a; 4 5 a.x = (sample 
>= isoValues.x)? 1.0 : 0.0; 6 a.y = (sample >= isoValues.y)? 1.0 : 0.0; 7 a.z = (sample >= isoValues.z)? 
1.0 : 0.0; 8 a.w = (sample >= isoValues.w)? 1.0 : 0.0; 9 10 return a -float4(a.yzw,0.0); 11 12 } Listing 
9.6: Cg shader function for calculating the isoIndex, which is used to check for intersection with four 
isosurfaces simultaneously.  process the next sample along the ray without changing the direc­tion of 
the ray(no scattering). This refersto aHenyey-Greenstein phase function with an anisotropy parameter 
g = 1. (Figure 9.6, top left)  process the next sample along the ray without changing the ray direction, 
but attenuate the radiance value. This refers to an ab­sorption of light energy by the volume (Figure 
9.6, bottom left) The radiance along the ray is multiplied by an attenuation factor t . [0, 1], similar 
to alpha blending. The attenuation factor may be a either a constant(e.g. t =0.99) or a function of the 
scalar .eld(implemented using a1D texturelookup).  scatter the next sample by changing the ray direction 
randomly. (Figure 9.6, top right) This refers to a Henyey-Greenstein phase function with an anisotropy 
value g . [0, 1[. The lower the anisotropy value g, the slower the convergence will be. We will need 
a high number of samples to calculate isotropic scattering at every sample point. In practice it is thus 
useful to restrict the anisotropy to values close to 1.  scattering scattering inbetween scattering 
no scattering at isosurfaces only isosurfaces only everywhere combineboth scattering and attenuation(Figure9.6, 
bottom right). Westartatransmissiveray atthehitpointwith the .rstisosurface. Thedirectionis scattered 
within aPhonglobe around the negative view­ingdirection orthe refracted vector(seeFigure7.5). Wetracethis 
ray (with HGscattering and attenuation) until it hits the second isosurface. At this second hit point, 
the gradient vector is estimated and the ray is re.ected randomly into the hemisphere centered around 
the gradient direction, and so on. A visual comparison of the di.erent scattering e.ects is shown in 
Equation9.7. Inthisexample,theopticalpropertieswerespeci.ed such that theinternal surface structures 
of the volume still remain noticeable. Isosurface scattering alone will render in this example at about 
0.3 0.5 secondsperframeingoodquality. When reducingthe number of samples such that theimagebecomes noisebutqualityis 
still acceptable, aframe rate of about 3 frames per second may be achieved. Ifscatteringisperformed with 
every ray step, a random value mustbe fetched from the noise texture at every step which signi.cantly 
increases the bandwidth load. The example images with scattering everywhere in the volume will render 
at about 1 seconds per frame in good quality. The di.erence between the di.erent scattering e.ect in 
many cases is only marginal,sothebene.t ofimplementing scattering eventsinhomo­geneous regions should 
be evaluated in any individual case. 9.3.1 Heuristic Simpli.cations To improve the performance of the 
Monte-Carlo path tracing approach, we may apply some heuristics for simpli.cation. I point out , however, 
that the techniques described in the following are not mathematically correct since they will lead to 
a bias in the Monte-Carlo estimator. If we cast rays starting from the eye, the .rst few scattering events 
are the most important ones with respect to the .nal color. Scattering events which occur later along 
the path of the ray, are less important. With this heuristic in mind, we may simplify our implementation: 
We may decide to scatter the viewing ray 2 or 3 times at isosurfaces, and then sample the ray with attenuation 
onlybut withoutfurther scattering untilitleaves the volume. This strategyavoids raysbeing re.ected again 
and again until their contribution becomes zero due to attenuation. Another simpli.cation is based on 
the assumption that the attenu­ation which is accumulated from the eye point to the a hit point with 
an isosurface is an appropriate estimate for the attenuation from the hit point back to the outside. 
Hence, we can square the accumulated attenuation and directly sample the environment map in the re.ected 
direction without tracing the ray any further. Although less accurate, this technique is much faster 
because we can sample the environment map many times atthe secondhitpointtodirectly estimatedi.use and 
specular re.ection. An example image using this technique is displayed in Figure 9.9. Our experimentshave 
shown that thereislittle visibledi.erenceifwe compare images generated by the more accurate and the faster 
method. We expect however, that this is due to the onion-like structure of iso­surfaces and should not 
be generalized to arbitrary surfaces inside the volume.   Light Map Approaches In combination withShadowVolumes 
orDeep ShadowMaps(See chap­ter shadows in part 2 of these course notes), alternative implementa­tions 
of scattering arepossible. These approaches,however, are restricted to point light sources or directional 
lights. A shadow volume is an additional voxel data set which contains in­formation about the incident 
light at each voxel. It can be calculated slice-by-slice from a given point light source using a modi.ed 
shadow map approach. The shadow volume covers the same space as the scalar .eld, and its slice planes 
are oriented perpendicular to the light direc­tion. The resolution ofthe shadow volume maybelower thanthe 
original scalar volume. To generate a shadow volume, its slices are processed in front-to-back order 
from the viewpoint of the light source. Each voxel of a slice to be processed is projected onto the previous 
slice(exceptforthe .rst slicewhich usestheincidentilluminationof the light source directly). For point 
lights the position of the light source is thecenter ofprojection(perspective). Fordirectionallightsourcesthe 
projectiondirectionis theinvertedlightdirection(parallelprojection). Each slice voxel reads the incident 
light from the previous slice. The incidentlightisattenuatedbytheabsorptionofthescalar .eldgenerated 
by an opacity transfer function.  We can integrate scattering e.ects into the shadow volume during generation 
by sampling the previous slice multiple times per voxel, at locations randomly scattered around the originalprojected 
sampleposi­tion. Thedistance of the randompointsform the original samplingloca­tionis controlled by aphasefunction 
model, such asHenyey-Greenstein. During raycasting theincident lightfrom the shadow volumeis added to 
the emission termduring ray traversal. An exampleimage of the shadow volume is shown in Figure 10.2. 
A similar approach may be used with a deep shadow map instead of a shadow volume. Inthis case,thedeep 
shadow map(which usually has a high resolution to account for .ne details) is resampled on a lower uniform 
voxel grid. The low resolution for the light map is su.cient since illumination caused by multiple scattering 
e.ects is relatively low frequent. The resampled volumeisthenprocessed exactly the same way as the shadow 
volume. An example image of the combination of deep shadow map and low resolution light map is shown 
in Figure 10.3. While being very e.cient, there are some points which should be noted about the light 
map approaches: Like shadow volumes, scattering light maps are restricted to single point light sources 
 The light map needs to be recomputed whenever the position or direction of the light source changes 
 Thelight map needstoberecomputed wheneverthetransferfunc­tion changes  The approachfor creating thelight 
map asdescribed aboveis more a soft shadow volume than a true scattering light map. In the al­gorithm 
described above and outlined in Figure 10.1, light is scat­tered only in slicing direction, so the scattering 
is most prominent for volumeslitfrombehind(forward scattering). Fortrue subsur­face scattering, more 
than one pass must be used to generate the 3D light map, to account for scattering events in all directions 
in­cluding backward scattering. Though this may be done easily, it will slow down the light map creation 
signi.cantly.   Bibliography <RefA>[1] MichaelD.Adams. TheJPEG-2000StillImageCompressionStan­dard. ISO/IEC(ITU-T 
SG8), September 2001. JTC1/SC29/WG 1: N 2412. [2] Chandrit Bajaj, Insung Ihm, and Sanghun Park. 3D RGB 
image compression for interactive applications. ACM Transactions on Graphics, 20(1):10 38, January 2001. 
[3] Kevin M. Beason, Josh Grant, David C. Banks, Brad Futch, and M.Yousu.Hussaini. Pre-computedilluminationforisosurfaces. 
In VDA 94: Proceedings of the conference on Visualization and Data Analysis 06(SPIEVol.6060),pages 1 
11, 2006. [4] Uwe Behrens and Ralf Ratering. Adding shadows to a texture­based volume renderer. In VVS 
98: Proceedings of the 1998 IEEE symposium on Volume visualization, pages 39 46. ACM Press, 1998. [5] 
Johanna Beyer, Markus Hadwiger, Torsten M¨oller, and Laura Fritz. Smooth Mixed-Resolution GPU Volume 
Rendering. In IEEE/EG International Symposium on Volume and Point-Based Graphics, pages 163 170, 2008. 
[6] Praveen Bhaniramka and Yves Demange. OpenGL Volumizer: A Toolkit for High Quality Volume Rendering 
of Large Data Sets. In Proceedings IEEE Visualization 2002, pages 45 53, 2002. [7] J. F. Blinn. Jim blinn 
s corner: Image compositing theory. IEEE Computer Graphics and Applications, 14(5), 1994. [8] Imma Boada, 
Isabel Navazo, and Roberto Scopigno. Multiresolu­tion volume visualization with a texture-based octree. 
The Visual Computer, 17:185 197, 2001. [9] A. R. Calderbank, Ingrid Daubechies, Wim Sweldens, and Boon-LockYeo. 
Wavelet transforms that mapintegers tointegers. Tech­nical report, Department of Mathematics, Princeton 
University, August 1996. [10] N. Carr, J. Hall, and J. Hart. GPU Algorithms for Radiosity and Subsurface 
Scattering. In Proc. Graphics Hardware, 2003. [11] NathanA.Carr,JesseD.Hall,andJohnC.Hart.GPUalgorithms 
for radiosity and subsurface scattering. In HWWS 03: Proceed­ings of the conference on Graphics Hardware 
03, pages 51 59. Eurographics Association, 2003. [12] Yi-Jen Chiang, Cl´audio T. Silva, and William J. 
Schroeder. In­teractive out-of-core isosurface extraction. In Proceedings of IEEE Visualization 98, pages 
167 174, 1998. [13] Yi-Jen Chiang, Cludio T. Silva, and Willam J. Schroeder. In­teractive out-of-core 
isosurface extraction. In Proceedings IEEE Visualization 1998,pages 167 174,530, 1998. [14] M. Colbert 
and J. K.riv´anek. GPU Gems 3, chapter GPU-Based Importance Sampling, pages 459 475. Addison-Wesley, 
2007. [15] Michael Cox and David Ellsworth. Application-controlled demand paging for out-of-core visualization. 
In Proceedings IEEE Visual­ization 1997, pages 235 244, 1997. [16] Franklin C. Crow. Shadow algorithms 
for computer graphics. In SIGGRAPH 77: Proceedings of the 4th annual conference on Computer graphics 
and interactive techniques, pages 242 248. ACM Press, 1977. [17] Franklin C. Crow. Summed-area tables 
for texture mapping. In Proceedings SIGGRAPH 84, volume 18, pages 207 212, 1984. [18] Carsten Dachsbacher 
and Marc Stamminger. Splatting indirect illumination. In I3D 06: Proceedings of the 2006 symposium on 
Interactive 3D graphics and games, pages 93 100, New York, NY, USA, 2006. ACM. [19] Ingrid Daubechies. 
Ten Lectures on Wavelets. Society for Indus­trial and Applied Mathematics, 1992. [20] Philippe Desgranges 
and Klaus Engel. US patent application 2007/0013696 A1: Fast ambient occlusion for direct volume ren­dering, 
2007. [21] Philippe Desgranges, Klaus Engel, and Gianluca Paladini. Gradient-free shading: A new method 
for realistic interactive vol­ume rendering. In VMV 05: Proceedings of the international fall workshop 
on Vision, Modeling, and Visualization, pages 209 216, 2005. [22] C. Donner and H. W. Jensen. Light Di.usion 
in Multi-Layered Translucent Materials. In Proc. ACM SIGGRAPH, 2005. [23] R. A. Drebin, L. Carpenter, 
and P. Hanrahan. Volume rendering. In Proceedings of SIGGRAPH 88, pages 65 74, 1988. [24] D. Ebert, F. 
K. Musgrave, D. Peachey, K. Perlin, and S. Worley. Texturing and Modeling: A Procedural Approach. Academic 
Press, July 1998. [25] Klaus Engel, Markus Hadwiger, Joe Kniss, Christof Rezk-Salama, and Daniel Weiskopf. 
Real-Time Volume Graphics. AK Peters, 2006. [26] James D. Foley, Richard L. Phillips, John F. Hughes, 
Andries van Dam, and Steven K. Feiner. Introduction to Computer Graphics. Addison-WesleyLongmanPublishingCo.,Inc.,Boston,MA,USA, 
1994. [27] Jinzhu Gao, Jian Huang, C. Ryan Johnson, and Scott Atchley. Distributed data management for 
large volume visualization. In Proceedings IEEE Visualization 2005,pages 183 189. IEEE, 2005. [28] Jinzhu 
Gao, Jian Huang, Han-Wei Shen, and James Arthur Kohl. Visibility culling usingplenoptic opacityfunctionsforlarge 
volume visualization. In Proceedings IEEE Visualization 2003,pages 341 348. IEEE, 2003. [29] Jinzhu Gao, 
Han-Wei Shen, Jian Huang, and James Arthur Kohl. Visibility culling for time-varying volume rendering 
using tempo­ral occlusion coherence. In Proceedings IEEE Visualization 2004, pages 147 154. IEEE, 2004. 
[30] S. Grimm, S. Bruckner, A. Kanitsar, and E. Gr¨oller. Memory e.­cient acceleration structures and 
techniques for cpu-based volume raycasting of large data. In Proceedings IEEE/SIGGRAPH Sym­posium on 
Volume Visualization and Graphics, pages 1 8, 2004. [31] S¨oren Grimm, Stefan Bruckner, Armin Kanitsar, 
and Eduard Gr¨oller. Memory e.cient acceleration structures and techniques for CPU-based volume raycasting 
of large data. In Proceedings IEEE Volume Visualization and Graphics Symposium, pages 1 8, 2004. [32] 
S¨oren Grimm, Stefan Bruckner, Armin Kanitsar, and Eduard Gr¨oller. A re.ned data addressing and processing 
scheme to ac­celerate volume raycasting. Computers and Graphics, 28:719 729, 2004. [33] StefanGutheandWolfgangStraßer.Real-timedecompression 
and visualization of animated volume data. In Proceedings IEEE Visu­alization 2001, pages 349 356, 2001. 
[34] Stefan Guthe and Wolfgang Strasser. Advanced techniques for high quality multiresolution volume 
rendering. In Computers &#38; Graphics, volume28,pages51 58.ElsevierScience,February2004. [35] Stefan 
Guthe, Michael Wand, Julius Gonser, and Wolfgang Straßer. Interactive rendering of large volume data 
sets. In Pro­ceedings IEEE Visualization 2002, pages 53 60, 2002. [36] Attila Gyulassy, Lars Linsen, 
and Bernd Hamann. Time-and space-e.cient error calculation for multiresolution direct volume rendering. 
In Mathematical Foundations of Scienti.c Visualiza­tion,ComputerGraphics, andMassiveDataExploration.Springer-Verlag, 
Heidelberg, Germany, 2006. [37] M. Hadwiger, C. Sigg, H. Scharsach, K. B¨uhler, and M. Gross. Real-time 
ray-casting and advanced shading ofdiscreteisosurfaces. In Proceedings of Eurographics 2005, pages 303 
312, 2005. [38] M. Hadwiger, C. Sigg, H. Scharsach, K. B¨uhler, and M. Gross. Real-Time Ray-Casting and 
Advanced Shading of Discrete Isosur­faces. In Proceedings of Eurographics, pages 303 312, 2005. [39] 
Markus Hadwiger, Andrea Kratz, Christian Sigg, and Katja B¨uhler. Gpu-accelerated deep shadow maps for 
direct volume ren­dering. In GH 06: Proceedings of the 21st ACM SIGGRAPH/Eu­rographics symposium on Graphics 
hardware, pages 49 52, New York, NY, USA, 2006. ACM Press. [40] W. Heidrich and H.-P. Seidel. Realistic, 
Hardware-accellerated Shading and Lighting. In Proc. ACM SIGGRAPH, 1999. [41] L. Henyey and J. Greenstein. 
Di.use radiation in the galaxy. As­trophysical Journal, pages p. 70 83, 93. [42] Frida Hernell, Patric 
Ljung, and Anders Ynnerman. E.cient am­bient and emissive tissue illumination using local occlusion in 
mul­tiresolution volume rendering. In Proceedings Eurographics/IEEE-VGTC Symposium on Volume Graphics. 
Eurographics/IEEE, 2007. [43] Frida Hernell, Patric Ljung, and Anders Ynnerman. Interactive GlobalLightPropagationinDirectVolumeRendering 
usingLocal Piecewise Integration. In IEEE/EG International Symposium on Volume and Point-Based Graphics, 
pages 105 112, 2008. [44] W.Hong,F.Qiu, andA.Kaufman. Gpu-based object-order ray­castingforlargedatasets. 
In Proceedings ofVolumeGraphics2005, 2005. [45] Insung Ihm and Sanghun Park. Wavelet-based 3d compression 
scheme for interactive visualization of very large volume data. Computer Graphics Forum, 18(1):3 15, 
1999. [46] HenrikWannJensen,StephenR.Marschner,MarcLevoy, andPat Hanrahan. A Practical Model for Subsurface 
Light Transport. In Proceedings of ACM SIGGRAPH, pages 511 518, 2001. [47] Ralf K¨ahler, John Wise, Tom 
Abel, and Hans-Christian Hege. Gpu-assisted raycastingfor cosmological adaptive mesg re.nement simulations. 
In Proceedings Eurographics/IEEE Workshop on Vol­ume Graphics 2006,pages 103 110,144, 2006. [48] D. Kalra 
and A. H. Barr. Guaranteed ray intersections with im­plicit surfaces. In Proceedings of SIGGRAPH 89,pages297 
306, 1989. [49] A. Kaufman. Voxels as a Computational Representation of Ge­ometry. In The Computational 
Representation of Geometry. SIG-GRAPH 94 Course Notes, 1994. [50] Tae-Yong Kim and Ulrich Neumann. Opacity 
shadow maps. In Proceedings ofthe12thEurographicsWorkshop onRenderingTech­niques, pages 177 182, London, 
UK, 2001. Springer-Verlag. [51] Joe Kniss, Gordon Kindlmann, and Charles Hansen. Multidimen­sional transfer 
functions for interactive volume rendering. IEEE Transactions on Visualization and Computer Graphics, 
8(3):270 285, 2002. [52] Joe Kniss, Simon Premoze, Charles Hansen, and David Ebert. In­teractive translucent 
volume rendering and procedural modeling. In VIS 02: Proceedings of the conference on Visualization 02, 
pages 109 116. IEEE Computer Society, 2002. [53] Joe Kniss, Simon Premoze, Charles Hansen, Peter Shirley, 
and Allen McPherson. A model for volume lighting and model­ing. IEEE Transactions on Visualization and 
Computer Graphics, 9(2):150 162, 2003. [54] M. Kraus and T. Ertl. Adaptive texture maps. In Proceedings 
of Graphics Hardware 2002, pages 7 15, 2002. [55] J. Kr¨uger and R. Westermann. Acceleration techniques 
for GPU­based volume rendering. In Proceedings IEEE Visualization 2003, 2003. [56] Eric C. LaMar, Bernd 
Hamann, and Kenneth I. Joy. Multiresolu­tion techniques for interactive texture-based volume visualization. 
In Proceedings IEEE Visualization 1999, pages 355 362, 1999. [57] EricC.LaMar,BerndHamann,andKennethI.Joy.E.cient 
error calculation for multiresolution texture-based volume visualization. In Gerald Farin, Bernd Hamann, 
and Hans Hagen, editors, Hier­achical and Geometrical Methods in Scienti.c Visualization,pages 51 62. 
Springer-Verlag, Heidelberg, Germany, 2003. [58] MichaelS.Langer andHeinrichH.B¨ultho..Depthdiscrimination 
from shading under di.use lighting. Perception, 29(6):649 660, 2000. [59] H. Lensch, M. Goesele, P. Bekaert, 
J. Kautz, M. Magnor, J. Lang, andH.-P.Seidel.Interactive rendering oftranslucent objects. Com­puter Graphics 
Forum, 22(2), 2003. [60] M. Levoy. Display of surfaces from volume data. IEEE Computer Graphics and Applications, 
8(3):29 37, May 1988. [61] F. Link, M. Koenig, and H.-O. Peitgen. Multi-Resolution Volume Rendering with 
per Object Shading. In Proceedings of Vision, Modeling and Visualization, pages 185 191, 2006. [62] Yarden 
Livnat, Han-Wei Shen, and Christopher R. Johnson. A near optimal isosurface extraction algorithm using 
the span space. IEEE Transactions on Visualization and Computer Graph­ics, 2:73 84, 1996. [63] Patric 
Ljung. Adaptive sampling in single pass, GPU-based ray­casting of multiresolution volumes. In Proceedings 
Eurographic-s/IEEE Workshop on Volume Graphics 2006, pages 39 46,134, 2006. [64] Patric Ljung. E.cient 
Methods for Direct Volume Rendering of Large Data Sets. PhD thesis, Link¨oping University, Sweden, 2006. 
Link¨oping studiesin science andtechnology.Dissertations no.1043. [65] Patric Ljung, Claes Lundstr¨om, 
and Anders Ynnerman. Multires­olution interblock interpolation in direct volume rendering. In ProceedingsEurographics/IEEESymposium 
onVisualization2006, pages 259 266, 2006. [66] Patric Ljung, Claes Lundstr¨om, Anders Ynnerman, and Ken 
Museth. Transfer function based adaptive decompression for vol­ume rendering of large medical data sets. 
In Proceedings IEEE Volume Visualization and Graphics Symposium 2004, pages 25 32, 2004. [67] Patric 
Ljung, Calle Winskog, Anders Perssson, Claes Lundstr¨om, and Anders Ynnerman. Full body virtual autopsies 
using a state­of-the-art volume rendering pipeline. IEEE Transactions on Vi­sualization andComputerGraphics(ProceedingsVisualization/In­formation 
Visualization 2006), 12:869 876, 2006. [68] Tom Lokovic and Eric Veach. Deep shadow maps. In SIGGRAPH 
00: Proceedings of the27th annual conference onComputergraph­ics andinteractive techniques,pages385 392,NewYork,NY,USA, 
2000. ACM Press/Addison-Wesley Publishing Co. [69] Eric B. Lum, Kwan-Liu Ma, and John Clyne. Texture 
hardware assisted rendering of time-varying volume data. In Proceedings IEEE Visualization 2001, pages 
263 270, 2001. [70] Eric B. Lum, Kwan-Liu Ma, and John Clyne. A hardware-assisted scalable solution for 
interactive volume rendering of time-varying data. IEEETransactions onVisualization andComputerGraphics, 
8:286 298, 2002. [71] Claes Lundstr¨om, Patric Ljung, and Anders Ynnerman. Local histograms for design 
of transfer functions in direct volume ren­dering. Transactions on Visualization and Computer Graphics, 
12(6):1570 1579, Nov.-Dec. 2006. [72] Gerd Marmitt, Heiko Friedrich, and Philipp Slusallek. Interactive 
Volume Rendering with Ray Tracing. In Eurographics State of the Art Reports, 2006. [73] Nelson Max. Optical 
models for direct volume rendering. IEEE Transactions on Visualization and Computer Graphics, 1(2):99 
108, June 1995. [74] Nelson Max. Optical models for direct volume rendering. IEEE Transactions on Visualization 
and Computer Graphics, 1(2):99 108, 1995. [75] J¨orgMensmann,TimoRopinski,andKlausHinrichs.Accelerating 
Volume Raycasting using Occlusion Frustum. In IEEE/EG Inter­national Symposium on Volume and Point-Based 
Graphics, pages 147 154, 2008. [76] Ky Giang Nguyen and Dietmar Saupe. Rapid high quality com­pression 
of volume data for visualization. Computer Graphics Fo­rum, 20(3), 2001. [77] Steven Parker, Michael 
Parker, Yarden Livnat, Peter-Pike Sloan, Charles Hansen, and Peter Shirley. Interactive ray tracing for 
vol­ume visualization. IEEE Transactions on Visualization and Com­puter Graphics, 5(3):238 250, 1999. 
[78] Steven Parker, Peter Shirley, Yarden Livnat, Charles Hansen, and Peter-Pike Sloan. Interactive ray 
tracing for isosurface rendering. InProceedingsofIEEEVisualization 98.IEEE-CS,ACM,October 1998. [79] 
A. Patra and M.D. Wang. Volumetric medical image compression and reconstruction for interactive visualization 
in surgical plan­ning. In Proceedings Data Compression Conference 2003, page 442, March 2003. [80] Eric 
Penner and Ross Mitchell. Isosurface Ambient Occlusion and Soft Shadows with Filterable Occlusion Maps. 
In IEEE/EG Inter­national Symposium on Volume and Point-Based Graphics, pages 57 64, 2008. [81] Matt 
Pharr and Greg Humphries. Physically Based Rendering. Morgan Kau.man, 2004. [82] William T. Reeves, David 
H. Salesin, and Robert L. Cook. Ren­dering antialiased shadows with depth maps. In SIGGRAPH 87: Proceedings 
of the 14th annual conference on Computer graphics and interactive techniques, pages 283 291. ACM Press, 
1987. [83] S. Roettger, S. Guthe, D. Weiskopf, and T. Ertl. Smart hardware­accelerated volume rendering. 
In Procceedings of EG/IEEE TCVG Symposium on Visualization VisSym 03, pages 231 238, 2003. [84] Timo 
Ropinski, Jens Kasten, and Klaus H. Hinrichs. E.cient Shadows for GPU-based Volume Raycasting. In Proceedings 
of the 16th International Conference in Central Europe on Computer Graphics, Visualization(WSCG08), pages 
17 24, 2008. [85] Timo Ropinski, Jennis Meyer-Spradow, Stefan Diepenbrock, J¨org Mensmann, and Klaus 
H. Hinrichs. Interactive Volume Rendering with Dynamic Ambient Occlusion and Color Bleeding. Computer 
Graphics Forum(Eurographics 2008), 27(2):567 576, 2008. [86] StefanR¨ottger,MichaelBauer,andMarcStamminger.Spatialized 
transfer functions. In EuroVis, pages 271 278, 2005. [87] Marc Ruiz, Imma Boada, Ivan Viola, Stefan Bruckner, 
Miquel Feixas, and Mateu Sbert. Obscurance-based Volume Rendering Framework. In IEEE/EG International 
Symposium on Volume and Point-Based Graphics, pages 113 120, 2008. [88] C. Rezk Salama. GPU-Based Monte-Carlo 
Volume Raycasting. In Proc. Paci.c Graphics, 2007. [89] MirkoSattler,RalfSarlette,ThomasM¨ucken,andReinhardKlein. 
Exploitation of human shadow perception for fast shadow render­ing. In APGV 05: Proceedings of the 2nd 
symposium on Ap­pliedperceptioningraphics andvisualization,pages131 134.ACM Press, 2005. [90] H. Scharsach, 
M. Hadwiger, A. Neubauer, S. Wolfsberger, and K. B¨uhler. Perspective Isosurface and Direct Volume Rendering 
forVirtualEndoscopy Applications. In Proceedings of Eurovis 06, pages 315 323, 2006. [91] Henning Scharsach. 
Advanced GPU raycasting. In Proceedings of the 9th Central European Seminar on Computer Graphics, May 
2005. [92] Jens Schneider and R¨udiger Westermann. Compression domain volume rendering. In Proceedings 
IEEE Visualization 2003, 2003. [93] Peter-Pike Sloan, Jesse Hall, John Hart, and John Snyder. Clus­tered 
principal components for precomputed radiance transfer. In SIGGRAPH 03: ACM SIGGRAPH 2003 Papers, pages 
382 391. ACM Press, 2003. [94] Peter-Pike Sloan, Ben Luna, and John Snyder. Local, deformable precomputed 
radiance transfer. In SIGGRAPH 05: ACM SIG-GRAPH 2005 Papers, pages 1216 1224. ACM Press, 2005. [95] 
IrwinEdwardSobel. Camera models and machineperception. PhD thesis, Stanford University, Stanford, CA, 
USA, 1970. [96] Lisa M. Sobierajski and Arie E. Kaufman. Volumetric ray trac­ing. In VVS 94: Proceedings 
of the 1994 symposium on Volume Visualization 94, pages 11 18. ACM Press, 1994. [97] S. Stegmaier, M. 
Strengert, T. Klein, and T. Ertl. A simple and .exible volume rendering framework for graphics-hardware 
based raycasting. In Proceedings of the International Workshop on Vol­ume Graphics 05, pages 187 195, 
2005. [98] A. James Stewart. Vicinity shading for enhanced perception of volumetric data. In VIS 03: 
Proceedings of the 14th IEEE Visu­alization 2003(VIS 03), page 47. IEEE Computer Society, 2003. [99] 
Wim Sweldens. The lifting scheme: A custom-design construction of biorthogonal wavelets. Journal of Applied 
and Computational Harmonic Analysis,(3):186 200, 1996. [100] MartinVetterliandDidierLeGall.Perfect reconstructionFIR.lter 
banks: some properties and factorizations. IEEE Transactions on Acoustics, Speech, and Signal Processing, 
37(7):1057 1071, July 1989. [101] Joachim E. Vollrath, Tobias Schafhitzel, and Thomas Ertl. Em­ploying 
complex GPUdata structures for theinteractive visualiza­tion of adaptive mesh re.nement data. In Proceedings 
Eurograph-ics/IEEE Workshop on Volume Graphics 2006, pages 55 58,136, 2006. [102] IngoWald,HeikoFriedrich,GerdMarmitt,andHans-PeterSeidel. 
Faster isosurface ray tracing using implicit kd-trees. IEEE Trans­actions on Visualization and Computer 
Graphics, 11(5):562 572, 2005. Member-Philipp Slusallek. [103] Ingo Wald, Thomas Kollig, Carsten Benthin, 
Alexander Keller, and Philipp Slusallek. Interactive global illumination using fast ray tracing. In EGRW 
02: Proceedings of the 13th Eurographics workshop on Rendering, pages 15 24, Aire-la-Ville, Switzerland, 
Switzerland, 2002. Eurographics Association. [104] M. Weiler, R. Westermann, C. Hansen, K. Zimmerman, 
and T. Ertl. Level-Of-Detail Volume Rendering via 3D Textures. In Proceedings of IEEE Symposium on Volume 
Visualization, pages 7 13, 2000.  [105] Manfred Weiler, R¨udiger Westermann, Chuck Hansen, Kurt Zim­merman, 
and Thomas Ertl. Level of detail volume rendering via 3dtextures. InProceedingsIEEEVolumeVisualization 
andGraph­ics Symposium 2000, pages 7 13. ACM Press, 2000. [106] R¨udiger Westermann. A multiresolution 
framework for volume rendering. In 1994 Symposium on Volume Visualization, October 1994. [107] G. Wetekam, 
D. Staneker, U. Kanus, and M. Wand. A hard­ware architecture for multi-resolution volume rendering. In 
Pro­ceedings ACM SIGGRAPH/Eurographics Conference on Graphics Hardware, pages 45 51, New York, NY, USA, 
2005. ACM Press. [108] Jane Wilhelms and Allen Van Gelder. Octrees for faster isosurface generation. 
ACM Transactions on Graphics, 11:201 227, 1992. [109] Lance Williams. Casting curved shadows on curved 
surfaces. In SIGGRAPH 78: Proceedings of the 5th annual conference on Computer graphics and interactive 
techniques, pages 270 274. ACM Press, 1978. [110] C. M. Wittenbrink, T. Malzbender, and M. E. Goss. Opacity­weighted 
colorinterpolationfor volume sampling. In Proceedings of IEEE Symposium on Volume Visualization, pages 
135 142, 1998. [111] Chris Wyman, Steven Parker, Charles Hansen, and Peter Shirley. Interactive display 
of isosurfaces with global illumination. IEEE Transactions on Visualization and Computer Graphics, 12(2):186 
196, 2006. [112] Boon-Lock Yeo and Bede Liu. Volume rendering of DCT-based compressed 3d scalar data. 
IEEE Transactions on Visualization and Computer Graphics, 1:29 43, March 1995. [113] C. Zhang, D. Xue, 
and R. Craw.s. Light propagation for mixed polygonal and volumetric data. In CGI 05: Proceedings of the 
Computer Graphics International 2005, pages 249 256, Washing­ton, DC, USA, 2005. IEEE Computer Society. 
[114] Caixia Zhang and Roger Craw.s. Shadows and soft shadows with participating media using splatting. 
IEEE Transactions on Visu­alization and Computer Graphics, 9(2):139 149, 2003.</RefA> 
			
