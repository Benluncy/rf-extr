
 A Performance Anaiysls of the Gamma Database Machme David J DeWm Shahram Ghandehanzadeh Donovan Schneider 
 Computer Scrences Depaltment Umversity of Wlsconsm Abstract This paper presents the results of au nunal 
performance evaluation of the Gamma database machme In our experunents we measured the effect of relation 
size and mdrces on response nme for selecnon, Join. and aggregation queries, and smgle-tuple updates 
A Teradata DBC/lOlZ database machme of slmdar size 1s used as a basis for mterpretmg the resulta obtamed 
We also analyze the per- formance of Gamma relanve to the number of processors employeed and study the 
Impact of varymg the memory size and disk page size on the execution tune of a variety of selechon and 
~otn quenes We analyze and mterpre+ the results of these expen- ments based on our understanding of the 
system hardware and software, and conclude with an assessment of the strengths and weaknesses of Gamma 
1 Introduction This report presents the results of a smgle-user performance evaluanon of the Gamma database 
machme [DEWI86, GERB861 TIN evaluanon is based on two pnnclpal metncs the absolute per- formance achieved 
by Gamma and the performance relative to the number of processors used AS a basis for detennmmg the absolute 
performance of Gamma, we have used results obtained from a semi- lar study [DEWI87] of the Teradata DBC/lOlZ 
database machme [TERA83] When determmmg the performance of Gamma relanve to the number of processors 
used, simply mcreasmg the number of processors has the side effect of mcreasmg the amount of buffer space 
avadable for processmg Jam operations Thus, a Jam that does not cause a Jam hash table overflow with 
8 processors may result m 7 overflows when the query 1s executed usmg a single pro- cessor Whde one could 
change the size of the test relations to avoid dus problem, we decided instead to keep the total (summed 
across all processors) amount of buffer space constant when varymg the number of processors Then, m a 
separate set of tests, we kept the number of processors constant whde varymg the total amount of buffer 
space avadable In the final smte of tests, we kept the number of buffer pages and processors constant 
while vatymg the dtsk page size In Sechons 2 and 3, respectively, we descnbe the Gamma and Teradata configuranons 
that were evaluated Sectlon 4 presents an overvlew of database used for the expenments Whde four types 
of Permtss~onto copy wIthout fee all or part of this materk4 1s granted provided that the copws are not 
made or dlstrlbuted for dtrect commercial advantage, the ACM copy&#38;t nottce and the title of the pubhcatlon 
and Its date appear, and notlce IS given that copymg IS by permlwon of the Assoclatlon for Computmg Machmery 
To copy otherwIse, or to repubhsh, reqmres a fee and/or specific permIssIon @ 1988 ACM 0-89791-268-3/88/OCO6/0350 
$1 50 tests were conducted selecnons, JOIIU, aggregates. and updates, space precludes us from presenwlg 
the results from the aggregate tests The interested reader is referred to [DEWI A descnphon of the exact 
quenes used and the results obtamed for each query are contained m Secnons 5 through 7 Our conclusions 
are presented m Secnon 8 2 Overview of the Gamma Database Machme In thus section we present an overview 
of the Gamma database machme mcludmg a descnpuon of the current hardware configuration and the software 
techmques used m the lmplementa- non For a complete descnpnon of Gamma see [DEWI86, GERB861 Gamma consists 
of 17 VAX 11/750 processors, each ~rlth two megabytes of memory An 80 megabn/second token nng [PROT85] 
1s used to connect the processors to each other and to another VAX 1 In50 runmng Berkeley UNIX Thus processor 
acts as the host machine for Gamma Attached to eight of the processors are 333 megabyte FuJitsu disk 
dnves (8 ) whch are used for data- base storage One of the d&#38;less processors 1s cum?ntly reserved 
for query scheduhng and global deadlock de&#38;non The remammg dlskless processors are used to execute 
Jam, proJection, and aggre- gate operations Selection and update op&#38;atrons are executed only on the 
processors with disk dnves attached In Gamma, all relanons are horuontally partrtloned [RtES78] across 
all disk dnves m the system Four altemanve ways of dlstnbutmg the tuples of a relation are provided round-robin, 
hashed, range parhtioned wnh user-specified placement by key value, and range pamboned with umform dntnbuuon 
As Implied by its name, m the first strategy when tuples are loaded mto a rela- uon, they are dlstnbuted 
in a round-robm fastion among all dlsb dnves This 1s the default strategy m Gamma for relanons created 
as the result of a query If the hashed strategy is selected, a random- lzmg function 1s apphed to the 
key attnbute of each tuple to select a storage unit In the Uurd strategy the user speafies a range of 
kcv values for each site In the last partlhomng strategy the user speclfi:\ the partltlonmg attnbute 
and the system tistnbutes the tuples urufonnly across all sites G,lmma WCS tradthonal xelahonal techmques 
for query par mg, o@mzatlor .SELI79], and code generauon Quenes are com- pded mto a tree of operators 
with ptilcates compded mto machme language After bcmg parsed, optlmlzed, and compded, the query 1s sent 
by the host software to an Idle scheduler process through a dispatcher process The dispatcher process, 
by controlhng the number of active schedulers, unplements a simple load control mechamsm based on mformahon 
about the degree of CPU and 1 Several processors have more than 2 megabytes of memory so that the pm 
query spedup tests could be conducted w&#38;out causrq hash table overflow to oc- cur when only 1 or 
2 processors are used memory utlhzanon at each processor The scheduler process, m turn. activates operator 
processes at each query processor selected to execute the operator me task of assigmng operators to processors 
1s performed 111 part by the optlmlzer and m part by the scheduler assigned to control the execunon of 
the query For example, the operators at the leaves of a query tree reference only permanent relanons 
Using the query and schema mformatlon, the optunlzer IS able to determme the best way of asslgmng these 
operators to pro- CeSsOrS In Gamma, the algontbms for all operators are wntten as If they were to be 
run on a single processor The input to an Operator Process 1s a stream of tuples and the output 1s a 
stream of tuples that 1s demultiplexed through a structure we term a split table After bemg nutlated, 
a query process wats for a control message to arnve on a global, well-known control port Upon remvmg 
an operator control packet, the process replies with a message that identifies itself to the scheduler 
Once the process begms execuuon, It contmuously reads tuples from its mput stream, operates on each tuple, 
and uses a splrt table to route the resultmg tuple to the process indicated m the split table Consider, 
for example, the case of a selechon operanon that is producmg tuples for use in a subsequent JOUI operation 
If the Join is bemg executed by N processes, the spbt table of the selecoon process will contam N entnes 
For each tuple sahsfymg the selechon predicate, the selechon process will apply a hash fun&#38;on to 
the Join attnbute to produce a value between 1 and N This value 1s then used as an index mto the split 
table to obtam the address (e g machmeld, port #) of the Join pm- cess that should receive the tuple 
When the process detects the end of its input stream, It first ,closes the outpIt streams and then sends 
a control message to its scheduler mdlcatmg that it has completed execution Closing the output streams 
has the side effect of sending end of stream messages to each of the destmahon processes With the excephon 
of these three control messages, execution of an operator 1s completely self-scheduling Data flows among 
the processes executmg a query tme m a dataflow fation If the result of a query is a new relation. the 
operators at the root of the query tree dlstnbute the result tuples on a round-robm basis to store opera- 
tors at each disk site which assume the responslbdlty for wntmg the result tuples to disk To enhance 
the performance of Jam operations an array of bit vector filters [BABB79] can be inserted mto the split 
table by the optlmlzer Gamma IS built on top of an operatmg system developed specifically for supportmg 
database management systems This operatmg system provides hghtwelght processes with shared memory and 
reliable, datagram commumcanon services usmg a muluple bit, slldmg wmdow protocol Messages between two 
processes on the same processor m short-ctrcurted by the commun- Icanons software File services m NOSE 
are based on the Wiscon- sm Storage System (WiSS) [CHOUSS] WlSS provides structured sequential files, 
clustered and unclustered B+-tree mdlces, and sort and scan utllmes 3. Teradata Hardware and Sonware 
Configuration The Teradata machme tested has 4 Interface Processors (IFPs), 20 Access Module Processors 
(AMPS), and 40 Disk Storage Umts (DSUs) The IFPs commumcate wth the host, and parse, optmnze, and direct 
the execution of user requests Quenes are executed on the AMPs IFPs and AMPs are interconnected by a 
dual redundant, tree-shaped mterconnect called the Y-net [TERA83] which has an aggregate bandwidth of 
12 megabytes/second Intel 80286 proces- sors with 2 megabytes of memory are used m all 1FP.s and AMPS 
Each AMP has two 8 S , 525 megabyte Hitachi disk dnves The host processor was an AMDAHL V570 runmng the 
MVS operatmg system Software release 2 3 was used for the tests conducted All relations on the Teradata 
machme were also honzontally partmoned across all AMFs Whenever a tuple is to be inserted into a relahon, 
a hash function is applied to the pnmary key2 of the t ela- tion to select an AMP for storage Once a 
tuple amves at a site, that AMP applies a hash funchon to the key attnbute m order to place the tuple 
in its fragment (several tuples may hash to the same value) of the appropnate relation The hash value 
and a sequence number are concatenated to form a umque tuple-id Once an enhx relahon has been loaded, 
the tuples m each honzontal fragment are m what 1s termed hash-key order Thus, given a value for the 
key attnbute, it 1s possible to locate the tuple m a smgle disk access (assuming no buffer pool h&#38;s) 
This 1s the only physical file orgam- zahon supported at the present hme It IS Important to note that 
Dven dus orgamzatlon, the only kmd of indices one can construct are dense, secondary mdlces The index 
1s tenned dense as It must contam one entry for each tuple m the mdexed relation It 1s termed secondary 
as the index order 1s &#38;fferent than the key order of the file Furthermore, the rows m the mdex are 
themselves hashed on the key field and are NOT sorted m key order Conse-quently, whenever a range query 
over an mdexed attnbute 1s per- formed, the entlre index must be scanned 4 Descrrptlon of Benchmark Relahons 
The relanons used for the benchmarks are based on the stan- dard Wisconsin Benchmark relahons [BIlT83] 
Each relation con- sists of thnteen 4-byte integer attnbutes and three 52-byte stnng attnbutes In order 
to more meamngfully stcess both database machmes, 100,000 and 1.000,000 tuple versions of the standard 
1,000 and 10.000 tuple relahons were also constructed The umquel and umque2 attributes of each relauon 
were generated m such a way as to guarantee that each tuple has a umque value for each attnbute and that 
therp 1s no correlation between the values of umquel and umque2 withm a single tuple Two copies of each 
rela- tion were created and loaded using Umquel as the key @aNhONng) attnbute m all cases The total database 
size is approximately 464 megabytes (not mcludmg mdices) For the Teradata machme al! test relahons were 
loaded 111 the NO FALLBACK mode Except where otherwise noted, the results of all quenes were. stored 
m the database Stormg the result of a query m a relation Incurs two costs not incurred if the resultmg 
tuples a~ returned to the host processor First, the tuples of each result n&#38;non must be Qstnbuted 
across all processors with disks In the case of the Teradata database machme, umquel was used as the 
pnmary key of both the source and result relanons wlule we had expected that no commumcahons overhead 
would be mcurred m stonng the result tuples, smce the low-level commumcauons software does not recogmze 
tis sltuauon, the exe- cuhon tnnes presented below mclude the cost of redlsmbuhng the result tuples Since, 
the current version of Gamma redlstnbutes result tuples m a round-mbm fashon, both machmes incur the 
same redlsmbuhon overhead wme stormg the result of a query m a rela- hon The second cost associated with 
stormg the result of a query m a relahon is the impact of the recovery software on the rate at which 
tuples are inserted m a relahon In thus case, there are substanhal differences between the two systems, 
due, pnmanly, to a difference m the semantics betwen QUBL and SQL Gamma, wluch provides an extended version 
of the query language QUEL [STON76], uses the construct retrreve mto result-relation to specify that 
the result of a query is to be stored in a relation If, for some reason the transachon runmng the query 
is aborted, the only action that the recovery manager must take 1s to delete all files associated with 
the result Elation The query language for the Teradata database machme 1s based on an extended vexslon 
of SQL In SQL, one uses Insert ~nro to store the results of a query m a relation Since It 1s possible 
for the target xelauon to already contam tuples, the code for mert Into must log all mserted tuples carefully 
Since the Teradata insert code 1s currently optlmlzed for single tuple and not bulk updates, at least 
3 I/OS are incurred for each tuple inserted (see [DEWI87] for a more complete descnpuon of the problem) 
A stnughtfonvard optlmlza- bon would be for the msert mto code to recogmze when It was operatmg on an 
empty relation This would enable the code to pro- cess bulk updates much more efficiently 5 Selection 
In tlus sechon, we first explore Gamma s performance for a vanety of SelCchOn quenes clb the Size of 
the input relahons is increased The results obtained are compared with the results of rumung the same 
set of quenes on the Teradata database machine For a subset of these quenes, we then vaned the number 
of proces- sors and the disk page size to determme how these factors affect pelfOHMll~ 5 1 Performance 
Relatme to Relation Size The selection quenes were designed with two objectives in mmd Rrst, we wanted 
to know how the Teradata and Gamma machmes wouId Espond as the size of the source relahons was increased 
Ideally, gven constant machme configuranons, the response urne should grow as a hnear funchon of the 
size of input and result relahons Second, we were interested m explonng the effect of indices on the 
execuhon nme of a selection on each machme wtile holdmg the seletivlty factor constant Our tests used 
two sets of selechon quenes firAt with 1% selectivity and second ullth 10% selectivity On Gamma, the 
t&#38;o sets of quenes were tested with three different storage orgamza- tlons a heap (no index), a clustered 
mdex on the key attnbute (index order = key order), and a non-clustered mdex on a non-key attnbute (index 
order #key order) On the Teradata machme, since tuples m a relauon are orgamzed m hash-key order, it 
1s not possible to construct a clustered index Therefore, all mdlces, whether on the key or any other 
attnbute, are dense, non-clustered indices In Table 1, we have tabulated the results of testmg the dlf- 
ferent types of selecuon quenes on three sizes of relations (10,000, 1OO,ooO, and l,OOO,OCO tuples) Two 
mam conclusions can be drawn from thus table First, for both machmes the execution hnk?. of each query 
scales m a linear fastion as the size of the input and output relaaons are mcreased Second, as expected, 
the clustered B-tree orgamzatlon provides a sigmfildnt unprpvement m perfor- mance As discussed m [DEWIS 
I], the results for the 1% and 10% selecaon usmg a non-clustered index (rows three and four of Table 
1) for the Teradata machme look puzzling Both of these quenes selected tuples usmg a predicate on the 
umque2 attnbute, an attn- bute on which we had constructed a non-clustered index In the case of the 10% 
selectmn, the optnmzer decided (correctly) not to use the mdex In the 1% case, the observed execution 
time is almost identical to the result obtamed for the nonmdexed case While these results seems to contradict 
the query plan produced by tic opumlzer, which states that the non-clustcrcd index on unique2 IS to be 
used to execute the query, the storage orgamLatlon used for mdlces on the Teradata machme provides a 
partial cxplanauon Smcc the index entnes are hash-based and not m sorted order, the entlre index must 
be scanned sequentially instead of scanning only the portion correspondmg to the range of the query Thus, 
exactly the same number of attnbute value corn, ansons 1s done for both index scans and sequenhal scans 
However, it 1s expcctcd that the number of I/OS reqmred to scan the index 1s only a fracuon of the number 
of I/OS reqmred to scan the relation Apparently, the response nme is not reduced sigmficantly because, 
whllc the mdcx can be scanned sequentially, each access to the relallon rcqmrcs a random seek Gamma supports 
the noDon of non-clustered indices through a B-tree structure on top of the actual data file As can be 
seen frop Table 1, in the case of the 10% selecbon, the Gamma ophmizcr also decides not to use the index 
In the 1% case, the mdex is used Consider, for example, a scan with a 1% selecnvlty factor on a 10,000 
tuple n&#38;non if the non-clustered mdex is used. in the worst case lOO(+/- 4) I/OS will be requued 
(assummg each tuple causes a page fault) On the other hand, If a segment scan 1s chosen to access the 
data, with 17 tuples per data page, all 589 pages of data would be read The difference between the number 
of I/OS 1s slgmficant and IS confnmed by the difference m response nme between the entnes for Gamma m 
rows 3 and 4 of Table 1 Gamma also provides clustered mdlces (the underlying rela- non 1s sorted according 
id the key attnbute and a B-tree search structure 1s built on top of the data) The response trme for 
the 1% and 10% selections through a clustered mdex are presented m rows five and SIX of Table 1 Since 
the tuples are sorted (key order = index order), only that pornon of the relauon conespondmg to the range 
of the query 1s scanned Thus results m a further reduction of the number of UOs compared to the correspondmg 
search through a file scan or a non-clustered index Thus saving IS confirmed by the lower response tunes 
shown m Table 1 One important observahon to be made from Table 1 IS the relative consistency of the cost 
of selection using a clustered mdcx m Gamma Notice that thr response tune Tfor both the 10% selection 
from the 10,000 tuple relation and the 1% selection from the 100,000 tuple relatlon usmg a clustered 
mdex ts 125 seconds The reason 1s that m both cases 1,000 tuples are retneved and stored, resultmg m 
the same I/O and CPU costs The SekchOn results reveal an Important hmltauon of the Teradata design Since 
there are no clustered indices, and since non-clustered indices can only be used when a relattvely small 
number of tuples are remeved, the system must resort to scanning Table 1 Selection Quenes (All Execution 
Times m Seconds) Number of Tuples m Source Relation 10,ooo 10,ooo 100,000 100,ooo l,ooo,ooo l,ooo,ooo 
Query Description Teradata Gamma Teradata Gamma Teradata Gamma 1% nomndexed selecnon 6 86 163 28 22 13 
83 213 13 134 86 10% nonmdexed selechon 15 97 211 11096 17 44 1106 86 18172 1% selecuon using non-clustered 
index 781 103 2994 5 32 222 65 53 86 10% selection using non-clustered index 1682 2 16 11140 17 65 1107 
59 18200 1% selection usmg clustered mdex 059 -125 -750 10% selection using clustered mdex 126 -727 -69 
60 single tuple select 0 15 108 015 -020 352 entire files for most range selecuons While hash files are 
certainly the opmal file orgamzahon for exact-match quenes, for certain types of applicanons. range quenes 
are Important In such cases, it should be possible for the database admmlstrator to specify the storage 
orgamzahon that is best suited for the applicahon As discussed in Sechon 4, since the SemanhCS of QUEL 
and SQL are different, the results presented m Table 1 (and Table 2 below) for the two database machmes 
are not directly comparable In parncular, the Teradata tunes could be reduced ngmficantly If a bulk update 
mechamsm were implemented The overhead Imposed by the current recovery mechamsm 1s estimated m [DEWI87] 
and [DEWISS] 53 An Analysrs of Selection Performance m Gamma In tlus sechon we study how the response 
time for both the nonmdexed and indexed selecnon quenes on the 100,000 tuple rela- tions is affected 
by the number of processors used and the disk page size Ideally one would hke to see hnear unprovement 
m perfor- mance relanve to the number of processors used 5 2 1 Constant Page SW.+ Varymg Configuration 
In the first set of expenments, the disk page size was kept at 4 Kbytes while the number of processors 
with disks was mcreased from 1 to 8 Thus, as the number of processors 1s mcreased, the number of tuples 
stored at each site IS reduced propomonally NonIndexed SelectIons Wlthout an index, all the data pages 
in the relahon must be. read from disk and processed Increasmg the number of processors used to process 
a non-indexed selection mcrease.s both the aggregate CPU power and I/O bandwidth avadable wlnle reducing 
the number of tuples that must be processed at each processor In Figure 1, the average response hme for 
0%. l%, and 10% selecuons on the 100,000 tuple relation 1s presented as a funchon of the number of processors 
with d&#38;s As expected, the response tune for each of the quenes decreases as the number of sites 1s 
increased The response hme for the quenes with 1% and 10% selechvity factors is worse than the 0% query 
due to the cost of transmithng and stonng the result tuples Wlule for selecuon-only quenes one might 
store all result tuples locally, by partlnonmg all result relations m a round-robin (or hashed) fation 
one can msure that each fragment of every result relation will contam approxl- mately the same number 
of tuples The speedup curve correspondmg to Figure 1 1s presented m Figure 2 As shown m Figure 2, almost 
hnear speedup IS obtamed for all three quenes The reason that the 0% selechon query does not achieve 
perfect speedup is that the number of end of stream messages (see Secbon 2) each processor must send 
increases as pm- cessors are added to the system The 10% selectivity speedup curve IS not as close. to 
lmear as the 0% or 1% curves due to the effects of short-crrcurnng (agam see Section 2) When a single 
processor is used, all result tuples are short-arculted by the low-level com-mumcauons software As more 
processors are used, the fraction of tuples short-clrcmted decreases (urlth n processor, l/n th the result 
tuples will be short-circuited) While the actual Mwork is never a bottleneck [GERB86, GERB87], the bandwidth 
from memory to the commumcahons network 1s lumted by the speed (4 megabits/second) of the Umbus on the 
VAX 1 l/750 As the selec- uvlty factor of a query is mcreased and the number of short clr- culted tuples 
decreases, the path to the network becomes a bottleneck This fact IS Illustrated by the differences among 
the curves m Figure 2 Indexed Selections For tius smte of tests, we constructed, respechvely, clustered 
and non-clustered mdlces on the Umquel and Umque2 attnbutes of the 100,CUO tuple relations In Rgure 3, 
the average response time 1s plotted as a funchon of the number of processors wltb d&#38;s for RESPONSE 
TIME (SECONDS) 130 - A 0% selection 120. 1% selection 110. 10% selechon 100. 90. SO- 70. 60. 50. 40. 
30. 20. lo- 04 . .  . - . . s 0 1 2 3 4 5 6 7 8 PROCE SSORS WITH DISKS Figure 1 SPEEDUP t 0% SekChvlty 
1% selecnvlty q 10% selechvity 8. 7. 6. 5- 4. 3- 2- I- -0 1 2345678 PROCESSORS WITH DISKS Figure 2 the 
followmg three quenes 1% selection using a clustered ,ndex, 10% selection usmg a clustered index, and 
1% selecbon usmg a non-clustered mdex The correspondmg speedup curves arc presented m Figure 4 along 
\~ltb the speedup curve obtamed for a 0% selechon through a non-clustered mdex No results are presented 
for a 10% non-clustered mdex SCkChOn as our optimlzcr IS smart enough to choose to use a segment scan 
for tis query The speedup curves presented m Figure 4 reveal a number of mtereshng mslghts mto the effects 
of mcreasmg the amount of parallelism when maces are employed First, m the case of the 0% selechon query, 
the response hme for the query aChdy mcreascs (from 025 to 058 seconds) as the number of processors IS 
increased Tlus happens because the cost of lmtiatmg a select and store operator at each processor appears 
to be shghtly higher than the cost of performing l-2 I/O operations to search the index before dlscovenng 
that no tuples satisfy the predicate Of the remammg quenes, only the 1% selection through a non-clustered 
mdex comes RESPONSE TIME (SECONDS) 36-A 1% clustered index selectlon 0 10% clustered index selechon 32. 
0 1% non-clustered index selecnon 28. 24. 20. 16. 12. 8. 4- 0 1 2 3 4 5 6 7 8 PROCESSORS WITH DISKS 
 Figure 3 SPEEDUP IO-Cl 1% non-clustered index selecnon 9. 0 10% clusterd mdex selechon A 1% clustered 
index selection 8- + 0% clustered index selechfin 04 - = - - - 1 0 1 2 3 4 5 6 7 8 PROC ESSOR S W 
ITH DISKS Figure4 close to achlevmg lmear speedup Why is Uus, when, without an Index, the same quenes 
obtamed nearly hnear speedups? Consider first the 10% selection query Without an index, each processor 
executmg thus query will produce one network packet (2 Kbytes) of result tuples for appmxnnately every 
five 4 Kbyte pages It reads from disk With 4 Kbyte &#38;Sk pages the system IS I/O bound When the same 
query IS executed using a clustered mdex, once the first page contalmng quahfymg tuples IS found, every 
subsequent page read from disk wdl be completely full of result tuples Thus, for each data page read, 
two commumcatlon packets must be sent As the number of processors IS increased, the fraction of these 
packets that axe shoxt nrculted decreases Since the disk IS producmg pack- ets faster than the wmmumca~ons 
interface can place them on the network, performance degrades In the case of the 1% selection through 
a clustered Index. the same effect occurs but, as the number of processors IS mcreased, the nme to lmnate 
the query and process 2 levels of the mdex at 8 sites (0 58 seconds) becomes a slgnilicant fracnon of 
the total execunon hme of the query (2 seconds) Finally, the reason that the 1% selecbon through a non-clustered 
mdex achieves very close to lmear speedup IS that each disk page read requires a random seek, thus sigmficantly 
reducmg the rate at which the disk produces pages 5.2 2 Effect of Disk Page Sue on Selection Performance 
In tlus expenment, the configuranon SIZ was kept constant (8 processors with disks), wMe the disk page 
size was vaned fmm 2 Kbytes to 32 Kbytes usmg both sequennal and mdex scans on the 100,000 tuple relanons 
NonIndexed Selections Non-mdexed selecnons with O%, 196, 10%. and 100% selec- nvlty factors were executed 
with &#38;sk pages sizes rangmg from 2 to 32 Kbytes The response tunes for these quenes am plotted m 
Fig- ure 5 and the correspondmg speedup curves am presented m Figure 6 These results (most parncularly 
the 0% selection curve which does not generate any network traffic) clearly mdlcate that unth a 2 Kbyte 
sdlsk page the system IS Qsk bound and that once the page sue 1s mcreased to 16 Kbytes the system becomes 
CPU bound For the VAX 1 lI750 CPU (0 6 MIP), any mcrease m the sue of the disk page beyond 8K bytes has 
little or no effect on the response hme of the query Repeatmg these expenments with a faster CPU would 
be interestmg The results presented m Figures 5 and 6 provide further evl- dence that the network Interface 
can become a bottleneck With a 2 Kbyte page size, the response nme for the 10% selecnon IS 19 per- cent 
slower than the response time for the 0% selecnon With a 32 Kbyte page size, the 10% selection IS 50 
percent slower than the 0% selecnon It IS very clear that as one mcreases the rate at which result tuples 
are produced, (either by mcreasmg the size of the disk page or through the. use. of a clustered mdex), 
the network interface mcreasmgly becomes a bottleneck Indexed Selections We repeated the same set of 
expenments after construcnng a clustered mdex on the Umquel attnbute and a non-clustered mdex on the 
Umque2 attnbute In these tests, however, mcreasmg the sue of the disk page also Increases the fan out 
of the nodes of the B-tree index RESPONSE TIME (SECONDS) 80- A O%selechon 0 l%selemon 0 10% sele4zloll 
+ 100% selection 30. 20. 10. 0     * - 0 4 8 12 16 20 24 28 32 DISK P AGE SIZE IN KBYTES Figure 
5 SPEEDUP BESPGNSE TIME (SECONDS) 6~ /FA 0% selectron O 1% selectton q 10% selectton 01 - . . . * 
 - 0 4 8 12 16 20 24 28 32 DISK PA GE SIZE I N KBYTES Figure 6 The average response tune and correspondmg 
speedup curves for the quenes tested are presented m Figure 7 and 8 The most mterestmg results am those 
obtained for the 1% selecnon through a non-clustered mdex As mdtcated m both figures, any mcrease m disk 
page size degrades the perfommnce of thts query Since each tuple mtneved requires fetchmg two index pages 
plus one data page, the longer transfer trme for the larger pages dommates any advan- tage provided m 
terms of fan-out (For a 32 Kbyte disk page, the transfer nme is 13 mrlhseconds - whtch rs very close 
to the hme requtted to perform a random disk seek The Qsk used has a 40 Kbyte track srze) When a clustered 
index is employed, thts degradatton m per- formance does not occur because once the proper leaf page 
of the index is located all subsequent tuples Jn that page and all subse- quent leaf pages will sansfy 
the query Whtle the 10% selectton contmues to show unprovement wrth larger disk pages, the response me 
for the 1% selectron actually mcreases shghtly when the page size is mcreased from 16 to 32 Kbytes The 
longer transfer nme 1s agam the source of the problem Wrth 8 ptocessots, each sne will produce approximately 
125 tuples With 32 Kbyte pages, each page wrll hold approximately 150 tuples If the 125 tuples sattsfymg 
the query span two pages (the expected case), mom than 50% of the tuples read will not sahsfy the query 
6. Jom Queries In testmg loin performance m Gamma, we first we wanted to explore how a typtcal Gamma 
configuratton performs on a fixed set of Iom quenes as the stze of the input relanons rs mcreased Second, 
we wanted to explore how Gamma s performance IS affected as the number of processors wtth disks 1s mcreased, 
as the dtsk page size 1s mcreased, and as the amount of avatlable memory 1s reduced Join Algonthms The 
Jam algorithm used by the Teradata machme (for the quenes tested) involves first redtstnbutmg the two 
source relations by hashmg on the Join atmbute As each AMP recetves tuples, it stores them tn temporary 
files sorted m hash-key order After the redrstnbunon phase completes, each Ah4P uses a convenhonal sort- 
mergeJOT algonthm to complete the loin Gamma also partrnons rts source relations by hashmg on the Jam 
attnbutes but, mstead of usmg sort-merge to effect the Jam, 10 9 I\ 8 7. 6. 5. 4-3. 2. 1. 220-200- 180. 
160-140- 120. A 1% clustered mdex selectton 0 10% clustered mdex selechon 0 1 Q non-clustered mdex selechon 
 4 8 12 16 20 24 28 32 DISK PAGE SIZE IN KBYTES Rgure 7 SPEEDUP A 1% clustered index selechon 0 10% 
clustered index selectton 0 1% non-clustered mdex selecnon + 00% non-clustered mdex selectron  // - 
# /------A 0404 - - 0 4 8 12 16 20 24 28 32 DISK PAGE SIZE I N KBYTES Hgure 8 Gamma employs an algonthm 
based on hashmg (see [KITS83, DEWI85. DEWI86, GEBB86]) Dunng the first phase of the algo- nthm, Gamma 
phhOnS the smaller source mlanon by hashmg on the Iommg attnbute and butlds mam-memory hash tables Dunng 
phase two, Gamma partmons the larger source relanon and uses the correspondmg tuples to Immediately probe 
the hash tables bum dur- mg phase one Of course, whenever mam-memory hashmg ts used there IS a danger 
of hash-table overflow Gamma currently uses a drstnbutcd version of the Sample hash-pamnoned Jan algonthm 
described in [DEW1851 to handle thrs phenomenon Bastcally, whenever a pro- cessor detects hash-table 
overflow rt spools tuples to a temporary iile based on a second hash functron unttl the hash table 1s 
success- fully built. The query scheduler then passes the funcnon used to subparttnon the hash table 
to the select operators producmg the probmg tuples Probing tuples correspondmg to tuples in the overflow 
partmon are then spooled to a correspondmg temporary file, all other tuples probe the hash table as normal 
The overflow pamtions are recursively Joined usmg tis same procedure untd no more overflow parnnom are 
created and the Jom has been fully computed Gamma can actually run Joins m a vanety of modes The selecnon 
operators will, of course, run on all disk sites but the hash tables may be built on the processors with 
d&#38;s, the diskless proces- sors, or both sets of processors These three altemahves are referred to 
as Local, Remote, and Allnodes, respecnvely Querws Three Join quenes formed the bans of our Jom tests 
The first ~0x1 query, JOmABpnme, IS a sunple JOUI of two ElanOns A and Bprune The A relation contams 
either 10,000, 100,000 or 1,000,ooO tuples The Bpmne relanon contams, respechvely, 1,000, 10,000, or 
100,000 tuples The second query, ~omAselB, performs one Jom and one selecnon A and B have the same number 
of tuples and the selection on B reduces the size of B to the size of the Bprune relation in the corresponding 
JomABprnne query For example, if A has 100,000 tuples, then JomABpnme JOTS A with a Bprune relahon that 
contams 10,000 tuples, whde m ~omAselB the selecuon on B resmcts it from 100.000 to 10,000 tuples and 
then JOmS the mS&#38; with A The thud Jom query, JoinCselAselB contams two ;oms and two resmcts Fwt, 
A and B are restncted to 10% of thtlr ongmal stze (lO,COO, 100,000, or l,OOO,OOOtuples) and then JXned 
with each other Smce each tuple moms with exactly one other tuple, this Jam ylelJs an mtermediate relation 
equal m size to the two input relanons Tlus mtermedlate relation is then Jomed mth relation C, which 
contams l/10 the number of tuples in A The result relation contams as many tuples as then: are m C For 
example, assume A and B contam 100,000 tuples The relations resultmg from selec- uons on A and B will 
each contam 10,000 tuples Then Jam results m an mtermedrate relanon of 10,000 tuples This relanon WLU 
be. Jomed with a C relation contammg 10,000 tuples and the result of the query will contam 10,000 tuples 
6 1 Performance Relative to Relation Size The first vanmon of the three quenes tested involved no indices 
and used a non-key (non-partmonmg, non-indexed) attnbute (umque2D or umque2E) as both the Join and selection 
atmbutes Smce all the source relations were dlsmbuted using the key attn- bute, the Jom algonthms of 
both machmes reqmred redistnbuhon phases The results from these tests are contamed m the first 3 rows 
of Table 2 For thus senes of tests, Gamma used 4 Kbyte disk pages and all Join quenes were performed 
m the Remote mode m which the Joins are done only on the diskless processors The second vananon of the 
three Join quenes used the key attnbute (umquelD or umquelE) as the Jom attnbute (Rows 4 through 6 of 
Table 2 contam these results ) Since, m tis case, the relahons are already dismbuted on the Jom attnbute, 
the Teradata machme demonstrated substantial performance improvement (25- 50%) because the redlsmbution 
step of the Jom algonthm could be slupped In Gamma s case, however, both relanons stdl had to be redlsmbuted 
smce only d&#38;less processors were used for the loins From the results m Table 2, one can conclude 
that the execu- non time of each of the quenes increases m a farly linear fation as the size of the input 
relations are increased Gamma does not e&#38;- bit lmeanty m the mdhon tuple quenes because the size 
of the building relation (20 megabytes) far exceeds the total memory avadable for hash tables (4 8 megabytes) 
and the Sunple hash- parhnon overflow algorithm detenorates exponennally with muln- ple overflows In 
fact, the computanon of the million tuple JOT quenes required six partitlon overflow resoluuons on each 
of the diskless processors In Secnon 6 2 2, we explore m more detad the impact of limited memory on the 
performance of Join quenes in Gamma The observant reader may have noticed that the Teradata can always 
do JomABpnme faster than ~omAselB but that Just the oppo- site IS true for Gamma We will explam the &#38;ffemnce 
by analyzing Table 2 with the 100,000 tuple moms Selechon propagation by the Gamma optimizer Rduces JOtthhelB 
to ~omselAse.lB This means that although both 100,OCO tuple relauons will be read m then ennrety only 
10% of each of the xelahons will be sent over the net- work and pamcipate in the Jom Although JomABpnme 
only reads a 100,000 and a 10,000 tuple relaaon It must send all 100,000 tuples to the diskless processors 
to effect the Jom Thus, the. cost to dlstn- bute and probe the 100,OCO tuples outweigh the mfference 
m read- ing a 100,000 and a 10,000 tuple Ne On the other hand, the Tera- data database machme will compute 
JomABpnme by reading and somng a 10,000 tuple relabon and a 100,000 tuple relahon and then merging them 
JomAselB wdl read two 100,000 tuple relanons and then sort and merge a 10,000 and a 100,ooO tuple relanon 
Thus JomAselB wdl be slower by the difference m reading the 100,000 and 10,000 tuple It?lahOnS 6.2 An 
Analysis of Jom Performance m Gamma In thts secnon. we explore the effects of changmg the size of the 
disk page, reducmg the amount of buffer space aviulable for Jom hash tables, and the performance of Jom 
quenes relattve to the Table 2 Join Queries (All Execution Times m Seconds) Number of Tuples m Source 
Relatron 14m 10,ooo 100,ooo 100,ooo 1,ooo,ooo 1,ooo,000 Query Descr@on Teradata Gamma Teradata Gamma 
Teradata Gamma JomABpnme urlth non-key attnbutes 349 65 321 8 47 6 3,419 4 2,938 2 of A and B used as 
Join attnbute JOi&#38;&#38;lB with non-key attnbutes 356 51 331 7 34 9 3,534 5 703 1 of A and B used 
as Mom atMute JomC:elAselB with non-key aflnbutes 278 70 191 8 380 2,032 7 7312 of A and B used as Join 
atmbute ~omABpnme with key attnbutes 222 57 x31 3 45 6 1,265 1 2.926 7 of A and B used asJom atmbute 
JomASelB wnh key atmbutes 250 50 1703 341 1,584 3 737 7 of A and B used as Join attnbute JomCselAselB 
with key attnbutes 238 72 156 7 37 4 1,509 6 712 8 of A and B used asJoin atmbute number of processors 
avarlable Whde we would have preferred to use the mllllon tuple relations for these expenments, we do 
not have enough aggregate memory to execute the mdhon tuple Join quenes without expenencmg partmon overflow 
Thus, smce we did not want the cost of processmg the overflows to unpact every test con- ducted, we chose 
to nm the expenments usmg the 100,000 tuple RlilhOll.9 6.2.1 Constant Memory, Constant Page Bze, Varymg 
Configuration In the first senes of tests we wanted to explore how Joins per- formed when we mcreased 
the number of processors with disks attached 3 In order to concentrate on the effects of changmg Gamma 
s conliguraUon we kept the disk page size constant at 4K bytes and kept the amount of memory avmlable 
for Join hash tables large enough to msure that no pamuon overflow would occur Figures 9 and 10 present, 
respecuvely, the response tune for the JOtdBpnme query when the Joimng attnbutes are also the key (pamuomng) 
attnbutes and when they are not the partmonmg attn- butes From the shape of these graphs it is obvious 
that Gamma ngmficantly reduces response Ume as addmonal processors are added One, though, might expect 
Remote JOIUS to be twice as fast as Local moms because Remote loins use twice as many processors As was 
pomted out m [DEWI86] tlus 1s not the case because the bmldmg and pmbmg phases of the jam operator are 
not overlapped and hence the response Ume of the query IS bounded below by the sum of the elapsed hme 
of these two phases In a multmser environment, though, it 1; expected that offloading the Join opera- 
tors to remote processors will allow the processors with disks to effecuvely support more concurrent 
selecUon and store operators The validity of thus expectation ~rlll be determined m future mul- Uuser 
benchmarks of the Gamma database machme An mterestmg feature of Figures 9 and 10 is that, for larger 
ConliguraUons, the relauve performance of Local and modes loins 1s mirmred with respect to Remote moms 
(which remam constant) For JO~S on pamUonmg attnbutes, the Local ConfiguraUon is RESPONSE TIME (SECONDS) 
 360" k330. 300. A 0 Local JOlnS Remote Joms 270. o Allnodes Joms 240. 210. 180. 150. 120. 90' 60. 30. 
o+ . - - * . - i 0 1 2 3 4 5 6 7 8 PROCESSORS WITH DISKS Figure 9 3 Remember when we add a processor 
wtb a disk we also add a proce.wor wuhout a disk These diskless processors are exploItedby the Remote 
and AlllIds pm RESPONSE TIME (SECONDS) 360. 330. g 300. 270. 240. 210. 180. 150. 120. 90. 60. 0 1 2345678 
PROCESSORS WITH DISKS Figure 10 fastest, followed by Allnodes and Remote JOUIS When an attnbutc other 
than the parUUomng attnbute is used as the Joinmg attnbute, the Remote configurauon 1s the fastest followed 
by Allnodes and then finally Local Both graphs are ldenucal for the smgle process01 configuraUon because 
the relauons are stored enurely on the smgk disk and hence no parUhomng of the data occurs Ihs mnmr-bke 
performance funcuon occurs because Garnm? uses the same hash fimcUon to pamuon relauons when they are 
being loaded and when they are being Jomed Hence, when the Jommg and parUuonmg atmbutes are the same, 
Local ~olns will short-arcun all mput tuples and gam a conespondmg performance advantage Conversely, 
when JOinS an? performed on non-partmoning attnbutes, Local moms perform worst because short- arctutmg 
provides no benefit and we have substanhtiy increased contention for the CPUs wuh disks since the bulldmglprobmg 
of the hash tables competes with the selecUon and stem operators The performance of the Allnodes configurauon 
falls between the Remote and Local configurauons as it shares the benefits and drawbacks of both. The 
aSS0~1ated SpeedUp curves for the JOmABpIXne quenes are shown in Figures 11 and 12 Nohce that near lmear 
speedups are obtamed Both speedup cmves were plotted using the response tune for two processors as a 
referer.ce pomt 111 order to reduce skew- mg the curves due to short-clrcultmg Tlus can be best explamed 
by the f&#38;owing example Consider a smgle-processor configurauon with Joins bemg done on then non-partmomng 
attnbutes For Local JOI% a11 tup1es will short-arcuit the network With two processors, approximately 
half the Nples will be short-cn cmted In general, as the number of processors 1s mcreased, the number 
of short-arcmted packets IS reduced proportionally Because these mtra-node packets are much less expensive 
than their correspondmg mter-node packets, smaller configurauons will benefit more from short-cncmtmg 
Smce one Intent of plottmg these speedup curves 1s to proJect Gamma s per- formance as addluonal processors 
are employed, usmg the response tune obtained wth a single processor as their basis WIII give arUficially 
low expected pe3formance eshmates for larger configurauons A slmdar argument can be made for Allnodes 
Joins although the degree of short-cncultmg will be approximately half that of Local JoIns Remote JOIIB 
are @sically unaffected by the change in reference pomt Since the two processor configuration stdl short-circuits 
half its tuples, the speedup results still SPEEDUP 6.2 2 Jorn Overflow 8- 7. 6. 5. 4- 3. 2. l- v-0 i 
2 ;45i;i PROCESSORS WITH DISKS Figure 11 SPEEDUP * Local Joms o Remote Joms q Allnodes Joms P 1 2 345678 
PROCESSORS WITH DISKS Figure 12 underestimate the scalablhty of Gamma As an example, the speedup from 
four to eight processors for non-hash partmoned Jams done locally 1s approximately 175 Whde these expenments 
only tested Gamma when the source relations are hash partitioned, the loins performed usmg non-parhhomng 
attnbutes (Figures 10 and 12) have the same perfor- mance as loins on eqmvalent size relat.Ions partluoned 
m a mund- robin fastion 4 Additionally, recall that Joins done on non hash- partmoned attnbutes can be 
performed faster remotely than locally This shows that Join operators can indeed be off-loaded to remote 
processors even for large relations Tlus substantiates results obtamed m [DEWI86] for smaller relations 
Round-robm parutloma 1s the default strategy for relatmns created as the result of a query In Uus set 
of expenments, we kept both the contiguranon size (16 query processors) and disk page size (4 Kbytes) 
constant but vaned the total amount of memory Avadable memory was imhally set to be sufficient to hold 
the total number of tuples reqmred m the budding phase of the lOO,!XO tuple Jam queries, 1 e sufficient 
to build 10,COO tuples across the avadable processors The total amount of avrulable memory was then mcrementally 
lowered by evenly reducmg memory from each of the processors From the. shape of the curves in Figure 
13 it IS obvious that performance detenorates rapidly as memory becomes more hmited due to our use of 
a dlstnbuted version of the Simple Hash Jam algo- nthm to resolve hash-pamhon overflow (as predicted 
analfically m [DEWIU]) When viewmg the graphs it should be kept in mmd that the number of overflows represents 
the number of overtlows detected at each of the eight Jommg sites Thus, the total number of occurrences 
of parUUon overflow IS the labeled number tunes eight A few very mterestmg pomts can be dIscovered by 
careful exammanon of the curves m Figure 13 First, why do the response mes for the Local and Remote Jam 
cmvers cmssovefl Recall, from the previous subsechon, that JOW on partmomng attnbutes can be done faster 
locally than mmotely, but that lust the opposite is tme for the same loins done on their non-parUtlomng 
attnbutes The crossover can be erplamed because, after the imtml overllow, Gamma switches hash funchons 
This has the effect of changmg the Jommg attnbutes to non-partmomng attnbutes ms change In hash functions 
1s necessary m order to ensure that allJommg proces- sors are. used m the case when only a subset of 
s&#38;s overfiow If the same funcuon was used to dlstnbute both overflow tuples and the ongmal tuples, 
the same seta of tuples would contmuously re-map to the same processors Thus, processors that do not 
expenence overflow would not be used for subsequent overflow pmcessmg Also the relative flatness of the 
response time curves from zero to two overflows mdicates that Simple hash-Jam is effective when only 
a small number of overflows occur ms result is tmpor- tant because it means that the optumzer can be 
off by a factor of two m estunatmg either the amount of memory available or the selec- hvity factor of 
an operator without sigmficantly affectmg the mpmse nme of the query RESPONSE TIME (SECONDS) mo- A Local 
Jams o Remote Joms 450. 400. 350. 300. 250. 200. 150. 100. 50. 04 I 000 020 040 060 080 100 120 AVAILABLE 
MEMORY/SMALLER RELATION Figure 13 6.2 3. Effect of Disk Page Size on Jom Performance In the next set 
of cxpenments we wanted to explore the effect of alternative disk page sizes on JOIII execunon time A 
constant Gamma configuration consrstmg of 16 query processors (8 with disks) and a scheduhng processor 
was used Memory was also kept constant and large enough so that no hash-table overflows would occur Figure. 
14 shows the results of the JomAselB query as the disk page size 1s vaned from 2 to 32 Kbytes As can 
be seen, mcreasmg the disk page size sigmficantly reduces Join response nme although the performance 
improvement levels off at 16 Kbyte pages The associated speedup curves are presented m Figure 15 One 
may wonder why the speedup curves level off m the observed manner Recall that, m Gamma, JOUIS are bounded 
by the RESPONSE TIME (SECONDS) 70 A Local Joins 0 Remote Jams 63 o Ahodes Joins I System Constant Configuration 
(17 nodes) Constant Memory (no overflow) 100,000 tuple relauons Query JomAselB (Jom attrs = Parhhomng 
attrs) . 0 8 16 24 32 DISK PAGE SIZE IN KBYTES Figure 14 SPEEDUP 1 A Local Jams 0 Remote Joms q AIlnodes 
Joms - - -  . 0 4 8 12 16 20 24 28 32 DISK PAGE SIZE IN KBYTES Figure 15 tune to select tuples from 
the Jommg relanons Since Figure 15 presents the results obtamed using the JomAselB query, 10% selec- 
tions were performed on both source relahons Hence, the results obtamed are sumlar to those presented 
for the 10% non-indexed selecnon in Figure 6 One result we have not been able to explam to our sahsfachon 
IS the performance of the Allnodes configuranon Inhutlvely. one would expect AJlnodes to always fall 
between Remote and Local because it shares the benefits and drawbacks of both One possible explanahon 
is the increased cost of schedulmg A&#38;odes loins when the relahons bemg Joined are not large enough 
to fully exploit the addmonal processmg power Since Gamma requires four messages to schedule a query 
operator per node and since a Join 1s logically composed of two operators (bmld and join) Allnodes WIU 
require 64 addlhonal schedulmg messages Assummg seven mdhseconds for a small mter-node message about 
a half of a second of addmonal scheduling overhead is mcurred Thus explanahon appeafi to be a posslblhty 
because AlJnodes JOIIIS do fall between Local and Remote JOUIS when perforfmng JOtiBpNXIe quenes 7 Update 
Querw The Enal set of tests Included a mix of append, delete, and mod@ quenes The Teradata machme was 
execuhng with full con- currency control and recovery, whereas Gamma used full con-currency control and 
pamal recovery for some of the operators The results of these tests are presented 111 Table 3 The first 
query appends a smgle tuple to a t&#38;&#38;on on which no mdlces exist The second appends a tuple to 
a relatton on wluch one mdex exists The thmi query deletes a smgle tuple from a rela- tion, usmg an mdex 
to locate the tuple to be deleted (m the case of Teradata, it 1s a hash-based mdex, whereas m the case 
of Gamma, it IS a clustered B-tree mdex, for both the second and thud quenes) In the first query no m&#38;ces 
exist and hence no mdtces need to be updated, whereas m the second and thnd queries, one mdex needs to 
be updated The fourth through SIX&#38;I quenes test the cost of mtifymg a tuple m three Qfferent ways 
In all three tests, a non-clustered index exists on the wque2 attnbute on both ma&#38;me-s, and m addl- 
hon, m the case of Gamma, a clustered mdex exists on the umquel attnbute In the East case, the m&#38;tied 
attnbute 1s the key attnbute, thus reqmnng that the tuple be relocated Furthermore, since the tuple 1s 
relocated, the secondary mdex must also be updated The Efth set of quenes mod@ a non-key, nomndexed atmbute 
The Enal set of quenes modify an attnbute on wluch a non-clustered index has been constructed. usmg the 
mdex to locate the tuple to be mod&#38;d As can be seen from Table 3, for the fomth and sixth quenes, 
both machmes use the m&#38;x to locate the tuple to be modified Since modlfymg the mdexed attnbute value 
wdl cause the tuple to move posltlon wtthm the mdex, some systems avoid using the Index to locate the 
tuple(s) to be modified and mstead do a fde scan Whde one must mdeed handle tis case carefully, a fde 
scan 1s not a reasonable solunon Gamma uses deferred update Eles for mdlces to handle 011s problem We 
do not know what solution the Tera- data machme uses for thy problem Although Gamma does not provide 
loggmg, it does provide deferred update files for updates using index structures The deferred update 
Ele corresponds only to the index structure and not the data file The overhead of mamttumng Uus funchonallty 
is shown by the difference m response times between the East and second rows of Table 3 Tim problem 1s 
known as the Halloween problem m DB folklore Table 3 Update Queries (All Execution Tlmcs m Seconds) 
 Number of Tuples m Source Relation 10,ooo 10,ooo loo@0 100,ooo 1,ooo,ooo 1,ooo,ooo Teradata GammaTeradata 
Gamma Teradata Gamma Append 1 Tuple (No mdlces exist) 0 87 Append 1 Tuple (One mdex exists) 094 Delete 
1 tuple 071 Modify 1 tuple 2 62 Modify 1 tuple (Mod&#38;d a non-indexed attnbute) attnbute is odd100 
0 49 Modify 1 tuple usmg a non-key attnbute with rim-clistered ixiex - 0 84 8 Conclusions In tis report 
we presented the results of an imhal evaluation of the Gamma database machme both by companng its performance 
to that of a Teradata DBC/1012 database machme of similar size and by exammmg the performance of Gamma 
relahve to the number of processors used Based on these results one em draw ,I number of conclusions 
Gamma s most glarmg deEclencles are the lack of full recovery features and the ermely poor performance 
of the dlstnbuted Simple hash-Jam algorithm when a large number of overflow opemhons must be processed 
The solution we are m the process of adoptmg is to replace the current algorithm with a paral- lel version 
of the Hybnd hash-Join algontbm [DEWI84, DEW1851 We also mtend on Implementmg a recovery server that 
wdl collect log records from each processor Based on the experunents m which we vaned the disk page size 
used by Gamma, one can conclude that we should mcrease the default page size from 4 to 8 Kbytes Wlule 
mcreasmg the page size beyond 8 Kbytes provides slight unprovement for some quenes, the impact on quenes 
that use m&#38;ces (m particular, non- clustered mdlces) 1s very negahve While these results may not 
be generally apphcable, they seem to mdlcate that adoptmg track-size pages (as a number of expemnental 
systems are talkmg about doing) may not be a wise declwon Fmally, it 1s very clear that the network interfaces 
used m Gamma present a senous bottleneck We have almost completed the implementanon of a co-processor 
board that can transfer packets from memory onto the token nng at a rate of 40 megabtts/second We are 
also plannmg on portmg the Gamma software to &#38; Intel IPSC-32 multiprocessor 9 Acknowledgements A 
number of people helped make this paper possible Bob Gerber deserves special recogmhon for his work on 
the design of Gamma plus his leader&#38;p on the unplementatlon effort M Murahknshna, Anoop Sharma, RaJlv 
Jauhan, Goetz Graefe and Joanna Chen were mstrumental m turnmg Gamma mto a workmg system ms research 
was partially supported by the Defense Advanced Research ProJects Agency under contract NOOO39-86-C- 
0578, by the National Science Foundanon under grants DCR- 8512862, MCS82-01870, and MCS81-05904, and 
by a Digdal Equipment Corporation External Research Grant The funding for the Teradata study described 
m [DEW1871 was provided by MCC 10. References <RefA>[BABB791 Babb, E, Implementmg a Relahonal Database by 
Means of Speclahzed Hardware ACM TODS, Vol 4, No 1, March, 1979 0 18 129 018 147 020 060 1 62 063 173 
066 044 042 056 071 061 101 2 99 086 4 82 1 13 036 090 036 1 12 036 050 I 16 046 372 052 [BITT831 Bltton 
D , D J Dewitt, and C Turbyfill, Benchmarkmg Database Systems - A Systematic Approach, Proceedmgs of 
the 1983 Very Large Database Conference, October, 1983 IBRATS41 Bratbenrseneen. Knell. Hashm~ Methods 
and Rela- uoni Algebra-O&#38;n&#38;o&#38; Proceedm&#38; of the 1984 Very Large Database Conference, August, 
1984 [CHOU851 Chou, H-T, Dewitt. D J, Katz, R, and T Klug, Design and hnplementahon of the Wisconsm Storage 
System (WlSS) Software Practices and Expenence, Vol 15, No 10, October, 1985 [DEW1841 DeWm, D, et al, 
Implementahon Techniques for Mam Memory Database Systems, Pmceedmgs of the 1984 SIGMOD Conference, Boston, 
MA, June, 1984 [DEWISS] Dewitt, D, and R Gerber, Multiprocessor Hash-Based Join Algonthms, Proceedings 
of the 1985 VLDB Conference, Stockholm. Sweden, August, 1985 [DEWIS Dewitt, D , Gerber, B , Graefe, G 
, Heytens, M , Kumar, K and M Murahknshna, GAMMA - A High Perfor- mance Dataflow DatabaTe Machme. Proceedings 
of the 1986 VLDB Conference, Japan, August 19b6 [DEWI87] Dewitt, D , Smith, MI and H Boral, A Smgle-User 
Performance Evaluation of the Teradata Database Machine, MCC Techmcal Report Number DB-081-87, March 
5,1987 [DEWI88] Dewitt, D , Ghandehanzadeh, S , and D Schneider, A Performance Analysis of the Gamma 
Database Machine. Computer Sciences Techmcal Report #742, Jan 1988 [GERBS61 Gerber, RI Dataflow Query 
Pmcessmg using Mul- uprocessor Hash-Partitroned Algonthms, Computer Sciences Techmcal Report #672, October 
1986 [KITS831 titsuregawa, M , Tanaka, H , and T Moto-oka, Apph- canon of Hash to Data Base Machme and 
Its Architecture, New Gcneratlon Computmg, Vol 1, No 1. 1983 [PROTSS] Protean Associates, Waltham, Mass, 
1985 [RIES78] RIGS, D and R Epstein, Evaluahon of Dlstnbutlon Cn- ter 2 for bstnbuted Database Systems, 
UCB/ERL Technical Re?ort M78f22, UC Berkeley, May, 1978 [SELB?] Selmger,P G , et al, Access Path Selection 
m a Rela- uwd Database Management System, Proceedings of the 1979 SIGMOD Conference, Boston, MA, May 
1979 [SlON76] Stonebraker, Michael, Eugene Wong, and Peter Krcps, The Design and Implcmentahon of INGRES 
, ACM TODS, Vat 1, No 3, September, 1976 [TERA83] Teraddta Corp. DBCIIOIZ Data Base Computer Con- cepts 
&#38; Facdltm, Teradata Carp Document No CO2-OOOl-00,1983 </RefA> 
			
