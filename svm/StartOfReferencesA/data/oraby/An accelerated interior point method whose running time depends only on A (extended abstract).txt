
 An Accelerated Interior Point Time Depends Only on Stephen A. Vavasis* Abstract We propose a layered-step 
interior point (LIP) algorithm for linear programming. This algorithm follows the central path, either 
with short steps or with a new type of step called a layered least squares (LLS) step. The al­ gorithm 
returns the exact global minimum af­ ter a finite number of steps in particular, after *Department of 
Computer Science, Upson Hall, Cor­nell University, Ithaca, NY 14853. Email: vava­sis@cs.cornell. edu. 
This work is supported in part by the National Science Foundation, the Air Force Office of Scientific 
Research, and the Office of Naval Research, through NSF grant DMS-8920550. Also supported in part by 
an NSF Presidential Young Investigator award with matching funds received from AT&#38;T and Xerox Corp. 
Part of this work was done while the author was visiting Sandia National Laboratories, supported by the 
 U.S. Department of Energy under contract DE-AC04­76DPO0789. tDepartment of Management Sciences, The 
Uni­versity of Iowa, Iowa City, IA 52242. E-mail: bbuyinG%.xa.weeg.uiowa.edu. This author is supported 
in part by NSF Grant DDM-9207347. Part of this work was done while the author was on a sabbatical leave 
from the University of Iowa and visiting the Cornell Theory Center, Cornell University, Ithaca, NY 14853, 
supported in part by the Cornell Center for Applied Mathematics and by the Advanced Computing Research 
Institute, a unit of the Cornell Theory Center, which receives major funding from the National Science 
Foun­dation and IBM Corporation, with additional support from New York State and members of its Corporate 
Re­search Institute. Permission to copy without fee all or part of this material is granted provided 
that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice 
and the title of the publication and its date appear, and notice is given that copying is by permission 
of the Association of Computing Machinery. To copy othetwise, or to republish, requires a fee and/or 
specific permission. STOC 94-5194 Montreal, Quebec, Canada @ 1994 ACM 0-89791 -663-8/94/0005..$3.50 
Method Whose Running A (extended abstract) Yinyu Yet 0(n35c(A)) iterations, where c(A) is a function 
of the coefficient matrix. The LLS steps can be thought of as accelerating a path-following inte­rior 
point method whenever near-degeneracies occur. One consequence of the new method is a new characterization 
of the central path: we show that it composed of at most n2 alter­nating straight and curved segments. 
If the LIP algorithm is applied to integer data, we get as another corollary a new proof of a well­known 
theorem by Tardos that linear program­ming can be solved in strongly polynomial time provided that A 
contains small-integer entries. Our complexity result, however, generalizes to real-number data. 1 Interior 
point methods In this report we consider solving a linear pro­gramming problem (LP) in the following 
form: minimize b~y (1) subject to ATy > c. Here, A is an m xn matrix assumed to have rank m, b G lR~ 
and c c IR are given vectors, and y c lRm is the unknown vector. This format of LP is commonly known 
as dual format . We propose a path-following interior point method for this problem. In the taxonomy 
of interior point methods, our method would be called a dual path following method, but it has some features 
of a primal-dual method. In particular, our algorithm produces both a primal and dual optimal solution. 
Interior point methods were originally intro­ duced by Karmarkar [9], and the first path­following method 
is due to Renegar [16]. Some of the main ideas of interior point methods in particular, the log-barrier 
function described below date back to the 1960 s [6]. Existing path-following methods take small steps 
along the centraz path until they are sufficiently close to the optimum. Once sufficiently close, a rounding 
procedure such as Khachiyan s [10] or least-squares computation such as Ye s [29] is used to obtain a 
global minimum, In our new method, we interleave small steps with longer Jagered least squares (LLS) 
steps to follow the central path. Thus, our method is always at least as efficient as existing path­following 
interior point methods. The last LLS step moves directly to the global optimum. Thus, our algorithm, 
which we call layered­step interior point (LIP), terminates in a finit e number of steps. Furthermore, 
the total num­ber of iterations depends only on A: the run­ning time is 0(n3 5c(A)) iterations, where 
c(A) is defined by (8) below. This is in contrast to all previous interior methods, whose complexity 
depends on b and c as well as A. This is important because there are many classes of problems (e.g., 
flow prob­lems) in which A is well-behaved but b and c are arbitrary vectors. In order to provide intuition 
on how the LIP algorithm works, and why our complexity bounds are stronger, we consider the linear pro­gramming 
problem presented in Figure 1. The problem solved in this figure is: minimize 2y1 + 5yz subject to O< 
VI < 1, O<yz <l, !/1+ W2 2 ~, where c = 0.1. The dashed line indicates the boundaries of the feasible 
region defined by these constraints. The solid line is the central path, which is defined in Section 
2. It connects the point A, known as the analytic center, to the point D, the optimum solution. Note 
that the optimum in this example is at a vertex of the feasible region it is always the case that the 
Figure 1: Example of an interior point method. 11 r ------------------------I I i J 0.8 ­ 0.6 ­ / % 
x I I 1 0.4 ­0.2 ­0 ­ <li I. ----- --- I o 0.2 0.4 0.6 0.8 1 optimum is achieved at a vertex, and for 
a non­degenerate problem (such as this one) the opti­mum is unique. The asterisks show the iterates of 
a conventional path-following interior point method. Such a method generates iterates ap­proximately 
on the central path with spacing between the iterates decreasing geometrically. Once the iterates are 
sufficiently clclse to the op­timum D, where sufficiently close means that the current iterate is closer 
to D than to any other vertex of the feasible region, then the ex­act optimum may be computed. Figure 
2 pro­vides a close-up view of the region near (0, O) from this problem. Now, consider what happens as 
we let c get smaller in this problem. This creates a near degeneracy near (O, O), and the dliagonal seg­ment 
in Figure 1 gets shorter and shorter. This means that the conventional methcjd must take more and more 
iterations in order to distinguish the optimal vertex (c, O) from the nearby ver­tex (O, ~/2). In fact, 
it can be proved that a standard method applied to this problem would require 0( I log c1) iterations. 
Note that e is a component of right-hand side vector c; this ex­plains why existing methods have complexity 
depending on c. The running time of all previ­ 0.1 0,08 -1 I ! I I I 1 I I I 0.06 -:   L_  ~ o 0.02 
0.04 0.06 0.08 0.1 Figure 2: A close-up of the lower-left corner of Figure 1. ous polynomial-time interior 
point methods is written as O(@L) (or sometimes 0(7zL)) iter­ations, where L is the number of total bits 
re­quired to write A, b, c. Thus, for this example, L depends logarithmically on c. (Each iteration requires 
O (m2n) operations to solve a system of linear equations.) In contrast, our method would jump from point 
B directly to point C without interven­ing iterations. This is accomplished by solving a weighted least-squares 
problem involving the three constraints VI 2 0, ya 2 0, VI + 21J2 > E, which are near active at optimum. 
In a more complex example, there would several lay­ers involved in the least-squares computation. When 
point C is reached, the LIP method takes small interior point steps, the number of which depends only 
on the coefficient matrix A. After these small steps, the LIP method jumps again directly to optimal 
point D. Ours is not the first complexity bound for linear programming that depends only on A; Tardos 
[20] earlier proposed such a method. Tardos method, however, probably should be considered a purely theoretical 
contribution [20, p. 251] because it requires the solution of n complete LP s. In contrast, our method 
is a fairly standard kind of interior point method ac­companied by an acceleration step; accordingly, 
we believe that it is quite practical. Another difference is that we regard our method as primarily a 
real number method. For example, our method is the first polyno­mial time linear programming algorithm 
that also has a complexity bound depending only on A for finding the global minimum in the real-number 
model of comput ation. In contrast, Tardos uses the assumption of integer data in a fairly central way 
because an important tool in [20] is the operation of rounding down to the nearest integer. In the real-number 
case. our al­gorithm depends on parameters XA, ~~ defined below that we currently do not know how to 
compute efficiently. The algorithm can still be used even when the parameters are unknown see Section 
8. 2 The central path It is common to rewrite (1) by introducing slack variables as follows: minimize 
bTy subject to ATy s = c, (2) S>o. We will assume this format for the remainder of the paper. Assume 
that a strictly interior point, that is, a point with s >0, exists. Let us int reduce a log-barrier ten-n 
into the objective function, yielding the new optimization prob­lem: minimize b~y p ~~=1 ins, subject 
to ATy s = c, (3) S>o. Here, p is a positive parameter. If we assume the set of feasible points for (2) 
is bounded and nonempt y, there is a unique minimizer for (3). This point (s(u), y(p)), the minimizer, 
is called the central path point for parameter p, for prob­lem (2). Note that as M -co, (s(p). y(p)) 
tends to the analytic center of the feasible re­gion. As ~ ~ O, (s(~), y(~)) tends to an opti­mum since 
(3) tends to (2). The definition of the central path is credited to Bayer and La­garias [2, 3, 4], Megiddo 
[12], Sonnevend [17], and it was partially analyzed by McLinden [11] earlier. The relation between the 
log-barrier method and linear programming is surveyed by [7, 28]. If we introdu ce Lagrange multipliers 
into (3), we find that (s(~), y(y)) exactly satisfy the equations: pAS-le = b, ATy s=c. Here and for 
the rest of the paper, S denotes diag(s) and e denotes the vector of all 1 s. These nonlinear equations 
generally have no closed form solution nor any finite algorithm. Accordingly, an interior point method 
gener­ates points that are approximately centered. In particular, the method generates a sequence of 
iterates y(k) with geometrically decreasing pos­itive values of p(k) such that 6(y(k), p(k)) < d, where 
for this paper 6 = 0.25. The function 6( ,.) measures proximity to the central path and is defined by 
b(y, p) = [(b/~ -AS-le)T . (AS-2AT)-1 ~(b/~ -AS- e)]1 2. Observe that for an exactly centered point, 
b(y, ~) = O. If this proximity measure is strictly less than 1, then the central path may be fol­lowed 
with Newton s method [7]. 3 The parameters XA, ~A Let Abeanmxnmatrix ofrank m. Let us define D to be 
the set of all positive definite n x n diagonal matrices. Let us define XA = SUP{l\AT(ADAT)-lADll : D 
G D}. (All norms are 2-norms.) Stewart [19] and Todd [21] independently proved that this quantity is 
always finite. Equivalently, one can show that XA = SUP{l\(A~AT)-lAD\l : D c D}. is also finite. These 
bounds apply for any in­duced matrix norm, but for this paper, we confine ourselves to 2-norms. Our complexity 
bounds depend on these two parameters. Note that ~A is unchanged when A is scaled by a Constant, whereas 
XA SCdf3 iINeHX!lY. Since our complexity bounds are independent of const ant scaling, we naturally can 
expect the bounds to depend on ~A and on the product XA IIAI[. These parameters are important in the 
analy­sis of stable numerical algorithms for weighted least-squares and finite element methods; see Vavasis 
[24, 25]. 4 Layered least squares In this section we describe one main-loop it­erat ion of the LIP algorithm. 
Assume at the beginning of the iteration that we have a cur­rent approximate cent ral path point (y, 
p) mit h 6(Y, p) <0.25. Recall that 6(, ) was defined as proximity to the central path in Section 2. 
Par­tition the constraints into p layers J1, . . . . JP by using s = ATy c as follows. Find a permut 
a­tion m that sorts the slacks into ascending order: %(l) < 57r(2) <  < %(n). Let g > 1 be a gap size 
defined in terms of A by (6) below. Find the leftmost ratio-gap of size greater than g in the sorted 
slacks, i.e., find the smallest i such that sT(i~l)/sT(,] > g. Then let J1 = {m(l),..., x(i)}. Now, put 
T(i + l),7r(i + 2), . . . in J2, until another ratio­gap greater than g is encountered, and so on. Thus, 
the constraints indexed by Jk for any k are within a factor of g of each other, and are separated by 
more than a factor of g from con­straints in Jk+l. To formalize this, define and ~k = 11111 S2. The 
construction above ensures that for each k, Ok < @k+l/9. and that @..0~<g @~. Define A, an nxn diagonal 
matrix, tobe approximately S (the exact definition of A is in the full paper [26] and involves some primal 
data as well). We introduce a new kind of step, the LLS step, to generate a direction r for the interior 
point method. Let Al,. ... AP be the partition­ing of the columns of A according to J1, . . ., JP. Similarly, 
let Al,..., AP be diagonal matri­ces that are the submatrices of A indexed by Jl,... , JP. Various other 
vectors and matrices are partitioned in the same way. The layered least squares (LLS) step from point 
y is as follows. Define LO = lR~. Then fork=l ,. ... p compute affine subspace L~ := {minimizers r of 
llA~l(A~r s~)l[ subject to r G Lk_l}, so that Lo o L1 o  > Lp. Finally, let r be the unique element 
in LP (unique because A is assumed to be full rank). Note that, for example, if A; spans lRm, then L1 
has a unique element, and thus L1 = c . = LP = {r}. The idea of partitioning the slacks into lay­ers 
based on their relative sizes has been pro­posed by Kaliski and Ye [8] and Tone [22], who both propose 
a decomposition into two layers. The interest of these authors is in improving the running-time of computing 
one iteration of an interior point method, rather than in obtaining new bounds on the number of iterations. 
The layered least-squares step may be thought of as weighted least-squares with the weights in J1 infinitely 
higher than the weights of J2, and so on. Indeed, this idea is formalized in the full paper. We now update 
~ = y (1 d)r, and P = pa. Here, ~ is a scalar parameter between O and 1. The idea is to choose a as close 
to zero as pos­sible (or perhaps equal to zero, in the case of the final LLS step) as long as the point 
~ re­mains approximately centered. In the full paper we give a precise formula for such an d, which involves 
solving some additional least-squares problems. An inexact line-search is also possi­ble. Algorithm LIP 
Input: A, b, c and (y, M) satisfying 6(y, p) <8. Output: Optimal points y* for (2) and x* for the primal. 
Repeat: 1. Compute LLS solution r as above. 2. Compute d E [0, 1]. 3. Update ~ := y (1 tl)r, p:= 
PG. 4. If ~ = O,then set y* := ~, and HALT. 5. Recenter ~ with three Newton steps, yielding Y . 6. 
From (y , pa), compute approximately cen­tered point (y , @/C(A)), using existing path-following techniques. 
 7. Update y := y , p := @/C(A), and con­tinue.  Note that the algorithm assumes that an ap­proximately 
centered point is part of the input. This in turn implies that the feasible region for (2) is nonempty 
and contains a strictly interior point. In Section 6 we explain an initialization procedure that makes 
these assumptions valid. The output y* of Algorithm LIP is a dual op­timal solution, but information 
from the final LLS step can be used to compute a primal op­timal solution x*. The above algorithm uses 
existing path­following techniques applied to the dual prob­lem (2) to reduce the central path parame­ter 
from ~a to j.m/C(A). For the analysis of such a technique, see, for example, [16] or [7]. Usual path-following 
techniques decrease the value of central path parameter by a fac­tor of 1 l/(const@) on each iteration 
using a Newton step. Thus, 0(nli2 log C(A)) tradi­tional interior point steps are used for Step 6 above. 
5 The main theorems The proofs of all the main theorems have been omitted and are in the full paper [26]. 
First, the parameter a G [0, 1] must be defined. This definition is in the full paper. The intuition 
is Note that one must have i # j for (5) to hold, that we pick ti as close to zero as possible so since 
g > 1. that the structure of the partition J1, . . . . JP Definition. We say that two crossover events 
is maintained at the new point ~. (P, P , i, ~) and (PI, u;, il,jl) are disjoint if The first main theorem 
is as follows. (P >P) n (P;, pl) = 0. Theorem 1 Let a 6 [ti, 1] be chosen arbitrar­ily. In the case 
that ti = O, assume also that Lemma 1 Let a >0. Then Y, defined to be y (1 a)r, is a strictly feasible 
point. Furthermore, (Pi, P;, ~l,.11),.. ~ ,(/4,/JL&#38;.i) (f(y, /.La) <0.75. be a sequence of t disjoint 
crossover events for a particular instance of (2). Then t :; n(n 1)/2. This shows that the LLS step 
maintains ap­proximate centering. Step 5 of Algorithm LIP The proof follows from the observation that 
two reduces the proximity from 0.75 to 0.758, i.e., disjoint crossover events cannot have the same less 
than 0.25. values of i, j because this would contradict (5) This theorem has a second consequence as 
far for one of the events. Now we COIM to the sec­as Algorithm LIP is concerned. In Algorithm ond main 
theorem: LIP, if ii = O then we terminate. Theorem 2 Consider one pass through the Corollary 1 Suppose 
6 = O. Then y = y r main loop of the LIP algorithm. Assume a >0 is an optimum solution for (2), and 
in the case in the LLS step. Then at least one event takes of degeneracy, it is an approximate analytic 
place during this pass as the central path param­center for the dual optimal face. Moreover, by eter 
drops from p to pti/C (A), where C(A) is solving a single least-squares problem we can defined by (7). 
generate an optimum solution x* for the pri- The formulas for C(A) and g are based on mal. various norm 
bounds arising in the proofs of Next, we demonstrate the finiteness of Algo-the theorems and are: rithm 
LIP. We show that the number of main­loop iterations is at most n(n 1)/2. The key g = 2. 302(1 + 6)n2~~ 
(6) concept for developing a complexity theory is a and crossover event. Definition. Given an instance 
of (2) with a C(A) =40 . 303(x~ + l)2X~[[A@5g3n. (7) strictly feasible interior point, we say that the 
This theorem shows that Algorithm LIP has4-tuple (p, #, i, j ) defines a crossover event, finite termination; 
in particular, it must ter­for~>#> O,andfor i,j~{l,..., n}, if minate after n(n 1)/2 main-loc)p iterations, 
s&#38;u) since each main loop iteration forces at least < 3gn (4) one crossover event to occur. These 
crossoversj(P) events are clearly disjoint because the central and for all p c (O, p ], path parameter 
decreases monotonically during the algorithm. s~(p ) > Sgn (5) An immediate consequence of Theorems 
Sj (/-/ ) and 2 is a new characterization of the central Here Si(p) denotes the ith slack for a point 
on path as being composed of 0(n2 ) alternating the central path of (2). curved and straight segments. 
Corollary 2 Consider an instance of(l) that is bounded and has a strictly interior feasible point. Let 
M > 0 be arbitrary. Then there exist a sequence of k + 1 breakpoints ~=~Q~~~~~Z< -<~h=M such that k s 
n(n 1) and such that: 1. Fora/li COO,..., k 1} such that i is even, there exist vectors u,, Vi such 
that J(Ui + PVZ, P) <0.75 for all p ~ (p;, Kz+l). 2. ForalJi <{O,..., k 1} such that i is odd,  ~i+l/~i 
iS bounded above by a constant C (A) depending only on A. PROOF. Introduce breakpoints at the begin­ning 
and end of each LLS step of Algorithm LIP, and the corollary follows. fl A natural question to ask is: 
if the central path is approximately straight over intervals [#i, Pz+l] for i even, then shouldn t a 
traditional path-following interior point method be able to take larger steps and achieve the same com­plexity 
as the LIP method? The difficulty is that the tangent to the central path at y(Pi+l) (which is the direction 
followed by traditional algorithms) does not necessarily point in the right direction. This is most clearly 
seen on the last LLS step of Algorithm LIP (see, e.g., Figure 2), when the global minimum is found. The 
tangent to the central path at y(p) is never precisely aligned with the displacement vector between the 
y(~) and the global minimum y* for any positive p. In contrast, the LLS step r is precisely this displacement. 
The curvature of the central path has re­ceived attention previously in the literature. It is already 
known that, as observed in the last paragraph, the central path becomes asymptot­ically straight as the 
optimum is approached. This fact is the basis for results on asymptotic quadratic convergence of interior 
point methods such as the result of Ye et al, [30] and earlier works cited therein. Sonnevend, Steer 
and Zhao [18] have consid­ered the total integral of the curvature in the path in order to bound complexity. 
Megiddo and Shub [13] have considered bends in the lim­iting behavior of the central path trajectory 
as the boundary of the feasible set is reached. We do not know of any direct connection between these 
results and the preceding corollary. We now estimate the overall complexity. There are 0(/7i log C(A)) 
small steps per main-loop iteration, and the small steps dom­inate the total work. Thus, the total number 
of small steps is O (n 2 5log C(A)). Taking a log of (7) and keeping the dominant terms, we can write 
the complexity as O(n3 5c(A) ) small steps, where c(A) = 10gn+lOg(~A+l)+ lOg(xA]lA[])/n. (8) Each small 
step requires solution of a system of linear equations, which takes 0(m2n) arith­metic operations. 6 
Initialization We now examine the question of initializing Al­gorithm LIP, that is, given A, b, c, find 
an ap­proximately centered y, ~. The procedure used is as follows: we start with a basic subset of m 
constraints, and we also create dummy reflec­tions of these constraints far away. These 2m constraints 
define a parallelepipeds Fm whose analytic center is known in closed form. We then add the remaining 
constraints one at a time, each time running Algorithm LIP to find the analytic center of a new polytope. 
This yields a sequence of polytopes I m, l?~+l, . . . . Fn such that F. is the feasible set for the problem 
under consideration plus the extra dummy con­straints, which do not change the solution. At each step 
there is a certain linear programming problem defined on Fh such that the analytic center of ~h+l lies 
on the central path of this LP. Thus, we use LIP to follow the central path, not to optimum, but only 
until the analytic center of F~+l is found. Once F. is found, we then run Algorithm LIP. At first glance, 
it seems that the running time of initialization will be a factor n m larger than the running time 
of Algorithm LIP. With a more careful analysis, however, we are able to show that initialization has 
the same running time bound (up to a constant factor) as Algo­rithm LIP. 7 Integer data and Tardos theorem 
In this section we specialize the LIP algorithm to the case that all of the entries of A are in­tegers. 
Suppose that the total number of bits required to write A is LA. We have the follow­ing result (similar 
results can be found in [23] and [27]): Lemma Z The parameters XA and ~A are both bounded by 2°tLAJ. 
This leads to a new proof of the following corol­lary, which is a well-known result due to Tardos. Corollary 
3 [20] Consider solving an LP of the form (1) on a Turing machine. Suppose that A, b, c have integer 
entries, and suppose that LA is the number of bits to write m x n ma­trix A. Then there is an algorithm 
to solve this problem in polynomial time, and furthermore, the number of arithmetic operations is polyno­mial 
in m, n, LA. We can apply Algorithm LIP to flow prob­lems such as max-flow or rein-cost flow. For the 
rest of this section, let m and n denote the number of arcs and nodes in the net­work. For flow problems, 
Algorithm LIP is strongly polynomial-time because Lemma 2 im­plies XA, ~A < 2°t~j, and hence c(A) in 
(8) is polynomial in m, n. In fact, an even bet­ter bound is possible for XA, ~A in the case of minimum-cost 
flow; it can be shown that both parameters are O(mn) (see the full paper). Therefore, the overall complexity 
for Algo­rithm LIP applied to the rein-cost flow prob­lem is 0(m3 5 log m) iterations, where each it­eration 
requires 0(m3) arithmetic operations. According to [1], the best currently-known strongly polynomial 
algorithm for this problem is O(m2 log n + mn(log n)2) and is due to Orlin [15]. 8 Conclusions We have 
proposed the first known interior point method whose running time depends only on A. This is attained 
by including a step called the LLS step that accelerates the convergence. There are many open questions 
raised by this work. Here are some of the more interesting questions. 1. The biggest barrier preventing 
Algorithm LIP from being fully general-purpose is the fact that we don t know how tc) compute or obtain 
good upper bounds on XALor ~A in the real-number case. The only known algorithm for computing these parameters 
is implicit in the results of Stewart [19] and O Leary [14] and requires exponential time. We suspect 
that com­puting them, or even getting a good upper bound, may be a hard problem. In the absence of an 
algorithm to compute them, one possibility would be an LP algo­rithm that makes a low guess at the param­eters, 
and then increases the guess as more knowledge of the parameters is gained from the running of the algorithm 
itself. A very straightforward guessing algorithm was suggested by J. Renegar. Simply guess XA = 100//[A\[ 
and ~A = 100 and run the en­tire LIP algorithm. If it fails in any way, then guess XA = 1002/l[A[[ and 
~A = 1002 and try again. Repeatedly square the guesses, un­til Algorithm LIP works. (Note that, since 
Algorithm LIP produces complementary pri­mal and dual solutions, we have a certificate concerning whether 
it terminates accurately.) Thus, for a problem in which these parame­ters are not known in advance, the 
running time of Algorithm LIP is increased by a factor Of 10glOg(XAllAll). 2. It is also interesting 
to obtain upper bounds on these parameters for classes of problems occurring in practice, as we did with 
the flow  example. Good candidates for special-case analysis would be linear programs arising in scheduling, 
optimization problelms involving finite-element subproblems (see example 3 of Coleman [5] ), and LP 
relaxations of combi­ natorial optimization problems. 3. Some practical issues include the question of 
choosing g when running Algorithm LIP in standard floating-point arithmetic, and de­termining the most 
stable way to compute the LLS solution. 4. Although our algorithm has primal-dual as­pects, it is principally 
a dual algorithm. We suspect that there is a primal-dual version that is simpler. 5. Our upper bound 
on the number of crossover events is n(n 1)/2, and indeed, M. Todd has constructed an example where 
our al­gorithm requires n(n 1)/2 main-loop iter­ations. Todd s example, however, involves only n bends 
in the central path. We have not been able to construct any examples with more than 0(n) bends in the 
central path. Is there a bound better than 0(n2) on the num­ber of bends, and, if so, is there an algorithm 
that is a factor of n faster than ours? A sec­ond factor of n in the final complexity bound comes from 
g3n appearing (7), which we also suspect could be improved. Indeed, it should be noted that the bound 
0(n3 5c(A)) on the number of iterations could be larger than the conventional bound 0(nl/2L), even though 
our algorithm is al­ways at least as fast as the conventional method. This suggests that our current 
anal­ysis could be improved.  References <RefA>[1] R. K. Ahuja, T. L. Magnanti, and J. B. Or­lin. Network 
Flows: Theory, Algorithms, and Applications. Prentice-Hall, Engle­wood Cliffs, New Jersey, 1993. [2] 
D. A. Bayer and J. C. Lagarias. The nonlinear geometry of linear programming. I. Affine and projective 
scaling trajecto­ries. Transactions of the AMS, 314:499­526, 1989. [3] D. A. Bayer and J. C. Lagarias. 
The non­linear geometry of linear programming. II. Legendre transform coordinates and cen­tral trajectories. 
lkansactions of the AMS, 314:527 581, 1989. [4] D. A. Bayer and J. C. Lagarias. The non­linear geometry 
of linear programming. III. Projective Legendre transform coordinates and Hilbert geometry. Transactions 
of the AMS, 320:193-225,1990. [5] T. Coleman. Large-scale numerical opti­mization: int reduction and 
overview. In A. Kent and J. G. Williams, editors, Ency­clopedia of computer science and technol­ogy. 
Marcel Dekker, New York, 1993. [6] A. V. Fiacco and G. P. McCormick. Non­linear Programming: Sequential 
Uncon­strained Minimization Techniques. J. Wi­ley and Sons, Chichester, 1968. [7] C. C. Gonzaga. Path-following 
methods for linear programming. SIAM Review, 34:167-224, 1992. [8] J. A. Kaliski and Y. Ye. A short-cut 
po­tential reduction algorithm for linear pro­gramming. Management Science, 39:757­773, 1993. [9] N. 
Karmarkar. A new polynomial-time al­gorithm for linear programming. Combi­natorics, 4:373 395, 1984. 
[10] L. G. Khachiyan. A polynomial algorithm in linear programming. Dokl. Akad. Nauk SSSR, 244:1093 1086, 
1979. Translated in Soviet Math. Dokl. 20:191-194. [11] L. McLinden. An analogue of Moreau s proximation 
theorem, with applications to the nonlinear complementarily problem. Pacific Journal of Mathematics, 
88:101­ 161, 1980. [12] N. Megiddo. Pathways to the optimal set in linear programming. In N. Megiddo, 
editor, Progress in Mathematical Program­ming: Interior Point and Related Method, pages 131 158. Springer, 
New York, 1989. [13] N. Megiddo and M. Shub. Boundary be­havior of interior point algorithms in linear 
programming. Mathematics of Operations Research, 14:97-146, 1989. [14] D. P, O Leary. On bounds for scaled 
pro­jections and pseudoinverses. Linear Al­gebra and its Applications, 132:115-117, 1990. [15] J. B. 
Orlin. A faster strongly polynomial minimum cost flow algorithm. In l%c. 20th ACM Symposium on the Theory 
of Computing, pages 377-387, 1988. [16] J. Renegar. A polynomial-time algorithm based on Newton s method 
for linear pro­gramming. Mathematical Programming, 40:59-94, 1988. [17] G. Sonnevend. An analytical center 
for polyhedrons and new classes of global al­gorithms for linear (smooth, convex) pro­gramming. In Lecture 
Notes in Control and Information Sciences 84, pages 866 876. Springer Verlag, New York, 1985. [18] G. 
Sonnevend, J. Steer, and G. Zhao. On the complexity of following the central path of linear programs 
by linear extrap­olation II. Mathematical Programming, 52:527-553, 1991. [19] G. W. Stewart. On scaled 
projections and pseudoinverses. Linear Algebra and its Ap­plications, 112:189 193, 1989. [20] E. Tardos. 
A strongly polynomial algo­rithm to solve combinatorial linear pro­grams. Operations Research, 34:250-256, 
1986. [21] M. J. Todd. A Dantzig-Wolfe-like variant of Karmarkar s interior-point linear pro­gramming 
algorithm. Operations Research, 38:1006 1018, 1990. [22] K. Tone. An active set strategy in an interior 
point method for linear program­ming. Mathematical Programming, 59:345 360, 1993. [23] L. Tuncel. A 
pseudo-polynomial complex­ity analysis for interior-point algorithms. Technical Report CORR 93--16, Depart­ment 
of Combinatorics and Optimization, University of Waterloo, Waterloo, Ontario, Canada, 1993. [24] S. A. 
Vavasis. Stable numerical algorithms for equilibrium systems. Technical Report 92-1280, Department of 
Computer Science, Cornell University, Ithaca, NY, 1992. Ac­cepted to SIAM J. Matrix Analysis. [25] S. 
A. Vavasis. Stable finite elements for problems with wild COf3ffiCiE!IIltS. Techni­cal Report 93 1364, 
Department of Com­puter Science, Cornell University, Ithaca, NY, 1993. To appear in SIAM J. Numeri­cal 
Analysis. [26] S. A. Vavasis and Y. Ye. An accelerated interior point method whose running time depends 
only on A. Technical Report 93­1391, Department of Compu~ter Science, Cornell University, Ithaca, NY, 
1993. [27] S. A. Vavasis and Y. Ye. Condition numbers for polyhedra with real number data. Technical 
Report 93-1398, Depart­ment of Computer Science, Cornell Uni­versit y, 1993. [28] M. H. Wright. Interior 
methods for con­strained optimization. In A. Iserles, editor, Acts Numerics 1992 , pages 341-407. Cam­bridge 
University Press, Cambridge, 1992. [29] Y. Ye. On the finite convergence of interior-point algorithms 
for linear pro­gramming. Mathematical Programming, 57:325-336, 1992. [30] Y. Ye, O. Giiler, R. A. Tapia, 
and Y. Zhang. A quadratically convergent O(@L)-iteration algorithm fcm linear pro­gramming. Mathematical 
Programming, 59:151-162, 1993.</RefA> 
			
