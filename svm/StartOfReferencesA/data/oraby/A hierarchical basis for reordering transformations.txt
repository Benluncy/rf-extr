
 A Hierarchical Basis for Reorderln8 Transformations Joe Warren T. J. Watson Research Center * In this 
paper, we propose a new dependence baaed program representation. This representation is the union of 
two previously separate concepts: loop carried dependence and hierarchical abstrac-tion. The resulting 
form has the property that all information necessary to reorder the set of all exe- cutions of the statements 
contained in a given loop exists in the representation of that loop. Thus, this representation provides 
an ideal basis for reordering transformations such as vectorisation and loop fusion. As evidence of this, 
we give efficient algo- rithms for these two transformations based on this representation. 1. Introduction 
Many compilers perform optimisations which reorder the statements in a program. Of course, reordering 
the statements of program should not change the semantics of the program. One use- ful tool for statement 
reordering is the concept of dependence. A statement S 2 depends on t state- ment S s if S t must be 
executed before S s to preserve the semantics of the original program. The set of all dependences for 
a program may be viewed u s partial ordering on the sequence of exe- cution of the statements in the 
program which preserves the semantics of the original program. Dependences arise as the result of two 
separate effects. First, a dependence exists between two statements whenever a variable appearing in 
one statement may have an incorrect value if the two statements are reversed. Current address is : Depsrtment 
of Computer Science Cornell University Ithaca. NY 148,53 Permission to copy without fee all or part 
of this material is granted provided that the copies are not made or distributed for direct commercial 
advantage, the ACM copyright notice and the title of the publication and its date appear, and notice 
is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, 
or to republish, requires a fee and/or specific permission. &#38;#169; 1983 ACM0-89791-125-3/84/001/0272 
$00.75 A--B'C S t D=-A'E+I S 4 in this example, S s depends on S t since executing S s before S s would 
result in S s using an incorrect value for A. Dependences of this type are data depen- dences. Second, 
a dependence exists between two statements whenever the results of one statement control the execution 
of the other statement. IF (A) THEN S I B--enD S e ENDW In this case, Sg depends on St since the value 
yielded by A determines whether S s is executed. Dependences of this type are control dependences. In 
this paper, we restrict our attention entirely to d t dependence and transformations based on d t dependence. 
To guarantee the representation and algorithms given in this paper apply to all programs, the transformations 
given in [AKPW 82] should be applied to input programs as preproceseing phase. The resulting programs 
con- sist of three types of statements: assignments, FOR loops and WHILE loops. Moreover, all control 
dependences in the original program have been transformed into an equivalent set of data depen- dences. 
As a result, any subsequent transformation may be based 8olely on dato dependence. Finally, we restrict 
FOR loops to those in which the initial Â°value and the increment of the index variable are one. This 
is not a serious restriction, since say more general FOR loop can be converted into this form using the 
techniques in [AIIK 82]. The representation proposed in this paper is refinement of the dependence graph. 
Formally, a dependence graph for program S is a directed graph O .0 (N,E). N is set of nodes with each 
node corresponding to a unique statement in S. E is the set of directed edges (Ns, No) such that N s 
can be executed before N s and either (1) Statement N s uses a variable defined by statement N s (true 
dependence) or (2) Statement N s defines a variable defined by statement lq t (output dependence) or 
(3) Statement Ns defines a variable used by statement N s (antidependence) [Kuck 80]. In the rest of 
this paper, we refer to s state- ment in s program and its corresponding node in the dependence graph 
interchsngetbly. Likewise, we refer to an edge in the dependence graph and s dependence interchangeably. 
Later in the paper, we refer to dependence graph of this type as s name dependence graph since the edges 
link statement, using and defining variables with common names. This paper fuses two important concepts 
from the theory of dependence, layered dependence for arrays [Kenn 80] [Alle 83] and graph abstraction 
[KKLP 80]. The result of this fusion is s new type of dependence graph known as s hierarchical depen- 
dence graph. In the next two sections, we review layered dependence and graph abstraction. |, Layered 
Dependence Layered dependence is a refinement of the concept of dependence to provide more precise information 
concerning the effects of loops and arrays. A layered dependence tony he classified as being one of two 
types, loop carried or loop independent. [Kenn 80] defines a loop carried dependence as a dependence 
that arises because of the iteration ofloops. DO 200 I--1,100 C(1)== E(1) * 2 S, DO IO0 J..l,lO0 A(I,J) 
-- C(I-I) + A(I,J-I) S s I00 CONTINUE CONTINUE In this example, S s depends on S s since S s calculates 
values of C used by Ssr However, on any given iteration of the outer loop, S t and S o refer to separate 
elements of C. Only on different iterations of the outer loop do S s and Sg actually refer to common 
elements of C. Since this dependence exists as s result of the iteration of the outer loop, it is clamified 
as loop carried. Likewise, there exists a loop carried dependence from S e to itself sines S a uses values 
it computes on previous iterations of the inner loop. Kennedy defines the level of s loop carried dependence 
as being the nestinl; level of the loop whose iteration causes the dependence. The loop carried dependence 
from S s to Sg has a level of one since the iteration of the outer loop causes the dependence. Similiarly, 
the loop carried dependence from Sg to itself has a level of two since it is s result of iteration of 
the inner loop. The second type of layered dependence, loop independent dependence, exists even in the 
absence of loop iteration. Loop independent dependence is solely a result of textual ordering in the 
original program. Dependences between statements con-tained in no common loops must be loop indepen- 
dent. Moreover, dependences between statements contained inside common loop may also be loop independent. 
DO I00 I~1,100 A(I) --B(I) + t Ss C(I+I) --A(I) + E(I) S, 100 CONTINUE In this example, S a &#38;pen&#38; 
on S t. However, the dependence would exist even in the absence of the containing FOR loop. [Alle 83] 
describes depen-dences of this type as being loop independent. Together, the loop carried and loop independent 
dependence partition all pomible dependences I le hi. The above descriptions give only a rough ides of 
loop carried and loop independent depen- dence. To be more precise, we must flint introduce some notation. 
An iteration vector [Kuhn 801 for a statement is u vector containing one entry for each loop surrounding 
the statement. The elements of an iteration vector correspond to the values of the loop induction variablce 
for u particular execution of that statement. Given the following piece of code, DO 200 I--1,10 DO 100 
J--l,10 St 100 CONTINUE 20O CONTINUE possible iteration vectors for S t are (!,1), (2,5) and (10,1) 
where the first element of the vector represents the value of ! and the second element represents the 
value of J. In general, L,, the k'th ieftmost element of iteration vector J (boldface for vectors), is 
the value of the induction variable of the containing FOR loop whose header is st nesting level k- !. 
One useful tool for relating iteration vectors is u direction vector [Wolf 80 I. If i and j are itera- 
tion vectors, D(i~j) is s direction vector each of whose elements is defined as follows: D(i,j)k -- '<' 
if it, < Jk '--' if it. ." j~ '>' if i h > Jk For example, D((!,1),(2,$)) --(4~,~) and D((5,8),(5,1)) 
m (,,,). Now using direction vec- tors, we define both loop carried and loop indepen- dent dependence. 
Let it be s vector consisting of the k leftmost elements of i. Let RPT(Z,q) be a vector consisting of 
q Z'a Let JJ be the concatena- tion operator for vectors 273 Definition Let S x and S I be two statements 
nested in k common loops. There exist a loop carried dependence at level p (l<==p<--k) from S x to Sy 
if and only if There exist iteration vectors i and j such that 1) S a and Sy reference a common memory 
location on iterations i and j respectively, with at least one of S z and Sy modifying that location. 
2) D(ikJk) is a vector of the form  RPT(-f,p-I)II(<)IIRPT(*,k-p) where * represents any of <,,,,,,,>. 
There exists a loop independent dependence from S= to Sy if and,,only if There exist iteration vectors 
i and j such that 1) S z and Sy reference a common memory location on iterations i and j respectively, 
with at least one of S m and Sy modifying that location. 2) D(ik,Jk) iS a vector of the form RPT(~,k) 
3) S m textually precedes Sy Direction vectors provide a firm mathemati- cal basis for dealing with 
layered dependence. They also can provide a practical basis for dependence testing. The definition above 
equates the existence of a particular layered dependence with the existence of a particular direction 
vector. One approach to dependence testins would be to calcu- late the set of all possible direction 
vectorsfor a given pair of statements. Definition Let S ! and S 2 be statements nested in k common loops. 
V(SvSe) == {D(i,.,jk) I such that S t references some location M on iteration i and S t references M 
on iteration j, with at least one of S t and S t modifying M} Testing for a layered dependence would 
proceed as follows. Construct a name dependence graph as described in the introduction. For each edge 
in the name dependence graph, test for existence of the iteration vectors i and j in part I of each definition 
using a GCD test as in [Alle 83J. If no such i and j exist, no dependences can exist between the statements 
For each remaining edge, calculate the set V defined above. For arrays each of whose subscripts are linear 
functions of a single induction variable, the calculation of this set is relatively easy nsln S techniques 
similar to those in [Lamp 76]. For loops which have no induction vari- able (e.g. WHILE loops) the corresponding 
entry in set of direction vectors would always be e. Now, the test for the existence of specific lay- 
ered dependences is simply s set membership test. Consider the following piece of code. DO 200 I.-1,100 
DO 100 J==l,100 D(I) =, A(I-I) * 2 S 1 E(I,J) --D(I) Ã· E(I,J-1) S s 100 CONTINUE 200 CONTINUE  In this 
example, V(Sl, Ss) --((--,<), (--,m), (.,>)). A loop carried dependence from S 1 to S a at level one 
exists if and only if V(Sl, Ss) contains (<,0). Thus, there exists no level one dependence. Similarly, 
V(SI, Sg) contains (--,<), 8o there must exist a loop carried dependence at level two. The test for the 
existence of a loop indepen- dent dependence consists of two parts. First, deter- mine if S z textually 
precedes SÂ¢ Second, check if V(St, Ss) contains (--,--). Both conditions are satisfied so there exists 
s loop independent depen- dence. V(S~Ss) consists of the single element (--,<) which implies a single 
loop carried depen- dence at level two. As we have seen, given the set of direction vectors that can 
exist between two statements, the tests for both loop carried and loop independent dependence translate 
into set membership tests. In addition, tests for further properties of a depen-dence such as interchange 
prevention or inter-change sensitivity falls 83] can also be translated into simple tests of set membership. 
The set of direction vectors provides a concise, compact sum- mary of the dependence information relevant 
to two statements 8. Graph Abstraction [KKLP 80] describes graph abstraction as a process in which given 
a graph a mt of nodes and their internal arcs may be merged to form a com-pound node. Any edps incident 
to (or from) the set are made incident to (or from) the compound node. Our goal k to perform a modified 
type of graph abstraction on a layered dependence graph. We col- lapse the set of statements contained 
in a FOR or WHILE loop into a compound node that 274 summarizes the effects of that loop. We refer to 
these compound nodes as loop nodes. Nodes representing statements are referred to as statement nodes. 
The result of this graph abstraction is a col- lection of related graphs. Each ~aph is a depen-dence 
graph for the statements and loops contained in a given loop. We refer to each of these depen- dence 
graphs as a component dependence graph or CDG. The CDG's are related as follows. There exists a single 
CDG for all statements and loops st nesting level sero. Given n CDG G representing n set of statement8 
and loops at nesting level k, asso- ciated with each loop node L in O there exists CDG representing 
the statements sad loops at nest- in s level k+! in L. We refer to this hierarchy of component dependence 
graphs as hierarchical dependence graph or HDG. The depth of a CDG in the HDG is equivalent to the nesting 
level of the statements sad loops contained in the CDG. 4. Hierarchical Dependence Graphs To construct 
the HDG, we perform the above sraph abstraction on layered dependence graph. However, we add one slight 
modification. This modification reflects the desire to have loop node summarize the effects of its corresponding 
loop. Consider statements S t sad S s nested in exactly k common loops with level p (p~k) loop carried 
dependence from S z to Sg. Let L t sad Ls be the p'th sad p+l'st loops contsinins S t sad S s, After 
nor- mal graph abstraction, the edge from S t to S s would appear in CI)G at depth k. However, on a 
specific iteration of L s, S t and Sz do not refer to say common memory locations sad ,as result, are 
independent. The edge should reflect that a value created inside L z on given iteration of L t is used 
by L s on subsequent iteration of L r As rceult~ the edge should appear in the CDG at depth p link- 
ins Lz to itself. To accomplish this, the ~aph abstraction procedure moves loop carried depen-dencce 
upward durins con~ruction of the HDG. Figure 1 outlines the steps neceusJ7 to construct the layered dependence 
~aph for program. Fig-ure 2 gives the algorithm for creation of the hierarchical dependence graph siven 
s fist layered dependence graph. Given program as follom. X --10 S s DO 800 1 ,,- 1,100 L s DO 100 J 
== !,100 L s A(I,J) --X + B(J,J) S s D(J,J) ,.- A(J,J-I) 2 S, 100 CONTINUE DO 200 J,,-l,100 L s F(l+l,J) 
-D(I,J) * 2 S 4 C(J,J) --rp, J) S. 200 OONTIN~ 800 OONTIN~ The hierarchical dependence sraph corresponding 
to this prosrsm is given below.  Depth 00DG ODG for L s CDG for L s ODG for L 8  Â® &#38;#169; Â® @ 
t ! I i &#38;#169;c@ Â® @ ; loop carried dependences -- -~ loop independent dependences This example 
is relatively etrslghtforward. The only subtle point is the loop carrisd depen- dence from S 4 to Se 
at level one now links loop node L s to itself. As stated previously, this reflects the fact that values 
produce by L e on a siren itera- tion of L s are then used by L e on n subsequent iterltion of L r if 
NG is the name dependence ~sph created in CONSTRU~VF sad k is the maximum nesting depth in the program, 
then the time spent procem- ins the FOR loop (0) in CONSTRU~'~F is O(koJNE D. We exprece the efficiency 
of this sad other algorithms given in this paper in terms of the eke of NG to allow comparison to slgorithms 
not based on layered dependence. The time spent pro- eemdns GRAPH..ABSTRAOT as given in flsure 2 is O(k'.(JNVJ+JNEJ))since 
each edge in NE can give rise to up to k+! edges in LE and each edge in LE can be moved up to k tlmea 
However, in practice, GRAPH,ABSTRACT csa he implemented in O(ko(JNVJ+JNEJ)) time by simply insertins 
each layered dependence into its proper CDG as it is created by CONSTRUCT. After performing GRAPH.Y~BSTRACT, 
 dependence may no longer link the actual statements givins rise to the depen- dence. To insure that 
these statements may be 275 procedure CONSTRUCT(S) /* S b~ abstract syntax tree representation */ /* 
of input program */ perform transformations in [AKPW 82] on S and normalize resulting FOR loops to iterate 
from I to an upper bound by I [AIIK 82] build the name dependence graph NO =.. (NV, NE) for S as described 
in the introduction let LG --(LV,LE) with LV --NV and LE empty (1)/* Expand each edge in NG into */ /* 
layered dependences in LG */ for each edge (Sj,S~ in NE do begin let k be the common nesting depth of 
S t and Sj /* Test for existence of i and j */ if there exist iteration vectors satisfying GCDTEST in 
~Alle 83] then begin compute V(Si,Sj) /* Test for loop independent dependence */ if S t textually precedes 
Sj and RPT(m,k) is in V(S,,$j) then add edge (SeSi) marked loop independent to LE /" Test for loop carried 
dependences ./ for i:==l to k do if RPT(--,i-I)H( < )[[RPT(o,k-i)is in VlS,,Sj) then add edge (Si, Sj) 
marked loop carried at level i to LE end end /. return layered dependence graph ./ return(LG) end Figure 
1. Construction of Layered Dependence Graph procedure GRAPH.,ABSTP,.ACT(S, LG,L,k) /e S is abstract syntax 
tree representation of e/ /0 set of statements st nesting level k el /e LO is layered dependence graph 
for S e/ /* L is the loop node with which the computed e/ /e CDOIS associated ./ /0 Procem each contained 
loop first e/ for each loop header LH at level k in S do begin create loop node L' with the same upper 
hound is LH and ndd to LO let S' be nbtres representing the statements in the loop headed by LH let LG' 
be the subgr ph of O induced by S' /0 Relink to create compound node ./ for each edp (Sj,Sj) betweenLG' 
and LG-LG' begin remove edge (SvSI) from LE it Sj in O' then add (L',$) to LE else add (S,,L') to LE 
 end let LG :-- LG - LG' GRAPi.I.,ABSTRACT(S',LG',L',k + 1) end [* Move loop carr. dependences to higher 
CDG *[ for each edge D in LE do if D is loop carried st level p and p<k then begin remove D from LE add 
(L,L) to CDO containing L end make LG the Cq)G mmoci ted with loop node L. end Figure 2. Construction 
of the HI)(] referred to if necesmuT, we amume GRAPH.,~STRACT also malntslns pointers to the them istemente 
us the dependence is moved. 6. Slnsle Loop Transformations The layered dependences in the HDG ace partitioned 
so that s CI)G st depth k contslns only loop independent dependences and kvel k loop car- tied dependences. 
This partitioning causes the HI)G to he especially malted for certain types of program transformations. 
JAils 83] defines reordering transformation as any transformation which changes the order of execution 
or a set of statements without adding or deleting any execu-tions of any statement in the set. A reordering 
transformation preserves a dependence from S t to S s if after the transformation S s is still executed 
after S t. A reordering transformation is valid if and only if it preserves all dependences in a pro- 
gram. We now identify a certain subset of reorder- ink transformations which we will refer to as single 
loop transformations. Definition Let L be a loop iterating from 1 to some upper bound N by 1 containing 
a set of atements {or loops) Sl, .. Let S~ be the j'th execution of S i in L. A single loop transformation 
of L is a reordering transformation in which the sequence of execution of the SU's is permuted. Later 
in the paper, we show complex transforma- tions, such as vectorisation, can actually be expressed as 
a sequence of single loop transforma- tions. The following theorem states the key pro- perty of the HDG 
with respect to the effects of a single loop transformation. THEOREM Let C be a loop node in the HDG 
corresponding to the loop L. A single loop transformation for a loop L is valid if and only if the transformation 
preserves all dependences present in the CDG associated with C. PROOF Let L be a loop whose header 
is at nesting level k-l. Let G be the CDG associated with G. Implication from left to right: By definition, 
any reordering transformation is valid if and only if it preserves all dependences in the original program. 
Thus, if a single loop transformation for L is valid, this implies that the transformation preserves 
all dependences including those in *G. Implication from right to left: We must show that this single 
loop transformation preserves all depen- dences in the program. First, only dependences between statements 
contained in L (or possibly nested in loops in L) can violated by a reordering transform ,ion restricted 
entirely to L. So consider a dependence from S x to Sy existing in a flat layered dependence graph with 
both S x and S r contained in L. After construction of the HDG, the edges may exist in one of three places. 
First, it could exist in G. By hypothesis, the transformation preserves these dependences. The dependence 
could also occur in a CDG G' st depth p (p>k) which is a descendent of some loop node S t contained in 
G. This dependence must be either loop independent or loop carried at level p. In either case, the frst 
k elements of the direc- tion vector associated with this dependence are all ~.'s. Thus, any memory locations 
referenced in S. ,J by S x must also be referenced by S in SiX Thus, any single loop transformation 
on L must preserve these dependences. Finally, the dependence could exist in CDG G" at depth q (q<k) 
which is an ancestor of G in the HDG. This implies the dependence must be loop carried at level q. However, 
the direction vec- tor associated with this dependence consists of q-I --'s followed by a <. This implies 
that the memory locations referenced by S must be com- pletely independent of the locations referenced 
by S~ for any given execution of the loop L. Thus, any single loop transformation of L must preserve 
these dependences. QED This theorem allows algorithms for single loop transformations to restrict their 
scope to one part of the HDG at a time. More importantly, a single loop transformation can be extended 
to han- dle programs with multiple nested loops by simply performing the transformation on each component 
dependence graph in the ItDG. As a result, we can derive compact, efllcient algorithms for more gen- 
eral transformations by expressing them as s sequence of single loop transformations. 0. Veetorlsatlon 
The first transformation for which we formu- late an algorithm is vectorisation. In vectorisation, scalar 
code inside a FOR loop is transformed into an equivalent set of vector statements. This is not straightforward 
process since the semantics of a sequential FOR loop differ from those of a vector statement. Consider 
the following example: DO 100 I"1,100 A(1) --A(I-S) "e(1)  100 CONTINUE A direct translation to vector 
statement would produce: .~1:100) .-X(O:09) * B(l:lO0) 277 However, this translation would introduce 
s seman- tic difference, in the origlnal code, the value fetched by A(I-I) on one itsrntion is the value 
etored by A(1) on the previous iteration. The vector statement, however, fetches all of its operands 
before storing any operan&#38;, in general, direct translation to vector statement is valid if the statement 
in question does not depend, either directly or indirectly, on itself. JKI(I,P 80] gives an algorithm 
for vectorislns set of statements contained inside single FOR loop. The alsorithm consists of five steps 
(I) Constructs dependence graph G for the statements in the loop (2) Find the strongly connected components(SCC) 
in G (3) For each statement not part of SCC in G vectorise the statement (4) For each SCC in G generate 
a FOR loop around the statements in the SCC and col- lapse the SCC into 8in Is node (5) Generate the 
loops and vector statements in the new G in topologlcal order  Vectorisation is reordering of the set 
of all executions of the statement in loop 8o that ale executions of a given statement are Srouped together, 
but with no specified ordering among that group. Thus, vectorisstion of a set of |tatemente contained 
in one loop is a single loop transforma- tion. Extending the above algorithm to vector'me statements 
contained in multiple loops is now easy. The extended algorithm performs steps two, three and four of 
the above algorithm on each component graph of the hierarchical dependence Srsph. While applying these 
steps to CDG, it updates the sraph to reflect the new vector statements and dis- trlbuted loops being 
generated. The loop indepen- dent dependences in the updated HI)G reflect the new topolosical statement 
order required by step five. Finally, the extended algorithm procemes the CDG's in bottom up order. 
This order Suarantse8 compound node C contained in a CDG t depth k represents a CI)G at depth k+l which 
is single strongly connected component. Hence, we conclude  statement is contained in SCC of the flat 
laY-ered dependence graph for loop L if' and only if it is contained in SCC in the CDG representins 
the statements in L.  Fisure 3 ~ves the complete alsorithm for vectorlsstion. Given the G' m. (V'~') 
create by step (1) in VECT, the time spent procemln S step (2) is O([V'J+JE'J) usinS Tsrjan'e algorithm 
iT re 72]. Similinrly, the time spent processing steps (a) and (4) is ~ O(IV'J+JE'J). Thus, the time 
spent in a single call to VECT is O([V'J+JE'J).  procedure VECT(L,k) /e L is loop node for Seep to be 
vectorised e/ /e k is ncetins depth of statements in L e/ let O be the (X)G umcisted with L let G' :-- 
O (I)/0 Call VECT on each contained loop 0/ for each node L' in G do if L' is loop node /e Update 
G' to reflect vectorised loop o/ then repince L' by VECTOL',k+I) in G' /0 If L b WHILE, no vectorkation 
pomibk ./ if L b WHILE no&#38; then besin let the CI)G mmociated with L be G' return(the srsph coneisting 
of L) end  I' use Tacj n', alprithm [Tacj 72] ,/ find the strongly connected resiGns in G' (8)/0 rectories 
-I! ,tatemente not in t SCC o/ for each node N in G' not contained in s SCC besJn if N is n statement 
node then let N' be N vectorised in k'th dimension else besin let N' be loop node with the same upper 
bound u L let the CDG nseoclated with N' be N end replace N by N' in G' for each edp E incident to (or 
from) N' do mark E u loop independent end (4)/* Dbtribute L around each 8CC o/ for each strongly connected 
component S in G' do he~n let L' be a copy of loop node L replace S by L' in G' let 8 become the 01)G 
mmoclated with L' for each edse E incident to (or from) L' do mark E u loop independent end /o Return 
graph of vectorked L e/  return(O') end Fisum 8. Vactorisation A~|orithm i 278 To construct a time 
bound for sum of all calls to VECT, let NG --(NV, NE) be the name dependence graph from which the layered 
depen- denes graph and the HDG were constructed. Let k be the depth of the deepest CDG in the HDG. A 
given edge in NE can result in at most k loop car- ried dependences. These k edges exist in k different 
CDG'e, Oi, G 7 ..... Gk, where G, is the direct ances- tor of Gt+ t in the ltD(]. The given edge in NE 
can also result in single loop independent dependence in the deepest of these CDG'S" G t. In the worst 
case, VF, CT may transfer both the loop carried dependence and the loop independent dependence in G t 
into Of. i during the update of G k. However, since both these dependences are now loop indepen- dent, 
we may collapse them into single loop independent dependence. Now, Oh. 1 contains st must two layered 
dependences for the edge in NE. By inductively repreating this argument for each Gi, we conclude that 
VECT examines at most 2ok layered dependences for the given edge in NF~ Each node in NV is processed 
once. However, the update process can result in k new nodes for every node in NV. Thus, the total time 
required for nil eaib to VF~T is O(ko(INV]+I~I) }. This algorithm compares well with previous algorithms. 
For example, [Kenn 80] gives veetod- sation algorithm based on loop carried dependence. Nots that this 
algorithm and VECT produce the same vector code. The worst case time for this algorithm is O(ko(JLVI+JLEJ) 
) where LG --(LV~E) is the layered dependence graph for program and k is the maximum nesting depth for 
that program. Sines single edge in the name dependence graph can actuaily result on k+i edges in the 
layered dependence graph, this algorithm is O(kt*INEI+koINVI) ). Thus, for programs in which JLEI is 
O(k*lNEI) (NE consi~ primarily of edge linking scalars), VF, CVF performs better, in the ease of vectorisation, 
the hierarchical dependence graph makes possible an efficient algorithm. T. Loop Fudon Another transformation 
well suited for use with the EIDG is loop fusion. As the name suggests, loop fnsion is transformation 
in which the bodies of FOR loops are fused together to form a single FOR loop. While not single loop 
transformation, loop fusion can be viewed as the inverse of a single loop transformation in which a single 
FOR loop is split in to two separate FOR loops. Taking advan- tMe of this fact, we formulate the general 
fusion algorithm as a sequence of calls to simplified fusion algorithm performed on each component graph 
in the iIDG. One might uk why loop fusion is useful [AIIC T21 suggests the use of loop fusion in eombl- 
nation with loop unrolling. The loop fuaio algorithm would automatically fuse copies or inner loops created 
by unrolling outer loops. JAlle 83] and [KKLP 80] suggest performing loop fusion to optim- ise the use 
of vector registers. Another use, sug- gested by John Cocks, is in compiling APL for sequential machines. 
A naive translator could pro- dues set of sselgnmente each nested inside severs/ loops. The loop fusion 
algorithm could then automatically collect these statement8 inside com-mon loops whenever pomible. After 
fusion, more optimiJatious such as the replacement of temporary arrays by scalars could be applied to 
the resulting code. in general, two adjacent FOR loops may fused together if two conditions are satisfied. 
(I) The induction variables for both loops iterate to common upper bound. (2) Fusing the bodies of 
the two FOR loops preserves Ill dependences from the first loop to the second loop.  For programs containing 
nested loops, cri-terion two can be expressed in terms of layered dependence. Two adjacent FOR loops 
wboee headers are at nearing level k may be fused it the fusion does not induce any level k+l loop carried 
dependences from the body of the second loop to the body of the &#38;rat loop. To understand this, remember 
the direction vector for a level k+l loop curried dependence consist, of k ,-'e followed by ~. Before 
fusion, the direction vector consisted of k --'S" 8o the dependence must have been loop independent 
dependence from the first loop to the second loop. Than, the fudq of the two loops revenmd this dependence 
causing the transforma- tion to be inv~d. Definition Let D be loop independent dependence both incident 
to and incident from s FOR node in CDG at depth k. Let S t and yj be the actual statements giving rue 
to D. D is fusion preventlq if V(Sj,S,) for the new fused loop contains the direction vector RPT(--,k}[[(<). 
 Previous disemmlons of loop fusion have only given the conditions under which adjaes t FOR loops may 
be fused. We formulate an algorithm which reorders statements to bring as many FOR loops together m, 
pomible. Sines the concept of adjaes cy is nebulous for n dependence graph, we use the topological order 
imposed by the loop independent dependences to guide our efforts. This assumption is valid sines only 
loop independent dependences force textual ordering. Now, two loops are candidates for loop fusion it 
the 279 topolngical order imposed by the loop independent dependences allows the two compound nodes 
to be generated sequentially. The goal of the algorithm is to generate together as many fusible FOR nodes 
as po~ible. Informally, the algorithm consists of the following steps applied to each CDG. (I) Topologically 
generate as many statement nodes as possible (2) Let L be a loop node that can be topologi- caily generated. 
 (3) Fuse L with as many other loop nodes as topologically available. (4) Generate L. (5) Repeat this 
process until all nodes are gen-erated.  By generating all possible scalar statements, the algorithm 
guarantees that as many FOR nodes as possible are available for steps (2) and (8). Moreover, after step 
(I), only FOR nodes can be generated. Since at least one FOR node must now be generated, the algorithm 
chooses FOR node and attempts to fuse it with as many FOR nodes as topologically possible. This process 
is repeated for the remaining dependence graph. Figure 4 contains precise specification of the loop 
fusion algorithm. This algorithm performs the transformation above, but in more el~cient manner. Step 
(1) in FUSE topologically generates as many statement nodes and WHILE nodes as pos- sible. Note, whenever 
 new WHILE node or state- ment node becomes available to be generated, it is added to STATQUEUE. When 
 FOR node becomes available, the algorithm calls the pro-cedure add. Add looks up the upper bound of 
the loop in LOOPTABLE. If another FOR node with the same upper bound already exists in LOOPT- ABLE, add 
fuses the two FOR nodes and reinserts them in LOOPTABLE. Otherwise, add inserts the new FOR loop in LOOPTABLE. 
No fusion preventing dependences can exist between the fused FOR nodes, since both nodes being available 
to be generated implies no loop independent dependences exist between them. Since termination of step 
(1) forces some FOR node to be generated, step {2) chooses FOR node, RL, to be generated. Step (3) attempts 
to fuse as many successors of RL to RL as topologically possible. FUSEQUEUE holds FOR nodes ready to 
be fused to RL. To analyse the time required by FUSE, we consider the time for one call to FUSE on the 
CDG G --(V,E) above. Both STATQUEUE and FUSE- QUEUE are normal queues so operations such as insert, delete, 
and empty take constant time. The inner most loop in step (1) is iterated at most ~EJ times with the 
add operation being done at most iN[ times. So, the time for step one is O([E[ + [NJ*cost(add)). The 
time for step (2) is O(Â¢ost(remove)). To find a bound for step (3), we must analyse the time required 
to test for fusion preventing dependence. Since we restricted the tests used in calculating sets of direction 
vectors to linear subscripts, we can update the set of direction vec- tors to reflect the new fused loop 
in constant time. Moreover, since the original set of direction vectors contained a loop independent 
dependence, we can also test for fusion preventing dependence in con- stant time. The iteration of the 
innermost loop in step (3) is bounded by JEJ with at most JNJ acld's being performed. The worst case 
time for (3) is O([E [ + [Niecnst(sdd)). The total for one call to FUSE k O(JE[+JNJecost(add) Ã· cost(remove)). 
Now, we addrem the cost of the add's and the remove. If these operations are implemented using 2-3 tress 
[AhHU V4], then O([NJ*cost(add) + cost{remove)) is O([NJ*I~JNJ). in practice, the algo. rithm above could 
use s hash table to. achieve linear average case time. For our analysis of worst case time, we use 2-$ 
trees. To bound the cost of all calls to FU~E, we let again NO R (NV, NE) be the original name dependence 
graph. Using an argument similiar to the one given in the previous section, we can show that the time 
processing all layered dependences associated with a given name dependence is O(k) where k is the depth 
of the deepest CDG. To bound the time processing nodes, we only need note that each node in NV is processed 
only once. Addition- ally no new nodes are created by the update in FUSE. Thus, the total time proceming 
nodes is O(INVI*IgINVI).The worst cue time bound for all calis to FU~ is O(k*INEI+INVI*lgINVl). This 
algorithm does not always yield maxl- really fused loops. It may fail for one of two re~- sons, First, 
the choice of s FOR node to fuse in step (2) can aiTect the success of the fusion algo- rithm in later 
stages. However, since most programs tend to have FOR loops st s given nesting level iterate to the same 
upper bound, the set of avail- able FOR nodes to choose from in step (2) will usu- ally contain only 
one member. Second, the algo-rithm alsu limits its scope to only one component dependence graph st a 
time. To guarantee maximal loop fusion, the algorithm should, when choosing which FOR nodes to fuse, 
consider the success or failure of attempts to fuse subsequent loops con-tained in new fused loop*. However, 
altering the algorithm to attempt this type of look ahead would severely affect its performance. In general, 
the above algorithm should perform well on tasks such as those given earlier in this section. 8. Conclusion 
The IID(] stills needs work in few areas. First, the use of a normalisation routine prior to construction 
of the HDG is too restrictive. The code produced by this routine may not be as efficient as the original 
code because of the simplified control else add(LP', LOOPTABLE) else inserqLP',STATQUEUE) end procedure 
FUSE(L) 1. L is loop node containing loops to be fused ./ let G be the nodes and loop independent dependences 
in the CDG for loop node L let G' be the empty graph let STATQUEUE, FUSEQUEUE be empty queues let LOOPTABLE 
be an empty table for each node N in G which has no predecessors do if N is a FOR node then add(N, LOOPTABLE) 
else insert(N, STATQUEUE)  while ('empty(STATQUEUE) or "empty(LOOPTABLE)) do begin (s) /. Topologically 
generate statements ./ while('empty(STATQUEUE)) do begin let N : -- delete(STATQUEUE) move N from G to 
G' for each M a successor of N do if M has no predeceseom in O then if M is s FOR node then add(Y, LOOPTABLE) 
else insert(Y, STATQUEUE) end (2) /* Choose s FOR loop to generate 0/ if "empty(LOOPTABLE} then begin 
let RL :-- re, move(LOOPTABLE) sdd(RL, FUSEQUEUE) let L' be loop node with empty CDG and same upper bound 
as RL end  (3) /* Topologically fuse FOR loops to RL ,1 while ('empty(FUSEQUEUE)) do begin let LP :-- 
delete(FUSEQUEUE) let L' :-- merge(L', LP) /0 Merge CDG's 0/ remove LP from G for each LP' s successor 
of LP in O do if LP' has no predecessors in O then if LP' is FOR node then /. UPB is upper bound for 
loops 0/ it (UPB(LP') -,, UPB(L')) and ("fusion_preventing(L',LP')) then insert(LP', FUSEQUEUE) fuss(L') 
/* Recureive call on nested loops */ add L' to G' end let O' be the CDG amociated with loop node L end 
Fignre 4. Loop fusion Algorithm structure. One approach being explored is the incor- poration of the 
control dependence structure described in [FeOW 83]. Using thi~ approach, the control dependences are 
represented as they exist in the original program. Another ires of concern is the design of an implementation 
of the HDG. The techniques used in [AIIK 82] should provide a firm basis for an implementation of the 
llDG. The most crucial deci- sion concerning any implementation is how loop nodes and their associated 
CDG's are represented. Finally, more transformations suitable for use with HDG need to be identified. 
One candidate transfor- mation is the automatic conversion of a sequential FOR loops into the LOOP CONC 
construct described in [Love 77]. Since this is s single loop transformation, an efficient algorithm 
for transforming nested FOR loops into LOOP CONC's should be possible. In summary, the HDG provides an 
attractive intermediate representation for program transfor-mations. By carefully partitioning the information 
available from layered dependence into CDG's, the hierarchical dependence graph allows construction of 
efficient algorithms for s variety of reordering transformations. In addition, the representation of 
loops as compound nodes allows the formulation of these algorithms in more elegant terms. ThuB, this 
form satisfies one of the fundamental requirements of any intermediate form: algorithms based on it should 
be both efficient and elegant. Acknowledgements i would like to thank Mike Burke, Jeanne Ferrante and 
Fran Allen for their time and willing- hess to discuss the ideas in this paper. I also would like to 
thank Randy Allen and Doug Moore for their help in reading this paper. 9. Blbllousphy <RefA>JAhlIU 741 A.V. 
Alto, J.R. Hopcroft, J.D. Ullman, The Design and Analysis of Computer Algorithms, 281 Addison-Wesley, 
Reading, Musachusetts, 1074 [Kuek 78] D.J. Kuek, The Structure of Computers Ind Compute, ions, Volume 
1, John W'dey and Sons,  V~cPw s2] New York, 1978. J. R. Allen, K. Kennedy, C. Porterfleld, J. War- 
ren, "Conversion of Control Dependence to Data Dependence," Conference Record of the Tenth [Pa s01 Annual 
ACM Symposium on Principles of Pro- D.A. Padua, D.J. Kuck, and D.IL Lswrle, STammins Languages, Austin, 
TX., Jan. 83 "High-Speed Multiproc_,~-~ rs stud Compilation Techniques," Special Imue on Parallel Procem- 
in&#38; ~ Trssm. on Computers, Vol. 0-29, No.  ~c 72J 0, pp. 708-770, Sep. 1080 F.E. Allen sad J. Cocke, 
"A cstalosue of optim- ising transformations," Design and Optimisation of Compilers, R. Rue,in, ed., 
Prentice-Hall, ITari Englewood Cliffs, New Jersey, 1072, 1-30 R.E. Tarian, "Depth first eesrch and linear 
Srsph algcrithms," SIAM J. Computing I, 2, 1072, 146.180 [Alle 83] J.R. Allen, "Dependence analysis 
for subscripted varisbles and its pplicstion to proKrsm transformations," Ph.D thesis, Rice University, 
Department of Mathemstical Science, Ms)' 1983   [AZIK S~] J.R. Allen, K. Kennedy, "PFC: program to 
convert Fortran to parallel form," Report MASC TR 82-6, Department of Mathematical Sciences, Rice University, 
Houston, Texas, March, 1082  FeOW 831 J. Ferrante, K. Ottermtein, and J. Warren, "The Prosrsan Dependence 
Graph and it~ uses in optimisstion', IBM Technical Report RC 10208 August 12, 1083 [Lamp 715] Lamport, 
Leslie, "The Coordinste Method for the Parallel Execution of lterative Do loops," SRI Technical Report 
CA-7606.0~1, August 2, 1076 [Love 77] D.B. Loveman, "Program improvement by source to source transformation," 
J. of the ACM, Vol. 24, No. I, pp. 121-145, Jan. 77 [I enrlt80] K. Kennedy, "Automatic translation of 
Fortran programs to vector form," Rice Technical Report 476-029-4, Rice University, Oct. 1900   [KKLP 
S0] D.J. Kuck, R.H. Kuhn, B. Leuure, DJL Padua, and M. Wolfe, "Dependence Ip'aphs and corn- plier optimization 
," Conference Record of Eighth Annual ACM Symposium on Principles of Pro~amming Languages, Williamsburg, 
V~., Jan. 81.</RefA>  282  
			
