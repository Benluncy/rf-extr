
 A Highly Available, Scalable ITV System Michael N. Nelson and Mark Linton Silicon Graphics, Inc. Susan 
Owicki Independent Consultant Abstract As part of Time Warner s interactive TV trial in Orlando, Florida, 
we have implemented mechanisms for the construction of highly available and scalables ystem services 
and applications, Our mech­anisms rely on an underlying distributed objects architecture, simi­lar to 
Spring[ 1]. We have extended a standard name service interface to provide selectors for choosing among 
service replicas and auditing to allow the automatic detection and removal of unre­sponsive objects from 
the name space. In addition, our system sup­ports resource recovery, by letting servers detect client 
failures, and automated restart of failed services. Our experience has been that these mechanisms greatly 
simplify the development of ser­vices that are both highly available and scalable, The system was built 
in less than 15 months, is currently in a small number of homes, and will support the @ial s 4,000 users 
later this year. Introduction Interactive TV (ITV) is a new application domain for distributed systems. 
Although various authors have analyzed the requirements for system software, storage systems, networks, 
and settop boxes [2, 3,4,5, 6, 7], the structure and functionality of tlese systems is still being defined. 
One clear goal is to provide a variety of ser­vices in a format accessible to subscribers who have no 
experience with computers. In this domain, availability and scalability are key requirements for system 
services and applications alike. Customers expect their TV to function whenever they turn it on, and 
they are not likely to tol­erate anything but rare unavailability for interactive TV services, While 
computer users are accustomed to occasional pauses for CPU, disk, or network intensive tasks, interactive 
TV users expect no response to be longer than the familiar half-second delay asso­ciated with changing 
a TV channel, or the few seconds required for VCR operations. Thus the system must not only be highly 
avail­able, but it also must be scalable so that it can handle many simul­taneous users, each expecting 
fast response. We have implemented a distributed system for Time Warner s interactive TV trial in Orlando, 
Florida. In this system, the servers run IRIX, SGI S version of UNIX, and the settops run a custom Permission 
to make digitalhard copy of part or all of this work for personal or classroom use is granted without 
fee provided that copies are not made or distributed for profit or commercial advantage, the copyright 
notice, the title of the publication and its date appear, and notioe is given that copying is by permission 
of ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires 
prior specific permission and/or a fee, SIGOPS 95 12/95 CO, USA Q 1995 ACM 0-89791-71 5-4/95/0012...$3.50 
real-time kernel. Interaction between system components takes place via distributed objects. Typically, 
a service is implemented by one or more objects residing on one of the servers. A client residing on 
another machine can invoke the obj ect s methods to obtain its services. Thus distributed objects combine 
the features of remote procedure call and object-oriented programming. Our distributed objects resemble 
those of Spring [1], All client-server interfaces are specified in the CORB A Interface Definition Lan­guage 
(IDL) [8]; they correspond to object type definitions. The base level of our system is called the Object 
Communication System (OCS). In addition to implementing distributed objects, OCS includes several other 
facilities: a name service for locating service objects, an authentication service for validating client 
requests, a database, managers for individual servers and server clusters, and the Resource Audit Service, 
which provides notifica­tion when system components fail. One of tie aims of OCS, and the focus of this 
paper, is to simplify the development of scalable and highly available services. Because interactive 
TV applications typically implement a service as well as an interactive user interface, all developers, 
not just sys­tems gurus, must be able to design highly available and scalable services. All of the OCS 
components except the authentication service have a role to play in that task, Replication is the key 
to both scalability and availability, so an essential function of OCS is supporting replication. Let 
us fwst consider how replication is used for scalability. A service is scal­able if an increase in its 
workload can be accommodated with a proportional increase in its resources. Scalable services in our 
sys­tem are typically implemented with a replica running on each server. All the replicas are normally 
active in acting on requests from clients. To expand the systems capacity, one acquires a new server 
to run an additional replica for each service. In our system, most service replicas operate nearly independently, 
so that system capacity grows linearly with the number of servers. Replication is used in a slightly 
different way when the goal is availability rather than scaling. In this case, only one of the repli­cas, 
the primary, accepts client requests. Other replicas remain passive unless the prtmsry fails. At that 
time, one of the backup replicas becomes primary and continues to provide service. Our system includes 
a number of services that use primary/backnp rep­lication, a number that run with multiple active replicas, 
and one service (the Connection Manager) which combines both approaches. OCS support for replication 
is chiefly provided by the name sm­vice. Clients use the name service to locate other services, so it 
is an ideal place to provide a client interface which hides replication. Two novel features of the name 
service specifically support repli­cated servers. Selectors offer a mechanism for choosing one of several 
object replicas that provide the same service, giving a sim­ple way to implement load balancing schemes 
when replication is used for scaling. Auditing is a mechanism for automatically removing unresponsive 
objects from the name space. It provides a simple way to implement the transition between a failed primary 
and its backup. Two other OCS services provide support for availability. The ser­vice managers monitor 
the status of active services and restart rep­licas if there are failures. The Resource Audit Service 
can provide notification of failures of processes, servers, and settops. It is typi­cally used by services 
that need to clean up after a failed client e.g. by reclaiming resources allocated to that client. For 
example, if a client on a settop fails in the midst of playing a movie, failure notification makes it 
possible to recover the network and disk resources that had been reserved for playing that movie. Our 
experience using OCS for the Orlando software has been very positive. We have been able to build roughly 
25 services in under 15 months (including the development of OCS itself). Application developers find 
it relatively easy to implement services using the features in OCS. The remainder of this paper is organized 
as follows. In Section 2 we provide background on our choice to use distributed objects and other OCS 
facilities. In Section 3 we describe the overall interactive TV system as built for the Orlando trial, 
tracing the ser­vice invocations for several examples of normal operation and failure. Next we describe 
the name service, emphasizing its novel features, in Section 4. Section 5 describes the mechanisms that 
we tsse for replicating services. Section 6 describes the service manag­ers, and Section 7 the Resource 
Audit Service. Section 8 illustrates how the facilities in the preceding sections are combined to pro­vide 
high availability. Finally we summarize our experience, related work, and conclusions in Sections 9, 
10, and 11. Background When the project team was formed to build the system for the Orlando trial, we 
were faced with the monumental task of design­ing and implementing a complex ITV system in a little over 
a year. Most of the developers had no experience with distributed objects, and their inclination was 
to base communication on standard lJNIX networking techniques such as sockets and Sun RPC. In this environment, 
each developer implemented his or her own commu­nication protocol on IRIX, dealing with the complexities 
of mar­shaling, @ansports, and server location. Server-addresses were often hard-coded into client programs, 
read from environment variables, or determined in some idiosyncratic way. It quickly became clear that 
this anarchic approach was hard on developers and would lead to a system with unmanageable com­plexity. 
To provide a flexible, higher-level infrastructure, we decided on distributed objects as the basic model 
for communica­tion. JWth this approach, developers need not write networking code. They need only write 
interface definitions for their services; stubs generated from the interfaces by an IDL compiler use 
the OCS runtime to implement remote method invocation. Besides simplifying communication, distributed 
objects offer the usual benefits of object-oriented programming, including type safety and the option 
of multiple implementations for a single interface. We believe that the use of distributed objects was 
a major reason that it was pxisible to develop our large system in such a short time. To give developers 
a better means of locating services, we chose to provide an object-oriented name service. With the name 
ser­vice, there was no reason to hard-wire service addresses into client programs, and this made it feasible 
to migrate and replicate ser­vices. We came to realize that the name service could be leveraged to support 
replication, which was essential for performance and recovery from service failure. As the system took 
shape, it became clear that high availability was a requirement for all services, and not j ust for key 
system components. (This became especially obvious as we dealt with the numerous bugs in early versions 
of the system!) It was essential that availability be designed into each service from the start, and 
developers needed tools to help in the task. This observation led us to extend the name service with 
support for replication, and pro­vide additional services for managing active replicas and failure notification. 
These tools provide the basis for recovery from server and client failures, However, they do not provide 
mechanisms for maintaining replicated state, so it is up to developers to determine how to initialize 
the state of a newly-active replica. We believe that the three techniques described above, distributed 
objects for client.lserver interactions, leveraging the name service to support replication and recove~, 
and designing availability into all services, have been crucial to the successful development of our 
system. In the next section we give an overview of the sys­tem, and then consider the facilities for 
availability sod scalability in more detail. 3 System overview Interactive TV means different things 
to different people. The goal for the Orlando trial is to understand the vulue of a wide range of services 
that can be offered, as opposed to focusing on the cost of providing a specific set of services. The 
rationale is that future cost is easier to predict than the potential value of untested services. All 
user interaction with the system is through a remote control; there is no keybomd. Choices that would 
be made through menus on a personal computer are typically made by navigating through a series of video 
clips that play seamlessly to give a natural, com­fortable interface. Our approach therefore has been 
to try to make interactive TV more like TV than a personal computer. One consequence of this model is 
that we believe users will expect that switching applica­tions, such as from video-on-demand to home 
shopping, should be as fast as changing channels on a normal TV. A related require­ment is reliability 
-people don t ex~ct TVs to crash. From a scalability perspective, we expect that one interactive TV site 
must be able to support tens or hundreds of thousands of simultaneous users. Each active user typically 
represents at least one video and audio stream. For the Orlando trial, the requirement was to support 
1,000 concurrent users from a community of 4,000. Applications available in the Orlando trial include 
video-on­demand, home shopping, and multiplayer games. Applications are themselves distributed, with 
a portion to control the user interface running on the settop and a portion to provide access to data 
and other services rnnning on a server machine. 3.1 Network configuration Figure 1 shows a typical configuration 
for the interactive TV sys­tem. Several multiprocessor servers are connected via an ATM network to the 
settop computers that are in each cable subscriber s house. The servers are themselves connected via 
FDDI. FDDI 1 1 SGI SGI O* Challenge Challenge eo Settop Settop FIGURE 1. Typical Configuration The settop 
computers can vary significantly in CPU power and memory capacity, but normally do not have any local 
disk storage. The amount of network bandwidth available from the settop to the server (upstream) and 
the server to the settop (downstream) can also vary widely. For the Orlando deployment, each settop is 
allowed a maximum of 50 Kbits per second from the settop to the server and 6 Mbits per second from the 
server to the settop. For both load balancing and administrative reasons, we partition the settops into 
neighborhoods. The neighborhood is determined by the settop s 1P address. Each server is responsible 
for some set of neighborhoods. Neighborhoods are important for replication and failure recovery. 3.2 
Object exchange At the lowest level of OCS is the object exchange layer that pro­vides transparent method 
calls across the network. Callers perform a C++ virtual function call that maps to a local implementation 
or to stubs that perform a remote procedure call. OCS is similar to CORBA environments in that it uses 
the CORBA Interface Description Language (IDL) to define objects. An object provides a service defined 
by an interface. Each object hides the representation of its state and the methods used to imple­ment 
its operations. For example. an object may be a file, whose interface includes the operations read and 
write. A requestor of a service is a client of the object providing that ser­vice; the object providing 
the service is a server of the requestor. Typically an object has both client and server relationships 
with other objects. 3,2.1 Client view Clients request services by invoking operations on an object refer­ence, 
which they obtain from the name service or some other ser­vice. The operations that are available on 
a particular object are defined in the IDL specification for the object. An object refenmtce denotes 
a particular object; it will identify the same object each time the reference is used. Object references 
may be passed as parameters to operations and returned as the result of operations. Wkh a few exwptions, 
notably the name service, object references are only good as long as the implementor of the object reference 
is alive. If the implementor crashes or halts, the object reference becomes invalid. The client will 
detect this on the next attempt to use the object reference. At that poin~ if the service is designed 
to be highly available, the client can obtain a reference to another object providing the same service. 
The implementation of an object reference must uniquely identify the object and allow communication with 
it. Our system uses sev­eral different representations; the one used for remote invocation contains 
1P address and port number of the server process implement­ing the object  timestamp, used to prevent 
use of this reference after the implementing process dies  object type identifier, used to determine 
the object s type at runtime object id, which identifies this object amongst those defined by the implementing 
process. Typically the object id is null, because most services export only one object. 3.2.2 Server 
view A process that provides a service first creates an object reference, using facilities of the OCS 
runtime. It then makes the object avail­ able to clients by binding it into the name service, and responds 
to method invocations as they arrive from clients.  3.3 Services Figure 2 shows the services that are 
typically available on each server machine. The name service allows clients to locate each of the other 
services. (The name service itself is located using infor­mation provided at start-up time; see Section 
3.4. 1.) The authentication service implements the system security policy. Security is essential in an 
ITV system, both to protect from attacks generated from settops and to isolate third-party services running 
on the server machines. The authentication service supports a Ker­beros-like security scheme[9]. The 
OCS library uses the authenti­cation service so that, when an object method is invoked, the object can 
securely determine the identity of the caller. Thus an object implementor can securely determine what 
rights to grant each caller. Likewise, a client knows that any replies it receives come from the intended 
recipient. Calls and returns can optionally be signed andlor encrypted. By default, calls are signed 
but not encrypted; this allows the server to authenticate a customer with­out entailing the overhead 
of encryption. In addition to the name service and authentication service, OCS includes the following 
services that provide basic support to ser­vice developers: Resource Audit Service (RAS). Provides support 
for reclaim­ing system resources.  . Settop Manager. Maintains information on settop status (up or down). 
mmm   EE3!!Elm FZE!!EIEI Other Services Bmm HDF!!!Y7 OCS Services Object Exchange SGI Challenge Server 
FIGURE 2. Server Services * Database. Provides access to persistent data via exported IDL interfaces. 
a Service Controllers. Manage the assignment of service replicas to servers, starting services when a 
server becomes active and restarting them after faihtres. The remaining services in Figure 2 provide 
the basic building blocks for interactive TV applications, relying on the OCS ser­vices for help in providing 
high availability: Connection Manager. Allocates Am connections between settops and servers.  Media 
Delivery Service. Delivers constant bit rate data (e.g. MPEG video) to settops.  Reliable Delivery Service 
(RDS). Downloads to the settop such data as fonts, images, and binaries, using a variable bit rate connection. 
 Media Management Service (MMS). Selects which Media Delivery Service to use to deliver a movie to a 
settop and sets up the required ATM connection.  Boot Broadcast Service. Broadcasts boot parameters 
to set­tops.  Kernel Broadcast Service. Broadcasts the kernel to settops.  File Service. Provides 
settops access to UNIX tiles. 3.4 Example: Playing a movie The steps involved in playing a movie illustrate 
how OCS is used in our system. The description below starts with booting the settop and proceeds through 
downloading an application to playing and closing a movie. Later sections of this paper give more details 
on the mechanisms illustrated in this example. 3.4.1 Booting Because settops are diskless, the kernel 
and first application are broadcast to settops using a secure protocol. This broadcast also provides 
the settops with basic configuration information, such as the 1P address of the name service replica 
to be used by this settop. The application obtained during boot is the Application Manager (AM). The 
AM receives channel change events from the remote control and downloads the appropriate application when 
a sub­scriber tunes to a channel that provides interactive services. 3.4.2 Downloading an application 
The Application Manager (AM) downloads applications via the Reliable Delivery Service (RDS). Figure 3 
shows the sequence involved in downloading an application. The AM Erst creates an object reference to 
the name service, using the IP address obtained at boot time. It then uses this object reference to obtain 
an object reference for the RDS. It next invokes the openData operation on the RDS object with the name 
of the application; openData returns the application executable. The AM copies the executable into memory 
and starts it. Challenge Server Name RDSService ~ / l.resolve( svc/rds ) / 2. opcnData( vod ) / r r 
Application New Manager 3. Create Application I Application I Settop FIGURE 3. Downloading an Application 
 The AM only contacts the name service for a reference to the RDS the first time it downloads an application. 
After this it uses the same RDS reference for all future downloads. If at some point the RDS reference 
stops working, the AM will obtain a new object reference and retry the download. The first application 
that the AM loads after booting is called the navigator. This application provides a convenient way for 
settop users to find applications of interest. 3.4.3 Selecting the application Once the Application Manager 
is downloaded into the settop, the user can select an application with the remote control. The naviga­tor 
can be used to find the desired application, or the user can enter the appropriate channel number directly. 
Some channels corre­ spond to single applications. others to venues through which a user can find a set 
of applications, e.g. games. 3.4.4 Playing a movie Once an application is downloaded, it can choose to 
play a movie. Figure 4 shows the steps involved: 1. The application resolves a reference to the Media 
Manage­ment Service (MMS). 2. The application tries to open the movie named T2° by invoking the open 
method on the MMS object. 3. The MMS resolves the connection manager object for the settop s neighborhood, 
neighborhood 1 in this example. 4. The MMS chooses a Media Delivery Service (MDS) replica to play the 
movie, based on where the movie is available and the current loads at servers. It then asks the Connection 
Manager to allocate a high-bandwidth connection between the chosen MDS and the settop. 5. The MMS obtains 
an object reference for the appropriate MDS replica. 6. The MMS invokes the open method on the MDS object, 
which returns a movie object. 7. The MMS returns this movie object to the settop. 8. The settop invokes 
the play method on the movie object and the MDS begins delivering data. 9. The MMS obtains a reference 
to the local Resource Audit Service (RAS) object. 10. The MMS periodically polls the RAS asking it the 
status of the settop that is playing the movie.  Most of the name resolutions occur only the fiist time 
a movie is opened. 3.4.5 Closing the movie When an application has no more use for a movie objec~ it 
notifies MMS by invoking the close method on the MMS object. This allows the MMS to reclaim resources 
allocated to playing the movie. In particular, it tells the MDS to deallocate movie resources and tells 
the connection manager to deallocate network bandwidth. 3.5 Failure scenarios If components fail while 
a movie is being played, the system re­configures so that play may continue. 3.5.1 Settop or application 
crash Normally, applications close movies when they are through with them. However, if an application 
or a settop crashes before the application releases all its movies, the system must reclaim these resources. 
This reclamation is done by the MMS using the RAS. As shown in Figure 4, the MMS periodically polls the 
RAS, checking to see if the application that has opened a movie is still running. If the RAS indicates 
that it is not, the MMS closes the movie and reclaims resources. 3.5.2 MDS crash If the MDS, or the server 
on which it is running, crashes while the settop is playing a movie, the application detects the failure 
when it stops receiving data. The application recovers by closing the original movie and then asking 
MMS to open the movie again. Since movies are replicated on more than one server. and each server runs 
its own instance of the MDS, most MDS failures can be covered. Connection Resource Manager Audit Service 
/0 4. cmgr->allocateo 10. ras->checkStatuso Media Media Name 6. mds->openo Service 4 > Management 4 
Delivery 3. cmgr = context->resolve( < svc/cmgr/l ) Service Service 5. mds = context->resolve( svc/mds/forge 
) i + 9. ras = context->resolve( svc/ras ) 2. mms->open( T2 ) 1. mms = context->resolve( svchnms ) 
 7. retum(movie) 8. movie->playo Settop FIGURE 4. Opening a Movie The MMS also tracks the status of 
each MDS replica. Once an attempt to open a movie from an MDS replica fails, the MMS assumes that the 
replica is dead. The MMS will periodically re­resolve and retry the MDS object reference for the failed 
MDS. This will succeed once the replica has been restarted by the mech­anisms described in Section 6. 
3.5.3 MMS crash The MMS can also crash. A client will detect an MMS crash by invoking an operation on 
an MMS object reference and receiving an exception. To recover from this exception, the client must obtain 
a new reference for the MMS from the name service. Because the MMS is implemented using the availability 
tech­ niques described in later sections, a new instance of the MMS will shortly be bound into the name 
service and available for client use. 4 Name service The name service is a fundamental part of our system. 
It allows services to publish their object references and clients to obtain them. It is also the basic 
tool used in implementing replication. In this section we will describe our object-oriented naming system. 
  4.1 Goals The naming system was designed with the following goals in mind: Allow objects of all types 
to be named. Allow multiple implementations of the name service interface. System components should be 
able to export objects by imple­ menting the context interface. Allow the implementation of the name 
service to be distributed for both scalability and availability. Provide support for building replicated 
services. 4.2 Naming model Our naming model supports a hierarchical name space, which resembles the 
space of Unix file names. Figure 5 shows an exam­ple of such a name space and how it could be represented 
by a par­ticular name service implementation. The root of the tree, and the circles labelled c 1, and 
c2, represent contexts. A context which is analogous to a Unix directory, is an object that contains 
a set of name-object bindings. Each name in a context is unique. Resolving a name is an operation on 
a context to obtain the object denoted by the name; binding a name is an operation to associate a name 
with a particular object. These operations return or take as a parameter the object itself The objects 
in a context s bindings maybe of any type, and in par­ticular may be contexts themselves. By binding 
contexts, we can create a naming graph such as the one in Figure 5. Given a context in some naming graph, 
a sequence of names can be used to refer to an object relative to that context. In Figure 5. for example, 
the name cl/bl can be resolved in the context at the root of the tree to obtain the object in the lower 
left-hand comer. The naming graph spanned by a context is a name space. which includes all bindings of 
names and objects that are accessible directly or indirectly through that context. b5 I by, cl CT I 
local context object remote context I II I ,I ,I 11 FIGURE 5. Naming Context  4.3 Example name service 
implementation Figure 5 shows how a sample name space could be represented by an implementation, A naming 
context can contain basically three classes of objects: Naming contexts that are implemented by the 
name service  Naming contexts that are implemented by other name services  Other types of objects 
 All three classes of objects are represented in Figure 5. If the name cl/bl is resolved, the name service 
discovers that cl is bound to a locally implemented context and then recursively looks up bl in this 
locally implemented context. If the name /73 is resolved, the name service simply returns the object 
reference bound to the name b3. If the name c2/b4 is resolved, the name service discovers that C2 is 
bound to a remotely imple­mented naming context. In this case the name service recursively invokes the 
resolve method on the naming context object reference that is bound to c2, passing b4 as an argument. 
 4.4 NamingContext interface The basic naming object is of type NamingContext. The key oper­ations on 
a context are: Object resolve(in Name name) Resolve a name to an object. void bind(in Name name, in 
Object obj) Bind an object to a name. void unbind(in Name name) Unbind the named object. void bindNewContext(in 
Name name) Bind a new NamingContext to the given name.  bind it to the given bl) in the named context 
void bindReplContext(in Name name) Make a new ReplicatedContext and name. void Iist(in Name name, out 
BindingList Return the list of bindings contained object.  4.5 Replicated contexts The ReplicatedContext 
object type is a subtype of the Naming-Context type; it provides support for replicated services. This 
object type is our main extension to the Spring naming model, and we use it extensively. A ReplicatedContext 
object contains (see Figure 6): a list of duplicates bound under a name  a selector object bound under 
the name selector  T  selector bh vod vod FIGURE 7. Replicated Context containing contexts  4.6 Implementation 
The name service implements the NamingContext and Replicated-Context interfaces through which other services 
register objects. This is the fundamental level of naming in our system. Each server cluster has one 
shared name space used by services and clients. Because the name service is essential to all services, 
it is replicated on every server node with master-slave replication. The master is elected using a majority 
scheme sinilar to the one in the Echo file  Aselector FIGURE 6. Replicated Context Objects are bound 
and unbound in a ReplicatedContext in the same way as in a NamingContext. The two types behave differently, 
however, with respect to the lookup functions resolve and list. When the name of a replicated context 
is resolved, the list of bound objects and their names is passed to the select operation on the selector 
object. The selector object chooses one of the replicas, which is then returned by the name service. 
The implementation of Selector objects can be arbitrarily complex. A Selector object may implement a 
simple policy, like returning the fist object in the list, or a complex policy, like choosing the most 
lightly loaded service. Section 5 contains examples of the use of selectors in our system. Figure 6 shows 
an example of a replicated context. In this example, if the name rds is resolved, then the select method 
on the object named selector is invoked with a list containing the two bindings l and 2 : object = selector-> 
select( .=/ 1 , object>, < 2 , object> ); The bound objects in a ReplicatedContext may themselves be 
contexts (see Figure 7). In this case the selector picks the context in which to complete the name lookup. 
In the example in Figur c 7, if the name bin/vod is resolved, the object named vod under context 1 or 
context 2 is returned depending on the selector s choice. When a replicated context is listed, the name 
service that implements the replicated context contacts the selector and returns binding information 
about the selected object. The operation listRepl, defined in the ReplicatedContext type, returns binding 
information about all of the bindings in a replicated context. system [11 ]. Once a master is elected, 
all updates are forwarded to the master, which serializes them and multicasts them to the slaves. Any 
name service replica can process a resolve or list operation without contacting the master. By replicating 
the name service on each server node, we greatly increased its availability and scalability. Availability 
is improved because the name service is available as long as a majority of replicas are alive. Scalability 
is improved because any server can process a name lookup locally. We expect updates to the name space 
to be infrequent -updates only occur when services are started or restarted. Thus requiring all updates 
to be serialized through the master should not impact the scalability of our system, Other services also 
implement naming context interfaces. For example, the file service implements a subclass of the NamingContext 
interface called a FileSystemContext. It exports additional operations for file creation. The file system 
exports its objects by binding FileSystemContext objects into the cluster-wide name space. Figure 8 shows 
a sample name space; it is a subset of the name space for the Orlando deployment. 4.7 Removing dead 
objects Another feature of our name service is the removal of objects when the service implementing them 
goes away. The name service uses the Resource Audit Service to determine if a service object is alive 
or dead (see Section 7) and removes an object within a few seconds of its death. This feature is used 
in implementing one type of replicated service, as described in the next section. 5 Replicated services 
Services in our system are replicated for both performance and availability. Two styles of replication 
are used. In the fns~ multi­ple active replicas exist, each capable of handling client requests. apps 
1 2 selector 1 2 selector cmgr 1 2 forge kiln 1 2 selector 192.26.65.83 ~ selector File Service Contexts 
 @ NamingContext ReplicatedContext FIGURE 8. Name Space In the second, one primary replica services 
client requests, while one or more backup replicas are available to take over if the pri­mary fails. 
Name service support for both approaches is described below. 5.1 Multiple active replicas In this form 
of replication, all replicas are active, and a client can, in principle, receive services from any replica. 
The replicated con­texts and selectors provided by the name service allow clients to contact a service 
without being aware of replication. For example, suppose the RDS service has three replicas, exported 
to the name service as svc/rds/1 , svc/rds/2 , and isvc/rds/3 . Clients obtain a reference to a service 
object by resolving the name svc/rds . The selector associated with the replicated context svc/rds chooses 
one replica and returns its object reference. Different load-balancing strategies can be provided by 
different selectors. At present, our system uses two forms of selectors, both implementing a static assignment 
based on the 1P address of the caller. With services replicated on a per-neighborhood basis, each replica 
binds into the name space under its neighborhood number. The neighborhood selector object determines 
the neighborhood number of the caller from its 1P address, and returns an object ref­erence for the appropriate 
replica. For services replicated on a per­server basis, the selector we use chooses the replica whose 
1P address matches the callers . Dynamic load-balancing could be accomplished with a selector that bases 
its choice on the current loads of the replicas. However, static policies, which are quicker and easier 
to implement, have proved adequate for almost all of our services. The one exception is the MDS; when 
a client attempts to open a movie, the choice of an MDS replica is based on movie locations as well as 
the avail­ability of disk and network bandwidth. Although this choice might have been implemented by 
a selector, it is sufficiently complex that it made more sense to implement it by a separate service: 
requests to open a movie are directed to the MMS, which chooses axrappropriate MDS replica. 192.26.65.82 
Selector o 0 Other object type  5.2 Primary/backup replieas As primary/backup replication is used in 
our system, the backup replicas do not provide service to clients unless the primary fails. When the 
replicas begin execution, they try to bind themselves in the global name space under the service name. 
The first one to suc­ceed becomes the primary. The others periodically retry the bind­ing request, which 
will fail so long as the primary is alive. If the primary fails, its binding will be removed from the 
name service, through dre mechanism to be discussed in Section 7. Subsequently one of the backup replica 
s bind requests will succeed. Services using primary/backup replication include the MMS, the Kernel Broadcast 
Service, and the Connection Manager. The Con­nection Manager actually uses both forms of replication. 
It has active replicas for each neighborhood and each server, and the neighborhood replicas are backed 
up by passive replicas. 6 Service Controllers Service controllers are responsible for the initial start-up 
of ser­vices as well as restarts occasioned by failures or service migra­tion. The Server Service Controller 
manages services on a single server, while the Cluster Service Controller manages the distribu­tion of 
services among servers. 6.1 Server Service Controller A Server Service Controller (SSC) replica runs 
on each server and manages the services running there. It starts and stops services, monitors running 
services, and restarts them in the case of failure. The set of services to run on the server is obtained 
from the Clus­ter Service Manager. In addition to starting and stopping services, the Service Controller 
provides two interfaces that allow the Resource Audit Service, de­scribed in Section 7, to track the 
state of service objects. The noti­ fyReady operation accepts a process id plus a list of objects and 
records an association between the listed objects and the process id. Each service invokes not&#38;yReady 
to register its exported objects. The registerCallbaclc operation allows the caller to register a call­ 
back object to be invoked whenever the set of live objects changes. Whenever a service registers objects 
with the ServiceController, the callback object is invoked with a list of the newly registered ob­jects. 
When a process is stopped or crashes, the callback is invoked with the list of objects associated with 
that process. The Resource Audit Service uses this facility to monitor the status of service ob­jects, 
as explained in Section 7.2. The SSC is started by the IRIX init process when a server is booted. If 
the SSC crashes, all services that have been started by the SSC will exit as welll. Thus it is imperative 
that the SSC not crash. We have attempted to keep the SSC as simple as possible to reduce the probability 
of a crash. However, if the SSC does in fact crash. it will be automatically restarted by the IRIX init 
daemon. 6.2 Cluster Service Controller The Cluster Service Controller (CSC) is responsible for managing 
the services for an entire cluster. The CSC determines whereto run services and when to move services 
between machines for 10ad balancing and failure recovery. The CSC directs the SSC on each machine to 
start and stop services as required. At least two servers run replicas of the CSC. One replica is designated 
the primary. During normal operation, only the primary is active. If the master CSC crashes, one of the 
backups takes over. This backup discovers the cluster state by querying each SSC to determine what services 
it is running. The mechanism used for primary/backup fail-over is discussed in Section 5.2. The current 
implementation of the CSC is relatively primitive. It reads a static configuration from the database 
to determine which services to run on each node. There are simple tools that allow an operator to cause 
a service or group of services to be stopped, started, or moved between nodes. A more sophisticated CSC 
implementation could determine the service configuration dynami­cally; this is an area for future work. 
  6.3 Start-up sequence and recovery The start-up sequence for cluster servers and servers includes 
the following steps 1. When a server boots. the SSC is started automatically. 2. The SSC starts the 
basic services, including the name ser­vice, the authentication service, the Resource Audit Service, 
and the data base service. 3. Once a majority of name service replicas are active, they elect a primary 
and begin accepting requests. At this point, base level services can register. 4. The master CSC reads 
the service configuration from the  database and directs each SSC to start the appropriate ser­vices. 
Once the cluster is functioning, the CSC periodically pings the SSC on each server to detect failures 
or recoveries. If a server 1. Children of the SSC exit because the SSC uses the wait system call to determine 
when to restart a service, and it is not possible for a newly restarted SS(2 to attach to children of 
a previous SSC. Thus it would not be possible for a new SSC to track services started by an old SSC. 
machine is restarted in a functioning cluster, the CSC detects the presence of the new SSC and instructs 
it to start the appropriate services. In the future, we intend to handle server failure by having the 
CSC distribute services among the remaining servers. In the current implementation, those services which 
have replicas on other serv­ers will continue to function. Other services will be unavailable until the 
server is restarted, or an operator re-assigns them to an available server. This is dkcussed further 
in Section 8. 7 Resource auditing Services need to be able to recover resources when clients that have 
allocated them fail to release them. Normally the receiver of a resource explicitly releases the resource 
when it is no longer needed. However, clients may fail to deallocate resources for two reason: The client 
or the machine where the client is running crashes.  The client is buggy and omits the deallocation. 
 In either case, resources are lost unless some action is taken to reclaim them. The Resource Audit 
Service described in this sec­tion deals with the first problem, client and machine crashes. The problem 
of buggy client programs is only addressed in a limited fashion; it is discussed at the end of this section. 
 7.1 Implementation alternatives In designing the mechanism for resource recovery, we considered four 
alternatives, two based on time-outs, and two based on detect­ing failed components. All of them allow 
some delay in the time between client failure and resource reclamation, but this is tolera­ble in our 
system. In most cases, resources are reclaimed because they are explicitly released. Although we need 
a mechanism for reclaiming resources after a client fails, this should be infrequent enough that some 
delay can be tolerated. It is only necessary that reclamation be fast enough to keep leakage from becoming 
a prob­lem. The first alternative we considered uses time-outs based on expected duration of usage. The 
service that allocates a resource estimates how long it will be needed, and revokes the allocation when 
that time is exceeded. For example, the MDS could close a movie after giving the client ample time to 
play it. In the initial stages of development, the MDS actually used this mechanism. We found that it 
was too conservative, especially in a development environment. A frequent failure mode for client code 
under devel­opment was to allocate a movie and then crash without releasing it. As the number of developers 
in the project increased, resource leakage began to make the system unusable. The second mechanism uses 
time-outs more aggressively. Resources are only granted for short periods of time. It is up to the client 
to periodically reallocate the resource. The allocation inter­ val must be kept short enough to prevent 
too much resource leak­age. However, short intervals mean numerous reallocation requests, especially 
with resources like movies that are held for a long time. We discarded this approach because of concerns 
about scaling. With thousands of clients, each holding several resources. this approach could consume 
too much network bandwidth and server CPU cycles with unnecessary reallocation messages, The last two 
mechanisms involve detecting client failure. In these approaches, a service will not revoke a resource 
until the client that allocated it has crashed. One approach is to let each service track the status 
of clients to which it has granted resources. Another option is to have a single service, the Resource 
Audit Ser­vice, track the state of all clients in the system. In this itnplementa­tion, services contact 
the RAS to determine the status of clients and other services. We chose the Resource Audit Service implementation 
because we believed that it would scale best. As we will show in Section 7.2.1, it requires only a small 
number of network messages to monitor clients and notify services of their failure. 7.2 Resource Audit 
Service implementation The Resonrce Audit Service (RAS) is a set of replicas, that cooyr­atively track 
the state of clients. To use the RAS, a service regis­ters a callback object associated with the entity 
(service or settop) that is receiving an allocation. If the entity subsequently fails. the RAS will notify 
the registering service by performing the callback. This callback interface is actually implemented by 
a combination of library code and a RAS object. The RAS object provides a sin­gle operation, checkStatus, 
which accepts a list of service and set­top objects and returns the status of each. To provide client 
callbacks, the library code periodically invokes checkstatus for all entities with callbacks. If checkStatus 
indicates that an entity is no longer active, the library code performs the callback to the client. The 
advantage to using this scheme. as opposed to having the RAS service directly export a callback interface, 
is that the RAS is not forced to remember callbacks when it recovers after a failure. In fact, we will 
see below that the RAS does not have to remember any state across failures, and this leads to a very 
simple recovery mechanism. An instance of the RAS runs on each server. When it is started, or restarted 
after failure, it does not know the state of any entities. The first time that it is asked about the 
state of a service or settop, the RAS records that entity with status unknown. It subsequently monitors 
the entity s status in one of three ways: 1) For settops, the RAS periodically polls the Settop Manager. 
2) For service objects running on the same server, it relies on a callback from the local SSC for notification 
of the object s failure. 3) For service objects running on another server, it periodically polls the 
RAS instance on that server. Thus, the RAS builds up its state over time. After failure it can recover 
state automatically as clients ask it questions. In addition, any call to the RAS returns immediately 
and does not block for the RAS to contact other services. The SSC callback for local service objects 
(item 2 above) is setup by the RAS when it begins execution. At this point, the RAS invokes the SSC method 
register Callback, described in Section 6.1. In addition, processes that create service objects regis­ter 
them with the SSC using notljjReac@. When the RAS registers its callback with the S SC, the SSC invokes 
the callback with the list of all active service objects at the time of registration. In addi­tion the 
SSC invokes the callback each time that a new service reg­isters its service objects. Tbe SSC can detect 
the failure of any process it started. When such a failure occurs, it performs the callback to notify 
the RAS of the faihrre of the process s registered objects. We originally tracked the state of service 
objects by periodically pinging them. If the object failed to respond within a few seconds, it was declared 
to be dead. However, we found that many single­threaded services were not able to respond to pings in 
a timely manner. Rather than requiring services provider; to-deal with the complexity of multi-threading 
when it was otherwise unnecessary, we chose to use callbacks from the Service Controller. 7.2.1 Messages 
In our RAS implementation, very few network messages are required. Services contact the RAS on their 
local machine, and each RAS instance registers a callback with the SSC on its local machine. The only 
network messages exchanged are between the RAS instances. Currently, each RAS instance polls the others 
every five seconds. The time between polls is somewhat arbitrary and could be increased to reduce the 
number of messages. We do not have enough experience with the system to know the best choice for this 
polling interval. However, because the RAS is used by the name service to remove dead objects, polling 
intervals can­ not grow too high without adversely impacting fail-over speed (see Section 9.7). 7.3 
Handling buggy clients We return now to the problem of recovering resources from long­running buggy clients. 
We considered two methods of addressing this problem resource limits and resource accounting. Resource 
limits are the mechanism currently implemented in our system. A settop client is only allowed to open 
a certain number of network connections and audiolvideo streams. If the settop attempts to acquire more 
resources, either its request is denied or one of the previously allocated resources is freed. Resource 
accounting would also be useful in dealing with buggy clients. If the system kept track of each client 
s resource consump­tion, buggy clients can be discovered through their inordinate resource consumption. 
The hope is that these bugs would be dis­covered before the application was allowed onto a production 
net­work. We currently do not attempt to do resource accounting -we rely on resource limits. However, 
accounting is needed both for discover­ing buggy clients and for charging properly for resource usage. 
8 Service availability The previous three sections have presented the chief system com­ponents that support 
availability: the name service, the service controllers, and the Resource Audit Service. In this section 
we dis­cuss how these components can be used to provide highly avail­able services. The system provides 
three mechanisms in support of availability: . Automatic (re) start of services Automatic rebinding 
of clients after service recovery  Optional notification of failures among clients or services  Each 
of these mechanisms is discussed in more detail below. 8.1 Restarting services Services can be designed 
to survive two kinds of failure: failure of the service itself (it may crash or be stopped by the CSC), 
and fail­ure of a server machine. Service failure is easy to handle: the appropriate SSC merely restarts 
the service. Server failure is more problematic. In our current implementation, services intended to 
survive server failure must have replicas running on two or more servers. Ultimately we expect the CSC 
to be able to automatically restart services on other servers after a machine failure, but this is not 
yet implemented. Services that are critical to the system s operation are replicated on two or more nodes 
with primary and backup replicas. These include the CSC, the MMS, and the Kernel Broadcast Service. Where 
services are replicated for performance, there is no reason to restart the replica until the failed server 
has recovered. For example, the Media Delivery Service, which moves movie data from disk to network, 
has one replica on each server. If a server is unavailable, there is no reason to restart its MDS replica 
on mother server, since that server s resources can be fully utilized by the replica already running 
there. A number of services are replicated on a per-neighborhood basis, with the replica for each neighborhood 
being assigned to a particu­lar server. One example is the Reliable Delivery Service, which transmits 
data from servers to settops. In the present implementa­tion, these services are not restarted automatically 
when their assigned server fails. As a result, server failure means that service to a group of settops 
will be interrupted. As mentioned before, the CSC could automatically restart these services on other 
servers. This is not done at present, because it requires judgments about how best to reallocate network 
bandwidth. Instead, tools are pro­vided that allow operators to direct the CSC in reconfiguring the services. 
   8.2 (Rebinding to services In order to access a service, a client first resolves the service name 
in the name service. The result is a reference to an object exported by a particular service replica. 
(How the name service determines which object to return when there are multiple replicas is dis­cussed 
Section 4.5). The client obtains service by invoking the methods through the object reference. If a service 
fails, a reference to one of its objects is no longer valid, even if the service is restarted. When the 
client attempts to invoke an object from a failed service, the object communication system raises an 
exception. At this point, library code in the client auto­matically returns to the name service to obtain 
another object ref­erence for the service. Eventually, the old object reference will be removed by the 
name service, and a new one will be bound in as soon as a replica takes over or the service is restarted. 
The advantage of this type of recovery technique is that it allows the implementor of objects to be relocated 
as necessary. A service can be moved at any time. and its clients will automatically recover. The disadvantage 
is that it presents the possibility of recover y storms [12]. If a populzu service crashes, many clients 
may invoke the name service at once to ask for a new object. Because the resolve operation is quite fast, 
we do not expect this to be a problem. If performance difficulties arise, we can modify the library routine 
to back off when repeating requests for a new ser­vice objects.  8.3 Failure notification The use of 
the RAS for failure notification has already been dis­ cussed in Section 7. In the current system, the 
RAS has two cli­ ents, the name service and the Media Management Service (MMS). The name service registers 
callbacks for all objects that are bound into the name space. When the name service is called back by 
the RAS, it deletes the dead objects from the name space. The Media Management Service (MMS), described 
in Section 3.3, allocates the dkk and network bandwidth required to play a movie from the server to a 
settop. The MMS uses the RAS to detect set­top failures and recover those resources. 9 Experience The 
system described in this paper has been implemented and is deployed at the SGI campus and as part of 
Time Warner s File Ser­vice Network in Orlando, Florida. We are currently gaining expe­rience using the 
mechanisms described in this paper and using the system itself. 9.1 Time to build One of the most strikhg 
properties of thk system is the speed with which it was built. The first members of the group that built 
it were hired in August 1993. The system was operational in Spring 1994 and was deployed in Orlando in 
December 1994. The project has involved up to 30 engineers workhtg on the many facets of the project. 
Five of them worked on the development of the Object Communication System described in this paper; the 
others worked on the other services and applications provided in the system. There were also other engineers 
working on third party applications. The resulting system contains about 25 services. We believe that 
we were able to build the system quickly because of the techniques described in this paper. The use of 
distributed object technology made it relatively easy for developers to create distributed applications. 
The typical steps to building a distributed application in our system are: 1. Write the IDL interface 
file. 2. Run the compiler on the interface file to generate the client and server-side stubs. 3. Run 
a tool that creates a skeleton service implementation. 4. Fill in the service implementation. 5. Create 
and export the implemented service object through the name service. 6. Write client code that looks 
up the service object and then invokes methods on it.   9.2 Use of objects The typical service in our 
system only exports a single object that implements the service interface that was defined in the IDL 
tile for the service. For example the Media Management Service exports a single object of type MMS that 
provides the methods needed to open and close movies. We don t normally need more than one object per 
service because, unlike Spring [1], we don t use a capability based security model. In Spring, if a service 
wished to grant different access rights to different clients, each cli­ent would need a different object 
that encapsulated the different access rights. In our system each incoming call on an object con­tains 
the caller s identify and it is up to the service to determine if the caller is allowed to invoke the 
desired operation. Since most services only export one object, we have managed to avoid the general problem 
of network garbage collection that arises in a system like Spring. The typical service has no need to 
track which clients possess object references to the service. The only services that dynamically create 
objects are the Media Deliv­ery Service, which creates one object for every open movie, and the name 
service, which creates one object for every context. Movie objects are cleaned up using the Resource 
Audit Service and the name service context objects are persistent so that they can be activated on demand. 
 As our system becomes used to solve different problems, we may discover that more services are forced 
to create objects dynami­cally. We hope that we can use our resource recovery scheme to reclaim the state 
of these dynamically created object references when the objects are no longer referenced. We have not 
tried to develop a more general solution so far because it has not been nec­ essary. 9.3 Response time 
Our goal was to respond to user requests within 0.5 seconds. The slowest operation is tuning to a new 
digital channel that presents a rich experience with movies, fonts, and images. In our system, various 
constraints (notably a download bandwidth of lMByte per second) lead to a start-up time of 2-4 seconds 
for such applica­tions. However, our system makes it easy for an application to pro­vide cover for this 
period: cover may consist of a still image or an animation generated at the settop. Our applications 
are able to dis­play cover within 0.5 seconds, although not all of them are imple­mented to do so. Thus 
viewers see a response within 0.5 seconds, which meets their expectations. 9.4 Support for replication 
One of the key aims for OCS was easing the task of developers in writing replicated services for scalability 
and availability. Our experience has been that the additional work needed to replicate a service is straightforward 
for most services. Most could use the strategy of independent replicas outlined in Section 5.1. With 
this approach, the extra work for replication consists of recovering state when a replica begins execution 
in an active system. Almost all of our services can recover state either by reading it from the database 
(slow-changing state) or recovering it from client invoca­tions. The notable exception is the name service. 
Here backup rep­licas are kept nearly up-to-date, so that a backup can become primary with minimal adjustment 
of its state. This substantially increases the complexity of the name service implementation, as does 
the need to elect a new primary after failure. A small set of services do not need to be replicated for 
scaling (a single replica can handle the load of our current clusters), These services use a primary/backup 
replication scheme based on the strategy outlined in Section 5.2 to support availability. For these services, 
the problem of choosing a new primary is solved using the name service. The problem of recovering state 
must be handled using the techniques mentioned above. So far, our decision not to include support for 
state recovery has not caused difficulties to developers, and it greatly simplifies the functionality 
needed from OCS. If we encounter services where state recovery is important, we will need to re-evaluate 
this choice, 9.5 Availability The OCS mechanisms for supporting high-availability can be evaluated in 
two ways. One is to ask how much they assist devel­opers. As indicated abo~e, our experience in this 
regard is quite positive. The other is to ask how effective the recovery techniques are in providing 
available services. We have been generalfy well satisfied here also. Most failures of services and settop 
programs (and there were many during debugging) were covered with only a very brief interruption. Server 
failure is still somewhat problemat­ical. As described in Section 6.3, it currently requires operator 
intervention to migrate some services when their server fails. We expect to develop tools for automatic 
reconfiguration in the future. We have found that the ease of service recovery is very useful dur­ing 
debugging. When we find a bug in a service, we can simply copy a conected binary to the appropriate servers 
and kill the ser­vice. The service will be restarted running the new version. Clients using the service 
see no disruption; the normal recovery mecha­nisms make the stop and restart invisible. 9.6 Scalability 
As with availability, one can evaluate the scalability features of OCS both in terms of their usefulness 
to developers and in terms of the scalability of the system. On the first count. we are satisfied that 
developers have been able to design replicated services with reasonable effort. However, we cannot say 
for sure that the result­ing system is truly scalable, because we have not been able to test a large 
configuration. We currently have fewer than 100 settops in our largest network. We have not encountered 
any scalability problems at this relatively small size. Before the end of year, we will have a larger 
number of settops and will be able to measure the true scalability of our system. Our current deployment 
has three servers, each serving two neigh­borhoods (see Section 3.1 ). We believe that the system will 
grow smoothly. As more settops are added, they will be accommodated in more neighborhoods, supported 
by more servers and more repli­cas of the various services. Since the services can be accessed through 
replicated contexts, applications will automatically have access to the new service instances. The unanswered 
question is whether there are unsuspected bottlenecks that will prevent the replicas from performing 
at full capacity, and this can only be determined by full-scale testing. 9.7 Fail-over speed The speed 
of primary/backup recovery, described in Section 5.2 is determined by three parameters: The interval 
at which the backup retries to bind into the name space  The interval at which the name service polls 
the local RAS to determine the state of service objects  The interval at which the RAS on the name service 
master s host polls the RASS on the other machines.  These numbers can be tuned to give the desired 
fail-over time, as long as it is not less than a few seconds. Times less than a few sec­onds are possible, 
but could cause scaling problems if the polls are too frequent. The current settings for these parameters 
are: . Backup retries bind every 10 seconds . Name service polls RAS every 10 seconds RAS polls other 
RASS every 5 seconds This gives a maximum fail over time of 25 seconds. As we use the system we will 
experiment to determine the proper settings for these parameters. Since failure is expected to be infrequent, 
a fail­over time of a few seconds should be acceptable. 10 Related work 10.1 Availability There has been 
extensive work on techniques for supporting avail­ability through replicated services. Two approaches 
are common. In the process group approach, all replicas see all requests. In the primary/backup approach. 
requests are directed to one replica, with other backup replicas ready to lake over in the case of failure. 
(Sometimes the backups are allowed to process queries indepen­dently, as is the case with our name service). 
There are two areas in which comparison of our work to existing approaches is enlight­ening. First, both 
the state machine and primary/backup approaches typically aim to keep the current state of all replicas 
as nearly consistent as possible, while our system provides no sup­port for replicated state. Second, 
the process group approach uses some form of mtriticast to distribute information about replicas that 
we entrust to the name service. These differences are dis­ cussed further below. 10.1.1 Support for replicated 
state The major omission in our system is support for maintaining reph­cated state. A common technique 
for making services highly avail­able is the hot standby, a replica whose state is kept nearly identical 
to the true state. One way of implementing this is to have a primary replica push all updates to the 
backup replicas [11, 13, 14]; this is the approach taken by the name service in our sys­tem. Another 
technique is to ensure that all updates are delivered to all the replicas in the same order, or in orders 
that are semanti­cally equivalent [15, 16, 17]. We chose not to provide support for state replication 
because the mechanisms for doing so are cumbersome, and we believed it would not be necessary for most 
of the interactive services. Hap­pily, this has turned out to be the case. Only two of our services require 
state replication; the name service, and the connection manager. The others are able to regenerate any 
required state by querying other services or relying on clients to provide required information (the 
stateless server approach of NFS [18] ). The MMS is an example of a service that recreates its state 
by polling other services. The volatile state of the MMS can be recon­structed by querying each MDS in 
the cluster and by querying the Connection Manager. Thus the volatile state of the MMS is distrib­uted 
amongst other services in the cluster. This technique is also used by the Cluster Service Controller. 
The Resource Audit Service recovers its state based on client calls. as described in Section 7. The Media 
Delivery Service likewise waits for clients to call into restart the movie they were viewing at the time 
of failure. The Video on Demand service, which is one of the applications that can request the MDS to 
play movies, main­tains information about the current point in movie play both in the settop and in its 
own service. If either the settop or the service fails, the other can supply the information needed to 
start the MDS at the point where the movie stopped. 10.1.2 Multicast groups Another mechanism for high 
availability, is to have groups of ser­vice replicas which can be addressed by multicasting [17, 19, 
20, 21]. Multicast with a consistent delivery order goes a long way in supporting replicated state, as 
observed in the last section. Even with unordered deliveries, multicast can be used to solve the prob­lem 
of binding to services. In this approach, a client desiring a ser­ vice issues a mukicast to the name 
associated with the service; the multicast is received by all group members. At this point, several options 
are possible. All group members could respond, leaving the client to choose the one it wanted, or the 
choice might be made by the service replicas, with only one responding. In either case, the client might 
continue to reach the service through multicast, or might bind to one service and subsequently contact 
it directly. This use of multicast parallek the use of selectors in the name service, with a replicated 
context corresponding to a mukicast group. 1S1S[ 17] also provides the option of notifying group members 
of changes in group membership; the notifications are consistently ordered with respect to each other 
and to ordinary multicast mes­sages. In our system, the RAS provides notification of failures but not 
start-up. We have not observed any need for the latter. In our system, implementing binding and notification 
through the name service and RAS is simpler than implementing multicast. (Maintaining multicast groups 
would require most of the mecha­nisms of the name service.) Since there is no advantage to deliver­ing 
messages to more than one replica, the name service approach is more appropriate. 10.2 Other ITV systems 
Other commercial ITV efforts are in progress, but their developers have not been in a hurry to publish 
details about their operation. The best known are Microsoft s Tiger [22] and Oracle s Media Server [23]. 
Tiger appears to be primarily aimed at media delivery; at least, other functionality has not been disclosed 
in the public literature. Its goals include both high availability and scalability. Its architec­ture 
is based on PC-class servers: one controller (a Tiger) can direct a number of disk servers (Tiger Cubs) 
to deliver media from disk into the network. We do not have enough information about Tiger to compare 
their mechanisms for achieving availability and scalability with ours. The overall structure of the Oracle 
Media Server appears to be similar to SG~s ITV system. Applications are split between a set­top component 
and a server side, and there are basic system ser­vices with functionality similar to our Connection 
Manager, Media Delivery Service, Reliable Download Service, and authentication service. Mediaobjects 
are integrated into a general-purpose data­base, and Laursen et al. indicate that the server side of 
applications can be built with database tools. Communication between client and server uses Lightweight 
RPC, with interfaces defined in an interface definition language. Like our system, the Oracle Media Server 
supports availability and scalability through replication, but the replication is structured somewhat 
differently. A service consists of one or more cooperat­ ing threads, which may be distributed across 
several machines. Rather than binding a client to a service replica at first use, as we do using the 
name service, the Oracle Media Server routes each request~o a service control point, which forwards it 
to a replica based on current load. Service control points themselves may need to be replicated for scalability 
and availability. A service manager, much like our Server Service Controller, detects software failures 
and restarts failed processes. Thus the overall structure of Oracle services has much in common with 
ours. However, we do not have enough information about the underlying mechanisms to know how they compare 
to ours. 11 Conclusions Interactive TV is an example of a distributed system with the demanding requirement 
that services be available to respond quickly to user input. This requirement applies to applications 
as well as the system, so it is important to provide an easy mecha­nism for service availability and 
scalability. Our experience with the mechanisms described in this paper has shown that it is possible, 
in a relatively short time, to construct a complex system that meets the availability and scalability 
needs of interactive television. We believe that the use of distributed object technology and a flexible, 
powerful name service allowed us to construct this system so quickly. We have found the replicated context 
and selector concepts very useful in service replication. The use of replicated contexts has allowed 
us to hide replication from client programs and made it simple for service providers to export replicas. 
However, we have not exploited the full power of replicated contexts; our selectors to date are quite 
simple. We believe that replicated contexts and selectors can be used to implement a variety of dynamic 
load bal­ancing policies. We plan to experiment with more powerful selec­tors in the future. We are very 
happy with the system that we have built. It is highly available, and we expect it to scale well as the 
number of settops increases. 12 Acknowledgements The construction of the interactive television system 
involved many people. The design and construction of the Object Commu­nication System involved several 
people in addition to the authors. Sathis Menon is responsible for building the Server Service Con­troller 
and the Cluster Service Controller; in addition Sathis worked on the OCS runtime. Anil Gangolli is responsible 
for all facets of security in our system. He built the authentication ser­vice, implemented all of the 
security runtime that is part of OCS. and implemented other parts of the OCS run time. Anil also was 
deeply involved in figuring out how to boot settops securely. We only touched on a subset of the services 
that makeup our ITV system. For the services discussed in this paper, Paul Close is responsible for the 
MMS, Mike Abbot is responsible for the MDS, and Tom Speeter is responsible for the Connection Manager. 
There are numerous other people, too many to mention here, that have contributed to the design of this 
system and built other crucial components. 13 References <RefA>[1] J. Mitchell, et al., An Overview of the 
Spring System . Pro­ceedings of Spring 1994 Compcon, Feb. 1994. [2] Y. H. Chang et al., An Open-Systems 
Approach to Video on Demand, IEEE Communications, Vol. 32, No, 5, May, 1994, pp. 68-80. [3] J. Diaz-Gonzales. 
Broadband Network Computing , in M. Hodges and IL Sasnett (eds.), Multimedia Computing, Addi­son-Wesley, 
1993, pp. 251-266. [4] W. Hodge, S. Martin, and J. T. Powers, Jr., Video on Demand: Architectures, Systems, 
and Applications, SMPTE Journal, Vol. 102, No. 9, Sept. 1993, pp 791-803. [5] T. D. C. Little and D Venkatesh. 
Prospects for Interactive Video-on-Demand, IEEE h4ultiMedia, Vol. 1, No. 3. Fall 1994, pp14-24. [6] G. 
Miller, G. Barber, and M. Gilliland, News on Demand for Multimedia Networks, Proc. ACM Multimedia 93. 
ACM Press, New York, 1993, pp. 383-392. [7] C. Mercer, S. Savage, and H. Tokuda, Processor Capacity Reserves: 
Operating System Support for Multimedia Appli­cations, Intl Conj on Multimedia Computing and Systems. 
Boston, MA, May, 1994, pp 90-99. [8] Object Management Group (OMG), The Common Object Request Broker: 
Architecture and Specification, OMG Doc­ument Number 91.12.1. Rev. 1.1, 10 December 1991. [9] J. Steiner, 
C. Neuman, J. Schiller. Kerberos: An Authentica­tion System for Open Network Systems, Proceedings of 
the USENIX Winter Conference, Jan. 1988, pp. 191-202. [10] S. Radia, M. Nelson, and M. Powell, The Spring 
Name Ser­vice, Proceedings of the 14th ICDCS, June 1994. [11] T. Mann, A. Hisgen, and G. Swart, An Algorithm 
for Data Replication, DEC Systems Reesearch Center Research Report 46, 1989. [12] M. Baker, J. Hartman, 
M. Kupfer, K. Shirriff, and J. Ouster­hout, Measurements of a Distributed File System, Proceed­ings of 
13th SOSP, 1991, pp. 198-212. [13] B. Liskov, S. Ghemawat, R. Gruber, P. Johnson, L. Shrira, and M. Williams, 
Replication in the Harp File System , Pro­ceedings of 13thSOSP,91, pp. 226-238. [14] A. Borg, W. Blau, 
W. Graetsch, F. Hermann, and W. Oberle., Fault-tolerance under Unix; ACM Trans. on Computer Sys­tems, 
Vol 5, No. 1 pp 1-24. [15] F. Schneider, Implementing fault-tolerant services usingtbe state machine 
approach: a tutorial, A CM Computing Sur­veys, Vol. 22, No. 4, Dec. 90, pp 299-319. [16] F. Cristian, 
Understanding Fault-Tolerant Distributed Sys­tems , Comm of the ACM, Vol. 34, no. 2, Feb. 91, pp. 57-78. 
[17] K. Birman. The process group approach to reliable distrib­uted computing , Comm. of the ACM, V0136, 
no. 12 Dec., 93, pp 37-53. [18] R. Sandberg, D. Goldberg, S. Kleiman, D. Walsh, B. Lyon, The Design and 
Implementation of the Sun Network File System, Proc. Usenix Conf. Portland Or, 1985. [19] D. Cheriton, 
D. and W. Zwaenepoel, Distributed Process Groups in the V Kernel , ACM Trans. on Computer Systems, Vol. 
3, No. 2, May. 85, pp. 77-107. [20] B. Oki, M. Pfluegl, A. Siegel, and D. Skeen, The Informa­tion Bus 
--an Architecture for Extensible Distributed Sys­tems , Proceedings of the 14th SOSP, pp 58-68. [21] 
M. Rozier, V. Abrossimov, F. Armand, I. Boule, M. Gien, M. Guillemot, F herrman, C. Kaiser, S. Langlois, 
P. Leonard, and W. Neuhauserl, Chorus Distributed Operating Systems, Computing Systems Journal, Vol 1. 
No. 4, pp 305-370. [22] Bill Bolosky, Robert Fitzgerald, Steve Levi, Rick Rashid, Microsoft s Tiger Media 
Server , presentation at the Net­works of Workstations Workshop, ASPLOS. 1995. [23] Andrew, Laursen, 
Jeffrey Olkin, Mark Porter. Oracle Media Server Providing Consumer Based Interactive Access to Multimedia 
Data. SIGMOD 94, pp 470-477.</RefA>  
			
