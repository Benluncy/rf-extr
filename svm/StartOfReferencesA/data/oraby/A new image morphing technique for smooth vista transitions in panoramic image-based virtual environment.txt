
 A New Image Morphing Technique for Smooth Vista Transitions in Panoramic Image-Based Virtual Environment 
 Cheng-Chin Chiang Der-Lor Way Jun-Wei Shieh Li-Sheng Shen Advanced Technology Center Computer and Communication 
Research Laboratories Industrial Technology Research Institute Chutung, Hsinchu, Taiwan 310, R. 0. C. 
 Email: ccchiang@atc.ccl.itri.org.tw 1. ABSTRACT This paper presents a novel method for achieving smooth 
vista transitions in panoramic image-based virtual worlds. The proposed method incorporates both epipolar 
geometry analysis, image flow analysis and image morphing technique to get satisfactory visualization 
in vista transitions. With the proposed method, the interactivity of panoramic image-based virtual reality 
applications can be significantly enhanced since it enables the emulation of free navigation among neighboring 
panoramas. 1.1 Keywords Epipolar Geometry, Morphing, Panoramic Image-based VR. 2. INTRODUCTION In recent 
years, the panoramic image-based virtual reality (VR) technology is an alternative way to provide a user 
an immerse visual-rich virtual world. Many related commercial products, such as QuickTime VR [l], Real 
Traveler [2], Surround Video [3],. . ., etc, have been released for many interesting applications. In 
comparison with conventional geometry-based virtual reality technologies, the approach of panoramic image-based 
virtual reality has the following advantages: (1) Hardware cost is affordable. An affordable video/graphics 
card is powerful enough since there are no complex 3D graphics operations for rendering images. Thus, 
applications can be built on low-cost personal computers. (2) The rendering work is independent of the 
scene complexity. Now matter how complicate the virtual worlds are, panoramic image-based virtual  Permission 
to make digital or hard copies of all or part of this work for personal or classroom use is granted without 
fee provided that copies are not made or distributed for profit or commercial advantage, and that copies 
hear this notice and the full citation on the first page. To copy otherwise, to republish, to post on 
servers or to redistribute to lists, requires prior specific permission and/or a fee. 01998 ACM l-58113-019-8/ 
98/ 0011/%5.00 worlds comprise a fixed dimension of pixels. (3) The authoring work is very easy. A panoramic 
image-based virtual world does not require professional 3D model designers for content authoring. The 
major authoring works are the easy jobs of photographing and image stitching using stitching software. 
Although the panoramic image-based VR has many desired features, a major weak point of panoramic image-based 
VR is the lack of interactivity. For example, users cannot interact with objects within the panoramic 
image-based virtual world since no 3D information of objects is available. For another example, users 
are limited to fix their viewing position at the center of the panorama. Thus, users cannot have a free 
navigation within a panoramic image virtual world. To enable a user to interact with objects within an 
image-based virtual world, QuickTime VR proposed the approach to use the object movie technology [4], 
which utilizes multiple pictures of an object from different viewpoints, for approximating users free 
examination. Another way is to adopt a hybrid approach that superimposes 3D geometry-based interactive 
objects onto a panoramic scenery image background [5]. The above two methods do enhance the interactivity 
to some extent for the panoramic image-based virtual worlds. In achieving the free navigation among panoramic 
image virtual vistas, to obtain a smooth vista transition becomes a major problem. Simple switching among 
the vista scenes is the most straightforward way, which is too trivial to provide a satisfactory visual 
effect to users. A more acceptable solution is to perform continuous zoom-in operations of the source 
scene until the zoomed image becomes very similar to the target scene, and then perform the scene switching. 
Although this zooming operation alleviates the abrupt scene changes suffered in the simple scene switching 
method, the achieved visualization is still not acceptable for human eyes. As known by many people, the 
image morphing technology is a good solution to smooth the abrupt change caused by image transitions. 
In [6], Chiang et. al proposed a simple image-morphing method for smooth vista transition. They developed 
an algorithm to automatically locate two corresponding transition windows between the source and destination 
vistas for image morphing. This algorithm can estimate the TAIPEI, TAIWAN Nov. 2-5 1998 VRST 98 64 Figure 
1. Stitching of two individual images into a part of a panoramic image. (a) individual images; (b) cylindrical 
warping of individual images; and (c) stitched partial panoramic image. appropriate viewing orientations 
and zooming factors for these two vistas, by manually specifying a few corresponding points on source 
and destination images. Since the scene images look very similar for these two located corresponding 
transition windows, the morphing result is very smooth. However, one problem for this approach is, for 
those scenes with larger disparity (depth) differences in objects, the source image and destination image 
usually can not be registered very well, due to the motion parallax. As a result, the user may see the 
misalignment of image edges in the in-between image frames during the image morphing. In [7], Seitz and 
Dyer proposed the view morphing technique to synthesize interpolated views between two different viewing 
locations. Their method comprises three steps, i.e., pre-warping, morphing, and post-warping. A prominent 
feature of their technique is the prevention of ridiculous shape distortions, which are usually seen 
in the many existing image morphing methods [ 141. However, a problem is their proposed pre-warping procedure 
does not work for singular views. Two views are said to be singular if the optical center of a view is 
within the field of view (FOV) of the other view. Unfortunately, the singular views are general cases 
in vista transitions, because the direction of camera motion during a transition is usually parallel 
to the viewing direction. Hence, the view morphing can not be directly applied. This paper is to present 
a novel method for achieving smooth vista transitions in panoramic image-based virtual worlds. Basically, 
the proposed method is based on the combination of epipolar geometry analysis and image morphing technique. 
With the proposed method, the neighboring vistas can be smoothly transited without ridiculous shape distortions. 
The playback of generated in-between image frames looks very like the video obtained by moving a video 
camera from one vista to another. 3. PANORAMIC IMAGE-BASED VIRTUAL ENVIRONMENT The first commercially 
panoramic image-based virtual QuickTimeTM VR released from available environApple product ment is Computer, 
of the Inc [ 1,4]. In a panoramic image-based virtual world, users can pan and tilt their viewing angles 
toward any directions at the center of the vista. They also can zoom in (or out) the scene image viewed 
from any directions. Basically, stitching a set of consecutive pictures taken at an interested vista 
is usually the first step to make an image-based panorama. Several intelligent stitching methods have 
been proposed in a number of literatures [8,9]. Briefly speaking, the individual images of the scene 
are usually impossible to be stitched seamlessly due to the perspective effect of the camera. To remove 
the perspective effect of camera for a successful stitching, these images have to be reprojected onto 
a simple geometry, e.g., a cube, a cylinder, or a sphere. Hereafter, we will call the final image generated 
by the stitching process as panoramic image. On choosing the appropriate reprojection geometry, it is 
found that the cylinder shape is much easier to handle than the other two shapes, in terms of the difficulty 
for image acquisition. It is the reason why most products adopt the cylinder as the geometry of the panoramic 
images. Figure 1 shows an example of stitching two individual images into a part of a cylindrical panoramic 
image. Notice that the cylindrical reprojection would bend some straight lines into curved lines. Hereafter, 
we call this reprojection as image warping. Definitely, these curved lines are not what the users want 
to see when browsing within the panorama. Therefore, a panorama player is required to dewarp the panoramic 
image to generate a normal view. The dewarping process is to reproject a part of the panoramic image 
onto the view plane. Figure 2 shows the geometrical relation between the view plane and the cylindrical 
panoramic image. Let (u,v) be the coordinate of a pixel on panoramic image and (XJ) the coordinate of 
 82 VRST 98 TAIPEI, TAIWAN Nov. 2-5 1998 the mapped pixel on the view plane. For the convenience used. 
of descriptions, the following variables are used: j , j! radius of the cylinder; d: distance from the 
center of cylinder to center point of view plane; z: zooming factor (= d/J>; 8: panning angle (longitude, 
O~B127c); 4: tilting angle (latitude, -7cQ 57~). Using simple geometric analysis, the transformation 
between (u,v) and (XJ) is derived as follows: BW x u= +ftan- ), v=Js. 297 dcos$+ysin$ Eq. (1) \ \ JJ 
W (2) With the above two transformation equations, the panorama player can dewarp the panoramic image 
into normal views, according to the specified viewing angle and zooming factor. The real-time dewarping 
speed can be achieved by using some proposed accelerating methods [ lo].  4. SMOOTH VISTA TRANSITION 
As mentioned previously, an image-based virtual environment may comprise many panoramas. In this virtual 
environment, the designer has to define the navigable paths within the environment, by specifying links 
between neighboring panoramas. To set up the links for panoramas, the designer also has to determine 
the parameters for each vista transition link. The required parameters may include the name (of identifier) 
of the destination panorama, the moving direction, which is specified by the two viewing angles B (longitude) 
and 4 (latitude), and a zooming factor, if zooming operation is c\View Plane The major goal of this 
paper is to present another effective approach to achieve the smooth vista transitions. The detailed 
steps of the proposed method are presented in the following subsections. 4.1 Epipolar Geometry Analysis 
for View Alignment To emulating the navigation from source vista to destination vista, it s the first 
step to find the exact direction to align the two camera positions of source vista and destination vista. 
The accuracy of this view alignment significantly affects the results of the smooth vista transitions. 
Poor view alignment causes bad transition effects because the aligned source scene and the aligned destination 
scene may differ dramatically. In this case, there will be no good transition result even though a good 
image morphing technique is available. Generally, the manual alignments are usually not accurate enough. 
Furthermore, it is usually difficult for the designer s eyes to identify the correct direction between 
two vistas if other persons photograph the pictures. Hence, an accurate aligning method is necessary 
to achieve good results and to improve productivity. As shown in Figure 3, source vistas A and destination 
vista B are to be aligned. The view alignment process is to find the proper values for the two set of 
viewing angles ( BA, @,J and (~9~ $J in both vistas (0 parameters are not shown in the figure). No zooming 
factors are required since no zooming operations are used in the proposed method. In computer vision 
technology, the line connecting two different camera positions is closely related to the epipolar geometry, 
which is a very useful concept in stereo matching [ 131. The three key items, epipole, epipolar line, 
and fundamental matrix, for epipolar geometry analysis are very essential to the proposed smooth transition 
method. As shown in Figure 4, for each pair of images (I, and I,), there are two epipoles (E, and E,). 
The epipoles E, and E, are actually the intersections of the line (G, ) that connects the two optical 
centers (C, and C,) and the two images I, and I,, respectively. Notice that it is possible that there 
are no intersections between the line G, and the two images I, and I,. In this case, the epipoles are 
said to be at infmity. For the case of vista transition, since the line Figure 2. Relation between view 
plane and panoramic image during the dewarping of panoramic image. (a) Top view; (b) Side view. TAIPEI, 
TAIWAN Nov. 2-5 1998 VRST 98 Dewarped . . . . . . . . . . . . . . . . . . . Non-aligned Image Aligned(Destination) 
Image Figure 3. View alignment for two panoramic vistas (side view). G, defines exactly the moving direction, 
the two epipoles after view alignment should be at the center of the source and destination scene images. 
Therefore, locating the epipoles on the two vistas and then finding the corresponding viewing angles 
according to the two located epipoles are two major tasks for view alignment. Generally, the epipoles 
can be located by finding the fundamental matrix [12] with respect to the two images. The fundamental 
matrix relates the viewpoints of two images through a 3x3 matrix F. A well-known constrain, called epipolar 
constrain, is very important to the determination of fundamental matrix. The epipolar constrain states 
that, for a given point p, on image I,, its corresponding point p2 on the other image I, must lie on 
the epipolar line of p,. As shown in Figure 4, there are two epipolar lines E,p, and I, and I,, respectively. 
These projection of line PC, and I,, respectively. The physical E 2 p 2 on the two images two epipolar 
lines are the line PC, on image I, and meaning of the epipolar constrain is easy to understand because 
line p62 contain exactly the possible world space to project on point p, and image I, and I,, respectively. 
From the line PC, and points in the point p2 over perspective of projective geometry [ 131, the epipolar 
line G over image I, with respect to point p, is defined by Fop,, while the epipolar line G is define 
by FT*p,. Therefore, according to the epipolar constrain, the fundamental matrix F can be estimated by 
the following two steps: Step 1. Provide a number (say N) of matched point pairs over the two images. 
Step 2. Estimate the fundamental matrix F by minimize the following criterion E: wherePi.1 andPr.2 denotes 
the projective coordinates of the fh matched points on image I, and I,, respectively. In fact, E is the 
summation of the distances (&#38;pi,,, Fp,,J and &#38;pi,, FTpi,J) from the specified points to the corresponding 
epipolar lines. Thus, the minimization of E comes from the epipolar constrain. The matched point pairs 
can be obtained automatically if there is a good image registration method available. But, in the case 
of vista transition, the source and destination images are usually very difficult to register because 
of the obstacles of non-uniform image scaling and object occlusions. In fact, the handling of these two 
cases in image registration is still an open problem in the research of computer vision. For this reason, 
the manual operation is the current way to specify the matched point pairs over the two images. An interactive 
tool is necessary for the authoring work of achieving smooth vista transition. We will present an authoring 
tool later in this paper. It is easy to locate the two epipoles as long as the fundamental matrix F is 
available. The epipole on an Figure 4. Epipolar geometry analysis for two images. VRST 98 TAIPEI, TAIWAN 
Nov. 2-5 1998 image is exactly the intersection of any two epipolar lines on this image. In our implementation, 
we devise the following iterative procedure for locating the accurate positions of the two epipoles on 
the given source and destination vistas. Algorithm: View alignment for two panoramic vistas Step 1: Initialize 
the approximated viewing angles for the two vistas. Step 2: Generate the source image and destination 
image by dewarping the panoramic images from source vista and destination vista respectively, according 
to the initialized viewing angles. Step 3: Specify a set of matched point pairs (say P) over the source 
and destination images. Step 4: Map the coordinate in the view plane for each point in P to the coordinate 
in the panoramic images according to Eq. (l), and thus a new set of coordinates for the provided matched 
point pairs (say Q) is generated. Step 5: Perform the minimization process to locate the two epipoles 
(say E, and EJ on the two images, and return the value (say E> of the criterion defined in Eq. (3). Step 
6: Transform the coordinates of E, and E, in the view plane to coordinates (say EP, and EP,) in the panoramic 
images. Step 7: Calculate the new viewing angles (say O,, e,, +,, and &#38;) for source vista and destination 
vista according to the coordinates of EP, and EP,. Step 8: Align the two vistas with the new viewing 
angles and transform the coordinates of points in Q to coordinates on view plane according to Eq. (2), 
with respect to the newly estimated viewing angles. Step 9: Repeat Step 5 through Step 8 until the returned 
value of E in Step 5 stop decreasing, and then return the viewing angles for source vista and destination 
vista with the minimal value of E. Note that the dewarped source and destination images will change 
after we align the two vistas along each pair of newly located epipoles. Consequently, the coordinates 
of the matched point pairs on view plane will change, too. It is the reason why we use the above iterative 
approach to find better epipoles since it seeks to find the lowest value for criterion E. 4.2 Image Flow 
Calculation Using Control Lines After locating the epipoles and aligning the views, we have the two corresponding 
epipoles located at the centers of the two new dewarped images. We now have to perform a smoothing image 
morphing on these two images. In the following, we propose another morphing technique which incorporates 
the image flow analysis to determine the moving speed of each image pixel during image morphing. Determining 
the image flow for each pixel requires the dense matching between the source image and destination image. 
However, as mentioned previously, automatic and accurate registration of two images is currently not 
very practical to computers, particularly for the cases of non-uniform scaling and object occlusions. 
Hence, like the approach used in [14], the proposed method requires the user to specify some control 
lines on both images to be morphed. A pair of control lines can precisely identify the image flow of 
a line of corresponding pixels on both images. Meanwhile, the image flow of pixels, which are not on 
the control lines, can be inferred based on these control lines. In principle, when moving the camera 
along the direction defined by these two epipoles (image centers), all pixels on the source image should 
move radially outward at different speeds, with respect to the epipoles (image centers). The moving speed 
of each pixel depends on the depth of that pixel. The nearer the pixel is to the viewer, the faster the 
pixel moves. According to this preconception, we design the following morphing method for smooth vista 
transitions. The first step is to automatically infer the dense matches between all pixels on the source 
image and destination image according to the specified control lines. As shown in Figure 5, suppose that 
p is a pixel on the source image, whose coordinate is (xJ), let the two sets of specified normal control 
lines on the source and destination images be L, and L,, respectively. For the pixel p, we get a line 
by connecting E, and p. Let line Z, intersect line E I p at a point pp. If the point pP satisfies the 
following condition: pi is the intersecti on of 6 and d(p,,,p) = min d(p,,p) the i conrol line in L, 
,<,<A, and d(p,,E,) 6 d(p,E,). 1 1 where d() means the Euclidean distance function and M is the total 
number of control lines in L,, then we call Z, the predecessor line of p. Similarly, we also define a 
control line I, to be the successor line of p if the following condition holds: p, is the intersecti 
on of 6 and d(P,$,P) = min d(p,,p) the i hconrol line in L, 16,iAl ami d(pi,E,) 2 d(p,E,). where ps 
is the intersection of line E,p and line 1,. Supposing that the origin of the image is E,, and the coordinates 
of the endpoints of the predecessor control line I, are (x,, y,) and (x,, JJJ, then the coordinate &#38;, 
p,) of pPis derived as TAIPEI, TAIWAN Nov. 2-5 1998 VRST 98 x(x, -X*)Y, P, = X2Y + XY I -XlY -XY 2 X(Y, 
-YZJYZ P, = XZY + XY I -X,Y -XY 2 where (x,y) is the coordinate of pixel p. In a similar way, given 
the two endpoints (X,,Y,) and (X,,Y,) for the successor control line I,, then the coordinate (s, ,s,) 
of p, is derived as X(X, -x 2 )Y, s, = X,Y + XY, -X,y -XY, x(Y, -Y, )Y, sy = X,Y + xy, -x,y -XY, Now, 
before calculating the corresponding pixel q for pixel p on the destination image, we have to determine 
the two corresponding pixels, qP and q,, on the destination image for the intersection pixel pP and pS, 
respectively. Let the corresponding control line specified on the destination image for line 1, be L,, 
whose two endpoints are (a,, b,) and (a,6,). Then, we can infer the coordinate (qy, q,) of qP as if a 
, = a2. 4, = (P, -xl)(a2 -a,) + a aI otherwise. 1 : (x2 -x1) if 6, = 6, b, qy = (P, -y,)(bz -b,) + 
b otherwise. I (Y2 -Y,) Similarly, let L, be the corresponding line for l,, whose two endpoints are 
(A,, B,) and (A2, B,). Then, we can infer the coordinate (QX, Q,) of qS as r if A, = A, Q, = it -x,)(A, 
-A,) + A (x 2 _ x, ) , otherwise. if B, = B,. Q, = :i,-Y,)(B, -B,) + B I otherwise. (Y2 -Y,) With 
the calculated coordinates for qP and q,, we can infer the coordinate (u,6) of the corresponding pixel 
q for pixel p as follows: 0 if s, = p,, a = (x-P.NQ. -4,) + q otherwise. (s, -P,) x 0 if sy = p,, b 
= (Y -P,)(Q, -4,) + q otherwise. Y (SV -P,) There are two exceptional cases during the above inferring 
method. The first case occurs when no predecessor control line is found for a pixel p. In other words, 
we cannot find a control line to intersect the directional line 6 at a position which is closer to E, 
than the pixel p is. In this case, the pixel pP and qP does not exist. Then, the coordinate (u,6) of 
pixel q is inferred as follows: 0 if s, = 0, 0 if sy = 0, a= Xe, and b = Yi&#38; otherwise. otherwise. 
1 s.z SY I On the contrary, the other exceptional case occurs when no successor is found. Similarly, 
in this case, the coordinate (ab) of pixel q is inferred as follows: 0 if p, = 0, 0 if py = 0, a= Xq, 
and b= y4~ otherwise. otherwise. 1 PI PY I The proposed method requires the designers to specify the 
control lines to surround the epipole without any leak. With this requirement, we can ensure that each 
line originated from the epipole would intersect at least one control line. Thus, the case of finding 
no predecessor and successor control lines at the same time will not occur. With the inferred dense match 
for all pixels on the source image and destination, it is easy to calculate the flow speed for each pixel 
during morphing. Suppose that we want to generate (N+l ) frames during the morphing process, then the 
image flow in x and y directions (say v, and V~ ) can be calculated as follows: a-x =-, b-y v, = -and 
VY N N If a control line is very close to another control line, : : : i i : Figure 5. Inference of image 
flow using specified control lines. 86 VRST 98 TAIPEI, TAIWAN Nov. 2-5 1998 then the pixels between these 
two control lines become high-disparity pixels if the moving speeds of these two control lines differ 
very much. The occurrence of high- disparity pixels implies that some scene objects, displayed by these 
pixels, are going to be occluded or exposed during the image morphing. In such cases, these high-disparity 
pixels have to be labeled beforehand for special processing during image morphing. The following rule 
is used to label the high-disparity pixels. Let pPand pSbe respectively the intersection points of line 
E,p and the predecessor and successor control lines for pixel p. Then p is labeled as a high-disparity 
pixel if the following condition holds: 4P> p,)+d(p, P,)<T, where d() denotes the Euclidean distance 
function. T is a prescribed small constant in units of pixels (e.g. 3 pixels). The above condition states 
that if the two intersection points are very close, no matter what their moving speeds are, p is labeled 
as a high-disparity pixel. The reason why we do not consider the moving speeds is because even though 
the speeds differ not very much, it still can be well handled in our morphing method which is to be described 
later. 4.3 Image Morphing Based on Image Flow Once the image flow (v, v,) for each pixel gets available, 
the in-between image frames can be synthesized by combining two image sources. The first source is the 
pixels resampled from the source image and the other source is the pixels resampled from the destination 
image. We call former resampling as forward resampling, while the latter resampling backward resainpling. 
For forward resampling, suppose that N frames are demanded during the image morphing. Let p (i, j) denotes 
the pixel value at the ifh column and the j row for the th image frame obtained from forward resampling. 
Then the following equation holds p (i + v, (4 j), i + vY (4 j)) = ~ (4 j), where v,(u) and v,(i,j) 
denote respectively the horizontal and vertical image flow components of the pixel at the ilh column 
and the j row. In the forward resampling, it is possible that a lot of pixels get no propagated values 
from other pixels. In this case, a lot of holes would appear. However, in the backward resampling, the 
image is generated by squeezing the destination image, thus there would be no pixel holes formed. To 
handle the hole problem in the forward resampling, the following grid-based tilling method is used. For 
the image at the tth frame, each quadruplet of the four neighboring pixels p(x,y), p(x+l,y), p(x,y+l) 
and p(x+l,y+l) forms a 2x2 pixel grid. At the (t+l)lh frame, these four pixels flow to p(x+v,(x YxY+v,(x> 
Y)), p(x+ 1 +vr(x+ LY), y+v*b+ l,Y)h p(x+vX(x,y+l),y+l+vY(x,y+ 1)) and p(x+l+v,(x+l,y+l), y+l+v,(x+,l 
,y+l)), respectively. Then, the pixel value of p(x,y) is filled into the polygon defined by the above 
four new pixels. Using this filling method, the hole problem can be well solved. In addition to the hole 
problem, we still has to deal with the labeled high-disparity pixels. As mentioned before, the occurrence 
of these pixels implies that some scene objects are to be exposed or occluded. For the case of exposure, 
the exposed pixels should be invisible on the source images. Therefore, it is incorrect to use any visible 
pixel values on the source image to till these pixels to be exposed. Hence, in forward resampling, if 
any one of the four neighboring pixels is labeled as a high-disparity pixel, then this polygon is not 
filled. Although this would cause pixel holes in the forward resampling, however, these holes will be 
filled when combining the images from backward resampling because the invisible pixels on the source 
image should become visible on the destination image. Similarly, if the high-disparity pixels imply occlusion, 
then the filling work becomes redundant because the filled pixels value would be overwritten by some 
other visible pixel values. In conclusion, if any one of the four neighboring pixels is labeled as high-disparity 
pixel, then the above polygon tilling processing is not applied in forward resampling. Another important 
problem we have to deal with is the visibility problem. This problem is to decide the final outputted 
value when several pixels value propagated into the same pixel position. In conventional image morphing 
techniques, the outputted value would be the weighted sum of these propagated values. However, this is 
not correct for visibility processing since it would cause the blurring of image pixels. For the proposed 
morphing method, the visibility problem is very easy to solve due to our epipolar geometry analysis. 
In principle, a pixel with small depth would move faster than a pixel with large depth. In addition, 
for the image obtained from forward resampling, each pixel moves outward at different speeds, with respect 
to the epipole. Therefore, given the origin at the epipole, we can decide the visibility of a pixel by 
calculating the distance from a pixel to the epipole. Let p/(x, y) denotes the value for the pixel with 
coordinate (x,y) at the $ frame. Suppose that we have N pixels, Vi) for P (Xi> 1 I i I N, which propagate 
their values into the same pixel p"'(x, y) at the (t+I)*h frame of forward resampling. Then, the final 
value of p + (x, y) should be determined by the following rule: p (x,y) = p (~~,y~) where ,/m = m,in 
(dm).I<r<N For the backward resampling, the flow directions of pixels are opposite to the forward resampling. 
Therefore, the decision rule becomes P + (x,Y)= p (x,,y,)where &#38;%f= y.zzz (&#38;??I With the above 
visibility processing, the occlusion problem can be solved very well. After forward resampling and backward 
resampling, each final in-between image frame is synthesized by merging the two resampled images. The 
merging is a simple time-varying weighted summation on these two TAIPEI, TAIWAN Nov. 2-5 1998 VRST 98 
a7 images. The time-varying weighted summation is given below P + (TY)= (y)P;(x,y)+ip:(x,y) ifpf,(x,Y)isnotahole, 
( f&#38;Y) otherwise, where p; (x, y) and pi (x, y) denote the pair of corresponding pixels from forward 
resampling and backward resampling, and N is demanded number of in- between frames during the vista transition. 
With the above merging operation, the changes of pixel values during the vista transition are smoothed 
very well. 5. IMPLEMENTATION, EXPERIMENT RESULTS AND DISCUSSIONS To demonstrate the feasibility and performance 
of the proposed method for smooth vista transitions, we designed and implemented a user-friendly tool 
for authoring the vista transitions. This authoring tool opens two windows for loading the source and 
destination panoramic images and generating the dewatped images at real-time speed. Users can draw normal 
or hidden control lines on images using the mouse. The three main functions, i.e., view alignment_ image 
flow calculation, and in-between image generation can be accessed from the menu Therefore, users can 
interactively verify the results of the smooth vista transitions on line and modify their design if needed. 
In Figure 6(a), two unaligned vistas are loaded. Ten corresponding point pairs drawn by white dots are 
specified to locate the two epipoles. Figure 6(b) shows the two views after view alignment. The line 
passing through the image center on an image is the epipolar line with respect to the pixel pointed by 
the mouse on the other image. The epipole line would rotate when the mouse moves. It is very easy to 
verify the epipolar constrain through this tool by checking the mouse position (indicated by the small 
arrow) and its corresponding point passed by the epipolar line shown in Figure 6(b). In Figure 7, the 
control lines are specified on the aligned view images. Based on these control lines, the image flow 
is calculated for each pixel. Once the image flow gets available, the images from forward resampling 
and backward resampling (see Figure 8) are combined to generate the in-between images. Figure 9 shows 
the snapshots of several frames captured during the vista transition. The continuous playback of the 
generated in- between frames is just like what we see when moving a video camera from the source vista 
to the destination vista in real world. With the pre-calculated image flow, the current synthesizing 
speed of the in-between image frames is about 1 frame per second on Pentium 200, under no programming 
optimizations. The speed can be improved by using some optimization techniques, such as using the faster 
fixed-point (or integer) operations. However, the current preferred utilization of the proposed morphing 
technique is to make a video clip using the generated in- between image frames. When a user want to tmnsit 
from a vista to another, the corresponding video clip is played. In this way, the speed is no longer 
a problem and the visual effect of vista transition would be very good. 6. CONCLUDING REMARKS AND FUTURE 
WORKS (b) FIgme 6. View Alignment. (a) Source vista view and destination vista view before alignment; 
@) Source vista view and destination vista view after alignment. 88 VRST 98 TAIPEI, TAIWAN Nov. 2-5 1998 
This paper presents a novel image morphing method for smoothing the abrupt change of scenes during the 
vista transitions in panoramic image-based virtual environments. This method enables the emulation of 
free navigation within a panoramic image-based virtual environment. The visual effects of this method 
are very satisfactory according to our experiments. Therefore, with the proposed technique, we can develop 
a lot of potential applications which are more lively than others related products. In the future, we 
will focus on the following issues for this research topic: Reducing the human work by developing more 
intelligent algorithms for image processing and registration, such as automatic finding of corresponding 
points, and automatic control line identification. Studying the possible extension of the technique on 
the virtual environments which comprise both 3D graphics objects and panoramic images. Enhancing the 
functions of the authoring tool for providing an easy-to-use user interface to efficiently design image-based 
virtual reality contents.  7. ACKNOWLEDGEMENT This paper is a partial result of the project no. 3N22400 
conducted by the ITRI under sponsorship of the Minister of Economic Affairs, R.O.C. 8. REFERENCES <RefA>[l] 
QuickTime VRTM Apple Computer, Inc. http:llwww.apple.com/quicktime/qtvr [2] Real Traveler. Live Picture 
Inc. http://www.livepicture.com. [3] Surround Video. Black-Diamond Consulting Inc., http:llwww.bdiamond.com 
Shenchang Eric Chen. QuickTime VR: an image- [41 based approach to virtual environment navigation. Proceedings 
SIGGRAPH 95, pp.29-38, 1995. [51 Cheng-Chin Chiang et. al. PanoVR SDK---a software development kit for 
integrating photo-realistic panoramic images and 3D graphical objects into virtual worlds. Proceedings 
of VRST 97, pp. 147-154, 1997. PI Cheng-Chin Chiang et. al., A method for smooth node transition in panoramic 
image-based virtual worlds. 1997 Computer Graphics Workshop, pp 1-4, 1997. [71 Steven M. Seitz and Charles 
R. Dyer. View morphing, Proceedings SIGGRAPH 97, pp. 21-30, 1997. PI Richard Szeliski and Heung-Yeung 
Shum. Creating full view panoramic image mosaics and environment maps. Proceedings SIGGRAPH 97, pp. 251-258, 
1997. Richard Szeliski. Video mosaics for virtual [91 environments. IEEE Computer Graphics and Figure 
7. Specified control lines on source and destination vistas. (b)(a) Figure 8. Resample images during 
image morphing. (a) Forward resampling from source image; (b) Backward resampling from destination image. 
TAIPEI, TAIWAN Nov. 2-5 1998 VRST 98 Figure 9. Snapshots of in-between image frames generated for smooth 
vista transition using the proposed image morphing method. Applications, pp. 22-30, March 1996. [lOI 
Shenchang Eric Chen. Cylindrical to planar image mapping using scanline coherence. U.S. patent No. 5396,583, 
1995. illI Q.T. Luong and Olivier Faugeras. Self-calibration of a stereo rig from unknown camera motions 
and point correspondences. Research Report RR-2014, Inria, France. u21 Q. T. Luong and 0. D. Faugeras, 
The fundamental matrix: Theory, algorithms, and stability analysis. Intl, Journal of Computer Vision, 
17 (1) pp. 43-75, 1996. [I31 Olivier Faugeras. Three-Dimensional Computer Vision: A Geometric Viewpoint. 
The MIT Press, Cambridge, MA, 1993. iI41 T. Beiter and S. Neerly. Feature-based image metamorphosis. 
Proceedings SIGGRAPH 92, pp. 35-42, 1992.  90 VRST 98 TAIPEI, TAIWAN Nov. 2-5 1998</RefA>  
			
