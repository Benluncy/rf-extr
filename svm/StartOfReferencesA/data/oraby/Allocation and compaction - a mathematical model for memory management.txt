
 ALLOCATION AND COMPACTION -A MATHEMATICAL MODEL FOR MEMORY MANAGEMENT Dennis W. Ting Computer Science 
Department The Pennsylvania State University University Park, PA. 16802 ABSTRACT A mathematical model 
for the processes of al- location and compaction is developed. The behavior of the most commonly used 
compaction algorithm is studied under this model. The relationship between time and space and trade-offs 
between "conservative" and "non-conservative" compaction algorithms are also investigated. INTRODUCTION 
 One important problem in the area of what is generally known as "systems programming" is the problem 
of dynamic memory allocation: given a fixed size memory, how do we allocate parts of it to satisfy program 
demands. This problem is of criti- cal importance in the design of operating systems and some language 
processors because: i) ~in memory is one of the most expensive com- ponents in a computer system. Good 
performance of the entire system hinges to a very large extent on the efficient utilization of main 
memory. 2) In some memory configurations, many programs will be forced to abort for lack of a single 
free area of main memory sufficiently large. The con- figurations are not under the control of any single 
program. Hence a program may or may not complete successfully depending on wha@ other pro- grams do. 
This introduces some amount of non- determinism which is undesirable in a good computer system. The attitude 
of "if it doesn't work this time, submit it again and maybe it will work the next time" is certainly 
intolerable. Furthermore, a designer should be equally con- cerned with the speed of an allocation algorithm 
considering how often it is called upon to perform its function. Thus while a memory allocation strat- 
egy should conserve as much memory as possible, this should not be done at a great expense of speed. 
 Thus the dynamic memory allocation problem is central to operating systems design. A complement- ary 
problem is the compaction problem: given that memory is already badly fragmented, what steps can be taken 
to compact all the busy areas and consoli- date some (or all) of the free areas in memory? These two 
problems are complementary in that when we speak of dynamic memory management, we really mean both the 
allocation and, when necessary, compaction as well. We try to be as clever with allocation as possible 
to reduce memory fragmenta- tion, but when overflow (i.e., when memory is so fragmented that although 
there are enough free words in total for some request, none of the individual contiguous free areas is 
large enough to fit the request) does occur, we also want to do compaction in such a way that as few 
words as possible would have to be relocated. A few very well known allocation algorithms are the best-fit, 
the first-fit, and the buddy system. (We exclude the boundary-tag system since the system itself does 
not define an allocation algorithm). All three algorithms use some sort of linked lists which chain all 
of the free areas in memory together. The first-fit algorithm searches the linked-list sequentially and 
allocates the first free area which fits the current request. The best-fit algorithm finds the free area 
which "best" fits the current request in the sense that the size of the area used is closest to, and 
yet not smaller than, the size of the request. The buddy system breaks the memory up into blocks of 
sizes which are powers of 2. The 311 smallest such block which can fit the request is allocated. There 
are good general discussions of these allocation algorithms in Knuth [Kn]. On the subject of compaction, 
however, very little is known. In most (if not all) of the appli- cations of compaction, only one compaction 
algorithm is employed -namely the "conventional" compaction algorithm which "slides" all busy blocks 
to the low end of memory. On the subject of this particular compaction algorithm, most papers dealt with 
the de- tailed implementation techniques of this algorithm (e.g., Lang and Wegbreit [LW]). It is the 
purpose of this paper to investigate behaviors of compaction algorithms. Measure their performances, 
and study the trade-off between time and space. ALLOCATION The process of assigning an area of free 
stor- age to a request is called (storage) allocation. By allocating storage judiciously, overflow does 
not take place as often. In this paper, we view an allocation policy as a mapping from one memory state 
 (the one prior to satisfying the request) to another (the one after an area has been assigned). We further 
define the notion of a conservative alloca- tion as one which does not fragment memory unneces- sarily, 
and which will always use whatever free space is available, i.e., does not fail unnecessari- ly. Of the 
three allocation algorithms mentioned before, both the first-fit and best-fit algorithms are conservative, 
as they always succeed whenever there is in memory a free area which fits the re- quest. The buddy system, 
however, is not conserva- tive because when a request for 2 k+l words is pre- sented to the allocator, 
and there is a free area of size 2 k+l which consists of two subareas of size 2 k each, but they are 
not "buddies", the allocator will not succeed in allocating this area. It has been shown by Robson [Ro], 
however, that none of the three allocation algorithms mentioned above is the optimal allocation strategy 
in that even in the very restricted case where requests are made only for 1 or 2 words, there is an 
allocation strategy which uses less memory to satisfy all request se- quences without possibility of 
overflow than any of the three methods mentioned above. In fact, Robson showed that there is a request 
sequence which would cause any allocation to fail if the store used is only even one word fewer than 
that used in the opti- mal algorithm proposed by him. So aside from effi- ciency considerations, the 
allocation algorithms used do affect the amount of memory needed to prevent overflow. Any cost used must 
reflect the overhead time spent doing compaction. Depending on the im- plementation of the accompanying 
allocation al- gorithm, the cost may or may not reflect the time spent on "marking" the free space in 
memory. In any case, compaction involves relocating busy blocks in memory. The relocation in turn involves, 
on the one hand, the actual moving of busy blocks, and on the other hand, the modification of pointers 
to the re- located blocks. If the cost of the actual moving is dominant, we would conceivably want to 
minimize the number of memory words moved (with respect to the number of words requested). If the cost 
of modify- ing pointers dominates, however, we would then want to minimize the number of blocks relocated. 
Practi- cally, it is conceivable to limit the total number of busy memory words in order to prevent (or 
at least cut down on the number of) overflows. It is out of the question, however, to limit the number 
of jobs (or segments) in memory as a large number of very small jobs would under-commit memory. It is 
for this reason that we are unable to amrive at a cost function which would reflect to a closer ex- tent 
the cost of modifying pointer in the process of compaction. For the rest of this paper we there- fore 
define the cost of compaction to be the ratio of number of words moved to the number of words re- quested, 
(i.e., the size of the request which prompted the compaction). Fortunately, minimizing the number of 
words relocated would at the same time minimize the number of busy blocks relocated to some extent. 
COMPACTION The operation of consolidating some or all of the free areas into fewer but larger free areas 
is called compaction. Compaction involves the relo- cation of some of the busy areas. In this paper we 
view compaction also as a mapping from one memory state (before compaction) to another (after compac- 
tion). The net effect of a compaction is that some larger free area is created as a result. In order 
to evaluate a compaction algorithm, it is necessary to define a cost associated with each application 
 of compaction. In this paper, we also analyze com- paction algorithms in light of the cost function 
we just defined. Any compaction algorithm we propose in this paper will be in consideration of the 
cost. There is another criterion by which we must judge a compaction algorithm, namely the "instantaneous 
 cost". Over a relatively lengthy period of time (i.e., over a lot of requests), there may have been 
 only a few overflows, so the overall ratio of words moved to the number of words requested may be low. 
 However, there may have been quite a few words re- located when the few overflows did occur. Thus 
without some knowledge of the ratio: number of words moved to the number of words requested immediately 
 preceding the compaction, which we call the "instan- taneous cost", we never know how much work is 
in- volved in doing a given compaction. While devising compaction algorithms with low costs, we should 
keep in mind given any request for some number (say x) of words, and overflow occurs, the algorithm 
should not relocate too many busy words relative to x. A low instantaneous cost tells how much compaction 
has to be done at any time before the compaction takes place. Fortunately, these two measures (cost 
and instantaneous cost) are compatible in that a low instantaneous cost automatically implies a low 
 overall cost (although the converse is not true). NOTATION AND DEFINITIONS We view the entire memory 
as a vector V = (vi) of M components, each of which takes on non-nega- tive integer values, where M is 
the memory size. If v i=vi+ I= ... =v k=p >0, and vi_ l#p, Vk+ l#p, we say that the words i through 
k are occupied by (allocated to) the same request. We denote by .th v. = 0 that the j word is free. 
The choice of 3 p#0 in any instance is not important, as long as it serves to distinguish one request 
from another. We will use e to denote an arbitrary!allocation algorithm and g to denote an arbitrary 
compaction algorithm. We use B to denote some pair (~,g), which we will call a memory management scheme. 
We also denote by an integer q >0, a request for q words of memory, and by an integer q < 0, the re- 
lease of those words of memory i through k such that v i=vi+ I= ... =v k=-q. A request-release string 
Q is simply some (valid) concatenation of requests and releases. Given 8, V, and q, we let r(B,V,q) 
denote the instantaneous cost of compact- ing memory with algorithm 8, memory state V, and request 
q. Given B and Q, i.e., the algorithm and the entire request-release string, we let K(B,Q) (or alternatively 
K(~,g,Q)) denote the "overall" cost, i.e., the ratio of the number of words relocated throughout the 
entire sequence, of B working on Q. If Q is of infinite length, we use the limit as time approaches 
infinity instead. Finally we let K*(8), or alternatively K*(~,g) be the maximum K*(8) = max{K(B,Q)}. 
Q Definition i: A memory state V is an n-dimension- al vector which specifies the status (free or busy) 
 of each memory word, if the memory size is n. A zero entry in the i th component means that the i th 
 word is free. If v i=vi+ I= ... =v k=p#O then words i, i+l,...,k were all allocated to the same request. 
 Definition 2: An allocation algorithm ~ is a mapping ~: VxN÷Vu{~} such that, if q>0, e(V,q) =V' #~ 
for some V' with vi= ... =Vi+q_l=O but v' I' =p#v.3 for all j, and p#0. =.i ..--Vi+q_ For all other 
j such that j <i or j >i+q-l, v~ =v.. If, on the other hand, for all i such 3 3 that there exists a 
j, i~j ~i+q-l, vj #0, then ~(V,q)=~. If q<0 then ~(V,q) =V' for some V' with vii= 0 if v.1 =-q' and 
v i-' if -v i v.l #-q" Definition 3: A compaction algorithm g is a map- ping g: VxN÷V. g(V,q) =V' 
for some V' if and only if ~i such that v i=vi+ I= ... =v k# 0, where k_>i, ~c such that Vl+ c=v~.+l+c= 
... =V'k+c=Vi, and furthermore, ~j,Z such that V'.j= ... =V[= but not both V. = 0 and vz= 0. 3 Definition 
4: A compaction algorithm g is a "complete" compaction if g(V,q) =V' and v'°= 0, 1 ' #0, for some i, 
then Vj > i, vj #0. vi+ 1 In other words, a compaction g is a complete compaction if all the free areas 
are consolidated into one. Definition 5: Given allocation and compaction al- gorithms ~ and g, we define 
the "composition" B to be as follows: 8(V,q) = ~(V,q) if ~(V,q) # ~, and B(V,q) = ~(g(V,q),q) if ~(V,q)=~. 
 Definition 7: We extend the definitions of ~, g, and B from mappings of VXN÷V to VxN +÷V by ~(V,q) 
=~(V,q), and ~(V,Qq)=~(~(V,Q),q). Where there is no ambiguity, we use ~ to denote ~. ~ is extended similarly. 
 Since we measure the performance of a compac- tion algorithm in connection with the associated al- 
location algorithm, we shall parameterize the prob- lem by making some assumptions about the allocation 
algorithm, namely we assume that (i) the total number of busy words in memory at any time does not exceed 
m, and (ii) the size of each request does not exceed n. Given an allocation ~, a compaction g, and 
a specific request sequence Q, a certain number of words are relocated each time when g is called into 
action. We define the notion of "instantane- ous cost" which relates the amount of compaction to be done 
at any overflow relative to the size of the request causing the overflow here: Definition 8: We say 
a memory state V' is "reachable" from another memory state V by ~ if there exists a valid request sequence 
Q such that B(V,Q) = v'. number of words relocated Definition 9: The ratio: size of req. causing 
ovfl. at any particular compaction, denoted r(B,V,q), is called the "instantaneous cost" of B acting 
on re- quest q (when the compaction in question took place), at a state V reachable from O, the state 
where all of memory is free. Definition I0: The instantaneous cost of ~ acting on Q is defined to be 
r"(B,Q) =ma~{r($,q)} if Q is finite, and r*(B,Q) = lim sup{r~(~,q)} if Q is infinite, t-~o qgQ Definition 
ii: The uniform instantaneous cost of B is defined to be r'(B)=max{r*(B,Q)}. Q Thus the instantaneous 
cost r*(B,Q) tells us how much compaction may be involved for any word requested in the sequence Q, 
whereas the uniform instantaneous cost r'(B) gives an index to how well the pair (~,g) performs with 
respect to limiting the words relocated for any one word re- quested. Besides the uniform instantaneous 
cost, we need some overall measure of the performance of 8. This overall measure should reflect the 
percen- tage over-head time involved in compaction. Thus we define a cost function K() associated with 
each B and Q to be: Definition 12: K(B,Q) = lim number of words relocated t -~° number of words requested" 
 While the cost function K() measures the cost of doing allocation and compaction for Q using 8, we would 
like to have a measure of efficiency on B uniformly, i.e., over all Q. A natural choice for worst case 
analyses would of course be the worst possible Q. So we define the uniform cost K*(~) to be: Definition 
13: K*(B) = lim sup number of words relo- t -~° Q number of words re- cated quested" Finally, we want 
to single out those compaction strategies which preserve "locality" in some sense of the word, since, 
as experiments [CV] show that preserving locality is one of the most impor- tant considerations for efficient 
execution of pro- grams. Thus we define Definition 14: A compaction algorithm g is lo- cality-preserving 
if, for some state V, request q, g(V,q) =V' such that for all i, j with vi=v~#O , vj =Vk#0 , vi#v j. 
i<j implies ~<k. In other words, a locality-preserving compac- tion algorithm is one which "slides" 
busy blocks rather than "picking up" a busy block and places it somewhere else beyond its neighboring 
busy blocks. The conventional compaction algorithm is therefore a complete, locality-preserving compaction 
strategy. THE EFFICIENCY OF THE CONVENTIONAL COMPACTION We show a theorem here which states that the 
conventional compaction algorithm is grossly inef- ficient (in the worst case sense). Theorem i: K*(~,g*) 
> m-2 -3 for all conservative ~. Proof: (We shall use g* to denote the convention- al compaction). 
To establish the upper bound, it is sufficient to demonstrate a request sequence Q which has a cost 
K(e,g*,Q) of at least -Pm-~ for all conservative e. Recall that an allocation is conservative if it does 
not fragment memory unne- cessarily. Now consider the following request-re- lease sequence: i) Make 
m requests for one word each. 2) Release the two words on the two edges. 3) Make a request of 2 words. 
4) Release the two-word block requested in 3. 5) Make a request of one word. 6) Release the one word 
block on an edge not adja- cent to a "gap". 7) Repeat steps 3 through 6 indefinitely. It is obvious 
that after step 2 there are two free words which are not adjacent to each other. Fur- thermore, there 
are m-2 busy words between them. The two-word block requested in step 3 will thus cause an overflow and 
then the m-2 busy words must therefore be moved. The one word block requested in step 5 will fill one 
of the two empty spaces vacated by the release in step 4. After step 6, memory will look exactly the 
same as it did after step 2. The repetition will therefore force the cost to be (m-2)/3. Illustration 
in Figure 1 will help make this analysis clearer. after 1 I~ X X X X ............... X X X X X after 
2 [ X X X X ............... X X X X after 3 I~ X X X X ............... X X X Y Y after 4 IX x x x x 
............... x x x after 5 Ix x x x x ............... x x x x after 6 h x x x x ............... 
x x x x Figure 1 Corollary: r'(~,g*) = (m-2)/2. Proof: We merely observe that a block of 1 cannot 
cause an overflow, so it takes a request of size at least 2 to cause overflow, and thus the total num- 
ber of busy words in memory cannot exceed m-2. D MOTIVATION If we inspect the proof of Theorem 1 closely, 
we would discover that the way to introduce a high cost is for a request sequence to somehow place a 
single large block in the "middle" of memory. A small request would then force the relocation of the 
large block. If the allocator can keep large blocks on the edges, such relocation involving a lot of 
words would not be possible. Since it is impossible to define what a "large" block is, a viable solution 
would be therefore to keep busy blocks in such a way as to have the smallest blocks in the middle. Algorithm 
A: When a request for s words is pre- sented, search memory from both ends and find the first memory 
word to the "inside" of the last busy block whose size is at least s. Allocate the s words starting at 
the word chosen above going in- wards to the request. If any of the s words so allocated is busy, i.e., 
occupied by other (neces- sarily smaller than s) busy blocks, relocate them by making requests for their 
sizes recursively. Proposition 2: Algorithm A eventually stops with the request properly allocated. 
 Proof: When the request arrives, either there are s free words to the inside of the last busy block 
whose size is at least s or not. If there are, the request is satisfied with the s free words without 
further action. If there are not, just enough busy blocks are removed to make room. Since these removed 
blocks are each smaller than s, eventually all requests (generated by the removals) have to be satisfied. 
[] We now consider the cost of this algorithm. Given sufficiently large n, suppose all sizes from i 
to n are allowed and assume that sufficiently many busy blocks of sizes 1 through n-i occupy both ends 
of memory with the only free area in the middle. Suppose now that the next request is for a block of 
n words. Since for any i> 2, i-l~i, the request for n words will result in the removal of 2 blocks of 
size n-i each. The 2 blocks of size n-i will in turn cause the removal of 3 blocks of size n-2 which 
will finally cause the removal of blocks of size 2. Hence for the block of size n, a total of 2(n-l)+3(n-2)+...+(n-1)(2) 
words would have to be relocated. But 2 (n-l)+3 (n-2)+...+(n-l) (2) = (2n+3n+... + (n-i)(n)) -(2x l+3x 
2+... + (n-l) × (n-2)) n-i n-i  = ~ n'i-Z i(i-l) i=2 i=2 n-i n-i = (n+l) ~ i-~ i 2 i=2 i=2 n(n-l) 
i)-1 n(n 1  = (n+l) ( 2 ~ -~) (n-l) -1 315 3 2  = 23 n3-2n) -(n+l)-i n n 5n  -~ 2. 6 2 3 3  So 
a total proportional to n words are to be re- located. Although this compares favorably with any algorithm 
which has to move a total proportional to m, rather than n, words, it is not as desir- able as something 
proportional to, say, n. We in- vestigate this algorithm further with some restric- tions in hope to 
reduce the cost somewhat. Algorithm B: For a request of size s, where i-i c i, c <s ~ for some constant 
c, a request of i size c is generated instead, and the algorithm proceeds as defined in Algorithm A. 
 With this restriction (or rounding up), we see i  that we never have to remove more than c words i 
 for a request of size c We summarize the effect of this modification in the following theorem: Theorem 
3: r(~,g,q) ~q logcq Vq where (~,g) is the algorithm described in Algorithm B. Proof: At the overflow 
due to a request for q k words, where q= c , at most q words are removed at the current recursion. Since 
all blocks are of size c j for some J, it takes at most k recur- sions to reach blocks of size I. So 
at most qk words have to be moved. But qk=q logcq , and the theorem is proved. Corollary: K*(B) ~ logcn 
 Proof: This follows dizectly from the theorem and the definition of K(). D In any case, the performance 
of the algorithm is now bounded by a function of q, the current re- quest size, instead of m, the maximum 
number of words needed, as in the case of the conventional compaction. In the restrictive case, the instan- 
taneous cost is bounded from above by logcq , which is quite an improvement over the non-restrictive 
case for which the instantaneous cost is of order q2. The intuitive idea behind Algorithm B is that the 
work of compacting memory is "spread around" at almost every request so there will be no unexpected large-scale 
compaction due to small requests. It is well worth pointing out that the price of requiring each request 
size to be a power of some constant c is not altogether so unusual. After all, the buddy system requires 
rounding up to the next power of 2. A comment on the choice of c may be appro- priate here. It is evident 
that as c increases, logcn decreases. But with n fixed, the number of choices of permissible request 
sizes decreases ex- ponentially as c increases. Another way of looking at it is that the amount of "internal 
fragmentation" increases as c becomes large. Note that this algorithm employs a non-conser- vative allocation 
algorithm (because it calls the compactor even when there may be a free area large enough to hold the 
current request) as well as a non-locality-preserving compaction algorithm. (If the compactor preserves 
locality, much more than qlogcq words may have to be relocated.) Observe also that the function logcn 
increases as n in- creases, while the bound m-n applied for any com- paction algorithm. There must be 
a point beyond which the logcn bound is no longer applicable. The question is then this: Does this point 
actually coincide with m-n for some value of n? We notice, however, that we can "approach" the middle 
of memory from both edges. In other words, we can "choose the better side" in which to do our compaction. 
In this 1  case, the obvious bound would be ~(m-n) as opposed to m-n, for a savings of a factor of 
two. We observe that at the expense of generality, and possibly some running time, we can indeed achieve 
the goal of being able to predict the cost of compaction in advance of the compaction process. Furthermore, 
we notice that Algorithm B is not a locality preserving compaction algorithm. So the trade-off is not 
between time and space, but rather a choice between whether a "large" compaction or lots of "small" compactions 
is more desirable. We now investigate the trade-off between time and space, i.e., we try to minimize 
compaction efforts at the expense of more memory (with respect to m). When the management algorithm 
is given a re- quest for s words, and all free areas in memory are of sizes less than s, it is conceivable 
that we may get away with relocating just a few busy words to make enough room for the incoming request. 
Furthermore, it seems natural that the place to do the compaction is where busy blocks are the most 
sparse. We can thus give a preliminary algorithm this region, no more than s/(k-l) busy words are using 
these ideas: Algorithm C: Let s be the current request size, km be the memory size where k> i, ~ be 
any con- servative allocation algorithm, and V be the cur- rent memory state. If ~(V,s) = ~ then divide 
mem- ory into m(k-l)/s regions of size ks/(k-l) each. (So the total size is km). We consolidate all 
the free areas in the region with the least number of busy words and leave all the other regions intact. 
After this step we then call on ~ to do the allo- cation. We must now show that this allocation-compac- 
tion algorithm succeeds in allocating an area to the current request. Lemma 4: There is at least one 
region (as described in Algorithm C) with at least s free words. Proof: If the current request size 
is s, consis- tent with our assumption that there are at most m busy words in memory at any time, there 
can only be less than or equal to m-s busy words in memory at the time of the current request. Thus 
the overall density of busy words in memory is at most (m-s)/km. Since all of the m(k-l)/s regions are 
of equal size, namely ks/(k-l), there must be at least one such region with density at most (m-s)/km. 
So there are no more than m-s ks m-s s m s s < km k-i m k-i m k-I k-i busy words in this particular 
region. Hence there are at least ks s k-i k-i k-i s ( ~ )=s free words in the region. Since our algorithm 
consolidates all free areas (which total at least s words) in- to one, after the compaction there is 
a free area of size least s in this region. Since the allo- cation algorithm ~ is conservative, it will 
allocate (part of) this free area to the request, and thus completing its job successfully. D Corollary: 
There is a region (as described in Algorithm C) with at most s/(k-l) busy words. Proof: Since there 
is at least one region with at least s free words, and the size of the region is sk s sk/(k-l), so 
there are at most ~-f~-s = k-i busy words in the region. D Since all the relocation activities take 
place in relocated. So we have Theorem 5: K(~,g) ~r'(~,g) ~k--~l where ~ is any conservative allocation 
algorithm, g is the com- paction algorithm described in Algorithm C, and km is the memory size, where 
k > i. Proof: The theorem follows directly from Lemma 4 and the definitions of r*() and K(). D Note 
that if we choose k=2, then for any request of s, we need only to relocate no more than s words. In addition 
to the advantage of being a locality-preserving compaction strategy, the al- gorithm has the definite 
edge on costs. It not only is independent of m, but is a very nice linear function of the current request 
size only. To summarize the paper, we gave a mathematical model to the process of memory management 
and we studied how various desirable properties of memory management algorithms can be achieved. Note 
that the author is not of the opinion that Algorithms B and C should be implemented in every memory manager. 
Rather than proposing Algorithms B and C as useful algorithms per se, we merely used them to study the 
relationships among various parameters in the allo- cation--compaction process. REFERENCES <RefA>[Kn] Knuth, 
D. E., "The Art of Computer Program- ming", VI, Addision-Wesley, Reading, Mass., 1968. [LW] Lang, 
B. and Wegbreit, B., "Fast Compactifi- cation", Report No. 25-72, Center for Re- search in Computing 
Technology, Harvard University, Nov. 1971. [Ro] Robson, J. M., "An Estimate of the Store Size Necessary 
for Dynamic Storage Alloca- tion", JACM 3 (1971), pp. 416-423. [CV] Coffman, E. G., Jr. and Varian, 
L. C., "Further Experimental Data on the Behavior of Programs in a Paging Environment", CACM ii, 7 (July 
1968), pp. 471-474.</RefA>  
			
