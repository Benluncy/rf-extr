
 A Heuristic Multiprocessor Schedulin Al orithm for Creating Near-Optimal Schedules Using lvask ystem 
Graphs Farideh A. Samadzadeh and G. E. Hedrick Computer Science Department Oklahoma State University 
 Abstrsct Scheduling can be detined as the process of designing . procedure for aquenckrg . tot of dasirod 
activities that tie place over . set of obj=ts. This sequencing is based on the time constraints for 
the delivery of the results, or .vailability of the resources necessaryforperforminganaction.Thecurrentpaperis 
concerned with the scheduling of task systems on . computer with multiple identical processors, Precedence 
relationships between pairs of tasks impose constraints on the order in which the tasks can be scheduled 
on processors. We discuss heuristics and present srt .lgorithm for scheduling of task systems on multiprocessors 
with the objccxive of cresting schedules with near-optimal makespans. We define . makwpan or aobedule 
length to be the elapsed time between the time the tirst teak is scheduled and the time the last task 
is completed. Introduction Depending on the field of reseuoh, the term ;ckcdmling refera to different 
.ctivities, For example, in Operations Research, there is a distinction between squencing and scheduling 
[18]. The term sequencing refers to ordering of events without any reference to time constraints while 
scheduling involves specifying the exact start and finish times of operations. lo genersl, scheduling 
can he defined as the process of decigning . prooedure for squencing . set of desired .ctivities that 
txke place over . set of objects. This sequencing is baaed on the time constraints for the delivery of 
the results, or .vailability of the resourw necessary for performing .n action. Different taxonomics 
can be defined dspending on the type and the number of machines svailable .nd .lso based on the type 
of jobs that must be scheduled on the given machine(s). The questions that generslly mum bs answered 
are: 1) How many machines us available? 2) Arc the machines identical in their capabilities? Permission 
to copy without fee all or part of this material is granted provided that the copies ore not made or 
distributed for direct commercial adventage, the ACM copyright notica and the title of the publication 
and its date appear, and notice ia given that copying is by permission of the Association for Computing 
Machinery. To oopy otherwise, or to republish, raquiras a fee and/or specific permission. e 1992 ACM 
O-89791-502-X/92/0002/071 1...$1.50 3) Are there any precedence constnints among tha jobs that akt the 
processing squencc of these jobs? The collection of answers to -ch of these qusstions detinsa . particular 
configuration of a system. The first question is .imed at determining whether the system is . single-or 
multiplwrracbine system. The answer to the second question reveals the interrelationship between the 
different machines. For ex~mple, if the system is composed of machines with d@rent capsbiliti~, we are 
probably dealing with . flow-line production system where rach job pessibly will go through every single 
machine (e.g., assembly lines and pipelining). However, if the system is compcmd of machines with identical 
capabilities, then unit jobs probably wiU oompletc on the same machine that they us assigned to. In this 
situation, we are deding with a paralleI­machine problem aimed at increuing the system throughput if 
the unit jobs arc independent or increasing the execution speed of the same job if the unit jobs are 
related and uc in fact tasks constituting a job. The answer to the third question reveals the interrelationships 
.mong the jobs to be scheduled. [f there are no precedence rslstionships among the jobs, the squencing 
of the jobs is done with regard to the delivery constraints and due dates (if any). lo the .bsence of 
due date constraints, the overall performance of . schedule is masuted by .verage tumsmund time in the 
case of a single-machine problem, and minimum makqan (schedule length), .rid/or tumxround time, in the 
csse of parallel machine problems. Ptwxdencc relationships between pairs of jobs imposc constraints on 
the order in which the jobs cart be scheduled on rrtxchines. This problem is easier to deal with in . 
single-machine system than . multiple-machine environmezrt. Typical] y, the complexity of devising an 
optimal and even a reo.sonubfe schedule increases exponentially as the number of machines and the jobs 
increase. Further discussion about scheduling complexity is deferred to the next ssction. The discussion 
so far concentrated on the general problem of scheduling jobs on machines. This discussion could be qtplied 
to the problam of scheduling in many different fields such es joh­shop scheduling [5], project scheduling 
~], rrtass-trattsit 711 scheduling [1], .nd multiprocessor .nd task scheduling [3]. This WPCSil int~sted 
mainly in the scheduling of task systems on multiprocessors. However, . study of the problem of scheduling 
in other fields can be beneficial since there are many similarities between these problem domains. For 
example, bin packing algorithms .pplied to memory allocation end processor scheduling [8] [6] were modeled 
after . more general problem known as the cutting stock problem in Gperetions Research [9]. The current 
paper is concerned with the scheduling of task systems on a computer with multiple identical processors. 
The task systems are assumed to contain ptwedence cortstreints. We will discuss presenting heuristics 
for scheduling of tuk systems on multiprocessors with the objective of creating schedules with near-optimal 
makespans, We define a makeqatt or schedule length to be tho elapsed time between the time the lint teak 
is scheduled end the time the lest task ix completed. The organization of the remainder of this paper 
is es follows. The next Section discuss-the complexity of scheduling algorithms, NP-completeness of the 
problem of scheduling, and the need for heuristics for solving the scheduling problem. The definitions 
and the notation used throughout this paper are described in the subsequent section. The review of relevant 
literature is presented in the Section titled Background . The Section on Dependent Task Systems discusses 
the scheduling of dependent task systems end presents en algorithm developed as pert of this research. 
The conclusions and future work are presented in the final se4tion. Scheduling Problem and NP.Completeness 
Multiprocessors scheduling, in the context of this reseuch, cen be defined as the scheduling of a set 
of n tasks on p independent and identical processors. The execution squence of the tasks may be constrained 
by certain precedence relations. The objective is to devise en assignment of tasks to proceaeora, considering 
the precedence catstreints, such that the overall execution length of the task system is tninimiaed. 
Assignment .nd squencing of tasks on processors is referred to as . sc~dufc. The scheduling problem has 
. seemingly simple end straightforward solution in which every pouible input sequence must be examined. 
The solution will be the input sequence that optimixes the objective functiott (i.e, the shortest possible 
schedule length) while satisfying the constraints of the problem. Unfortunately, such en enumeration 
in seuch of en optimal schedule is not feasible in general since the computation time squired grows exponentially 
as n .nd p grow, where n is the cerdinelity of the input set end p is the number of .vailable ~. The 
dominant factor of the amplexity of the scheduling problem is the input sise n, the number of tasks to 
be scheduled. An .lgorithm is said to work in polynomial time if the complexity of the algorithm is . 
polynomial in n. If the complexity of aa .lgorithm is exponential in n, the algorithm is said to work 
in exponential time which results in classifying the elgorithm u an NP-complete .lgorithm, indicating 
that the .lgorithm may yield . solution in non-polynomial time, The general problem of schukulirtg squires 
computational time that grows exponentially with the number of tasks in the task system and thus is known 
to be NF-herd [21]. In light of the fact thatoptimal schedules cannot be found within a reasonable time 
frame, many research endevours concert$rete on finding near-optimal solutions in polynomial dtne. From 
among polynomial time solutions, those with relatively doww growth rate (as the input siz8 increases) 
sre rated to be superior to other cases. We are interested in developing scheduling algorithms that yield 
optimal solutions for . subset of the genezal problem domain end seasonable solutions for others. Notation 
and Debitlotts The notation end definitions used in this pepct are irttroduced ie this section. We denote 
each task by t,, lSi*, wbete n is the number of teaks in a task system. Such s task system can be u J 
The weight of task tti s represented 8 set, T= {11, ta,. f . denoted w (~i)l . non-negative integer which 
is normally the expected or estimated execution time of the task. A teak system% is assumed to have . 
squential execution time, denoted by T,, which is defined es: T,= ,~lw (L) Parallel execution time is 
represented as T,. We refer to T, xs the schedule length. We are interested in the speed-up, S~, of the 
task system %, when several processors are employed in the execution of the task system. Sped-up can 
be calculated u Svs t,lt,. Finally, we define . path through a graph to be the traversal of . squence 
of nods through the graph that starts at the 6ret node (the Iortrce node or source) and ends in the lest 
node of the grepk (the sink node or sink). We define the criticdpath in . graph u the path that has the 
largest cumulative weighh w (Q, of all the nodes, titvisited in . path traversal, We use TCto deoote 
the length (weight) of the critical path. It is eesy to observe that given a directed acyclic graph (DAG), 
T. -OPT, is the optirttal schedule length. That is, it is not possible that any scheduling algorithm 
, whether preemptive or non-preempdve, can yield a schedule shorter then T~, (Gther notation used in 
more tesgeted end specific discuuions will be introduced whese .ppropriate.)  Background Genetxl cherecteristict 
of scheduling schemes cetr be defined in terms of the following pmpcrties. A scheduling scheme is either 
exact or heuristic uses either a static or a dynamic .pproach, end is performed either on ~ preemptive 
or non-preemptive basis. Discussion of the choice of exact and heuristic .pproaches was presented in 
Section on Scheduling Ptmblem end NP­Completeness .There are argumentsin the literature supporting both 
static end dynamic approaches. There are two major arguments egsinst dynamic scheduling. The firet argument 
states that since compile time information and general topology of the task system graphs are not used 
in the dispatching of tasks, the selection of tesb may not led to the best possible choice. The second 
argument concetttrxtea on the run-time overhead of dispatching which can be quite high. These arguments 
lead to two more favorable choices. One choice is quasi-static scheduling in which compile time information 
is used to produce preliminary schedule, which is then adjusted through processor synchronization if 
the .ctual processing times vary from earlier extimates [12]. The other choice is .uto-scheduling where 
compile time information is used for sequencing of task executions [19]. The arguments against fully 
static scheduling are ptedicatcd on the fact that the compiler produccc schedules baaed on estimates 
.nd that rdistic schedules are not always possible except in cues where .ll nccessery information ix 
-dily .vailable .t the time of scheduling (e.g., systolic amys). A preemptive scheme generally yields 
better results than a non­preemptive scheme from . tkmetical point of view. A preemptive scbc.dule is 
more likely to yield an optimal schedule. Under this scheme, the idle time created between the scheduling 
of two tasks can be filled with mnning a portion of a ready task .nd therefore giving the scheduler a 
gruter degree of flexibility in the scheduling of the teaks. However, in practice, preemptive scheduling 
has its own disadvantages, namely, incurring the overh~d of context switching. McNaughton [16] was the 
first researcher to introduce parallel machine scheduling. He introduced . scheduling algorithm that 
performed . preemptive scheduling scheme on . set of jobs. He defined the minimum makespen for a set 
of preemptive jobs xx max(t,, ta, .... r., ~~~mli/m) which is known u the McNaughton lower bound. McNaughton 
introduced three performance criteria for parallel execution of jobs. He used the lower bound mentioned 
above as . meaure for scheduling of preemptive jobs. He refers to this type of scheduling as completion 
time bead ~cheduling (CTB). He showed that his algorithm produces optimal schdtles with a maximum of 
m-1 job preemptions, where m is the number of svailablc machines, The other two performance measures 
defined are due-date baaed (DDB) and flow-time baaed (FTB) measures. In the DDB performance masure, the 
total weighted tardiness of the jobs is the performance measure. 10 the FTB measure, the ducdatcs are 
equal to sero .nd therefore, the flow time of the jobs ix used es the perforrnencc measure. Most of the 
other approaches developed by reseuchcrs can be defined to belong to one of the three categories defhwl 
by McNaughton and discussed above. Because of the NP-completeness of the genetal problem of scheduling, 
most known optimal solutions owe their OP-IY to rigid conditions imposed on the problem. Three of the 
well­known scheduling algorithms yielding optimal solutions are due to Hu [13] , Coffmen and Graham [3], 
and Munta and Coffman [17]. All three of these algorithms are completion time beae!d .lgorithms. Hu s 
.lgorithm operates on . special class of graphs where the precedence relations define a directed singly-rooted 
tree in which (except for the root vertex that has . fen-out degree of acre) each vertex has a fen-out 
degree of exactly one. Additionally, the tasks (vertices) are astumed to have tmit or uniform exe4ution 
tima. The urgency 22210(the prlaity scheme defined for the selection of tasks) used in scheduling of 
teaks on the next .vailable processor causes . tesh that has no predecessors and lies on . path with 
the longest distance from the root vertex, to be selected. Hu s .lgorithm runs in O (n). Hugs method 
is referred to es the kighe~t level fist .pproach or the criticaf poth method. Coffman end Graham s algorithm 
operates on directed acyclic graphs (DAG) with general precedenu reletions. They employ Hu s highest 
level 6rst .pproach for selection of the tasks for scheduling. The Iirttitetiotts of this .lgorithm lie 
in the fact tha~ analogously to Hu s .lgotithrm the tasks must have unit execution times and that optitrtality 
of the schedule is guaranteed only with two processors. Coffmen .nd Gmham s .lgorithm has O (nz) complexity. 
Muntz end Coffrnan s .lgorithm USM. preemptive .pproach .nd operates on arbitrary task systems (e.g. 
independent tasks, tree graphs, end DAGs). Their algorithm rquires that tasks have cotmmenwwoble execution 
times. This algorithm produces an optimal schedule of n length I*P,= max ( max (w(t,)), llp X w (ti)). 
h xchcdul-the n IQ* 1-1 tasks with mn preemptions end runs in O (n2). This algorithm does not guarantee 
optimality when p>2. Them are numerous algorithms .nd approaches that are bead on Hu s basic idea of 
highest level first .pproach. Motivated by Hu s approach, Graham [10] introduced . new approach for task 
scheduling known es lIM .!ch#dulin#. Marty schedulirtg algorithms in the literature, which use urgency 
roles for selection or diapetohing of taskJ, w be categorized as list scheduling algorithms. In list 
scheduling, tasks are ordered in . list baaed on partial order. The assignment of tasks to procasom takes 
place by scanning the list .nd selecting task that setisfica the defined urgency rule for dispatching 
of tasks. Some typical urgency rules are largest processing time tirxt (l-pT), shortest ~~~ing time first 
(SIT), and greatest number of immediate successors first. Certain .nomalies and boundsrelated to list 
scheduling [10] are listed below, 1. flterc exist cases where no list scheduling scheme results in art 
optimal schedule for a given number of processors. 2. The reduction of task weights may result in inc~sed 
schedule  length, 3, Allocation of a larger number of processors may result in en ittcreued schulule 
length. 4. Switching the execution times of tasks in the sarttc graph (without changing the topology 
of the graph) may result in an /13 increase/decrease in the schedule length (e.g., refer to the schedules 
for Figure 1). 5, A decrease in the number of arcs in a graph (i.e., relaxing the proxdertce conshnittta) 
may red in an incrcuc/dccreue in the schedule length. It has been reported [10] that in worst caae list 
scheduling yields schedules that are twice as long u the optimal schedule. It was diacusacd cerlier that 
scheduling schemes are either completion time baaed (CTB), due-date baaed (DDE), or flow­time based (FTB). 
CTB performance moaaurw are diacuaaed in this paper, The reader u rcfetred to a stete-of-tbe-art review 
of machine scheduling by Cheng .nd Sin for DDE and ITB schemes [2]. Dependent Task Systems Consider the 
precedence graph of a teak system represented as G (V, E). The tasks in a task system are aaid to be 
independent of one another if E= ~. This paper concentrates on teak syatcm graphs in which E* ~. The 
arcs in such task system represent dcpcrrdcncica (precedence constraints) between pairs of teak Therefore, 
the scheduling of the tasks must be such that correct execution squence is guaranteed. Most heuristic 
achcduling schemes use . CTB list achaduling .pproach. Hu s labeling of tasks according to their distance 
from the mot is known as the critical path approach. The CP/MISF (Critical Path/Mosl Immediate Succcazora 
First) heuristic [15] combines the critical path approach and . n additional heuristic which determines 
the priority of the tasks in a layer baaed on the number of their immediate successors, Tasks with larger 
number of immediate successors have higher priority. This algorithm is claimed to have incorporated the 
beatheuristic available for scheduling of DAGs. Polychronopoulos [19] argua that tasks can be characterized 
as either aerial or parallel tasks (where serial tasks contain a single execution thread while parallel 
txaka may be composed of a number of independent tial tasks). Thcrcforc, using the number of immediate 
aucccasora might not be a realistic priority scheme, He proposes a parametrized heuristic for assignment 
of priorities to tasks in his algorithm using the following rules. He gives priorities to -tasks that 
lie on the critical path, -tasks with long execution times, -tasks with largest number of immediate successors, 
and -tasks with successors that have long proccaaing times. He then uaee these Nlca to define three parameters 
that can &#38; uad for pmportiortal .llocation of processors to teaks, Hia allocation scheme assigns 
. toeach aerial task.  single processor Any number of remaining processors arc distributed to parallel 
tasks bucd on their pritities. The flexibWy of Polychmnopoulos .lgorithm is due to the capability of 
altering the weight of the three parameters. By changing the relative weight of the parameters, the .lgorithm 
cart assign diiYercnt priority weights to the above stetcd rules, The Ranked Weight Method The MISF 
heuristic in Kaaahara and Narite a algorithm givea a higher priority to tasks with largest number of 
irnrnediat~ successors. Unless used in the context of Hu s algorithm, whm tasks have qua] proceaaing 
times, the disadvantage of the CP/MISF method is that the processing times of the teaks are not used 
as a factor for load balancing. The hcuriatic used in the .lgorithm presented in this paper takes into 
account she procczaisrg time squid by task and all its successors, For example, the processing time of 
the source node (assuming that the task aystcm graph starts with a single node with no prcdcceeaon) will 
be equal to the total execution time of the attire task ayatem. We will refer to this number as the ranked 
weighf of the task, Ready tasks will be scheduled in the descending order of their ranked weights. This 
seems to be a stronger heuristic than the MISF heuristic because it selects the teaks that consume a 
larger amount of proccesor time through their successor path(s). This heuristic performs at heat u well 
u the MISF heuristic because if task with large number of successors.lso haa a large ranked weigh~ it 
will be givers . higher priority. On the other hand, however large the number of successors of task U, 
if the ranked weight of this tuk is l-a than that of some other ready task, this teak will be given a 
lower priority, Task System Graplta .nd Scitedttle Bounds The algorithm presented in this paper has been 
dceigoed with precedence graphs in mind that stert with . airtgle root node (source) and end in a airtgle 
terminal node (sink). lo the absence of a single source and/or single sink, a dummy node with . processing 
time of zero can be added to the graph. Thus, the algorithm cm be .pplied to arbitrary precedence graph, 
including the tree graph considered in Hu s algorithm. A typical prcccdcnce graph considered in this 
rcacerch is shown in Figure 1.   A (1)tt (2) (3) (4)  Figure 1.A teaksystemprecedence graph. 1[is 
not possible to calculete the exact schedule lengths for non­precmptive scheduling schemm unless restrictive 
end sometimes unmxlistic constraints we plxced on the task md the mechine chxrecteristics. Calculation 
of the optimel schedule length is possible in preemptive schemes, Since the scheduling elgorithm developed 
in this seseerch uses . non-preemptive scheme, only bounds on the schedule length cen be d@erntinetf, 
Considering precedence gnphs with sin~le source .nd single sink (e.g., Figure 1),il is obvious that the 
source end the sink ttoda cettnot be executed in perxllel with any other node, Therefore, that portion 
of the tesk sys~ which might be emenable to pemllelizetion, inolud-those tasks tlmtam between the source 
end the sink, That is, perellel pmccssable time .t most is f, -(/,-+ J-). Therefotc, . lower bound for 
the execution of the parxllel proce.mable portion of such s txsk system can be de!hm es Ll=(l, -(t,-+ta))/P 
 However, coasideting . precedence greph such es the one in Figure 2, it cen be obmrved ttmt f.,1= 11, 
which is not achievable under . non-preemptive scheduling scheme. Thus, . second lower bound which is 
bssed on the critical petb length, T,, of txsk system cm be defined es follows L.2= T,-(1,-+ I*), Using 
these two bounds, en loose lower bound for the scheduling of the entire tesk system, LB, can be defined 
es: ~ =mex{L 1,L2)+ 1,-, + tti Thus the schedule length for the greph of Figure 2 is 28 xttd not 190 
Figure 2. A simple precedence greph. It should be noted that in the cese of en unlimited number of PSUCOSDOXS. 
os in cxscs where the number of aveileble procwors is greeter then or quel to the width of the greph, 
the schedule length would be quel to the critical petb length. JO tbu CXS% 1,= f.= t-end en optimxl schedule 
length would be xchieved. A Mttltlproceesor ScltedulJng Algorithm In this section, m infonnxl description 
of the proposed .lgorithm is presented end followed by sevcrel exemples end a formxl description. AS 
discussd eerlier, the lebeling introduced in HU S algorithm in which txsks em divided into levels eccording 
to their distence f%omthe root, is known u the Critical Path or Highest bvcl First method. In the cxse 
of generel precedettco gsephs, we will refsr to this qproech es the Latest Schedule Pestitiotting (UP). 
LSP produces lxyers of tesks .ccording to the letest time . tesk cut be scheduled ( see for example, 
[41). It is elso possible to pertition the tuk precedence grxph eccordhtg to . txsk s dietettce from 
the source. This will pmdttco leyers of texks .ccording to the euliew time . task CM be scheduled. We 
refer to this petitioning u Schedule Pxstitiotting @P). the EerlistThe pestitiotting used by the elgorithm 
presented in this section b besed on the ESP scheme. Algorlthm Jttput The elgorhhm tekcs es input two 
edjscency lists (defined below), the renkcd weight of exch task (informally defined in subsection The 
Ranked Weight Method .nd formxlly defined below), end the level xssocixted with each tesk. This infortttetion 
cxtt be extrxcted by a single pess through the gxaph, The lLSP/LSP algorithm [20] developed by the authors 
es pert of . scheduling elgorithm in en edier rescsrch, cxn be augmented with necessery ~et~ to supply 
the required input. The ESP/LSP elgorithm wes designed besed on ideas from topologicef sorting. h identifies 
those texks in a pmaetktce graph that can be executd in parallel. Eech set of such txsks ere deiitted 
to belong in the sente layer, A top-down .pplication of the ESP/LSP AIgorithm produces en ESP partitioning 
.nd a bottom-up .pplication yields en LSP partitioning of the txsk greph, One of the required input edjecency 
lists identifies the immediate SUCCUSOS(S)mid the other list identifies the immedietc predecessor(s) 
ofs tesk. The tutked weigh~ rw (4). of tuk 4 is defined to be the sum of the weights of XUsuccessor tesks 
of/, plus the weight of txxk t, defined es follows fw(f,)= W(ti)+ (Z W(fJ), for xl] II E [ successors 
o~(r, ) ) ). The renked weights xre used es an urgency criterion for scheduling of the tesks. The Ranked 
Weisbt Algorithm Istptttt Li~t-: The successor edjaccncy list. Listz: The predecessor xdjacency list. 
Q[k]: A multilevel list with k levels. fevel (ii) : The txxk system grxph level emociatcd with txsk (,. 
 715 output: An assignment of tasks to processors Method: Q[f]+ $, V lSfSk where k = numberof levels 
in the gmph. Q[l]+ Q[l] U (SOM~C~?dc), 1) Let 1= mitt{ level number, 1 ,such that Q [l? x~ ], t + hed 
of Q[/J, Schedule task ton the first, lowest indexed, cvailable processor, Q[l]-Q[l]-(t). 2) For all 
t G List-[t ] do f.-isl~[l~ 4-f.istp,~[ll -{/1. If Listp~ [/ j= $ then Insert t in the sorted list Q 
[ Icvef (J )] Wcording to rw (t ). 3)lfQ[l]=~, VISf Sk, then HALT, else 00 TO Step 1.   Algorithm Description 
The pral~aor adjacency list in this algorithm is used to identify the tasks that arc ready to be scltedulcd. 
A task is ready when its predecessor set is empty. The successor adjacency list is used to identify the 
successors(s) of a taak after its oomplction. The use of the successor adjacency list eliminates the 
search involved in identifying ready tasks. The urgency rule used in the aoheduling of tasks involves 
the selection of the ready task that is at the highest level (highest non-empty subliat) and has the 
highest mnked weight. Ties arc broken by selecting the teak that has the highest processing time. Ready 
tasks arc added to the sublist with a stiblist number that comcsponds to the task level, The algosithm 
opesata by threading the ready tasks (i.e. tasks whose predecessor sets are empty) to the appropriate 
subliat. The ready subliits are smted based ott the ranked weight of the tasks in each sublist. Use of 
different sublists cuts down on the cost of ordering the ranked weighta. After completion of . task, 
the task is deleted from the predecessor set of its immediate succcaaor(s). Elimination of predwcssors 
may leave some tasks with no prc-deccssors indicating that these tasks are ready to be scheduled. Tbcse 
operations are continued until all tasks are scheduled, The following example helps clarify the .lgorithm 
pnm.ntcd in this paper. Let us consider the precedence graph in Figure 1. Numbers in parcntheaeJ identify 
the level of the tasks. Table 1 shows the ranked weights ~(ti) and the expected processing times, W(tj) 
of ach task. Note that the tasks .ppear in sorted order based on the ranked weight of the tasks. . c 
bedhfgi nv(li)76 42 34 27 26 24 18 12 4 ~(~i) 4 16 5 3 2201484 Table 1, Ranked weight and expected processing 
timca of the tasks in Figure 1. The successor and predecessor adjacency lists of the graph itt Figure 
1 are shown in Figure 3. m m (a) m mm El El El mm mm  El El mm  m mm El+ m+m Figure 3. Successor 
list (a) and predecessor list (b). [nitially, task u is added to sublist one (the highest subliat). 
After schcdttling and completion of task a, the succcsaora of u arc identified through the successor 
list as tasks b and c, which serve as indices to the predw,xssor list. Task a is then deleted from the 
predecessor lists of tasks b and c. This laves tasks b and c with empty prcdwcasor lists and causes these 
tasks to ho plxced itt sublist two ss dy tasks, Notice that in the 0SS0 of . task witi multiple predeccssora, 
the completion and subsequent removal of, say, either task d or c alone from the predecessor Setoftukh 
does not oause the placement of task h into the ready list. The sohedule corresponding to the precedence 
graph of Figure 1 on two pmoessors ix shown in Figure4. - Figure4.AscheduleforthetasksystemofFigure 1. 
 The .bove schedule yields makespan of f,= 46. It should be mentioned that the dove algorithm does not 
guarantee optimality in all cases. However, it can yield optimel non­preemptive schcdulcs in some cases 
and some very close to optimal ~chedulcs in other cases. It is expleined in the lest section of this 
paper that stochastic evaluation of the performance of the algorithm presented in this paper is being 
performed, The simulationmodel under considerationis .imed at evaluatingthe performanceof this algorithm 
on a comparative basis with other comparable algorithms (e.g., CP/tvflSF [15]), The shottest non­preesnptive 
schedule length for the given graph is 45, with sequencing tasks (a, b, d, c, ~, i) on one processor 
end (e, h, g) on the other. The speed-up of the task system ~ in Figure 1 uncles the scheduling .lgorithm 
presented in this paper is S, .76/46 which is the ratio of the sequential execution time of the teak 
system and the schedule length pmduccd by this algorithm. The maximum possible speed-up S=-=76/45 which 
is the mtio of sequential execution length of the task system end the optimal schedule length of the 
teak system in Figure 1 on two processors undernon-preemptionpolicy. A modified version of the graph 
in Figure 1 results in a different schedule. The modification involves exchanging the processing times 
of tasks d and f, and those of tads e and g. The schedule of the modified version of Figure 1 is shown 
in Figure 5 with . schedule length of 52 which is indeed the optimal schedule for this particular tesk 
system graph under non-preemption policy. Figure 5, Schedule for modified version of the task system 
of Figure 1,  Conclusion and Future Work The general problem of scheduling and its complexity was discumed. 
Different approaches and schemes elong with performance measures for scheduling of task system graphs 
on multiprocessors were analyzed. A heuristic elgorithm developed for scheduling of task system graphs 
with precedence relations was presented and discussed. Even though our experience in testing the performance 
of this algorithm is limited to . number of task system gnphs, the tesults have been very promising. 
h was described how the ranked weight heuristic presented in this papa, is at lcest as strong as that 
of the CP/MISF by Kasahara .nd Narite, which is the best known heuristic. Our future .ttempts wiIl concentrate 
on producing simulation model that will take specific as well es stochastically created precedence graphs 
and evaluate the generated schedules. It is also the future goal of this reseuch to elaborete on end 
re6ne the .pplicability of this algorithm to ditkrettt scheduling strategies, namely, dynxmic (run-time) 
versus stxtic (compile time) scheduling. The .lgorithm presented has the potential to adapt to either 
of these strategies or their variations where, for example, compile time information can ISOused to produce 
etYicientrun-time schedules. Additionally, since the teak weights are generelly assumed to be compil~time 
estimates or expected execution times, it is also our intention to present in the fuhuc strategies that 
cars be used for processor syttchronixdon in situations where the ectuel execution times ero dilferent 
from tho estimated execution t-so that rusottable mittitttiaation of the schedule length can be gttaresttecd. 
References <RefA>[1] L. Bodia, Routing .nd Scheduling of Vebiclu end Crews, Computer; and Operations Research, 
Vol. 10, No. 2,1983. [2] T. C. E. Cheng end C. C. S. Sin, A State-of-the-Art Review of Penllel-Machine 
Scheduling Reaurch,-Esuopean Journal of Operational Research, Vol 47, Elsevier Science publishers, North-Hollestd, 
1990, pp. 217-292. [3] E.G. Coffmen, .nd R. L. Graham, Optimal Scheduling for Two-Processor Systems, 
Acts Informatica, Vol. 1, No. 3, 1972, W. 200-213. [4] E. G. Coffmatt end P. J. Denning, Operating SysfewIS 
Theory, Prentic&#38;HaIl Publishing Co., N. J., 1973.  [5] E. G. Coffman, Computers and Job-Shop Scheduling 
Theory, John Wileyand Sons, N. Y, H, 1976. [6] E.G. Co5man,Jr.,M.R.Gerey,endD.S.Johnson, An Application 
of Bitt-Packing to MUM-SOY Scheduling, PI E. W. Davis, Project Scheduling uttdm Rcumrce Constraints -Historical 
Review end Categorization of ProcedurcsS AIIE Tranwctions, Vol. 5, No. 4, 1973, pp. 297-313. [8] M. A. 
Franklin, G. G. Graham, end R. K. GUpW Anomalies with Variable Partition Paging Algorithms, Communic~ions 
of &#38; ACM, Vol. 21, No. 3,1978, pp. 232-236. [9] P. C. Gilmore and R. E. Gomory A LitseaY Progmtnmittg 
Approach to the Cutting Stock Programs, Operations Research, Vol. 9, 1%1, pp. 849-859. 717 [10] R. L. 
Graham, Bounds on Multiprocesaor Timing Anomalies, SIAM Journal of Applied Mathematics, Voi. 17, No. 
2, Mar. 1969, pp. 416-429. [11] R. L. Graham, Bounds on the Performance of Scheduling Algorithms, Computer 
and JobtShop Scheduling Theory, (E. G. Coffman, Ed.), John Wiley, New York, 1976. [12] S. Ha and E. A. 
Lee, Compil*Time Schedulingand Assignment of Data-Flow Program Graphs with Data-Dependent Iterations, 
IEEE Transactions on Computers, Vol. 40, No. 11, Nov. 1991, pp. 1225-1238. [13] T. C. Hu, Parallel Squencing 
and Assembly Line Problems, Operations Research, Vol. 9, Nov.-Dec. 1961, pp. 841-848. [14] D. S. Johnson, 
A. Demera, J. D. Ullmatt, M. R. G=Y, and R. L. Graham, Worst-Caae Performance Bounds for Simple One-Dimensional 
Packing Algorithms, SIAM Journal of Computing, Vol. 3, No. 4, Dec. 1974, pp. 299-326. [15] H. Kasahatn 
and S. Narita, Practical Multiprocessor Scheduling Algorithms for Efficient Parallel Processing, IEEE 
Transactions on Computers, Vol. C-33, No. 11, NOV.1984, pp. 1023-1029. [16] R, McNaughton, Scheduling 
with l)sadhn~ attd Loss Functions Management Science, Vol. 6, Jan. 1959, pp. 1-12. [171 R. R. Mum and 
E. G. Coffrnan, Optimal Multiprocsasor Scheduling on Two-Processor Systemso IEEE Transactions on Comptiers, 
Vol. C­18, No. 11, NOV. 1969, pp. 1014-1020. [18] S. J. Noronhs and V. V. S. Satma, Knowledg~Bascd Approach 
for Scheduling problems: A Survey, IEEE Transactions on Knowledge and Data Engineering, Vol. 3, No. 2, 
June. 1991, pp. 160-171. [19] C. Polychronopoulos, Parallel Programming and Compifers, Kluwcr Academic 
Publishing, Norwel, MA, 1988. [20] F. A. Samadzadeh and G. E. Hedrick, Neu-Optimal Mul~iproccssor Scheduling, 
to .ppcu in the Proceedings of the 1992 Computer Science Conference, Kansss City, MO, March 1992. [21] 
S. D. Ulhnan, Complexity of Squsacing Problems: in Computers and Job-Shop Scheduling, E. G. Coffman, 
Ed., John Wiley and Sons, N. Y. 1967.</RefA> 718  
			
