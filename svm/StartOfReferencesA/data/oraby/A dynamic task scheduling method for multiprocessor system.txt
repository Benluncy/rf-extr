
 A DYNAMIC TASK SCHEDULING METHOD FOR MULTIPROCESSOR SYSTEM Ming-fang Wang University of Central Arkansas 
Abstract Eecauseof itspracticalworking environment requirement, dynamic task scheduling methods meet 
a wide range of applications for task scheduling on a multiprocessor system. However, extra load balancing 
messages of these methods may seriously affect the system @ormance. Round Robin dynamic task scheduling 
methcds is an extension of tbe traditional RR task scheduling method. It does not produce any load balancing 
messages. Active tasks ars executed by different PEa in a rotation. Such rotational execution mmida the 
generation of any idle PEa. The RR method does not require the task migration either. The rotational 
task execution make tasks available to all the PE?a.These features of RR scheduling method can drastically 
reduce the extra burden often incurred by other dynamic task scheduling methcxls while adjusting tbe 
PE loads. lntrodrrction High throughput of a multiprocessor system can be achieved only if processing 
elements, (PE), of the multiprocessor systemhave been kept busy all the time. In other words, the work 
load of the PEs must be balanced. This is the goal of task scheduling in a multiprocessor environment. 
There are twu major groups of scheduling methods for multiprocessor system task scheduling static task 
scheduling methods and dynamic task scheduling methods[l]. Static task scheduling methods prcduce task 
schedules before actual task executions. It doea not incur any extra scheduling work during the run time. 
Static task scheduling methods have limited applications, however, due to the restriction that the run 
time behavior of the tasks must be available for prcxiucing schedules[21[3][4]. Dynamic task scheduling 
methods performs (he scheduling without using the task execution time information. The execution times 
of the tasks can be unknown. This assumption fits into a very wide range of applications[l] [5]. Different 
from static task scheduling methods, the dynamic scheduling methods adjust the load of the PEs during 
the task execution. Thus, the dynamic task scheduling methods are mom flexible and versatile. Many dynamic 
task scheduling methods impcme a scheme that during the task execution, heavily loaded PEa should send 
the tasks to those lightly loaded Pl?s. This is also called task migration. To achieve this, every PE 
should have a global view of tbe work load situation so that tasks can be sent in a right dimtion. Extra 
message is used to carry the load balancing information from PE to PE. An idle PE may send out messages 
to other Pm to solicit work. An heavily loaded PE may send Permission to oopy without fee all or part 
of this materiel is granted provided that the copies are not made or distributed for direct commercial 
advantage, the ACM copyright notice and the title of the publication and its date appear, and notice 
is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, 
or to republish, requires a fee and/or specific permission. e 1992 ACM 0.a9791.50z.X/9 z/OOOz/Oa~..,$l 
.50 out messages to other PEa to find out the PEa that it ean share the work with [S][6][~[8]. Obviously, 
the more such messages are sent, the better global view each PE can get. However, those mesaagea compete 
the communication resources with the regular data transfer messages which are key factors for task execution. 
If there at s tm many load balancing messages, the interconnection network can be congested, causing 
the degradation of s@em perforntanee. MesaagGpassing load balancing methcxis also incur the synchronization 
problem. If a PE has completed the execution of a task, in order to select the next teak to run, the 
PE must be able to select a new task that wnot running on any other PE [5]. Otherwise, many PEa may run 
the same task. To achieve synchronized task execution some expensive measures such es execution role-back 
should be adopted. The system burden will further increased. With the same assumptions, we introduce 
our dynamic task scheduling method which extends the Round Robin (RR concept or RR policy) task scheduling 
method on singfe CPU computers. The RR Poficy. When we extend the exaution environment from a single 
CPU computer system to a multiprocessor computer system, the basic principle of the round robin execution 
policy-that every task receives the same amount of execution time-is presetved. So the fairness is achieved. 
The time period is defined by  LcM(n,~) ~Q. (1) m Here, n is the number of concurrent tasks and m the 
numbsr of PE s LCM(n, m) is the function whiih providea the least eomrnon multiple of n and m. Ifm= 1, 
there is only one P@ LCM{n, m) . * and the time period beeomes m n x ~~ . So, the aingfe PE round robin 
exeeution concept is a special case for the generalized multiproeeaaor round robin exeeution concept. 
Let tO,tl, .... t, be 5 tasks and let m= 3. F@te 1 gives a round robin execution pattern. Each square 
box represents a time quantum (TQ). Ilre Cha~CteK inside the box indicate the task that is run during 
that time quantum. According to (l), the time period is 5 quanta. lle execution pattern in the time pried 
is tepeated again and again until the executions of some tasks are finished. Tfdi execution pattern provides 
that within a time period, every task receives the same amount of execution time which is 3 lQa for thw 
example. When the execution of a task is preempted, some extra time is needed to perform context switch. 
The above example gives tbe worst ease situation in which, after each time quantum, a context switch 
occurs. If a task is exexuted by a PE earfier, say PEi, and then executed by another P~ say Pf+, P% should 
not start the execution earlier than the moment that the execution on PE, is finished. The execution 
pattern shown in F@ure 1 obeys this requirement. There arc 15 context switches in this execution pattern. 
Afgorithm 1 provides a general method to produce an execution pattern such that the number of context 
switches and communication among tasks are minimized. For the sake of simplicity, we assume that a time 
quantum is 1 time unit. Algorithm 1: Execution Pattern Generating Algorithm Input: n, the number of 
the tasks; m the number of PE s. Output: An execution pattern. The execution pattern for the first time 
period is generated by step 1 through 4. 1 Let TP -LCM(12,m) / m 7 k-[ TP/ndt UNIT -LCM(n, m) /m~ and 
 R= TP-kxl13JIT. 2. Here the tasks are considered as blocks with size equal to UNIT and PE s as bins 
with size TP. At first, no PE is given any job. Task assignment starts from P%. From to, PE s are tilled 
with tasks, A PE can receive no more than TP number of jobs, 3. If a PE, say PEi, cannot hold one more 
task, but the total job received is less than TP, a portion of the next task is put into PE, so that 
the total amount of the job received by PE, is equal to TP. The remaining part of the task is given to 
PEi+l. 4. The execution pattern of the PEs for the initial time period is decided by the order in which 
the tasks are put into bins. Each task has its own time zone that indicates which P~s) is running this 
task from which moment(s). A time zone is a continues time interval during which a task is executed by 
a PE without interruption,  Step 5 and 6 generate the execution patterns for the later time periods 
until the number of concurrent tasks changes. 5. If R=O, a PE executes k tasks during first time period. 
Let ti~ be the jth task executed by PE,, O < = j < = k-1. If the previous execution pattern of PE, is 
t,.o, ti,l, .... t,~.l the current execution pattern should bc t,~.l, t,,], t,,O. 6. If R> O,the PE 
s can be divided into GCD(m,n) groups. There are g= m/GCD(m,n) PE s ineach group. From P%, the fimt g 
PEs are in the first group, Then, the next g PEs are inthesecond group and so on. Let P%, PEI, .... PE, 
be  a muP and to, tl, .... tb be the tasks that arc running on this group of PEs. The execution pattern 
for PEJis the same as for PE, +l in the previous time period, 0< =j<i. The execution pattern for PEI 
is the wme as for P% in the previous time period. Finally,accordingto this newexecution pattern, the 
time zones for task to and t~ are exchanged. [n fact, it is not necessary to start from P% with to We 
can assi~ tasks to PEA from any point of a ring of tasks. A continuous portion sized TP is assigned to 
a PE, resulting in some execution patterns homomorphic to the initial execution pattern produced by algorithm 
1. Algorithm 1 generates execution patterns for a number of time periods during which the number of concurrent 
tasks does not change. These time priods define a stage. Figure 2 shows the execution pattern generated 
by Algorithm 1. The number of concurrent tasks is S and the number of Pfis is 3. During each time pcrkd 
there are 4 context switch operations. This execution pattern will continue until the number of concurrent 
tasks, n, is changed. This execution pattern eliminates the context switches on the border of each time 
period except for the last time period of the stage. The initial execution pattern can be stored as 
a table. Each row of the table corresponds to an execution sequence of a PE. To determine the execution 
sequence, each PE needs only to access the next row of the table. The access to the table takes only 
a minimal amount of time. When a time period ends, each PE assumes that there will be the same number 
of concurrent tasks in the next time period. The execution sequence is determined by the next row of 
the table. Then the execution is resumed. Since during a time period, the execution of some tasks may 
end, the number of concurrent tasks in the next time pmiod may be different. Should such a situation 
occur, a new execution pattern is needed in the new time period. Also,it is necessa~ to inform every 
task of the situation. If a task is finished, it will send messages to all other PE s so that all the 
concurrent tasks can be informed. The message includes the identification of the finished task, the number 
of newly activated tasks and their identifications. We call this message the refreshmessage. When every 
PE has received a copy of this message, the total number of concurrent tasks can be figured out. We can 
always produce a series of execution pattern for m taska, m + 1 tasks, .... n tasks and keep this series 
of execution pattern in each PE. When a new execution pattern is needed, the proper execution pattern 
can be immediately retrieved from a table. It is not necessary to run the algorithm each time a task 
is terminated. We take this idea in the simulation. After a new execution pattern is obtained, according 
to the new execution pattern, the execution is resumed. [f a time period ends at moment MT, the refresh 
messages that affect the execution pattern during the next time period are those from the tasks which 
finish no later than the moment MT-m. If a task finishes at moment MT, the refresh message from this 
task may reach all the PEs at moment MT+~. Thus, if the time period that ends at moment MT is the ith 
time period, the information provided by this message is used to compute the number of concurrent tasks 
for the time period i +2. Generally, the execution of a task in one time period depends on the rtsult 
generated from the previous time pried. When a time zone of a task finisbea, the result fmm current execution 
must be sent to the PE that will start the next time zone of the same task. Here, we assume that such 
data communication takes no longer than a time quantum. To guarantee that the execution pattern can be 
properly followed, if a task is executed by several PE s, at least one time quantum must be placed between 
any two time zones so that the result produced by the previous execution can be received before the next 
execution begins. Of coursz, if two time zones are located on a same PE, there is no need to place any 
time interval in between. Theorem 1: The execution patterns from Algorithm 1 ensure that at Icast one 
time quantum is placed between a pair of time zones if they are located on different PEs. Proofi According 
to Algorithm 1, we can consider each PE a bin and taska as blocks which are to be distributed to the 
PE bins. [f a portion of a task, say t, is put into one PE bin, say P~, and the remaining portion of 
t is put into another PE bin, say PE1, then the first portion oft is put at the end of P% and the remaining 
portion is put at the front of PEI. This implies that the execution of t starts first on PE1. After PEI 
finishes the execution of t, there is a time intewal of at least one time quantum long before the P% 
starts execution oft. We can shift the portion of t on P% to PEI. If the interval dcm not exist, the 
amount of the job received by PE1 is equal to UNIT which means that the time period is equal to UNIT. 
In this case, R must be equal to O. According to Algorithm 1, if R = O, no task is split and executed 
by two PE s, a contradictory conclusion to the 841 precondition. . which is equal to md. . Since there 
is always a time interval between two time zones of a task, when the task is executed by two PEs, the 
time interval ensures that the communication can be accomplished before the new time zone starts. So, 
the execution pattern is not influenced by the communication factor. A given execution pattern also specifics 
a communication pattern. If R= O,since a task is only executed by one PE throughout the stage and there 
is no inter-PE communication, the configuration of the interconnection network is immaterial at this 
point. For R # O,since a task maybe executed by two PEs at diffemst moments, the inter-PE communication 
occusa. Let P% Pli+ , .... PEi be a group of PE s and to, t,, .... th be a set of tasks which are running 
on a group of PEs. Except for to and th, if a task is executed by PEj during the current time period, 
it will be executed by 1%}1 during next time period. So, the communication pattern from these tasks forms 
a ring PEi -> PE&#38;i -> ... -> P% .> PEi, which means that the distance between a source and a destination 
is only one hop away if the PE s are connected by a one direction ring network. In step 6 of Algorithm 
1, time zones of% and th are exchanged with each other in each time period. Such an exchange reduces 
the number of context switches since to or th is executed continually for two UNIT S of time with the 
same PE. If tO is last executed by a PE during the current time period, it will be executed by the same 
PE at the beginning of the next time pcricxl Unlike with the other tasks, between these two time period 
there is no inter-PE communication by to or th So, the communication from one execution of to or th to 
the next execution happens only once for each two time periods. The communication pattern formed by these 
two tasks is specified as: P% -> PE(i.2) ~ i -> ... -> P%, and PEi -> PE(i.2) ~ i -> ... -> PEi for % 
and th respectively. For a one direction ring network, a communication from a source PE to a destination 
PE needs two hops. But since they communicate two times less frequently, the one direction ring network 
can sti)l carry on the communication efficiently. According to the above communication patterns, any 
one source task only sends result to one PE which will execute the task in next time period. The situation 
of several messages Compting for one link will never happen even on a one direction ring network. So, 
RR execution policy can be implemented on any connected multiprocessor system. If the connection of such 
a system is as rich as that of an one direction ring RR execution policy will cause no delay of execution. 
Theorem 2 There are at most m-1 tasks that are partially executed by exactly two PE s during one time 
period. Theorem 3 If R= O, the number of context switch is n-m in each time period. Prod Among the n 
tasks, there are m tasks that are on the far border of a time period. According to Atgorithm 6.1, these 
tasks do not incur any context switch. So the total number of context switches is n-m. Lema 1: The number 
of context switches is less thatt n in any time period. Prod If R=O, according to Theorem 3, the Iema 
is correct. If R> O, according to Tlworem 2, at most m-1 extra context switches art generated due to 
the sptit execution of the tasks. The total number of context switches is n-m+ m-1 which is n-1. This 
is the largest possible number of context switches, and it is less than n. . During an execution stage, 
say stage i, the RR execution poticy guarantees that every task of the ni eottcurrettt tasks receives 
an qual amount of execution time. In other words, we can consider that ni tasks are running on ni slower 
virtual P)?+ (VPEa) instead of m regular PEs when m <nr M Ti be the last time period of stage i. During 
this time period, all the ni VPES are busy executing the ni tasks, one task for each VPE. Simulations 
and Results of RR Method To evaluate the performance a simulator is programmed wh~h cats simulate a multiprocessor 
sptem with 4, 8, 16, or 32 PEs connected by a cube. The simulation we haw carried on focuses on the speedup 
behavior and the effkiettcy of the RR method. The major influence from RR scheduling method to the speedup 
behavior is the data transfer delay that is caused during the context switch. The speedup is defined 
~ ~ll(ti) Speedup - -1 TET . Proofi t-et LA= be a set of the tasks that arc on the far border of a time 
period. IL4SPI = m. We first assume that m is prime ton. Thus, ~ha~: LCM(rt, m) -n x m ,u~=t%afld~= n. 
According to algorithm 1, only one task in IASP receives a UNIT of execution time from one PE. This task 
is tn., and the PE executing twl is PEm-l in the initial execution pattern. The remaining tasks in IASP 
are executed by twu PE s. Otherwise, if there is another task in MST which also receives a UNIT of execution 
time from one P~ let this PE be PEi in the initial execution pattern, i!= m-1, then, (~ +1)xT Q can be 
completely divided by m, contradictory to the condition that n is prime to m. %, the number of tasks 
in LAST that are running on two PEs is m-1. If m is not prime ton, then let d = GCD(m,n) (Greatest tlsmmon 
Divider), m = m/d, and n = n/d. For the m PITs and n tasks, according to the above statements, there 
arc m -l tasks that are executed by two PE s. The total number of tasks executed by two PE s is dx (~f 
1 ) Here ~ is the weight function of tasks and ~ stands for the Total Execution Ilme. A large speedup 
indicates a better scheduling method. The efficiency is defined ax Speedup  efficiency - In Here m is 
the number of pmcesso m. The ideal effkietrcy should ckrse to 1. The input to the simutator is a set 
of Dlruted Acycfic Graph. llre nodes in the DAG are the tasks and the edges arc the dependency relation. 
The weight of tasks are selected randomly betwun 1 and S00. The data transfer time is selected between 
1/20 of a time quantum to 1 TQ. 200 such DAGs are generated and fed to the simulator. F@re 3 to Figure 
8 gives the speedup behavior baaed on the average of these simulation results. F@re 3 and Figure 4 show 
the speedup behavior and the efficiency of the methods respectively. F@ure 5 to F@we 8 show the spccdup 
behavior when differentnumberofPEaamused.In InternationalConferenceonParallelPromaain&#38;1990,pp.l­ 
gcncrai, the system is quite tolerant to the data transfer delay until it CIOSCto 1 TQ which implica 
that the execution time and data transfer time arc almost the same. In the another aimulat~ wc usc some 
real instances. F@sre 9@maaapecdupbehaviorofamatrixmultiplication program. Figure 10 gives the spcedup 
behavior of a prime number generation program. F~rc 11 sham the apccdup behavior of the N-Quccn program. 
The simulation rcaultsshow that when the number of PE is increased, the apccdup is also increased proportionally. 
This property indicates that RR method might provide a promising mault when applied to a Iargcrscale 
of parallel system. Howevsr, the simulator wh~chmm on a IBM PC bccomcs estmncly slow when the number 
of PE ia incmaaed to 32. This prcwrsta us from making further teat. In the real example simulations, 
the worakasc assumption is atsmgiwn wttich takca 1 TQ for the data tnutsfcr time. Tire CUSYCS show the 
similar apccdup behavior. l hc simulator that w conatructcd simulatca a multiprocessor system withdistributedmemory.Howcwr, 
theRRmethodcanalaobe adopted in a sharwf memory multiprocessor system. The data transfer 0ss such a SyStcm 
is eliminated bccausc it cars bc acccsscd from the aha~d memoiy. Further study is needed on this aspect. 
Conclusion RR task scheduling method imprwe the current dynamic task scheduling methods in two pints. 
L Not like the moat other mcthoda, the RR method dots not issue load balance message. Tlris improvement 
rcduccs the burden 0ss the tmmmunicationnetworkof a multipoccaaorsystem. 2. A RR method dots not usc 
task migration mechanism to achieve the load balance. This feature.furtherrcduccs the network load. When 
the RR execution potii is enforced, since all the concurrent tasks arc executed during each time pcnod, 
there is no need to send load balancing mcsaagc. Futiher, if the number of tasks is greater than the 
number of proccsaor&#38; there is no idle PE either. RR policy, because its efficiency and flexibility 
can bc applied to a wide range of application. In future research, the focus cast be on a more extemiw 
simulation with an emphasize on the factors of task size VS.context switch time and their influence to 
the ayatcmperformance. Raferencas <RefA>[1] Csaawsrst,T.L .ndJortG. Huht A Taxonomy of Scheduling in Gerseml 
Putpou Dutributed Computing Systems IEEE Tmttsaction0sssoftware EngineeringFebruary1988,p.141. [2] Ravi, 
T., M.D. Esogovac, T. Lang and RR Mun@ Static Allocation for a Data FlowMultip~r System/ 2nd Int 1 Cortf.on 
SupcrComputin&#38;May 1987. [3] Sarkar, V. Partitioning and Scheduling Pamllel Programs for Execution 
on Mukiproccsaom Technical Report No. CSL-TR-ft7-32&#38; Computer System Labomtory, Stanford Univcraity, 
Stanford, CA9430S-2192. [4] Shirazi, B. and M.F. Wang Analyxia and Evaluation of Heuristic Methods for 
Static Task Scheduling Journal of Pamlleland Diatriiutcd Cornputin&#38;19!X3,pp222-232. [5] Willebcek-LcMair, 
Marc and Anthony P. Rccvca Local vs Global Stmtegie.s for Dynamic Load Balancing Proceeding of 1990 International 
Costfercncc on Pamllel Promaain&#38; 1990, pp.I-S69,L5Zt. [6] Sib, Gilbert C. and Edward A. Lee scheduling 
to Account for Interpronxmr Communication within htterconrtection-COnstmincd Proccsaor Nctwdsa Pmcccding 
of 1950 9,1-16. vl Xu, Jmn and Kai Hwang Dynamic Load Bafancissg for Pamllcl Program Eruution on a Mcsaagc-Pasaing 
Mu!ticomputer Procccding of the second IEEE Syqmaium on Pamllel and Diatributcd Processing, 1990, pp402@6. 
[8] Rotithor H.G. and S.S. Pyo DcccsttralizedDecision Making in Adaptive Task Sharing Pmcceding of the 
second IEEE Symposium on Pamllel and Distributed Proccsain&#38; 19%, pp34,41</RefA>. 1 time period I I * Time 
 FIgurc1. The Round Robin exccutiortpattern. Tf 1 q 2 T73 1 ­ F@mc 2. An execution pattern gcrtcmtcd 
by Afgorithm 1. 20  15 ­ 10 ­ s- I Numberof PE s 1Ii I s 10 1s 2?32s 20 FQrrrc3. Spccdup behavior of 
different communication dcfss~. Efficiency 19 1!?.5 0.4 0: ComnlunicationDdv = 1/20 TQ 18 t 17.5 Number 
of PE s II I1 - . 1, 5 10 152025 20 &#38;O TQ 1/1s TQ 1/1; TQ 1/5 TQ 1TQ Figure 4. T%e efficiency 
of different communication delays. FIgurc 8. The apccdup behavior of a 32-PE a@m with diffezent context 
switch time. sTu?  4 Spe w 30 25 GmmOnicUioOWhy 1I 1 1/20TQ 1/15 TQ 1/10TQ ]/$ TQ 1 TQ 2.8 20 Pigurc 
5. The apcedup behavior of a 4-PE ayatcmwithdifferentcontext switchtime. 15 S dup 7.8 7.  7.s la i 
.4 5 7.2  7.2 IL t I Numtp of PE s [ 1 I1 C0mmu7:wtionD*18y , * 7.1 I 5 10 15m 25 30 1/20TQ 1/]3TQ 
1/10TQ l/5 TQ 1TQ .: OTQ for each Con~xt Switch md Reorgmization 0:1 410TQ for each Context Switch mid 
Reorganization O:1 Q for esch Context .%itcb or Reorganization   I@urc 6. TIE apccdup behavior of an 
8-PE ayatemwith different contextswitchtime. Pigure9. Tltc spccdupbehaviorof matrixmultiplkationprogram. 
12 11.s 11 10.s I I1 C4mmunhtion D&#38;y 1 ­ &#38;OTQ 1/1STQ 1/10TQ 1/6 TQ 1 TQ P&#38;we 7. The apcedup 
behavior of . 16-PI? ayctem with different contact twitch time. Speedup ), 13~ Speedup Limit 12- 11 
- 10- 0 - 8 - 7 1­ 6 - 5 - 4 3 ~umber of PE s 2; ; 16* .:0 TQ for Contest Switch and Reorganization 
O: l/10 TQ for Context Switch and Reorganization D: 1 TQ for Context %vitch and Reorganization Fire 10. 
IRe -up behavior of . prime number gcaemting p-m. Speedup  13 I 12­ 11­ 10­ 9­ 8­ 7 6­ 5­ 4­ 3 ~ 4 
8 .:0 TQ for Context Switch and Reorganization O: l/10 TQfor Context Switch md Reorganization o: 1TQ 
for Context Switch and Reorganization FIgurc 11. l%cspccdup bcluviorIXthe IWuccn pzogzam.   
			
