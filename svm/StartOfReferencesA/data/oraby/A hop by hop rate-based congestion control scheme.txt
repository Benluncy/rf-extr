
 A Hop by Hop Rate-based Congestion Control Partho P. Mishra* Systems Design and Analysis Group Department 
of Computer Science University of Maryland partho@cs,umd.edu Abstract The flow/congestion control scheme 
of TCP is based on the sliding window mechanism. As we demonstrate in this pa­per, the performance of 
this and other similar end-to-end flow control schemes deteriorates as networks move to the gigabit range. 
This haa been the motivation for our search for a new flow and congestion control scheme. In this pa­per, 
we propose aa an alternative, a hopby-hop rate-hazed mechanism for congestion control. Due to the increasing 
so­phistication in switch architectures, to provide quality of service guarantees for real-time as well 
as bursty data traf­fic, the implementation of hop-by-hop controls haa become relatively inexpensive. 
A cost-effective implementation of the proposed scheme for a multi-gigabit packet switch is described 
in [12]. In this paper, we present results of a simulation study comparing the performance of this hop­by-hop 
flow control scheme to two end-to-end flow control schemes. The results indicate that the proposed scheme 
dis­plays stable behavior for a wide range of traffic conditions and diverse network topologies. More 
importantly, the per­formance of the scheme, measured in terms of the average number of occupied buffers, 
the end-to-end throughput, the network delay, and the link utilization at the bottleneck, is better than 
that of the end-to-end control schemes studied here. These results present a convincing case against 
popu­lar myths about hopby-hop control mechanisms. *This research was initiated while the author was 
at Bell Lab­oratories. This work was supported in part by Rome Labs and DARPA under contract F306O2-9O-C-CO1O 
to UMIACS at the University of Maryland. The views, opinions, and/or findings contained in this report 
are those of the author(s) and should not be interpreted as representing the official policies, either 
expressed or implied, of the Defense Advanced Research Projects Agency, RL, or the U.S. Government, Permission 
to copy without fee all or part of this material is granted provided that the copies are not made or 
distributed for direct commercial advantage, the ACM copyright notice and the title of the publication 
and its data appear, and notice is given that copying is by permission of the Association for Computing 
Machinery. To copy otherwise, or to republish, requires-a fee and/or specific permission. COMM 92-8/92/MD, 
USA 01992 ACM 0-89791-526-7/92/0008101 12...$1.50  Scheme Hemant Kanakia Computing Science Research 
Center AT&#38;T Bell Laboratories Murray Hill kanakia~research .att .com  1 Introduction The current 
efforts at building a gigabit nationwide network has led many researchers to re-examine the issue of 
flow con­trol and congestion control. Two of the most commonly used flow control protocols are the Jacobson-Karels 
modifications to TCP [6] and the DECbit scheme [14]. Both use a modified sliding window mechanism with 
the size of a sender s win­dow being dynamically adapted according to the observed network conditions. 
A lot of ingenuity has been used in detecting congestion rapidly and accurately, and adapting the size 
of the window to increase or reduce the amount of traffic injected into the network [6, 13, 14]. However, 
these flow control mechanisms are not expected to scale well as networke move to the gigabit range. We 
pursue a fundamentally different approach that scales well with a network s bandwidth-delay product. 
The ap­proach is characterized as hopby-hop rate-hazed flow and congestion control. As the name hopby-hop 
suggests, in this approach control is exercised at each switch along the path of a traffic stream. Recent 
developments in packet switching make it feasible to consider this approach seri­ously. Future packet 
switches are being designed to play an active role in ensuring quality of service for traffic streams 
such as compressed video, using reservation and schedul­ing mechanisms [4, 5]. The additional coet of 
implementing hopby-hop controls at these switchee is small. Moreover, recent advancee in microprocessors 
provide packet switches with enormous processing power so that a portion of the pro­cessing power could 
be utilized for signaling and controlling traffic over each link. We have described an implementation 
of the proposed scheme and the associated costs in [12]. In this paper, we address the question of whether 
a hop-by-hop control mechanism is more effective in controlling network 1The sljd,ng window mechanism 
was originally designed with fixed window sizes to handle the mismatch of rates of a receiver and a sender. 
 Intuitively, hopby-hop control mechanisms have certain advantages, particularly in networks with a large 
propaga­tion delay-bandwidth product. As the propagation delay­bandwidth product of a networh gets larger, 
one would ex­pect a larger number of users to transmit data with trans­mission duration comparable to 
or less than the end-to-end delay through a network. To control short bursts of traf­fic one could aggregate 
several bursty traffic streams into a group and subject the group to feedback-based control. Since traffic 
aggregation occurs at a switch, a hopby-hop control mechanism is a natural choice. An additional advan­tage 
of hopby-hop control mechanisms is the shorter delay in its feedback cycle compared to the end-to-end 
round-trip time in a network. The reduced IIag time, we believe, leads to more responsive and accurate 
control of traffic. Finally, since hopby-hop control mechanisms restrict traffic overloads to the periphery 
of a network, well behaved users are protected from malicious users. The simulation results presented 
here support these intuitive arguments. The hopby-hop congestion control strategies that have been proposed 
to date are node by node sliding windows or ON/OFF type controls [1, 2, 16]. Our preliminary investi­gations 
suggested that these schemes are inappropriate for high speed networks. Hence, we have developed a rate-based 
control strategy in which service rates of individual circuits or aggregates at a switch are dynamically 
adjusted, using feedback information provided by the neighboring switches. The desired service rate is 
arrived at using a control equa­tion which utilizes a model of the system. Old feedback information is 
used to correct inaccuracies in the model and to predict future service rates. To evaluate the performance 
of the proposed scheme, we first compare its performance with that of an existing dy­namic sliding window 
based flow control mechanism, namely that of TCP. The simulation results show that the perfor­mance of 
our scheme is much better. Due to lack of space, we do not present the results of a comparison with DECbit 
but there also the performance of our scheme is much better. We study further the effectiveness of the 
proposed hopby-hop control by comparing it with an end-to-end scheme using an identical control strategy. 
The results show that the hop by-hop scheme reacts faster to traffic changes. As a result it utilizes 
resources at the bottleneck better and loses fewer packets, as compared to the end-to-end scheme. The 
rest of the paper is organized as follows. In Section 2, we describe the proposed control strategy. In 
Section 3, we compare the performance of the proposed scheme to TCP and another end-to-end scheme. In 
this section, we also examine how to choose values for the parameters of the scheme. In Section 4, we 
discuss the related work. We conclude with a summary of results and future directions. 2 Description 
of the Controller In the hopby-hop control strategy proposed here, packet switches play an active role 
in regulating traffic. Each switch controls the sending rates for individual traffic streams. The sending 
rates are computed based on a control mechanism described in this section. Each switch also monitors 
traffic per stream and periodically exchanges traffic statistics with its nearest neighbors. The traffic 
statistics are used in the control mechanism. While several schemes of this kind in­volving cooperation 
of one or more switches were feasible, we chose a decentralized scheme where only the feedback received 
from the nearest neighbors is used by a switch for regulating network traffic. The more complicated schemes 
were expensive to implement and provided only marginal improvements in performance. The controller implemented 
at each switch adapts the sending rates of either a single connection or a group of con­nections. Neither 
rate reservation nor explicit establishment of a connection is necessary for the mechanism to work cor­rectly. 
The mechanism uses only the information carried in each packet, such as a connection identifier or a 
group iden­tifier. We consider the concept of traffic aggregation and control of the aggregate sending 
rate to be most useful when short bursts of traffic form a single group, aJthough the con­ cept can obviously 
be extended to groups formed on other considerations.2 The hopby-hop approaches proposed in [2, 16] 
do not distinguish between connections. In these schemes, switches feeding traffic to a congested link 
are forced to stop forward­ing any more traffic to the link. A scheme of this kind is sim­pler to implement 
than a scheme such as ours that requires monitoring and controlling of traffic on a per connection ba­sis. 
However, such a scheme is potentially unstable because with a source-blind regulation mechanism, prolonged 
con­gestion at a particular switch causes all switches in the }eed­in tree of the congested switch to 
also get congested. Con­gestion spreads due to the blocking of connections traversing paths not passing 
through the initial congested point. 2In the rest of the paper, the term connection includes the aggregated 
grOUP and we shall not explicitly mention a group of connections. In the control mechanism proposed here 
the feedback in­formation consists of the buffer occupancy for each traffic stream. The service rate 
of a connection at a switch is cal­culated from buffer occupancies by noting the change in the number 
of waiting packets and the net inflow of packets dur­ing the period between measurements. A switch measures 
and collects buffer occupancies for each connection per out­going link. This information is provided 
periodically to each nearest upstream switch sending traffic via that link. We assume that the source 
also has the ability to adjust rates in response to the feedback provided by the adjacent switch. The 
primary gord of the controller is to match the send­ing rate of a connection to the service rate observed 
at the downstream switch. The rate adaptation procedure also at­tempts to maintain a reservoir of waiting 
packets at each hop. The reservoir of waiting packets allows a connection to quickly increase its sending 
rate when any additional ca­pacity becomes available. The desired number of packets waiting at each hop, 
referred to as a buffer set point, is a user-specified parameter. We examine in Section 3.3 how to tune 
this parameter to trade-off the throughput against the delay or packet losses for a connection. The maximum 
sending rate of a connection is revised whenever a switch receives feedback information from the downstream 
switch. The rate is computed as follows. Con­sider the evolution of the system state in the discrete 
time space, at instants when feedback packets are received at a sender. Let l= denote the sending rate 
at time z. Assume that k feedback packets are sent with time interval T, such that k ~ T is the propagation 
delay for the link under con­sideration. For the connection under consideration, let x. be the number 
of occupied buffers, at time z, at the down­stream switch. At time step Z, the sender receives the buffer 
occupancy level, Zz_k, measured k time steps earlier at the downstream switch, Let tz+~ be the estimated 
number of occupied buffers, at time step z + k. Let x* denote the de­sired buffer setpoint and let ~j 
and fij denote, respectively, the acturd service rate and the estimated service rate for the connection 
at the downstream switch. We define a gain parameter, /3, that controls the rate of convergence to the 
desired buffer occupancy level. The sending rate 1. at time step z is computed as (1) The first term 
on the r.h.s is zero when the buffer setpoint is equal to the expected occupancy level. In this case, 
the sending rate is chosen to match the estimated future service rate. Otherwise, the sending rate is 
increased or decreased so as to have the buffer occupancy level at a future time instant equal the buffer 
setpoint. Zz+k is estimated as (2) For a given link, the choice of k determines the value of T, the period 
between two updates referred to hereafter as the control period. The control period affects the frequency 
of rate adjustment and thus the overhead incurred. The service rates, mj, depend on the traffic characteristics 
of input sources as well as the service discipline used at the downstream switch. Since in general, 
accurate knowledge of the future service rates is not available, we use the estimated service rate, rn~, 
in Equation (1). The estimated service rate, riz~, is calculated with the following first order auto­regressive 
filter wit h the correction factor, a, cent rolling how closely the changes in the service rate are tracked. 
In this equation, m~ -1 is the actual service rate calculated from previous feedback messages about the 
buffer occupancies.  iJ=a*m~-l(3) +(l-a)*tij-l, O<cr<l Experimental results with the controller designed 
as above suggested a minor improvement in the design. When a con­nection starts up, the sending rate 
increases in a slow and oscillatory fashion. A simple modification to Equation (1) that damps the initial 
oscillations is to increaze the sending rate at a fixed rate as long as the feedback indicates that the 
number of queued packets is less than one. With this change a connection starts sending at its available 
bandwidth only after a few steps. This gmduai start for a connection has the important side-effect of 
minimizing the total number of buffers used; this happens because while a connection is gradually increasing 
its rate, existing connections decrease their rates to the new equilibrium value. The modified design 
equation is X*  +k + I/k * Xj~j]_*k-l rhj, otherwise(4) = k* T* l//3 One could choose a buffer setpoint 
in a number of differ­ent ways. If a fixed number of buffers are reserved for a connection during its 
lifetime then a static buffer setpoint value is acceptable. If the buffers are shared among all ac­tive 
connections, an adaptive control of the buffer setpoint is preferable. This may be done as follows. Let 
B denote the total number of shared buffers available to all connections sharing the same output link 
and C denote the output link s buffer setpoint is H2 n z = (C /C) *B (5) The behavior of the controller 
described here is controlled with five different parameters: the correction factor, a, the gain, ~, the 
rise rate, 6, the control period, T, and the buffer setpoint, x*. The tuning of these parameters and 
their im­pact on the performance is discussed in Section 3.3. The implementation expense was also a critical 
factor in our de­liberations on the design of a traffic control mechanism ap­propriate for next-generation 
networks. While we omit de­tails about the implementation here due to lack of space, we discuss in [12] 
how our scheme is efficiently implemented. 3 Results In this section we compare the lhop-by-hop scheme 
that we propose, referred to hereafter as HBH, with the flow con­trol scheme implemented in TCP [6]. 
Next, we compare the HBH scheme with the end-to-end scheme using the feed­back controls based on Equations 
2-5, referred to hereafter as E2E. The latter comparison provides better insights about the effectiveness 
of hop-by-hop controls vis a vis end-to-end controls. Finally, we discuss various trade-offs in choosing 
the parameters of the HBH scheme, The results reported here were obtained through discrete-event simulations 
using a network simulator that has also been used in several other performance studies [17, 18, 19]. 
The TCP code used in the simulator was taken directly from the 4.3-Tahoe BSD Unix (Tahoe) release. 3.1 
Performance Comparison with TCP The dynamic window adjustment mechanism for flow con­ trol in TCP, suggested 
by Jacobson [6], was chosen for this comparison due to the popularity and the effectiveness of the scheme 
in low-speed networks. Figure 1 shows the net­work configuration used for the comparison. The connection 
from Host 1 to Host 3, referred to as the direct traffic, tra­verses 1 switch and 2 links. The bottleneck 
link s capacity is 10 Mbps. The other links are 200 Mbps. All three links have a propagation delay of 
10 ms. The traffic is one-way, towards Host 3 and the packet size is 500 bytes. Using a longer delay 
path and a bottleneck link of higher speed (say 1 Gbit/s), results in a TCP flow controlled connection 
taking much longer to show stable behavior. In the TCP flow control scheme, the detection of a packet 
loss is the only event that leads to a reduction in the window 2MWS ms Conn2 1   FI=-6=EI Connl Figure 
1: Simulation configuration 1 .i k ..ili..ea-ai BMAX .100 IIII IIII 0.0 Mm ! (M  0.8 0.7 o.e 0,6 
04 . 0.3 O.* 0.1 1 00 a Tm19 (* e.> Figure 2: Bottleneck utilization with TCP and HBH size. The primary 
reason for packet losses is buffer overflow at the bottleneck. As a result the performance of TCP is 
strongly dependent on the amount of buffering available at the bottleneck. The maximum number of buffers 
available to a connection, at the bottleneck, in Configuration 1, is 100 packets. The buffer setpoint 
of HBH is chosen to be half of the maximum amount of buffering available, so that the average end-to-end 
delay 3 is comparable to that of TCP. In Figures 2 and 3 we show the bottleneck utilization and the end-to-end 
delay for a connection running under TCP and HBH. The link utilization is measured over 4 ms intervals 
and the end-to-end delay is measured for each successfully transmitted packet. The throughput curve which 
we omit here has a profile similar to that of the link utilization curve. Figure 2 shows that when the 
connection is HBH con­trolled the bottleneck link is fully utilized within 4-5 round­ 3The end-to-end 
delay we measure is the time duration from the point at which the source host transmits a packet to the 
point at which the receiver host receives the packet.  C.01.v (in .-) BMAX . lW am em S.SO SW 4.s0 
4.03 S.XJ 3M 2.s0 aal nllm 0 000.) Figure 3: End to end delays with TCP and HBH trip times. In contrast, 
with the TCP scheme, it takes about 100 round-trip times before the link is fully utilized. More­over, 
in this phase, a TCP controlled connection loses about 190 packets whereas a connection controlled by 
HBH loses no packets. After a connection reaches stable state the bot­tleneck link remains fully utilized 
by a connection controlled with HBH; with TCP flow control the utilization drops occa­sionally. The slow 
and erratic start-up with TCP is not the only problem shown up in this comparison. The end-to-end delay 
for the TCP connection, shown in Figure 3, shows the oscillations characteristic of TCP. These oscillations 
occur even in the absence of cross-traffic, because of the nature of the control mechanism. In contrast, 
the HBH scheme shows almost no oscillations. The oscillations in queue length at the bottleneck can result 
in significant under-utilization of the bottleneck capacity when existing connections go down and in 
packet losses when new connections come up. In con­trast, under HBH the bottleneck utilization will be 
high even when traffic changes occur because of the fixed number of waiting packets at the bottleneck. 
We have performed the HBH-TCP comparison for sev­eral other scenarios. We briefly summarize the behavior 
ob­served. When the number of buffers at the bottleneck is less than the bandwidth-delay product, the 
performance of the TCP scheme deteriorates considerably, but the perfor. mance of the HBH scheme is not 
affected significantly. In the presence of randomly varying cross-traffic patterns, there are oscillations 
in the end to end delay of a HBH controlled connection. However, these oscillations are of much smaller 
amplitude compared to the oscillations observed with TCP, with or without cross-traffic. If the modified 
window ad­ justment algorithm embedded in the 4.3 BSD Unix (Reno) version of TCP, is used then the bottleneck 
link utilization improves somewhat once a connection reaches steady state but the large start up time 
and the oscillations in the end­ to-end delay are not reduced. In [12], we explain the poor behavior, 
observed here, of a connection running under TCP. The performance observed here is worse than that seen 
in many recent studies [17, 19]. The difference, we believe, is due to our having used a con­figuration 
with a large bandwidth-delay product. In this en­vironment, the performance of the TCP flow control scheme 
is far worse than that in low speed networks.  3.2 Is Hop by Hop Better? The superior performance of 
HBH compared to TCP is mostly due to the nature of the control policy. Since TCP is designed for use 
in an environment where switches do not provide feedback, the control algorithm uses packet loss as an 
implicit feedback thus leading to the oscillations in end­to-end delay and possible loss of throughput. 
The question about what advantages are provided by hopby-hop control over end-to-end control is not addressed 
by the comparison with the TCP since it may be justifiably argued that the difference in performance 
is due to the choice of control pol­icy. In order to dkectly address this question, we compare the performance 
of HBH to that of the end to end scheme, E2E, where Equations 2-5 are used to adjust the sender s rate 
at the source based on the feedback received from the bottleneck node. The network configuration used 
in these experiments is shown in Figure 4. Increasing the number of hops, by adding switches S1, S2 and 
S3, allows the interactions among mul­tiple hopby-hop controllers to be studied. In this configura­tion, 
the end to end propagation delay for the direct traffic and the cross traffic is 60 ms and 30 ms, respectively. 
The direct traffic, connection 1, comes up at time O with infinite data to send. The cross-traffic switches 
on at time 2000 ms and disappears at 3000 ms. We study the behavior of con­nection 1 as it comes up, 
stablizes, reacts to changes in the cross-traffic, and stabilizes again. The rise rate, 8, is picked 
to be 1/4 of the bottleneck rate per end-to-end round trip time. The values of the correction factor, 
a, and the gain, ,L3, are 0.4 and 0.45, respectively. The control period, T, is 4 ms leading to five 
updates in one hop round trip time. We ran experiments with cross-traffic provided by a constant rate 
traffic source, a Poisson traffic source and infinite data source with rates controlled using identical 
controls as that on the direct traffic. The uncontrolled cross traffic corre­sponds to voice/video traffic 
which may share the service capacity but be subject to a different set of controls. .Ms. / x­ 1IIII I 
M s 0Tf12 L M 2(OWS mo.od- HI w R2WMSP %4 S5 M MS MIS Ml lhns Kills lhsls n  -G--ouLll Cmnl Figure 
4: Simulation configuration 2 600.00 . 500.00 . 400.00 . 300.00 200.00 . J : : I m L.­ 100.00 0.90 
. a ( /.,,,,.O)IT 4,. y .-:-..:.-..-.l\ ,,r.. . ...... , w I I I 0 lCOO 2000 3om 4000 m- O. -) Figure 
5: Buffer occupancy for Connection 1 Clearly, the performance of tlhe scheme depends on the value chosen 
for the buffer setpoint. In the HBH scheme, there are buffered packets at eachl hop for any connection; 
in the E2E scheme packets are queued only at the bottleneck node. Hence, in steady state, the number 
of packets in the network for a connection with 4 switches in its path is the same for both the HBH scheme 
with a buffer setpoint of 50 and the E2E scheme with a setpoint of 200. The HBH scheme with a buffer 
setpoint of 200 uses the same number of buffers at the bottleneck queue as the E2E scheme with a buffer 
setpoint of 200. For the configuration used in these experiments, these buffer setpoint values are much 
smaller than the pipe size, which is 1500 packets for connection 1 when the entire bottleneck link capacity 
is used. In Figure 5, we show the evolution of buffer occupancy at the bottleneck point, the output link 
from switch 4 to switch 5, for connection 1. The cross-traffic is generated by a constant rate source 
that comes up at 2000 ms with a rate equal to 40% of the bottleneck capacity. Note that the time 117 
 ,* tin ..) Figure 6: Sending rate at the source scale of this figure is much smaller than the time scale 
used in the figures showing the performance of TCP. This is so because both the E2E and the HBH scheme 
start up much more quickly than the TCP scheme. The start-up time of a TCP connection with this configuration 
would be very long. At the instant when cross-traffic comes up additional buffers are used up at the 
bottleneck due to the delay in adjustiug the input rate to the reduced service rate. Due to the smaller 
delay in the feedback cycle time, much fewer ad­ditional buffers are needed for the HBH scheme. The number 
of additional buffers required is independent of the value of buffer setpoint chosen. When more capacity 
becomes avail­able, the buffered packets allow a connection to quickly take advantage of the available 
bandwidth. As shown in Figure 5, the HBH scheme with 200 buffers is able to react more rapidly than the 
E2E scheme with 200 buffers. On the other hand, the HBH scheme with 50 buffers is slower compared to 
the E2E scheme in using the additional capacity. The utilization of the additional capacity depends both 
on the buffer setpoint as well as the time delay in adjust­ing the sender s rate. For brevity, we do 
not show the link utilization, the throughput and the end-to-end delay. The link utilization rises as 
quickly as the buffer occupancy does and it remains at 100 Yo as long as the buffer occupancy does not 
fall to zero. In this experiment that happens only for the E2E scheme and the HBH scheme with a buffer 
set point of 50 at the instant when the cross-traffic has just dis­appeared. The end-to-end delay curves 
closely follows the shape shown by the buffer occupancy curves. In Figure 6 we show the actual sending 
rate at the source; clearly, us­ing HBH, the source sending rate stabilizes to the new value almost as 
quickly as it does using E2E. ,mak.t. hdlwldu.1 md told buffw .cufmrml ml th. bWut8d4 I II II I 120 
 @ 4- 4 I Figure 7: Buffer occupancies with E2E(200) mmm(1 m,) Figure 8: Buffer occupancies with HBH(200) 
If the cross traffic stream generates packets according to a Poisson cross traffic stream then the results 
are not qual­ it at ively different, although the buffer occupancy fluctuates because of the randomness 
in the cross traffic. The ampli­tude of the fluctuations is smaller with the HBH scheme than with the 
E2E scheme. In steady-state, the total number of buffers, occupied at a switch, depends on the number 
of ac­ tive connections and their buffer setpoints.4 We study the transient behavior for the total number 
of buffers in use in Figures 7, 8 and 9. In this set of experiments, the cross­traffic is controlled 
the same way the direct traffic is. With the end-to-end controls, a large overshoot occurs because the 
feedback-control delay of the cross connection is much smaller and as a result the rise rate of the cross 
connection is 4 we ~sme here that buffer setpoints remain static. The dy­ namic bufTer setpoint algorithm 
suggested in Section 2 is explored in the next set of experiments. 11 mm mm Omol­d I I I I I i I 0 Imtl 
m XcO 4UX! The [h tns) Figure 9: Buffer occupancies with HBH(50) Pm=ik.t . Mtvld.1 .rld tow - pmo!n 
m Ill. bowuld III I III -1- I 1s0. Km. t I xmOl­ .lJ I1 IIIII o ICrJO moo .wm 4000 Tbn. (WI m.) Figure 
10: Buffer occupancies with HBH(dyn) much higher than the decrease rate of the original connection causing 
a big overshoot. If the buffer setpoint is static then as the number of con­nections increases, the total 
number of occupied buffers, at a switch, also increases. Since the number of buffers available at a switch 
is likely to be limited and shared among all the active connections, we use the adaptive scheme, described 
in Section 2, to decide the buffer setpoint value. Figures 10 and 11 show how the bottleneck occupancy 
evolves when the buffer setpoints are dynamically adjusted for the HBH and the E2E scheme, respectively. 
It is clear from these figures that the HBH scheme is more successful at keeping the sum of the buffer 
occupancies for both connections at or around the desired levels than the E2E scheme. This hap­pens because 
the end to end delays of the two connections are different and the second (cross) connection incremes 
its 118 - . y\ \ - . \ Wld_ow -. -. &#38; - 1. -. +=---­ lW.W . /­ / f -.000 , am.-4 II 1II 0 Imo mm 
3000 4400 Imm (In ml.) Figure 11: Buffer occupancies with E2E(dyn) rate faster than the first connection 
decreases its rate. In the previous experiments we did not set any limits on the total number of buffers 
available to a connection. The large increases in buffer occupancy that occur when new connections come 
up, suggest that packet losses are likely if buffers are limited. In the next set of experiments, we 
study the effect of uncontrolled bursty traffic on a connec­tion controlled by HBH and E2E for different 
buffer sizes. The cross connection delivers a burst of 500 packets every Q seconds, where Q is exponentially 
distributed with a mean of i?oo ms. We assume that the setpoint is half of the total amount of buffering 
available. The duration of the experi­ment is 12000 milliseconds with the cross traffic coming up at 
1000 milliseconds and disappearing at 11000 milliseconds. The throughput and loss values a,re averaged 
over the entire interval. As in previous experiments, the same rise rate is used for both HBH and E2E. 
The results shown in Table 1 indicate that as the amount of available buffering per connection increases 
the perfor­mance of both end-to-end control and hop-by-hop control improves; however, hopby-hop ccmtrol 
requires fewer buffers to produce the same level of performance. Thus when a max­imum of 400 buffers 
is available per connection, HBH haa al­most no packet loss and the maximum possible throughput for the 
particular cross traffic sample. For the same amount of buffering E2E haa a small loss rate and slightly 
lower throughput. When the maximum amount of buffering is increased to 1000 the performance of H BH and 
E2E is com­parable, with E2E still having a small loss rate. As the num­ber of buffers available is decreased 
to 100 the performance of both E2E and HBH, in terms of both packet loss and throughput, worsens with 
HBH performing slightly better. schemes Throughput Lossl LOSS2 HBH(50, 100) 9.36 pk/ms 2.13% 10.91% E2E(50, 
100) 9.19 pk/ms 13.48% o% HBH(200, 400) 10.57 pk/ms o% ,031?70 E2E(200, 400) 10.26 pk/ms 4.29% o% HBH(500, 
1000) 10.57 pk/ms o% o% E2E(500, 1000) 10.57 pk/ms .043% o% Entries in the first column are in the format: 
scheme(setpt, bmaz) where setpt is the buffer setpoint (in packets) and brrum is the total number of 
buffers available to a connection (in packet8). Lossl is the percentage of packets lost at the bottleneck 
switch. LOSS2 is the percentage of packets lost at switches other than the bottleneck. Table 1: The average 
throughput and loss rates for dif­ferent values of maximum buffering and setpoint However an important 
thing to notice is that the packet loss with HBH occurs mainly at non-bottleneck nodes because the nodes 
before the bottleneck throttle back quickly. If the buffering available to a connection at non-bottleneck 
nodes is more than at the bottleneck nodes the number of packet losses decreases. For example if 200 
buffers are available at non-bottleneck nodes then the loss rate at non-bottleneck nodes with HBH(5O,1OO) 
drops to almost O. Note that al­though both HBH and E2E have the same rate of rise the HBH has a slightly 
higher throughput for lower setpoints because it has a setpoint worth of buffers accumulated at every 
node. Another point to note is that for both HBH and E2E using a lower setpoint value results in fewer 
losses but a lower throughput. 3.3 Sensitivity Analysis of design pa­rameters The behavior of the controller 
is affected by several param­eters such as the buffer setpoint, the gain, the correction factor, the 
control period and the rise rate. In this section, we discuss how changes in the values of these parameters 
affect the performance of the controller. 3.3.1 The Buffer Setpoint The difference in the performance 
of the HBH scheme with the value of the set point equal to so and 200 packets, as shown in Figure 5, 
illustrates the impact of the buffer setpoint. If connections are long lived then the choice of setpoint 
may be made based on the optimization of some steady-state performance criterion such aa power [13, 14]. 
However, if connections are short lived or bursty, it is im­portant to have a set point that is high 
enough so that no 119 throughput is lost if excess capacity opens up, and yet low enough so that buffers 
do not overflow if available capacity decreases. The maximum size of the overshoot is limited by the 
product of a connection s throughput at the bottleneck and the feedback-control lag for the system; thus 
to eliminate any possibility of overflow the amount of buffering available at any instant to a connection 
should be equal to the value of the setpoint plus the maximum possible overshoot. Sim­ilarly the bottleneck 
queue occupancy drops to zero only if the queue occupancy at any point is less than the increase in bottleneck 
service rate times the feedback-control lag. The start-up time taken to get toequilibrium is decided 
bytwodlstinct phases. Theduration of the first phase is the time taken by the sending rate at the source 
to exceed the bottleneck source rate for the first time; this depends on the rise rate. The duration 
of the second phase is equal to the time taken by the controller to push the occupancy at the bottleneck 
from around 1 packet to the set point value; this dependson the gain and the setpoint value.  3.3.2 
The Gain The gain value determines how closely the sending rate at a hop tracks the sending rate at the 
upstream node. The sending rate rises and falls faster when a higher gain value is used. With the higher 
gain value, the number of buffers used takes longer to stabilize at the setpoint value and the undershoot 
and the overshoot around the setpoint is more pronounced. In our experiments, we have found that it is 
important to make sure that the gain is not too high, espe­cially when the value of the setpoint is fairly 
high, otherwise there are large swings in the sending rate. For most of our experiments the gain value 
used is 0.45 which implies that the controller attempts to drive the system to the setpoint in two round 
trip times. If the policy goal is to consider con­gestion more harmful than loss of throughput, then 
a higher value of gain should be used while decreasing rates.  3.3.3 The Correction factor and the Control 
Period Both, the correction factor a and the duration of the control period T affect the responsiveness 
of the controller. The mu­tual dependence of these parameters is explained as follows. Let the high frequency 
transients in the buffer occupancy have a frequency of k Hz. Let us assume that we want to ad­just parameter 
values such that these transients are ignored. If the duration of the control/feedback interval is less 
than 1 /k sees then the value of alpha should be chosen to be less than one; otherwise the value should 
be one. Ideally, the value of the feedback interval should be long enough to filter out undesirable transients 
but small enough to respond to significant changes in the cross-traffic.  4 Related work Although there 
is a large and growing body of work on the design of flow/congestion control schemes for data traffic 
[3, 6,8, 11, 13, 14], very little work has been done so far on hop-by-hop control schemes. In the past, 
two hop-by-hop schemes have been suggested: one of these is based on the sliding window mechanism [1], 
and the other is based on the use of ON/OFF messages [2, 16]. In the cent rol mechanism proposed in [2], 
the traffic flow is turned off in response to an OFF message from a congested node; the flow resumes 
either through a timeout mechanism or when an ON message is received. The HBH scheme that we have proposed 
differs significantly from these schemes. We control rates and not windows. A window-based hop-by-hop 
control would require that accurate state information about each connection, such as the sequence numbers 
of packets, be available at a switch. In contrast, in the scheme we propose one can cent rol flows of 
either an individual circuit or aggregated traffic. The proposed controls work correctly without the 
participation of a switch in the connection establishment phase. Autonet, a high-speed local area network 
uses a ON-OFF type of flow control mechanism between nodes [16]. As pointed out in [16], this scheme 
is not designed to handle long-term congestion at a switch. We have observed in our preliminary studies 
that ON-OFF type controls which are source-blind result in unfair service to users, spread con­gestion 
to other nodes and cause oscillations in the queue occupancy at the bottleneck. Because of these factors, 
the control mechanism used in Autonet does not appear to be useful for a wide-area, high-speed network. 
Congestion may be prevented by subjecting all demands to call admission controls and by policing offered 
traffic at the admission points to the network. This philosophy is accept­able if one believes that sources 
can be accurately modeled and their demands can be predicted. The proposed hop-by­hop control mechanism 
does not make such an assumption. It would work well with or without call admission controls and does 
not require an a-priori characterization of source behavior. Most of the flow control schemes proposed 
for wide area data networks are feedback-based flow control mechanisms. These systems base their control 
decisions on the old values of buffer occupancy or traffic conditions and exclude any considerations 
of the predicted future state of the system. 120 controls due to the delay in the feedback. To improve 
the performance of time-delayed control systems, special control strategies, called model-based predictive 
controls, have been developed in the field of process control [15], In a model-based predictive strategy, 
a controller uses a model of the controlled system to compute the value of the output so as to satisfy 
some objective function. If the model is reasonably accurate, the performance of a predictive con­trol 
scheme is better than that c)f a purely feedback control scheme. This is because decisions are made based 
on a pre­dicted future system state and not on the old state of the system. In a model-based predictive 
control system, feed­back messages are used to compensate for prediction errors or inaccuracies in the 
model so as to minimize the drift of the model from the real state of the system. The controller used 
here has evolved from the experience of one of the authors obtained with the predictive controller reported 
in [9, 10]. The control strategy in the earlier effort was a split-range cent rol. The horizon of control 
was one time step with only onedecision being taken per round trip time. Theevolution of thequeue waspredicted 
with amodel based fluid approximation. The service rate was predicted using the moving average equation. 
 An end-to-end predictive model-based control strategy is investigated in [8]. Instead of using feedback 
from switches, the bottleneck rates are estimated in this scheme by a packet-pair probe mechanism. The 
goal of the control pol­icy is to drive the buffer occupancy at a future point to a particular value. 
The queue occupancy at the bottleneck is estimated using the fluid modell. Two filters are used for predicting 
the service rate, a Kadman filter and a fuzzy ex­ponential predictor. 5 Conclusions anid Future Work 
In this paper we have proposed a new hop-by-hop mecha­nism for controlling congestion in a packet switched 
network. Simulation results show that the scheme displays better dy­namical behavior than that of congestion 
control schemes based on sliding windows, such as that of TCP. The scheme requires that each packet switch 
generate information about buffer occupancy and current service rates per traffic stream. The cost of 
the additional information required is reason­ able, particularly for future packet switches that are 
ex­pected to provide sophisticated scheduling mechanisms in order to support real-time applications. 
The scheme has being developed by us. We believe that hop-by-hop schemes have not received serious attention 
in the past due to the following popular myths about hop-by-hop controls (HBH). * Myth #1 HBH reacts 
SIOW~y. As is clear from the results discussed in Section 3.2, the proposed HBH scheme results in the 
source learning the new rate within a couple of end to end round trip times. As shown in Figures 2 and 
3, the HBH scheme adjusts more rapidly to changes than the TCP scheme does. In general, we have found 
that the responsiveness is determined by the exact nature of the controller and the parameter values 
that are used. Myth #2 HBH displays wild oscillations. Oscillations in the buffer occupancy occur with 
all the congestion cent rol algorithms that we have examined. In the HBH scheme, we find that the oscillations 
that occur are small and are quickly damped. No such damping is observed in the case of the TCP. Undamped 
oscillations have also been reported in the end-to-end congestion control strategy proposed in [3]. We 
hypoth­esize that the extent of oscillations is determined by the lag time in the feedback cycle and 
by the actual form of controls, e.g. predictive versus reactive strategies. Purely reactive schemes such 
as the scheme in [3] show continuous oscillations even if there is no cross traffic. Myth #3 HBH is inherently 
unstable. A similar fear has also been voiced against rate-based controls in general. These fears about 
the inherent instability are hard to disprove. We have found no theoretical or experimental evidence 
to support these beliefs. Using the simulator described here, we have conducted a large number of experiments 
over a vari­ety of configurations and traffic conditions. In none of these experiments, have we observed 
any unstable behavior. When a node became congested, the con­gestion remained localized and no catastrophic 
traffic conditions occurred. This fear of instability may stem from past experience with the non-discriminatory 
on­off type controls and it does not seem relevant to the scheme we propose. Myth #4 HBH is wstly. The 
additional cost of the HBH mechanism is marginal, particularly in switches already supporting quality 
of service for a variety of different traffic classes. Further­more, the trend towards mightier microprocessors 
will provide future packet switches, using these micropro­ cessors, the processing power to easily handle 
the addi­tional cost. In our implementation, we have measured that a revision in a rate entails executing 
170 machine cated than the proposed one. We plan to experiment mostly instructions. This is an acceptable 
cost since the rates within the framework of the model-baaed predictive controls. are changed after few 
hundreds of packets are transmit-In the future we also propose to study the performance of ted, The cost 
is further mitigated since with the HBH HBH when used to control data traffic in integrated traf­scheme 
fewer acknowledgement packets are required at fic environments where real-time traffic with additional 
de­the transport layer because these packets do not serve mands such as control of jitter or rates in 
a narrow range any clocking functions in contrast to the role played in is also carried. In [7], a rate-based 
scheduling mechanism, end-to-end control schemes such as TCP. Consequently, called Hierarchical Round-Robh 
was proposed to provide acknowledgements could be sent for a block of data quality of service to real-time 
traffic sources. The HBH packets rather than for each data packet. Moreover, scheme has been integrated 
with the HRR scheme such that in applications where forward error-correction method there are classes 
of traffic operating under the influence of is used or where the data has real-time significance, one 
separate control policies. We plan to study the performance could suppress acknowledgement traffic without 
affect-of this integrated control scheme in controlling both real­ing flow control. time and bursty data 
traffic sources. In a homogeneous environment where all switches employ References HBH controls, there 
is no reason to use additional end-to­end controls at the transport layer. We have shown here that <RefA>[1] 
D. Bertsekas and R. Gallagher. Data Networks. Pren­ tice Hall, 1987. HBH can handle large amounts of 
data as well aa short data bursts better than the corresponding end-to-end scheme. For [2] D. R. Cheriton. 
Sirpent: A high-performance internet­a long lived connection, the sending rates of a sender follows working 
approach. In Proc. ACM SIGCOMM 89, pages the changes in the service rate at the bottleneck within a cou-158-169, 
Sep. 1989. ple of round-trip delays (Figure 6). Due to the reduced lag [3] K. W. Fendick, M. A. Rodrigues, 
and A. Weiss. Analy­ time, fewer packets accumulate at the bottleneck, when new sis of a Rate-Based 
Control Strategy with Delayed Feed­ cross-traffic is added (Figures 5, 7, 8 and 9). HBH also re­back. 
In Proc. ACM SIGCOMM 92, Aug. 1992. quires a smaller number of packets waiting at the bottleneck [4] 
D. Ferrari. Real-time communication in packet­ in order to use the increased service capacity when cross­switching 
wide-area networks. TechnicaJ Report TR-89­ traffic decreases. Due to the reduced lag time, HBH suffers 
022, International Computer Science Institute, Berke­ fewer packet losses when the total packet storage 
capacity is ley, CA, May 1989. limited (Table 1). [5] J. M. Hyman, A. A. Lazar, and G. Pacifici. Real 
time scheduling with quality of service constraints. IEEE In a heterogeneous environment, some sub-networks 
may Journal on Selected Areas in Communications, 9:1052­ not use H BH cent rols. In such environments 
an end-to-end 1063, Sept. 1991. control is likely to be used at the transport layer. In this en­ [6] 
V. Jacobson. Congestion avoidance and control. In vironment the HBH controlled sub-networks may be viewed 
Proceedings of A CM SIGCOMM 88, Aug 1988. as single links, with the virtual service capacity of the link 
[7] C. R. Kalmanek, H. Kanakia, and S. Keshav. Ratebeing determined by the HBH control mechanism. If, 
say, controlled servers for very high-speed networks. In R-o­ the end-to-end control mechanism of TCP 
is used and a par­ceedings IEEE GLOBECOM 9o, San Diego, CA, Dec. ticular HBH controlled sub-network is 
the bottleneck then 1990. the typical oscillatory behavior seen in Figures 2 and 3 will be observed at 
the first switch in the sub-network; within [8] S. Keshav. A control theoretic approach to flow control. 
the rest of the sub-network the behavior will remain unaf- In Proceedings of ACM SIGCOMM 91, Sept 1991. 
fected. If none of the HBH controlled sub-networks is the [9] Keng-Tai Ko, Partho P. Mishra, and Satish 
K. Tri­bottleneck, then the dynamic behavior seen in Figures 2 and pathi. Interaction among Virtual Circuits 
using Pre­3 will be observed at the bottleneck link. Moreover, the use dictive Congestion Control. To 
appear in Computer of TCP S end-to-end flow control will not have an adverse Networks and ISDN Systems. 
impact on the behavior observed within the HBH controlled [10] Keng-Tai Ko, Partho P. Mishra, and Satish 
K. Tripathi. sub-network. Predictive congestion control in high-speed wide area networks. In Proc. Second 
IFIP WG6.1/WG6.4 In-In the future, we expect to experiment with different hop-ternational Workshop on 
Protocols for High Speed Net­ by-hop control strategies, both simpler and more sophisti-works, Palo Alto, 
CA, Nov. 1990. 122 [11] A. Mankin. Random drop congestion control. In Pro­ceedings of ACM SIGCOMJ4 90, 
pages 1-7, Sept 1990. [12] P. Mishra and H. Kanakia. A hop by hop rate based congestion control scheme. 
Technical Report 11273­920225-OITM, AT&#38;T Bell Labs, Murray Hill, NJ, Mar 1992. [13] D. Mitra and 
J. B. Seery. Dynamic adaptive windows for high speed data networks: Theory and simulations. In hceedings 
of ACM SIGCOMM 90, pages 20-30, Sept 1990. [14] K. K. Ramakrishnan and R. Jain. A binary feedback scheme 
for congestion avoidance in computer networks with a connectionless network layer. In Proceedings of 
ACM SIGCOMM 88, Aug 1988. [15] D. Seborg, T. Edgar, and D). Mellichamp. Pmces9 Dy­namics and Control. 
Wiley, 1989. [16] M. Schroeder et al. Autonet: A High-speed, self­configuring local area network using 
point-to-point links. IEEE Journal on Selected Areas in Communi­cations, Vol. 9, No. 8, October 1991, 
pp. 1318-1335. [17] S. Shenker, L. Zhang, and D. Clark. Some observa­tions on the dynamics of a congestion 
control algorithm. Computer Communication review, pages 30-39, Oct. 1991. [18] L. Zhang. Virtual clock: 
A new traffic control algorithm for packet switching network. In Proceedings of ACM SIGCOMM 9o, pages 
8-18, Sept 1990. [19] L. Zhang, S. Shenker, and ID. Clark. Observations on the dynamics of a congestion 
control algorithm: The effects of two way traffic. In Proceedings of A CM SIG-COMM 9I, pages 30-39, Sept 
1991</RefA>. 123  
			
