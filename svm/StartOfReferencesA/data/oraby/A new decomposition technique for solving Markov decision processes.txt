
 A New Decomposition Technique for Solving Markov Decision Processes Pierre Laroche Yann Boniface René 
Schott LITA -Université de Metz LORIA IECN and LORIA IUT -Ile du Saulcy Campus Scienti.que, BP 239 Campus 
Scienti.que, BP 239 57045 Metz Cedex 1 54506 Vand uvre-l`es-Nancy 54506 Vand uvre-l`es-Nancy laroche@iut.univ-metz.fr 
boniface@loria.fr schott@loria.fr Keywords Planning under uncertainty, Markov Decision Process, De­composition, 
Parallelism.  ABSTRACT In this paper, we present a new tool for automatically solving Markov Decision 
Processes. Using a prede.ned partition of the MDP, a directed graph is built to decompose the global 
MDP into small local MDPs which are independently solved. An approximate solution for the global MDP 
is obtained using local solutions. Our approach has been tested on a mobile robotics application. It 
allows near-optimal solutions to be obtained in signi.cantly reduced time. We also present preliminary 
results concerning a parallel implementation. 1. INTRODUCTION Recently, there has been a lot of interest 
within the AI and planning community in using stochastic models, which pro­vide a good framework to cope 
with uncertainties. Among them, Markov Decision Processes [1] have been the subject of several recent 
studies [6]. The optimal policy is computed with respect to all the uncertainties, but unfortunately, 
clas­sical algorithms for building these policies are intractable for large environments with many states 
[8]. Several re­cent studies have concentrated on obtaining a good approx­imation of the optimal policy. 
Among them, decomposi­tion techniques have been the subject of particular atten­tion. Decomposing an 
MDP consists of (1) partitioning the state space into regions, i.e. transforming the initial MDP into 
small local MDPs; (2) independently solving the local MDPs; (3) combining the local solutions to obtain 
a policy for the global MDP. The main originality of our approach is to represent the MDP as a directed 
graph. This allows the cost of going from one region to another to be precisely de.ned, which is important 
to obtain good policies. We show how this general technique can be applied to a mobile robotics planning 
problem. Permission to make digital or hard copies of part or all of this work or Permission to make 
digital or hard copies of all or part of this work for personal or classroom use is granted without fee 
provided that copies are personal or classroom use is granted without fee provided that copies are not 
made or distributed for profit or commercial advantage and that copies not made or distributed for pro.t 
or commercial advantage and that copies bear this notice and the full citation on the first page. To 
copy otherwise, to bear this notice and the full citation on the .rst page. To copy otherwise, to republish, 
to post on servers, or to redistribute to lists, requires prior republish, to post on servers or to redistribute 
to lists, requires prior speci.c specific permission and/or a fee. permission and/or a fee. SAC 2001, 
Las Vegas, NV SAC 2001, Las Vegas, NV &#38;#169; 2001 ACM 1-58113-324-3/01/02...$5.00 &#38;#169; 2001 
ACM 1-58113-287-5/01/02 $5.00 2. MARKOV DECISION PROCESSES A Markov Decision Process models an agent 
which interacts with its environment, taking as input the states of the en­vironment and generating actions 
as outputs. An MDP can be formally de.ned as follows: S is a .nite set of states of the environment; 
 A is a .nite set of actions;  T : S×A×S . [0, 1] is the state transition function, which encodes the 
probabilistic e.ects of actions;  R : S. IR is the reward function. It is mainly used to design the 
goal the agent has to reach.  This model is very appropriate to compute policies in un­certain environments. 
As the transition function represents uncertainty about action execution, policies computed with this 
model are more robust than those computed with tra­ditional planning techniques. Within the MDP framework, 
an optimal policy p de.nes an optimal action to be taken for each state of S. The optimal action for 
a state is the one which maximizes the expected value of this state. To compute these values and the 
optimal actions, two algorithms are usually used: Value Iteration [1] and Policy Iteration [5]. Using 
the latter, the value of a state is de.ned as follows: '' Vp(s)= R(s)+ . L T (s, p(s),s)Vp(s) (1) s'.S 
Basically, the value of a state is given by the reward gained for being in this state, to which are added 
the values of connected states, discounted by the . factor and the transi­tion probabilities. Policy 
Iteration consists of two steps: an evaluation step (solving the linear equations to compute the value 
of the current policy, which needs |S|3 operations), and an improvement step (looking for an action to 
improve the value in every state, which is done in |A||S|2). Starting from any initial policy, the algorithm 
iteratively repeats these two steps until the optimal policy is found. The number of it­erations needed 
to converge is quite di.cult to determine. [7] shows that this depends mainly on |S|, |A| and .. 3. 
DECOMPOSITION 3.1 Introduction As we have stated in previous section, the complexity of al­gorithms used 
to compute optimal policies highly depends on the number of states in the MDP. As many applica­tions 
need a large amount of states, these algorithms are intractable for real-time problems. As a consequence, 
much research focuses on approximation techniques, trying to re­duce the search space to obtain near-to-optimal 
policies in a reduced computing time [2]. Decomposition techniques try to solve the global MDP by solving 
small local MDPs [3]. To illustrate how an MDP can be decomposed, we use the exam­ple on Figure 1. This 
shows a weakly-coupled MDP: in such an MDP, states can be partitioned into regions, and only a reduced 
number of states connects one region to another. Decomposition techniques are usually applied to these 
kinds of MDPs; this is also the case in our approach. In the left part of the .gure is represented the 
MDP with all transitions (actions generating transitions are deliberately omitted). In the right part 
is shown the optimal policy for reaching the goal, which is the state labelled with a G . When decom­posing 
an MDP, the problem is mainly to .nd a good partition of the state space and to determine how regions 
communicate. We propose to solve these problems using a graph to represent the MDP. But before introducing 
our approach, we .rst review related work. Figure 1: An MDP and its optimal policy. 3.2 Related Work 
Decomposition techniques for planning in stochastic do­mains have been introduced by Dean and Lin [3]. 
They give a general framework to decompose MDPs into smaller ones. Assuming that an expert has given 
a partition of the state space into regions, a policy is independently computed for each region. These 
local solutions are pieced together to obtain a global solution. The links between regions are represented 
using periphery states (dark-.lled circles in Fig­ure 1). Periphery states of a region r are the ones 
(outside of r) reachable when executing a single action starting in r. These states are used as goals 
of the local MDP associated to region r. The authors show that if these states values are equal to the 
ones given by the optimal policy of the global MDP, then piecing together the local solutions will lead 
to the optimal solution. Of course, these values can not be determined a priori, so two algorithms are 
proposed to compute them. The .rst one, called Hierarchical Policy Construction, solves an abstract MDP 
(each region of the global MDP becomes an abstract state). For each pair of adjacent regions, a policy 
is computed going from one region to the other. These policies are then used as actions for the abstract 
MDP. This algorithm .nds for each region the op­timal local policy, i.e. the optimal region to reach. 
To take into account this new action space, reward and transition functions are transformed. Solutions 
are generally approx­ime: as the optimal action of a region is a policy going to another region, each 
base state of a region leads to the same region, which is not realistic. For instance, on the optimal 
policy of Figure 1, some states of the top-left region are reaching the bottom region, whereas some others 
are reach­ing the right one. To solve this problem, a second algorithm is proposed: Iterative Approximation 
Approach. The goal of this algorithm is to iteratively approach the real value of the periphery states. 
To achieve that, several policies are computed for each region, which may reduce the speed-up provided 
by decomposing the MDP, but this approach con­verges to the optimal policy. Closely related to Hierarchical 
Policy Construction, two ap­proaches have been proposed in [9] and [4]. They also solve an abstract MDP 
composed of one abstract state per re­gion, but many policies are computed in each region. These policies, 
called macro-actions, are used as high-level actions for the abstract MDP. To ensure the high quality 
of the so­lution, many macro-actions have to be computed in each region. As the transition and reward 
functions have to be transformed to cope with these macro-actions, considering too many macro-actions 
can be slow. But to o.set this draw­back, [9] proposes several methods to compute only a small set of 
macro-actions per region, without loss of optimality.  4. OUR APPROACH 4.1 Basic ideas Our approach 
aims to e.ciently solve weakly-coupled MDPs. Compared to the work described above, it is quite di.erent: 
 Only one policy is computed in each region. As stated earlier, computing many policies can be time consum­ing 
especially when new transition and reward func­tions have to be computed for each local policy.  Base 
reward and transition functions are used, for the same reason.  To approximate periphery state values, 
we use a heuristically-valuated graph.  To solve an MDP using decomposition techniques, the main problem 
to solve is: for each region, .nd the optimal next region, i.e. the region which is on the optimal path 
to the goal. We show in this section how this problem can be solved using a graph. To illustrate this, 
the same example as above is used. Figure 2 is a representation of the MDP of Figure 1, only containing 
periphery states. In this .gure, solid lines represent direct transitions, whereas dotted lines represent 
paths between two distant but linked states. As in Figure 1, the right part of the .gure represents the 
optimal policy. By Figure 2: Periphery states of MDP of Figure 1 representing only periphery states, 
it is clear that our MDP can be cast into a graph. In the left part of the .gure is the initial graph, 
including every possible transition (arcs) between periphery states (vertices). The right part of the 
.gure can be viewed as the solution graph: for each vertex, the optimal arc remains, i.e. the one which 
leads to the optimal next vertex. The goal of our approach is to .nd this graph, of course without knowing 
the optimal policy. Once the graph has been computed, it is very simple to solve the MDP. The region 
to reach starting from an initial region is given by the arc leaving the vertex representing this region. 
The optimal policy of a region is computed using a local MDP containing every state of this region, to 
which are added the periphery states. These states are the goals of the local MDP. As in related approaches 
presented above, the crucial point is to assign values to these states. To achieve that, values are assigned 
to each vertex of the graph. These values are computed using a classical Dijkstra algorithm. First, a 
0 value is given to the goal vertex, the value of its neighbours are then computed, and the process iterates 
until every vertex has its value. The key problem is now to compute the value of a vertex as a function 
of its neighbours. Executing an action from an initial state could lead to various states according to 
various probabilities. All these states are used to compute the value of the initial state. As explicitly 
computing these values would be time­consuming, we need a heuristic valuation of the states. This heuristic 
formula is application-dependent, and we will give an application of our approach on a motion planning 
problem in the next section. But before that, here is a sum­mary of the basic ideas of our approach: 
1. In each region, create a vertex for each periphery state; 2. Find all transitions between these states 
and create an arc between corresponding vertices; 3. Using Dijkstra s algorithm and a heuristic valuation 
formula, assign a value to each vertex; 4. Create and solve a local MDP for each region, using periphery 
states as goals; 5. Piece together local policies to obtain a policy for the global MDP.  In relation 
to other work, the originality of our approach is given by point 3. Optimality of the global solution 
is not guaranteed: it is of course highly dependent on the heuristic valuation formula. We show on an 
example in the next section that this permits nevertheless near-optimal policies to be obtained in a 
considerably reduced computing time.  4.2 Application to motion planning In this section is shown how 
our approach can be used to .nd near-optimal policies in a motion planning application. Placed in a known 
but uncertain environment, a robot has to reach a prede.ned goal. We are interested in o.ce-like environments, 
composed of corridors, o.ces and intersec­tions. Before explaining how the graph is built, we have to 
clarify how we represent such environments using an MDP, which is done in next section. 4.2.1 Model Used 
Without going into details, our model is similar to [2]. The environment is represented using a grid 
of 20-inch squares; in each square, our robot can be oriented along four direc­tions (North, South, East 
and West), thus four states are necessary to represent one square of the grid. Three actions have been 
chosen: go forward to the next square, quarter turn to the left and quarter turn to the right. We use 
a discounting factor . =0.99. The reward function is de.ned in a very simple way: if the robot is in 
the goal state, it gets a reward of 0, and it gets -1 in all the other states. The goal and obstacles 
are represented as absorbing states (once the robot is in this state, it can not go out of it). That 
makes the value of an obstructed state s highly negative: V (s)= -1/1 - .. This value is very important 
as this is the worst value a state may have; in the following of the paper, we will call it Vo. Thus 
states take their value in [Vo, 0]. To represent actuator uncertainty, we use a learned transition function: 
a Go Forward action succeeds 57% of time, while 43% of the time the robot moves to one of ten other possible 
adjacent states; turning actions succeed 85% of time. 4.2.2 State Space Partition Before showing how 
the graph can be valuated, we have to show that such environments can be cast into weakly­coupled MDPs. 
Here we use a characteristic of our applica­tion: a policy can be formulated as a sequence of intersec­tions 
to reach. So when the robot is in a corridor, it will have to reach one of the adjacent intersections. 
An intersection is de.ned as a place connecting one corridor to another, or a corridor to an o.ce. We 
have made the assumption that an o.ce is just a special case of corridor: whether the robot is in an 
o.ce or in a corridor, its goal will be to reach an intersection. In Figure 3 is shown a very simple 
environment which leads to a partition of 8 regions. Each corridor be­comes a region, except for the 
corridor containing the goal to reach (state labelled with a G ), which is partitioned into two distinct 
regions (the goal location is considered as an intersection). This is an intuitive partition as only 
a few states connect a corridor to another. This partition shows Figure 3: Example environment also 
an important characteristic of policies, which is not al­ways taken into account in other decomposition 
techniques: within a region, the next region may vary, so there is not always a unique next region to 
reach. As shown in Figure 3, if it is clear that all states of R3 will lead to R1, this is not true for 
region R8. In this region, the optimal policy will either lead to the south (in the bottom part of the 
region), or to the north (in the upper part of the region). Our graph representation will allow such 
policies to be approximated. To decide which regions to reach from a particular region, each intersection 
is given a value, which represents the at­traction of a region over its adjacent regions. We show how 
this is achieved in the next section.  4.2.3 Heuristic Valuation Formula To de.ne the attraction of 
a region over its neighbours, a value is assigned to each vertex of the graph. This value is dynamically 
computed, adding to the value of the next ver­tex the cost of reaching it. The next vertex to reach is 
the lowest valuated one, as given by Dijkstra s algorithm. The key problem is to .nd the cost of going 
from one vertex to an­other, i.e. crossing a corridor. Figure 4 is used to illustrate that. What is to 
be computed is the cost of going from the Xn position to the goal (state with a G ). On the left part 
of the .gure is shown how this cost is computed by classical algorithms, giving a value to the states 
composing position Figure 4: Graph valuation. Xn. These state values are computed using adjacent state 
values. As each state has several reachable neighbours, a system of linear equations has to be computed. 
To avoid that, we use a heuristic valuation of states, as shown in the right part of the .gure. First, 
we consider that whatever the corridor width, only a single abstract state is necessary to represent 
a part of corridor. Second, only two kinds of transitions are considered from these abstract states: 
1. the robot has reached a free state, i.e. a state without any obstacle. In this case, we make the assumption 
that the robot has reached the next state on the way to the goal. 2. the robot has touched an obstacle 
or a wall. As such states have highly negative values (see Section 4.2.1), possible collisions have a 
negative in.uence on adja­cent states values.  The risk of collision from an abstract state Xn is computed 
using an average of transition probabilities to obstructed states s ' (states containing obstacles), 
as follows: ' LL T (s, Go Forward,s ) s.Xn s ' .reach obs(s) rsk(Xn)= |Xn| reach obs(s) is the set of 
obstructed states reachable exe­cuting a Go Forward action in state s. Then the value of a state Xn is 
computed as follows: Vg(Xn)= R(Xn)+..[rsk(Xn)×Vo+(1-rsk(Xn))×Vg(Xn-1)] This valuation formula is very 
similar to Equation 1, but is much simpler to use as it does not require solving a sys­tem of equations. 
To assign a value to a vertex v1 of a graph, we just have to recursively compute the values of states 
Xn,Xn-1, ..., X0 placed between vertex v1 and its next best vertex, given by Dijkstra s algorithm. As 
an ex­ample, the graph obtained for our small example of Figure 3 is given in Figure 5. In this .gure 
are given the arc values using our heuristic valuation. Vertex values are computed by summing arc values 
along the best path to the goal. The plain-line arc between vertex 3 and vertex 1 means that the best 
neighbour of vertex 3 is vertex 1. At the opposite, the dashed-line arc between vertices 6 and 5 means 
that there is a link between these two vertices, but this arc is not a part of a best way to the goal. 
 4.2.4 Local MDPs construction Once the graph has been computed, a local MDP is built for each region, 
i.e. each arc of the graph. As shown in Figure 5, a plain-line arc corresponds to a single-goal MDP. 
In this region, each action leads to the same next region, so there is no need for several goals. For 
instance, arc 3 . 1 and some other actions lead to another next region r ' . So two-goal MDPs are built 
for this kind of region. Such an MDP is built for region R4, between vertex 4 and vertex 2. The values 
of these vertices are used to valuate the goals of this MDP, representing the attraction of regions R2 
and R5 over R4. Once these small MDPs have been built and solved using Policy Iteration, a simple combination 
is used to obtain a solution for the global environment.  4.2.5 Results Results we present are based 
on two big environments con­taining respectively 3360 and 1640 states. The .rst one is a classical environment 
composed of corridors, in which the goal has been randomly placed in 20 di.erent locations. To test the 
accuracy of our approach as a function of the number of obstacles in the environment, we have created 
10 instances of this environment, the .rst one containing all obstacles (191), the last one totally free 
of obstacles. As a consequence, results presented in Table 1 are an average over 200 problems. To evaluate 
our approximate policies qual­ity, we compare their value to the corresponding optimal policy. To give 
a better idea of our approach s performance, we also give results of shortest path policies, i.e. policies 
using distance to the goal as optimality criterion. For the .rst environment, results are very good: 
on average our ap­proach gives a policy which is more than 95% of the optimal policy, whereas a shortest 
path policy has a quality of only 65% of the optimal. And while the shortest path policy may be very 
bad (only 26% of the optimal solution), the worst policy given by our approach is still more than 86% 
of the optimal solution. As we have made the assumption that an o.ce is just a special case of corridor, 
a second environment (composed of o.ces) has been designed. Results are similar to those of Table 1: 
the average quality is nearly 97%. Not only is the quality of our method close to optimal, the computing 
times are signi.cantly reduced compared to .nd­ing the optimal policy, as shown in Table 2. The .rst 
col­umn is the total time needed by our approach for building Our approach Optimal policy Environment 
1 29 1510 Environment 2 5 134 Table 2: Sequential computing times (in seconds). the graph, solving the 
MDPs and combining the local solu­tion. The second column is the time needed to obtain the optimal policy 
using Policy Iteration, starting from a short­est path policy. These results are very satisfying as for 
the .rst environment the computing time has been divided by 52, and by 26 for the second one.   5. 
PARALLELIZATION 5.1 Technical aspects We present in this section preliminary parallelization work. This 
is not the best solution, but this one did only neces­sitate small changes in our application. Our parallel 
ma­chine is a 64 processors Origin2000 (SGI). This is an MIMD machine (Multiple Instructions Multiple 
Data), using cc-NUMA (cache coherent Non-Uniform Memory Access) Dis­tributed Shared Memory [10]. On these 
machines, data can be shared among every processor, and each processor can execute di.erent tasks. Our 
approach has the following characteristics: (1) a lot of local MDPs are sequentially solved; (2) these 
MDPs have di.erent sizes, so policies computing times may vary; (3) each local MDP is independent. The 
.rst steps of our ap­proach, until the graph is built and valuated, are executed sequentially. Then each 
pair of adjacent vertices is sent to the parallel part of our tool, which executes the following tasks: 
local MDP building, policy computing on the local MDP and transfer of this solution into the global policy. 
As each local MDP is independent, there is no need for synchro­nization. Each processor works on its 
own data, and global variables are only used as inputs. Our .rst idea was to create a thread for each 
local MDP to be solved. This is possible because there are more processors than local MDPs. This method 
can improve computing times, but as local MDPs can have various sizes, we can not hope to divide comput­ing 
time by n when using n processors. Some processors, solving large local MDPs achieve more work than others, 
which are solving small MDPs. To improve load balancing, we have designed a second parallel version of 
our tool, using a workpool. Each MDP is placed in a queue, and each time a processor has solved a local 
MDP, it takes another MDP in the queue to solve it. The goal of this approach is not to reduce computing 
times, but to use only a small number of processors to obtain the same results. 5.2 Results on our application 
Using the .rst approach (each MDP is treated by a pro­cessor), computing time has been approximately 
divided by 4 on our two test environments. We used respectively 32 (environment 1) and 19 (environment 
2) processors (one processor for the main program and one processor per lo­cal MDP). Dividing computing 
times by 4 when using 32 processors may be disappointing, but it was not a surprise, as local MDPs sizes 
are heterogeneous. Using a workpool, we obtained the same computing times, only using a small number 
of processors. Figure 6 shows the computing times we obtain as a function of the number of processors 
used. On our two test environments, 8 processors were su.cient to obtain the same results as before. 
Of course this approach Figure 6: Speed-up w.r.t. the number of processors. could be improved. We have 
still done no e.ort to provide similar size local MDPs. We could do that while building the graph, by 
adding vertices in order to divide big regions into smaller ones. This should accelerate computing times, 
as no processor would achieve more work than another.  6. CONCLUSION When using decomposition for solving 
MDPs, it is very im­portant to accurately de.ne the cost of going from one re­gion to another. Our approach, 
using a directed graph to achieve that, is quite general as it can be applied to all kinds of weakly-coupled 
MDPs. The only application-dependent part of it is the heuristic valuation formula. Results pre­sented 
show that policies are near-optimal, and are com­puted very quickly. 7. REFERENCES <RefA>[1] R. Bellman. Dynamic 
Programming. Princeton University Press, 1957. [2] T. Dean, L. Kaelbling, J. Kirman, and A. Nicholson. 
Planning under time constraints in stochastic domains. Arti.cial Intelligence, 76(1-2):35 74, 1995. [3] 
T. Dean and S.-H. Lin. Decomposition techniques for planning in stochastic domains. In Proceedings of 
International Joint Conference on Arti.cial Intelligence, 1995. [4] M. Hauskrecht, N. Meuleau, L. P. 
Kaelbling, T. Dean, and C. Boutilier. Hierarchical solution of markov decision processes using macro-actions. 
In Proceedings of Uncertainty in Arti.cial Intelligence, 1998. [5] R. A. Howard. Dynamic Programming 
and Markov Processes. MIT Press, Cambridge (Mass), 1960. [6] L. P. Kaelbling, M. L. Littman, and A. 
R. Cassandra. Planning and acting in partially observable stochastic domains. Arti.cial Intelligence, 
101(1-2):99 134, 1998. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling. On the complexity of solving 
markov decision problems. Proceedings of the 11th Conference on Uncertainty in Arti.cial Intelligence, 
1995. [8] C. H. Papadimitriou and J. N. Tsitsiklis. The complexity of markov decision processes. Mathematics 
of Operations Research, 12(3):441 450, August 1987. [9] R. Parr. Flexible decomposition algorithms for 
weakly coupled markov decision problems. In Proceedings of Uncertainty in Arti.cial Intelligence, 1998. 
[10] J. Proti´c, M. Tomasevi´c, and V. Milutinovi´c. Distributed shared memory: concepts and systems. 
IEEE Parallel &#38; Distributed Technology, 1996.</RefA>  
			
