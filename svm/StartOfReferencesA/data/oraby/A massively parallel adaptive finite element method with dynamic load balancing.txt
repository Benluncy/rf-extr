
 A Massively Parallel Adaptive Finite Element Method with Dynamic Load Balancing ~hm ~ ~accabe2,3 Karen 
D. Devinel 12and Joseph E. Flahertyl Stephen R. Wheatl Department of Computer Science Massively Parallel 
Computing Department of Computer Science Rensselaer Polytechnic Institute Research Laboratory, Dept. 
1424 The University of New Mexico Troy, NY 12180 Sandia National Laboratories Albuquerque, NM 87131 kddevin@cs.sandia.gov 
Albuquerque, NM 87185-5800 maccabe@cs.unm.edu flaherje@cs.rpi.edu srwheat@cs.sandi a.gov Abstract We 
construct massively parallel adaptive jlnite element methods for the solution of hyperbolic conservation 
laws. Spatial discretization is performed by a discontinuous Galerkin jinite element method using a basis 
of piecewise Legendre polynomials. Temporal discretization utilizes a Runge-Kutta method. Dissipative 
jkxes and projection limiting prevent oscillations near solution discontinuities. The resulting method 
is of high order and may be parallelized efficiently on MIMD computers. We demonstrate parallel eficiency 
through computations on a 1024-processor nCUBE12 hypercube. We present results using adaptive p-refinement 
to reduce the computational cost of the method, and tiling, a dynamic, element-based data migration system 
that maintains global load balance of the adaptive method by overlapping neighborhoods of processors 
that each perform local balancing. 1. Introduction We are studying massively parallel adaptive finite 
element methods for solving systems of d-dimensional hyperbolic conservation laws of the form ,+&#38;i,u)x, 
= O,xE!a.t>o* (la) i=l subject to the initial conditions U(xj o) = UO(X), XE L2uaf2, (lb) and appropriate 
well-posed boundary conditions on ilQ. High-order methods and the combination of mesh refinement and 
order variation ( hp -refinement) have been shown to produce effective solution techniques for elliptic 
[16] and parabolic [1, 2,3] problems. It is, thus, natural to 1 This work was partially supported by 
Sandia Nationsl Laboratories under Research Agreement #67-8709. 2 This work was performed at Sandla National 
Laboratories, operated for the U.S. Department of Energy under contract #DE-ACCM-76DPO0789. 3 On Faculty 
Sabbatical to Sandla Nationsl Laboratories. @1993 ACM 0-8186-4340493/0011 $1.50 determine whether or 
not they will be as efilcient when applied to hyperbolic systems. High-order Total Variation Diminishing 
(TVD) [19. 211 and Essentially Non-Oscillatory (IWO) [17] finite difference schemes for (1) use a computational 
stencil that enlarges with order. The wide stencil inhibits efilcient implementation near irregular 
boundaries and complicates processor communication on massively parallel computers. Finite element methods, 
however, can easily model problems having complex geometries and have stencils that are invariant with 
method order. With a motivation to explo~ adaptive high-order parallel methods, we use a local discontinuous 
Grtlerkin method [8, 9, 10] rather than the traditional ftite element formulation. Spatially, the solution 
is approximated by a basis of piecewise continuous Legendre polynomials that may have discontinuities 
at interelement boundaries. By not enforcing global discontinuous solutions element boundaries approximate 
Riemann applied to keep the continuity, we can approximate of (1) more accurately. Fluxes at are computed 
by solving an problem with a projection limiter average solution monotone near discontinuities [21]. 
An adaptive limiting procedure maintains high-order accuracy near smooth extrema while improving global 
monotonicity near discontiuuities Aative to other techniques [7, 10]. Time discretization is performed 
by an explicit Runge-Kutta method. Nearly perfect scaled parallel speed-up [11] on an nCUBE/2 hypercube 
degrades substantially when adaptive p-=fmement is incorporated into the local finite element method 
due to processor load imbalance. Parallel ftite element methods often use static load balancing [12, 
13] as a precursor to obtaining a ftite methods, however, require adjust changing processor proceeds. 
In our work, we fine-grained, element-based called tiling that maintains overlapping neighborhoods perform 
local load balancing. element solution. Adaptive dynamic load balancing to loads as the computation have 
developed a dynamic, data migration algorithm global load balance by of processors that each The tiling 
system supports a Pemmion to CODVwIthOut fee all or D@ of this malerid is mankd. provided that the &#38;les 
are no! m?de or his!ributed fm dkct conkrercial advantage. the ACM copyright nctice and the titfe of 
the fubtication and w date appear, and notice is given that copying is by permission of the Aswciahon 
for Computing Machhwy. To copy othenvise, or to republish, large class of ftite element and ftite difference 
In regions where the solution of (1) is smooth, the applications. Its effectiveness has been demonstrated 
for scheme (3, 4) produces the O(A.# + 1,, standard ftite difference and ftite element methods Ax= max 
Axj, whose geometries and boundary conditions create load j = 1,2, ....J imbalance [22]. We incorporate 
the adaptive p -mfmement method into the tiling system to recover the parallel convergence expected for 
a p h-degree approximation eftlciency lost to load imbalance during order enrichment, [10]. When p >0, 
projection limiting is needed to prevent and demonstrate the tiling system s effectiveness with spurious 
oscillations near solution discontinuities. With several experiments on an nCUBE/2. projection limiting, 
the solution Uj(&#38; t), j = 1,2, . . .. J, is restricted after each Runge-Kutta stage to eliminate 
2. The Discontinuous Galerkin Method oscillations. Cockburn and Shu [10] describe a procedure To simplify 
the presentation, consider a for the projection limiting of scalar problems that prevents one-dimensional 
(d = 1) system of conservation laws (l), U}&#38; t) from taking values outside of the range spanned by 
and partition G? into subintervals (xj_ ~, Xj) , the neighboring solution averages. While preserving 
monotonicity of the average numerical solution, this Y. Construct a weak form of the problem by muhiplying 
(la) by a test function v 6 L2(.xj -1, .xj) md limiting scheme flattens solutions near smooth extrema 
so integrating the result on (xj _ ~, Xj) while integrating the that fust-order accuracy is obtained 
there. To overcome flux term by pSrtS to obtain this deficiency, we developed a limiting scheme [7] that 
maintains monotonicity of solution moments on ~ = 1)2)...! Xj Xj J neighboring elements. Using the orthogonality 
properties vTuak+v~f(u) 1; ~-J V;f(u)dx = o, (2) of Legendre polynomials and (3), solution moments of 
a% scalar problem are given by j-1 x.J-l 1 for all v E L2 (xj _ ~, Xj) . Use a linear transformation 
to map (xj _ 1, xj) onto a cm20nical element -1< ~ <1 and approximate u(<, t) 6 L (-1, 1) by the pth 
-degree polynomial Uj(~$ t) expressed in terms of a basis of k=o, l ,...,l ,1, j=l,2, . ..l.l. Legendre 
polynomials as Thus, to keep the I?h moment monotone, we keep Cjk monotone on neighboring elements by 
limiting Cj, k+ ~ as U(!g, t) -Uj(gj t) = ~ Cjk(t)pk(t). < E (-1,1,. (3) k=O (2k+l)cj, k+1= Substituting 
the polynomial approximation (3) into (2), minmod( (2k+ l)cj, k+l . (5b)SekXtiUg V to be pI OpOItiOIld 
tO Pk (<) , and Using the orthogonality properties of Legendre polynomials [20], we k -Cj, pcj, k-c. 
k ). Cj+l, J-l, determi.necjk, j = 1,2, . . ..k. k = 0,1, . . ..P. N where 2k + 1 d - (f(uj(l, t)) -(-1) 
kf(uj(-l, t)) minmod (a, b, c) =ZCjk = AXj sgn(a)min (lal, lbl, lcl). (SC)+ (t)f(uj)d~ ) . 4) if sgn(a) 
= sgn(b) = sgn(c), -1 O, otherwise. { The Cjk are initialized by L2 projection of the initial data (lb) 
onto the space of Legendre polynomials. Integral The limiter (5) is applied adaptively. First, the terms 
in (4) are evaluated exactly for linear problems, using highest-order coefficient is limited. Then the 
limiter is the properties of Legendre polynomials [20], or applied to successively lower-order coefilcients 
when the numerically using (p+ 1) /2-point Gauss-Legendre next higher coefficient on the interval has 
been changed by quadrature. The boundary flux f (Uj ( 1, t) ) is the limiting. In this way, the limiting 
is applied only where approximated by a numerical flux function it is needed, and accuracy is retained 
in smooth regions. h (Uj(l, t), U.+ 1(-1,0) [10, 19]. Runge-Kutta For vector systems, the scalar limiting 
(5) can be applied integration o~order p is used for temporal integration. component-wis~ however, Cockburn 
et al. [9] showed that this approach does not have a total variational bounded ~) theory and can produce 
spurious oscillations even for linear problems. To improve accuracy at the price of additional computation, 
we apply the limiter to the characteristic fields of the system [9, 141. The diagonalizing matrices T 
(u) and T-l (u) (consisting of the right and left eigenvectors of the Jacobian f ) are evaluated using 
the average values Coj OP u., ~ = 1)2,...! J. The scalar limiter (5) is apphed to eaci component of the 
characteristic vector. The result is projected back to the physical space by post-multiplication by T-l 
(Uj) . The two-dimensional method is a direct extension of the one-dimensional method. Restricting Q 
to be rectangular, partition it into rectangular elements i = lj2, ....l. j = 1,2, ....J. Representing 
U(X, y, t) on Clij by a basis of tensor products of I..egendre polynomials on the canonical element = 
{(g, n)]-l~g$q~l}! c and constructing a weak form of (1) yields a system of ODES similar to(4) for c~km. 
Following Biswas et rd. [7], we apply the one-dimensional projection limiter (5) along each of the two 
spatial directions. Experimental results [6, 7] support this approati, however, a more rigorous analysis 
is necessary. Example 1. Consider the periodic initial value problem for Burgers equation U2 Ut+ (T) 
= O,t>o, (7a) x (X) ;+;SWCX). /.4°= (7b) Solutions of(7) at time t = 1.1, obtained on a 32-element mesh 
for p = O, 1, and 2 using an upwind numerical flux and moment limiting (5), are shown in Figure 1. The 
improved solution accuracy when p is increased from O to 2 can easily be seen. With p = 2, the limiter 
(5) maintained third-order accuracy in smooth regions of the solution (see Table 1), produced a sharp 
shock, and preserved average as well as global monotonicity on all but one subintexval. Example 2. The 
one-dimensional Euler equations of gas dynamics can be written in the form (l), with u (x, t) = [p, m, 
e] , and 1 0.4-1 I ­ - 1 -0.5 0 0:5 - 1 -0.5 0 0:5 -.1 -0.5 0 0:5 Figure 1. Exact (line) and numerical 
(M) solutions of Exam Ie 1 for p=O, 1, and 2 using the moment limiter r5). (Finite element solutions 
are shown at eleven points per element.) f(u(x, t)) = [pq, w+p, eq+f ql , where p, P, m, q, and e are 
the density, pressure, momentum (m = pq), velocity, and energy, respectively. The system is completed 
with the ideal equation of state P = (y-1) (e-~p 2 29) where y is the ratio of spectilc heats, taken 
here as 1,4, We consider Sod s shock tuke Riemann problem [18] Number of Elements Error in L1 Norm Order 
32 2.39585e-05 64 1.64509e-06 3.86 128 1.68787e-07 3.28 256 1.79387e-08 3.23 512 1.9o090e-09 3.23 Table 
1. Convergence of (7) with p= 2 and limiter (5) in smooth regions of Example 1. Error was measured for 
xc (-1,-0.5675) u (-0.375,1) using (8). [l, o,l]~, ifx<o.5, [po,go Po]T= 9 { [0.125,0,0.1]T, if x>O.5, 
 which we solve using a piecewise quadratic approximation (p = 2) on a 64-element mesh. In Figure 2, 
we show the density, pmssme, and velocity at time t = 0.1 using limiter (5). The solution method sharply 
captures shccks, contact discoutinuities and expansions. The high-order coefficients are determined to 
preserve average and, to a large extent, global monotonicity. 3. Adaptive p-Refinement We have developed 
an adaptive p-refinement version of the two-dimensional method using a method-of-lines approach. A spatial 
error estimate is used to control order variation procedures that attempt to keep the global L 1-error 
U(o, o,t)-u(o, o,t) ~= JJ Iu(x, y,t) -U(x, y,t)ldxdy 8) w less than a specified tolerance by maintaining 
tolerance Eij(t) sTOL = ~J , (9) i=l,2, . . ..I. j=l,2, . . ..Y. where Eij is the maximum local L1 -error 
estimate of the solution vector on element Q... We initialize U.., i= 1,2,...,1, j= 1,2, . . .. J, ~~o 
the lowest-degr~ polynomial satisfying QI UO(X,y) -Uij(@), q(y), O) dydx <TOL. (10) ij co  After each 
time step, we compute Eij, i = 1,2, ....1, ~=l,2J.J J, and increase the polynomial degree of U.. by one 
if Eij > TOL. The solution Ui. and the err~i r estimate are recomputed on emiched e ements and the density 
1 0.8­ 0.6­ 0.4­ 0.2­ 4 0 0.5   0.8­ 0.6­ 0.4­ 0.2­ 0 0 05 pressure 1 08­   0.6­ 0.4­ 02­ F;gure 
2. Density, preO&#38;re, and velocity a; f=O.1for Exam~le 2with D =2usina the moment limiter. (Finite 
element solutions ;re shown at eleven points per element.) degree is increased until Eijs TOL on all 
elements when the time step is accepted. For these experiments, we use a p-refinement spatial error estimate: 
1 1 up+ 1(<, q, t) -Ufi(&#38; q, t) 44 Ei{t) = ij (11)JJI -1-1 w where Ufl. is the p h -degr~ approximation 
of u. while this estimate is computationally expensive, it is still less expensive than mesh-refmement 
techniques and can be u~d to reduce the effort involved in recomputing Uij ~d its error estimate when 
enrichment is needed. Thus, instead of recomputing Uij ( t+ At) when a higher-order .. needed, set ~p~tx+mA~~m= 
U?.+ 1 ( tlg At) , initialize the+~ew error es%ate U?.+ 2(t/Jat time t,and compute U; over the timestep.&#38;erlessexpensiveerrorestimationprOdws 
are described by Biswas et al. [71, and Bey and 0den[41. Additional computational savings are possible 
by predicting the degree of the approximation needed to satisfy the accuracy requirements during the 
next time step. After a time step is accepted, if Ei. > HmaxTOL, H ~ax E (O, 1], we increase the degree 
of fiij (t+ At) for the next time .@. If Ei. c HminTOL, Hmin c [0, 1), we decrease the degree of dij 
(t + At) for the next time step. Example 3. We solve Ut + 2UX+ 2U~ = O,o<x, y<l, t>o, (12a) by both fixed-order 
and adaptive p-refinement methods on Oc t< 0.1 with initial and Dirichlet boundary conditions specified 
so that the exact solution is U(x, y,t) s; (l-tarlh (20x-loy-2ot+5)), ~12b) Osx, ysl. In Figure 3, we 
show the exact solution of (12) at time t = O and the adaptive 16x 16-element mesh generated to satisfy 
the initial data for TOL = 1.0x 10-5. In F@.we 4, we show the global L1 -error versus the CPU time for 
fixed-order methods with p = O, 1, and 2 on 8 x 8, 16x 16, 32x 32, and 64 x64-element meshes, and the 
p-adaptive method with Hmax = 0.9, H. = 0.1, and local error tolerances TOL rauging from 5 !Jfo-g to 
5 x 10-4 on a 16 x 16*lement mesh. The p-adaptive method requires more computation than the freed-order 
methods for large error tolerances, but because of its faster convergence rate, it requires less work 
than the freed-order methods to obtain small errors. 4. Parallel Implementation The Discontinuous Galerkin 
method is well suited to parallelization on massively parallel computers. The computational stencil involves 
only nearest-neighbor communication ~gardless of the degree of the piecewise polynomial approximation 
and the spatial dimension. D p=o D p=l D p=2 D p=3 Figure 3. Exact solution of (12) at time t=Oand the 
adaptive prefinement initiai mesh generated for a iocsi error toierance 0.00001. Additional storage 
is needed for only one row of ghost elements along each edge of a processor s uniform subdomain. Thus, 
the size of the problems solved can be scaled easily with the number of processors. We meawne performance 
on an nCUBE/2 hypercube wmputer by considering the method s scaled parallel eflciency, the ratio of unipromssor 
execution time for a problem of size W to execution time on N processors for a problem of size NW [11]. 
Thus, the amount of work per processor is kept constant as processors am added, and we expect the solution 
time to be constant for each trial. In two dimensions, W = number of elements (13) x number of timesteps 
x (p+ 1)2. Example 4. In Table 2, we show the execution time for the twdimensional spatially periodic 
problem Ut+ux+u ~=o, t>o, (14a) Ixloo lx 10-1 1x 10-2 a E Ix 10-3 w lx 10- Ix 10-5 Adaptive p-refinement 
lxlo~ Ixloo 1X101 1.1+ 1X1O J 1X104 1: 5 CPU Time Figure 4. Convergence of the padaptive method compared 
with fixed-order methods for Example 3. u(X,y, o) = sin (7CX)sin (rcy) , (14b) using various numbers 
of processors and a standard method with p freed at 2. Each processor s subdomain contained 128 elements, 
and the problem was solved for 46 time steps. As indicated, the solution times increase only slightly 
with the dimension of the hypmube, demonstrating the high parallel efilciency of the basic method. We 
also show the ratio of the average execution time on all the processors to the maximum execution time 
among the processors. The average/maximum processor work ratio is above 0.98 for all hypercube dimensions 
due to the natural load balance of the standard method. 5. Dynamic Load Balancing via Tiling While the 
staudard method exhibhs near-perfect scaled parallel efilciency, processor load imbalances degrade the 
parallel performance of the adaptive p-refinement method. Non-uniform and changing processor work loads 
make dynamic load balancing necessary. Tiling is a modified version of a technique develo~d by Leiss 
and Reddy [15] which uses balances performed within overlapping prccessor neighborhoods to achieve a 
global load balance. Work is migrated from a processor to others within the same neighborhood. Leiss 
and Reddy define a neighborhood as a processor at the center of a circle of some predefmed radius and 
all other processors within the circle along the hardware interconnections. Each processor may be a neighborhood 
center and individual processors may belong to several neighborhoods. In tiling, a neighborhood is defined 
as a hmberof Work (W) ixecution Parallel AvgjMax locessors Time 3ff3cienc~ Processor (Sees.) Work Ratio 
52,992 268.96 1.000 ; 105,984 276.39 97.3% .998 4 211,968 276.77 97.2% .992 8 423,936 276.79 97.2% .998 
16 847,872 276.80 97.2% .997 32 1,695,744 276.80 97.2% .9% 64 3,391,488 276.80 97.2% .988 128 6,782,976 
276.84 97,2% .995 256 13,565,9S 276.80 97.2% .995 512 27,131,9(Y 276.80 97.2% .993 1024 j4,263,80t 276.80 
97.2% .995 Table 2. Scaled parallel efficiency for Example 4. Times were measured on an nCUBE/2. center 
processor and all processors that share subdomain boundaries with the center processor and, thus, require 
interprocessor communication with it in the application. Every processor is a neighborhood center. In 
F@re 5, we show examples of 12 processors in 12 neighborhoods using the Leiss/Reddy and tiling definitions. 
Processors within a given neighborhood are bald with respect to each other using local performance measurements. 
A processor s load depends on the number of elements in its local subdomain and the per-element processing 
cost. When each processor has the same number of elements and all elements have the same degree, as in 
Example 4, global load balance results; however, an imbalance occurs when one processor has a higher 
px-element load due, for example, to adaptive p -refwment. In Figure 6, we illustrate an example of the 
dynamic balancing provided by tiling. Without a priori knowledge, the data set is divided evenly among 
16 processors. After some pried, processors (0,1) and (3,2) are discovered to be more heavily utilized 
than their neighbors. At this time, processor (0,0) receives some of the data originally allocated to 
processor (0,1), and precessor (3,2) gives processor (3,3) some of its data, as shown in Step 1 of the 
figure. Processors (0,0) and (0,1) ate now equally balanced yet out of balance with other processors. 
Thus. in Step 2, some data are migrated from processor (O.1) to processor (1,1). The ripple effect continues 
to move through processors (2,1) and (3,1) in subsequent balancing steps. 6. The Tiling Algorithm Programs 
to be integrated into the tilii environment are partitioned into (i) a computation phase and (io a Figure 
5. ExamPles of 12 processors in 12 neighborhmds using the Leiss/Reddy (top) and the tiling definitions 
(bottom). balancing phase. The computation phase corresponds to the application s implementation without 
load balancing. Each processor operates on its local data, exchanges inter-processor boundary data, and 
processes the boundary data. A balancing phase follows a given number of computation phases in order 
to restore load balance. Each balancing phase consists of the following operatiou i. Determine work loads. 
Each processor determines its work load as the time to process its local data since the previous balancing 
phase less the time to exchange inter-processor boundary data during the computation phase. Neighborhood 
average work loads are also calculated. ii. Determine processor work requests. Each processor compares 
its work load to tke work load of the other processors in its neighborhood and determines which processors 
have greater work 0123 0123 0 0 1 1 2 2 3 3 Initial Step 1 0123 0123 0 0 1 1 2 2 33 H Step 2 Step 3 0123 
0123 0 0 1 1 2 2 33 m Step 4 Step 5 Figure 6. Migration Example loads than its own. If any are found, 
it selects the one with the greatest work load (ties are broken arbhrarily) and sends a request for work 
to that processor. Each processor may send only one work request but may receive several work requests. 
iii.Determiue which work requests to satisfy. Each processor prioritizes the work requests it receives 
in order of size and satisfies requests until its work load equals the neighborhood average. iv. Select 
export elements. WWn the exporting processors, each element is assigned an export priority, initially 
zero. The priority is incnmsed by 2 for each neighboring element in the importing processor, decreased 
by 1 for each neighboring element in its own processor, and decreased by 2 for each neighboring element 
in a foreign processor other thsn the importing processor (see the example in Figure 7). In this way. 
elements are peeled off the processor boundary in an attempt to pnwent the creation of narrow, deep holes 
in the tile. Priority determination is completely local through pointers within an element s data structure 
to neighboring and ghost cells. v. Notify and transfer elements. Importing processors and those processors 
with ghost cells for the migrating elements are notified. Importing processors allocate space for the 
incoming elements, and the elements are transferred.  Each processor knows how many computation phases 
to perform before entering the balancing phase. Synchronization guarantees that all processors will enter 
the balancing-. phase at the same time. Exportin g Processm m4z-W : ;~~~,,.,,,. ,.,.:::::,:::,:.: Non- 
Exportin g Proceaso ;ug+~~~ ;g=:u$~n~: $Uggg,; #z, ~34z$= Non- Figure 7. Element distribution and migration 
priorities (with the contributions from the north, east, south, and west neighboring elements, respectively) 
of exportable (shaded) elements before (top) and after (bottom) migration. Example 5. We solve (12) 
using the adaptive p-refinement method and tiling on a 32x 32-mesh using 16 prcxxssors of the nCUBlY2 
hypacube. In Figure 8, we show the processor domain decomposition after 10 time steps using tiling. The 
shaded elements have higher-degree approximations and thus, higher work loads. Initially, each processor 
is assigned au 8 x 8 ~lement subdomain. The tiling algorithm redistributes the work so that processors 
with high-order elements have fewer elements than those processors with low-order elements. The total 
processing time was reduced 25.9% from 29.49 seconds to 21.86 seconds by balancing once each time step. 
The average/maximum processor work ratio without balancing was0.353,and with balancing, it was 0.609. 
Dp=o Dp.z %lp=3 Hp=4 Figure 8. Domain decomposition after 10 time steps for Example 5 using adaptive 
pwefinement and tiling on 16 processors. Shaded squares are high-order element% dark lines represent 
processor subdomain boundaries. Example 6, We solve (12) for 225 time steps using all 1024 processors 
of the nCUBE/2 without balancing and with balancing once each time step, With balancing, the maximum 
computation time (exclusive of communication and balancing time) was reduced by 58.0% (see Table 3). 
The irregular subdomain boundaries created by the tiling algorithm increased the average communication 
time by 48.3%. Despite the extra communication time and the load balancing time, however, we see a 35.6% 
improvement in the total execution time. Without With Balancing Balancing Max. Execution Time 871.42 
SeCS. 561.62 Sf?CS. Max. Computation Time 801.95 S12CS. 336.49SeCS. Avg. Communication Time 68.54 SeCS. 
101.66 Sees. Max. Balancing Time 0.0 Sees. 38.55 KCS. Avg./Max. Work Ratio 0.392 0.934 Parallel IMicieney 
36.04% 55.95% Table 3. Performance comparison for Example 6 without and with load balancing each time 
step. In Figure 9, we show the unit standard deviation curves of the maximum computation time for each 
time step. Initially, the deviation is large, indicating that the processors are far horn a global balance. 
The deviations quickly become smaller, indicating that the processors rapidly approach balance. In Figwe 
10, we show the 5th, 35&#38;, median, 65*, and 95th-percentile processor loads, where the 95ti-percentile 
load, for example, is greater than or equal to 95% of the loads. The large negative slope of the 95ti-percentile 
curve indicates significant improvement in the load balance early in the program s execution. .­ 1.: 
Mean+SD 1. ~1.4 a c o 1. 0 al ~1.~ 1. 1.2 0 50 100 150 200 Time Steps Figure 9. Processor work ioad 
mean and standard deviation for each time step.   1.65 ~ ~} 95th percentile 1 1.3 F, 4 0 50 100 150 
200 Time Steps Figure 10. Sampied processor iosds for each time step. In Figure 11, we show the maximum 
processing costs per time step, including the computation and balancing times. The dashed and solid lines 
rep~sent the maximum cost per time step without and with balancing, nqectively. The balanced computation 
s maximum cost per time step is significantly lower than that without balancing. The spikes in both curves 
occur when the adaptive p -method s error tolerance was not satisfied during the time step, and the step 
had to be repeated using a higher-order approximation on the high-error elements. Since computation occurs 
on relatively few elements during this back tracking, the work load is extremely unbalanced. Balancing 
immediately after back tracking would remove too much work from those processors that had to repeat a 
step. The tiling algorithm ignores back tracking when determining work loads, avoiding thrashing whae 
elements are repeatedly assigned and ~moved from a processor s domain. In Figure 12, we show the cumulative 
maximum processing times with and without balancing. The immediate and sustained improvement of the application 
s performance is shown. r, i 5 t 4.5 : 4 : $ :3.5 . u :3 : 2.5 : 2; m !. ...... .._l o 50 100 150 ) 
Time Steps Figure 11. Maximum work ioad in each time step with (soiid) and without (dashed) baiancing. 
 800 . E.. Ee ## Without baiancing ~ / 600 . $ c a UI 200 . 1 0 50 100 150 200 Time Steps Figure 12. 
Cumulative maximum ioads with (soiid) and without (dashed) bsiancing. 7. Conclusion We have shown the 
viability of the discontinuous fiite element method for solving systems of hyperbolic conservation laws 
on massively parallel computers. By not enforcing continuity at inter-element boundaries, we can accurately 
model problems with discontinuities. A moment-based projection limiter eliminates oscillations near solution 
discontinuities while maintaining high-order accuracy near smooth extrema. Because of the compact stencil, 
the method can be parallelized on MJMD computers with scaled parallel efficiencies exceeding 90%. Adaptive 
p-refinement is used to solve problems to a prescribed accuracy with less computational expense than 
f~ed-order methods. Using tiling to migrate data between processors, we can recover the parallel efficiency 
lost to the adaptive p-xefmement method. The basic method (4) ~adily extends to non-rectangular geometries 
and unstructured meshes; however, it remains to determine whether the moment limiting (5) is likewise 
extensible. Inexpensive and, apparently, asymptotically correct error estimates of Biswas et al. [7] 
show promise, but need further examination and testing on rectangular and unstructured meshes, We plan 
to combine adaptive mesh (h -) refinement with p ,-refmement techniques to obtain an adaptive hp-refmement 
technique that can optimize computational effort in both smooth and discontinuous solution regions. The 
hp -method will present a serious challenge to the tiling algorithm, as the processor work loads and 
communication patterns are much mom complex. As seen in Example 6, back tracking when a time step is 
rejected is a very expensive and unbalanced computation. To Educe the amount of back tracking. we propose 
to use pattern matching ideas similar to those of Bieterman et al. [51 to pr~lct regions that will need 
enrichment during the next time step. The predicted mesh and order information can also be used to estimate 
processor work loads in the next time step, and deteet potential load imbalance. The tiling algorithm 
could migrate data prior to performing the actual computation, thus reducing load imbalance. 8. References 
<RefA>[1] Adjerid, S., and J. E. Flaherty. Second-Order Finite Element Approximations and a posteriori Error 
Estimation for Two-Dimensional Parabolic Systems. Numer. Math., 53 (1988), 183-198. [2] Adjerid, S., 
J. E. Flaherty, P. K. Moore, and Y. Wang. High-Order Adaptive Methods for Parabolic Systems. Physica 
D, 60 (1992),. 94-111. [3] Amey, D.C. and J. E. Flaherty. An Adaptive Local Mesh Refinement Method for 
Time-Dependent Partial Differential Equations. App. Num. Math., 5 (1989), 257-274. [4] Bey, K.S. and 
J.T. Oden. An A Posteriori Error Estimate for Hyperbolic Conservation Laws. in preparation. [51 Bieterman, 
M., J. Flaherty, and P. Moore. Adaptive Refinement Methods for Non-Linear Parabolic Partial Differential 
Equations. Accuracy Estimates and Adaptive Refinements in Finite Element Computations. L Babuska, et 
al., Ills. Wiley &#38; Sons, (1986) 339-358. [61 Biswaa, R. Parallel and Adaptive Methods for Hyperbolic 
Partial Differential Systems. Ph.D. Dissertation, Dept. Comp. Sci., RensselaerPolytechnic Institute, 
Troy, 1991. [7] Biswas, R., K. Devine, and J. Flaherty, Parallel, Adaptive Finite Element Methods for 
Conservation Laws. Applied Numerical Mathematics, (1993), to appear. [8] Ccxkburn, B., S. Hou, and C.-W. 
Shu. The Runge-Kutta Lccal Projection Discontinuous Galerkirr Finite Element Method for Conservation 
Laws N The Multidimensional Case. Math. Comp., 54 (1990), 545-581. [9] Cockburn, B., S.-Y. Lm, and C.-W. 
Shu. TVB Runge-Kutta Local Projection Discontinuous Galerkin Finite Element Method for Conservation Laws 
III: One-Dimensional Systems. Jrrd. of Comp. Phys., 84 (1989), %)-113, [10] Cockburn, B., and C.-W. Shu. 
TVB Runge-Kutta Local Projection Dkcontinuous Galerkin Finite Element Method for Conservation Laws II: 
General Framework. Math. Comp., 52 (1989), 411-435. [11]Gustafson, J., G. Montry, and R. Benner. Development 
of Parallel Methods for a 1024-Processor Hypercube. SIAM JrnL Sci. Stat. Comp. 9 (1988), 609-638. [12] 
Hammond, S. Mapping Unstructured Grid Computations to Massively Parallel Computers. Ph.D. Dissertation, 
Rensselaer Polytechnic Institute, Dept. Comp. Sci., Troy, 1992. [13] Hendrickson, B., and R. Leland. 
Multidimensional Spectral Load Balancing. Ssndia National Laboratories Tech. Rep. SAND93-0074. [14] Lsfon, 
F. and S. Osher. High-Order Filtering Methods for Approximating Hyperbolic Systemsof Conservation Laws. 
ICASE Report No. 90-25, March 1990. [15] Leiss, E., and H. Reddy. Distributed Load Balancing: Design 
and Performance Analysis. W.M.Keck Research Computation Laboratory. 5 (1989) 205-270. [16] Rank, E. and 
I. Babuska. An Expert System for the Optimal Mesh Design in the hp-Version of the Finite Element Method. 
IntL Jrnl. Num, Meth. in Engng., 24 (1987), 2087-2106. [17] Shu, C.-W., and S. Osher. Efficient Implementation 
of Essentially Non-oscillatory Shock-Capturing Schemes, II. Jrnl. of Comp. Phys., 83 (1989), 32-78. [18] 
Sod, G. A Survey of Several Fhite Difference Methods for Systems of Nonlinear Hyperbolic Conservation 
Laws. Jrnl. of Comp. Phys., 27 (1978), 1-31. [19] Sweby, P.K. High Resolution Schemes Using Flux Limiters 
for Hyperbolic Conservation Lswa. SL4M .1. Numer Anal., 21 (1984), 995-1011. [20] Szabo, B. and I. Babuska. 
Introduction to Finite Element Analysis, Wiley, New York, 1990. [21] Van Leer, B. Towards the Ultimate 
Conservative Difference Scheme. IV, A New Approach to Numerical Convection. Jrnl. of Comp. Phys., 23 
(1977), 276-299. [22] Wheat, S. A Fine Grained Data Migration Approach to Application Load Balancing 
on MP MIMD Machines. Ph.D. Dissertation. Dept. Comp. Sci., Univ. of New Mexico, Albuquerque, 1992.</RefA> .. 
11   
			
