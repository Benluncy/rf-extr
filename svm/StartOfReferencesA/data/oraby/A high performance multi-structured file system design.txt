
 A High Performance Multi-Structured File System Design Keith Muller and Joseph Pasquale Computer Systems 
Laboratory Department of Computer Science and Engineering University of California, San Diego Abstract 
File system I/O is increasingly becoming a perfor­mance bottleneck in large distributed computer systems. 
This is due to the increased file I/o demands of new appli­cations, the inability of any single storage 
structure to r,espond to these demands, and the slow decline of disk access times (latency and seek) 
relative to the rapid increase in CPU speeds, memory size, and network bandwidth. We present a multi-structured 
jile system designed for high bandwidth I/O and fast response. Our design is based on combining disk 
caching with three different file stomge structures, each implemented on an independent and isolated 
disk array. Each storage structure is designed to optimize a different set of file system access chamc­teristics 
such as cache writes, directory searches, file attri­bute requests or large sequential reads/writes. 
As part of our study, we analyze the performance of an existing file system using trace data from UNIX 
disk I/O-intensive workloads. Using trace driven simulations, we show how performance is improved by 
using separate storage structures as implemented by a multi-structured file system. 1[. Introduction 
Technology improvements in CPU speeds, memory :;izes, and network bandwidths are proceeding rapidly. 
This progress has begun to alter the characteristics of job execution in distributed computer systems. 
Whereas the CPU was the performance bottleneck for job execution in the pasL file I/O delays are making 
the file system the bottleneck. These technology improvements are also fuel­ing the creation of new I/O-intensive 
applications, such as multimedia and visualization applications, which produce workload characteristics 
different from the commonly observed rapid cycle of creation and deletion of small files 118]. For example, 
ten minutw of video at 30 frames/second and 1 megabyte/frame requires 1.8 giga­bytes, to be transferred 
at 30 megabytes/second. The lkrge file size combined with the bandwidth requirement lprwludes any solution 
primarily dependent on increased ]main memory (or cache) sizes to increase performance. Permission to 
copy without fee all or part of this material is granted provided that the copies are not made or distributed 
for direct commercial advantage, the ACM copyright notice and the title of the publication and its date 
appear, and notice is given that copying is by permission of the Association for Computing Machinery. 
To copy otherwise, or to republish, requires a fae and/or specific permission. @ 1991 ACM 0-89791-447-3/91 
/0009 /0056 . ..$1 .50 A major impediment to improving file system per­formance can be attributed to 
the design age of current file systems, particularly with regard to their storage struc­tures. Many of 
these storage structures tend to optimize storage efficiency to reduce the number of disk drives and 
lower overall system cost rather than focusing on minim­izing disk access delays. This problem is aggravated 
by the difficulty of file systems to effectively divide load over their available disks. Furthermore, 
this will not be solved by solely focusing on making faster disks. Technology improvements related to 
disk drives have increased the storage density, decreased the physical size, increased transfer rates, 
and dramatically reduced COSLbut have not significantly reduced access delays, and indications are that 
they will not improve in the near future. In this paper, we describe a multi-structured jile system (MFS). 
We expect this design to be used for Iarge-scale file servers supporting a wide range of inten­sive workloads. 
It addnxses the requirements of applica­tions with very high 1/0 requirements, such as databases supporting 
large objects, multimedia applications which include video and audio, and scientific applications including 
visualization, executing on computers ranging from high-performance workstations to general-purpose supercomputers. 
Our design is based on combining the positive attri­butes of several well known techniques, such as caching, 
disk striping, disk mirroring, using append-only log storage, and using disk arrays, to achieve performance 
improvements which increase with the number of disks used. In Section 2 we describe past work. In Section 
3 we examine the characteristics of a typical file system workload in terms of these modes of disk access. 
In Sec­tion 4 we describe a design for MFS. In Section 5 we present a trace driven simulation study comparing 
an existing Berkeley Unix Fast File System (FFS) [13], and MFS. We present our conclusions in Section 
6, 2. Past Work Disk striping is a t.eehnique for improving throughput by dividhtg a logical disk storage 
unit evenly across multiple disks such that whenever that storage unit is requested, concurrent transfers 
of blocks from each of the disks occurs. However, average access times can be large in disk striping 
systems since access time is governed by the disk with the largest access time. Garcia-Molina [7] proposed 
a solution based on immedi­ate reading which allows a disk read to occur anywhere within a block, and 
this was found to improve perfor­mance considerably. Kim [10] developed a solution to this problem based 
on hardware synchronization of the disks. Disk striping has difficulty in supporting concurrent access 
I/O although bandwidth can be increased for large data requests, the disks cannot be used in an independent 
manner to service many small independent concurrent file requests. Livny [12] conducted a study of clustered 
stomge, where logically related data blocks are clustered on a single large disk, and declustered storage, 
where log­ically related data blocks are spread over multiple disks, as in disk striping. Declustering 
performed better than clustering for most workloadx however, under high loads with uniform access patterns, 
clustering worked better. Furthermore, as the load increases, declustered storage shows a noticeable 
increase in the number of seeks and length of seek delays. Reddy [22] compared the perfor­mance of combinations 
of disk synchronization and data detlustering/disk striping techniques. Configurations which consisted 
of only synchronized disks performed well at low request rates, while increasing the amount of declustering 
improves performance at high request rates. These studies all demonstrate the sensitivity of striped 
storage to request rates. Disk arrays are systems of multiple disks arranged to provide higher I/O bandwidth 
than a single disk of aggregate size of the disk array. There is the problem of reliability when file 
information is spread across multiple disks: a failure in any disk makes (at least part of) the file 
inaccessible. A simple improvement is through redun­dancy using mirrored disks, as described by Britton 
and Gray [1]. This also reduces access time for concurrent reads, although writing incurs additional 
delays, Patterson [20] showed that large arrays of lower performance disks can have better performance, 
lower cost and improved reliability over a lesser number of more expensive disks. EITOr correction blocks 
are spread over all the disks so that single disk failures could be handled, and no single disk has to 
be devoted to ECC, which would be a potential bottleneck. Disk block caching [25] also improves file 
system performance by keeping recently used data blocks in memory to service future requests without 
incurring disk delays. Ousterhout [18] studied dynamic access methods of a Unix file system, and found 
that using a very large cache leads to a drastic reduction in disk I/O traffic since, in the workloads 
observed, most files are small and most are short lived. Satyanarayanan [24] made similar obser­vations 
of file sizes and lifetimes by studying static snapshots of a TOPS-10 system. Nelson [16][15] studied 
tk effects of combining local and remote caching in a diskless workstation environment with a central 
file server, again noting the benefits of a very large cache at tha file server, but reducing server 
traffic by local caching. Logging is a technique where data is always written to a new unused area of 
storage, thus old data is never overwritten. With the advent of WORM optical disk drives, log structured 
storage has become attractive for designing reliable file systems with version support, as demonstrated 
by the early work of Svobodova [26], and more recently by Gait [6] and a number of others. Hag­mann [8] 
designed a log structured file system for a workstation programming environment. Cheriton [21 describes 
a file system extension to implement log files on WORM drives. Log structural storage has the interesting 
property that the next write will always start at the position where the last write was finished. For 
workloads which allow most file reads to be serviced by an in-memory cache, a log structured storage 
can effectively be used as the per­manent storage of a file system. Ousterhout [19] and Rosenblum [23] 
describe a circular log structured file sys­tem that is optimized for writes, and works well for office 
environment workloads which involve accessing mostly small files. However, performance problems may mise 
for workloads where the working set of file data is difficult to keep in the cache. When read requests 
must go to log store, the disk head must move betwexm the current write point and the location of the 
file on log storage. Under workloads with concurrent read and write requests this can become very expensive. 
Such workloads are not unusual, and may become more prominent with time. Examples include workloads exhibiting 
sequential reading of very large files (e.g., a file containing a library of fonts, or a video file which 
can easily fill up any cache), or shell commands which touch a large number of files (e.g., a sting search 
command like the Unix grep, recursively applied to a large file system tree). For workloads where these 
type of requests are made at a high rate, even the largest of caches can be easily defeated. Other relevant 
file system studies and/or techniques for performance improvement include the overhead of looking up 
file names in directories by Floyd [4], the value of contiguous storage allocation for files by Tanen­baum 
[27] and Yokoyama [28], and the value of storing file data in file headers by Mullender [14]. 3. Characteristics 
of File System Workloads To understand file system operation under actual operating conditions, we extensively 
instrumented the file system of a 4.3BSD-Tahoe kernel running on a Unisys 7000/51 under a real workload. 
The hardware consisted of 64 megabytes of main memory, 64 user ports, ethernet, and three disk controllers, 
each controlling a single Fujitsu 2361A (Super Eagle). The buffer cache size was 7 mega­bytes, and all 
file systems consisted of 8 Kbyte primary blocks with 1 Kbyte fragments (see [13] for a description of 
FFs). The file system was measured over periods charac­terized by fairly high workloads. There were a 
total of nine 75-minute measurement periods, with system activity characterized by the following statistics. 
The average number of users was 60, about 7590 of these users were performing edit-compile-test cycles 
to build the code gen­erator for a compiter, and the remaining users were per­forming database operations 
on large disk files. The run queue contained an average of 9 to 15 active jobs with close to 100% CPU 
utilization. The virtual memory sys­tern had between 30 to 50 megabytes free at all times. There were 
no pageouts, process swaps, or page scans. The file system was finely tuned, with load being well bal­anced 
between the three disks. We obtained trace data by placing checkpoints [3] at the entry and exit points 
of evexy system call that might potentially cause any file system operation, and within all major routines 
which affect file system-related data struc­tures. Each trace event was timestamped by a hardware clock 
providing a resolution of 2 milliseconds. This allowed us to determine the execution flow from any sys­tem 
call entry point to its destination, including any cache or any disk, determine every internal file system 
event (such as a disk request or cache miss) generated by a specific tile system reques~ and determine 
the seMce rate-s of all major internal file system functions (e.g., name lcokups, buffer cache lookups, 
disk operations). Figure 1 illus~ates the location of the checkpoints. DIS K:: KERNEL ,;~~~~; O; :::. 
. . MDISK HARD WARE ; . CHECKPOINTS i F@rre 1. Location of File System Chedpoints To reduce measurement 
interference and not affect locat disks, tmce data was transmitted over a network and logged on a remote 
host. The total CPU overhead due to all measurement activity (including transmission overhead which was 
minimal) was under 5.0% of measured capa­city. We distinguish lwo types of file requests: data and control. 
Data requests mainly operate on data storage, which is that part of the file system storage containing 
the actual contents of files. The Unix read and write system calls correspond to data requests. Control 
requests operate on control storage, which is that part of the file system storage containing access 
control and other descriptive information about files. Requests for file crea­tion, deletion, renaming, 
truncation, and identification are examples of control requests. Table 1 shows statistics for events 
generated by read, write, and control storage requests (which include open, close, unlink, sync, access, 
chdir, chrnod, chown, dup2, exit, fsync, ftrunc, mkdir, link, rename, rlink, rmdir, stat, utinze) which 
make up the remaining file system­related system calls. AU statistics axe averages taken over all the 
trace periods. Data requests account for 39%J of all file system requests, while the remaining 6170 are 
due to control requests. Furthermore, read system calls dominate write system calls by a ratio of 3.2 
to 1 (this agrees with meas­urements in [18]). Data requests cause 36% of all disk operations, and of 
these, disk writes dominate disk reads by a ratio of 1.6 to 1. When control requests are included, this 
ratio increases to 3.6 to 1. When considering only control requests, disk writes dominate disk reads 
by a ratio of 7.1 to 1. System Call Events Per Period System Number Total Disk Disk Call Requests Disk 
Read Write read 55475 6424 5368 1056 write 17413 9062 499 8563 control 116215 27377 3379 23998 Totals 
189103 42863 9246 33617 Table 1 Caching is a major influence in converting read dominated system call 
requests to write dominated disk requests, as was observed in [16]. Upon measuring the hit rates of various 
caches, we found that the disk cache had a 87% hit rate, the name lookup cache had a 83% hit rate, and 
the inode cache (system inode table) had a 63 %0 hit rate. Consider the relationship of read and write 
system calls to read and write disk requests. 5870 of disk reads are due to read system calls, while 
only 25% of disk writes are due to write system calls. Furthermore, while the rate of disk reads due 
to read system calls is slkhtlv meater than the rate of disk reads d;e to control requ mts; ~e rate of 
disk writes due to control requests is much greater than the rate of disk writes due to write system 
calls. Finally, we considered tie completion time and the data request size for data and control system 
calls. These statistics are summarized in Table 2. All statistics are per periods averaged over all nine 
periods. The completion times for control system calls had a wide variation, from 10 milliseconds to 
303 milliseconds, with an average value of 27 milliseconds, which is ahnost double the average twmpletion 
times of nearly 15 mil­liseconds for read and write system calls. The main rea­son for this is that control 
requests access the disk more often than data requests. However, note that the average amount of data 
requested by control system calls is almost 20 times smaller than that of read or write system calls. 
Given that the average disk request time was 23 mil­liseconds, the memory caching of control information 
and data was, to differing degrees, effective for both control and data requests. System Call Completion 
Time and Request Size Per Period System Call I Time (msec) I Size (bytes) Iread I 16I4235 I write 14 
4688 control I 10-303 I 252 { Table 2 In Figure 2, we show the arrival of disk requests broken down 
by data reads, data writes, control reads and control writes. Notice how control writes are continuous 
and control reads are clustered. The frequency of control requests is so high that it is difficult to 
maintain disk posi­ tioning locality for data reads and writes. Dkk Requests by Type .. .. .. ...... 
... ....... .. . .... Dlfnw ­ maw . ... .... .. . Gmtw -. .- --._ ______ ____ ._ -.-. ., . r( 0 100 
2W X4 400 cd m Figure 2. Disk Requests by Type Our observations indicate that control storage has very 
different access characteristics than data storage, They also suggest the importance of considering the 
requirements of control storage operation for improving file system performance, and whether a single 
storage structure can effectively respond to both data and control requests. Upon analyzing our trace 
data in the time 59 domain, we noticed the degree to which small control requests mixed with much larger 
data requests (generally stored at different locations on disk) conflicted. These confiicts are due to 
induced disk head seeks, and result in increased seMce times for both control and data requests, We emphasize 
that the conflict between control and data requests is a general problem; our trace data allowed us to 
observe this problem, however, we believe these observations would appear on any file system where ccm­trol 
and data is mixed (which includes the vast majority of file systems in existence today). We expect the 
trend towards larger data files with larger average data request sizes to continue. As the size of data 
requests increase, the conflict between control and data request characteris­tics also increases. This 
trend provides further motivation to develop techniques which address the requirements of both control 
and data storage for improving file system performance. 4. The Design Our approach is based on a suggestion 
by Powell in a study of the DEMOS file system [21], where one tries to understand properties of the file 
request distributions and then develop solutions to take advantage of those proper­ties. We looked for 
properties in request patterns of sys­tem calls and disk accesses which we expect are present in a wide 
range of file system workloads. Our observa­tions led to the following two design principles Separate 
control requests from data requests. Control requests are typically much smaller than data requests, 
but they occur more often. Separate storage structures for control and data are provided, each designed 
to perform best under their respective request-pattern characteristics. This also provides a natural 
division of load across parallel storage structures.  Separate cache backups from -the flushes. We distinguish 
between a lxzckup request that writes cached information to non-volatile storage to guard against information 
loss due to crashes, and a @.dt request that writes cached information, which is not expected to be used 
soon, to non-volatile storage to make room for other information. To insure the highest level of reliability, 
backup requests must be done immediately upon file system modification. This leaves flush requests to 
be made at more con­venient times as determined by the file system. Based on the different timing characteristics 
of these write requests, separate optimize-d storage structures are provided  4.1. Design Overview MFS 
contains three non-volatile storage structures control, data, and log storage. In addition, three caches, 
two for control storage and one for dam storage, are used to improve read performance. Figure 3 illustrates 
the basic relationships of these major components. Control storage and data storage provide long term 
storage for information that has not been recently modified. The caches store recently-accessed informa­tion. 
They are filled fkom control or data storage as appropriate, and they are eventually flushed to a corresponding 
long-term storage structure if modititxl. Log storage is used purely for cache backup (based on a write-through 
policy) to guard against information loss during a crash. During normal system operation, ordy append-writes 
are issued to log storage. r Figure 3. Multi-Structured File System 4.2. Control storage Control storage 
is the long term stomge forjile con­trol in.jormatwn, which includes file attributes, storage maps, and 
directory information. Control information is organized in two distinct tables, the jile header table 
and the name map table. A tile header consists of an attribute record, which describes the attributes 
of a file such as ownership, access protection, access times, etc., and a storage map, which describes 
how to locate the file s &#38;ta on data storage. Tbe storage map is an extent-based table. Each entry 
contains a starting block address and a length. Data storage is allocated in units of striped blocks 
or striped tracks, depending on the request size. If the file data size is smaller than the entire storage 
map size, then the file data is stored directly in the file header replacing the storage map implementing 
an immediate jile [14]. This prevents small file &#38;ta requests from having to be served by data storage 
and improves performance for small files. Block size, striping factor, and storage map size are hm­able 
parameters. The name map table binds fzle names to ~es and organizes file names into directories. All 
naming informa­tion is isolated from file data (unlike many file systems which store the name information 
as special file data). Each file header is referenced by one or more entries in the name map table, allowing 
a file to have more than one name. Ihe name map table is organized as a chained tree structure similar 
to Yokoyama [28] to cluster directory information. Control storage is physically allocated as follows. 
For either the file header table or the name map table, disk storage is allocated by cylinder. The first 
blocks in each cylinder contain configuration data followed by a bh map that describes storage for that 
cylinder. The configuration data contains the number of disks and disk geometry data (e.g., tracks, blocks 
per track, etc). The bit map is used to locate those blocks that contain at least one free entIY (file 
header or name map). Following the allcxation map, the name map or file header tables are allocated in 
consecu­ tive blocks, then track by track for the entire cylinder. The characteristic access pattern 
of control storage is that of many concurrent small read/modify/write cycles this suggests the effective 
use of an array of very small disks to contain control storage. (Although the size of control storage 
is typically less than ten percent of data storage [18], for the very large file systems warranting our 
multi-structured design, the absolute size of control storage can require one or more disks). The regular 
struc­ture of the tables described above allows control storage performance to be scaled through the 
use of a modified version of disk mirroring techniques [1] [20]. This improves concurrent access and 
reduces the queueing delays that tend to dominate control storage requests. Since many disks are capable 
of handling a reques~ load is easily divided among the mirrored disks. The control cache keeps track 
of which disks have been updated and does not flush a modified entry until all disks have been updated. 
(Log storage will alway have a valid copy, so a crash while the mirmred disks are out of sync is recover­able). 
This technique avoids single threaded writes to mirrored disks and extends concurrent operations to both 
reads and writes. 4.3. Data Storage Data storage is the long term storage for all Iile data (except 
those files that are small enough to be stored as immediate files). Data storage is structured using 
disks conl@red for striped acces to optimize for high bandwidth sequential I/O. Allocation is simply 
in units of striped blocks and striped tracks, which contain the same positioned blocks and tracks on 
each disk drive, and is based on methods similar to those used for contiguous files [27][28]. The first 
track(s) on each disk contains configuration data and the allocation bitmap for data storage. The configuration 
data contains information about drive geometries and the number of drives. The allocation bitmap is structured 
as a two-level tree, the lowest level corresponding to striped blocks, and the second level corresponding 
to striped tracks. A striped hack entry is marked free only if all the striped blocks it comprises are 
marked free. To find a free striped block (or blocks), or a striped track the conwsponding level of the 
tree is examined. This two level bit map is similar in concept to that used in [28]. Figtue 4 illustrates 
the layout of a data relocation map. Track St@e AIIoeation Map I ;. ; .... .... 1 0 1 0 .... .... ~geg~ 
/\ ~ Iolllolololowlo 1111111111111111 Block S@ Allocation Map ............................................................................................. 
Figure 4. Data Allocation Map 4.4. Cache Operation Cache entries are filled with &#38;ta read from either 
control or data storage as appropriate. A LRU (least recently used) ~placement policy combined with a 
write­through update strategy [251 to log storage as the backing store are used. A cache flush operation 
writes dirty entries to either control or data storage as appropriate, by writing the current version 
tiom cache memory. To improve performance, a hash table (with chain­ing) and three lists (LRU, free, 
and locked) are used to organize the entries. The LRU list orders valid enties from most+ecently accessed 
to least-recently accessed. The free list contains flushed entries (but may still contain valid &#38;ta). 
The locked list contains entries which should remain locked in the cache, such as those currently involved 
in an I/O operation, or those containing tile headers for currently open files. Each cache consists of 
a table of entries whose con­tent varies by the type of cache. Each cache entry con­tains valid and dirty 
flags, the location on disk storage where this entry is locati a pointer to an in-memory copy of the 
st.cxage, pointers to support the hash chain, the LRU, &#38;e, and locked lists, and the log storage 
chain (described in Section 4.5). A data cache entry includes a pointer to a memory @py of data storage, 
which may range in size from a striped block to a striped track, and is hashed by storage location. The 
file header cache is hashed by file header table index. A file header cache entry includes a reference 
count, a pointer to a memory copy of the file header and a pointer to data cache memory for large I/O 
operations (described below). Two hash tables provide fast access to the name map cache entries, one 
providing access by name map table index, and the other by path name (similar to what exists in FFS [13]). 
A name map cache entry includes a pointer to a mcmmry copy of a name map block and a reference count 
of path cache entries that point at it, The system tries to keep a minimum number of entries on the free 
list for fast allocation whenever a cache miss occurs. This is implemented using a watermark­based strategy 
to liit the number of dirty entries (a dirty entry has a copy of data that is a newer version of what 
is contained in data or control storage). Associated with each cache is a high and a low watermark. In 
normal opexating mode, the cache does not flush dirty entries. When the number of dirty cache entries 
reaches the high watermark, the cache goes into a flushing mode. While in flushing mode, whenever a file 
system request generates a cache hiL a cache flush operation is also started for the oldest dirty cache 
entry. The effect we wished to achieve was that of spreading flush operations over time to amortize their 
cost, and overlapping cache flush operations with file requests which are successfully serviced by the 
same cache. Cache hits are used as a heuristic to indicate low expeeted traffic to the cormponding non-volatile 
storage structure. The cache remains in flushing mode until the number of dirty entries is reduced to 
the low watermark level. Certain file system requests patterns may cause degenerate behavior in the file 
system. The primary prob­lem addressed by our design concerns sequential I/O on large files. Sequential 
I/O on large files can quickly flush the contents of the data cache and degrade overall perfor­mance. 
(This is a problem with all file systems that use caches, not just our design). To redu~ this effec~ 
a pointer is maintained in the file header cache. As the result of sequential I/O requests on a large 
file, caching for this file is limited to a single entry in the data cache. (The size of the memory storage 
referenced by this single entry is the size of a large I/O reques~ which can be at most a stripi track). 
The pointer in the file header cache is set to the address of this entry. This causes all subse­quent 
data I/O operations on that file to reuse the single cache entry. For large write operations (e.g., the 
entire strip@ track), the cache entry is flushed immediately to data storage and not written to log storage. 
Although the cache entry is valid, it is never marked as dirty. If the pat­tern of data access changes, 
the pointer is cleared and nor­mal operation is resumed. 4.S. LOg Storage Log storage backs up all caches. 
It is designed to optimize writes and is never read during normal opera­tion. (However, during crash 
recovery operations which precede normal operation, it would be read to update con­trol and data storage). 
When the file system begins nor­mal operation, log storage is empty. Whenever a cache entry is written, 
the log storage is updated. Regardkxs of the kind of data involved (control data or file data), all updates 
are appended to the log storage. Log storage updates are either cache backup records, or cache jlu.rh 
recordr. A cache backup record consists of a header block followed by one or more &#38;ta blocks. The 
header block describes the permanent loca­tion of these data blocks on either control or data storage. 
A cache jkh record consists only of a header block, and is written after a cache flush operation completes 
to record the time at which a specific data or control storage loca­tion was made consistent with the 
copies in a cache (which happen w also be recorded on log storage). This is required for crash recovery 
as data and control storage blocks do not have time stamp fields. Log storage is organized in a circular 
structure such that the oldest written area of storage is reclaimed for a new record to be appended. 
Any data block in the log storage can k characterized as follow.x dirty, the only other copy exists in 
one of the eachex jlushed, an up-to­date copy exists in eontml or data storag~ or invakf, a newer version 
exists in control, data or log storage. For reclamation, only dirty blocks need to be c&#38;sider~ since 
they are not sawxi anywhere but the caches. (However, since dirty blocks are contained in the caches, 
no read operations on log storage are ever requid for reclamat­ion). A data structure called the /og 
storage chain keeps track of locations of @ data blocks, and each entry is double linked to the corresponding 
cache table entry. Entries in the log storage chain are ordered according to the order in which dirty 
data blocks are written, (which atso occur in the same order on log storage). The entry at the head of 
the chain is the dirty &#38;ta block closest to the current log write poin4 while the end of the chain 
is farthest away. Whenever a eaehe entry is first modified, a new entry is added to the log storage chain. 
Whenever a cache entry is flushed, the correqonding entry in the log storage chain is simply removed. 
To efficiently implement storage compaction (again, without having to read from log storage), the sys­tem 
maintains two pointers to blocks in log storagtx the wn te point (the current end of the log), and the 
examine point. A cache write to log storage moves the write point and the examine point by the number 
of rqqxmded blocks. A new entry is then added to the end of the log storage chain to describe this newly 
written data. If the examine point has moved past the storage location of the first entry on the log 
storage chain, the tirst entry is rewritten at the write point (nothing is ever actually read or written 
ftom log storage at the examine point). The log storage chain entry describes the cache entry where the 
data can be obtained to be written to log storage (so no read operation from log storage is needed). 
This data is rewritten by appending it to log storage, the write and examine pdnts are updated by the 
number of blocks written, and the log storage chain entry is updated and moved to the end of the chain 
(it is now the furthest away from the current write point). The examine point is again checked against 
the first entry to see if another entry neads to be moved. If so, the operation of compacting storage 
is repeatd Figure 5 illustrates a log stomge chain and a portion of log storage. The size of log storage 
should be made large enough so that the majority of log storage will be reclaim­able without performing 
a compact operation, in order to avoid extended periods of continuous log storage move­ment. This is 
achieved if log storage size is at least twice the sim of total cache storagq in uractice, this ratio 
would be tailored to the Wcddoad.- Cacha Data S?tuctwes .:. .... . . .... :.. .. ... ... . cam II-N* 
..... .... . .. .... .. 8 d,. . C4em hN&#38;ld I r--+:G4w:GcGi ..... Wrd Aint Exami&#38; Poht [@r~;ii 
 -- ~tiw@&#38;=J .. . ............................................................................... 
... Figure 5. Log Storage Structures Log storage is allocated as follows. Assume for the moment that 
there is only one disk with a single two-sided platter. Tracks m allocated sequentially from outermost 
to innermos~ using only the top face of the platter. When all these tracks are allocated, then the tracks 
on the bottom face are allocated sequentially, this time starting from the innermost to outermost. This 
closes the loop, creating a cimdar structure where the next track is always one disk seek away. If multiple 
disks with separate controllers are avait­able, disk head propositioning can be done in such a way that 
seek and latency delays can be minimized. The gen­eral dhcation algorithm (assuming two-sided ptatters) 
is as follows. Given D disks (with separate controllers), P platters/diSk, and T trackdplatter. where 
tracks are num­bered 1 to T/2 on the top side physically ordered outer­most to innerrnox continuing with 
(T/2)+-l to T on the bottom side physically ordered innermost to outermosfi do forever fort= lto T forp=lto 
P ford= lto D allocate track (d, p, t) Figure 6 illustrates the physiea.1 allocation of a log stomge 
with two disks and two platters. l%e numbers show the order in which the first eight tracks are allocated. 
26 48 Figure 6. Physical Atloeation of Log Storage After a track has been written, a seek operation 
is immediately issued for that disk to preposition the disk head. However, a write to that disk will 
not occur until a track on eaeh of the other disks have been written. Conse­quently, if (D -l)t&#38;4C, 
> twek + t~e,, where t~,r k the time to transfer a track of data to disk, and t,.=k M the time to seek 
to an adjacent track, the disk head will always be prepositioned cmrectly for the next write to that 
same disk. Furthermore, if disk cmnrollers are equipped with track buffers, rotational delays mv minimized. 
The net result is that cache backup operations incur minimal disk aeeess delays and ean complete asynchronously. 
In summary, the advantage of log storage is that the only operation it must support is append-writes. 
Sin@ the size and allocation order of storage within the disk loop is predetermined, the exaet location 
of the next write is always known. If the number of disks in the loop is large enough, both a data transfer 
and a seek operation can be completed on a disk before the next write operation to that disk is iSSUed. 
S. Performance To verify our claims, we conducted a performance evaluation study using a trace driven 
simulation of our file system design. Our primary goal was to verify that separating storage into the 
three isolated areas (eontml, da@ and log) would signitieantly irnpmve performance. We constructed two 
simulators, one of the existing FFS to which we had aeeess, and one of MFS. Each simu­lator was driven 
by system call-level trace data. Most of the trams we used were generated fkom real workloads (deseribed 
in Section 3) to validate the FFS simulator and to compare the performance of the two file systems. Oth­ers 
were generated from artificial workloads we con­structed for controlled experiments designed to test 
the performance limits of MFS. The simulators are quite detailed. Most of the in­memory data structures 
are real, such as caches with LRU block replacement (in fiic~ much of the FFS simulator executed the 
actual FFS kernel code). The disk hardware is simulated such that each disk s aeeess time depends on 
seek and transfer time based on the current head position for that disk and the size and location of 
the next requesL Mechanical parameters for the all disks such as rotational speed and seek time as a 
function of distance are based on a Fujitsu 2361a [5J. We used disks with the same aeeess­performanee 
characteristics in both simulations to elimi­nate the effects of differeru hardware performamx in our 
studies. Distribution of Real System Service Times 5101520 253O354O455O 5560 Milliseconds Distribution 
of Simulated Service Times 4)00 3000 2000 1000 0 5 1015202530354045505560 Milliseconds Figure 7. Validation 
Results We validated our model of FFS by emitting seMce times it calculated for eaeh individual system 
call and for all disk level aeeesses, and compared these to the meas­ured system call and disk aeeess 
serviee times by compar­ing the respective distributions. In almost all cases, the mean squared error 
between the distributions was under 5%. Figure 7 shows a distribution of serviee times for access of 
the disk (which is the main simulated object) measured in the actual system, and the distribution pro­duced 
by the simulator.  5.1. Ability to Handle Increasing Loads In these experiments, we compare the ability 
of the two file systems to handle file loads of increasing inten­sity. For MFS, we used one small disk 
for control storage, one large disk for data storage, and two small disks for log storage. The total 
disk space for MFS and FFS were effectively the same (although the space was organized differently with 
a different number of disks and disk con­trollers). The tile systems also had the same effeetive cache 
sizes. Seven experiments wexe carried OUG each with a progressively more intensive load. Loads were con­structed 
by combining multiple sets of d.tflerenr traces so that requests are issued concurrently. AU the traces 
indivi­dually generated a load of approximately the same level of intensity. Thus, the load produced 
by a single trace is taken to be a unit of intensity of Iot@ and a load of n signifies the composition 
of n traces read in parallel. Figure 8 illustmt.es the higher rate of performance degra&#38;tion with 
increasing load for FFS relative to MFS. Service Time Versus Load . + 60 CpMHMHx c cperkmulti : 50 
cMwnca %40 cldir-mula = ~ raa.$unix g 30 a read-m.h r 20 10 0 123 567 Lo:d Aggregate Service Time Versus 
Load  01 /  I 02 68 Lo:d  Figure 8. Comparison of Service Time Versus Load MFS is less sensitive 
to increased load than FFS, and it exhibits a significantly better performance decay rate. 5.2. Ability 
to Handle Large I/O We now examine the performance of the two fde systems when handling large data I/O 
requests for reading and writing. The purpose of these experiments is to meas­ure the best large I/O 
performance each file system is capable of providing under optimal conditions. Trace data was obtained 
by tracing our real FFS with an artificial execution environment of programs we con­structed which continually 
make large file LO rrxpests. We conducted four experiments; in each one, 50 Mbytes of data was read from 
or written to a file. In the fist experiment 50 Mbytes were written sequentially to a file in 1 Mbyte 
increments. The second experiment is the same, except that data is read. In the third experimen~ 50 MbytOS 
Were read in randomly OKktXi 1 Mbyte lVqUt%3tS contains the results. For MFS plots, performance is shown 
for different data storage sizes (i.e., number of striped disks). The significant differences in performance 
are due to the different techniques used by the two file system to allocate and reference data storage. 
Under FFS, the file map (inOde) points to blocks that are no larger than 8 Kbytes each, while the file 
map in MFS supports pointers to Striped blocks and Striped tEICkS. Large File 1/0 (No Load) ~ . s~wm 
sw_Rw Rmd_WriIORard.Ra~ Type of Access Figure 9. Large I/O Performance 5.3. Large I/O with Competing 
Load We conducted three experiments where a 1 Mbyte data request is made which competes with no loa~ 
one unit of load, and two units of load. The first experiment with no load tells us the maximum throughput 
for each file system by transferring 1 Mbyte of data as fast as possible, which was 500 Kb/sec for FFS, 
and 1300 Kb/sec for an equally configured MFS. Although the latter s perfor­mance is much better, we 
are more interested in the rate of performance degradation of each file system when competing load is 
added, which is the purpose of the second and third experiments. Figure 10 shows the 1 Mbyte read throughput 
for each file system, expressed as a percentage of the maximum possible throughput for that file system. 
1 Mbyte Read With Competing Load 100 90 so m 60 60 40 30 20 10 0 &#38; UNIX Multi-Structured Ml No Load 
1 Load 2 Loada (however, nothing is read more than once). The fourth Figure 10. Large I/O with Competing 
Load experiment is similar, except that data is written. Figure 9 20­: = = = 14­12­0 ~:: . 4 1 o 1 1200­lolm 
-aoo - Create 2 4 6 L06d Write Lozd Read 5 2 4 6 L06d Cache Flush 8 I f For FFS, the 1 Mbyte read rate 
quickly declines to 20% of the maximum throughput with 1 load, and to less than 5% with 2 loads. For 
MFS, the performance degra­dation is noticeably less. The source of performance degradation here is due 
to the conflicts behveen the many concuntmt small control requests of the competing load with the single 
large 1 Mbyte data rquest that severely limits the performance of FIN. MFS performs well because it separates 
control storage horn data storage, lxxause of their characteristically different access pat­terns. 5.4. 
Performance Sctilng In this experiment, we examine the performance of MFS under different disk storage 
configurations while varying the number of concurrent request loads from 1 to 7. The goal of this experiment 
is to verify the ability of the file system to increase performance to offset increases in the request 
load. In Figure 11 we plot performance of various system calls as we vary the size of control and data 
storage structures concurrently from 1 to 4 disks each. In each graph, different curves are plotted for 
each disk configuration. These curves are labeled by the number of disks in control and data storage. 
For example Cent-2 Data-2 means 2 mirrored disks in control store and 2 striped disks in data store. 
A noticeable improvement is obtained by increasing the storage size. In fact, with 4 disks in control 
storage and 4 in data storage, under 7 concurrent loads, the ser­vice times for all system calls an3 
better than for 1 disk in eaeh of data and control storage structure with only a sin­gle load. The lower 
plot in Figure 11 shows the service time for cache flush opemtions. Cache flushing requires writ­ing 
dirty blocks to eitha control or data storage, and is a very good indicator of the level of workload 
those two storage areas are servicing. The service time is noticeably high for the smallest configuration 
of one disk in each of control and data storage. When these two storage struc­tures are increased in 
size, the service times for cache flush operations are significantly reduced. These experiments have 
illustrated the ability of MFS to scale performance by simply adding additional disk drives to match 
workload requirements. 6oo­400­2m -o-i 0 , 2 4 6 I--O­Cmnt-i Data-l I ~ COnt-2Data-2 ~ Ckmt-4Data4 I 
5.5. Discussion Under MFS, control and data storage are heated on different disks, while on more traditional 
file systems each disk is divided into distinct areas for control and data storage (e.g., cylinder groups 
in FFS). Separating the location of control and data on different disks versus stor­ing lmth control 
and data on every disk does not create any additional software complexity, as the same type of data structures 
are used. In face, this division results in a more uniform usage of disk storage which simplifies the 
 allocation of disk storage. Figure 11. Performance Scaling Effects This simple separation performance 
improvements. of control and data leads to Access delays of file data requests may be classifml as direc~ 
which are determined by the cost of accessing the file header or the file data, and indirec~ which are 
the created by contention from other file system requests. Separating control and data addresses the 
minimization of indirect delays. The design of MFS gives the ability to match request types to distinct 
storage structures and storage devices. Each storage structure may be built with dif­ferent sized disks 
to increase storage utilization and reduce costs without reducing performance. The small size of control 
storage allows the use of small mirrored disks to be cost effective. Control storage requires con­current 
access, but also has a small storage requirement when compared to data storage. Since the storage capa­city 
of log storage is pro~rtional to the size of main memory caches, small inexpensive disks can be used. 
The majority of storage capacity would be invested in data storage. Here disks with higher transfer rates 
and storage density would be used. Separating control and data storage also simplifies the integration 
of different storage device technologies into data storage, such as video data stored on read-only disk. 
Control storage can integrate tiles stored on dif­ferent devices into the same name space. The organiza­tion 
of data within that disk is easily modified within con­trol storage, and data from different storage 
areas could be merged within a single file if desired. Furthermore, dif­ferent types of data (e.g., texg 
graphics, audio, video) have different access patterns, and could benefit from dif­ferent data storage 
structures. MFS supports multiple stomge structures integrated in a single file system. A major advantage 
of the structures described for control, data, and log storage is their scalability, not just for space 
but for response and throughpu~ The addition of mirrored disks for control storage enhances response 
and concurrency, which matches the characteristic control request pattern of a high rate of small read/modify/write 
requests which can be serviced concurrently. The addi­tion of striped disks for data storage enhances 
throughput (and space of course), which matches the characteristic data request pattern of large sequential 
data requests. As for log storage, although the addition of disks certainly enhances response and throughput 
for cache backup requests by lowering the rate of writes due to compaction, even a single disk helps 
achieve significant benefits. This is due to our log storage allocation scheme which minimizes disk head 
movemeat-Furthermore, if two disks are available, disk heads can be prepositioned so that append-writes, 
which are the only operations allowed, complete with minimal disk access delays. Under MFS, the ratio 
of storage size of control storage to data storage does not differ from more tradi­tional file systems 
(the same information is stored in both cases). The only additional cost which is required is for log 
storage, whose size depends on the combined cache sizes, and not the size of control storage or data 
storage which is where the overwhelming majority of disk space is located. (An effective log storage 
can be built using two 100MB disks with separate SCSI disk controllers for approximately $1500 in today 
s markeL) Adding addi­tional mirrored disks for control storage, is a more significant expens~ however, 
this addition is not a requirement, and provides a performance benefit if added. Overall, these costs 
lead to improved performance which is significantly higher than traditional file systems, and can be 
scaled to meet load. As the cost of disks decrease, and the overall size of file systems bwome larger, 
MFS becomes more cost-effective. 6. CONCLUSION The motivation behind the MFS design was to improve performance 
for a broad range of file system workloads. The design is based on distinguishing between control and 
data requests, and between cache backup and flush requests. These distinctions lead to a disrnbution 
of labor which allows its featured storage structures, control, data, and log, aided by caches, to carry 
out specialized work for which they are well suited. Many of the individual storage structures in isolation 
have already been carefully researched in prior studies; our contribution is a design which illustrates 
their effective integration. In summary, the features of a MFS are the follow­ing: Multiple isolated 
storage structures optimized to respond to characteristic access patterns. Isolation avoid delays created 
by the conflicting modes of disk access.  Improved bandwidth for large sequential I/O by using disk 
striping, and improved response time for concurrent accesses of control storage and small files using 
mirrored disks.  Lowered access times by reducing the number of disk accesses through caching while 
maintaining very high reliability through logging.  Scalable performance to a specitied I/O requirement 
by the addition of disk drives.  Load balancing across disks due to a natural divi­sion of storage by 
access type (i.e., control and data), and due to the way storage is structured (i.e., mirrored and striped). 
 7. ACKNOWLEDGEMENTS We thank the reviewers who provided us with con­structive comments. We gratefully 
acknowledge support by DEC, IBM, NCR, NSF, and the Powell Foundation. References <RefA>[1] Britton, D., and 
Gray, J., Disk Shadowing , Proceedings of the 14th Very Large Data Base Conference, September 1988, pp. 
331-338. [2] Cheriton, D. and Finalayson, R., Log Files: An Extended File Service Exploiting Write-Once 
Storage. , Proceedings Eleventh Symposium on Operating Systems Principles, November 1987, pp. 139-148. 
[3] Ferrari, D., Computer Systems Performance Evaluation. , Prentice Hall, 1981. [4] Floyd R., and Ellis, 
C., Directory Reference Pat­terns in Hierarchical File Systems , ZEEE Transac­twns on Knowledge and Data 
Engineering, Volume 1, Number 2, June 1989, pp. 238-247. [5] Fujitsu M2361A Mini-Disk Drive Customer 
Engineering Manual. [6] Gait, J., The Optical File Cabinefi A Random-Access File System For Write-Once 
Optical Disks. , IEEE Computer, Volume 21, Number 6, June 1988, pp. 11-22 [7] Garcia-Molina, H., and 
Salem, K., Disk Striping. , International Conference on Data Engineering, February 1986, pp. 336-342. 
 [8] Hagmann, R., Reimplementing the Cedar File Sys­tem Using Logging and Group Commit. , Proceed­ings 
Eleventh Symposium on Operating Systems Principles, November 1987, pp. 155-162. [9] Katz, R., Gibson, 
G., and Patterson, D., Disk Sys­tem Architectures for High Performance Comput­ing. Technical Report, 
University of Cal$ornia, Berkeley, UCB/CSD 89/497, March 1989, pp. 1-39. [10] Kim, M., Synchronized Disk 
Interleaving. , ZEEE Transactions on Computers, Volume C-35, Number 11, November 1986, pp. 978-988. [11] 
Lazowti E. et al., File Access Performance of Diskless Workstations. , ACM Transaction on Computer Systems, 
Volume 4, Number 3, August 1986, pp. 238-268. [12] Livny, M, Khoshalian, S., and Boral, H., Multi- Disk 
Management Algorithms. , Proceedings Eleventh Symposium on Operating Systems Princi­ples, November 1987, 
pp. 69-76. [13] McKusick, M.K, et al., A Fast File System for UNIx. , Transactions on Computer Systems, 
Volume 2, Number 3, August 1984, pp. 181-197. [14] Mullender, S., and Tanenbaum, A., Immediate Files 
, Soflware Practice and Experience, Volume 14, Number 4, April 1984, pp. 365-368. [15] Nelson, M., Physical 
Memory Management in a Network Operating System. , PhD. Thesis, Uw ver­sity of Calijorm a, Berkeley, 
November 1988. [16] Nelson, M., Welch, B., and Ousterhout, J. Cach­ing in the Sprite Network File System, 
, ACM Tran­sactions on Computer Systems, Volume 6, Number 1, February 1988, pp. 134-154. [17] Ohm M,, 
and Tezti H., A Fast /tmp File System by Delay Mount Option , USENLY-Summer 90 Technical Conference, 
June 1990, pp. 145-150. [18] 0usterhou4 J., et al., A Trace Driven Analysis of the Unix 4.2 BSD File 
System. , Proceedings Tenth Symposium on Operating Systems Principles, December 1985, pp. 15-24. [19] 
Ousterhout, J., and Douglis, F., Beating the I/O Bottleneck: The Case for Log-Structured File Sys­tems. 
, Operating Systems Review, January 1989, pp. ~1-27, [20] Patterson, D., Gibson, G., and Katz, R., A 
Case for Redundant Arrays of Inexpensive Disks (RAID). , ACM SIGMOD 88, June 1988, pp. 109-116. [21] 
Powell, M., The DEMOS File System. , Proceed­ings Sixth Symposium on Operating Systems Princi­ples, November 
1977, pp. 33-42. [22] Reddy, A., and Banerjee, P., Performance Evalua­tion of Multiple-Disk I/O Systems. 
, 1989 Interna­tional Conference of Parallel Processing, June 1989, pp. 315-318. [23] Rosenblum, M,, 
and Ousterhout, J., The LFS Storage Manager , USENIX-Summer 90 Technical Conference, June 1990, pp. 315-324. 
[24] Satyanarayanan, M., A Study of File Sizes and Functional Lifetimes. , Proceedings Eighth Sympo­sium 
on Operating Systems Principles, December 1981, pp. 96-108. [25] Smith, A., Disk Cache -Miss Ratio Analysis 
and Design Considerations. , ACM Transactions on Computer Systems, Volume 3, Number 3, August 1985, pp. 
161-203.  [26] Svobodova, L., A Reliable Object-Oriented Data Repository for a Distributed Computer 
System. , Proceedings Eighth Symposium on Operating Sys­tems Principles, Decemkex 1981, pp. 47-58. [27] 
Van Renesse, R., Tanenbaum, A. S., and Wilschut, A., The Design of a High-Performance File Server , IEEE 
Transactions on Knowledge and Data Engineering, Volume 1, Number 2, June 1989, pp. 22-27. [28] Yokoyama, 
S. and Yamada, S., A Contiguous High Performance File System , EUUG Spring 89, April 1989, pp. 197-206.</RefA> 
 
			
