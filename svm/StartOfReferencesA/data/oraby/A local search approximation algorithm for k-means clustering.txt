
 A Local Search Approximation Algorithm for k-Means Clustering Tapas Kanungo* David M. Mount Nathan 
S. Netanyahu Christine D. Piatko§ Ruth Silverman¶ Angela Y. WuI ABSTRACT In k-means clustering we are 
given a set of n data points in d­dimensional space .d and an integer k, and the problem is to de­termine 
a set of k points in .d, called centers, to minimize the mean squared distance from each data point to 
its nearest center. No exact polynomial-time algorithms are known for this problem. Although asymptotically 
ef.cient approximation algorithms exist, these algorithms are not practical due to the extremely high 
con­stant factors involved. There are many heuristics that are used in practice, but we know of no bounds 
on their performance. We consider the question of whether there exists a simple and practical approximation 
algorithm for k-means clustering. We pre­sent a local improvement heuristic based on swapping centers 
in and out. We prove that this yields a (9 + E)-approximation algo­rithm. We show that the approximation 
factor is almost tight, by giving an example for which the algorithm achieves an approxima­tion factor 
of (9 - E). To establish the practical value of the heuris­tic, we present an empirical study that shows 
that, when combined with Lloyd s algorithm, this heuristic performs quite well in prac­tice. *IBM Almaden 
Research Center, San Jose, California, 95120. Email: kanungo@almaden.ibm.com. Department of Computer 
Science, University of Maryland, Col­lege Park, Maryland. The support of the National Science Founda­tion 
under grant CCR-0098151 is gratefully acknowledged. Email: mount@cs.umd.edu. Department of Mathematics 
and Computer Science, Bar-Ilan Uni­versity, Ramat-Gan 52900, Israel and Center for Automation Re­search, 
University of Maryland, College Park, Maryland. Email: nathan@macs.biu.ac.il. §The Johns Hopkins University 
Applied Physics Laboratory, Lau­ rel, Maryland. Email: christine.piatko@jhuapl.edu. ¶Center for Automation 
Research, University of Maryland, College Park, Maryland. Email: ruth@cfar.umd.edu. IDepartment of Computer 
Science and Information Systems, American University, Washington, DC. Email: awu@american.­edu. Permission 
to make digital or hard copies of all or part of this work for personal or classroom use is granted without 
fee provided that copies are not made or distributed for pro.t or commercial advantage and that copies 
bear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on 
servers or to redistribute to lists, requires prior speci.c permission and/or a fee. SoCG 02, June 5-7, 
2002, Barcelona, Spain. Copyright 2002 ACM 1-58113-504-1/02/0006 ...$5.00.  Categories and Subject Descriptors 
I.5.3 [Pattern Recognition]: Clustering; F.2.2 [Analysis of Algo­rithms]: Nonnumerical Algorithms and 
Problems Geometrical problems and computations General Terms Algorithms  Keywords Clustering, k-means, 
approximation algorithms, local search, com­putational geometry 1. INTRODUCTION Clustering problems arise 
in many different applications, includ­ing data mining and knowledge discovery [13], data compression 
and vector quantization [16], and pattern recognition and pattern classi.cation [10]. There are many 
criteria for de.ning clusters, for example, splitting and merging methods such as ISODATA [5, 18], randomized 
approaches such as CLARA [22] and CLARANS [29], and methods based on neural nets [24]. For further informa­tion 
on clustering and clustering algorithms and applications see also [7, 17, 18, 19, 20, 22]. One of the 
most popular and widely studied clustering methods for points in Euclidean space is called k-means clustering.Given 
a set P of n data points in real d-dimensional space, .d, and an integer k, the problem is to determine 
a set of k points in .d, called centers, so as to minimize the mean squared Euclidean distance from each 
data point to its nearest center. This measure is often called the squared-error distortion [16, 18]. 
Clustering based on k-means is closely related to a number of other clustering and location problems. 
These include the Euclidean k-medians and the Weber problem [3, 25], in which the objective is to minimize 
the sum of distances to the nearest center, and the Euclidean k-center problem [1], in which the objective 
is to mini­mize the maximum distance. There are no ef.cient exact solutions known to any of these problems, 
and some formulations are NP­hard [15]. Given E> 0,an E-approximation algorithm returns a distortion 
. that is within a relative error of E of the optimal distortion .o, that is, . = (1 + E).o. Matou.sek 
[28] achieved an important breakthrough in this area by presenting an asymptotically ef.cient O(n logk 
n) time E-approximation algorithm for k-means cluster­ing, under the assumptions that k and d are .xed. 
First, Matou. sek showed how to compute a set of O(nE-d) candidate centers, called an E-approximate centroid 
set, from which an approximately opti­mal solution may be drawn. He then showed that a near-optimal solution 
may be assumed to be well-spread, generalizing the con­ cept of well-separated pairs to k-tuples (see 
[28] for de.nitions). He proved that there are O(nE-k2d) such well-spread sets. The algorithm generates 
all these sets and returns the set with the mini­mum distortion. Unfortunately, the constant factors 
are well beyond the practical range unless d and k are very small. For example, by consider­ vv ing points 
uniformly distributed in a two-dimensional k × k square it is not hard to show that to produce a relative 
error of E, the points of an E-approximate centroid set must be spaced at in­ v tervals of roughly E/2. 
It follows that the number of well-spread k-tuples that the algorithm generates at least E-k. In typical 
appli­cations k ranges from 10 to 500, so this is well beyond practical limits. The dynamic program approximation 
algorithm presented by Kolliopoulos and Rao for the k-median problem [25] is also a candidate for modi.cation, 
but also suffers from similarly large constant factors. Many practical heuristics for k-means clustering 
are known and widely used in practice, based on methods such as simulated annealing and genetic algorithms 
[6, 11, 33] but we know of no proven approximation bounds for any of these heuristics. One of the most 
popular heuristics for solving the k-means prob­lem is based on a simple iterative scheme for .nding 
a locally opti­mal solution. This algorithm is often called the k-means algorithm [14, 27]. There are 
a number of variants to this algorithm, so to clarify which version we are using, we will refer to it 
as Lloyd s algorithm. Lloyd s algorithm is based on an easily proven cri­terion for local minimality. 
De.ne the neighborhood of a center point to be the set of data points for which this center is the clos­est. 
In any locally minimal solution, each center lies at the centroid of its neighborhood. Such a solution 
is said to be centroidal [9, 12]. Lloyd s algorithm works by starting with any feasible solu­tion and 
then repeatedly computes the neighborhood of each center and moves this center to the centroid of its 
neighborhood. This is repeated until some convergence criterion is satis.ed. Computing nearest neighbors 
is the most expensive step in Lloyd s algorithm. A number of practical algorithms have been discovered 
recently [2, 21, 30, 31, 32]. Unfortunately, it is easy to construct situations in which Lloyd s algorithm 
converges to a local minimum which is arbitrarily bad compared to the optimal solution. Consider, for 
example, Fig. 1 for k =3 and where x<y<z. The optimal solution has a distortion of x 2/4. It is easy 
to verify that the heuristic solution shown below is at a stable point for Lloyd s algorithm and has 
a distortion of y 2/4. By increasing the ratio y/x the approximation ratio for Lloyd s algorithm can 
be made arbitrarily high. y zx Data points Optimal centers Heuristic centers Figure 1: Lloyd s algorithm 
can produce an arbitrarily high approximation ratio. This raises the question of whether there exists 
a simple and practical heuristic for k-means clustering, which is both easy to im­plement and provides 
guarantees on the quality of the .nal results. Such simple heuristics exist for the facility location 
and the metric k-median problems [4, 8, 26]. These algorithms are based on local search, that is, by 
incrementally changing the solution by swapping a small number of points in and out of the solution. 
In this paper we present such a heuristic based on a simple swapping process. In Sections 3 and 4 we 
derive an approximation ratio of 9+E for the heuristic. Our approach is based on the heuristic for k-medians 
presented by Arya et al. [4]. However, due to the different nature of the k-means problem, the analysis 
is quite different and relies on geometric properties that are particular to the k-means problem. In 
Section 5 we show that this bound is essentially tight by presenting a multi-dimensional example in which 
the algorithm achieves an approximation ratio of 9- E. In Section 6 we present empirical ev­idence that 
this algorithm, in conjunction with Lloyd s algorithm, can provide quite good results even for large 
data sets, performing as well or better than the popular Lloyd s algorithm with similar running time. 
 2. PRELIMINARIES Let d denote real d-dimensional space. Given u, v .. d , let .(u, v) denote the squared 
Euclidean distance between these points, that is d .(u, v)= dist2(u, v)= (ui - vi)2 =(u - v)· (u - v), 
i=1 where u · v denotes the dot product of vectors u and v. Given a .nite set S .. d, de.ne its distortion 
relative to any point v to be .(S, v)=u.S .(u, v). Consider a set P of n data points in d and an integer 
k.Given any set S of k points, for any q .. d de.ne sq to be the closest point in S to q. Our goal is 
to compute the k-element point set S that minimizes the total distortion of S relative to P , de.ned 
as .P (S)=.(q, sq). q.P When P is understood, we will refer to this simply as .(S). The principal dif.culty 
in extending existing approaches for the metric k-medians problem to k-means is that squared distances 
do not de.ne a metric, and in particular they do not satisfy the triangle inequality, which states that 
for any points u, v, and w, dist(u, w)= dist(u, v)+dist(v, w). When considering squared distances we 
have .(u, w) = (dist(u, v)+dist(v, w))2 = dist2(u, v)+2dist(u, v)dist(v, w)+dist2(v, w) = .(u, v)+.(v, 
w)+2dist(u, v)dist(v, w). The .nal product term can be bounded by observing that 2ab = a 2 +b2, for any 
a and b. Hence we have the following doubled triangle inequality. .(u, w)= 2(.(u, v)+.(v, w)). One obvious 
idea for producing a local improvement heuristic for k-means would be to generalize the methods of Arya 
et al. [4] for the metric k-median problem using the doubled triangle in­equality. Unfortunately, this 
does not seem to work because their analysis relies crucially on the triangle inequality. In particular, 
a critical cancellation of terms that arises in their analysis fails to hold when the triangle inequality 
is doubled. Our approach is based on two ideas. The .rst is the introduction of an alternative to the 
triangle inequality, which, unlike the dou­bled triangle inequality is sensitive to the ratio of the 
optimal and heuristic solution (see Lemma 3 below). The second is based on the fact that the optimal 
solution is centroidal. Let NS(s)denote the neighborhood of s, that is the set of data points that are 
closer to s than to any point in S. By treating points as vectors, the centroidal property implies that 
1 s = u. |NS (s)| u.NS(s) An important property of centroidal solutions, which we will use in our analysis, 
is presented in the following lemma. It states that for the purposes of computing distortions, a set 
of points may be treated almost like a point mass centered about its centroid. LEMMA 1. Given a .nite 
subset S of points in d, let c be the centroid of S. Then for any cd .. d , .(S, cd)=.(S, c)+ |S|.(c, 
cd). PROOF. By expanding the de.nition of .(S, cd) we have dddd .(S, c)= .(u, c)= (u -c) ·(u -c) u.Su.S 
dd = ((u -c)+(c -c)) ·((u -c)+(c -c)) u.S d = ((u -c) ·(u -c)) + 2((u -c) ·(c -c)) u.S dd +((c -c) ·(c 
-c)) () d =.(S, c)+2 (c -c) · (u -c) u.S +|S|((c -cd) ·(c -cd)) =.(S, c)+ |S|.(c, cd), The last step 
follows from the fact that if c is S s centroid then u.S (u -c) is the zero vector. 3. THE SINGLE-SWAP 
HEURISTIC To illustrate the method, we .rst present a simple local search that provides a (25 + E)-approximation 
to the k-means problem. Our approach is similar to approaches used in other local search heuristics for 
facility location and k-medians by Charikar and Guha [8] and Arya et al. [4]. In the statement of the 
k-means problem, the centers may be placed anywhere in space. In order to apply our local improve­ment 
search, we need to assume that we are given a discrete set of candidate centers C from which k centers 
may be chosen. As mentioned above, Matou. sek [28] showed that C may be taken to be an E-approximate 
centroid set of size O(nE-d log(1/E)), which can be computed in time O(n log n+nE-d log(1/E)). Henceforth, 
when we use the term optimal, we mean the k-element subset of C having the lowest distortion. This single-swap 
heuristic operates by selecting an initial set of k centers S from the candidate centers C, and then 
it repeatedly attempts to improve the solution by removing one center s . S and replacing it with another 
center sd . C -S. Let Sd = S - {s}.{sd}be the new set of centers. If the modi.ed solution has lower distortion 
then Sd replaces S, and otherwise S is unchanged. In practice this process is repeated until some long 
consecutive run of swaps have been performed with no signi.cant decrease in the distortion. It can be 
formally proved that by sacri.cing a small factor E> 0 in the approximation ratio, we can guarantee that 
this procedure converges after a polynomial number of swaps. We refer the reader to Arya et al. [4] for 
further details. For simplicity, we will assume that the algorithm terminates when no single swap results 
in a decrease in distortion. Such a set of cen­ters is said to be 1-stable. Letting O denote an optimal 
set of k centers, a set S of k centers is 1-stable then we have .(S -{s}.{o}) =.(S) for all s .S, o .O. 
(1) (In fact this is true no matter what O is, but this is the only property of 1-stable sets that will 
be used in our analysis.) Using this fact along with the fact that the optimal solution is centroidal, 
we will establish the main result of this section, which is stated below. THEOREM 1. Let S denote a 1-stable 
set of k centers, and let O denote the optimal set of k centers. Then .(S) =25.(O). Note that the actual 
approximation bound is larger by some E> 0, due to the errors induced by using a discrete set of candidate 
cen­ters C and the approximate convergence criterion described above. Our analysis is similar in structure 
to that given by Arya et al. [4], but there are two signi.cant differences. The .rst is that our notion 
of capturing a center is different from theirs, and is based on the distance to the closest center, rather 
than on the numbers of data points assigned to a center. The second is that the permutation function 
p is not needed in our case, and instead we rely on the centroidal properties of the optimal solution. 
For each optimal center o .O, let so denote its closest heuristic center in S. We say that o is captured 
by so. Note that each optimal center is captured by exactly one heuristic center, but each heuristic 
center may capture any number of optimal centers. We say that a heuristic center is lonely if it captures 
no optimal center. The analysis is based on constructing a set of swap pairs, considering the total change 
in distortion that results, and then apply Eq. (1) above to bound the overall change in distortion. We 
begin by de.ning a simultaneous partition of the heuristic centers and optimal centers into two sets 
of groups S1,S2,...,Sr and O1,O2,...,Or for some r, such that |Si|= |Oi|for all i.For each heuristic 
center s that captures some number m = 1 of op­timal centers, we form a group of m optimal centers consisting 
of these captured centers. The corresponding group of heuristic cen­ters consists of s together with 
any m -1 lonely heuristic centers. (See Fig. 2.) S1 S2 S3 S4S5 Heuristic centers Optimal centers O1 
O2 O3 O4O5 Partition S1 S2 S3 S4S5 Heuristic centers     Optimal centers O1 O2O3 OO5 4 Swap pairs 
Figure 2: Partitioning of the heuristic and optimal centers for analysis and the swap pairs. On the top, 
edges represent the capturing relation, and on the bottom they represent swap pairs. We generate the 
swap pairs as follows. For every partition that involves one captured center we generate a swap pair 
consisting of the heuristic center and its captured center. For every partition containing two or more 
captured centers we generate swap pairs between the lonely heuristic centers and the optimal centers, 
so that each optimal center is involved in exactly one swap pair and each lonely center is involved in 
at most two swap pairs. It is easy to verify that: (1) each optimal center is swapped in exactly once, 
(2) each heuristic center is swapped out at most twice, and (3) if s and o are swapped, then s does not 
capture any optimal center other than o. We establish an upper bound on the change in distortion resulting 
from any such swap pair (s, o)by prescribing a feasible (but not necessarily optimal) assignment of data 
points to the centers S - {s}.{o}. The data points in NO(o) are assigned to o, implying a change in distortion 
of (.(q, o) -.(q, sq)). (2) q.NO(o) Each point q . NS (s)\NO(o) has lost s as a center and must be reassigned 
to a new center. Let oq denote the closest optimal center to q. Since q is not in NO(o) we know that 
oq = o and hence s does not capture oq. Therefore, soq , the nearest heuristic center to oq, exists after 
the swap. We assign q to soq . Thus the change in distortion due to this reassignment is at most (.(q, 
soq ) -.(q, s)). (3) q.NS(s)\NO(o) By combining over all swap pairs the change in distortion due to optimal 
assignment and reassignment together with Eq. (1) we have the following. LEMMA 2. Let S be a 1-stable 
set of k centers, and let O be an optimal set of k centers, then 0 = .(O) -3.(S)+2R, where R = q.P .(q, 
soq ). PROOF. Consider just the swap pair (s, o). By Eqs. (2) and (3) and the fact that S is 1-stable 
we have (.(q, o) -.(q, sq))+ q.NO(o) (.(q, soq ) -.(q, s)) =0. q.NS (s)\NO(o) To bound the sum over all 
swap pairs, we recall that each optimal center is swapped in exactly once, and hence each point q con­tributes 
once to the .rst sum. Note that the quantity in the second sum is always nonnegative (because soq . S 
and s is the closest center in S to q). Hence by extending the sum to all q .NS (s) we can only increase 
its value. Recalling that each heuristic center is swapped in at most twice we have 0 = (.(q, oq) -.(q, 
sq))+2 (.(q, soq ) -.(q, sq)) q.Pq.P 0 = .(q, oq) -3 .(q, sq))+2 .(q, soq ) q.Pq.Pq.P 0 = .(O) -3.(S)+2R, 
from which the desired conclusion follows. The term R above is called the total reassignment cost. By 
ap­plying Lemma 1 to each optimal neighborhood, we have R = .(q, so)= .(NO(o),so) o.Oq.NO(o) o.O = (.(NO(o),o)+ 
|No(O)|.(o, so)) o.O = (.(q, o)+.(o, so)) o.Oq.NO(o) = (.(q, o)+.(o, sq)) o.Oq.NO(o) = (.(q, oq)+.(oq,sq)), 
q.P where the next to last step follows because so is the closest heuristic center to o, and so for 
each q .NO(o), .(o, so) =.(o, sq).By applying the triangle inequality to the last term and then squaring 
and simplifying we obtain R = .(q, oq)+ (dist(oq,q)+ dist(q, sq))2 q.Pq.P = .(q, oq)+ (dist2(oq,q)+ q.Pq.P 
2dist(oq,q)dist(q, sq)+ dist2(q, sq)) =2 .(q, oq)+ .(q, sq)+ q.Pq.P 2 dist(q, oq)dist(q, sq) q.P =2.(O)+.(S)+2 
dist(q, oq)dist(q, sq). q.P To bound the last term we will apply the following technical lemma. LEMMA 
3. Let (oi) and (si) be two sequences of reals, such that a2 =( i si 2)/( i oi 2), for some a> 0. Then 
nn 1 oisi = si 2 . a i=1 i=1 PROOF. By Schwarz s inequality we have ()1/2 ()1/2 n nn oisi = oi 2 si 2 
i=1 i=1 i=1 ()1/2 ()1/2 nn n 11 22 2 = a2 si si = asi , i=1 i=1 i=1 as desired. To complete the analysis, 
let the oi sequence consist of dist(q, oq) over all q . P, and let the si sequence consist of dist(q, 
sq). Let a denote the square root of the approximation ratio, so that n 2 dist2(q, sq) s 2 .(S) q.Pi=1 
i a== = . .(O) dist2(q, oq) n o2 q.Pi=1 i By applying Lemma 3 we have 2 R = 2.(O)+.(S)+ dist2(q, sq) 
a q.P 2 =2.(O)+.(S)+ .(S) a () 2 =2.(O)+1+ .(S). a Now we combine this with Lemma 2, yielding (()) 2 
 0 = .(O) -3.(S)+2 2.(O)+1+ .(S) a () 4 = 5.(O) -1 - .(S). (4) a Through simple rearrangements we can 
express this in terms of a alone. 5 .(S) = = a2 1 -4/a .(O) () 4 5 = a2 1 - a 0 = a2 -4a -5=(a -5)(a 
+1). This implies that a =5, and hence the approximation ratio of the simple heuristic is bounded by 
a2 =25. This completes the proof of Theorem 1.  4. THE MULTIPLE-SWAP HEURISTIC We generalize the single-swap 
approach to provide a factor 9+ E approximation ratio. Rather than swapping a single pair of points at 
any time, for some integer p, we consider simultaneous swaps between any subset of S of size p d =p with 
any p d-element subset of candidate centers. Otherwise the algorithm is the same. We say that a set of 
centers is p-stable if no simultaneous swap of p elements decreases the distortion. Our main result is 
given below. As before, there is an additional E term in the .nal error because of the use of the discrete 
candidate centers and the approximate convergence conditions. THEOREM 2. Let S denote a p-stable set 
of k centers, and let O (r2 denote the optimal set of k centers. Then .(S) =3+ p 2 .(O). Again our approach 
is similar to that of Arya et al. [4], but using our different notion of capturing. We de.ne our swaps 
as follows. Recall the simultaneous partitions of heuristic and optimal centers used in the simple heuristic. 
If for some i, |Si|= |Oi|=p, then we create a simultaneous swap involving the sets Si and Oi. Oth­erwise, 
if |Si|= |Oi|= m>p, then for each of the m -1 lonely centers of Si we generate individual 1-for-1 swaps 
with all m op­timal centers of Oi. For the purposes of the analysis, the change in distortion due to 
each of these 1-for-1 swaps is weighted by a multiplicative factor of 1/(m -1). (For example, Fig. 3 
shows the swaps that would result from Fig. 2 for p =3. The swaps ap­pearing in shaded boxes are performed 
simultaneously. The 1-for-1 swaps performed between S1 and O1 are each weighted by 1/4.) It is easy to 
verify that: (1) each optimal center is swapped in with total weight 1, (2) each heuristic center is 
swapped out with weight at most 1+1/p, and (3) if sets Sd and Od are swapped, then Sd captures no optimal 
centers outside of Od . The analysis proceeds in the same manner as the simple case. Because of the replacement 
of the factor 2 with (1 + 1/p), the S S1 S2 3S4S5   weighted by 1/4 O1 O2 O3 O4O5 Figure 3: Swaps for 
p =3. Shaded regions indicate swaps that are performed simultaneously. inequalities in the proof of Lemma 
2 now become 0 = (.(q, oq) -.(q, sq)) + q.P () 1 1+ (.(q, soq ) -.(q, sq)) p q.P () () 11 0 = .(O) - 
2+ .(S)+ 1+ R. pp The analysis and the de.nition of a proceed as before, and Eq. (4) becomes () 1 0 = 
.(O) -2+ .(S)+ p ()( ()) 12 1+ 2.(O)+1+ .(S) pa ()(()) 2 21 = 3+ .(O) -1 - 1+ .(S). pap Again, by rearranging 
and expressing in terms of a we have 3+(2/p) .(S) = = a2 1 -(2/a)(1 + 1/p) .(O) ()() 12 0 = a2 -2a 1+ 
-3+ pp (( )) 2 = a - 3+ (a +1). p This implies that a =3+2/p, and hence the approximation ratio of the 
general heuristic is a2, which approaches 9 as p increases. 5. A TIGHT EXAMPLE It is natural to ask 
whether the factor 9 is the correct approxi­mation factor for swap-based heuristics, or whether it arises 
from some slackness in our analysis. In this section we provide evidence that this is probably close 
to the correct factor, or if not, then signif­icantly different considerations would be required in the 
analysis. We show that for any p, there is a con.guration of points in a suf.ciently high dimensional 
space such that the p-swap heuristic achieves a distortion that is (9 -E) times optimal. This example 
has the nice property that it is centroidal. This implies that it is also a local minimum for Lloyd s 
algorithm. Hence neither the swap heuristic nor Lloyd s algorithm would be able to make further progress. 
We make the assumption that centers may only be placed at a given discrete set of candidate locations. 
This candidate set is reasonable in that it contains an E-approximately optimal solution. Overcoming 
this assumption would imply that the entire analysis method would somehow need to be generalized to handle 
swaps with points other than the optimal centers. Arya et al. [4] presented a tight example for their 
heuristic in a metric space. However, their example cannot be embedded in Euclidean space of any dimension 
and does not even allow centers to be placed at data points. Our approach is quite different. THEOREM 
3. Given p and E> 0, there exists an integer k,a dimension d, a .nite set of points P .. d, a .nite set 
of candidate centers C, and a set S . C of k centers, such that the following hold. (i) C contains an 
E-approximately optimal solution. (ii) S is p-stable.  (iii) .(S) = (9 - E).(O), where O is the optimal 
k-means solution. In the rest of this section we provide a proof of this theorem. Let d (dimension) and 
N be integer parameters to be speci.ed later. Our framework consists of a large d-dimensional integer 
grid, G =[1..N]d . The grid points are d-dimensional integer vectors, where each coordinate is in {1,2,...,N}. 
Let Gd =[2..N - 1]d denote a smaller central grid contained within G. The points of G are labeled even 
or odd, depending on the parity of the sum of co­ordinates. Consider a parameter x, 0 <x< 1/2, to be 
.xed later. Let T(x) be the following set of 2d points displaced at a distance +xand -xfrom the origin 
along each of the coordinate axes. (±x,0,0,...,0),(0,±x,0,...,0),...,(0,0,0,...,±x). The data set P consists 
of the union of translates of T(x) each centered about an even grid point. (See Fig. 4.) We set k =(N 
- 2)d/2 and place one point of S at each of the odd grid points in Gd. Consider a solution Od consisting 
of the even grid points in Gd . It is easy to see that, except for some of the points of G- Gd, the neighborhood 
of each point of S consists of 2d points at distance 1 - x and the neighborhood of each point in Od consists 
of 2d points at distance x. Since x< 1/2, it is easy to see that the best that any solution can achieve 
would be to place one center at the centroid of each copy of T(x), that is, at each of the even grid 
points in G. However, k is not large enough to permit this, but for all suf.ciently large N, .(Od) can 
be made arbitrarily close to the optimum. Henceforth, we ignore these boundary affects, since they play 
an insigni.cant role in the distortions. Each optimal center has a neighborhood of 2dpoints at distance 
x, and each heuristic center has a neighborhood of 2d points at distance (1 - x). Thus we have .(S) (1 
- x)2 = . .(O) x2 We argue below that by choosing x =1/(4 - p/d),no p-swap involving points of Sand C 
can improve the distortion. By making dsuf.ciently large relative to p, this implies that the approximation 
ratio is arbitrarily close to (3/4)2/(1/4)2 =9, as desired. To show that no p-way swap improves the distortion, 
consider any simultaneous swap between two p-element subsets Sd and Od of heuristic and optimal centers, 
respectively. Because the optimal neighborhoods are disjoint and each contains 2dpoints, the change in 
distortion due to assigning these points to their new optimal cen­ter is 2dp(x 2 - (1 - x)2)=2dp(2x- 
1). No other points are assigned to a closer center. Now consider the 2dp neighbors of heuristic centers 
that have now been removed. These data points must be reassigned to the Data point Optimal center Heuristic 
center o   s  Figure 4: Example of the lower bound in the plane. Black cir­cles are the data points, 
hollow circles denote the optimal cen­ters, and crosses denote the heuristic centers. nearest existing 
center. After performing the swap, there are at most p 2 pairs (s,o), where s . S and o . O, such that 
sand oare adjacent to each other in the grid. For these points no additional reassignment is needed because 
the point has been moved to its op­timal center. For the remaining 2dp- p 2 neighbors of the heuristic 
centers, we need to reassign them to a new center. The closest such v ( (2dp- p )((1 + x ) - (1 - x))=2dp1 
- 2x. center is at distance to reassignment is x2 +1. Hence the change in distortion due 2 2 2 p r 2d 
Combining these two, the total change in distortion is ((rr ((rr p p 2dp2x- 1+ 1 - 2x =2dp2 - 2x- 1 . 
2d 2d This is nonnegative if we set x =1/(4 - p/d), and hence the p-swap heuristic cannot make progress 
on this example. This es­tablishes Theorem 3.  6. EXPERIMENTAL RESULTS Given the relatively high approximation 
factors involved and the tight example, an important question is whether the swap-based heuristics perform 
well enough to be of any practical value. In this section we argue that indeed these heuristics can be 
of signi.cant value, not so much as a stand-alone method, but rather in conjunc­tion with a local improvement 
algorithm such as Lloyd s algorithm. It is quite easy to see why such a merger is pro.table. As men­tioned 
earlier, Lloyd s can get stuck in local minima. One com­mon approach for dealing with this is by running 
the algorithm re­peatedly with different random starting sets. In contrast, the swap heuristic is capable 
of moving out of a local minimum, but it may take very long to move into one. By alternating between 
the two methods we have a simple heuristic that takes advantage of both method s strengths. This is similar 
in spirit to methods based on simulated annealing [23], but without the complexities of de.ning temperature 
schedules and with provable performance guarantees. Our implementation of the swap heuristic differs 
slightly from the description in this paper in a couple of respects. First, we sam­pled pairs randomly, 
rather than applying some systematic enumer­ation. Second, for simplicity we use the data points as candidates 
rather than an E-approximate centroid set [28]. Also, rather than performing p swaps simultaneously, 
our algorithm performs swaps one by one. As soon as the distortion improves, this swapping pro­cess stops, 
and a new process is started. In this way prior gains in distortion are never lost. We also implemented 
an iterative version of Lloyd s algorithm. In this algorithm, centers are chosen randomly, and some number 
of stages of Lloyd s algorithm are performed. Recall that each stage consists of computing the neighborhood 
of each center point, and then moving each center point to the centroid of its neighborhood. Stages are 
repeated until the relative decrease in distortion over three consecutive stages is less than, say 10%. 
Then a new ran­dom set of centers is generated and the process is repeated until the total number of 
stages exceeds a prespeci.ed bound. The centers producing the best distortion are saved. Finally, we 
implemented a combination of the swap heuristic with Lloyd s algorithm. This algorithm augments the swap 
step by .rst applying one step of the swap algorithm and then follows this with one run of Lloyd s algorithm. 
The programs were written in C++ and run on a Sun Ultra 5 workstation with 256 MB of memory. We considered 
two syn­thetic distributions and three real data sets taken from applications in vector quantization 
and pattern classi.cation. ClusGauss: The data consist of 10,000 points in 3, which were generated from 
a distribution consisting of 100 clusters of roughly equal sizes, with centers uniformly distributed 
in a cube of side length 2. The points of each cluster are drawn from a multivariate Gaussian distribution 
centered at the clus­ ter center, where each coordinate has a standard deviation of 0.05. We ran experiments 
with k .{75, 100, 200}. MultiClus: The data consist of n =10, 000 points in 3 which were generated from 
a distribution consisting of a number of multivariate Gaussian clusters of various sizes and standard 
deviations. Again cluster centers were sampled uniformly from a cube of side length 2. The cluster sizes 
are powers of 2. The probability of generating a cluster of size 2i is 1/2i, and hence there are many 
small clusters. The standard v deviation of a cluster of size m is 0.05/m, and hence each cluster is 
expected to have roughly equal distortion of 0.025. Lena22 and Lena44: These were taken from an application 
in im­age compression through vector quantization. The data were generated by partitioning a 512×512 
gray-scale Lena image into 65,536 2 ×2 tiles. Each tile is treated as a point in a 4­dimensional space. 
Lena44 was generated using 4 ×4 tiles, thus generating 16,384 points in 16-dimensional space. Kiss: This 
is from a color quantization application. 10,000 RGB pixel values were sampled at random from a color 
image of a painting The Kiss by Gustav Kimt. This resulted in 10,000 points in 3-space. Forest: This 
data set came from the UCI Knowledge Discovery in Databases Archive. The data set relates forest cover 
type for 30 ×30 meter cells, obtained from the US Forest Service. The .rst 10 dimensions contain integer 
quantities, and the remaining 44 are binary values (mostly 0 s). We sampled 20,000 points at random from 
the entire data set of 581,012 points in dimension 54. For all algorithms the initial centers were taken 
to be a random sample of the point set. All the algorithms were started with these same initial centers. 
Each time the set of centers is changed, the distortion is recomputed. The combination of modifying the 
set of centers and recomputing distortions is called a stage. We measured convergence rates by tracking 
the lowest distortion encountered as a function of the number of stages executed. We also computed the 
average CPU time per stage. We use the .ltering algorithm from [21] for computing distortions for all 
the algorithms. The results in each case were averaged over .ve trials having different random data points 
(for the synthetic examples) and different random initial centers. We ran the swap and hybrid algorithms 
for p .{1, 2}swaps. All algorithms were run for 500 stages. Figure 5: Comparison of the average distortions 
versus number of stages of execution for MultiClus data. Note that the y-axis does not start at zero 
and is on a log scale. A rather typical plot of the best average distortion found versus the number of 
stages is shown in Fig. 5 for the Multi-Cluster distri­bution and 500 centers. After a small number of 
stages both Lloyd s algorithm and hybrid algorithms rapidly converge. However, af­ter this initial start 
it rarely makes signi.cant gains in distortion. The swap heuristics tend to converge very slowly, and 
even after 500 stages they do not surpass the progress that Lloyd s algorithm makes in its .rst 10 50 
stages. In contrast the hybrid methods per­form at least as well as Lloyd s algorithm. In most instances 
the hybrid method and Lloyd s method produce essentially the same distortion. However, for some data 
sets, the hybrid methods can perform signi.cantly better. Our results are given in Table 1. It shows 
the best distortions at stages 50, 100, and 500, and CPU times. To facilitate comparison, single-swap 
and single-swap hybrid are given as percentages rela­tive to Lloyd s. The 2-swap heuristics performed 
very similarly to single-swap, and are not shown here. Table 1: Summary of Experiments. Absolute values 
are indicated for Lloyd s algorithm, and the other values are given as a percent­age of increase (positive) 
or decrease (negative) relative Lloyd s algorithm.  7. REFERENCES <RefA>[1] P. K. Agarwal and C. M. Procopiuc. 
Exact and approximation algorithms for clustering. In Proceedings of the Ninth Annual ACM-SIAM Symposium 
on Discrete Algorithms, pages 658 667, San Francisco, CA, January 1998. [2] K. Alsabti, S. Ranka, and 
V. Singh. An ef.cient k-means clustering algorithm. In Proceedings of the First Workshop on High Performance 
Data Mining, Orlando, FL, March 1998. [3] S. Arora, P. Raghavan, and S. Rao. Approximation schemes for 
Euclidean k-median and related problems. In Proceedings of the Thirtieth Annual ACM Symposium on Theory 
of Computing, pages 106 113, Dallas, TX, May 1998. [4] V. Arya, N. Garg, R. Khandekar, V. Pandit, A. 
Meyerson, and K. Munagala. Local search heuristics for k-median and facility location problems. In Proceedings 
of the 33rd Annual Symposium on Theory of Computing, pages 21 29, Crete, Greece, July 2001. [5] G. H. 
Ball and D. J. Hall. Some fundamental concepts and synthesis procedures for pattern recognition preprocessors. 
In International Conference on Microwaves, Circuit Theory, and Information Theory, Tokyo, Japan, September 
1964. [6] S. Bandyopadhyay, U. Maulik, and M. K. Pakhira. Clustering using simulated annealing with probabilistic 
redistribution. International J. Patt. Recog. and Artif. Intell, 15:269 285, 2001. [7] V. Capoyleas, 
G. Rote, and G. Woeginger. Geometric clusterings. Journal of Algorithms, 12:341 356, 1991. [8] M. Charikar 
and S. Guha. Improved combinatorial algorithms for the facility location and k-medians problem. In Proceedings 
of the 4th Annual IEEE Symposium on Foundations of Computer Science, pages 378 388, 1999. [9] Q. Du, 
V. Faber, and M. Gunzburger. Centroidal Voronoi tesselations: Applications and algorithms. SIAM Review, 
41:637 676, 1999. [10] R. O. Duda and P. E. Hart. Pattern Classi.cation and Scene Analysis. John Wiley 
&#38; Sons, New York, NY, 1973. [11] A. E. ElGamal, L. A. Hemanchandra, I. Shperling, and V. K. Wei. 
Using simulated annealing to design good codes. IEEE Trans. Information Theory, 33:116 123, 1987. [12] 
V. Faber. Clustering and the continuous k-means algorithm. Los Alamos Science, 22:138 144, 1994. [13] 
U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy. Advances in Knowledge Discovery and 
Data Mining. AAAI/MIT Press, 1996. [14] E. Forgey. Cluster analysis of multivariate data: Ef.ciency vs. 
interpretability of classi.cation. Biometrics, 21:768, 1965. [15] M. R. Garey and D. S. Johnson. Computers 
and Intractability: A Guide to the Theory of NP-Completeness. W. H. Freeman, New York, NY, 1979. [16] 
A. Gersho and R. M. Gray. Vector Quantization and Signal Compression. Kluwer Academic, Boston, MA, 1992. 
[17] M. Inaba, N. Katoh, and H. Imai. Applications of weighted Voronoi diagrams and randomization to 
variance-based k-clustering. In Proceedings of the Tenth Annual ACM Symposium on Computational Geometry, 
pages 332 339, Stony Brook, NY, June 1994. [18] A. K. Jain and R. C. Dubes. Algorithms for Clustering 
Data. Prentice Hall, Englewood Cliffs, NJ, 1988. [19] A. K. Jain, P. W. Duin, and J. Mao. Statistical 
pattern recognition: A review. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(1):4 
37, 2000. [20] A. K. Jain, M. N. Murty, and P. J. Flynn. Data clustering: A review. ACM Computing Surveys, 
31(3):264 323, 1999. [21] T. Kanungo, D. M. Mount, N. S. Netanyahu, C. Piatko, R. Silverman, and A. Y. 
Wu. An ef.cient k-means clustering algorithm: Analysis and implementation. IEEE Trans. Patt. Anal. Mach. 
Intell., 24, 2002. (To appear). [22] L. Kaufman and P. J. Rousseeuw. Finding Groups in Data: An Introduction 
to Cluster Analysis. John Wiley &#38; Sons, New York, NY, 1990. [23] S. Kirkpatrick, Jr. Gelatt, and 
M.P. Vecchi. Optimization by simulated annealing. Science, 220:671 680, 1983. [24] T. Kohonen. Self-Organization 
and Associative Memory. Springer-Verlag, New York, NY, 3rd edition, 1989. [25] S. Kolliopoulos and S. 
Rao. A nearly linear-time approximation scheme for the Euclidean k-median problem. In J. Nesetril, editor, 
Proceedings of the Seventh Annual European Symposium on Algorithms, volume 1643 of Lecture Notes Comput. 
Sci., pages 362 371. Springer-Verlag, July 1999. [26] M. Korupolu, C. Plaxton, and R. Rajaraman. Analysis 
of a local search heuristic for facility location problems. In Proceedings of the Ninth Annual ACM-SIAM 
Symposium on Discrete Algorithms, pages 1 10, San Francisco, CA, January 1998. [27] J. MacQueen. Some 
methods for classi.cation and analysis of multivariate observations. In Proceedings of the Fifth Berkeley 
Symposium on Mathematical Statistics and Probability, volume 1, pages 281 296, Berkeley, CA, 1967. [28] 
J. Matou.sek. On approximate geometric k-clustering. Discrete and Computational Geometry, 24:61 84, 2000. 
[29] R. T. Ng and J. Han. Ef.cient and effective clustering methods for spatial data mining. In Proceedings 
of the Twentieth International Conference on Very Large Databases, pages 144 155, Santiago, Chile, September 
1994. [30] D. Pelleg and A. Moore. Accelerating exact k-means algorithms with geometric reasoning. In 
Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 
277 281, San Diego, CA, August 1999. [31] D. Pelleg and A. Moore. x-means: Extending k-means with ef.cient 
estimation of the number of clusters. In Proceedings of the Seventeenth International Conference on Machine 
Learning, Palo Alto, CA, July 2000. [32] S. J. Phillips. Acceleration of k-means and related clustering 
problems. In Workshop on Algorithm Engineering and Experiments (ALENEX 02), January 2002. [33] J. Vaisey 
and A. Gersho. Simulated annealing and codebook design. In Proc. IEEE Int l. Conf. on Acoustics, Speech, 
and Signal Processing (ICASSP), pages 1176 1179, 1988.</RefA>  
			
