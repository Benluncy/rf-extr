
 Permission to make digital or hard copies of part or all of this work or personal or classroom use is 
granted without fee provided that copies are not made or distributed for profit or commercial advantage 
and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, 
to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. SC 
'95, San Diego, CA &#38;#169; ACM 1995 0-89791-985-8/97/0011 $3.50 A Novel Approach Towards Automatic 
Data Distribution Jordi Garcia, Eduard Ayguadé and Jesús Labarta Computer Architecture Department Universitat 
Politècnica de Catalunya Abstract: Data distribution is one of the key aspects that a parallelizing compiler 
for a distributed memory architecture should consider, in order to get ef.ciency from the system. The 
cost of accessing local and remote data can be one or several orders of magnitude different, and this 
can dramatically affect performance. In this paper, we present a novel approach to automatically perform 
static data distribution. All the constraints related to parallelism and data movement are contained 
in a single data structure, the Communication-Parallelism Graph (CPG). The problem is solved using a 
linear 0-1 integer programming model and solver. In this paper we present the solution for one-dimensional 
array distributions, although its extension to multi-dimensional array distributions is also outlined. 
The solution is static in the sense that the layout of the arrays does not change during the execution 
of the program. We also show the feasibility of using this approach to solve the problem in terms of 
compilation time and quality of the solutions generated. 1 Introduction Data distribution is one of the 
key aspects that have to be considered in a parallelizing environment for Massive Parallel Machines, 
in which each processor has direct access to a local (or close) memory and indirect access to the remote 
memories of other processors. The cost of accessing a local memory location can be more than one order 
of magnitude faster than the cost of accessing a remote memory location. In these systems, the choice 
of a good data distribution can dramatically affect performance because of the non-uniformity of the 
memory system. However, the data distribution problem has been proved to be NP-complete in [Krem94]. 
Two different alternatives are currently offered to the user to address this problem. The .rst one is 
to give him the responsability of deciding how data structures have to be aligned with respect to each 
other, and distributed among the memory modules of the system. In this case, the sequential code is annotated 
with directives and executable statements, offered by current languages such as Fortran D [FHKK90], High 
Performance Fortran [HPFF93], or Vienna Fortran [ZBCM91]. The second alternative is to leave this task 
to the compiler. In any case, classical aspects such as data movement, parallelism, and load balance 
have to be taken into consideration in a uni.ed way to ef.ciently solve the data distribution problem. 
Once the data distribution strategy has been chosen, a sophisticated compiler must take care of the explicit 
management of local name spaces, and generation of a message-passing single­program multiple-data code 
for a given distributed-memory target machine, where all processors execute the same program, but operate 
on distinct data items. The basic compilation rule used by most of the compilers is the owner computes 
rule, where the processor that owns a data item is the one that performs all the computations to update 
it. Several researchers have targeted their research efforts to this topic. For instance, the Crystal 
compiler and language project [LiCh90], the implementation of PARADIGM [Gupt92] on top of Parafrase-2 
and its continuation on the PTRAN II compiler [GMSS93], the compiler for the ALEXI language [Whol92], 
the framework for the automatic determination of array alignment and distribution presented in [CGST93, 
CGSS94], or the automatic data layout strategy [BiKK94] for use in the D programming environment. All 
these automatic or semi-automatic data distribution methods perform the job in two main independent steps: 
alignment and distribution. The alignment step tries to .nd appropriate alignments between all arrays 
in a block of code, that is, to decide for each array the dimensions that will be aligned into the dimensions 
of another array called the template (interdimensional alignment), and for each aligned dimension, to 
decide whether it is better to shift the array with respect to the template or not (intradimensional 
alignment). A good alignment will minimize the overhead of interprocessor communication. This goal could 
be trivially satis.ed assigning all the data to one processor, which minimizes the communication overhead. 
The distribution step decides which dimension or dimensions of the template are distributed, and the 
number of processors assigned to each of them. The mapping of the arrays is determined by their alignment 
with respect to the template and its distribution. A good distribution maximizes the potential parallelism 
of the code, and offers the possibility of further reducing communication by serializing. This goal could 
be trivially satis.ed assigning a datum to each processor, which maximizes parallelism. The distribution 
step depends on the alignment one, that is, once all arrays have been aligned, then they have to be distributed. 
But there is a trade-off between communication and parallelism. The layout that achieves the best performance 
could be one with a non optimal alignment. In other words, to obtain the alignment strategy, some information 
about distribution should be considered. Our approach builds the Communication-Parallelism Graph (CPG), 
a directed weighted graph that holds information about both the data movement and parallelism inherent 
in a block of code. This information is used to formulate a minimal path problem with a set of additional 
constraints, which is solved using a general purpose linear 0-1 integer programming solver. This solver 
.nds the optimal solution, in a small amount of time, to the two problems in a single step. This allows 
us to minimize communication while maximizing parallelism, avoiding the use of heuristics and possibly 
wrong assumptions about distribution during the alignment phase. This paper presents the method that 
.nds the optimal static solution in a block of code for one-dimensional array distributions, given the 
number of processors and the problem size. The extension to multi­dimensional distributions is also outlined. 
A block of code can be any set of loop nests (a single loop nest, a routine, a program), although no 
control .ow constructions are handled. At this point the intradimensional alignment problem (offset and 
stride alignment) is not solved. Figure 1 shows the code that will be used as working example along the 
paper, and the corresponding HPF [HPFF93] data mapping directives that are selected by our approach after 
performing the corresponding analysis and .nding the minimal cost solution. The directive before loop 
j allows its parallelization, and has been speci.ed using the syntax de.ned in [APR94]. This code is 
arti.cial, but it helps to illustrate some aspects of our approach. CHPF$ TEMPLATE TARGET(N, N, N) CHPF$ 
ALIGN A(I,J,K) WITH TARGET(K,J,I) CHPF$ ALIGN B(I,J,K) WITH TARGET(I,J,K) CHPF$ ALIGN C(I,J) WITH TARGET(1,I,J) 
CHPF$ ALIGN D(I,J,K) WITH TARGET(I,J,K) CHPF$ DISTRIBUTE TARGET(*,BLOCK,*) do i = 1, N CAPR$ DO PAR 
ON B<:,1~1,:> do j = 1, N do k = 1, N B (i, j, k) = C (j, k) + i A (k, j, i) = B (i, j, k) + 1 enddo 
 do k = 2, N D (i, j, k) = D (i, j, k - 1) enddo enddo enddo Figure 1: Working example with data 
mapping directives. The rest of this paper is organized as follows: in Section 2 we describe the Communication-Parallelism 
Graph (CPG), the main structure of our approach. Section 3 shows how the constraints in the CPG can be 
formulated as a linear 0-1 integer programming problem, when a single array dimension is distributed. 
Section 4 gives some ideas about the extension of this work to multi­dimensional distributions. Section 
5 summarizes our implementation and shows our .rst experimental results. Section 6 outlines some related 
work with special emphasis in the differences between their and our approaches. And .nally, a few concluding 
remarks are given in Section 7. 2 The Communication-Parallelism Graph The main structure of our method 
is the Communication-Parallelism Graph (CPG). It is a directed graph that contains all the information 
related to communication and parallelism in a block of code. The CPG is created from the analysis of 
all assignment statements within loops that contain one array in the left hand side of the assignment. 
2.1 CPG Nodes The nodes in the CPG are organized in columns. Each column represents an array, and it 
contains as many nodes as the maximum dimensionality d of all arrays in the block of code. Each node 
in a column, thus, represents one dimension of the array that will be mapped into one of the dimensions 
of a common array called template with dimensionality d. If the array has dimensionality d < d, then 
the column is padded with (d - d ) additional nodes. These nodes are included to allow an embedding of 
the array on the template (sequentialization). As seen in the example in Figure 1, array C is embedded 
into the .rst dimension of the template. Since four arrays are used in our working example (three of 
them with dimensionality three, and one with dimensionality two) the CPG consists of 12 nodes (four columns 
with three nodes each). This is the basis of the CPG, over which the communication and parallelism information 
will be added in terms of data movement edges and parallelism hyperedges. 2.2 CPG Data Movement Edges 
Data movement information is obtained from the analysis of reference patterns. The meaning of reference 
patterns is de.ned in [LiCh90], and represents a collection of dependences between arrays in both sides 
of an assignment statement. When the same array is used in both sides, the reference pattern is called 
a self­reference pattern. For instance, consider the code example of Figure 1. From the array assignment 
statements inside the loop nest, two reference patterns can be extracted: B (i, j, k) . C (j, k) A (k, 
j, i) . B (i, j, k) and one self-reference pattern: D (i, j, k) . D (i, j, k - 1) For each reference 
pattern between two different arrays, (d * d) directed edges are added, where d is the maximum dimensionality 
of all arrays. The edges connect all nodes of the array to be read (right hand side) to all nodes of 
the array to be updated (left hand side), and represent all alignment possibilities between these two 
arrays. The weight that is assigned to the edge is a symbolic expression that represents the cost of 
the data movement that has to be performed when these two dimensions are aligned with respact to each 
other and distributed. This cost re.ects the number of remote memory accesses that have to be performed, 
and it is a function of the number of processors that will be assigned to this dimension, and the communication 
rate between two processors. When a self reference pattern is found, d self edges are added in the column 
of the referenced array, one in each node. As in the previous case, the weight is a symbolic expression 
that represents the cost of the data movement that has to be performed when this dimension is distributed. 
Several edges between a pair of nodes are replaced by a single edge with a weight equal to the sum of 
the original ones. In order to estimate data movement costs, reference patterns are matched with a set 
of prede.ned data movement routines. The routines that have been used are listed in Table 1, together 
with the reference patterns that matches them. All the information in this table is machine dependent 
and should be tailored to the speci.c target machine [LiCh91]. Pattern Primitive ip . jp LOCAL MEMORY 
ACCESS const (ip - jp) ONE-TO-ONE const (ip) MANY-TO-ONE const (jp) ONE-TO-MANY ip . jp MANY-TO-MANY 
 Table 1: Matching between reference pattern and communication primitive. In the example of Figure 1, 
when analyzing the .rst assignment statement, the following edges between arrays B and C are inserted: 
BC dim 1 dim 2 dim 3 (sequentialization) The communication primitive that is assigned as weight in the 
edge that connects the .rst dimension of array C to the second dimension of the array B (C[1]-B[2] for 
shorter) is a local memory access. This means that if these two dimensions are aligned and distributed, 
when loading the elements of array C to update array B all the memory accesses will be local. The same 
happens if C[2]-B[3] are aligned. The cost of a many-to-many data movement will be assigned to the edge 
C[1]-B[1]. This means that if these two dimensions are aligned and distributed, a data movement involving 
all processors will be performed while executing the loop. The same happens if C[1]-B[3], C[2]-B[1], 
or C[2]-B[2] are aligned. And the cost of a one-to-many primitive will be assigned to the edges C[3]­B[1], 
C[3]-B[2] and C[3]-B[3]. Distributing the third dimension of array C means to sequentiallize it. So the 
processor owner of array C will have to send the whole one A similar set of edges and data movement information 
is added when analyzing the assignment of B to array A. Finally, from the last assignment, a self-reference 
pattern is extracted. The data movement edges added are self-edges. The communication primitive that 
is assigned as weight to the self edge of the .rst and second dimensions of array D, is a local memory 
access. A one-to­one data movement is assigned to the self-reference pattern D[3]­D[3]. After adding 
the data movement information, the CPG (without weights) that is obtained is shown in Figure 2.a. In 
this graph, the edges due to the assignment of array B to array A, the assignment of array C to array 
B, and the self assignment of array D can be seen. The cost functions have not been represented. ABCD 
 (a) ABC D  (b) Figure 2: CPG with (a) data movement edges and (b) parallelism hyperedges. Note that 
the CPG, when only .lled with the data movement information, is not the Component Af.nity Graph (CAG) 
used by other authors [LiCh90, Gupt92]. The meaning of the edges in the CAG is a preference for alignment, 
that is, how good is to align two dimensions. The weight should re.ect the extra communication cost incurred 
if those dimensions are not aligned. The problem is that one can not identify two unique alignment con.gurations 
under which the communication costs may be estimated and compared to determine the penalty. In our approach, 
the meaning of one edge in the CPG is just the opposed, that is, how costly is (in terms of data movement) 
to align and distribute the two dimensions. This cost is independent of any other alignment strategy, 
and it will be useful to determine the distribution strategy. 2.3 CPG Parallelism Hyperedges Parallelism 
information is obtained from data dependence analysis. First, all loops that can be executed in parallel 
are detected. In distributed memory machines you can fully parallelize one loop when the loop does not 
carry any data .ow dependence [Tsen93]. When all possible parallel loops have been marked, the analyzer 
must only look at the left hand side of the assignments that are inside any parallel loop. According 
to the owner computes rule, the processor that owns a data element is the one who performs all the computations 
to update it. So if one loop is going to be parallelized, it must be ensured for the arrays that appear 
in the left hand side of each assignment statement inside the loop, that the dimension subscripted by 
the loop control variable of the parallel loop is the one that will be distributed. Concretelly, for 
each parallel loop, the analyzer searches for all the assignment statements inside it with an array in 
the left hand side. If this array uses the loop control variable in any dimension of its subscript, then 
the analyzer links the corresponding node to an undirected hyperedge. The hyperedge is a generalization 
of an edge, as it can connect more than two nodes. Each parallel loop has a hyperedge in the CPG. If 
all the nodes connected by a hyperedge are aligned and distributed, then the corresponding loop can be 
parallelized. But if one of the nodes connected by a hyperedge is not aligned, then the loop can not 
be parallelized, and guard expressions must be inserted before each assignment in that loop [Tsen93]. 
The weight of a hyperedge represents the execution time that is saved when the loop is parallelized. 
This time is a function of the sequential execution time of the loop, the number of processors that will 
be assigned to that dimension and the overhead due to the parallel execution itself. In the example of 
Figure 1, after performing the data dependence analysis, one can see that there is a single .ow dependence 
carried by the second k-loop. This means that the i, j, and the .rst k loops can be executed in parallel. 
So the parallelism information that the analyzer will add to the CPG is shown in Figure 2.b. Three hyperedges 
have been added: one linking B[1], A[3], and D[1]; this one is associated to loop i. Another hyperedge 
links A[2], B[2] and D[2], and it is associated to loop j. The last one links A[1] and B[3], and it is 
associated to the .rst loop k. 2.4 Problem Formulation Once the CPG is built, it contains all the necessary 
information regarding data movement and parallelism in the block of code analyzed. The weights in the 
CPG are expressed in a symbolic form, function of the number of processors that will be assigned to that 
dimension. If all symbolic expressions in the CPG are replaced by its constant value in seconds assuming 
P processors, then the weights in the edges will represent the data movement overhead of aligning and 
distributing the corresponding dimensions; and the weights in the hyperedges will represent the computation 
time saved when parallelizing the corresponding loops. If one node (dimension) of each column (array) 
is selected, the sum of the weights of all edges that connect any two nodes inside the selection is the 
total data movement cost of the block of code when distributing the selected dimensions. Similarly, the 
sum of weights of all hyperedges whose nodes remain completely inside the selected set is the total execution 
time saved when parallelizing. In this section, the CPG can be considered as an undirected graph, and 
all pairs of edges connecting two nodes in different direction, can be replaced by a single undirected 
edge with weight sum of the original edges. It can also be assumed that there exists a path between any 
pair of arrays in the CPG. If any set of arrays is not connected, then this set will be analyzed independently, 
and assigned a different template. The reason is that a relation (alignment) between two unrelated sets 
of arrays should not be imposed. If an array is connected to the CPG but only through a hyperedge, then 
all nodes of this array will be connected to all nodes of any other array with a data movement edge, 
and assigned a null weight. This is done because the formulation of the model requires data movement 
edges connecting each array. Now, it can be ensured that the CPG is an undirected connected graph, where 
every pair of columns are connected by a path, and all edges and hyperedges have constant weights. The 
problem to solve is to .nd the set of nodes (one for each column), that minimizes the total execution 
time. The total execution time can be computed as the sequential execution time plus the cost in time 
due to data distribution and parallelization; according to the weights in the CPG, this cost is the data 
movement overhead minus the time saved due to the parallel execution of the loops. The problem is formulated 
as a linear 0-1 integer programming problem, where the objective function to minimize is the total execution 
time.  3 One-Dimensional Distribution Linear integer programming is a tool for solving optimization 
problems. As stated by [BiKK94] data layout problems can be very ef.ciently solved using linear integer 
programming. In this case, the problem to solve is to .nd a path in the CPG that includes exactly one 
node of each column, so that the sum of weights of the edges (data movement edges) minus the sum of weights 
of the hyperedges (parallelism hyperedges) that connect nodes inside the selected path, is minimized. 
This problem can be formulated as a linear 0-1 integer programming problem, that is, a linear integer 
programming problem where each variable has two possible values: 0 or 1. The model that we have selected 
is a shortest path problem with a set of additional constraints. When formulating it, two dummy nodes 
are considered: a source S and a sink T. All edges going from the dummy source S to each node in the 
.rst column, plus all edges going from each node in the last column to the dummy sink T, must be de.ned 
as well. In addition, each columns Q with self-edges will be replaced by two columns named Q and Q , 
and each node in column Q will be linked by an edge to each node in column Q . The weights of the new 
edges will be the same of the self-edge if it links two nodes of the same level, or in.nite otherwise. 
This in.nite weight prevents from selecting two nodes of different level from the same column. Figure 
3 shows the graph that .nally will be considered to solve the problem for the working example of Figure 
1. Neither in.nite weighted edges are present, nor hyperedges. ABC DD Edges added to connectEdges added 
to column D linked onlyreplace a self-edge by a hyperedge Figure 3: CPG with all the data movement edges 
mentioned in Section 3. As it is usual in this kind of problems, one 0-1 integer variable associated 
to each edge, plus one 0-1 integer variable associated to each hyperedge are de.ned. Let YPQ denote the 
set of edges connecting nodes in column P to nodes in column Q. When YPQ is not empty, it contains (d 
x d) elements. If S denotes the column associated to the source, the sets YSP are all empty, except when 
P denotes the .rst of the columns, and it has d elements. Similarly, if T denotes the column associated 
to the sink, the sets YQT are all empty except when Q denotes the last of the columns, and it has d elements. 
Let YPQ[i, j] be the variable associated to the edge connecting node i in column P to node j in column 
Q. Its value will be one if the corresponding edge belongs to the path, and zero otherwise. Note that, 
as the graph is undirected, YPQ[i, j] is the same than YQP[j, i]. Finally, if an index is assigned to 
each hyperedge, Zm will denote the 0-1 integer variable associated to the m-th hyperedge. Similarly, 
its value will be one if the nodes it links belong to the path, and zero otherwise. To ensure the correctness 
of the solution, some constraints must be de.ned. There are four types of constraints: C1: The solution 
is a path. C2: The path traverses a single node in each column. C3: All edges connecting selected nodes 
must also be included. C4: If a node of a hyperedge is not selected, neither it is. Constraints C1 ensure 
that the set of edges in the solution is connected. That is, for each column Q connected to more than 
one column P and R, if one edge leading to a node in Q is selected in the set YPQ, one edge leaving this 
same node must be selected in the set YQR. This can be graphically seen in the left hand side of the 
next .gure. In the right hand side there is a wrong case: P QRP QR right wrong In terms of the variables 
and their values, it can be stated that at each node of each column Q (except S and T) connected to more 
than one column P and R, the sum of the values of the edges that connect this node to column P, must 
be equal to the sum of the values of the edges that connect this node to column R. This must be accomplished 
for each pair of sets YPQ-YQR with a column in common. nnodes nnodes .YPQ ji,=.YQR [],;i =1 nnodes [] 
ij j =1j =1 Constraints C2 and C3 can be speci.ed together. They ensure that only one node can be selected 
in each column, and that all edges connecting two selected nodes, must also be included. These can be 
accomplished forcing that, for each non empty set of edges YPQ, exactly one edge must be selected. An 
example of this constraint can be seen in the left hand side of the next .gure. On the right hand side 
there is an example where the selection is wrong: PQ PQ  right wrong This can be stated, in terms of 
variables and their values, that the summatory of a non empty set of edges YPQ must equal one. This set 
of constraints, must be accomplished for each non empty set of edges. nnodesnnodes ij,=1 ;PQ = [] 1 nsets 
..YPQ i =1j =1 As a consequence of these constraints, the path found in the solution can be not simple. 
This means that it could contain cycles, or that the same node could be more than once in the path. Note 
that this is not contradictory with the 2nd constraint, since exactly one different node in each column 
belongs to the path. Finally, constraints C4 ensure the correct behavior of the hyperedges. A hyperedge 
m belongs to the selection if all nodes linked by it have been selected. One node n of column P belongs 
to the selection, if the value of one of the edges that connects it to any other column Q equals one. 
Or in other words, if the summatory of the values of the edges that connect it to column Q equals one. 
This can be stated, in terms of variables and their values, that the sum of the values of the edges that 
connect the node n of column P to any other column Q, is greater or equal than Zm, where Zm is the 0-1 
integer variable associated to the m-th hyperedge. nnodes YPQ () nj,=Z . m j =1 This must be accomplished 
for each node linked by the hyperedge. For instance, in the CPG corresponding to the example code of 
Figure 1, the hyperedge labeled i links nodes B[1], A[3] and D[1]. Column A is connected to column B, 
column B is connected to column C, and column C is connected to column D. This set of constraints, can 
be stated as: nnodes nnodes nnodes .YBC 1j,;.YAB () 3j;.YCD ,=Zi ( ) =Zi ,=Zi () i1 j =1j =1i =1 The 
0-1 integer variable associated to hyperedge labeled i is Zi. In this case, if any of the three summatories 
are zero, then Zi will also be zero. This constraint must be formulated for each hyperedge m in the graph. 
Once all constraints have been formulated, the objective function to minimize must be speci.ed: the sum 
of the weights of all selected edges, minus the sum of the weights of all selected hyperedges. Let p 
be the total number edges in the CPG. The sum of the weights of all selected edges can be expressed as 
the scalar product in the space Rp of Y and C: CostOfEdges = Y · C where Y represents the vector of all 
0-1 integer variables associated to edges, and C represents their respective weights. Similarly, let 
m be the number of hyperedges in the CPG. The sum of the weights of all selected hyperedges can be expressed 
as the scalar product in the space Rm of vectors Z and T: CostOfHyper = Z · T where Z represents the 
vector of all 0-1 integer variables associated to hyperedges, and T represents their respective weights. 
Finally, the objective function can be expressed as: Cost = CostOfEdges - CostOfHyper The objective function 
must be minimized. When all constraints and the objective function have been speci.ed, the integer linear 
programming solver .nds the optimal solution (minimum) subject to the speci.ed constraints. In the Appendix, 
the full description of the constraints set for the working example of Figure 1 can be found, using the 
syntax speci.ed by the LINGO optimization modeling language [LIND94]. 4 Multi-Dimensional Distribution 
This section introduces some ideas to adapt this framework to the multi-dimensional problem. The introduction 
will be made in two steps. In the .rst one we will assume that the number of processors that is assigned 
to each dimension is known. In the second step, the problem will be to .nd the combination of processors 
that achieves the best performance. 4.1 The Number of Processors is Known In this step, it is assumed 
that the number of processors that will be assigned to each dimension is known. This means that if the 
template is a d-dimensional array, then a set (N1, N2, ..., Nd) is known beforehand, where (N1 x N2 x 
... x Nd) = P, the number of available processors. This general case includes the trivial one, when (N1, 
N2, ..., Nd) is the set (P, 1, ..., 1). Data movement and parallelism cost expressions are function of 
the number of processors assigned to each dimension, and the block size. In the worst case, all Nk are 
different each other, so d different constant costs must be computed. To do this, the original CPG is 
replicated d times, and the cost expression in the k-th copy replaced for its corresponding constant 
value assuming Nk processors. Now the problem is to .nd a path in each copy of the CPG that includes 
exactly one node in each column, with the additional restriction that paths can not have any node in 
common. The sum of the values in the data movement edges minus the sum of the values in the parallelism 
hyperedges that remain inside all paths must be minimized. This is also modeled as a linear 0-1 integer 
programming problem. Similarly, one 0-1 integer variable is associated to each edge, and one 0-1 variable 
is associated to each hyperedge. Now, each variable has a new index k, specifying the copy of the CPG 
it corresponds to. In this case, let YkPQ[i, j] be the variable associated to the edge that connects 
node i of column P to node j of column Q, in the k-th copy of the CPG. The same notation is used for 
the hyperedges, where Zk represents the m-th hyperedge in the k-th mcopy of the CPG. In order to ensure 
the correctness of the solution, the sets of constraints of the previous section must be speci.ed for 
each copy of the CPG, and an additional constraint must be added: C5: Different copies of the same node 
can not be in different paths. This set of constraints ensures that paths will not have nodes in common. 
That is, for each non empty set of edges YPQ, the number of selected edges that enters to each node and 
the number of selected edges that leave from each node in all copies must be one. An example of a valid 
selection of a non empty set of edges YPQcan be seen in the next .gure: P QPQP Q Y1PQ [1, 2] Y2PQ [3, 
1] Y3PQ [2, 3] COPY #1 COPY #2 COPY #3 In terms of variables and their values, it can be stated that 
at each node i of column P, the summatory of values of the edges that connect different copies of this 
node to a non empty column Q must be 1. And at each node of column Q, the summatory of values of the 
edges that connect different copies of this node to column P must also be 1: ncopiesnnodes k ..YPQ ij,=1 
;i = () 1 nnodes k =1j =1 ncopiesnnodes k ..YPQ ij,=1 ;j = () 1 nnodes k =1i =1 This model can be simpli.ed 
in some cases. If the same number of processors is assigned to several template dimensions, a single 
copy of the CPG for them is enough, in which case two different paths must be selected in this copy. 
Another simpli.cation is to eliminate any copy k of the CPG when its associated Nk is 1. In this case, 
as the corresponding dimension is not distributed but it is sequentialized, its cost does not affect 
the overall performance. In both cases, the set of constraints must be slightly modi.ed. The problem 
that one can .nd with this formulation of the problem is that the weights in the different copies of 
the CPG are not related. For instance, the execution time of a parallel loop is its sequential time divided 
by the number of processors assigned to that loop. But if this loop has a nested one, and it is also 
parallelized, then the execution time of the outer loop is its sequential time assuming that the inner 
loop has been parallelized, divided by the number of processors. This means that there is a relation 
between costs of the paths selected in each copy. We are currently working in this direction to have 
a more accurate model. 4.2 Finding the Number of Processors In this section we outline two different 
alternatives to the problem of .nding the number of processors that should be assigned to each dimension 
of the template. The naivest alternative would be to generate iterativelly all different sets (N1, N2, 
..., Nd) of processors, where (N1 x N2 x ... x Nd) equals the number of processors P; for each combination 
solve the 0-1 problem and .nally choose the best combination. The second alternative tries to solve the 
problem in a single step. If P is a power of 2, then log P different number of processors power of 2 
(different than 20 = 1) could be assigned to a given dimension. This is the number of copies of the CPG 
required. Cost expressions in the k-th copy are replaced by its constant value assuming 2k processors, 
for each k = 1 ... (log P). Some additional constraints have to be added to ensure that only d copies 
out of the (log P) copies of the CPG are selected, and that these copies belong to a valid set (N1, N2, 
..., Nd) of processors such that (N1 x N2 x ... x Nd) = P. For instance, if the number of processors 
P equals 8, and the dimensionality of the template is 3, then all valid sets of processors are: (8, 1, 
1), (4, 2, 1), and (2, 2, 2). In this case 3 copies of the CPG are required, the .rst one with 2 processors, 
the second one with 4 processors, and the third one with 8 processors. The set of additional constraints, 
must ensure that: If one of the selected paths belong to the third copy (8 processors), then no other 
path can be selected in any other copy.  If one of the selected paths belong to the second copy (4 processors), 
then one more path in the .rst copy (2 processors) must be selected.  Otherwise, three paths must be 
selected in the .rst copy.   5 Some Experimental Results The main elements of our tool are described 
next. The parsing of the code is performed using the parser module of DDT [AGGL94], a tool to analyze 
reference patterns in Fortran programs, and built on top of ParaScope [KeKT90], an interactive parallel 
programming environment. DDT obtains all reference patterns after performing some well known optimizations, 
like expression substitution, subscript substitution, and induction variable detection that improve the 
quality and quantity of reference patterns analyzed. A pro.le of the sequential execution of the program 
is used to estimate the costs of the parallel hyperedges in the CPG. Machine speci.c information is used 
to estimate the costs of the data movement edges. The analyzer creates a data .le with the set of constraints 
and the objective function to minimize. This .le is the input of a general purpose linear programming 
solver, which minimizes the objective function, and generates an output .le with the .nal value of each 
0-1 integer variable. The general purpose solver used is LINGO, developed by LINDO Systems Inc [LIND94]. 
The solution .le is used to annotate the original sequential Fortran .le with the data mapping and loop 
parallelization directives. The generation of these directives is straightforward. This .le is compiled 
using the xHPF [APR94] compiler from APR Inc, which generates a message-passing Fortran parallelized 
program. Finally, the Forge Performance Simulator is used to predict performance, and to validate the 
solutions generated. All parallel simulations have been done assuming 8 processors and connected through 
a network with a bandwidth of 1Mbyte/second. The set of programs and routines selected1 to measure this 
proposal is listed in Table 2. Routine e.ux has been extracted from the FLO52 program in the Perfect 
Club, and tred2 is a routine taken from the Eispack library. These have been analyzed in [Gupt92]. rhs 
is the more time consuming routine in the APPSP NAS benchmark, included in the xHPF benchmark set. From 
program BARO, also in the xHPF benchmark set, routines intba1 and comp have been extracted. And .nally, 
the erlebacher benchmark is a 3-dimensional tridiagonal solver that uses Alternating Direction Implicit 
integration (also used in [BiKK94] to illustrate their dynamic layout techniques and selected here because 
of its complexity). The static characteristics of the set of programs analyzed are summarized in Table 
2. This includes the number of valid code lines in each program (this is, the number of lines once all 
comment and null lines have been removed), the number of loops that can be executed in parallel in our 
model of architecture (this is, loops without any loop carried .ow dependence), the number of reference 
patterns between different pairs of arrays and in brackets the total number of reference patterns, the 
number of arrays used in the programs, and .nally the maximal dimensionality Name # lines # parallel 
loops # patterns # arrays Max Dim eflux 58 11 4 (161) 5 3 tred2 96 11 8 (38) 4 2 rhs 353 37 5 (346) 4 
4 intba1 71 10 15 (30) 10 2 comp 174 18 43 (162) 24 2 erlebacher 288 72 56 (153) 25 3 Table 2: Characteristics 
of the selected programs. The complexity of the method is a function of the 0-1 integer variables and 
the number of constraints required in the model, and this is a function of the number of different patterns, 
the maximal dimensionality of the arrays, and the number of possible parallel loops in the program. Table 
3 summarizes these characteristics, and the time spent in .nding the optimal solution in a Sun SuperSparc 
20. As a summary, one can see that the analysis of program erlebacher is the one that requires more time, 
as it is also the one that de.nes 1. If the programs selected contain routine calls, these have been 
inlined. And if the code selected is a routine, it has to be transformed into a pro­gram, replacing parameters 
and arguments by local variables. Name # 0-1 variables # constraints time eflux 47 24 0.2 secs tred2 
43 31 0.3 secs rhs 117 63 0.5 secs intba1 70 73 0.5 secs comp 190 180 1.3 secs erlebacher 576 318 3.4 
secs Table 3: Complexity of the selected programs. more number of 0-1 integer variables. Routine comp 
spends 1.3 seconds, and all other analyses spend less than one second. Finally, Table 4 summarizes the 
run-time behavior of the selected programs. In the second column the sequential execution time is listed. 
The third column re.ects the simulated execution time, when parallelizing the programs and assuming a 
default mapping. The default mapping for each program is the distribution of the .rst dimension of all 
arrays, except for routine rhs, where the .rst dimension only contains .ve elements, and the second one 
has been selected as default. The speed-up achieved with this distribution is listed in the fourth column. 
The .fth column shows the simulated execution time when evaluating the solution suggested by our tool, 
and its respective speed-up can be seen in the last column. All times are expressed in milliseconds. 
Name Sequential time Default parallel time Speed-up Suggested parallel time Speed-up eflux 1727.6 282.6 
6.1 282.6 6.1 tred2 707.3 2473.0 0.3 546.8 1.3 rhs 4371.6 1218.0 3.6 1053.0 4.2 intba1 400.9 138.7 2.9 
81.3 4.9 comp 3618.6 1624.7 2.2 653.5 5.5 erlebacher 2571.1 4798.1 0.5 1795.5 1.4 Table 4: Run-time 
behavior of the selected programs. From this table one can see that the mapping selected by the tool 
for the .rst routine is the default one. The same was selected in [Gupt92]. For routine tred2, the selected 
mapping achieves a poor speed-up of 1.3, although the simulated execution time for the default one is 
more than 3 times the sequential execution time. In [Gupt92] the selected mapping was the default one, 
but in a cyclic manner. Our tool does not yet generate cyclic distributions. However, the simulated execution 
time for the cyclic distribution is very close to the default one, as all loops that can be parallelized 
(without performing any kind of loop transformation) just include a single statement, whereas the parallelized 
loops in our strategy are the outermost loops, and the bene.ts of its parallelization are higher. Routine 
rhs results in a speed-up of 4.2 with the suggested mapping, this is, distributing the third dimension 
of all arrays; while a speed-up of 3.6 is achieved with the default one. The parallel version of this 
routine given in the xHPF benchmark set suggests a distribution of the fourth dimension of all arrays. 
The run-time behavior of these two alternatives is very similar. Routines intba1 and comp also increase 
the simulated performance with respect to the default mapping, doubling its speed-up. The selected parallelization 
strategy is the same than the one suggested in the hand coded parallel version of these routines, also 
included in the xHPF benchmark set. Finally, program erlebacher reduces 30% the sequential execution 
time when distributing arrays as suggested by the tool, while the simulated execution time of the program 
with the default distribution is almost twice the sequential one. The selected mapping strategy parallelizes 
almost all outer loops in the program. However, the poor speed-up achieved is because of the amount of 
communication involved, in particular some reductions not detected by the parser module of our tool. 
 6 Related Work Most of the fully automatic data distribution methods [LiCh90, Gupt92, Whol92, CGST93] 
perform the job in two main steps: alignment and distribution. The main differences between different 
methods, is the kind of structure selected to represent the problem, and the way used to formulate and 
solve it. For the alignment step, [LiCh90] proposal de.nes and uses the Component Af.nity Graph (CAG) 
to represent alignment preferences, and it uses a heuristic algorithm to solve it. [Gupt92] proposal 
uses the CAG, but weighted with data movement costs. To do this, a default distribution has to be assumed. 
This proposal is more accurate than the previous one when weighting the edges of the CAG, but the solver 
is based in the same heuristic algorithm. [Whol92] proposal uses the preference graph de.ned by [KnLS90] 
in the framework of SIMD machines. This graph includes alignment preferences to preserve parallelism, 
but the resolution when the graph is in con.ict is also based in heuristics. [CGST93] de.nes the Alignment 
Distribution Graph (ADG) where nodes represent computations, and edges represent the number of data items 
being communicated along that edge. The alignment is found using a compact dynamic algorithm as a heuristic 
to determine the minimum cost. In all cases, the alignment is solved independently of the distribution, 
and the method used to .nd the solution is a heuristic algorithm. Once the alignment has been decided, 
a distribution must be selected. [LiCh90] generates the communication routines involved in the program 
in a parametrized general form. Then, assuming a speci.c target machine, more accurate data movement 
cost functions can be obtained. Finally, this cost function is minimized and a particular distribution 
strategy is selected. [Gupt92] decides what dimensions to distribute (maximum two dimensions), assuming 
a default number of processors in each one and minimizing the total communication plus computation time. 
Then, if more than one dimension must be distributed, it decides what number of processors to assign 
to each dimension generating all possible combinations. [Whol92] uses a hill climbing search method. 
Initially assigns all array elements to one processor and estimates this cost. Then it doubles the number 
of processors and chooses the dimension to assign the news ones, until all available processors are utilized 
or until distribution no longer reduces the total cost. This method considers the case in which data 
is distributed through less processors than available. [CGSS94] selects a set of candidate distributions 
based on the characteristics of the array objects, but not on any parallelism consideration. Then, using 
the divide-and­conquer paradigm, a dynamic distribution is determined. In all cases, the distribution 
is selected after the alignment has been .xed, and except for [LiCh90], an iterative method is used. 
In our approach, the CPG is built, a graph with information about both the data movement and parallelism 
inherent in a block of code. The alignment and the distribution problems can then be solved in an uni.ed 
way, and in a single step. This allows us to minimize communication while maximizing parallelism. The 
solution found is optimal, as it uses a general purpose linear 0-1 integer programming solver, and avoids 
the use of heuristics. In the framework of automatic data layout, the use of linear 0-1 integer programming 
solvers was proposed by [BiKK94] when looking for a dynamic solution for the data layout problem. There 
are some works done that consider dynamic layout, like [BiKK94, CrPe93], or mobile alignment and dynamic 
distribution like [ChGS93], but this is out of the scope of this paper. Some other approaches try to 
.nd a communication-free data layout [RaSa91, HuSa91, ChSh93]. These methods solve the alignment and 
distribution problems together by using linear algebra. The solution, when found, is optimal. But most 
scienti.c code is not communication free, so these methods can not .nd a valid solution. 7 Conclusions 
and Future Work In this paper we have presented a novel approach to perform automatic data distribution 
for distributed memory architectures. The novelty of the approach resides in the following two aspects: 
problem formulation and problem solving. Most of the current approaches to perform automatic data distribution 
work in two steps: .rst they solve the alignment problem (how arrays are aligned with respect to each 
other) and then they solve the distribution problem (which array dimensions are distributed and how many 
processors are allocated to each of them). In this paper we use the Communication-Parallelism Graph (CPG), 
a data structure able to hold all the information required to solve the two problems altogether. The 
CPG contains edges that show communication constraints and edges that show parallelization constraints 
in the program. Edges are weighted according to their cost, in terms of data movement and computation 
time. Constraints in the graph are formulated using a linear 0-1 integer programming model, where the 
problem to solve is to .nd a path in the CPG that minimizes the overall execution time of the application. 
A general purpose linear programming solver has been used in our experiments to .nd the optimal solution 
of the model. The preliminary experimental results show the quality of the data distributions generated 
by the tool, and the feasibility of using linear 0-1 integer programming models to solve these kind of 
problems. In addition, it allows us to .nd an optimal solution in a small amount of time. We have compared 
the solution generated by the solver with a default solution that would be generated by a naive tool, 
and with the best solution proposed elsewhere. We have shown all the details about how the problem can 
be solved for one-dimensional array distributions. The main ideas about multi­dimensional array distributions 
have been outlined. A lot of additional aspects should be considered in the problem formulation in order 
to improve the accuracy of the model and therefore the quality of the solutions generated, such as modeling 
communication optimizations [HiKT92] (detection and elimination of redundant communication, overlapping 
of computation and communication, and combination of communication messages) and integrating control 
.ow analysis. We are also working in the direction of extending this approach to dynamic data mappings, 
where a set of computational intensive phases are detected, the mapping for each of them is obtained, 
and remapping actions (realignment and redistribution) between phases introduced. 8 Acknowledgements 
We would like to thank all other members of the DDTeam - Mercè Gironès, and M. Luz Grande -for their 
support for the implementation of this work, and to Uli Kremer for the fruitful discussions in the topic. 
The authors especially thank Elena Fernández for her insight comments and suggestions in the speci.cation 
of the linear programming model. This work has been partially supported by the Ministry of Education 
of Spain under contracts TIC880/92 and TIC429/95, the CIRIT under grant BE94/1-100, the CEPBA (European 
Center for Parallelism of Barcelona) and by CONVEX Computer Corporation, that supports the development 
of the DDT tool. 9 References [AGGL94] E. Ayguadé, J. Garcia, M. Gironés, J. Labarta, J. Torres and 
M. Valero, Detecting and Using Af.nity in an Automatic Data Distribution Tool . Proc. of the 7th Annual 
Workshop on Languages and Compilers for Parallel Computing, August 1994. [APR94] Applied Parallel Research 
Inc., xHPF Version 1.2, User s Guide . May 1994 Release. [BiKK94] R. Bixby, K. Kennedy and U. Kremer, 
Automatic Data Layout Using 0-1 Integer Programming . Proc. of the International Conference on Parallel 
Architectures and Compilation Techniques, August 1994. [CGST93] S. Chaterjee, J. R. Gilbert, R. Schreiber 
and S.-H. Teng, Automatic Array Alignment in Data-Parallel Programs . Proc. of the 20th Annual ACM Symposium 
on Principles of Programming Languages, January 1993. [CGSS94] S. Chaterjee, J. R. Gilbert, R. Schreiber 
and T. J. Shef.er, Array Distribution in Data-Parallel Programs . Proc. of the 7th Annual Workshop on 
Languages and Compilers for Parallel Computing, August 1994. [ChGS93] S. Chaterjee, J. R. Gilbert and 
R. Schreiber, Mobile and Replicated Alignment of Arrays in Data-Parallel Programs . Proc. of Supercomputing 
93, November 1993. [ChSh93] T.-S. Chen and J.-P. Sheu, Communication-Free Data allocation Techniques 
for Parallelizing Compilers on Multicomputers . 1993 International Conference on Parallel Processing, 
vol. II, August 1993. [CrPe93] P. Crooks and R. H. Perrott, An Automatic Data Distribution Generator 
for Distributed Memory MIMD Machines . 4th International Workshop on Compilers for Parallel Computers, 
December 1993. [FHKK90] G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng and M. Wu, 
Fortran D Language Speci.cation . Technical Report CRPC TR 90-141, Department of Computer Science, Rice 
University, December 1990. [GMSS93] M. Gupta, S. Midkiff, E. Schonberg, P. Sweeney, K. Y. Wang and K. 
Burke, PTRAN II - A Compiler for High Performance Fortran . 4th International Workshop on Compilers for 
Parallel Computers, December 1993. [Gupt92] M. Gupta, Automatic Data Partitioning on Distributed Memory 
Multicomputers . PhD thesis, University of Illinois at Urbana- Champaign, September 1992. Also available 
as technical report UILU-ENG-92-2237 and CRHC-92-19. [HiKT92] S. Hiranandani, K. Kennedy and C.-W. Tseng, 
Compiling Fortran D for MIMD Distributed-Memory Machines . Communications of the ACM, vol. 35, August 
1992. [HPFF93] High Performance Fortran Forum, High Performance Fortran Language Speci.cation. Version 
1.0 . Scienti.c Programming, May 1993. [HuSa91] C.-H. Huang and P. Sadayappan, Communication-Free Hyperplane 
Partitioning of Nested Loops . Proc. of the 4th International Workshop on Languages and Compilers for 
Parallel Computing, August 1991. [KeKT90] K. Kennedy, K. McKinley and C.-W. Tseng, Interactive Parallel 
Programming Using the ParaScope Editor . Technical Report CRPC- TR90096, Center for Research on Parallel 
Computation, Rice University, October 1990. [KnLS90] K. Knobe, J. D. Lukas and G. L. Steele, Jr., DataOptimization: 
Allocation of Arrays to Reduce Communication on SIMD Machines . Journal of Parallel and Distributed Computing, 
vol. 8, February 1990. [Krem94] U. Kremer, NP--completeness of dynamic remapping . 4th International 
Workshop on Compilers for Parallel Computers, December 1993. [LiCh90] J. Li and M. Chen, Index Domain 
Alignment: Minimizing Cost of Cross-Referencing Between Distributed Arrays . Proc. of the 3rd Symposium 
on the Frontiers of Massively Parallel Computationd, October 1990. [LiCh91] J. Li and M. Chen, Compiling 
Communication-efficient Programs for Massively Parallel Machines , IEEE Trans. on Parallel and Distributed 
Systems, vol. 2, no. 3, July 1991. [LIND94] LINDO Systems Inc., LINGO Optimization Modeling Language 
. &#38;#169; April 1994 by LINDO Systems Inc. [RaSa91] J. Ramanujam and P. Sadayappan, Compile-Time Techniques 
for Data Distribution in Distributed Memory Machines . IEEE Transactions on Parallel and Distributed 
Systems, vol. 2, no. 4, October 1991. [Tsen93] C. Tseng, An Optimizing Fortran D Compiler for MIMD Distributed- 
Memory Machines . PhD thesis, Rice University, January 1993. Rice COMP TR93-199. [Whol92] S. Wholey, 
Automatic Data Mapping for Distributed-Memory Parallel Computers . Proc. of the 1992 ACM International 
Conference on Supercomputing, July 1992. [ZBCM91] H. Zima, P. Brezany, B. Chapman, P. Mehrotra and Schwald, 
Vienna Fortran - A Language Speci.cation . Technical Report, Austrian Center for Parallel Computation, 
University of Vienna, 1991. Appendix In the implementation, nodes source and sink are not considered. 
An integer identi.er n has been assigned to each non empty set of edges. In the example there are four 
different sets of edges: YAB, YBC, YCD and YDD respectivelly. The set of all 0-1 integer variables is 
represented by Y, and it is declared as an array of dimensionality [n x d x d], where n is the number 
of patterns between different pairs of arrays, and d is the maximal dimensionality of all arrays. Cis 
the array of the same dimensionality that holds the weights for each edge. Z represents the set of hyperedges 
and is declared as a vector of dimensionality [m], where m is the number of parallel loops. Vector T 
holds the corresponding weights associated to hyperedges. ! Constraints file for example routine; ! 1-D 
DISTRIBUTION. EDGE BASED MODEL; MODEL: SETS: ! Declaration of variables; maxdim /1..3/ : ; numedg /1..4/ 
: ; numhyp /1..3/ : ; edges (numedg, maxdim, maxdim) : Y, C; hyper (numhyp) : Z, T; ENDSETS DATA: ! 
Communication cost values; C = ...list of (numedg x maxdim x maxdim) constant values... ! Parallelization 
cost values; T =...list of (numhyp) constant values... ENDDATA ! Declaring all variables as 0-1 integer; 
@FOR (edges: @BIN (Y); ); @FOR (hyper: @BIN (Z); ); ! 1st constraint: the solution is a path, all edges 
are connected; @FOR (maxdim(i): @SUM (maxdim(j): Y(1, j, i) ) = @SUM (maxdim(j): Y(2, i, j))); @FOR (maxdim(i): 
@SUM (maxdim(j): Y(2, j, i) ) = @SUM (maxdim(j): Y(3, i, j))); @FOR (maxdim(i): @SUM (maxdim(j): Y(3, 
j, i) ) = @SUM (maxdim(j): Y(4, i, j))); ! 2nd and 3rd constraints: exactly one edge must be selected; 
@FOR (numedg(n): @SUM (maxdim(i): @SUM (maxdim(j): Y(n, i, j) ) ) = 1); ! 4th constraint: correct behavior 
of hyperedges; @SUM (maxdim(j): Y(1, 1, j) ) >= Z(1); @SUM (maxdim(j): Y(2, 3, j) ) >= Z(1); @SUM (maxdim(i): 
Y(3, i, 1) ) >= Z(1); @SUM (maxdim(j): Y(1, 2, j) ) >= Z(2); @SUM (maxdim(j): Y(2, 2, j) ) >= Z(2); 
@SUM (maxdim(i): Y(3, i, 2) ) >= Z(2); @SUM (maxdim(j): Y(1, 1, j) ) >= Z(3); @SUM (maxdim(j): Y(1, 
3, j) ) >= Z(3); ! Objective function to minimize; CostOfEdges = @SUM (edges: Y * C); CostOfHyper = 
@SUM (hyper: Z * T); Cost = CostOfEdges - CostOfHyper; END Jordi Garcia received the BS degree in computer 
engineering from the Universitat Politècnica de Catalunya (UPC) in 1991. He has been an assistant professor 
of the Computer Architecture Department of the UPC since 1992 and is currently pursuing the PhD degree 
in Computer Science. His research interests are parallel computers and parallelizing compilers. e-mail: 
jordig@ac.upc.es URL: http://www.ac.upc.es/~jordig Eduard Ayguadé received the engineering degree in 
 telecommunications in 1986 and the PhD degree in computer science in 1989, both from the Universitat 
Politècnica de Catalunya (UPC). In 1986 he joined the Computer Architecture Department of the UPC where 
he is an associated professor. His main research interests are computer architecture and optimizing and 
parallelizing compilers for high-performance multiprocessors. e-mail: eduard@ac.upc.es URL: http://www.ac.upc.es/~eduard 
Jesús Labarta received the engineering degree in telecommunications in 1981 and the PhD degree in 1983, 
both from the Universitat Politècnica de Catalunya (UPC). He joined the Computer Architecture Department 
of the UPC in 1981 where he is a full professor. His main research interests are computer architecture, 
operating systems, performance evaluation and compilers. e-mail: jesus@ac.upc.es URL: http://www.ac.upc.es/~jesus 
Copyright &#38;#169; 1995 by the Association for Computing Machinery, Inc. (ACM). Permission to make 
digital or hard copies of part or all of this work for personal or classroom use is granted without fee 
provided that copies are not made or distributed for profit or commercial advantage and that new copies 
bear this notice and the full citation on the first page. Copyrights for components of this work owned 
by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, 
to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request 
permissions from Publications Dept, ACM Inc., via fax at +1 (212) 869-0481, or via email at permissions@acm.org. 
 
			
