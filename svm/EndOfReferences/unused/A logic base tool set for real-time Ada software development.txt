
 A LOGIC BASE TOOL SET FOR REAL-TIME ADA SOFTWARE DEVELOPMENT Michael Moore SotTech,Inc. Anti Pruitt 
GE Abstract. This is a report on work conductedprivately that explores the useof predicatelogic to reasonaboutreal-time 
design. Safetyandreliability issuesassociatedwith embeddedreal­time systems make greater demands on development 
engineers than non-real-time systems whose functional complexity is of similar degree. The effort undertakenhere 
attempts to provide methods and tools for dealing with both the functional and temporal aspects of software 
engineering. The goal is to improve the effectiveness of engineering and thus the productivity of a development 
team. The problem was approached using a variant of the IDEFO (SADT) modeling methodology, an adaptation 
equipped with primitives for expressing data types, time constraints, and process activation rules. The 
SADT methodology serves as the interface between an engineer and a deep structure representation of a 
system expressed using logic prexiicates. The SADT framework supports the modeling of a system with an 
arbitrary number of processing resources using hierarchical specifications of time constraints and resource 
utilization limitations, The concepts of resource utilization, period, as well as hard and soft deadline 
are built into the approach. This particular report describes a tool set that has been built using these 
concepts. The tools illustrate that the logic based models are sufficient for conducting control path 
analysis and schedulability analysis. An example is provided of tool capability in which schedulable 
entities are identified in the SADT model assigned priorities, and then are shown to be schedulable by 
application of the Critical Instant test. The associated PDL, annotated with time constraints, is produced 
as well. All steps are automatic. Rate monotonic prioritization is selected for the example. Experience 
with the tools indicates that this is a practical, cost effective approach to real-time systems engineering, 
COPYRIGHT 1991BY THE ASSOCIATION FOR COMPUTING MACHINERY, KNC. Permission to copy without fee rdt or 
part of this materiat is granted provided that the copies are not made or distributed for duect commercial 
advantage, the ACM copyright notice and the title of the publication and its date appear, and notice 
is given that copying is by permission of the ACM. To copy otherwise, or to republish, requires a fee 
and or specific permission. @ 1991 ACM 0-89791 -393-0/91/0600/0102 $1.50 Introduction. Software engineers 
face ever-more challenging development projects as hardware costs fall, and hardware functionality increases. 
The increasing availability of hardware power in small inexpensive packages is an incentive to manufacturers 
to incorporate automation into new products. As the hardware systems become more complex, so too must 
the software which directs the work of the machines. When software complexity is very great our ability 
to produce new products is impeded by overwhelming cost and schedule slippage. llte impediment becomes 
a new opportunity to compet~ developers acquire special tools to manage the complexity in hope of being 
able to control the impediment better than the competition. When the new products incorporate computers 
to control machines or interact with humans (or other machines) in a timely manner, the hardware/ software 
system may be required to carry out work under constraints for which we would classify it as a real-time 
system. In this context real-time is a term used to denote time constraints exist for the hardware/ software 
system. Real-time software engineering is considered to be one of the most difficult challenges for a 
system developer. Part of the effectiveness of a system development effort is the ability of the engineering 
staff to accurately and clearly specify the requirements of the system and the design of the system. 
Using the analogy of Civil Engineers building a bridge, the job of the engineering staff is fiist to 
precisely define the bridge s requirements, and then to develop a design which meets the requirements. 
The requirements for a bridge might be its span, height, load capacity, thermal properties, maintenance 
characteristics, earth quake resistance, wind sheer resistance, cost, materials availability, and soil 
mtxhanics constraints, to name a few. By analogy the Software Engineerfacesrequirementslike hostcomputerconstraints, 
size, cost, maintenance characteristics, functions performed, numeric accuracyandprecision, computational 
deadlines,anddevelopmentsystemavailability. There are many usesfor the design produced by the Civil Engineer. 
The design is compared to the bridge requirementstoverify thatrequkemen~aremet,thatthey areconsistentandcomplete. 
Thedesignk usedasamedia for communicating the intent of the engineer to those fabricating, assembling 
and testing parts. The design is WashingtonAda SymposiumProceedings-June1991 102 used as the definition 
of what the bridge is during those times when requirements are redirected, to determine feasibility and 
impact. In all these regards the Civil Engineer can use the design to reason about the bridge. That is 
to say, the design is a basis for obtaining answers to questions. The design is a living document that 
is refined and corrected as the development proceeds. The Software Engineering analogy to a Civil Engineer 
s bridge design should be a way to reason about the system being developed. It should be useful for verifying 
requirements are me~ that they are consistent and complete. It should be a means of communicating what 
is to be fabricated, assembled (integrated, as we say) and tested. When a software design fulfills these 
purposes the development of the product is efficient and cost effective. The impediments to controlled 
cost and schedule are removed and the product developer stays competitive. The Problem, Effective disciplines 
for real-time system development are not widely known or practiced within the software engineering profession. 
Stankovic makes this point in his paper Real-Time Computing Systems: The Next Generation published in 
the tutorial Hard Real-Time Systems (11). Indeed University text books on the topic of Software Engineering 
apply little tribute to real-time software as a separate engineering discipline with a scientific basis. 
Real-time system development common practice is to conduct it with a functional decomposition approach 
for expressing requirements and design. Often times little more than ad-hoc provisions are made for reasoning 
about the temporal constrains of a system. When real-time systems are developed without provisions in 
the requirements and design for reasoning about time, the result quite frequently is a well reported 
appearance of a design that may not be useful for applying the engineering disciplines needed to carry 
out the development. In the end, real-time engineering analysis and design rigor may not be applied. 
These designs are some times abandoned during integration, or compIeted over budget. Project management 
may find itself having to react to crisis after crisis as inconsistencies become apparent late into integration. 
The project s best and most expensive talent may even become distracted from its primary role as it gets 
over extended attending to crisis. The apparent problem with engineering direction is probably not the 
fundamental problem. We have taken the view that the fun&#38;mental problem is that sound methods for 
real-time engineering are not widely known. Tools which assist engineers with specification and analysis 
are avaitable, but engineers are not likely to apply the methods behind the tools. Perhaps this is true 
for a number of reasons. Tool developers are understandably more interested in promoting tools than methods. 
Tools that are available are very expensive, beyond the reach of individuals. Access is lirnitcd to at 
work, on the licensed machinery provides little opportunity to explore, to adapt and to become competent. 
But it might also be true because the methodologies behind the tools are not quite the right methodologies 
for the hardest problems. It is cumbersome to use a non-real-time design methodology on a real-time proj~t. 
Even when expensive real-time CASE tools are purchased by a company and made available to engineers, 
it is not clear that the engineers are properly equipped with methods for reasoning about a real-time 
system and directing its construction. In these cases the tools could lx a distraction. It makes no sense 
when the sole use a development team makes of a CASE tool is to prepare simple bubble diagrams for presentations. 
Ada itself has played a similar role. Ada s organizational features qualify it as the language of choice 
for large projects. The not so occasional project planner has drawn the conclusion from promotional briefings 
that Ada simplifies not only the large project management issues, but the real-time issues as well. Therefore, 
it is concluded, little effort has to be put into identifying and rtxonciling real-time requirements. 
Further Evidence. There is little argument about real-time methods not being widely understood within 
the Software Engineering profession. Our view is that this absence of an appropriate discipline for real-time 
engineering is at the center of the competitiveness issue. There is evidence that our view is shared. 
The Software Engineering Institute (SEI) has a charter from its sponsor (,The US Government) to promote 
good Software Enginem-ing Practice. The existence of this institution, and its commendable attempts to 
promote real­time methods such as Rate Monotonic Scheduling, are an affirmation that an influential body 
of policy makers recognize the problem of engineers not being properly equipped. The developer who cultivates 
real-time engineering acumen within its engineering staff has a significant edge over most of the competition. 
Rigorous, disciplined engineering during the initial stages of a project can dramatically reduce the 
total length of the development and the amount of labor needed to conduct it. Indeed, software development 
is so labor intense that a percentage labor reduction nearly represents a percentage of total cost reduction. 
Effective methodical practices reduce the risk of schtxiule slipping as well. The establishment of a 
disciplined team is not achievable by executive fiat, nor is it likely to be assembled in haste through 
hiring and transfer to forma team for an ambitious project. The team should be cultivated over time, 
with mentorship in both the technical and management ranks that is established on sound practice. Our 
Work. Our objective is to promote disciplines for reasoning about real-time systems by developing appropriate 
tools which run on unpretentious machines. We wish to identify methods and provide tools with which engineers 
can describe the functional and temporal characteristics of software running on one or more computers, 
and then validate the timing characteristics. Should the engineer not be satisfied with a result, the 
tools should guide the engineer s efforts to adjust time constraints in parts of a requirements or design 
model. Using this projmt.ive method an engineer can assess the affects of hypothetical changes to requirements 
and design. The tools should be capable of documenting temporal constraints in a software design for 
communicating them to fabricators and testers. Meeting this objective with tools that run on unpretentious 
machines, we reasoned, would remove barriers to the common masses and contribute to unifying how engineers 
view and manage real-time system developments. We have approached this objective by adapting the Structured 
Analysis and Design Technique (SADT) invented by Ross (1) with conventions which support reasoning about 
machine utilization and meeting deadlines. SADT is a SofT echtrade mark. SADT is also known by the alias 
lDEFO, a USAF ICAM methodology. It is a natural methodology for real-time engineering becauseit relatesanactivity 
notonlytoinput,outputandcontrol,but alsoto resourceconsumption. Hardreal-timeschedulabilityanalysisworkreportedby 
Liu &#38; Layland (2), Sha (3) and Baker (4), were selected as the most promising basis for real-time 
software engineering. These contributors illustrate approaches for designing in deadline schedulability 
that make use of process control strategies such as Rate Monotonic Scheduling, Deadline Scheduling, Priority 
Ceiling Protocol, and Semaphore Control Protocol. Sha and Goodenough (5) demonstrate how to apply Rate 
Monotonic Scheduling in Ada. Work by Puschnerand and Zainlinger (6), Baker (4), Ramamrithham and Stankovic 
(7) advocating predictability of timing behavior was influential as well. The general theme we used from 
the work by all these contributors is the importance of constraining real-time system design and fabrication 
in a way that temporal properties are predictable. Most of the present literature on software engineering 
emphasizes prioritized event driven process management and de-emphasizes the more ad-hoc techniques associated 
with cyclic process scheduling. We feel that cyclic exec techniques are practical and should be supported 
along with Rate Monotonic and Deadline scheduling. There is little impact on the modeling tools because 
all these techniques rely on the same time constraint information; deadline, period, and resource utilization. 
That is to say, the conventional cyclic exec approach and the event driven preemptive exec approaches 
all make use of the notions of hard and soft real-time, they deal with deadlines, the notions of atomic 
operations, and with process schedule period and CPU utilization. Frame oriented cyclic process control 
is a conventional approach to avionics systems. With this method the hard deadlined processes are allocated 
to cyclic processing frames, and must complete within their own frames. Soft real-time processes in these 
environments are typically hamdled by a background process manager which may even impose time slicing 
or sporadic serving constraints on the processes. This cyclic exec method is a target of criticism amongst 
promoters of prioritized event oriented scheduling, supposedly because it is less flexible than the latter 
technique, It continues to be a successful approach because it is very intuitive for engineers. With 
this technique very little abstraction is required for designers to appreciate time constraints. SADT. 
SADT, as described by ROSS (1) is a graphics language that is hierarchical in nature. An SADT model describes 
a system as an activity expendable through hierarchy to an arbitrary level of detail. An SADT model is 
shown in figure 1. Inputs are objects used by the activity but not involved in the activation of the 
activity. Controls are like inputs, except that they are involved in the activation of the activity. 
Outputs are objects produced by the activity. Resources (mechanisms) are things that do the work of the 
activity, such as people, CPUS and software. Control Inputs output > Activity Title . > h 4 Mech~nisms 
(Resources) Figure 2 illustrates SADT S hierarchical decomposition. The activity portraying an entire 
system is the top level activity in the diagram. Each activity can be represented by two or more (up 
to six) increasingly detailed activities each which, in turn, can be represented by even more layers 
of detail. Rational for the limit of six is Miller s Law (it s too hard for a person to keep track of 
more than six things.) Arrows, like activities, decompose hierarchically to reveal more structural detail. 
Figure 3 illustrates this with the joining of two arrows. A complete description of SADT can be found 
in (8) or (9). 104 WashingtonAda SymposiumProceedings-June 1991 Real-Time Requirements More General ~ 
~le u / system Hard Rk-Tme Soft RtkTim !7r / ) Figure 2 SADT Hierarchical Structure Figure 3 A Join Our 
adaptation of SADT is to view all real-time process management in terms of being either hard real-time 
or soft real-time. Our definition of a hard real-time process is a process with a deadline which must 
be met. A soft real­time process is one whose deadline must be met on the average. Further definition 
of on the average does have guidelines but is mostly determined by the developer. The tetm may for example 
be defined as a standard deviation variance about tie numeric mean. Central to the hard real-time schedulability 
concept is the issue of being able to show that a certain set of processes can always meet their deadlines. 
Not all processes have the same kind of deadlines. A general approach is useful for reconciling the limited 
resources of an embedded system with the types of deadlines required for the system. We propose a hierarchy 
of deadline classes that divide hard real-time requirements into critical and non-critical hard real-time 
subclasses. Figure 4 is a drawing of the classifications. In thecritical hardreal-timeclassarethoseprocesseswhich 
absolutely must run and complete by their deadlines. For theseprocessesit is important to prove that 
they form a hard schedulableset, The critical instant test (2) can be used for this, but only if deadline, 
CPU consumption limitsgmd worst case time between activating stimulations for individual processes in 
the set are known. These criteria are discussed in references (2) and (4). f cr!ticat Non-Critical Multipnority 
Cyclic Frarn e Cyclic Frame Time Slice Serving Control Conlrol Spuratic Serving 1 RMS in Event RhM in 
Event DrNen En-Driven Environments Vironments Dynamic Scheduling &#38; dropping dead processesV igure 
4 Classes of Real- rime Constraint and Suggestive Scheduling Techniques The non-critical hard real-time 
class consists of processes for which there is no value to perform when they can not meet their deadlines. 
These processes must be shown to be schedulable a certain percentage of the time. A simple utilization 
measure can be computed for these and the critical hard real-time set by summing the ratios of their 
CPU use (in seconds) to their periods. For example, three process (Pa, Pb and Pc) may activate at 4 Hz, 
5 Hz and 10 Hz, respectively. Pa may require up to 10 MS per activation, Pb 7 MS, and PC 3 MS. Their 
total utilization can be determined by computing: U= (CPU TIME)/(PERIOD) = (CPa/PPa) + (CPb/PPb) + (CPc/PPc) 
= (0.010/0.25)+ (0.007/().2) + (0.003/0.1) = 12% Total utilization of all the hard real-time processes 
must be less than one for schedulability to be possible. Reference 10 describes a schedulability criteria 
developed by the Software Engineering Institute which says that utilization meeting the criteria U c 
n(21/n-1) is hard real-time schedulable if the prioritization is rate monotonic. The critical instant 
test is likewise applicable, and thus the non-critical hard real-time processes can be handled uniformly 
with the critical hard real-time set when both sets fit into the hard schedulable set. Less efficient 
dynamic scheduling methods can be applied when the non­critical set does not fit into the hard schedulable 
set. Scheduling the process with the least laxity (time till deadline minus time needed to execute the 
process) is an example of this. Using this technique a scheduler would terminate a process when its laxity 
becomes negative. Termination may happen at any time, even before the process is ever dispatched. This 
technique drops dead processes with the least cost. Soft real-time processes can be handled with one 
or more priority queues, perhaps using a mixture of time slice and sporadic scheduling when process execution 
time ceilings are indeterminate. For a modeling methodology (like SADT) to support this type of analysis 
it must capture these criteria in its models. Our Approach. Our approach to combining SADT with the real-time 
schedulability principles has been to represent both the SADT modeling primitives and the real-time constraints 
with logical relationships. The PROLOG language was selected as the media for a deep structure logic 
representation. The use of a relational knowledge representation and a logic language provides a direct 
mechanism for rule based reasoning. The development of tools which analyze and manipulate the information 
in a system model expressed in a relational knowledge base is relatively straight forward. All of the 
tools were written in PROLOG. The approach makes the tool set open ended. An activity box can be expressed 
as a relation associating the title, arrows and its place in the model hierarchy as follows: activity(Title, 
Lineage, Box_Number, Inputs, Outputs, Controls, Resources). Title is a statement of purpose, usually 
a sentence describing what the thing being modeled does. Lineage is a list of box numbers defining the 
parental heritage of the activity (its hierarchical trace of ancestors). A complete SADT model contains 
activation rules and behaviors. We interpreted activation rules to mean the set of conditions necessary 
to make an activity become active. A deep structure representation of an AR might be: ar(Activity_title, 
parameter_lis~ behavior_ruIe_ name, delay, deadline, condition_list). In this representation Activity_title 
is the title of an activity; it associates am activity to its AR. A parameter list is a list of inputs, 
controls and outputs involved in doing the activity, much like a subroutine s parameters are involved 
with executing a subroutine. The behavior_rule_name identifies a functionality associated with the activation. 
Delay is the minimum amount of time to delay from the time activation conditions are met and the actual 
commencement of the activity. Deadline defines the last moment relative to the time activation conditions 
are met that the activity can complete. Condition list is a list of conditions which must be met to activate 
the activity. These are conditions like period (cyclic activation), value conditions, and the arrival 
of data. The behavior rules (BRs) capture other time constraints and some functionality of the activity. 
A BR is represented as: br(Behavior_rule_name, Number_of_Parameters, Parameter_Flow_Pattem, Hard_soft-constraint, 
resource_utilization). The BehaviorTrule_name and Number_of_Parameters (arity) associate the BR with 
an AR. The Parameter_flow_pattern defines which parameters are input, output, and in_out. Resource utilization 
is represented as a minimum and a maximum. The Hard_soft_constraint simply identifies the BR as a hard 
or soft real-time functionality. Other relations are used to describe an arrow object s type, initial 
value constraints, and physical representation. Ada types are the base types in our tool set. Physical 
representation consists of attributes such as queue, pool, level, and stack for data,and disk, cpu, memory, 
software and person for arrow mechanisms. Physical representation attributes are typically used on mechanisms 
to identify and distinguish software, hardware and people-ware. These enrichments to the SADT methodology 
are atrnbutes useful for describing constraints. One flaw with the SADT graphics language is that it 
s method for showing arrow decomposition is an ambiguous graphics idiom. Figure 3 illustrates this join 
concept with an arrows A abd B merging into arrow C. Looking at the graph alone one can not distinguish 
whether A and B are distinct entities within C, or whether A might be a part of B which in turn is part 
of C. It might even be true in some cases that C is a subset of A UNION B. We chose to represent the 
join concept with the relation element_of(X,Y). This relation means that X is an element of Y. The set 
of relations element_of(A,B). element_of(B ,C). clearly define how A, B and C are related. Alternative 
relationships such as element_of(A,C). element_of(B ,C). have different meanings and are well denoted 
in the relational knowledge base representing an SADT model. The graph fragment from either of these 
two examples would both project into SADT as Figure 3 illustrates, so while the graphics are ambiguous, 
the logical representations are not. Making Use Of Logic. The interpretation of SADT knowledge as a 
graphic form is accomplished by querying a knowledge base representing a model, and producing the graphs 
as a side effect of the query. This is how the tools use SADT as a man-machine interface between the 
deep structure representation and the WashingtonAda SymposiumProceedings-June1991 106 human s dealing 
with the model. The relational models could be used to make other man-machine interface methods, such 
as menus or bubble charts, by providing appropriate queries. I Validation # DesignExamining Assistance 
PROLOG 1- Knowledge base representation of a Document system Maker mr Figure 5 The Tool Set Architecture 
The use of logic promotes a certain open endedness. An engineer may wish to gather information not readily 
available through the use of existing tools which query the relational models. Custom queries are relatively 
straight forward uses of PROLOG. For example, an Engineer may wish to know the set of all activities 
conducted by a particuhu person (say a pilot) in a man in the loop system. A custom query for this would 
be something like this: activity(ACTIVITY, _, _), mechanism(pilot,ACTIVITY), wnte( k The pilot does ,ACTIVITY), 
fail. Those who understand PROLOG will recognize the FAIL as a method for forcing backtracking so that 
all solutions will be reported. A slightly more common problem is to assemble a list of messages between 
CSCIS. Messages would normally be modeled as data arrows. The query would go something like this: findall(X,is_csci(X),CSCI_LIST), 
list_set(CSCI_LIST,CSCI_SET), member(SOURCE_CSCI,CSCI_SET), member(DESTINATION_CSCI,CSCI_SET ), not(SOURCE_CSCI 
= DESTINATION_CSCI), mechanism(SOURCE_CSCI,S_ACTIVITY), output(ARROW,S _ACTIVITY), input_or_control(ARROW,D_ACT 
IVITY), mechanism(DESTINATION_CSCI,D_ACTIVITY), write( lnSOURCE_CSCI sends ,ARROW, to ,DESTINATION_CSCIj, 
fail. Washington Ada Symposium Proceedings -June 1991 There are a few simplifications in this query 
for the sake of brevity. The input_or_control predicate is equivalent to finding ARROW to be either an 
in/out or a control (or both) of S_ACTIVITY. The predicate is_csci finds an example of a CSCI. The predicate 
list_set converts a list (a bag) into a set. The complexity of what a query can do is bounded only by 
the limits of logic and the resources of the computer hosting the query. The adaptation of expert system 
shell technology is a natural direction for tools which capture a system s characteristics relationally. 
We have not yet harnessed shell technology for our tools, but we have built a number of tools that incorporate 
elaborate queries as built in capabilities. The SALDT Tool Set. We implemented a tool set which we designated 
S ALDT t or the Structured Analysis, Logical Design Technique. The user sees the tool set as an implementation 
of SADT that has been modified for Software Engineering. The term Logical was adopted in the name to 
denote that the tool set is rule based. There are six programs in the basic tool set, and a seventh in 
an experimental configuration. The tools all run nicely on MS-DOS machines. The tool which an engineer 
interacts with the most is the SALDT model builder, SALDT.EXE. It accepts textual model information from 
the keyboard, and makes use of its own pop-up menus. SALDT displays SADT graphics on the user s screen 
(CGA or EGA resolution) as information about the model is entered. Function keys are used instead of 
a mouse, since no operator work is involved with drawing. Indeed, the drawings are merely a projection 
of the deep structure forms in the knowledge base. This tool is thus an editor of the knowledge base, 
with some features for spotting common SADT errors. There are two validation tools. The f~st, which we 
call SAV.EXE does a structured analysis validation. It reads the SADT information in a knowledge base 
and reports inconsistencies. The tool is thorough enough that it has consistently caught problems in 
SADT models we have borrowed from the literature, and from other practitioners for dry running the SALDT 
tools. The second validator focuses on checking (hierarchically) the activation rule logic, behavior 
rules, activation rules, and object type descriptions. Since the lion s share of this work is to verify 
various aspects of a model related to timing, we call it the Temporal Validator (TV.EXE). There is a 
documentation tool that makes a dictionary of terms it extracts from a model. This is more of a demonstration 
of auto-documentation potential than an implementation of a conventional document. The dictionary it 
generates defines all data objects, resources, activities, behaviors and types. It is a good source for 
cross reference. The fifth tool prepares an activity tree t SALDT,the tool set, is copyrighted by Michael 
Moore document. This document is an indented listing of the activities in a model, showing hierarchical 
relationships. The listing is a clear bird s eye view of the modeled system. The sixth tool prepares 
hard copy SADT diagrams with some automatically generated commentary, and with activation and behavior 
rules. It is compatible with many impact printers with graphics capability, and with HP Laser Jet printers. 
These first six tools have been in use for approximately three years, and were developed over a five 
y~ period. The seventh tool is under development at the present time, and it is partially operational. 
Designated CPATH.EXE, it performs a critical path analysis of a model by examining its activation and 
behavior rules. The path analysis identifies all individually schedulable entities (which we call paths, 
see Fig 6), and then classifies them into soft and hard real-time categories using information in the 
knowledge base. CPATH then performs a Rate Monotonic Analysis on the hard real-time set and reports assigned 
priorities and schedulabilities. The paths are listed in PDL format (Ada, J73 and C are supported). Finally, 
it performs a utilization analysis for all paths and reports the minimum, average and worst case utilizations. 
Each of these steps are performed for the software residing on individual processors. In cases where 
the model describes a distributed system, each node is assessed separately. SALDT Threads are schedulable 
entities that can be implemented by an accept AdaTask < . region /!!7 FJ Figure 6 SALDT Threads Combining 
The Tools And The Methods. The development of hard real-time software is some what of a bottom up experience. 
This is because the designer must deal with software which has srnct upper limits on the amount of process 
blocking it does, and strict upper limits on the amount of CPU (disk or channel) time it consumes, One 
does not know how long a piece of software takes to execute until it is built and tested. Nevertheless, 
it is much more useful and efficient to know execution time early on, long before units are built, because 
we wish to deal with them at design time. Advocates of predictable real-time computing point out that 
real-time operating system primitives should exhibit predictable timing characteristics (5). 108 Users 
of SALDT can describe timing constraints in a top down manner when the situation demands i~ Resource 
use timing constraints handled in this manner are in effect budgets that are tied to activation rules. 
To illustrate this consider a system with four main functions. Each function has a set of activation 
rules describing, abstractly, when they become active. Perhaps we have a stability controller activating 
at 8 Hertz, a track algorithm activating at four Hertz, a navigation database activating whenever a query 
occurs (limited to 1 query per second), and a decision aid activating on demand(limited to 0.2 Hem). 
Each function could be given activation rules, with deadlines, and behaviors with hard/ soft attributes 
and resource consumption ranges. This information can be used to assess resource utilization and schedulability. 
As each function is hierarchically defined in more detail the design can be reassessed at each step. 
The lowest level of detail are SALDT activities whose behaviors define time constraints for the software 
as design objectives. SALDT does not demand top down descriptions of timing constraints. A purely functional 
decomposition can be done and filled in with time constraints at the lowest level. In either case practitioners 
must view a system as a combination of soft and hard real-time processes which can be modeled using SADT 
S activity diagrams. Software, hardware and people-ware are identified in the model as mechanisms associated 
with activities. Activity decomposition is carried out at least to the level that activities each have 
one engine mechanism, either a piece of hardware or a person that carries out the work of the activity. 
These activities may have software associated with them as well. (For cases where people are the engine, 
the software arrow represents the policy or list of rules followed by the person doing the work.) Practitioners 
are required to supply activation rules and behaviors at least in the bottom tier of activities. These 
rules describe the utilization of the resources and their timing characteristics. Behaviors represent 
the instantiation of software functionality. Whereas a mechanism arrow may indicate that CSCI XYZ directs 
the activity, the behaviors associated with the activity are the procedures in the CSCI that are directly 
involved. For example, an activity may compute a vertical projection using two software modules named 
XYZ and MATH_LIB. The behavior may be SINE(ALPHA). ALPHA is a control arrow or input arrow object name, 
SINE may be a method in MATH.LIB, and the call to SINE is a statement in XYZ. Thus behaviors can be used 
as a real-time generic instantiated not just by TYPE, but also by arity (number of parameters), timing 
characteristics, and flow pattern (in/ out relationships of the parameters.) We are presently investigating 
the feasibility of the behavior concept as a cornerstone of a software reusability concept. A Small Example 
Of SALDT Application. For purposes of illustration we conducted what we feel is an interesting and relevant 
example. We present here the WashingtonAdaSymposiumProceedings-June1991 problem of automating a micro-brewery 
for producing a fine malt beer. While we present this as a home micro­brewery problem, the process is 
performed on a larger scale in every brewery around the world. When making beer malted grains are mashed 
and left to brew in a hoMing tank where enzymes, trappcxl in the grain, break down starches into sugars. 
Depending on temperature, PH, and length of time differing amounts of sugars are converted into alcohol 
by the yeast. The key to a successful beer is the ability to consistently achieve and maintain the same 
combination of mashing conditions. The brewing process involves 5 phases characterized by the temperature 
of the grain and the amount of time it spends at a given temperature. Figure 7 illustrates a typical 
Time vs Te-mperatw-graph. - T 200 SACCHARIFICATION- II E M SACCHARIFICATION- I P 1150 E DOUGHING-IN R 
PROTINE-REST A 100- ACID-REST : R 50- E I I 1 I f 2 3 Ho~rs ~ l-. -. r]bmre m-..I: Kest nL---­rnases 
 In the DOUGHING-IN phase crushed, malted grain is added to the water. The correct pH of a mash depends 
on the type of malt used. It must never begin above pH 6.2 or below pH 4.7. The acid rest phase is used 
for correcting the initial pH when it is out of range (water pH differences may require a correction.) 
The Protine REST phase is for the breakdown of enzimes into sizes benitltial to the yeast. The f~st SACCHARIFICATION 
REST phase is for producing sugars used to flavor the beer, while the second is for producing sugers 
which are consumed by the yeast. The hardware for this process is illustrated in Figure 8. A double boiler 
is required to hold the grain and water mixture while the mix is maintained through rest phases at predetermined 
temperatures. In the tank is an agitator, a temperature sensor, a low water level detector, a heater 
and a PH sensor. A computer reads the sensors and controls the heater and agitator so as to follow the 
phase time and temperature profile, and to atert the Brew Master of out of range conditions. The computer 
has a real-time clock. Figure 9 illustrates the interfaces to the computer. Agitator pH Sensor G> Thermostat 
 Low Level Sensor Heater fH Figure 8 Mash Tank Temperature Agitator Motor pH Sensor HeatingElement Control 
Computer  ~aterLevel Sensor Console Console 41 ..- ­ 13gure 9 computer inputs a Wnputs Requirements. 
The control software must meet the following requirements: 1) Read a time/ temWrature profde from a diskette. 
2) Maintain the temperature through the profile to within a 1 degree Fahrenheit tolerance. 3) Agitate 
the mix during heat cycles to avoid heat gradients. 4) Detect PH out of range conditions (not within 
[4.6 .. 6.2] and signal an alert. 5) Signal a temperature alert if the mix is not within two degrees 
of its target Temperature. 6) Scan sensors at 8 Hz. 7) Turn the heater ancl agitator off and signal an 
alert when the water level is low. 8) Turn theheateron when the temperature is one degree too cool, and 
off when it is one degree warm. The Model. The micro brewery SALDT model consists of 16 activity boxes 
shuctured as illustrated in Figure 10. Figure 10 is an outline showing the functional decomposition used. 
Unfortunately the graphics model itself is too large to include in its entirety in this forum. Figures 
11 and 12 are the fwst tier graphs. o O-Manage Microbrewery Double Boiler [A/0] 1: l-Elaborate Initial 
Conditions [A/l] 2: l-mana~e the REST PHASES of the mixture [AM 3: 2-Coun~downrestperiod [A2/1] 4: 2-Switch 
to next phase [A2/21 s I-Monitor Double Boiler Conditions [A/3] 6 2-Monitor thewaterlevel [A3/1] 7: 
2-MonitorthePHofthesolution[A3D] 8: 3-ScalePH sensordatato [1 .. 14] [A32/1] 9: 3-Handle PH alerts [A32/2] 
10: 2-Monitor the temperature [A3/3] 11: 3-Convert Sensor Data To Degrees Fahrenheit [A33/1] 12: 3-Compare 
temperaturewith wget [A33EI 13: 3-Control the stirring motor [A33/31 14: 3-Control the heater element 
[A33/4] 15: 3-Control over/ under temp alert [A33/51 16: l-Display Microbrewer Status [A/4] Figure 10: 
The Micro Brewery Activity Tree Stafi_Switch Water_L.evel_Se.nsOr Time LJ_ Reset_Phase_Displayempemture.%nsor 
Manage Micrckewely Operator_Ale.m b He.atin@eznem_Control H s~~ ~ Double Boiler A 4 Conkol_Softwam OEMComputer 
TITLE: MANAGE M3CROBREWEY NUMBER: o IXXJBLEBOILER VIEW ATOMIC vfartzge Micro Brewery Double Boiler is 
the complete system being described. W cmtml is caused by its mviromnenL Cyclic p messing is involved. 
Both Iard and soft real-time m@mnertK are specified Figure 11 The top Activity Box of the Brewery Model 
110 The PDL. The Ada PDL was produced by the CPATH tool. This particular PDL does not identify tasks 
per se . Instead it identifies separately schedulable paths of control and computes the critical path 
through each. For each critical path CPATH computes minimum and maximum CPU use and wall clock times 
(minimums are a consequence of the modeling attributes.) Each path is then considered to be a schedulable 
entity for Rate Monotonic Analysis purposes. A priority is assigned to each hard real-time path and a 
schedulability analysis is performed. Each path is documented (in PDL) with its utilization, priority, 
path times, entry conditions and control flow. It is (for now) up to the engineering staff to determine 
a tasking model horn the PDL path model. As a general rule each path can be considered an accept region 
of a task. - iIi I I 3: II 1111 I I 11 Ir1 I I NODE A/01 TITLE Man&#38;?. Mkmbmwerf Double Boiler I 
h UMBSRO VIEW Cm@ GI1: pH Scnm B311: Targot_Tempmtum GE Tempentmc Sensor B312. PH_SMaOr GCl: Start_Switch 
B313: tmFmtum_Senaor GC2 TinE B3C1: Rcat_Pkusc 00 Wmk_LcvclSc.nsm B3C2 Tum GO1 : 0p=ruor_Ak17.s B3c3: 
Watc_Imcl_Scnsor G~ Sdrxing_Momr_Ccmmd B301 : Opcm@lc.rm G03: Heating Iiem%t_Conuol B302: Stining_M.tor_Cmtrd 
G(M Rcst_s%a@hplzy B303: 3hting_Fknmt_ContmlGMt: OEM-Computer B3cW Terqeratum.P GM2 Comrol_Sofnvm B305: 
pH_Vd~B] CS: stm-Swir&#38; B3M1: Monitor B1c2 Rest.pbzu B3M2 OEM_l%qukxEl 01: Rcst_Pah9e B411: Water_kvd-Alti 
BI 02 ti.kngttu B4k Tcrqmtum-FB] 03: ti-Tcmpxxtumx B-K? PH_Vak El Ml: Rcinihdimtimt B41k ikS1_k Blhfz 
OEM_ Compu!m B4c1: Tm B2U: Flmw_IxJ@ B401 : ks!-phas@i@ty B2E t%ax_Tcqmwu B4MI : cmmO_sOfhvUe B213: Phmc_Rdu&#38;_Timt 
B4M2 C)F.bf_cnuqYJEr B2ct: TmE B2Q R@_Ybax B201: Rcst_pbax B202 T.FLTe~* B203! -Jdtive. rim B2Mt: b-MsI 
 Figure 12 The Top Activity Box of the Brewery Model 2nd Level. The PDL listed below is an example of 
a path generated by CPATH. The ENTER WHEN construct is not Ada. It is used to convey the conditions of 
the ACCEPT. Note that the path is annotated with time constraints and that a critical Washington Ada 
Symposium Proceedings -June 1991 Tlis paper discusses software patents, their economic implications for 
the computer industry, and possible ways of improving the system so as to improve the economic competitiveness 
of the United States. PATENT CHARACTERISTICS Patents are a powerful form of protection for technology, 
Obtaining a patent is, however, far more difficult, costly, and uncertain than is making use of other 
mechanisms such as copyrights and trade secrets. The standard of innovativeness required to obtain a 
patent is high the invention must be novel (not previously discovered), and its creation must not have 
been obvious to someone of ordinary skill.4 Obtaining a patent generally takes 18 to 24 months (sometimes 
longer for software patents), and can cost from about $5000 to $15,000 or more for attorney s fees, plus 
from about $3500 to nearly $7000 or more for fees paid to the Patent and Trademark Office (PTO).5 Applications 
for a patent, which must include a complete description of the invention along with claims that define 
the boundaries of the protected invention,G are examined to determine whether they are novel, unobvious, 
and meet other requirements. Because these claims describe an invention at a conceptual level, rather 
than a mere implementation, patent protection for software is much broader than is copyright. Patents 
protect an invention in a manner that is relatively independent of its particular implementation, whereas 
copyright law is designed to protect an expression of an idea rather than the underlying idea itself. 
A successful patent holder gains a powerful right the right to exclude others from using the invention 
for 17 years after the patent is issued. A patent, once issued, is presumed to be valid.7 The first inventor 
obtains all rights to an invention independent discovers of the invention have no 4. 35 U.S.C. $103 (1988). 
 5. PTO fees include an application fee, issue fee, and maintenance fees, the last due at intervals throughout 
the life of the patent. Patent fees were only a few hundred dollars before 1980, when application and 
issue fees were increased and maintenance fees introduced, as part of a desire by Congress for the PTO 
to be self-supporting, They were increased again in 1990 and are now set at a total of $3340 for an individual, 
smrdl business, or nonprofit institution or $6680 for a large corporation, plus slightly more for especially 
complex patent applications. 6. 35 U.S.C. ~ 112 (1988). 7. 35 U.S.C. Q 282 (1988).  rights and may 
be sued for intilngement. Although the actual first inventc)r not the first to apply for a patent~enerally 
gains these rights, an inventor who is issued a patent is presumed to be the true inventor, and attempts 
to change this presumption usually require long and costly litigation. The right to exclude others from 
use or sale of an invention is a particularly powerful one. In general, a patent holder can choose to 
license or not to license an invention, and can ask any license fee he or she chooses. THE HISTORY OF 
SOFTWARE PATENTS  Whether and what aspects of software might be patentable has been the subject of controversy 
and litigation for nearly 25 years. There are a number of reasons why software patentability became an 
issue. The very nature of software, which seems to have similarities to both writing and to machinery, 
without being clearly either, caused difficulties early on, since it was not clear whether software might 
be better suited for protection by copyright or by patents. In addition, there has never been agreement 
within the computer industry on whether patent protection was desirable for software, and thus neither 
the courts nor Congress could act on the basis of industry consensus. The first guidelines adopted by 
the Patent and Trademark Office in the 1960s for the patentability of software provided that computer 
programs, whether claimed as a machine or as a process, were not patentable. These resulted in part from 
a Presidential Commission report* that recommended that computer programs be expressly excluded from 
coverage by the patent laws, based primarily on the inability of the Patent and Trademark Office to properly 
examine applications for software patents, and also as a result of opposition from IBM and other major 
computer manufacturers.9 The Patent and Trademark Office strongly opposed software patents until the 
early 1980s, with software patentability the subject of a continuing legal battle involving the PTO and 
the Court of Customs and Patent Appeals (CCP.A). 8. Report of the President s Commission on the Patent 
System, To Promote the Progress of ... Useful Arts in an Age of Exploding Technology. (1966). 9. See 
Hauptman, How Computer Sojtware Has Come 10 be Protected Under the U.S. Patent Law. 6 COMPUTER LAWYER 
11.  Unlike the Patent Office, the CCPA consistently took a favorable view of software patentability, 
and in a series of court decisions from 1968 through 1970, it briefly eliminated the legal arguments 
against patentability. A landmark 1972 decision by the Supreme Court, however, in the case of Gottschalk 
v. Benson,10 reversed these trends by ruling that an invention involving a method for converting binary-coded­decimal 
(BCD) numbers to binary numbers w7as unpatentable. The court said that phenomena of nature ... mental 
processes, and abstract intellectual concepts are not patentable, and the patent, if grant ed, would 
wholly preempt the mathematical formula and [the] practical effect would be a patent on the algorithm 
itself. The Benson ruling caused much confusion because of the lack of clear rationale and reasoning 
for its decision, and was widely criticized by legal scholars. 11 Its principal effects were to reverse 
the trend toward patentability that had been led by the CCPA, vindicate the Patent Office s long-standing 
opposition to software patents, and to create the perception in the computer industry that software was 
not patentable, resulting in a strong chilling effect discouraging patent applications. Even so, many 
patents for software were issued after the Benson decision, with the CCPA interpreting the Supreme Court 
decision in increasingly liberal terms as time passed. In a 1981 decision, Diamond v. Diehc12 the Supreme 
Court ruled again on software patents, this time in favor of patentability. The invention itself was 
a process for improving the curing of rubber in a mold, using a sensor that continuously measured the 
temperature in the mold, and an equation that was used by a computer to recalculate the time until the 
curing was complete so that the mold could be opened. Though the equation itself was not new and presumably 
unpatentable as a scientific principle the process using the computer and equation was held patentable. 
As a result of this decision, the Patent and Trademark Office switched from a policy of opposing software 
patentability to one that generally favored patentability. Although the number of patents issued for 
software continued to be relatively small through 1987, the Supreme Court ruling slowly changed the perception 
of those in the software 10.409 U.S. 63 (1972). 11. See, e.g., Chisum, lle PaientabJity of Algorithms. 
47 UNIVERSITY PITTSBURGH LAW REVIEW 959 (1986).  12.450 Us. 173 (1981). industry toward the view that 
software was patentable, and in recent years, increasing numbers of software patents have been issued 
as applications that were filed earlier made it through the backlog. By one count, there were 200 software 
patents issued in the first 4 months of 1989.13 DO PATENTS REALLY ENCOURAGE INNOVATION? Whether patenting 
software would be, on the whole, good or bad for the software industry has never been directly considered 
by the courts, w7ho resolved the question on narrow legal issues. While critics of software patents may 
see the software industry as unique, most of the specific problems that they see resulting from software 
patents also occur with other areas of technology. It is thus worth first looking at the broad issue 
of what is known about the extent to which the patent system actually does successfully encourage innovation. 
The patent system, though now firmly entrenched in nearly every industrialized country in the world, 
is more the result of history than any real evidence that its benefits necessarily outweight its costs, 
particularly in every area of technology. For example, one social scientist, in presenting evidence to 
a Canadiam Royal Commission studying the economic effects of the patent system, stated that no economist, 
on the basis of present knowledge, could possibly state with certainty that the patent system @ the United 
States], as it now operates, confers a net benefit or a net loss upon society. If we did not have a 
patent system, it would be irresponsible on the basis of our present knowledge, to recommend instituting 
one. But since we have had a patent system for a long time, it would be irresponsible, on the basis of 
our present knowledge, to recommend abolishing it. 14 Questionnaire and interview data has generally 
found that the tendency of managers to view patents as important varied very substantially depending 
on the industry.is Patents were seen as critically 13. Kahin, The Sojtware Patent Crisis. TECHNOLOGY 
REVIEW, April, 1990, p. 53.  14. Quoted in Firestone, Economic Implications of Pa[ents. Ottawa, Canada: 
University of Ottawa Press, 1971, at 231. 15. The principtd studies are the following: Scherer (Ed.), 
Pa/ents and the Corporation: A Report on Indu.rtrial Technology under Charrging Public Policy Boston: 
J. J. Cafvin, 1958. Taylor and Silberston, 7he Economu Impact of the Patent System: A Study of [he Britzsh 
Experience. Cambridge: Cambridge University Press, 1973. Levirr, Klevorick, Nelson, and Winter, Appropriating 
the Returns from Industrial R&#38;D. Brookings Papers on Economic Activity, 1988.  112 Washington Ada 
Symposium Proceedings -June 1991 important for investment in research in some industries, such as pharmaceuticals-this 
results from the large investments needed for research and development and clinical testing of drugs, 
the ease of duplicating such drugs once known (as generic drugs), and the effectiveness of composition-of­matter 
patents in protecting such drugs. In other industries, however, patents were viewed as far less important. 
Having a technological lead on one s competitors and having superior sales or service were in most industries 
viewed as more important to successfully introducing new technology as were patents. Patent systems are 
very similar throughout the world, and, although there are many variations on details, even these differences 
are starting to disappear as attempts are made to standardize and allow broad patent rights in many countries 
through reciprocal treaties and centralized organizations such as the European Patent Office. While most 
systems give patent holders an exclusive monopoly on the invention, some systems provide that compulsory 
licensing of patents be required under certain conditions. Thus, for example, Canada in response to complaints 
about price-gouging in the pharmaceutical industry passed a law in 1969 allowing the Canadian Commission 
of Patents to grant compulsory licenses to importers of drugs patented in Canada.16 The question of compulsory 
licensing of patents is a particularly significant one, because requiring licensing is one way of weakening 
a possibly too­powerful patent system and eliminating some of the associated economic efficiencies. It 
is also significant because most schemes for compulsory licensing involve some discretion on the part 
of a regulatory system that approves applications for licenses on a case-by-base basis and thus can, 
in a way that the rules of a general patent system cannot, decide based on the circumstances of each 
particular case. Relatively good evidence is available on the effects of compulsory licensing, including 
both questionnaire data and, significantly, analysis of research and development investment by companies 
forced to license patents by antitrust decrees. Interviews conducted by Scherer and his colleagues in 
1958 resulted in a conclusion that there was no significant discouragement of research and development 
resulting from the antitrust decrees, but they did find distinct e~ridence that companies subjected to 
antitrust mandatory licensing decrees 16. Firestone, op. cit., at 208. were patenting fewer of their 
inventions and keeping relatively more of their new technology secret. 17 This tended to be particularly 
true of process patents, which are more amenable to protecting by secrecy. A study of 678 U.S. corporations 
with significant research and expenditures in 1975, in which 42 companies were subject to an antitrust 
decree involving pat ents, was done to see if the compensatory licensing resulting affected the amount 
of research expenditures. The researchers concluded that the analysis provides no significant indication 
that the companies subjected to compulsory patent licensing under antitrust decrees sustained less intense 
R&#38;D efforts than other firms of comparable size and industry origin. If anything, the opposite tendency 
is revealed, with a significant indication that expenditures increased slightly,18 BENEFITS AND COSTS 
OF PATENTS Whether a patent system ought to exist and, if so, what particular system is best-depends 
upon an assessment of whether the benefits of a given system exceed its costs. In the modern corporate 
environment, invention is only a small part c)f what is necessary to bring a product to market and to 
maintain a corporation s market share. Investment in manufacturing facilities, control of distribution 
channels, investments in marketing, brand-name recognition, technological leadership and know-how, and 
other factors may all dwarf the effects of patents. Certainly, the software industry has thrived for 
decades with no apparent need for patent protection. The claimed benefits of patents are well known primarily, 
encouragement of investment, an increased flow of information resulting from the public availability 
of the patent specification, and the provision of a mechanism that can allow small companies sufficient 
protection to enter a market. A less recognized benefit is the promotion of standardization and software 
reuse. With regard to the latter, patents might provide a mechanism that could help establish a reusable 
components industry, if, for example, developers who invent a method simply refuse to make it available 
in any form other than in standard reusable components. 17. Scherer, The Economic Eflorts of Compulsory 
Patent Licensing. Monograph 1977-2, New York University, 1977, at 64. 18. Scherer, op. cit., at 75. 
 basis of the drawing and the wording of the most significant claim for the patent as shown in the Of)icial 
Gazette. The results are shown in the table below: Category No. of Pat. Percent Software 5 12!40 Probably 
software 4 109 0 Either software or 15 3670 hardware Probably hardware 8 19 Y. Software &#38; hardware 
8 1970 combinations Interface standard or 2 data storage format TOTAL 42 1000/0 The five patents in 
the software category included a technique for providing interactive control over running computer programs, 
a technique for natural language translation (e.g., Japanese to English), a vehicle diagnostic system, 
a silicon compiler method, and an interactive statistical system. The probably software category included 
a speech recognition technique and two different vehicle brake control systems. The either software or 
hardware category included a data communications system and a method of arranging data on a random-access-memory 
for display. The probably hardware category included several video signal processing patents. The sof~tare 
and hardware combination category included two patents for autofocusing systems in cameras and a camera 
flash, all of which included microprocessors. The interface standard or data storage format category 
included a telecommunications transmission format and a magnetic storage media format. While the sample 
here is very small, it does provide evidence that software patents are indeed being issued at a modest 
rate, perhaps 500 to 1200 per year, depending upon how a software patent is defined. What is particularly 
striking is the difficulty in distinguishing behl een patents that are clearly hardware or software. 
More than a third were in the category either hardware or software, and almost as many seemed to be 
one or the other but could have been otherwise. These decisions were made only on the basis of a single 
(the primary) claim and drawing and, indeed, without a great deal of thought. Detailed analysis of the 
patents would probably allow most of the patents in the either sof~are or hardware category to be placed 
in the Washington Ada Symposium Proceedings -June 1991 probably hardware or probably software categories. 
But while the detailed specification of the patent would clearly indicate how the best embodiment of 
the invention was implemented, this does not mean that the invention cannot be implemented differently. 
Indeed, some patent specifications have been described as so ftw7are because they were first reduced 
to practice in that form, but with the intention that they would eventually be manufactured as hardware. 
Deciding that a patent was definitely hardware or software would, fc]r many patents, be quite difficult 
. The construction of increasingly complex systems and the continuing trend to use multiple microprocessors 
will increasingly blur the distinction between software and hardware. This makes it very difficult to 
determine what a software patent is. A PROPOSED SOLUTION What should be done about software patents? 
Are software patents a major threat to innovation in the software industry? Will expensive litigation 
drive small, innovative cc~mpanies out of business, resulting in a less competitive and less innovative 
industry? More to the point, do the overall benefits of software patents exceed their costs, and, if 
so, what should be done about it? Unfortunately, we have little data upon which to answer these questions. 
In particular, we need reliable data on the extent of software patenting, on the types of software-related 
technolog that is being patented, and on the breadth and effects of specific software patents. However, 
my own study has led me to two beliefs: First, the costs of the present strong patent system outweigh 
the benefits, and that a better balance would be achieved if the patent system was less strong. Second, 
that although there are particular problems with software patents, it makes no sense to attempt to distinguish 
between hardv7are and software in issuing patents. These beliefs lead to the following proposal for 
changes to the patent statute: 1. A clear criteria for software patentability would be provided that 
allows any algorithm to be patented. This would expand software patentability slightly, and, most significantly, 
remove a heavy cloud of uncertainty that now hangs over a minority of software patents and a slight cloud 
that hangs over nearly all software patents. To reduce any concern that patenting basic algorithms may 
prevent students or researchers from using algorithms in legitimate ways, a clause should also be added 
to the statute that would clarify and codify the case law that already holds that experimental use of 
a patented algorithm for student or research purposes is not an infringement, as long as that algorithm 
is not embedded in a product and sold, 2. Patents relating to information processing and computation 
would be subject to comptisory licensing. In this procedure, patents would be presumed when issued to 
be available for licensing on reasonable terms. If, however, special circumstances existed, the patent 
applicant could apply for an exception to compulsory licensing. Legitimate conditions for an exception 
might be small companies who need exclusive rights to a patent to establish a niche in the market, or 
to help establish a reusable components industry. The Commissioner of Patents and Trademarks would be 
empowered, in such special cases, to allow exclusive rights for a portion of the patent term, after which 
licensing would be required. This proposal is only one possible approach to reducing some of the problems 
that exist now with hardware and software patents in the computer industry, and that may help maintain 
a balance between benefits and costs of patents for software in the future, The overall effect would 
be to reduce the power of the patent system and to increase competition in the industry. Technology would 
be more broadly available, and the tendency for patent holders to demand excessive royalties even when 
they were willing to license their patents would be eliminated because they would be held to a cap of 
reasonable levels under threat of arbitration, ACKNOWLEDGEMENTS I would like to thank Reg Meeson, Sarah 
Nash, and Dick Wexelblat of the Institute for Defense Analyses, Computer and Software Engineering Division, 
for valuable comments on the paper. A Note on Citations and Sources The federal statutes and court decisions 
cited in this paper can be found in any law library Statutes relating to patents are found in Title 35 
of the United States Code. The citation 35 U. S.C. ~ 103 (1988), for example, refers to title 35, U.S. 
Code, section 103, as codified in 1988. Decisions of the appeals courts are cited, for example, as 726 
F.2d 734 (CAFC, 1984), which refers to volume 726 of the Federal Reporter, 2nd series, page 734, in a 
1984 decision of the Court of Appeals for the Federal Circuit. 118 Washington Ada Symposium Proceedings 
-June 1991 
			
