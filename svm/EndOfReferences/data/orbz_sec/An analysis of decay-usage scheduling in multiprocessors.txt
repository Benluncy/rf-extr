
 An Analysis of Decay-Usage Scheduling in Multiprocessors D.H.J, Epema Department of Mathematics and 
Computer Science Delft University of Technology P.O. Box 356, 2600 AJ Delft, The Netherlands e-mail: 
epema@cs.tudelft .nl Abstract Priority-ageing or decay-usage scheduling is a time-shar­ing scheduling 
policy capable of dealing with a work­load of both interactive and batch jobs by decreasing the priority 
of a job when it acquires CPU time, and by increasing its priority when it does not use the (a) CPU. 
In this paper we deal with a decay-usage scheduling pol­icy in multiprocessor systems modeled after widely 
nsed systems. The priority of a job consists of a base priority and a time-dependent part based on processor 
usage. Because the priorities in our model are time dependent, a queueing-theoretic analysis, for instance 
for the mean response time, seems impossible. Still, it turns out that as a consequence of the scheduling 
policy, the shares of available CPU time obtained by jobs converge, and a deterministic analysis for 
these shares is feasible: for a fixed set of jobs with very large (infinite) processing demands, we derive 
the relation between their base pri­orities and their steady-state shares. In addition, we an­alyze the 
relation between the values of the parameters of the scheduler and the level of control it can exercise 
over the steady-state shares. We validate the model by simulations and by measurements of actual systems. 
1 introduction Time-sharing systems that support both interactive and batch jobs are steadily becoming 
of less importance in this age of workstations. Instead, for compute-intensive jobs, the use of multiprocessors 
and clusters of unipro­cessors is increasing. Currently, most of the latter sys­ tems run some variation 
of the UNIXTM operating sys­tem, which employs a classical time-sharing policy. In this paper we aualyze 
a scheduling policy in multipro­cessor systems modeled after scheduling in UNIX and Mach, which has 
a scheduler similar to the one in UNIX , under a worldoad consisting of long, compute­intensive jobs. 
 Permission to copy without fee all or par-l of this material is granted provided that the copies are 
not made or cfktributed for direct commercial advantage, the ACM copyright notice and the title of the 
publication and its date appear, and notice is given that copying is by permission of the Association 
of Computing Machinery.To copy otherwise, or to republish, requires a fee andlor specific permission. 
SIGMETRICS 95, Ottawa, Ontario, Canada 0 1995 ACM 0-89791 -695-6/95/0005 ..$3.50 In this scheduling policy, 
the priority of a job is equal to the sum of its fixed base priority and a time­dependent component. 
The latter part, the accumulated and decayed CPU usage, increases in a linearly propor­tional fashion 
with CPU time obtained, and is period­ically (at the end of every decay cycle) divided by the decay factor 
(a low priority corresponds to a high level of precedence. ) Jobs with equal base priorities belong to 
the same class, Jobs are selected for service in a processor-sharing fashion according to their priorities. 
Because the priorities are time dependent, a queueing­theoretic analysis of the policy, for instance 
solving for the mean response time, possibly assuming a Poisson arrival pro cess and an exponential service-time 
distri­bution for each class, seems infeasible. However, pro­vided-that the set of jobs in the system 
is fixed and that all jobs are present from the same time onwards, a deterministic analysis is feasible. 
Our main results are proving the convergence of this scheduling policy in the sense that the fractions 
of CPU time obtained by jobs (the service fractions or shares) have limits (the steady-state shares), 
and deriving the relation between the base priorities and the steady-state shares. In order to validate 
our model and to assess the impact of the continuous-time hypothesis without any influence of implementation 
details of actual systems, we simulate the UNIX scheduler. In general, the re­sults of the simulations 
agree well with the model wheu the quantum size is small enough. In addition, we per­formed measurements 
on uni-and multiprocessors un­der UNIX versions based on 4.3 Berkeley Software Dis­tributions (4,3BSD) 
UNIX, and under Mach. The results for the LI.3BSD uniprocessor agree extremely well with the model, for 
the 1.3BSD multiprocessor they do not agree, but detailed traces of this system show that its scheduling 
policy does not comply completely with the model. For the Mach systems, the measurements match the results 
of the model as we will describe it in Section 2 not very well; however, after adapting the model in 
an obvious way to include the somewhat different way in which Mach effectuates decay, the measurements 
match the (adapted) model very well indeed. In classical time-sharing systems, the two main per­formance 
objectives are short response times of inter­active work and high throughput of large jobs, For a compute-intensive 
workload, it is a natural objec­tive to deliver pre-specified shares of the total compute power to groups 
of jobs (e.g., all jobs belonging to a sin­gle user). Schedulers for uniprocessors with this objec­tive 
have been treated in the literature under the name of fair-share schedulers ([3, 7, 8]), though share 
sched­ulers would be more appropriate. Our results show how to achieve share scheduling for long, compute-intensive 
jobs in UNIX systems without scheduler modifications, while in each of [3, 7, 8], an additional term 
in the pri­orities of jobs is needed. In [1I], a probabilistic share scheduler is presented. Our analysis 
extends the analysis by J.L. Hellerstein [5] of scheduling in UNIX System-V uniprocessors and the short 
treatment by Black [2] of Mach multiproces­sors to a unified treatment of scheduling in uni-and multiprocessors 
running UNIX System V, 4.3BSD UNIX, or Mach. The extension to 4.3BSD and Mach involves more complicated 
formulas for the priorities; the main difficulty in the extension to multiprocessors is to prove the 
convergence of the scheduling policy and to deter­mine how many processors serve each of the classes 
in the steady state. Hellerstein [6] gives a more detailed ex­position of the results in [5], and also 
deals with control issues relating to the range of possible ratios of steady­stat e shares and the granularity 
of service-rate cent rol. In [5, 6], measurements are presented for uniprocessors under UNIX System V; 
also they match the predictions of the model remarkably well. For lack of space, we cannot go into the 
convergence rate of our decay-usage scheduling policy. Suffice it to say here that when a system with 
a fixed set of jobs (as in our model) is considered, or when an arrival or departure occurs, for reasonable 
numbers of jobs (say less than 10 per processor), we can prove that it takes at most a few tens of seconds, 
and usually much less, before the jobs start receiving their steady-state shares. 2 The Model In this 
section, we describe scheduling in UNIX and in Mach, our model of this type of scheduling, and the differences 
between them. 2.1 Scheduling in UNIX Systems Scheduling in UNIX follows a general pattern, but de­tails 
vary among versions, such as UNIX System V [I], 4.3BSD UNIX [10], and the related operating system Mach 
[2], which is the basis of the operating system of the Open Software Foundation (OSF)l. The descrip­tion 
below applies to both uni-and multiprocessors (it does not apply to System V Release 4). UNIX employs 
a round-robin scheduling algorithm with multilevel feed­back and priority ageing. Time is divided into 
clock ticks, and CPU time is allocated in time quanta c~fsome fixed number of clock ticks; in most current 
UNIX and Mach systems, a clock tick is 10 ms. When a time quantum expires, the scheduler selects the 
process at the head of the highest non-empty run queue (lowest value of priority) to run, The set of 
priorities (typically 0,1 , . . . . 127) is partitioned into the kernel-mode priori­ties (usually O through 
49) and the user-mode priorities 10pen Software Foundation, lnc , 11 Cambridge Center, Cambridge, Ma 
, USA (50 through 127). A job (in UNIX called a process) is represented by a job descriptor, with the 
following three scheduling-related fields (also for Mach, we use UNIX terminology): 1. p.nice: The nice-value, 
which has range Othrough 19. 2. p.cpw The accumulated and decayed CPU usage, which is incremented by 
1 for every clock tick re­ceived, and is periodically decremented (priority ageing, see below). 3. p.usrprti 
The priority of a iob, which depends on p-nic~ and p.c~u. A-high &#38;lue corresp~nds to a low precedence. 
In the sequel, in the context of UNIX, priority refers to p.usrpr~.  The fields p.cpu and p.-usrpri 
of the running pro­cess are regularly updated. In many implementa­tions, there are 32 instead of 128 
run queues; then, the two least-significant bits of p.usrpri are ignored when deciding to which run queue 
to append a process. The priority of a process is equal to p-usrpri := PUSER + R . p..cpu + ~ . p-nice, 
(1) with PUSER a constant (usuaJly 50) separating kernel­mode priorities from user-mode priorities, 
and R and -y system parameters. At fixed intervals (a decay cycle, usually a second), the following recomputation 
is per­formed for every process: p-cpu := p-cpu/D + 6 ~p_nice, with D > 1 the decay factor, and with 
6 > 0, and p.usrpri is set according to (1). In System V, R = 0.5, D = 2,y= 1,and6= 0, In4.3BSD, R = 
0.25, D = (2.10ad +l)/(2.load), y = 2, and 8 = 1, with load equal to the number of jobs in the run queues 
divided by the number of processors as sampled by the system. In Mach, there is a direct match between 
priorities, which range from O through 31, and the 32 run queues. Here, PUSER = 12, and R = lR /T, with 
1, the load factor, defined by 1 = max(l, iV/P), where N is the number of processes and P the number 
of processors, with R the increment of the priority due to one second of re­ceived CPU time when 1 = 
1 (R s 3.8), and with T the number of clock ticks per second. The effect of the load factor in R is to 
keep the priorities in the same range regardless of the number of processes and pro­cessors (see Subsection 
4.2, Remark 3). Furthermore, D = 1.6, y = 1, and 6 = O. The way decay is effectu­ated in Mach is somewhat 
different from that in UNIX, but until we discuss Mach measurements in Section 6, we assume Mach operates 
in the same way as UNIX. In 4.3BSD UNIX ([10], p. 87) and Mach, a time quantum is 100 ms (10 clock ticks), 
in System V, it is 10 ms (1 tick). Because we will only consider systems in their steady states and with 
more processes than processors, the definitions of load for 4.3BSD and of the load factor / for Mach 
coincide, and we will in the sequel denote both by 1. Currently, T = 100 in almost all implemen­tations 
of these operating systems the author knows of, and throughout this paper we will assume this value for 
T. 2.2 The Decay-Usage Scheduling Model Our model of decay-usage scheduling is defined as fol-IOWS. 
 1. There are P processors of equal speed.  2. There are K classes of jobs. We consider the model from 
time t = O onwards. There is a fixed set of jobs, all present at t = O; no arrivals or de­partures occur. 
There are Jl~ class-k jobs, k =  1,....K. We write ~~ = ~~=1 Ml, and assume MJ; > P. Let k. be the index 
such that tik, ~ P andflkO+l > P. IfMl > P, thenlet ko = O. 3. A class-k job has base priority bk, with 
bk real and non-negative, and bk < bL when k < i, k, i = 1, . . . . 1[. The priority of a class-k job 
at time t is gk(t) = Vbk + RrJk(t), (2) with ~ and R positive, real constants, and Vk(t) explained below. 
 4. Time is divided into intervals of length T, called decay cycies, from t = Oonwards. Let tn= nT,n 
= 1,2,... The n-th decay cycIe [t~_l, t~] is denoted by Tn. The scheduling policy is a variation of a 
policy known as przority processor sharing [9] and also as disc rzrninatory processor sharing [4], that 
is, jobs simultaneously progress at possibly dif­ferent rates (called their processor shares), which 
may change over time. The functions v~(.) are defined as follows: v~(0) = O, and if during an interval 
[tl, t2]contained in one decay cycle a job of class k receives a constant fraction ~ ~ 1 of a processor, 
then vk(t)=W(tl )+.f. (i tl), for tl <t<tz. (3) Furthermore, at the end of a decay cycle, the fol­lowing 
recomputation is performed: vk(n~) := Vk(?t~)/~+6bk, k = 1,. .. ,1(, (4) where D and 8 are real constants, 
D > 1, 8 z O, 5. At any point in time t,the set of jobs that receive service and their processor shares 
are determined as follows. Order the classes such that gk, (t) < K-1, with {lc,li = 1,...,1[} = {1,2 
,. ... K}. Let r be the lowest index such that gk. (t) < q~.+, (t), that z~=l Mk, < P, and that ~~~~ 
Mk, > P, and let a be the index such that ~k,+,(t),~=l>..., qk.+1 (t) = qks(t) and that m,(t) < wt.+,(t). 
If such an r does not exist, let r = O; if snch an s does not exist, let s = K. Now at time t,each of 
the jobs of classes kl, . . . . L has a dedicated processor, the jobs of classes k,+l, . . . . k, evenly 
share the remaining P ~~=1 M~, processors, and classes k,+l, , kT; do not receive service. There are 
a few things to note about this schedul­ing policy. Because all processors have equal speeds, any number 
M of jobs can be given equal shares on any num­ber P of processors, by means of processor sharing, pos­sibly 
also across processors, when M ~ P:, which shows the feasibility of 5. above. Shares only change during 
a decay cycle when two priorities qk(t)and ql (t) become equaI. If at time t,jobs (of possibly different 
classes) have equal priorities, they will receive equal shares at any time during the remainder of the 
decay cycle, and their priorities remain equal. As a consequence, there are at most K 1 points in a 
decay cycle where shares change (these are then recomputed according to 5.) In­tervals between two consecutive 
points where this hap­pens are called epochs. The l-th epoch of decay cycle n is denoted by T.(l). For 
n z 2 we have n 2 n l n-2 ~ qk(o) + RT~D- + Rtibk~ D . (5) u&#38; J=o j=o The lower (upper) bound is 
only reached when class­k jobs starve (have dedicated processors) during decay cycles 1,....n (to seethis, 
we use (2), (3) with ~ = O or 1, and (4)). The dimension of the parameter T is time, expressed as elapsed 
time, or as the number of clock ticks deliv­ered by a processor during a decay cycle. This gives us the 
opportunity to compare the behavior of our model for processors of different speeds by choosing different 
values for T (different numbers of clock ticks in a decay cycle, each clock tick representing the same 
amount of useful work). The parameters R and L) may depend on other parameters (such as P and the Mk), 
but should be constant in any instance of the model. In the se­quel we will use the terms base priority 
and nice-value interchangeably. There are three points where our model differs from real UNIX scheduling 
(except for the shift in priority by PUSER, which is inconsequential), viz.: (1) In our model we use 
continuous time and a continuous range for the parameters in the scheduler, while actual sys­tems use 
discrete time and integers; (2) In many UNIX systems, the two least-significant bits of p-usrpri are 
ig­nored when determining to which run queue to append a process; and (3) The UNIX scheduler uses priorztg 
clamping p cpu and p-usrpri have maximum values. For instance, in 4.3BSD, p-cpu cannot exceed 255 and 
p-rwrprz cannot exceed 127 ([10], p. 87). The issue of clamping in the schednler, in particular of p-usrpri, 
is addressed in [6] (see also Subsection 4.2, Remark 4). 3 The Operation of the Model In this section, 
we describe the operation of our decay-usage scheduling model. At the beginning of the first decay cycle 
T1, all jobs in classes 1,2,. ... kO get dedicated processors, and if P > fikO, the jobs of clasa kO 
+ 1 share the remaining processors. The priorities of classes 1, . . . . k. all increase at the same 
rate (R), so if P = tik,, this operation continues until either g~. (t) is equcd to qkO+l(t) or until 
T1 finishes, whichever occurs first. If ~ > fik,, this operation continues until one of four things happens: 
1. The priority of class kO becomes equal to the pri­ority of class kO + 1. Then, the jobs in classes 
 priority priority bs Qn ..............................  b~ Q. RT l----------- b~ 1  w(t;-,---­ . 
)1--------~bz I II I bl ,1 1 M] Mz -A!f3-M, 4 numbers of jobs (a) Figure 1: Examples of decay-usage scheduling 
1,2 ,. ... kO 1 continue having dedicated proces­ sors, and the jobs in classes kO and kO + 1 start 
sharing P Mko-l processors. 2. The priority of class kO + 1 becomes equal to the priority of class kO 
+ 2, In this case, the jobs in classes 1,2, , . . . kO continue having dedicated pro­cessors, and the 
jobs in classes ko + 1 and ko + 2 start sharing P ~ko processors. 3. The priorities of classes k. 
and k. + 1 become equal to the priority of class kO + 2 at the same time. Then, the jobs in classes 1,2, 
..., k. I con­tinue having dedicated processors, and the jobs in classes kO, kO + 1 and kO +2 start 
sharing P ~~O_l processors. 4. Before any of 1.-3. happens, T1 finishes,  Continuing in this way, it 
is clear that TI consists of at most K epochs T1 (1 ), TI (2), . . .. with service deliv­ered as follows. 
During Tl(l), iobs in classes 1, 2, ., ., ZI (/) have dedicated proce&#38;orsj jobs in classes ZI(1) 
+ 1:.. Tj jI (1) share P ~,, ~~, processors, and the remaining classes do not receive service, for some 
Z1(1), jl (1). In addition, recalling 1.-4. above, we have ii (t + 1) = i] (1) oril(l+l) = ii(l) 1, andjl(i+l) 
=jl(i)oryl(l+l) = A(q+l. We conclude that there are values Z] and jl such that iobs of classes 1. . . 
. . i? have dedicated ~rocessors durin~ the entire firs t dec~y cycle T1, jobs of classes 2] +1,..., 
j] receive processor time during TI but do not have dedicated processors during at least part of T], 
and jobs of classes jl + 1, . . . . h do not receive service during T1. Also, It easily follows that 
gk(t~) < g~+l (t~), k = 1,2, ..., K­ 1. As a consequence, the operation of the scheduling algorithm during 
Tz is analogous, though the starting values of the priorities, the lengths of corresponding epochs, and 
even the number of epochs may be differ­ent. Putting ~ = 7(D 1)/RD + 6, in general, we have fork =1, 
. ..j K. q~(t~)= q~(t; )/D+ BR.b~, n=l,2,..., (9) I -----_ ................. gz(t; ) - J bs \ ql (t;, 
~ bz :.::1 I m(lLl)l--------l I bl ,,t t, Ml-M2-M3-M4-M5-MS+ numbers of jobs (b) on (a) a uniprocessor 
and (b) a multiprocessor. and so by induction on n, qk(t~) < ~~+l(t~),k = 1, ..., h 1, n ~ 1, and therefore, 
the operation of the model in T. is analogous to that in T1. We now define t. as the index such that 
jobs of classes 1, . . . . In have dedicated processors during T., and those of class in + 1 do not (if 
there are no such classes, we set in = O), jn as the highest index such that jobs of class jn receive 
a positive amount of processor time during T., and Q. = as the highest priority attained at the end of 
a class that receives service. Obviously, in s in ~ ko. If tikO = P, then jn ~ ko, otherwise jn ,The 
operation of the model is illustrated in 1. The dotted lines indicate the priorities q~(t~_l qjm(t;) 
Tfi by ]~ and > ko. Figure ) (we take y = 1). In Figure 1 for P = 1, class 1 catches up with class 2 
at the end of Tn (l), classes 1 and 2 catch up with class 3 at the end of T~ (2), but T. ends before 
class 4 gets any service (in = O,j~ = 3). In Figure 2 for a multiprocessor, Mz < P < MS, and in = 2, 
j. = 5. Note that the area between the graphs of the priorities at the beginning and end of a decay cycle 
is RPT. 4 Analysis of the Model In this section we analyze our decay-usage model. First, we derive a 
set of equations and inequalities, indexed by the decay-cycle number, showing how to compute the shares 
obtained during T. and the priorities of all classes at the end of T~, given the priorities at the start 
of T~, for any n ~ 1. Because the latter only depend on the priorities at the end of Tn-l, this allows 
us to compute the shares in any decay cycle iteratively. Next we show that the decay-usage policy converges 
in the sense that the priorities of all classes at the end of Tn have a limit for n ~ co. It follows 
that the shares of the jobs of all classes in a decay cycle and the priorities at the start of a decay 
cycle also have a limit. This result enables us to suppress the decay-cycle index in the set of equations, 
and solve for the limits of the shares. 4.1 Formulation of the Solution We introduce the following notation 
for k = 1,. ... A_: Ck(?2) = (q~(t;) -g~(t~-, ))/R, n = 1,2,. ..,(10) ffk(n) = Zlk(t; ),rz= l,%.... (11) 
By (2) and (3), Ck(n) is the amount of CPU time ob­tained by a class-k job during Tn. From (4), (10) 
and (11), we have ~k(n) = ~k(~ 1) +8b~ +c~(n), n = 1,2, . . . . (12) D From the description of the 
scheduling policy it is clea~ that {cl(n), . . . . c~;(n), in, j~} is the solution of ck(~) = T,k=l, 
. . .. in. (13) q~(ti 1) + Rc~(n) = qtn+l(tt 1) + Rcin+l(n)) k=in+2, . ..jjn. (14) ck(~) = O,k= jn+ l,..., 
K, (15) 1< (16)~~k@) = l T) kel c,.+,(n) < T, (17) qen(t:-1) + RT < q,.+l(f:_l) + +Rcim+l(n), (18) c,m(~) 
> 0, (19) (20) qj?l(Ll) + ~cin(~) < 4%+1(L1 ).  4.2 Convergence In this subsection we sketch a proof 
that the decay­usage scheduling policy converges in the sense described in the introduction of Section 
4. The main step is to prove that the set of classes with dedicated processors is non-increasing and 
that the set of classes that receive CPU time is non-decreasing in successive decay cycles, from decay 
cycle T2 onwards (Proposition 1, the proof is omitted for lack of space). Proposition 1. i.+l s i. and 
jmql ~ .i~, for n = 2,3, .... Remark 1. The condition n ~ 2 in Proposition 1 is necessary; it is easy 
to construct a counter example for n = I. One can prove that for System V, 4.3BsD, and Mach, it does 
hold for n = 1. Corollary. There exist N > 0, zo ~ O, jO < K, such that in = zoand ~n = j. for all n 
~ N. In fact, from TN onwards, the scheduling algorithm operates as if the P-way multiprocessor were 
partitioned in fi,O uniprocessors, one for each job of classes 1, . . . . Z., and a (f fiCO)-way multiprocessor 
serving classes i. + 1,<. .,.io. Theorem, The decay-usage scheduling policy converges in the sense that 
the limits Ck = limn+m c~(n), u~ = limti+w u~(n), and qk = limn+m q,k(t~) e$ist. For k = 1,. ... io we 
have fork =io+l,..., jo we have (defining ; = ~;=,o+l ~kbk/(tijO -&#38;,O)): qk = = z+ (p -. JDT ,(21) 
~+~ 1 ( ) (ti,o -ML,)(D -1) D 1 Ck = ~qk bbk, Vk = (qk ybk)/&#38; (22) and fork= ~0+1, . . .. If we 
have qk = E bk, ck=o, Vk=z. ~+~ 1 () PROOF. Because of Proposition 1 and by the Corollary, from TN onwards, 
jobs of classes 1, ... , Z. always have dedicated processors, the jobs of classes i. + 1,. ... j. are 
jointly served by P M,. processors and have equal priorities at the end of Tn, for n z N, and classes 
10 + 1, . . . . K starve. For the first and last of these three groups of classes, qk can be computed 
from (5); for the second, qk can be computed from (2)-(4). For k < zo and k > Jo, the value of ck is 
obvious and ?Jkcan be found from Vk= (c$b, + c,)D (23) D 1 which is obtained by taking the limit for 
n ~ co in (12). Fork =m+l , . . . . .? o,Ck and Vk can be derived from qk= Ybki-Rvk and (23). Remark 
2. Forn~ N, qk(t~) = Q. for k = i. + 1,.. .,~o, so qk(t~) ql(t$) =9R(bk bf) forzo+l ~ k <1 ~ jo, andsoc~(n) 
= CkfOrn ~ N+l. Wesay that the model is in the steady state when the allocation to classes does not change 
anymore, i.e., once Zn = Z. and .I~ = JO. While the limiting priorities of the Theorem are never attained, 
in the steady state, the shares are equal to their limits. Remark 3. Assume that zo = O, JO = h , and 
let Q = qk, k = I,..., Ii-. Then by (21), in System V, Q = ~+ 100/1, in 4.3BSD, Q = (9/4+ l/2)~+ 25/1+ 
50, and in Mach, Q = ~/2+ R D/(D 1) % ~/2+ 10. Note that in Mach, the priority Q is invariant with respect 
to the load, to P, and to T. Remark 4. With the help of the Theorem, one can show that the bounds of 
P-CPUand p-usrprz are hardly ever reached in System V and Mach. However, in 4.3BsD, the bound of p-cpu 
is easily reached. One can show that then the jobs of classes with low base priorities get larger shares 
than predicted by the model.  4.3 Steady-State Shares Assuming the numbers of jobs J.fk, k = 1,.. , 
, K, to be fixed, we now show how to compute the steady-state shares sk = ck / PT, given the base priorities 
bk, and conversely, how to compute the base priorities bk (or rather the differences bk bl ), given 
the required shares sk (Or the ck). In order to find the c~, we have to solve for c1, . . . . CJ{, zo,JOthe 
set of equations and inequalities obtained by taking the limit for n ~ co in (13)-(20). Using (9) (tak­ing 
the limit for n -co) and (23) in (14), (18), and (2o), we find input: p, T, R, D,y,6, h , Mk, bk, k=l,2, 
. . ..K OUtPUt: Ck, k=l,2, . . ..~. iO, j0 51:2:= ko+ 1;if (fi~O = P) then j := k. else j:=ko+l; s2: 
for k:= 1to k. do ck:= T od; s3:fork:= j+lto~f dock:= Ood; s4: do s5:2:=2 1; j:=j 1 56: do s7:j:=j+l 
s8: Solve for ck, k = ;+1, . . . ,j, the following set of equations: ( Ck=C, +l ~(bk bi+l), k= 1+2, . 
. ..j (32) (33) until ( (j = K) or (CJ ~ /3(bj+l bj)) ) until ( (i = O)or (c~+l ~ T + ~(bt b,~l)) ) 
59: io=i; jcl=j Figure 2: Algorithm for the computation of the steady­state shares. Ck = T,lc=l,..., 
zo, (24) Ck = C,O+I ~(bk -b,o+l), k=to+2,..., jo, (25) O,k=jo+l,. ... K, (26) Ck = 1< E MkCh = PT, (27) 
k=l C,O+I < T, (28) C,o+l z T + p(bto ho+l), (29) Cjo > 0, (30) (31) CJO S P(hotl ho). It seems that 
there is no closed-form expression for the ck, but they can be computed by the algorithm in Figure 2. 
We start by assuming that the jobs in classes 1,, . . . ICO have dedicated processors throughout a decay 
cycle in the steady state (step s2), and that if fikO = P (~ko < P), classes k.+ 1,..., 1{ (k. + 2,..., 
K) starve (steps S1 and 53). Whenever step 58 is executed, z and ~ indicate the highest-numbered class 
that is assumed to have dedicated processors, and the highest-numbered class that is assumed to receive 
service, respectively. In step 58, we solve the linear system consisting of Equa­tions (24)-(27), which 
can be rewritten as in (32) and (33). prOpOSitbII 2. (a.) ~he Ck, k = 1, . . . . ~, of CZT730Z4dSCPU 
time and the class inchces zo and j. are correctly computed by the algorithm in Figure 2. (b) Fork =io+l,..., 
jo, zuehave PROOF. (a) Clearly, the solution produced by the algo­rithm satisfies Equations (24)-(27), 
and (29) and (31). One can show that ((c$+l < T) and (CJ > O)) is an invariant of the algorithm. (b) 
Solving the linear system in step 58 of the algorithm by substituting (32) in (33) yields (34). cOnVf3Hdy, 
one may Want to set the ck, the sk, or the totzd shares MkSk of classes, and compute a suit­able set 
of base priorities. Obviously, we can then as­sume T>cl >.. . >c~(>O(or l/P> sl >...> SK > O), so ~0= 
O,jO = ~, and ~f=l Mkck = PT (or ~~=1 Mksk = 1). Then, inverting (25), we find bk bl=(cl ck)/~, k=2,..,, 
I( , (35) which for P = 1 coincides with Eq. 20 of [6]. In [6], the behavior of the decay-usage policy 
(for a uniprocessor) is also analyzed in the zmderloaded case, characterized by ~ Mkck < PT, and the 
overloaded case, defined by ~Mkck > PT, with Ck, k = 1,. . ., h , the required amounts of CPU time in 
a decay cycle. It is shown that in either case, s~ sk = sj s1, k=l ,..., K, where the sk, s; denote the 
required and the obtained steady-stat e shares, respectively, provided that no star­vation occurs in 
the overloaded case, i.e., that s!{ > 0. Measurements in [6] show that the policy indeed be­haves in 
this way in practice. This property of equal differences between required and obtained shares clearly 
carries over to multiprocessors in those cases when no class has dedicated processors and no class starves: 
the underloaded and overloaded cases correspond to length­ening or shortening the decay cycle, which 
in general amounts to lengthening or shortening the last epoch, in which all jobs of all classes evenly 
share all proces­sors. So the decay-usage scheduling policy is fair in the sense that an excess or deficit 
of capacity is spread equally over all jobs. It would perhaps be a more desir­able, and fairer, policy 
if it enjoyed the property that i/sk=si/Sl, k=~, . . .. ~{. 5 Control Considerations In this section 
we deal with the level of control that can be exercised by the decay-usage scheduling policy over the 
share ratios. 5,1 Uniprocessors versus Multiprocessors Consider the share ratios given by (34). An important 
thing to note is that it is immaterial whether there are P processors of equal speed (each delivering 
T clock ticks per decay cycle), or one processor which is P times as fast (delivering PT clock ticks 
per decay cycle): in either case, an amount PT of processor capacity is delivered in one decay cycle, 
and the steady-state shares are equal. 40 20 I I I I I I 35 4.3BSD 30 15 - System V 25 Mach ratio of 
shares ~. 10 - 15 10 5 - 5 0 0 I I I I I I I I 1357 91113151719246 8 10 12 14 16 18 20 base priority 
of class 2 load (a) (b) Figure 3: Ratio of shares (s1/s2) versus (a) the base priority of class z (~2) 
and (b) the load (1) for different parameterizations of the decay-usage scheduler (K = 2,bl = O; in (a), 
Ml = 5P, M2 = P; in (b), Ml = M, = MP,l = 2M, b2 = 10). 5.2 Scheduler Parameterizations and the Range 
of Share Ratios In this subsection we will trace the impact of the values of the parameters in the scheduler 
on the share ratios given by (34). Let us first assume that R and D are constants. Then, as far as the 
steady-state shares are concerned, P, T, R, D, y, 6 are not independent: only the value of /3/PT is relevant. 
The larger the value of /3/PT, the larger the range of possible share ratios. In fact, ~ = 7(D 1)/R~ 
+ 5 can take any positive value, and there is no loss of control by taking D = 2, y = 1,6 = O (these 
are the values in System V, which has R = 0.5, and so ,B = 1). So, assuming P and T to be fixed, the 
scheduler could be parameterized by one instead of four parameters. Also, in order to have the same range 
for the ratios of steady-state shares, the range of base priorities has to be proportional to both the 
number of processors P and the processor speed T. In multi­processors, one may have the opportunity to 
partition logically the system into a set of multiprocessors with a smaller number of processors each, 
for instance, in order to reduce contention for the central run queue or so as to assign parts of the 
machine to different applications. Here we see that in a partitioned multiprocessor, one can exercise 
more control over the steady-state shares when the ranges of base priorities are the same. In 4.3BSD 
and Mach, /3 depends on the other pa­rameters. In 4.3BSD, ~ = (21 + 9)/(21 + 1), so /3 varies from 11/3 
when 1 = 1 to 1 when the load is very high. In Mach, R = MIcR /PT, D = 1.6, Y = 0.5, and 8 = O, so ,b 
= 3PT/16MA-R E 5/1, and by (34), invariant with respect to P and T, so the range of base priorities does 
not have to be adjusted for multiproces­sors or for different speeds. Note that in Mach, the ratios sk 
/sl only depend on the ratios kfk/kfl (and the base priorities). For the case of two classes, the levels 
of control for the three systems are depicted in Figure 3 for varying base priority of class 2 and for 
varying load. In either case, 4.3BSD has the highest level of control, and Mach the lowest. 6 Validation: 
Simulation and Measurements In order validate our model, we have done measure­ments on UNIX and Mach 
systems. In addition, in or­der to exclude any effects of implementation details of such systems so that 
we might isolate the influence of discrete time versus continuous time, we have built a simple simulator 
which mimics the UNIX scheduler. We will compare the ratios of shares Sk/Sl as predicted by our model, 
as obtained from the simulations, and as measured in actual systems. For the model, the ratios of shares 
are computed by means of a program imple­menting the algorithm in Figure 2. Measurements were taken on 
1-and 4-processor Sun Sparcstations running SunOS 4.1.3, on a 4-processor Sequent running DYNIX version 
V3. O.17.9, all based on 4.3BSD , and on a Se­quent running Mach 3.0 with 1,4,8 or 16 CPUS enabled. 
We found that the 4-processor Sun uses 1 = fi~< (i.e., not divided by P = 4); it can easily be shown 
with (22) that then the bound 255 of p.cpu for class 1 is reached for any set of jobs. As a consequence, 
measurements on the 4-processor Sun are worthless for our purposes. Also in DYNIX did we find a deviation 
from the scheduling policy, and the way Mach effectuates decay deviates somewhat from the description 
in Section 2. We will discuss the details and the consequences of these differences between the model 
and these systems below. Because ~ = 0.5 in Mach, we only consider even values of base priorities in 
that system. For the measurements, the jobs ran for (at least) 20 minutes in each experiment, and the 
ratios Sk /sl are computed as the quotient of the average amounts of CPU time obtained during the last 
10 minutes by jobs of classes k and 1, respectively. The reason for having the jobs first run 10 minutes 
before actually measuring is to let the system build up the load 1. In the simulations, the simulated 
time in each ex­periment was also 20 minutes, and the ratios of shares were computed in the same way 
as for the measure­ments. In coarse-grained (cg) simulations, we used the 10 IIIIII1I [ model System 
V ~. . . simulation System V * 8-4 * model 4.3BSD simulation 4.3BSD 6-3 ratio of shares 4-2 2 -o 
~~.~1 13579 . *..*.*..*.* base priority 11 13 of class 2 15 17 *, 19 I ./1 ;W24681012141618 base priority 
 of class 2 (a) (b) Figure 3P; in 4: Ratio (b), P= of shares (s1/s2) versus the base priority 1,4,8,16, 
MI =5 P). of class 2 (bz) (K= 2, MZ = P, bl = O; in (a), P = 1,4, JfI = values of y, $ given in Section 
2, put T = 100, and let the number of clock ticks per quantum be 1 (System V) or 10 (4.3BsD and Mach). 
It turned out that the results of such simulations can deviate greatly from the model output. Therefore, 
in some cases we also ran jine-gmined (fg) simulations, in which T = 1000, -y, 8, and for Mach R , were 
replaced by 10-y, 108, aud 10R , respectively, and the number of clock ticks per quan­tum was always 
10; by (34), this does not change the ratios of shares, but cent inuous time is approxirnat ed more closely. 
Simulations were coarse-grained, unless otherwise stated. In our model, the ratios of steady-state shares 
$k/s~ are realized during euer~ single decay cycle. Clearly, if a steady state is reached in a simulation, 
in the sense that at the start of successive decay cycles all priorities have the same values and the 
jobs have the same order in the run queues, there is only a very restricted set of possible share ratios, 
and we can expect large deviations from the modei. In both the simulations and the measurements, there 
was lit tle variability y in the share ratios, so we simply always report the results of one simulation 
run or one measurement. Also the amounts of CPU time obtained by single jobs within a class in one experiment 
did not vary much. 6.1 The Model versus Simulation We compare the model output with simulation results 
for three representative sets of experiments. I) Two classes, fixed numbers of jobs, increasing base 
priority of ciass 2 (Figure 4). It turns out that for all three systems, the simulation output is identical 
for the shown values of P (as is of course the model output). The model output and the simulation results 
match quite well. Furthermore, 4.3BSD discriminates much more bet ween jobs with the same difference 
in base pri­orities than System V does, as was to be expected (cf. Section 5.2). For Mach, the (cg) simulations 
deviated somewhat from the model, so we also ran fg simulations, which match the model better. II) Two 
classes, jixed base prioritzesj increasing number Of @s with the lowest base priority (Figure 5). The 
deviation between the model and the (cg) simulations for some values of Ml/P is considerable, and is 
caused by the discrete values in the scheduler, as we found from simulation traces. For instance, for 
System V, when P = l,M1 = 14, M2 = 1, the model gives sl/s2 = 6.63 and the simulation gives S1/s2 = 3.5o, 
and it turns out that in the simulation a steady state is reached in the sense described above, and that 
the class-1 jobs obtain 7 clock ticks in a decay cycle, and the class-2 job gets 2 ticks (one of which 
is the last one in the decay cycle). For 4.3BsD, it is even more difficult to achieve the ratios predicted 
by the model because there are only 10 quanta per second, so it is not strange that for M1/P = 10, the 
(cg) simulation is far from the model. Note that in the fg simulations, which approximate the model much 
more closely, in 4.3BSD the number of quanta per second is 100 instead of 10, but for System V it remains 
100; in the latter system, the better approximation of the model is solely due to a finer distinction 
in priorities. For Mach, the fg simulations are close to the model, but the cg simulations are not (when 
no point is de­picted for cg simulations, starvation of class 2 occurs). Traces of the cg simulations 
showed that for Ml/P = 6, 7,8, 9, the class-2 jobs get almost exactly the same amount of CPU time: for 
r = kfl/M2 = 7, 8, 9, the share ratios are virtually equal to 6/r times the ratio for M1 /M2 = 6. What 
happens is that for each of these values of Ml /P, the following sequence of events during a decay cycle 
occurs on the average the same number of times: the class-1 jobs make their way to the queue with the 
class-2 jobs by receiving a quantum, and then the class-2 jobs get a quantum before the decay cycle finishes. 
For Ml/P ~ 10, in every decay cycle, each of the class-1 jobs gets a quantum before it reaches the queue 
with the class-2 jobs, and so the latter starve. III) Three classes. Table 1 shows the ratios of shares 
and the limiting pri­ority Q at the end of a decay cycle (in the simulations taken as the average priority 
of all jobs at the end of the last decay cycle in the simulation, and including 12II I I I I I I II 
I model System V 10 -simulation System V * fg simulation System V + 8 -model 4.3BSD simulation 4.3BSD 
 ratio of 6 shares 4­ 2 - .,, o I I I I I I I I I I I I I 1 23456 789101112131415 number of class-1 
jobs per processor (a) Fimre 5: Ratio of shares versus (SI /s2 ) versus the number O;~n (a), P= 1,4, 
bz = 6; in (b); ~= 1,4,8,16, bz = 18). PUSER = 50) for a set of experiments with three classes on a uniprocessor 
with 4.3BSD parameters. The simulations with four classes on a four-way multiproces­sor that we carried 
out, produced results with an equally small error margin, except when the share ratios became very large 
(higher than 20).  6.2 Model Adaptation for Mach In the next subsection we will find that especially 
when there are many jobs (per processor), the Mach measure­ments may deviate considerably from the model 
output. We now argue that this is probably due to a combination of the way decay is effectuated in Mach, 
which is some­what different from that described in Section 2, and the large quantum size. Mach maintains 
a global l-second counter C, and for each process p, a local decay counter CP. Whenever at a clock interrupt 
at a processor, the running process p finds CP # C, it performs decay with D c cp let~s ~all this local 
decay) and sets decay factor ( CP equal to C. At the end of every 2-second interval, decay is performed 
for every process in the same way (call this global decay, and call such a 2-second inter­val an extended 
decay cycle). The effect is that during the first half of an extended decay cycle, local decay does not 
occur, and that when a job is selected for ex­ecution for the first time during the second half of an 
extended decay cycle, it is chosen based on its priority to which no (local or global) decay has been 
applied yet during the current extended decay cycle. In addition, if a job gets an amount c of CPU time 
during either half of an extended decay cycle and nothing during the other half (which happens frequently 
when there are many jobs), then, denoting the decayed CPU usage of the job at the end of an extended 
decay cycle by v, we have w = v/D2 + c, so the job experiences a decay factor of D2 and a decay-cycle 
length of 2T. We conclude that Mach is much more closely mod­eled with a decay factor D = 2.56, equal 
to the square of the original one, and with a decay cycle of T = 200 clock ticks. However, as the increment 
in priority due to one 1. 7IIIIII[III II * x 6-x +­x 5­ * 4­ simulation Mach 1­ fg simulation Mach 
x II IIIIIIIIIII!I o 12345678910111213 number of class-1 jobs per processor (b) of class-1 jobs per 
processor (Ml/l ) (~= 2, M2 = P, h = .,. . second of CPU time is built into the system, it remains the 
same, and we still have R = lR /100. We refer to the model of Mach with these values for D, T, and R 
as the adapted model, the share ratios for which can still be found with (34). We included the way in 
which Mach effectuates decay in our simulator, and found that such coarse-grainea! simulations proved 
to be rather close to the adapted mode!, while such fine-grained simulations were close to the original 
model. Two other possible causes for deviations between the model and the measurements are that in our 
experi­ments (1) the reported load is usually higher than that caused by our jobs, and (2) that the percentage 
of CPU time obtained by our jobs is about 75-85%, 92-95%, 95~0, and 97~0 for P = 1,4,8, 16, respectively. 
In one set of experiments below, we also depict the impact of these two phenomena (labeled adapted modei 
(measured load and percentage CPU)), which turns out to be small.   6.3 Measurements We compare measurements 
of 4.3BSD-based UNIX sys­tems and Mach systems with model output for two rep­resentative sets of experiments. 
For System V, mea­surements of uniprocessors were reported in [5, 6]. It can be shown with (21) and (22) 
that in all exper­iments reported below, unless explicitly stated other­wise, the model is valid, i.e., 
that the bounds of p.cpu and p_usrprz in the scheduler are not reached. I) Two classes, fixed base priorttzes, 
increasing number of jobs with the lowest base pnoraty (Fzgures 6 and 7). On the 4.3 BSD-based systems, 
for Ml = P, the model is invalid, and indeed, measurement and model differ, at least for P = 1. For the 
Sequent under DYNIX, the measurements show larger share ratios than the model. Traces of this system 
revealed that jobs may get longer time slices than 100 ms, even up to 600 ms, while their priorities 
do not justify this. Because high­priority processes (i.e., with low base priorities) are el­igible for 
running earlier during a decay cycle, this fa­vors the lower-numbered classes. We have observed the Table 
1: Ratios of shares and the limiting priority Ml Mz M3 S1 82 model simulation 1 1 1 1.58 1.58 1 1 2 
1.58 1.53 1 2 1 1.68 1.67 1 2 2 1.67 1.68 2 1 1 1.82 1.85 2 1 2 1.78 1.78 2 2 1 1.92 1.92 2 2 2 1.87 
1.87 same phenomenon -higher share ratios -in some mea­surements of the same Sequent/DYNIX system with 
one CPU enabled, and we conclude that the deviation is not a multiprocessor issue. On Mach, the match 
between the adapted model and the measurements is only reasonable. For this set of experiments, the impact 
of adapting the model is considerable (compare Figures 5.(b) and 7). II) Three classes (Figures 8 and 
9). On the Sun, in Figure 8.(b), the model is invalid for Ml <2. 7 Conclusions We have analyzed a decay-usage 
scheduling policy for multiprocessors, modeled after different variations of UNIX and Mach. Our main 
results are the convergence of the policy and the relation between the base priorities and the steady-state 
shares. Our simulations validate our analysis, but also show that discrete time may be a source of considerable 
deviation from the model. The measurements of the 4.3BsD uniprocessor and of Mach match the model remarkably 
well, those of the 4.3BSD multiprocessor do less so, which is probably caused by an implementation detail 
we do not know of. We have shown t hat share scheduling can be achieved in UNIX, but unfortunately, in 
the decay-usage schedul­ing policy we have analyzed, the shares depend on the numbers of jobs in the 
classes in an intricate way. We have seen that in order to have the same range of share ratios in a system 
that does not employ the Mach load­factor technique, the range of base priorities should be proportional 
to both the number and the speed of the processors, or in other words, the leverage of decay­usage scheduling 
with the same range of base priorities is much larger in small or slow multiprocessors than in large 
or fast ones. Also, of the systems considered, 4.3BSD has the highest level of control over the share 
ratios, and Mach the lowest. Finally, a scheduler with a constant decay factor and a constant increase 
of pri­ority due to a clock tick of obtained CPU time, only needs one parameter instead of four, at least 
as far as the steady-state behavior is concerned. 8 Acknowledgments The support of the High Performance 
Computing De­partment, managed by W.G. Pope, of the IBM T.J. Watson Research Center in Yorktown Heights, 
NY, USA, where part of the research reported on in this paper was S1 53 Q model simulation model simulation 
3.75 3.71 142.1 140.7 3.78 3.83 154.1 152.5 5.25 5.00 144.5 142.8 5.11 5.00 156.3 154.6 10.07 9.25 134.9 
134.0 7.98 8.01 147.7 145.8 24.11 22.59 139.2 137.2 14.66 14.04 151.4 149.7 Q (4.3BSD, P = 1,IS-= 3, 
bl = O,bz = 9, ba = 18) performed, and of IBM The Netherlands, is gratefully acknowledged. In addition, 
the author owes much to stimulating discussions with J.L. Hellerstein of the IBM Research Division. Furthermore, 
the author thanks the OSF Research Institute in Grenoble, France, for the op­portunity to perform measurements 
on their multipro­cessor Mach system, and Andrei Danes and Philippe Bernadat of OSF for their help. References 
<RefA>[1] M.J. Bach, The Design of the UNIX Operating Sys­tem, Prentice-Hall, 1986. [2] D.L. Black, Scheduling 
and Resource Management Techniques for Multiprocessors, Report CMU-CS­90-152, Carnegie Mellon University, 
1990, [3] R.B. Essick, An Event-Based Fair Share Sched­uler, USENIX, Winter, 147-161, 1990. [4] G. Fayolle, 
I. Mitrani, and R. Iasnogorodski, Shar­ing a Processor among Many Job Classes, J. of the ACM, Vol. 27, 
519-532, 1980. [5] J.L. Hellerstein, Control Considerations for CPU Scheduling in UNIX Systems, USENIX, 
Winter, 359-374, 1992. [6] J.L. Hellerstein, Achieving Service Rate Objec­tives With Decay-Usage Scheduling, 
IEEE Trans. on Softw. Eng., Vol. 19, 813-825, 1993. [7] G.J. Henry, The Fair Share Scheduler, AT&#38; 
T Bell Lab. Techn. Journai, Vol. 63, 1845-1857, 1984. [8] J. Kay and P. Lauder, A Fair Share Scheduler, 
Comm. of the ACM, Vol. 31, 44-55, 1988. [9] L. Kleinrock, Time-Shared Systems: A Theoret­ical Treatment 
, J. of the ACM, Vol. 14, 242-261, 1967. [10] S.J. Leffler, M.K. McKusick, M.J. Karels, and J.S. Quarterman, 
The Design and Implementation of the 4 .3BSD UNIX Operating System, Addison-Wesley, 1989. [11] C.A. 
Waldspurger and W.E. Weihl, Lottery Scheduling: Flexible Proportional-Share Resource Management, Proc. 
of the Ftrst USENIX Sympo­ sium on Operating Systems Design and Implemen­tation (OSDI), 1-11, 1994.</RefA> 
10 IIIIIIIIIII 12IIII I I I I I I 10 P=4,M2=4 x 8 ti measurement measurement 8 -model model 6 simulation 
X ratio of fg simulation -t­ 6­ shares 4 4-  2 * 2 -* * ()~ ()~ 1234567891011 1 2 3 45 6 7 8910 
number of class-1 jobs number of class-1 jobs per processor Figure 6: Ratio of shares (s1/s2 ) versus 
the number of class-.1 jobs per processor (M1/P) (4.3BSD, K = 2, bl = O,bQ = 6). 5 IIIIIIIIIIIII  5r 
 7 4- on 00 n Q.e ;Cl. b 3 -e. ~ ratio of shares P=l 1­ fw2=l I I I I II III I I II I()~ 01 12345678910111213 
12345678910111213 4-4* t1 e nnad * nn gQ n 3 -**. . ** 3- ratio of shares 2-2 P=8 P=16 1-1 hfz=4 
&#38;f2=4 1 ()~ IIII[tIIII 0 2345678910111213 45678910111213 ratio of numbers of jobs ratio of numbers 
of jobs measurement * adapted model adapted model (measured load and percentage CPU) Figure 7: Ratio 
of shares (s1/s2) versus the ratio of the numbers of jobs of classes 1 and 2 (Ml /Mz) (Mach, K=2, bl=O, 
b2 =18). 84  10 iII 10Ii I I I 1. IIIIIIIIIII Ml=5j Mz=2, Ma=l, bz=2 M2=2, Ma=l, bz=3, Z)a=5 * k 8 -measurement, 
SI /s2 8 -measurement, s] /s2 . model, sl/sz model, sl/s2 measurement, SI /s3 * measurement, S1 /S3 
* *.. 6 -model, sI/ss 6 -model, sl/s3 ratios of * shares 4-. 4­ ,* . * ,* * ,.,* ,*. ,.* ..* ,.*. 2 . 
.. * 2 -* .*. .* * : * ... .. ..0. I I IIIIfIII I IIII IIIII oo 3456789 12345678910111213 (a) base 
priority of class 3 (b) number of class-1 jobs Figure 8: Ratios of shares (s1/s~, k = 2,3) versus (a) 
the base priority of class 3 (bs) and (b) the number of class-1 jobs (MI) (4.3BSD, P = l,K = 3,b1 = O). 
4 IIIIIII4IIIII I I P=4 P=l 3.5 -3.5­M1=5 Ml =10 3 -*.-3-*. M2=2 . . Mz=4 .. * ,.. 2.5 - ., 2.5 -Ms=2 
*.. MA=l ,. *,. , *.. ratios of ~ 2­ *,., .* .. shares ,, *,. .* .. 1.5 -.*. . 1.5 -.,* . ...* . 
.*. 1­1­ 0.5- 0.5 IIIIIII 1IIIIII o­o 6 8 1012141618 6 8 1012141618 4 IIIIIII 4IIIIIII P=16P=8 3.5 -3.5 
- M, =20 M, =30 3­k. 3 - *. M2 =12 .. M2=8 .. *,,. 2.5 ­ 2.5 ­ M3=4 M3=6 ,. * ratios of ~ .*.  .*. 
2­shares .* . ,. .* .* .. ,, .*. *,... 1.5 -*. 1.5 ­... .... ..* . 1­1 0,5- 0.5- IIIII i II[III II 
o 6 8 1012141618 6 8 1012141618 base priority of class 3 base priority of class 3 0 measurement, .91/ss 
. measurement, S1/s3 * adapted model, SI /s2 adapted model, sl/sa . . . Figure 9: Ratios of shares (sl/s~, 
k = 2,3) versus the base priority of class 3 (ha) (Mach, k-= 3, bl = O,bz = 4).  
			
