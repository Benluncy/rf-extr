
 ANALYSIS OF MULTITERM QUERIES IN A DYNAMIC SIGNATURE FILE ORGANIZATION Deniz AKTUG Fazli CAN* Department 
of Systems Analysis Miami University Oxford, OH 45056 Abstract Our analysis combines the concerns of 
signature extraction and signature file organization which have usualIy been treated as separate issues, 
We at so relax the uniform frequency and single term query assumptions and provide a comprehensive analysis 
for multiterm query environments where terms can be classified based on their query and database occurrence 
frequencies. The performance of three superimposed signature generation schemes is explored as they are 
applied to one dynamic signature file organization based on linear hashing: Linear Hashing with Superimposed 
Signatures (LHSS). First scheme (SM) allows all terms set the same number of bits regardless of their 
discriminatory power whereas the second and third methods (MMS and MMM) emphasize the terms with high 
query and low database occurrence frequencies Of these three schemes, only MMM takes the probability 
distribution of the number of query terms into account in finding the optimal mapping strategy. Derivation 
of performance evaluation formulas is provided together with the results of various experimental settings, 
Suggestions as to how to implement the given techniques in real life cases are also provided. Results 
indicate that MMM outperforms the other methods as the gap between the discriminatory power of the terms 
gets larger. The absolute value of the savings provided by MMM reach a maximum for the high query weight 
case, However, the extra savings decline sharply for high weight and moderately for the low weight queries 
with the increase in database size. 1. INTRODUCTION Information retrieval systems (IRSS) serve the purpose 
of finding the data items that are relevant to the submitted user queries. A multimedia IRS can consist 
of formatted stored objects as well as the unformatted ones like text, voice or image. Full text/object 
scanning, inverted indexes, clustering and signature files are some of the IRS techniques [CAN90, CHR84, 
FAL85, SAL89, TIB91, ZEZ91]. The concern of our study is signature files which are widely used in formatted 
and unformatted databases for mtdtiattribute query processing. Throughout the paper, data items stored 
in the database (formatted, unformatted or combined) will be refereed to as . -- --- -- ____________ 
* address of correspondence: fc74sanf@miamiu. bitnet Permission to COPY without fee all or part of this 
material is granted provided that the copies are not made or distributed for direct commercial advantage, 
the ACM copyright notice and the title of the publication and its date appear, and notice is given that 
copying is by permission of the Association for Computing Machinery. To copy otherwiee, or to republieh, 
requires a fee and/or specific permission. ACM-SIGIR 93-6/93 /Pittsburgh, PA, USA a 1993 ACM 0-89791 
.605.0 /93/0006 /0096 . ..$1 .50 records or objects. A signature file is created when contents of the 
data items are represented as bit strings which are stored in a separate file ~HA88]. The main virtue 
of a sigmture file is to act as a filter to eliminate a large number of non-qualifying records and reduce 
the number of disk accesses required to process a particular query. The size of a signature file is typically 
ten to twenty percent of the actual size of the source database from which signatures are extracted [CHR84, 
ZEZ91]. Hence signature files provide retrieval efficiency during query processing without generating 
much of a storage overhead. Besides, they are flexible enough to efficient y handle object insertions 
and deletions which happen frequently in dynamic environments [ZEZ91]. This study concerns the comparison 
of three signature generation schemes as they are applied to a dynamic signature file organization structure 
known as Linear Hashing with Superimposed Signatures (LHSS) or Quick Filter [ZEZ88, ZEZ91]. The first 
scheme treats all terms in an identical way, neglecting the differences in their occurrence and query 
frequencies, whereas the second and third schemes actually make use of such differences in generating 
the term signatures. The second scheme uses an optimaf signature generation strategy which is based on 
the assumption that ord y single term queries are submitted whereas the third scheme considers multiterm 
queries as well ~AL85, FAL87, FAL88]. The main contribution of the paper is the derivation of the performance 
evaluation formulas for the selected file organization for each of the above schemes in an environment 
where both single and multitetm queries are submitted. We not only relax the uniform frequency and single 
term query assumptions, but also present the application of the suggested schemes in the dynamic file 
structure LHSS together with an anatyiicat anatysis for queries with any number of terms. Furthermore, 
with some modification, our findings can be used to incorporate the performance evaluation of other partitioning 
methods (for examples see [LEE89]) with various signature generation schemes in multitertn query environments. 
Section 2 explains LHSS and presents the performance evaluation forrnnlas. Section 3 defines the three 
signature generation schemes. The derivation of the performance evaluation formulas for these schemes 
is given in Section 4, whereas their application to our specific test cases is explained in Section 5. 
Section 6 presents the results of the experimental anal ysis. The guidelines for the application of our 
findings to real life cases are provided in Section 7 and the conclusion is presented in Section 8. 2. 
LINEAR HASHING WITH SUPERIMPOSED SIGNATURES (LHSS) Linear hashing is an efficient way to organize partitioned 
dynamic files [LIT80]. A derived method, which is originally introduced by Zeztda and his coworkers, 
is linear hashing with superimposed signatures [ZEZ88]. 2.1The Method LHSS provides a method for mapping 
signatures to storage pages and processing the queries to find qualifying signatures. Other methods on 
signature file partitioning can be found in ~EE89]. The primary component of LHSS is a split function 
which converts the key of each signature into an integer in the address space {O, 1, . . . ,n-1} where 
2h-1 < n < 2h is satisfied for some integer h. The hashing function is defined as follows [ZEZ88, ZEZ91]. 
h-1 h-1 zbF-r2rifzbF-r2r<n g(si, h, n)= (1) h-2 bF-r2r otherwise z (where bi is the value of the i h 
binary digit of the object signature, F is the signature size, h is the hashing level, n is the number 
of addressable (primary) pages and s; is the object signature i. (For easy reference the definition of 
the important symbols of this section is provided in Table I.) For the initial condition, h=O, n=l, and 
g(si, O, 1) is defined as O. In simple terms, the hashing function, g, uses the last h or (h-l) bits 
of a signature to determine the number of the addressable page where signature si is to be stored. If 
the storage limit of a primary page is exceeded, an overflow page is created, linked to the primary page 
and the last signature that has caused the overflow is placed in the overflow page and, a split is initiated, 
i.e., a new primary page is created. A split pointer, SP (with an initial value of O), keeps track of 
the next primary page to be split. Whenever a split takes place, all signatures on the page pointed to 
by SP, together with those in the associated overflow page(s) are rehashed. The nature of the hashing 
function guarantees that the rehashed signatures either remain in the same page or are transferred to 
the page that has just been created. The hashing level is increased by one just before page zero is split, 
and following each split process the new value of SP is computed as SP = ((SP + 1) mod 2h-1). Note that 
at a given time in the signature file it is possible to have pages which are hashed at levels h and (h-1). 
Note also that linear hashing is space efficient and does not lead to many overflows ~IT80]. During query 
processing a page qualifies if all bit positions that are set in the query signature are also set in 
the page sigmture. For simplicity, if we assume that n = 2h and if there is a query signature with k 
1s in its h-bit suffix, then it is necessary to access 2h-k primary pages (and the associated overflow 
pages). More number of 1s in the last h-bit suffix of a query makes the query processing faster. Note 
that even if a signature in the selected page seems to qualify the query the associated data object might 
not contain all query terms. Hence a false drop resolution is required using the original query before 
the qualifying objects are returned to the user. Table I. Definition of Important Symbols for Section 
2 bi : value of the ith binary digit of the term signature h : hashing level k : no. of bits set to 
1 in thefinal query signature m : no.of bitsaterm setsto 1(when eachterm setsthe samenumber of bits) 
 n : no. of addressable pages s, : ith objeet signatrne  EXPH(W(Q),h) : expected number of bita set 
in the h -bit suffix of a query signature whose weight is W(Q) F : size of a signature in bits N(n, 
h, W(Q)) : no. of pages that does not need to be accessed PQ) : probability that j bits are set in the 
h-bit suffix of the quety F(W(Q), h) : probability of access savings R(h) : no. of pages hashed at level 
h W(Q) : query weight, i.e., the no. of 1s in the query si mature 2.2 Performance Evaluation It has 
been shown that the number of page savings can be computed as a function of the number of addressable 
pages (n), the hashing level (h), and the number of 1s in query signature, i.e., the query weight (W(Q)) 
provided that the signature size is kept fixed at F [ZEZ91]. Let EXPH(W(Q), h) be the expected number 
of bits set in the h-bit suffix of the query signature. min h W(Q) EXPH(W (QM = j P(j) (2) xJ= where 
P(j) is the probability that j bits are set in the h-bit suffix of the query and can be written as follows. 
( F-h \ (h) (3) Uw3J) Next probability of access savings, P(W(Q), h), can be defined as the proportion 
of the number of pages that do not need to be accessed (while processing a particulm query) to the total 
number of addressable pages. Hence P(W(Q),h) = 1-~ (4) n where npa is the number of pages accessed, n 
is the number of addressable pages in the signature file. Note that only 2h-EXpMW(Q). h) number of pages 
need to be accessed. So when 2h= n n (5) pa = JXPH(W(Q ,h) and ~ , ~EXPH(W(Q , h) P(W(Q,h) = 1足n 1 =1- 
(6) ~EXPH(W(Q ,h) The final step is to find an expression for the total number page savings, N(n, h, 
W(Q)). Let R(h) denote the number of addressable pages hashed with level h. Then R(h) = 2(n-2h1) = 2n-2h 
and R(h-1) = 2h -1 Finally, the total number of page savings, N(n, h, W(Q)), is defined as the number 
of pages that need not be accessed for a given query and can be expressed as follows. N(n3,W(Q)) = R(h) 
P(W(Q),h) + R(h-1) P(W(Q)&#38;l) (7) 2.3 Advantages of LHSS LHSS is a dynamic file organization which 
is proved to provide considerable reduction in search space and high retrieval efficiency. Its performance 
improves with the query weight which makes it a desirable choice for the multimedia applications where 
queries with high weights are very common [ZEZ91]. Unlike many other file organizations, LHS S provides 
even higher savings as the database size increases [ZEZ88]. Hence for many real life applications where 
large databases are used, LHSS might efficiently be used. The benefits of LHSS can be increased when 
it is coupled with a signature generation scheme that takes term occurrence and query frequencies into 
account. Using LHSS not only the retrieval efficiency will be improved but also the relevancy-efficiency 
balance will be established [AKT93]. 3. LHSS BASED ON TERM CHARACTERISTICS FOR SINGLE AND MULTITERM QUERIES 
3.1 Introduction and Discussion of the Test Cases Faloutsos has suggested grouping all temls in the database 
into ns number of disjoint sets (S1, S 2, . . , S ~s) based on the frequency with which they are specified 
in the queries ~AL85]. All terms in a given Si (1< i ~ ns) set the same number of bits in generating 
their signatures and the optimal number of bits set by the terms in set i (s, ), m ~, is computed by 
taking the query and occurrence frequency of the terms into account. The approach is based on the observation 
that the terms with lower database occurrence frequency are specified more frequently in the queries. 
Such terms are said to have high discriminatory power in the sense that they efficient y detetmine those 
documents that are most relevant to the query. Since terms with high discriminatory power are more important, 
they should be given the privilege to set relatively more number of bits in their associated term signatures. 
Unlike some other studies ~EN92, FAL87b], this approach can be used to account for multiterm queries 
and eliminates the need for a lookup table. The purpose is to minimize the false drop probability by 
using the differences between the term discriminatory power values, The query frequency is represented 
by ql where qi is the probability that a query term is from Si, and (ql + qz + + qn~) = 1. The occurrence 
frequency, on the other hand, is reflected in the Di values where D i is the average number of terms 
in a record that are from Si, and D= (D1 + D2+ . . . Dns), and D is the average number of terms in a 
record. (For quick reference, the definition of the symbols for Sections 3 to 6 are provided in Table 
II.) able II. Definition of the Important Symbols for Sections 3-6 bi : no, of terms from Si m a query 
~. no. of bit set by the jth query term D : expected no of distinct terms in a record Di : expcted no. 
of distinct terms of set S i in a record h : hashing level k : possible values for the additional no. 
of bits set to 1 after the first stage m : no, of bits a term sets to 1 (when each term sets the  same 
number of bits ) : no. of bits set by each term m the i ti term setmi n : node number in Figure 1 (O 
s ns t) : number of disjoint seta % : no. of terms in a query (nqt St) nqt qi : probability that query 
term is from S i, given a term signature t : maximum no. of terms in a query EXPH: : expected no. of 
bm set in the h suffix of the query for a specifically identified outcome EXTSAV extra percent savings 
provided by MMM over SM or MMS : size of as] gnature in bm : casein which queries with high weights are 
frequent : case nr which queries with low weights are frequent : expected no. of bits set in the h suffix 
of the query rmnsidering all possible query outcomes : probability that exactly k terms will be specified 
from Si : probability that j terms are specified in a query : percentage of the addressable pages that 
do not have to be accessed : set i of the terms with similar discriminatory power : case in which the 
probabdlty distribution of the no, of query terms is uniform, i.e., the Pi (k) values are equal : weight 
of a query signature afters term signatures are superimposed Table III shows the formulas for the optimal 
assignment strategies for thrtx signature generation schemes which are based on this approach. Single 
m (SM) case refers to the method in which all terms are assumed to have the same occurrence and query 
frequencies and hence set the same number of bits regardless of their discriminatory power. This is a 
crude way to generate term signatures but the results can serve as a reference point against which the 
performance of other more sophisticated signature creation schemes can be evaluated. The derivation of 
the formula for the Multiple m based on Single queries (MMS) case is based on the occurrence of single 
term queries otd y and hence the resulting mi values tend to be sub optimal when they are applied to 
the environments where multiterm queries are also possible. Multiple m based on Multiterm queries (MMM) 
case not only treats terms differently based on their discriminatory power, but also takes mtdtiterm 
queries into account. Hence it is expected to give the largest savings in retrieval for our experimental 
settings. Yet there is additional practical overhead incurred in finding the optimal mi values with this 
method rising from the need to estimate the Pi(k) values where PI (k) is the probability that exactly 
k terms will be specified from Si. In fact, this is one valid reason to consider the performance of MMS: 
It might be plausible to be content with the output provided by MMS if we are convinced that MMS provides 
satisfactory amount of savings. The mi formula for MMM case is a good approximation of a complex method 
which gives the exact solution. The formula in Table III gives better results for large mi values when 
Pi (0) *O, P1 (1) * O and they are of the same order of magnitude. TABLE III. O@imum Weight Assi.znment 
Formulas fen SM, MMS an~ MMM-Cases Method Formula (equation no.) Ref. I Ftn2 [FALS5]m= (8) SM D ns M 
Ftn2 1 Q ln~ [FAL85] mi= M D +ln2 1= s [z] qi ln~-D I (9) ~Di L1 M i=1 Fln2 h mj = _ [FAL87, D + Dln2 
ln2 M FALS8] where M =1 F-Di  1 1 ) 4. PERFORMANCE EVALUATION FOR SM, MMS and MMM CASES 4.1 Finding 
the Distribution of Query Weight In order to compute the value of PERSAV (percentage of the addressable 
pages that do not have to be accessed) for a particular experimental setting, expected number of 1s in 
the h-bit suffix of the query signature should be known. This expected value is a function of the query 
weight (see equation 2). For single term queries, query weight is a known constant and equals to the 
number of bits set by the only query term. For multiterm queries, however, query weight is a random variable 
and therefore has a probability distribution. When nqt terms are used in a query, the query signature 
can be generated by superimposing the nqt individual term signatures. Let Cj be the number of bits set 
by the jth query term (where 2s j s nqt)andFbede size of the signature. Then the possible values for 
the query weight range from rnax{cj } (1 S j < nqt) to fin {(c I + c2 + . . . + Cnqt), F}. The lower 
limit is associated with the case where all (nqt-1) terms set the same bits that have already been set 
by term nqt* where cnqt* = mu {cj}. DMcally, c I +C2 +... + cnqt < F is guaranteed and the upper limit 
refers to the case where all nqt terms set different bits. The process of superimposing nqt term signatures 
can be viewed as an algorithm consisting of nqt stages. At the initial stage, c 1 of the F bits are set 
by the first term. If we let Yj indicate the number of bits set to 1 after the j h stage, Y1 = c1 holds 
by definition. Notice that c1 = Y 1 <Y2 <.. ._< Ynqt = w (~ and our concern is to find the probability 
distribution of Ynqt, which is the number Of 1S in tie query signature at the nqt h stage, i.e., after 
nqt term signatures are superimposed. The study reported in MUR92] has indicated that Pfls=ul Ys-1 =y, 
Ys-2= z,... ,Yl =c1}= PVs = u I Ys.1 = y}which means that the random variables Y1, Y2,, ,. ,Ynqt forma 
Mmkov Chain ~L68]. Using the concepts of one-step-transition probabilities together with some matrix 
manipulation techniques from Linear Algebra, MUR92] have come up with an expression for the probability 
distribution of the query weight, conditioned on the number of 1s set by the first query term. That is 
P(W(Q)=k+cl ICI)= /\  $ ( ?)(w)(.lf+if (11) 1足 r In the above expression, k stands for the possible 
values for the number of additional bit positions that are set after the first stage and F is the signature 
size as usual. Recall that our aim is to find a way to compute the expected number of 1s set in the 
h suffix of the query signature which is dependent on the query weight. The query weight, however, is 
no longer a constant and has a probability distribution which depends on the number of term signatures 
that are superimposed, nqt, together with the number of bits set to 1 by each term, cj, and is conditioned 
on the number of bit set by the first query term, c1. Hence, we not only should differentiate among the 
queries based on the number of terms they have, but also on the cj values of these terms and the number 
of bits set by the first term, c1. 4.2 A Closer Look at the Query Structure Assume that the terms in 
the database can be grouped into two sets, SI and S2, where S1contains the ones with high discriminatory 
power. The terms from S i set m i (1 s i <2) nnmber of bits and therefore cj wnals to ml or m2. Let t 
be the maximum number of terms that can be used in a query and let Pj indicate the occurrence probability 
of a query withj terms where (PI + P2 +. .. + Pt ) = 1 is satisfied. The tree diagram in Figure 1 enables 
us identify all different query combinations based on the criteria described Section 4.1. At any such 
eornbination, which is represented as a final outcome, we know the answers to these three questions: 
 1. How many terms are therein the query? 2. How many terms are from S 1 and how many are from S2, i.e., 
how many of the query terms set m 1 bits and how many of them set m2 bits? 3, Which set does the first 
query term belong to, i.e., how many bits does the first term set? P{bl =OInqt = 1] ..P3 ---- b:~;::::l~l=ml 
bl=l cl=m2;. P{bl =2Inqt =3] 2/3 bl=2 cl=ml 1/3 \k v c l=m2 -P{bl = 3 Inqt= 3] bl=3 \ P{bl = OInqt = 
t] bl=O L-足 t {bl-llnqt=t] l/t bl=l cl=ml (t-l)/t c 1=m2 P{bl =tInqt =t]Y----> h bl=t Figure 1. Tree 
diagram for query outcomes. This information enables us to compute the value of P(W(Q) = k+cl I c1 ) 
for each query outcome for those values of k that are realizable. Next, we can compute the value of the 
expected number of 1s in the h-suffix of the query for every such outcome. (The derivations for this 
computation will be provided in the next section.) So far we have clarified our reason to identify different 
query outcomes, now we can concentrate on the way the tree diagram is constructed: If the tree is traced 
from left to right, the correspondence between the branching procedure and the logical sequence of the 
events can be seen. Starting from the leftmost node, numbered as O, we encounter t possibilities, each 
corresponding to a query with nqt terms, where nqt stands for the number of query terms and ranges from 
1 tot. Each of the t branches symbolize one of these t events (i.e., specification of a query with nqt 
terms) and the probability associated with each event is indicated on its corresponding branch. Note 
that the sum of the probabilities associated with the branches emanating from a particular node adds 
up to 1. The submission of a single term query takes us to node 1 at which we have two possibilities: 
The term is either from S1 orS2. Let bi : number of terms from Si (1 s i ~ 2) in a query. Then 2 bi =n~ 
41= should be satisfied. Therefore, it is sufficient to use just b 1 (or b2) to specify a query combination, 
once nqt is known. For a single term query, the possible values for bi are O and 1 where P{bl=ll nqt=l}= 
qlaud P{bl=Olnqt= l}=q2 These two conditions take us to two final outcomes which can not be split up 
any further. From any node n (2< ns t), where n = nqt, (nqt+ 1) branches emanate, each corresponding 
to one possible value for bl in the range O to nqt. v v (v-v) P{bl=v I n@=V} = v ql ~ (12) () where O~v<Vand2<V<t 
Starting from node u, we can end up in any one of the n+ 1 outcomes. However, some of them can further 
be split up so that we will have the information to answer the three questions that are listed at the 
beginning of this section. At each of these n+ 1 outcomes, the number of query terms and the number of 
1s set by each term are known. For the first and (n+ 1) t outcomes, the number of bits set by the first 
term is also known since these outcomes correspond to the cases where all query terms come from a single 
set. For the remaining n-1 outcomes, we need further simplification depending on whether the first query 
term is from S1 or from S2. For each such case, let P{FT E Si} be the probability that first term is 
from Si (1 s i s 2). Then and hence P{Ff esl }+ P{Fres 2}=1 At this point, it might be useful to look 
at a numeric example for clarification. Lets concentrate on the case where we have three query terms. 
Then nqt = 3 and we are at node 3 from which four branches originate. The probability associated with 
each branch can be computed using equation (12). Of the four outcomes, two are final. When b 1 = 3, all 
three terms are from set 1, each term setting m 1 bits and when bl = O, all terms are from set 2, each 
setting m2 bits. For the case where b 1 = 2, we know that 2 out of 3 query terms are from S 1 but we 
still do not know the number of bits set by the first term. We compute P{lW G S 1} as 2/3 and P{IT G 
S2}as 1/3. For the case where bl = 1, these values are 1/3 and 2/3, respectively. Hence for the queries 
with three terms, we have six query outcomes that we need to treat separately. In general, for t term 
queries, there are t+ 1 branches and hence t+ 1 outcomes. Two of these are final, the remaining t-1 split 
into two. Henc~ we have (2 + 2(t-1)) , i.e., 2t final outcomes. 4.3 Derivation of an Expression for EXPH 
for Each Outcome Recall that P(W(Q = k+cl I c I ) can be computed for each query outcome identified in 
Figure 1. Since substituting k+cl for W(Q gives h min{h,k+cl} ~+~-hj j EXPH= j F  ( )() T d 1= (1 
\ ~k+cl) where EXPH is the expected number of 1s in the h suffix of a particular query when k additional 
bits are set after stage 1. The range of values for k must be specified for each final outcome, since 
it depends on the number of terms and the number of bits set by each term. Then the EXPH value for au 
outcome with nqt number of terms can be written as kmax in{h,kw 1} ( )() k+:-; h lJ j (13) 27 J= F k+cl 
() where nt nt kmax 2 = 1=2 Ci resuming that 21= ci<F Note that for the two outcomes corresponding to 
the single term queries, we only need to insert the value for c1 for W(Q) and compute EXPH using equation 
(2). Since the probability of occurrence of an outcome is the product of all probabilities of the branches 
from node O to the outcome, the overall EXPH (OEXPH) value can be computed by multiplying the probability 
of occurrence of each outcome with the EXPH value associated with it and summing them up. This OEXPH 
value can then be substituted in equation (6) to compute probability of access savings, which in turn 
will be used to compute number of pages that need not to be accessed. Finally, PERSAV can be obtained 
as a function of the number of pages, n, the hashing level, h , and the overall expected number of bits 
set in the last h suffix, OEXPH, P=SAV . N(n,h,OEXPH) * lfjo (14) n 5. APPLICATION OF PERFORMANCE EVALUATION 
FORMULAS TO THE TEST CASES In order to evaluate the performance of the three test cases, we need only 
to substitute the mi values computed by each method in the performance evaluation formulas. For the SM 
case, we just set m 1 = m2 = m, where m is computed using equation (8). The mi values are computed using 
equations (9) and (10) for MMS and MMM cases, respectively. For MMM, we need to compute PI (k) values 
such that Pi(k) : probability that exactly k terms will be specified from the ith set for (1 ~ i =2, 
O= k < 1). In our experiments, we allow a maximum of 10 terms to be specified in a query and hence set 
t = 10. Recall from Figure 1 that n+ 1 branches emanate from node n. Since we have 10 nodes, we end up 
with 10 ~n+l) = (lo) (11) + 10 = 65 outcomes 2 zn= Although some of these outcomes are not final, we 
will not go on any further since this much splitting is sufficient to compute the Pi(k) values. Assuming 
we give a sequence number to each of these 65 outcomes, ranging from 1 to 65, (bl =OInqt =1)being thefirst 
and(bl =tInqt =t)being the 65th, we now need to specify the sequence numbers (see Table III) of the outcomes 
for each of the following four conditions: bl =0, bl = l,b2 =0, b2= 1. Given the outcome sequence number, 
its occurrence probability y can be computed by simply taking the product of the probabilities of the 
branches originating from node O and ending at the outcome. Summation of all occurrence probabilities 
of the outcomes in one row of Table III will give us the associated Pi(k) value. Note that although Table 
III, is designed for the case where t = 10, it can be easily extended using the pattern in which the 
sequence numbers appear in a row. 6. EXPERIMENTAL ANALYSIS All experiments are based on the assumption 
that terms in the database are grouped into two sets, S I and S2. This assumption not only enables us 
to emphasize the points of interest without going into unnecessary complexity but also represents many 
real life cases [FAL135]. We specify the maximum number of query terms that can appear in a query as 
10, which we believe is appropriate to simulate many real life applications and is consistent with the 
choice of the values of the other input parameters. As for the Pj values, ~ = 1,2,. . . ,10), three different 
query cases (QC) are considered: Uniform Distribution (uD), Low Weight (LW) queries and High Weight (HW) 
queries. The definition of each of the query cases are given in Table IV. Table IV. Definition of the 
Qu ery Cases Q2 PI P~ P~ Pq P~ P~ P, Pg Pg P~~ UD 10 .10 .10 .10 .10 .10 .10 .10 .10 .10 LW .30 .25 .20 
.10 .00 .CO .00 .00 .00 .00 Hw .00 .00 .00 .00 .00 .10 .15 .20 25 30 Different values for the parameters 
F, qi (1< i <2) will be used in the experiments. Our main purpose is to show how PERSAV is affected by 
the amount of change in the values of the input parameters for three different probability distributions 
for each of the three schemes. Note that an experimental design that will achieve complete coverage of 
all possible combinations of the input parameters is impractical if not unnecessary. Hence we attempt 
to derive inferences about the change in the behavior of the system (represented as the change in the 
PERSAV) as a result of the alteration of the values of the input parameters. We then can use these observations 
to come up with generalized statements on the performance of the system under certain conditions. Experiment 
1. Purpose The purpose of the experiment is to compare the performance of SM, MMS and MMM cases in three 
different environments represented by the three different probability distributions (UD, HW and LW) specified 
above. Parameters The signature size (F) equals to 100, the values for D 1 and D2 are 15 and 25, and 
those for ql and q~ are 0.80 and 0.20, respectively. Results Figure 2 summarizes the results of the experiment 
for the UD case. Here PERSAV provided by MMS is above the amounts provided by MMS and SM. Figures 3 and 
4 show the results for LW and HW cases, respectively. These two cases correspond to two extreme situations 
where a deliberate non uniformity in the probability distribution for the number of query terms has been 
created. We expect MMM to provide excessive savings over MMS and SM in these cases since it makes use 
of more information about the system characteristics in determining the optimal assignment strategy. 
More specifically, MMM considers the type and relative frequency of queries with any number of terms 
that can be submitted to thes ystem whereas MMS computes the optimal mi values as if only single term 
queries are submitted to the system. MMM/mi=4,1 MM S/mi=3,1 SM/mi=2,2 0: III1I4 02468 10 12 h Figure 
2. PERSAV in UD. MMM/mi=4,1 MM S/mi=3,1 SM/mi=2,2 01I II 1I 02468 10 12 h Figure 3. PERSAV in LW Experimental 
results turn out to be consistent with intuition: MMM outperforms MMS for both LW and HW cases. Note 
that MMM emphasizes the terms from S I even more by letting them set 4 bits whereas optimal number of 
bits set by terms from S1 is 3 for MMS. The comparison of Figures 2, 3, and 4 show that PERSAV provided 
by all three schemes are higher in the HW case compared to those in LW. This is because as the probability 
of having queries with high weight increases, the OEXPH value also increases, resulting in larger percent 
savings. For MMM, the gain is even more, since the number of bits set by the terms from set 1 is larger 
than the values for the two other cases. As the frequency of the queries with many such terms increase, 
the OEXPH value increases even more, further increasing the amount of PERSAV . Figure 5. provides a closer 
look at the results from a new standpoint. Let EXTSAV = (15) PERSAV(MMM) -PERSAV(other) * ~00 PERSAV(other) 
where PERSAV(MMM) : percent savings provided by MMM PERSAV(other) : percent savings provided by any other 
scheme, SM or MMS, and EXTSAV is the extra percent savings provided by MMM over SM or MMS. MMM/mi=4,1 
MM S/mi=3,1 SM/mi=2,2 20 10 0 02468 10 12 h Figure 4. PERSAV in HW. -S sM/Hw + SM/UD +-SMLW + MMS/HW 
 * MMS/UD * MMS/LW  o~ 02468 10 12 h Figure 5. EXTSAV for UD, LW and HW. Naturally, EXTSAV over SM 
is the highest in all three cases: UD, LW, HW. However, the amount of EXTSAV decline for larger h values. 
For both of the HW cases, EXTSAV is the highest of all other cases at the beginning, but decline sharply 
with increasing values of h. This is due to the fact that OEXPH has a large value at HW regardless of 
the type of the scheme used, The increase in retrieval efficiency of all three methods become more effective 
as more bits of the signature suffix are taken into account. Hence the extra contribution introduced 
by MMM becomes less significant. For the LW cases, on the other hand, the savings are somewhat moderate 
but remain almost unaffected from the increase in h values, i.e., the database size. Although EXTSAV 
over MMS is smaller than that over SM, the decrease in the first one with larger h values is slower. 
The fact that the amount of savings are substantially high for our experiments do not guarantee that 
such savings can be expected all the time. For those cases where the mi values are considerably smaller 
relative to the signature size, the expected query weight will be lower and hence the resulting savings 
will be less significant. Experiment 2. Purpose The purpose of the experiment is to compare the optimal 
assignments (i.e., the suggested mi values) for SM, MMS and MMM for larger signature sizes at three different 
settings, each corresponding to one probability y distribution. Parameters The values for D I and D2 
are 15 and 25, respectively. Three values are used for the signature size: 400,500,600 and two values 
for the (q 1, q2) pair are selected: (0.60, 0.40) and (0.80, 0.20). Since Di values are kept constant, 
the first pair of the qi values corresponds to the case where terms from both sets are close in discriminatory 
power whereas the second pair represents the case where terms are more distinctive. Results The (m 1, 
m2 ) values at different settings for SM, MMS and MMM are summarized in Table V.A, V,B and V.C , respectively. 
 The values for the (ql, q2) pairs are given on the upper left corner of each table. Note that the optimal 
assignment in SM case depends only on the values of F and D and hence is insensitive to changes in the 
q, values and the probability distribution of the query weight. MMS, on the other hand, considers q i 
and Di values but its assignment is independent of the nature of the probability y distribution. Hence 
for a specified F and (q 1, q2) pair, the mi values remain constant for different probability distributions. 
Using all available information in the input parameters, MMM considers the values of Di, qi F together 
with the nature of the probability distribution. The insensitivity of the SM scheme to the changes in 
the values of the various input parameters leads to its inferior performance. Comparison of the two other 
methods shows that as the gap between the discriminatory power of the terms increases, the MMM assignment 
emphasizes the terms with higher power by letting them set more bits. Hence the gap between the mi values 
is the largest for the MMM case. This observation suggests that using MMM will be especially beneficial 
if the terms are highly distinctive based on their discriminatory power. Increasing the sigmture size 
has two effects working in the opposite directions: On one side, higher values of F attempt to raise 
the mi values. (See equations 8, 9, 10.) However, since these equations compute real rni values which 
need to be converted to integer numbers for practical applications, an increase in F increments the integer 
mi only if it is substantially large. Assuming that such a change occurs, larger mi values cause the 
query weight to increase resulting in higher OEXPH values, which in turn cause higher PERSAV. At the 
same time, however, as the signature size is increased, there are more bits to choose from when a term 
signature is generated and consequently the OEXPH value tends to drop. The resulting effect of increasing 
the value of F depends on the relative power of these two opposite changes. HIV! 3;3 I 3;3 I 4;4 Table 
V.B. (m 1, m2) Values for MMS Case (ql=o.60, q2.=o.40) (ql=o.80, q 2=o.20) QC F=400 F=500 F=600 F=400 
F=500 F=600 UD 3,2 4,3 5,4 4,2 5,2 6,3 LW 3,2 4,3 5,4 4,2 5,2 6,3 Hw 3,2 4,3 5,4 4,2 5,2 6,3 Table V.C. 
(m 1, m2) Values for MMM Case (ql=o.60, q2=o.40) (ql=o.w, q 2=o.20) QC F=400 F=500 F=600 F=400 F=500 
F=600 UD 3,2 4,3 5,4 5,2 5,2 6,3 LW 4,2 4,3 5,4 5, 1 6,2 6,3 Hw 4,2 4,3 5,4 5, 1 6,2 7,2 In most cases 
when the increase in F is large enough to cause an increase in the integer mi value(s), the reverse effect 
is suppressed and PERSAV values are raised. The amount of increase in PERSAV is substantial for the HIV 
case in particular, since in HW queries with many terms are more frequent and hence larger ml values 
have a more significant effect on the result. For the LW case, the increase in savings is moderate, if 
any. In some cases, even though the integer ml value increases, savings decline. This can be attributed 
to the fact that higher weight queries are not frequent enough to supplement the positive effect caused 
by the increase in ml and hence the reverse effect overrules. 7. APPLICATION OF FINDINGS TO REAL LIFE 
CASES Prior to selecting one of the three methods (SM, MMS, MMM) for a practical application, the database 
(formatted, unformatted or combined) characteristics of the system of concern should be explored (to 
create the non triviaf term list [CAN87, OZK86, SAL89] and to determine the occurrence frequencies) together 
with the query characteristics (like query contents and frequencies). The findings about the occurrence 
and query frequencies based on sufficiently large sampIe sizes can then be used to determine the number 
of sets to be used. In most cases, two sets might be good enough unless terms can apparently be classified 
in more than two groups based on their occurrence and query frequencies ~AL85]. Next, each record are 
examined to determine the Di values and the sample queries are reconsidered to come up with the qi values 
together with a tentative frequency distribution for the number of terms in a query. This frequency distribution 
can enable simulating the system behavior within an acceptable margin of error provided that the query 
sample is of appropriate size and is selected in a proper manner. Then a value for the signature size, 
F, is selected based on the values of these input parameters and the amount of tolerable false drop @?AL871. 
Next, one of the three methods (SM, MMS, MMM), is selected considering the database and query characteristics 
of the system which should have been revealed up to this step. If, for instance, there exists a huge 
difference among the term discriminatory power values, using W will provide extra savings over the other 
two methods, especially when the frequency distribution is not uniform. Further computations for the 
sake of comparison can be carried out to find out the mi values provided by each method. If still undecided 
as to which method to choose, the PERSAV for each method can be detemined using the sample queries at 
hand and our derived formulas. However, that updating will be necessary if either the contents of the 
database or the nature of the submitted queries change significantly. Hence the intensity and frequency 
of substantial change in system properties is also a characteristics of the system and must be considered 
in selecting a signature generation scheme. 8. CONCLUSION Our analysis combines the concerns of signature 
extraction and signature file organization which have usually been treated as separate issues. We also 
relax the uniform frequency and single term query assumptions and provide a comprehensive anal ysis for 
multiterm query environments where terms can be classified based on their query and database occurrence 
frequencies. We present the performance evaluation analysis of three specific signature generation schemes 
(SM, MMS, MMM) as they are applied to LHSS in a single and multiterm query environment. First scheme 
(SM) allows all terms set the same number of bits regardless of their discriminatory power whereas the 
second and third methods (MMS and MMM) emphasize the terms with high query frequency and low occurrence 
frequency. Of these three schemes, only MMM takes the probability distribution of the number of query 
terms into account in finding the optimal mapping strategy. We point out that the complexity in evaluating 
the performance of the cited schemes evolves from the challenging task of finding the distribution of 
the query weight for each of the query outcomes identified. A recent study has defined the partition 
activation ratio, PAR, as the ratio of the number of partitions activated by a query to the total number 
of partitions and have come up with an approximate formula to compute it [CIA92]. For a multitenn query 
environment, once the distribution of the query weight is found for each of the possible query outcomes, 
we might as well use this approximate formula to come up with the expected savings provided by each signature 
generation scheme. However, we have prefemed to use the original performance evaluation formulas which 
give more accurate results (see equations 2-7), We have shown that both MMS and MMM are clearly superior 
to SM in various query environments. Our results also indicate that the extra savings provided by MMM 
over MMS increase as the gap among the discriminatory power values of the terms get larger and the probability 
distribution of the number of terms in the query depicts a non uniform pattern. This is because MMM considers 
the nature of the probability distribution of the number of query terms in determining the optimal assignment 
strategy and emphasizes the terms with high discriminatory power in particular. Our study shows how different 
system input parameters interact in the overall working mechanism of the signature generation schemes 
and the LHSS organization. Once this point is clear, one can make a knowledgeable selection on the values 
of the input parameters and predict the expected savings, With some modification, our findings can be 
used to incorporate the performance evaluation of other partitioning methods (for examples see LEE89]) 
with various signature generation schemes in multiterm query environments. ACKNOWLEDGMENTS We thank Pavel 
Zezula of Technical University of Bmo for providing us with timely results of his work before they are 
published in the media, and Emily Murphree of Miami University for her valuable discussion. REFERENCES 
<RefA>[AKT93] Aktug, D., Can, F. Signature file hashing using term occurrence and query frequencies. In Proceedings 
of the 12th Annual IEEE International Phoenix Conference on Computers and Communications. (To appear, 
March 1993). [CAN87] Can, F., Ozkarahan, E. A. Computation of term/document discrimination values by 
use of the cover coefficient concept. Journal of the American Socie~ for Information Science. 38,3 (1987), 
171-183. [CAN90] Can, F., Ozkarahan, E. A. Concepts and effectiveness of the cover-coefficient-based 
clustering methodology for text databases. ACM Transactions on Database Systems. 15, 4 (Dec. 1990), 483-517. 
[CIAC92] Ciaccia, P., zezul~ P. A note on estimating acmxses in partitioned signature file organizations. 
Technical Note. (lst author s address: DEIS -University of Bologna, Viale Risorgimento 2-40136 Bologna 
-Ital y.) [CHR84] Christodoulakis, S,, Faloutsos, C. Signature files An access method for documents 
and its anal ytical performance eval uati on. ACM Transactions on O@ce Information Systenw 2,4 (October 
1984), 267-288. [FAL85] Faloutsos, C., Christodoulakis, S. Design of a signature file method that accounts 
for non-uniform occurrence and query frequencies. In Proceedings of the Ilth International Conference 
on VLDB (Stockholm, Aug. 1985). VLDB Endowment, 1985, pp. 165-170. [FAL87] Faloutsos, C., Signature files 
An integrated access method for text and attributes, suitable for optical disk storage. In University 
of Maryland Computer Science Technical Report Series, June 1987. [FAL87b] Faloutsos, C., Christodoulakis, 
S. OptimaI signature extraction and information loss. ACM Transactions on Databare Systems. 12,3 (September 
1987), 395-428. [FAL88] Faloutsos, C., Signature Files: An integrated access method for text and attributes, 
suitable for optical disk storage. MT. 1988,736-754. [FEL68] Feller, W. An Introduction to Probability 
Theory and its Applications, 3rd ed. John Wiley&#38; Sons, New York, 1963. [LEE89] Lee, D. L., Leng, 
C.-W. Partitioned signature files: Design issues and performance evaluation. ACM Transactions on Information 
Systems. 7,2 (Apr. 1989), 158足 180. [LEN92] Leng, C.-W R., Lee, D. L. Optimal weight assignment for 
signature generation. ACM Transactions on Database Systems. 17,2 (June 1992), 346-373. [LIT80] Litwin, 
W. Linear hashing A new tool for files and tables addressing. In Proceedings of the 6th International 
Conference on VLDB. (Montreal, Oct. 1980), pp. 212-223. [MUR92] Murphree, E., Aktug, D. Derivation of 
probability distribution of the weight of the query signature. (Preprint, 1st author s address: Department 
of Mathematics and Statistics, Miami University, Oxford, OH 45056, USA. ) [OZK86] Ozkarahan, E. A., Can, 
F. An automatic and tunable document indexing system. In Proceedings of the 9th Annual International 
ACM-SIGIR Conference (September 1986), ACM, New York, pp. 234-243. [SAL89] Salton, G. Automatic Text 
Processing: The Transformation. Analysis, and Retrieval of Information by Computer. Addison-Wesley, Reading, 
Mass., 1989. [TIB91] Tiberio, P., Zezula, P. Selecting signature files for specific applications. In 
Proceedings of Advanced Computer Technology, Reliable Systems and Applications, 5th Annual European Conference. 
(BoIogna, Italy, May 1991) IEEE, 1991, pp. 718-725. [THA88] Tharp, A. L. File Organization and Processing. 
John Wiley and Sons, New York, N. Y., 1988. [ZEZ88] Zezula, P. Linear hashing for signature files. In 
Proceedings of the IFIP TC6 and TC8 Open Symposium on Network Information Processing Systems. (Sofia, 
Bulgaria, May 1988), pp. 243-250. [ZEZ91] Zezula, P., Rabitti, F., Tiberio, P. Dynamic partitioning of 
signature files. ACM Transactions on Information Systems. 9,4 (Oct. 1991), 336-367.  </RefA>
			
