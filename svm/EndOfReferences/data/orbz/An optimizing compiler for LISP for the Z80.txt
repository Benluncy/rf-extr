
 AN OPTIr~IZI~[G COMPILER FOR LISP FOR THE Z~ Jed }[arti University ABSTRACT This paper describes an 
optimizing compiler for the Z8g. Described are the compilation mechanisms, optimization techniques, and 
perform- ance statistics. Introduction UOLISP [i] is a subset of Standard LISP [2] implemented for 
the Z8@ microprocessor. It rmls in a minimum of sixteen thousand bytes of storage and most effectively 
in thirty-two thousand or more. The system is more than just a basic LISP inter- preter. The entire facility 
consists of: a program to load precompiled "fast load" files. a parser for a subset of RLISP [3]. 
a function trace and break facility. a LISP structure editor. un online help facility and text formatter. 
 a pretty print facility. the Little ~TA translator w~ihing systei~ [4] . a compiler and optimizer. 
 arbitrary precision integer package. This paper addresses the mechanisms of the compiler and its optimizer. 
 The compilation process is divided into three passes: the first translates LISP into pseudo- assembly 
code called LAP (for Lisp A_ssembly Program), the second pass performs a peephole optimization on the 
LAP assembly code, the third pass translates this LAP into absolute machine code and places this in storage 
for execution or dumps it to a file for later restoration. Permission to copy without fee all or part 
of this material is granted provided that the copies are not made or distributed for direct commercial 
advantage, the ACM copyright notice and the title of the publication and its date appear, and notice 
is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, 
or to republish, requires a fee and/or specific permission. &#38;#169; 1982 ACM 0-89791-083-4/82/008/0003 
$00.75 of Oregon Overview  The LISP interpreter contains code for reading functions into the LISP 
system and executing them interpretively much like other microprocessor based systems. Unfortunately 
interpreted functions require large amounts of storage and execute very slowly. A more efficient scheme 
reads functions in the interpretive form, and then compiles them to machine code to be executed directly 
by the micro- processor. The interpreted version of the function disappears, its storage becomes available 
for use at a later time. For example, the function FACT, which comuutes the factorial of a n~iber recursively, 
is defined in UOLISP as follows: (DE FhCT (N) (COND ((LESSP N 2) i) (T (TIMES ~ (VACT (SUB1 N) ) l~) 
) ) ) In UOLTSP, dotted-pairs, of which this function is composed, take 4 bytes each. 22 dotted-pairs 
are used to define FACT for a total of 88 bytes. UOLISP's compiler generates the following code for FACT: 
 Loc. Code LAP INTEL @@@@ : ENTRY FhCT,EXPR ; FACT : ~ : CD96B3 CALL ALLOC ; CALL ALLOC ~}~3 : 92 DEFB 
2 ; .BYTE 2 ~4 : D7FE STOX HL,-I ; *STOX HL, -i @}9@6 : iI@24~ LDI DE,2 - LXI DE ,2 ~@9:E7 RST LINK ; 
RST LINK ~A: 172~ DE~.7 LESSP ; .ADDR LESSP @~@C: 28~5 JREQ $i ; J Z $i @~@E: 21~!4@ LDI HL,I ; LXI HL 
,i ~II: 18~m JR $~ JMPR $Z @~13 : $I : $i: @@13: CFBF LDX HL,-I ; *LDX HL, -i @@15:E7 RST LINK ; ~ST 
LINK ~16 : 832~ DE~,7 SUB1 ; . ADDR SUB1 @~18:E7 RST LINK ; RST LINK ~19 : A82~ DEF~d FACT ; . ADDR FACT 
~IB : CF7V LDX DE ,-I ; *LDX DE,-i ~ID: E7 RST LINK ; RST LINK @~IE : ID21 DEFW TIMES2 ; .ADDR TIHES2 
~@2@ : $~ : ;$~: ~@2@ : CD@8B4 CALL RDLLOC ; CALL RDLLOC @@23 : FE DEFB -2 ; . BYTE -2 * means macro 
form. A total of 36 bytes are used, less than half the size of the interpreted version. The compiled 
 version runs over 40 times as fast. Compilation ~chanisms Compiled programs move information between 
registers and call subroutines to perform most operations. In this section we describe how important 
LISP constructs are implemented in LAP and enumerate the various support functions required. Parameter 
Passing Zero to three parameters may be passed to a function. The first argument of a function (if it 
has any) will always be in the HL register pair, the second in DE, and the third in BC. Functions with 
more than three arguments cannot be compiled. This particular mode of execution is called the register 
model as opposed to the more common stack model. We believe that the register model is inherently more 
efficient than the stack model though perhaps more difficult to compile for. Stacks Function parameters 
and PROG type variables are kept in a stack frame, sometimes called an activation record, a contiguous 
block of locations pointed to by the IX index register. ~'~en a function is invoked it creates a new 
frame on the top of the stack by calling the ALLOC support routine. When a function terminates it calls 
the DALLOC routine which subtracts the number of locations used from IX, freeing the space for use by 
the next function. Storing and retrieving values from the stack frame is accomplished by the two support 
routines LDX and STOX. Since these operations occur frequently in compiled code it is necessary that 
they use as little storage as possible. Therefore the LDX and STOX routines are called using the Z8~ 
RST instruction with the following byte containing which register pair is to be stored (or loaded), and 
the displacement from the top of the stack frame. The LAP instructions generated by the compiler are 
also called LDX and STOX and contain the register pair name and what displacement is to be used. Both 
LAMBDA expressions and PPOG forms generate the ALLOC and DALLOC calls to handle stack frames. One of 
the optimizations performed is to substitute the appropriate number of increment or decrement IX instructions, 
or for larger frames, a sequence to add to IX. This has the disadvantage of not checking for stack overflow. 
 The Z8~ internal stack is used for saving return addresses and intermediate values during function 
evaluation. A call to a function FUN3 with three arguments stores the results of evaluation of the first 
two arguments on the Z8~ stack while the third is being computed. The values are popped into the appropriate 
registers just before the function is invoked. (FLrN3 (FUNA ...) (FUNB ...) (FUNC ...)) would generate 
the following code sequence: ... evaluate FUNA ... PUSH HL ;Save result of FUNA. ... evaluate FUNB 
... PUSH HL ;Save result of FUNB. ... evaluate FUNC ... LDHL BC ;~ve HL to BC. POP DE ;FUNB is second 
argument. POP HL ;FUNA is first argument. RST LINK ;Call FUN3. DEF~ FUN3 Function Invocation The 
compiler will not always know the address of a function being called because it might not be defined 
yet. Even if the function is defined the compiler does not know whether it will be compiled or interpreted 
at run time. A special internal subroutine called LINK is used to transfer control at run time. Since 
both compiled and interpreted functions can exist at the same time, LINK will perform either of two functions. 
If an interpreted function is being called from compiled code the LISP interpreter will be invoked for 
that function. If the function being called is compiled or is a system function the call to LINK will 
be replaced by a direct call to that function. The call to the LINK function must be an RST type link 
so that the three byte Z8~ CALL instruction will exactly replace the compiled call. If the system global 
variable ~*FLINK is NIL the substitution will not take place and the slow link form will remain. This 
is a useful debugging tool as it allows you to compile functions and change their definitions (for tracing) 
without reloading the system. Compiled as: Changed by LINK to: RST LINK CALL function-address DE~W 
function-name The two byte DEFW attached to the LINK contains the symbol table pointer of the function 
being called. At execution time the LINK routine looks for either a compiled or interpreted function 
attached to the name and either invokes EVAL, generates the CALL, or if the ~*FLINK flag is on, just 
transfers to the function. If no such function is defined, an error will occur and the name of the function 
will be displayed. The LIST Function The LIST function is compiled in a special way to take advantage 
of the Z8@ internal stack. The arguments of the LIST function are compiled and the results of each are 
pushed onto the stack. When all have been computed the support function CLIST is called. (LIST (FI ...) 
... (Fn ...)) compiles to: ... evaluate F1 . . . . . . PUSH HL ;Save result of F1 for CLIST. ;Evaluate 
other arguments. ... evaluate Fn ... PUSH HL ;Save result of Fn for CLIST. MVI A,n ;Number of values 
on stack. CALL CLIST ;Call to CLIST routine. COND Compilation The LISP COND function is compiled into 
a series of tests and conditional jumps. The CM2NIL support routine compares the result of a predicate 
to NIL and sets the ZS~ NZ and Z flag bit which controls the conditional branch instructions generated. 
If the last predicate of the COND is T, the predicate and jump will not be compiled (the usual case). 
 (COND (a~ c~) ... (a n Cn)) generates the following code: ... evaluate a ... RST CMPN~L ;Is a~ NIL? 
 JPEQ G~I ;Yes, jump to next antecedent ... evaluate c ... JP G~@@~ ;First consequent done, quit. 
G~I: ;Come here if a~ not T. ;Evaluate other a -c pairs. S~x : ;Try last predicate. *... evaluate 
a ... * RST CMP~IL ;Is last one NIL?  * JPEQ G~@@2 ;Go return NIL if yes. ... evaluate c ...  n 
S~@@2: ;Always come here when done. Lines preeeeded by an asterisk are not generated if the last predicate 
is T. PROG, GO, and RETURN The PROG function and the control constructs GO and RETURN are compiled 
by plugging lablels and values into a template. (PROG (X) LBL ... ... (RETURN value) (GO LBL) ... 
)  compiles to: CALL ALLOC ;Space to save X allocated. DEFB 2 LDI HL,NIL ;PROG variable set to NIL. 
STOX HL,-I  LBL: ;A PROG label generated. evaluate value ... JP G@@@l ;Jump to the end of PROG. JP 
LBL ;(GO LBL) generates JP. G@@@I: ;RETU~'s come here. CALL DALLOC ;Deallocate stack frame for DEFB 
-2 ; storage of X. AND and OR Compiled. AND and OR are compiled identically except that the evaluation 
of the arguments of ~ND terminates if one is NIL, and the evaluation of OR terminates if one is non-NIL. 
The compilation of AND generates JPEQ instructions after a comparison to NIL, and the compilation of 
OR generates JPNEQ instructions. (AND a~ ... an) compiles to: ... evaluate a ... RST ~PN~L ;Is result 
of a NIL? JPEQ G~I ;Stop evaluatio~ if yes. ;Evaluate other arguments. ... evaluate a ... G~I: n ;Always 
end up here. Constants, Variables, an__dd Quoted Values These items are loaded directly into the correct 
register for the function to which they are to be passed. I~c~l and global variables may have values 
assigned to them with the appropriate store instruction. Quoted items are saved on a list of compiled 
quoted values so that the garbage collector will not remove them. The value representing the quoted item 
is loaded into the appropriate register. Compiling FEXPR Calls When compiling calls to user or system 
defined FEXPR's the argument list is passed as a list to the function for evaluation. This interpreted 
form interacts poorly with compiled code for the following reason. All local variable names declared 
in a function are replaced with their stack frame locations by the compiler. When the FEXPR tries to 
evaluate its argument in the environment of the calling routine, the variable name in the S-expression 
cannot be found. The solution is to declare any variables to be passed to an FEXPR for evaluation as 
GLOBAL. This need not be done for COND, PROGN, PROG, OR, and AND because these forms are compiled into 
object code rather than as calls to functions. The Optimizer  The optimizing phase is divided into 
two passes and features two levels of optimization and a speed or space choice. The first phase is an 
extended peephole optimization, the second removes function prologs and epilogs from routines which do 
not need stack frames. The three levels of optimization include a "safe set", a set of speed optimizations 
which increase code size, and a "dangerous set" which removes some error checking. The Closing Window 
 There has been considerable research on peep- hole optimization for retargetable compilers [5-7]. The 
version used in the UOLISP optimizer might be more aptly called a "closing window" optimizer. The hole 
examined by the o~timizer initially includes the entire program. Each instruction is removed from the 
window in turn. The advantage of this mechanism is that the entire program may be scanned for each instruction 
examined. Most of the optimizations do not scan very far ahead. Redundant Instruction Removal This 
optimization removes several forms of instructions which replicate data already in registers. For example: 
 STOX HL,-I becomes STOX HL,-I LDX HL,-I  The closing window method permits any number of instructions 
between the STOX and LDX which do not modify the contents of HL (or whatever register is used). A second 
optimization removes store instruc = tions whose location is never referenced. This optimization is very 
imporCant in small sub routines. If all store instructions are removed, the stack frame allocation orolog 
and epilog may also be removed. Many very small routines can be reduced in size by as much as 85%. Since 
a great deal of time is spent in small routines, this optimization can be very important. J~mp Instructions 
 Several optimizations of this type are performed. The simplest removes unreachable code. JP label 
a  labelb: . .. All instructions between the JP instruction and the first label (labelh) following 
it are removed since they cannot be reached from anywhere. The same optimization is performed when a 
subroutine is called from which no return can be expected. Functions which always generate an error or 
use the THROW function have this feature. Another jump optimization removes wolthless forward jumps. 
Thus : JP label a label a : results in the jump instruction being removed completely. Conditional 
expressions are examined for multiple inversions. Thus:  CALL NOT becomes P~T C~'~PNIL RST CMPNIL JP-not-cond. 
label JPcond. label The final jump optimization garners the most savings of all optimizations. It determines 
the distance jump instructions must travel and if it is less than 127 bytes in either direction the instruction 
is converted to its short form. Since most LISP functions are very short, most jumps end up in their 
short forms saving 1 byte. Unfor- tunately short jumps are usually 20% slower. Stack Frame Optimizations 
 Many times the end of a PROG form is also the end of its corresponding L~iBDA expression and two DALLOC 
calls will occur in a row. In this case the optimizer combines the two calls into one by adding their 
sizes together. A further optimization occurs if the last CALL DALLOC is immediately followed by a RET 
instruction. The call to DALLOC is replaced by a call to the special routine PdgLLOC which automatically 
does the extra return. The use of this routine saves 1 byte and about 5 microseconds (for the 4 mthz. 
ZS~A) on each function exit. Reduction in Strength  This class of optimizations replaces several lonq 
form instruction~: (or sets of instructions) with a simpler Z8@ instruction. Thus moving HL to DE has 
an XCHG instruction substituted, saving a single byte. A 3 byte call to any of the C~hR, CADR, CDAR, 
and CDDR is replaced with two single byte calls on CAR and CDR saving a single byte. This optimization 
is disabled on machines which do not have the 1 byte calls on CAR and CDR. Finally, the 4 byte version 
of the LHLD instruction is replaced with its shorter and faster 3 byte version. Fast Optimiz ations 
 The LDX and STOX stack frame referencing functions take two bytes for each use. _~ne functions themselves 
take approximately 50 micro- seconds to execute. Approximately 50% of the execution of compiled code 
is spent in these two routines. By open coding them as indexed MOV instructions, the time is reduced 
to less than i@ microseconds at the expense of 4 additional bytes. This particular optimization can be 
turned on and off by the user so that very important functions are optimized and less important ones, 
slower but much smaller. In the factorial example, use of this optimization results in a 24% speed improve- 
 ment at a cost of a 38% increase in size. Dangerous Optimizations This set of optimizations removes 
a number of error checks to increase execution efficiency. With selective use they cause no problems. 
One such optimization replaces the stack frame alloc- ation routine calls by a string of increment or 
decrement register IX instructions: CALL ALLOC becomes INX X DEFB 4 INX X INX X INX X Larger stack 
frames use a DADX instruction rather than the increments. CALL ALLOC becomes EXX DEFB 16 LXI HL,16 
DADX HL EXX The corresponding decrement forms are used for the stack frame deallocation calls. The deallocation 
is done as part of the fast optimization because it is never dangerous. The second optimization is open 
coding of the ADD1 and SUB1 functions. These are replaced by INX HL, and DCX HL instructions. They are 
not dangerous as long as the sign of the number does not change. A sign change causes overflow into the 
tag field of a number changing it into a bad identifier or string pointer. Second Optimization Pass 
 The second optimization pass removes the function prolog and epilog if no stack frame is used. Thus 
the function: (DZ CAAAAR (X) (CAAR (C~R X))) is compiled without optimization into: ENTRY CAAAAR, 
EXPR CALL ALLOC DEFB 2 STOX HL, -1 LDX HL, -1 CALL CAAR CALL CAAR CALL DALLOC DEFB -2 RET This version 
uses 19 bytes. After the first optimization pass the following code is produced: ENTRY CAAAAR, EXPR 
CALL ALLOC DEFB 2 RST CAR RS T CAR RST CAR RST CAR CALL RDLLOC DEFB -2 are no STOX or LDX instructions). 
The final pass produces : ENTRY CAA/IAR, EXPR RST CAR RST CAR RST CAR RST CAR RET The final version 
takes only 5 bytes, a savings of about 75%. Execution Statistics We now examine the effect of the 
optimizer on code size and execution speed. A rough approximation of two different types of programs 
and their size and execution statistics are given. The first program is the factorial example. 6: was 
computed i@,@~@ times on a 4 megahertz, 64k CP/M system. The second test does a complete reversla to 
all levels of a binary tree. It is also executed i@,@@@ times and experiences 6 garbage collections. 
 (DE SUPER'-REVERSE (A) (COND ((ATOM A) A) (T (CONS (SUPER'.-REVERSE (CDR A)) (SUPER:-REVERSE (CAR A) 
) ) ) ) ) The tree ((A . B) (C . D)) was reversed to ((D . C) . (B . A)). Size A/B Time A/B Bytes 
Seconds No optimization 42 / 44 48 / 43 Safe optimization 37 / 38 45 / 37 Safe and fast 51 / 56 34 / 
26 Fast and dangerous 49 / 56 27 / 23 At best the ol)timizez provides ~ 47% speed up at the expense 
of a 20% space increase. To get a view of the effectiveness of each of the individual optimizations 
over a class of programs, 8 different programs were compiled and the number of bytes saved by each of 
the reduc- tion in size optimizations were tallied. Opt. Program No. A B C D E F G H Total 1 44 16 
16 12 ~ 24 12 12 136 2 52 43 2@ 5 13 16 7 42 198 3 34 34 38 2 26 28 8 98 268 4 18 28 8 ~ 2 2 2 36 96 
5 56 118 52 i~ 3 4 ~ ~ 233 6 ~ 6 ~ ~ 6 ~ @ 15 27 7 12 54 ~ 3 @ 6 6 33 114 8 47 8 26 4 i@ 21 18 77 211 
9 16 64 16 @ Z 7 7 84 194 i@ 22 @ 36 2 8 8 2~ 2 98 ii 66 27 27 9 6 18 24 30 207 12 129 17@ 75 2~ 33 55 
6~ 135 677 13 12 1 31 ~ 4 2 5 8 63 14 33 21 @ 9 4 12 ~ ~ 79 % 12 12 13 I~ 14 12 14 14 12.5 This version 
takes 12 bytes. The second pass notices that the stack frame is never used (there The most important 
space optimization by far is the short jump conversion, the second, the removal of redundant load register 
instructions, the third the conversion of 4 byte LHLD instructions to 3 bytes, the fourth, the conversion 
of 16 bit move HL to DE instruction (actually two instructions) to an XCHG instruction, the fifth, the 
inversion of conditional jumps, and the sixth the use of the RDLLOC stack deallocation routine. The least 
important is the removal of dead code after functions which do not return. The average reduction in 
size achieved by the optimizer is a little over 12.5%. This compares very favorably with other peephole 
optimizers which gather about 15 % (one of these has over two hunderd separate optimizations). A final 
test compares UOLISP generated compiled code with that produced by various compilers for mainframes. 
 Test A B C UOLISP 1 132 391 51 145~ 2 135 3502 i@37 40~@ 3 117 748 1173 15500 4 562 4692 2312 18500 
 5 2072 8313 2023 37@0@ 6 1098 9231 12806 i~900@ 7 1062 1972 136~ 137~ 8 i~19 18326 6800 49~00 The 
8 different programs tested were designed to exercise various features of compiled LISP code. The tests 
for the first three LISP compilers were taken from [8] and have been subsequently improved. Machine 
A is a large DEC 2~6~ running LISP 1.6 with the Portable LISP Compiler [9], machine B is a VAX ii/75@ 
running Franz LISP, machine C is a VAX 11/750 running Portable Standard LISP Version 2 [i~], ~id UOLISP 
r1~,s on a 64k Z80A system with CP/M 2.2. A few of the time tests reflect the relatively small amount 
of space available and a large number of garbage collections. The statistics show that compiled UOLISP 
code is on the average one fiftieth the speed of a DEC 2@60 running LISP 1.6, one seventh the speed 
of Franz LISP, and one tenth the speed of Portable Standard LISP on the VAX 11/750. Conclusions The 
UOLISP compiler runs on almost any Z80 based machine with a minimum storage configuration of 32k bytes 
and a disk drive. The compiler and optimizer have been tested under both CP/M and the TRS-80 ~del I and 
III with success. Turning on all of the optimizations slows down compilation by approximately 40 percent. 
The UOLISP compiler occupies 375@ bytes of storage and the optimizer with statistics collection another 
3@0@ bytes. Standard Use has debugging done without the presence of the optimizer and the final run with 
the optimizer enabled.  List of References <RefA>I. J. Marti, 'UOLISP Users Manual', University of Oregon 
Depart~%ent of Computer and Information Science Technical Report CIS-TR-80-18. 2. J. Harti, A. C. Hearn, 
M. L. Griss, C. Griss, 'The Standard LISP Report', SIGPLAN Notices, Vol. 14, No. I@, (October 1979), 
pp. 48-68.  3. A. C. Hearn, 'REDUCE 2 Users Hanual' , Utah Symbolic Computation Group UCP-19, University 
of Utah, 1973.  4. J. Marti, 'A Session with the Little Meta Translator ~riting System', University 
of Oregon Technical Report CIS-TR-82-01.  5. J. W. Davidson, C. ~. Fraser, 'The Design and Application 
of a Retargetable Peephole Optimizer' ACM TOPLAS, Vol. 2, No. 2, (April 1980), pp. 191-202.  6. A. S. 
Tannenba~m, H. van Staveren, J. 71. Stevenson, 'Using Peephole Optimization on Intermediate Code', ACM 
TOPLAS, Vol. 4, No. i,  (January 1982), pp. 21-36.  7. D. A. L~mb, 'Construction of a Peephole Optim- 
izer', Software Practice and Experience, Vol. ll, No. 6, (June 1981), pp. 639-647.  8. M. L. Griss, 
PSL Interest Group Newsletter #4, February 1982.  9. H. L. Griss, 'A Portable LISP Compiler', Soft- 
  ware Practice and Experience, Vol. ii, (June 1981) , pp. 541-605.  10. 'The Portable St~ndard LISP 
Users Manual', The Utah Symbolic Cc nputation Group, University of Utah, January 1982. </RefA>
			
