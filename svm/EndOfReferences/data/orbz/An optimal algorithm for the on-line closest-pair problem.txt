
 An optimal algorithm for the on-line closest-pair problem Christian Schwarz* Michiel Smid* Jack Snoeyinkt 
Abstract and for t= m, it is defined by We give an algorithm that computes the closest pair W(P} q) = 
~~$~k IF% %1 .­ in a set of n points in k-dimensional space on-line, in O(n log n) time. The algorithm 
only uses algebraic func- We observe, as many other researchers have observed, tions and, therefore, 
is optimal. The algorithm main­ that if we are satisfied with computing a closest pair tains a hierarchical 
subdivision of k-space into hyper­ without knowing their exact distance we can compare rectangles, which 
is stored in a binary tree. Centroids t-th powers of distances to avoid the computation of the are used 
to maintain a balanced decomposition of this t-th root. If t is an integer or co, then these comparisons 
tree. can be performed using algebraic functions. In this pa­ per, we fix t and measure all distances 
in the Lt-metric. We write d(p, q) for c&#38;(p, q). Introduction In the off-line version of the problem, 
the complete The closest pair problem is one of the classical prob-set of points is known at the start 
of the algorithm, lems in computational geometry. In this problem, we This version of the problem has 
been solved optimally have to compute the closest pair or its distance in for a long time, In 1975, Shames 
and Hoey [9] gave an a set of n points in k-dimensional space. Distances O(n log n) algorithm for the 
planar case. One year later, are measured in an arbitrary, but fixed, Lt-metric. Let Bentley and Shames 
[1] gave an O(n log n) algorithm for . . ..pk) and q = (ql the k-dimensional case. See also Vaidya [14], 
who solved P = (PI, ,....~~) be two points in k-dimensional space. Then the Lt-distance dt (p, q) be­ 
the all-nearest-neighbors problem within the same time tween p and q is defined by bound. All these algorithms 
can be implemented in the algebraic decision tree model, for which an O(n log n) lower bound holds. See 
Preparata and Shames [6]. In this paper, we consider the on-line closest pair t(p ) =($ pi-q tift< sm 
sm  problem. Here, the points arrive one after another. Af­ * Max-Planck-Institut fiir Informatik, W-6600 
%arbriicken, ter a point arrives, we have to update the current closest Germany. These authors were 
supported by the ESPRIT II B* sic Research Actions Program, under contract No. 3075 (project pair. This 
version of the problem has only been studied ALCOM). recently. t Dept. of Computer Science, University 
of British Columbia, In Smid [1 I], an algorithm is given that computes Vancouver, B.C. V6T 1Z2, Canada. 
the closest pair on-line, in O(n(log n)k -1, time. This algorithm only uses algebraic functions. Therefore, 
it is optimal for the planar case. Smid [11] and Schwarz and Smid [8] give al­ gorithms that run in 
O(n(log n)2/ log log n) and in O(n log n log log n) time, respectively, for any fixed di­ mension k. 
These algorithms, however, use the non­provided that the copies are not made or distributed for direct 
algebraic floor function. If additionally the functions commercial advantage, the ACM copyright notice 
and the title of the publication and its date appear, and notice is given that copying is by permission 
of the Association for Computing Machinery. To copy other­wise, or to republish, requires a fee andlor 
specific permission. Permission to copy without fee all or part of this material is granted 8th Annual 
Computational Geometry, 6/92, Berlin, Germany @1992 ACM 89791-518-6/92/0006/0330 . . . . . . . .. $1.50 
330 EXP and LOG are available at unit-cost, the run-2 The basic algorithm ning time of the algorithm 
in [8] can be improved to O(nlogn). In this paper, we give an O(n log n) algorithm for any fixed dimension 
k that uses only algebraic functions. Hence, the algorithm is optimal. More precisely, we give a data 
structure that maintains the closest pair in O(log n) amortized time per insertion. Our structure can 
also solve the problem of computing on-line the clos­est pair that existed over the history of a fully 
dynamic point set in O(log n) amortized time per insertion or deletion. Note that recently there haa 
been much interest in the dynamic closest pair problem. For the case, where only deletions are allowed, 
see Supowit [13]. For the fully dynamic case, see Smid [10, 12], Salowe [7] and Dickerson and Drysdale 
[3]. The algorithm in this paper is based on the algorithm of Smid [11]. To update the closest pair when 
a point is inserted, that algorithm makes some queries into a data structure for the k-dimensional rectangular 
point loca­tion problem. In this data structure, one query takes O((log n)k -1, time, which causes the 
entire algorithm to have an amortized insertion time of O((log n)h -l). In this paper, we also use a 
data structure for the rectangular point location problem. The subdivisions of k-space that arise, however, 
are regular enough to allow point location queries to be solved in logarithmic time. The data structure 
for these queries is implemented us­ing centroids and tree decompositions. Chazelle [2] in­troduced such 
decompositions to computational geom­etry with his polygon cutting theorem. Guibas et al. [4] gave a 
procedure to compute them in linear time. In Section 2, we give the basic algorithm for main­taining 
the closest pair under insertions. We define the subdivision that is used during this algorithm, and 
give an initial data structure that implements the insertion operation using point location. In Section 
3, we use centroids for the implementation of point location. In this way, the time for one query is 
improved to O(log n). (In Section 2, this time could be linear.) In order to maintain this improved data 
structure, we use the partial rebuilding technique. (See e.g. Overmars [5].) In Section 4, we apply our 
solution to computing the closest pair in history and give some concluding re­marks. In this section, 
we give a data structure th~at maintains the closest pair in a point set under insertions of points. 
The basic idea is the same as in Smid [11]. We give all details, however, to keep the paper self-contained. 
The algorithm maintains a subdivision of k-space into axes-parallel hyperrectangles, called k-bolces 
for short, Formally, a k-~oz has the form [a, :b~]x[az: b~]x,,. x[a~:b~], where ai c ~U { co}, bi c 
lRU {00} and ai < bi for i=l, . . ..k. We say that a point p = (P1, pz, ..., p~)t is contained in the 
above k-box, if ai ~ pi < bi for all i. In this way, even if a point Iies on the boundary of many k-boxes, 
the notion of containment is uniquely defined. The data structure: The essential component of the closest-pair 
data structure is a hierarchical subdivi­sion of space into k-boxes. Let V be the current set of points, 
and let n be its cardinality. The data structure stores the following information: A pair of points (P, 
Q) that are a closest pair in V and a variable 6 whose value is the distance cl(P, Q). A binary tree 
T representing the current subdi­vision of k-space. The nodes of T store k-boxes, where the k-boxes stored 
in the leaves form a sub­division of k-space. For each non-leaf node v, the k-box stored in it is equal 
to the union of the two k-boxes that are stored in the two children of v. With each leaf of T, we store 
a list of all points in V that are contained in the k-box stored in this leaf. (These points are stored 
in an arbitrimy order.) The k-boxes stored at the leaves of this data structure have some additional 
constraints that we enforce. (1) each leaf k-box has sides of length at least 6, where 6 is the distance 
of the closest pair in V. (2) each k-box contains at leaat one and at most (2k -!­2)k points of V. 
(3) all k-boxes are non-overlapping and together they partition the entire k-space.  Initializing the 
structure: Suppose that we start with a set V of size two. Then the initial subdivision of k-space consists 
of one k-box, namely the entire space. The binary tree T consists of one leaf node, whose k-box is the 
only box in the subdivision. With this leaf, we store a list containing the two points. The pair (P, 
Q) stores the two points, and the value of 6 is equal to its distance. Clearly, after the initialization, 
the subdivision and the data structure satisfy the above constraints. Our algorithm to insert a new point 
will use point location as a subroutine. Thus, before giving the algo­rithm we describe a simple-minded 
method to use the binary tree T to answer point location queries in linear time. In Section 3 we improve 
point location to loga­rithmic time. Point location: Let p be a point in k-space. In a point location 
query, we have to locate the k-box in the current subdivision that contains p. This query is answered 
as follows. Starting in the root of the tree T, we visit the nodes of T on the path to the leaf whose 
k-box contains p. We maintain as an invariant that p is contained in the k-box that is stored in the 
current node. Suppose we have reached the non-leaf node v. Point p is contained in exactly one of the 
k-boxes that are stored in the two children of v. The search proceeds in the child storing this k-box. 
The procedure ends if we reach a leaf. By the in­variant, the k-box stored in this leaf cent ains the 
query point p. The insertion algorithm: Let P = (PI,..., P~) be the point to be inserted, The algorithm 
makes two steps. The first step updates the closest pair; the second updates the rest of the data structure. 
1. Update the closest pair: Observe that only boxes intersecting the 6-ball around the new point p can 
contain points q such that d(p, q) < 6. Therefore, we first identify these boxes, For this purpose, we 
perform 3k point location queries, with query points (PI + cl , ..ojn+~k)j forcl, . . ..cheb. o, b},}, 
 Then, for each k-box that is located, we walk through its list of points. For each point q that is in 
one of these lists, if d(p, q) < 6, we set (P, Q) := (p, q) and 6 := Ct(p, q). tree T whose k-box contains 
point p. We insert p into the list that is stored with v. If afterwards this list contains at most (2k 
+ 2)k points, the algorithm is finished. That is, the subdi­vision is not changed, Otherwise, if.it contains 
1 + (2k + 2)b points, we per­form a split operation on the k-box stored in v. This split operation is 
defined as follows. Suppose we want to split the k-box B = [al : bl] x . . . x [ak : bk] of the current 
subdivision. Let V be the set of points that are stored in the list of B. Fori=l ,. ... k, we compute 
the values mi and Mi, which are, respectively, the minimal and msximal i-th coordinate of any point of 
V . Let i be an index such that Mi m~ >28, (In Lemma 2, we show that there is such an index.) Let ci 
:= mi + (Mi mi)/2. Then we split the k-box B into two k-boxes Br = [al :bl] x ...x [ai_l :hi-l] x[a~ 
:c~] X [a~+l :b~+l] x ... X [a~ :b~] and B, = [al : bl] x ...x [ai_l :hi-l] x[c~ :b~]X [a~+l :b~+l] 
x ...x [a~ :b~]. In the tree T, the leaf v corresponding to B gets two children, one child for the k-box 
Bt and one for the k­box B,. The list that is stored with v is removed, and it is split in two lists 
for the new leaves. This concludes the insertion algorithm. First, we prove a sparseness result that 
is needed in the proof of Lemma 2. Lemma 1 Let V be a set of points in k-dimensional space, and let 
6 denote the minimal distance in V. Then any k-dimensional cube having sides of length 26 con­tains at 
most (2k + 2)k points of V. proof: Partition the cube into (2k + 2)k subcubes with sides of length 6/(k 
-i-1). Now assume that the cube contains at least (2k + Z)k + 1 points of V. Then one of the sub cubes 
contains at least two points of V. These two points have a distance that is at most equal to the L~-diameter 
of this sub cube. This diameter, however, is at most k . 6/(k + 1) <6. This contradicts the fact that 
the minimal distance of V k 6, m 2. Update the rest of the data structure: In the In the next lemma, 
we show that the index i that is previous step, we have located the leaf v of the binary used in the 
split operation indeed exists. Lemma 2 Let V be a set of points in k-space, and let 6 be the distance 
of a closest pair in V. Let B be a k­ box that contains more than (2k + 2)k points of V. For i= l,..., 
k, define the minimal, mi, and maximal, Mij i-th coordinates of any point in Vfl B. Then there is an 
index i, such that AIi mi >26. Proofi Assume that Mi mi <26 for all i = 1,..., k. Then, there is a 
k-cube B having side lengths 26 that cent ains all points of V n B. By the previous lemma, however, the 
cube B contains at most (2k + 2)k points of V. This is a contradiction. m Lemma 3 Let B be a k-box in 
the subdivision of k­space whose list contains 1 + (2k + 2)k points of V. Let 6 be the minimal distance 
of V. Suppose, we perform a split operation on B. Afier this operation, the sides of the two newly created 
k-boxes have length at least 6, and each such k-box contains at least one and at most (2k + 2)k points 
of V. Proof: The lemma follows immediately from the split algorithm. m Lemma 4 The insertion algorithm 
correctly maintains the closest pair data structure. Proofi Let 6 be the minimal distance just before 
the insertion of point p. If this minimal distance changes, there must be a point inside the L~-ball 
of radius 6 centered at p. This ball is contained in the k-box X ~k 6 : Pk -f-6]. Therefore, it suffices 
to compare p with all points of the current set V that are in this box. Let kl ~ Pl+6] x ~:=vn(~l 6:pl+6]x... 
x~6 6 :Pk+d]) be the set of these points, and let W be the set of points that are contained in the lists 
corresponding to the k­ boxes that result from the 3k point location queries. The algorithm compares 
p with all points in W . Hence, if we show that W ~ W , then it is clear that the algorithm correctly 
maint sins the closest pair. Let q = (ql, . . . . qk) be a point in W. Assume w.1.o.g. that qi~pifori=l,. 
..jk. 1 henpi<~i<pi+~ fori=l ,..., k. Let B be the k-box in the current subdivision of k-space whose 
list cent sins q. Assume that q @ W . Then B does not contain any of the 2k points (pl + al, . . ..pk+Q 
k). WheKQ 1, . . ..GE {0,6}. These 2k points are the corners of the k-box B := ~1:~1+6] X... X~k:lU +6], 
having sides of length 6. (Note that in general B is not part of the current subdivision of k-space.) 
Since q c B , and since B does not contain any of the corner points of B , it follows that the box B 
must have at least one side of length strictly less than 6. This contradicts the definition of our data 
structure. Hence, q G W and, therefore, W c WI. This proves that the insertion algorithm correctly maintains 
the closest pair. It remains to show that the new subdivision satisfies the invariants ( l) (3). Consider 
a k-box of the current subdivision that is not split during the insertion. Since the value of 6 can only 
decrease, the side lengths of this box remain at least equal to 6. Clearly, if the box contains at least 
one point before the insertion, so it does afterwards. Also, the box still contains at most (2k + 2)k 
points. If a k-box is split, then Lemma 3 guarantees that the new k-boxes have sides of length at least 
6, that they cent ain at least one and at most (2k + 2)k points. Finally, it is clear that the k-boxes 
that are not split, together with the two new k-boxes, are ncm-overlapping and partition k-space. n The 
central operations of the insertion algorithm are point location and splitting a k-box of the subdivision. 
The following theorem expresses the running time of the algorithm in terms of the cost of these two operations. 
Theorem 1 Let Q(n) be the time for one point location query and S(n) be the time for one split operation. 
The given data structure has linear size and maintains the closest pair of the set V in O(Q(n) + S(n)) 
time pe~ insertion. Proofi The binary tree T has at most n leaves, because each leaf corresponds to a 
non-empty k-box. Therefore, T has linear size. Since any point is stored in exactly one list, all these 
lists together also have linear size. This proves the space bound. In the insertion algorithm, we need 
0(3hQ(n)) time for all point location queries. Then, we walk through at most 3k lists, each of which 
has size at most (2k + 2)k. This takes time 0(3k(2k + 2)k). In case no split operation is necessary, 
the data struc­ ture needs O(1) time to update the rest of the data structure. Otherwise, we need S(n) 
time for the split operation. It follows that the overall insertion time is bounded by 0(3kQ(71) + 3k(2k 
+ 2)k + S(n)), which is O(Q(n) + S(n)), because k is a constant. E Let h denote the height of the binary 
tree T. Then, clearly, it takes O(h) time to solve one point location query. Since h can be linear in 
n, it follows that Q(n) can be ~(n). Consider a split operation. First, it takes 0(k(2k + 2)b) time to 
find the index i. Then, the op­eration can be completed within the same time bound. Hence, since k is 
a constant, S(n) = O(l). Therefore, an insertion takes O(n) time in the worst-case. In the next section, 
we build an additional search structure on T that improves the point location time, Q(n), to O(log n). 
In order to maintain the search struc­ture, we increase the split time, S(n), to O(log n) in the amortized 
sense. Hence, it follows from Theorem 1 that the insertion algorithm will need O(log n) amor­ tized time 
for one insertion. 3 Point location using the tree decomposition In the previous section, the point 
location algorithm started in the root of the tree T and followed a path until it reached a leaf. We 
observe, however, that it is not necessary to start in the root; the algorithm can start in an arbitrary 
node. Suppose we start in node v, Let B, be the k-box that is stored with v. If the query point p is 
contained in L%, the search continues in one of the two subtrees rooted at children of v, Otherwise, 
if p is not cent ained in B , the search continues in the tree that is obtained by re­moving the subtree 
rooted at v. These searches proceed recursively, i.e., again they do not necessarily start in the root 
of the subtree. If we choose our initial node v such that the two subtrees have roughly equal size and 
repeat choosing nodes in this way for the recursive searches, then we get a logarithmic search time. 
In the rest of this section we do three things. First, we define the fl-decomposition tree TP on the 
nodes of the tree T. A procedure of Guibas et al. [4] can be used to compute ~-decomposition trees suitable 
for our purpose. Second, we define our new data structure that uses the tree decomposition. Third, with 
this new data struc­ture, we implement the two central operations of our on-line closest pair algorithm 
(cf. the remark preceding Theorem 1): we show how to do logarithmic-time point location in the subdivision 
of k-boxes, and we show how to do a split operation on a leaf of the tree representing the subdivision. 
We call an internal node v 6 T a $centroid if the re­moval of v results in three connected components, 
each containing at most /31T1 nodes. (Here, ITI denotes the number of nodes in T.) Notice that a fl-centroid 
is also a @-centroid for all @ > ~. A ,f?-decomposition tree of T, denoted Tp, is defined recursively: 
The B­decomposition tree of a leaf is just the leaf. Otherwise, the root of T@is a $centroid v E T, and 
the children are @-decomposition trees for the connected components of T v. The trees T and T@ have 
the same set of leaves and the same set of internal nodes. Since T is binary, the &#38;decomposition 
tree TP is ternary. For any node v c T@ we have three point­ers, left(v), right(v), and up(v), that point 
to the f?­decomposition trees for the connected components of T v that contain the left child of v in 
T, the right child of v in T, and the parent of v in T, respectively. The nodes that are stored in the 
subtree of TO rooted at v form the component of v, denoted by C(v). From the decomposition scheme, we 
have C(v) = C(ieft(v)) U C(f ight(v)) U C(tip(v)) u {v}. Finally, we note that the depth of T@ is O(logfllp) 
ITI). In [4], Guibss et al. give an algorithm that computes a centroid decomposition of a binary tree 
T in linear time. In that paper, the tree T is decomposed by re­moving a centroid edge which decomposes 
T into two parts, each of size at least ~(ITI + 1)/3J. A straight­forward modification of theu aigorithm, 
however, also computes a @decomposition tree Tp for T in linear time! with @ = 1/2. The improved data 
structure: As in Section 2, the data structure comprises the . i,sest pair and a tree T storing a subdivision 
of k-space into k-boxes, whose leaves store the current subdivision and satisfy (l)-(3). Each internal 
node v stores the union of the k-boxes stored in the leaves of the subtree rooted at v. For each leaf 
c, there is a list of the points of V lying in the k-box stored at z. We also maintain a ternary &#38;decomposition 
tree Tp of T, where ,6 = 3/4. As described above, C(v), the component of v, consists of the nodes in 
the subtree of Tp rooted at v. With each node v E Tp, we store the size of C(v). Point location: The 
~-decomposition tree To guides point location in the tree T that represents the subdi­vision of k-space 
into k-boxes. Let p be a point in k-space, and let s be the unique leaf in T whose k-box contains p. 
Our task is to find s. The algorithm consecutively checks nodes v, starting in the root of the decomposition 
tree T~. We maintain the invariant that, if v is the current node, then s E C(v). At the start of the 
algorithm, when v is the root of TP, the invariant is trivially true, since in this case all nodes are 
in C(v). Now let v be the current node. By induction, we assume that the invariant holds for v, which 
means that s c C(v). If v is a leaf of T, then the invariant implies that v=s,andwearedone. Ifvisnot 
aleafofT, then v # s. Since s G C (V) by the invariant, we have IC(V)I >1 in this case. This means that 
the subtree of v in T@ has more than one node, which in turn implies that v is not a leaf in T@. Let 
z = left(v), ~ = right(v), and z = up(v) be the children of v in T@. We know that at least one of the 
nodes z, y, z exists. We examine v as follows: in constant time, we check whether point p is inside BV, 
the k-box corresponding to v, If p is inside B , then we check which one of the two k-boxes of children 
of v in T also contains p. With this knowledge, we can choose the correct child of v in T6 to continue 
the search: If p lies in the box stored in the left child of v in T, then s must be in the left subtree 
in T. In this case, we choose z to be the new current node. Since the only part of C(v) which lies in 
the left subtree of v in T is C(Z), we have s E C(x). The case that p lies in the right subtree of v 
in T is symmetric. It remains to consider the case p @ B.. Here, we choose z to be the new current node. 
Since p @ Btj, we know that s @ C(Z) and s @ C(y), which implies s 6 C(z). The search proceeds via edges 
of Tp and ends in a leaf of TP. From our invariant, this leaf must be s. Therefore we have the following 
lemma: Lemma 5 Let T be a binary tree of size n, storing a collection of k-boxes in k-space as defined 
in the pre­ vious section, and let p be a query point. Given a B­ decomposition tree of T, point location, 
i.e., identify­ ing the k-box containing p, can be done in O(log(llp) n) time. From the definition of 
the improved data structure, we have @ = 3/4, and it follows that Q(n) = O(log n). Next, we discuss how 
to maintain the tree TO if split operations are performed. We shall show that the im­proved data structure 
can be correctly maintained, and that S(n) = O(log n) amortized. Split operation: Let z be a leaf of 
T and suppose that we perform a split operation on the k-box stored in x, Then, x is turned into an internal 
node and is given two new children xl, X2, in T as well as ink Tfl. Updates gradually unbalance the fl-dlecomposition 
tree TP; we must maintain T@ in amortized logarith­mic time. For each node v G T@, we store the size 
of C(v), as prescribed in the definition of the improved data structure. When we add leaves to T, the 
nodes of T@ whose counts change are those on the search path to the leaves we can update these counts 
in O(log n) time. Then we can determine the higheat node in To that is no longer a (3/4) -centroid and 
rebuild its subtree in T@. Using the algorithm of [4], we compute a (l/2)­decomposition for this subtree 
in time proportional to its size. If we build the subtree of Tfl rooted at v, then v is a (1/2)-centroid. 
In order to rebuild v a aubtree, one must increase the size of the subtree by a quarter. Thus, if every 
leaf inserted into the subtree brings along a credit, we can use these credits to pay for the rebuilding. 
Since the depth of Tfl is O(log n), each additional leaf needs only O(log n) credits. It follows that 
the total coat of rebuilding over n insertions is O(n log n). We have the following lemma: Lemma 6 The 
amortized cost of a split operation is O(logn). From Lemma 5 and Lemma 6, we have point location cost 
Q(n) = O(log n) and split cost S(n) = O(log n) amortized. Combining this with Theorem 1 gives the following 
result. Theorem 2 The imp~oved data structure has linear size and maintains the closest pair in the point 
set V in O(log n) amortized time. Corollary 1 The closest pair in a set of n points in k­dimensional 
space can be computed on-line in O(n logn) time, using O(n) space. This is optimal in the algebraic decision 
tree model.  4 The closest pair in history and open problems We have given an optimal solution to the 
problem of maintaining a closest pair as points are inserted on-line. It is natural to ask about fully 
dynamic point sets in which points can be inserted and deleted. While we cannot efficiently maintain 
the closest pair under on-line insertions and deletions, we can solve the problem of recording the closest 
pair in history Symp. on Computational Geometry, 1991, pp. 234­recording the closest pair of points that 
existed simul-238. taneously during an on-line sequence of point insertions<RefA> [4] L. Guibas, J, Hershberger, 
D. Leven, M. Sharir and and deletions. Such a record could help verify that an R.E. Tarjan. Linear time 
algorithms for visibility appropriate step size was used in a dynamic system sim­ and shortest path 
problems inside triangulated sivn­ ulation. p/e potygons. Algorithmic 2 (1987), pp. 209-233. Theorem 
3 The closest pair of points over the history [5] M.H. Overmars. The Design of Dynamic Data of a sequence 
of insertions and deletions can be com- Structures. Lecture Notes in Computer Science, puted on-line 
in O(n log n) time, using O(n) space. Vol. 156, Springer-Verlag, Berlin, 1983. [6] F.P. Preparata and 
M.I. Shames. ComputationalProof: We use the improved data structure with the Geometry, an Introduction. 
Springer-Verlag, New insertion operation specified above. To delete a point York, 1985. p, we locate 
the leaf node v whose k-box B. contains p. We delete p from v s point list in constant time. If some 
[7] J .S. Salowe. Shallow interdistance selection and in­points remain in B. then we are done-the invariants 
terdistance enumeration. Proc. WADS 91, LNCS still hold. (Note that 6 can only decrease, so there is 
no Vol. 519, Springer-Verlag, Berlin, 1991, pp. 117­problem with the side lengths of the k-boxes.) 128. 
Otherwise, we must delete the node v and contract the parent and sibling of v into one node. As in the 
[8] C. Schwarz and M. Smid. An O(n log n log log n) algorithm for the on-line closest pair problem. To 
splitting algorithm, we update the component counts appear in: Proceedings 3rd Annual ACM-SIAM that change 
in T@in O(log n) time. Then we can rebuild Symp. on Discrete Algorithms, 1992. the subtree of the highest 
node in To that is no longer a (3/4) -centroid. [9] M.I. Shames and D. Hoey. Closest-pair problems. Since 
rebuilding forms a (1/2)-decomposition, the Proc. 16th Annual IEEE Symp. on Foundations of number of 
updates between two rebuilding at a node Computer Science, 1975, pp. 151-162. is still proportional to 
the size of its subtree. Thus, if [10] M. Smid. Maintaining the minimal distance of a each deletion 
also comes with O(log n) credits to give point set in less than linear time. Algorithms Re­ to the nodes 
on the search path in Tfl, these credits can view 2 (1991), pp. 33-44. be used to pay for the rebalancing. 
Thus, deletion costs also amortize to O(log n) per deletion. E [11] M. Smid. Dynamic rectangular point 
location, with Other open problems remain. In some applications, an application to the closest pair problem. 
Report one would like to change the amortized time bounds MPI-I-91-101, Max-Planck-Institut fiir Informatik, 
for insertions to worst-case bounds. Algorithms using Saarbrucken, 1991. See also: Proc. 2nd Annual In­floors, 
randomness, or other models of computation may ternational Symp. on Algorithms, 1991. have o(n log n) 
running times. [12] M. Smid. Maintaining the minimal distance of a point set in polylogan thmic time 
(revised ver- References sion). Report MPI-I-9 1-103, Max-Planck-Institut fur Informatik, Saarbrucken, 
1991. See also: Proc. [1] J.L. Bentley and M.I. Shames. Divide-and-conquer 2nd Annual ACM-SIAM Symp, 
on Discrete Algo­in multidimensional space. Proc. 8th Annual ACM rithms, 1991, pp. 1-6. Symp. on Theory 
of Computing, 1976, pp. 220-230. [13] K.J. Supowit. New techniques for some dynamic [2] B. Chazelle. 
A theorem on po/ygon cutting with closest-point and farthest-point problems. Proc. applications. Proc. 
23rd Annual IEEE Symp. on 1st Annual ACM-SIAM Symp. on Discrete Algo-Foundations of Computer Science, 
1982, pp. 339­ rithms, 1990, pp. 84-90. 349. [14] P.M. Vaidya. An O(n log n) algorithm for the [3] M.T. 
Dickerson and R.S. Drysdale. Enumerating k all-nearest-neighbors problem. Discrete Comput. distances 
for n points in the plane. Proc. 7th ACM Geom. 4 (1989), pp. 101-115. 
			
</RefA>
