
 Adaptive Performance Prediction for  Distributed Data-Intensive Applications Marcio FaermanAlan SuRichard 
WolskiFrancine BermanAugust 9, 1999 Abstract For many distributed data-intensive applications, data movement 
across the network is a critical determinant of ap- The computational grid is becoming the platform of 
choice plication performance. In particular, in addition to being data­for large-scale distributed data-intensive 
applications. Accu-intensive, such applications may also be network-bound, with rately predicting the 
transfer times of remote data .les, a fun-application performance heavily determined by the bandwidth 
damental component of such applications, is critical to achiev-available on network links used during 
data transfers. Ex­ing application performance. In this paper, we introduce a amples of network-bound 
distributed data-intensive applica­performance prediction method, AdRM (Adaptive Regression tions include 
JPL s Synthetic Aperture Radar Atlas (SARA) Modeling), to determine .le transfer times for network-bound 
application [35], which allows the user to select and view distributed data-intensive applications. images 
generated from a large, replicated, and distributed We demonstrate the effectiveness of the AdRM method 
on database of radar data, and SDSC s Storage Resource Broker two distributed data applications, SARA 
(Synthetic Aperture (SRB) [6, 30], which provides a uniform interface for users to Radar Atlas) and SRB 
(Storage Resource Broker), and discuss obtain data from a heterogeneous and distributed collection of 
how it can be used for application scheduling. Our experi-data repositories. ments use the Network Weather 
Service [36, 37], a resource Ef.cient execution of network-bound distributed data­performance measurement 
and forecasting facility, as a basis intensive applications on computational grids can be challeng­for 
the performance prediction model. Our initial .ndings in­ ing. Although grids offer considerable performance 
potential dicate that the AdRM method can be effective in accurately through aggregation of resources, 
application execution per­predicting data transfer times in wide-area multi-user grid en­ formance may 
be dif.cult to achieve in practice. In particular, vironments. the load and availability of shared resources 
such as networks may be hard to predict, affecting the ability of an application scheduler to develop 
performance-ef.cient application execu­ 1 Introduction tion strategies. In this paper, we present a 
method, Adaptive Regres- Ensembles of distributed computational, storage, and other re­ sion Modeling 
(AdRM), for predicting the performance sources, also known as computational grids [18], are becom­  
of data transfer operations in network-bound distributed ing an increasingly important platform for 
applications which data-intensive applications. Our technique predicts perfor­ perform computations over 
large datasets. Such applications mance in production, multi-user distributed environments by include 
image acquisition and processing computations, digi­employing small network bandwidth probes (provided 
by the tal library searches, high performance massive data assimila-Network Weather Service (NWS) [36, 
37]) to make short-term tion, distributed data mining and others [3, 6, 16, 17, 20, 25, predictions of 
transfer times for a range of .le sizes. The NWS 35]. Aggregating distributed resources on the grid presents 
the gathers performance probe data from a distributed collection opportunity to employ or acquire data 
from massive datasets of resources and catalogs that data as individual performance which are too large 
to be stored at a single site.histories for each resource. It then applies lightweight time  This research 
was supported in part by NASA Graduate Student Re-series analysis models to each performance history 
to produce search Grant #NGT-2-52251, Department of Defense Modernization Contract short-term forecasts 
of future performance levels. In this pa­ 9720733-00, DARPA contract N66001-97-C-8531, National Science 
Foun­ per, we make use of the performance probe data, but not of the dation Grants ASC-9701333 and ASC-9619020, 
and CAPES/Brazil Grant BEX0488/95-2.performance forecasts generated by the NWS. Our approach X Department 
of Computer Science and Engineering, Uni-is to combine NWS measurements (which are non-intrusive versity 
of California San Diego, La Jolla, CA 92093-0114 and relatively frequent) with instrumentation data taken 
from ({ mfaerman,alsu,berman} @cs.ucsd.edu) m actual application runs (which are potentially intrusive 
but in- Department of Computer Science, University of Tennessee, Knoxville, TN 37996-1301 (rich@cs.utk.edu) 
frequent) to predict the future performance of the application. Permission to make digital/hard copy 
of all or part of this work for personal or classroom use is granted without fee provided that copies 
are not made or 1 distributed for profit or commercial advantage, the copyright notice, the title of 
the publication and its date appear, and notice is given that copying is by permission of ACM, Inc. To 
copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific 
permission and/or a fee. SC 99, Portland, OR (c) 1999 ACM 1-58113-091-8/99/0011 $3.50 To capture the 
relationship between NWS probes and appli-Client Server cationbenchmark data, we use regression models 
which cali­ brate application execution performance to the dynamic state of the system measured by the 
NWS. The result is an accurate performance model that can be parameterized by live NWS measurements to 
make time-sensitive predictions. The development of performance methods such as AdRM is critical to achieving 
application performance for network­bound distributed data-intensive applications in multi-user computational 
grid environments. Accurate predictions of data transfer operations such as those provided by AdRM are 
used in compositional models of data-intensive applications. Such models, used by AppLeS schedulers 
[8], are often equational, precluding the use of relative ranking of resource performance to perform 
scheduling decisions. Models which provide an accurate ranking, although useful for some applications 
[31], may be insuf.cient for others. AdRM provides relatively ac­curate predictions and relies on observable 
performance mea­surements only, and thus can be continuously updated to adapt to current network conditions 
automatically and in real-time. We demonstrate the effectiveness of the AdRM method for two network-bound 
data-intensive applications with dissimilar data requirements: the SARA image acquisition application 
which generally targets relatively small .les (1-3 MB), and the SRB query tool which is designed to handle 
much larger (16 MB or more) .les. This paper is organized as follows: In Section 2, we brie.y describe 
the characteristics of network-bound distributed data­intensive applications, in particular, SARA and 
SRB, and dis­cuss the importance of accurate predictions for application per­formance. Section 3 presents 
several performance models for predicting data transfer times for this application class. We present 
the AdRM prediction method and present experiments which demonstrate its effectiveness for both SARA 
and SRB in Section 4. In Section 5, we summarize and brie.y discuss related and future work.  2 Network-Bound 
Distributed Data-Intensive Applications We use the term data-intensive applications to denote com­putations 
which access and perform operations on numerous or massive datasets. Within this application class, we 
iden­tify a subclass of network-bound distributed data-intensive applications for which a prime determinant 
of application per­formance is movement of data across the network. In general, network-bound distributed 
data-intensive appli­cations can be partitioned into computational and communi­cation subcomponents that 
will be assigned over multiple dis­tributed grid resources. Figure 1 represents a typical network­bound 
application, having a data source where the application data is stored, User Application Processes Data 
Source Figure 1: Network-bound distributed data-intensive applica­tions generally involve retrieving 
data from a server to a client with various data manipulation operations performed at either the client 
or the server. The computational processes in the .gure (the ellipses) are logical entities, representing 
very com­plex operations (e.g. data compression or image generation) to extremely simple operations (e.g. 
sending data to the network unchanged). a user application, running at the client site, several computational 
processes running at the server and client site, capable of performing varying data manipula­tion operations 
of varying degrees of complexity, and a network connection, over which data moves between the client 
and server (and a principal determinant of ap­plication performance for network-bound applications) To 
illustrate the characteristics of network-bound dis­tributed data-intensive applications we provide a 
brief descrip­tion of JPL s Synthetic Aperture Radar Atlas application and SDSC s Storage Resource Broker. 
The Synthetic Aperture Radar Atlas (SARA) [31, 35], de­veloped at JPL and SDSC, is a Web-based distributed 
data­intensive application which allows users with access to the World-Wide Web to view images of the 
Earth s surface taken by a synthetic aperture radar. The SARA datasets are repli­cated across several 
high-capacity storage sites. Via a Java applet, users of the SARA system can request an image of an arbitrary 
sub-region with certain features of the data high­lighted. A collection of distributed data servers and 
computa­tional processes retrieve, .lter, and convert the raw synthetic aperture data into an image .le. 
SARA s Java user-interface applet allows the user to supply input parameters and view the resulting input 
.le. During certain phases of SARA operation, .les typically ranging in size from 1 to 3 MB are transferred 
between sites. SDSC s Storage Resource Broker (SRB) [6, 30] is middle­ware that provides data-intensive 
applications with a uniform API to access heterogeneous distributed storage resources sys­temsincluding 
.le systems, databases, and hierarchical and m archival storage systems. SRB provides users the capability 
to access and aggregate massive quantities of data scattered across wide area networks. In addition to 
the .le transfer se­mantic, the SRB has a meta-data catalog providing the user the capability of performing 
queries over the data according to a relational database discipline. Data-collections supported by the 
SRB include -the Elib Flora collection from UC, Berke­ley [15], the Alexandria Digital Library from UC, 
Santa Bar­bara [1], the NARA (National Archives and Records Admin­istration) [24] and the Art Museum 
Image Consortium (AM-ICO) collections [2]. In order to achieve performance, both SARA and SRB usu­ally 
need to coordinate the data transfer operations as part of a larger application framework. In this case, 
.le transfer time may be a component of a larger performance model. Accurate predictions of remote .le 
transfer performance are critical in the scheduling of these applications over a distributed environ­ment. 
Note that there are several important differences between SARA and SRB. Most obviously, SARA .les are 
typically smaller (1 to 3 megabytes) than SRB .les, which can be quite large (tens to hundreds of megabytes). 
SARA is speci.cally designed and implemented to be an interactive satellite im­age processing Web application, 
whereas the SRB is general­purpose middleware providing facilities for a wide range of distinct distributed 
data-intensive applications. The differ­ences in their designs, implementations, and the way that they 
are typically used result in different performance behavior for SRB and SARA when performing .le transfers, 
particularly in the presence of varying network performance. In this study, we chose to compare SARA 
and SRB under different usage models. Since SARA is designed to be an in­teractive visualization tool, 
we implemented the SARA client to make .les accesses relatively frequently approximately once every 
.ve minutes. We hypothesized that large-scale SRB transfers, however, would be relatively infrequent 
due to their potentially lengthy durations (up to tens of minutes per transfer). As such, we programmed 
the interval between SRB transfers to range from 2 to 25 hours. These differences pro­vide a wider experimental 
framework to validate the effective­ness our proposed prediction method, since not only are the applications 
different, but they access the network on different time scales as well. 2.1 Predicting Application Performance 
In order to execute network-bound distributed data-intensive applications, the computation, communication 
and data trans­fer components of the application must be mapped to re­sources. To achieve performance 
in multi-user grid environ­ments, adaptive scheduling of these components is critical. As part of the 
AppLeS project, we have found that we can achieve good schedules by using dynamic system resource informa­tion 
in application models which predict application perfor­mance [8, 26, 27, 28, 29]. Even for the simple 
client-server model in Figure 1, effec­tive adaptive scheduling may be non-trivial. For example, for 
the model shown, the scheduler must determine how these op­erations can be split between client and server. 
To illustrate, suppose the user of an SRB geographical image server appli­cation requests data about 
Los Angeles. Due to the nature of the raw data to which the server has access, .ltering operations on 
the relevant data (e.g. highlighting speci.c characteristics of the dataset) may be necessary. For instance 
the Los Angeles information might be stored in a large .le which also contains data about many other 
California cities -which however are not relevant to the user. Such .ltering operations can be per­formed 
either at the server site, at the client site, or partially executed on both sites. The decision of where 
and how the .ltering will be con­ducted deeply affects performance. If the requested informa­tion is 
extracted at the server site, then less information will need to be transferred across the network and 
processed at the client site. However, if the server site is heavily loaded and the network connection 
between server and client is relatively fast, scheduling the .ltering operation on the client location 
may yield better overall application performance. Conversely, if the network connection is particularly 
slow then it might be more ef.cient to use data compression at the server and de­compression at the client. 
Although this strategy would con­sume more CPU power and time at both sites, the application may perform 
better overall by avoiding the large data trans­fer. Andresen et. al. describe additional workload allocation 
and performance trade-offs issues on distributed data-intensive scenarios in [4]. As the example above 
illustrates, the decision of how to par­tition application operations over the resources of a distributed 
environment may deeply affect performance. Moreover, on a multi-user distributed environment the challenge 
is further complicated since the deliverable performance of each indi­vidual resource can vary dynamically. 
To accurately forecast application behavior the prediction model must re.ect load and performance characteristics 
of both the application and its target environment. However, developing prediction models which re.ect 
dynamic computational and communication phe­nomena is dif.cult, albeit important. In Section 3, we will 
de­scribe several models for predicting .le transfer performance in dynamic grid environments.  2.2 
Using Performance Prediction Models In order to support scheduling, predictions are generally used in 
two ways: to rank alternative candidate schedules or re­sources according to a given cost function, and 
as an estimate of the absolute performance of components of a larger appli­cation model, for coordinating 
workload allocation over re­sources. Note that both these uses impose different constraints 3 Performance 
Models u on whatit means for predictions to be good . It would be reasonable to expect that a simple 
performance For the ranking scenario, it is important for the prediction model to provide relative predictions 
which should be ranked the same as the actual execution under the chosen performance criteria execution 
time, throughput, etc. For example, if the application executes in less time (under the same load condi­tions) 
with schedule A than with schedule B, a good ranking prediction model would yield execution time predictions 
PPA and P B such that P AA::P B . Note that we are not necessarily concerned whether PPA is a close match 
with the actual exe­cution time for the application under schedule A or P B for schedule B, since the 
prediction model and cost function only serve to rank alternatives. However, certain applications or 
situations such as work­load allocation may require a higher level of precision and accuracy regarding 
absolute predictions. Errors in the predic­tion of the deliverable performance of subcomponents of larger 
compositional models may aggregate so that the overall appli­cation performance model itself might become 
inaccurate. In particular, we want PAA. T A (where T A is the actual execu­tion time) when schedule A 
is part of a larger compositional application performance model. In this case, we are more in­terested 
in the error between the actual execution time and the predicted execution time, rather than simple ranking. 
To illustrate, a scheduler for the geographical data applica­tion described in the previous section requires 
predictions of the effective computational capacity on the computation sites as well as the deliverable 
data transfer throughput between the two sites at run time. Having these predictions on hand, the scheduler 
can .nd the best allocation of the .ltering operation over the client and server computers, coordinating 
it with the data transfer across the network. By contrast, consider a server selection scenario in which 
a data consumer must choose between various data servers which contain portions of a replicated dataset. 
In this case, pre­dictions of .le transfer are used to choose between data servers storing a desired 
replicated data .le. In particular, ranking pre­dictions are suf.cient in this situation, as described 
in [31] in the context of the SARA application. In contrast, applications which use SRB for data access 
require reasonably accurate .le transfer time estimates in order to make scheduling decisions when the 
prediction of data transfer time is part of a larger application execution performance model. This paper 
focuses on a method to provide accurate abso­lute predictions of remote .le transfer performance. In 
the next section we discuss performance models concerning the characterization and prediction of remote 
.le transfer perfor­mance. model of remote .le transfer time for a network-bound dis­tributed application 
would suf.ce for scheduling and appli­cation execution. In particular, the straightforward model RBW 
(Raw BandWidth model)1 shown below: wa w Da t aaSaA dw aAAtwTur a n s JrwTiueaa AaabAtwwoa a n ddwd t 
h could be used to estimate the time for transferring .les between various servers based on the value 
of iveaAAaabAlwwo aa n d waddt h between the client and each of the servers. This value could be supplied 
by a network monitor such as the Network Weather Service (NWS) [36, 37], which measures and forecasts 
the load and availability of system re­sources (including network bandwidth). The RBW model can be used 
to predict the .le transfer performance of distributed data-intensive applications, like SARA and SRB, 
in a wide­ iueaaAAa bAtwwo area grid environment, if the a n ddwad t h measure accurately represents 
the effective bandwidth an application would experience. Our initial experiments with SARA [31] show 
that the RBW model using NWS forecasts can be used effectively to rank alternative candidate schedules. 
The NWS generated ivea aaAAaabAlwwoa n d wddt h forecasts, based on standard NWS 64 kilobyte data transfers 
rather than the 1 to 3 megabyte .le transfers that SARA uses. Although this scheduler was able to rank 
the servers, the resulting absolute transfer time predic­tions were quite inaccurate. The NWS measurement 
probes can be parameterized to use arbitrary transfer sizes, however each probe consumes network bandwidth. 
To obtain an ac­curate value of iueaaAAa bAtwwoa n ddwad t h for SARA, we would have had to con.gure 
the NWS to probe the network us­ing the same message sizes that SARA uses thereby dramat­ically increasing 
the intrusiveness of the monitoring. More­over, this solution would not necessarily guarantee an accurateiveaaAAaabAlwwoa 
n d waddt h measure for applications with different characteristics, such as SRB, which typically has 
larger trans­fer sizes than SARA. In addition, RBW fails to consider application computa­tional and internal 
message buffering overheads and overlap between communication and computational operations which may 
have non-trivial effects on performance for even the most network-bound applications. It is not surprising 
then that RBW model predicts performance relatively inaccurately, as shown in Table 1. Note that while 
the 64 kilobyte probes do not yield an absolute measure of available bandwidth, the peaks and valleys 
as predicted by the NWS probes visu­ally correlated with observed .le transfer behavior (Figures 2 and 
3). Our work in this paper attempts to exploit the corre­lation between non-invasive probes and actual 
application be­ 1In the scope of this paper we consider .les large enough such that initial transfer 
latency is negligible compared to the whole .le transfer time. SRB NMAE site RBW AdRM U.C. Davis NCSA 
W.U. St. Louis Rutgers 54.10% 105.20% 96.31% 101.56% 11.09% 9.20% 11.95% 1.15% SARA NMAE site RBW AdRM 
Utah 34.40% 9.98% UIUC 36.08% 11.10% Caltech 15.80% 11.67% Table 1: Normalized Mean Absolute Errors 
(NMAE) for .le transfer throughput forecasts obtained directly from band­width measurements RBW and from 
the Adaptive Regression Modeling (AdRM) forecaster. havior to yield a performance prediction with good 
absolute accuracy. 3.1 Regression Modeling Although a sophisticated and detailed performance model could 
possibly represent almost all the intricacies present in the performance behavior of remote .le transfers, 
the com­plexity level of this mechanistic model might be so high that its dynamic parameterization at 
run-time might prove in­feasible. Regression modeling is a simple and lightweight method for establishing 
a functional relationship among vari­ables [9, 11, 14]. In order to achieve more accurate predic­tions, 
we considered the use of a linear regression model to address the discrepancy between the performance 
behavior of small NWS probes and larger data transfers exhibited in the RBW model. We developed two linear 
models that map NWS bandwidth measurements to the observed .le transfer behav­ior of the network-bound 
distributed data-intensive application within a speci.ed time-frame. To determine the upper bound on 
absolute accuracy, we started by regressing large-data.le transfer times with 16MB NWS bandwidth probes. 
This probe size is too large to be practical for the NWS in general, but it allowed the NWS to more accurately 
mimic the actual network load during .le transfer. Figure 2 shows the results of using this technique 
to model .le transfer times from the University of California at Davis to the University of California 
at San Diego. The exper­iments show this regression model parameterized by data from 16MB .le transfers 
and 16MB NWS message size probes. The results were very impressive. We observe in the graph how the modeled 
data.le transfer performance closely tracks the actual throughput measurements. Our next step was to 
investigate a less intrusive approach to determine if the technique could be U.C. Davis Time (hours) 
Figure 2: Linear Regression mapping NWS (16 MB messages) to SRB (16 MB .le transfers) implemented practically. 
 3.2 A Practical Approach NWS with 64 kB Probes Only In order to decrease overhead, we considered a 
new regression model which uses NWS inputs with 64 kB probes. This is the probe size that is commonly 
used by the Network Weather Ser­vice to perform bandwidth measurements and forecasts. Using small probes, 
the NWS is able to maintain a low level of intru­siveness on the network [36, 37]. Representative results 
using the low overhead model, are shown in Figure 3. Note that the model still tracks the actual data.le 
transfer measurements very closely. This regression model with 64 kB probes can be used successfully 
to form the basis for an ef.cient application performance prediction method. In the next section, we 
describe the development and use of a performance prediction method which we call Adaptive Regression 
Modeling (AdRM).   4 Using AdRM to Dynamically Predict Execution Performance of SARA and SRB In this 
section we describe an adaptive regression modeling (AdRM) method to predict the execution performance 
of the SRB and SARA grid applications at run-time. Since linear U.C. Davis 3.5 0 Time (hours) Figure 
3: Linear Regression mapping NWS (64 kB messages) to SRB (16 MB data.le transfers) regression is cheap 
to compute,2 we start by deriving an ini­tial regression model from historical application and NWS performance 
data. As the application executes, we monitor its performance and add the monitored values to the perfor­mance 
history of the program. When a prediction is required, we recalculate the regression coef.cients on-the-.y 
using the original performance history, the most recent performance measurements, and the corresponding 
NWS data for the most recent time frame. In this way, the prediction model evolves and adapts in response 
to changing performance conditions. We term the initial set of samples required to heat-up the regression 
model the Start-up Window. Having an initial re­gression model derived from the Start-Up Window, we obtain 
a new bandwidth sample from NWS, and use this value in the regression model to generate a prediction 
of the .rst data.le transfer throughput value. While the application executes, the regression function 
is updated at each new application .le transfer. The new .le transfer sample and the network bandwidth 
value (from the NWS) are incorporated as a new pair within history of network and data.le transfer samples 
that will be used in the calcula­tion of the next regression model. Then, the updated regression model 
is used to forecast the next .le transfer, and so on. We refer to the set of samples used to generate 
each new re­gression model at execution time as the Running Time Window. 2We ran our linear regression 
kernel, written in C, for a total of 8000 itera­tions (in sets of 1000 iterations at various times during 
the day) with a dataset of 15 samples. The average execution time for the regression was 13.72 ms on 
a Sun SparcStation-20 with light to moderate loads from other users. Further­more, analysis of the linear 
regression technique shows that the algorithm s asymptotic running time is linear in the dataset length. 
The Running Time Window slides over the past history of net­work and application measurements at each 
new application transfer. The regression model is updated at each new sample assuring that the forecaster 
will adapt to the most recent appli­cation resource requirements and observed network behavior. At each 
update, the oldest sample pair of the Running Time Window is discarded at each new transfer. In this 
way, we .l­ter out of the regression model past history that is no longer relevant for new trends of 
system behavior. The pseudo-code of the AdRM prediction method is provided in Appendix A of this paper. 
Note that AdRM generally resembles the ARX (Auto Regression with with eXogeneous variables) modeling 
approach [22] in the sense that the ARX model structure is also based on regression. The AdRM method 
re.ts the regres­sion function at each new performance sampling, adapting to changes in the system over 
time. The sizes of the Start-up Window and Running Time Win­dows are free parameters for the AdRM method. 
While we do not, at present, have an automatic method for determin­ing these parameters from the data, 
we investigated the effect of varying the size of the Start-up Window and Running Time Window respectively. 
After simulating the forecaster for sev­eral Start-Up Window and Running Time Window lengths, we found 
that the the modeling technique remains accurate for a wide range of window sizes. The algorithm is least 
sensi­tive to the length of the Start-Up Window, since its effect is felt only initially. Figures 4 and 
5 show plots of the variation of mean absolute errors of AdRM predictions as the Running Time Window 
lengths range from 2 to 25 past history samples, for the sites analyzed in this work 3. The Start-up 
Window length is .xed at 2 initial samples. Notice that for small win­dow sizes the error is quite large 
for the majority of the sites. The error curves .atten at low error values for window sizes larger than 
5. For some windows longer than 15 samples, the AdRM method starts to exhibit a slightly higher error. 
We suspect that excessive past history, which is no longer relevant for the future trend of the performance 
behavior, is being in­corporated into the prediction. Furthermore, longer windows are undesirable since 
they would also require a larger compu­tational cost. Usually a Running Time Window length between 8 
and 20 samples will yield good predictions for the environ­ments we have investigated. For completeness 
we have in­cluded graphs which show how the mean squared error varies with Running Time Window lengths. 
in Appendix B. Figures 6 and 7 are representative of a comprehensive set of experiments executed to predict 
.le transfer times using the AdRM method for SRB and SARA respectively. In the ex­periments, SARA transferred 
.les of 3MB, at every 5 min­utes from data servers running at the University of Utah, Uni­versity of 
Illinois Urbana-Champaign and Caltech. The SRB 3In general small window lengths generated high error 
levels. To facilitate the visualization of the error variation over the window length range, we omit 
from some graphs small window size data points which generated very high errors. Davis: MAE Variation 
x Window Length Ncsa: MAE Variation x Window Length 4 0.36 0.34 3.5 0.32 3 Mean Absolute Error Mean 
Absolute Error Mean Absolute Error Mean Absolute Error 0.3 2.5 0.28 0.26 0.24 2 1.5 0.22 0.2 0.18 0.16 
0 0.45 0.4 0.35 0.3 0.25 0.2 5 10152025 Running Window Length (a) mahler.cipic.ucdavis.edu Wustl: MAE 
Variation x Window Length 1 0.5 0 0 5 10152025 Running Window Length (b) vor.ncsa.uiuc.edu Rutgers: 
MAE Variation x Window Length 0.05 0.045 0.04 0.035 0.03 0.025 0.15 0.1 0.05 0 5 10152025 Running Window 
Length (c) brainmap.arl.wustl.edu 0.02 0.015 0.01 0.005 0 5 10152025 Running Window Length (d) bionic.rutgers.edu 
 Figure 4: Variation of Mean Absolute Errors of AdRM predictions with Running Time Window length for 
data transfers from SRB sites. client transferred 16MB .les at intervals ranging from 2 to 25 hours from 
the University of California at Davis, University of Washington at St. Louis, NCSA and Rutgers. Both 
clients ran at the University of the California, San Diego. In the .gures, the differences (the error) 
between predicted (in black) and ac­tual execution times (in white) are represented by the vertical distance 
between each pair of points. The dashed lines are pre­dictions using the RBW model. In Table 1, we summarize 
the error results for this data. 4.1 Analysis To determine prediction accuracy, we use the Normalized 
Mean Absolute Error (NMAE), given by M AE M a w wwww ;Mssta tdPrrJaassuurdPrrJd atdwww . aassuurdPrrJ 
a where is the total number of predicted measurements and www aassu rdPrrJ is the mean measured data.le 
transfer per­formance for a particular site. The concept behind the Normal­ized Mean Absolute Error is 
to calculate the mean prediction error, then normalize it by the mean data.le transfer through­ www 
put which is given byaassuurdPrrJ . In other words, the error is given as a proportion or percentage 
of the average per­formance perceived by an application. In this way we provide Utah: MAE Variation 
x Window Length Mean Absolute Error Mean Absolute Error Mean Absolute Error 3 2.5 2 1.5 1 0.5 0 0 5 
10152025 Running Window Length (a) perigee.chpc.utah.edu UIUC: MAE Variation x Window Length 0.5 0.45 
0.4 0.35 0.3 0.25 0 5 10152025 Running Window Length (b) sitar.cs.uiuc.edu Caltech: MAE Variation x 
Window Length 3 2.5 2 1.5 1 0.5 0 0 5 10152025 Running Window Length (c) spin.cacr.caltech.edu Figure 
5: Variation of Mean Absolute Errors of AdRM pre­dictions with Running Time Window length for data transfers 
from SARA sites. a metric of effectiveness comparable between different appli­cations and sites with 
distinct performance characteristics. Observe that the AdRM predictions very closely track the actual 
measured performance of both SARA and SRB ap­plications. Table 1 numerically con.rms the ef.cacy of this 
method. As it can be observed, the highest relative errors (NMAE) for AdRM predictions are on the order 
of only 10%, whereas the errors for predictions using the RBW model reach up to 100%. The largest difference 
occurs for the Rutgers data server, where the RBW model resulted in an error of 101.56%, while the AdRM 
forecasting method predicted with an error of just 1.15%. Furthermore, it is important to emphasize that 
we were able to successfully obtain this high level of prediction accuracy with a low level of intrusiveness 
on the system using only small 64 kB messages to probe the network behavior. In Table 2 we show the 
mean squared errors (MSE) for RBW and AdRM. The mean squared error calculation empha­sizes large errors 
and attenuates low error samples. Again, we can see that AdRM had overall better performance when con­sidering 
the (MSE) metric. Table 3 shows the mean data.le transfer throughput for each data server.  4.2 Using 
AdRM for Scheduling The Application-Level Scheduling (AppLeS) approach in­corporates both application-speci.c 
system requirements and dynamic resource performance information to schedule dis­tributed applications 
in multi-user distributed environments [8, 31, 38]. AppLeS application-level schedulers use a perfor­mance 
model based on the application s communication and computational needs. Performance models can be represented 
by mathematical equations, in which numeric values for re­source performance forecasts are variables 
[26]. The schedul­ing agent can compare candidate schedules by evaluating the value of the performance 
equation for different resource mixes, and choose the resource combination that maximizes application 
performance. An AppLeS scheduler delays eval­uation of the model until run-time, at which point the model 
parameters are supplied by dynamic performance predictions of various system components, such as remote 
.le transfer throughput and CPU load. The AdRM method uses dynamic information provided by the Network 
Weather Service(NWS) [36, 37] and past ap­plication performance history to generate predictions of fu­ture 
.le transfer performance. For the general distributed data-intensive application scenario illustrated 
in Figure 1, the AppLeS agent would optimize overall application perfor­mance, coordinating the computational 
workload between the client and the server sites with the data transfer operations over the network. 
AdRM would provide .le transfer perfor­mance predictions which would be incorporated in the compo­sitional 
performance model used by AppLeS for its schedul­ing decisions. These .le transfer predictions would 
be inte­ NCSA U.C. Davis  2.5 3 2.5 2  Bandwidth (Mb/s) Bandwidth (Mb/s) 2 Bandwidth (Mb/s) Bandwidth 
(Mb/s) 1.5 1.5 0.5 0.5 0 0-0.5 (a) mahler.cipic.ucdavis.edu (b) vor.ncsa.uiuc.edu Washington St. Louis 
Rutgers 1.5 1  2 1.8 0.5 1.6 1.4 1.2 0.8 0.6 0 0 50 100 150 200 Time (hours) Time (hours) (c) brainmap.arl.wustl.edu 
(d) bionic.rutgers.edu Figure 6: Forecasting SRB data.le transfer behavior with the AdRM method. grated 
with other dynamic system forecasts (e.g. processor load) to parameterize the AppLeS performance model. 
Re­source performance forecasts would be provided by systems such as the Network Weather Service [36] 
or [13] developed by Dinda et. al . The initial AppLeS schedulers we have developed [5, 8, 29, 31, 38] 
have focused on the development of adaptive custom schedules for individual grid applications. We are 
currently developing AppLeS templates for scheduling structurally sim­ilar classes of grid applications. 
We are focusing on several application classes, including network-bound distributed data­intensive applications. 
  5 Conclusions and Future Work This paper presents Adaptive Regression Modeling (AdRM), a dynamic forecasting 
method to predict the performance of data transfer operations for network-bound distributed data­intensive 
applications. Our method achieves a high level of accuracy for exemplar applications SARA and SRB. AdRM 
is a useful and effective application performance prediction method with the following characteristics: 
The prediction model is derived automatically and in real­time, by using a regression model to map measurements 
of system behavior to observed application performance. AdRM dynamically adapts in time to changes in 
the en­vironment by accounting for changes in the workload and SRB MSE Utah site RBW AdRM 3.5 U.C. 
Davis 0.7536 0.0534 3 NCSA 0.3320 0.0195 W.U. St. Louis 1.2147 0.0112 2.5 Rutgers 0.8129 0.0004 2  SARA 
 MSE 1.5 site RBW AdRM 1 Utah 0.9652 0.1449 UIUC 0.8690 0.1524 RBW Measured SARA 0.5 Caltech 0.5128 
0.2975 (ARM) Predicted SARA 0 0 5 10152025 Time (hours) Table 2: Mean Squared Errors (MSE) for .le 
transfer (a) perigee.chpc.utah.edu throughput forecasts obtained directly from bandwidth mea­surements 
RBW and from the Adaptive Regression Modeling (AdRM) forecaster. These values are not normalized. UIUC 
3  SRB Mean Throughput site Mbits/s 2.5 U.C. Davis 1.4399 2 NCSA 1.0218 W.U. St. Louis 0.5659 1.5 Rutgers 
0.8792 1  SARA Mean Throughput site Mbits/s 0.5 RBW Measured SARA Utah 2.6825 (ARM) Predicted SARA 
0 UIUC 2.3577 0 5 101520 Time (hours) Caltech 3.4825 (b) sitar.cs.uiuc.edu Table 3: Mean Measured File 
Transfer Throughput values for SRB and SARA Caltech 4.5 4 system recon.gurations, which are used to parameterize 
frequent updates of the model. 3.5 . 3 Predictions can be computed with low overhead costs 2.5 the linear 
regression is applied to a sliding window of a small number of sample pairs making it possible to adapt 
quickly to changing system conditions. 2 1.5 . 1 Internal details of the underlying system are hidden. 
Ba- RBW 0.5 sically no knowledge about communication protocols, Measured SARA (ARM) Predicted SARA 0 
 network topology, or .le system con.gurations was nec­ 0 5 10152025 Time (hours) essary to achieve 
good predictions. Predictions are based on the measured effect of the system on application ex­ (c) spin.cacr.caltech.edu 
ecution performance, rather than on explicit knowledge about the system intricacies. For example, even 
though the SARA application exhibits higher .le throughput than Figure 7: Forecasting SARA .le transfer 
behavior with the the NWS measured bandwidth and exactly the opposite AdRM method. happens for the SRB 
transfers, the forecaster is able in Bandwidth (Mb/s) Bandwidth (Mb/s) Bandwidth (Mb/s) both instances 
to make accurate predictions. Several groups are working on research related to our work. Performance 
analysis and scheduling of data-intensive appli­cations are described by the ADR group from University 
of Maryland in [34] and by Thakur in [32]. However, they fo­cus on parallel data servers running over 
local area networks. Lowekamp also employs statistical techniques considering network and application 
performance information to generate data transfer predictions at application-level, though his focus 
regards local area network transfers instead of remote .le ac­cess operations over wide area networks 
[23]. Dinda et. al. have examined linear prediction models in many regimes and found such models to accurately 
predict host load performance over a wide range of system conditions [13]. Andresen et. al. study compositional 
models and performance trade-offs for distributed data-intensive applications in [4]. Performance monitoring 
and forecasting of wide-area networks is discussed in works such as the NWS [36, 37], GloPerf [19], ReMoS 
[12] and [7, 10]. The Netlogger system [33] presents a pro.ling framework for distributed storage systems. 
Many open problems remain. Some preliminary investiga­tion and experimental results indicate that the 
aggregation of 64-kilobyte network samples collected at short-term intervals increases the correlation 
level with long .le transfers. This method allows a compromise between prediction accuracy and sampling 
intrusiveness. Furthermore, for low level of correla­tion between time series, the AdRM method behaves 
as a slid­ing moving window predictor taking into account mostly past application benchmarking information. 
We want to investigate further the implications of this scenario. We intend to extend this work to account 
for data transfers performed on hierarchical storage systems such as HPSS [21], including systems with 
tertiary storage (tapes). We also hope to be able to use the AdRM method to generate network fore­casts 
for varied communication protocols and con.gurations and accurate relative ranking among several data 
servers. In extending the scope of this work to new scenarios, we also in­tend to look at the possibility 
of on-demand re.nement of the regression model to include additional factors, such as disk or tape behavior 
and server load, in order to do a better job of assessing end-to-end .le transfer, and hence application, 
per­formance.  Acknowledgements The authors would like to thank NPACI researchers Reagan Moore, Chaitanya 
Baru, Arcot Rajasekar and Michael Wan for their invaluable help and for providing us access to the SRB 
in­frastructure, their constant support, and insightful comments. The SARA application was developed 
by Roy Williams at Cal-Tech and George Kremenek at SDSC, both of whom helped immensely in our experiments 
with SARA. We are also very grateful for the important ideas resulting from discussions with our colleagues 
in the AppLeS group. Essential help was also provided by Jim Hayes in adding new capabilities to the 
NWS, necessary to run our experiments. Finally, we would like to thank the NPACI sites that provided 
the data servers for this work, as well as the anonymous reviewers who provided help­ful and substantive 
comments.  References <RefA>[1] Alexandria Digital Library Project webpage at http://alexandria.sdc.ucsb.edu/. 
 [2] Art museum image consortium (amico) webpage at http://www.amico.org/home.htmland http://www.npaci.edu/DICE/AMICO/. 
 [3] A. Amoroso, K. Marzullo, and A. Ricciardi. Wide-Area Nile: A Case Study of a Wide-Area Data-Parallel 
Application. In ICDCS 98 -International Conference on Distributed Comput­ing Systems, 1998. ¨ Adaptive 
Partitioning and Scheduling for Enhancing WWW Application Performance. Journal of Parallel and Distributed 
Computing, 1998. [4] D. Andresen, T. Yang, O. H. Ibarra, and Omer E.gecio.glu. [5] AppLeS webpage at 
http://www-cse.ucsd.edu/groups/hpcl/ apples. [6] C. Baru, R. Moore, A. Rajasekar, and M. Wan. The SDSC 
Storage Resource Broker. In IBM CASCON 98, 1998. [7] S. Basu, A. Mukherjee, and S. Klivansky. Time Series 
Models for Internet Traf.c. IEEE Comput. Soc. Press, 1996. [8] F. Berman, R. Wolski, S. Figueira, J. 
Schopf, and G. Shao. Application level scheduling on distributed heterogeneous net­works. In Proceedings 
of Supercomputing 1996, 1996. [9] G. E. P. Box, G. M. Jenkins, and G. C. Reinsel. Time Series Analysis 
 Forecasting and Control. Prentice Hall, 1994. [10] R. L. Carter and M. E. Crovella. Dynamic Server Selection 
Using Bandwidth Probing in Wide-Area Networks. Techni­cal Report TR-96-007, Computer Science Department, 
Boston University, 1996. [11] S. Chatterjee and B. Price. Regression Analysis by Example. John Wiley 
&#38; Sons, Inc., 1991. [12] T. DeWitt, T. Gross, B. Lowekamp, N. Miller, P. Steenkiste, J. Subhlok, 
and D. Sutherland. ReMoS: A resource moni­toring system for network-aware applications. Technical Re­port 
CMU-CS-97-194-REVISED, School of Computer Sci­ence, Carnegie Mellon University, December 1998. [13] P. 
A. Dinda and D. O Hallaron. An evaluation of linear models for host load predictions. In Proc. 8th IEEE 
Symp. on High Performance Distributed Computing, 1999. [14] A. L. Edwards. An Introduction to Linear 
Regression and Cor­relation. W. H. Freeman and Company, 1984. [15] UC Berkeley Digital Library Project 
webpage at http://elib.cs.berkeley.edu/. [16] U. Fayyad and R. Uthurusamy. Data Mining and Knowledge 
Discovery in Databases. Communications of the ACM, 1996. [17] R. Ferreira, B. Moon, J. Humphries, A. 
Sussman, J. Saltz, R. Miller, and A. Demarzo. The Virtual Microscope. In Proc. of the 1997 AMIA Annual 
Fall Symposium, 1997. [18] I. Foster and C. Kesselman. The Grid: Blueprint for a New Computing Infrastructure. 
Morgan Kaufmann Publishers, Inc., 1998. [19] GloPerf webpage at http://www-fp.globus.org/details/ gloperf.html. 
[20] R. Grossman, S. Kasif, R. Moore, D. Rocke, and J. Ull­man. Data Mining Research: Opportunities and 
Chal­lenges -A Report of three NSF Workshops on Mining Large, Massive, and Distributed Data, 1998. Avail­able 
at http://www.ncdm.uic.edu/M3D-final­report.htm. [21] High Performance Storage System webpage at http://www.sdsc.edu/hpss/hpss1.html. 
[22] L. Ljung. System Identi.cation -Theory for the User. Prentice Hall, second edition, 1999. [23] B. 
Lowekamp, D. O Hallaron, and T. Gross. Direct net­work queries for discovering network resource properties 
in a distributed environment. In Proc. 8th IEEE Symposium on High-Performance Distributed Computing (HPDC-8), 
Re­dondo Beach, CA, Aug. 1999. [24] NARA Project webpage at http://www.sdsc.edu/NARA/. [25] B. R. Schatz. 
High-Performance Distributed Digital Libraries: Building the Interspace on the Grid. In Proc. 7th IEEE 
Symp. on High Performance Distributed Computing, 1998. [26] J. Schopf and F. Berman. Performance prediction 
in produc­tion environments. Proceedings of the IPPS/SPDP Conference, April 1998. [27] J. M. Schopf. 
Performance Prediction and Scheduling for Parallel Applications on Multi-User Clusters. PhD thesis, University 
of California, San Diego, 1998. Also available as UCSD CS Dept. Technical Report, Number CS98-607, http://www.cs.nwu.edu/ 
jms/Thesis/ thesis.html. [28] J. M. Schopf and F. Berman. Stochastic schedul­ing. In Supercomputing 99 
(to appear), 1999. Also avilable as Northwestern University, Computer Science Department Technical Report 
CS-99-3, or http://www.cs.nwu.edu/ jms/Pubs/ TechReports/sched.ps. [29] N. Spring and R. Wolski. Application 
level scheduling of gene sequence comparison on metacomputers. In Proc. 12th ACM International Conference 
on Supercomputing, Jul 1998. [30] SDSC s Storage Resource Broker project webpage at http://www.npaci.edu/DICE/SRB/ 
index.html. [31] A. Su, F. Berman, R. Wolski, and M. M. Strout. Using AppLeS to schedule simple SARA 
on the computational grid. Interna­tional Journal of High Performance Computing Applications, 13, 1999. 
[32] R. Thakur, W. Gropp, and E. Lusk. A Case for Using MPI s Derived Datatypes to Improve I/O Performance. 
In Proc. of SC98: High Performance Networking and Computing, 1998. [33] B. Tierney, W. Johnston, B. Crowley, 
G. Hoo, C. Brooks, J. Lee, D. Gunter, and S. Kim. The NetLogger Methodology for High Performance Distributed 
Systems Performance Analysis. In Proc. 7th IEEE Symp. on High Performance Distributed Com­puting, 1998. 
[34] M. Uysal, T. Kurc, A. Sussman, and J. Saltz. A Performance Prediction Framework for Data Intensive 
Applications on Large Scale Parallel Machines. In Proc. of 4th Workshop on Lan­guages, Compilers, and 
Run-time Systems for Scalable Com­puters. 1998. [35] R. Williams. Caltech s Synthetic Aper­ture Radar 
Atlas project webpage at http://www.cacr.caltech.edu/ roy/sara/ index.html. [36] R. Wolski. Dynamically 
Forecasting Network Performance to Support Dynamic Scheduling Using the Network Weather Service. In Proc. 
6th IEEE Symp. on High Performance Dis­tributed Computing, August 1997. [37] R. Wolski, N. Spring, and 
J. Hayes. The network weather service: A distributed resource performance fore­casting service for metacomputing. 
Future Generation Computer Systems (to appear), 1999. available from http://www.cs.utk.edu/ rich/ publications/nws-arch.ps.gz. 
[38] D. Zagorodnov, F. Berman, and R. Wolski. Application Scheduling on the Information Power Grid. to 
appear in In­ternational Journal of High-Performance Computing, 1998.</RefA>   A AdRM Pseudo-Code AdRM Predictor( 
Wi n Sd t a r , Wi n R ) If (Initialization) Then S n m pult R li g Get NWS Bandwidth History(tAo : 
tAo ++ Wi n Sd t a ) S n m pult R B W g Get Application Performance History(tAo : tAo ++ Wi n Sd t a 
r )tAL g Wi n S t p /* Time of Last Sampling */ Else /* Running Time */ S n m pult Rli g Get NWS Bandwidth 
History(tL s Wi nR w : tL ) S n m pult RB W g Get Application Performance History(tL - Wi nR : tL ) 
End If = SWWs S GAG e gS n m pult R li S n m pult R B W Calculate Linear Regression Function( , ) /* 
Use previously calculated linear regression model to * * predict the file transfer throughput performance 
* * perceived at application-level */ New Bandwidth Sample = NWS Get Bandwidth Sample()  g = SWWs S 
GAG e Predicted Performance (New Bandwidth Sample) /* Prepare for next prediction by storing a bandwidth 
* * sample and updating the "last sample" timestamp. * * Application history is stored by the AppLeS 
* * scheduler, which is calling AdRM_Predictor. */ Store NWS Bandwidth History(New Bandwidth Sample) 
 g tAL t r Return(Predicted Performance) END                    B Variation of MSE 
with Running Time Window Ncsa: MSE Variation x Window Length Davis: MSE Variation x Window Length 0.9 
0.4 0.8 0.35 0.7  0.2 0.1 0.1 0 0 5 10152025 0 5 10 15 20 25 0.05  Running Window Length Running Window 
Length (a) mahler.cipic.ucdavis.edu (b) vor.ncsa.uiuc.edu Wustl: MSE Variation x Window Length 6 Rutgers: 
MSE Variation x Window Length 0.03 5 0.025 Mean Squared Error Mean Squared Error Mean Squared Error 0.3 
0.6 0.5 0.4 0.25 0.2 0.3 0.15  Mean Squared Error 4 3 2 0.02 0.015 0.01 1 0.005 0 0 Running Window Length 
 (c) brainmap.arl.wustl.edu (d) bionic.rutgers.edu Figure 8: Variation of Mean Squared Errors of AdRM 
predictions with Running Time Window length for data transfers from SRB sites. Utah: MSE Variation x 
Window Length UIUC: MSE Variation x Window Length 1.4 2.5 1.3 1.2 2 Mean Squared Error 1.1 1 1.5 0.9 
0.6  0.5 0.5 0.4 0 0 5 101520250 5 10152025 Mean Squared Error Running Window Length Running Window 
Length (a) perigee.chpc.utah.edu (b) sitar.cs.uiuc.edu Caltech: MSE Variation x Window Length 7 0.8 1 
0.7 Mean Squared Error 6 5 4 3 2 1  0 0 5 10152025 Running Window Length (c) spin.cacr.caltech.edu 
Figure 9: Variation of Mean Squared Errors of AdRM predictions with Running Time Window length for data 
transfers from SARA sites.  
			
