
 A locality-sensitive hash for real vectors Tyler Neylon Abstract We present a simple and practical 
algorithm for the c-approximate near neighbor problem (c-NN): given n points P . Rd and radius R, build 
a data structure which, given q . Rd , can with probability 1 - d return a point p . P with dist(p, q) 
= cR if there is any p * . P with dist(p * ,q) = R. For c = d + 1, our algorithm determin­istically (d 
= 0) preprocesses in time O(nd log d), space O(dn), and answers queries in expected time O(d2); this 
is the .rst known algorithm to deterministically guarantee an O(d)-NN solution in constant time with 
respect to n for all ip metrics. A probabilistic version empirically achieves useful c values (c< 2) 
where c appears to grow minimally as d .8. A query time of O(d log d) is available, provid­ing slightly 
less accuracy. These techniques can also be used to approximately .nd (pointers between) all pairs x, 
y . P with dist(x, y) = R in time O(nd log d). The key to the algorithm is a locality-sensitive hash: 
a mapping h : Rd . U with the property that h(x)= h(y) is much more likely for nearby x, y. We introduce 
a somewhat regular simplex which tessellates Rd , and e.ciently hash each point in any simplex of this 
tessellation to all d +1 corners; any points in neighboring cells will be hashed to a shared corner and 
noticed as nearby points. This method is completely independent of dimension reduction, so that additional 
space and time savings are available by .rst reducing all input vectors. 1 Introduction. In this paper, 
we focus on a practical variation of the traditional nearest neighbor search problem (NNS): given points 
P . Rd and query point q . Rd, .nd p . P which minimizes dist(p, q). The usual approach is to preprocess 
P so that a later query can quickly retrieve the closest point. Although much work has been done to e.ciently 
solve NNS, the known non-probabilistic approaches thus far are not much more e.cient than a brute force 
linear search over P , except in small dimensions d (see [15] for approaches in small d). Therefore, 
practical algo­rithms have been developed which can e.ciently solve probabilistic approximate versions 
of NNS. These less exact variations still support many applications, includ­ing data mining, information 
retrieval, media databases (such as images or video), pattern recognition, and du­plicate detection. 
For example, a video database may want to avoid the insertion of videos which are slightly this case, 
we expect a near-duplicate video q to be many times closer to a particular point in P than any other, 
so a fast approximate algorithm is very useful. The authors of [11] show how to reduce a probabilis­tic 
approximate version of NNS to a version in which the allowed distances between p and q are restricted 
by a .xed parameter R. In particular, let us de.ne the c-near neighbor problem (c-NN) as follows: Definition 
1.1. Given points P . Rd, radius R> 0, and probability tolerance d> 0, preprocess P so that for any q 
. Rd, we can, with probability at least 1 - d, .nd p . P with dist(p, q) < cR whenever there is a pi 
. P i with dist(p,q) <R. The value c = 1 is the approximation factor; ideally we can solve this for c 
= 1+ E, where both error terms E and d are small. The exact value of R is immaterial since input points 
can easily be scaled to match the .xed radius of any particular c-NN implementation. The work in [11] 
also introduced the idea of a locality-sensitive hash a hash function h with the property that h(x)= 
h(y) is more likely for nearby points x, y. A good locality-sensitive hash can serve as a basis for solving 
c-NN, and thus for approximate approaches to NNS as well. Since [11], a number of such hashes have been 
proposed for use on various point sets (including strings [13] and families of subsets [3]), and in various 
metrics (cf. [7], [2], [8]). In this paper, we focus on solving c-NN in Rd un­der the £p norm for p . 
[1, 8] (equation (2.6)), with emphasis on £2 (equation (2.12)). Previous work has centered around reducing 
the time complexity depen­dence on n = |P |. However, locality-sensitive hashes more directly solve c-NN 
than variants of NNS, and lend themselves to approaches more sensitive to dimen­sion d, accuracy c, and 
certainty 1 - d, than to speed in terms of n. We propose that any advantages previ­ously conveyed by 
smaller theoretical exponents on n can be expressed in the form of better speed, proba­bility of success, 
and approximation factors within this framework. altered versions of existing data, and may do so by 
solv­ 1.1 Anatomy of a locality-sensitive hash A typ­ ing an approximate version, such as c-NN (de.ned 
be­ ical locality-sensitive hashing scheme on Rd consists low), on real vector representations of the 
videos. In of three components: dimension reduction, the local­  ity hash, and ampli.cation. It is widely 
utilized that a random projection of n points in Rd to a smaller space Rt , t = O(log n), is likely to 
preserve interpoint dis­tances ([7], [12]). Thus we can apply a locality hash to a reduced vector with 
good results. In addition, most locality hashes improve with some form of ampli.cation a repetition 
of some portion of the hashing scheme which improves the accuracy. Ex­amples include using multiple hash 
tables with slightly di.erent locality hash functions, or performing multi­ple queries via random points 
close to the given q . Rd ([14]). The locality hash we present in §2 is independent of either dimension 
reduction or ampli.cation, so that both of these techniques can be used to augment the base performance. 
This paper focuses on the locality hash itself; we do not directly address the question of how to optimally 
combine the hash with these tech­niques. 1.2 Our contributions We present two versions of a simple and 
general algorithm based on a tessellation of Rd by simplices (a simplex is the d-dimensional generalization 
of a triangle the convex hull of d +1 points in general position). In each case, any given point is 
hashed to all d + 1 corners of the cell (one simplex) in the tessellation which contains the point. In 
this way, we can be sure to match any two points in neighboring cells of the tessellation. By using simplices 
instead of hypercubes, we can work with O(d) corners instead of O(2d); thus we confront the curse of 
dimensionality. We can achieve storage in time O(d log d) per point, and lookup in expected time O(d2), 
or O(d log d) at some accuracy cost as explained in §2.4. Our main theoretical result (in §2) is to show 
that one version of our hash deterministically (d = 0) solves c-NN with c =2d in the £p norm for any 
p . [1, 8], and that the other version gives an approximate factor c = d + 1 for £2. Previous algorithms 
([4]) can solve c-NN in query time independent of n, with c = O(d) in £1, and in select other £p metrics 
via embedding ([5], [9], [10]). To our knowledge, this is the .rst proven to achieve c = O(d) for all 
£p, p . [1, 8]. We also give empirical evidence (in §3) that the algorithm is practical as a probabilistic 
approach to c-NN for smaller c on a wide range of dimensions. For example, using arti.cial data, we approximate 
the quantity ß.1, an estimated upper bound on c for d =0.1 (90% con.dence). In table 1, we see that this 
algorithm achieves ß.1 =1.6 in R300 with at least 90% con.dence, without dimension reduction. d = hash 
Lt = 10 5 100 5 300 5 10 d + 1 20 d + 1 here pstable [7] sphere grid [2] unary [8] 1.6 5.6 6.0 8.3 1.7 
4.7 5.1 6.0 1.6 5.1 5.1 5.6 1.5 3.9 4.1 4.4 1.5 2.8 3.1 3.3 Table 1: Empirical estimates of ß.1 = D.05/D.95, 
further explained in §3. 2 The algorithm In this section, we describe a general class of hash algorithms 
which provide guarantees of .nding close enough neighbors while excluding those su.ciently far apart. 
Next we describe a particular version of this algorithm, which we refer to as the orthogonal version, 
which is easy to implement and analyze. Finally we present an improved version, the vertex-transitive 
version, along with some thoughts towards improved time and space complexity. 2.1 The general method 
In this section, we sup­pose that we have a partition P which is a family of subsets of Rn with the following 
properties: (i) Each el­ement c .P, called a cell, is the convex hull of .nitely­many points; (ii) Rn 
= .P, while the intersection of any two cells has zero measure; and (iii) any bounded region contains 
.nitely many cells. The general frame­work of our algorithm, described next, works for any such partition. 
The partition algorithm Based on such a par­tition, we are now ready to de.ne a general locality­sensitive 
hashing algorithm. The idea is to build a map from each corner of the partition to a list of all data 
points in adjacent cells. Conceptually, start with a fast­lookup mapping (such as a hash map) from every 
cor­ner of every cell to an empty linked list (sparse, created lazily). For point x . Rd, choose a cell 
c .P containing x, and append (a pointer to) x to the list mapped from each corner of c. This process 
can be repeated for an arbitrary number of points, at each step augmenting the lists mapped to by each 
corner. We say that two points x, y collide in this hash if any corner point maps to both x and y; write 
this as x ~ y. This general algorithm is also outlined in the pseudocode in .gure 1. In practice, the 
result of a query will be a set of lists of points; it may su.ce to simply work with the .rst point found, 
avoiding the time required to parse all the points. An advantage of this algorithm is that it determin­istically 
guarantees that all close enough points are con­sidered nearby, while all far enough are not. The chal­ 
 def locality hashes(x): cell c = cell containing(x) return corners(c) def preprocess points(P): map 
= {} # An empty mapping. for p in P: for h in locality hashes(p): map[h].append(p) return map def lookup 
query(map, q): neighbors = [] # An empty list. for h in locality hashes(q): neighbors.append(map[h]) 
return neighbors Figure 1: The general partition-based LSH algorithm lenge from here is to preserve speed 
by choosing a good partition. A naive choice of P, such as all unit hyper­cubes cornered at integer points, 
can result in many corners (2d) per cell, which in turn requires running time exponential in d. Accordingly, 
we will now con­sider how our choice of P a.ects the performance of this algorithm. Time and space complexity. 
The resources used by this algorithm are dominated by the calls to locality hash. Suppose one call to 
locality hash takes time t and returns an object of size s = O(t). Our preprocessing time of n points 
then takes O(nt) time, and O(ns) space. If we do not bother to traverse the neighbors list and simply 
return pointers to these lists, then a single lookup also takes time O(t ). When we .ll in the details 
below, (§2.4) we ll see that we can achieve t = d log d and s = d, where d is the dimensionality of our 
points. Unfortunately, verifying the identity of each hash key requires O(d) time, so that queries (but 
not storage) are actually O(d2). If we re willing to probabilistically verify hash keys at the cost of 
some false collisions, we can still achieve O(d log d) lookups. Accuracy. Next we d like to give some 
useful char­acterizations concerning which pairs of points are, or are not, considered nearby neighbors 
of this algorithm. Our main goal will be to discover, for a given partition P, the optimal values D0 
and D1 such that dist(x, y) >D0 . x ~ y and dist(x, y) <D1 . x ~ y. This algorithm solves c-NN where 
c = D0/D1 and d = 0. The notation here is inspired by the idea, expanded below, that Dp, for p . (0, 
1), can represent the distance at which dist(x, y)= Dp . P (x ~ y)= p in a certain probabilistic context. 
We ll work with a particular type of partition which is more conducive to analysis. A sliced partition 
is a division of Rd into cells created by cutting the space along a series of parallel hyperplanes so 
that every cell is bounded by exactly two hyperplanes in each direction. We further require that the 
hyperplanes be oriented in .nitely-many directions, and that every bounded subset of Rd intersects .nitely-many 
of the hyperplanes. It is not hard to verify that a sliced partition is a special case of our general 
partition described above. As a quick example, the hypercube partition in which we cut Rd = {x =(x1,x2,...,xd)} 
along every hyperplane xi = j; i . [d],j . Z is a canonical example of a sliced partition (notation: 
[d] := {1, 2,...,d}). We can e.ectively de.ne D1 = D1(P) as the quantity (2.1) D1 := inf{dist(x, y): 
x ~ y; x, y . Rd}. Let cx ~ cy indicate that cells cx and cy share a boundary point. Toward characterizing 
D1 in terms of the cells of P, we ll say that points x, y are barely neighbors when x ~ y yet there are 
cells cx,cy with x . cx, y . cy, and cx ~ cy. This may occur when x, y are in the boundary of neighboring 
cells, or the same cell, and this boundary is shared with two cells which are not neighbors of each other. 
Lemma 2.1. Given any points x ~ y, there are barely ii neighboring points x~ ybetween x and y along the 
line segment xy. We defer this proof and others in this section to the appendix. This last lemma clari.es 
that (2.2) D1 = inf{dist(x, y): x, y are barely neighbors}, so that, in .nding D1, we may focus exclusively 
on such point pairs. Combining this fact with the next lemma will allow us to gain a powerful tool for 
extracting D1 from the shape of the cells in any sliced partition. Let corners(c) denote the minimal 
set of points whose convex hull is the cell c. We will say that points x, y cross a cell c i. x, y . 
c and there are disjoint subsets Cx,Cy . corners(c) with x . convex(Cx) and y . convex(Cy). Intuitively, 
we can think of this notion as stipulating that any edge path from x to y must contain at least one full 
edge.  Figure 2: The orthogonal partition O in R2 Lemma 2.2. Suppose x, y are points in cell c in sliced 
partition P. Then x, y are barely neighbors . x, y cross c. Lemma 2.1 allowed us to restrict the de.nition 
(2.1) of D1 to the reduced form of (2.2). With lemma 2.2, we can move one step further, in sliced partitions, 
to (2.3) D1 = inf{dist(x, y): x, y cross some cell c}. In particular, if all the cells of a sliced partition 
P are isometric, then D1 is exactly the minimum distance of any points which cross the canonical cell. 
We can now formally de.ne D0 = D0(P) := sup{dist(x, y): x ~ y; x, y . Rd}. The next result summarizes 
the main points we will use from this section. Property 2.3. If P is a sliced partition with isomet­ric 
cells, then D1 = min{dist(x, y): x, y cross c}, where c is the canonical cell. Moreover, if there ex­ist 
colinear points x, y, z such that xy is the diame­ter of one cell, and yz the diameter of another, then 
D0 = 2 max{dist(x, y): x, y . c} =2 diam(c). Applying this result to hypercubes, we see that v D0 =2 
d, and D1 = 1in £2. But each hypercube has 2d corners, which would lead to exponential time complexity. 
The next two sections are devoted to .nding the values of D0,D1 for two sliced partitions whose canonical 
cell is the convex hull of d + 1 corners, allowing for much faster algorithms. 2.2 The orthogonal partition 
O We de.ne the or­thogonal partition, denoted by O, as the sliced parti­tion given by the slices xi = 
z : z . Z,i . [d], and xi - xj = z : z . Z,i = j . [d], in Rd = {x : x =(x1,x2,...,xd)}. We will see 
below that all the cells in this partition are isometric, so that we can determine D0 and D1 from a single 
cell, using property 2.3. To begin, notice that this partition is a re.nement of the hypercube partition; 
it contains strictly more slices. Also notice that every integer-cornered hypercube is sliced in the 
same way, so we may conveniently focus on the partition of [0, 1]d . Given any permutation p : [d] . 
[d], we can de.ne the set Sp := {x :0 = xp(1) = xp(2) = ··· = xp(d) = 1}. Each inequality de.nes a halfspace; 
since Sp is a bounded, nondegenerate intersection of d + 1 halfspaces in Rd, it is a simplex. For any 
permutations p1 = p2, the set Sp1 n Sp2 has measure zero. It is also clear that .pSp = [0, 1]d . These 
simplices are precisely the cells in [0, 1]d of the orthogonal partition. We also note that, for any 
permutations p1 = p2, the mapping y = f(x) on Rd which follows yp2(i) = xp1(i), is an isometry (in any 
£p) mapping Sp1 . Sp2 . Thus all d! cells in the hypercube are isometric, and since the same decomposition 
of the cube is repeated throughout space, all cells of the entire partition are isometric. Hence we can 
focus on a particular cell to study. We ll choose Sid, where id is the identity permutation, and Sid 
is the cell for which 0 = x1 = x2 = ··· = xd = 1. This cell is cornered by the points fi := (0, 0,..., 
0, 1, 1,..., 1), for i =0,...,d. . . .. . . d-ii Whenever a = b = c, we have Ufc,fa) = Ufb,fa) so that 
Ufc - fb,fa) = 0. Now consider any triangle of corners fi,fj ,fk with i<j<k. Then Ufk-fj ,fj -fi) = Ufk 
- fj,fj )-Ufk - fj,fi) =0 - 0 = 0. Every boundary triangle in every cell of this partition is a right 
triangle this is why we call it the orthogonal partition. D1(O) Next we ll .nd the value of D1(O) in 
any £p norm. To begin, we ll need the following elementary Lemma 2.4. If (xi)d (yi)d are nondecreasing 
se­ i=1, i=1 quences with min(xi,yi) = max(xi-1,yi-1), then ||x - y||1 = max(xd,yd) - min(x1,y1). Proof. 
Let ai = min(xi,yi) and bi = max(xi,yi). d d Then ||x - y||1 =(bi - ai) =(bi - bi-1)+ i=1i=2 (b1 - a1)= 
bd - a1. D We re now ready for the main result of this section, Theorem 2.5. D1(O)= d1/p-1 in £p Proof. 
We ll begin by showing that (2.4) D1 = 1 in £1. Since ||x||p = d1/p-1||x||1, for any x . Rd, this will 
give us D1 = d1/p-1 (2.5) in £p. Suppose that x, y cross Sid. Then there are distinct corner sets Fx,Fy 
.{f0,f1,...,fd} whose convex hulls contain x and y, respectively. We can see that each point can also 
be viewed as a nondecreasing sequence, and that xi <xi+1 . yi = yi+1 and yi <yi+1 . xi = xi+1, since, 
e.g., xi <xi+1 . fd-i . Fx, so that zi = zi+1 for any point z . convex(Fy), including y. Our points x, 
y meet the conditions of lemma 2.4. Since x, y cross Sid, we must have min(x1,y1)=0, max(xd,yd)= 1, and 
thus ||x - y||1 = 1. By property 2.3, this gives us (2.4).  It remains to be seen that we can .nd cross­ing 
points x, y . Sid which actually achieve the lower bound given by (2.5). To do so, let x = (1/d) · (1, 
1, 3, 3, 5, 5,..., 2id/2l- 1) and y = (1/d) · (0, 2, 2, 4, 4,..., 2ld/2J). Then x . convex(Fx), where 
fi . Fx i. the parity of i matches that of d, and y . convex(Fy), with Fy the complement of Fx. So x, 
y do indeed cross Sid. We also have x - y = (1/d) · d1/p-1 (1, -1, 1, -1,..., ±1), and ||x - y||p = , 
which completes the proof. D D0(O) Each cell in [0, 1]d contains the line segment from 10 to 11, which 
is a diameter of the hypercube and hence also of each cell in any £p norm. We can also clearly see 
that this diameter does occur in a colinear fashion as required by property 2.3, so that D0(O)=2d1/p 
in £p. 1 We can summarize these results as D0(O)= 2 1 in £8 and D1(O)=1 in £1. Recall that D0/D1 gives 
us the value of the ap­proximation factor c for the non-probabilistic version of c-NN. We can summarize 
our .ndings thus far by the surprisingly simple equation (2.6) c = D0/D1 =2d in £p, for any p . [1, 8] 
for the orthogonal partition. So far, we ve examined the theoretical deterministic (d = 0) bounds provided 
by the orthogonal partition. In order to turn this into actual code, we still need to check how feasible 
it is to compute the corners of a cell containing any given point. We do this in the next section. Computing 
locality hashes(x) in the orthog­onal partition In this section we suppose there is a point x . Rd for 
which we want to compute locality hashes(x). We do this by implicitly .nd­ing cx = cell(x), the cell 
in O containing x; and then .nding the set corners(cx), which is the ultimate out­put of locality hashes 
needed for the overall locality sensitive hash. Any point x =(x1,x2,...,xd) . Rd can be decomposed as 
x = xint + xfrac, where xint = (lx1J, lx2J,..., lxdJ). Since every (Zd-cornered) hy­percube looks the 
same in O, the corners of cx are ex­actly xint + corners(cell(xfrac)). Thus it su.ces to .nd def locality 
hashes(x): a corner = x int = map(math.floor, x) corners = [] # an empty list add corner(corners, a corner) 
p = sort as permu(x -x int) d = len(x) for i in 1..d: a corner[p[d-i]] += 1 add corner(corners, a corner) 
return corners Figure 3: The orthogonal partition s algorithm Figure 4: A rotated snapshot of V in R2 
 a grid of equilateral triangles the corners of cell(x) for any x . [0, 1]d . Suppose x . Sp for some 
permutation p :[d] . [d]. This means that (2.7) xp(1) = xp(2) = · ·· = xp(d). Our job is to .nd this 
permutation based on the value of x. In other words, we simply have to sort the coordinates of x, where 
we think of sorting x as .nding the permutation p which puts the coordinates in ascending order. Once 
we know p, the corners are those of [0, 1]d which follow (2.7). Let ei denote the canonical unit basis 
vector ei = (0, 0,..., 0, 1, 0, 0,..., 0). Then i-1 d-i corners(Sp)= {10,ep(d),ep(d) + ep(d-1),...,1 
}. The pseudocode in .gure 3 computes locality hashes(x) for a point x given as a list of coordinates. 
2.3 The vertex-transitive partition V In .gure 2, we saw the R2 version of partition O, in which each 
cell is an isosceles right triangle. Intuitively, one may be tempted to improve this partition by searching 
for even more regular cells, since this would seem to give a lower D0/D1 ratio. The next partition we 
discuss achieves this type of improvement. We can de.ne the vertex-transitive partition, de­noted V, 
as a particular linear transformation of O. For any dimension d, let matrix T = T (d) have diag­  v 
onal elements (1 + (d - 1) d + 1)/d and o.-diagonals v (1 - d + 1)/d. Then V is characterized by the 
slices Th for each hyperplane h which is a slice of O. We say that a polytope p is vertex transitive 
i., for every pair of vertices v1,v2, there is an isometry . : p . p with .(v1)= v2. The following fact 
is proved in the appendix: Property 2.6. In £2, the cells of V are isometric, and each is a vertex-transitive 
simplex. It is interesting to note that, beyond R2, regular simplices cannot tessellate Rd , so that 
this may be considered a reasonable compromise in which some form of regularity is maintained within 
an isometric, simplicial tiling of space. In the following analysis, we restrict ourselves to £2. Since 
V is isometric, we can focus on a single particular cell to study we choose T (Sid). The corners of 
this cell are the points pi := T (fi). This means that (2.8) i v t pi =1 + d +1-i, . . . , -i, d - i, 
. . . , d - i, d d-i times i times from which it is a straightforward computation to see that Upi,pj) 
= i(d +1 - j) for i = j, and that ||pi-pj||2 = |j-i|(d+1-|j-i|). This surprisingly simple 2 formula reveals 
that the diameter of T (Sid) is clearly given between any two points pi,pj which approximate j - i =(d 
+ 1)/2: __ _ _ dd d + 1 odd d, D0(V)=2+1= 22 d(d + 2) even d. We will invoke a general lemma giving the 
shortest distance from any face of T (Sid) to the convex hull of the other vertices. By minimizing over 
all faces, this allows us to .nd the shortest distance between any points which cross the cell; this 
distance is exactly D1(V). The following lemma deals with subsets of vertices whose indices are in Nd 
:= {0, 1,...,d}. It will be useful to consider the indices as residue classes modulo d + 1. To this end, 
we will extend the usual interval notation [a, b) . Nd to allow the case a>b, de.ned by: i . [a, b) i. 
i . Nd and (a = i or i<b). Lemma 2.7. Suppose A . Nd and A = .k [ai,bi), i=1 written so that k is minimized 
(i.e., there are no contiguous or overlapping intervals). 1 Then the points xA := i pai + pbi-1 (mod 
d+1) 2k 1 and xB := i pbi + pai-1 (mod d+1) minimize the 2k distance between convex({pi}i.A) and convex({pi}i.B). 
d+1 Furthermore, ||xA - xB||2 = . 22k 2k Proof. Let y := (xA - xB). The .rst goal of d+1 the proof is 
to see that (2.9) y . pi - pj when either i, j . A or i, j . B. This su.ces to show that xAxB is the 
shortest line segment between the two a.ne linear subsets. In order to decompose y, we introduce the 
notations e-i := ed+1-i and e0 := -11. We will write ti for T (ei), and t(2) for T 2(ei). Observe that 
(d + 1)y = - ii pai pai-1-(pbi -pbi-1)= Tv(e-ai -e-bi )= t-ai -t-bi . Observe that T = d + 1(I - µJ), 
where µ = v (1 - 1/d + 1)/d and J is the all-one matrix, so that T 2 =(d + 1)I - J. Then, for i> 0, 
d t (2) Ut-i,pj ) = Ue-i,T 2fj) =e-i ,t kk=d+1-j (2.10) = (d + 1)(i = j) - j, where the notation n(boolean) 
denotes value n if the boolean is true, 0 otherwise. Using (2.8), we also have Ut0,pj) = U-1 ,pj) = -j. 
Using this, we can re-write (2.10) to include the i =0 t case as Ut-i,pj) =(d + 1)j . [i, 0) - j, where 
[0, 0) is interpreted as the empty set. 1 Let gi := t-ai - t-bi . Then Ugi,pj) = 1(j . d+1 [ai,bi)) - 
1(0 . [ai,bi)). Let w := #{i :0 . [ai,bi)}. 1 Notice that y = i gi, so that d+1 (2.11) 1 t 1 - w if 
j . A Uy, pj ) = Ugi,pj ) = d +1 -w if j . B. i From this, we have that Uy, pi - pj) = 0 whenever i, 
j . A or i, j . B. This .nishes the demonstration of (2.9), the .rst half of this proof. Next we con.rm 
the distance between xA and xB. Using (2.11), (d + 1)Uy, y) = iUy, (pai + tp2 bi-1) - (pbi + d+1 d+1 
pai-1)) =2k. Thus ||xA - xB||2 =Uy, y) = , 22k 2k which concludes the proof. D The lemma tells us that 
we can achieve the mini­mum distance by maximizing k, the number of intervals needed to express A (or 
B). For instance, we could al­ways choose A = {i . Nd : i is even}, in which case k = id/2l. Hence D1(V)=(d 
+ 1)/d for even d, and 1 for odd d. Combining this with D0(V), we see that, in £2, d +1 if d is odd, 
D0 + (2.12) c == 1 D1 d 1+ if d is even. d+1 We always have c = d + 1, so that the guaranteed locality-sensitive 
hashing accuracy of V is essentially twice as good as that of O; although the accuracy of V has only 
been veri.ed in £2, whereas our analysis of O applies for any £p.  Computing locality hashes(x) in V 
We could summarize the de.nition of this partition via V = T (O). Accordingly, if locality hashes in 
O(x) com­putes the corners of the cell in O containing x, then locality hashes in O(T -1x) does the same 
for V. T -1 v 1 We have = I + µJ, where µ = d+1 v 1 1 - v 1 . If y = T -1x, then yi = xi/d + 1+µsx, 
d+1d where sx = i xi. This is computable in O(d) time. 2.4 Time and space complexities As in §2.1, we 
will use s to denote the size of the return value of locality hashes; and t to denote the time needed 
for computation. The algorithms presented here need at least t = O(d log d) worst-case time in order 
to sort the coordinates of xfrac, as described in §2.2. Unfortunately, the output has size s = .(d2) 
 there are d +1 corners, each in Zd so that we need at least t = O(d2) time simply to store and return 
all coordinates of each cell corner. In the next section, we mention a method to reduce these complexities. 
A trick for faster hashes The bottleneck in the algorithms thus far is the fact that we need at least 
O(d2) space to store and retrieve the list of all d +1 corners (each in Zd) of the cell containing x. 
If the hashed value h(v) of each vector v . Zd is of the form h(v) =Uu, v) (mod N) for some constant 
vector u and constant modulus N, then we can achieve complexity O(d log d) for s and t. To do so, it 
su.ces to .rst compute h(xint) and then subsequently add up(i) modulo N, for each i = d, d - 1,..., 1, 
where p is the sorting permutation as discussed in §2.2. For fast lookups, we only need to store (xint,p-1) 
in a new table, and a value k associated with each hash key corresponding to xint + ep(d) + ... + ep(k). 
The exact ith coordinate of a key can be retrieved in constant time as (xint)i + 1(k = p-1(i)). Fast 
lookups can be achieved by only examining log d random coordinates to verify the key of each hash table 
entry. This allows for O(d log d) query time with a small risk of extra hash table collisions ([1]). 
3 Empirical performance In this section we present empirical evidence on arti.cial data suggesting that 
our algorithm compares favorably against previously known locality hashes. 3.1 Related hashes We compare 
our algorithm to three other locality hashes for real vectors. The .rst, presented in [8], is based on 
a projection of the points into Hamming space, followed by a projec­tion onto Zk . This algorithm is 
very simple to imple­ment, although it requires a .xed bound on the input vectors; because it utilizes 
the unary representation of coordinates, we refer to it as the unary hash. In [7], an algorithm was presented 
based primarily on dimension reduction tailored to preserve distances well in certain £p metrics. The 
reduced vector is then abridged in a coordinate-wise fashion into Zk . Intuitively, the equivalence classes 
of this hash can be thought of as hypercubes in the reduced space. We refer to this as the pstable hash. 
The authors of [2] speci.cally design their hash so that the equivalence classes, in a reduced space, 
are spheres (or occluded spheres) instead of hypercubes, which will always result when concatenating 
a series of 1-dimensional hashes. We refer to this as the sphere-grid hash, as it is built on a sequence 
of grids of spheres. Also mentioned in [2] is the idea to use the Leech lattice for d = 24, or dimension 
reductions thereto. We have not compared against the Leech lattice primarily because, as a locality hash 
(i.e., without dimension reduction), it exists for only one d, while we are interested in the ability 
of other hashes which also scale to arbitrary dimensions. 3.2 Test data and performance metrics All of 
the locality hashes were tested by repeatedly generating a uniform random point x . [0,C]d, then a random 
unit vector w . Rd, and checking to see if x ~ x + Dw for a given distance D, where x ~ y means x, y 
are considered nearby according to whichever algorithm is being tested. In this way, we simulate data 
that has been randomly shifted and rotated before processing, and can estimate the probability P (x ~ 
y | dist(x, y)= D), over the probability space of this random data movement, for each distance D. E.ectively, 
we are estimating worst­case bounds on the approximation factor c for any query/point pair. Figure 5 
shows P (x ~ y|D) versus distance D for each algorithm. For easier comparison, the scale of each algorithm 
has been chosen so that the 50% point is at D = 1; better accuracy is indicated by a steeper descent 
around this threshold. We exclude sphere grid from the d = 500 graph because our implementation had di.culty 
scaling to this dimension. It is also interesting to estimate which approxima­tion factors are available 
at certain con.dence levels for each algorithm. Let f(D) := P (x ~ y|dist(x, y)= D). We assume that f 
is a strictly decreasing function when f(D) . (0, 1), which appears to be true for all tested algorithms. 
Then we can de.ne Dp := f-1(p) for Figure 5: Hit probabilities versus point distance for dimensions 10, 
100, and 500, top to bottom.  hash ß.01 ß.1 ß.3 unary 18 6.2 3.1 sphere grid 16 5.6 2.8 pstable 16 
5.1 2.8 here 2.2 1.6 1.4 Table 2: ßd estimates for d = 20. p . (0, 1). Next, let ßd := Dd/2/D1-d/2. 
We know with con.dence at least 1 - d that dist(x, y) <D1-d/2 . x ~ y, and dist(x, y) >Dd/2 . x ~ y; 
when both are true, we have c = ßd for any single point p. Figure 6 plots ß.05 and ß.1 for various dimensions. 
Note that c grows minimally as d increases. Table 2 gives ß estimates in d = 20 for con.dence levels 
99%, 90%, and 70%. Parameters Our choices for the parameters of each algorithm was based primarily on 
a requirement that each query lookup be fast, and next on practical choices hinted at by the various 
authors. Some previous algorithms use a parameter L to indicate the number of hash tables used, where 
each hash table t . [L] is slightly altered, such as by translating the input point by some vector ut 
. Rd . In those cases, the number of hash buckets per point (we ll call this Lb) was equal to the number 
of hash tables (call this Lt) but here, we conceptually have Figure 6: Left: ß0.05, right: ß0.1 versus 
dimension. d + 1 buckets per point in a single table. Because speed is a focus of this paper, we compare 
algorithms with the same Lt value, which is a more uniform measure of time complexity. In particular, 
we use the constant value Lt = 5 for most empirical data. In table 1, we see that setting Lt = d + 1 
o.ers modest accuracy gains at the cost of slower running time (.gure 7). All empirical data is run using 
the O(d log d) query time version of our algorithm. For unary, we chose to use the .xed number of projected 
coordinates per hash vector k = 700, a value suggested in [8]. For pstable, we used r = 1 as the denominator 
for each 1-dimensional hash. In both pstable and sphere grid, we used k = 2 log d as the reduced dimension 
(the reduced dimension is called t in [2]). In sphere grid, we used s = 2; larger values (such as s = 
4, as suggested in [2]) required an order of magnitude more grids to .ll the space, and were thus signi.cantly 
slower. Reproducing the experiments Source code to easily reproduce all our experiments, along with scripts 
to produce our particular graphs and tables, is available from: http://thetangentspace.com/lsh/ 4 Future 
work A natural next step is to corroborate the empirical performance measurements with provable timing-vs­accuracy 
bounds. Many locality-sensitive hash families H (including those considered here) can be intuitively 
understood as Figure 7: Timing data, in ms per trial. A trial is two point storages and one lookup. Note 
the di.erent scales on both axes, which further emphasize the (not unexpected) speed di.erence between 
the Lt = 5 and Lt = d + 1 cases.  matching a point x primarily to an equivalence class near(x, i) := 
{y | hi(y)= hi(x)} for each particular hash hi .H, and ultimately to a larger set near(x, I) := .i.I 
near(x, i). It is inevitable that x ~ y for any y . core(x, I) := ni.I near(x, i). Ideally, we would 
also have near(x, I)= .y.core(x,I)B(y, R), where R is the radius of detection for the hash. Intu­itively, 
this perspective gives us a strong hint toward making sure that each near(x, i) is sphere-like an in­tuition 
followed by [2], and, less directly, here. It s not too di.cult to check that near(x, I) for our hash 
is a Voronoi cell in a lattice solving the sphere­packing problem for d =2, 3, although not for any larger 
d. Perhaps there is a trade-o. in maintaining that the core set is a simplex while each near(·,i) set 
is sphere-like. The excellent sphere-packing properties of the Leech lattice ([6]) suggest it would give 
a good individual hash ([2]), and perhaps perform better when complemented by the other Neimeier lattices 
(although they could not .t exactly into our simplicial framework since there are only 24 of them we 
need d +1 = 25 corners, one from each hash). It has not escaped our notice that the tessellations presented 
in this paper may themselves be of geometric interest. Are they, in some sense, the most regular simplices 
which isometrically tile space? For the pur­poses of locality-sensitive hashes, we propose the fol­lowing 
optimality of our tessellation: Conjecture 4.1. Among all sliced partitions into iso­metric simplices, 
the canonical simplex c of V minimizes the ratio between diameter and the shortest distance be­tween 
any points which cross c. 5 Thanks Thanks to Dennis Shasha, Mehryar Mohri, Corinna Cortes, and Manfred 
Warmuth for their invaluable mentoring and generous support. References <RefA>[2] A. Andoni and P. Indyk, Near-optimal 
hashing algo­rithm for approximate nearest neighbor in high dimen­sions, Communications of the ACM, 51:117 
122, 2008. [3] A. Broder, M. Charikar, A. Frieze and M. Mitzen­macher, Min-wise independent permutations, 
Proceed­ings of the 30th annual ACM symposium on theory of computing, 1998. [4] T.M. Chan, Approximate 
nearest neighbor queries re­visited, Disc. Comp. Geom., 20(1998), pp. 359 373. [5] T.M. Chan, Closest-point 
problems simpli.ed on the RAM, Proceedings of the ACM-SIAM Symposium on discrete algorithms, 2002. [6] 
J.H. Conway and N.J.A. Sloane, Sphere packings, lattices, and groups, Springer, 1993. [7] M. Datar, N. 
Immorlica, P. Indyk and V. Mirrokni, Locality-sensitive hashing scheme based on p-stable dis­tributions, 
Proceedings of the 20th annual symposium on computational geometry, 2004. [8] A. Gionis, P. Indyk and 
R. Motwani, Similarity search in high dimensions via hashing, Proceedings of the 25th international conference 
on very large databases, 1999. [9] V. Guruswami, J. Lee and A. Razborov, Almost Eu­clidean subspaces 
of i1 via expander codes, Proceedings of the ACM-SIAM Symposium on discrete algorithms, 2008. [10] P. 
Indyk, Uncertainty principles, extractors, and ex­plicit embeddings of i2 into i1, Proceedings of the 
39th annual ACM symposium on theory of computing, 2007. [11] P. Indyk and R. Motwani, Approximate nearest 
neigh­bor: toward removing the curse of dimensionality, Pro­ceedings of the 35th IEEE Symposium on Foundations 
of Computer Science, 1998. [12] W. Johnson and J. Lindenstrauss, Extensions of Lip­schitz maps into a 
Hilbert space, Contemp. Math., 26(1984), pp. 189 206. [13] G. Manku, A. Jain and A. Sarma, Detecting 
near­duplicates for web crawling, Proceedings of the 16th international conference on WWW, 2007. [14] 
R. Panigrahy, Entropy-based nearest neighbor algo­rithm in high dimensions, Proceedings of the ACM-SIAM 
Symposium on discrete algorithms, 2006. [15] H. Samet, Foundations of Multidimensional and Metric Data 
Structures, Elsevier, 2006.  </RefA>A Appendix Proof of lemma 2.1. Let yi be the .rst point on line xy moving 
from y to x which is the entry point to a new cell; i.e., such that there is some cell cy' for which 
yi . cy' but yii . cy' for ii any previous point yon this line. Note that we may i have y= y. Notice 
that yi must be on the boundary of the new cell, so that there is some cell ci containing both y and 
i y. Thus, if yi ~ x, then we are done, since x is in some i cell cx ~ c. From here on, we ll assume 
that yi ~ x. Let x0 = x, and, similar to our choice of yi, let x1 be the next point on the line moving 
from x to y which is the entry point to a new cell. We can continue to de.ne a sequence x1,x2,... with 
each successive point being the next along xy toward y which is the entry point to a new cell. Let xk 
be the .rst in this sequence such that xk ~ yi. Since x ~ yi, we have k> 0. As above (for yi), there 
must be a cell ck containing both xk-1 and xk. By our ii choice of xk, we know that ck ~ c. Thus x= xk 
and yi are the desired pair of barely neighboring points. D Figure 8 shows an example case to illustrate 
the above proof. Proof of lemma 2.2. We begin with the . direction. Suppose x, y don t cross c. If either 
point is not in the boundary of c, then clearly they cannot be barely neighbors. Instead, let s assume 
that the corner sets Cx,Cy . corners(c) with x . convex(Cx),y . convex(Cy), both contain a common point 
z . Cx n Cy. If there were a cell ci containing x but not z, then the polytope cnci would be de.ned by 
corners excluding z, yet this shape would include x. This contradicts the necessity of z in Cx, so there 
cannot be any such cell. This same argument applies to all cells containing y. iii Therefore, any pair 
of cells c,ccontaining x, y must also share the point z, so that ci ~ cii, and x, y cannot be barely 
neighbors. Next we show the . direction. We will refer to any k-dimensional set of the form convex(F 
), for any F . corners(c), as a k-face of c. Thus a point is a 0-face, an edge is a 1-face, etc. We claim 
that, for every k-face f of c, there is another cell ci such that c ~ ci and c n ci = f. First we ll 
complete the proof using this claim, and then justify it. Suppose that x, y cross cell c. Let fx,fy denote 
the minimal k-faces of c which contain x and y, respectively. Now choose cells cx,cy so that x . cx,y 
. cy and cx n c = fx, cy n c = fy, using the claim. If cx ~ cy, then there is some point z . cx n cy. 
By the convexity of the cells, cx and cy contain the line segments xz and yz respectively. This means 
that there can be no slicing hyperplane which intersects the triangle formed by x, y, z, which in turn 
implies that z . c (see .gure 9). By our minimal choices of the k-faces, we have that z . fx and z . 
fy. But this contradicts the fact that x, y cross c! Hence there can exist no common point z between 
cx,cy; and cx ~ cy so that x, y are barely neighbors. Now let s justify the claim that there is a cell 
ci with i c n c= f for every k-face f of cell c. Suppose f is a k-face of cell c. Then f is the intersection 
of d - k hyperplanes with c, where each hyperplane is a slice of the partition that determines some d 
- 1-face of c. Let ci be any cell on the opposite i side of all of these hyperplanes, and with c ~ c. 
If i x . c n c, then x must be a point in each of these d - k hyperplanes, so that x . f. It is also 
clear that f . c n ci; so we may conclude that f = c n ci, which was our goal. D Proof of property 2.6. 
V is isometric in £2 Let s check that V is an iso­metric partition that every pair of cells is isometric. 
Similar to our previous argument, we ll begin by show­ing that every image, under T , of a lattice-cornered 
hy­percube is isometric to the image of [0, 1]d . Let u denote the unit hypercube [0, 1]d and u = T (u). 
Similarly, let c denote an o.set hypercube c = u + z, z . Zd, and c = T (c). We would like to .nd an 
isometry from c . u . Let Az be the shift Az(x)= x-z, so that Az(c)= u. Then clearly A z := T .Az .T 
-1 maps c to u. Notice that (TAzT -1)(x)= T (T -1(x) - z)= x - T z, so that A z is indeed an isometry. 
Next we d like to check that any two simplices T (Sp1 ),T (Sp2 ) are isometric as well. Suppose p :[d] 
. [d] is a permutation, and let Up denote the mapping given by yi = xp(i) when y = Up(x). At this point 
we note that T can be written in the form T = aI + ßJ, where I is the identity matrix, J is the all-one 
matrix, vv a = d + 1, and ß = (1 - d + 1)/d. Then UpT TUp = UT (aI + ßJ)Up p = aI + ß(UpT JUp)= aI + 
ßJ = T. Using the fact that Up is unitary (so UT = U-1), we pp can see that TUpT -1 = Up. So if Up : 
Sp1 . Sp2 , then we still have Up : T (Sp1 ) . T (Sp2 ), also isometrically. As a .nal note, observe 
that any two isometries A = T . A . T -1 and B = T . B . T -1 may be composed, and preserve the conjugate 
relationship A . B = T . (A . B) . T -1 . This justi.es that a series of the above tranformations translations 
and permutations  su.ces to produce an isometry between any two cells of V, and this partition can be 
studied by considering a single canonical cell. The canonical cell is vertex-transitive in £2 Next we 
ll choose a particular canonical cell to study, and justify the name of this partition by showing that 
it has the type of polytope regularity known as vertex transitivity. Recall that a polytope p is vertex 
transitive i., for every pair of vertices v1,v2, there is an isometry . : p . p with .(v1)= v2. Our canonical 
cell in V is T (Sid), with corners pi := T (fi). To show that this cell is vertex-transitive, it will 
su.ce to construct mappings mi,j : T (Sid) . T (Sid), for any i, j .{0, 1,...,d}, such that mi,j is an 
isometry and mi,j(pi)= pj . We proceed by constructing a (non-isometric) map V : Sid . Sid which acts 
as a shift operator on the corners of this simplex. If we de.ne the matrix 1 -1 I W := , 1 -10 and let 
V (x) := Wx + f1, then V (fi)= fi+1 for i<d; V (fd)= f0. Since V is an a.ne transformation, V (convex(f0,f1,...,fd)) 
= convex(V (f0,f1,...,fd)), verifying that V : Sid . Sid. To operate in the new partition, we extend 
V via V := TVT -1 . Clearly, V maps the canonical cell to itself, and shifts the corners pi . pi+1 (mod 
d + 1). Hence any desired mapping pi . pj can be acheived by the correct number of iterations of V ; 
that is, we are close to seeing that mi,j = V k, where k = j - i (mod d + 1). The last step is to con.rm 
that V is an isometry. This is the case i. W = TWT -1 is itself isometric. vv Let µ := (1 - 1/d + 1)/d, 
so that T = d + 1(I - v µJ); and T -1 = v 1 (I + d +1µJ). Then W =(I - d+1 v µJ)W (I + d +1µJ), and from 
here one may tediously simplify the corresponding formulae for Uwi,wj) (where wi is the ith column of 
W ) to con.rm that W T W = I. This su.ces to demonstrate that T (Sid) is indeed a vertex-transitive simplex. 
D Figure 8: Example from the proof of lemma 2.1. Here, x ~ y and xi ~ yi are barely neighbors Figure 
9: For the proof of lemma 2.2: If xz and yz are both intact i.e., do not intersect a slicing hyperplane 
then all of .xyz must exist in a single cell.  
			
