
 ALGORITHMS TO DETECT CHAINED-INFERENCE FAULTS IN INFORMATION DISTRIBUTION SYSTEMS* Yih-Feng Hwang  
   WorldCom There has previously been considerable work in the quality assessment of critical control 
systems, such as Command, Control, Communication, and Intelligence (C3I) systems used in the battlefield, 
to verify and validate system knowledge bases and to check them for completeness and consistency. During 
the execution of a C3I system, structural faults in a rule set, such as inconsistency, can decrease the 
implemented system s performance and even cause more serious failures, such as performing unexpected 
actions. Traditional ways to detect faults in a rule set include comparisons of two rules at once. However, 
faults could be introduced by rule inferences such that, in the implemented system, one or more rules 
will be fired due to another fired rule. This paper presents newly researched and developed algorithms 
that can effectively and efficiently detect chained-inference faults in rule sets of information distribution 
system (IDS), which is a subsystem within a C3I system. A new directed graph paradigm called Transition-Directed 
Graph (TDG), used to represent IDS rule sets at nodes of the IDS, is presented. In this research, there 
are six categories of chained-inference rule faults defined using a TDG presentation. Based on these 
definitions of faults, algorithms used to detect such faults have been researched and developed. Key 
Words or Phrases: C3I systems, information distribution systems, fault detection, transition-directed 
graph, chained-inference rule faults. 1. INTRODUCTION A major difficulty that occurs in the construction 
of large production rule-based systems is maintaining the correctness, consistency, and completeness 
of the knowledge base [David and King 1977; Liu and Dillon 1991]. Many systematic verification and validation 
efforts in critical control systems, such as a Command, Control, Communication, and Intelligence (C3I) 
system used in the battle field, have been applied to improve the quality of knowledge bases and the 
performance of such systems. During the execution of a C3I system, faults in a rule set such as circularity 
and inconsistency can decrease the system performance and even cause more serious failures [Nazareth 
1989]. Preece [1991] presents a set of fault definitions that can be detected by using verification programs 
that appeared earlier in several papers [Suwa et al. 1982, 1984; Nguyen 1985, 1987; Cragun and Steudel 
1987; Miller et al. 1986; Preece 1989; Jafar and Bahill 1990]. These traditional verification programs 
used to detect faults in a rule base compare two rules at once (i.e., at a local level [Agarwal and Tanniru 
1992]). However, faults could be introduced by rule inferences such that one or more rules will be fired 
due to another fired rule. There are few research papers that have discussed the important issues regarding 
the faults in chained­inference rules [Ginsberg 1988; Liu and Dillon 1991; Agarwal and Tanniru 1992]. 
However, those verification programs used to detect faults at a global level [Adelman 1992; Agarwal and 
Tanniru 1992] are neither efficient nor effective. Thus, there exists a significant research problem: 
Effectiveness and efficiency of algorithms for detecting chained-inference rule faults must be improved. 
This paper presents researched and developed algorithms that can effectively and efficiently detect chained-inference 
faults, such as redundancy, subsumption, inconsistency, contradiction, circularity, and unreachability, 
in rule sets of information distribution system (IDS) which is a subsystem within a C3I system [Chamberlain 
1990. The effectiveness of algorithms that detect such faults is a qualitative attribute that increases 
as the number of faults that are found increases. On the other hand, the efficiency of algorithms that 
detect such faults is David C. Rine George Mason University a quantitative attribute that increases 
as the time to find a number of faults decreases. An IDS contains a set of IDS nodes, such a node has 
one or more connections to other nodes within the IDS. Like a directed graph, a set of vertices/nodes 
represents that graph. Within each one of IDS nodes, a set of Event-Condition-Action (ECA) rules is used 
to control the distribution of information between connected nodes. That is, communications between nodes 
are controlled and cooperated by pre­defined ECA rules within all IDS nodes. An ECA rule format, for 
instance, could be 'If an event happened and a condition is satisfied, then an action will be executed 
. A new graphs-based paradigm called transition-directed graph (TDG) is introduced and used in this paper. 
A TDG is a directed graph (i.e., digraph) that can be used to represent a set of EAC rules. Using graphic 
representations [Cormen et al. 1990; Sedgewick 1992; Brassard and Bratley 1996] and propositional logic 
[Nerode and Shore 1993] as well as formal statements, six categories of faults defined in later section 
cover all main faults mentioned in other prior research papers published. In this paper, the developed 
algorithms with O(n) complexity for TDG­oriented rule sets can detect one or more faults while all rules 
are examined at once, where n denotes the number of rules. This paper is organized as follows: Sections 
2 presents some related works. Section 3 describes the research approaches from two different perspectives, 
namely theoretical and programmatic. The framework for developing the algorithms to detect chained-inference 
rule faults is presented. Section 4 shows the results of this research. A comparison between the analytical 
and experimental complexity is made. Section 5 presents the conclusions and the future works.  2. RELATED 
WORKS Many kinds of approaches to detecting faults in an knowledge base have been proposed, such as matrix-representation-based 
[Nguyen et al. 1985, 1987; Bahill et al. 1987], decision-table-based [Cragun and Steudel 1987], graph-based 
[Nguyen 1987; Liu and Dillon 1991; Nazareth and Kennedy 1991], network-based [Miller et al. 1986; Murray 
and Tanniru 1987], Petri-Net-based [Giordano and Saitta 1985; Looney 1988; Murata and Matsuyama 1988; 
Agarwal and Tanniru 1992], statistical analysis­based [Landauer 1990], and deductive logic-based [Ginsberg 
1987; Stachowitz et al. 1987; Preece 1989]. Based on such approaches, many automated tools have been 
developed and used to check for the important types of faults in different domains. These tools include 
algorithms that check for known potential faults, such as ESC (Expert System Checker) [Cragun and Steudel 
1987]; CHECK [Nguyen et al. 1985]; ARC (ART (Automated Reasoning Tool) Rule Checker) [Nguyen 1987]; RCP 
(Rule Checking Program) [Suwa et al. 1982, 1984]; Validator [Jafar and Bahill 1990]; and COVER [Preece 
1989]. Most tools only address a local verification of a rule base. These tools are limited to the comparison 
of two rules at once. There are few research papers that have discussed the important issue regarding 
effectiveness of chained-inference rules in a large rule base. However, those approaches to detect faults 
at a global level are not efficient enough. Ginsberg [1988] proposed an approach called knowledge-base 
reduction (KB-Reducer) to check knowledge bases for inconsistency and redundancy. However, the complexity 
of the reduction steps in a KB takes O(3n) operations, where n denotes the number of findings. Liu and 
Dillon [1991] presented an approach used to detect different faults such as redundancy and inconsistency. 
In their paper it is reported that it * The funding was supported by Army Research Laboratory (ARL) 
through George Mason University and SONEX Enterprise, Incorporated. Authors addresses: Computer Science 
Department, George Mason University, Fairfax, VA 22030-4444. Email: Yih-Feng Hwang, Yih-Feng.Hwang@wcom.com; 
David C. Rine, drine@cs.gmu.edu. Permission to make digital or hard copies of part or all of this work 
or personal or classroom use is granted without fee provided that copies are not made or distributed 
for profit or commercial advantage and that copies bear this notice and the full citation on the first 
page. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior 
specific permission and/or a fee. SAC 2001, Las Vegas, NV &#38;#169; 2001 ACM 1-58113-287-5/01/02 $5.00 
takes O(n) operations to construct a reachability tree. However, there were no algorithms in their paper 
proposed to detect faults in the reachability tree. Agarwal and Tanniru [1992] proposed a Petri-Net approach 
to detect faults such as redundancy and subsumption in a rule base at both local and global levels. This 
approach takes O(mn2) operations to detect redundancy and inconsistency faults at the global level, where 
n and m denote the numbers of rules and places, respectively. Perl and Shiloach [1978] proposed an algorithm 
to find two vertex-disjoint paths between two pairs of vertices in a directed acyclic graph (DAG). The 
relevance of this reference is that the algorithm in their paper can be expanded to detect redundancy 
or subsumption faults. However, their algorithm s complexity is O(|V|*|E|), where |V| is the number of 
vertices and |E| is the number of edges in the given DAG. In the research represented in this paper, 
based on a TDG representation, five DFS-based algorithms detect instances of six categories of chained­inference 
faults with a complexity O(n), where n denotes the number of vertices/rules in a given TDG. Once one 
of the six fault patterns is found, it will take O(n) operations to locate those rule identifiers involved 
in the pattern found, because a rule identifier has been represented as a vertex in a given TDG. For 
instance, to locate the vertices/rule identifiers in a cycle pattern found, every vertex in the active 
path found will be discovered by a forward search starting at the starting vertex of the cycle (the starting 
vertex of a cycle is detected during a DFS). It takes O(n) operations to locate every rule involved in 
the cycle pattern found. Unlike the Agarwal and Tanniru [1992] approach where the local detecting must 
be performed before the global detection, the five DFS-based algorithms detect faults at both local and 
global levels at one time. Moreover, these five algorithms can detect one or more patterns for each category 
of chained-inference faults after use of one DFS. These five algorithms are efficient. A summary of the 
complexities for above approaches used to detect faults at the global level will be presented in the 
later section. Inconsistency: there exists a path whose starting vertex is conflicting with the ending 
vertex. Contradiction: there exist two paths with identical starting vertex whose ending vertices are 
conflicting. Circularity: there exists a path whose starting vertex is the same as ending point (i.e., 
a cycle). Redundancy: a starting vertex reaches the same ending vertex along two or more paths. Subsumption: 
this is a specific pattern of a redundancy fault, where one of the paths is a single rule. Unreachability: 
there exists an internal vertex that cannot be reached from any one of the input vertices or exists an 
internal vertex that cannot reach any one of the output vertices. Figure 1. Definitions of Faults Categories 
Using a TDG Representing Figure 2. TDG Representation for Chained-Inference Rule Faults. Within a period 
of time, effectiveness of an algorithm can be represented as an average number of detected fault patterns 
per time unit. In the prior related researches, there is little evidence for effectiveness of tools/algorithms 
in prior related research. That is, there is no approach claiming that two or more fault patterns at 
the global level can be detected after examining all rules once. Each DFS-based algorithm can detect 
one or more instances of one of six categories of fault patterns. Moreover, the time complexity to detect 
fault instances in given rule sets for each DFS-based algorithm is optimal (i.e., O(n)).  3. THE RESEARCH 
APPROACH A new digraph paradigm called Transition-Directed Graph (TDG) is presented in this section. 
Based on this TDG representation, faults in IDS rule sets are defined as undesirable patterns appearing 
in a given TDG, where the TDG represents the faulty input IDS rule sets. Five efficient and effective 
algorithms are used to detect fault patterns in a given TDG that has been developed and implemented from 
the IDS rule sets. To search for those fault patterns (in TDGs), the concept of a depth­first search 
(DFS) algorithm is employed extensively to develop specific algorithms. These DFS-based algorithms with 
O(n) complexity, where n denotes the number of rules, can detect one or more faults (fault patterns) 
in IDS rule sets, represented as TDGs, when all rules have been examined. These algorithms improve efficiency 
and effectiveness over previous algorithms. Based on the TDG representation, faults in IDS rules are 
defined as undesirable patterns in a given TDG, which is transformed from the input IDS rule sets. Figure 
1 and Figure 2 show the definitions of fault categories/patterns using a TDG representing and the details 
of graphical representations for each fault pattern, respectively 3.1. The Transition-Directed Graph 
(TDG) In Agarwal and Tanniru [1992], the authors present an algorithm with O(n2) complexity, where n 
denotes the number of rules, to verify rule faults at both local and global levels. A Petri-Net is employed 
to represent the rule sets. However, some limitations are reported because of the lack of flexible representations. 
For instance, in a Petri-Net approach, the negation operator cannot be represented and only well­structured 
rule formats [Pederson 1989] can be dealt with. Moreover, there are only two categories of faults, subsumption 
and redundancy, that can be verified at the global level. In a digraph representation, directed edges 
represent the control flow of a system behavior characterized by the rule sets. Instead of using tokens 
in a Petri-Net, the truth values, true or false, which can be assigned to each vertex in a digraph, are 
employed to represent the semantic part of system behavior. This TDG representation overcomes those above 
drawbacks in the Petri-Net representation and offers more flexible rule formats, for instance, two or 
more predicates in the RHS of one rule are allowed. The rule identifiers are embedded into the TDG representation 
so that the entire components/predicates of one rule can be assessed when only one of components in a 
rule is involved in the pattern found. This rule identifier information will be very helpful when one 
predicate involved in a detected fault pattern is within several rules. D R: A BCr2: B . C r3: C . E 
. A r1: A . D . B r1 r2 r3 E Figure 3. An Example of Dependency Fault. 3.2. Definitions for Chained-Inference 
Rule Faults Using TDG Based on a TDG presentation, we present with graphical representations detailed 
definitions for six categories of chained-inference rule fault patterns in Figure 2. Each category of 
fault has different fault patterns in a TDG. For instance, a circularity fault pattern can be represented 
as a cycle in a TDG. As depicted in Figure 2 (c), a rule set R comprises four rules that introduce a 
chained-inference circularity fault. Figure 1 shows the definitions of six categories of faults in terms 
of fault patterns in a TDG. Each fault can be introduced either at the local level or the global level. 
To detect an instance of those categories of faults in Figure 1, algorithms search such patterns in a 
given TDG representing the input IDS rule sets. A dependency fault refers to a fault pattern in a TDG 
such that there exists at least one vertex * in such a fault pattern whose in­ * There are only rule-identifier 
vertices that will cause dependency fault patterns. For a fault pattern, except unreachability and inconsistency 
faults, whose starting and/or ending vertices are rule-identifier predicates, those starting and/or ending 
vertices are excluded. degree is greater than 1. As depicted in Figure 3, from the logic point of view, 
the circularity fault, from the predicate A back to itself, will not be able to occur if either the predicate 
D (in and connective) or the predicate E (in and connective) is false. We call this circularity fault 
a dependency fault. A fault that is not a dependency fault is called non­dependency fault. Fault patterns 
in a TDG detected by algorithms include both dependency and non-dependency faults. However, it is easy 
and cheap to identify a dependency fault within those detected faults by using a DFS to compute each 
vertex s in-degree in the pattern detected. [Hwang 1997] shows the algorithm to compute the in-degree 
for each vertex in a TDG. During a DFS, if there exists one vertex in a pattern whose in-degree is greater 
than 1, than such a fault pattern is a dependency fault. The complexity of an algorithm used to identify 
those dependency faults will be proved to be O(n), where n denotes the number of vertices in a TDG. Therefore, 
the total analytical algorithm complexity for detecting non-dependency faults still remains O(n) because 
O(n) + O(n) = O(n). In the rest of this paper, we will use faults to refer to both dependency faults 
and non-dependency faults , except for specific claims. That is, a detected fault pattern can be either 
a dependency fault or a non-dependency fault. 3.3. The Graph Properties Used in Graph Algorithms We 
define a sparse digraph as a digraph G whose number of edges is close to the number of vertices in G. 
In the following sections, any digraph refers to a sparse graph . We will not consider a dense digraph 
whose number of edges is close to the square of number of vertices, because a rule base in a real IDS 
cannot be represented as such a digraph. The searching techniques in a graph are the heart of the field 
of graph algorithms. To explore graph algorithms, many properties of graphs have been proposed. For instance, 
changing the colors of vertices during a search in a graph is such a property. Three colors are employed 
to distinguish different statuses of a vertex during a search, namely white, gray, and black [Cormen 
et al. 1990]. A vertex with white color refers to a vertex which has not been visited yet. A vertex s 
color can change from white to gray when it has been visited the first time. The status of a vertex with 
gray color is defined as active. A vertex with black refers to a vertex which has been visited (and grayed) 
and the search finishes examining all such vertex s adjacent vertices. The status of a vertex with black 
color is defined as inactive. After a DFS, all vertices in the examined graph are blackened. That is, 
all vertices became inactive. Another property that will be employed in this paper is that of exclusive 
vertices. This property is introduced for graphs (e.g., TDGs) whose vertices are logic propositions. 
A vertex E is an exclusive vertex, in a graph G, if there exists another vertex E which is logically 
conflicting x with vertex E. The vertex Ex itself is an exclusive vertex. A pair comprises two exclusive 
vertices containing logic propositions, such as P and ¬P, where P is a predicate, is an exclusive pair. 
A source vertex [Agarwal 1992; Foulds 1992] refers to a vertex such that there only exist out-going edges 
(i.e., in-degree = 0). Input vertices are some specified source vertices (e.g., possible input events 
to an IDS from the environment which are called external events). On the other hand, a sink vertex refers 
to a vertex such that there only exist in-coming connections (i.e., out-degree = 0). Output vertices 
are some specified sink vertices (e.g., possible output results specified in the requirements). An internal 
vertex is a vertex that is neither an input vertex nor an output vertex. To detect unreachability fault 
patterns, a token indicator is employed to determine whether one vertex can reach any one of the output 
vertices. During a DFS, if one vertex v has a token indicator, then all its ancestors [Brassard and Bratley 
1996] will have a token indicator by passing backward from the vertex v. 3.4. The Criteria for Detecting 
Faults Algorithms were not designed to detect all possible faults in IDS rule sets at one time. The reason 
is that detecting certain patterns in a digraph, such as longest circuit and longest path, are NP-complete 
problems [Garey and Johnson 1979]. That is, there does not exist a polynomial time algorithm to solve 
such hard problems [Lalement 1993]. The developed algorithms in this paper will, after using a DFS, detect 
one or more fault patterns for each category of six faults defined in Figure 2. Based on the graph properties 
mentioned in the previous section 3.2, the criteria to detect such fault patterns are developed. Figure 
4 summarizes the criteria for detecting each faults. The details of each developed algorithm will be 
discussed in the next section. Type of Fault Criteria Circularity during a DFS, if one vertex is active 
and is visited at the 2 nd time, then the pattern is found. (a back edge is found, i.e., a cycle) Redundancy 
during a DFS, if one vertex is inactive and is visited at the 2 nd time, then the pattern is found. (a 
forward edge is found) Subsumption Unreachability (same as Redundancy) after a DFS, if one vertex without 
a token indicator or one vertex is unvisited, then the pattern is found. Inconsistency during a DFS, 
if there exists an exclusive pair such that one exclusive vertex in the pair can reach the other, or 
vice versa, then the pattern is found. Contradiction during a DFS, if there exists an exclusive pair 
and one distinguished vertex s such that two paths, from s to one exclusive vertex and from s to another 
exclusive vertex, are encountered, then the pattern is found. Figure 4. The Criteria for Detection Faults. 
 4. ANALYTICAL AND EMPIRICAL RESULTS Details of those algorithms used to detect chained-inference faults 
in IDS rule sets will be described in this section. From both theoretical and programmatic perspectives, 
algorithmic efficiency analyses use the asymptotically computational complexity measure of each algorithm 
and the empirical computational complexity for each algorithm. A comparison of results between analytical 
complexity (i.e., Big-Oh) and empirical complexity (i.e., actual experimental execution time) is presented 
in the later section. In Sections 4.2 and 4.3, the algorithmic analyses and the comparison of results 
between theoretical and programmatic perspectives are presented, respectively. 4.1. The DFS-Based Algorithms 
for Detecting Faults in Chained-Inference Rules Based on the definitions of chained-inference faults 
in IDS rule sets using a TDG representation, as shown in Figure 2, five DFS-based algorithms used to 
detect instances of six categories of chained-inference fault patterns have been developed, implemented, 
and tested using the Ada 95 programming language running on a SunSparc platform. The following subsections 
describe how these five DSF-based algorithms detect six categories of chained-inference rule faults in 
a given TDG, based on the criteria defined in the Figure 4. As shown in Figure 5, the rule set R comprised 
of 10 predicate rules is used to construct Test_G to illustrate the use of each algorithm developed in 
this paper. Figure 5. An Example of A TDG, Test_G. 4.1.1. Inconsistency There are two steps to detect 
an instance of an inconsistency fault pattern in Test_G. The first step finds all pairs of exclusive 
vertices in Test_G, such as P and ¬P, where P is a predicate. An exclusive pair refers to two exclusive 
vertices in such a pair. After constructing these exclusive pairs, the next step checks whether there 
exist one or more paths between two exclusive vertices. If such a path exists, then an instance of an 
inconsistency fault pattern is found. From now on when we refer to fault pattern we will mean instance 
of a fault pattern . To check if there exists a path, from one exclusive vertex to the other or vice 
versa, a DFS is used to search every reachable vertex, until the expected fault pattern is found or all 
vertices in Test_G are visited. The vertices involved in the pattern found are visited and active. Within 
Test_G, the algorithm used to detect inconsistency fault patterns first finds two exclusive pairs (C, 
¬C) and (E, ¬ E). Next, the DFS starting at vertex C checks whether there exists a path from C to ¬C. 
Thus, an inconsistency fault pattern is detected, a path from C to ¬C. For the other exclusive pair (E, 
¬E), no fault pattern has been found after a DFS starting at E and at ¬E. 4.1.2. Contradiction A contradiction 
chained-inference fault pattern occurs because one antecedent will result in two opposite rule consequences, 
such as P and ¬P, where P is a predicate. There are two steps to detect a contradiction chained-inference 
fault in Test_G. The first step in the algorithm finds all exclusive pairs in Test_G. The next step checks 
whether there exists one vertex in the TDG such that there exist two paths, from that vertex to two other 
exclusive vertices. To check whether there exist two such paths, a DFS is used to search every reachable 
vertex, until the expected fault pattern is found or all vertices in the Test_G are visited. Within Test_G, 
the algorithm used to detect contradiction fault patterns first finds two exclusive pairs (C, ¬C) and 
(E, ¬ E). Next, the DFS starting at vertex A checks whether there exist two paths, from A to C and from 
A to ¬C. From the Test_G, we cannot find two such disjoint paths for the exclusive pair (C, ¬C). Next, 
the algorithm tries the other pair (E, ¬E) and finds a contradiction fault pattern, two paths from C 
to ¬E and from C to E. 4.1.3. Circularity A cycle in a TDG is a circularity fault. A DFS starting at 
one of the input vertices will be employed to detect cycle patterns in a given TDG. During a DFS, the 
vertices statuses (active and inactive, as mentioned in section 3.2) will determine whether cycle patterns 
are detected. A cycle is found when an active vertex (i.e., with gray color) is visited the second time. 
Such an active vertex is a starting-vertex in a cycle, a path from the starting-vertex back to itself. 
To display/record those vertices in the cycle found, a forward search starting at the starting-vertex 
is employed to find vertices involved in the cycle detected. Within Test_G, during a DFS, the search 
starting at vertex A checks whether there exists an active that is visited the second time. A cycle starting 
from active vertex C is found in G. 4.1.4. Redundancy/Subsumption A DFS starting at one of the input 
vertices Test_G will be employed to inspect whether there exist redundancy/subsumption fault patterns. 
During a DFS, the vertex s statuses will determine whether redundancy/subsumption fault patterns are 
detected. If an inactive vertex has been visited the second time, then a redundancy/subsumption pattern 
is found. The subsumption fault categories contains a specific kind of pattern of a redundancy fault. 
Here we define a subsumption fault as a path that comprises one single rule and is subsumed by another 
path in which at least one rule is connected. To display/record those vertices involved in the detected 
pattern which comprises two paths, from one starting-vertex to the same ending-vertex, a backward search 
starting at the starting-vertex traverses each vertex in two such paths (i.e., an inactive path and an 
active path) until the starting-vertex in the pattern is found. Within Test_G, during a DFS, the search 
starting at vertex A checks whether there exists an inactive vertex that is visited the second time. 
Two patterns are found after a DFS. The first pattern comprises two paths, from B through r2 to D (i.e., 
inactive path) and from B through H to D (active path). The inactive path comprises exactly three vertices 
(i.e., a single rule). Thus, this is a subsumption fault. The second detected pattern comprises two paths, 
from C through E to G (i.e., inactive path) and from C through I to G (active path). The number of vertices 
in each detected path are greater than 3. Thus, this is a redundancy fault pattern. 4.1.5. Unreachability 
To detect the unreachability fault, both input and output vertices need to be provided. As mentioned 
in section 3.2, an input vertex is a specified source vertex (i.e., in-degree = 0); An output vertex 
is a specified sink vertex (i.e., out-degree = 0); an internal vertex is a vertex that is neither an 
input vertex nor an output vertex. Using a DFS starting at one of the input vertices, each internal vertex 
in a given TDG will be examined to determine whether it is reachable from an input vertex or it can reach 
any one of the output vertices. A vertex without a token indicator means that an output vertex cannot 
be reached by a path from this given vertex. After a DFS, vertices which have not been visited and/or 
do not have a token indicator are unreachable. Within Test_G, a DFS starting at vertex A examines whether 
there exists a vertex that is not reachable from A or cannot reach any one of the output vertices, F, 
G, or K. After a DFS, the vertices J and ¬C are unreachable vertices.  4.2. The Analyses of The Algorithms 
The Big-Oh computational complexity method has been used to analyze the theoretical performance of each 
algorithm used to detect instances of one of six categories of chained-inference faults. Algorithmic 
theorems and corollaries are presented with their proofs in [Hwang 1997], using Natural Deduction Logic 
System (NDLS [Gries 1981]): a formal system of axioms and inference rules for deducing proofs for each 
theorem are used. Figure 6 presents the summary of the complexities for related researches. 4.3. Comparison 
of Results Between Analytical and Experimental Complexity Given a graph G = (V, E), where V denotes the 
set of vertices; E denotes the set of edges. To test these five DFS-based algorithms, each test case 
comprised of a sparse digraph is constructed to evaluate algorithmic performances. To gain accurate results, 
five test cases with different sizes (20, 40, 60, 80, and 100) of vertices in a TDG have been simulated 
to evaluate the performance of each procedure, namely Check-Inconsistency, Check-Contradiction, Check-Circularity, 
Check-Redundancy, Check-Unreachability. The CPU time for running each procedure, including the time to 
locate/display each vertex involved in the pattern found, has been compared with the analytical computational 
complexity as discussed above. The unit for the CPU running time is millisecond (msec). Figure 7 presents 
the average CPU running time in a single Sun platform machine for each test case of six categories of 
faults. Figure 8 show that the average CPU running time to detect one or more instances of six categories 
chained-inference rule fault pattern is close to a linear function of the number of vertices in a given 
TDG. I n c o n s is te n c y C ont r a di ct i o n Circ u l a r i t y R e d und ancy / Su bs u m pt i 
o n U n r eachab i l i t y Ag a r wa l &#38; T a n n i r u * O (m n 2 ) - - O (m n 2 ) O (n 3 ) Gi 
n s b e rg O (3 n ) O (3 n ) - O (3 n ) - N guy en ** O (n 2 ) O (n 2 ) - O (n 2 ) O (2 n ) W a r s 
h a ll a l g o r ith m O (n 3 ) - O (n 3 ) - O (n 3 ) P e r l &#38; S h i l o ach - - - O (n 2 ) - 
D F S - ba s e d a l go r i t h m s ** * O (n ) O (n ) O (n ) O (n ) O (n ) N o te s : * m an d n deno 
t e t h e nu mber s of pl aces and r u l e s i n a g i v e n P et r i N e t , r e s p ect i v el y. ** 
R u l e s i n v o l v ed i n t h e de t ect ed f a u l t s cann o t b e i d en t i f i ed. *** T h e 
compl e xi t y f o r each o f t h e f i v e D F S - bas e d al g o r i t h ms , i n cl ud i n g t h at 
t o di s p l a y v e r t i ces i nvo l v ed i n t h e f a ul t p a t t er ns f o und .  Figure 6. Summary 
of The Complexities in The Related Researches CPUTime(msec.) Test Cases Algorithms Test Case 1 (20 vertices) 
Test Case 2 (40 vertices) Test Case 3 (60 vertices) Test Case 4 (80 vertices) Test Case 5 (100 vertices) 
Check-Inconsistency 0.073 0.115 0.147 0.202 0.255 Check-Contradiction 0.078 0.129 0.199 0.297 0.384 Check-Circularity 
0.118 0.156 0.167 0.182 0.203 Check-Redundancy (Redundancy) 0.159 0.283 0.332 0.435 0.505 Check-Redundancy 
(Subsumption) 0.264 0.374 0.426 0.485 0.556 Check-Unreachability 0.023 0.032 0.045 0.057 0.064 Figure 
7. The Results of Testing DFS-Based Algorithms. Check-Inconsistency Check-Contradiction Check-Circularity 
Check-Redundancy (Redundancy fault) Check-Redundancy (Subsumption fault) Check-Unreachability Figure 
8. The CPU Time for Detecting Chained-Inference Rule Faults.  5. CONCLUSIONS AND FUTURE WORKS To accomplish 
a higher quality IDS performance, it is necessary to discover how to ensure that rules in a rule base 
of an IDS are complete, consistent, and free from faults. Many approaches have been proposed to detect 
faults in a rule base by comparing two rules at one time. However, faults could be introduced by rule 
inferences such that one or more rules will be fired due to another fired rule. In this paper, we focus 
on the problem how to effectively and efficiently detect faults at both local and global levels. A new 
paradigm using digraphs called Transition-Directed Graph (TDG) is introduced and applied in the research. 
Based on the TDG presentation, rule sets are modeled as digraphs. In this paper, five DFS-based algorithms 
used to detect chained-inference faults in IDS rule sets have been researched, developed, implemented, 
and tested. The criteria used to detect instances of six categories of the faults defined in Figure 4 
are presented. Based on these criteria, five DFS-based algorithms detect chained-inference rule faults 
by searching those expected patterns in a given TDG. The complexities of fault detecting algorithms in 
prior published papers vary from O(2n) to O(n3), where n denotes the number of rules. Algorithms with 
O(n) complexity represented in this paper can detect one or more faults while all rules are examined 
once. The time complexity for each algorithm takes O(n) operations, where n denotes the number of vertices/rules 
in a given TDG. Moreover, each algorithm is developed and designed to detect only one of six chained-inference 
faults in IDS rules. Five algorithms can be used to verify the completeness, consistency, and fault-freeness 
of a rule base whose rule format is either if-then or ECA. These five algorithms can be applied to other 
different application domains, such as fuzzy logic control systems, rule-based expert systems, intelligent 
rule-based systems, graph theory, and graph-oriented object database. The following are future works 
related to this paper. (1) Algorithms presented in this paper cannot memorize the instances of six categories 
of fault patterns defined in Figure 4. That is, the outputs for each algorithm will be the same when 
the input data is identical. A good future addition to this research will allow the users to look at 
different fault patterns when each algorithm is executed at the second time along with the same input 
data. One possible way to achieve this additional feature is to memorize every component in a fault pattern 
detected. (2) It is difficult to determine whether there exists a dependency fault pattern in a given 
TDG, because a dependency fault pattern can only be identified in those detected fault patterns, and 
all fault patterns in the TDG cannot be detected at one time.  References <RefA>Adelman, L. (1992), Evaluation 
decision support and expert systems, John Wiley&#38;Sons. Agarwal, R. and Tanniru, M. (1992), A Petri-Net 
based approach for verifying the integrity of production systems, in Int. J. Man-Machine Studies, 36, 
3, p447-468. Brassard, G. and Bratley, P. (1996), Fundamentals of Algorithmics, Prentice-Hall. Chamberlain, 
S. C. (1990), The Information Distribution System: IDS­an overview, technical report BRL-TR-3114, July. 
Cormen, T., Leiserson, C, and Rivest, R. (1990), Introduction to Algorithms, McGraw-Hill, NY. Cragun, 
B.J. and Steudel, H.J. (1987), A Decision-Table-Base processor for checking completeness and consistency 
in rule-based expert systems, in Int. J. Man-Machine Studies, 26, 5, 633-648. David, R. and King, J. 
(1977), An overview of production systems, in Machine Intelligence (Elcock, J. and Michie, D. Eds.), 
300-332, Ellis Horwood. Foulds, L. R. (1992), Graph Theory Applications, Springer-Verlag, NY. Garey, 
M. and Johnson, D. (1979), Computers and intractability: a guide to the theory of NP-completeness, W.H. 
Freeman and Company, San Francisco. Ginsberg, A. (1987), A new approach to checking knowledge bases for 
inconsistency and redundancy, in 3rd Annual Expert Systems in Government Conference, IEEE, 102-111. Ginsberg, 
A. (1988), Knowledge-Based Reduction: A new approach to checking knowledge bases for inconsistency &#38; 
redundancy, in 7th National Conference on Artificial Intelligence (AAAI-88), 2, 585­ 589. Giordano, A. 
and Saitta, L. (1985), Modeling production rules by means of predicate transaction networks, in Information 
Science, 35, 1, 1-41. Gries, D. (1981), The Science of Programming, Springer-Verlag. Jafar, M. and Bahill, 
A.T. (1990), Validator, A tool for verifying and validating personal computer based expert systems, in 
Operations Research and Artificial Intelligence: The Integration of Problem Solving Strategies (Brown, 
D. E. and White, C. C., Eds.), Boston, 373-385. Landauer, C. (1990), Empirical methods in expert system 
validation, in proceedings of the AAAI-90 Workshop on Knowledge Based systems Verification, Validation, 
and Testing, Boston, MA. Lalement, R. (1993), Computation as Logic, Prentice Hall. Liu, N.K. and Dillon, 
T. (1991), An approach towards the verification of expert systems using Numerical Petri Nets, in Int. 
J. Intelligent Systems, 6, 255-276. Looney, C. G. (1988), Fuzzy petri nets for rule-based decision making, 
in IEEE Transactions on Systems, Man, and Cybernetics, 18, 1, 178­ 183. Miller, P. L., Blumenfrucht, 
S. J., Rose, J. R., Rothschild, M., Weltin, G., Swett, H. A., and Mars,N. J. I. (1986), Expert system 
knowledge acquisition for domains of medical workup: an augmented transition network model, in proceedings 
of the 10th Annual Symposium on Computer Applications in Medical Care, 30-35, Washington, DC. Murata, 
T. and Matsuyama, K. (1988), Inconsistency check of a set of clauses using petri net reductions, in Journal 
of the Franklin Institute, 325, 1, 73-93. Murray, T. J. and Tanniru, M. R. (1987), Control of inconsistency 
and 20th redundancy in PROLOG-type knowledge bases, in Hawaii International Conference on System Sciences. 
Nazareth, D.L. (1989), Issues in the verification of knowledge in rule­based systems, in Int. J. Man-Machine 
Studies, 30, 255-271. Nazareth, D.L. and Kennedy, M.H. (1991), Verification of rule-based knowledge using 
directed graphs, in Knowledge Acquisition, 3, 339­ 360. Nerode, A. and Shore, R. (1993), Logic for Applications, 
Springer-Verlag, NY. Nguyen, T. A., Perkins, W.A., Laffey, T.J., and Pecora, D.(1985), Checking an expert 
systems knowledge base for consistency and completeness, in IJCAI 85, 1, 375-378, AAAI. Nguyen, T. A.(1987), 
Verifying consistency of production systems, in Proc. 3rd Conference on Artificial Intelligence Applications, 
(IEEE) Computer Society, 4-8. Nguyen, T. A., Perkins, W.A., Laffey, T.J., and Pecora, D.(1987), Checking 
a knowledge base for consistency and completeness, in AI Magazine, 8, 2, 69-75. Pederson, K. (1989), 
Well-structured knowledge bases (Part I), in AI Expert, 4, 44-55; .Part II, in 7, 45-48; Part III, in 
11, 36-41. Perl, Y. and Shiloach, Y. (1978), Finding two disjoint paths between two pairs of vertices, 
in Journal of the Association for Computing Machinery, 25, 1, 1-9, January. Preece, A.D. (1989), Verification 
of rule-based expert systems in wide domains, in Research and Development in Expert Systems VI: Proc. 
Expert Systems 89 (Shadbolt, N., Ed), 66-77, Cambridge University Press. Preece, A.D. and Shinghal, R. 
(1991), DARC: A procedure for verifying rule-based systems, in Proceedings Expert systems World Congress 
(Liebowitz, J., Ed), 2, Pergamon Press, 971-979 Sedgewick, R. (1992), Algorithms in C++, Addison-Wesley, 
NY. Stachowitz, R., Chang, C., Stock, T., and Combs, J. (1987), Building validation tools for knowledge-based 
systems, in proceedings of the first Annual Workshop on Space Operations Automation and Robotics, 209-216, 
Houston, Texas. Suwa, M., Scott, A. C., and Shortliffe, E.H. (1982), An approach to verifying completeness 
and consistency in a rule-based expert system, in AI Magazine, 16-21. Suwa, M., Scott, A.C., and Shortliffe, 
E.H. (1984), Completeness and consistency in a rule-based system, in Rule-Based Expert Systems: The MYCIN 
Experiments of The Standard Heuristic Programming Project (Buchanan, B.G. and Shortliffe, E.H., Eds.), 
Addison-Wesley, 159-170</RefA>.  
			
