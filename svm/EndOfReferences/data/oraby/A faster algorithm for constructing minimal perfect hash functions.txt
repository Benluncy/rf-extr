
 A Faster Algorithm for Constructing Minimal Perfect Hash Functions * Edward A. Fox*# and Qi Fan Chen*# 
and Lenwood S. Heath* Department of Computer Science* and Computing Center# Virginia Polytechnic Institute 
and State University Blacksburg VA 24061-0106 Abstract Our previous research on one-probe access to 
large collec­tions of data indexed by alphanumeric keys has produced the first practical minimal perfect 
hash functions for this prob­lem. Here, a new algorithm is described for quickly finding minimal perfect 
hash functions whose specification space is very close to the theoretical lower bound, i.e., around 2 
bits per key. The various stages of processing are detailed, along with analytical and empirical results, 
including timing for a set of over 3.8 million keys that was processed on a NeXTsta­tion in about 6 hours. 
 Introduction Next generation information systems must support integrated access to large-state data, 
information, and knowledge bases. That integration must facilitate efficient operation, as well as ease-of-understanding, 
for both users and developers. Infor­mation retrieval and filtering, hypertext, hypermedia, natural language 
processing, scientific data management, transac­tion processing, expert systems, library catalog access, 
and other applications can all be built upon such an integrated environment. We have worked toward this 
goal of integrated access from two directions. First, the CODER (COmposite Docu­ment Expert/extended/effwtive 
Retrieval) system serves as a prototyping vehicle for our theories, models, approaches, and implementation 
efforts [6]. Its architecture allows black­board as well as client-server style communication in one 
or more communities of experts or algorithmic modules. A knowledge representation language has been developed 
for This work was funded in part by grants from rite National Science Foundation (Grants IRI-8703580 
and IRI-91 1699 1) and PRC Inc. Permission to oopy without fee all or part of this material is granted 
provided that tha copies are not mada or distributed for diract commercial advantage, the ACM copyright 
notice and the titla of the publication and its date appear, and notica ie givan that copying is by permission 
of the Association for Computing Machinery. To copy otherwise, or to rapublish, requiras a fee and/or 
specific permission. 15th Ann Int 1 SIGIR 92/Denmark-6/92 Q 1992 ACM 0-89791 -524-0192 /0006 /0266 . 
..$1 .50 266 CODER to give us control over inter-module communica­tion, facilitating transmission in 
a distributed environment of various types of dat&#38; information, and knowledge structures (including 
atoms, frames, and relations) [17]. As different versions have been developed, CODER has matured to han­dle 
a variety of applications such as electronic mail messages [71, Navy intelligence messages [1], and access 
to litera­ture on cardiology [10]. Lexical information, bibliographic records, thesauri, reference works, 
full-text, facsimile and other images, tabular data, hypertext, frames, semantic net­works, and other 
forms have been processed. The collections of information already integrated into CODER have grown to 
hundreds of megabytes, and current efforts involve work on collections measured in gigabytes. Our second 
direction has been to develop an object­oriented databases ystem tailored to the information retrieval 
environment of interest, using minimal perfect hash func­tions (MPHFs) to ensure space and time efficient 
index­ing. The LEND (Large External object-oriented Network Database) system, used in CODER, has evolved 
as well, through two major versions. While CODER originally used Prolog database facilities, or relied 
upon special manager routines coded in C to provide access to large collections of data or information, 
all shared access to information by CODER modules now involves use of Version 1 or Version 2 of LEND. 
A complete description of the current version of LEND appears in [4] and will also be published elsewhere. 
This paper focuses on a key part of LEND our most recent algorithm for finding minimal perfect hash 
functions, which obtains results close to the theoretical lower bound. 2 Minimal Perfect Hashing In our 
implementation of LEND, we use optimal hashing techniques to make operations as efficient as possible, 
pro­viding: one-probe access to a record, given its key, no collisions to be resolved, and full utilization 
of hash table space. o n-1 iII1III1 KeySetS < 1 I Hash Table T I II m-1 o n-1 1I I II II I IKeySetS ) 
I 1I I I I I IHashTableT o n-l Figure 1: Perfect and Minimal Perfect Hash Functions Optimal speed for 
hashing means that each key from the key set S will map to a unique location in the hash table T, thus 
avoiding time wasted in resolving collisions. That is achieved with a perfect hash function (PHF), whose 
op­eration is illustrated at the top of Figure 1. When the hash table has minimal size, i.e., is fully 
loaded, with ISI = IT!, the hash function is called minimal. When both properties hold, one has a minimal 
perfect hash function (MPHF) as shown at the bottom of Figure 1. Note that, in reality, key set S itself 
is usually neither ordered nor sequential, but can clearly be indexed by the integers (1. ., n 1 for 
convenience of illustration. These hash functions can be grouped further into several categories. First, 
there are static and dynamic functions, when static or dynamic key sets are involved. Our emphasis has 
been on static functions, since in many retrieval and other applications the key sets change slowly, 
if at all (e.g., on CD-ROM). Second, hash functions can point to individual objects, or to bins of objects. 
While LEND does support bin hashing [4], that discussion is beyond the scope of this paper, and in any 
case the methods used are derived from those considered here. Further, with large or variable size objects, 
or in-memory applications, direct location of single objects is desired, Third, hash functions can preserve 
an a priori key order­ing, or ignore that when ordered sequential access is not needed. In [8], we present 
some methods for building order­preserving minimal perfect hash functions. Since some types of OPMPHFs 
can be derived from MPHFs, we do not discuss that further here. In this paper, we consider minimal perfect 
hash functions pointing at individual objects in static collections where there is no a priori key order 
that must be maintained. We explain a new algorithm to find MPHFs and give experimental evi­ dence of 
its efficiency with large key sets.  3 MPHF Algorithm 1 To simplify discussion, we define essential 
terminology. U: key universe. IU I = N. . S.actualkey set.S c U,ISI=n<<N. ~ hash table. IT] = m, m > 
n. h: hash function. h: U -+ T. h is a Pcx-&#38;t hash function (HIP): no collisions, h is one-to-one 
on S. h is a minimal perfwt hash function (MPHF): no colli­ sions (i.e., PHF) and m = n, For a given 
key set S taken from universe U, we desire a MPHF h that will map any key k in S to a unique slot in 
hash table T. Until the 1980 s there were no known algorithms to find MPHFs for large key sets. Since 
1980, important contribut­ions to the theory and practice of perfect hashing were made by various investigators 
including Cichelli [5], Jaeschke 1[13], Mehlhom [14], Cercone, Krause, and Boates [2], Chang [3], Fredman 
and his colleagues [11, 12], and Sager [16]. The first practical algorithm for finding practical MPHFs 
for very large key sets, i.e., including millions of keys, was reported by Fox et al. The description 
[9] gives further detains on earlier work as well. The basic approach in [9] is to treat the problem 
as a search for desired functions in a large search space s. In actuality, preparatory Mapping and Ordering 
steps are needed so that fast Searching can take place. The overall Mapping­Ordering-Searching (MOS) 
scheme is illustrated in Figure 2. Mapping transforms the problem of hashing keys into a different problem, 
in a different space. Ordering paves the way for searching in that new space, so that locations can be 
identified in the hash table. Hashing then involves mapping from keys into the new space, and using the 
results of Searching to find the proper hash table location. From that perspective, the key results in 
[4,9] areas follows. . Search space s requires at least 1.4427 n specification bits (at least 21w27n 
distinct values must be in the space). s Finding an MPHF is a search problem that determines some appropriate 
value ins for an instance S (which is the key set). S is related tos through partitioning both .S ands 
into subsets Siandsi, fori = 0,1,2,... The basic algorithm discussed in [9], herein referred to as Algorithm 
1: Key Set S I Hash Table T Figure 2: Illustration of the Key Concepts is a probabilistic algorithm; 
is based on ordering the vertices in a bipartite depen­ dency graph; requires expected linear running 
time handles large sets containing millions of keys; and yields MPHFs of size c logz n bits per key (0.5< 
c < 1). Its behavior in terms of bits per key required to find art MPHF in a reasonable amount of time, 
for varying size key sets, is illustrated in Figure 3. Note that Algorithm 1 requires less than one word 
of spec­ification space for each key in S. However, this is signifi­cantly more space than the theoretical 
lower bound, which is roughly 1.5 bits per key. An altemativealgorithm discussed in [9], called Algorithm 
2 herein, did manage to produce MPHFs for large key sets with specification space size below 5 bits per 
key. Unfortu­nately, this method is relatively complicated, and finding the address for keys using an 
Algorithm 2 MPHF involves expen­sive multiplications. We have developed a new algorithm, described in 
the next section, which eliminates the need for multiplication, yields MPHFs with specification space 
be­low 2.5 bits per key, and is relatively easy to understand and implement. 12 10 8 e M il. .-9 m6 
4 2 0 1Oohw 200 W0 300000 4oocoo 5c&#38;o 6oc&#38;e Key SetSize Figure 3: Bits per Key for Algorithm 
1  4 MPHF Algorithm 3 The new algorithm for finding an MPHF, called herein Algo­rithm 3, is fully described 
in [4]. The basic results follow in this Section. Algorithm 3 corrects many of the problems with Algorithm 
 1. First, Algorithm 1 makes use of moderately large tables to specify the mapping for the characters 
that make up keys, that in turn lead to the pseudo-random numbers used in the Mapping stage. By using 
and extending Pearson s method [15], mapping tables containing only 128 characters are pro­duced. The 
results of the Mapping stage are sufficiently random so that more space-expensive approaches are unnec­essary. 
Thus, only 128 bytes are used in the hash function specification to describe the Mapping process. Second, 
in Algorithm 1, the Searching phase was less so­phisticated, requiring many unsuccessful operations to 
locate an acceptable solution. By adding an auxiliary index data structure, we have been able to reduce 
the searching time significantly in Algorithm 3. Third, Algorithm 3 deals with the need to reduce the 
size of the specification of the MPHF by radically changing the Mapping, Ordering, and Searching phases 
of Algorithm 1. In particular, no use is made of the bipartite dependency graph first suggested by Sager 
[16]. Rather, S is related tos in two steps: Keys are mapped to a bucket set B. (See Figure 4.) b= Il?[= 
(crz/(log2n+ 1)1,2 s c <4. Keys in each bucket are separately mapped to T. (See Figure 5.) In order 
to have space measured in bits per key instead of words per key, it is necessary to search for values 
whose Key Set [1 T  49=- RR 1I I o Pi n-l hll h12 0 /0 Buckets Bn I, i 0 P2 b-1 B2 B1 Figure 4: 
Mapping Stage of Algorithm 2 Buckets B u R? 0sort Bucket Sequence 1, 1 1 I Bo,o BO,I Bo,bl -=k5- Hash 
Table T 111111111111111111111111111111111111~ Figure 5; Ordering and Searching with Algorithm 2 269 
number is proportional to [en/ (logz n + 1)1 instead of n, as was done in Algorithm 1. This partially 
explains the need to introduce buckets into the process. The Mapping stage, and the Ordering and Searching 
stages, are illustrated in Figures 4 and 5, respectively. Further details are given in the following 
subsections. 4.1 Mapping The Mapping stage accomplishes several important goals. First, each of the n 
keys is mapped to an integer value, in therange O... n 1. This is done by pseudo-random hash function 
hlo which maps several keys onto some values and may leave other address values without any keys. See 
the top of Figure 5 for an illustration of the process.  hlo : S+{o,... )l]l] Second, we wish to shrink 
the range of integer values from n to b so that later we need only search among b values. Finding an 
MPHF which has specification size close to the lower bound can be accomplished when c is close to 2, 
i.e., when b is roughly 2n/ logz n. We can accomplish this by composing hlo with another function that 
will map into the range O...l -l. However, in the process we can, if we are clever, accom­ plish a third 
goat. In particular, we wish to separate the lkeys into two major groupings. Our second function, then, 
is re­ ally accomplished by two functions that operate upon disjoint portions of O...l -l. hll : {o ,..., 
P1}+{O,O, P2.1}2 -1} h12 : {Pl ,... ,1}+{ ~,~,, b,1} l} These, together with hlo, accomplish the mapping 
from keys to buckets. hll o hlo if hlo(k) < PI bucket(k) = h12 o hlo otherwise { Thus, the mapping function 
bucket(k) is composed of three functions: hlo randomly distributes keys into an auxil­iary integer set 
{O,... , n 1}, hll and h12 in turn randomly deliver them into B, in particular into the unequal size 
subsets B1 and B2. Note that h] 1 and h12 depend on two parame­ters pl and ~. Good values for these two 
parameters are experimentally determined to be around 0.6n and 0.3b, re­spectively. What this means is 
that roughly 60910of the keys (since P1 = 0.671 and hlo is likely to be relatively uniform at a coarse 
level) are mapped into roughly 30% of the buckets (since ~ = 0.3n), i.e., l?l = {O,..., ~ 1}. In effect, 
we are forcing the buckets produced by hl 1 to each hold many keys. This is fine, since our earlier work 
with searching indicates that large groups of keys can be managed if dealt with early in the search process. 
At the same time, the other 40% of the keys are spread by hlz into 70% of the buckets, i.e., B2 = {~,... 
,b 1}, yielding fewer keys per bucket. This is handy since during searching it is desirable to have small 
groups of keys processed towards the end of the operation. In summary, the Mapping stage, illustrated 
in Figure 5, accomplishes our goals of mapping to integers, compressing the range of integers, and separating 
big from smatl groupings of keys. 4.2 Ordering During the Ordering stage, illustrated in the top portion 
of Figure 5, we use the organization developed during Mapping to prepare for Searching. The key features 
of this stage are as follows. Buckets are ordered by decreasing sizes to obtain the bucket sequence {Bo)O,Bo,J,..., 
Bo,b_l}. (where the subscript o designates ordered buckets as opposed to initial buckets) Bucket sorting 
can be used as the maximal number of keys in B is known. Analysis indicates that because of our use 
of pseudo­random functions at each stage of the Mapping stage, we can estimate the number of buckets 
of each size. Even for very large key sets the largest buckets will have relatively small sizes. Clearly 
then a single pass through the buckets will yield the desired bucket sequence. Searching processes all 
keys in a bucket together, and proceeds from the largest to the smallest buckets. 4.3 Searching The 
Searching stage involves choosing a logz n + 1 bit pa­rameter value go for each of the buckets, so that 
each key in each bucket can be mapped by the finally constructed hash function, h, to a previously unused 
slot in the hash table T, Essentially, the group of keys in a bucket must all be fit into Tat the same 
time, since they are mutually constrained by virtue of the earlier processing that put them into the 
same bucket. Choosing the parameter value for the bucket must assure that its pattern of entries can 
be fit into open slots in T. As we try different g () values, we rotate the pattern until we find a good 
fit. The Searching process maps keys in each bucket Bo,i to T via the function h: h20 : Sx{o,l} +{o,...,1}l} 
h(~) = {ko(k, d) + g(B.,j)} mod n. This final hashing function ho has simple form and is easily computable 
for any key in S. It is formed as the sum of two values. h20 is a pseudo-random function mapping keys 
in each B.,i to distinct values in {O, n 1}. Recall that log2 n+ 1 bits are allocated to each bucket. 
A designated bit d in these bits is used by h20 as part of the seed. As O and 1 can be the value ford, 
hzo can generate two different sets of integers for keys in Bo,i. This adds a degree of freedom to the 
searching, avoiding failures by changing the d values as needed. An integer r globrd to atl buckets has 
also been used as part of the seed to hzo. Should some bucket fail to be mapped to distinct integers, 
a new value for r is tried. With the help of r, the same bucket sequence can be maintained. In the following, 
we use the term pattern set Pi for the set of values of h20 corresponding to keys in Bo,i. Each g(BO,i) 
takes log2 n bits. g(&#38;+ ) ro~tes the pattern set for a fit. g(Bo,i ) can be selected by aligning 
an item in thepattem with an empty slot in T. (This is an important heuristic to improve efficiency.) 
 4.3.1 Auxiliary Data Structure During the Searching phase, a considerable speedup results from using 
an auxiliary index data structure to locate empty slots. Recall that a tit means that h maps each member 
in the pattern set Pi to an empty slot. Therefore, an arbitrary member in Pi can be aligned with an empty 
slot, and testing can then determine whether the remaining members fit into other empty slots. A proper 
alignment then yields a proper g value. We define x E the index of the empty slot, and u s the member 
of Pi to be aligned with z. The rotation offset or g(Bo,i ) is (x u) mod n. The method gives considerable 
speedup when key sets are of moderate to large size. Figure 6 illustrates the auxiliary index data structure, 
along with the hash table. In the program and diagram, there are three arrays called randomTable, mapTable 
and hashTable. The randomTable [0, n 1] array is used to remember currently empty slots in the hash table. 
As it is preferable for each Pi to fill the hash table in a Before After Slot Switched filledcount Slot 
Selected fiUedCount O r?.ndomTable mapTable I# hashTrible v rmdmnTab,. M contains the in&#38;x of tbe 
B a filled da 1 first empty S1C4 Stands fel tk htd?x of o :%%-x f l! %%%% f hashTable mapTsble stmclsfor 
an empty centaim8Pintcrto an entry u slot 0 in the Randm Tabk stmdsfmllfilkd 8 slot Figure 6: Auxiliary 
Index Data Structure and Filling of a Hash Table Slot random fashion, this array initially contains a 
random per­mutation of the hash addresses in [0, n 1]. The pointer f illedCount is initially O. It is 
an invariant that any slots to the right side of f i 1 ledCount (inclusive) are empty and any ones to 
the left are filled. This property guaran­t~ only empty slots are searched to fit Pi. For any un­filled 
slot z in hashTable [ ] , the mapTable [0, n-1] array contains pointers pointing at randomTable [ ] such 
that randomTable [mapTable [z] ] s x. Thus, given an empty slot z in the hash table, we can locate its 
posi­tion in the randomTable [ ] array through mapTable [ ] . Suppose a slot v of ha.shTable, which is 
referred to in location y in randomTable [ ] , needs to be filled and the invariant needs to be maintained 
after the filling ac­tion. Then we can just switch the pointers corresponding to mapTable [ randomTable 
[ f i-lledCount ] ] and mapTable [y] and advance f illedCount right one po­sition. See the positions 
of the two differently shaded boxes in the topmost part of Figure 6. When IPi I > 1, a sequence of switching 
is required. 4.3.2 Analysis of Tests Required to Fit a Pattern Analytical study of the Search process 
lets us predict the number of tests that are needed during searching. The cost of fitting a pattern of 
size j into a n slots hash table T with ~ slots already filled can be approximated in the following way. 
The total number of slot subse~ of size j from T is (~), out Table 1: Number of Tests Average vs. Expected 
Value of which only ~ ~f) can fit the pattern. Imagine (~) subsets as (~) balls in a bag, and among them 
W_ G ~ ~f) are white bal~ and Bj s ~) ~~~) are black balls. The cost of fitting the pattern is equivalent 
to repeatedly drawing batls from the bag until the first white ball is seen, without putting back previously 
drawn black balls. Let ~ be a random variable equating to the number of draws to obtain the first white 
ball in such an experiment. We have ? 1 Bj r WJ Pr(~ =z) = n ,=OBj+Wj ~ Bj+Wj ) ( When j is small and 
n is huge, the fitting process can be approximated as a Bernoulli experiment where the balls after being 
drawn are returned to the bag. Let -Zj be a random variable equating to the number of draws to obtain 
the first white batl in the Bernoulli experiment. We have Pr(Zj =%) =   (Bj~wj)z- Bj~w Let Wj p = Bj+wj 
Then, the expected value of Zj is 1/p. This simple formula can be used in the algorithm to predict the 
number of tests for a fit. If the predicated value is too large (> n), then there is no point to attempt 
fitting the pattern. The expected values closely match those found empirically, as given in Table 1. 
This situation is further illustrated in Figure 7, which records the number of tests required during 
the Searching T@s F!ucketSize 50W . ......... ..................................................... 
. ..... .... ........ ....... ~ 4m 15 Som to XiM 1 5 IIlow 0 0 1 84 167 254 333 4t6 499 582 W5 74s Bucket 
Sequence Tests ve Bucket Sizes (keys 4)ss, bilsikey = 2.s) Figure 7: Tests vs. Bucket Sizes 4K keys, 
2.6 bits/key stage. The horizontal axis shows the progression over time as buckets are processed. The 
staircase curve shows the size of the buckets as they are handled. The labels on the right show the range 
of bucket sizes, from less than 20 down to 1. l%e number of tests required to handle each bucket, as 
indi­cated by labels on the left, is shown by the many spikes near the horizontal axis (but sometimes 
rising to over 4000, which indicates that the original designated bit d for the bucket did not work and 
had to be changed). In general, the number of tests required is relatively small. In summary, the Searching 
phase computes go values that properly rotate patterns until all elements of a bucket fit into the hash 
table. Our auxiliary data structure speeds up searching by assuring we make tests in a random fashion, 
by avoiding tests involving previously filled slots, and by reducing the number of memory accesses for 
each test. Our analysis yields an estimate for the number of tests needed at any given stage of the processing, 
allowing us to omit testing when failure is likely. Empirical studies show our estimates to be quite 
accurate, indicating that Searching is generally fast. 4.4 Timing Results Algorithm 3 has been applied 
to a wide variety of key sets, frOm small ones to very hrge onm. A 256-key get md the resulting hash 
function specification, for example, is shown in its entirety in [4]. The 3.8 million key set used in 
previous studies has been processed using Algorithm 3 with param­eters set to obtain MPHFs with 2.4 up 
to 3.5 bits per key. Results are given in Table 2. Note that our algorithm pro­cesses very large key 
sets in chunks which are saved on disk when all cannot fit into primary memory. Figure 8 illustrates 
how the Searching time, and hence the total time, for finding MPHFs varies with bits per key requirements. 
We have found similar behavior with a key set of French words numbering approximately one half million. 
Bits/Key Map Order Search Total 2.4 1890 5928 35889 43706 2.5 1886 5936 25521 33343 2.6 1887 5978 18938 
26802 2.7 1887 6048 14486 22421 2.8 1897 6170 11602 19669 2.9 1894 6088 9524 17506 3.0 1905 6108 8083 
16095  3.1 1894 6119 6998 15011 3.2 1885 6141 6110 14136 3.3 1884 6224 5436 13544 3.4 1886 6197 4958 
13041 3.5 1886 6191 4586 12663 Not&#38; CPU times are for NeXTstation (68040, 64MB), cc++ v.1.36.4, GNU 
g++ library v.1.39, 3,875,766 keys in 5 chunks (of 800K) Table 2: Timing Results for Algorithm 2 0 2.4 
 2.6 2.8 3.0 3.2 3.4 6 BiwKP.y Figure 8: Plot of Table 2 Timing Results Algorithm 3 seems to be able 
to find MPHFs for very large key sets using less than 3 bits per key of specification space, the most 
space efficient results that have been reported to date. Note that when more than 3 bits per key are 
used, there is a linear relationship between key set size and total time to find an MP~, as the lower 
bound for bits per key is approached, the time required grows rapidly as more and more time is used to 
fit a bucket into the hash table during the Searching process. Note that Algorithm 3 was able to find 
an MPHF for the 3.8 million key set in about 6 hours on a NeXT workstation, with 2.7 bits per key. This 
translates into about a megabyte of space needed to store the MPHF specification for one of the largest 
key sets we have been able to identify, suggesting that Algorithm 3 is quite feasible for use on modem 
workstations. Conclusion This paper has discussed a new fast algorithm for finding minimal perfect 
hash functions. Even the largest key set we have found can be processed in a number of hours on modem 
workstations using our new algorithm. With about 2.5 bits per key of space for the MPHF specification, 
single access to a key is guaranteed, using a fully loaded hash table. The algorithm described has been 
applied to the problem of tie compaction, yielding efficient duced space utilization. A related where 
m = kb keys are perfectly each holding a group of k keys, These two methods are included operation with 
greatly re­ algorithm, for bin hashing, distributed into b buckets has also been developed. in the LEND 
system and explained in [4], along with discussions regarding the utility of hashing methods for information 
retrievat applications. Acknowledgements Professor Abraham Bookstein provided the large French word 
list used in our study from the ARTFL Project at the University of Chicago. Dr. Martin Dillon, of OCLC 
Inc. in Dublin, Ohio provided the data used in our 3.8 million key file from their catalog records. 
References <RefA>[1] Richard Bamhart. The Advanced Naval Message An­alyzer. Videotape production, VPI&#38;SU, 
November 1990. Richard Bamhart as host, producer, script-writec Edward Fox as executive producer, project 
director. Discusses the CODER system. [2] N. Cercone, M. Krause, and J. Boates. Minimat and almost minimal 
perfect hash function search with ap­plication to natural language lexicon design. Computers and Mathematics 
with Applications, 9:215-231, 1983. [3] C. C. Chang. Letter oriented reciprocal hashing scheme. Information 
Sciences, 38:243 255, 1986. [4] Qi Fan Chen. An object-oriented database system for efficient information 
retrieval applications. PhD thesis, Virginia Tech Dept. of Computer Science, March 1992. [5] R. J. Cichelli. 
Minimal perfect hash functions made simple. Communications of the Association for Com­puting Machinery, 
23: 17 19, 19g0. [6] E. A. Fox and Robert K. France. Architecture of an expert system for composite document 
analysis, rep­resentation and retrieval. International Journal of Ap­proximate Reasoning, l(l): 151-175,1987. 
[7] [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] Edward A. Fox. Development of the CODER system. A 
testbed for artificial intelligence methods in informa­tion retrieval. Information Processing &#38; Management, 
23(4):341-366, 1987. Edward A. Fox, Qi Fan Chen, Amjad M. Daoud, and Lenwood S. Heath. Order-preserving 
minimal perfect hash functions and information retrieval. ACM Transac­tionson Information Systems, 9(3):281 
-308,July 1991. Edward A. Fox, Lenwood S. Heath, Qi Fan Chen, and Amjad M. Daoud. Practical minimal perfect 
hash func­tions for large databases. Communications of the Asso­ciation for Computing Machinery, 35(l): 
105 1 21, Jan-Ullly 1992. Edward A. Fox, M. Prabhakar Koushik, Qi Fan Chen, and Robert K. France. Integrated 
access to a large med­ical literature database. Technical Report TR-91-15, Department of Computer Science, 
Virginia Polytechnic Institute and State University, Blacksburg, VA, May 1991. M. L. Fredman and J. Kom16s. 
On the size of separating systems and families of perfect hash functions. SLAM Journal on Algebraic and 
Discrete Methods, 5;61-68, 1984. M. L. Fredman, J. Kom16s, and E. Szemer6di. Storing a sparse table with 
0(1) of the Association for 544,1984. G. Jaeschke. Reciprocal ating minimal perfect of the Association 
for 833,1981. worst case access time. Journal Computing Machineq, 31 :538 hashing a method for gener­hash 
functions. Communications Computing Machinery, 24:829 K. Mehlhom. On the program size of perfect and 
univer­sal hash functions. In Proceedings of the 23rd Annual IEEE Symposium on Foundations of Computer 
Science, pages 170-175,1982. P. K. Pearson. Fast hashing of variable-length text strings. Communications 
of the Association for Com­puting Machinery, 33(6):677-680, June 1990. T. J. Sager. A polynomial time 
generator for minimal perfect hash functions. Communications of lhe Associ­ation for Computing Machinery, 
28:523 532, 1985. M. Weaver, R. France, Q. Chen, and E. Fox. Using a frame-based language for information 
retrieval. Inter­national Journal of Intelligent Systems, 4(3):223 257, 1989. </RefA> 
			
