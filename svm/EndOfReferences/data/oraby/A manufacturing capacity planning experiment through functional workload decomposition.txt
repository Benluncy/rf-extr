
 A Manufacturing Capacity Planning Experiment Through Functional Workload Decomposition T. Paul Lee AT~T 
Bell Laboratories Holmdel, NJ 07733 ABSTRACT In this paper, we describe an experiment to evaluate a 
distributed architecture via functional database workload decomposition. A workload in a circuit pack 
assembly environment was decomposed and mapped onto a frontend/backend distributed computer architecture. 
To evaluate this distributed architecture, an operational model for capacity planning was devised, and 
performance and cost- effectiveness measures were chosen. Model parameters were estimated through benchmark 
experiments in a distributed system consisting of various super-microcomputers connected by a CSMA/CD 
local area network with INGRES as the database management system. The frontend/backend architecture consists 
of a backend data repository and analysis computer system and a few frontend computer systems dedicated 
for data collection and manufacturing process verification. Because of the significant software overhead 
in communication protocol and database processing, information exchange was batched between the backend 
and frontend systems to amortize such cost to improve overall system performance. Results of the experiments 
were analyzed to gain quantitative insight on the feasibility of such decomposition and its mapping onto 
the proposed architecture. With sufficient batching, the proposed distributed architecture not only has 
 Permission to copy without fee all or part of this material is granted provided that the copies are 
not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the 
publication and its date appear, and notice is given that copying is by permission of the Association 
for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specfic permission. 
&#38;#169; 1988 ACM 0-89791-254-3/88/0005/0141 $1.50 141 more overall system capacity, but also is more 
cost-effective than the typical centralized architecture. The approach described is applicable in more 
general contexts. Advantages of such distributed systems include the relative robustness of the distributed 
architecture under single point failure mode and the ease of capacity growth by upgrading the computer 
systems and/or by increasing the number of frontend systems. 1. INTRODUCTION In this paper, we describe 
an experiment to evaluate a distributed computer system architecture via functional workload decomposition. 
There were many proposed distributed architecture and distributed prototypes in the literature[I]; however, 
very few dealt with the design and performance of distributed applications that ran on these systems. 
We propose a methodology to evaluate the overall cost-effectiveness of such systems emphasizing the workload 
decomposition and its mapping onto the distributed architecture. This performance study stems from a 
characterization effort of the application workload for continuous in-line manufacturing lines. The workload 
of these lines is in essence the tracking and control process of an assembly line of circuit packs. This 
workload was used to study issues in the capacity planning of a process information control system (PICS) 
and in the feasibility of a distributed version for this system. We decomposed this workload into a distributed, 
but functionally equivalent one, and mapped it onto a frontend/backend distributed computer architecture. 
The frontend/backend architecture consists of a backend data repository and analysis computer system 
and a few frontend computer systems dedicated to data collection and manufacturing process verification. 
To evaluate this distributed architecture, an operational model for capacity planning was devised, and 
performance and cost-effectiveness measures were chosen. Model parameters were estimated through benchmark 
experiments in a distributed system consisting of super-microcomputers connected by a CSMA/CD [2] local 
area network with INGRES [3] as the database management system. Because of the significant software overhead 
in communication protocol and database processing, information exchange was batched between the backend 
and frontend systems to amortize such cost to improve overall system performance. Results of the experiments 
were analyzed to gain quantitative insight on the feasibility of such decomposition and its mapping onto 
the proposed architecture. With sufficient batching, the proposed distributed architecture not only has 
more overall system capacity, but also is more cost-effective than the typical centralized architecture. 
The results presented in this paper are not limited to the specific computers and the local area network 
used in the experiment; the approach is applicable in more general contexts. Advantages of such distributed 
systems include the relative robustness of the distributed architecture under single point failure mode 
and the ease of capacity growth by upgrading the computer systems and/or by increasing the number of 
frontend systems. The workload is described in detail in the next section. Decomposition of the workload 
and the proposed distributed computer system architecture is discussed in Section 3. The capacity planning 
model is presented in Section 4. Section 5 describes the distributed system on which our experiments 
were performed, and the parameters we used in the design of our experiment. Section 6 contains the analysis 
of measurement data for the proposed distributed architecture. We summarize the results and conclude 
in the last section. 2. WORKLOAD DESCRIPTION The workload used in this study is that of a typical manufacturing 
line for circuit pack assembly and test/repair. The system runs under UNIX Â® System V on a minicomputer 
See . and uses a INGRES-like relational database t The query syntax used in this paper is similar to 
that used INGRES; it is strictly for illustrative purpose. management system. We introduce the workload 
by first giving a simplified view of assembly line operations; then, the workload is characterized in 
terms of database queries that were used in the PICS applications. 2.1 Tracking Assembly Line Operations 
The assembly and test/repair process takes a number of operations (processing steps) on the production 
lines. This is the number of tracking or bar-code scanning points that are visible to the PICS system. 
The amount of information kept regarding each circuit pack is proportional to the number of tracking 
points a pack visits during the manufacturing process. Typical operations include label generation, insertions, 
soldering, mechanical assembly, baking, testing, repairing and shipping. This number is determined by 
the design of the production process. The PICS system keeps track of the operations performed, as well 
as the time in and the time out of each work station for each circuit pack. Furthermore, it checks whether 
an operation should be performed at a particular point of the manufacturing process and whether the machine 
is properly configured to do the operation in question.  2.2 Workload In Terms Of Database Queries The 
workload activities of the production lines can be classified into two major categories: 1. tracking 
and verification of operations 2. batch report generation for the manufacturing process  In the context 
of a database management system, workload activities can be understood and characterized in terms of 
database queries. The batch report generation is called build. One such build example is "to find the 
last station each circuit pack was at." The query looks like the one shown belowt: (batch report query) 
retrieve (pcs.Code, pcs.Series, ph.Station, ph.PackId, ph.OpTime) where ph.PackId = pcs.PackId and ph.OpTime 
= max(ph.OpTime by ph.PackId) Two range variables, pcs and ph, used in this query correspond to relations 
PackCodeSer and PackHistory respectively. PackCodeSer is the relation that tabulates code and series 
numbers for each circuit pack. A circuit pack is uniquely identified by the PackId assigned at the time 
of label generation. PackHistory is the relation that PackHistory relation. keeps a record for each 
operation performed on a circuit pack. This relation contains station and operation timestamps as well 
as other information. In database terms, this query is a join operation between two relations on the 
attribute Packld and only the most recent (in terms of OpTime) record will appear in the result of this 
query. This is a rather expensive query to run since it reads thousands of records from these two relations 
and also consumes many CPU cycles. A work-in-process (WIP) query is considered as another type of batch 
query. One such query looks like the following : (WIP query) retrieve (p.PackId, pcs.Series, sa.LaNum) 
where p.PackId = ph.PackId and ph.PackId = pcs.PackId and ph.PackId = sa.PackId and ph.Station ----CurStation 
and ph.OpTime = max(ph.OpTime by ph.PackId) This is another complicated join involving four relations. 
The result of this query is the pack series (Series) number and lot number (LaNum) of each circuit pack 
found at station CurStation. This type of WIP information is useful for production management and control. 
A query of this kind ran at regular intervals. Timely information like this is critical to the monitoring 
and adjustment of the manufacturing process. The remaining sections of this paper will focus on the first 
type of workload activities, that is, process tracking and process verification. Corresponding to each 
bar-code scanning, three "bookkeeping" queries are invoked. These constitute almost all queries found 
in the first type of activities. They look like the following queries :  (bookkeeping/interactlve queries) 
retrieve (lasts ---- p.LastSta, lasto = p.LastOp, lastti = p.LastTimeIn) where p.PackId--id replace p 
(lastSta = lasts, LastOp = lasto, LastTimeIn ----lastti) where p.PackId----id append to PackHistory (PackId----id, 
TimeIn=lastti, TimeOut=timeo, Station=lasts, Operation=lasto) The first query retrieves the relevant 
information about a circuit pack from the Pack relation; the second query, after some minimum CPU processing, 
replaces the corresponding record with up-to-date information. The third query is to append relevant 
tracking information to the These three queries constitute primarily the PICS workload of the circuit 
pack assembly process. The ability to handle these interactive workloads in a timely fashion is critical 
to the operation of the manufacturing lines. These requirements form the basis for evaluating PICS system 
capacity in comparing different kinds of computer system architectures. It was recognized, through on-line 
monitoring of resource consumption in the PICS system, that the CPU usage is a good estimator for system 
capacity; thus, our benchmarking and measurement effort in the following sections will take this into 
account. 3. DISTRIBUTED ARCHITECTURE AND WORKLOAD DECOMPOSITION From the description of the PICS workload, 
we find two distinct processing requirements for the database management system. One is the batch query 
that scans through many data records in the storage devices and deduces some snapshot information about 
the production line. This type of query consumes many computer resources and it is not unusual for it 
to run for several minutes before finishing. The other is the bookkeeping or interactive query that updates 
or appends new information at appropriate points of the manufacturing process for each circuit pack. 
This uses a small amount of computer resources; however, it has to compete for resources with batch queries 
if they get to run too often. The system has to be responsive for these interactive queries in order 
to run a production line smoothly. To allow incremental growth and to be more cost-effective for the 
PICS system, we propose a distributed frontend-backend architecture and a correspondingly decomposed 
workload. They are described in the following subsections. 3.1 Distributed Frontend-Backend Architecture 
A group of computers are connected together by a local area network. Functionally, one is designated 
as the backend computer system that serves as the data depository and analysis center. The rest of them 
are frontend computer systems that collect tracking information and perform process verification. Each 
frontend system is responsible for a few work stations, i.e., tracking points, of the manufacturing line. 
One example of this distributed architecture is depicted in Figure 1. In this example, we show a Sbck 
system as the backend machine and four Sf,~t systems as the frontend machines all connected with a CSMA/CD 
local area network. Usually, the backend computer needs to be more powerful in its CPU speed and has 
more storage capacity than that of a frontend computer system. This distributed architecture is quite 
cost-effective since the workload can be divided up naturally. It also facilitates incremental growth 
in overall capacity by adding more frontend systems to the line or by upgrading the frontend and/or backend 
systems. The batching strategy used when data is exchanged between a frontend system and the backend 
system provides another advantage of this distributed architecture. Given the relative performance level 
of database and network operations, it is cost-effective to transmit large amount of data through the 
local area network and to do bulk database appends/updates with a single query. This will become apparent 
as we discuss the batching strategy used in the design of our experiment.  8.2 Workload Decomposition 
The decomposition of the workload yields a functionally equivalent frontend workload and backend workload. 
The backend and frontend systems cooperate to provide an equivalent view of the data to the user at the 
backend system. Users at the backend system see a complete PICS database for their analysis work and 
batch queries. While the Pack and PackHistory relations are retained in the backend system, the frontend 
machine creates two temporary relations to hold tuples for the Pack and PackHistory relations. After 
a pre-defined number of transactions are collected at the frontend machine, the contents of these temporary 
relations are sent to the backend machine for updates. Delays in data updates are minimal relative to 
the times required by various operations in the manufacturing process. Details of the database queries 
corresponding to the decomposed workload are given in the following subsections. 3.2.1 Decomposed Frontend 
Workload The frontend workload (in terms of database queries) looks like the following : (frontend -initialization) 
create subph (PackId=cl2, TimeIn=i4, TimeOut=i4, Station----cl2, Operation=cl2) create subpack (PackId=cl2, 
LastSta=cl2, LastOp=cl2, LastTimeIn=i4)  (frontend -inner loop) append to subpack (Packld=id, LastSta="S-001", 
LastOp = '~abeling", LastTimeIn----1020) append to subph (PackId----id, TimeIn----1000, TimeOut=1020, 
Station----"S-001", Operation ---- '~Labeling") (frontend -exit) copy subph 0 into "/usr/frontend/phfile" 
copy subpack 0 into "/usr/frontend/ptfile" "send" phfile and ptfile to backend system using the file 
transfer utility destroy subpack, subph The first part of this frontend workload is the initialization 
phase of one frontend/backend interaction; the frontend computer creates two temporary relations subph 
and subpack to collect tuples for relations PackHistory and Pack, respectively. The middle part is the 
inner loop that cumulates tuples as operations are performed and circuit packs are scanned. The last 
part copies temporary relations into standard UNIX files, and sends them to the backend machine through 
the network file transport utility. The temporary relations are destroyed after the data is received. 
 3.2.2 Decomposed Backend Workload The backend workload (in terms of database queries) looks like the 
following : (backend -initialization) create PackTemp (PackId=c12, LastSta----c12, LastOp=c12, LastTimeIn=i4) 
(backend -bulk replace and append) copy PackTemp 0 from "/usr/backend/rje/ptfile" replace p (lastSta 
---- pt.lastSta, LastOp ---- pt.lastOp, LastTimeIn = pt.LastTimeIn) where p.PackId=pt.PackId copy PackHistory 
0 from "/usr/backend/rje/phfile" (backend -exit) destroy PackTemp The first part of this backend workload 
is the initialization phase. The middle part is the bulk replace and bulk append operations to the relations 
Pack and PackHistory, respectively. The last part cleans up the temporary relation used in the update 
process. 4. An Operational Model for Capacity Planning Figure 2 is an operational model of a manufacturing 
line which depicts the material flow and information flow in relation to the centralized PICS system 
that monitors and controls the process. From the PICS point of view, the manufacturing line is a sequence 
of work stations with circuit packs arriving at certain rate X i to the front of the line. Operations 
are performed as circuit packs move down the line; for each operation, one interactive transaction (three 
queries in the case of centralized architecture) with CPU service requirement 1/# i is completed by the 
PICS system. We assume that there are, on the average, n operations performed on each circuit pack. The 
CPU utilization Pi attributable to these interactive transactions is as follows : where n is the number 
of operations or work stations The batch query arrives at a much smaller rate )~b, but with a large CPU 
service requirement 1/# b at the PICS system. As we suggested earlier, we use CPU utilization as the 
capacity estimator for the PICS system. Since CPU cycles left by interactive queries are used by batch 
queries, we conveniently express system capacity as the capability of handling interactive queries. More 
directly, the system capacity C is thus defined as the circuit pack arrival rate the system is capable 
of handling : C- n where n is the number of operations or work stations Since this is a throughput measure, 
the stochastic nature of the arrival process to the PICS system is not important. Figure 3 depicts an 
operational model of the distributed frontend-backend architecture. As an example, four frontend computer 
systems are shown here; each is in control of three work stations. The clustering factor c, the number 
of work stations controlled by a frontend system, is an important factor in determining the system capacity 
and overall system cost. With this distributed frontend/backend architecture, circuit packs arrive at 
the same rate ~; to the front of the line as to that of the centralized PICS system. The number of operations 
n for each circuit pack remains the same as well. For each operation performed, one interactive transaction 
is completed by the frontend system. The records stored at the frontend system are sent to the backend 
every b transactions (this is one frontend-backend interaction) with CPU service requirement 1/~tBE ' 
at the backend system and CPU service requirement 1/~tFE ' at the frontend system. The service requirements 
1/~FE ' and 1/~BE ' are functions of batch size b, and both CPU service requirements include transaction 
processing and communication overhead. The CPU utilization PFE,at the frontend attributable to these 
interactive transactions and frontend-backend interactions is as follows : c)~ i PFE, = "b #RE, where 
c is the clustering factor, and b is the batch size Similarly, the CPU utilization PBE, at the backend 
attributable to these frontend-backend interactions is as follows : n)x i PBE, ~ b#BE ' where n is the 
number of operations or work stations, and b is the batch size The batch query arrives at a same rate 
kb, and with the same CPU service requirement 1/~ b at the backend of the PICS system. Similar to the 
capacity measure used for the centralized PICS architecture, the system capacity C a for the distributed 
architecture is defined as : C a = min ( bttfE~', b~BE' ) c n where n is the number of operations, c 
is the clustering factor, and b is the batch size 5. DESIGN OF EXPERIMENT 5.1 Experimental Environment 
Experiments were designed and carried out in an environment consisting of a few 8bck supermicros connected 
via a CSMA/CD local area network. A UNIX System V 2.0.4 swap-based operating system was used. The low 
level CSMA/CD protocols are very similar to the better known ETHERNET protocols[4]. The physical network 
medium consists of a coaxial cable with 10 MHz transceivers and tabs connected to the interfaces of the 
Sbc k supermicros. AT3~T INGRES/CS version 1.0/02CS.05 was used on the Sbck supermicros as the database 
management system[3]. Workload (application) benchmarks were coded in language C using the embedded queries 
(EQUEL) of INGRES/CS. The "simulated" PICS database was populated with a minimum number of records at 
the beginning of each measurement session. A file transfer utility with status report option was used 
to do file transfer, and UNIX mail was used to detect the completion of file transfer. The UNIX system 
activities report (sar) facility was the primary tool used for the measurement. The CPU times consumed 
in various experiments were inputs for the capacity planning model.  5.2 Parameters of Experiment In 
this experiment of architectural comparison, we have included two parameters that have practical implications. 
The first is the batch size in terms of the number of information records retained at the frontend before 
they are sent to the backend system. The second is whether process information is "cached" at the frontend 
systems. For example, this information may include routing (the sequence of operations to be carried 
out) for a particular type of circuit pack. 5.2.1 batch size We shall use the number of interactive 
transactions retained in the frontend as the unit of hatching in the experiment. Information batching 
meshes very well with the physical operation of the production line since circuit packs are transported 
in magazines containing some fixed number of circuit packs. This batching strategy is needed mainly for 
two reasons. First, user-perceived speed of the local area network is well below the raw bit rate of 
the physical network. The discrepancy comes from the CPU overhead in the interactions between user program 
and system kernel, and various layers of network communication protocols. Second, the interpretative 
nature of the INGRES database management systems makes repetitive executions of the same kind of transactions 
costly. A query gets re-parsed and the execution plan gets re-evaluated each time although it might have 
been done previously many times. Hence, batching tuples and performing bulk updates and appends are very 
cost-effective. 5.2.2 frontend with caching versus frontend without caching It is natural to expect 
that when (static) information is retained/cached in the frontend systems, many process verification 
operations can be done without help from the backend system. Examples of these mostly static data are 
work station profiles (status, capability, and authorization) and circuit pack information (operations, 
code, version, and lot numbers). This is particularly applicable to the automatic in-line manufacturing 
line environment in which the manufacturing process deals with a small number of circuit pack types (codes). 
Large volumes of circuit packs flow through a somewhat rigid path. This corresponds to the "frontend 
with caching" scenario of the proposed distributed PICS architecture. On the other hand, if we have a 
flexible manufacturing process, there are more codes with smaller volumes. The caching benefit described 
above will be much smaller. The cache has to be updated often since work stations are doing many jobs 
with different operational requirements. The manufacturing control and verification of co-existing circuit 
packs in the system becomes more complicated; frequently, only the backend has enough up-to-date information 
to make the proper decisions. In the extreme, this corresponds to the "frontend without caching" scenario. 
In the experiment, we incorporated this overhead in terms of extra queries that had to be executed in 
both the frontend and backend systems. The extra work in the frontend is given below : (frontend- initialization) 
create req (PackId--c12) (frontend -inner loop) retrieve (lasts = p.LastSta) where p.PackId=id append 
to req (PaekId=id) (frontend -exit) copy reqO into "/usr/frontend/reqfile" "send" request file to backend 
system using the file transfer utility "get" information back from the baekend system copy Pack() from 
"/usr/backend/rje/cache" destroy req Notice that the request is also batched, and the unit of batching 
is chosen to be the same as that used in the earlier discussion. The extra work in the backend is given 
as follows :  (baekend -initialization) create Request(PackId~c12) (baekend - bulk replace and append) 
"read" request sent by the frontend system copy Request() from "/usr/backend/rje/reqfile" retrieve into 
temp(p.all) where p.PackId -- x.PackId copy TempO into "/usr/frontend/cache" "send" information to frontend 
system using the file transfer utility (baekend -exit) destroy Temp, Request These two scenarios represent 
the extreme cases for manufacturing process verification and control. It is useful to visualize the range 
(between these two extremes) that we have to consider for determining the tradeoffs in the architectural 
comparison. 6. RESULTS OF BENCHMARK RUNS AND THEIR USE IN CAPA CITY PREDICTION   6.1 CPU Service Times 
CPU service times (Sbck) consumed in the benchmark runs, as a function of batch size, (0,10,20,30) are 
depicted in Figure 4. We estimated ~i, lZFE,, and /~BE, from these results as a function of batch size. 
Recall that the batch size is the number of transactions that a frontend system batches before it sends 
them to the backend computer system. Each transaction at the frontend corresponds to one bar-code scan 
for each operation (work station) performed on a circuit pack. The curves included both cases with frontend 
caching (Cached case) and without frontend caching (Non-Cached case). The intercepts of these curves 
at the y-axis when batch size equals zero represent the overhead required to do network communications 
and to do minimum database operations. Roughly speaking, slopes of these curves are the costs per transaction 
in the frontend and backend computer systems. CPU service time required by the centralized system was 
also benchmarked and measured for comparison. Network load for these types of frontend-backend interactions 
was small, and it caused no concern performance- wise.  6.2 Offered CPU Load Given that the expected 
processing power required for each processor is dependent on the workloads described above, we use different 
scaling factors for See,, Sbek, and Sm. Using CPU processing power scaling 1.00, 0.65, and 0.50 among 
See,, SbÂ¢k, and S/gt, respectively, we scaled the service rate parameters p~, ~tBE,, and #FE," This relative 
scaling is based on a multi-user benchmark[ 5]. The circuit pack arrival rate ()'i) to the production 
line that we chose to use was 55 circuit packs per hour. This is derived from the capacity requirement 
of 200,000 packs per year for a hypothetical manufacturing line. We further assumed that the number of 
operations required by each circuit pack is 12 for the calculation of CPU loading (n=12). These scaled 
offered CPU loads are derived and depicted in Figure 5. The curves include a case with frontend caching 
and one without frontend caching. Offered CPU load at each frontend system clearly depends on the clustering 
factor or, equivalently, the number of frontends in the distributed complex. We plotted two curves for 
 illustrative purpose, one with two frontends and the other with 6 frontends for a total of 12 work stations 
(c--6 and c:2). As expected, large batch size decreases the CPU utilization at the backend system significantly. 
On the other hand, the CPU utilization at the frontend system for the non-cached case does not decrease 
significantly since the network overhead is now relatively small with respect to other required work 
for each frontend-backend interaction. The cached case falls somewhere between these two cases.  6.3 
Overall System Capacity Overall system capacities were derived for three different architectures, i.e., 
the typical centralized architecture, and the distributed frontend/backend architecture with and without 
frontend caching. System capacities in terms of circuit pack arrival rates that the PICS architecture 
can handle are depicted in Figure 6 corresponding to batch size 20. The region or envelope between the 
cached and non-cached curves is the area representing all manufacturing tracking workload since these 
two curves represent the extremes of the workload variations. There are two observations from these results. 
First, large batch size definitely helps to absorb the overhead involved in network communication and 
database operations. Larger batch sizes, e.g., 26 in Figure 6, make the distributed architecture, both 
cached and non-cached, very attractive once enough frontends are added to the distributed complex. Second, 
a minimum number of frontends are necessary to handle tracking operations to insure that they will not 
become the bottleneck of the overall system throughput. It is unwise to expect similar response time 
characteristics when we load systems of different processing power with the same level of CPU utilization. 
A system of lesser processing power should be utilized to a lesser degree in order to achieve comparable 
response time characteristics. As an example of adding this constraint, we require the response times 
at the frontend and backend systems to be the same as that of a centralized See n system at 90% CPU utilization. 
As a rule of thumb, we use the response time result 1 of the M/M/1 queueing system [6] u~(1-pi) Knowing 
the relative CPU scaling and given the 90% utilization of a Sce~, we can compute desired (lower) utilization 
levels for the Sf, t frontends and the Sbck backend. Limiting the centralized system at 90% utilization 
and requiring response times at the frontend and the backend to be equal to that of a Sc~ n for the same 
work, we replot overall system capacities in Figure 7 with these constraints. The shapes of the curves 
and the envelope are very much the same as those shown in Figure 6. However, the cutover points and carrying 
capacities are very different in sizing the systems and comparing the architectures. 6.4 Cost-Effectiveness 
To appreciate the overall cost-effectiveness of the proposed distributed architecture, a normalized cost 
figure is used. The normalized cost figure is defined as the ratio of system procurement cost over CPU 
processing power. The following relative normalized cost figures are used for reasonably configured systems 
including disks and network interfaces: 7.00, 1.50, and 1.00 among Sc~ . system, Sbc k system, and S/,t 
system, respectively. With the CPU scaling information used earlier, the cost-effectiveness measure is 
shown in Figure 8. Even in the non-cached case, the distributed PICS architecture is more cost-effective 
than the typical centralized PICS architecture. The capacities are estimated using batch size of 20 and 
we impose the constraints of 90% peak utilization for the Sc~, and equivalent response times at the frontend 
and backend with lower levels of utilization. To achieve maximum cost-effectiveness measure, optimal 
number of frontends can be determined. These peaks can also be increased by upgrading the backend system. 
By examining the required production line throughput, CPU processing power, and figures of merit, a growth 
strategy for the distributed system can be readily evaluated. There is little flexibility in the centralized 
architecture besides upgrading the See, system. Indeed, the newly available small system technology allows 
us to examine different and potentially cost-effective architectures. 7. CONCLUSIONS In this paper we 
describe an experiment to evaluate a distributed computer system architecture via functional w()rkload 
decomposition. This decomposed workload was mapped to a distributed frontend/backend computer architecture. 
The frontend/backend architecture consists of a backend data repository and analysis computer system, 
and a few frontend computer systems dedicated for data collection and manufacturing process verification. 
We evaluate this distributed architecture by comparing system capacities and cost-effectiveness while 
varying several parameters such as batch size and clustering. Our results showed that the dynamics of 
manufacturing process information and the need to have a global view to control the manufacturing process 
play important roles in such a tradeoff. We presented two extreme cases, cached and non-cached, to bracket 
the workload region. The automatic in-line manufacturing environment corresponds to the cached case where 
most of the information is static and can be cached. The flexible line environment I7] is the non-cached 
case where the information changes dynamically and communication to the backend for control directives 
becomes necessary. The batch size also plays an important role to amortize the fairly large overhead 
of communication and database operations. The batching strategy is useful in its own right; however, 
it is indispensable when we have to circumvent a less-than-desirable environment to build distributed 
applications. A CKNO WLEDGEMENT The author is grateful to Y. T. Yu, Y. T. Wang and B. Farrell for their 
discussions and insights on this work. Comments from an anonymous referee greatly improve the presentation 
of this paper. REFERENCES <RefA>1. S. Ceri and G. Pelagatti, Distributed Databases : Principles and Systems, 
McGraw-Hill Book Company, 1984. 2. Carrier Sense Multiple Access with Collision Detection (CSMA//CD) 
Access Method and Physical Layer Specifications, ANSI/IEEE Std 802.3-1985. 3. ATS~T INGRES System Overview, 
AT2:T Select Code 999-803-002IS  4. R. M. Metcalfe and D. R. Boggs, "Ethernet: Distributed Packet Switching 
for Local Computer Networks," Commun. of ACM, vol. 19, 7, July 1976, pp. 395-404. 5. S. Gaede, "A Scaling 
Technique for Comparing Interactive System Capacities," Conference Proceeding of CMG XIII, December 1982, 
pp. 62-67.  148 6. L. Kleinrock, Queuing Systems Volume I: Theory, John Wiley ~ Sons, 1975. 7. C. Dupont-Gatelmand, 
"A Survey of Flexible Manufacturing Systems," ]. Manuf. Syst., Vol. 1, No. 1, 1982, pp. 1-6.</RefA>  RR',T" 
I '~TERM,HAL ~ 1 :~ TERM,NAL I Sbck (BACKENO) Xz/b ~l/b ~.I/0 Xi/b CBMAICOLAN Sfnt Sfnt , b-BATCH SIZE 
n-NUMBER OF WORK STATIONS Xb n~t ~ INFORMATION FLOW ,." ~*-~.. (-.MATER=L ,LOW ) PFE " b"~rE I FIGURE 
I. A OISTRIBUTED FRONTENO/BACKENO FIGURE 3. AN OPERATIONAL MODEL OF THE DISTRIBUTED PICS ARCHITECTURE. 
FRONTEND/SACKEND PIGS SYSTEM. Figure 4. CFU Service Time Versus Batch Size 10o I O0 8O >(Non-(lached) 
Pronte t,l L i / / J 70 I , .... CPU Times (seconds) 60 ( =_: ,:::?=~oFt~:ow) n - NUMBER OF OPERATIONS/WORK 
STATIONS ~'l - ARRIVAL RATE OF CIRCUIT PACKS X b- ARRIVAL RATE OF BATCH JOOS per Interaction 50 / , , 
. Centra ized , , kb ~  P " Pb + PI 7 i I n'~,i ~ Ib Xb Fb + /n Fi 20 ,o :,'::: =/ r ""/;" 4 ..... 
.=-(.":.a- C:z.,:L~2) :0 ~.c kc r..i ~-(wacae-]) r~uck~nd ; 0 ~" I I I I I I I I I FIGURE 2. AN OPERATIONAL 
MODEL OF THE CENTRALIZED PICS SYSTEM. O 5 1O 15 20 25 30 35 40 Batch Size (per frontend-bsekend interaction) 
including communication overhead on the backend system 45 0.40 0.35 0.30 (Scaled) Offered CPU Load 0"25 
0.20 0.15 0.I0 0.05 0.0 Figure 5. Offered CPU Load Versus Batch Size =..... . ...... .,(Non-(.'ached) 
.2 Fro~ ends ! ~Centra~ized S~ste m , ........ (Cachq t) 2 Fr ntend,, ' "'.I ! I I I-.... ._ ---., 
 ........ 2. ~on-I !ached) hF~on ads ~1~on-t ~acnea} X~acKet . '"l. ".....,,,,,: "::::::::. ......... 
~(qacbe~d) fi F~qnt~nd~ ' "~e.cut-a) D~C~ cud 5 I0 15 20 25 30 35 40 45 Batch Size (per frontend*backend 
interaction) transaction arrival rate = 55/hour, number of operatm.s = 12 8O0 70O 6OO CircuitP ack.~ 
00 per Hour 4O0 3O0 200 100 0 Figure 7. System Capacity with Response Time Constraint Dis ribt ted Ca 
he( / y~ :hed) :17 ,qut~ ale: ; re :port te t! lea at Oq ~% eni mllz !d ( :PU ut i 'aft V , i ; , 
I I I I I I I t I I I J 2 3 4 5 6 7 8 g I0 II 12 13 14 15 16 17 Number of Frontends batch size ~ 20, 
number of operations = 12  Figure 6. Overall System Capacity With Batch Size 20 Figure 8. Overall System 
C~t-Effectlvene~ 8OO 7OO BOO 5OO Circuit Packs per Hour 4OO ;// Dis ribt ted : Ca, bed Circuit Packs 
per Hour per Unit Co~t 4 3 / /--\ [ / L .X--' / I / I \ "~ ,Dis rib ted 'Ca< ~ed 30O 200 II; i Dis rlbt 
Led No:-Ca :hed) Gel tral zed / ~, is1:~.l~er r b~ 'No] -C~ hed) 100 *] iqui' alex L re Lpon ~ tl I aes 
at 9, I% ',enl rali~ -'d q fPU utili iati n 0 2345 6 7 8 g I0 II 12 13 14 15 16 17 1 2 4 5 6 7 8 9 I0 
11 12 13 14 15 16 17 Number of Frontends number of operations = 12 Number of Frontends batch size = 20, 
number of operations = 12 150  
			
