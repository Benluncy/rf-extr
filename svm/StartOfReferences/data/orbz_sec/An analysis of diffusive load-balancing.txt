
 An Analysis of Diffusive Load-Balancing * Raghu Subramanian Isaac D. Scherson ~aghu@ics .uci. edu isaac~ics 
.uci. edu Department of Information and Computer Science University of California Irvine, CA 92717-3425 
U.S.A. Abstract Diffusion is a well-known algorithm for load-balancing in which tasks move from heavily-loaded 
processors to lightly­ loaded neighbors. This paper presents a rigorous analysis of the performance of 
the diffusion algorithm on arbitrary networks. It is shown that the running time of the diffusion algo­ 
rithm is bounded by: aud where N is the number of nodes in the network, a is the standard deviation of 
the initial load distribution (which represents how imbalance the load is initially), and r and @ are 
the network s electrical and fluid conductance respec­ tively (which are measures of the network s bandwidth). 
For the case of the generalized mesh with wrap-around (which includes common networks like the ring, 
2D-torus, 3D-torus and hypercube), we derive tighter bounds and con­ clude that the diffusion algorithm 
is inefficient for lower di­ mensional meshes. 1 Introduction The load-balancing problem is as follows: 
Let G be an undi­ rected connected graph of N nodes, and let lf tasks be scattered among the nodes. Re-distribute 
the tasks such that each node ends up with either [IW/J V] or (J14/Nl tasks. (This formulation is borrowed 
from [lo]t.) Diffusion is a well-known distributed algorithm for load balancing, i.e., each node s decisions 
are based only on local This research was supported In part by the Am Force Office of Scientific Research 
under grant number F49620-92-J-0126, the NASA under grant number NAG-5 -1S97, and the NSF under grants 
number MIP-9106949 and number MIP-9205737 Incidentally, the paper descr]bes a surprising application 
of load-balancing. It is well known that permutation routing can be reduced to sorting. The authors show 
that general (many to many) routing can be reduced to sorting plus load-balancing, Permission to copy 
without fee all or part of this material is granted provided that the copies are not made or distributed 
for direct commercial advantage, the ACM copyright notice and the title of the publication and its date 
appear, and notice is given that copying is by permission of the Association of Computing Matiinery. 
To copy otherwise, or to republish, requires a fee and/or specific permission. SPAA 94-6/94 Cape May, 
N.J, USA @ 1994 ACM 0-89791-671 -9/94/0006 ..$3.50 knowledge. The intuition behind diffusion is that 
if a pro­cessor has more tasks than a neighbor, then it sends some of its tasks to the neighbor, the 
number sent being propor­tional to the differential between the two processors. The proportionality constant 
is a characteristic of the connecting edge. In this paper, we present a rigorous analysis of the per­formance 
of the diffusion algorithm on an arbitrary network. We derive both lower and upper bounds on the running 
time of the algorithm. These bounds are stated in terms of (a) the number of nodes in the network; (b) 
the standard deviation of the initial load distribution, which represents how imbal­ance the load is 
initially; and (c) the network s electrical and fluid conductance, which are measures of the network 
s bandwidth (these terms are defined formally in Section 3): o If N is the number of nodes in the network, 
r is the network s electrical conductance, and a is the stan­dard deviation of the initial load distribution, 
then the running time of the diffusion algorithm is bounded as follows: If @ is the network s fluid conductance, 
and a is as above, then the running time of the diffusion algorithm is bounded as follows: For the special 
case of an (rzl x nz x . . . x rtd) mesh with wraparound, we provide the following tighter bound: d log 
u Q(.2 n ,)) s ime s 0(fin2(mj~~,.dn,)) ln (max, =l... ~ n, Figure 1 gives a feel for the above bound 
by showing the form it assumes in certain common cases. From the table, it is clear that the diffusion 
algorithm is inefficient for lower dimensional meshes. For example, on a ring and 2D torus, the diffusion 
algorithm takes at least linear time, indicating that it is no better than a centralized algorithm (in 
which one processor collects all information and directs the load­balancing). 1.1 Comparison with Prior 
Work Traditional formulations of the load-balancing problem al­low the tasks execution times to differ 
[2]. These formu­lations are more general than ours, but with the generality 220 Network Lower-Bound 
Upper-Bound Ring fl(N log a) 0(N2V) Figurel. Bounds ontherunning time of thediffusion algorithm forcertain 
comon networks. fVdenotes thermmberofnodesin the net work, and u denotes the standard deviation ( imbalance 
) of the initial load distribution. comes intractability: the simplest such formulations turn out to 
be A@-complete. As a result, most work consists of proposing ad hoc heuristics and comparing them with 
sim­ulations. Moreover, centralized (as opposed to distributed) algorithms abound, because in the face 
of intractability, scal­ability is a moot issue. In contrast, our formulation assumes that all the tasks 
have roughly the same execution time, and that the load­balancing phases attempt to equrdize only the 
number of tasks at each node. We argue below that the case of fixed­sized tasks is sufficiently important 
to merit study. First, it mav be anrued that fixed-sized tasks adeauatelv model . .. variable-sized tasks 
which can be pre-empted when they ex­ceed a certain time quantum. Second, the case of fixed-sized tasks 
is tractable. Finally, and most importantly, there are several highly parallel applications where the 
assumption of fixed-sized tasks is valid. Examples include back-track searches, branch-and-bound optimizations, 
theorem prov­ing, interpretation of PROLOG programs, adaptive refine­ment techniques for solving PDEs, 
and ray tracing [7]. Let us consider two applications to illustrate that the tasks are indeed fixed-size: 
Back-tracking: Consider a search space consisting of vectors where each component can assume a finite 
number of values. Back-tracking is an algorithm to find a vector in the search space that satisfies some 
fea­sibfity condition. In back-tracking, the solution vec­tor is constructed incrementally, component 
by com­ponent. A task corresponds to a partially constructed vector. Each partial vector (task) resides 
in the local memory of same processor. The back-tracking algorithm alternates between ex­pansion phases 
and load-balancing phases. In an ex­pansion phase, each processor in parallel takes a partial vector 
(task), if one exists, from its local memory. If the partial vector is in fact complete, and aleo satisfies 
the feasibility condition, then the back-tracking algo­rithm returns the completed vector and terminates. 
If it is clear that the partial vector can not be completed feasibly, then the partial vector vanishes 
from the lo­cal memory, and the task is said to have terminated unsuccessfully. Otherwise, several new 
partial vectors appear in the local memory of the same processor, cor­responding to each way of extending 
the original par­tial vector by one more component, and the task is said to have spawned children. In 
a load-balancing phase, partial vectors (tasks) are red~tributed among processors evenly. If the load­balancing 
phase is omitted, then with each expansion phase, the d~tribution of tasks among processors gets skewed. 
Some processors get overwhelmed with t aeke, while others stay idle. This slows down the averall back-tracking 
algorithm. If the load-balancing phase proves to be too expen­sive, then its cost may be amortized by 
invoking sev­eral expansion phases for every invocation of the load­balancing phase. Solving PDEs: Consider 
a typical iterative algo­rithm to solve a partial differential equation (PDE). The problem is first d~cretized 
by partitioning space into regions, and the function is approximate ed as con­stant within each region. 
During each iteration, each region in parallel gets the function values in the neigh­boring regions, 
and uses them to update its own func­tion value. If, in the process, a region discovers that its function 
value differs drastically from the neighbors , then it realizes the function can t be approximated as 
being constant withh the region, so it subdivides itself inta finer regions. Here, a task corresponds 
to a region. Each task resides in the local memory of a processor. During an expan­sion phase, each processor 
picks a task, if one exists, from its local memory and executes it. This may result in additional tasks 
appearing in the local memary of a processor, reflecting that a region subdivided itself. During a load-balancing 
phase, tasks are redistributed evenly. Diffusion is only one of several load-balancing algorithms in 
the literature [13]. The diffusion algorithm is studied in detail in [1] and [4]. Our presentation of 
the algorithm in Section 2 closely follows [4]; however, our analysis of the algorithm in Section 3 significantly 
extends theirs. For ex­ample, they provide explicit bounds for the running time of the diffusion algorithm 
only in the case of a hypercube. In contrast, we provide bounds for an arbitrary network. Both [1] and 
[4] make the simplifying assumption that tasks can be divided into arbitrary fractions. Section 4 shows 
that this assumption raises many thorny issues, and suggests modifications to handle indivisible tasks. 
2 Diffusive Load-Balancing (with Divisible Tasks) In this section, we review the diffusion algorithm 
for load­balancing. For simplicity, we assume that tasks are dh%ible into arbkrary fractions. (For example, 
we allow half-a-task to move across an edge, blithely ignoring that such a thing is meaningless. ) Recall 
that the original aim was to end up with [JI/Nj or [Af/Nl tasks at each node. Now that we allow fractional 
tasks, the revised aim is to end up with exactly hfJN tasks at each node. In Section 4, we will re­consider 
the indivisibility of tasks, and show how to modify the diffusion algorithm accordingly. The intuition 
behind the diffusion algorithm is that if a processor has more tasks than a neighbor, then a few tasks 
diffuse to the neighbor. The number of tasks that dWuse is proportional to the difference in the number 
of tasks at the two processors. The proportionality constant is a character­istic af the connecting edge, 
and is called its diffusivity. Algorithm DIFFUSE: (1) for iteration + 1 to m begin (2) All processors 
i parbegin (3) loa~i] + number of tasks at i (4) Broadcast toa~i] to sll neighbors (5) for each j 
that is i s neighbor begin (6) if loa~i] > loafi] then (7) Send P,j (loa~i] loa~]) tasks to j (8) 
end (9) parend (lo)end   Figure 2. Algorithm for load-balancing (with divisible tasks) Figure 2 shows 
the diffusion algorithm in detaiL Algo­rithm DIFFUSE makes use of an IV x N diffusivity matrix, P, which 
satisfies the following conditions: b P,: z 1/2. (The number half 1/2 is chosen only for simplicity: 
any positive constant will do. The import of the condition is that P,, should be bounded away from zero.) 
 Fori#j,P,,>0 if~isanedgeinG,andP,l=Oif ~ is not an edge in G.  P is symmetric: P,, = Pji  P is stochastic: 
~~=1 P,j = 1  Each processor i has a variable called loa~i]. At the beginning of each iteration, loa~i] 
is set to the number of tasks at vertex i (line (3) of Figure 2). Normally loa~i] would be an integer 
variable, but since we are assuming that tasks are divisible, it is a real variable. Then, each processor 
sends its load to all its neighbors (line (4)). As a result, each processor knows the loads of all its 
neighbors. If a processor s load is heavier than a neighbor s, then the processor sends some of its tasks 
to the neighbor (lines (6) and (7) ). The number of tasks sent is proportional to the difference in load, 
the constant of proportionality being the appropriate entry of P. (Observe that this number may be non-integral. 
) On the other hand, if a processor s load is lower than a neighbor then it does not send any tasks ­rather, 
it receives tasks from the neighbor. The parend in line (9) tacitly implies a barrier synchro­nization. 
Thus, no processor may start the next iteration until all processors have completed the current iteration. 
[1, page 515] shows that the Algorithm DIFFUSE works just as well wit bout the barrier synchronization. 
We ret sin the bar­ rier synchronization to simplify analysis; this yields slightly pessimistic results. 
In practice, the barrier synchronization is dispensed with. We do not intend the co in line (1) to mean 
that the number of iterations is infinity. The analysis in Section 3 will show that, even though the 
load distribution becomes increasingly balanced with each iteration of Algorithm DLF-FUSE, it may never 
ever become eza ctly balanced. We sym­bolically denote this gradual convergence by an co in line (]). 
In practice, the user decides on some tolerable imbalance, and runs enough iterations to reach within 
that tolerance. 3 Analysis of the Load-Balancing Algorithm with Divisible Tasks Now we analyze the performance 
of Algorithm DIFFUSE, still retaining the assumption of divisible tasks. We derive both lower and upper 
bounds on the running time of the algorithm. These bounds are stated in terms of the net­work s elect 
rical conduct ante and fluid conduct ante, which are measures of the network s bandwidth. For the case 
of a generalized mesh (with wrap-around), we derive tighter bounds. To state the main result of this 
section, let us first intro­duce some terminology. For each t z O, define the load distribution as where 
. ~(t) w the number of tasks at vertex i after iteration tof Alg~rithm DIFFUSE. (Thus, do) denotes the 
initial load distribution.) Let the total load, M, be ~~1 do), and define the balanced distribution as 
M/N M/N b= .. M;N ()  Next, we define the electrical conductivity and jluid con­ductivity of G, which 
are two meaaures of its bandwidth. Imagine G to be an electrical network, with edges rep­resenting resistors. 
Set the resistance of each edge ~ to 1 /Pi$, the reciprocal of the corresponding entry of the diffusivity 
matrix. Let u and v be vertices of G. Define RES(u, u) as the effective electrical resistance between 
u and o, that is, the voltage of u with respect to u if a unit of current enters thj network at v and 
exits u. The e~ectrical conductance of G is defined as r = min ti, REs(u, v) Consider G to be a gas distribution 
network, with edges representing pipes. Set the capacity of each edge ~ to P,,, Let S be a subset of 
the vertices of G, and ~ Since P is a real symmetric matrix, it is diagonalizable, be its complement. 
Define CAP(S, ~) as the effective and eigenvectors corresponding to different eigenvalues form an orthogonal 
basis [12, page 296]. Let Al, ~z, ..., AN be the eigenvalues of P, and without loss of generality, let 
j?uid conductance $ of G is defined as capacity between S and ~, that is, ~ie~,jcy P,J. The them be 
ordered such that IA1 I ~ IA2 I ~ . . . ~ l~N1. Let J1),0(2),,0., V( ) be the corresponding eigenvectors. 
From CAP($ ~) the theory of Markov chains, it is known that Al = 1, that @ = $ min(lsl, IsI) u(1) is 
some scalar multiple of We have introduced enough terminology to state the 1 main result of this section: 
1 ., Theorem 3.1 (Correctness and Complexity of DIFFUSE). The load distribution converges to the balanced 
distribution 1 () (regardless of its initial value): and that IA21 <1 [11]. Since the components of 
e( ) sum to zero and the com­lim 1( ) = b. t-co ponents of the first eigenvector rr(l) are all equal, 
their inner product is zero. So e( ) has no component in the direction Moreover, the running time, defined 
as the time for II l( ) -bll of basis vector U(1,, and hence can be expressed as a lin­to fall below 
any prespecified constant tolerance, satisfies the V(W . Observe that P scales theear combination of 
V(2), ..., folJowing bounds: lengths of o(z),..., O(N) by l~z I ,..., IAN I respectively, d of which 
are < IA2 I < 1. Therefore P scales the length of e(t) Q(y) < Time < ~(~) by a factor ~ IA21 <1. Therefore, 
II e(*+l)  II=llpe( )ll~IAzlIle(t)ll hich r­ eplies ~(~) < Time S 0($), I]e(t)ll < ]~zlt Ile(o)ll . 
(3) where f7 = IIl( ) -bll represents the imbalance in the We call equation 3 the Error Bound. Informally, 
the initial load distribution. Error Bound says that the length of the error vector shrinks For the case 
of an (nl xn2 x . . . x nd) mesh with wraparound, geometrically, where the shrinking factor is ~ IAz 
[. tJle running time satisfies the following tighter bounds: The Error Bound is tight. Indeed, if we 
choose e(o) to d log u be V2 then each application of P shrinks the length of e(t) ~)< Time ~0( am ~). 
 Q(.2 m by a factor of exactly IA21. Hence for this choice of e(o), ln ( max,=~...d n, Sinz( max,=l... 
d n, I[e( )ll = (lkl)(t) Ile(o)ll. For clarity, we present the proof of Theorem 3.1 in two subsections. 
The first subsection derives the Error Bound, 3.2 Conclusion of Proof and the second subsection concludes 
the proof. From the Error Bound, it follows that limt+~ e(t) = O, which implies that lirnt-m l( ) = b, 
proving the correct­3.1 The Error Bound ness part of the theorem. For each t ~ O, define the error distribution, 
e( ), aa .&#38;tJ -b. Let us say that we desire a tolerance of r. Then we must Observe that from one 
iteration to the next, the load execute the body of the line (2) loop T times, where T is such distribution 
changes according to the equation Iwil that lle(~)ll < T. By the Error Bound, T < loglil~zl ~ . ~ +1) 
= p~ ), The time for any processor to execute lines (3) and (4) (1) is at most a constant, say c. The 
time for processor i to execute the loop from line (5) to line (8) k-proportional since, by inspection 
of Algorithm DIFFUSE, ~t+l) = ~t) ­to the number of tasks it has to send, which is at most &#38;t) = 
(Pdt)),. Also note ~;=, P,,(4*) -4*)) = Z:l P*, , ~;, P,, <*-1) -&#38; ) = ~;l pi, .$-1) -e: -l) < that 
Pb=b, (2) ~~1 P,,( e! - ) + e$ -l) ) ~ ~~1 2P,, max~ e$-l) ~ since Pbi = ~~1 Pi,bi = ~~1 pij(M/N) = M/N 
= bi. ~~12P,1 Ile( -l) 1 II ~ z Ile(i-l)ll. SO the total tirr!e for ~ny Equation 2 haa the nice significance 
that if we start with processor to execute iteration tisat most c + 2 II e( -l) II. a load distribution 
of b, then after one iteration we end up with b again. Therefore the time for all T iterations is ~~=1 
(c+2 IIe(t) II ~ From Equations 1 and 2, it follows that ef*tl) = .&#38;*+lJ = CT + 2 ~~=1 II e( ) 
II. Using the Error Bound, this expres-PC(*) . So the error distribution transforms P(~ J = in the same 
way as the load distribution. b = -b) sion can be bounded as ~ Fluid conductance is not a standard term 
in interconnection net­work literature, but the idea occurs in several guises, such as the load-factor 
defined in [9]. w Figure 3. The number of tasks at adjacent vertices differ by one. Since the weight 
on each edge is 1/3, we would have liked to trans­ fer 1/3 of a task across each edge, but in the floor 
scheme, no tasks move. Thus, the load distribution has converged wrongly. It only remains to bound on 
IA-z1. Here are several for­mulaz to do so, drawn from the theory of rapidly mixing Markov chains: The 
second eigenvalue of P is bounded by the electrical conduct ante of G as follows [3, Theorem 7]: l 2r~l,421~l-~. 
 The second eigenvalue of P can be bounded by the fluid conductance of G as follows [6]:  Let G be 
the nl x nz x . . . x nd mesh with wraparound.  Define the_matrix P as follows: Set all diagonal entries 
to1/2. Iftj isanedgeofG,then setP,,=~. Setd other non-diagonal entries to zero. The second eigenvalue 
of the matrix P is given by the following equation [3, Theorem 10]: &#38;=l-~sin2( r ).maxi=l...d ni 
 Substituting these bounds for l~z I in Equation 4 proves the complexity part of the theorem. 4 Handling 
the Indivisibility of Tasks Algorithm DIFFUSE assumed that tasks are divisible. In this section we give 
examples to show that the indivisibility of tasks raises non-trivial problems. Then we show how to modify 
Algorithm DmFUSE to handle indivisible tasks. Once we recognize that tasks are atomic, Algorithm Dm-FUSE 
has an obvious problem in Line (7): Send Pi, (loa~i] loafi]) tasks to j, because this quantity may not 
be integral. Let us try replacing line (7) with Send [PiJ (loa~i]-loafi])j tasks. The problem, as Figure 
3 shows, is that the load distribution may converge to an unbalanced distribution. If we try replacing 
line (7) with Send ( P,J(/oa~i] load(j])l tasks, Figure 4. The number of tasks at adjacent vertices differ 
by two. Since the weight on each edge is 1/3, we would have liked to trans­fer 2/3 of a task across each 
edge, but in the ceiling scheme, a whole task moves. Thus, the load distribution oscillates. then an 
immediate problem is that a processor may not have enough tasks for all its neighbors. Even if we are 
willing to ignore that, Figure 4 shows that the load distribution may keep oscillating between unbalanced 
distributions. A more sophisticated approach combines floors and ceil­ings: Flip a biased coin that lands 
HEAD with proba­bility P,, and TAIL with probability 1 P,, If a HEAD is obtsirted then send (Pi3 (ioa~i]-loa~])l 
tasks.For a a TAIL, send [P,j(loa~i] loa@])j tasks. The intuition behind this approach is that sending 
$ task (for example) is the same as sending a whole task with prob­ability ~. However, the algorithm 
turns out to have a curi­ous behaviour: the load distribution balances to an extent, but fails to balance 
any further. More precisely, the en­ tropy stabilizes at non-zero value. (Of course, a fortuitous sequence 
of coin flips may balance the load, but that is very unlikely to happen.) As the above examples show, 
the convergence of the al­gorithm greatly depends on how the fractions are rounded. Below, we state a 
rounding scheme that guarantees con­vergence to the balanced distribution. We omit the proof because 
it is straightforward and uninstructive. Case 1: G is biconnected. Find an orientation of G (that is, 
aasign a direction to each edge of G) such that b there are no directed cycles  there is a unique maximaf 
vertex (i. e., with an out-degree of zero) and a unique minimal vertex (i.e., with an in-degree of zero). 
 there is an edge joining the maximal to the min­imal vertex  Such an orientation maybe found as follows: 
Find an References open ear decomposition of G (one exists because G is biconnected [5]). Orient the 
first open ear El arbitrar-[1] ily. Assume, by induction, that the edges of the open ears E1, Ez, ..., 
E,_l have already been oriented; that there are no directed cycles yet; and that the endpoints [2]of 
the open ear El are the minimum and maximum vertices. Now we wish to orient the open ear E,. Let the 
points of attachment of E, be the vertices u and v. Because the current partial orientation is acyclic, 
directed paths can not exist both from u to v and v [3] to u. Without loss of generality, assume that 
there is no directed path from u to v. Then orient all edges of the open ear E, from v to u. Clearly, 
this will create no directed cycle, and the endpoints of the open ear [4] El remain the minimum and maximum 
vertices, thus maintaining the inductive assertion. Now, modify line (7) of Algorithm DIFFUSE as follows: 
[5] Processor i sends either (P,J(loa~i]-Joa~])l or 1P,, ( loa~i] loa~j])j tasks to j, according w the 
edge ~ is directed from i to j or from [6] j to i. Case 2: G is not biconnected. Find a biconnected 
su­pergraph H that can be simulated by G with constant delay. The graph H may be constructed by adding 
[7] edges to G as follows: for each cut-vertex u of G, chain the neighbors of u in a cycle. It is easy 
to see that H can be one-to-one embedded in G with dilation 2 and edge-congestion 4. By [8, page 404], 
G can simulate H with a constant-factor delay. [8] Thus, even though the network at hand is G, the al­gorithm 
can load balance as though the network were H, incurring only a constant-factor delay. [9] Conclusion 
We presented a rigorous analysis of the performance of the [10] diffusion algorithm on arbitrary networks. 
We derived both lower and upper bounds on the running time of the algo­rithm. These bounds were stated 
in terms of the network s [11] bandwidth. For the case of the generalized mesh with wrap-around, we 
derived tighter bounds, and concluded that the diffusion [12] algorithm is inefficient for lower dimensional 
meshes. As shown in the back-tracking and PDE examples of Sec­tion 1, load-balancing usually arises as 
a part of another [13] algorithm. This suggests that a load-balancing algorithm should be not be judged 
in isolation, but by how it improves the overall sJgorithm. Thus, in lower dimensional meshes, even t 
bough the diffusion algorithm may be inefficient, it may be good enough for certain algorithms. It would 
be interesting to see such an analysis. Acknowledgments We thank Brian Alleyne, Sandy Irani and David 
Eppstein for several valuable insights and suggestions. <RefA>D.P. Bertsekas and J.N. Tsitsiklis. Parallel 
and Dis­tributed Computation: Numerical Methods. Prentice Hall, Englewood Cliffs, NJ, 1989. T.L. Casavant 
and J.G. Kuhn. A taxonomy of scheduling in general-purpose distributed computing systems. IEEE Transactions 
on Software Engineering, 14(2):141-154, 1988. A.K. Chandra et al. The electrical resistance of a graph 
captures its commute and cover times. In Symposium on Theory of Computing, pages 574 586, may 1989. 
G. Cybenko. Dynamic load baJancing for distributed memory multiprocessors. The .lournal of Pamllel and 
Distributed Computing, 7(2):279-301, 1989. H. Whitney. Non-separable and planar graphs. Trans­actioru 
of the American Mathematical Society, 34:339 362, 1932. M. Jerrum and A. Sinclair. Conductance and the 
rapid mixing property for markov chains: the approximation of the permanent resolved. In Symposium orI 
Theory of Computing, pages 235-243, may 1988.  R.M. Karp. Parallel combinatorial computing. In Jill 
P. Mesirov, editor, Very Large Scale Computation in the 21st Century, pages 221 238. Society for Industrial 
and Applied Mathematics, Philadelphia, PA, 1991. T. Leighton. Introduction to Parallel Algorithms and 
Architectures: Arrays Trees Hypercubes. Morgan Kauff­man, San Mateo, CA, 1991.  C.E. Leiserson and B.M. 
Maggs. Communication effi­cient parallel algorithms for distributed random-access machines. Algorithmic, 
3:53 77, 1988. D. Peleg and E. Upfal. The token distribution prob­lem. In Symposium on Foundations of 
Computer Sci­ence, pages 418 427, 1986. E. Seneta. Non-negative Matrices and Markov Chains. Springer-Verlag, 
New York, NY, 1981. G. Strang. Linear Algebra and its Applications. Har­court Brace Jovanovich, San 
Diego, CA, 1988.  M.H. Willebeek-Lemair and A.P. Reeves. Strategies for dynamic load balancing on highly 
parallel comput­ers. IEEE Transactions on Parallel and Distributed Sys­tems, 4(9):979-993, 1993.</RefA>  
			
