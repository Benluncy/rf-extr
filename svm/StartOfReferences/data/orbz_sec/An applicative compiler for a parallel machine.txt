
 An Applicative Compiler for a Parallel Machine lan W. Moor Department of Ccmputing, Imperial College, 
London SWT, UK A compiler for the applicative language HOPE is described. The compiler is itself written 
in HOPE and generates a machine independent compiler target language, suitable for execution on the parallel 
reduction machine ALICE. The advantages of writing a compiler in a very high level applicative language 
are discussed, and the use of program transformation end other techniques to turn the initial 'runnable 
specification' into a more efficient (if less clear) program are outlined. Extensions to the HOPE language 
and the compiler which can exploit the parallelism and various execution modes of ALICE are described. 
~I~ ~tzmz t~E~ HOPE is an experimental applicative language developed at.Edinburgh, [ Burstall, MacQueen 
and Sannella 1981] based on the earlier recursion equation language NPL [Burstall 1977]. Its most important 
features are its applicative (side-effect free) nature, strong polymor phic typing and higher order constructs; 
a modularisation facility is also provided. A HOPE program consists of a list of equations which exhaustively 
define functions by cases. Execution of such a program takes the form of the evaluation of an expression. 
Data structures in HOPE are represented by terms built up using constructor functions -functions which 
have no associated rewrite rules. Expressions are evaluated by repeatedly applying the equations as directed 
rewrite rules until the expression reduces to a term involving only constructor functions and constants. 
HOPE programs also manipulate ' tuples', which can be considered as unnemed data-objects with no explicit 
constructor. For example the argument list of a function can be treated as a single object -a tuple. 
Permission to copy without fee all or part of this material is granted provided that the copies are not 
made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication 
and its date appear, and notice is given that copying is by permission of the Association for Computing 
Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. &#38;#169; 
1982 ACM 0-89791-074-5/82/006/0284 $00.75 HOPE has strong polymorphic type checking, based on that described 
in [Milner 1978] which combines the error detection provided by strong typing with the convenience of 
being able to write general polymorphic functions. One can declare one function, Append, for example, 
which will Join two lists with elements any type, providing the elements of both have the same type. 
Figure 1 is a HOPE program illustrating the main features of the language. Comments appear inside paired 
exclamation marks. module lists~and=trees ; uses orderings; ! Import definitions ! pubconst append, 
tree=insert, empty=tree; pubtype tree; i Export the type tree ! typevar alpha; dec Append : list alpha 
£ list alpha -> llst alpha; data tree(alpha) :: empty=tree++ node(tree(alpha)£elpha£tree(alpha)); ---Append(nil,z) 
<: z; ---Append(x::y,z) <: x::Append(y,z); dec tree, insert: alpha £ treeCalpha) -> tree(alpha); I ordered 
tree insert I tree~inser t(x, ampty=tr ee) <= node(empty=tree, x, empty=tree); ---tree=inser t (x, node 
(lef t, item, right ) ) <= node(new.left, item, new.right) where nea=left == tree~insert(x,left) if x 
< item else left where new=right :: tree~insert(x,right) if x > item else right ; end; Figure I : 
An example HOPE program. rood Thetule brackets module and end delimit a outside this only those functions 
and types explicitly exported as pubconsts or pubtypes are visible. Correspondingly, inside the module 
only certain global predefined types (numbers, characters, lists, sets etc.) and the pubconsts and pubtypes 
of modules whose names appear in a uses statement are ,ailable. The use of modules allows programs to 
be written in separate parts with clearly defined interfaces hiding their internal details. The integrity 
of data- structures can also be maintained by concealing their primitive constructors in the defining 
module and exporting only functions which check the validity of operations on the the structures. For 
 example, in figure I the data constructor node is not public, the only way to put an item in a tree 
is to use the function treeuinsert which will insert the item into the correct place on the tree to maintain 
its ordering. Following the definition of the interface. alpha is declared as a type variable and Append 
as a function of two lists which returns a list. The elements on the lists may be of arbitrary type (alpha), 
provided the elements of all three are of the same type. The compiler will reject an expression involving 
Append which binds alpha to different types. The type list alpha ~is a built in parameterised type. HOPE 
allows the user to define such types. HOPE types are built up from basic or user-defined types and type 
operators like £ (cross-product), -> (function arrow), --> (map arrow), set. list. A data type is defined 
by a data statement, listing the possible forms the type might take, the example shows the definition 
of the parameterised type tree: a tree may be empty or a node consisting of an object and two branches. 
The _m symbol serves to introdJce an equation defining a function. In this case Append is defined by 
two cases, one where the first list is nil, and the second where it has head x and tail y ('::' is an 
 infix list constructor). HOPE is a higher-order language: functions are treated as just another data 
type. As well as the named functions defined by equations, anonymous function constants may be constructed 
using lambda-expressions. This encourages a style of programming which makes use of higher order functions 
rather than expli6it recursion [Wadler 1981] and simplifies the definition of functions on complex recursive 
data structures. Map functions similar to LISP's MAPCAR is provided for use on lists and can be defined 
for other types. Figure 2 defines the higher order function reduce on lists, which resembles the FP [Backus 
1978] or APL [Iverson 1965] reduction operator, and shews its uses to define a non-recursive function 
to sum a list of numbers, the result accumulates in the last argument. The basic language does not enforce 
any ordering on the appllcation of rewrite rules or the evaluation of sub-expressions. As there a~e no 
side-effects, this makes the language an ideal candidate for a parallel machine, with sub-expressions 
evaluated in parallel. Common sub-expreJsions can be extracted using a let or where to ensure that they 
are evaluated only once. By using constructors in the pattern on the left of a let or where components 
of a structure can be referred to; in figure 3 the last element of a list is moved to the front. typevar 
alpha, beta; dec reduce : (alpha£beta -> beta) £ list alpha £ beta -> beta; dec sum~list : list hum 
-> num; ---reduce(f,nil.b) <: b ; --- reduce(f.x::l.b) <= reduce(f.l.f(x.b)); ---sLm~.list(1) <= reduce((lambda 
n, total=> n+total),l.0); Figure 2: The HOPE definition of reduce. head(reverse(x))::reverse(tail(reverse(x))) 
 may be written: let first::rest == reverse(x) in first::reverse(rest) Figure 3: Using "let". In the 
compiler described provision is made for controlling the mode and order of execution of a program where 
this is appropriate. In addition to lists, two extemely useful and powerful data-structures are built 
into HOPE: sets and maps. Sets are the true mathematical objects, bounded only by storage size, and have 
the conventional operations of union, intersection and membership defined on them. Set constructors are 
concealed from the user, instead an operator which inserts an object, without duplication and a decomposition 
function which splits a set into an arbitrary element and the remainder, are provided: :: : alpha £ 
set alpha -> set(alpha); singleton~split : set alpha-> alpha £ set alpha ; map and reduce functions 
are defined over sets in the same way as for lists. Maps resemble SNOBOL tables and can be viewed as 
sets of ordered pairs, lookup being done on the first element to yield the second element of the pair. 
Maps may be composed if their domains are compatible, pairs may be added, and membership in the map tested; 
again, no primitive constructors are visible. The only full implementation of HOPE is a compiler written 
in POP-2 at Edinburgh Unigersity. running on a DEC-10. It generates code for an abstract stack machine 
which is then interpreted by a POP-2 program. The compiler was written by the language designers [Burstall.MacQueen 
and Sannella 1981]who give its performance as being at least 9 times slower than LISP on their machine. 
(It should be remembered that the compiler was intended as a research tool rather than a production program). 
Other possible implementation methods ~ivin~ zncreased performance on conven~zonal 285 machines are 
being studied at Edinburgh and Imperial College. A source interpreter for a large subset of HOPE has 
been written in PascalSy Victor Wu of the Department of Computing [Wu 1982] at Imperial College. and 
a preliminary version has been used for teaching undergraduates functional progranming. The storage of 
the progrem in source form makes tracing and debugging the program easier but limits the size of the 
program and depth of recursion. A compiler for a subset of HOPE (also written in Pascal) which generates 
code for an abstract three address machine and which is capable of being run on an 8-bit microcomputer 
is being developed by Sue Eisenbach at Westfield College London [Eisenbach 1982]. The code generated 
by the compiler can be run by a stand-alone interpreter, increasing the space available for code and 
data at run-time. The compiler generates a high level abstract assembler in textual form. While Compiler 
Target Language (CTL). [Reeve 1982] is the assembler of a specific machine ALICE. it is designed to be 
machine independent. There is no dependence on the parallelism of ALICE or the hardware details of the 
machine. Currently the CTL is assembled into code for execution by an ALICE simulator, [Reeve 1982a,1982b] 
but it is intended to replace this by a smaller and faster CTL interpreter for software development and 
the bootstrapping of the compiler. The language is free format, sections of programs being delimited 
by paired syntax words. An expression is represented by a linear form of its graph, where each node of 
the graph is a 'packet' containing a function name and references to the packets representing the arguments, 
if any. Figure 4 is a CTL program as produced by the compiler for the Append function defined above. 
Blank lines and line numbers as comments (inside '(*' and '*)' brackets) have been added and the program 
suitably indented to improve readabilty. In lines 2 and 3. nil and cons are specified as data constructors 
and Append as an evaluable function. The conditions which make a packet processable are determined by 
the compiler and encoded in the target language as a REQUIRES statement. The one on line 5 which indicates 
that argument I of a packet with function Append must be reduced to a constructor before the packet can 
be processed. In general a packet becomes processable when sufficient of its arguments have been evaluated 
to make it possible to chose between the equations for the function. Reduction of a processable packet 
is done as a guarded command. The guards are Boolean expresslons in the values of the packet fields (usually 
the arguments) and are generated by the compiler from the left-hand (*I*)PROGRAM (*2*) CONSTRUCTOR 
nil, cons (*3*) RBgRITEABLE Append (*4*) RULES~FOR Append (*5*) REQUIRES ARG(1) (*6*) RULE ARG(1).F!JNCTION 
= nil (*7*) RHS (*8*) @ Synonym (ARG(2)); (*9*) END~RHS ('10") NEGATIVF~DELTAS ARG(1):I ('11") END~RULE 
 ('12") RULE ARG(1).FUNCTION = cons ('13") POSITIVF~DELTAS ARG(1).ARG(1):I ('14") ARG(1).ARG(2):I ('15") 
RHS ('16") I Append (ARG(1).ARG(2) ARG(2)); ('17") @ cons (ARG(1).ARG(1) &#38;1); ('18') END~RHS ('19") 
NEGATIVF~DELTAS ARG(1):I (*20*) END~RULE ('21") ENDL, RULES (*22*)END~PROGRAM Figure 4: The CTL translation 
of Append sides of the equations. There is one (possibly compound) guard for each equation defining 
a given function, appearing in the target language RULE statement. The guard on line 6 selects the case 
of Append where the first argument is nil. The second case when the first argoment is a list having cons 
as a top-level function is selected on line 12. The target language allows a choice of the arbitration 
method to be applied when more than one guard is true. The consequents of the guards are collections 
of packet skeletons representing the right-hand sides of the equations. Each packet skeleton is specified 
by giving the value of its reference count, followed by the function name and argument list; other fields 
may be set by optional keywords following the end of the argument list. The n'th packet in a given collection 
is referred to as '&#38;n'. A r~rite is performed by making copies of the selected skeletons with the 
values of the bound variables filled in. The topmost packet in the expression tree is marked by an '@' 
symbol in place of the reference count. When the expression is evaluated this replaces the packet representing 
the original expression. In the first equation (line 5-11) the packet returned is a copy (Synonym) of 
the second argument packet (possibly still not reduced). As the rewrite takes place the reference counts 
of the packets referred to in the equation are updated, the changes ('DELTAS') are specified in the CTL. 
avoiding the overhead of their repeated computation at runtime. The reference to the first argument 
(x) is lost (line 10) and its reference count is reduced by one. In the case of the second equation 
lines 13 and 14 indicate that the components of the first argument ( x and y in theHOPE program) haveextra 
references to them generated by the rewrite. This rule generates two packets, the recursivecall bo Append, 
on line :6 and the top-level list built using cons line t7. The statement on line 19 indicates that the 
original list cell in the first argqanent is not part of the result and can be dereferenced by one. 
The ALICE (Applicative Language Idealised Computing Engine) machine is a parallelmultiple processor reduction 
machine under development at Imperial College. The machine has been described in detail elsewhere by 
its designers [Derlington andReeve 1981],the description here is intended only to provide enough detail 
to allow understanding of the compiler and its objectives. ALICE directly evaluates an assembled form 
of CTL, a packet is the unit of work and packets in use are held in a 'packet pool'. A number of independent 
processing agents operating in parallel remove packets which become processable from the pool, rewrite 
them according to a stored program and place the resulting packets back in the pool. Non-processable 
packets are returned to the packet pool, marked as non-processable and with pointers back to them from 
the arguments they are waiting for. Thus an agent is never in a busy wait state for required arguments. 
Hardware mechanisms in the machine will prevent agents from having to search the packet pool for work 
by circulating the identifiers of processable packets. When a packet is evaluated, packets waiting for 
it are inspected to see if they have now become processeble and if so, become candidates for evaluation 
themselves. This eager evaluation mode can be overridden by setting flags in a packet. An expression 
can be suspended: its evaluation is then only started when signalled; or a function can be defined as 
lazy: only evaluating its arguments when their values are required [Henderson 1980]. A Pascal simulator 
of ALICE at the functional level and an assembler for CTL into the 4nternal code of the simulator have 
been written by Mike Reeve at Imperial College. Detailed studies of the VLSI implomtation of ALICE and 
work on a register level simulation are currently under way. The ALICE machine contains fields in each 
packet to indicate the mode of its evaluation. CTL contains two mechanisms for setting these fields, 
the packet function and argument list may be followed by a keyword: LAZY, or a collection of packets 
can be surrounded by the brackets BEGIN=LAZY=EXPRESSION, END~LAZY=EXPRESSION; suspended expressions are 
handled in an identical manner. As well as allowing the HOPE programmer a similar facility of suspending 
or making lazy expressions, the compiler automatically generates suspensions for some constructs. Suspension 
of expressions is a means of explicit sequencing, either to give the correct meaning to a program or 
to prevent failure during execution. Usually the arms of a conditional expression will be suspended and 
one of them activated after the evaluation of the condition. This prevents expressions like 0 if x = 
0 else I/x from failing as they would if both arms were always evaluated. Explicit sequencing is also 
needed for input-output on a parallel machine, a function to output a list of results, like that in figure 
5 for example, must not output the tail of the list until it has output the head. In the figure, output=int 
is a primitive which outputs its first arg~nent and THEN activates the second. dec writelist : list 
num -> list num; m_ writelist(nil) <: nil; writelist(front::rest) <= output~int(front, suspend(writelist(rest))); 
 Figure 5: Explicit sequencing in output. Lazy evaluation is used to limit the computation at any point 
to only that required to select a rewrite rule. This allows finite representation of infinite objects, 
for example the list of integers from n upwards is defined by: ---integers(n) <= icons(n.integers(n+1)) 
 where lcons is a lazy list constructor. Under the normal mode of execution, evaluation of head(tail(integers(1))) 
would never terminate. However the function icons does not cause its arg~nents to be evaluated immediately, 
instead evaluation procedes a bit at a time, the expression reduction is shown in figure 6 . head(tail(integers(1))) 
 head(tail(lcons(1,integers(2))) head(integers(2)) head(lcons(2,integers(3))) 2 Figure 6: Evaluation 
of a lazy expression The un-needed (and infinite) integers(3) is never evaluated. The concept of lazy 
evaluation is similar to coroutines in conventional languages, while its most frequent application, the 
lazy list, behaves like the stream [Landin 1965]. Input to the compiler is done using the function input 
: list char -> list char ; which, given a file name, returns the contents as a lazy list. As originally 
defined HOPE contained no means for specifying execution modes, but assumed that the conditional has 
suspended arms and incorporated the predefined lazy list constructor lcons. In general, the execution 
modemight be specified as part of the function definition or in an individual expression, experience 
has not yet shown which method is the better. The compiler currently incorporates the following mechanisn 
: two special functions lazy and suspend are provided, they are translated into target language statements 
which make their arguments lazy or suspended. The CTL function activate which awakens its argument is 
treated as a built in function by the compiler. The compiler additionally translates the conditional 
expression: el if b else e2 into cond(b,suspend(el),suspend(e2)) where cond is predefined by the equations: 
 --- cond(true.x,y) <= activate(x); --- cond(false.xoy) <= activate(y); and expands the expression 
icons(x,y) into lazy(x)::lazy(y). The evaluation mode of other expressions may be specified by the explicit 
use of these functions. When an expression is not suspended the activate function behaves as an identity; 
it is therefore simple to use the cond function combined with suspend to produce conditional expressions 
which evaluate one or both arms in parallel with the condition. It is intended to introduce a form of 
guarded expression into the language to avoid the use of deeply nested conditionals which force explicit 
sequencing on the machine. The compiler performs a syntax analysis of the HOPE program and builds a tree 
representing it. During the translation of each HOPE module, the symbol tables are suitably altered to 
provide for the export and import of only the required items. The equations are type-checked in their 
internal representation using a form of Milner's algorithm [Milner 1978]. The equations are partitioned 
according to the function they define and translated a group at a time. The left-hand side of each equation 
is inspected to determine which arguments are constructors and which are constants. Target language REQUIRES 
and RULE statements are generated from this information. The right-hand sides of the equations are written 
out as collections of packet skeletons by listing the parse trees for the expressions in post-order. 
The compiler also computes the changes in the reference counts of argunent packets. The target language 
has various restrictions: suspended expressions cannot be nested, for example, the compiler detects such 
nesting and replaces the inner expression by a call to a compiler-generated auxilliary function. The 
compiler is a large HOPE program (the current version is about 3000 lines), built from a number of HOPE 
modules. At the bottom level are a set of utility modules shared by sections of the compiler (and other 
HOPE programs) performing commonly required functions on numbers, lists, sets. trees. etc. and defining 
the data-types such as the parse tree and the symbol table. In terms of the logical structure, the compiler 
splits into five parts, the first three being target-independent: lexical scan. syntax analysis and type-checking; 
the two code generation passes are specific to the target language. The lexical scan and parser were 
initially developed as part of a program transformation system which treats HOPE programs in parse tree 
form as data; and the current versions are shared by this. necessitating the careful partitioning of 
syntax analysis from code generation. The compiler was first written as the composition of five functions 
implementing the five phases, with each function processing the whole of the source before passing it 
to the next. Once non-trivial programs were to be translated, this consumed too much store and lazy evaluation 
was introduced to make the parts of the compiler behave as coroutines. The compiler is still the composition 
of the same five functions, but each returns a lazy lists of structures for input to the next. An input 
function provides the characters from a source file as a lazy list for use by the lexical analyser. Each 
part provides output when requested by the following one, the final part being driven by an output function, 
which writes the code to a file. This avoids having to hold a representation of the entire program in 
memory. The introduction of co-routining between the lexical scan and parser serves as a demonstration 
of the advantages of applicative programming, since it involved no changes beyond introduction of a lazy 
list constructor, took less than a day and worked first time. This may be compared to the complexities 
of introducing paralleliem into a conventional compiler [Banatre et al 1979]. The first pass or function 
is a simple lexical analysis which replaces reserved symbols by single items and groups other c~iaracters 
together into tokens. Identifiers are not replaced by references to the symbol table, as this would result 
(in most HOPE implementations) in a copy of the table entry being placed in the input, increasing the 
size of the items on the input to the parser. The lazy list of tokens output from the lexical function 
is passed to the parser. The declaration of the top level function lex and a sample output frem it are 
shown in figure 7 HOPE lists may be written in brackets i'[' and ']'). with x::(y::nil) written as [Xoy]. 
 dec lex : list char -> list token; lex( "append( y. z)") : [ id ( "append"). i paren, id ( "y"). camma. 
id("z") .rparen] Figure 7: Example of the lexical function ~ ~. The parser uses recursive descent modified 
to handle operator priorities and user-defined operators. The applicative nature of HOPE rules out the 
usual recursive descent technique in which the parsing functions have the side-effect of advancing the 
input stream. Instead the functions all return a modified copy of the input list as part of their value. 
In general, parsing functions have the same type as the function expression shown in figure 8 (stdenv 
is a constant-symbol table containg a predeclared standard environment). dec expression : list token 
£ symbol=table -> truval £ set err=msg£ I I object was found errors if any ! 1 hope~obj £ list token 
; I the parse-tree I rest of input I 1 expression( [id("append").iparen, id("y").comma. id("z"),rparen].stdenv) 
= true, nil~set, expr~apply(const(ident("append")), tuple~expr([variable(ident("x")), variable(ident("y))])),nil 
 Figure 8: The action of a Parsing function The first components of the tuple returned by parsing functions 
indicates if the object has been found, the second is a set of a type used to indicate syntax errors 
and will be empty if the object is correct (or not found). An item of the recursive data type hope~obj 
representing the parse tree and the list of tokens for the next parsing function are also returned. 
Error recovery at the moment is somewhat simplistic: input symbols are removed free the input as determined 
by a ceepiler function defined for each error condition and class of input token. The tokens removed 
free the input are incorporated as a special string item into the parse tree for output as part of the 
error messages and possible processing by a HOPE structure editor. It is anticipated that the ability 
to modify the ceepiler input cleanly may lead to more sophisticated error recovery and repair. Edinburgh 
HOPE gives the programmer the ability to define his own distributed-fix (distfix) operators, where the 
operands appear embedded in the operator, for example SORT 1 USING=ORDER o  to sort a list 1 using 
the ordering o. This facility greatly enhances the usability of systems built on top of HOPE. but has 
been omitted from the current version of the ceepiler to simplify the pa rser. ~ ~g Syntactically correct 
parse-trees are passed to the type-checking phase. Thisis a function based on the algorithm W [Milner 
1978]. The type constraints in the language. the types of built-in objects, the user declarations in 
the program, and the use of the functions in expressions together give rise to a set of 'type equations' 
which are solved usinga method based on resolution [Robinson 1965] to give the most general type assignment. 
An example of the typing of an expression is given in figure 9 Append(cons(1,nil),nil) given the following 
known types: I : nee nil : list alpha cons : beta £ list beta -> list beta nil : list gamma Append: 
list delta £ list delta -> list delta the following equations result: beta = nee list alpha = list 
beta list beta = list delta list gamma = list delta with the solution list num. Figure 9: Typing 
a simple expression.  If no solution is found seee function must have been used in a way conflicting 
with its declaration or with an earlier use in the same equation. HOPE differs free the language [Gordon 
et al 1978]. which uses the same type checking algorithm, in requiring the explicit specification of 
function types before their use. While it is possible ( and perhaps convenient for an interactive user) 
to deduce a type for an object free its context, declaration serves to prevent incorrect but consistent 
uses of a function and to document the program. As Milner states his algorithm in an applicative form. 
its use in the compiler has involved only the extension of the algorithm fram the simple example language 
of the paper, and the addition of the ability to cope with overloading of operators. Operator overloading 
is a complementary facility to polymorphism, where different meanings associated with different occurrences 
of the same symbol depending on the type it has. For example the symbol '+' might be overloaded by declaring 
it as the insertion operator for trees as defined above: dec + : alpha £ tree(alpha) -> tree(alpha); 
--- object + themtree <= tree~,insert(object, theL, tree); Provided that an unambiguous type can be 
deduced for an operator free its context, and that theoverloadingtypes are disjoint, use of such an 
operator provides no problem. The compiler parses type expressions using similar methods to those used 
for conventional expressions, and represents the types using a recursive HOPE structure hope~type. Substitutions. 
required to implement the unification in the type resolution algorithm are expressed as an extension 
of the built in type map. with the added property that if a substitution is not defined on an item then 
it behaves as an identity. The code generating functions make considerable use of higher order reduction 
functions, operating on the parse tree. With the data structure in the form of a tree. pre-order, in-order 
and post-order reduction may be defined. All these functions are defined with polymorphic types making 
them as widely applicable as possible. Code generation is done in two parts: the first is a restructuring 
of the parse-tree and the second produces the code to be output, In the first part the parse-tree is 
transformed into a form closer to the compiler target language. Function symbols are rewritten in a form 
acceptable to the CTL assembler, nested suspended expressions removed, and explicit selector functions 
introduced into certain types of expressions involving tuples. A reference count for variables on the 
right of each equation is required before the start of the main code generation to set the reference 
counts of packets representing expressions bound to variables in let or where clauses. The prepass is 
done using a post-order reduction of the function fix=code declared in figure 10 over the equation. 
data arg~descriptor == argdesc( hope~obj £ ! modified arg ! truval £ ! contains suspensionl hope~obj-->num);! 
ref counts of vars ! data ex~spec -~ exs( list arg~descriptor£! arguments ! symbol~table £ ! typing 
info ! set hope~obj); ! generated aux fns! dec fix~code : hope~obj £ ex~spec ->ex~spee; Figure I0: 
The main function of the pro- pass. The function examines the HOPE object and the list of descriptors 
for the modified arguments, when processing an application of suspe.nd if one of its arguments is flagged 
as conDalnlng another suspended expression, an auxilliary function definition with the argument as is 
right side is added to the set. and the operand is replaced by a call to it. The main code generation 
is done by a tree walk over both sides of each equation. The left-hand side is traversed in pre-order 
building path expressions describing each node. When a constructor or a constant is found a pair containing 
the path expression and node value is generated and added to a map for use when processing the right 
of the equation. The maps generated by the left of the second equation for Append are illustrated in 
figure 11 Maps are printed in brackets, with => between elements of each pair. [ data~constr(ident("cons")) 
=> [I .0]. variable(ident("x")) => [1.1] variable(ident("y") ) => [2. I] variable(ident("z")) => [1.2] 
] Figure 11: Map produced from Append lhs. After processing the left-hand side of the Re~tion the map 
is used to generate the IRES and RULE statements describing when the equation is applicable. The packet 
skeleton representation of the right-hand side of an equation is produced by a modified post-order reduction 
of the parse- tree. When generating code for let or where clauses the pattern (usually a variable) bound 
to the expression does not translate into packet skeletons. Consequently the post- order reduction function 
used in code-~eneration must be modified to prevent it aversing this particular part of the parse tree 
and generating packet skeletons. If a lambda expression is encountered in the equation, the function 
which generates code for a function definition is called (recursively) to translate the list of equations 
making up the body of the expression. When a variable is encountered on the right of an equation, it 
is looked up first in a list of local bindings associated with any current let or where clauses, and 
then in the map generated by processing the left-hand side of the equation (and any surrounding equations 
if code is being generated for a lambda expression). In figure 12 we give the type declarations and 
one case for the code generation function, which is applied by reduction. The case corresponds to function 
application at top level in the source program. The argument path expression or packet identifier representing 
a node is passed back up the tree and added to the argument list of the parent node, When a function 
application is encountered the function name is output followed by the list of its arguments. 290  
data r~,code =: the( name~,list £ names of args list(list loc~bind) £ local bindings errset £ errors 
in cgen num £ packet number num £ lambda expn no list (list char)£ code list packet~args £ packet arg 
listl hope=obj £ packet functionl list pack £ partial packetsl refs~list); ref counts ! dec rhs~comp 
: hope=obj £ I Object to compile I r~code ->rh~code; ---rhs~comp(expr~apply(const(ident(fname)),J, rhc(names,lbind,errs, 
packnum,lnum. code,packargs,pf,nil,re)) <=rhc(nemes,lbind,errs.succ(packnum),lnum, code <*> ["@ ",fname, 
print~packet~args(packargs), ';':: crlf],nil,empty,nil,rc); Figure 12: Code generating function. While 
the compiler is currently a research project, with high performance definitely not the primary goal, 
it has always been intended to demonstrate that significant programs can be written in a ±anguage like 
HOPE. Additionally a useable HOPE compiler will be required to make use of ALICE when the hardware is 
produced. Following current ideas on software development, [Darlington 1981], [Hcare 1981] primary attention 
has been focused on producing a clear program which can be demonstrated to be correct and then to try 
and improve its performance. The applicative nature of the program makes it a candidate both for verification 
and for correctnesspreserving program transformation. The latter is particularly attractive since a transformation 
system written in HOPE is also being developed at Imperial College. At the current stage of development, 
only ~mall optimisations have been made. This is due partly to the desire to complete the logic of the 
program first, and also to the realisation that optimisation for execution on the current sequential 
implementation may not be appropriate in viow of the intention to transport the compiler to a parallel 
machine. The changes made have been made mostly to reduce the amount and level of recursion; for example 
the repeated appending of items to the ends of arg~ent lists has been eliminated by keeping the lists 
in reverse order until they are printed out. Similarly. the cost in time and space of appending large 
lists onto the end of the code list has been reduced by making it into a list of lists. Introduction 
of lazy evaluation, described earlier, has served to remove the requirement to hold the program in store. 
 The most dramatic performance improvement has been produced not by changes to the program but to the 
execution environment; Following advice frem the implementers of the Edinburgh HOPE system, the behaviour 
of the POP-2 garbage collector was monitored while the compiler was running. The storage of the POP-2 
and hence the HOPE System is allocated in different areas (cages) for each record type. By determining 
which type was most heavily used and enlarging the cage size before running the compiler, the garbage 
collect time was substantially reduced giving a noticeable improvement in both CPU and real time used. 
It is worth noting that ALICE performs garbage collection in parallel with computation, so this problem 
will not arise in our implementation. Writing a program the size of the compiler in HOPE has proved 
an instructive experience. The absence of side effects and the presence of a modularisetion facility 
have proved most helpful in understanding the program and finding logical errors, The use of higher order 
functions (particularly on recursive data structures) localises the recursion, and the function which 
does the work (is applied by the higher order function) is made much simpler. An example of this has 
been the code seneration for the right-hand sides of equatzons: the generating function becomes a set 
of non-recursive equations dealing with different cases corresponding to alternative nodes in the parse-tree. 
Extensions to the code generation were made simply by adding now equations without any changes to existing 
 equations. ~¢~ ~8 A~ ~]~ ~ A version of majority of the HOPE compiler structures hanis dling the currently 
 running on the Edinburgh DEC-10. Code generated by this compiler has been transferred via SERCNET to 
the departmental IBM 4331, assembled and executed on the ALICE simulator running on that machine. Code 
for the remaining unimplemented HOPE features and a primitive separate compilation faeilty has been written 
and work is in progress integrating this with the existing c~mpiler. The parser module is also in use 
in a metalanguage-controlled program transfemation system written in HOPE [Darlington 1981]. The transformation 
system and compiler are intended to form part of an applicative programming environment running on the 
ALICE machine. Design work is being done on an applicative operating system to form the basis for this. 
A structure editor for HOPE and a compiler for a relational PROLOG-like language for ALICE [Clark and 
Gregory 1981] are also being written. In conjunction with these projects, extensions to HOPE to allow 
more convenient control of the mode and order of execution are being designed. As a preliminary to the 
bootstrap of the HOPE compiler onto the Imperial College ALICE simulator, a study of the optimisations 
required to allow the compiler to compile itself in a reasonable time is being made. It is believed 
that the compiler described here is unique in being written in a totally applicative manner (except for 
a call to a function written in POP-2 to create a file from the code list). The advantages from the programming 
point of view have been described above. The size of the source (as compared say to the Edinburgh compiler 
in POP-2. an estimated 7000 lines) and the ease of programming and modification have been very gratifying 
At the moment, running under two levels of interpretation on a yon Neumann machine, performance has not 
been good (about 5-10 minutes real time and 180 CPU seconds. half that in garbage collection, for compilation 
of 20 lines) An improvement is expected after the bootstrap to the simulator and a much larger one when 
parallel machines intended for applicative programming (ALICE in particular) become available. At this 
point the parallelism inherent in the compiler can be exploited and co-routining replaced by parallel 
evaluation. ~W~e~g~. The author gratefully acknowledges the support of the Science Research Council 
of the United Kingdom during the first part of the work described in this paper. Mike Reeve's assistance 
in the running and debu~gin~ of ALICE CTL programs and his cooperatlon in incorporating suggestions into 
the language and assembler have contributed greatly to the progress of this work. Thanks are also due 
to Roger Bailey. John Darlington and Mike Reeve for their comments and suggestions regarding this ~aper 
and to Don Sannella for his help in the use of Edinburgh HOPE. <RefA>(I) Backus J. Can Programming be Liberated 
from the yon Neumann style? ACM Turing Lecture 1978.CACM Vol 21 no 8 August 1 978. (2) Banatre J.P. 
° Routeau J.P. and Trilling  L. An Event-Driven Compiling Technique. CACM Vol 22 no 1 January 1979. 
  (3) Burstall. R.M. Design Considerations for a Functional Language. Infotech State of the Art Conference. 
Copenhagen 1977.  (4) Burstall R.M., MacQueen D.B. and Sannella D,T. HOPE: An experimental Applicative 
Language. Internal Report CSR-62-80 (Updated Feb. 1981). Department of Computing Science. University 
of Edinburgh.  (5) Clark K. and Gregory S. A Parallel Relational Language. Proceedings ACM Conference 
on Functional Programming Languages and Computer Architecture Boston 1981.  (6) Darlington J. The Structured 
Description of Algorithm Derivations. Invited Paper. International Symposium on Algorithms Amsterdam. 
1981.  (7) Darlington J. and Reeve M ALICE: A MultiProeessor Reduction Machine for the Parallel Evaluation 
of Applicative Languages. Proceedings ACM Conference on Functional Programming Languages and Computer 
Architecture Boston 1981.  (8) Eisenbach S. HOPE SWURCC Micro Software Quarterly Issue 5 Nov 1981. University 
of Bath Computer Center.  (9) Gordon M.J.C . Milner. A.J R G.o Morris L.. Newey M. and Wadsworth C A. 
A Metalanguage for Interactive Proof in LCF. Proc 3rd ACM. POPL, 1978.  (I0) Henderson Functional Programming. 
Application and Impl ementa tion. Prentice Hall 1980 (11) Hoare C.A.R. The 198oEmperor's Old Clothes 
 ACM Turing Lecture. CACM Feb 1981.  (12) Iverson K.E. A Programming Language. Wiley 1962.  (13) Landin 
P.J. A Correspondence between ALGOL 60 and Church' s Lambda Notation, Part I. CACM 8 1965.  (14) Milner 
R. A Theory of Type Polymorphism in Programming. JCSS 17° 3 December 1978  (15) Reeve M. An Introduction 
to the ALICE Compiler Target Language. Internal Report, Department of Com~uting. Imperial College London. 
19~2  (16) Reeve M. Using the ALICE Abstract Simulator. Internal Report. Department of Computing. Imperial 
College London, 1982.  (17) Reeve M. Using the ALICE CTL Assembler for the ALICE Abstract Simulator 
Internal Report. Department of l~puting. Imperial College London,  (18) Wadler P. Applicative Style 
Programming. Program Transformation and List Operators. Proceedings ACM Conference on Functional Programming 
Languages and Compiler Architecture Boston 1981.  (19) Wu W.H. Using the Imperial College HOPE interpreter. 
Internal Report. Department of Computing, Imperi~ ~. College, 1982. </RefA>
			
