
 An Analysis of Trends in Productivity and Cost Drivers over Years Vu Nguyen LiGuo Huang Barry Boehm 
Faculty of Information Technology Computer Science &#38; Engineering Dept. Computer Science Department 
University of Science, Vietnam National University - Ho Chi Minh city Southern Methodist University PO 
Box 750122 University of Southern California 941 W. 37th Pl, SAL 326 227 Nguyen Van Cu, Dist 5 Dallas, 
TX 75275-0122, USA Los Angeles, CA 90089, USA Ho Chi Minh city, Vietnam +1-214-768-3709 +1-213-740-8163 
+84-90-817-5957 lghuang@lyle.smu.edu boehm@usc.edu nvu@fit.hcmus.edu.vn ABSTRACT Background: Software 
engineering practices have evolved considerably over the last four decades, changing the way software 
systems are developed and delivered. Such evolvement may result in improvements in software productivity 
and changes in factors that affect productivity. Aims: This paper reports our empirical analysis on how 
changes in software engineering practices are reflected in COCOMO cost drivers and how software productivity 
has evolved over the years. Method: The analysis is based on the COCOMO data set of 341 software projects 
developed between 1970 and 2009. We analyze the productivity trends over the years, comparing productivity 
of different types and countries. To explain the overall impact of cost drivers on productivity and explain 
its trends, we propose a measure named Difficulty which is based on the COCOMO model and its cost drivers. 
Results: The results of our analysis indicate that the overall productivity of the projects in the data 
set has increased noticeably over the last 40 years. Our analysis also shows that the productivity trends 
and productivity variability can be explained by using the proposed Difficulty measure. Conclusions: 
Our analysis provides empirical evidence that the productivity trends can be characterized by the improvements 
in software tools, processes, and platforms among other factors. The Difficulty measure can be used to 
justify and compare productivity among projects of different characteristics, e.g., different domains, 
platforms, complexity, and personnel experience. Although we define the measure using the COCOMO cost 
drivers, it may not fully represent the most important factors influencing productivity. One direction 
for our future work is to analyze the effectiveness of the measure using more cost drivers on more data 
points. Categories and Subject Descriptors D.2.9 [Software Engineering]: Management---Cost Estimation, 
Productivity Permission to make digital or hard copies of all or part of this work for personal or classroom 
use is granted without fee provided that copies are not made or distributed for profit or commercial 
advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, 
to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or 
a fee. PROMISE '11, September 20-21, 2011, Banff, Canada Copyright 2011 &#38;#169; ACM 978-1-4503-0709-3/11/09... 
$10.00." General Terms Measurement, Economics, Management.  Keywords Software productivity, productivity 
trends, cost drivers, product factors, COCOMO. 1. INTRODUCTION Over the decades, software processes have 
transformed from code-and-fix and waterfall-like to spiral, iterative, and more agile approaches to developing 
software [4]. Likewise, more advanced development tools, methods, and technologies have emerged. Such 
progressive advancements have changed software development practices, leading to lower software development 
costs, reduced development time, and improved software productivity and quality. Characterizing factors 
that influence software productivity and how these factors have changed due to the evolution in software 
practices is important for researchers and practitioners in shaping software practices. However, there 
are many difficulties involved in characterizing and understanding software productivity. One reason 
is that a large number of factors affect the productivity of software development projects [8][9]. They 
reflect various characteristics of the software product being developed, platform on which the product 
to be deployed, personnel involved, project and process environments in which the software project is 
executed [2]. Another reason is that the relative impact of each factor differs from one environment 
to another [8]. Moreover, factors and their impact have evolved over the years, reflecting changes in 
processes, development tools, and technologies. For example, the storage and computing capabilities were 
the major constraints for systems 40 years ago, but they no longer are. Many reported studies have been 
focused on identifying factors and their impact on software productivity and hence costs [1][7][11]. 
Of these, software estimation is a software engineering area that has extensive focus on investigating 
software cost drivers [6]. Most software estimation models incorporate significant cost drivers and adjust 
effort according to the cost driver rating. Going beyond identifying the most relevant cost drivers, 
cost estimation models also determine relative impact of each cost driver which is used to adjust the 
estimated effort. For example, COCOMO 81 is one of the earliest cost estimation models that describe 
the multiplicative relationship between effort and cost drivers. The use of cost drivers enables the 
estimation models to be applied to contexts that differ from which the model were based on. However, 
the complex nature of software practices makes this assumption invalid. On the one hand, these estimation 
models have to be calibrated to organizations to improve estimation accuracies. On the other hand, changes 
in software engineering practices make the relative impact of cost factors change. To the best of our 
knowledge, although previous studies have significantly contributed to understanding software productivity, 
there is a lack of empirical studies on how the impact of these factors evolves over time to reflect 
the changes in software development practices. This deficiency would result in low estimation accuracies 
in estimating future projects and causing strategies unsatisfactory to improve software productivity 
measurement and management. In this paper, we present our empirical analysis of the trends of productivity 
and cost drivers based on the COCOMO data set of 341 projects collected over the last 15 years of the 
model s development and extension effort. These projects were completed in between 1970 and 2009. Although 
many factors influencing software productivity worth investigation, we believe that COCOMO data set captures 
a good set of software project characteristics. In fact, Trendowicz and Munch [8], in their comprehensive 
review of factors influencing software productivity, notes that COCOMO cost drivers are the most widely 
selected productivity factors in published productivityÂ­related studies. In this study, we also introduce 
a measure of overall impact factor namely Difficulty using 22 cost drivers defined in COCOMO. The results 
show that the productivity, which is defined as the total equivalent lines of code per person-month, 
has generally increased over the last 40 years. This productivity trend and productivity variation can 
be characterized by the changes in the cost drivers. The results also show that factors related to the 
use of software tools, process maturity, platform, and personnel experience have improved significantly. 
The rest of the paper is organized as follows. Section 2 provides the background information. Section 
3 describes the study method. Sections 4 and 5 present results of the study and discussion. And the conclusion 
is given in Section 6.  2. BACKGROUND 2.1 COCOMO and Cost Drivers The Constructive Cost Model (COCOMO) 
was originally published in the text Software Engineering Economics [1]. This original model is often 
referred to as COCOMO 81. The model was defined based on the analysis of 63 completed projects from different 
domains during the 1970s and the early 1980s. To address the issues emerging from advancements and changes 
in technologies and development processes, the USC Center for Systems and Software Engineering has developed 
and published COCOMO II. The model was initially released in [5] and then published in the definitive 
book [2]. Among the main upgrades are the introduction of new functional forms that use scale factors, 
new cost drivers, and a new set of parameters values. COCOMO II comprises of three sub-models, Applications 
Composition, Early Design and Post-Architecture. The Applications Composition model is used to compute 
the effort and schedule to develop the system that is integrated from reusable components and other reusable 
assets using integrated development tools for design, construction, integration, and test. The Applications 
Composition model has a different estimation form from the other models. It uses Application Points or 
Object Points [11][12] as the size input and a productivity rate to calculate effort. The Early Design 
model is used in early stages of the project when the project information is not detailed enough for 
a fine-gain estimate. When the detailed information is available (e.g., the high level design is complete, 
development environment is determined), the Post-Architecture model is used instead. The Early Design 
and Post-Architecture models use source lines of code as the basic size input and follow the same arithmetic 
form. The general form of the effort formulas of the Early Design and Post-Architecture models can be 
written as p PM = A* Size B *.EMi (2.1) i Where, PM is effort estimate in person months.  A is a multiplicative 
constant, which can be calibrated using historical data.  Size is an estimated size of the software, 
measured in KSLOC.  B is an exponential component comprising of scale factors.  EM specifies effort 
multipliers, which represent the multiplicative component of the equation. B is defined as a function 
of scale factors, in form of 5 B =Ã0 +Ã1 .SFi(2.2) i=1 where Ã0 and Ã1 are constants, and SFi is one 
of the five scale factors. The exponential component B accounts for the relative economies or diseconomies 
of scale encountered due to differences between small and large projects. Table 1. COCOMO II Cost Drivers 
 Category Cost Dri ver Scale PREC Precedentedness of Application Factors FLEX RESL TEAM PMAT Development 
Flexibility Risk Resolution Team Cohesion Equivalent Process Maturity Level Product RELY Required Software 
Reliability Factors DATA CPLX RUSE DOCU Database Size Product Complexity Developed for Reusability Documentation 
Match to Lifecycle Needs Platform Factors TIME STOR PVOL Execution Time Constraint Main Storage Constraint 
Platform Volatility Personnel ACAP Analyst Capability Factors PCAP PCON APEX LTEX PLEX Programmer Capability 
Personnel Continuity Applications Experience Language and Tool Experience Platform Experience Project 
Factors TOOL SITE SCED Use of Software Tools Multisite Development Required Development Schedule COCOMO 
II uses 7 in the Early Design model and 17 in the Post-Architecture model. The effort multipliers are 
the cost drivers that have multiplicative impact on the effort of the project while the scale factors 
have exponential impact. The Early Design and Post-Architecture models have the same set of scale factors 
while the cost drivers in the Early Design model were derived from those of the Post-Architecture model 
by combining drivers that were found to be highly correlated. The COCOMO II Post-Architecture model s 
rating values of the model cost drivers were calibrated using the Bayesian technique on a database of 
161 project data points [13].  2.2 COCOMO Calibration The constants and numeric ratings of the cost 
drivers in the model (2.1) are determined using historical data through a process called calibration. 
The first step in this process is to transform the nonÂ­linear form of Equation 2.1 to a linear form by 
applying the natural logarithmic transformation: log(PM) = Ã0 + Ã1 log(Size) + Ã2 SF1 log(Size) + + 
(2.3) Ã6 SF5 log(Size) + Ã7 log(EM1) + + Ã23 log(EM17) Equation (2.3) is a linear form, and its coefficients 
can be estimated using a typical multiple linear regression approach such as ordinary least squares regression 
and the Bayesian analysis. The Bayesian analysis was applied to calibrating COCOMO s constants and cost 
drivers that are published in [2, 24]. This approach was shown to help alleviate problems, such as counterÂ­intuitive 
estimates of coefficients, often caused by irregularities in the software data set. In this study, we 
use the Bayesian analysis to calibrate the model s constants and cost drivers as it is the currently-preferred 
and commonly-used calibration approach for the COCOMO model. As space limits a thorough discussion of 
the approach, we refer the reader to the detailed description of the approach given in [2, 24]. 2.3 
Software Productivity Improving software productivity is an important goal in software engineering. The 
first step in this process involves defining the productivity measure, identifying productivity influencing 
factors, and characterizing their impact on software productivity. According to Boehm [3], software productivity 
is defined as the ratio of outputs produced divided by inputs consumed by the software process. Typically, 
the inputs are measured in terms of the amount of effort or cost spent and the outputs in terms of the 
size of software delivered. While the effort unit can be consistently defined, there is a lack of a uniform 
and uncontroversial metric for software size. This deficiency results in various productivity units having 
been used, dependent on the size metric used. Most commonly, the productivity is either computed as the 
number of source line of code (SLOC) or function points per person-month. Even so, many SLOC and function 
point units are used. This problem makes software productivity measurement and management difficult. 
It is important to note that, these productivity units are not always convertible, which means that an 
improvement in productivity seen in one unit may not be seen in another. In this study, we measure productivity 
as KSLOC (2.3) Productivity = PM Where, PM is the project effort measured in person-month. KSLOC is kilo 
logical source lines of code or logical statements in the source code [2][22]. For new code, KSLOC is 
the new source code delivered while it is measured as an equivalent KSLOC for adapted or maintained code. 
It should be noted that maintenance projects involving in maintaining and upgrading the existing systems 
while maintaining the system s core functionality. For adapted and maintained code, KSLOC is the adjusted 
KSLOC size based on amount of software to be adapted, the percentage of design modified (DM), the percentage 
of code modified (CM), the percentage of integration and testing (IM), the degree of Assessment and Assimilation 
(AA), understandability of the existing software (SU), and the programmer s unfamiliarity with the software 
(UNFM) if the software is fully or partially adapted or reused from other components or products [2]. 
That is, Equivalent KSLOC is related to the equivalent amount of effort required to develop a new software 
system or adapt it from preexisting software components or products. In reuse and adaptation systems, 
Equivalent KSLOC differs from the total delivered KSLOC in the software. It does not include black-box 
COTS, automatically generated code, open source code, and library code that are not modified or tested 
by the software team.  3. METHOD 3.1 Research Questions In this study, we attempt to answer the following 
main research questions: Research question 1: What trends does the productivity of the software projects 
present over the years from 1970 to 2009? Research question 2: To what extend can the COCOMO cost drivers 
explain possible trends in productivity? Research question 3: How has the overall rating of each of the 
COCOMO cost drivers changed over the years from 1970 to 2009? The cost drivers reflect actual characteristics 
of the software project at the time it is completed. They can be considered an instance reflecting the 
software practices that captures important information of the project. Thus, changes in software practices 
overtime would affect the overall ratings of the cost drivers. 3.2 Dataset As an important part of the 
effort to extend and upgrade the COCOMO model, the USC Center for Systems and Software Engineering has 
continued collecting COCOMO-like data from completed projects in industry. The center s Affiliate membership, 
which presently consists of over 40 organizations, has been the main source of the COCOMO database. We 
use the current set of the database which consists of 341 data points provided by 25 organizations in 
four countries including United States, Brazil, Vietnam, and Thailand (see Table 2). The projects in 
the data set are either new development (developing from scratch) or maintenance (maintaining and upgrading). 
These projects have the completion years ranging from 1970 to 2009. They represent a variety of application 
domains such as management of information system (MIS), command and control, signal processing, and scientific 
systems. This dataset contains 161 data points that were used to calibrate the constants and cost driver 
rating values that were often referred to as COCOMO II.2000 [2]. Many of the data points in the 1970-1979 
period were converted from the COCOMO 81 data set using a system called the Rosetta Stone [10]. There 
are additional 149 projects that were collected after the release of the COCOMO II model in 2000. An 
approximate two-third of the data points were collected between 1995 and 2004, indicating an intensive 
data collection and model development effort carried out by the COCOMO team during these years. Table 
2. Number of Data Points by Countries and Project Types Country New Development Maintenance Total US 
191 116 307 Brazil 16 0 16 Vietnam 0 14 14 Thailand 0 4 4 The core attributes of each data point are 
actual effort, actual size measures, and rating levels of the cost drivers (Table 3). The cost driver 
is a categorical variable whose value range is divided into six rating levels of VL Very Low, L Low, 
N Nominal, HI High, VH Very High, and XH Extra High. In many cases, a rating that falls between two 
defined rating levels is specified, allowing fine-grained increments in the rating scale to more closely 
describe the true value of the cost driver under evaluation. The dataset stores these symbolic rating 
levels instead of numeric scales, separating the observed information and the modelÂ­calibrated constants 
and thus allowing different sets of numeric scales to be mapped and evaluated. If the rating of a cost 
driver falls in between two defined rating levels, we use a linear extrapolation from the defined values 
of the levels. Table 3. Attributes in the data set Attribute Type Description Effort Ratio Actual project 
effort measured in personÂ­month. Size1 Ratio Size in KSLOC. Cost Drivers Nominal Twenty two cost drivers 
(see Table 1) Completion Year Ratio The year in which the project is completed. Project Type Nominal 
Either New Development or Maintenance Country Nominal Country from which the project was collected  
3.3 Study Design To investigate the trend of productivity and cost drivers, we grouped all projects that 
were completed in the same year into the same group. We validated potential trends by testing the significance 
of the correlations between the completion year and the cost drivers and productivity. That is, we tested 
the null hypothesis that the difference between the correlation coefficient and zero is not statistically 
significant. In testing the significance of the correlation between two variables, the hypothesis testing 
technique used is largely determined by considering whether the populations from which the variables 
were drawn are normally distributed. As the completion year and productivity attributes are not guaranteed 
to be normally distributed, we used the Kendall rank correlation coefficient to validate potential trends, 
Mann-Whitney U test to check the statistical differences between two groups, and Kruskal-Wallis test 
to validate the differences among three or 1 Other size attributes as described in Section 2.3 are also 
included for computing equivalent KSLOC in maintained and adapted code. more groups. We chose the level 
of significance of 0.05 as a criterion to make conclusions about the hypothesis tests.  4. EMPIRICAL 
ANALYSIS 4.1 Productivity Trends Each box plot in Figure 1 shows the 1st quartile, median, 3rd quartile, 
and outliers of productivities of the projects completed in each year. The x-axis shows both the project 
completion years and the number of projects completed in each year. There are no data points that have 
the completion year between 1980 and 1985. The line across the box plots is a lowess nonparametric regression 
line using all data points. Although the productivity varies significantly in each year group, the median 
productivities and the lowess smoothing line indicate that the productivity increases significantly over 
the years. This trend is confirmed by the Kendall rank test which shows that there is a strong positive 
correlation between the productivity and the completion year (p-value = 7.89-13, Kendall s t = 0.27). 
Comparing the productivity of the projects completed in 1970 and 2008, for example, we see that the median 
productivity improves from 300 SLOC per person-month in 1970 to 747 SLOC per person-month in 2008 or 
a 149% increase. It should be noted that the project in 2009 has a high productivity but it is the only 
project completed in this year. By examining the lowess smoothing line in Figure 1, we can split the 
trend into three sub-trends for three respective separate periods. The first 10-year period between 1970 
and 1979 has 48 projects whose productivities are generally low. This period is marked by a significant 
increase in productivity in the last four years. The result of Kendall rank test (p-value = 0.005, t 
= 0.3) confirms the positive correlation between the productivity and the completion year during this 
period. The second sub-trend spans from 1986 to 1999. Surprisingly, the productivity of the projects 
in this period steadily decreases as indicated in the result of Kendall rank test with p-value = 0.013 
and t = -0.15. The productivity decrease in this period is probably dictated by 48 projects in 1997. 
This year has the largest number of projects completed in a year, but most of its projects have low productivities. 
The last sub-trend starting from 2000 to 2009 is seen to have a sharp increase in productivity, which 
is demonstrated by the result of Kendall rank test with p-value = 0.0065 and t = 0.16. In an attempt 
to explain these observations, we will pursue further analysis in the following sections on the productivity 
by examining the factors that affect productivity and explaining how they reflect the productivity trend. 
 4.2 Productivity and Project Type Two different types of project exist in the data set, new development 
and maintenance. In the new development type, the system is built from scratch with possible reuse of 
existing components. In the maintenance type, the system already exists, and the project focuses on modifying, 
enhancing, and adding new functionality. Unlike the new development project, which spends much time in 
defining requirements, architecture and design, the maintenance project usually deals with the legacy 
code, the existing design, architecture, and supporting documentation. Their productivity is highly dependent 
on the quality of these work products and their experience on the existing system.  Figure 1. Productivity 
distribution over the years The data set has 207 projects classified as new development and 134 as maintenance. 
All of the maintenance projects were completed between 1989 and 2009, and 24 out of 26 projects completed 
between 2006 and 2009 are maintenance. Figure 2 shows the box plots of productivity distribution for 
new development and maintenance. The productivity of the maintenance projects is significantly higher 
than that of the new development projects. Median productivity of maintenance is twice as much as that 
of new development. Still, there are a number of new development projects that have high productivities, 
which are shown as outliers in Figure 2. A Mann-Whitney U test confirms that there is a statistically 
significant difference in the productivity between development and maintenance (p-value = 0). As most 
of the maintenance projects were completed in the last four years, it is clear that the productivity 
of the projects completed in these years which is shown in Figure 1 is explained by the fact that the 
productivity of maintenance is greater than that of new development. Figure 2. Productivity Distribution 
between New development and Maintenance Figures 3 and 4 show the box plots of productivity distribution, 
similar to those of Figure 1 but separating the maintenance projects and the new development projects. 
The productivity trend in the first 10 years in Figure 1 and Figure 4 is identical as all of the projects 
in this period are new development. In both maintenance and new development, the productivity seems to 
decrease during the period from 1986 to 1999 and increase in the last 10 years. Overall, the productivity 
increase trends are found in both maintenance and new development, which is evidenced by the Kendall 
rank tests with p-value = 0.01 for maintenance and pÂ­value = 1.8 * 10-5 for new development.   4.3 
Productivity and Country Now we investigate whether the productivity is different among the countries 
from which the projects were performed. CountryÂ­specific factors such as culture and labor regulations 
may affect the productivity of the projects. As shown in Table 2, the data were collected from four countries, 
U.S., Brazil, Vietnam, and Thailand with about 90% of the projects from the vendors in the U.S. With 
the imbalance number of projects among these countries, it would be misleading to compare the productivity 
distribution from these countries. Nonetheless, treating these data sets from different countries as 
random samples, it would be useful to explain the difference in the productivity among them. As shown 
in Figure 5, the U.S. projects have lowest productivities while there are still outlier projects that 
have extremely high productivities. A Kruskal-Wallis test indicates that there is a significant difference 
in the productivity among the countries (pÂ­value = 0.00017). Meanwhile, there is no statistical difference 
in the productivity among the countries other than the U.S. (p-value = 0.907). This means that the difference 
in the productivity between the US projects and the other countries is statistically significant, and 
indeed, the US projects have lower productivities. KSLOC/PM Brazil Thailand U.S.   Vietnam Figure 
5. Productivity Distribution among the Countries  4.4 A Difficulty Measure In the previous sections, 
we showed and compared the raw productivities of the projects over the years and raw productivities of 
different project types and countries. While the productivity is affected by many factors, the comparisons 
above isolated the effects of the influencing factors. In this section, we analyze the productivity by 
considering the overall impact of the influencing factors. The COCOMO model uses software size and 22 
other factors considered to be the most relevant and influential drivers of software cost. These cost 
drivers capture finer-grained and more comprehensive information than project type, country of development, 
and domain. Moreover, the model provides an approach to aggregating different effects of the cost drivers. 
The COCOMO s cost drivers are classified into multiplicative and exponential components as presented 
in Section 2.1. Based on the model s formula (2.1), we define a difficulty measure to take into account 
the overall impact of the cost drivers on productivity as 17 100B * EM . i (4.1) i=1 Difficulty = 100 
Where 5 , EMi and SFi are effort multipliers and B =Ã0 +Ã1 .SFi i=1 scale factors, respectively. In 
the definition of Difficulty, we used a 100 KSLOC project as the baseline. That is, Difficulty measures 
the overall impact of the cost drivers on productivity of a project as if it is scaled to the baseline 
project. Difficulty represents how difficult it is in terms of the effort required to build the system. 
The higher Difficult is, the more effort is required or less productive the project is. Difficulty and 
the productivity have an inverse relationship, i.e., Difficulty increases as the productivity decreases, 
and vice versa. The value of Difficulty is greater than zero. If Difficulty = 1.0, then the cost driver 
ratings of the project do not adjust the effort. If Difficult < 1.0, then the effect of the cost drivers 
on the project is to reduce the effort. If Difficulty > 1.0, then the effect of the cost drivers on the 
project is to increase the effort. As Difficulty has the above characteristics, we use it to explain 
the productivity trend and productivity difference among the projects. It should be noted that the B 
component in Difficulty takes into account the relative economies or diseconomies of scale. If B >1.0, 
then the overall impact leads to diseconomies of scale, and if B < 1.0, then the overall impact leads 
to economies of scale. To compute the Difficulty value for each project, we first calibrated the COCOMO 
model using all 341 data points following the procedure described in Section 2.2. This calibration process 
generated the model s constants and the rating scales of the cost drivers (e.g., effort multipliers and 
scale factors). We then converted the symbolic ratings of the cost drivers of each data point into numeric 
values using the calibrated rating scales. As a part of the calibration process, we performed the regression 
analysis using the log-transformed formula (2.3). Fitting the data set into the regression formula (2.3) 
using ordinary least squares regression we obtained R2 = 0.91 for the log-transformed regression formula 
(2.2). This result clearly indicates that 91% of the variance in effort can be explained by the cost 
drivers in the model. This high correlation between effort and the cost drivers is consistent with the 
previous COCOMO calibrations [2, 24]. Figure 6 shows the distribution of the Difficulty values of the 
projects over the years. This figure shows similar information to what presented in Figure 1 except that 
the y-axis represents Difficulty instead of productivity. As shown in Figure 6, most of the projects 
have Difficulty values close to 1.0. Some projects in 1973 and 1974 have extremely high project Difficulty 
values, reflecting high ratings of the cost drivers in these projects.  Figure 6. Difficulty Distribution 
over Years The Difficulty trends presented in Figure 6 seem to be an inverse of the productivity trends 
shown in Figure 1. This observation is further confirmed by a Kendall rank test which suggests a strong 
inverse correlation between Difficulty and the productivity (pÂ­value = 2.2 * 10-16, t = -0.55). Difficulty 
tends to decrease over the years while the productivity tends to increase as described in Section 4.1. 
A Kendall rank test suggests that the project Difficulty and the completion year have a statistically 
significant correlation with p-value = 3.7 * 10-8 and t = -0.21. The three sub-trends observed and described 
in Section 4.1 are also reflected in the Difficulty trend. The project Difficulty decreases in the period 
between 1970 and 1979 while the Difficulty productivity increases in this same period. The project Difficulty 
Figure 7. Difficulty Distribution between New development then increases in the following period from 
1986 to 1999. The last 10-year period is seen to have decreasing project Difficulty values. As the project 
Difficulty is a measure representing the overall impact of the cost drivers, it can be implied that the 
productivity trends generally reflect the changes of the cost drivers over the years. Now we use Difficulty 
to explain the difference in the productivity between new development and maintenance and among the countries. 
 and Maintenance Difficulty  As shown in Figure 7, the maintenance projects have lower Difficulty than 
those of the development projects (Mann-Whitney test s p-value = 8.04*10-13). This result indicates that 
the maintenance projects are more productive than the new development projects as they are less difficult. 
The similar explanation using Difficulty can also be applied to the difference in productivity between 
the U.S. and non-U.S. projects. The U.S. projects are less productive because they are more difficult 
than the projects from other countries (see Figure 8). Brazil Thailand U.S. Vietnam Figure 8. Difficulty 
Distribution among Countries  4.5 Trends in COCOMO Cost Drivers This section describes changes in the 
ratings of 22 cost drivers collected in the 341 projects spanning from 1970 to 2009. Table 4 shows Kendall's 
Rank Correlation Coefficients between the completion year and the cost drivers. The cost drivers are 
listed in the order of their degrees of correlation with the completion year. The p-value indicates the 
null hypothesis test that the estimate of correlation coefficient is statistically equal to zero. The 
sign of Kendall s t value implies the direction of the correlation. Use of Software Tools Thirteen out 
of the twenty-two cost drivers were found to have statistically significant correlations with the completion 
year (pÂ­value < 0.05). They include one project factor (TOOL), two scale factors (PMAT, PREC), three 
product factors (DATA, CPLX, RELY), four personnel factors (PLEX, APEX, LTEX, ACAP), and all three platform 
factors (TIME, STOR, and PVOL). Of these, TOOL has the strongest correlation (|t| = 0.37) followed by 
PMAT, STOR, and TIME (|t| = 0.26). All but APEX, DATA, and ACAP have negative correlations with the completion 
year, indicating the decrease in overall numeric rating scales of these cost drivers on the projects 
over years. Table 4. Kendall's Rank Correlation Coefficients between the Completion Year and Cost Drivers 
(sorted by degrees of correlation) Cost driver Kendall s t p-value TOOL Use of Software Tools -0.37 2.20E-16 
PMAT Process Maturity -0.30 1.22E-13 STOR Main Storage Constraint -0.29 1.31E-11 TIME Execution Time 
Constraint -0.26 6.62E-10 PLEX Platform Experience -0.17 1.98E-05 PVOL Platform Volatility -0.18 2.04E-05 
APEX Applications Experience 0.17 4.88E-05 LTEX Language and Tool Experience -0.15 2.84E-04 DATA Database 
Size 0.13 1.81E-03 RELY Required Software Reliability -0.10 1.42E-02 CPLX Product Complexity -0.10 1.58E-02 
PREC Precedentedness of Application -0.09 2.13E-02 ACAP Analyst Capability 0.08 4.87E-02 SCED Required 
Development Schedule -0.09 5.49E-02 DOCU Documentation Match to -0.07 8.29E-02 Lifecycle Needs RUSE Developed 
for Reusability -0.07 8.81E-02 RESL Risk Resolution -0.06 1.64E-01 PCAP Programmer Capability 0.05 2.40E-01 
SITE Multisite Development -0.04 3.52E-01 FLEX Development Flexibility -0.04 3.78E-01 TEAM Team Cohesion 
0.01 7.66E-01 PCON Personnel Continuity 0.01 7.93E-01  Figure 9 shows the distribution and the lowess 
nonparametric regression line and its spread lines of the Use of Software Tools (TOOL) cost driver on 
the completion year with the y-axis showing numeric scales and corresponding symbolic rating levels. 
Each little circle in the figure represents the TOOL rating for one project. We can see that the numeric 
rating scale of TOOL decreases considerably over the period from 1970 to 2009. This observation is confirmed 
by the Kendall test s p-value = 0. This means that the use of software tools in the projects increases 
from Very Low (1.17) in the early 1970 s to Nominal-High (0.95) in the late 2009 s. That is, if other 
factors are held constant, the improvement in software tools helps reduce software effort by approximately 
19%. Process Maturity (PMAT) is also found to have the second strongest negative correlation with the 
completion year, suggesting that PMAT has improved over the year. The overall numeric rating scale of 
PMAT decreases significantly over years. The overall PMAT rating increases from Low (6.24) to High (3.12), 
corresponding to an improvement in software processes from SW-CMM Level 1 Upper-equivalent to SW-CMM 
Level 3Â­equivalent. Among the four personnel factors that have high correlations with the completion 
year, Applications Experience (APEX) and Analyst Capability (ACAP) increase in their rating scales while 
the rating scales of Platform Experience (PLEX) and Language and Tool Experience (LTEX) tend to decrease 
over the years. That is, the personnel level of experience in platform (PLEX), languages and tools (LTEX) 
tends to increase while their level of experience in applications (APEX) and the capability of analyst 
(ACAP) decrease over the years. This phenomenon can be interpreted that while people have been more experience 
with platforms, languages, and tools, they tend to switch between different applications more frequently. 
Project personnel s average application experience slightly decreases from High (3 years) in the 1970-1975 
period to Nominal-High (2 years) in the 2005-2009 period. In the same periods, project personnel s average 
platform (PLEX), languages and tools (LTEX) experience modestly improves from Nominal-Low (less than 
1 year) to Nominal-High (2 years). All three platform factors Execution Time Constraint (TIME), Main 
Storage Constraint (STOR), and Platform Volatility (PVOL) were found to have a statistically significant 
correlation with the project completion year (p-value < 2.04*10-5). The projects in 1970-1980 tend to 
have higher TIME and STOR rating levels than those in 2000-2009, indicating that the platform s execution 
and storage capabilities were the more important constraints for projects completed during 1970-1980 
than during 2000-2009. The average TIME and STOR rating levels in 2000-2009 is close to Nominal. Note 
that Nominal is the lowest defined level for both TIME and STOR, corresponding to less than 50% of available 
execution and storage to be used by the system being investigated. This result reflects the fact that 
the advancements in information technology have changed the platforms on which the software systems operate 
and loosen the constraints of computation resources on the project. Although the correlation is not highly 
strong, the product complexity (CPLX) tends to decrease over the years, indicating that that recent systems 
are not necessarily more complex than the ones built earlier. It should be noted that in COCOMO the product 
complexity is measured by the complexity of control operations, computational operations, device-dependent 
operations, data management operations, and user interface management operations. As shown in Table 4, 
nine out of the twenty-two cost drivers were found to have no statistical correlation with the completion 
year (p-value > 0.05). These include schedule compression (SCED), Level of Required Documentation (DOCU), 
Developed for Reuse (RUSE), Risk Resolution (RESL), Programmer Capability (PCAP), Multisite Development 
(SITE), Development Flexibility (FLEX), Team Cohesion (TEAM), and Personnel Continuity (PCON). This result 
indicates that despite improvements in software processes, development tools, languages, and technologies 
over the years, the project development characteristics captured in these cost drivers have not generally 
been much affected.  5. DISCUSSION The productivity trends seen in the historical data of 341 projects 
over 40 years largely support previous projections about general trends of software productivity [3][15]. 
However, the disagreement is in the magnitude of improvement. In [15], Jones showed that, although much 
of the data was projected and hence less reliable, the average U.S. software productivity increases 3 
times from 1975 to 2010, half as much as that of our result for the same period. Boehm [3] projected 
a much higher rate of 10 times or more by taking into account the use of fourth-generation languages, 
application generators, component reuse, etc. Premraj et al. analyzed the productivity of 600 Finland-based 
projects over the period from 1978 to 2003, finding that the productivity improved significantly over 
the time although at some periods the productivity tends to decrease [23]. These findings are also seen 
in our data set dominated by the U.S. projects. The overall impact of the COCOMO cost drivers which is 
defined in the Difficulty measure clearly explains the productivity trends observed in the data set. 
This measure also reflects the productivity variability among the project types and the countries from 
which the projects were performed. The maintenance projects were found to be more productive than those 
of new development projects, and this observation was explained by the fact that the maintenance projects 
are less difficult than the new development ones. Similarly, the Difficulty measure can explain the differences 
in the productivities of projects among the countries. The increase in productivity is a result of the 
decrease in the overall numeric scales of ten cost drivers that have negative correlations with the completion 
year. Only three cost drivers Applications Experience (APEX), Database Size (DATA), and Analyst Capability 
(ACAP) were found to be positively correlated with the completion year, but these correlations are not 
as strong. Nine out of the 22 cost drivers were found to have no clear trend over the years despite the 
changes in software processes, development tools, methods, and technologies. The Multisite Development 
(SITE) cost driver is determined by a subjective average of two factors, site collocation and communication 
support. The advancements in communication support over the years seem to have reverse effects in comparison 
with site collocation, thus possibly canceling out trends in SITE. Programmer Capability (PCAP) and Analyst 
Capability (ACAP), which measure the capabilities of analysts and programmers working on the project, 
are measured on percentile based on the current average. Thus, they are, by definition, not correlated 
with the time when the project is executed. Unlike PCAP, ACAP tends to increase over the years, but the 
trend is not as strong as those of the cost drivers having high correlations with the completion year. 
There are several major threats to the validity of this study. One concern is related to the reliability 
of the data set. The possible unreliability of the data may be caused by the complexity and possible 
ambiguity in the definitions of the cost drivers, inconsistency in the definition of SLOC among the data 
vendors. Most of data vendors are the users of the COCOMO model, thus, there is a certain level of confidence 
in the data provided. In many cases, the COCOMO team followed up with the vendor to validate the data. 
Moreover, detailed descriptions of the model given in the COCOMO II book and the model s user manual 
help ensure a certain level of consistency. Another major threat is that although the data set consists 
of projects of different sizes, complexity, domains, and sources, it may not represent general populations 
of software projects. Certainly, it is not a random sample. This concern may limit the generalizability 
of the findings of this study. Collecting historical project data has been and continues to be one of 
the most challenging tasks in software engineering. Researchers do not have the luxury of randomly selecting 
data samples at will. They are limited to certain organizations and application domains that are available 
to them. Nonetheless, the COCOMO data set was collected from 25 organizations that represent various 
sectors such as banking, military, aerospace, government, and software. The data set includes projects 
from diverse application domains such as management of information system (MIS), command and control, 
operating systems, process control, simulation, signal processing, and scientific systems. 6. CONCLUSION 
This paper has described our empirical analysis of the trends of productivity and cost drivers. The study 
was based on the COCOMO data set of 341 historical projects. We found that the overall productivity of 
the projects in the data set has improved considerably over the last 40 years. The changes in productivity 
are largely characterized by the combination of the cost drivers modeled in COCOMO. We devised an overall 
impact factor namely Difficult to aggregate the effects of all 22 cost drivers. This measure helps explain 
the productivity trends and the productivity variability among project types and the development countries. 
We also found that 13 out of 22 cost drivers were statistically correlated with the completion year, 
reflecting the changes in software processes, tools, methods, and technologies. Besides the Use of Software 
Tools (TOOL), Process Maturity (PMAT), Execution Time Constraint (TIME), and Main Storage Constraint 
(STOR), which are directly dependent on processes, tools, and platforms, three personnel experience factors 
Applications Experience (APEX), Platform Experience (PLEX), and Language and Tool Experience (LTEX) are 
also affected by the evolution in software engineering practices. This result implies that the improvement 
in productivity over the years is a result of the improvements in software processes, tools, platforms, 
and technologies. Clearly, there are several limitations that our study faces. The 22 COCOMO cost drivers 
may not be the most significant factors affecting the productivity of the projects. Indeed, it is likely 
that there are other influencing factors that are not captured in the data set. Moreover, the relative 
impact of the cost drivers may differ across application contexts and domains. Further study on the effectiveness 
of the Difficulty measure by taking into account other productivity influencing factors on more data 
sets is one of our interests for future work. Despite these limitations and validity concerns, our study 
suggests several useful contributions. To the best of our knowledge, it is the first to provide empirical 
evidence about the trends in cost drivers on a large set of projects completed since 1970 and introduce 
an overall impact measure Difficulty to characterize the productivity trends and productivity variability. 
It is misleading to characterize the productivity by a few influencing factors individually because the 
productivity is affected by a large number of factors and interaction effects exist among the factors. 
Researchers and practitioners can use the Difficulty measure to justify the productivity of a project 
or compare the productivities among projects. 7. ACKNOWLEDGMENTS We would like to thank the members 
of the USC CSSE Affiliates Program and data vendors form Brazil, Vietnam, and Thailand for their effort 
in collecting and contributing the data. 8. REFERENCES <RefA>[1] B.W. Boehm, Software Engineering Economics, 
Prentice Hall, 1981. [2] B.W. Boehm, E. Horowitz, R. Madachy, D. Reifer, B. K. Clark, B. Steece, A. W. 
Brown, S. Chulani, and C. Abts, Software Cost Estimation with COCOMO II, Prentice Hall, 2000. [3] B.W. 
Boehm, Improving Software Productivity, Computer, Vol. 20, Issue 9, Sept 1987, pp.43-57 [4] B.W. Boehm, 
A View of 20th and 21st Software Engineering, The 28th International Conference on Software Engineering 
(ICSE), 2006. [5] B.W. Boehm, B. Clark, E. Horowitz, C. Westland, R. Madachy, R. Selby, Cost models for 
future software life cycle processes: COCOMO 2.0, Annals of Software Engineering 1, Dec., 1995, pp. 57 
94. [6] L. Briand, I. Wieczorek, Resource Estimation in Software Engineering, Encyclopedia of Software 
Engineering, Pp. 1160 1196, Wiley-Interscience Publishing, 2001 [7] C. Jones, Estimating Software Cost, 
McGraw-Hill, New York, NY, 1998. [8] A. Trendowicz, J. Munch, Factors Influencing Software Development 
Productivity State-of-the-Art and Industrial Experiences , Advances in Computers, Vol. 77, 2009, pp.185-241, 
[9] W. Scacchi, Understanding Software Productivity , Advances in Software Engineering and Knowledge 
Engineering, Vol.4, 1995. Page 37-70. [10] D. Reifer, B.W. Boehm, S. Chulani, The Rosetta Stone: Making 
COCOMO Estimates Work with COCOMO II, Crosstalk, The Journal of Defense Engineering, Feb, 1999 [11] R. 
Kauffman and R. Kumar, Modeling Estimation Expertise in Object Based ICASE Environments, Stern School 
of Business Report, New York University, January, 1993 [12] R. Banker, R. Kauffman, R. Kumar, An Empirical 
Test of Object-Based Output Measurement Metrics in a Computer Aided Software Engineering (CASE) Environment, 
Journal of Management Information System, 1994 [13] S. Chulani, B.W.Boehm, B. Steece, Bayesian analysis 
of empirical software engineering cost models, IEEE Transactions on Software Engineering, vol. 25 n.4, 
pp. 573Â­583, July/August, 1999. [14] A. Trendowicz, M. Ochs, A. Wickenkamp, J. MÃ¼nch, Y. Ishigai, T. 
Kawaguchi, An Integrated Approach for Identifying Relevant Factors Influencing Software Development Productivity, 
Lecture Notes in Computer Science, 2008, pp.223-237 [15] C. Jones, Applied Software Measurement , McGraw-Hill, 
New York, NY, 2008 [16] A. De Lucia, E. Pompella, and S. Stefanucci, Assessing effort estimation models 
for corrective maintenance through empirical studies , Information and Software Technology 47, 2005, 
pp. 3 15 [17] M. Jorgensen, Experience with the accuracy of software maintenance task effort prediction 
models , IEEE Transactions on Software Engineering 21 (8), 1995, pp. 674 681. [18] M. Jorgensen and M. 
Shepperd, A Systematic Review of Software Development Cost Estimation Studies, IEEE Transactions on Software 
Engineering, v.33 n.1, 2007, p.33Â­53 [19] K.D. Maxwell, P.Forselius, "Benchmarking Software-Development 
Productivity", IEEE Software, Issue 17, Jan 2000, pp. 80-88 [20] Z. Chen, T. Menzies, D. Port, B.W. Boehm, 
Finding the right data for software cost modeling, IEEE Software, Nov 2005, pp. 38-46. [21] T. Menzies, 
Z. Chen, J. Hihn, K. Lum, Selecting Best Practices for Effort Estimation, IEEE Transactions on Software 
Engineering, 2006 [22] V. Nguyen, S. Deeds-Rubin, T. Tan, B.W. Boehm, A SLOC Counting Standard, COOCOMO 
II Int l Forum, 2007. DOI = http://csse.usc.edu/csse/TECHRPTS/2007/usc-csse-2007Â­737/usc-csse-2007-737.pdf 
[23] R. Premraj, M. Shepperd, B. Kitchenham, P. Forselius, An Empirical Analysis of Software Productivity 
over Time, metrics, pp.37, 11th IEEE International Software Metrics Symposium (METRICS'05), 2005 [24] 
S. Chulani, B. Boehm, and B. Steece, Bayesian analysis of empirical software engineering cost models, 
IEEE Transactions on Software Engineering, vol. 25 n.4, pp. 573Â­583, July/August 1999.  </RefA>
			
