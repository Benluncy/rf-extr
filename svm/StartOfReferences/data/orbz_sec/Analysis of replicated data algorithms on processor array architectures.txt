
 Analysis of Replicated Data Algorithms on Processor Array Architectures* P. J. Narayanan Center for 
Automation Research and Department of Computer Science University of Maryland College Park MD20742-3411 
Abstract modest computing capabilities and usually compute in unison at all times, making these machines 
SIMD Processor array machines have gained popularity (single instruction stream, multiple data stream) 
in in practical parallel processing, particularly in data nature. Each processor is directly connected 
to at least parallel application areas such as image processing. its nearest neighbors in the lattice. 
The structure of The data parallel paradigm underutilizes a massively interconnection among the PEs distinguishes 
a pro­parallel machine when relatively small data struc-cessor array machine more than the characteristics 
of tures are processed on it, which occur due to popu-its PEs. A number of such machines are commercially 
lar algorithmic techniques such as divide-and-conquer. available in the market today such as the MasPar, 
the We devised the technique of data replication com-AMT DAP, and the Connection Machine. Advances bining 
operation parallelism with data parallelism to in VLSI have made larger arrays realizable and com­speed 
up some image processing algorithms analyti-mercially viable. Machines with 64K processing ele­cally 
and practically. In this paper, we analyze repli-ments are already commercial y available; processor 
cated data algorithms on three popular array intercon-arrays containing one million or more processing 
el­nection networks to determine the conditions under ements are likely to be available by the turn of 
the which speedup can be obtained. Our analysis shows century. that the data replication technique can 
attain signif- Processor array machines embody a data parallelicant speedup even on non-global architectures 
such model of computation, which exploits the parallelismas the 3-D mesh. We also compare the analysis 
with in the data by processing all data elements simultane­experimental speedup obtained on the hypercube 
con­ously [6]. This is achieved by assigning one processingnected Connection Machine. element to each 
data element (DE) and processing all DEs in parallel. The structure of interdependencies Introduction 
among the DEs should be mapped as close as pos­ sible to the interconnection structure of the PE ar- 
The processor array or cellular array machine has ray in order to optimize the communication among become 
a popular parallel processing architecture in the PEs. This computation paradigm does not ef­data intensive 
applications such as image processing fectively utilize the higher degree of parallelism of a and scientific 
modeling. A processor array typically PE array larger than the size of the data structure. consists of 
a regular lattice interconnection of a large Thus, a large processor array does not solve a prob­ numb 
er of processing elements ( PEs) in two or more lem faster than one that is only as large as the data 
dimensions, in which each PE has access to its own structure. Not all problems solved on these machines 
block of local memory [4]. The PEs are ordinarily of involve large data structures even today. Due to 
the popularity of divide-and-conquer and other techniques*The support of the Defense Advanced Research 
Projects that decompose a problem into simpler subproblems,Agency (ARPA Order No. 6350) and the U. S. 
Army Engineer Topographic Laboratories under Contract DACA76-S8-C-0008 many of the problems solved on 
these machines in­are gratefully acknowledged. volve, at some stage, relatively small data structures 
764 @ 1991 ACM 0-89791-459-7/91/0764 $01.50 that are extracted from larger ones. In image process­ing, 
for example, techniques such as multiresolution image analysis and focus-of-attention image analysis 
reduce the processing of a large image to analyzing a few 16x 16 or 32x 32 subimages in greater detail. 
Im­ages of that size fill only a fraction of the processing elements of a 16K PE MasPar or a 641< PE 
Connection Machine. As larger processor array machines become available in the future, we need to devise 
techniques to utilize the larger degree of parallelism present in them to speed up the processing of 
small data structures. We developed replicated data algorithms for efficient processing of small data 
structures on large processor arrays and demonstrated their speedup on some im­age processing operations 
[9]. They combine operation parallelism with data parallelism and employ the sur­plus PEs usefully in 
the computation by storing many copies of the data structure on the machine. The tech­nique of data replication 
attempts to transcend the SIMD limitations of the architecture by parametriz­ing the computation in a 
manner that allows different copies to work on different regions of the parameter space simultaneously 
without violating the architec­ tural constraints. The operation performed on the data structure is distributed 
among multiple copies of it stored in different parts of the processor array. Partial results from the 
copies are combined to gen­erate final results of the operation. Distributing the data structure to the 
copies and combining the par­tial results from them incur overheads that depend on the architectural 
parameters of the specific processor array machine. The speedup obtained by the applica­tion of the technique 
depends on how these overheads compare with the savings in computation time. In this paper, we present 
the analysis of the data replication technique on three popular interconnection topologies of processor 
array machines, namely, the 2-D mesh, the 3-D mesh, and the hypercube. The purpose of the analysis is 
to derive the conditions under which the technique of data replication achieves speedup and to analyze 
the speedup as a function of the problem pa­rameters and architect ural parameters. In the next section, 
we introduce replicated data al­gorithms and discuss some of their common features. The embedding scheme 
used in the analysis is also presented there. Section 3 presents a replicated data histogram algorithm 
and its analysis on the intercon­nection networks. Section 4 presents a replicated data con~olution algorithm 
and its analysis, We present some concluding remarks in Section 5. Some results of implementing the histogram 
and convolution algo­rithms are presented in Appendix A. 2 Replicated Data Algorithms The technique of 
data replication involves replicat­ing the data structure many times on the processor array, and decomposing 
the main task into subtasks that can be solved simultaneously by the individual copies. The partial results 
from the copies are com­bined to generate the overall solution to the problem, The technique makes use 
of data parallelism within each copy of the data structure, and operation paral­lelism across the copies. 
Some of the basic issues that concern data parallelism and operation parallelism are fundamental to the 
technique of data replication also. For instance, the mapping should meet the following criteria: Data 
elements that are adj scent in the data structure should be mapped onto PEs that are prox­imate in the 
interconnection network in each copy of the data structure. Additionally, the PEs that repre­sent the 
same DE in different copies of the data struc­ture should have an efficient interconnectivity struc­ture 
among them for fast intercopy operations. Given an embedding of the data structures onto the processor 
array, data should be distributed to all the copies quickly. After the copies complete their individual 
computations, the partial results should be collected from them efficiently. Distribution and collection 
involve communication across the copies of the data structure and can be speeded up if the em­bedding 
provides an efficient interconnection mecha­nism among the corresponding DEs of different copies. These 
steps add overheads to the savings in computa­tion time, which should be kept as low as possible to achieve 
high speedups. The most important issue in a replicated data algo­rithm is the decomposition of the operation 
into parts that can be solved simultaneously on a different copy of the data structure, without violating 
the architec­tural constraints. Replicated data algorithms have to partially overcome the SIMD nature 
of the machines in order to achieve this. This is done by parametriz­ing the computations within each 
copy. Each COPY uses its copy number the only distinguishing factor when the data structure is replicated 
 to index the parameter space to extract the appropriate set for the computation it is supposed to perform. 
The limited independence the architectural model provides each PE with such as the independence in selecting 
the neighbor with communicate to or the address to fetch the operands from needs to be exploited to 
address different parts of the parameter space independently. The speedup gained by a particular replicated 
data algorithm depends on how the savings in the computa­tion time compare with the overheads of distribution 
765 and collection. The overheads depend on the inter­connection topology and some architectural parame­ters 
of the machine. Thus, the speedup of a replicated data algorithm using the same decomposition strat­egy 
will be different on different machines. We have developed replicated data algorithms for a few fun­damental 
problems in image processing. The details of their analysis and results of implementing them on the hypercube 
connected Connection Machine can be found in [9]. In this paper, we analyze the image his­ togram algorithm 
and image convolution algorithm on three popular interconnection topologies, namely, the 2-D mesh, the 
3-D mesh and the hypercube. Analysis of a replicated data median filtering algorithm can be found in 
[10]. We first discuss how multiple copies of 2-D image arrays are embedded on these interconnec­ tion 
networks. 2.1 The Embedding We use a simple scheme to embed the two­ dimensional pixel array representation 
of the image on the three interconnection topologies. The scheme used is the same for both algorithms, 
but different on different interconnection networks. For the sake of our discussion, the 2-D mesh is 
assumed to be square in its dimensions. The 3-D mesh is assumed to consist of a number of parallel planes, 
each of which is square in size. The hypercube is assumed to have d dimensions and 2d PEs. The image 
is assumed to be of size n x n. Each copy of the two dimensional image maps nat­ urally to the 2-D mesh 
structure. The k copies are ar­ ranged as a two dimensional grid also, with G copies to a side. Thus, 
to store k copies of the image, the mesh should be at least &#38; n x &#38; n in size. Under this embedding 
scheme, a scan or parallel prefix op­ eration can be performed simultaneously within each copy of the 
image in 2n ste~fi -A scan operation across the copies of the image needs 2 W n steps for comple­ tion, 
as the diameter of a &#38;n x &#38;n mesh is 2 &#38;n. Each copy of the image is mapped to one of the 
 parallel planes of the 3-D mesh. To embed k copies of the image, the 3-D mesh needs to be at least lc 
x n x n in size. A man operation within a copy of the image can be done in 2n steps, and across the k 
PEs corresponding to each pixel in different copies in k steps, under this embedding on a 3-D mesh. Two 
dimensional images can be mapped onto the hypercube using gray codes [3], preserving all the ad­jacency 
relations of the pixel grid. For simplicity, we assume that image size is a power of two, i.e., n = 21. 
Each copy of the image then maps to a 2j­dimensional subhypercube. To embed k copies of the image, the 
hypercube should be of dimension log(knz) or greater. By the topological properties of the hy­percube, 
k copies of the image can be mapped onto it such that the k PEs that represent each pixel on different 
copies of the image form a log k-dimensional hypercube. Therefore, a scan operation can be per­formed 
simultaneously across the pixels of each copy of the image in 2 log n steps, and across the k PEs corresponding 
to each pixel in different copies in log k steps. In the next section, we analyze the image histogram 
 algorithm on the three interconnection networks. The corresponding analysis of the image convolution 
oper­ ation is presented in Section 4. 3 Analysis of Histogramming The histogram of a digital image 
is a table that lists each possible gray level against the number of times it occurs in the image. The 
histogram of an image can provide information useful for segment ation, gray scale modification etc. 
[1 1], and is an important oper­at ion in image processing. For this discussion, we as­sume that the 
image consists of s-bit gray level values, and has 2S entries in its histogram. We present the replicated 
data algorithm to compute the histogram below. It uses k copies of the image, where k is the largest 
power of two that will fit into the processor ar­ray, subject to the condition 1 < k ~ 2S. The range 
of gray levels is distributed equally among the k copies of the image. The algorithm, its analysis and 
imple­mentation details are presented in greater detail in [9]. The histogram algorithm is a simple one, 
but its anal­ysis on the interconnection networks provide us with some useful insights. The Histogram 
Algorithm 1. Copy the image stored in copy O to other copies using a scan operation with copy. 2. Perform 
step 3, for j ranging from O through ~ 1: 3. Select every PE belonging to copy i that has gray level 
value g = i ~ + j. Count the number of these PEs using a segmented scan operation. This is the histogram 
value for that gray level for the entire image. Store this value in the node with coordinates ( [g/nj, 
g mod n) in copy i. 4. Coalesce the k local histograms and store the re­sult in a selected copy of the 
image, say copy using a scan operation with copy operator.  766 which We explore 1 and the mesh, time 
to perform a scan operation with copy across the the 3-D mesh and the hypercube. k copies of each pixel 
of the image. Step 3 is executed 2s /k times. Each execution takes time tjcanAdd,which Steps 1 and 4 
each take time t~cancopy,is the now equations 2 for 2-D 3.1 2-D Mesh is the time to perform a scan operation 
with add across all pixels of one copy of the image, Thus, the total As described in Section 2.1, our 
embedding on the time taken by the replicated data histogram algorithm 2-D mesh has the following parameters: 
(T,) is  2 ~ d=2n, 2JIn. ;canAd ;.anCOpy TV = ~ S.anAdd + z t;.a.copy The conventional single copy 
data parallel algo-The running time of the replicated data algorithm rithm to compute the histogram using 
counting pro-on a 2-D mesh can be given by T. = 2n ~ + 4 ~n ceeds in 28 steps, each step using a scan 
with add and that of the data parallel algorithm by T* = 2 +1 n. operation to compute the histogram for 
one gray level The condition given by Equation 1 can be written [8]. There are other ways of computing 
the histogram, aa such as sorting the image and performing a segmented scan to count, which are more 
eficient in specific cases on specific architectures. We compare our algorithms or with the conventional 
data parallel algorithm to illus­ (3)trate how operation parallelism can be combined with data parallelism 
in computing the histogram of an im-Since 1< k ~ 2 , equation 3 reduces to age. The total time to compute 
the histogram using one copy of the image (T$ ) is: S>2, T. = 2 t;CanA~d which is satisfied in all practical 
situations. Thus, dataWe can now compare the running times of the repli­replication technique always 
speeds up the computa­cated data algorithm with the data parallel algorithm. tion of histograms on 2-D 
mesh architectures. The technique of data replication will result in a faster Equation 2 for the speedup 
on a 2-D mesh becomes algorithm if T. <T, k or Sk = l+g~# k Scan Add + 2 !$.a.COpy _< 2s ;.anAdd To 
find the maximum speedup Sm.= with respect which reduces to to the number of copies, differentiate sk 
with respect ~ zs-1~ I to k and set it to zero. This yields (1) !$.anCopy _ Scan Add kCondition 1 sets 
a limit on the parameter t~.a~copv, s -, whenk=2$a. mac = a measure of the overhead in copying the 
data and 3 the results, in terms of tjcanAdd,a measure of the Thus, the replicated data histogram algorithm 
iscomplexity of one computation. faster than the data parallel algorithm on a processorThe corresponding 
speedup of the replicated data array with a 2-dimensional mesh interconnection pat­algorithm is given 
by tern, as long aa the number of gray levels is greater Sk=g than 4. For s-bit gray level images, the 
speedup using r k copies of the data is sublinear by a factor of 1 + ~. which reduces to The maximum 
achievable speedup is ~, when 22 f3 2s ;canAdd copies of the image are used, each copy computing the 
Sk= ~tl complete histogram for 2 /3 gray levels. If the num­ k ScanAdd f 2 $canC opy ber of copies 
is increased beyond that figure, the extrak (2) overhead in distributing the data results in reduced 
l++t+~ speedups. Scan Add  3.2 3-D Mesh Assume that the 3-D mesh is at least k x n x n in size. For 
our embedding scheme, we have d = 2n, =k ;canAd ;canCOpy The running time of the replicated data algorithm 
ona3-Dmeshcanbegiven byTr=2n$+2kand that of the single copy algorithm by T. = 2 +1 n. The speedup condition 
1 for speedup on a 3-D mesh becomes k 1 k~2 (4) kn We also have 1 < k ~ 28, as the number of copies 
cannot exceed the number of gray levels in this scheme. Thus, equation 4 is always satisfied, and the 
histogram computation is always speeded up by our technique on the 3-D mesh. The speedup on a 3-D mesh 
is given by Differentiating Sk with respect to k and setting it to zero, we find the maximum speedup 
on a 3-D mesh to be k s when k= m. max = , 2 We also have 1< k < 2s, which implies that the above maximum 
is possible only if n < 2S. If n is greater, the maximum speedup occurs when 2 copies of the data are 
used. The speedup in that case is 2 s if n>2s< ?72 ..= ~+;, Summarizing, the replicated data histogram 
algo­ rithm is always faster than the single copy version on a 3-D mesh architecture. The speedup attained 
is sub­linear by a factor 1 + &#38;, when k copies of the im­age are used. The maximum achievable speedup 
is q , using = copies of the image, if n <2 and 2s/(1 + ~), using 2s copies of the image, if n > 2s. 
 3.3 Hypercube Our embedding scheme has the following parame­ters for the hypercube: = lognz, = log 
k ;canAdd $canCOpy The running time of the replicated data algorithm on ahypercube can be given by Tv 
= 2log n $ + which is always satisfied, since 1 < k ~ 2s. Thus, the replication technique always has 
computational advantages on hypercube-connected processor arrays. 2 log k and that of the data parallel 
algorithm by T, = 2 +1 log n. The Equation 1 for speedup on a hypercube can be written as k 1 logk < 
2s ~ logn, (5) The speedup on a hypercube is given by k  k= l+*gg-(6) Differentiating sk with respect 
to k and setting it to zero, we see that the maximum speedup on the hypercube occurs when k = 2s log 
n. But, since k ~ 2s, the maximum speedup is when k = 2 and is given by 2S Iogn 2s logn s when k= 28 
 = logn +s = log(2S n) In summary, the data replication technique always speeds up the computation of 
histograms on hyper­cube connected processor arrays. The speedup at­tained by data replication using 
k copies of the data is sublinear by a factor of 1 + ~ ~. The maximum speedup occurs when the numb2&#38;1%gfncopies 
equals the number of gray levels and is sublinear only by a factor of @##l. The single copy and replicated 
data histogram algo­rithms were implemented on the hypercube-connected Connection Machine [5]. Table 
1 in Appendix A com­pares their running times. The theoretical speedup given by Equation 6 and the experimental 
speedup are plotted against the number of copies in Figure 1. 4 Analysis of Convolution Digital image 
convolution by a 2-dimensional ker­nel of weights is used in a variety of operations in image processing, 
such as smoothing, edge detection, matching etc [1 1]. Digital image convolution by a kxk kernel K, for 
an odd number k, is defined as follows. Assume the indices of the kernel range from [~] to [~]. Each 
pixel P(u, v) of the image is mapped to a convolved value C(u, v) as given below: k/2 k/2 C(IJ, v)= ~ 
~ P(u +i, v+ j)K(i, j), i=-kf2j=-kf2 where P is the image array, and K the kernel array. 768 Figure 
1: Experimental and theoretical speedups of the histogram algorithm on hypercubes Speedup 85.00-1 13xperlmental 
.... . .. ................... %heorettcal 80.00- /­ 75.00 - 70.00 65.00 60.00 55.00 50.00 45.00 40.00 
35.00 30.00 25.00 20.00 15.00­ 10.00 5.00 - i 0.00 -, 0.00 I 50.00 I 100.00 .. K The replicated data 
algorithm to compute the con­volution of an n x n image by a k x k kernel of weights is presented below. 
The algorithm uses kz copies of the image. These copies can be visualized as form­ing a 2-dimensional 
square array, whose indices range from k/2 to k/2. The kernel weights are distributed one per copy, such 
that the copy (i, j) gets the value K(i,j), k/2 ~ i < k/2 and k/2 < j < k/2. Each copy of the image is 
shifted by a variable amount, such that all the pixel values that contribute to the convolved result 
of a pixel position (i, j) are lined up at the position (i, j) across the copies. All multiplica­tions 
are performed simultaneously, followed by a sum reduction across the copies of each pixel of the image 
to yields generate the resuIt. The Convolution Algorithm 1. Copy the image stored in copy O to all copies 
using a scan operation with copy operator. 2. Broadcast the kernel weights to the copies in k2 steps 
such that all PEs of the copy (i, j) gets the kernel weight I<(i, j).  3. The PE allocated to pixel 
(u, v) of copy (i, j) of the image performs the following computation: Obtain the pixel value from PE 
(u + i, v + j) within the same copy (assume that the image is circularly connected) and store it in P 
as its pixel value. Multiply this value by the kernel weight allocated to the copy, i.e., K(i, j), and 
store this result in C. 4. Sum the C values using a scan operation with add as the operator across the 
copies of each pixel of the image and store the result in a designated copy, say copy (O, O).  Step 
1 takes t~~ancopytime to execute. Step 3 takes t~ time to perform one multiplication, and t: for the 
communication, where t: is the time to perform a gen­eral communication operation on the architecture 
to shift the image in each copy. Step 2 takes k2 tB time, where tE is the time to broadcast a value from 
the front end to n2 PEs of the processor array. Thus, the total time to compute the convolution using 
the tech­nique of data replication ( Tr ) is T, = trn + t; +t;&#38;CoPy + t;~anAdd + k2 tB The data 
parallel convolution algorithm proceeds in k2 steps. Each step involves a broadcast, a multi­plication, 
a communication and an addition [7]. The total time to compute the convolution using one copy of the 
image (T$ ) is T$ = k2(tm +ta +t~ +tB), where tais the time for one addition operation and t: the time 
to perform a near neighbor communication operation. The data replication technique speeds up convolu­tion 
if T. ~T, or ~ + ! + ;~anCopy + ;;.nAdd + k2 tB ~ k2(tm +ta +t~ +tB) which reduces approximately to 
~ + !7ZanC0py + ::anAdd < k2 (% + t. + t:) (7) The condition in equation 7 puts a limit on the long distance 
and general communication parameters in terms of the more basic parameters, such w time to add, multiply, 
and perform a near-neighbor com­munication operation on the architecture in question. If the long distance 
communication is fast, the data replication technique will show gains. The speedup of the replicated 
data algorithm is given by (ignoring the broadcast time parameter tB) IP S=$= (8) r T,/(tm + t= + t~) 
We now explore the above conditions on the three interconnection structures. 4.1 2-D Mesh No pixel moves 
more than l;] positions in any di­rection in step 3 of the convolution algorithm. Hence, the shifting 
can be achieved in time 2 k t: on a2-D mesh, using [~] mesh communications in each of the four directions. 
The relevant parameters for our em­bedding scheme on a 2-D mesh are t: = 2kt~ , t~~=nCOPY = 2k ~ t; , 
and d=2knt~+2kta !!3;anAd Substituting these, we see that the data parallel convolution algorithm on 
the 2-D mesh runs in time T, = k2 (tm +ta + t;). The replicated data convolution algorithm runs in time 
T. = tm +2kta + 2k (2n+ 1) t!. Equation 7 becomes t. < kt~+(k 2)ta ~ (tm + t.) (9) c 4n k+2 = 4n Since 
k is usually much smaller than 4n, equation 9 gives a condition on the near neighbor communication time 
on the mesh that is rarely satisfied practically, unless multiplication is a very slow operation, and 
near-neighbor communication is very fast. Hence, it is unlikely that the replicated data algorithm to 
com­ pute image convolution outperforms the single copy algorithm on most 2-D mesh architectures. The 
speedup given by the equation 8 on the 2-D mesh becomes /2 s = (tm+2k(2n +l)t:+2k nta)/(tm+ta+ t:) Which 
reduces approximately to s= ~tm+ta+t; 2 2nt; + t. We see that S <1 since t ~n~~+~~; is smaller than ~, 
unless near-neighbor communication is fast and mul­tiplication is very slow. Hence the replicated data 
al­gorithm is not likely to perform better than the single copy algorithm in computing image convolutions 
on 2-D mesh connected processor array architectures.  4.2 3-D Mesh Our embedding scheme has the following 
relevant parameters on the 3-D mesh: t~=2kt; , = k2 t;, and !7;anC0py tka SCanAdd = k2 (t; + ta) Therefore, 
the data parallel algorithm runs on the 3-D mesh in time T, = k2 (tm +t. +t~). The replicated data algorithm 
runs in time T, = tm + 2k(k + l)t~ + k2ta . From Equation 7, replication speeds up convolution on a 3-D 
mesh if t: < -L., (lo) k+2 i.e., if mesh communication is faster than multiplica­tion. This condition 
is satisfied by most 3-D mesh architectures and hence it is advantageous to use data replication on them. 
The speedup on a 3-D mesh is given by k2 s = (tm+2k(k +l)t; +k2ta)/(tm+ta+t~) which reduces to (k2 l)tm 
k(k+2)t; S=l+ k2ta+2k(k+l)t; For larger kvalues, k~ k+ 1w k+2,and the speedup can be given approximately 
by s = 1 + :~+ 2:\ a c The ratio -~n t.+z~g (~ ~), a measure of the time to multiply relative to the 
the time to add and com­ municate, is critical to the speedup. This parameter is greater than 1 for most 
practical 3-D meshes, and replicated data convolution algorithm achieves over­ all speedup on them. The 
speedup, however, is not very attractive aa it saturates at a value of 4 even if tm = 10 t: and at lower 
values if multiplication time compares more favorably with near-neighbor commu­ nication time. An interesting 
observation about 3-D meshes is that the speedup is relatively independent of the number of copies or 
any of the problem param­eters for large kernel sizes, and depends only on the specific architectural 
parameters. 770 4.3 Hypercube The relevant parameters for our embedding scheme on a hypercube are t:= 
t:, = 2t~ logk, and Kane.py t::anAdd = 2(tB + t.) 1%~, where tg is the time to perform a general hypercube 
communication. The single copy algorithm on a hypercube runs in time T, = kz(t~ + ta + t;) (hypercubes 
cannot do better than 2-D meshes for a local operation such as convolution in the worst case). The replicated 
data algorithm runs in time T, = t~ + t ~ + (4t~ + 2t a) log k. In the worst case, t: = 2kt~ (using near-neighbor 
communications alone for shifting the image) and the worst case running time is Tr = tm + 2(k + 2 log 
k)t~ + 210gkta. The condition for speedup on the hypercube can be given by th< Ptm+(P 210gk) t. + (kz 
 410gk)t: c This condition sets an upper bound on the hyper­cube communication parameter in terms of 
the com­putation and local communication parameters. The replicated data convolution algorithm will ordinarily 
outperform the single copy version on hypercube ar­chitectures, as the condition is satisfied by most 
of them. The speedup on a hypercube is given by s=(tg+tm +4t:logk +Logk)/(tm+t=+ t:) which reduces to 
F s= (11) 2 log k t:ftu t:+ %+:l+t: The speedup is greater than 1 in most cases (unless thc = O(k2tm)), 
and is a function of the kernel size, and the communication and computation parameters of the hypercube 
machine. The speedup is maximized when the general hypercube communication time t!is minimized. Table 
2 in Appendix A compares the running time of the two convolution algorithms on the hypercube­connected 
Connection Machine. Figure 2 plots the ex­perimental speedup and the theoretical speedup given by Equation 
11 on the hypercube against the kernel size k for a 32x 32 image using 16K PEs. Speedup 17.00 -16.00 
- I I I I Theoretical TZx32Enage . . . ........ . --­-­-----­64x64~mage 15.00 - 14.00 - 13.00 - 1200 
11.00 10.00 9.00 S.oo 7.00 6.00 5.00 4.00 3.00 ­ 200 - 0.00 - IIII ~ O. Sizek 5.00 10.00 15.00 . Figure 
2: Experimental and theoretical speedups of convolution on hypercubes 5 Conclusions In this paper, we 
analyzed replicated data algo­rithms to compute image histograms and image con­volutions on three popular 
interconnection networks. Replicated data algorithms attempt to speed up com­putations on large processor 
array architectures by decomposing the problem among multiple copies of the data structure. Replicated 
data algorithms com­bine operation parallelism and data parallelism. The speedup gained by it depends 
on how the savings in computation time compares with the overheads in copying the data and combining 
the results. Not sur­prisingly, our analysis shows that the technique attains better speedup on architectures 
with global intercon­nectivity such aa hypercubes, as the communication overhead among the copies is 
lower on them. Our anal ysis, however, shows that even a restrictive model such as the 2-D mesh can speedup 
the computa­tion if the operations performed by the different copies are relatively independent, as is 
the case in histogram computation. The hypercube architecture speeds up the operations in all cases. 
Our analysis shows that the data replication technique can attain very good speedup on the 3-D mesh. 
This point merits atten­tion, because 3-D meshes with faster communication capabilities may be easier 
to build than the high wire cross-section networks like hypercubes as the number of processors increases. 
Inferior communication diam­ et er of 3-D meshes could be offset by its superior com­ munication speeds. 
 This paper studied the efficiency of the processor array architectures in performing similar computa­tions 
on copies of the same data structure stored in different parts of a large processor array. Blelloch studied 
how the same computation can be performed on different sets of data on a parallel vector model of computation 
in his thesis [1]. He laid out the criteria to be satisfied by a computation in order to process multiple 
data sets simultaneously in the replicating theorem. Blelloch and Sabot stressed the need to ex­ploit 
nested parallelism for speeding up the computa­tion [2]. These efforts aim at expanding the scope of 
data parallelism by taking it to lower levels of data representation. Replicated data algorithms operate 
in an orthogonal direction by combining operation par­allelism with data parallelism to gain further 
speedup when more processors are available. This effort con­stitutes an initial attempt at studying how 
variants of the same computation can be performed on multiple copies of the same data structure simultaneously. 
For the present, the analysis is both problem dependent and architecture dependent. Based on this work, 
we can develop a more general theory of parametrizing the operations in order to perform them simultane­ously 
on replicated data structures without violating the SIMD nature of the machine. This will, hopefully, 
lead to new computational paradigms on the data par­allel processor array machines, in which the number 
of processors available to a computation will be consid­ered a less serious limitation an assumption 
that the processor array machines of the future will vindicate. Acknowledgements: The author wishes to 
thank Prof. Larry S. Davis for his comments on the contents and form of this paper. Thanks are also due 
to the technical staff of the University of Maryland Institute for Advanced Computer Studies (UMIACS) 
for their support on the Connection Machine. References <RefA>[1] Guy E. Blelloch. Scan Primitives and Parallel 
Vector Models. PhD thesis, The Massachusetts Institute of Technology, November 1988. [2] Guy E. Blelloch 
and Gary W. Sabot. Compil­ing collection-oriented languages onto massively parallel computers. In Ronnie 
Mills, editor, The Second Sympowum on the Frontiers of Massively Para!lel Computation, pages 575-585, 
1988. [3] T. F. Chan and Y. Saad. Multigrid algorithms on the hypercube multiprocessor. IEEE Transac­tions 
on Computers, 35:969-977, 1986. [4] Terry Fountain. Processor Arrays: Architecture and Applications. 
Academic Press, London, 1987. [5] W. Daniel Hillis. The Connection Machine. MIT Press, Cambridge, MA, 
1985. [6] W. Daniel Hillis and Guy L. Steele Jr. Data par­allel algorithms. Communications of the ACM, 
29:1170-1183, 1986. [7] S. Y. Lee and J. K Aggarwal. Parallel 2-D convo­lution on a mesh connected array 
processor. IEEE Transactions on Pattern Analysis and Machine Intelligence, 9:590-594, 1987, [8] James 
J. Little, Guy E. Blelloch, and Todd A. Cass. Algorithmic techniques for computer vi­sion on a fine-grained 
parallel machine. IEEE Transactions on Pattern Analysis and Machine Intelligence, 11:244-256, 1989. [9] 
P. J. Narayanan and Larry S. Davis. Replicated Data Algorithms in Image Processing. Techni­cal Report 
CAR-TR-536/CS-TR-26 14, Center for Automation Research, University of Maryland, University of Maryland, 
College Park, Feb 1991. [10] P. J. Narayanan and Larry S. Davis. Replicated Image Algorithms and their 
Analyses on SIMD Machines. In Proceedings of the International Colloquium on Parallel Image Processing, 
pages 35 52, Paris, June 1991. [11] Azriel Rosenfeld and Avinash Kak. Digital Pzc­ ture Processing. Academic 
Press, New York, 1982. </RefA> 772 A Appendix: Experimental Results We present some results of implementing 
the his­togram and convolution algorithms on a Connection Machine in this appendix. For details of the 
imple­mentation, see [9]. A.1 Histogram Algorithm Timing of histogram alg (in seconds) for 8-bit grey 
level images Image No. of Repl data One copy size copies alg alg Using 16K processing elements 32X 32 
16 0.005 0.062 32X 32 32 0.002 0.062 32X 32 64 0.001 0.062 64X 64 4 0.024 0.087 64X 64 8 0.009 0.087 
32xii2 8 u 0.011 0.062 32x32 16 0.007 0.062 32x32 32 0.004 0.062 64X 64 2 0.046 0.086 64X 64 4 0.042 
0.086 64X 64 8 0.015 0.086 Table 1: Comparison of the histogram algorithms on the Connection Machine 
We implemented the single copy histogramming al­gorithm and the replicated data histogramming algo­rithm 
on the Connection Machine. Table 1 compares their performances. It may be noted that the best algorithm 
to compute histograms on the Connection Machine uses the capability of the machine to store the sum of 
the contending numbers in a concurrent write operation. Each PE writes a 1 to the node that holds the 
histogram value of its gray level value, with add as the combiner for the concurrent write operation. 
Though n2 PEs could attempt to write to the same location in the worst case, collision resolution done 
in hardware is fast. The comparison given in Table 1 is still relevant for a general hypercube architecture 
with no concurrent write capability. The virtual pro­cessor capability of the Connection Machine, in 
which each physical PE simulates multiple PEs, was used in the implement ation whenever the number of 
PEs re­quired was larger than what was physically available. The number of virtual PEs each physical 
PE is simu­lating is called the virtual processor ratio or VP ratio. The timings in the table have been 
adjusted taking VP ratio into account. A.2 Convolution Algorithm Timing of convolution (in seconds) 
for random image and kernel Image Kernel Repl data alg One copy Size Size Router I Mesh Alg I Using 
16K ~rocessin~ elements 32x~2 ] ix3 i.oo3 I 0.002 I 0.005 32x32 I 5x5 0.008 I 0.005 I 0.009 64X 64 3x3 
0.014 0.006 0.005 64X 64 5x5 0.023 0.013 0.024 64X 64 7x7 0.050 0.030 0.091 I 64x64 I 11x11 II 0.098 
I 0.076 I 0.367 64X 64 15X15 0.212 0.184 1.363 64X 64 21X21 0.400 0.480 5.355 Table 2: Comparison of 
the convolution algorithms on the Connection Machine Both convolution algorithms were implemented on 
the Connection Machine. Table 2 compares their per­formances. The third column of the table gives the 
time taken by the replicated data algorithm when the copies were shifted using the general router commu­nication 
instructions. The fourth column gives the timing of the replicated data algorithm when shifting was done 
using near-neighbor communications alone. In most cases, using near-neighbor communication is quicker 
on the Connection Machine. The last col­umn gives the timing of the single copy convolution algorithm. 
The VP ratio has been factored into the timings in the last column. The algorithms would be faster by 
the VP ratio factor, which]] is 128 for the last row, had the CM array been large enough. On the whole, 
the replicated data algorithm achieves im­ pressive speedup over the single copy version for image convolution 
operations. 
			
