
 An Undergraduate Compiler Laboratory by Frank Friedman Judith A. Stebulis Department of Computer 
and Information Sciences Temple University Philadelphia, Pennsylvania 19122 Abstract: A one semester, 
upper-division under- graduate course in compiler techniques is de- scribed. The course is based upon 
the material contained in Chapter 5 of the text Algorithms + Data Structures = Programs, by Niklaus Wirth. 
The goals of the course are (i) to introduce students to the fundamental concepts of the design and 
translation of higher level languages, and (2) to provide an introductory exposure to the related literature. 
A compiler-writing project of about nine weeks duration is required of all students. Simulators supporting 
generated code for static and dynamic run-time environments are provided for student use. INTRODUCTION 
 In this paper we describe a one semesters upper-division undergraduate course designed to introduce 
students to the fundamental concepts of language design and translation. The course is structured around 
the material in the fifth chapter, on Language Structures and Compilers, of the text Algorithms + Data 
Structures = Programs by Niklaus Wirth [15]. The course, Introduction to the Translation of Higher Level 
Languages, is offered once a year in the Temple University Computer and Information Sciences Department. 
Entering students are ex- pected to have taken a minimum of two courses in programming in a higher-level 
language, and one or two courses in basic systems programming. They are expected to have a working knowledge 
of such topics as hashing; stacks, lists, and tree structures, and associated algorithms; assembler and 
machine language programming; the assembly and loading processes; and the fundamentals of input and output 
processes. An introductory (four to eight week) exposure ~o PASCAL is also required. COURSE SUMMARY 
 The outline for the course is shown in Figure i; a time chart is illustrated in Figure 2. The introductory 
section on compiler structure and translation techniques (Section II) is kept brief; readings are assigned 
in Glass [5] and Gries [7] (selected portions of Chapters 1 -4). Introduction to the Translation of 
Higher Level Languages Course Outline I. Review: fundamental notions of translator/ system interfaces 
for a static run-time en- vironment. Loader tables, loading and link- ing; relocation. II. Introduction 
 A. The Structure of a Compiler B. An Overview of Translation Techniques III. Compiler Construction, 
Part A. Lexical Analysis (An Introduction to PL/CP): line reconstruction, conversion to internal form, 
"token" recognition; symbol table use IV. Language Specification and Structure A. Language Syntax: 
rules defining set of correct sentences; BNF notation B. Mathematical Definition of a Language: context 
free vs. context sensitive C. Sentence Recognition: parsing algorithms i. Top down parsing with one 
symbol 1ookahead without backtracking 2. Rules for identifying undesirable language features D. Syntax 
Diagrams i. Diagram construction 2. Parser construction from diagram representation III. Compiler 
Construction (continued), Parts B, C, D.  B. Syntax Analysis: construction of the translator from the 
syntax diagrams; dependency diagrams C. Error Analysis and Recovery: Diagnostic Generation D. Code 
Generation i. For a static run-time environment (no recursion) 2. For a limited dynamic run-time en- 
vironment  V. (As time permits) An analysis of code genera- tion and compiler/system interfaces for 
an existing compiler and computer system.  Figure i: The Compiler Laboratory Course Outline 28 Code: 
_ _ lecture and/or reading ..... project coding and testing circled numbers indicate approximate number 
of lecture hours Section v error ~_ syntax od~_ analysis 't lexl~-al III I ana 1 ys ~s ~® >, -I IV 
II @l : Wo.,k 0 l 2 a 4 f, 7 tt 9 10 1 1 12 13 14 (3 ho.r~ of lecturo per week) Figure 2: Time Chart 
for the Figure 1 Course Outline Following this introduction, the "laboratory" aspect of the course is 
initiated (Section III, Part A); class lectures focus upon the lexical analysis phase of a compiler. 
Students are given the description of the syntax and semantics of the language PL/CP, a slightly modified 
version of Wirth's PL/0. (The syntax diagrams for PL/CP are shown in Appendix A of this paper; the diagrams 
for PL/0 are shown in Wirth, Chapter 5, Section 7). Algorithms for lexical analysis are examined. Line 
reconstruction, token recognition, and the use of the symbol table are discussed; the algo- rithms for 
PL/0 (Section 5.8 in Wirth) are studied and then modified so as to accommodate PL/CP. Most of the details 
of modification are left to the students, who are encouraged to begin immedi- ately writing and testing 
the algorithms required for the lexical analysis phase of the PL/CP com- piler. Once the project has 
been initiated, lectures on language definition and structure (Section IV) are begun. The material in 
Sections 5.1 through 5.4 of the Wirth text is followed closely. A mathematical formalism for language 
definition is developed that is sufficient to provide the stu- dent with a satisfactory understanding 
of the parsing algorithms to be introduced later. Two criteria for the definition of a language L of 
reasonable complexity are established:* i. Given the production A ÷ §ii§21'''I§ n (A e N, §i 6 (NUT)* 
for all i) the sets of initial symbols of all sentences that can be generated from the §.'s must be 
disjoint. l 2. For every symbol A 6 N which generates the empty sequence, the set of its initial symbols 
must be disjoint from the set of symbols that may follow any sequence generated from A. The list of 
initial symbols for a non-terminal is the set of symbols that is legal as the first symbol of the construct; 
the list of follow sym- bols is the set of symbols that may legally occur following the construct. The 
effect of these criteria upon compiler complexity is illustrated by example and demon- strated using 
the mathematical formalism developed in the text. It is shown how backtracking and look-ahead of more 
than one symbol can be avoided in the compiler for any language that satisfies these criteria. Syntax 
diagrams are also introduced in Section IV, and the entire syntax of PL/CP is described in terms of 9 
diagrams (see Appendix A) each corres- ponding to a non-terminal construct in the lan- guage. The construction 
of a language parser in terms of a collection of subparsers that are sys- tematically developed from 
the language syntax diagrams is described. Emphasis is placed upon the relationship between the structure 
and com- plexity of the language and that of the parser. The virtually automatic translation of each 
dif- ferent component of a syntax diagram into a PASCAL algorithm for recognizing that component is illus- 
trated. Goal-oriented parsing and the relation- ship between goals and subparsers is discussed. Once 
the fundamentals of parser design have been covered, attention is once again focused upon construction 
of the compiler. The syntax of PL/CP is analyzed. PL/CP is a PASCAL-like language which allows the use 
of integer type data only. Integer constants, variables, and single-dimension arrays are permitted; named 
constants (symbolic names ~sed to represent integer values) are allowed, and string constants are permitted 
for the annotation of printed output. Local procedures with argu- ments (passed by value-return) and 
the convention- al control statements for selection and repetition L = L(T,N,P,S) where T = the vocabulary 
of terminal symbols N = the set of non-terminal symbols P = the set of productions and S is the start 
symbol (S ~ N)  are included in the language. Assignment, goto, and halt statements, and simple read 
and write operations round out the major features of PL/CP. One to three digit statement labels are permitted, 
but not at the beginning of a block, the body of a loop, or a then or else alternative. If desired 
for simplicity, the for, halt, and goto statements (and statement labels), and the use of arguments 
 may easily be eliminated from PL/CP with a minimal pedagogic loss. The syntax of PL/CP satisfies the 
two criteria needed to ensure that a parser using one symbol look-ahead and no backtracking can be built 
for the language. Several statements, notably read and write, and some of the footnoted restrictions, 
such as for array sizes and label sizes, appear as they are due to the nature of the static or dynamic 
run-time environments in which translated PL/CP programs must execute. These environments are des- cribed 
later in the paper. Students are asked to study carefully the PL/CP language as described in Appendix 
A, and to read Sections 5.6, 5.7, and 5.8 in Wirth, concen- trating on the material related to the use 
of syn- tax diagrams for translator construction (Section III B of the outline). They are then asked 
to produce a BNF for PL/CP which satisfies the two previously-given criteria, to write a PL/CP pro- gram 
and to produce a PL/CP dependence diagram, along with the lists of initial and follow symbols for each 
of the nine non-terminal symbols in the language. The program should require the use of most of the features 
of PL/CP; it is used later as one of the compiler test programs. The dependence diagram illustrates 
the rela- tionships of the syntax graphs of each non-terminal symbol. The dependence diagram for PL/CP 
is shown in Figure 3. For each syntax graph G, the diagram indicates all graphs in terms of which G 
is de~ fined. The diagram correspondingly mirrors the entire structure of the PL/CP compiler; if each 
syntax graph is represented by a procedure in the parser, then the dependence diagram also shows which 
procedures will be called by others, and indicates the hierarchical structure of the par- ser. The 
loops in the dependence diagram indicate recursion. It is strongly recommended that the PL/CP compiler 
be implemented in a language that permits recursion. At Temple, we use a slightly modified version of 
the University of Minnesota PASCAL running on a CYBER 174 computer. 7~t 7~-L,,~-I  / i_) / /[-_<o,:-II 
I Figure 3: Dependence Diagram for PL/CP A table listing the initial and follow sym- bols for each 
non-terminal symbol of PL/CP is shown in Figure 4. The initial and follow symbols are used to show that 
PL/CP satisfies the criteria for avoiding backtracking and look-ahead of more than one symbol. They are 
also used extensively in the error-recovery analysis performed by the compiler. This analysis is described 
in a later section. no,1~termi,la! initial symbols follow symbols Prooram ]~bol const vat end procedure- 
begl1~ Dlock lah~l to.st var end endproc procedure begin Slntoment ident 9oto halt end endproc call 
read write endlf endwhlle~=E If while foe endfor else J Line Control page tab line , ) Condition + -tdPnt 
number then do ( ~xpr~s~Jon ~ -Ident number E ; to down to ( do RI then , ) ] To, Fro hl~nt number ( 
E ; + -to downto do R l then, ) ] Factor [dent number ( E ; dlv rood + -to downto do R~ then , ) ] Designator 
|dent E ; dlv mod ÷ -to downto do Rt then , ) ] tR is the set of relational operators  Figure 4: Initial 
and Follow Symbols for PL/CP While the students are familiarizing themselves with PL/CP, BNF, dependence 
diagrams, and initial and follow symbols, class lectures focus upon the syntax and error analysis phases 
of the PL/CP com- piler. Again, the Wirth text is followed closely, but the code is extended to handle 
the analysis for PL/CP rather than Wirth's PL/0. Some of the expanded algorithms are presented in class 
for illustrative purposes. The remaining extensions are assigned to the students as part of the labor- 
atory project. Once this phase of the project is underway, lectures and reading on code generation are 
begun. In the remainder of the paper we pre- sent a discussion of those aspects of error analy- sis 
and code generation that are stressed in the course.* ERROR RECOVERY While powerful techniques for 
error detection and correction are directly applicable to languages such as PL/CP [12], these techniques, 
especially as they pertain to error correction, are beyond the scope of this course. Some rather simple 
correc~ tions specific to PL/CP are discussed and imple- mented. However, emphasis is placed upon the 
de- tection and marking of program errors, and the generation of accurate and meaningful diagnostic messages 
to aid in debugging. Above all, it is stressed that no input sequence should be allowed to cause the 
compiler to terminate abnormally. Regardless of the number or severity of errors, the entire program 
must be parsed, and exit from the compiler must be through its normal termination point. Special attention 
is given to the discussion of error recovery and correction with regard to declarations. The goal is 
to prevent the propaga- tion of errors resulting from undeclared identi- fiers by processing as much 
of a declaration state- ment as possible, even when the statement contains errors. Simple text errors 
(such as the misuse or ommission of commas or semicolons) are corrected. Duplicate declarations or declarations 
occurring out of order should be processed if at all possi- ble, after appropriate diagnostics are issued. 
 With regard to the parsing of executable statements, virtually all discussion focuses on issues of detection 
and diagnostic generation. The efficient, automatic correction of errors in- volving poorly constructed 
statements or improper- ly nested structures, etc., with minimal loss of input string information requires 
the use of more advanced techniques that are not discussed in the course. Recovery requires that the 
parser scan the input string starting at the point of an error * Should time permit, additional lectures 
in the course may be devoted to an analysis of the code generation for an existing production compiler 
on your system, or to a discussion of selected papers (such as Backus et al [I], Graham [6], Bauer and 
Samuelson [ 2], Rosen et al[ 14], or Kanner et al [ii], or portions of text material (such as Chapters 
5, 6, ii, 12, 17, and 18 from Gries ~7]). At Temple, we have devoted remaining lecture time to an analysis 
of the code generated by the Univer- sity of Minnesota FORTRAN Compiler (MNF), for the CDC Cyber 114 
computer. until it recognizes a symbol with which parsing can resume. It is important that the portion 
of the input string skipped over by the parser during this scan be kept at a minimum. The ability to 
recognize the start of a legal language construct for which parsing may resume requires the exis- tence 
of key words and/or special characters in the source language. PL/CP uses both. The sets of legal initial 
and follow symbols are defined for each non-terminal symbol (see Figure 4). These symbols are used to 
aid in recovery follow- ing the detection of an error. At the point of detection, the input string is 
scanned either for a legal initial symbol or a legal follow symbol, depending upon the nature of the 
error encountered. The occurrence of such a symbol marks the point at which parsing may be resumed. 
CODE GENERATION Introduction Code generation may be carried out for either a static or a dynamic run-time 
environment. Since there is apparently little difference in the time required to work in one environment 
versus the other, the choice primarily depends upon the de- sired focus of the course, as defined by 
the in- structor. Software in the form of simulators for hypo- thetical computers is provided to support 
the code generation study for both environments. For the static environment analysis, a simulator for 
the FACET Computer (FAcility for Computer Education and Training)[3] is used. This simulator was written 
in FORTRAN and is highly por~able~ having been implemented on a number of different com- puters over 
the past several years. Another simulator, written in PASCAL, is used to support the code generation 
study for a dynamic run-time environment. This program currently runs on a CDC Cyber 174 computer and 
simulates a stack- architecture machine that is an extension of the PL/0 machine described in Wirth. 
 The simulators provide a facility that enables students to study the fundamental aspects of assembly 
language code generation divorced from the peculiarities of the machine code for a particular computeT. 
Both simulated machines and their instruction sets are conceptually simple in structure and easily understood. 
 In the following two sections, we will describe the basic features of the FACET and stack machines, 
and indicate for each machine the four major issues of code generation that should be consid- ered in 
class lecture and discussion. These in- clude memory allocation, control structure proc- essing, expression 
handling, and procedure calls. Because the handling of control structures and ex- pressions is similar 
in both environments, these topics have been omitted from the stack machine discussion. 31  The Static 
Environment--The FACET Computer FOR iv:= exPl TO exP2 DO The FACET computer is a hypothetical decimal 
__} s machine with a memory consisting of i000 identical cells. Each cell contains a sign (+ or -) and 
 five decimal digits. All instructions are single- address, and instructions and data may be stored 
 anywhere in memory. The FACET computer has four special purpose registers: an accumulator, an overflow 
indicator, an index register, and a pro- gram counter. All arithmetic operations must be carried out 
in the accumulator which, like the memory cells, contains a sign and five decimal digits. Both the index 
register (used for addres- sing) and the program counter (which contains the address of the next instruction 
to be executed) hold three decimal digits, sufficient for repre- senting the address of any cell in 
memory. The overflow indicator is affected only by addition and subtraction operations. The FACET instruction 
set includes the standard arithmetic operators for both floating-point and fixed-point arithmetic. Instructions 
are provided for data movement between memory and the accumu- lator, between memory and the index register, 
and between the accumulator and the index register. Transfer instructions include an unconditional jump, 
a subroutine jump, and conditional jumps based upon the contents of the accumulator, index register, 
and overflow indicator. Input and output facilities include the ability to read and write fixed or floating 
point numeric values and charac- ter strings. Only one operand may be transmitted in a single operation. 
Some output formatting capability is also provided. Additional features include instructions for index 
register modification, and for shifting and extracting digits in the accumulator. I. Memory Management 
 In the static environment, FACET memory is permanently allocated at compile time for both code and 
data items. Instructions are placed in- to memory beginning at address 21 and working to- ward the high 
end of memory (address 999). Mem- ory for data items is allocated beginning at address 999 and continuing 
toward the low end of memory. Locations i through 20 are reserved for the run-time stack. Because there 
are no FACET instructions with literal operands, named con- stants (declared via const) and in-line constants 
must be placed in memory and entered into the sym- bol table. 2. Control Structures  The main focus 
of the discussion of code gen- eration for control structures is the handling of transfers and generated 
labels. The use of the accumulator in evaluating decision structure and loop repeat conditions is also 
described. As an example, we consider the translation of the FOR loop shown in Figure 5. ENDFOR 1 code 
to evaluate exPl (result in accumu- lator) 2 store accumulator in iv 3 code to evaluate exP2 (result 
in accumu- lator) 4 subtract iv from accumulator 5 add 1 to accumulator 6 store accumulator in LPRCOi 
(loop repeti- tion counter for ith nested loop) 7 GLi: if accumulator N 0 jump to GL2 8 --r~ S (body 
of loop) J 9 load value of iv into accumulator i0 add 1 to accumulator ii store accumulator in iv 12 
load LPRCOi into accumulator 13 subtract 1 from accumulator 14 store accumulator in LPRCOi 15 jump to 
GLi 16 GL2: Fisure 5: Translation of the FOR Loop  Code must be generated to calculate initial values 
for the loop variable and the loop repeti- tion counter (lines 1 through 6 in Figure 5). The value of 
the loop repetition counter is saved in a temporary variable, LPRCOi. Following the body of the loop, 
code is needed to modify both of these variables. The loop variable is incremented by one and the loop 
repetition counter is decre- mented by one (lines 9 through 13). The value of the loop repetition counter 
is left in the accumu- lator for the test of the repeat condition on line 7. Two transfer instructions, 
a forward reference and a backward reference, are required to estab- lish the appropriate control flow 
for a FOR loop. (Two transfers are also required for the while loop and the double alternative decision 
struc- ture. Only one transfer instruction is needed to translate the single alternative decision struc- 
ture.) In order to determine the target addresses for these instructions and insert the addresses into 
the instructions, generated labels (GLi and GL2) corresponding to each address are entered into the symbol 
table. The symbol table index for each label is saved in a temporary location, and is used to access 
the symbol table entry as re- quired during code generation. Since the syntax of PL/CP permits the use 
of nested control structures, a stack is required to save the symbol table indices of the generated variables 
and the loop repetition counter. How- ever, if the PL/CP compiler is implemented in a recursive language 
as is suggested, the generated- label stack will be handled automatically by the compiler run-time system. 
 3. Procedures  PL/CP procedures are implemented as FACET sub- routines. Control is transferred to a 
subroutine via the JSUB instruction which causes the address of the next sequential memory cell to be 
loaded into the index register. Arguments are made available to a FACET sub- routine through the use 
of an argument list imme- diately following the subroutine call statement. Except for arrays, the value 
of each argument is 'placed in this list prior to the subroutine call. In the case of array arguments, 
the base address of the array is placed in the corresponding argu- ment list element. In PL/CP, procedures 
may return values to cal- ling routines through the arguments. Therefore, code must be generated following 
the argument list to restore the values of all variables used as arguments in a procedure call. The 
FACET subroutine calling sequence is illus- trated in Figure 6. i LDAC X load the value of the argument 
2 STAC Ai into the argument list 3 JSUB SUBR 4 Ai LOC argument list (single element) 5 LDAC Ai restore 
the value of the 6 STAC X argument Fisure 6: FACET Code Generated for CALL SUBR (X) Within the subroutine, 
references to arguments are made using indexed instructions with operands that specify the offset of 
the argument from the first memory cell following the JSUB instruction. The same mechanism is applied 
for the jump instruction that returns control to the calling program. Since the index register is also 
used for array access, code must be generated to re- load the index register prior to each argument reference 
and the return transfer instruction. 4. Expressions  The handling of expressions requires the use of 
a compile-time stack, and the simulation of a run-time stack as it is used to save intermedi- ate results 
and subscript values. The compile- time stack is used to store the address of each operand that is encountered 
in the scan of an expression, and to keep track of the contents of the run-time stack. For each array 
reference, two compile-time stack entries, one for the base address of the array and the other for the 
run- time stack entry for the subscript value, are re- quired. Flags are used to distinguish subscript 
entries from the addresses of variables, arrays, and intermediate results. Considerations regarding 
operator precedence are implied by the language syntax and therefore are handled automatically by the 
sequence of pro- cedure calls required to evaluate an expression. No explicit operator stack is required 
because operators are saved automatically as recursive calls are made to procedures to evaluate terms, 
factors, and expressions. The Dynamic Environment--The Stack Machine The stack machine consists of 
a program store for instructions, a stack for data items, and four special-purpose registers: an instruction 
regis- ter, a program address register, a base register, and a top-of-stack register. All registers are 
used in the standard way for a stack machine. The stack machine instructions operate on data in the 
first one or two locations at the top of the stack. The instruction set includes the standard arithmetic 
and relational operators, and instructions for loading variables and literals onto the stack and for 
storing the value at the top of the stack into another stack memory cell. Indirect addressing with the 
load and store oper- ations is provided. Two transfer instructions, an unconditional jump, and a conditional 
jump (based on the value at the top of the stack) are included. I/0 operations are accomplished via instructions 
for reading or writing a single variable, writing character strings, and formatting output. i. Memory 
Management  The main issue in the discussion of the dynam- ic run-time environment is the mechanism 
for referencing information in the data store. Since data items do not exist in memory until run-time, 
the compiler cannot provide absolute memory ad- dresses for this data. During the execution of a program, 
a new data segment is stored on the top of the stack with each activation of a procedure. Along with 
inter- nal linkage information, memory is allocated with- in the data segment for each variable or array 
that is local to the procedure, A data segment remains on the stack only while the corresponding procedure 
is active. This process is illustrated in Figure 7. In order to reference an entry on the stack, the 
proper data segment must be known and the po- sition of the entry within the data segment must be established. 
Therefore, I PROCEDURE A 2 VAR a , a~ ; 3 PROCEDURE B ; 4 VAN [)1; bl Data segment5 for Procedure B 
G reference to b I ? reference to a 2 3 celia 8 used for 9 ENDPROC ; internal linkage i ~I Datafor Procedureseg~e~ 
A I0 CALL S ; informa-l1 tion~L~ , 1~ ENDPROC PL/CP program The contents of the stack after execution 
of the instruction on llne 7. Fisure 7: Stack Management: An Example during compilation, the address 
of a stack entry is specified as an ordered pair of integers (£, d). The second integer of the pair, 
d, is the displace- ment of the data element from the base of the data segment. The level difference, 
£, provides a means for locating the data segment corresponding to the procedure in which the element 
declaration occurred. Within procedure B, (shown in Figure7), for example, reference is made to two variables, 
h I (declared locally) and a 2 (declared in the outer procedure). The compiler generated address of b 
I is (0,3). The level difference of zero in- dicates that b I is referenced within the proce- dure in 
which it was declared. An address of (1,4) is generated for the reference to a2, indi- cating that a 
2 is declared in the next outer level of nested procedures. 2. Procedures Two basic aspects of code 
generation for pro- cedures should be presented in lecture: the con- vention for passing arguments to 
procedures, and the mechanisms of control transfer between proce- dures. As noted earlier, the call-by-value-result 
method of passing arguments is assumed for PL/aP. To implement this, arguments are passed to proced- 
dures through an argument list maintained at the top of the data segment of the calling routine. Code 
must be generated in the calling routine to load argument values into the argument list at the top of 
the stack before transferring control to the called procedure. When control is returned to the calling 
procedure, any argument values that might be returned must be restored. When a called procedure references 
an argu- ment, it is actually referencing a cell in the data segment of the calling routine. Therefore, 
 addresses for arguments are generated with a level difference of zero and a negative displacement 
from the base of the current data segment. A procedure is activated when a CAL instruc- tion is executed. 
The effect of this instruction is to establish all necessary linkage information before transferring 
control to the called proce- dure. Techniques for carrying out this step may be found in Wirth [15], 
Griffiths [8], and Bulman [ 4]. Any of these techniques may be implemented in the PL/CP compiler. CONCLUDING 
REMARKS As of this time, there does not seem to be any single text that can be recommended for this 
course. The Wirth text [15], supplemented with Gries [7] and Hopgood [i0], as well as selected papers 
[2,5,6,11,14] can be used effectively to provide the student with an adequate introduction to compiling 
techniques and to the related litera- ture. The production of a compiler for PL/CP, either Linkage 
information includes the static and dy- namic links and the return address the complete or the reduced 
(without the FOR loop, arguments, goto's, or labels) versions, is an es- sential part of the course. 
It is only through such an implementation effort that the problems discussed in lecture can be clearly 
understood and the solutions properly motivated and appreciated. Regardless of which run-time environment 
is chosen for implementation, we suggest that both environ- ments should be discussed in some detail. 
In addition, we believe that some discussion of an implementation using a non-recursive language is 
highly useful. Above all, it must be remembered that many of the programming techniques used in the 
implemen- tation may be new to most of the students in the class. Considerable time will therefore have 
to be allotted to analyzing and motivating these techniques. BIBLIOGRAPHY <RefA>I. Backus, John, et al., 
"The FORTRAN Automatic Coding System," Section 2A in Programming Systems and Languages, Saul Rosen, ed., 
McGraw-Hill, 1967. 2. Bauer, F. L., and K. Samelson, "Sequential Formula Translation," Section 3B in 
Pro- gramming Systems and Languages, Saul Rosen, ed., McGraw-Hill, 1967.  3. Bergman, Samuel and Steven 
Bruckner, An In- troduction to Computers and Computer Pro- gramming, Addison-Wesley, 1972.  4. Bulman, 
David M., "Stack Computers, An Intro- duction," Computer (10,5), May, 1977, pp. 18-29.  5. Glass, R.L., 
"An Elementary Discussion of Compiler/Interpreter Writing," ACM Comput- ing Surveys (i,i), March, 1969, 
pp. 55-77.  6. Graham, R.M., "Bounded Context Translation," Section 3A in Programming Systems and Lan- 
guages, Saul Rosen, ed., McGraw-Hill, 1967.  7. Gries, David, Compiler Construction for Digi- tal Computers, 
John Wiley, 1971.  8. Griffiths, M.W., "Run-Time Storage Manage- ment," Lecture Notes in Computer Science 
No.21, Compiler Construction, (F.L. Bauer and J. Eickel, eds.), 1974, pp. 195-221.  9. Grogono, Peter, 
Programming in PASCAL, Addi- son-Wesley, 1978.  i0. Hopgood, F.R.A., Compiling Techniques, American 
Elsevier, 1969. ii. Kanner, H., P. Kosinski and C.L. Robinson, "The Structure of Yet Another ALGOL Com- 
piler," Section 3D in Programming Systems and Languages, Saul Rosen, ed., McGraw-Hill 1967. 12. Mikunas, 
M.D., and John A. Modry, "Automatic Error Recovery for LR Parsers," CACM (21,6) June, 1978, pp. 459-465. 
 34 13. Mikunas, M.D., and Henry D. Shapiro, "A New Approach to Teaching a First Course in Com- piler 
Construction," Joint Bulletin of the SIGCUE/SIGCSE Symposium, February, 1976, pp. 158-166.  14. Rosen, 
Saul, et al., "PUFFT--The Purdue Uni- versity Fast FORTRAN Translator," Section 3E in Programming Systems 
and Languages,  Appendix A:  PROGRAM BLOCK ~8fray sJz@s and labels must be positive and less than 
1000 21f Ident is used, it must be a previously defined constant with a value between I and 999 3procedure 
names may not be used Saul Rosen, ed., McGraw-Hill, 1967.  15. Wirth, Niklaus, Algorithms + Data Structures 
= Programs (especially Chapter 5, pp. 280- 350), Prentice-Hall, 1967.  16. Wirth, N. and Kathleen Jensen, 
PASCAL User Manual and Report (Second Edition), Springer-Verlag, 1974.  The PL/CP Syntax STATEMENT 
 --4 I 1constant identtftec not a]lowed 21 ~ numhec ~ 999 3loop parameter expressions must satisfy usual 
constraints or loop execution will be skipped  4call by value-result 35  CONDITION EXPRESSION FACTOR 
T ER___~ LINE CONTROL DESIGNATOR Terminal Symbol s ~I reserved words ~ speclal characters W.~ : ) > p_~..~ 
I~ -(--j dtgt~s:o t t '3 ..+ ~ :: < = c, (~,+,~+++ m S+~ I o'P+h~,h) " 36  
</RefA>			
