
 AN INTERMEDIATE LANGUAGE FOR SOURCE AND TARGET INDEPENDENT CODE OPTIMIZATION DENNIS J. FRAILEY TEXAS 
INSTRUMENTS INCORPORATED Overview Abstract This paper describes an intermediate language to be generated 
by a syntax analyzer and processed by a code generator. An (essentially) optional code optimization phase 
may be used before code generation. The language is designed to exclude source and target dependencies 
(these are specified in a set of auxiliary tables) and has been used to implement a general purpose 
code optimization module. This module has been incorporated into compilers for several source languages 
and has resulted in production-quality target code. The paper focuses on the features of the language 
and reviews the optimizations for which it has been designed. A major benefit of this approach has been 
to isolate many of the characteristics which lead to code optimization opportunities. Of particular note 
are numerous facts which can be gathered in a target-independent manner, even though their utilization 
will depend on the details of the target machine. The intermediate form of code used internally in a 
multi-pass compiler depends on the kind of processing to be done in the various phases of compilation. 
Some compilers have used different intermediate forms between different phases. Generally an intermediate 
form represents a program as a series of operations to be performed in a given sequence. Each operation 
is specified by a "verb" or operator and a sequence of "nouns" or operands. The intermediate form is 
seldom a complete programming language, and is typically augmented by tables, flags, and other information. 
 The form to be discussed in this paper was developed as part of a larger effort to create a general 
purpose code optimization module (see reference 6). The effort was aimed at providing a vehicle for the 
study of code optimization which would be independent of any particular source language or target machine, 
yet could actually be a part of production quality compilers. The kinds of optimizations performed and 
some of the results are also discussed. One potential difficulty with defining an intermediate language 
for a general purpose optimizer is that different source languages will require different interpretations 
of similar constructs. The scheme developed addresses this problem via a set of auxiliary tables which 
are constructed to reflect the particular source language and target machine. &#38;#169; 1979-ACM 0-89791-002-8/79/0800-0188 
$00.75 see ii 1 8 8 Language Compiler Phase SOURCE LEXICAL AND SYNTACTICAL ANALYSIS f N-TUPLES / 
CODE OPTIMIZER N-TUPLES CODE GENERATOR OBJECT (TARGET) CODE Figure I : Compiler Organization Compiler 
Structure The optimization module is designed to be called after the lexical and syntactic analysis 
phases are complete, and prior to code generation (Figure I). The intermediate language, called N-tuple 
notation, is generated as input to the optimizer, where It is revised and augmented and then passed to 
the code generator. The idea of a machine independent intermediate language traces its origins to the 
early days of computing (see reference 7, for example). The optimizer reads N-tuples and places them 
in a linked list corresponding to the order in which they are read. Certain initial processing and local 
optimizations are performed during the reading process, which may result in deletion or revision of 
N-tuples. Once an entire program or program segment has been read, additional optimizations are performed, 
typically resulting in deletion, addition, revision, and/or rearrangement of N-tuples. Certain information 
about operand usage is gathered during this process and inserted into operand fields. N-tuples are then 
passed to the code generator. N-tuple Notation The processing to be carried out by a program is represented 
as a sequence of N-tuples, each of which indicates a particular operation to be performed. An N-tuple 
consists of an operator and an arbitrary number of operands, as follows: OP OPDO OPDI OPD2 OPDn The 
operator specifies the type of computation or other processing to be performed; and the operands, which 
may be constants, variables, statement labels, function or subprogram names, etc., indicate information 
or data needed to perform the operation and/or the result(s) produced by the operation. For example, 
the Fortran statement: X = A + B * C might be represented as: * TI B C + T2 A TI --X T2 In this example, 
the Ti's represent "temporary" or "compiler-generated" variables containing intermediate parts of the 
calculation. Typically, these names are reused at the end of each program statement. They may even be 
reused within a single statement. The optimizer assumes, however, that each temporary value will be used 
onl~ once before the name is assigned to some other value. (This may not be true upon exit from the optimizer). 
In practice, this means the optimizer assumes the parser has used a straightforward approach and has 
not tried to do any optimization. Further details on handling of temporary variables are given in the 
sequel. A list of typical operators for an algebraic programming language is found in the Appendix. 
 189 Both the operator and the operands have attributes which may be important to code optimization 
and/or code generation. These attributes are represented by flags and descriptors. Operand attributes 
are generally carried along as part of the N-tuple. The operator's attributes, however, are described 
in tables. This approach was chosen because operator attributes tend to be numerous but, for a given 
source language and target machine, are generally fixed and independent of context, whereas operand 
attributes are few in number, often context dependent, and in many cases are actually determined by 
the optimization process. The relationship between N-tuples and various tables is illustrated in Figure 
 2.  Certain operand attributes are determined by the language and/or source context, and are specified 
prior to the optimization phase. For example, an operand representing a result is flagged as a result 
operand, and one needed to perform an operation as a source operand. Each operand should be either a 
source or a result operand, and it may be both.  OP OPNDO OPNDI OPND2 . I.... OPERATOR TABLE (simple 
attributes) The other operand attributes supplied prior to optimization are the identifier, a field 
which uniquely identifies that operand (typically a symbol table pointer) and the type, which indicates 
its basic properties such as constant, variable, label, procedure name, procedure argument, etc. The 
operator field consists of a pointer to an operator table. All attributes are specified in this or other 
tables (Figure 2). The properties indicated in these tables determine the applicability of various optimization 
algorithms to particular N-tuples. It is noteworthy that the optimizer remains ignorant of the actual 
operation represented by an N-tuple. Operator properties include such things as associativity, commutativity, 
eligibility for common subexpression elimination, and the number of operands expected. Also included 
are properties which certain operands are expected to have, such as "first operand should be a label" 
or "third operand should be an address". A detailed list of these properties and their significance to 
the optimization process are given below. SYMBOL TABLE (not used by optimizer) OTHER TABLES (more complex 
and machine dependent attributes) Figure 2 : N-tuple -Table Relationship 190 Special Features The 
N-tuple notation makes no restriction on the number of operands, and although most operators always 
use the same number of operands, some allow a variable number. For example, the Boolean "or" might be 
implemented as a binary operator expecting one result operand and two source operands, or it might be 
an n-ary operator with one result and a variable number of inputs. The "number of operands expected" 
for an operator may therefore be interpreted as a fixed, exact requirement or as a "minimum" requirement. 
A separate flag distinguishes these cases. For operators allowing a variable number of operands, the 
optimizer will convert to the maximum form during the input phase. Thus, for example, the sequence: 
 + TI A B  + T2 TI C  is converted to: + T2 A B C This transformation is part of "conversion to 
canonical form", which is discussed below. Some operations, such as those involved with subscripting 
in algebraic languages, may require (or generate) the addresses rather than the values of certain of 
their operands. Thus operator attributes may specify that the value or the address (or both) of an operand 
will be involved in the operation. Since addresses may be generated as results, a temporary may represent 
an address during one part of a computation and a value elsewhere. The set of operations chosen for 
lexical and syntactical analysis of a language does not necessarily coincide with that best suited to 
optimization or code generation. One case is the situation arising when two operations have different 
syntactical meanings but will generate the same target code and have the same properties for optimization 
purposes (for example, the integer and real forms of "test if positive"). To avoid duplicate table entries 
for such cases, it is possible to indicate that one operation is an "alias" for another. A more frequent 
problem occurs when the optimizer requires more detailed information than is needed for syntactic analysis. 
Consider, for example, the Fortran expressions: X + I and I * C where I is an integer and X and C 
are real's. A syntactic analysis might represent these calculations as a real-integer addition on X 
and I and an integer-real multiplication on I and C. Each case requires a conversion of from integer 
to real form. If each expression were written as a sequence of two operations: conversion of I to real 
followed by addition or multiplication, the optimizer would be able to detect the conversion as a common 
subexpression and avoid performing it twice. The N-tuple scheme makes provision for such cases by allowing 
an operator to be listed as a primitive or a macro. In the macro case, each N-tuple using the operator 
is replaced by a sequence of other N-tuples. (The latter may also be macros, in which case the process 
of replacement is continued until all primitives are reached. Recursion is not permitted in this process.) 
A macro breakdown table is provided to provide the details"~uch reductions. These reductions take place 
as the N-tuples are being read by the optimizer, and the actual optimization process involves only primitive 
operators. Note that some primitive operators may have no counterpart in the source language The Fortran 
example given above suggests a problem found in extensible and very general purpose languages in cases 
where a variety of different data formats are permitted and certain operations may be performed on any 
combination of these. The number of different cases can easily become huge, although each typically involves 
one or more type conversions followed by a common operation. The macro operator capability of N-tuples 
has proven to be a significant help in reducing such cases to a manageable set of primitive operations. 
(Some of these ideas originated with Laurance, reference 8.) One debugging problem which optimizers 
typically cause is to scatter the object code for a single source statement over several sections of 
the target code. The N-tuple notation therefore recognizes a special "line number" operator whose single 
operand is the "number" of the source line which generated the N-tuples which follow. During the input 
phase, the optimizer deletes all line number N-tuples and inserts line numbers into a special field of 
each subsequent N-tuple (until another line number is read). This number is used for all error messages 
 191 and is passed on to the code generator (where it might be printed as part of an object code listing, 
used in other error messages, or placed in actual target code.) A final special operator is "reassign 
temporary names", typically issued at the end of a statement, which tells the optimizer that temporary 
 variables generated prior to this point are no longer needed and their names will be reassigned to new 
values. This optional operator enables use of a smaller number of distinct temporary operand names, which 
results in more efficient optimization and (often) better target-machine code. Special Operand Properties 
 Any operand may have the property "complemented by Op", where Op is any self complementing, unary operator 
(see class 2 operator properties, below). Thus the N-tuple: * TI A B(eomp -) has the same meaning as 
the sequence: -TO B * TI A TO and corresponds to the expression: A * (-B). Several unary operators 
have this self complementing property and, in addition to allowing more compact representation of code, 
the "complemented operand" form allows a number of optimizations to be performed (such as elimination 
of consecutive unary operators, as in "-- A"). The optimizer converts the N-tuples to the more compact 
form during the input process. Additional information on this subject is found below. Operator Properties 
and Optimizations Operator properties can be placed into three classes: class I properties apply to 
each operator but have a distinct value for each operator (e.g., "number of operands expected"); class 
2 properties may or may not apply to a given operator and can be represented by a single flag (e.g., 
"associativity"); class 3 properties, when applicable, require more than a simple flag to specify (they 
generally relate the operator to some other operator or some special constant). If an operator has a 
class 3 property, its operator table entry may contain a pointer to another table describing the details 
of that property. The class I properties include the "expected" (or minimum) number of operands and, 
for each operand, whether it represents a source or result, whether its address or value is needed, and 
whether it is a "label" -i.e., whether it identifies some N-tuple. Each label is characterized as an 
"entry name" (if it identifies the current N-tuple) or a "jump name" (if it identifies some other N-tuple 
to which the code generated by the current N-tuple may cause a jump). Information on label operands is 
used by the optimizer to determine the program's basic block structure and flow of control between blocks. 
 The class 2 properties are: AJF - this operator always causes a jump (branch, transfer of control) 
- used in control flow analysis. ASS - this operator is associative - used in conversion to canonical 
form. COM - this operator is eommutatiEe - used in conversion to canonical form. CRF - the source operands 
of this operator must be located in main memory rather than registers - used in register allocation (typically 
applies to subprogram arguments). CSF - this operator is eligible for deletion if it is a common subexpresion'-(would 
not apply, for example, to an I/O operation). DPF - this N-tuple may be deleted if there is no path 
to it ~i.e., if the program can never reach this code; this would apply only to executable code). DUF 
- this N-tuple may be deleted if its result operands are never used TI~ situation often~appens after 
other optimizations have been performed. It may also indicate a logic error in the source program.) 
ENF - this N-tuple defines a flow entry point (label definition, program entry point) used in flow analysis. 
 JPF - this operation may produce a jump, but not necessarily -used in f'~w analysis. 192 RMF this 
operation may cause the - contents of arbitrary high speed registers to be modified (might apply 
to a procedure call or I/O operation) -used by code generator in register allocation and by optimizer 
in operand use analysis. As implemented, this flag also implies the potential for side effects, thus 
the optimizer assumes no programmer defined values are preserved across such an operation. The side effect 
condition should probably have been implemented with a different flag, since some modern programming 
languages support functions with no side effects. SCM - this operator is self complementing. A self 
eomplementfng operator is a unary operator which, if applied "~ in succession, yields the original operand. 
Some examples of self complementing operators are: unary ---X : X NOT NOT NOT B : B Self complementing 
operators enter into a number of optimization techniques, several of which are described below. For further 
elaboration, see reference 5. The Class 3 properties are described briefly in the text below, and 
the related optimizations are discussed in the sequel. In some cases, however, appreciation of their 
significance requires further understanding of the optimization techniques being used. References 
5 is recommended for additional ~nformation. In most cases, various details are needed to make use 
 of class 3 properties, and these are provided in tables which are not described in this paper, but 
whose structure is straightforward. The class 3 properties are: CMP - This operator has a complement. 
If class 2 flag SCM is set, it is its own complement. Otherwise its complement must be specified in a 
special "complement table". DCF - If certain of the operands of this operator are complemented, or 
are special constants (typically O, I, true, or false), the complements or constants may be deleted. 
In some cases, the operator must be replaced by another, often simpler one. Some examples: -A * -B becomes 
A * B A and True becomes A A and False becomes False X + 0 becomes X Y * 1 becomes Y DSF - This operator 
is unary and may be distributed across any of a certain set of other operators, provided certain changes 
are made. Examples: - (A -B) becomes -A + B  - (A * B) becomes -A * B  not (A or B) becomes A nor 
B Note: These transformations may be applied in either direction. (See UCM or "flexible form" transformations, 
below.) In general, one form is more suitable for detecting potential optimizations whereas another generates 
more efficient code. The optimizer applies the transformation one way initially and the reverse way prior 
to code generation. MCF If a complement operator is - applied to any of the source operands of this 
operator, it is safe to move the complement to any other source operand. In general, such complements 
are moved to the leftmost source operand, as part of the canonical form (discussed below). Examples: 
 A * -B becomes -A * B A * -B * -C becomes --A * B * C (which becomes A * B * C because "-" is self 
complementing) 3 * -A becomes -3 * A (which is more efficient because the constant is complemented during 
compilation) FCM - Although not commutative, this operator may be viewed as "partially commutative" 
in the sense that its operands may be swapped by use of a different operation. Examples: X < Y becomes 
Y > X A -B becomes B inv-A 193 (the operator "inv-" or inverse subtract, invented for optimization 
purposes but seldom available on a target machine, causes the first operand to be subtracted from the 
second) UCM - This operator, which lacks either associativity, commutativity, or both, may be replaced 
by one which has one or both of these properties. Such an operator is considered ~or¢ flexible. Examples: 
 A -B becomes A + (- B) R / S becomes R * (u/ S) (real arithmetic only) (The operation "u/" or unary 
divide is the same as dividing into 1.0. It is self complementing and contributes to several optimization 
techniques.) (There are numerous "flexible form" conversions involving Boolean operators, especially 
if all 16 of the possible Boolean functions are considered. Those not available on a given machine can 
generally be eliminated at the end of the optimization phase, during conversion to computationally efficient 
form.) Code Optimization Sequence Code optimization proceeds in several stages: I) expression optimization 
and block structure analysis; 2) operand use analysis; and 3) conversion to most computationally efficient 
form. These correspond to three "passes" over the input in forward, backward, and forward directions. 
Space limitations do not permit extensive discussion of these, so only the highlights are given in this 
paper. The expression optimizations, many of which are performed as the code is read into the optimizer, 
are relatively local in scope and are similar in nature to those described in references 2 and 5. Fundamentally, 
the approach is based on reducing all code to a flexible form (easy to convert to any of several other 
forms, e.g., commutative, associative, etc.); then reduction to a canonical form (so that expressions 
of equivalent value but different appearance will be detected as equivalent); then removal of redundancy 
(detection made easier because of canonical form and "complemented operand" notation). The canonical 
form is reached by applying applicable transformations so that all source operands are in a standard 
sequence (symbol table pointer order), with constants at the far left, and so that complement operators 
are applied to constants rather than variables, where possible. Common subexpressions are detected between 
sequences such as: A+B+D+E and B+C+D+F by means of a simple search technique which is outlined in reference 
5. Most constant folding is done by the code generator (due to possible machine dependencies), but operation8 
on constants are easily detected in canonical form. The other function of this phase is to determine 
the block structure of the program. In the process of doing this, the optimizer removes useless jumps 
(such as those which jump around other jumps or jump to the immediate successor statement) and unreachable 
(dead) code. These processes are based on the operator properties described above, but are fundamentally 
no different from work done elsewhere (such as described in references I, 9, and 11) except for the 
 definition of a basic block. Block Structure Most literature describes a ,,basic block" as beginning 
with any code entry point (label, program entry point, or "through" path from conditional jump) and 
ending with any jump (conditional or otherwise) or other code entry point. In some cases, a subroutine 
or procedure call will be interpreted as a conditional jump and will terminate a block. Although this 
is a fine fit to much global flow analysis theory, where each block is treated as a node in a graph 
and each possible flow path as an edge, the frequency of conditional jumps (often compiler-generated) 
can be a problem. In such languages as PL/I, where virtually any computation can produce an "exception" 
condition which would cause a jump, the effect would be very many, very small blocks. Global flow 
analysis and loop detection generally require times at least proportional to the number of basic blocks 
and the number of inter-block transfers. Therefore, a large number of blocks may be inefficient to 
process. (See reference 12, especially pages 479 and 480, for a survey of the best known algorithms 
for global flow analysis and strong component (loop) detection.) Moreover, if global analysis is not 
performed, larger basic blocks produce better intra-block optimization. Thus 194 we chose instead to 
copy an idea from Mendicino, et. al. (reference 10) and define the end of a block as occurring only 
with an absolute jump (or new code entry). A conditional jump or procedure call (although requiring special 
handling) does not terminate a block. The result is to represent the program by a smaller number of 
larger blocks. (The major drawback is to put restrictions on code motion within a block.) After block 
boundaries are defined and blocks are rearranged to avoid unnecessary jumps, each block is scanned to 
obtain operand use information and apply additional optimization transformations. Operand Use Properties 
 There are three characteristics of an operand which are of interest to the optimizer and code generator: 
constancy, busy status, and normal storage type. Of these, only busy status changes with each N-tuple. 
With some exceptions (discussed below), the other two characteristics are established prior to entering 
the optimizer. Although these characteristics can be of considerable importance in global optimization 
(removing code from loops, strength reduction, test replacement, etc.), they also have important local 
uses. References I, 3, 4, and 9 discuss global uses. Operands may be categorized as constant, locally 
constant, and variable. Constants never change in value whereas variables do. A variable whose value 
does not change within a particular program block is classified as locally constant within that block 
and may be treated as a constant in many respects Addresses are generally constant, but not always (as 
will be discussed below). Borrowing from common usage (see reference 9), an operand is said to be busy 
if it may be needed again within the current program block. (Busy information is used to determine which 
computational results should be kept in registers, stored in memory, or discarded.) Since our notation 
allows reference to the "value" or "address" of an operand, either of these may be busy at a given time. 
For example, in the sequence: .... .... ... we generate code for the first line and find that the address 
of A and the value of J are both busy. (The original system design allowed for reference to the "indirect 
address" of an operand, in an attempt to optimize use of pointers and subroutine arguments. Experience 
showed, however, that these were seldom busy in a typical program and could be handled (albeit awkwardly) 
as "addresses of addresses". Thus the implemented system did not support the indirect address feature.) 
 The third distinction made by the optimizer, storage type, is based on a model in which memory consists 
of two classes: computational (i.e., registers or a stack) and permanent (i.e., standard random access 
memory). The optimizer assumes there are no limits to the size of either class of memory, but biases 
use of each class in appropriate ways. The values of programmer-defined variables are considered "permanent 
memory resident" which means that, regardless of how their values are computed, they must eventually 
end up in permanent storage The values of temporary or compiler-defined operands are assumed to normally 
reside in computational storage, which is assumed to be more efficient for computational activities. 
Constants which are small enough to be accessed via "immediate" addressing (a target-machine-dependent 
parameter supplied to the optimizer in a table) are treated as though they reside in computational storage, 
whereas all others are permanent. As discussed earlier, temporary operand names are reused frequently 
and thus may represent different quantities and different portions of memory at different times. As a 
consequence, their "addresses" are variable (or, possibly, locally constant), whereas the address of 
a permanent operand is always a constant The code generator must perform the operation implied by an 
N-tuple regardless of where the operands are located. If, however, there are two available copies of 
an operand and one of them is in computational storage, that copy is presumably preferred. (The opposite 
may occasionally be true, as with procedure arguments -see class flag CRF, above.) The net result of 
operand usage analysis is a set of six "usage flags":  constant or variable (determined prior to optimi~'at~on) 
  temporary constant (yes or no) (for constants, this implies suitable for immediate access; for variables, 
this implies locally constant)  195  address busy in computational storage T~es or no-7  address 
busy in permanent storage (yes or  value busy in computational storage y~ oF no)  value busy in permanent 
storage (yes  ~r no) Note that when an address is generated as a result (as, for example, with subscription), 
the operand's "address busy" flag refers to the "value" computed (i.e., the address) and the "value busy" 
flag refers to the "value" to which this address points The "temporary constant" information is gathered 
during the initial scan of the N-tuples, whereas the last four of these are determined during a backward 
scan of each block. Initially, before the backward scan, the status of each permanent operand is "value. 
busy in permanent storage" (see explanation below). All other busy flags are initially "no" This status 
information is kept in an optimizer table and will be transferred to operand fields of N-tuples as the 
block is scanned. When a permanent operand is scanned, its status is copied from the table to the N-tuple 
field (indicating future needs). If it is a source operand, the table entry is then augmented to include 
its current use. Thus, for example, if its value was already busy in computational storage on the basis 
of some previously discovered need, and the current operation needs its address, the updated table will 
 indicate that both address and value are busy in computational storage For result operands, the table 
entry is reset to indicate that the address is u~sy ~ in computational storage (since it may still be 
needed to store the value), and the value is no longer busy. If an operand is both a source and a result, 
the table entry is reset to indicate its current use, both as a source and as a result. (Note that just 
because something is "busy" in the table does not mean that it has previously been computed or moved 
to computational storage. It simply means that, if it has been computed or so moved, it should not be 
discarded because it might be used again.) When a jump operation (conditional) is encountered, the 
table status of each permanent variable ~s flagged as "value needed in permanent storage". This is similar 
to the initial (end of block) setting and is used to assure that, if computed earlier in the block, it 
will be stored in memory before the jump takes place. Similar activities occur for operators having 
the RMF flag (typically, subroutine calls). Assignment of use flags for temporary operands differs as 
follows: no special action occurs for jumps because the value can be ignored if the jump occurs When 
a temporary operand is used as a source operand, its address (which is a "variable") is flagg'e'~ as 
"needed in computational storage", in addition to the processing of its value as described for permanent 
operands. This assures that the temporary name and the corresponding actual register will be "reserved" 
until no longer needed. If a result operand contains an address, it is actually a source operand, the 
true result operand being that to which it points. Thus, as with other source operands, its address is 
flagged as busy in computational storage. The value is flagged as busy in permanent storage. Moreover, 
unless additional information is available, the code generator will have to assume that any permanent 
operand may be modified by the current N-tuple. Occurrence of a register modifying (RMF) operation causes 
all operands currently busy in computational storage to be flagged as busy in permanent storage. This 
assures that they will be saved rather than recalculated. The point of the "highlight" description above 
is that the information is gathered on the basis of the properties of the operators and operands, and 
does not require the optimizer algorithms to be based on the characteristics of the source language or 
target machine. As will be discussed shortly, experience has shown that the primary beneficiary of this 
information is the code generator which, of course, will need to know the details of the real registers 
and instruction set. A subsequent paper will provide a detailed description of these algorithms and further 
elaboration on the memory model. References I-5 and 9 illustrate other methods of using such information 
(although, in most of these, address optimization is treated as a special case.) Final Optimizations 
 The final phase of optimization is a forward scan which removes all N-tuples whose results are not used 
(if permitted by operator flag DUF), removes all operand complements (optional -may be left alone if 
code generator can handle it), and converts each N-tuple to the most "computationally efficient" 196 
 form (which may differ from the canonical form used earlier for elimination of redundant code). At this 
point the code generator can take over, making use of the operand use attributes (and, if desired, the 
operator properties described in the produce target-machine code. tables) to Implementation and Experience 
The code optimization module has been implemented on CDC 6000 / CYBER series computers and has been 
incorporated into two compilers: one for Mad (an Algol-based general purpose language) and the other 
for a subset of PL/I. Performance was measured in two ways: running compilers with and without the optimization 
module; and comparison with CDC Fortran compilers on similar code sequences. The first approach was later 
augmented to include only those parts of the optimizer which gather information used by the code generator 
 (i.e., no optimization per se within the optimization module). Over a 30,000 source statement data base, 
consisting of a compiler and a variety of smaller, student programs, the results shown in Table I were 
achieved. These results indicate the relative unimportance of local expression optimization and common 
subexpression elimination. Note that, as implemented, the optimizer did no global optimization (and, 
in particular, no loop optimization), which would be expected to produce more significant benefits. Thus 
the major benefits of the optimizer's analysis were the results of operand use and constant value analysis 
which enabled the code generator to fold constants, avoid redundant loads and stores, and perform more 
efficient register allocation. Instruction count comparison with CDC's RUN and FTN compilers on code 
containing no loops (thus not subject to much global flow analysis) yielded the results shown in Table 
II. In comparison with hand code, most of the missing improvements were attributable to information 
which the hand-coder could determine but which was unavailable to the compiler (such as the fact that 
a particular variable would have a very small value and could be kept in an index register.) Conclusions 
 Excluding global optimization, which could have been implemented with additional time, the approach 
taken seems to have achieved results comparable to those of production compilers. Of particular note 
is the importance of the information which is gathered in a target-independent manner but utilized only 
later, in the target-dependent code generator. The optimization module runs 3-4 times slower than the 
corresponding production compilers, but it was not designed for production use and the languages being 
compiled are different, thus no true comparison is possible. Although used successfully with several 
source languages, the optimizer has yet to be tried on a different target machine. It would be interesting 
to compare the results on another target machine, but there is no reason to expect them to be substantially 
different. Acknowledgement The suggestions and encouragement of the late Dr. Maurice Halstead were 
of particular importance in the canonical form definition and performance analysis phases of this work. 
 Table I -Optimization Comparison Amount of Percent Reduction Optimization in Code Space Code Gen Only 
35.7 (using info. gathered Percent Reduction in Execution Time 66.8 by optimizer) Complete COMPILER 
 No Optimization Full Optimization RUN FTN Best Hand Code 39.1 Table II -Compiler Comparison (relative 
proportions) FETCHES STORES MISC. 6.0 2.1 4.8 1.9 1.7 1.6 2.0 1.6 1.5 I .8 I .6 I .2 1.0 1.0 1.0 197 
 70.0 TOTAL INSTRUCTIONS 4.5 2.1 2.0 1 .8 1.0 References <RefA>1. Allen, Frances E., "Program Optimization" 
in Annual Review in Automatic Programming, ed. Mark I. Halpern and Christopher J. Shaw. Vol. V. Oxford: 
Pergamon Press, 1969, 239-307. 2. Bagwell, Jr., John T., "Local Optimizations," ACM SIGPLAN Notices, 
V (July, 1970) (Proceedings of Symposium on Compiler Optimization), 52-66.  3. Cocke, John, and J. T. 
Schwartz, Programming Languages ~and Their Compilers, Preliminary Notes, 2nd revised edition. Courant'-~itute 
of Mathematical Sciences, New York University, 1969.  4. Earnest, C. P., "Some Topics in Code Optimization," 
Computer Sciences Corporation, 1969.  5. Frailey, Dennis J., ,,Expression Optimization Using Unary Complement 
Operators," ACM SIGPLAN Notices, V (July, 1970) (Proceedings of Symposium on Compiler Optimization), 
67-89. (Also see minor corrections to above paper in August, 1970 issue of ACM SIGPLAN Notices, 6-7.) 
 6. Frailey, Dennis J., A Study of Code Optimization Using a G~ne~ ~'~r'pose Optimizer, Ph. D Dissertation, 
Purdue University, West Lafayette, Indiana, 1971.  7. Halstead, Maurice, "Machine Independence and Third 
Generation Computers" in Proceedings of the Fall Joint Computer Conference. Washington: T~o'~pson Books, 
1967, 587-592.  8. Laurance, Neal, "A Compiler Language for Data Structures" in Proceedings of the 23rd 
ACM National Conference. Princeton: Brandon/Systems Press, Inc., 1968, 387-394.  9. Lowrey, Edward S., 
and C. W. Medlock, "Object Code Optimization," Communications of the ACM, XII (January, 1959), 13-22. 
 10. Mendieino, Sam F., et. al., "The LRLTRAN Compiler," Communications of the ACM, XI (November, 196~), 
747-755.  11. Rustin, Randall (ed.), Design and  Optimization of Compilers. Englewood Cliffs, N. J.: 
Prentice Hall, 1972. 12. Tarjan, Robert E., </RefA>',Complexity of Combinatorial Algorithms," SIAM Review, 
XX (July, 1978), 457-491. 198 Op LINE SB11 SB21 SBMI FUNC STR PARR PARV EPR START EQL ABSI UMII 
 ADD11 SUB11 SUBI11 MULl1 DIV11 P11 NEll LT11 GT11 ADDO0 AND OR NOT CVIO CVRI Appendix Typical 
Operators for an Algebraic Language Primitive Operators Description SOURCE LINE NUMBER SINGLE SUBSCRIPTION 
OF INTEGER ARRAY DOUBLE SUBSCRIPTION OF INTEGER ARRAY MULTIPLE SUBSCRIPTION OF INTEGER ARRAY (etc., 
for arrays of other size units) FUNCTION CALL BEGINNING OF FUNCTION PARAMETER LIST FUNCTION PARAMETER 
(by reference) FUNCTION PARAMETER (by value) (etc., for by name or such) END OF FUNCTION PARAMETER LIST 
(operators similar to the above may be needed for procedure or I/O calls if different calling sequences 
are involved) STARTING POINT OF MAIN PROGRAM REPLACEMENT (:= or =) INTEGER ABSOLUTE VALUE INTEGER 
UNARY MINUS INTEGER ADDITION INTEGER SUBTRACTION INTEGER INVERSE SUBTRACT INTEGER MULTIPLICATION INTEGER 
DIVISION (-A+B) INTEGER RAISED TO INTEGER POWER INTEGER INEQUALITY COMPARISON INTEGER LESS THAN COMPARISON 
INTEGER GREATER THAN COMPARISON (comparisons of other types) (note: see macro list for EQ, GE, LE) 
(above integer operations can be duplicated for real or other, types, such as: REAL ADDITION ) BOOLEAN 
AND OPERATION BOOLEAN OR OPERATION BOOLEAN NOT OPERATION (other Booleans, as needed) CONVERT REAL TO 
INTEGER CONVERT INTEGER TO REAL (other type conversions) Special Class I Properties I CONSTANT OPERAND 
EXPECTED RESULT IS AN ADDRESS RESULT IS AN ADDRESS RESULT IS AN ADDRESS I OPERAND I ADDRESS I OPERAND 
 NO OPERAND NO OPERAND EXPECTED OPERAND EXPECTED EXPECTED EXPECTED EXPECTED OPERANDS MUST BE SAME 
TYPE UNARY UNARY UNARY UNARY UNARY 199 Appendix (continued) Typical Operators for an Algebraic Language 
 Primitive Operators (continued) 02p Description Special Class I Properties JMP JUMP I LABEL OPERAND 
EXPECTED JMPT JUMP IF FIRST OPERAND IS TRUE SECOND OPERAND IS LABEL JMPF JUMP IF FIRST OPERAND IS FALSE 
SECOND OPERAND IS LABEL LOC LOCATION (LABEL DEFINITION) I LABEL OPERAND EXPECTED INFN BEGINNING OF INTERNAL 
FUNCTION I CONSTANT OPERAND EXPECTED EXFN BEGINNING OF EXTERNAL (SEPARATELY I CONSTANT OPERAND EXPECTED 
 COMPILED) FUNCTION ENTRY FUNCTION ENTRY I LABEL OPERAND EXPECTED RET FUNCTION RETURN ENDFN PHYSICAL 
END OF FUNCTION NO OPERAND EXPECTED ERETN "ERROR" DETECTED IN FUNCTION -NO OPERAND EXPECTED RETURN TO 
SUPERVISOR PARM STORE FUNCTION PARAMETER IN MEMORY I OPERAND EXPECTED (UPON ENTRY TO FUNCTION) STOP STOP 
EXECUTION (RETURN TO SUPERVISOR) NO OPERAND EXPECTED ENDPG END OF PROGRAM NO OPERAND EXPECTED Macro 
Operators 02 Description Translated Into ADDIO ADD01 EQLIO ADD INTEGER TO REAL ADD REAL TO INTEGER REPLACE 
INTEGER BY REAL CVRI, C VRI, CRIO, ADDO0 ADDO0 EQL (etc., for other operations) mixed-type GE11 NEll 
INTEGER INTEGER INTEGER ° LESS OR EQUAL COMPARISON GREATER OR EQUAL COMPARISON INEQUALITY COMPARISON 
NOT, NOT, NOT, GT11 LT11 EQ11 (etc., for other data types and mixed types Note that only half of the 
six comparisons are primitives, thus improving chances for optimization via common subexpression or excess 
"not" removal. The choice of which three are primitives is based on best object code, and may be reversed 
for different target machines. For machines on which all six are equally efficient, one can define all 
six as primitives and use the "distribution" and "self complementing" properties of the "not" operation 
in the optimizer.) 200  
			
