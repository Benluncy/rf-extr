
 A Framework for Assisted Exploration with Collaboration Eric A. Wernert Andrew J. Hanson . Computer 
Science Department Indiana University Bloomington, IN 47405 USA  Figure 1: Assisted collaborative exploration 
scenario with the leader s guide avatar being a dog, and two groups of following col­laborators. Observe 
that the guide avatar points to objects of interest even if they are not aligned with the direction of 
locomotion or the observer s ground plane, and that the local leader serves as the guide avatar for all 
attached followers. Abstract We approach the problem of exploring a virtual space by exploiting positional 
and camera-model constraints on navigation to provide extra assistance that focuses the user s explorational 
wanderings on the task objectives. Our speci.c design incorporates not only task-based constraints on 
the viewer s location, gaze, and viewing parameters, but also a personal guide that serves two important 
functions: keeping the user oriented in the navigation space, and pointing to interesting subject areas 
as they are approached. The guide s cues may be ignored by continuing in motion, but if the user stops, 
the gaze shifts automatically toward whatever the guide was interested in. This design has the serendipitous 
feature that it au­tomatically incorporates a nested collaborative paradigm simply by allowing any given 
viewer to be seen as the guide of one or more viewers following behind; the leading automated guide (we 
tend to select a guide dog for this avatar) can remind the leading live hu­man guide of interesting sites 
to point out, while each real human collaborator down the chain has some choices about whether to fol­low 
the local leader s hints. We have chosen VRML as our initial development medium primarily because of 
its portability, and we have implemented a variety of natural modes for leading and col­ .Email: fewernert, 
hansong@cs.indiana.edu laborating, including ways for collaborators to attach to and detach from a particular 
leader. Keywords: way.nding, locomotion, navigation, exploration, col­laboration, virtual reality, VRML 
 1 Introduction Getting around a virtual environment or data set while keeping track of one s whereabouts 
and task objectives is the subject of a num­ber of techniques often referred to by such names as way.nding, 
locomotion, and navigation. We advocate a family of approaches involving the incorporation of task-based 
constraints on the navi­gation parameters such as viewer position and orientation; this ap­proach enables 
the environment designer to provide extra assistance to keep the user s explorational wanderings and 
attention focused on the task objectives [10, 11]. The critical feature of such constraints is the enablement 
of be­haviors that are not possible with simple movie-like animations along one-dimensional parameter 
paths, and which are more goal­directed than free motion. For desktop systems, .shtank virtual reality 
displays, and fully immersive display environments, we sup­ply at least two degrees of freedom corresponding 
to mouse move­ 0-7803-5897-X/99/$10.00 Copyright 1999 IEEE ments or horizontal controller movements, 
plus a variety of options for controlling the gaze direction of the user s navigation frame. The same 
general theoretical structure naturally includes other lay­ers of interpolatable state variables such 
as points of interest, spot­lights, fog, gravitational attraction to critical positions, and vari­able 
controller sensitivity. We address a number of issues relevant to collaboration, which has the dual goals 
of facilitating information and viewpoint shar­ing while promoting individual exploration and the development 
of conceptual models. For example, by forcing collaborating avatars to adhere to the constraint manifold 
while following one of several rules for attaching oneself to a leader, we avoid some frequent prob­lems 
such as losing your tour group or crashing through the scenery while trying to catch up with the leader. 
Previous work in this area ranges widely in its objectives. Methods that intelligently focus on particular 
scene points include Mackinlay et al. [15], while Phillips et al. [20] construct constraint­based camera 
placement, and more general control systems are treated by Ware and Osborne [27] and Drucker et al. [6]. 
Robinett and Holloway [22] employ constraints in view selection, while ex­pert knowledge is utilized 
by Billinghurst and Savage [1]; way.nd­ing strategies in general are analyzed in Darken and Sibert [5], 
while viewpoint control and locomotion in immersive environments have been studied by Bowman, et al., 
[2, 3]. Other work by Ware and Fleet [26] focuses on the nature of .ying modes of naviga­tion, and Pierce 
et al. [21] observe the manner in which image plane interaction can be exploited for getting around the 
scene. An ex­haustive analysis of usability characteristics, including navigation methods, has been undertaken 
in a report by Gabbard and Hix [8]. Very recent work includes, for example, user studies showing the 
advantage of a hierarchy of usability methods in virtual reality by Hix, et al. [14], who conclude that 
navigation ...in a virtual world ...profoundly affects all other user tasks. They also compare ex­ocentric 
viewing to egocentric viewing, and perform experiments on the tradeoffs between providing user control 
of many degrees of freedom versus less control but more convenience. The results support the general 
philosophy of our system designs, which em­phasize situations where constraints enabling convenience 
and ef.­ciency seem to outweigh the freedom of detailed control, although experts may choose to alternate 
between full freedom and assisted modes. The behavior of groups of collaborators and the nature of lead­ership 
in shared virtual environments were studied by Steed et al. [25, 24], who found that, if only one collaborator 
was immersed, that person tended to lead, and that a sense of being in the same place aided collaborative 
performance. Related issues such as the effects of network bandwidth on collaborative tasks in virtual 
re­ality are treated, e.g., in Park and Kenyon [19], while Wartell, et al. [28] have dealt with the challenging 
problem of integrating nav­igation tasks with a large range of scales. 2 Concepts The critical issues 
addressed here include the following: Using constraints to implement assisted exploration. One way to 
view constrained navigation is as a generalization of a conventional animation. An MPEG .le, QuickTime 
movie, or videotape has the property that each camera position in the animation has a unique precedent 
and a unique antecedent, so the exploration of the environment is characterized by a 1D constraint manifold 
parameterized, say, by t, with a viewpoint x(t)and camera model preselected at each parameter value t. 
By playing the animation backward and forward at vari­ous speeds, one explores the space of viewpoints 
preordained by the director of the animation, but has no feeling of free motion or ego-determined control. 
We assert that by adding one or more degrees of freedom to the animation concept, the feeling of ego-determined 
control returns even though the de­signer the director of the generalized animation can in fact continue 
to impose substantial control so the viewer is invisibly assisted in exploring the space according to 
the de­signer s choices. Constrained navigation implements the de­signer s conception of what a viewer 
is supposed to see and learn while giving the exploration a feeling of freedom and self-discovery. Bene.ts 
of Constraints. When the viewer is given complete navigational freedom, he or she becomes in effect the 
direc­tor of a .lm with no script. Without constrained navigation, the best a designer can do is to make 
the important objects stand out and prevent the viewer from running into things; but even this simple 
assistance can be very expensive to imple­ment with traditional methods. The application of constraints 
to assisted exploration, in contrast, can do away with such headaches as collision detection and the 
associated expense entirely simply by restricting the guide manifold to unoccu­pied subsets of the domain. 
The viewer is provided with a self-guided script, and need not worry about getting into or mistakenly 
focusing on unimportant scene areas. Locomotion merged with assisted focus on interest points. We explore 
methods for single users to traverse a constrained space with two principal modes of gaze control. The 
case most like a standard animation has the viewer s gaze prede­termined at each position, so the viewer 
controls the position in parameter space but not the direction of viewing. One vari­ation permits a family 
of prestored gaze directions that may depend on the direction of motion. To provide an additional sense 
of control while preserving context, we also can pre­store the heads-up direction only, and use the remaining 
free­dom to choose the direction of viewing to be the direction of motion, as though driving a vehicle 
over a terrain that deter­mines heads-up. We then supply a separate indicator (typi­cally provided by 
an automated guide avatar) that points to­wards the object of interest at each point (or the selected 
di­rection out of a family of directions keyed to the motion direc­tion); if this hint interests the 
user, the user can stop moving and the gaze will be turned toward the interest point. With ad­ditional 
controller degrees of freedom, one can pivot the gaze at will while continuing to travel in a different 
direction, like looking out the side window of a car.  Gaze interpolation and sense of orientation. 
There are a variety of choices to make when the user stops moving, and when the user resumes motion. 
First, the gaze can be con­strained to the plane perpendicular to the heads-up direction and rotated 
in the general direction of the interest point, but typically not looking directly at it. A second choice 
is to in­terpolate the gaze from the current traveling direction to the actual interest point, wherever 
it may be; this has some ad­vantages for off-track points, but can be more disorienting. Finally, when 
motion resumes, we can either swing back to the remembered prior vehicle orientation, or we can force 
the vehicle to start moving in the direction of the current gaze (which is towards the current interest 
point).  Guide avatars as navigational assistants. Our experience is that individual navigators require 
additional help from an automated assistant to help point out where they should be looking. A simple 
model for this is walking a dog; the user holding the leash in principle chooses the route, where to 
turn, what roads to cross, but the dog may be continually pointing  its nose in various directions and 
tugging away at the leash suggesting alternative choices. While the user has absolute control of which 
sidewalk networks to traverse (i.e., the free­dom permitted in the guide manifold), the dog s more sen­sitive 
perceptions can lead to explorations in directions the leash-holder would not have thought to go. It 
is thus up to the designer of this multiple-degree-of-freedom movie to suggest task-effective exploration 
directions and provide them to the automated guide. Collaborative tools for assisted exploration. Collaboration 
modes occur almost automatically as extensions of the single­user assisted exploration modalities. Just 
as the guide avatar provided by the designer of the prestored environment tugs at the single user, any 
single user can have several followers for which he or she serves the same purpose, focusing their at­tention 
by turning towards signi.cant environmental features. This directly facilitates the goal of collaboration, 
i.e., the shar­ing of information and experiences. Additional richness de­rives from the fact that each 
person has the objective of help­ing the other collaborators achieve new understandings, and exactly 
who is teaching whom may be ambiguous and may change rapidly in a true collaboration. Thus we look at 
ways in which collaborators can have some freedom to wander with varying looseness of the tether to the 
leader, making it possi­ble to break away from the main group and rejoin at the touch of a button, and 
facilitate exchange of leadership roles. The VRML environment as a portable implementation. The original 
prototype of the system described here was im­plemented on the desktop in Iris Explorer and Inventor, 
and then ported for further testing to the CAVETM [4] immer­sive environment using Performer. The main 
advantage of these environments is high performance at the cost of porta­bility and ease of exposure 
to a larger base of users for feedback. As new projects arose with additional needs for portability, 
user-modi.ability of the data models, and long­distance multi-collaborator sharing of visualization experi­ences, 
it became apparent that a more portable implementa­tion was needed. The obvious choice of a relatively 
portable and user-modi.able 3D graphics environment was VRML as supported by a number of plugins for 
Web browsers [13]. A wealth of publicly available models and avatars become available for free, and the 
system is potentially available for a wider variety of future projects and users. Since VRML worlds can 
also be displayed in the CAVE using tools such as Open WorldsTM [18] and cave6u[16], we can also utilize 
our system to some extent in immersive display environments. More signi.cantly, VRML browsers can also 
run on almost any recent PC in addition to high-end SGI workstations that have been the traditional domain 
for such work. There are fundamentally two choices to make in the design decision to use VRML: whether 
to add private enhancements via VRMLScript, a close cousin of JavaScript,which is slow and somewhat awkward 
but widely supported, or via Java and the EAI (External Authoring Interface [7]). It turns out that Java 
and EAI are essential for any activities that must communicate among different machines .uidly for remote 
collaboration; however, if only a single user is involved in a given visualization, vrmlscript s simple 
data exchange interface makes it relatively easy to use and extends its porta­bility. One elegant feature 
of the VRML implementation is that the array of viewing parameters and the guide manifold them­selves 
may, with some labor in the scripting language, be initially stored and even viewed as vanilla VRML objects. 
  Controller Space Domain: Position (u,v); Velocity (u,v)  Figure 2: The constrained navigation approach 
to assisted exploration involves tandem interpolations among sampled parameter sets associ­ated with 
each point of the controller space. Then one has the option of inspecting the designer s navi­gation 
space before enabling it as a constraint system for its associated scene. Constraint manifolds and arrays 
of view pa­rameters simply become VRML .les that are loaded with the scene, but that have a special meaning 
for the navigation con­trollers implemented in Java and/or vrmlscript.  3 Constraints and Assisted Exploration 
In previous work, we described the general framework of con­strained navigation [10] in the context of 
an arbitrarily complex space of scene-viewing parameters attached to each point of a nav­igation manifold 
spanned by the motions of a chosen controller, as shown schematically in Figure 2. Typically, we map 
a controller space of dimension two or greater, (u;v;:::)and its optional ve­locity information (u;v:: 
__;:), into a sampled set of viewing param­eters denoted by the map G(u;v;:::;_v_;:::)from the domain 
u; of the control device to the full space .of parameters. G:(u;v;:::;_v_;:::)7!.:(1) u; The objects 
in the range .include such things as 1. Viewer position on guide manifold: the point in the universe 
where the virtual owner of the device appears to be standing. 2. Viewer gaze orientation: where the 
virtual user is actually looking. 3. Task-driven suggestion for gaze orientation: where the virtual 
user should be looking to perform well on the task at hand. This can be either a direction or a speci.c 
point of interest in the environment. 4. Viewing parameters: focal length (wide angle, telephoto lens), 
depth of .eld, and binocular convergence. 5. Viewing properties: fog, light attenuation, etc. 6. Control 
modi.ers: mouse response, importance weighting, gravity attraction, etc. 7. Visualization application 
parameters: streamline characteris­tics, particle source location, pseudo-color assignments, etc.  
 (a) (b) Figure 3: (a) Each point of the guide manifold permits one single viewing choice, interpolated 
among the sampled values selected by the designer. (b) A compass rose of view parameters is preselected 
at each sample point of the guide manifold to allow a viewpoint keyed to the user s direction of motion. 
 4 Fundamental Methods. 4.1 Single user locomotion/navigation. We begin our presentation of the implementation 
methods of the current system with the single-user navigation paradigms that we have found most useful. 
Experience with assisted navigation proce­dures has led us to work with two fundamental frameworks: Fixed 
Gaze and Motion-Weighted Gaze. Fixed Gaze. There are two subclasses of .xed gaze. The .rst, illustrated 
in Figure 3(a), permits the user no freedom; wherever the controller is positioned, the viewing parameters 
are given uniquely by multidimensional interpolation of the sampled values supplied with the guide manifold. 
Wherever one moves, the gaze direction is predetermined, even if it points in the opposite direction 
to the viewer s motion. This mode is well suited to applications such as molecular inspection that require 
the gaze to be continually read­justed to face towards a complex, often nonconvex, object. The second, 
illustrated in Figure 3(b), is somewhat more clever: by stor­ing a compass rose of .xed directions at 
each sample point, we can use the user s velocity vector to select from an interpolated family of appropriate 
directions. Instead of having the view parameters .xed by the position alone, the parameters are determined 
uniquely by the position and velocity (uv::__;:)taken together. ;;:;u;v:: Motion-Weighted Gaze. In motion-weighted 
mode, the user is imagined to be traveling in a sort of automobile or golf cart, or per­haps pulled by 
a sled dog. The heads-up direction of the viewing frame is .xed at sample points of the guide manifold 
by the de­signer, but the horizontal direction is undetermined. On the desk­top, the user s gaze follows 
the direction of motion as long as the velocity is above threshold, though the guide avatar (which we 
like to choose as a guide dog) may look in the designer-stored direc­tions of interest as they pass. 
In an immersive display environment, the navigation frame follows the same behavior, though of course 
the viewer can look off to the side if desired. When the user slows down, the gaze direction shifts to 
look at the interest point. Two modes are provided when motion resumes: either the original ve­locity 
is remembered and the gaze and motion shift back to the pre­vious values, or the interest-point gaze 
direction becomes the new direction of initial motion and the old parameters are discarded. One other 
option that we have used is to alter the predetermined heads-up direction temporarily when the user is 
stopped, and rotate the entire frame, including the heads-up direction, in the plane of the interest 
direction and the heads-up direction, so the point of in­terest is centered in the .eld of view; the 
predetermined heads-up is recovered when motion resumes. Navigation Formulas. The basic features of the 
navigation en­vironment include several variables. First, we have at each point (u;v)of the guide manifold 
a prior point (u,du;v,dv),sothe velocity direction is the difference (u_=du;v_=dv). The corre­sponding 
point x(u;v)in physical space thus has an instantaneous velocity V(u;v)=x_(u;v).x(u;v),x(u,du;v,dv):(2) 
Other environment variables at each navigation parameter include an interpolated, .xed, heads-up direction 
^ h(u;v), and a gaze point p(u;v)or a gaze direction ^)p(u;v), q(u;v); typically, q(u;v=x(u;v)and we 
use the unit normal direction ^ q=q/kqk. At each point in the journey, the velocity may be computed ap­proximately 
from neighboring differences as in Eq. (2); however, since the heads-up vector may not be related to 
the manifold nor­mal, we need a more speci.c convention. All that is really needed is to use the Gram-Schmidt 
procedure to .nd the part of each vector that is parallel to ^h(u;v)and subtract it: ^,^)q.(u;v)=q(u;v),h(u;v)h(u;v).q(u;v)(3) 
^^ ^^,^^) V.(u;v)=V(u;v),h(u;v)h(u;v).V(u;v);(4) ^ with the corresponding unit vectors ^.q/kqkand V= 
q=...^ V./kV. kas usual. V.(u;v)is the corrected velocity direction in the plane orthogonal to the heads-up 
direction, and should be our standard choice. Rotating about heads-up. To rotate about the heads-up di­rection 
to get as close to the interest-point direction as we can with­out changing heads-up, we .rst project 
to the ground plane using Eq. (3) to get ^.as shown in Figure 4(a). De.ning q ^ cos0=V. .^. q ^^ sign0=signh.(V. 
xq. ^); we .nd the basic linear interpolation in the parameter tto be ^ R(t0;h); (5) wherewetake R(c;^ 
n)to be the standard right-handed 3D rotation matrix about the .xed direction ^by angle c. n Tilting 
to a new heads-up. The alternative interpolation to temporarily relinquish heads-up in favor of centering 
the point-of­interest p(u;v)in the view is schematized in Figure 4(b). We repre­sent each frame as a 
standard OpenGL-like coordinate frame, with   Heads Up Interest Direction Direction Heads Up ^ ^q 
Interest Direction  Direction ^ h  ^h ^ q ^ h q .  ^ . V .  Motion Direction ^ V . ^^  Motion 
Direction q x h (a) (b) Figure 4: Motion-weighted gaze modes. The direction of user travel determines 
gaze direction while moving, but guide avatar may indicate points of interest as they go by. (a) In this 
mode, if the user chooses to slow or stop, the direction of the point of interest is projected to the 
plane perpendicular to the current heads-up direction and the gaze is swung around to align with that 
direction while leaving heads-up invariant. (b) An alternative is to determine two frames, one pointing 
towards the direction of motion, and the other F,1 towards the point of interest, and formulate an interpolation 
between them by .nding the eigenvector of the matrix F2.1 and using that to interpolate between the initial 
and .nal frames. the gaze direction the negative of the zdirection (in the third col-Moving. umn) and 
the heads-up being the positive ydirection (in the second column). Thus, if Fis the frame matrix corresponding 
to looking Fixed Gaze. 1 in the direction of travel at a particular point (u;v),and Fis the ^ q 2 
Single Direction. At each point of the guide manifold, frame formed by tilting in the plane of (^interest-point 
direction ^,we have hq )to gaze directly in the ; only one .xed set of gaze parameters is stored. Whether 
one moves backward, forward, or sideways, the direc­ .. tion of view and the family of viewing parameters 
is ^^^ V . .. hh ^ 1 . V (6) F= x . . , always the same, as illustrated in Figure 3(a). ^^^ 0 hcanbecomputedusingGram-Schmidtasusual, 
hq (7) F ^q, = 0 0 x .. Rose of Directions. A compass-rose of directions and 2 parameter .elds, typically 
four, is stored at each sample h where ^ q ^ ^^h0 where ^=h0/kh0k. The key to the interpolation is 
now to .nd compass rose. a parameterized rotation that starts at Fand follows a great circle Such an 
interpolation is computed starting hh point of the guide manifold, as represented in in Figure ), u;v)is 
used to interpolate (8) 3(b). The travel direction (__ ^q h0 ; = ,. an appropriate value given the sampled 
values on the 1 Motion Weighted Gaze. If full control of the gaze is re­ geodesic path to F. 2 with 
the rotation matrix linquished, we can store data or algorithms determining the heads-up direction at 
every guide manifold sample point, (9) n ^ are easily determined by standard methods [23, 17]. R(0 ),1 
)=F.(F ; while permitting any viewpoints within the remaining rota­ 2 1 tional degree of freedom. The 
mathematical framework for n where 0and ^Then the matrix dealing with such freedom is in fact isomorphic 
to the prob­ lem of assigning frames to surfaces with .xed normal direc­ (10) tions, and was treated 
thoroughly using a quaternion frame­ work in recent work by one of the authors [9, 12]. n ^ is easily 
veri.ed to reduce to Fat t=0and to Fat t=1. M(t)=R(t0; ).F 1 12 hq q Hybrid Tilt. An intuitively pleasing 
variant that seems to com- Watch where you are going. Using the model of a bine the best of both these 
methods is to rotate .rst about the heads-traveling vehicle such as an automobile or golf-cart, the ^ 
 up direction to align with ^, and then tilt by rotating about ^user will want to focus on the road, 
and so the gaze x . will be aligned with the direction of motion consistent qqq using the interpolated 
rotation R(tarccos(^^^ with the .xed heads-up direction. Note that the correct h qhq ^^ ^/k^ direction, 
making overall a kind of L -shaped (11) ); k): plane for the vehicle velocity is determined by interpo­ 
xx. . lating the vertex normals, so the proper velocity is found from Eq. (4) by projecting the actual 
difference between to look in the sweep. 4.2 Taxonomy of Assisted Navigation successive vehicle (u;v)positions 
to that local plane. Be slightly distracted. We assume that some, if not all, points along our route 
of locomotion have directions Most situations appropriate to assisted navigation can be realized of 
interest that may be pointed out to us by the guide with several basic classes of user options, including 
where to look avatar. If it is task-appropriate, we may wish to allow while moving, how to behave when 
stopping, and how to move our gaze to be distracted from the motion direction to­when restarting: wards 
the interesting direction using a certain percent­age or weight that would logically depend on our veloc­ity. 
Dual gaze and motion control. In an immersive environment as well as in a desktop system with multiple 
controls, one can alternatively modify one s gaze direction while continuing to move independently in 
a different direction. Stopping. Force the gaze to preserve heads-up. Assuming that a pleas­ant, scene-oriented 
heads-up constraint has been supplied for the guide manifold, we could assume that, wherever we are, 
we have only the freedom to pivot about that .xed (interpo­lated) direction determined by the sampled 
grid values. In this case, an interest-direction above or below the viewer s plane cannot be gazed at 
directly, but must be approximated; nor­mally this involves projecting the gaze to the user plane and 
pivoting to face in the direction of this projection, as shown in Figure 4(a).  Gaze in the direction 
of interest. If we are willing to risk the disorientation of losing the heads-up direction temporarily 
in the interest of having the object of attention centered in the view, we can perform a more complex 
view interpolation when the viewer stops to look. In this case, we determine the plane containing the 
desired gaze direction and the heads­up direction and force the temporary, new heads-up to lie in that 
plane. This determines all the new frame parameters, and we simply do a frame interpolation directly 
to this orientation starting from the original traveling frame, as shown in Figure 4(b).  Restarting. 
 Remember prior direction. Using the model of a vehicle as our mode of travel, we can assume the vehicle 
has stopped but has not turned. Thus no matter where we turned our head to look, the vehicle will start 
moving again in its old direction, and we will swing our heads gradually to align with that direc­tion 
as we gain velocity. The vehicle remains .xed as shown in Figure 5(a); only the view direction changes. 
 Redirect towards interest point. Alternatively, the vehicle can turn with us as we stop, as shown in 
Figure 5(b). The in­terpolation is computed according to Eq. (5) which was used also to compute Figure 
4(a). Then when we restart our jour­ney, there is no memory of the prior state, and we trundle on in 
the direction of the interesting object we stopped to observe.  The utility of these modes cannot be 
evaluated universally; the ap­propriateness depends on the task and context.  5 Guided Collaboration 
The basic environment that we have described embodies a natu­ral extension to a collaborative system. 
The assisting features of the guide avatar may be exploited in the extension to collaborative viewing. 
Among the relevant properties we note the following: Tracking and Watching. The leader s guide appears 
in the center of the view of the leader, but the leader appears in the center of the view of the followers, 
so as the leader tracks the guide, the collaborators track the leader. Each human leader s avatar is 
seen by the attached collaborators in a central loca­tion; if the leader is immersed in a system with 
a headtracker, the followers see the leader s head motions much as the leader can see the pointing motions 
of the automated guide avatar. And each following collaborator can in turn be a leader for another group 
of followers, who will see him or her in the leadership position, as shown in Figure 1. Sharing Points 
of Interest. When the leader stops to check a point of interest, all the following collaborators either 
turn in place, or swing, attached to the leader, to face that direc­tion while remaining within the legal 
guide manifold. This avoids many troublesome navigation problems for the collab­orators, since they can 
pay visual attention to what is being emphasized without extra navigation effort to get in a good position. 
In regions where the designer knows in advance that collaborative viewing of an object will take place, 
extra care can be taken to construct an arena a bowl-like area in the guide manifold that will let the 
collaborators place themselves in good viewing positions as though sitting on a hillside above the guide. 
Alternatively, in applications like molecule inspec­tion that may require gaze directions essentially 
orthogonal to the guide surface, it may be better to use a model in which the collaborators swing out 
of the guide manifold behind the leader (see, e.g., Figure 7(b)).  Tethering Freedom. Of course, there 
will always be some situations in which the collaborators will have to jockey their positions to get 
a point of view, but each has some degrees of control of their situation as well. Returning to the leash 
anal­ogy, the collaborators following a particular leader are treated as though constrained by elastic 
leashes. The can wander from side to side, pull themselves up close behind the leader for a better view, 
or drop back. One mode permits the following collaborators to detach completely from the leader and navi­gate 
the guide manifold with their own guide avatar if they are not interested in what the leader is pointing 
out. A press of the button re-attaches the leash or tether, and the straggler re­joins the group with 
a slow-in slow-out camera motion similar to the standard VRML browser s transitions among prestored viewpoints. 
 Speed Sensitivity When traveling at reasonable speed, the viewer will be moving in a particular direction 
and will be centered directly behind the local guide. Regardless of the speed, the human or automated 
guide avatar will typically turn its head or pointer to indicate interesting things that are being passed 
by. If the navigator slows below a threshold, all are coerced to look in the preferred direction. Upon 
resuming speed, travel resumes according to the chosen options from Figure 5. The redirection mode of 
Figure 5(b) may be pre­ferred for continuous over-the-shoulder collaborative viewers.  Intelligent Dragging. 
If a collaborative tethering paradigm is too rigid, the following viewers can easily bump into walls 
or get disoriented. For terrain-like environments, our system keeps the viewer tied to the constraint 
manifold at a .xed dis­tance and horizontal angle from the guide, thus eliminating the need for expensive 
collision detection, and permits bring­ing the reins up tight, wandering loosely behind the leader, or 
detaching and requesting an automated return later. For molecule-like environments, or situations where 
the collabo­rators need to look from a viewpoint that is behind their guide, but cannot be exactly on 
the guide manifold because the guide is facing perpendicular to the guide surface, the collaborators 
can be constrained to keep a .xed relationship to the guide per se, rather than being constrained to 
the navigable mani­fold; the leading guide, however, retains context and keeps a position on the constraint 
manifold while staying in full view of the collaborators.    Interest Direction ^ q . .  Interest 
Direction -> new motion direction ^ q . 000000000111111111 000000000111111111 111111111 111111111 
^ 111111111 V .  Motion Direction old motion direction (a) (b) Figure 5: Restart modes after stopping 
to inspect an interest point. (a) Leave vehicle pointing in original direction, turning only the gaze 
when inspecting the interest point; continue in original direction of travel when resuming motion. (b) 
Rotate the vehicle in the ground plane, basically as in Figure 4(a), to align as closely as possible 
with the gaze direction, and then resume motion in that direction, forgetting the original direction. 
6 Examples Terrain Following. In Figure 6, we show examples of an envi­ronment represented by a 3D terrain 
elevation map. This environ­ment is distiguished by the assignment of a .xed heads-up vector to each 
point of the plane, typically either oriented with gravity or with the pointwise terrain normal vectors. 
Each motion produces a new camera model constructed from the prestored normal and the shape of the object 
being navigated. Molecular Examination. We .rst get a hint of the value of con­strained navigation when 
we examine the cumbersome environment of a large molecule in Figure 7. Here, choosing the heads-up direc­tion 
requires signi.cant designer attention to match the particular application, which might be the examination 
of a protein binding site or unusual geometry. Non-Convex Buildings. Standard 3D viewers attempt to see 
all aspects of an object by rolling it over in your virtual hand. The critical problem with this approach 
is that it does not account for various non-convex nooks and crannies which could keep the viewer in 
a standard examiner from seeing important details. In Figure 8, we show how a bubble-like navigation 
.eld can settle over a very nonconvex building or an entire set of buildings without any need for complex 
user actions or prior knowledge of the interest points. 7 Conclusion and Future Work We have implemented 
a designer-assisted approach to the problem of exploring and visualizing a virtual space that permits 
a natural extension to shared collaborative experiences. The paradigm ex­ploits positional and camera-model 
constraints on navigation, as well as taking advantage of additional variables such as interest­points 
to provide extra assistance that focuses the user s explo­rational wanderings on the task objectives. 
Our speci.c de­sign incorporates not only task-based geometric constraints on the viewer s location and 
gaze, but also a personal guide that serves two important functions: keeping the user oriented in the 
naviga­tion space, and pointing to interesting subject areas as they are approached. Collaboration in 
a particular style is supported by recursively attaching following collaborators to a guiding avatar, 
which may be either an automaton or a human. By implementing the constraint con.gurations as VRML objects 
with additional nav­igational interpretation, we achieve a new level of portability and availability. 
Among the areas remaining for future work, we note: Scaling. Scaling up to large applications such as 
the entire earth requires the introduction of interesting new complexities such as multiresolution constraint 
grids. See [28] for an initial attempt at this problem.  Time Evolving Systems. Data, points of interest, 
and guide manifolds that vary with time present additional challenges due to the requirements imposed 
by additional dimensions of interpolation and context knowledge. An example of such a problem would be 
modeling an arctic hiker traversing an ice .ow that is in the middle of breaking up and drifting away 
in pieces.  User Testing. Gathering user data on effectiveness of the var­ious navigation modes is currently 
in progress. Speci.c mea­surement and data-recording tools need to be added to sup­port this task. A 
quite advanced system for supporting such inquiries has been presented by Hix et al., [14].  Collaborative 
Testing. Collaboration data are particularly dif.cult to come by because of the challenge of .nding groups 
of users with an appropriate problem and resources. Several projects currently underway in our laboratory, 
including work on immersive exploration of astrophysical data, are antici­pated to provide an appropriate 
framework. A sampling of the questions speci.cally related to virtual collaboration that need to be understood 
has been studied recently by Tromp, et al., and Steed, et al. [25, 24].   Acknowledgments This research 
was made possible in part by NSF infrastructure grant CDA 93-03189 and the support of the Indiana University 
Advanced Information Technology Laboratory. References <RefA>[1] M. Billinghurst and J. Savage. Adding intelligence 
to the in­terface. In Proceedings of VRAIS 96, pages 168 175, 1996. [2] D. Bowman, D. Koller, and L.F. 
Hodges. Travel in immer­sive environments: An evaluation of viewpoint motion con­trol techniques. In 
Proceedings of VRAIS 97, pages 42 52, 1997. [3] D. Bowman, D. Koller, and L.F. Hodges. A methodology 
for the evaluation of travel techniques for immersive virtual environments. Virtual Reality: Journal 
of the Virtual Reality Society, 3:120 131, 1998. [4] Carolina Cruz-Neira, Daniel J. Sandin, and Thomas 
A. De-Fanti. Surround-screen projection-based virtual reality: The design and implementation of the CAVE. 
In James T. Ka­jiya, editor, Computer Graphics (SIGGRAPH 93 Proceed­ings), volume 27, pages 135 142, 
August 1993. [5] R. P. Darken and J. L. Sibert. Way.nding strategies and be­haviors in large virtual 
environments. In Proceedings of Hu­man Factors in Computing Systems (CHI 96), pages 142 149, 1996. [6] 
S. M. Drucker, T. A. Galyean, and D. Zeltzer. Cinema: A sys­tem for procedural camera movements. In Computer 
Graph­ics, pages 67 70, 1992. Proceedings of 1992 Symposium on Interactive 3D Graphics. [7] EAI. External 
authoring interface. A Java interface speci.cation for VRML; see http://www.vrml.org/WorkingGroups/vrml-eai/Speci.cation/ 
for more information. [8] Joseph L. Gabbard and Deborah Hix. A taxonomy of usability characteristics 
in virtual environments. Re­port for ONR, November 1997. Obtainable from http://csgrad.cs.vt.edu/ jgabbard/ve/taxonomy. 
[9] A. J. Hanson. Constrained optimal framings of curves and surfaces using quaternion gauss maps. In 
Proceedings of Visu­alization 98, pages 375 382. IEEE Computer Society Press, 1998. [10] A. J. Hanson 
and E. Wernert. Constrained 3D navigation with 2D controllers. In Proceedings of Visualization 97, pages 
175 182. IEEE Computer Society Press, 1997. [11] A. J. Hanson, E. Wernert, and S. Hughes. Constrained 
nav­igation interfaces. In Hans Hagen and Hans-Christian Ro­drian, editors, Scienti.c Visualization. 
Springer Verlag, 1999. To appear in collection based on proceedings of Dagstuhl 97 Workshop on Scienti.c 
Visualization. [12] A.J. Hanson. Quaternion gauss maps and optimal framings of curves and surfaces. Indiana 
University Computer Science Department Technical Report 518 (October, 1998). [13] Jed Hartman and Josie 
Werneke. The VRML 2.0 Handbook. Addison-Wesley, 1996. [14] Deborah Hix, J. Edward Swan II, Joseph Gabbard, 
Mike McGee, Jim Durbin, and Tony King. User-centered design and evaluation of a real-time battle.eld 
visualization virtual environment. In Proceedings of IEEE VR 99, pages 96 103, 1999. [15] J. D. Mackinlay, 
S. Card, and G. Robertson. Rapid controlled movement through a virtual 3D workspace. In Computer Graphics, 
volume 24, pages 171 176, 1990. Proceedings of SIGGRAPH 1990. [16] Swaminathan Narayanan. cave6u. A VRML 
browser for the CAVE. A description is available at http://www.evl.uic.edu/swami/cave6u/vrml.html. [17] 
G. M. Nielson. Smooth interpolation of orientations. In N. M. Thalman and D. Thalman, editors, Computer 
Animation 93, pages 75 93, Tokyo, June 1993. Springer-Verlag. [18] OpenWorlds. A commercial VRML browser. 
Information is available at http://www.openworlds.com. [19] Kyoung Shin Park and Robert V. Kenyon. Effects 
of network characteristics on human performance in a collaborative vir­tual environment. In Proceedings 
of IEEE VR 99, pages 104 111, 1999. [20] C. B. Phillips, N. I. Badler, and J. Granieri. Automatic view­ing 
control for 3D direct manipulation. In Computer Graph­ics, pages 71 74, 1992. Proceedings of 1992 Symposium 
on Interactive 3D Graphics. [21] J.S. Pierce, A. Forsberg, M.J. Conway, S. Hong, R. Zeleznik, and M.R. 
Mine. Image plane interaction techniques in 3d immersive environments. In Computer Graphics, pages 39 
43, 1997. Proceedings of 1997 Symposium on Interactive 3D Graphics. [22] W. Robinett and R. Holloway. 
Implementation of .ying, scal­ing, and grabbing in virtual worlds. In Computer Graphics, pages 189 192, 
1992. Proceedings of 1992 Symposium on Interactive 3D Graphics. [23] K. Shoemake. Animating rotation 
with quaternion curves. In Computer Graphics, volume 19, pages 245 254, 1985. Pro­ceedings of SIGGRAPH 
1985. [24] A. Steed, M. Slater, A. Sadagic, A. Bullock, and J. Tromp. Leadership and collaboration in 
shared virtual environments. In Proceedings of IEEE VR 99, pages 112 115, 1999. [25] J.G. Tromp, A. Steed, 
E. Frecon, A. Bullock, A. Sadagic, and M. Slater. Small group behaviour in the COVEN project. IEEE Computer 
Graphics and Applications, 18(6):53 63, 1998. [26] C. Ware and D. Fleet. Context sensitive .ying interaction. 
In Computer Graphics, pages 127 130, 1997. Proceedings of 1997 Symposium on Interactive 3D Graphics. 
[27] C. Ware and S. Osborne. Exploration and virtual camera con­trol in virtual three-dimensional environments. 
In Computer Graphics, volume 24, pages 175 184, 1990. Proceedings of 1990 Symposium on Interactive 3D 
Graphics. [28] Z. Wartell, W. Ribarsky, and L. Hodges. Third-person navi­gation of whole-planet terrain 
in a head-tracked stereoscopic environment. In Proceedings of IEEE VR 99, pages 141 148, 1999</RefA>. (a) (b) 
Figure 6: Two modes of group terrain-traversal. (a) Constrained to the guide manifold using surface normal 
for heads-up.  (b) Using gravity for heads-up. Viewer on right respects the heads-up constraint of Figure 
4(a).  (a) (b) Figure 7: A topologically complex guide manifold. (a) Collaborators cluster around leader 
constrained to surface. (b) Collaborators hover with .xed orientation to leader, who remains in guide 
surface. (a) (b) Figure 8: Nonconvex structure that confounds examiner browsing mode. (a) Moving to 
the left gate, gaze is on left gate. (b) As we move to the right gate, the navigation constraints effortlessly 
shift the gaze.  
			
