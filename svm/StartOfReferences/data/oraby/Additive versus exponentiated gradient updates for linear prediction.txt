
 Additive Versus Exponentiated Gradient Updates for Linear Prediction Jyrki Kivinen* Manfred K. Warmutht 
Department of Computer Science Computer and Information Sciences P.O. Box 26 (Teollisuuskatu 23) University 
of California, Santa Cruz FIN-00014 University of Helsinki, Finland Santa Cruz, CA 95064, USA jkivinen@cs.helsinki 
.fi manfredtlcse.ucsc. edu Abstract We consider two algorithms for on-line prediction based on a linear 
model. The algorithms are the well-known Gradi­ ent Descent (GD) algorithm and a new algorithm, which 
we call EG *. They both maintain a weight vector using sim­ ple updates. For the GD algorithm, the weight 
vector is updated by subtracting from it the gradient of the squared error made on a prediction multiplied 
by a parameter called the learning rate. The EG* uses the components of the gra­ dient in the exponents 
of factors that are used in updating the weight vector multiplicatively. We present worst-case on-line 
loss bounds for EG* and compare them to previ­ ously known bounds for the GD algorithm. The bounds suggest 
that although the on-line losses of the algorithms are in general incomparable, EG * has a much smaller 
loss if only few of the input variables are relevant for the predic­ tions. Experiments show that the 
worst-case upper bounds are quite tight already on simple artificial data. Our main methodological idea 
is using a distance function between weight vectors both in motivating the algorithms and as a potential 
function in an amortized analysis that leads to worst-case loss bounds. Using squared Euclidean distance 
leads to the GD algorithm, and using the relative entropy leads to the EG* algorithm. 1 Introduction 
The goal of this research is to obtain strong worst-case loss bounds for simple on-line algorithms by 
using amortized analysis. Consider the problem of on-line learning of lin­ear threshold functions. There 
are two simple algorithms for this problem: the classical Perception algorithm [Ros58] and the more recent 
Winnow developed by Littlestone [Lit88]. The algorithms are similar in that they maintain one weight 
per dimension and use a linear threshold function to predict. However, the updates of their weights are 
radically differ­ent. Of particular interest to us is the class of (monotone) ± Supported by Emil Aaltonen 
Foundation and the Academy of Finland. This work was done while the author was vmtmg University of California, 
Santa Cruz. ts pported by NSF grant IRI-9123692. Permission to copy without fee all or part of this 
material is granted provided that the copies are not made or distributed for direct commercial advantage, 
the ACM copyright notice and the title of the publication and 4s date appear, and notice is given that 
copym ISby permission of the Association of Computmg Machinery. o cop otherwise, or to republish, requires 
T{ a fee andlor speci ICpermission. STOC 95, Las Vegas, Nevada, USA 01995 ACM O-89791-718-9195JO005..$3.5O 
k-literal disjunctions over N variables, where we expect N to be significantly larger than k. This class 
is a simple sub­class of linear threshold functions: if the instances x are N-dimensional Boolean vectors, 
i.e., x c {O, 1 }N, then the k-literal disjunction Z,l V X,2 V . . . V ~ik corresponds to the linear 
threshold function w . x z 0, where w is a coefficient vector with W,l = w,= = -.. = w,~ = 1 and wj = 
Ofor jf?{il, . . . . Zk }, and the threshold O is 1. The on-line algorithm Winnow is guaranteed to make 
at most O(k + k log (N/k)) prediction mistakes [Lit 88] when learning k-literaJ disjunctions. This mistake 
bound is optimal to within a constant factor since the Vapnik-Chervonenkis (VC) dimension [VC71, BEHW89] 
of the class of k-literal disjunctions is C2(k + k log(ff/k)) [Lit88] and this dimension is always a 
lower bound for the optimal mistake bound. On the other hand, an adversary can force the Per­ception 
algorithm to make f2(N k) mistakes [K W95]. Thus when k is small, the mistake bound of the Perception 
algo­rithm is exponential in the optimal mistake bound (in this case essentially the VC dimension). This 
may be seen as an instance of what is called the curse of dimensionality in the literature of neural 
networks and statistics [Hay93, DH73]. To understand the different behavior of both algorithms, we retreat 
to the simpler problem of linear regression. Now the algorithms simply predict with the scalar product 
of the weight vector and the input vectors rather than with a classification obtained by t hresholding 
the scalar product. Again, there are two fundamentally different algorithms. The first algorithm is the 
classical Gradient Descent algo­rithm (GD), in this context also known as the Widrow-Hoff rule and Least 
Mean Squares algorithm [WS85, Hay93]. It can be motivated as a gradient descent on the instantaneous 
loss of the algorithm and corresponds to the Perception algorithm. In this paper we introduce several 
new algo­rithms that apply the basic ideas of Winnow to the linear regression problem. The most basic 
one is the Exponen­ tiated Gradient (EG) algorithm, from which we obtain the main algorithm called EG 
± by a simple reduction. We prove worst-case loss bounds for both the GD and EG* akorithms using amortized 
analysis. This has already been lone for GD [CBLW93]. The focus in this paper is to prove similar bounds 
for the new algorithm EG* and then contrast the bounds and algorithms. Since the regression problem is 
continuous in nature, we can see connections that are less apparent in the thresholded classification 
problem. The first key observation we make is that both the GD and EG* algorithms can be motivated as 
approximate solutions to a minimization problem that has as one component the distance between the new 
and old hypothesis of the algorithm. The solution depends on the choice of the distance function between 
the hypotheses, which in our case are N-dimensiomd real weight vectors. Using the squared Euclidean distance 
leads to the GD al­gorithm, and using the relative entropy leads to the EG* algorithm. The second keyobservation 
is that the distance function that motivates the update of the weight vector gets a second use as apotentird 
function for the amortized anal­ysis of the learning algorithm employing that update. In the full paper 
we derive a number of additional algorithms using this framework and prove loss bounds for them using 
similar techniques [K W94]. As with the Perception al orithm and Winnow, the per­ % formances of the 
GD and EG algorithms are radically dif­ferent, the EG* algorithm performing better when only few of the 
input variables are relevant. As a simple example, as­sume that the value to be predicted is the sum 
of k variables, out of a total of N variables, and the variables assume only values 1 and 1. This is 
the continuous-valued analogue of k-literal disjunctions. The total squared error of the predic­tions 
made by the EG* algorithm is 0(k2 log iV). The GD algorithm can incur a total squared error of fl(k~), 
which is much larger when N > k. Note again the logarithmic ver­sus the linear growth in the input dimension 
N. Our bounds are worst-case bounds, but our experiments show that the worst-case bounds are quite tight 
even on simple random data. The bounds can be generalized to situations in which no linear combination 
of the input variables can explain the values to be predicted, either because of noise or because of 
nonlinear dependencies. The general forms of the loss bounds for the algorithms are rat her complicated, 
so in the int reduction we only con­sider the basic ideas of the bounds. Consider a situation in which 
the learner is given inst antes xt, t = 1, 2, ..., and tries to predict outcomes yt. Assume now that 
some co­efficient vector u is a good predictor, i.e., the scalar prod­uct u . xt in some sense tends 
to be close to the outcome yt. Our loss bounds for both algorithms can now be stated in terms of the 
norm IIul I of the good predictor and the largest norm max~ Ilx~ II of the instances. However, for differ­ent 
algorithms the bounds depend on different norms. The bounds for GD depend on a product X2 Uz, where X2 
is an upper bound for the largest Euclidean norm maxt IIxt112of the instances and U2 is an upper bound 
for the Euclidean norm I Iul 12. For the EG* algorithm, the bounds depend on the product XM U1, where 
Xm and U1 are upper bounds for the Lw and L1 norms, respectively. As the products X2 U2 and XQ U1 are 
incomparable, we expect that also the per­formances of the algorithms are incomparable. Indeed, it is 
easy to construct cases in which the GD algorithm outper­forms the EG* algorithm. However, we feel that 
these are less natural than the cases that favor EG* over GD. Note that the above pairs of norms corresponding 
to both algorithms are dual norms [Roy63]. We can give a lower bound that contains the product of an 
arbitrary pair of dual norms. We conjecture that there are regression algorithms for arbitrary pairs 
of durd norms with loss bounds close to our lower bounds. Finding these algorithms and the distance functions 
that motivate them within our framework is one of the key open problems. However, we believe that the 
special cases discussed here for the pairs (L2, ~z) and (LM, L1 ) are the fundamental ones. We were surprised 
to find that the method of amortized analyses can be such a powerful tool in the comparative study of 
simple machine learning algorithms. We have suc­cessfully applied our framework to an unsupervised learn­ing 
problem [HSSW95] and to temporal difference learning [SW94]. In each case our framework can be used to 
derive the algorithms as well as proving worst-case loss bounds for them. Whenever there is an algorithm 
based on Gradi­ent Descent our method leads to a competitor that is de­rived using the relative entropy. 
For example, our method gives an alternative to the back-propagation algorithm for training feed-forward 
neural networks. Whenever there are local minima, it seems impossible to prove worst-case up­per bounds. 
However, we have some preliminary results [HKW95] that show that our amortized analysis technique can 
give worst-case loss bounds for a single linear neuron with a sigmoid activation function. The latter 
problem does not have local minima if we use the entropic loss instead of the square loss. We are looking 
for high dimensional natural problems that would bring out the advantages of the new algorithms. The 
logarithmic growth of the loss bounds with the dimen­sion makes the following approach feasible. Try 
a large set of basis functions (non-linear functions of the original inputs) and do a pass over the data 
with EG* training one weight per basis function. Then exchange basis functions which have small remaining 
weight with new guesses of good ba­sis functions and iterate. The new basis functions could, for example, 
be derived from basis functions that received a large weight in the last iteration. At the end of the 
paper we give an experimented comparison of the two competing algorithms for a case when the instances 
are expanded to a large set of basis functions and the target uses only a few of them. We propose a systematic 
study of on-line learning algo­rithms using amortized analysis. The learning algorithms should be derived 
and analyzed using as few parameters as possible. We propose to use the potential function as the main 
parameter which encodes a way of searching through the solution space. The goal is to explain most simple 
learn­ing algorithms by simply changing the potential function. For example, for a simple mixture problem 
[HSSW95] we were able to reverse-engineer the potential function deriving the algorithm corresponding 
to another statistical technique called Estimate Maximize. Going to a continuous domain led to the development 
of a unifying framework which lets us see new connections. It seems that all basic algorithms are incomparable 
and in the linear case our loss bounds can be used to predict which algorithm will do best. The full 
version of this paper [KW94] contains additional theoretical and experimental results. 2 Framework of 
analysis A linear on-line prediction algorithm is an algorithm that predicts real outputs from N-dimensional 
real inputs accord­ing to a certain protocol. In this protocol, the predictions are made in a sequence 
of.4 trials for some positive integer L At the beginning of the trial sequence, the algorithm chooses 
some start vector WI c RN. Then at trial t, for t= 1, ....t , the algorithm 1. receives the instance 
xt c RN, 2. gives its prediction jt = wt . xt, 3. receives an outcome yt, and 4. chooses for the next 
trial the vector w~+ I G RN that may depend on wt, xt, and yt.  Thus, the weight vector w~ can be 
considered the algo­rithm s hypothesis before trial tfor the best way of predict­ing the outcome as a 
linear combination of the components of the instance. The mapping that gives wt+l as a function of Wt, 
Xt, and yt is called the update rule of the algorithm. For the purpose of the worst-case analysis we 
are in­terested in, the on-line prediction protocol can be consid­ered a game between the prediction 
algorithm and an ad­versarial environment. The algorithm may choose its start vector and update rule, 
the environment may choose the instances and outcomes subject to some mild constraints. For scoring the 
game, we use a loss function L that is a mapping from R2 to [0, co). In a trial sequence S = ((X1, W),..., 
(x4, 9/)), the total loss of the algorithm is de­ fined to be LOSSL(A, S) = ~~=1 L(y~, j,). Analogously, 
the tot$ loss of a fixed weight vector u is given by LOSSL(U, S) = 2,=1 L(lh,Ux t). We omit the subscript 
L if the 10SS func­tion is the square 10SSgiven by L(y, z) = (y z)z, which is the case for the main results 
of this paper. For the square loss, the value Loss(u, S) is the total loss of the solution inf ydtN to 
the linear east squares problem defined by the instances and outcomes. More generally, we are interested 
comparing the loss of the algorithm to the value infueu LOSSL (u, S) for some comparison class U c RN. 
We sometimes refer to the vectors u c U as comparison vectors. The value inf u~u LOSSL (u, S) is the 
lowest loss that COUIC1be achieved if it were Dossible to choose in advance the best comparison vector 
for predicting. The algorithm cannot see the whole sequence in advance, and therefore incurs an additional 
loss LOSSL (A, S) infucu LOSSL (u, S) as it searches for the best vector u and makes on-line predictions 
while it searches. Our goal is to obtain a small additional loss for interest­ing comparison classes 
when the trial sequence satisfies some mild constraints. Recall that for p G (O, CO), the LP norm for 
vectors in RN is defined by Ilwllp = (~~1 Iw,lp) l P; this is generalized for p = cm by IIwIIM = max, 
Iw,]. We take the comparison class U to consist of all the vector u c RN that have Lp norm at most U, 
for some p and U. Similarly, we assume that the instances xt have a bounded La norm for some g. We consider 
the case p = q = 2 and the case p = 1, q = co. These pairs of norms satisfy I/p + l/q = 1 and are hence 
In known duaJ [R.oy63]. addition to norms, we as Kullback-Leibler dis use tance) the relative en tropy 
(also N dRE(U, S) = ~ U, h : (1) ,=1 between vectors u and s. Using the relative entropy requires that 
the vectors have only positive components, and the components of each vector sum to 1. Subject to the 
norm constraint, the instances can be ar­bitrary. We make no assumptions about the process that generates 
them. Neither do we make assumptions about how the individual outcomes and inst antes relate to each 
other. If there is no reasonably accurate linear relationship between the instances x~ and the outcomes 
~t, then the loss LOSSL (A, S) of the algorithm is expected to be high. How­ever, in that case the loss 
infueu LOSSL (u, S) of the best comparison vector will be high as well, so it is still feasi­ble to try 
to achieve a small additional loss LOSSL (A, S) inf Ueu LOSSL (u, S). This is in contrast to more common 
approaches where statistical assumptions about the distri. bution of the instances and the dependence 
of the outcomes on the instances are applied in deriving probabilistic loss bounds for the prediction 
algorithm [Hay93, WS85]. The research reported in this paper was inspired by Littlestone [Lit 88, Lit 
89], who proved worst-case on-line loss bounds for the case of thresholded linear functions. Recently, 
there has been some work on the special case where the comparison class consists of the N unit vectors 
(0,...,0,1,0,..., O) [LW94, VOV90, CBFH+95, HKW94]. The immediate predecessors of this work are the papers 
by Cesa-Bianchi et al. [CBLW93] and Littlestone et al. [LLW91] on linear on-line prediction. 3 The algorithms 
For simplicity, we introduce the algorithms only for the square loss, although they can easily be generalized. 
In all the algorithms we consider here, the update is based on the gradient of the loss at trial t. We 
define for this gradient a shorthand notation by v, = Vw, qyt,wt . Xt) . (2) For the square loss we 
therefore have a(yt Xt)z Wt . Vt,, = = 2(jt yt)zt,: (3) awt,, where jt = wt . xt. Notice that for any 
loss function, the vector Vt is parallel to the instance xt. The other basic ingredient of the updates 
we consider is a positive iearning rate qt, which typica13y depends on some norm of the in­stance Xt 
and some parameters fixed at the beginning of the trial sequence. One of the simplest learning algorithm 
is the Gradient Descent (GD) algorithm for linear prediction, also known as the Widrow-Hoff algorithm 
and the Least Mean Squares algorithm. Using the gradient Vt given in (3), the update of the GD algorithm 
can be written as wt+l = wt qtVt, or componentwise as Wt+l,: = wt, z ?ltVt,i . (4) Thus, the weight 
vector moves to the direction of the steep­est descent of the loss by an amount that depends on the learning 
rate. Note that if the initial weight vector of the algorithm is W1, we have w~ = WI + ~~jl atxt for 
some real coefficients at. Typically, one COU1~-choose the zero start vector WI = O. The choice of the 
learning rates qt can have a great effect to the performance of the algorithm. We shall discuss this 
in the context of our upper bounds in Section 5. A simple example could be qt = l/(411xt 112). In this 
paper we contrast the GD algorithm with a new on-line predict ion algorithm, which we call the Ezponen­tiated 
Gradient (EG) algorithm. This basic algorithm uses weight vectors wt for which wt,, >0 for all i and 
~fll wt,, = 1. After the basic EG algorithm we consider variants with­out these restrictions, most importantly 
one which we call EG*. The EG algorithm is closely related to the algorithm of Littlestone et aL [LLW91]. 
The update rule of the EG algorithm is given by -qtv~,, wt, te W+l, z= (5) ~~=1 wt,,e-~tvt, Thus, the 
ith component V~,, of the gradient defined in (2) now appears in the exponent of the factor that mul­tiplies 
wt,,, and the weights are normalized to sum to 1. 4 Motivating the algorithms A typical start vector 
could be the uniform vector WI = (l/N,..., l/N). Again, the learning rates qt shall be dis­cussed in 
Section 5. A very basic setting would be qt = 2/(3Rt) where Rt = max, rt,: rein, zt,, gives the range 
of the components of the tth instance. The components Win,, of the weight vector wm satisfy in Win,, 
= in wl,, + ~&#38;l at~t,: b, where the at are again some real coefficient as }or the GD algorithm and 
the value b gives the proper norm-ahzation. Hence, the EG algorithm is similar to applying the GD algorithm 
to the logarithms of the weights and maintaining the normalization of the weights. However even if we 
ignore the normalization, the EG algorithm is not the same as gradient descent on the logarithmically 
parameterized weights, since the predictions are made using the weights and not their logarithms. An­other 
comparison between the algorithms can be made by observing that for the GD algorithm, the change caused 
by the update in the differences between the weights can be written as wt+l,, wt+l,j = (wt,t -wt,~) 
-qt(vt,i Vt,J ). For the EG algorithm, it is more useful to con­sider the ratios of the weights and 
write wt+l,: /wt+l,J = -W(vt, z-vt,j). (wt,t/wt,j)e The EG algorithm maintains the normalization property 
~~1 w,,, = I and, in addition, keeps all the weights wt,i posltwe provided that they are positive initially. 
These con­straints are of course inappropriate in many situations. To allow the algorithm to use unnormalized 
weight vectors, one can simply omit the normalization factor from the update rule (5) and apply the update 
wt+l,, = wt, te V vt, . This leads to a quite reasonable algorithm, called EGU, that is analyzed in the 
full version of the paper [KW94] and is closely related to the algorithm Winnow of [Lit 88]. However, 
even this unnormalized algorithm never changes the sign of a weight. Therefore, we introduce an algorithm 
which we call EG*. This algorithm is basically EG with a simple standard transformation for the instances. 
To understand the idea of EG*, consider a vector u E RN with IIul II ~ U for some given parameter U >0. 
For ;=1 ,. ... N, let UT = u, if u, >0 and u? = O otherwise, and u: = u, if w < 0 and u; = O otherwise. 
Define a vector u E [0, U] 2N by setting u; = U~ + (U llulll)/(2~) anduk+, = u:+(U llul ll)/(2N) fori= 
1,. ... N. We call this u the norm U representation for u. If we now define u = u /U, we have u: ~ O 
for all z and Ilu lll = 1, so U is in the class of possible hypotheses for EG. Further, if for ZNby ~: 
= Uxt and X N+t = x c RN we define x E R Ux, fori= l,..., N,wehave u.x=u . forallxlx. We now define EG* 
to be the algorithm that at trial t 1. receives the tth instance xt and gives the vector x; = , ~~~~~~1 
~~ ~~e%&#38; $%hmux 12N) a the tth 2. gives as its tth prediction jt the tth prediction of the EG algorithm 
(i.e. &#38; = wt .x; , where wt is the current weight vector of EG), and 3. receives the tth outcome 
yt and gives it as the tth out­come to the EG algorithm which updates its current weight vector wt. 
 Note that in addition to the start vector and learning rates for the EG algorithm, there is the additional 
parameter U that needs to be specified. Both the rdgorithms GD and EG can be motivated using a common 
framework. In making an update, the algorithm must balance its need to be conservative, i.e., retain 
the in­formation it has acquired in the preceding trials, and to be corrective, i.e., to make cert tin 
that if the same instance were observed again, the algorithm could make a more ac­curate prediction, 
at least if the outcome is also the same. Thus, with an old weight vector wt, it is a reasonable goal 
for the algorithm to minimize M(wt+l) = d(wt+l, wt) + Ttq?h, wt+l m) , where d(wt+l, wt ) is some measure 
of dist ante from the old to the new weight vector, L is the loss function, and the magnitude of the 
positive constant qt represents the impor­tance of correctiveness compared to the importance of con­servativeness. 
The minimization problem would be solved by setting ~lf(wt+l )/~wt+l,, = O for all z or, equivalently, 
by setting ad(wt+l, wt) + ~tzt,i aqyt, z) = O (6) az hh+l,l () z=wt+l.x~ for all i. Solving the equations 
(6) for wt+l,, may be difficult, but it becomes simpler if we approximate the left-hand side by evaluating 
the derivative of L(yt, z) at z = wt. xt instead of at z =Wt+l .xt. Thus, we use the gradient evaluated 
at the old weight vector wt, defined as Vt in (2), to replace the gradient evaluated at the new, still 
unknown, weight vector wt+l. For the squared Euclidean distance d(wt+l, wt) = ~llwt+l wtll~, solving 
the approximated equation easily gives the GD algorithm. The EG algorithm is obtained in a straightforward 
manner by using for d(wt+ 1, w t ) the relative entropy O?RE(wt+l, Wt ) defined in (1). The relative 
entropy assumes that all the components wt,, and wt+l,, are positive and satisfy xl wt+l:i = xi wt,, 
= 1. The positiveness of the weights follows dmectly from the update we obtain. The constraint ~, wt+l,, 
= 1 is observed in the usual way by adding a term with a Lagrange multiplier to the function M to be 
minimized. Note that neither the squared Euclidean distance nor the relative entropy is a metric. However, 
they do have the properties d(w, w) = O and d(w, s) >0 for s # w. For our work it is central that the 
distance measure is used in two different ways: first, it motivates the update rule, and second, it is 
applied as a tool in the analysis of the algorithm thus obtained. Amari s [Ama94, Ama95] ap­proach in 
using the relative entropy for deriving neural net­work learning algorithms is similar to the first use 
we have here for the distance measure. The use of a distance mea­sure for obtaining worst-case loss bounds 
was pioneered by Littlestone s [Lit89] analysis of Winnow, which also employs a variant of the relative 
entropy. This idea is explained in more detail in Section 5. 5 Worst-case loss bounds 5.1 The basic 
proof method In the most basic case, to prove a worst case loss bound for an on-line linear algorithm, 
we would define a potential function V(w) for weight vectors. We call the potential dif­ference V(wt) 
V(wt+l ) the progress made by the algorithm at trial t.We assume that the value V(w) is nonnegative and 
bounded from above by some value VO. If we can then prove that for all possible combinations of an old 
weight vector wt, inst ante xt, and outcome yt, the updated weight vector Wt+l of the algorithm A satisfies 
.L(yt, Wt .xt ) < V(wt ) V(wt+l ), we obtain LOSSL(A, S) S V(WI) V(w~+I) < Vo. Thus, if the total amount 
of progress is bounded and at each trial the loss is bounded by the progress, we get a bound for the 
total loss. Of course, we cannot define a potential function V for which the above holds in general, 
since the loss of the algo­rithm is expected to depend on the loss of the best compar­ison vector u E 
U. In the special case that LOSSL (u, S) = O holds for some u E U, we could take V(w) = c d(u, w) where 
c > 0 and d is some distance measure. Since we use the po­tential function only for analyzing the algorithm, 
it does not matter that the vector u is not known in advance. Consider now the general case, when even 
the best comparison vector incurs some loss over the trial sequence. Our basic idea is to simultaneously 
consider V(w) = c d(u, w ) for all comparison vectors u E U. We call d(u, wt) d(u, wt+l ) the algorithm 
s progress towards u at trial t. Ideally, we would wish to have for all comparison vectors u the algorithm 
s additional loss compared with u, given by L(yt, wt . xt) L(yt, u. xt), bounded from above by its progress 
towards u. This turns out to be not quite possible, but we can prove for all u the bound a-L(yt, wt . 
Xt) bL(yt, U . Xt) < ~(%wt) ~(%wt+l) (7) where a and lJ are suitably chosen constants with O < a < 
b. Hence, at trial t the algorithm makes progress towards u if u predicted at that trial notably better 
than the algorithm did. On the other hand, on any given trial some vectors u make a larger loss than 
the algorithm, and the algorithm can make even negative progress towards those u. To control the parameters 
a and b in (7) itturns out to be convenient to introduce a new parameter c and functions ~ and g such 
that for b = g(c), the condition (7) holds for all u if and only if a s ~(c). Proving (7) for the optimal 
values a = f(c) and b = g(c) is the main technical problem in the analysis. When this has been achieved, 
adding the bounds (7) fort= l,..., t with a = ~(c) and b = g(c) yields d(u, WI) (f(u, Wl+l)LOSSL(A, 
S) < (c)LOSSL(U, S) + f(c) f(c) d(u, Wl) !@ LossL(u, S) + f(c) (8)5 f(c) for all u c U. The bound for 
the algorithm compared to u is then obtained by choosing the value c for which the right-hand side of 
(8) is minimized. Since everything holds for an arbitrary u c U from the comparison class, the final 
upper bound is obtained by taking an infimum over u E U. 5.2 Upper bounds for G D In the full paper 
we show that the earlier results of Cesa- Bianchi et al. [CBLW93] for the GD algorithm fit well into 
our general framework for proving loss bounds. We cite here their main result for comparison with our 
results on the new algorithm EG*. Theorem 1 For a trial sequence S = ((x,, yl),..., (x?, w)), let X be 
an upper bound such that llxtl12 < ~ holds for all t. If the GD algorithm uses an arbitrary start uector 
wl and learning rates given by qt = l/(411xtll~), rue have for any vector u the bound Loss(GD, S) ~ 2 
(LOSS(U, S) + w,l@2) . 111.1(9) Further, let K and U be arbitrary constants and define a comparison 
class by U = { u E RN : Loss(u, S) ~ K and IIu WI 112< U}. Now for the learning rate Ux (lo) t= Ilx:ll; 
2JR+2UX the GD algorithm has for any u E U the loss bound Loss(GD, S) ~ fIOSS(U,5 )+2tiUX+llU-WIll;X2 
. (11) The bound (11) is a strong worst-case upper bound. The ratio Loss(GD, S)/ infueu Loss(u, S) approaches 
1 as infu~u Loss(u, S) approaches co, assuming the parameter h is infu~u Loss(u, S) (or somewhat higher). 
Further, we get good upper bounds for the two leading coefficients in the additional loss Loss(GD, S) 
-infu~u Loss(u, S). Our lower bound in Theorem 5 implies that these constants are, in fact, optimal. 
The less refined bound (9) is obtained with­out prior knowledge of h or U. 5.3 Upper bounds for EG The 
full papers contains many loss bounds for the algorithm EG and its variants. The main result is the following 
upper bound for the EG* algorithm. Theorem 2 Let S = ((xl, yl),..., (XZ, yl)) be a trial se­quence and 
X a bound such that llxtll~ < X holds for all t. Let u E RN be an arbitrary weight vector with IIuIII 
< U. If the algorithm EG* uses the uniform start vector and the learning rates given by qt = l/(3U211x~l&#38;), 
we have Loss(EG*, S) <3 (Loss(u, S) + U X2 in 2N) . (12) Further, let K and U be ar-bitrar constants 
and define a # comparison class by U = { u E R : Loss(u, S) ~ K and IIuIII < U}. Now for the learning 
rate  wiim . t= IIxjl% Um+ 2u2x/izim (13) the EG* algorithm has for any u c U the loss bound Loss(EG*, 
S) ~ fJOSS(U,S) + 2Ux/~ + 2U2X21n 2N . (14) Bounds similar to (12), but with worse constants, were already 
proved by Littlestone et al. [LLW91] for their algo­rithm. Here we have also the stronger bound (14), 
in which the ratio Loss(EG*, S)/ infueu Loss (u, S) approaches 1 as infucu Loss(u, S) approaches co. 
Notice that as with the GD algorithm, making full use of the tighter bound (14) requires good values 
for the parameters K and U. There are tighter and more complicated formulations of the bounds (12) and 
(14) that use the actual relative en­ tropy dRE (u /U, s) where s G R2N is the start vector of the EG 
algorithm simulated by EG* and u c R2N the norm U representation of u. The appearance of the fac­ tor2~ 
2N in the bounds is due to our use of the bound ~,=1 (~J/U) ln((~l/U)/s,) Sin 2N for the relative entropy 
for the uniform start vector s = (1/2 N,. . . . l/2 N). The following lemma is the main step that leads 
to The­orem 2. Lemma 3 Let w, be the weight vector of EG before trial t in a trial sequence S = ((xI, 
w),. ... (XZ, YZ)), and let u ~ [0, l]N be a vector with ~, u, = 1. Consider an arbitrarg trial t, and 
let R = max, Xt,i rein, Zt}i. For any constants a and b such that O < a 5 2b/(2 + R b), and the iearn~ng 
rate qt = 2b/(2 + R* b), we have a(yt wt .xt)2 b(yt u. xt)2 < tiRE(u, wt)-dRE(u, wt+l) . (15) proof 
we have Wt+l,, = zvt,t/3~ / ~j wt,j~f i where P, = eZ71~(Yi-fJ{),We can then write dRE(U, Wt) dm(U, 
Wt+l) x NN W+I, I = utln = thzt,tlnPt -lnfw,tP7° . Wt,i x ,=1 ,=1 i=l Hence, (15) can be written F(wt, 
xt, Wt Xt,yt,U xt,~t)<0 where N ~N(W,x, ~, g, r$~) = in ~ W,fi= rln /3 ,=1 + a(y -j)2 Z@ r)2 (16) 
 and ~t = e2nt(vi-ot). Let now B besuch that B < zt,i S B+Rholds for I s i s N. We then have OS (~t,i 
 B)/R < 1for 1S zs N. The bound &#38; < 1 z(1 a) holds for a z Oand O~ z~ 1,and is tight for x= Oand 
r = 1. By applying thw with a = ~~ we obtain P = F (F)( -BR Rs PB (1-~ (1-?R)) . Let now ~~1 Wi = 1 and 
w . x = j. Using the above gives us N in ~ Wt,iP t, <Bin/?+ln l-~(1-/3R)) . ( ,=1 G(j, y, T,~) = Bln,B+ln 
l ~ (1 -pR)) ( rln/3+ a(y-j)2-b(y-r)2 . Note that the inequality is tight if, for instance, N = 2 and 
xt =(B,B+R). To obtain (15), it is now sufficient to show that G(j, y, r,@ g O holds for all values of 
j, u, and r, when /3 = e2~(~-~) with q = ~~ = 2b/(2 + .R2b). Since the second derivative t32G(~, y, r, 
~)/6 r2 = 2b is negative, the value G(j, y, T, /3) is maximized when r is such that ?lG(j, y, r, B)/6 
T = O. Solving this gives r = y in/3/(2b). In particular, for P = e2q( -*), we see that proving 2~(Y-0)) 
< 0 for T = ~ + ~(j ~)/b implies G(j, y,r,e G(j, g, T, e2q(v-0)) <0 for fl values r. For T = y+q(~-y)/b 
ZV(Y-J)) = ~(j, y) where we have G(j, g, r, e H(j, y) = 2@(?/ j) +ln l-~~B (1 -e2 q -o)))( It remains 
to show that lf(j, y) s O. We apply the bound ln(l q(l -ep)) < pg + P2 /8, which can be shown to hold 
for all real values p and q. We get If(j, y) < S(j, y) where ~ B S(j, Y) = 2TWY j) +2@(?/ ~)~ + + (27m(Y 
 j))2 2TIY(v-i)+ a+$ (V i)2 () = (Y -?32 ~ ((2+ R2b)q2 -4bq + 2ab) . Therefore, we must show Q(q) ~ 
O where Q(q) = (2 + R2b)V2 4bq + 2ab. We easily see that Q(q) is minimized for q = 2b/(2 + R2b), and 
that for this value of q we have Q(q) <0 if and only if a < 2b/(2 + R.2b). 0 We now combine the bounds 
for the individual trials to give a bound for the total loss for EG. We introduce a new parameter c and 
later show how it can be suitably chosen to minimize the bound. Lemma 4 Let S = ((xl, Y1),..., (XI, VI)) 
be an arbitrary trial sequence. For t = 1,. ..,1, let R$ = max:zt,, mini Xt,,, and let R be an upper 
bound such that Rt < R holds for all t. Let c be an arbitrary positive constant, and let qt = 2c/(Rz(2 
+ c)) for all t. Then for any start vector SCRN and comparison vector u c RN, we have Loss(EG, S) ~ (1+ 
~) Loss(U, S) + ;+; R2dRE (U, S) (17) () for the EG algorithm with the start vector s and learning rates 
q$. Proof Fort =l,..., I?, let bt = c/R~ and at = 2bt/(2 + R; bt) = 2c/(R~ (2 + c)). Let w~ be the tth 
weight vector of EG on the trial sequence S with qt = at. Then Lemma 3 gives us at(yt w, . xt)2 bt(yt 
 U. Jb)2 < dRE(U, Wt) drm(% Vvt+l) (18) and hence &#38;(Yt Wt .X,)2 C(yt u .Xt)z < R: (dRE(U, W,) 
 dRE(U, Wt+l)) < R2 (dRE(U, Wt) -dRE(U, W$+l)) . (19) By adding the bounds (19) for t= 1,...,1 we get 
~Loss(EG, S) CLOSS(U, S) ~ R2 (dRE(U, S) dm(U, W4+1)) < R2dm(w s) , which is equivalent with (17). 
0 We now proceed to the proof of Theorem 2. In the Proof we first reduce the desired bound to a bound 
for th~ EG algorithm and then get the bound for EG by choosing the parameter c in Lemma 4 suitably. Proof 
of Theorem 2 We define a new trial sequence S = , (xi, y?)) by setting x; = (Uzt,I,.. ., Uz,,~,(($;;:;> 
. . . Uz~,~). The algorithm EG* has been defined in such a w~y that the predictions produced by EG* 
on the trial sequence S are the same as those produced by EG on the trial sequence S with the given start 
vectors and learn­ing rates. In particular, Loss(EG *, S) = Loss(EG, S ). We further note that for any 
u c RN with IIul II s U and the norm U representation u of u, we have Loss(u /U, S ) = Loss(u, S), and 
that max, z{,, rein, z;,, = 2Ullxtll~. We now apply Lemma 4 to the trial sequence S and comparison vector 
u /U with Rt = 2UI ]xt II~. For the uni­form vector s we have dRE (u /U, s ) ~ in 2N, so the bound (17) 
now yields Loss(EG*, S) s 1 + ~ Loss(u, S) () +(2+ ~) U2X21n2N . (20) The bound (12) follows from (2o) 
by choosin ~ c =:, which corresponds to the learning rates qt = l/(3U IIxtl I~). To get the bound (14), 
assume further that Loss(u, S) s K holds. Then (2o) implies Loss(EG*, S) ~ Loss(u, S) + 2U2X2 in 2N + 
~ + 4U2X21n2N . (21)2c Assume first h > 0. The right-hand side of (21) is mini­ mized for c = 2UX ~~, 
and we get (14). Accord­ing to Lemma 4, the value of c corresponds to the learning rate given in (13). 
The result for the special case h = O follows by a straightforward limit argument. 0 5.4 An application 
to a probabilistic setting We wish to illustrate our upper bounds by considering them in a simple probabilistic 
setting. Assume that xt is a random variable such that IIxt II~ < X with probability y 1 for some known 
bound X. Let u E RN be an unknown vector such that IIuI II ~ U holds for some known bound U. Assume that 
the outcome g~ is given by yt = u. xt + et, where et, t=l ,...,1, are independent random variables with 
mean O and variance U2. The proof of Theorem 2 can easily be modified to yield the bound E [Loss(EG*, 
s)] s .&#38;rz + 2UXa~ + 2UX in 2N for the expected loss of the EG* algorithm when the trial sequence 
S = ((xl, yl), . . . , (z1, g?)) is drawn according to this distribution. An analogous result obviously 
holds for the GD algorithm. 5.5 Lower bounds The following lower bound applied with p = q = 2 shows 
that the upper bound (11) is tight. This special case was already noted by Cesa-Bianchi et al. [CBLW93]. 
Theorem 5 Let p,g E R+ U {co} with l/p+ l/q = 1. Let A be an arbitrary on-line prediction algorithm, 
and let K, U, and X be arbitrary positive reals. Then for all positive integers N there are an instance 
xt c RN with Ilxllq = X, an outcome y E R, and a comparison vector u c RN with GD.C X2 .± .. 0. .. target 
norms: instance norms: GD s target norm EG s instance norm is smaller is smaller Figure 1: Schematic 
representation of the main factors af­fecting the loss bounds of the GD and EG* algorithms. llu]lP = 
U, such that for the l-trial sequence S = ((x, y)) we have Loss(u, S) = K and Loss(A, S) z K + 2UX&#38; 
+ (UX)2 . (22) Proof We define two potential target vectors u+ = (UN-llP, . . . . UN-llP) and u_ = u+, 
and an instance vector x = (XN-l Jq, . . . ,XN-ltg). Then Ilu+llp = Ilu-llp = U, 11x11, = X, and U.X 
= UX. Let j be the pre­diction of the algorithm A, when it sees the inst ante x at the first trial. We 
further choose y = UX + @ if j ~ O and y = UX -@ otherwise. Then either Loss(u+, S) = K or Loss(u_, S) 
= K. Since Loss(A, S) z yz, we get the stated bound. 0 For the case (p, q) = (1, co), the constants in 
the upper bound (14) do not match the constants in the lower bound (22). It is an open problem to find 
the optimal constants for the case (p, q) = (1, co), or to find any upper bounds close to the lower bound 
(22) for the case when (p, g) is neither (2, 2) nor (1, co). The fact that Theorem 5 holds when Lp and 
L~ are dual norms is due to the basic property that if the norms are dual, the bound Iw . xl s Ilwllpllxllq 
holds [Roy63]. 6 Comparison of the algorithms 6.1 Comparison of the worst-case bounds We now show that 
the different pairs of dual norms in the upper bounds for the GD and the EG* algorithms result in certain 
sit uations in radically different behavior for large N. For simplicity, we consider the case in which 
there is a per­fect linear relation between the instances and outcomes, and therefore some comparison 
vector u satisfies Loss(u, S) = O. We can then take K = O in the bounds (11) and (14). As­sume that all 
the other parameters are also set optimally, and write Xp = maxt llxtllP for p = 2 and p = m. Then the 
bound (11) simplifies to Loss(GD, S) < IIul l~X~ and the bound (14) to Loss(EG*, S) s 211ull~X&#38; in 
2N. Figure 1 illustrates the trade-offs between the different norms in the bounds. Recall that always 
llwll~ < IIwI12 < IIw[ II, and how tight these inequalities are depends on the vector w. Hence, the EG* 
algorithm has the advantage over the GD algorithm on the instance side of the figure, since its loss 
bound includes the factor X~ that is less than (or in special cases equal to) the factor X2 in the loss 
bound for GD. Similarly, GD has the advantage on the target side, since the factor Uz in the bound for 
GD never exceeds the factor UI in the bound for EG*. The additional factor 2 in 2N in the bound for EG* 
further favors GD. As the products X2 Uz and X~ U1 are incomparable, the total effect can favor either 
GD or EG*. For clarity, we consider two extreme cases. First, assume that u has exactly k components 
with value 1 and the rest N k components have value O. Thus, only k input vari­ables are relevant for 
the prediction task. Assume that the instances xt are from the set { 1, 1 } N of vertices of an N­ dimensional 
cube. Then IIuI12 = W, IIuII1 = k, Xz = W, and XM = 1. The bounds become Loss(GD, S) S kN and Loss(EG*, 
S) < 2k2 in 2N, so for N > k, the EG* algo­rithm has clearly the better bound. On the other hand, let 
u = (l,..., 1), and let the inst antes be rows of the N x N unit matrix. Then [Iu[lz = fi, Ilulll = N, 
and X2 = Xw = 1. The bounds become Loss(GD, S) ~ N and Loss(EG*, S) ~ N2 in 2N, so the GD algorithm has 
clearly the better bound. Thus, the bounds for GD and EG* are in­comparable, and for large N the difference 
can be arbitrarily large in either direction. The simplified scenario given above can be generalized. 
If only few of the input variables are relevant for predict­ing the outcomes, but all the input variables 
take values of roughly equal magnitudes, then the EG* algorithm has the better bound. The GD algorithm 
has the better bound if all the input variables are almost equally relevant for predicting and the Lz 
norms of the instances are not much larger than the L~ norms. This happens if most of the total weight 
in the instance vectors is concentrated on the largest compo­nents. The conclusions remain similar also 
when no com­parison vector u achieves Loss (u, S) = O, which is the case if there is noise in the instances 
or outcomes. However, the differences between the total losses of the algorithms become less pronounced 
in these less pure situation. 6.2 Experimental comparisons We first consider a situation in which the 
worst-case upper bounds suggest EG* should outperform GD. We generated the instances uniformly at random 
from the set { 1, 1 }100 of vertices of the 10O-dimensional cube. We chose a com­parison vector u = ( 
1, 1, 1, 0,..., O) with three nonzero and 97 zero components. We added roughly 10% noise to the outcomes 
by generating for each t a variable rt uni­formly at random from the interval [0.8, 1.2] and then set­ting 
yt = Ttu. xt. Figure 3 shows how the cumulative losses ~~l(!h -ilt)z for the algorithms GD and EG* increased 
as a function of the number m of trials in a typical trial sequence. The GD algorithm used a zero start 
vector. The EG* algorithm used a uniform start vector, which due to the reduction used in EG* leads to 
predictions being ini­tially zero and thus corresponds to using a zero start vector. The learning rates 
were chosen according to (10) and (13) using the comparison vector u. We feel that this gives a fair 
comparison between the algorithms, since our other experi­ments show that this setting leads to almost 
optimal learning rates. The figure also shows as horizontal lines the worst­case upper bounds for the 
losses. These bounds are based on slightly sharper versions of Theorems 1 and 2 [K W94]. I m­ 4 ~ 250­$ 
 GD ~ 20Q­ .3 EG+­150­ 100­ 50­ or o 50 100 150 2W 250 200 lrlab Figure 2: Cumulative losses of GD (solid 
line) and EG* (dotted line), with their upper bounds, for instances xt c {-1,1} 100 and target u = ( 
1,1, 1,0,...,0), and 10~0 noise. From Figure 2 we see that the worst-case upper bounds are quite tight 
for the random data we consider. They be­come even tighter if we do not add noise to the outcomes. Obviously, 
the difference between the algorithms can be made arbitrarily large by increasing the number of dimen­sions. 
On the other hand, we can make ex eriments in which 1 the GD algorithm outperforms the EG algorithm, 
again by an arbitrary amount of loss depending on the number of dimension. This is the case for inst 
ante if the compari­ son vector u is chosen from { 1, 1 } N and the inst antes are from the N-dimensional 
Euclidean sphere. For these and other experiments, see the full paper [KW94]. We feel that the situation 
that favors the EG* algo­rithm is much more natural and likely to arise in practice. Since linear predictors 
are very restricted, a natural exten­sion would be to expand the instance xt by including as new input 
variables the values ~~(xt ) for some suitable chosen basis functions $,. Then a linear prediction algorithm 
could actually use a linear combination of the basis functions as its predictor. As an example, we might 
include all the 0(N9) products of up to g originrd input variables [BGV92]. After such an expansion, 
one would expect relatively few of the new variables to be relevant for prediction. Assuming that the 
input variables are in the range [ 1, 1], this does not increae.e the L= norms of the instances. If the 
outcomes are actually given by some degree k polynomial of the input variables, the loss of the EG* algorithm 
after the expan­sion of the inst antes would be O(q in N). However, the GD algorithm would suffer from 
the fact that the expansion in­creases the Lz norms of the instances, and could have a loss 0(N9). Note 
that if the original N input variables contain r irrelevant random input variables, these are expanded 
into O(rk ) irrelevant pseudorandom variables, and our experi­ments show that such variables do increase 
the loss of the GD algorithm. Figure 3 shows the cumulative losses ~~1 (yt ~t )2 as a function of the 
number m of trials for the algorithms GD and EG* in a typical experiment with expanded instances. We 
have taken q = N = 8. As we see, the loss curve of EG* flat­tens earlier, indicating that EG* learns 
faster. The total loss is also smaller for EG*. The original instances have been chosen uniformly from 
{ 1, 1 }s, and an expanded instance 800[ t  &#38; o 50 100 150 2CKI 250 200 inals Figure 3: Cumulative 
losses of GD and EG ±, with their upper bounds, for sparse target and expanded inst antes. consist of 
the products of the components of the original in­stance. Since the components xt,, are from { 1, 1 }, 
we do not cousider products that include the same variable more than once. Hence, there are 256 products. 
We have chosen the target polynomial ~Z~.3~l +~2~2z3z6+ zlz2z3z4z6~7~8, with three terms, which for the 
encoding we use gives the target coefficient vector u with u59 = U9S = u251 = 1 and w = O for i @ {59,96,251 
}. The outcomes have been cho­sen to have yt = u.xt, with no noise. 6.3 Discussion OurexDeriments show 
that alreadv onsimrde artificial ran­dom d~ta, the actual losses of tie algorithms come close to their 
worst-case upper bounds. This is true both with and without noise in the outcomes. In particular, we 
have observed that the GD algorithm does suffer when there is a large number of irrelevant input variables. 
The number of examples it needs before it obtains an accurate hypoth­esis is roughly comparable to the 
number N of input vari­ables, even if almost all of the input variables are irrelevant for the prediction 
task. This is easily seen to be true in the special case that the number of instances does not ex­ceed 
N, and the instance vectors are orthogonal to each other. Since the weight vector of the GD algorithm 
satisfies Wm = WI +~.:;l atxt for some scalars at, the prediction - &#38; =w~.x~ win this case not at 
all affected by what the algorithm hasseen attrialsl,...,l -l. This would also be true if we at each 
trial solved a linear least squares problem to find a weight vector that fits all the examples and has 
the least Lz norm. For the EG* algorithm, the dependence on the number of irrelevant input variables 
is only logarithmic, so doubling the number of irrelevant variables results in only a constant increase 
in the total loss. It seems that the EG* algorithm has a strong bias towards hypotheses with few relevant 
vari­ables, so if only few variables are needed for prediction, then EG* avoids the curse of dimensionality. 
The GD algorithm and the linear least squares algorithm are biased towards hy­potheses with small Lz 
norm, and even if only few variables are relevant, they use all the dimensions in a futile search for 
a good predictor with a small norm. 7 Conclusion The following are the key methods used in this paper. 
1. We use worst-case bounds for the total loss for eval­uating on-line learning algorithms. The bounds 
are expressed as a function of the loss of the best fixed linear predictor. 2. We introduce a common 
framework for deriving learn­ing algorithms based on the trade-off between the dis­tance traveled from 
the current weight vector and a loss function. Different dist ante functions lead to rad­ically different 
algorithms. 3. The distance function also serves in a second role as a potential function in proving 
worst-case loss bounds by amortized analysis. The bounds are first expressed as a function of the learning 
rate and various norms of the instances and target vectors, as well as the loss of the target vector. 
Good loss bounds are then obtained by carefully tuning the learning rate.  In this paper we are clearly 
championing the EG* algo­rithm derived from the relative entropy distance measure. It learns very well 
when the target is sparse and the compo­nents of the instances are in a small range. Such situations 
naturally arise if we predict nonlinearly by first expanding the instances to include the values of some 
nonlinear ba­sis functions and then predict using linear functions of the expanded instances. Since the 
loss of the EG* algorithm increases only logarithmically in the number of irrelevant input variables, 
it is possible to have a good generalization performance even if the number of basis functions, that 
is the number of dimensions in the expanded instances, signif­icantly exceeds the number of training 
examples. Even for the linear single neuron we have been able to prove worst-case loss bounds (in terms 
of the loss of the best linear predictor) only for the square loss. Ideally we would like to have loss 
bounds for other standard loss functions such as the log loss. It would also be interesting to find new 
distance measures that would lead to new algorithms, for which the loss bounds depend on other pairs 
of dual norms than the pairs (LI, J5~ ) and (L2, Lz), which correspond to the algorithms EG* and GD, 
respectively. Our bounds for GD are provably optimal, but we still need matching lower bounds for the 
EG* algorithm. Applying gradient descent in multilayer sigmoid networks leads to the well-known back-propagation 
algo­rithm. The exponentiated gradient algorithms can simi­larly be generalized to obtain a new exponentiated 
back­propagation algorithm. As a long-term research goal, we suggest developing a whole family of algorithms 
derived us­ing the relative entropy as a dist ante measure. Many of the tradional neural network algorithms 
belong to the gradient descent family of algorithms that in our framework can be derived using the squared 
Euclidean distance. This fam­ily includes the Perception algorithm for thresholded linear functions, 
the GD algorithm for linear functions, the st an­dard back-propagation algorithm for multilayer sigmoid 
net­works, and the Linear Least Squares algorithm for fitting a line to data points. The new family includes, 
respectively, the Winnow algorithm [Lit88], the EG* algorithm, the ex­ponentiated back-propagation algorithm, 
and an algorithm for fitting a line to data points so that the relative entropy of the coefficient vector 
is minimized. The new family uses a new bias, which favors sparse weight vectors. In the linear case 
we have been able to verify that the new family indeed [KW94] performs better when the target is sparse. 
We expect to see similar behavior also in more general settings. Acknowledgments [KW95] [Lit88] [Lit89] 
[LLW91] [LW94] [Ros58] [Roy63] [SW94] [VC71] [VC)Y90] [WS85] We wish bold, and Schapire References <RefA>[Ama94] 
[Ama95] [BEHW89] [BGV92] [cBFH+95] [CBLW93] [DH73] [Hay93] [HKW94] [HKW95] [HSSW95] to thank Nicolb Cesa-Bianchi, 
David P. Helm-Yoram Singer for their comments and Robert E. for simplifying the proof of Lemma 3. S. 
Amari. Information geometry of the EM and em algorithms for neural networks. Tech­nical Report METR 94-4, 
University of Tokyo, Tokyo, 1994. S. Amari. The EM algorithm and information geometry in neural network 
learning. Neural Computation, 7(1):13-18, January 1995. A. Blumer, A. Ehrenfeucht, D. Haussler, and M. 
K. Warmuth. Learnabtity and the Vapnik- Chervonenkis dimension. Journal 36(4):929-965, October 1989. 
B. E. Boser, I. M. Guyon, and A training algorithm for optimal fiers. In Proc. 5th Workshop on Learning 
Theory, New York, NY, N. Cesa-Bianchi, D. Haussler, R. muth. How to pages 144 152. 1992. Y. Freund, D. 
of the ACM, V. N. Vapnik. margin classi- Computational ACM Press, P. Helmbold, E. Schapire, and M. K. 
War­ use expert Report UCSC-CRL-95-19, puter Research Lab, Santa extended abstract appeared N. Cesa-Bianchi, 
P. Long, Worst-case quadratic loss advice. Technical Univ. of Calif. Com-Cruz, CA, 1995. An in STOC 93. 
and M. Warmuth. bounds for on-line prediction of linear functions by gradient de­scent. Technical Report 
uCSC-CRL-93-36, Univ. of Calif. Computer Research Lab, Santa Cruz, CA, 1993. An extended abstract ap 
peared in COLT 93. R. O. Duda and P. E. Hart. cation and Scene Analysis. NY, 1973. S. Haykin. Neural 
Networks: Foundation. Macmillan, New D. Haussler, J. Kivinen, and Pattern Classifi- Wiley, New York, 
 a Comprehensive York, NY, 1993. M. K. Warmuth. Tight worst-case loss bounds for predicting with expert 
advice. Technical Report UCSC­CRL-94-36, Univ. of Calif. Computer Research Lab, November 1994. Preliminary 
versions ap­ peared in EuroCOLT 93 and EuroCOLT 95. D. P. Helmbold, J. Kivinen, and M. K. War­muth. Worst-case 
loss bounds for sigmoided linear neurons. Unpublished manuscript, 1995. D. P. Helmbold, R. E. Schapire, 
Y. Singer, and M. K. Warmuth. A comparison of new and old algorithms for a mixture estimation problem. 
Proc. 8th Workshop on Computational Learn­ing Theory, July 1995 (To appear). J. Kivinen and M. K. Warmuth, 
Exponenti­ ated gradient versus gradient descent for lin­ear predictors. Technical Report UCSC-CRL­94-16, 
Univ. of Calif. Computer Research Lab, June 1994. Retrievable from ftp.cse.ucsc.edu as /pub/tr/ucsc-crl-94-16 
.ps.Z . J. Kivinen and M. K. Warmuth. The Percep­tion algorithm vs. Winnow: Linear vs. logarith­mic mistake 
bounds when few input variables are relevant. Proc. 8th Workshop on Computa­tional Learning Theory, July 
1995 (To appear). N. Littlestone. Learning when irrelevant at­tributes abound: A new linear-threshold 
algo­rithm. Machine Learning, 2(4):285 318, April 1988. N. Lit tlestone. Mistake Bounds and Loga­rithmic 
Linear-threshold Learning Algorithms. PhD Thesis, Technical Report UCSC-CRL-89­11, University of California 
Santa Cruz, 1989. N. Littlestone, P. M. Long, and M. K. War­muth. On-1ine learning of linear functions. 
To appear in Journal of Computational Com­plexit y. A preliminary version appeared in Proc. ?Mrd Symposium 
on Theory of ComPut­ing, pages 465 475. ACM Press, New York, NY, 1991. N. Littlestone and M. K. Warmuth. 
The weighted majority algorithm. lnjormation and Computation, 108(2):212-261, February 1994. F. Rosenblatt. 
The perception: A probabilistic model for information storage and organization in the brain. Psychological 
Review, 65:386 407, 1958. (Reprinted in Neurocomputing (MIT Press, 1988).). H. Royden. Real Analysis. 
Macmillan, New York, NY, 1963. R. E. Schapire and M. K. Warmuth. On the worst-case analysis of temporal-difference 
lear­ing algorithms. To appear in the special issue of Machine Learning on reinforcement learning. A 
preliminary version appeared in Proc. i 1 th In­ternational Conference on Machine Learning, pages 266 
274, Morgan Kaufmann, San Fran­cisco, CA, 1994. V. N. Vapnik and A. Y. Chervonenkis. On the uniform convergence 
of relative frequencies of events to their probabilities. Theory of Proba­ bility and its Applications, 
16(2):264-280, 1971. V. Vovk. Aggregating strategies. In Proc. Srd Workshop on Computational Learnina 
Theoru. pages 371-383. Mo~gan Kaufmann, %&#38; Mate~: CA, 1990. B. Widrow and S. Stearns. Adaptive Signal 
Pro­cessing. Prentice-Hall, Englewood Cliffs, NJ, 1985.</RefA>  
			