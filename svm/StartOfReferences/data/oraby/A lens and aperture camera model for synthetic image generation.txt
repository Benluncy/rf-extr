
 Computer Graphics Volume 15, Number 3 August 1981 A LENS AND APERTURE CAMERA MODEL FOR SYNTHETIC IMAGE 
GENERATION by Michael Potmesil* and Indranil Chakravarty** Image Processing Laboratory Rensselaer 
Polytechnic Institute Troy, New York 12181 Abstract 1.0 Introduction In the past few years several algorithms 
have beenThis paper extends the traditional pin-hole camera developed for realistic rendering of complex 
three-projection geometry, used in computer graphics, to a dimensional scenes on raster displays [1,2,3]. 
These more realistic camera model which approximates the algorithms have been generically called hidden 
surfaceeffects of a lens and an aperture function of an actual algorithms in the sense they display only 
the visible camera. This model allows the generation of synthetic surfaces from a given vantage point. 
All theseimages which have a depth of field, can be focused on algorithms, however, have continued to 
use the pin-hole an arbitrary plane, and also permits selective modeling camera projection geometry 
which was developed forof certain optical characteristics of a lens. The model display of 3D line drawings 
on vector devices. The can be expanded to include motion blur and special purpose of this paper is to 
develop a more complexeffect filters. These capabilities provide additional tools camera model, which 
although computationally morefor highlighting important areas of a scene and for expensive, provides 
the means of generating moreportraying certain physical characteristics of an object in realistic synthetic 
images closely approximating a scenean image. imaged by an actual camera. These synthesized images are 
suitable for display only on raster devices. Key Words and Phrases: computer graphics, visible The purpose 
of generating such synthetic images, surface algorithms, raster displays, camera model, lens which in 
a sense incorporate the constraints of an optical and aperture system and the imaging medium, is twofold: 
1. It gives the ability to capture the viewers' attention CR category: 8.2 to a particular segment of 
the image, that is, it allows selective highlighting either through focusing or some optical effects. 
2. It permits adaptation of many commonly used cinematographic techniques for animated sequences, such 
as fade in, fade out, uniform defocusing of a scene, depth of field, lens distortions and filtering. 
It should be stated, however, that the objective is to model only those features which can be used to 
some advantage for special effects. No attempt will be made *Present address: Bell Laboratories, Holmdel, 
NJ 07733 to model flaws inherent in a lens such as optical**Present address: Schlumberger-Doll Research, 
aberrations or the lens transfer function. We will also Ridgefield, CT 06877 assume that the imaging 
medium is perfect, that is, a point is reproduced with perfect fidelity. The image generation process 
described here consists Permission to copy without fee all or part of this material is granted of two 
stages. In the first stage, a hidden-surfaceprovided that the copies are not made or distributed for 
direct processor generates point samples of intensity in the commercial advantage, the ACM copyright 
notice and the title of the publication and its date appear, and notice is given that copying is by image 
using a geometric pin-hole camera model. Each permission of the Association for Computing Machinery. 
To copy sample consists of image plane coordinates, RGB otherwise, or to republish, requires a fee and/or 
specific permission. intensities, z depth distance and identification of the visible surface. In the 
second stage, a post-processor converts the sampled points into an actual raster image. &#38;#169;1981 
ACM O-8971-045-1/81-0800-0297 $00.75 Each sampled point is converted into a circle of confusion whose 
size and intensity distribution are determined by the z-depth of the visible surface and the characteristics 
of the lens and aperture model. The intensity of a pixel is computed by summing the intensity distributions 
of the overlapping circles of confusion of all sample points. A circle of confusion and its intensity 
distribution may be stretched in the image along the projected path of a moving surface to approximate 
motion blur caused by a finite exposure time. Special effect filters such as star or diffraction filters 
may be convolved with the image at this stage.  2.0 The Camera Model 2.1 Camera Geometry This section 
describes the camera geometry used in projecting a 3D scene onto an image plane. The projection matrix, 
also called the camera transformation matrix, is a function of the camera parameters. These parameters 
are the pan, tilt and swing angles of the camera, the location of the camera lens, the focal length and 
the size of the image frame, as illustrated in Figure 1. z 1 ~ .... 0 2 1 ~Y / ~i 2 \ Z x 3 2 x I \\ 
O 3 image plane "~-~ I " | x2 sample ray viewing pyramid center of projection Fig. 1 Camera Geometry 
The use of homogeneous coordinates permits modelling this transformation as a single 4x3 matrix [5,6]. 
This transformation provides a perspective mapping of a point from the global coordinate system O z, 
in which a scene is defined, to the image plane coordinate system 0 3 . The pan, tilt and swing angles 
specify the direction the camera is looking at, the lens center is the vantage point relative to coordinate 
system 01, and the focal length along with the size of the image frame specifies the viewing pyramid. 
Points in O l which lie outside the viewing pyramid are clipped from the image plane. Volume 15, Number 
3 August 1981 2.2 The Finite Aperture Camera Model The basic law governing image formation through a 
lens can be described by the lens formula used in ray optics: ± + ± _ ! (]) U v F where U is the object 
distance, V is the image distance and F the focal length of the lens all measured along the optical axis 
[Figure 2]. We add to this basic lens model an aperture function which limits the lens diameter and fixes 
the location of the image plane at the focal plane of the lens. The introduction of these two constraints 
allows the notion of focusing the lens (by moving it relative to the fixed image plane), and associated 
with it, a depth of field. It should be stated that the notion of depth of field, that is, some objects 
appearing to be in focus while others are out of focus, is not a function of the optics but rather a 
function of the resolving capability of the human eye and the imaging medium. ~ ge plane optical axis 
 F 1"6" U :~ V :~ Fig. 2 Lens Law We will assume the accepted standard that for a viewing distance U, 
the smallest resolvable patch by the human eye has a diameter of U/1000. This means that anything larger 
than this diameter is viewed as an area (patch) rather than as a point. Alternatively, one could say 
that the angular resolving power of the eye is 1 rail. Based on this criteria, the depth of field can 
be derived as follows. Assume that at distance U, we have a patch diameter U/1000 which is in focus, 
that is, we can resolve it as a point. This is illustrated in Figure 3. Let diameter of the patch be 
MN. Extending the patch in both directions, towards and away from the lens, we obtain triangles Alvin 
and BMN. Note that LD is the effective lens diameter, defined as the focal length divided by the aperture 
number (F/n). The resulting extension of the patch MN, denoted by ~" and tf', on the optical axis, [Figure 
3], will also remain in focus since their respective diameters are less than U/IO00. Since AALD iS similar 
to AAMN and ABLD is similar to ABMN we obtain the following two equations: U+-t] u + U-O-tr- a~d (2) 
MN LD MN LD 298 Computer Graphics Solving for u + and U-we obtain: u + -u/(t o (F/,) 1000) (3) U IF 
-u/(l + (F/.) tO00 ) (4) The following observation can now be made: 1. If U -1000F then 0 ~ -,- ** and 
IF --500F. The tl I1 distance 1000F is a close approximation to the n hyperfocal distance of a lens. 
A lens focused at this distance yields the maximum depth of field. 2. As the effective lens diameter 
is decreased, by increasing the aperture number n, the hypeffocal distance becomes smaller in magnitude 
yielding greater depth of field. Let us denote the hyperfocal distance as H. Then if we are focused at 
some plane U, the limits of the depth of field is given by: U+ UH H-O' H> U (5) - ~ H<~U IF UH H+U (6) 
,__ D + Fig. 3 Depth of Field 2.3 Image Focusing In this section we develop a measure of how well a 
point appears to be in focus on the image plane. Since t:he image plane is fixed at F, the focal length, 
a point that is out of focus will converge on a plane away from F, projecting onto the image plane as 
a circle rather than a point. This circle is called the circle of confusion and is a measure of how defocussed 
the image point is. When we are focused at some image distance U, based on our earlier calculations, 
a U* and a IF exist which also appear to be in focus. This implies that the corresponding image points 
V, Y + and w form circles on the image plane whose diameters are less than or equal to F/1000. Thus, 
observed from a distance F, the  Volume 15, Number 3 August 1981 images of these points are resolved 
by the eye as points and appear to be in focus. The diameter of the circle of confusion for each point 
in the scene, can be expressed as follows. Let V, and Vp be the image of points U and P. ~ forms a point 
on the image plane whereas v, projects into a circle as it converges a distance vp -v, away. Following 
Figure 4 we note that ALDA and ACBA are similar. Thus we have LD CB FU where r,- U-¥ U>F (7) v. v.-v, 
FD Vp -P'"F P> F Since LD =F/n and solving for CB, the diameter of the circle of confusion, we obtain 
F (8) c-I v.-Vpl .v. ~ L mage plane P ~ ~ -Vp--~4 U ~ VU~ Fig. 4 Circle of Confusion Note that as 
U--P, the plane of focus, the circle of confusion approaches zero. Points at infinity approach a limiting 
diameter given by ,Fv, F The diameter n" of the circle of confusion is highly asymmetric as one moves 
away from the plane of focus in the two directions along the optical axis as illustrated in Figure 5. 
V D F Diameter of . C = -----Circle of Confusion nF n f i t I 1 1 U-~U + Distanc~ Object plane U in 
focus Fig. 5 Asymmetricity of Depth of Field 2.4 D~raction Effects It is well known from physical optics, 
that diffraction effects due to a finite aperture size cause the image of point source to be spread over 
a larger area. Our objective in studying these effects is to determine the distribution of light intensity 
in the circle of confusion. It can be physically observed that a defocused image results in a loss of 
contrast. Assuming no energy loss in lens transmittance, the distribution of energy in the image must 
change to account for the loss of contrast. We describe here methods to determine the light intensity 
distribution within the circle of confusion for defocused points. Consider the diffraction of monochromatic 
light by a finite circular aperture of diameter d in an infinite opaque screen [Figure 6]. As shown, 
the screen is planar with rectangular coordinate system (xl,y,) and the image plane is parallel to the 
screen, at a distance a = F (focal length), with coordinate system(xe,yo). Let the field distribution 
of the wave be written as a complex function U(P) -A (P) e -~t(e) (9) where A(P) and ~(P) are the amplitude 
and phase of the wave at position P. '1 Y image plane x I / observation / zI=F (Focal Length) / Fig. 
6 Diffraction by a Circular Aperture The field amplitude at point (xo,yo), using the Huygens-Fresnel 
principle, can be written as U(xo,yo) -f f h~(xo,yo,x~,yx)U(xx,yx)axx ay, (10) ri circ (~2) where ho(xo.yo.xl.y,) 
1 e eel --cos(~,7ol)J~. rex k -2=/X I" 1 circ(-~2)" [1 2rl/d~l [ 0 otherwise el -I z~ + (Xo -xl) ~ + 
(.v0 -yx)~l ~" If the distance rot is large compared to the diameter of the aperture, the Fresnel approximation, 
and assuming an ideal thin lens whose transmittance is simply a phase transformation, then U(xo,y0) reduces 
to the Fourier transformation of the portion of field subtended by the lens aperture. Assuming a unit-amplitude, 
normally incident plane wave and radius coordinates ,0 -,/~ + y0' and ,i -~ + y~ ejkF Jkro2 U(r0) -~ 
e ~r l-I {U(r,)} (11) where H is the Hankel transform (due to the radial rl symmetry) of the aperture 
circ(~-2). The light intensity distribution (power spectrum) is given by [ kd212 I Jl(kdro/F) ]2 l(,o)-I--I 
2 (12) I 8F I kdro/F which is the well-known Airy pattern. A cross-section of this function is shown 
in Figure 7 where Jx is the Bessel function of the first order. The amplitude is dependent upon the lens 
diameter d thus making the intensity a function of the aperture. 1.0 i .~,,, 1 -4 -3 -2 -1 1 2 3 4 Fig. 
7 Light Intensity Distribution Function The derivation of the light distribution, for defocused monochromatic 
images of a point source by a circular aperture, was first investigated by E. Lommel in 1885. Details 
of this derivation, based on the Huygens- Fresnel principle, can be found in [8]. The results due to 
Lommel can be used directly with some minor changes in interpretation. The Huygens-Fresnel integral used 
to determine the light intensity in the circle of confusion cannot be obtained in a closed form and must 
be evaluated in terms of Bessel functions. The light intensity in the circle of confusion is given as 
follows: l(u,v) -[2l',y2(u,v) + Y,(,,,)] , o (13) where kd 2 le-8F 2 and V, and V, are Lommel functions. 
These functions can be evaluated by a series approximation given by V,(u,y) ~(-I)' (_")(.+2,) J(.+2,)(~) 
(14) - y 8-O where J(,+2,) are the Bessel functions of the n + 2s order. The variables u and v are dimensionless 
and specify the position of a point within the circle of confusion. They can be expressed as follows: 
u -k(_-~d_)' z (15)21 + V -- k'~"~ r0 --k -~ + y02 (16) Based on the expression derived by Lommel, the 
following observations are made: 1. At u=0, that is, the distribution of light intensity of a point focused 
on the image plane, is given by which is the previously derived Airy pattern. 2. Consider a plane at 
object distance p which is not in focus. This implies that the image of a point on this plane will converge 
at some image distance Vp. if the plane s is in focus, then Vp-v, is the distance by which the plane 
p is out of focus. To find the intensity distribution from the resulting circle of confusion, note that 
z = v, -V, or  U -k (d)~ (r, -V,) (17) Z/" and d v-k -ff ro (18) gives the position in the circle of 
confusion. 3. Note that for v-0, the light distribution at the center of the circle of confusion, we 
obtain [ sin(u/4)]l #(.,o)-i (u/4) I *'° (19) This shows that the highly peaked light distribution of 
the Airy pattern obtained for points in focus reduces in amplitude until it actually becomes zero at 
u/4 --±~ or z-+-4,rF2/kd ~. Thus, the distribution changes significantly for points out of focus resulting 
in lack of contrast for defocused points. 3.0 Synthetic Image Generation 3.1 Hidden-Surface Processor 
The image formation process consists of two stages. In the first stage, a ray-tracing hidden-surface 
algorithm generates point samples of light intensity in the 3D scene within the field of view of the 
camera model. The program uses Whitted's recursive illumination model [3] to generate images with surface-to-surface 
reflections, refraction in transparent surfaces, illumination by point light sources and shadows in complex 
3D scenes. Planar, quadric and bicubic surfaces can be rendered by the program at the present time. The 
raster image generated by this program is considered to be a grid of square pixels lying in the image 
plane, that is, each pixel is a finite square area of the image. The 3D scene is sampled at points which 
project into the pixel corners by intersecting rays from sample points, through the center of projection 
(focal point) and into the 3D scene [Figure 1]. The intensity of a pixel is computed by averaging the 
four corner samples, which is equivalent to fitting a bilinear function into the four samples and integrating 
this function over the area of the pixel. Antialiasing is performed by subdividing a pixel, which has 
a large difference in the sampled values, into 2x2 squares [Figure 8] and recursively repeating the sampling 
process as described in [3]. This subdivision is done only in pixels which contain sharp intensity changes 
in the image, typically caused by edges, silhouettes or textures. : / i I~ pixel e/, : ~-'-I--~ subdivisiOn 
 "/" " image sample point /  L/_ (at ~_p~xe i corner) . I I I t I    /:: L_;I kedge "-.... Fig. 
8 Image Antialiasing The program can optionally, in addition to generating the actual raster image, save 
the sampled values into a separate file. Each sampled value contains the following information: 1. The 
x, y coordinates in 0 3 , the 2D image plane coordinate system, of the sampled point, 2. the red, green 
and blue intensity values of the 3D scene at the sampled point, 3. the z coordinate (depth) in O ~, 
the 3D camera coordinate system, of the visible surface, and 4. the identification number of the visible 
surface (to be used in motion blur).  The parameters of the geometric camera model are also saved in 
this file. 3.2 Focus Processor The focus processor can generate a raster image, which is focused and 
has a depth of field, from the image point samples and the geometric camera model supplied by the hidden-surface 
processor and from given lens and aperture parameters. The focus processor approximates the integration 
process that takes place on the film plane during an exposure. We consider each intensity sample to 
be an independent source of light represented by a delta function with magnitude equal to the light intensity. 
The response of the optical system (the lens and aperture) to this delta function is an intensity distribution 
function on the image plane. Assuming that the intensity distribution is negligible outside the circle 
of confusion and that there is no energy loss in the system, the integral of the intensity distribution 
function over the circle of confusion is equal to the magnitude of the input delta function. The size 
of the circle of confusion of a point sample is computed from its z depth and the lens and aperture parameters 
by equation (g), the intensity distribution within this circle is computed by equation (13). The integral 
of the intensity distribution over the area of a pixel [Figure 9] is the contribution of the sample point 
to the intensity of the pixel. The sum of such contributions, from all sample points in the image, yields 
the intensity of the pixel. The integral of the intensity distribution is also attenuated by the square 
of the z depth distance of the point sample. Finally, the brightness of the images generated by the focus 
processor should be the same as the brightness of the source image generated by the hidden surface processor. 
Brightness changes in the image caused by film sensitivity, aperture setting and exposure time are not 
modeled here although they would be simple to add. Hence the following expression is used by the focus 
processor to compute pixei intensity of the three primary colors: N f(x~,y,,z,) Q(X,X+AX, Y,Y+A Y) '~ 
~ 'q' (20) " N f(x,,y,,~) where Q = the final intensity at pixel area (X,X + AX)(Y,Y + &#38;Y)  N = 
the number of point samples in the image q, = the intensity of point sample i x, ----- the x coordinate 
of sample i in the image plane y, = the y coordinate of sample i in the image plane za --- the z depth 
of sample i and also X+AX Y+A ¥ f(X,,,i,Z,)" ~X ~ '(Z~, ~/(X--Xi)2+(,--)',)2)z~ dX (21) with the condition 
that "f l(g,'~(x--x,)2+ly-y,)2)dy dx -l -w-m T. xmage plane / / / / / / Yi / Y Y+ ~Y X+ AX inte( 
ral f over pixel area (X, X+~ X) (Y, Y+ AY) Fig. 9 Integration of Intensity Distribution over a Pixel 
Area The focus processor computes a number of two-dimensional lookup tables at equally spaced z depth 
coordinates between the minimum and maximum z depth sample points in the scene. Each table contains the 
integral of the intensity distribution function, evaluated at the given z depth by (13), divided by the 
square of the z depth. This value is computed at each pixel that the corresponding circle of confusion 
overlaps. The table entries outside of the circle of confusion are filled with zeros. The size of each 
table is therefore determined by the size of the circle of confusion, in pixel units, at that z depth. 
The processor maintains a block of four values for each pixel in the image. These values contain the 
numerator of equation (20), one for each primary color, the fourth value contains the denominator of 
equation (20), common to all primary colors. For each input sample point, the processor selects the two 
lookup tables nearest to the z depth of the sample. The coresponding pixel entries in the two tables 
are linearly interpolated into a temporary table of coeffcients for the z depth of the current sample. 
The center of this table is displaced to the image coordinates of the sample point and the four values 
of all pixels that this table overlaps are updated. After all samples have been processed, the three 
primary color intensities of each pixel are computed by dividing the three numerators by the common denominator. 
 4.0 Results The focus processor described in the previous section has been implemented and used to 
generate a number of versions, with different lens and aperture parameters, of a test image. The geometric 
(pin-hole) camera version of the test image, at 512x512 resolution,  imagery. The model has been applied 
to the output of a hidden-surface program to generate images focused on various planes and an associated 
depth of field as a function of the camera parameters. Further extensions of this work should include 
generation of motion blur and modeling of special effect camera filters.  6.0 Acknowledgments The research 
reported here was supported by the National Science Foundation, Automation, Bioengineering, and Sensing 
System Program, under grant ENG 79-04821, Professor Herbert Freeman, Principal Investigator. This support 
is gratefully acknowledged. The authors would also like to thank Schlumberger-Doll Research for the facilities 
made available for the preparation of this paper. 7.0 References <RefA>1. Catmull, E., "A Subdivision Algorithm 
for Computer Display of Curved Surfaces', UTEC CSc-74-133, Computer Science Department, University of 
Utah, 1974 2. Blinn, J.F., Carpenter, L.C., Lane, J.M. and Whitted, T., "Scan Line Methods for Displaying 
Parametrically Defined Surfaces', CACM, 23, (1), January 1980, 23-34 3. Whitted, T., "An Improved Illumination 
Model for Shaded Display', CACM, 23, (6), June 1980, 343- 349 4. Blinn, J.F., "Simulation of Wrinkled 
Surfaces', Computer Graphics 12,(3), Atlanta, Ga, 286-292 5. Carlbom, I. and Paciorek, J., "Planar Geometric 
Projections and Viewing Transformations', Computing Surveys, 10, (4), December 1978, 465-502 6. Newman, 
W.M. and Sproull, R.F., Principles in Interactive Computer Graphics, McGraw Hill, Inc., Hew York, 1973 
 7. Goodman, J.W', Introduction to Fourier Optics, McGraw Hill, Inc., New York, 1968, Chapters 4,5 8. 
Born, M. and Wolf, E., Principles of Optics, 3rd (revised) edition, Pergamon Press Ltd., London, 1965, 
Chapter 8</RefA>   
			
