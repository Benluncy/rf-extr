
 A Lazy Evaluator Peter Henderson University of Newcastle upon Tyne ,lames H. Morris, Jr. Xerox Palo 
Alto Research Center Abstract: A different way to execute pure LISP programs is presented. It delays 
the evaluation of parameters and list structures without ever having to perform more evaluation steps 
than the usual method. Although the central idea can be found in earlier work this paper is of interest 
since it treats a rather well-known language and works out an algorithm which avoids ' full substitution. 
A partial correctness proof using Scott-Strachey semantics is sketched in a later section. I. Introduction 
This paper studies a non-standard method of performing the mechanical evaluation of expressions in a 
purely applicative language;i.e, one without assignment. The intuitive ideas behind this method are two: 
-Perform an evaluation step only when it is necessary. -Never perform the same step twice. It is somewhat 
surprising that these objectives can be approached through the use of rather simple data structures and 
algorithms. The following example should serve to acquaint the reader with the basic idea. integer procedure 
g(x,y); g := if x = 0 then 1 else y*y An ALGOL-60 programmer who wanted to enhance this procedure's speed 
by choosing whether to declare y a call-by-name or a call-by-value parameter would feel most uncomfortable. 
The value of y is going to be used either twice or not at all, depending on the value of x. The lazy 
evaluation technique overcomes this dilemma because the evaluation of g(E,F) will proceed as follows: 
(1) Substitute pointers, ct and /3, to the expressions E and F for the formal parameters x and y.  (2) 
Evaluate (i.e. reduce to a numeral) the contents of a. (3) If the result is 0 return 1. (4) Otherwise, 
evaluate the contents of ,fl and replace them with the resulting numeral. (5) Evaluate the contents 
of/3 again (this takes little time since already a numeral) and multiply the results.  Thus a lazy evaluator 
will perform the work to e~'aluate the second parameter either once or not at all. This e×ample illustrates 
the call-by-need mechanism of Wadsworth [7] and and the delay rule of Vuillemin [6]. Here we shall carry 
this strategy one small, but important step further as suggested in [6]: list structures are evaluated 
incrementally. In LISP parlance, an argument of CONS is not evaluated until and unless it is selected 
and examined by some later operation. Thus the statement car[cons[x;y]] = x is always true, even if the 
evaluation of x or y never terminates. This extension is pragmatically important bccause it allows the 
possibility for significantly different styles of programming as the following examples illustrate. Example 
I. Infinite Lists The function defined by integers[i] = cons[i; integers[i+l]] is quite useful under 
a lazy evahlation regime, integers[0] denotes the infinite list (0 1 2 ... ) and the expression car[cdr[integers[0]]] 
will evaluate to 1 via the following intermediate steps: car[cdr[cons[O; integers[0+l]]]] car[integers[0+l]] 
car[cons[O+l; integers[O+l+l]]] 0+1 1 In a similar way, the list defined by L = cons[l; cons[2; L]] 
is useful and computable. Example 2. A leaf comparator The following functions solve a problem posed 
by Carl Hewitt [2] to illustrate the need for co-routines. EqLeaves[x;y] = EqList[Flatten[x];Flatten[y]] 
95 Flatten[x] = if atom[x] then cons[x;NIL] else Append[Flatten[car[x]]; Flatten[cdr[x]]] Append[x;y] 
= if null[x] then y else cons[car[x];A ppend[cdr[x];y]] EqList[x;y] = if null[x] then null[y] else if 
null[y] Ihen false else if eq[car[x];car[y]] then EqList[cd r[ x];cdr[y]] else false In other words, 
EqLeaves tests two S-expressions to see if their atoms are identical, independent of structure. This 
obvious solution uses Flatten to eliminate the structure, then uses EqList to compare the atoms. It would 
be an unnecessarily slow method under normal circumstances because applied to a pair of expressions like 
( A Huge1 ) and ( B Huge2 ) it would go to all the work of Flattening Hugel and Huge2 even though the 
answer is false because of the first atoms in each structure differ. If a lazy evaluator is used, however, 
there is no need to change the solution to one involving co- routines because the same computational 
effect will be achieved automatically. Suppose location ~r 0 holds the expression to be evaluated. The 
computation will follow this pattern: Wo: EqLeaves[(A Hugel);(B Huge2)] First ~r 0 is updated with the 
definition of EqLeaves with actual parameters substituted for formals. no: EqList[Flatten[(A Hugel)];Flatten[(B 
Huge2)]] Now pointers, 71/1 and 7r2, to the parameters are substituted into the definition of EqList 
without any evaluation of the parameters. 7to: if null[Trt] then null[Tt2] else if null[~r2] then false 
else if eq[car[~rl];car[~2] ] then EqList[cdr[Trl];Cdr[n2] ] else false 7rl: Flatten[(A Hugel)] ~2: Flatten[(B 
Huge2)] The primitive null now forces the lazy evaluator to go to work on the contents of ~r 1. ~rl: 
if atom[(A Hugel)] then cons[(A Hugel); NIL] else Append[Flatten[car[(A Hugel)]]; Flatten[cdr[(A Hugel)]]] 
 7rl: Append[Flatten[car[(A Hugel)]]; Flatten[cdr[(A Hugel)]]] ~rl: if nullDr3] then ~t. else cons[car[It 
3];A ppend[cdr[~3];~4] ] ~r3: Flatten[car[(A Hugel)]] 7r4: Flatten[cdr[(A Hugel)]] Again the primitive 
null forces evaluation steps on ~r 3. ~r3: if atom[~t5] then consists;NIL ] else A Plfend[ Flatten[cdr[~ 
5]];Flatten[cdr[It 5]]] ~r5: car[(A Hugel)] The primitive atom forces the evaluation of ~r s. ~r5: A 
7r3: cons[~rs;NIL ] since atom[A] is true ~rl: cons[car[n3]; Append[cdr[lr3]; ~r4] ] since null[cons 
...] is false 7to: if null[T2] then false if eq[carDrl];car[~r2] ] then EqList[cdr[Trl];Cdr[~r2] ] else 
false If we choose to view Flatten as a co-routine, at this point we would say that it has produced 
its first ,~u,~, ~a,t,,3j -A, and its context has been saved in ~r 4 for later reacti vation. Now the 
contents of ~t 2 is evaluated in the same way until we have ~r2: cons[car[T6]; Append[cdr[~6];~rT]] 7r6: 
cons[B;NIL] 7r7: Flatten[(B Hugel)] ~r0: if eq[car[~rl];car[Tr2] ] then EqList[cdr[nl];Cdr[~r2] ] else 
false The primitive eq forces ~1: cons[A; Append[cdr[zt3]; ,/r4] ] ~2: cons[B; Append[cdr[~r6]; lr7] 
] Finally the test is made and the computation terminates with n0: false Notice that Hugel and Huge2 
did not enter into any of the foregoing computation and that the work done to evaluate the subexpressions 
in ~r I and ~r 2 for the benefit of the null primitive is not repeated when the eq primitive examines 
the parameters. Generalizing from this example we can see that a large class of co-routine applications 
can be subsumed by this technique. A producer co-routine becomes a function that produces a long (possibly 
infinite) list and a consumer co-routine becomes the receiver of such a list. Also, notions such as streams 
and the dynamic lists of POP-2 are subsumed. The purpose of these programming constructs is to allow 
one to describe a sequence of values with a single sub-program, yet have them computed on a hand-to-mouth 
basis. This assumption is built into a lazy evaluator at the most basic level so there is no need to 
call for it explicitly. One the other hand. one might ask how to force a more conventional evaluation 
to occur. Suppose one wishes to cause the evaluation of f[s] to proceed conventionally, computing the 
S-expression s before invoking f. One could 96 say, instead of f[s], Force[f,s] where Force[f,s] = if 
Finite[s] then f[s] else don't care and Finite[s] = if atom[s] then true else if Finite[car[s]] then 
Finite[cdr[s]] else don't care The function Finite simply explores the complete S-expression, forcing 
every part of it to be evaluated. If it ever terminates, Force invokes the function on the now-evaluated 
argument. Example 3: Prime Numbers (due to P. Quarendon) primeswrt[x;I] produces a new list from I by 
removing all multiples of x primes[I] produces a new list from I by removing any element which is a multiple 
of a predecessor. primeswrt[x;I] = if car[I] rood x=O then primeswrt[x;cdr[I]] else cons[car[I];primeswr 
t[x;cd r[I]]] l)rimes[I] = cons[car[I];primes[primeswrt[car[I];cdr[I]]]] then primes[integers[2]] is 
the infinite list of prime numbers. Ii. A lazy evaluator for Hyper-Pure LISP In this section we shall 
describe a language and its implementation in order to crystallize the notion of lazy evaluation. Hyper-Pure 
LISP is a variant of LISP 1.0 which remains true to the principles of the ~.-calculus [1]. Specifically, 
FUNA~G binding is the only possibility. The syntax of expressions in this language is as follows: (expression> 
::= <variable> I (QUOTE (atom>) I (CONS <expression> <expression>) I (CAR (expression>) I (CDR (expression>) 
I (ATOM <expression>) I (EQ (expression> <expression~.) I (IF (expression> <expression> <expression>) 
I ((expression> (expression>) I (LAMBDA (variable> (expression>) I (LABEL <variable> <expression>) I 
(FUNARG (expression> <alist>) <alist> ::= <empty> I (variable> (expression> <alist> (atom> ::= <any string 
of capital letters> <variable> ::= <any atom except CONS, CAR, CDR, ATOM, EQ, IF, LAMBDA, QUOTE, LABEL, 
or FUNARG) The variations from the syntax of LISP are the replacement of COND by IF, the restriction 
of QUOTE to atoms, the restriction of LAMBDA-defined functions to one argument, and the elevation of 
the FUNARG construct from an internal bookkeeping device. The first three restrictions are inessential 
and the FUNARG phrase is an extension. Intuitively "(FUNARG el x e2 y e3)" means "el where x = e2 and 
y = e3". It will greatly simplify the discussion if we assume that the computer memory is of a very accommodating, 
if unrealistic, sort: each cell is capable of holding any of the forms listed as <expression>s where 
addresses are used for any component of type expression. In other words, if 90,. 91, etc. are addresses 
and ~0'~]' etc. are variables, a single memory cell is capable of holding (and discriminating among) 
items like ¢0 ' (CONS 91 92) , (90 ~71) , (LAMBDA ~1 ao)' and (FUNARG ~r 0 ¢1 91 ¢2 92)" Naturally, any 
real implementation would represent such variable-sized memory cells with linked lists; but the extra 
pointers would only complicate this discussion. The state of a computation is described by a partial 
memory function, g, which has the following consistency property: If a particular address, 7r, occurs 
as a component anywhere in the memory (i.e. in #'s range), then #(9) is defined. Thus the addresses for 
which # is undefined are the free cells and no non-free cell points at a free one. A computation is started 
by loading the memory with the expression in the obvious way, performing transformations for a while, 
and then examining the root cell of the expression. As the first step in describing the lazy evaluator 
we describe a set of reduction rules which transform the memory, #, to produce a new memory #'. Each 
rule changes just a few cells. To describe such an altered function we introduce some notation: #[ e 
/ 90 ] = ~.u. if 9=~t 0 then e else #(~) In other words the new function differs from the old at just 
one argument, 90 where its value has become E. Furthermore g[ tl/~tl' E:2hr2 ] = (#[ el/Irl 1)[ e2/~t 
2 ] In other words the rightmost pair represents the last change to the memory. There are seven transformation 
rules. [C] g(90) = (CAR 91) and #(?r l) = (CONS ~r 2 93) ;' = ~[ #(92) / ~o ] In programming terms, the 
contents of 92 are copied into 770. When convenient we shall dispense with mention of extra references 
by convening that p(Tr0) = (CAR (CONS 712 93)) implicitly asserts the existence of a 9 t for which the 
above is true. The rule for CDR is similar. p.(90) = (CDR (CONS 92 93) ) ~ kt'= #[#(93) / ~t0] [A] #(90) 
= (ATOM(QUOTE a)) /z' = #[(QUOTE "F) / ~t 0 ] ~(9o) = (ATOM(CONS ~r 92) ) /~ = #[(QUOTE NIL) / 90 ] 
I.e. "atom-hood" is simply the property of being QUOTEd. 97 [E] /Z(no) = (EQ(QUOTE al)(QUOTE a2) ) /tt'= 
/z[if % = a 2 then(QUOTE T)else(QUOTE NIL) /%] [I] /t(n0) = (IF (QUOTE T) 7r 1 n2) ~ #'= /t[#(nl) / 
no] p.(n0) = (IF (QUOTE NIL) n 1 n2) ~ /.t'= /t[/t0r2) / no] Thus IF is just an alternate form of the 
usual CONDprimitive. The following rule is a sort of incremental substitution operation which allows 
FUNARG expressions to bubble down through the memory. When the specific list of variable-address pairs 
in a FUNARG expression is not relevant we use 0 to denote it. IF] (a) #(~o) = (FUNARG n' ~1 nl "'" ~n 
lrn) and ttt(n') = ~i for some l<i<n (choose the smallest i) #' =/uE#(~r i) / % ] (b) p.(~ro) = (FUNARG 
(QUOTE a) 0) p' = ~[(QUOTE a) / n 0 ]  (c) #(N0) = (FUNARG (n t n2) 0)  =O #'= ~[(~l' ~2') / %' (FUNARG 
71 t 0) / ~l" (FUNARG 172 0) / n 2' ] where w l' and n 2' are distinct free cells in p (d) If a is one 
of CONS, CAR, CDR, ATOM, EQ, or IF then #(no) = (FUNARG (an l ... ~m) 0) p?= p.[(a n 1' ,.. rim') / 
n 0, , (FUNARG n I 0) / hi' ..... (FUNARG n m 0) / nrn' ] where hi', ~l',-", rim' are distinct free 
cells in p  Rules Fc and Fd are the only ones which use additional storage. Although it appears here 
that the list of pairs 0 is being duplicated, in a real implementation which represents a single, variable-sized 
cell with multiple linked cells, it would not be; only a pointer to the list would be duplicated. Notice 
that rule F does not tell what to do when a LAMBDA or LABEL construct is encountered. The following rules 
give an answer for some cases. The first one shows how variables are bound to values in FUNARG lists, 
and the second shows how recursion is implemented. [G] #(n0) = ( (FUNARG (LAMBDA ~ hi) 0) n2) /z'=#[(FUNARG 
n I ~ n 2 0) / ~r0] The following rule creates a circular structure. [L] Given p.(n) = (FUNARG (LABEL 
~ ~o) O) p' = ~[(FUNARG % ~ ~ 0) / u ] What can be said of these rules? The reader may wonder whether 
they "work". For example, one of the authors (Morris) thought that rule G might suffer from the usual 
capture of free variables problem, and was surprised to find it was not the case. This issue is taken 
up in the next section. Presently we shall continue with the description of the evaluator. Note that 
at most one rule can be applicable to any particular "location, so the only choices left open to an evaluator 
are where to perform the next reduction and when to halt. The procedure for a lazy evaluator is defined 
recursively as follows: Eval(n,#) : n is the location to be reduced /.t is the memory Eval returns a 
new memory if p(n) = (QUOTE a) then return p if p(n) = (CONS ...) then return p if p(n) = (FUNARG (LAMBDA 
...) ...) then return/1. [cH if #(n) = (CAR ~') then let #1 = Eval(u',/z) if #l(n') = (CONS ~I ~'2 ) 
then let #2 = Eval(nl,/~l) return p2[/z2(~t) / ~r] else there is an error, CAR applied to non-pair [C2] 
if B(n) = (CDR n') then let #1 = Eval(n',g) if pl(n') = (CONS It l 172) then let p. 2 = Eval(~r2,gl) 
return /t2[/.t2(n2) I n] else there is an error, CDR applied to non-pair [A] if/z(n) = (ATOM n') then 
let #1 = Eval(u'jz) it #l(n') = (QUOTE a) then return #l[ (QUOTE T) / hi; if #l(n') = (CONS ...) then 
return #i[(QUOTE NIL) / n]; else there is an error [E] if/u(u) = (EQ ~t ~2) then let Pl = Eval(n2,Eval(~l,P)) 
if #i(nl)=(QUOTE al) A /s.l(n2)= (QUOTE then if aj = a 2 then return #j[(QUOTE T) a2/)n] else return 
#i[(QUOTE NIL) / u] else there is an error 98 [l] if pot) = (IF ~'o ~'l ~2) then let /t I = Eval0r0,~) 
if btl(~r0) = (QUOTE T) then let #2 = Eval0rl'~l) return #2[P.2(~1) / ~ ] if ~(~o) = (QUOTE NIL) then 
let /.t 2 = Eval0r2,/tl) return /z2[/t2(~r2) / ~ ] else there is an error [Fa] if p.(Tr) = (FUNARG ~0 
~1 ~'1 ""~n ~n ) and /z(lr0) = ~0 then if i is the smallest such that ¢o = ~i then let #1 = Eval0ri,#) 
return #l[/tl(~ri) / Ir ] else there is no such i and there is an unbound variable error [Fb] if #(~) 
= (FUNARG (QUOTE a) ...) then return p[ (QUOTE a) / ~r] [L] if #(~r) = (FUNARG (LABEL ~ ~') O) then 
return EvaI0r,#[(FUNARG ~r' ,~ ~t 0) / rt ]) [FC] if P(~)e Or 0 ~l) ...) =thn (FUNARG apply rule Fc 
to yield/,t' return Eval(~,#') [Fd] if #(~r) = (FUNARG (a ... ~n) "") then apply rule Fd to yield/.t' 
return Eval0r,#')  [G] if bt(~r) = (~r 0 ~rl) then let #1 = Eval(~o~) if/~l(~r0) = (FUNARG (LAMBDA ...)...) 
then apply rule G to location ~r to get #2 return Eval(~,#2 ) else there is an error else there is 
an error The most subtle aspect of this algorithm centers about rules C, I, and Fa. For example, in C1, 
the first component of the pair is evaluated in situ before being copied into 7r. This policy maximizes 
the number of paths through the data structure which "see" the change. A conventional evaluator differs 
from a lazy evaluator in that the parameters of CONS are evaluated as soon as they are encountered and 
the actual parameter of. a function call are evaluated before they are bound. In terms of the foregoing 
definition of Eval this means that the second case is changed to if /z(~)e=th.(CONS ~l qr2 ) return Eval(Tr 
2 ,Eval0rÂ ,/t)) and the case [G] is changed to if/tOO = (If 0 ~I) then let #1 = Eval(~l'EVal(~o,~)) 
if #1(~o) = (FUNARG (LAMBDA ...)...) then apply rule G to location ~i and #1 to get #2 return Eval0r,~2) 
else there is an error These two changes make certain other invocations of Eval unnecessary. Specifically, 
the case C1 becomes if #(~) = (CAR ~') then let #I = EvaI(~',p,) if /z](~') = (CONS ~1 ~2 ) then return 
#lfP.l(Ttl) / ,//-] else there is an error, CAR applied to non-pair Case C2 changes analogously, and 
case Fa becomes if#(~) --(FUNARG ~o ~1 ~1 ""~n ~n ) and #(~o) = ~0 then if i is the smallest such that 
~o = ~i then relurn #[~(~i) / ~ ] else there is no such i and there is an unbound variable error All 
the other cases remain the same. We believe that the lazy evaluator never performs more reduction steps 
than the conventional one but shall not attempt to prove it here. Example 4. To illustrate the lazy evaluator 
we shall perform a full evaluation of the expression from example 1. To get things started it is necessary 
to embed the expression to be evaluated in a FUNARG expression with all empty alist. Also, we shall assume 
that PLUS is a primitive operator with the same general characteristics as EQ and that numerals are understood 
to be QUOTEd atoms. Not all addresses are explicit; each pair of parentheses indicates the presence of 
an unnamed address. a:(FUNARG ((LAMBDA INTS (CAR(CDR(INTS 0)))) (LABEL INTEGERS (LAMBDA I (CONS I(INTEGERS 
(PLUS I 1))))))) Apply Fc to ct a: ((FUNARG (LAMBDA INTS (CAR(CDR(INTS 0))))) n: (FUNARG (LABEL INTEGERS 
(LAMBDA I (CONS I(INTEGERS (PLUS I 1)))))) Apply G to a a: (FUNARG (CAR(CDR(INTS 0))) INTS n) Apply 
Fd to a a: (CAR "y) 77: (FUNARG (CDR tINTS 0)) INTS n) Stack a, Apply Fd to y ~: (CDR ~) 99 /5: (FUNARG 
(INTS 0) INTS/3) Stack v,Apply Fc to 8 /5: (t ~o) E: (FUNARG INTS INTS fl) ~0: (FUNARG 0 INTEGERS/3) 
Stack 6, E, Apply L to B /3: (FUNARG (LAMBDA i (CONS I(INTEGERS (PLUS i I)))) INTEGERS/3) Return to 
~, Apply Fa e: (FUNARG (LAMBOA i (CONS I (INTEGERS(PLUS I 1)))) INTEGERS fl) Return to /5, Apply G i5: 
(FUNARG (CONS ! (iNTEGERS(PLUS i 1))) irp li~ fEGERS fl) Apply Fd to ~5 /5: (CONS ~ 7) ~: (FUNARG I I 
~ INTEGERS fl) 7: (FUNARG (INTEGERS (PLUS I 1)) I ¢p INTEGERS/3) Return to T to note CDR, Stack T again, 
Apply Fc to r/ 7: (~ s) t: (FUNARG INTEGERS I ¢p INTEGERS 13) s: (FUNARG (PLUS I 1) I ¢p INTEGERS/3) 
 Stack O, Apply Fa to t t: (FUNARG (LAMBDA I (CONS ! (INTEGERS (.PLUS I 1)))) INTEGERS/3) Return to 7, 
Apply G ~7: (FUNARG (CONS ] (INTEGERS (PLUS I 1))) is INTEGERS/3) Apply Fd to n 7: (CONS ~ X) x: (FUNARG 
I I INTEGERS/3) h: (FUNARG (INTEGERS (PLUS i 1)) I s iNTEGERS fl) Return to T, Apply C T: (CONS ~ h) 
Return to a to note CAR, Stack a again, Stack x, Apply Fd to s s: (PLUS # ~,) /z:(FUNARG i I rp INTEGERS 
fl) v:(FUNARG 1 i ¢p iNTEGERS fl) Stack s, Stack #, Apply Fb to ¢p ¢p: 0 Return to# , Apply Fa to# #: 
0 Return to s to note PLUS, Stack s, Apply Fb to v v: 1 Return to s, Apply P s: 1 Return to x, Apply 
Fa K: 1 Return to a, Apply C a: 1 !11. Semantic Considerations There are several questions one might 
ask about the foregoing transformation rules and evaluator. (1) Is the final answer independent of the 
order in which the rules are applied; i.e. does the system have the Church-Rosser property? (2) Are 
there enough rules to allow an answer to be computed in all cases we consider legal? (3) is the evaluator 
complete in the sense that will compute an answer whenever any application of the rules will do so? 
(4) is the evaluator optimal in the sense that it performs the minimum needed steps to compute an answer? 
 We believe that the answer to the first three is yes, and know that the fourth is not true. However, 
they are not very meaningful questions unless we have an independent definition of what an "answer" is. 
To get one we shall first define the meaning of an expression in terms of Scott-Strachey semantics [4] 
and then define an answer as a certain finite amount of information about that meaning. An analogous 
approach for arithmetic would be as follows: 100 (1) Given the class of arithmetic expressions involving 
numerals and no variables (e.g. "4", "4+5", "5-(6+2)"), consider the domain of integers (i.e ..... -2,-1,0,1,2,...). 
 (2) Define a semantic function, V, mapping expressions into domain elements. V("9") : 9 and V("4+5") 
: 9 (3) In this case the definition of an answer is obvious: Any computer should reduce its input expression 
to the (possibly signed) numeral which has the same value as the original expression.  The central idea 
is that the computer does not find the value for the expression, but only reduces the input to a more 
comprehensible form which has the same value. When, as in the case of arithmetic, there is a unique, 
finitely representable canonical form for any value the distinction between a value and the output expression 
is not interesting. On the other hand, here we are dealing with values that can be infinite list structures 
and functions so the distinction between a value and an answer is real. The definition of a semantics 
for the h- calculus has already been carried out by Wadsworth and others, see [5,7]. Our approach follows 
theirs but extends it somewhat to introduce the notion of a semantic memory. This approach allows us 
more easily to make the connection between the semantics and the evaluator, In particular, it allows 
us to deal with the sharing and sometimes circular data structures more directly. The Domains A -the 
primitive domain of atoms, a,a',a 1, etc. denote atoms. C -the primitive domain of variables. ~ denotes 
a variable. There is no reason why atoms cannot be used as variables, but things seem clearer if they 
are kept distinct. R the primitive domain of references (addresses). It denotes a reference.  E : C 
+ A + RXR + R + R + R + RXR + RXRXR + R×R + CXR + CXR + R×(C×R)* the domain of expressions corresponding 
to the twelve possibilities listed in section II. In other words (CAR 77) is a member of the fourth part 
of the disjoint union, e denotes an expression. M : R ~ E -the domain of memories, each cell of which 
is capable of holding an expression. # denotes a memory. V = A + ( V X V ) + ( V ~ V ) -the domain of 
values. A value can be an atom, a pair of values, or a function from a value to a value. This domain 
is the one which requires Scott's theory as it contains its own function space. Furthermore we use Reynolds's 
[3] version of the disjoint union operator so that _L, (QUOTE ..L), (CONS _L _L), and (LAMBDA X _L) all 
have distinct values, with _L being the bottom of the whole lattice. N : C ~ V -the domain of environments, 
p denotes an environment. S : R ~ ( N ~ V ) -a semantic memory, mapping references and environments 
into values, o denotes a semantic memory. A semantic memory has addresses just  like a conventional 
one but its cells can hold genuinely infinite objects. Roughly speaking the semantic object that a cell 
w holds is what one gets by tracing out, in the conventional memory, the structure of pointers emanating 
from that cell. Tile Semantic Function V In the following, to reduce parentheses, we shall adopt the 
convention that fa,B~ means ((f(a))(fl))(~);i.e. f is applied to a, the result is applied to fl, etc. 
V maps conventional memories into semantic memories;i.e. V:M~S it is defined recursively by Vp~p = U(V#)(#~)p 
where /.k~Ep = if E E C then p~ else if e = (QUOTE a) then a if ~ = (CONS ~1 7t2) then <a~lp , a~2p> 
else if c = (CAR ~') then (an'p) 1 else if ~ = (CDR ~') then (of'p) 2 else if ~ = (ATOM ~") then [ if 
a~'p E A then "T" else if aTt'p C V X V then "NIL" else .L ] else ife = (EQ~t 1 1t2) then [ if O~lp 
E A A a~2p E A then if a~lp = a~2p then "T" else "NIL" else _L ] else ife : (IF~0w! ~2) then [ if art 
0 p = "T" then a~tlP else if art 0 p = "NIL" then a~t2p else .L  ] else if e = (~0 ~1) then cr~Op (aztlp) 
else if E = (LAMBDA ~ ~') then ~x. aTt'#[ x/~ ] else if ~ = (LABEL ~ ~t') then Y {hx. aT'p[ x/~ ]} else 
if ~ = (FUNARG w0 ~I ~1 "'" ~n ~n) then a~ 0 P[ altnPl~n,...,a~tPl~ l ]  else .1. This definition depends 
upon several informal semantic operations: <,> forms pairs, a subscript selects components,etc. The most 
noteworthy are the last four: The application of a~0p to awlp is the function application which required 
Scott's construction to justify since both values reside in the same domain. Note that the h is also 
an informal notion and that the bracket notation is used to define the changed environment p[ x/~ ]. 
Y is the minimal fixed point operator. It happens that mapping a FUNARG construction into its semantics 
involves a complete reversal of the list of bindings. I01 It is instructive to compare this function 
with the Eval function of section II. The major difference is that V is quite happy to deal with completed 
infinite objects like functions. The operation of applying a function to its argument is taken as primitive 
here, while it was done very slowly and incompletely by Eval. This particular way of describing the function 
isolates the application of the memory function to a single place, namely the /tw in V's definition; 
U uses only the semantic memory (7. Now, given the definition of Eval, it should be clear what an answer 
is. Suppose we load the memory with an expression so that the root of the expression occurs in w0, and 
we start the lazy evaluator on that location. If it ever halts (ignoring the possibility of error stops) 
we know that w 0 will contain an expression with one of the three forms (QUOTE a), (CONS ..), or (FUNARG(LAMBDA...)...). 
Thus the answer tells which of the three components of V the value lies in; and, if it is an atom, what 
atom it is. In the other cases, nothing more is revealed, or computed. This fact indicates how we should 
define the correctness of an evaluator: If I'itw0_L = _L then Eval(w0,/u. ) does not halt. If I'jZwo.L 
= a then Eval(w0,g)(w0) = (QUOTE a). If Vgwo_L is a pair then Eval(w0,#)(w0)=(CONS ...). If V/uw0_L is 
a function then Eval(~0,#)(w0)=(FUNARG(LAMBDA..)...). We use the empty environment, _L, in these statements 
because it is assumed that the expression to be reduced does not contain free variables at the outermost 
level. Therefore, an environment function is not needed initially. Soundness For the present we shall 
content ourselves with sketching a partial correctness proof;i.e, that the last three clauses hold if 
Eval halts. This can be done by showing that each of the seven transformation rules leaves the semantic 
memory unchanged (except at newly allocated cells). Then if the evaluator starts with memory /t at location 
770 and halts with memory #' we know that V/tw0.L = V/t'Wo..L , and the last three c!auses are immediate 
since the Eval function halts only on the three forms in question. This method of proof also proves a 
qualified "yes" to the first question in this section; it doesn't matter in what order the transformations 
are api~lied. The argument goes as follows: On semantic grounds we know that (QUOTE a), (CONS ...) and 
(FUNARG(LAMBDA ...)...) all denote distinct objects, and that atoms that look different are different. 
If we then show that the transformations cannot change the semantic value in a cell, we know that all 
sequences of transformations which produces one of those configurations in a cell must produce the same 
one. The proofs for the various rules are very similar. We shall give two. Rule C: Suppose /t(n0) = (CAR 
Wl) and /t(nl) = (CONS w 2 w3) and rule C is applied. We shall prove that V/t = V#" where /t' = /t [ 
#(~2) / w 0 ]. First we define the truncations of V by V i =_L for i_<0 v~+~/twp: u(r,/t)(/t~)p so V 
= lim i V i Now a straight-forward computation shows that Vi/.t = ~wp.if w=w 0 then U(Vi_i/t)(CAR(CONS 
w 2 w3))p else U(Vi_l/t)(/t~)p = hwp.if ~=~r 0 then {U(Vi_2/t)(CONS w 2 7t3)p} 1 else U(Vi_l/t)(g~)p 
 = hwp.if w=w 0 then U(Vi_:)(/t~2) p else U(Vi_t/t)(/t~)p and V:'=hwp. if w=~ 0 then U(I'i_l/t')(#w2) 
p else U(Vj_lg')(/t~)p Now it is easy to show that (Vi)(3j)[ Vi/t C I~' ] and vice versa. Thus lim i 
Vi/t = limjlK~'. Rule L: Here the proof is somewhat different since it involves comparing a circular 
data structure with a minimal fixed-point. Suppose #(no) =(FUNARG (LABEL ~ ~t) ~2 ~2 "'" ~n Wn) and 
IC = #[(FUNARO w I ~ ~0 ~2 w2 "'" ~n qln)/q/0 ] First, we claim without proof that V# and V#' are the 
minimal solutions to the following equations, respectively: V# = hwp. if lz=Iz o then Y{Xx. V#wtP [ V#~tnPl~n...Vl~Tt2Pl~p 
x/~]} else U( V#)(#~)p Vg' = hwp. if w=w 0 then V#'w]p[ V#'WnO/~n...V/t'~2P/~2, V#'woP/~ ] else U(V/t') 
(/tw)p It is simple to show that V# satisfies the equation for V/t' so V#' C V/t. The proof in the other 
direction is more difficult. The difficulty seems to be that V# involves a loop, represented by the Y, 
within a loop, represented by the definition of V, while V/t' involves just one loop. To overcome this 
problem we will "cut" the loop represented by Y at the same time we cut the loop represented by V. First, 
we claim (based on continuity) that the following equations are equivalent to the ones above for defining 
V/t. Vi/t = ..L for i<0 hwp. if 7t=It 0 then Yi{hx.Vi_lgWlp [ Vi_lp.W~/,~n.. .Vi_I/tw2p/{2, x/{]} else 
U(Vi_l/t)(/tw)p Yi = _L for i<0 Yi = hf'f(Yi-lf) V = lira i V i Now we prove Vi/t C V/t' by induction 
on i. It is vacuously 102 true for i ~ 0, so assume it for all k<i. Then, by the Acknowledgement induction 
hypothesis I~# C_ X~rp. if ~r:lr 0 then Yi F else U( Vil')(#~r )p where F = {hx. Vg'~rtp [ VIl'TrrvO/~n...V#'~r2P/~2, 
x/,~]} The proof will be complete if we can show ViF C_ V#'~r0P This fact can also be shown by and induction 
on i. Assume the result for all k<i,then Yi F : V/l'Trl.p[ V~'~rnP/~n...Vtt'Tr2Pl~2, Yi_iF/~ ]   C 
vW~!p[ vW~,.p/~,...vW~2P(~2, v~'~op/~ ] by the induction nypothesis : y~'~ IV. Remarks The two objectives 
presented at the beginning of the paper can now be g ve ~ more substance. First, the general question 
of when an evaluation step is necessary needs to be answered. Initially, some external consideration 
must indicate that a particular location's value must be pursued; e.g. the user would like that value 
to be printed. Then, the "need to be ewduated" propagates itself to the descendants of that location 
according to rules peculiar to the semantics of the language. These rules were straightforward for the 
language studied here. Sometimes they are less so. For example, changing the semantics of IF so that 
(IF p x x) : x even when p is undefined would require simu!taneous evaluation of all three parts of an 
IF clause. In any case, it appears that performing "outer-most" reductions, i.e. those closest to the 
initial source of the need, is a good heuristic. The reason is that these reductions may make some parts 
of the structure which are farther away unnecessary. Second, the notion of "same step" needs clarification. 
Here, all that has been achieved through the use of pointers is that the advantages of evaluating an 
expression earlier have been I~LdlIICU. II t,C c^plc~tU, "3+7" arose twice in an evaluation from entirely 
different places there is no simple way to avoid its recomputation. Finally, a comment on the usefulness 
of Scott-Strachey semantics. We can hardly claim that the lazy evaluator is "right" because it is correct 
with respect to the semantics defined in section II1. It is obvious that the semantics can be adjusted 
to fit any mechanical evaluation method one chooses. One the other hand the use of a semantic model is 
a great aid in studying the implications of various evaluation rules without gettting involved in too 
many details. Section Ill of this paper was primarily the work of the second author with significant 
help from Howard Sturgis of Xerox PARC.  References <RefA>[1] Curry,H.B. and Feys,R., Combinatory Logic, vol 
!. North- Holland, 1958. [2] Hewitt, Carl, et. al., Behavioral semantics of non-recursive control structures, 
Proceedings, Colloque sur la Programmation, Springer-Verlag Lecture Notes in Computer Science, No. 19, 
1974. [3] Reynolds, J. R., Notes on a lattice-theoretic approach to the theory of computation, Lecture 
notes, Syracuse Unversity, 1971. [4] Scott, D. and Strachey, C., Toward a mathematical semantics for 
computer languages, Proc. of the Symposium on Computers and Automata, Polytechnic Institute of Brooklyn, 
and PRG-6 Oxford University Computing Laboratory, 1971 [5] Stoy, J., The Scott-Strachey approach to the 
mathematical semantics of programming languages, Course notes at M.I.T. Project MAC, 1973. [6] Vuillemin, 
J., Correct and optimal implementations of recursion in a simple programming language, Journal of Computer 
and System Sciences, vol. 9, No. 3, December 1974. [7] Wadsworth, Christopher, Semantics and Pragmatics 
of the Lambda-calculus, PhD. thesis, Oxford, 1971</RefA> 103 
			
