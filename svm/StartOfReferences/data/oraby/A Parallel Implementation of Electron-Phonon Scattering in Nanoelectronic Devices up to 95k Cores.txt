
 A parallel implementation of electron-phonon scattering in nanoelectronic devices up to 95k cores Mathieu 
Luisier Network for Computational Nanotechnology, Purdue University, West Lafayette, IN 47907, USA Email: 
mluisier@purdue.edu Abstract A quantum transport approach based on the Non­equilibrium Green s Function 
formalism and the tight-binding method has been developed to investigate the performances of atomistically 
resolved nanoelectronic devices in the presence of electron-phonon scattering. The model is integrated 
into a quad­level parallel environment (bias, momentum, energy, and spatial domain decomposition) that 
scales almost perfectly up to 220k cores in the ballistic limit of electron transport. In this case, 
the momentum and energy points form a quasi-embarrassingly parallel problem. The novelty in this paper 
is the inclusion of scattering self-energies that couple all the momenta and several energies together, 
requiring substantial inter-processor commu­nication. An ef.cient parallel implementation of electron-phonon 
scattering is therefore proposed and applied to a realistically extended transistor structure. A good 
scaling of the simulation walltime up to 95,256 cores and a sustained performance of 142 TFlop/s are 
reported on the Cray-XT5 Jaguar. I. INTRODUCTION With the rapid decrease of the semiconductor device 
di­mensions [1], [2], [3], technology computer aided design (TCAD) is entering a new era where one-dimensional 
models and classical approximations such as drift-diffusion are no more valid and need to be replaced 
by more evolved, but computationally more intensive approaches based on discrete quantum mechanics. A 
physics-based TCAD tool speci.cally dedicated to nanoscale transistors is the key to investigate the 
performances of novel transistor designs made of a countable number of atoms [4], [5], [6], [7], [8]. 
It should include an atomistic representation of the simulation domain, offer a full-band description 
of the semiconductor properties, and be capable of solving quantum transport in devices of size comparable 
to what experimentalists can produce [9]. Such a tool called OMEN has been developed in the last 5 years 
[10], [11]. Quantum transport can be either simulated in the Non-equilibrium Green s Function (NEGF) 
[12] or in the Wave Function (WF) [13] formalism. The tight-binding method [14] has been chosen as bandstructure 
model for its accurate description of the bulk band gap and effective masses of several semiconductors 
[15], [16], [17], its straight-forward extension to nanostructures, and its atomistic representation 
of the simulated devices. Basically, OMEN computes the probability of .nding an electron with energy 
E and momentum kz at every position inside a pre-de.ned simulation domain under the application of different 
external bias conditions. Since several energies and momentum are required to obtain physical observables 
such as charge and current densities, OMEN has been massively parallelized on 4 levels (bias, momentum, 
energy, and 1-D spatial domain decomposition) to enable the simulation of large 2-D and 3-D transistor 
structures and to reduce the computational time [18], [19]. The bias points (highest parallelization 
level) are independent from each other and can be embarrassingly parallelized, while the lowest level 
(1-D spatial domain decomposition) requires a lot of inter-processor communication [20]. In the ballistic 
limit of transport, electrons do not interact with other particles or among themselves and keep their 
energy and momentum unchanged as they .ow through a device. The momentum and energy parallelization levels 
of OMEN can therefore be treated as quasi-embarrassingly parallel problems, only one MPI Allreduce operation 
per observable being re­quired to sum up the different contributions. In this case, using the WF formalism, 
it was possible to simulate a realistic III-V high-electron mobility transistor (HEMT) [21] with (i) 
an accurate reproduction of the available experimental data, (ii) an almost perfect scaling of the computational 
time up to 222,720 cores, and (iii) a sustained performance of 860 TFlop/s [22]. Although the comparison 
with the experimental data is good, it appears that the simulated device current overestimates the measured 
data by 10-20% at high drain-to-source voltages. The reason for such a behavior can be attributed to 
the inter­action of electrons with lattice vibrations known as electron­phonon scattering [23] that is 
neglected in the ballistic limit of transport. Electron-phonon scattering plays an even more important 
role in Si, the material of reference for electronics. Neglecting this effect overestimates the performances 
of ultra­scaled Si transistors by about 30-40%. Furthermore, a new class of low power devices based on 
band-to-band tunneling (BTBT) is currently attracting a lot of attention [24]. The design and understanding 
of these BTBT transistors rely on the proper modeling of phonon-assisted tunneling. Hence, accounting 
for electron-phonon scattering in nanoscale device simulations is becoming a critical issue. If electron-phonon 
scattering is essential to properly model ultra-scaled transistors, one might wonder why it has not 
&#38;#169; 2010 IEEE Personal use of this material is permitted. However, permission to reprint/republish 
this material for advertising or promotional purposes or for creating new collective works for resale 
or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works 
must be obtained from the IEEE. SC10 November 2010, New Orleans, Louisiana, USA 978-1-4244-7558-2/10/$26.00 
been implemented before. Simply because including electron­phonon scattering in an atomistic, full-band 
simulation frame­work is one of the Grand Challenge problems in nanoelec­tronic device simulation. It 
increases the computational burden by about 3 orders of magnitude, makes the parallelization of the workload 
much more dif.cult than for ballistic transport, and requires a speci.c implementation. This is why most 
attempts to solve the electron-phonon scattering problem in an atomistic basis have been so far limited 
to small molecules composed of no more than 20 atoms [25], [26], [27], 250× less atoms than what will 
be shown here. The additional complexity comes from scattering self­energies that couple all the momenta 
and energies together, i. e. the energy E and momentum kz of an electron evolve as it interacts with 
phonons. To account for this mechanism, a CPU storing an electron characterized by an energy-momentum 
con.guration (E,kz ) must be able to exchange information with several (>100) other CPUs storing electrons 
with energy E' and momentum k'where E-E' and kz -k'are determined zz by the electron-phonon coupling. 
Hence, an ef.cient scheme to treat inter-processor communication at the momentum and energy parallelization 
levels and to minimize its impact on the computational time has to be developed. This is the fundamental 
issue and the main achievement of this paper. OMEN has been recently upgraded to take electron-phonon 
scattering into account, but the chosen approach based on NEGF and tight-binding has been restricted 
to 3-D nanowire structures of diameters smaller or equal to 3nm [28]. The absence of momentum dependence 
in nanowires simpli.es the implementation and the parallelization, one energy point being only connected 
to around 50 other energies through the electron-phonon scattering self-energies. Nanoscale transistors 
generally have a 2-D planar structure whose third dimension is modeled via a momentum depen­dence of 
the current and charge densities. In addition to the coupling of different energy points, electron-phonon 
scattering induces a coupling of all the momentum points so that one CPU typically needs to send and 
receive in the order of O(109 ) bytes of data distributed over 400 to 1,000 other CPUs. This paper presents 
an original approach to address electron­phonon scattering in 2-D nanoscale transistors and its appli­cation 
to a realistic silicon-on-insulator (SOI) device. The key results are (i) a well-balanced distribution 
of the workload on a very large number of CPUs to reduce the computational time and (ii) an ef.cient 
communication scheme to calculate electron-phonon scattering self-energies. Using the Cray-XT5 Jaguar 
at Oak Ridge [29] and the three lowest parallelization levels of OMEN (momentum, energy, and spatial 
domain decomposition) with less than 10% embarrassingly parallel workload, the proposed implementation 
of electron-phonon scattering scales well up to 95,256 cores with a sustained performance of 142 TFlop/s. 
The paper is organized as follows: in Section II, the electron-phonon scattering model is described, 
its implemen­tation is given in Section III. A realistic nanoscale device is simulated in the presence 
of electron-phonon scattering in Section IV, followed by an analysis of the numerical performances and 
the scalability of the approach. Finally, the L Source g Drain Oxide y=(100) tbody  Buried Oxide x=<100> 
Fig. 1. Typical 2-D silicon-on-insulator SOI transistor structure. Transport occurs along the x-axis 
in the atomic channel oriented with the <100> crystal axis, of thickness tbody , and deposed on a buried 
oxide, y=(100) is the direction of con.nement, while z is assumed periodic and induces a kz -dependence 
of the simulated quantities. The gate contact of length Lg is separated from the channel by an oxide 
layer of thickness tox and controls the electron .ow from the source to the drain contact. paper is concluded 
in Section V. II. PHYSICAL MODEL The general structure of a 2-D nano-transistor is given in Fig. 1. Electrons 
.ow along the x-axis and are con.ned along the y-axis. The structure is assumed periodic along the z-axis 
which induces a kz -dependence for all the physical observables. In the NEGF formalism expressed in a 
tight­binding basis, the following steady-state systems of equations must be solved for each injection 
energy E and momentum kz {[E - En - V(Rn)] dl,n - Hnl(kz )- l } SRB ) - SRS nl (E,kznl (E,kz)· GR )= 
dnm, (1) lm(E,kz [ .B G. (E,kz )=GR (E,kz ) · S(E,kz )+ nmnl1 l1 l2 l1 l2 ] .S S(E,kz )· GA (E,kz), (2) 
l1 l2 l2 m with GA (E,kz )= GR (E,kz ). The indices l, m, and n nmmnrefer to the atomic positions Rl, 
Rm, and Rn situated in the x-y plane, respectively. The diagonal matrices E (injection energy), En (atom 
on-site energy at Rn), and V(Rn) (elec­trostatic potential at Rn)are allofsize tB × tB , where tB is 
the number of orbitals used to describe a single atom in the tight-binding model. Note that with ten 
atomic orbitals (1 s,3 p,5 d, and 1 excited s*) corresponding to tB =10, as in this paper, the material 
properties of most semiconductors can be well reproduced [15]. The nearest-neighbor coupling matrix Hnl(kz 
), the boundary (SB (E,kz)) and scattering nl(SS (E,kz )) self-energies as well as the lesser (G< (E,kz)), 
nllmthe greater (G> (E,kz )), the retarded (GR (E,kz )), and the lmlmadvanced (GA (E,kz)) Green s functions 
are of size tB × tB lmtoo and depend on the momentum kz . Each equation depends on a single energy E 
and momentum kz. The coupling of different energies and momenta arises from the electron-phonon scattering 
self-energies that are de.ned in the self-consistent Born approximation as [28], [30] p S.S dqz Vij 
 (E,kz)= (.ph,qz )· nmnl1 l2 m 2p l1 ,l2 ,i,j-p.ph ( . .iHnl1 ·nph(.ph)·G(E±l.ph,kz -qz)+ l1 l2 ) . (nph(.ph)+1)·Gl1 
l2 (E±l.ph,kz -qz )·.j Hl2 m, (3) SRS 1(S>S )-S<S (E,kz) (E,kz (E,kz)). (4) nmnmnm 2 In these two equations, 
.ph is the con.ned phonon frequency, nph(.ph)the Bose distribution of the phonons with frequency .ph, 
and .iHnm the derivative of the nearest-neighbor cou­pling matrix Hnm with respect to the coordinate 
i=x, y,or z along the bond Rm - Rn joining the atoms n and m. To simplify the calculation, all the self-energies 
are assumed .S local in position, i. e. only the diagonal Snn (E,kz) and SRS (E,kz ) elements are considered 
in this work [28]. The nn form factor Vij (.ph,qz) is given by n1 m1 m2 n2 p dqx l Vij (.ph,qz )=· n1 
m1 m2 n2 -p 2p 2..(qx,qz) . fi (Rm1 ,qx,qz)fi (Rn1 ,qx,qz) .. - · (5) MM m1 n1 j* j* f(Rn2 ,qx,qz )f(Rm2 
,qx,qz) .. - Mn2 Mm2 with the condition .ph -.E /l =..(qz ,qz )=.ph +.E /l. The choice of .E depends 
on the material and on the desired computational accuracy, it typically lies between 1 and 5 meV, and 
determines how many phonon energies are considered. The indices ., qx, and qz depict the phonon mode 
and wave vectors along x and z, respectively, the y-axis being also con.ned for phonons, Ml is the mass 
of the atom at position Rl. The normalized atomic displacement fi (Rl,qx,qz) and .the phonon frequency 
..(qx,qz )are calculated for each qx and qz according to the following harmonic oscillator eigenvalue 
problem [31] Fij (qx,qz ) nm v·fj (Rm,qx,qz )- MnMm. m,j ..2 (qx,qz)·f.i (Rn,qx,qz )=0. (6) where Fij 
(qx,qz ) represents the dynamical matrix of the 2­ nm D transistor in Fig. 1 with periodic boundary conditions 
along x and z and con.nement along y. What should be retained from the different equations above, beyond 
the physical interpretation of each term, is the con­nection of every energy-momentum (E,kz ) pair in 
Eq. (1) and (2) to many other (E±l.ph,kz -qz ) pairs through the electron-phonon scattering self-energies 
described in Eq. (3): . the computation of Snn(E,kz)requires several Green s Func­ . tions Gnn(E ±l.ph,kz 
-qz) which are not stored on the Fig. 2. De.nition of the lesser S< (E, kz ) (left) and greater S> (E, 
kz ) nnnn (right) electron-phonon scattering self-energies. Filled circles are occupied state (electrons), 
empty circles are free states (holes). The lesser self-energy represents in-scattering (an occupied state 
at (E±n.ph,kz -qz ) is scattered into an empty state at (E, kz ) through phonon absorption or emission). 
The greater self-energy refers to out-scattering (an empty state at (E±n.ph,kz - qz ) is .lled by the 
out-scattering of an occupied state at (E, kz )). . same CPUs as Gnn(E,kz ). Understanding that there 
is a mechanism that connects different energies and momentum to each other is crucial, but suf.cient, 
to follow the numerical implementation of the method presented in Section III. The number of connections 
depends on the number of points used to discretize the phonon momentum qz between -p and p and on the 
number of frequencies .ph necessary to span the entire phonon spectrum. Figure 2 is a graphical representation 
of the two interaction types present in Eq. (3), phonon ab­sorption (term proportional to nph(.ph)) and 
phonon emission (term proportional to nph(.ph)+1). It illustrates how the lesser S< (E,kz ) and greater 
S> (E,kz ) scattering self-energies nnnn couple different energy-momentum pairs. Finally, once that Eq. 
(1) and (2) have been solved for each energy E and momentum kz, the carrier and current densities can 
be calculated in the NEGF/tight-binding formalism as [11] p dkz dE ) n(r)=-i trG< (E,k) d(r -Rj ) (7) 
jj z-p 2p2p j and e p dkz dE ·G< J(r)= trHi1 i2 i2 i1 (E,kz )- 2l 2p2p -p i1 i2 ) G<i1 i2 (E,kz)·Hi2 
i1 (Ri2 -Ri1 )d(r -Ri1 ), (8) respectively. Equations (1) and (2) must be solved self­consistently with 
Eq. (3) and (4) till convergence is reached, which means that n(r) and J(r) do not vary any more [28]. 
After convergence, the charge density n(r) is used in the Poisson equation solved on a .nite element 
grid [32], [33] to update the value of the electrostatic potential V(r) in Eq. (1). The complete procedure 
is repeated till n(r) and V(r) do not change from one Poisson iteration to the other. III. NUMERICAL 
IMPLEMENTATION The electron-phonon scattering model is implemented in C++ and embedded in a quad-level 
parallel environment (bias points, momentum, energy and 1-D spatial domain decompo­sition) based on MPI 
[34]. In this work, only the three lowest parallelization levels are used, no bias point parallelization, 
resulting in a less than 10% embarrassingly parallel workload. All algebraic operations involving full 
matrices or vectors are performed with Blas [35] and Lapack [36], the sparse linear systems of equations 
are solved using standard available packages [37], [38], [39], [40], [41]. The device simulations are 
launched on P0 cores that are all dedicated to the same bias point. The number of electron momentum points 
Nkz is chosen by the user to ensure an accurate description of the device bandstructure. The number of 
cores Pkz assigned to each momentum point depends on the number of energy points NE (kz ) and is dynamically 
allocated [19]. Irrespective of P0 , all the momentum points are treated in parallel. The 1-D spatial 
domain decomposition is done on PE cores, which means that PE cores simultaneously solve the Green s 
functions in Eq. (1) and (2) and the self-energies in Eq. (3) and (4) for one given energy E and momentum 
kz. In this con.guration, each CPU treats nE/CP U =(NE (kz ) × PE )/Pkz different energy points, which 
is the same number for all the CPUs due to the dynamical allocation of Pkz . The distribution of the 
CPUs over the momentum, energy, and spatial domain decomposition parallelization levels is similar to 
a tree composed of Nkz branches at the top divided into NE (kz ) sub-branches ending up in PE different 
leaves. A group of Pkz cores is dedicated to each of the Nkz main branches, PE cores to each of the sub-branch, 
and 1 core per leave which represents a portion of the simulation domain. Each core treats the same number 
of sub-branches, nE/CP U . Although the mapping of the different tasks to a given number of P0 CPUs is 
the same as in the ballistic case described in Ref. [18], electron-phonon scattering drastically changes 
the simulation .ow of 2-D nanoelectronic devices. Hence, the different steps involved in the modeling 
of electron­phonon scattering are summarized below: A. Step I: Initialization The electron momentum grid 
is homogeneous and is entirely determined by the number of points Nkz selected as an input parameter 
(kz =-p:2p/(Nkz - 1):p). The phonon momentum grid is the same as the electron one, i. e. Nqz =Nkz and 
qz =kz so that a point kz -qz is automatically a grid point. Periodic boundary conditions are applied 
if kz -qz falls outside [-p, p]. The number of phonon energies .ph in Eq. (3) is an input parameter, 
, and directly sets the value .E used to N.ph calculate the form factor Vij (.ph,qz ) in Eq. (5). In 
n1 m1 m2 n2 Si, for example, the largest phonon energy is around 60 meV and the smallest 0 meV so that 
.E =60/(2*N.ph )meV. The initialization phase also includes the construction of the device structure 
atom-by-atom, the generation of the corre­sponding tight-binding Hamiltonian H, and the calculation of 
.iHnm, the derivative of the nearest-neighbor coupling matrix Hnm composing H and used to calculate the 
lesser and greater self-energies in Eq. (3). Finally, the .nite-element stiffness matrix of the device 
is constructed to enable the solution of Poisson equation. B. Step II: Calculation of the Form Factor 
V In the next step, the form factor Vij (.ph,qz ) is n1 m1 m2 n2 calculated for each phonon frequency 
.ph, momentum qz, atom positions Rn1 , Rn2 , Rm1 , Rm2 , and directions i, j (x, y,or z) . Since the 
scattering self-energies are assumed diagonal, the form factor, which has a real and imaginary part, 
reduces to Vij (.ph,qz ), where one atom located at Rn has nmmn only four nearest-neighbors situated 
at Rm. Assembling all the components, 36×Nqz ×N.ph ×NA×sizeof(complex) bytes are required to store the 
form factor, where NA is the number of atoms composing the transistor structure. The pre-factor 36 comes 
from 4 nearest-neighbors times 9 (i, j) combinations. For a 2-D device made of NA=5,000 atoms with N.ph=10 
and Nqz=20, storing all the components of the form factor represents 600MB of data. On top of that solving 
Eq. (1) and (2) for a single energy-momentum con.guration requires several 100MB of memory per core. 
On a machine like the Cray-XT5 Jaguar with 1.33GB of memory per core, it is clear that not all the cores 
should store the entire form factor. As an alternative, each core only computes and stores the form factor 
for one given phonon momentum qz so that only 30MB of memory are dedicated to this task. The P0 cores 
used in the simulation are divided into Nqz groups of P0 /Nqz cores that .rst calculate the phonon dispersion 
and atomic displacement according to Eq. (6) and then solve Eq. (5) for each phonon energy. C. Step III: 
Energy Grid Generation Steps I and II are only performed once at the beginning of every simulation. The 
remaining steps are repeated a considerable number of times as the simulation evolves. At a given electrostatic 
potential, the source and drain contact bandstructures are simultaneously calculated for all the kz points 
to determine the corresponding energy grid. Based on the number of energy points per momentum NE (kz 
),the P0 CPUs are dynamically distributed over Nkz groups of Pkz cores, where NE (kz ) and Pkz are different 
for each kz leading to a well-balanced work load [19]. The lesser and greater self-energies in Eq. (3) 
couple one energy-momentum pair (E,kz)to2×N.ph × Nqz other (E ± l.ph,kz -qz ) pairs as illustrated in 
Fig. 3. The factor 2 comes from the connection to higher (E + l.ph)and lower (E - l.ph) energies. The 
dif.culty of implementing electron-phonon scattering arises from these multiple energy and momentum connections 
resulting in a very complicated communication pattern. Once the energy grid has been created, a table 
summarizing the different self-energy induced connections is generated. For each (E,kz ), the table has 
two entries, (i) the phonon momentum qz and (ii) the phonon energy .ph. Hence, the size of each table 
is 2×N.ph × Nqz and its creation involves two for loops: a .rst loop over qz . Starting from the initial 
momentum kz, the connected momentum k ' =kz + qz is determined z and the corresponding energy vector 
E(k ' ) is considered. z As a reminder, note that the energy vector is different for each momentum. 
Fig. 3. Top-down view of the 2×N.ph ×Nqz Green s Functions G(E ± n.ph,kz -qz ) connected to a single 
energy-momentum (E, kz ) pair through electron-phonon scattering and corresponding CPU mapping. To compute 
the self-energy S(E, kz ), the Green s Functions located in the CPUs i, j, ···, k must be sent to the 
CPU n storing the (E, kz ) combination. a second loop over .ph. The energy point inside the vector E(kz' 
) lying as close as possible to E(kz ) ± l.ph is selected. Its index number, comprised between 1 and 
NE (k ' ), becomes the table entry. It can be converted into z a CPU rank. Since each CPU treats nE/CP 
U energy points, all with the same momentum, its contains nE/CP U connection tables as sketched in Fig. 
3. In all these tables, it is very important to check that if a CPU X is connected to a CPU Y , Y is 
also connected to X through the same qz and .ph. This avoids undesired dead locks. D. Step IV: Calculation 
of the Green s Functions G.,R The Green s functions in Eq. (1) and (2) are then computed for each energy 
and momentum, the .rst time with scattering self-energies equal to 0, then with scattering self-energies 
computed from the previous value of G.(E,kz ). This is the only part in the electron-phonon scattering 
problem that can be embarrassingly parallelized, each energy being independent of the others once the 
self-energies are known, but this phase represents less than 10% of the total work load as shown later. 
The computation of Eq. (1) and (2) is done with a recursive algorithm called recursive Green s function 
(RGF) [42] that cannot be easily and ef.ciently parallelized. In this paper, a maximum of PE =PE,space 
× PE,GF =4 cores can be used for the spatial domain decomposition reaching a speed-up factor of 3 as 
compared to a single core. In the best case, PE,space=2, which means that the spatial domain is decomposed 
into two parts along the transport axis x, and PE,GF =2, which forces the lesser and greater Green s 
functions to be computed simultaneously instead of sequentially [28]. E. Step V: Calculation of the Self-Energies 
S.,R The tricky part in the electron-phonon scattering problem is the calculation of the scattering self-energies 
in Eq. (3). The lesser and the greater components are computed in parallel when PE,GF =2 and PE =2 or 
4. Once they have been calcu­lated, the retarded self-energy can be evaluated with Eq. (4). As an example, 
the calculation of the lesser self-energy S<(E,kz) is considered here. To account for all the different 
contributions in Eq. (3) arising from the integral over the phonon momentum qz and the sum over the phonon 
energy .ph, the algorithm contains three for loops: an outer loop over the Nqz phonon momenta qz.Inside 
this loop, the .rst task consists in gathering the proper form factor Vij ) at the desired qz including 
 nmmn(.ph,qz the N.ph phonon energies .ph and 9 (i,j) combinations. As mentioned earlier, each core stores 
V for one given momentum, which is not necessarily the one that is currently needed. Hence, the form 
factor is broadcasted from a source CPU holding the desired qz to one single core per group of Pkz cores, 
which in turn broadcasts it within the group.  a middle loop over the N.ph phonon frequencies.  an 
inner loop over the nE/CP U energy points E with momentum kz that each CPU treats. This loop is in fact 
twofold degenerate as explained below.  Most of the work is done inside the loop over nE/CP U . There 
is one important challenge to solve: the connection maps of each (E,k) energy-momentum pair, as shown 
in Fig. 3, cross each other and form a very complicated network. However, at a given phonon momentum 
qz and energy .ph point, a (E,kz ) pair is only connected to a single other pair. What complicates the 
situation is the fact that each CPU treats nE/CP U different pairs. Each CPU (rank n) starts with the 
.rst (E,kz ) pair it stores, reads inside the corresponding connection table the (E ' ,k ' ) energy-momentum 
pair it is z connected to, converts it to a CPU rank (rank m), and tries to establish a send-receive 
operation with that rank. The problem is that the .rst energy-momentum pair on the rank m is not necessarily 
(E ' ,k ' ), but perhaps (E '' ,k '' ) so that m,at z z this time, might need data stored on a CPU with 
rank l. There is no way to reorder the nE/CP U energy-momentum pairs on each CPU so that the .rst pair 
on CPU n is connected to the .rst pair on CPU m and so one. This would work for one (qz,.ph) phonon con.guration, 
but not for the next one. The solution to this problem is the use of two successive loops over the nE/CP 
U energy-momentum pairs stored on each CPU instead of one single. In the .rst loop, each CPU sends the 
nE/CP U different lesser Green s functions G<(E,kz ) it stores to the MPI rank corresponding to the pair 
(E + l.,kz + qz ). The function MPI Isend is used for that purpose. The receiver rank is read from the 
table created in Step III, but it does not need to be ready to receive a packet and is also sending data, 
probably to an­other CPU. The amount of data sent per energy point is NA/PE,space ×tB ×tB ×sizeof(complex) 
bytes, corresponding to NA/PE,space Green s function matrices G< of size tB × nn tB . The usage of MPI 
Isend instead of MPI Send simpli.es the communication pattern by avoiding the separation of the CPUs 
between senders and receivers, all being .rst senders and then receivers. After completion of the send 
processes in the .rst loop over 0.2 Number of Electrons 2500 0.2 3 100 -0.2 -0.4 Fig. 4. Electron-phonon 
scattering .ow chart for a given bias point. There Id (µA/µm) 0 40 0 10 20 30 Injection Velocity (cm/s) 
 9 0 108500 7 60 0.2 0.4 0.6 0 5 1015 12 Vgs (V) NS (1/cm2) x 10 Fig. 5. (a) Transfer characteristics 
Id -Vgs at Vds =VDD =0.8 V of a SOI transistor as depicted in Fig. 1 with a body thickness tbody =5nm, 
a gate length Lg =20nm, and source and drain extensions of 10nm each with a donor doping concentration 
ND=1e20 cm-3 . The ballistic current (blue line with circles) is compared to the dissipative current 
including electron-phonon scattering (red line with crosses) on a linear and logarithmic scale. (b) Illustration 
of the are two self-consistent loops, one inner loop between the Green s functions in Eq. (1) and (2) 
and the self-energies in Eq. (3) and Eq. (4) and one outer loop over the charge density in Eq. (7) and 
the electrostatic potential V (r) in Eq. (1) through the Poisson equation. the nE/CP U energy-momentum 
pairs, in the second loop, each CPU receives the lesser Green s functions G<(E-l., kz -qz ) using MPI 
Recv when it needs it and evaluates the phonon absorption part of Eq. (3) (term proportional to nph(.ph)), 
one energy point after the other. A total of 72 full matrix multiplications of size tB × tB is required 
per atom n to compute dHi · G< (E - l., kz - qz ) · dHj for each nmmmmn (i, j) con.gurations and for 
its 4 nearest-neighbors m. The send and receive operations as well as the multiplica­tions are repeated 
for the phonon emission term in Eq. (3) with a sign change to all the terms depending on l.. Finally, 
each CPU sends and receives approximately nE/CP U × Nqz × 2 × N.ph × NA/PE,space × tB × tB ×sizeof(complex) 
byes of data to compute the lesser or greater self-energy, typically nE/CP U × O(109-10 ) bytes. When 
the loops over the phonon energy and momentum are complete, all the contributions are added and the electron­phonon 
scattering self-energies are ready. With PE =4 cores working on each energy point, a speed-up factor 
of 4is obtained as compared to PE =1 to solve Eq. (3) and (4). The scattering self-energies are then 
used in Step IV to re-compute the retarded, lesser, and greater Green s functions. F. Step VI: Calculation 
of the Charge and Current Densities After each (S,G) self-energy -Green s function iteration, the charge 
and current densities are calculated as in Eq. (7) and (8). The (S,G) iterations stop when the charge 
and the current do not vary anymore from one iteration to the other. In reality, a satisfactory convergence 
is obtained when the relative change of the charge and of the current becomes smaller than a pre-de.ned 
input parameter, chosen between 0.1 and 1%. Typically, this happens after 5-10 iteration for the charge 
and 20 to 40 iterations for the current. This means that Steps IV method to extract the injection velocity 
vinj and the inversion charge density NS at the virtual source (top-of-the-barrier potential). (c) Injection 
velocity as function of the inversion charge density under the ballistic limit (blue line with circles) 
and with electron-phonon scattering (red line with crosses). to VI must be repeated from 5 to 40 times 
before the self­energies and the Green s functions satisfy the self-consistent Born approximation. G. 
Step VII: Poisson Equation To save computational time, two convergence criteria have been introduced: 
after 5-10 (S,G) iterations, when the charge density does not vary any more, the Poisson equation is 
solved on a .nite-element grid [32] using a Newton-Raphson scheme [33] to give the next electrostatic 
potential. The sparse linear systems of equations arising from the Poisson equation are solved using 
Aztec [41], a parallel iterative linear solver. More details can be found in Ref. [18]. The whole process 
starts again at Step III. After 3-6 Poisson iterations (Steps III to VII), when the charge and the electrostatic 
potential do not change any more, 20 to 40 additional (S,G) iterations are computed to obtain the device 
current. Hence, with one criterion for the charge and one for the current, the total number of iterations 
between Eq. (1) and (2) and Eq. (3) and (4) is limited to 35 to 100 per bias point instead of 60 to 240 
with one single criterion. The inner loop between the self-energies and the Green s functions as well 
as the outer loop between the charge density and Poisson equation (Steps III to VII) are shown in Fig. 
4. IV. RESULTS A. Device Modeling To illustrate the effect of electron-phonon scattering on the performances 
of nanoelectronic devices, a realistic n­doped silicon-on-insulator (SOI) .eld-effect transistor as in 
Fig. 1 is simulated and analyzed in this section. The transfer characteristics Id -Vgs at Vds =VDD=0.8 
V (VDD is the supply voltage) and the structure de.nition of the SOI transistor are given in Fig. 5. 
The channel has a thickness tbody=5nm, the gate contact measures Lg=20nm, while the n-doped source Ballistic 
Dissipative 1.5 1.5 and drain extensions (ND=1e20 cm-3 ) have both a length of 10nm. The sp3 d5 s* tight-binding 
method without spin­ 1 1 E (eV) E (eV) E (eV) E (eV) E (eV) E (eV) orbit coupling gives a very accurate 
representation of the Si conduction bands and is used as bandstructure model Electron Transfer Electron 
Transfer 40  k z=-p/(5*a0) k =-p/(5*a) here so that tB =10. The structure contains NA=5,402 atoms, 
0.5 0.5 z0 the electron and phonon momentum grid are composed of 0 20 400 20 Nkz =Nqz =21 points, and 
N.ph =10 phonon energies are kept  x (nm) x (nm) (.E =3 meV). 1.5 1.5 Electron-phonon scattering decreases 
the SOI current by 35% as compared to the ballistic transport limit, as can be seen in Fig. 5 (a). This 
is due to a reduction of the electron injection velocity at the virtual source of the device caused 1 
 1  k =0 k =0 by electron backscattering [43]. Figure 5 (b) illustrates the 0.5 0.5 z z concept of virtual 
source (also called top-of-the-barrier ), 0 20 400 20 which is the maximum of the potential barrier between 
the  x (nm) x (nm) source and the drain, and injection velocity vinj , which is the 1.5 velocity of 
the electrons at the virtual source. The injection 1.5 velocity is extracted from the current magnitude 
Id and the inversion charge density NS at the virtual source. It is reported in Fig. 5 (c) for the ballistic 
and dissipative (with electron-phonon scattering) transport cases. At large inversion 1 1  k =7/10*p/a 
0 k =7/10*p/a00.5 0.5 z z charges NS ,a >30% decrease of the injection velocity due to 0 20 400 20 electron-phonon 
scattering can be observed, suggesting that  x (nm) x (nm) the considered SOI transistor with a gate 
length of 20nm operates at less than 70% of its ballistc limit. Apart from a reduction of the SOI current, 
electron-phonon scattering changes its spectral and spatial distribution as de­picted in Fig. 6 for three 
different momentum components of the current taken at kz =-p/(5a0 ), 0, and 7/10 × p/a0 . In the ballistic 
limit, the energy-resolved current remains the same all along the device channel while in the presence 
of electron-phonon scattering, the current magnitude decreases from source (x=0) to drain (x=40 nm) at 
kz =0 and increases close to the drain for kz =-p/(5a0 ) and 7/10×p/a0 . Electrons are transferred from 
one momentum kz to another at kz - qz by absorbing or emitting a phonon with momentum qz.Atthe same time, 
they move from one energy E to E+l. or E-l.. Current conservation (the current remains constant from 
source to drain) is an important requirement in device sim- Fig. 6. Energy-and position-resolved current 
extracted from the same SOI transistor as in Fig. 1 and 5 at Vgs =Vds =VDD=0.8 V. Dark red indicates 
high current concentrations, light green no current. The left column refers to the ballistic limit of 
transport, the right column to dissipative transport with electron-phonon scattering. Each subplot line 
represents a different kz value. The same color scale is used per line to directly compare the ballistic 
and dissipative currents. Ballistic Dissipative k =-2/5*p/a z0 0.3 0.3 k =-1/5*p/a z0 k =0 z 0.25 0.25 
k =1/10*p/a z0 sum of all kz k-resolved Current (arb. units) k-resolved Current (arb. units) 0.2 0.15 
0.15 0.1 ulations [44] and a good way to check whether the results are correct or not. The NEGF/tight-binding 
approach presented in this work ful.lls current conservation if it is correctly implemented, as proved 
in Appendix A. Obviously, in Fig. 6, there is no conservation of the dissipative current (right column) 
for the different kz-components taken individually. This is con.rmed in Fig. 7, where some spatially-resolved 
kz ­ components of the current are plotted. Basically, the energy­dependent current components in Fig. 
6 are integrated over energy (vertical axis) to give Fig. 7. Only the sum over all the momenta (yellow 
solid line) is conserved when electron­phonon scattering is included, exactly as predicted in Ap­pendix 
A. In the ballistic limit, current is conserved for each energy and momentum. To summarize, the electron-phonon 
scattering model gives a new insight into the physics of 2-D SOI transistors and their electron injection 
velocity. Furthermore, it satis.es the 0.05 0.05 0 0 0 20 400 20 40x (nm) x (nm) Fig. 7. Position-resolved 
current calculated under the same conditions as in Fig. 6. The ballistic (left) and dissipative current 
(right) components are compared for different kz values. The sums of all the kz components (total current) 
are also given (solid yellow line) showing that both the ballistic and dissipative currents are conserved 
all along the transistor channel as expected. Number of Cores P0 Walltime (s) Speed-Up TFlop/s || Ef.ciency 
3,276 15232 1x 8.3 100% 6,300 7933 1.9x 16.0 99.8% 12,096 4714 3.2x 26.9 87.5% 24,612 2452 6.2x 51.8 
82.7% 49,224 1450 10.5x 88.2 69.9% 95,256 908 16.8x 142 57.7% Fig. 8. Scaling performance of the electron-phonon 
scattering model on 3,276 to 95,256 cores on the Cray-XT5 Jaguar for the same SOI device as in Fig. 5 
including 1 bias point, 21 momenta, and a total of 23,753 energy points inhomogeneously distributed over 
the 21 momenta. The reported data refer to the self-consistent solution of the inner loop between the 
Green s Functions, Eq. (1) and (2), and the self-energies, Eq. (3) and (4), as illustrated in Fig. 4. 
Four self-consistent iterations are calculated: the Green s Functions are solved 5 times, the self-energies 
4 times. The walltime (blue line with circles) and the performance in TFlop/s (red line with stars) are 
given and summarized in the bottom table. The time on 3,276 cores is the reference for the speed up factor 
and for the parallel ef.ciency. current conservation law. Since there are several thousands of different 
terms that must cancel each other to ensure current conservation, its demonstration as in Fig. 7 is a 
strong indication that the scattering model is properly implemented. B. Scaling and Performance Analysis 
This section is dedicated to the analysis of the numerical performances of the electron-phonon scattering 
model. The SOI simulation in Section IV-A involves 9 bias points, for which 43 Poisson iterations and 
500 (S,G) self-energy ­Green s functions iterations must be solved. The complete simulation takes a total 
of 2.5M SUs (Service Units or just CPU hours). A massive parallelization of the work load is therefore 
a requirement and not just an option to obtain results within a reasonable amount of time. The SOI simulation 
ran during more than 50 hours on 47,880 cores on the Cray-XT5 Jaguar at ORNL. To investigate the scalability 
of the electron-phonon scatter­ing model, it is not possible to run a complete simulation of 9 bias points, 
not even of just 1 bias point, this would take too much time. The numerical performances are analyzed 
for the calculation of 4 (S,G) self-consistent iterations using the same SOI transistor as before at 
Vgs=0.4 V and Vds=0.8 V. The relative change of the charge density n(r) in Eq. (7) between P=3,276 P=95,256 
N=23,753 TotTot E,Tot (a) (c) (b) 5000 150 1000 4000 3000 100 500 2000 50 1000 0 0 0 0 10 20 0 10200 
1020 Number of Energies Number of Cores Momentum Index Fig. 9. (a) Distribution of the energy points 
across the different kz momentum points. The total energy points is 23,753 with a minimum number of energy 
points NE,min=770 for kz =±p/a0 (a0 : lattice constant of Si) and maximum NE,max=1,312 for kz=0. (b) 
Corresponding dynamic distribution of 3,276 cores over the different momentum points. (c) Same as (b), 
but for 95,256 cores. the 3rd and the 4th iteration is less than 0.1%, indicating that the inner loop 
in Fig. 4 is complete and the Poisson equation can be solved. One single bias point is considered and 
three levels of parallelization are used. The Nkz =21 momentum points are treated in parallel, the energies 
are parallelized with each CPU working on nE/CP U points, and the 1-D spatial domain decomposition is 
performed on PE=4 cores with PE,space=2 and PE,GF =2. Note that the scaling and .op/s performances reported 
in this Section would be the same if the complete current-voltage characteristics of the SOI transistor 
would be simulated. This would however take too much time and consume too many SUs. Figure 8 shows the 
walltime scaling and the .op/s perfor­mance on 3,276 up to 95,256 cores on the Cray-XT5 Jaguar for the4(S,G) 
self-consistent iterations mentioned above as well as one solution of Eq. (3) and (4) without scattering 
self­energies and one parallel solution of the Poisson equation. The table at the bottom summarizes the 
results and gives the speed-up factor and parallel ef.ciency with respect to 3,276 cores. Note that the 
numerical results do not vary by more than 1e-3% in going from 3,276 to 95,256 cores. On 3,276 cores, 
each CPU works on nE/CP U 30 en­ergy points while on 95,256 cores, nE/CP U 1. The quasi­homogeneous distribution 
of the energy points across all the CPUs, most of them treating nE/CP U points, few treating only nE/CP 
U -1 points, is the result of a dynamical alloca­tion of the computational resources according to the 
predicted work load [19]. The CPU distribution is illustrated in Fig. 9. The number of CPUs per momentum 
point shown in the subplots (b) and (c) for 3,276 and 95,256 cores, respectively, almost exactly follows 
the number of energy points per momentum in the subplot (a). If the energy grid changes from one Poisson 
iteration to the other, the CPU distribution automatically adapts itself to account for the new work 
load. In going from 3,276 to 95,256 cores, the simulation time in Fig. 8 decreases from 15,232 to 908 
seconds and the performance increases from 8.3 to 142 TFlop/s. The measured speed-up factor is 16.8, 
ideally, it should be 29-30. This represents a parallel ef.ciency of 58%. Hence, the electron­phonon 
scattering model operates at about 14% of the peak Number of Cores P0 Total (s) S. Calc. (s) Comm. (s) 
G. Calc. (s) 3276 3685 2746 609 330 6300 2012 1504 332 176 12096 1129 746 295 88 24612 635 390 201 44 
49224 344 195 127 22 95256 212 99 102 11 3,276 100% 74.5% 16.5% 9.0% 6,300 100% 74.8% 16.5% 8.7% 12,096 
100% 66.1% 26.1% 7.8% 24,612 100% 61.4% 31.7% 6.9% 49,224 100% 56.7% 36.9% 6.4% 95,256 100% 46.7% 48.1% 
5.2% Fig. 10. Time decomposition for one self-consistent iteration consisting in .rst computing the 
self-energies as in Eq. (3) and (4) and then the Green s Functions according to Eq. (1) and (2) on 3,276 
to 95,256 cores. The same example as in Fig. 8 is used. The blue curve with circles measures the total 
time, the green curve with triangles the time to compute the self-energies (only the algebraic operations, 
without the communication and synchronization time), the red curve with stars the total communication 
and synchronization time, and the cyan curve with crosses the time to calculate the Green s functions. 
The results are reported in the bottom table, both in seconds and in percentages. performance of Jaguar 
on 95,256 cores, 142 TFlop/s vs 991 TFlop/s, while the ballistic transport (no scattering) model used 
in Ref. [22] reaches 37%. To understand the performance degradation induced by the electron-phonon scattering 
model, a careful inspection of the major operations involved in the calculation of a (S,G) iteration 
is of great interest. In Fig. 10 the total work load (blue line with circles) is broken down into three 
components: (i) the time to compute the self-energies in Eq. (3), only the algebraic operations, no communication 
(green line with triangles), (ii) the communication time, including the gathering of the form factor 
V and the sending and receiving of the Green s functions (red line with stars), and (iii) the time to 
solve the Green s functions in Eq. (1) and (2) (cyan line with crosses). The accompanying table summarizes 
the data in seconds and in percentages. The time to compute GR and G. (cyan curve with crosses) scales 
perfectly, as expected, since it is an embarrassingly parallel problem. However, it represents only 9% 
of the total time on 3,276 cores and 5% on 95,256. The time to compute the electron-phonon scattering 
self-energies, without the communication costs (green curve with triangles), is much more important, 
scales very well too, but does not deliver good .op/s performances. As explained in Section III Step 
V, the calculation of the self-energies involves 72 matrix multiplica­tions of size tB ×tB per atom, 
phonon energy, and momentum, i. e. a total of O(108 ) matrix multiplications of size 10×10 per self-energy 
at one given (E,kz) pair in the present case. Currently, the Blas library is used to perform all the 
algebra operations involving full matrices. However, the size of the matrices in Eq. (3) is much too 
small for the Blas functions dgemm and zgemm to be computationally very ef.cient and an alternative must 
be found. The underperforming Blas functions is the .rst reason for the low .op/s performance of the 
electron-phonon scattering model. Since the number of multiplications is directly proportional to the 
number of atoms NA, the time to calculate the scattering self-energies increases linearly with NA. This 
has been veri.ed by simulating a SOI device composed of 10,878 atoms, about two times more than the transistor 
investigated so far: the time to calculate S< or S> increases from 99 up to 197 seconds. The real bottleneck 
of electron-phonon scattering is the communication time that becomes dominant on 95,256 cores, especially 
the exchange of the Green s functions imposed by the calculation of the self-energies. If the communication 
time could be eliminated, as in the ballistic example described in Ref. [22], the .op/s performance would 
more than double on 95,256 cores, reaching 28% of the peak. Communication is therefore the second explanation 
for the .op/s degradation of the electron-phonon scattering model as compared to the ballistic model. 
In the SOI example simulated in this paper, each CPU sends and receives a total of nE/CPU ×1.8GB of data 
(Section III, Step V) divided into more than 400 packets to calculate the lesser or greater self-energy 
for nE/CPU energy-momentum (E,kz) pairs. Some of these packets are sent to close CPUs situated on the 
same cabinet of the machine, others to CPUs located on a cabinet at the opposite side of the machine. 
Hence, the communication times does not scale very well and two knee points can be observed in the red 
line with stars in Fig. 10. At these two points, one between 6,300 and 12,096 cores and one between 49,224 
and 95,256 cores, the scaling slope of the communication time suddenly changes. Further investigations 
are needed to better understand this behavior. Finally, to conclude this section, an histogram distribution 
of the .op/s performance on 3,276 and 95,256 cores is shown in Fig. 11. The data are extracted from the 
same simulation as in Fig. 8. On 3,276 cores, the standard deviation (s=0.07 GFlop/s) of the performance 
with respect to the mean value (µ=2.4 GFlop/s) is very small as compared to the experiment on 95,256 
cores, s=0.19 GFlop/s for a mean µ=1.49 GFlop/s. Not all the energy-momentum pairs have the same number 
of connections, most of them have 400 connections, as mentioned earlier, but some of them have less connections. 
For example, an energy point that is close to the grid lowest energy cannot emit a phonon an fall to 
a lower energy E - l..Itistherefore only connected to 200 other (E,kz) pairs. On 3,276 cores, each CPU 
treats nE/CPU =30 energy points, some of them 6000 nology, by NSF PetaApps grant number 0749140, by the 
Na­noelectronics Research Initiative through the Midwest Institute 5000 for Nanoelectronics Discovery, 
and by NSF through TeraGrid resources provided by the National Institute for Computational 4000 Sciences 
(NICS). This research also used resources of the 1000 Number of Cores 50 0 0  2.3 2.4 2.5 2.6 2.7 1 
1.2 1.4 1.6 GFlop/s GFlop/s Fig. 11. Histogram distribution of the .op/s performance on 3,276 cores (left) 
and 95,256 cores (right) for the same simulation as in Fig. 8. The mean .op/s value µ per core and the 
standard deviation s are also reported. have less than 400 connections, but the average number of connections 
is almost the same for each CPU so that the .op/s performance has a small standard deviation. On 95,256 
cores, each CPU treats only nE/CP U =1 energy point and no averaging is possible. If one energy has less 
connections, the CPU hosting this point has less work to do, it delivers less .op/s, and the performance 
histogram becomes more scattered. In summary, 70% of the CPUs exhibit more than 1.5 GFlop/s and 80% more 
than 1.4 GFlop/s on 95,256 cores. V. CONCLUSION Electron-phonon scattering has been implemented in a 
quad­level parallel environment dedicated to the simulation of nano­electronic devices. Contrary to previous 
ballistic simulations, the new scattering model induces a lot of inter-processor communication, reduces 
the embarrassingly parallel work load to less than 10%, but scales well up to 95,256 cores per bias point 
with a parallel ef.ciency of 58% as compared to 3,276 cores. Being able to include electron-phonon scattering 
is essential to investigate the properties of not-yet-fabricated devices, especially the electron or 
hole injection velocity at the virtual source of the transistor. The newly implemented simulation capability 
allows to compare the performances of strained and relaxed devices with different transport and con.nement 
directions such as <100>/(100), <110>/(100), <110>/(110), or <112>/(111). Electron-phonon scattering 
can also be combined with other non-ideal effects such as interface roughness or alloy disorder so that 
the in.uence of each of them can be analyzed. As a next step, the electron-phonon scattering model will 
be used to investigate Si-Ge heterostructure band-to-band tunneling transistors. Such devices are characterized 
by an in­direct band gap and phonon-assisted tunneling is the dominant current mechanism. VI. ACKNOWLEDGEMENT 
This work was partially supported by NSF grant EEC­0228390 that funds the Network for Computational Nanotech- 
 National Center for Computational Sciences (NCCS) at Oak 3000 Ridge National Laboratory, which is supported 
by the Of.ce Number of Cores of Science of the U.S. Department of Energy under Contract 2000 No. DE-AC05-00OR22725. 
APPENDIX A. Current Conservation Irrespective of the basis choice (tight-binding) and level of approximation 
(self-consistent Born) the electron-phonon scattering self-energies must satisfy the current conservation 
law [44]. In this section it is proved that the self-energies as de.ned in Eq. (3) and (4) ful.ll the 
current conservation requirement, which can be expressed as dkz dE Jex ·S<S G> = tr(E,kz)(E,kz)- nnnn 
2p 2p n ) S>S (E,kz )·G< (E,kz). nn nn =0 (9) In other words, the in-and out-scattering current Jex in­duced 
by phonon absorption and emission must vanish. This means that each element composing the in-scattering 
rate G> ·S<S (E,kz )(E,kz)has a corresponding element in the nnnn out-scattering rate S>S (E,kz)·G< (E,kz 
)so that they can­ nn nn cel each other. As an example, the following term describing in-scattering from 
(E +l.ph,kz -qz)into (E,kz )due to phonon emission is considered Vij ()G> (E,k)·.· nmmn.ph,qznnziHnm 
G< (E+l.ph,kz -qz )·.j Hmn. (10) mm To compensate this term, there exists another term representing '' 
'' out-scattering from (E ' ,k )into (E -l.ph,k -q )due to z zz phonon emission i/ j/ ' Vn/m/ m/ n/ (.ph,qz 
)·.i/ Hn/ m/ · ' '' G> // (E -l.ph,k -q )·.j/ Hm/ n/ · mmzz ' G<n/ n/ (E ' ,kz ). (11) If now the following 
equalities are satis.ed '' ' k =kz -qz E =E +l.ph q =-q z ' ' '' m =nn =mi =jj =i, (12) it remains to 
show that Vji (.ph,-qz )=Vij (.ph,qz ) (13) mnnm nmmn to prove that Eq. (10) exactly compensates Eq. 
(11), which is trivial from the de.nition of Vij ()in Eq. (5). Such nmmn.ph,qz a procedure can be applied 
to each single term composing Eq. (9) so that current conservation is ensured. A numerical implementation 
of electron-phonon scattering that exhibits current conservation is a good indication that everything 
works as it should since there are several thousands of terms that must cancel each other. REFERENCES 
<RefA>[1] R. Chau, et al., Silicon Nano-Transistors and Breaking the 10nm Phys­ical Gate Length Barrier , Conference 
Digest of 61st Device Research Conference, Salt Lake City, Utah 123-126 (2003). [2] D. A. Antoniadis 
and A. Khaki.rooz, MOSFET performance scaling: Limitations and future options , IEEE International Electron 
Devices Meeting 2008, Technical Digest, 253256 (2008). [3] W. Haensch et al., Silicon CMOS devices beyond 
scaling , IBM J. Res. &#38;Dev. 50, 339-361 (2006). [4] B. Doris et al., Extreme scaling with ultra-thin 
Si channel MOSFETs , IEDM Tech. Dig. 2002, 267-270 (2002). [5] S. D. Suk et al., Investigation of nanowire 
size dependency on TSNWFET , IEDM Tech. Dig. 2007, 891-894 (2007). [6] Y. Q. Wu, W. K. Wang, O. Koybasi, 
D. N. Zakharov, E. A. Stach, S. Nakahara, J. C. M. Hwang and P. D. Ye, 0.8-V Supply Voltage Deep-Submicrometer 
Inversion-Mode In0.75Ga0.25As MOSFET , IEEE Elec. Dev. Lett. 30, 700-702 (2009). [7] X. Wang, Y. Ouyang, 
X. Li, H. Wang, J. Guo, H. Dai, Room Temper­ature All Semiconducting sub-10nm Graphene Nanoribbon Field-Effect 
Transistors , Phys. Rev. Lett. 100, 206803 (2008). [8] J. Appenzeller, Y.-M. Lin, J. Knoch, and P. Avouris, 
Band-to-band tunneling in carbon nanotube .eld-effect transistors, Phys. Rev. Lett. 93, 196805 (2004). 
[9] See the Modeling &#38; Simulation Section of the ITRS at http://www.itrs.net/Links/2009ITRS/Home2009.htm 
[10] M. Luisier, G. Klimeck, A. Schenk, and W. Fichtner, Atomistic Sim­ 3 d5 * ulation of Nanowires in 
the sps Tight-Binding Formalism: from Boundary Conditions to Strain Calculations, Phys. Rev. B, 74, 205323 
(2006). [11] M. Luisier and A. Schenk, Atomistic Simulation of Nanowire Transis­tors , J. of Computational 
and Theoretical Nanoscience 5, 1-15 (2008). [12] S. Datta, Electronic Transport in Mesoscopic Systems 
, Cambridge University Press (1995). [13] W. R. Frensley, Boundary conditions for open quantum systems 
driven far from equilibrium , Rev. Mod. Phys. 62, 745-791 (1990). [14] J. C. Slater and G. F. Koster, 
Simpli.ed LCAO Method for the Periodic Potential Problem , Phys. Rev. 94, 1498-1524 (1954). [15] J. M. 
Jancu, R. Scholz, F. Beltram, and F. Bassani, Empirical spds* tight-binding calculation for cubic semiconductors: 
General method and material parameters, Phys. Rev. B 57, 6493-6507 (1998). [16] T. B. Boykin, G. Klimeck, 
and F. Oyafuso, Valence band effective-mass expressions in the sp3 d5 s * empirical tight-binding model 
applied to a Si and Ge parametrization , Phys. Rev. B 69, 115201 (2004). [17] T. B. Boykin, G. Klimeck, 
R. Chris Bowen, and F. Oyafuso, Diagonal parameter shifts due to nearest-neighbor displacements in empirical 
tight­binding theory , Phys. Rev. B 66, 125207 (2002). [18] M. Luisier and G. Klimeck, A multi-level 
parallel simulation approach to electron transport in nano-scale transistors , Proceedings of the 2008 
ACM/IEEE Conference on Supercomputing, article 12 (2008). [19] M. Luisier and G. Klimeck, Numerical strategies 
towards peta-scale simulations of nanoelectronics devices , Parallel Computing 36, 117-128 (2010). [20] 
M. Luisier, A. Schenk, W. Fichtner, T. B. Boykin, and G. Klimeck, A parallel sparse linear solver for 
nearest-neighbor tight-binding problems , Proc. of the 14th international Euro-Par conference on Parallel 
Processing, 790-800 (2008). [21] D. H. Kim and J. A. del Alamo, 30-nm InAs Pseudomorphic HEMTs on an 
InP Substrate With a Current-Gain Cutoff Frequency of 628 GHz , IEEE Elec. Dev.Lett. 29, 830-833 (2008). 
[22] G. Klimeck and M. Luisier, Atomistic Modeling of Realistically Ex­tended Semiconductor Devices with 
NEMO and OMEN , Computing in Science &#38; Engineering 12, 28-35 (2010). [23] S. Oktyabrsky and P. Ye, 
Fundamentals of III-V Semiconductor MOS-FETs , Springer (2009). [24] Q. Zhang, W. Zhao, and A. C. Seabaugh, 
Low-subthreshold-swing transistors , IEEE Elec. Dev. Lett. 27, 297-300 (2006). [25] T. Frederiksen, M. 
Paulsson, M. Brandbyge, and A.-P. Jauho, Inelastic transport theory from .rst principles: Methodology 
and application to nanoscale devices , Phys. Rev. B 75 205413 (2007). [26] A. Pecchia, G. Romano, and 
A. Di Carlo, Theory of heat dissipation in molecular electronics , Phys. Rev. B 75 035401 (2007). [27] 
Y. Asai, Nonequilibrium phonon effects on transport properties through atomic and molecular bridge junctions 
, Phys. Rev. B 78 045434 (2008). [28] M. Luisier and G. Klimeck, Atomistic full-band simulations of silicon 
nanowire transistors: Effects of electron-phonon scattering , Phys. Rev. B 80, 155430 (2009). [29] http://www.nccs.gov/computing-resources/jaguar/ 
[30] R. Lake, G. Klimeck, R. C. Bowen, and D. Jovanovic, Single and multiband modeling of quantum electron 
transport through layered semi­conductor devices , J. of Appl. Phys. 81, 7845 (1997). [31] N. Ashcroft 
and N. Mermin, Solid State Physics , Rinehart and Winston (1976). [32] P. M. Gresho and R. L. Sani, Incompressible 
Flow and the Finite Element Method: Isothermal Laminar Flow , John Wiley and Sons, New York (2000). [33] 
R. E. Bank, D. J. Rose, and W. Fichtner, Numerical Methods for Semiconductor Device Simulation , IEEE 
Trans. Electron Dev. 30, 1031 (1983). [34] W. Gropp, E. Lusk, N. Doss, and A. Skjellum, A high-performance, 
portable implementation of the MPI message passing interface standard , Parallel Computing 22, 789 (1996). 
[35] J. Dongarra, Basic Linear Algebra Subprograms Technical Forum Standard , International Journal of 
High Performance Applications and Supercomputing, 16, 1 111 (2002). [36] E. Anderson, Z. Bai, C. Bischof, 
S. Blackford, J. Demmel, J. Don­garra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, and D. Sorensen, 
LAPACK User s Guide , Third Edition, SIAM, Philadel­phia (1999). [37] O. Schenk and K. G¨artner, Solving 
Unsymmetric Sparse Systems of Linear Equations with PARDISO, Journal of Future Generation Computer Systems 
, J. of Future Generation Computer Systems 20, 475 (2004). [38] X. S. Li and J. W. Demmel SuperLU DIST: 
A Scalable Distributed Memory Sparse Direct Solver for Unsymmetric Linear Systems , ACM Trans. on Math. 
Software 29, 110 (2003). [39] P. R. Amestoy, I. S. Duff, and J.-Y. L Excellent, Multifrontal parallel 
distributed symmetric and unsymmetric solvers Comput. Methods in Appl. Mech. Eng. 184, 501 (2000). [40] 
T. A. Davis, A column pre-ordering strategy for the unsymmetric­pattern multifrontal method , ACM Trans. 
on Math. Software 30, 165 (2004). [41] R. S. Tuminaro, M. Heroux, S. A. Hutchinson, and J. N. Shadid, 
Of.cial Aztec User s Guide: Version 2.1 (1999). [42] A. Svizhenko, M. P.Anantram, T. R. Govindan, R. 
Biegel, and R. Venu­gopal, Two-dimensional quantum mechanical modeling of nanotransis­tors , J. Appl. 
Phys. 91, 2343-2354 (2002). [43] M. S. Lundstrom, On the Mobility Versus Drain Current Relation for a 
Nanoscale MOSFET , IEEE Elc. Dev. Lett. 22, 293-295 (2001). [44] L. P. Kadanoff and G. Baym, Quantum 
Statistical Mechanics , W. A. Benjamin Inc, New York (1962). </RefA>  
			
