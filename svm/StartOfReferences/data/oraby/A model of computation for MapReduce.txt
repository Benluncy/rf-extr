
 A Model of Computation for MapReduce Howard Karlo.* Siddharth Suri Sergei Vassilvitskii Abstract In 
recent years the MapReduce framework has emerged as one of the most widely used parallel computing platforms 
for processing data on terabyte and petabyte scales. Used daily at companies such as Yahoo!, Google, 
Amazon, and Facebook, and adopted more recently by several universities, it allows for easy parallelization 
of data intensive computations over many machines. One key feature of MapReduce that di.erentiates it 
from previous models of parallel computation is that it interleaves sequential and parallel computation. 
We propose a model of e.cient computation using the MapReduce paradigm. Since MapReduce is designed for 
computations over massive data sets, our model limits the number of machines and the memory per machine 
to be substantially sublinear in the size of the input. On the other hand, we place very loose restrictions 
on the computational power of of any individual machine our model allows each machine to perform sequential 
computations in time polynomial in the size of the original input. We compare MapReduce to the PRAM model 
of computation. We prove a simulation lemma showing that a large class of PRAM algorithms can be e.ciently 
simulated via MapReduce. The strength of MapReduce, however, lies in the fact that it uses both sequential 
and parallel computation. We demonstrate how algorithms can take advantage of this fact to compute an 
MST of a dense graph in only two rounds, as opposed to O(log(n)) rounds needed in the standard PRAM model. 
We show how to evaluate a wide class of functions using the MapReduce framework. We conclude by applying 
this result to show how to compute some basic algorithmic problems such as undirected s-t connectivity 
in the MapReduce framework. 1 Introduction In a world in which large data sets are measured in tera-and 
petabytes, a new form of parallel computing has emerged as an easy-to-program, reliable, and dis­tributed 
paradigm to process these massive quantities *AT&#38;T Labs Research, howard@research.att.com Yahoo! 
Research, suri@yahoo-inc.com Yahoo! Research, sergei@yahoo-inc.com of available data. The MapReduce framework 
was orig­inally developed at Google [4], but has recently seen wide adoption and has become the de facto 
standard for large scale data analysis. Publicly available statis­tics indicate that MapReduce is used 
to process more than 10 petabytes of information per day at Google alone [5]. An open source version, 
called Hadoop, has re­cently been developed, and is seeing increased adoption both in industry and academia 
[14]. Over 70 compa­nies use Hadoop including Yahoo!, Facebook, Adobe, and IBM [8]. Moreover, Amazon 
s Elastic Compute Cloud (EC2) is a Hadoop cluster where users can upload large data sets and rent processor 
time. In addition, at least seven universities (including CMU, Cornell, and the University of Maryland) 
are using Hadoop clusters for research [8]. MapReduce is substantially di.erent from previ­ously analyzed 
models of parallel computation because it interleaves parallel and sequential computation. In recent 
years several nontrivial MapReduce algorithms have emerged, from computing the diameter of a graph [9] 
to implementing the EM algorithm to cluster mas­sive data sets [3]. Each of these algorithms gives some 
insights into what can be done in a MapReduce frame­work, however, there is a lack of rigorous algorithmic 
analyses of the issues involved. In this work we begin by presenting a formal model of computation for 
MapRe­duce and compare it to the popular PRAM model. We show that a large subclass of PRAM algorithms, 
namely those using O(n2-.) processors and O(n2-.) total mem­ory, for a .xed .> 0, can be e.ciently simulated 
in MapReduce. We conclude by demonstrating two basic techniques for parallelizing using MapReduce and 
show their applications by presenting algorithms for MST in dense graphs and undirected s-t connectivity. 
1.1 MapReduce Basics In the MapReduce pro­gramming paradigm, the basic unit of information is a .key; 
value. pair where each key and each value are binary strings. The input to any MapReduce algorithm is 
a set of .key; value. pairs. Operations on a set of pairs occur in three stages: the map stage, the shu.e 
stage and the reduce stage, which we discuss in turn. In the map stage, the mapper µ takes as input a 
single .key; value. pair, and produces as output any number of new .key; value. pairs. It is crucial 
that the map operation is stateless that is, it operates on one pair at a time. This allows for easy 
parallelization as di.erent inputs for the map can be processed by di.erent machines.  During the shu.e 
stage, the underlying system that implements MapReduce sends all of the values that are associated with 
an individual key to the same machine. This occurs automatically, and is seamless to the programmer. 
In the reduce stage, the reducer . takes all of the values associated with a single key k, and outputs 
a multiset of .key; value. pairs with the same key, k. This highlights one of the sequential aspects 
of MapReduce computation: all of the maps need to .nish before the reduce stage can begin. Sine the reducer 
has access to all the values with the same key, it can perform sequential computations on these values. 
In the reduce step, the parallelism is exploited by observing that reducers operating on di.erent keys 
can be executed simultaneously. Overall, a program in the MapReduce paradigm can consist of many rounds 
of di.erent map and reduce functions, performed one after another. 1.2 MapReduce Example To better understand 
the power of the model, consider the following simple example of computing the k-th frequency moment 
of a large data (multi)-set. Let x be the input string of length n, and denote by xi the ith symbol in 
x. To represent the input as a sequence of .key; value. pairs, we look at x as a sequence of n pairs, 
.i; xi.. Let$bea special symbol. The program is as follows: 1. Begin by mapping every tuple to a pair 
with the symbol as the key, and the position as the value. Thus the .rst mapper, µ1 is de.ned as: µ1(.i; 
xi.)= .xi; i.. 2. After the aggregation by the key, the input to each reducer will be a unique string 
symbol, and the list of positions in which this symbol appears. We pro­ceed to collapse that list into 
a single number, de.n­ing the .rst reducer .1 as .1(.xi; {v1,...,vm}.)= .xi; mk.. 3. At this point we 
just want to sum the number of remaining pairs. First, map each pair to have the same key: µ2(.xi; v.)= 
.$; v.. 4. Since all of the pairs now have the same key, they will all be mapped to the same reducer. 
We can simply sum them: .2(.$; {v1,...,vl}.)=  .$; i vi.. Another major attraction of MapReduce, besides 
its ease of parallelization, is its ease of use. As the example shows, the framework shields the programmer 
from the low-level details of parallel programming such as fault tolerance, data distribution, scheduling, 
etc. Also, all of the data shu.ing and aggregation are handled by the underlying system itself. The programmer 
only needs to specify the map and reduce functions; the system-level issues are handled by the underlying 
implementation. The drawback of this model is that in order to achieve this parallelizability, programmers 
are restricted to using only map and reduce functions in their pro­grams [4]. Thus, this model trades 
o. programmer .exibility for ease of parallelization. This is a complex tradeo., and it is not a priori 
clear which problems can be e.ciently solved in the MapReduce paradigm. The main contribution of this 
work is a model for what is e.ciently computable in the MapReduce paradigm. 2 The MapReduce Programming 
Paradigm In this section we give a more formal de.nition of the MapReduce programming paradigm. We begin 
by de.ning mappers and reducers. We then describe how the system executes these two functions along with 
the shu.e step. As mentioned above, the fundamental unit of data in map reduce computations is the .key; 
value. pair, where keys and values are always just binary strings. Definition 2.1. A mapper is a (possibly 
randomized) function that takes as input one ordered .key; value. pair of binary strings. As output the 
mapper produces a .nite multiset of new .key; value. pairs. It is important that the mapper operates 
on one .key; value. pair at a time. Definition 2.2. A reducer is a (possibly random­ized) function that 
takes as input a binary string k which is the key, and a sequence of values v1,v2, ... which are also 
binary strings. As output, the re­ducer produces a multiset of pairs of binary strings .k; vk,1., .k; 
vk,2., .k; vk,3., .... The key in the output tu­ples is identical to the key in the input tuple. One 
simple consequence of these two de.nitions is that that mappers can manipulate keys arbitrarily, but 
reducers cannot change the keys at all. Next we describe how the system executes MapRe­duce computations. 
A map reduce program consists of a sequence .µ1,.1,µ2,.2,...,µR,.R. of mappers and reducers. The input 
is a multiset of .key; value. pairs denoted by U0. To execute the program on input U0: For r =1, 2,...,R, 
do: 1. Execute Map: Feed each pair .k; v. in Ur-1 to mapper µr, and run it. The mapper will generate 
a sequence of tuples, .k1; v1., .k2; v2.,.... Let U.  r be the multiset of .key; value. pairs output 
by µr, that is, U. = µr(.k; v.). r .k;v..Ur-1 2. Shuffle: For each k, let Vk,r be the multiset of values 
vi such that .k; vi.. U. . The underlying r MapReduce implementation constructs the multi­sets Vk,r from 
U. . r 3. Execute Reduce: For each k, feed k and some arbitrary permutation of Vk,r to a separate instance 
of reducer .r, and run it. The reducer will generate a sequence of tuples .k; v1., .k; v2.,.... Let Ur 
be the multiset of .key; value. pairs output by .r, that is, Ur = .r(.k; Vk,r.). k The computation halts 
after the last reducer, .R, halts. As stated before, the main bene.t of this program­ming paradigm is 
the ease of parallelization. Since each mapper µr only operates on one tuple at a time, the system can 
have many instances of µr operating on dif­ferent tuples in Ur-1 in parallel. After the map step, the 
system partitions the set of tuples output by vari­ous instances of µr based on their key. That is, part 
i of the partition has all .key; value. pairs that have key ki. Since reducer .r only operates on one 
part of this parti­tion, the system can have many instances of .r running on di.erent parts in parallel. 
3 The MapReduce Class (MRC) In this section we formally de.ne the MapReduce Class (MRC). There are three 
guiding principles that we wish to enforce in our de.nitions: Memory Since MapReduce allows for computation 
to be executed on parts of the input in parallel, the full power of the programming paradigm is realized 
when the input is too big to .t into memory on a single machine. Thus we require that the input to any 
mapper or reducer be substantially sublinear in the size of the data. Otherwise, every problem in P could 
be solved in the MapReduce formulation by .rst mapping the whole input to a single reducer, and then 
having the reducer solve the problem itself. Machines In order for the model to have practical rel­evance, 
we also limit the total number of available 3 machines. For example, an algorithm requiring nmachines, 
where n is the size of the Web, will not be practical in the near future. We limit the total number of 
machines available to be substantially sublinear in the data size. Time Finally, there is a question 
of the total running time available. In a major di.erence from previous work [6], we do not restrict 
the power of the individual reducer, except that we require that both the map and the reduce functions 
run in time polynomial in the original input length in order to ensure e.ciency. Furthermore, we will 
only consider programs that require a small number of Map Reduce rounds, because shu.ing is a time consuming 
operation. We are now ready to formally de.ne the class MRC. The input is a .nite sequence of pairs .kj; 
vj ., for j =1, 2, 3, ..., where kj and.vj are binary strings. The length of the input is n =(|kj| + 
|vj|). j Definition 3.1. Fix an .> 0. An algorithm in MRCi consists of a sequence .µ1,.1,µ2,.2,...,µR,.R. 
of operations which outputs the correct answer with probability at least 3/4 where: Each µr is a randomized 
mapper implemented by a RAM with O(log n)-length words, that uses O(n1-.) space and time polynomial in 
n.  Each .r is a randomized reducer implemented by a RAM with O(log n)-length words, that uses O(n1-.) 
space and time polynomial in n.  The total space (|k| + |v|) used by  .k;v..U. .key; value. pairs output 
by r µr is O(n2-2.). The number of rounds R = O(logi n). We note that while technically RAMs produce 
a sequence of .key; value. pairs as output, we interpret the sequence as a multiset of the corresponding 
pairs. We allow the use of randomization in MRC, and de­mand the .nal correct answer with probability 
at least 3/4, but there are obvious Las Vegas and deterministic variants, the latter of which we call 
DMRC. We emphasize that mappers process pairs one at a time, and remember nothing about the previous 
pairs. It also is important to remember that each reducer gets a sequence of values, in some arbitrary 
(not random) order. Nonetheless the output of the reducer must be correct, or must be correct with a 
certain probability if the algorithm is randomized, regardless of the order. Note that both mappers and 
reducers run in time polynomial in n, not polynomial in the length of the input they receive. At this 
point a careful reader may complain that the example algorithm given in Section 1 does not .t into this 
model. Indeed, if the string consists of n copies of the same symbol, then the input to a single reducer 
will be at least n, in violation of the space constraints in the model. We give an MRC algorithm for 
the frequency moments problem in Section 6.1.1.  3.1 Discussion We now pause to justify some of the 
modeling decisions made above. 3.1.1 Machines As we argued before, it is unrealis­tic to assume that 
there are a linear number of machines available when n large. As such we assume that the to­tal number 
of machines available is T(n1-.). We admit that algorithms with too small an . will be impracti­cal should 
n be large, but it seems unnatural to to tie one s hands by limiting the number of machines to an arbitrary 
bound of say, O(n1/2). Recall that each key gets mapped to a unique reducer instance. Since the total 
number of distinct keys may be as large as O(n2-2.), the total number of reduce instances may be just 
as large. Therefore more than one instance of a reducer may be run on the same machine. 3.1.2 Memory 
Restrictions As a consequence of mappers and reducers running on physical machines, the total space available 
to any map or reduce compu­tation is O(n1-.). One important consequence of this memory restriction is 
that the size of every .key; value. pair must be O(n1-.). Another consequence of this memory restriction 
is that the overall amount of memory available across all machines in the system is O(n2-2.). Because 
the reducers cannot begin executing until after the last mapper has .nished, the key value pairs output 
by the mappers have to be stored temporarily. Thus, the total space taken by all of the .key; value. 
pairs in U. must be r O(n2-2.). That the total memory available, across all machines, is O(n2-2.) allows 
one to duplicate the input somewhat, but not absurdly one is not restricted to simply partitioning the 
input. In contrast, mappers operate on one tuple at a time, and therefore they can execute on tuples 
immediately as they are emitted by the reducers. As such, there is no space restriction on the total 
size of the output of the reducers. 3.1.3 Shu.e Step In the shu.e step the system partitions the tuples 
across the T(n1-.) machines so that all of the pairs with the same key go to the same machine. This allows 
for the reducer to be executed on that machine. Observe that two pairs (k, Vk,r), (k.,Vk.,r), with k. 
=.k, may be sent to the same machine to be executed sequentially by di.erent reduce instances. The system 
must ensure that the memory of no machine is exceeded. Next we prove that the space restrictions in De.nition 
3.1 allow the shu.e step to place all of the values associated with a key on one machine without violating 
the memory constraints. Lemma 3.1. Consider round r of the execution of an algorithm in MRC. Let Kr be 
the set of keys in U. , let rVr be the multiset of values in U. , and let Vk,r denote r the multiset 
of values in U. that have key k. r Then Kr and Vr can be be partitioned across 1-.) T(n1-.) machines 
such that all machines get O(nbits, and the pair .k, Vk,r. gets sent to the same ma­chine. Proof. For 
a set of binary strings B denote by s(B)= |b| the total space used by the strings in B. Since b.B the 
algorithm is in MRC, by de.nition, s(Vr)+s(Kr) = s(U.)= O(n2-2.). Furthermore, the space of the rreducer 
is restricted to O(n1-.); therefore for all k, |k| + s(Vk,r) is O(n1-.). Using Graham s greedy algorithm 
for the minimum makespan scheduling problem [7, 13], we can conclude that the maximum number of bits 
mapped to any one machine is no more than the average load per machine plus the maximum size of any .k, 
Vk,r. pair. Thus, s(Vr)+ s(Kr) = + max (|k| + s(Vk,r)) number of machines k.Kr 2-2.) O(n 1-.) = + O(n 
T(n1-.) = O(n 1-.). We emphasize that every memory restriction in De.ni­tion 3.1 is necessary for execution 
of the shu.e step. 3.1.4 Time Restrictions Just as one can complain that . may be too small, resulting 
in impractical algo­rithms, one can justi.ably object that allowing arbitrary polynomial time per mapper 
and reducer is unreason­able. Our goal in de.ning MRC is to rigorously de­.ne the limitations imposed 
on the algorithm designer by the MapReduce paradigm. Just as before, we ad­mit that algorithms with polynomial 
running times of too high a degree will be impractical should n be large. However, it seems unnatural 
to limit the running time to an arbitrary bound of, say, O(n log n). Finally, the time per MapReduce 
round in practice can be large, so it is important to dramatically limit the number of rounds. In fact, 
we strive to .nd algorithms in MRC0, but will show that there are many nontrivial algorithms in MRC1 
. 4 Related Work We begin by comparing the MapReduce framework with other models of parallel computation. 
After that we discuss other works that use MapReduce.  4.1 Comparing MapReduce and PRAMs Nu­merous models 
of parallel computation have been pro­posed in the literature; see [1] for a survey of them. While the 
most popular by far for theoretical study is the PRAM, probably the next two most popular are LogP, proposed 
by Culler et al. [2], and BSP, pro­posed by Valiant [12]. These three models are all ar­chitecture independent. 
Other researchers have stud­ied architecture-dependent models, such as the .xed­connection network model 
described in [10]. Since the most prevalent model in theoretical com­puter science is the PRAM, it seems 
most appropriate to compare our MapReduce model to it. In a PRAM, an arbitrary number of processors, 
sharing an unbound­edly large memory, operate synchronously on a shared input to produce some output. 
Di.erent variants of PRAMs deal di.erently with issues of concurrent read­ing and concurrent writing, 
but the di.erences are in­signi.cant from our perspective. One usually assumes that, to solve a problem 
of some size n, the number of processors should be bounded by a polynomial in n a necessary, but hardly 
su.cient, condition to ensure e.ciency. There are two general strands of PRAM research. The .rst asks, 
what problems can be solved in polylog time on a PRAM with a polynomial number of proces­sors? Polylog 
time serves as a gold-standard for parallel running time, and a polynomial number of processors provides 
a necessary condition for e.ciency. The class NC is de.ned as the set of such problems. The sec­ond strand 
of research asks, what algorithms can be e.ciently parallelized? That is, for which problems are there 
parallel algorithms which are much faster than the corresponding sequential ones, yet with processor-time 
product close to the sequential running time. While theoretically appealing, the PRAM model su.ers from 
the practical drawback that fully shared­memory machines with large numbers of processors do not exist 
to date (though they may in the future) and simulations are slow. Building a large computer with a large 
robust shared memory seems di.cult. Moreover, allowing an arbitrary polynomial number of processors allows 
the creation of theoretically beautiful parallel algorithms which will never be run for any substantial 
n. It seems natural to inquire about the relations between MRC, DMRC, and known complexity classes such 
as NC and P. Since the comparisons are cleaner in the deterministic case, we focus on DMRC here, but 
there are analogous questions for MRC. Strictly speaking, before comparing DMRC to NC and P, one has 
to convert the binary string input .b1,b2, ..., bn. into the MapReduce input format, which we can do 
by replacing the bit string by the sequence ..1,b1., .2,b2., ..., .n, bn... In what follows we abuse 
the terminology and compare these classes directly. It is easy to see that DMRC .P, but is P. DMRC? Similarly, 
what is the relationship between DMRC and NC? We partially settle the answer to the latter question in 
Section 7, showing that a large class of languages L . NC are in DMRC as well. The answer to the converse 
question is DMRC a subset of NC? is NO, unless P = NC, but trivially so. Theorem 4.1. If P. = NC then 
DMRC .. NC. Proof. Assume P.NC. = Then any P-complete lan­guage, such as Circuit Value is not in NC. 
Recall the de.nition of Circuit Value: given a Boolean cir­cuit with one output gate, having AND, OR, 
and NOT gates, and Boolean values for the inputs, does the circuit evaluate to TRUE? We now pad inputs 
to Circuit Value, getting a new language Padded Circuit Value, which will be in DMRC - NC. Speci.cally, 
de.ne a new language Padded Circuit Value as follows. To generate all strings in Padded Circuit Value, 
take each string in Circuit Value (for which the output evaluates to TRUE) and append n2-n zeroes, if 
the input length was 2 n. Let N denote the size of the padded input, N = n. The key-value language associated 
to Padded Cir­cuit Value is clearly in DMRC, for now one needs v only memory roughly N to solve an instance 
of Padded Circuit Value of length N on one reducer, after stripping out the padding. However, Padded 
Circuit Value is P-Complete, as Circuit Value can be reduced to Padded Circuit Value in log space, and 
hence does not lie in NC (by the assumption that P.. = NC). While we strongly suspect that the answer 
to the question as to whether P . DMRC is NO, we cannot prove that there is a language in P whose associated 
key-value language lies outside DMRC. Any language, like all of those in DMRC, solv­able in polynomial 
time on a RAM in quadratic space is, by the generic simulation of RAM s by Turing ma­chines, solvable 
on a Turing machine simultaneously in space O(n2 log n) and polynomial time. (We are abus­ing the terminology 
a bit here, since, strictly speak­ing, languages are not in DMRC.) It follows that an obvious candidate 
for a language in P - DMRC would be a language which can be solved by a Tur­ing machine in polynomial 
time but not simultaneously with O(n2 log n) space. However, such languages are not known to exist. Speci.cally, 
if L were such a lan­guage, then L .. LOGSPACE, but L .P. So the desired L would be in P - LOGSPACE, 
yet whether P = LOGSPACE is a long standing open question.  4.2 MapReduce: Algorithms and Mod­els MapReduce 
is very well suited for naive parallelization for example, counting how many times a word appears in 
a data set. However, more recently algorithms have emerged for nontrivially parallelizable computations. 
Kang et al. [9] show how to use MapReduce to compute diameters of massive graphs, taking as an example 
a webgraph with 1.5 billion nodes and 5.5 billion arcs. Tsourakakis et al. [11] use MapReduce for counting 
the total number of triangles in a graph. Motivated by personalized news results, Das et al. [3] implement 
the EM clustering algorithm on MapReduce. Overall, each of these works gives practical MapReduce algorithms, 
but does not rigorously de.ne the framework under which they should be analyzed. Previously, Feldman 
et al. [6] introduced the notion of Massively Unordered Distributed (MUD) algorithms, a model based on 
the MapReduce framework. While modeling the same underlying system, their approach has two crucial di.erences 
from ours. First, in the MUD framework each reducer operates on a stream of data, whereas, in our model, 
each reducer has random access to all of the values associated with the given key. Second, in MUD, each 
reducer is restricted to only using polylogarithmic space. These distinctions gives our model more power 
and play an important role in our algorithms. 5 Finding an MST of a Dense Graph Using MapReduce Now that 
we have formally de.ned the MapReduce model, we proceed to describe an algorithm in MRC for .nding the 
Minimum Spanning Tree (MST) of a dense graph. As we shall exhibit, this algorithm will take advantage 
of the interleaving of sequential and parallel computation that MapReduce o.ers algorithm designers. 
Thus, given a graph G =(V, E) on |V | = N m = N1+c vertices and |E| = edges for some constant c> 0(n 
still denoting the length of the input, not the number of vertices), our goal is to compute the minimum 
spanning tree of the graph. We give a new algorithm for MST and then show how it can be easily parallelized. 
Fix a number k, and randomly partition the set of vertices into k equally sized subsets, V = V1 . V2 
.· ··. Vk, with Vi n Vj = Ø for i .For every pair {i, j}, = j and |Vi| = N/k for all i. let Ei,j . E 
be the set of edges induced by the vertex set Vi . Vj . That is, Ei,j = {(u, v) . E | u, v . Vi . Vj}. 
Denote the resulting subgraph by Gi,j =(Vi . Vj ,Ei,j). Assume without loss of generality (say, by append­ing 
an index to each weight to break ties) that all of the edge weights are unique. Our algorithm proceeds 
as fol­ k lows. First, for each of the subgraphs Gi,j , compute 2 the unique minimum spanning forest 
Mi,j. Then let H be the graph consisting of all of the edges present in some Mi,j: H =(V, .i,jMi,j ). 
Finally, compute M, the minimum spanning tree of H. The following theorem proves that this algorithm 
is correct. Theorem 5.1. The tree M computed by the algorithm is the minimum spanning tree of G. The 
algorithm works by sparsifying the input graph and then taking the MST of the resulting subgraph H. We 
show that no relevant edge was thrown out, that is, the minimum spanning trees of G and H are identical. 
Proof. Consider an edge e = {u, v} that was discarded, that is, e . E(G) but e .. E(H); we show that 
e is not part of the minimum spanning tree of G. Observe that any edge e = {u, v} is present in at least 
one subgraph Gi,j. If e .. Mi,j then by the cycle property of minimum spanning trees, there must be some 
cycle C . Ei,j such that e is the heaviest edge on the cycle. However, since Ei,j . E, we have now exhibited 
a cycle in the original graph G on which e is the heaviest edge. Therefore e cannot be in the MST of 
G and can safely be discarded. The algorithm presented above is far from the optimal sequential algorithm; 
however, it allows for easy parallelization. Notice that the minimum spanning tree for the individual 
subgraphs, Gi,j , can be computed in parallel. Furthermore, by setting the parameter k appropriately, 
we can reduce the memory used by each MST computation. As we show below, with high probability the memory 
used is O(m/k) when computing Mi,j and O(Nk) when computing the .nal minimum spanning tree of H. These 
two facts imply that the algorithm is in MRC. Lemma 5.1. Let k = Nc/2, then with high probability O(N1+c/2). 
the size of every Ei,j is Proof. We can bound the total number of edges in Ei,j by bounding the total 
degrees of the vertices. |Ei,j |= deg(v) + deg(v). For the purpose of the v.Vi v.Vj proof only, partition 
the vertices into groups by their degree: let W1 be the set of vertices of degree at most 2, W2, the 
set of vertices with degree 3 or 4, and generally Wi = {v . V :2i-1 < deg(v) = 2i}. There are log N total 
groups. Consider the number of vertices from group Wi that are mapped to part Vj. If the group has a 
small number of elements, that is, |Wi| < 2Nc/2 log N, then  deg(v) = 2N1+c/2 log N = O (N1+c/2). If 
the v.Wi group is large, that is, |Wi|= 2Nc/2 log N, a simple application of Cherno. bounds says that 
the number of elements of Wi mapped into the partition j, |Wi n Vj | 1 is O(log N) with probability at 
least 1 - . Therefore N with probability at least 1 - log N : N deg(v) = deg(v) v.Vj iv.VjnWi = 2N1+c/2 
log2 N i O(N1+c/2). = Lemma 5.1 tells us that with high probability each part has O (N1+c/2) edges. Therefore 
the total input size to any reducer is O(n1-.). The algorithm uses the sequential computation available 
to reducers to compute the minimum spanning tree of the subgraph given to that reducer. There are Nc 
total parts, each producing a spanning tree with 2N/k - 1= O(N1-c/2) edges. Thus the size of H is O(N1+c/2) 
bounded by = O(n1-.), again being small enough to .t into the memory of a single machine. 6 An Algorithmic 
Design Technique For MRC We begin by describing a basic building block of many algorithms in MRC called 
MRC-parallelizable func­tions. We then show how a family of such functions can used as subroutines of 
MRC computations. After that we show how this can be used to compute frequency moments on large inputs, 
and s-t connectivity on undi­rected graphs. Definition 6.1. Let S be a set. Call a function f on S MRC-parallelizable 
if there are functions g and h so that: 1. For any partition T = {T1,T2,...,Tk} of S, where .iTi = S 
and Ti = Ø for i . n Tj = j (of course), f can be expressed as: f(S)= h(g(T1),g(T2),...,g(Tk)). 2. 
g and h can be expressed in O(log n) bits. 3. g and h can be computed in time polynomial in |S|and every 
output of g can be expressed in O(log n) bits.  Intuitively, this de.nition says that if one wants to 
evaluate f on a set S, one could do so by partitioning S arbitrarily, applying g to each part of the 
partition, and then applying h to the results. Next we show how a family of such functions can be computed 
under the memory restrictions imposed by MRC. Lemma 6.1. Consider a universe U of size n and a collection 
S = {S1,...,Sk} of subsets of U, where 2-2. 2-3. Si .U, .k |Si|= n, and k = n. Let i=1 F = {f1,...,fk} 
be a collection of MRC-parallelizable functions. Then the output f1(S1),...,fk(Sk) can be 1-.) computed 
using O(n1-.) reducers each with O(nspace. This lemma says that a family of MRC­parallelizable functions 
de.ned over subsets of the same universe can be computed as a subroutine of a MapRe­duce computation 
where the original input size is n. Since O(n2-2.) is be the global amount of memory avail­able to any 
MapReduce program, the lemma requires that the input has few enough subsets that they .t in memory, and 
that the sum of the sizes of the subsets also .ts into memory. The power of the MRC-parallelizable functions 
lemma is that it allows an algorithm designer to focus on the structure of the problem and the in­put; 
the lemma will handle how to distribute the input across the reducers in such a way as to not over.ow 
the memory of any one reducer. At a high level, we would like to assign a reducer for each set Si, map 
both the elements of Si and the function fi itself to the same reducer and compute the output. There 
is two technical challenge which we need to be wary of. The Si may be too large to .t on one reducer. 
In particular if |Si| >n1-. then the computation of fi(Si) needs to be spread across several reducers. 
To deal with these issues we use the fact that functions fi are MRC-parallelizable to our advantage. 
In the .rst round we partition the set of reducers into t di.erent blocks. Each set Si is then partitioned 
across the reducers in its assigned block, which computes the intermediate values gi(Si). This partitioning 
ensures that the input to any individual reducer is not too large. In the second round, we map all of 
the intermediate results of block Si to the same reducer, and compute the .nal output using the function 
hi. The mapping in this step will again ensure that no reducer is inundated with an input that is larger 
than its memory. Input The input to the subroutine consists of pairs of the form .i; u. indicating that 
u . Si, and the individual functions gi and hi for all i . [k]. Initialize Let M = n1-. denote the number 
of reduc­ers the subroutine will use. Partition them into blocks of size B = T(n.). Let t = .M/B. be 
the total number of blocks. Construct universal hash functions, hash1 and hash2 :[k] . [t]. Map 1: For 
each .i; u., output .r;(u, i). where r is chosen uniformly at random among the reducers in block Bhash1(i). 
Map each function gi and hi to  .b;(gi,i). and .b;(hi,i)., for every b in the block Bhash1(i). Reduce 
1: The input to this reducer is of the form .r; ((u1,i),..., (uk,i), (gi,i), (hi,i))., where {u1,u2,...,uk} 
= Tj . Si is one of the parts in the partition of Si induced by Map 1. The reducer computes gi(Tj) and 
outputs .r;(gi(Tj ), i, hi).. Map 2: The input to the mapper is of the form .r;(gi(Tj), i, hi).. The 
mapper outputs .hash2(i); (gi(Tj),hi)).. Reduce 2: The input to the .nal reducer is .hash2(i); ((gi(T1),hi), 
(gi(T2),hi),..., (gi(TB),hi))., where the set {T1,T2,...,TB} forms a partition of Si. The reducer computes 
hi and outputs .hash2(i); hi(gi(T1),gi(T2),...,gi(TB)). = .hash2(i); fi(Si).. The next lemma shows that 
hash1 prevents any reducer in the Reduce 1 phase from over.owing its memory. Lemma 6.2. Each reducer 
in step Reduce 1 will have O (n1-.) elements mapped to it with high probability. We prove the Lemma by 
showing that each n.-sized block of reducers gets O (n) elements mapped to it with high probability. 
Since the individual reducer for each element is selected uniformly at random from those in a block, 
an easy application of Cherno. bounds completes the Lemma. Proof. Partition the sets Si into groups, 
such that group Gj = {Si .S :2j-1 < |Si|= 2j}. Since |Si| is bounded by n, there are at most log n such 
groups. De.ne the volume of group j as Vj = |Gj |· 2j. Groups having volume less than O(n log n) could 
all be mapped to one block without violating the space restrictions of the reducers. We now focus on 
groups with Vj >n log n. Let Gj be such a group; then Gj contains between n log n 2n log n and elements. 
Fix a particular block 2j 2j of reducers. Since the size of the block is n, there 1-2. are nsuch blocks. 
Since hash1 is universal, the probability that any set S . Gj maps to a particular 2.-1 block is exactly 
n. Therefore, in expectation, the number of elements of Gj mapping to this block is . = 2n 2. log n 1-2. 
. A bad event happens if more than d = n 2j elements map to this block, as that would result in a total 
volume of O(n log n). However Cherno. bounds tell us that the probability of such an event happening 
is less than 2-(1+d). = O(1/n2). Taking a union bound over all n1-. blocks and log n groups, we can conclude 
that the probability of any block, and therefore any reducer, being overloaded is bounded below by 1 
- 1/n. Lemma 6.3. With high probability, each reducer in step Reduce 2 will have at most n1-. values 
of gi mapped to it. Proof. Since hash2 is universal, in expectation the k number of sets mapped to a 
block in Reduce 2 is . t If k<t then each set can be mapped to its own block. kn 1-. If k = t then = 
2-3. = n. Denote by Nb the 1-2. tn number of sets mapped to block b. By the Cherno. bound, < 2-(1+log 
n) k t Pr Nb > (1 + log n) k = 1/n. t Since there are t = T(n1-2.) blocks, applying the union bound shows 
that the probability any reducer gets 1 overloaded is O( 2. ). . n A similar argument to Lemma 6.3 shows 
that the reducers have enough memory to store the gi and hi functions. This combined with Lemmas 6.2 
and 6.3 and the fact that gi and hi are polynomial-time computable prove Lemma 6.1. 6.1 Applications 
of the Functions Lemma As mentioned above the power of the functions lemma is that it allows the algorithm 
designer to think of parallel algorithms without the worry of overloading a particular reducer. The memory 
restrictions of Lemma 6.1 allow it to be used as a subroutine when the size of input of the calling MRC 
algorithm is n. Because the subroutine uses O(n1-.) reducers each with O(n1-.) memory, it does not violate 
any constraints speci.ed by the MRC class when original input size is n. Next we show two explicit examples 
of the use of this subroutine. The .rst uses Lemma 6.1 twice to compute frequency moments where the fi 
are identical. The second uses Lemma 6.1 as a subroutine where the fi are di.erent for each i. 6.1.1 
Frequency Moments Suppose we would like to compute the kth frequency moment of a string. Let L be the 
string alphabet, and represent a length-n string as a set of pairs .i, .i. where i . [N] represents the 
position and .i .L is the symbol at position i. This set of pairs is also the universe U. For every element 
. .L, denote by S. . U the set of pairs in U containing letter .. To compute frequency moments we need 
to compute f. = |S.|k . Summing the values of the f. returns the frequency moment. It is easy to see 
that f. is an MRC­parallelizable function. De.ne g as the size function, g({t1,t2,...,tk})= k, and h 
as h(i1,i2,...,im)= (i1 + i2 + ···+ im)k . For any partition T =(T1,...,Tm) of S., h(g(T1),g(T2),...,g(Tm)) 
= |S.|k . Thus, one application of the functions lemma yields the values of the f.(S.). We can then use 
another simple application of the functions lemma to compute the overall frequency  moment: f.(S.). 
..L 6.1.2 Undirected s-t connectivity Suppose we are given an N-node graph G =(V, E) and two nodes s, 
t . V and we are asked whether there exists a path from s to t. Note that this problem can be e.ciently 
computed by PRAMs, thus we can use the Simulation Theorem (Theorem 7.1) to achieve such an algorithm. 
In this section, however, we give a more direct approach. In the case that the graph is relatively dense, 
with N1+O(1) |E| = , we can use matrix multiplication to Nth compute the power of the adjacency matrix 
in O(log N) rounds1 . If, however, the graph is sparse, the full adjacency matrix will not .t into memory 
across all of the machines (recall that the total memory available is N2-2., whereas the full matrix 
will be of size N2) and we need to resort to other methods. In what follows we give a simple labeling 
algorithm that computes s-t connectivity on sparse graphs in O(log N) rounds2 . We .rst give the high 
level details and then describe how to implement it in MapReduce. Throughout the algorithm, each node 
v . V main­tains a label .(v), describing the connected component it is in. Denote by Lv . V as the set 
of vertices with label v. Lv represents the connected component con­taining v. Following standard notation, 
we de.ne G(v) to be the set of neighbors of v. For a set S, denote by G(S) the set of neighbors of all 
nodes in S themselves not in S. Finally, denote by G.(v) = G(Lv). Let p denote an arbitrary total order 
on the vertices. 1. Begin with every node v . V being active with label .(v)= v. 2. For i =1, 2, 3, 
..., O(log N) do: (a) Call each active node a leader with probability 1/2. (b) For every active non-leader 
node w, .nd the smallest (according to p) node w * . G.(w). (c) If w * is not empty, mark w passive 
and relabel *   each node with label w by w . 3. Output true if s and t have the same labels, false 
otherwise. Lemma 6.4. At any point of the algorithm, if any two nodes s and t have the same label, then 
there is a path from s to t in G. 1Dense matrix multiplication is trivial in MRC partition each matrix 
into blocks and multiply the blocks before aggregating the results. 2We suspect that this is a standard 
connectivity algorithm in the PRAM literature. Proof. The proof proceeds by induction. At the begin­ning 
of the algorithm every node has its own label and the statement is vacuously true. Suppose the statement 
is true at the beginning of round i. The only interesting case is when .(s) .= .(t) before the iteration 
and .(s)= .(t) after the iteration. * Consider a non-leader node w, and a node w as described in the 
algorithm. Assume without loss of generality that s . Lw and t . Lw * . By induction, * there exist paths 
from s to w and from w to t. The de.nition of G.(v) ensures that there exists a node u, with .(u)= .(w), 
and the edge (u, w *) . E. Thus the * path s . w . u . w . t is in G (the w . u path existing because 
l(u)= l(w)). . Lemma 6.5. Every connected component of G has a unique label after O(log N) rounds with 
high probability. Proof. To prove the running time we show that the number of labels in any connected 
component decreases by a constant factor (in expectation) in every round, un­til, of course, every vertex 
in the connected component has the same label. Fix an active node u (note that the total number of distinct 
labels is equal to the number of active nodes.). If the component containing u has more than one label, 
then there must exist a node v. . G.(u) with a di.erent label from u. Let .(v.)= v. With prob­ability 
1/4 the active node v is selected as a leader and u is a non-leader. Then v. . G.(u), and u will be relabeled 
as v and marked passive. Therefore, the probability of any node s surviving a round while there is more 
than one label in a connected component is at most 3/4. An application of Cherno. bounds concludes the 
proof. . So far we have proven that the above algorithm is correct; we now show how to implement it in 
MapRe­duce. The key to the parallelization is that leader se­lection, follower selection and the relabeling 
can all be done in parallel. To make this more precise we turn again to the Functions Lemma. Selecting 
the set of leaders in parallel is trivial. To select the followers, let hash1 : V .{0, 1} be a universal 
hash function; the set of leaders is precisely those active v . V with hash1(v) = 1. The next task is 
for every non-leader node w to compute the * * node w that it will be following. Observe that w depends 
on G.(w) . V , in fact the algorithm requires the minimum label from nodes in G.(w). Since min is an 
MRC-parallelizable function, it .ts the conditions of the Theorem. The only thing that remains is computing 
the individual sets G.(w). We achieve this by scanning through all of the edges. For an edge {u, v} we 
can check if the labels of the endpoints agree. If not, then .(v) . G(.(u)) and .(u) . G(.(v)), where 
abusing notation we use .(v) to refer to the node that v is labeled with.  Finally, we describe the 
relabeling step. Let w and * w be as in the description of the algorithm. We need to relabel all of the 
nodes with the label .w to have the label .w * . For the subset Lw, let fw be such a relabel function. 
It is easy to check that the family of sets {Lw}and the family of functions {fw} satis.es the conditions 
of the Lemma 6.1. 7 Simulating PRAMs via MapReduce Theorem 7.1. Any CREW PRAM algorithm using O(n2-2.) 
total memory, O(n2-2.) processors and t = t(n) time can be run in O(t) rounds in DMRC. In this proof 
we will show that such a PRAM algorithm can be simulated by an algorithm in DMRC. At a high level we 
will use O(n2-2.) reducers where one reducer simulates each processor used in the PRAM algorithm and 
another reducer simulates each memory location used by the PRAM algorithm. Conceptually, we will use 
the mappers to route memory requests and ship the relevant memory bits to the reducer responsible for 
the particular processor. Each reducer will then perform one step of computation for each of the PRAM 
processors assigned to it, write out memory updates, and request new memory positions. The process then 
repeats. The authors of [6] give a similar simulation algorithm in their work. Proof. We reduce the simulation 
problem to only keep­ing track of updated memory locations. Therefore we ensure that every memory location 
is updated every round by modifying the PRAM algorithm to have an ex­tra O(n2-2.) processors (one for 
each location in mem­ory). At every time step each of these dummy pro­cessors requests a unique memory 
address and attempts to write the same value back to it. If at any point in time there are two writes 
to the same memory location, the dummy value gets overwritten. We now describe the simulation. At time 
t of the PRAM algorithm let bt denote the .address, value. pairs i that processor i reads from. Let bt 
= Ø if processor i i does not read from a memory location at time t. Let t w be the .address, value. 
pair that processor i writes i t to at time t. Let w= Ø if processor i does not write to i a memory location 
at time t. We will show how the computation at time t is executed by a constant number of MapReduce steps. 
Assume inductively that reducer .t has as input .i; bt.. 1 i Then .t will simulate one step of the computation 
for 1 t+1 tt+1 the processor and output .i; r ,w., where r is the ii i memory address that processor 
i will need during the t next time step; and wis the .address; value. pairs that i were written to during 
time t. t t+1t The next mapper µwill take as input .i; r,wi .. 1 The mapper outputs .rt+1; i. signifying 
the memory t location requested by processor i. Moreover, let w= (a, v) where a is an address and v the 
value written to t it, then the mapper also outputs .a; w,i. signifying the i update to the state of 
the memory. The next reducer .t takes as input tuples of two 2 types. The .rst type has form .aj ;(aj 
,vj ),i. which represents that the new value for address aj is vj . It will get such values for all writes 
that were done to address aj. Since the PRAM algorithm is CREW, this tuple will only occur once per memory 
address aj. The second type of input it will take has form .aj ; i.. This represents that processor i 
would like the value in address aj. Thus, .t ful.lls this request by outputting 2 t .aj ;(aj ,vj),i.. 
Finally map µmakes sure that the 2 processor i gets the new value for aj by taking as input .aj ;(aj 
,vj),i). and outputting .i; aj,vj.. . 8 Conclusion We have presented a rigorous computational model for 
the MapReduce paradigm. By restricting both the total memory per machine and the total number of machines 
to O(n1-.) we ensure that the programmer must paral­lelize the computation and that the number of machines 
used must remain relatively small. The combination of these two characteristics were not previously captured 
in the PRAM model. We strived to be parsimonious in our de.nitions, and therefore speci.cally did not 
re­strict the time available for a reducer to be, for example, linear. Rather we simply require that 
mappers, as well as reduces run in polynomial time. Acknowledgements We would like to thank the anonymous 
reviewers for their insightful comments. References <RefA>[1] D. K. G. Campbell. A survey of models of parallel 
computation. Technical report, University of York, March 1997. [2] D. Culler, R. Karp, D. Patterson, 
A. Sahay, K. E. Schauser, E. Santos, R. Subramonian, and T. von Eicken. LogP: Towards a realistic model 
of parallel computation. ACM SIGPLAN Symposium on Prin­ciples and Practice of Parallel Programming, 4:1 
12, May 1993. [3] A. Das, M. Datar, A. Garg, and S. Rajaram. Google news personalization: Scalable online 
collaborative .ltering. In Proceedings of WWW, pages 271 280, 2007.  [4] J. Dean and S. Ghemawat. Mapreduce: 
simpli.ed data processing on large clusters. In Proceedings of OSDI, pages 137 150, 2004. [5] J. Dean 
and S. Ghemawat. Mapreduce: simpli.ed data processing on large clusters. Commun. ACM, 51(1):107 113, 
2008. [6] J. Feldman, S. Muthukrishnan, A. Sidiropoulos, C. Stein, and Z. Svitkina. On distributing symmetric 
streaming computations. In S.-H. Teng, editor, SODA, pages 710 719. SIAM, 2008. [7] R. L. Graham. Bounds 
on multiprocessing anomalies and related packing algorithms. In AFIPS 71 (Fall): Proceedings of the November 
16-18, 1971, fall joint computer conference, pages 205 217, New York, NY, USA, 1971. ACM. [8] Hadoop 
wiki -powered by. http://wiki.apache.org/ hadoop/PoweredBy. [9] U. Kang, C. Tsourakakis, A. Appel, C. 
Faloutsos, and J. Leskovec. HADI: Fast diameter estimation and mining in massive graphs with hadoop. 
Technical Report CMU-ML-08-117, CMU, December 2008. [10] F. T. Leighton. Introduction to Parallel Algorithms 
and Architectures: Arrays, Trees, Hypercubes. Morgan Kaufmann, 1992. [11] C. E. Tsourakakis, U. Kang, 
G. L. Miller, and C. Faloutsos. Doulion: Counting triangles in massive graphs with a coin. In Knowledge 
Discovery and Data Mining (KDD), 2009. [12] L. G. Valiant. A bridging model for parallel computa­tion. 
CACM, 33(8):103 111, August 1990. [13] V. V. Vazirani. Approximation Algorithms. Springer, March 2004. 
[14] Yahoo! partners with four top universities to advance cloud computing systems and applications research. 
Yahoo! Press Release, 2009. http://research.yahoo. com/news/2743.</RefA>  
			
