
 A New Algorithmic Approach to the General Lovfisz Local Lemma with Applications to Scheduling and Satisfiability 
Problems* [Extended Abstract] t Artur Czumaj~ Dept. of Computer and Information Science New Jersey Institute 
of Technology University Heights Newark, NJ 07102-1982, USA  CZU maj @ cis. njit.ed u  ABSTRACT The 
LovAsz Local Lemma (LLL) is a powerful tool that is in- creasingly playing a valuable role in computer 
science. It has led to solutions for numerous problems in many different ar- eas, reaching from problems 
in pure combinatorics to prob- lems in routing, scheduling and approximation theory. How- ever, since 
the original lemma is non-constructive, many of these solutions were first purely existential. A breakthrough 
result by Beck and its generalizations have led to polynomial time algorithms for many ~f these problems. 
However, these methods can only be applied to a simple, symmetric form of the LLL. In this paper we provide 
a novel approach to de- sign polynomial-time algorithms for problems that require the LLL in its general 
form. We apply our techniques to find good approximate solutions to a large class of NP-hard prob- lems 
called minimax integer programs (MIPs). Our method finds approximate solutions that are --especially 
for prob- lems of non-uniform character --significantly better than all methods presented before. To 
demonstrate the applica- bility of our approach, we apply it to transform important results in the area 
of job shop scheduling that have so far been only existential (due to the fact that the general LLL was 
used) into algorithms that find the predicted solutions (with only a small loss) in polynomial time. 
Fhrthermore, ¢Work partly done while the author was with Heinz Nixdorf Institute and Department of Mathematics 
and Computer Science at the Paderborn University, Germany. *Research partially supported by DFG-Sonderforschungs- 
bereich 376 "Massive Parallelit/it: Algorithmen, Entwurfs- methoden, Anwendungen." tSee w~w.upb, de/cs/chrsch.htral 
for a full version of this paper. Permission to make digital or hard copies ofali or part of this work 
for personal or classroom use is granted without fee provided that copies are not made or distributed 
for profit or commercial advantage and that copies bear this notice and the lull citation on the first 
page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior 
specific permission and/or a fee. STOC 2000 Portland Oregon USA Copyright ACM 2000 1-58113-184-4/00/5...$5.00 
Christian Scheideler Dept. of Mathematics &#38; Computer Science and Heinz Nixdorf Institute Paderborn 
University 33095 Paderborn, Germany   chrsch @ uni-paderborn.de we demonstrate how our results can 
be used to solve satis- fiability problems. 1. INTRODUCTION The probabilistic method is used to prove 
the existence of objects with desirable properties by showing that a ran-domly chosen object from an 
appropriate probability distri- bution has the desired properties with positive probability. In most 
applications, this probability is not only positive but is actually high and frequently tends to 1 as 
the pa-rameters of the problem tend to infinity. In such cases, the proof usually supplies an efficient 
randomized algorithm for producing a structure of the desired type. There are, however, certain examples, 
where one can prove the existence of the required combinatorial structure by prob- abilistic arguments 
that deal with rare events; events that hold with positive probability which is exponentially small in 
the size of the input. This happens often when using the Lov£sz Local Lemma (LLL) [10]. We will use this 
lemma in its general form. LEMMA 1 (LOV~,SZ LOCAL LEMMA). Let A1,... , An be a set of "bad" events in 
an arbitrary probability space and let G be a dependency graph for the events A1,... , An. (That is, 
Ai is independent of any subset of events Aj with (i, j) G.) Assume there exist xi E [0, 1) for all 1 
< i < n with Pr[di]<xi 1-I (1-xj) (i,j)eG for all i. Then with positive probability no bad event occurs. 
In its symmetric form, the LLL is defined as follows. LEMMA 2 (SYMMETRIC LLL). Let Az,... ,An be a set 
of "bad" events with Pr[Ai] ~_ p for all i. If each Ai is mutually independent of all but at most d 
of the other events Aj and ep(d + 1) _< 1, then with positive probability no bad event occurs. Many 
applications of the LLL can be found in the literature (see, e. g., [2; 3; 4; 8; 10; 11; 12; 17; 19; 
20; 21; 22; 24; 26; 29; 30]). To turn proofs using the LLL into efficient algorithms, even random ones, 
proved to be difficult for many of these applications. In a breakthrough paper [7], Beck presented a 
method of converting some applications of the LLL into polynomial-time algorithms (with some sacrifices 
made with regards to the constants in the original application). Alon [1] provided a parallel variant 
of the algorithm and simplified the arguments used. His method was further generalized by Molloy and 
Reed [23] to yield efficient algorithms for a number of applications of the symmetric form of the LLL. 
There have been only very few cases for which polynomial time algorithms for applications of the general 
LLL have been found without requiring a reduction to the symmetric LLL. Molloy and Reed [23] found methods 
for the problems of/?-frugal coloring and acyclic edge coloring that could pos- sibly be applied to problems 
thai require the general LLL, but as it was pointed out by the authors, they may require to prove some 
(possibly difficult) concentration-like prop- erties for each problem under consideration. Recently, 
the authors [9] were able to design and analyze a polynomial- time algorithm for the problem of 2-coloring 
non-uniform hypergraphs and related coloring problems. In Section 2 of this paper, we will present a 
constructive form of the general LLL that cannot be proved (without significant modifications) by the 
method in [9]. We will demonstrate in Section 3 how to use this result to construct efficient approximation 
algorithms for so-called minimax in- teger programs (MIPs). For every k E IN, let [k] represent the set 
{1,... ,k}. Definition 1. A MIP has variables {xi,j : i E [n], j E [g~]}, for some integers ~. Let N 
-= ~e[-] ~ and let x denote the N-dimensional vector of the variables x~,j. A MIP seeks to minimize a 
real Y subject to: 1. a system of linear inequalities Am <_ y, where A E [0, 1] mxN and y is the m-dimensional 
vector with the variable Y in each component, 2. for all i E [n]: )'-~je[l~] xi,j = 1, and 3. for all 
i and j: xi,j E {0, 1}.  MIPs represent a general framework for choice problems (see properties (2) 
and (3)). Many applications in the area of routing and scheduling can be formulated as MIPs (for several 
examples, see [21]). In general, MIPs are NP-hard. A common strategy to find good approximate solutions 
to MIPs is to first solve its linear relaxation and then to use ran- domized rounding. This approach 
was pioneered by Ragha- van and Thompson [25] and was later extended and refined for MIPs by Srinivasan 
[30], Lu [22] and Leighton, Rao and Srinivasan [21]. The latter papers apply the symmetric form of the 
LLL to construct good rounding strategies. Leighton et al. [21] give as a high-level application of MIPs 
the hyper-graph partitioning problem [13]: Let H ---(V,E) be a set system with V ---- [n] and E --{S1,... 
, SM} C 79(V). Given a positive integer/?, the prob- lem is to partition V into ~ parts, so that the 
sets are split well: we want a X : V --~ [g] that minimizes Y = maxjeiM],keE~ 1 [{i ~ S~ : z(i) = k}l. 
The cost function above has the drawback that it only seeks to minimize the absolute split size. However, 
there axe ap- plications where it would be much more desirable to min- imize the relative split size, 
i. e., to find a function X that minimizes Y ---- maxje[M],ke[t] I{i E Sj : x(i) = k}l/iSj]. Problems 
of this kind can be found, for instance, in a paper by Feige and Scheideler [12]. Their solution (which 
heav- ily uses the general LLL) allowed to prove upper bounds on the makespan of job shop schedules that 
significantly im- proved the previously best results (see Section 4.1). Solving these problems with the 
techniques given in [21; 22] gives an approximation ratio that is as large as e( 1°~ ) (n is log log 
n the problem size), whereas an approximation ratio of partly as low as 1 + o(1) is required for the 
proofs in [12] to be constructivized. Our method presented in this paper will achieve this goal. 1.1 
New results Our main technical contribution is a novel approach to de- sign polynomial-time algorithms 
for problems that require the general LLL (see Section 2). We will apply this approach to find good approximation 
algorithms for MIPs: Consider any MIP. Our strategy is to start with an optimal solution {X;,j : i E 
[n], j E Jill} to the LP relaxation of the MIP (i. e., the integrality constraints in Definition 1 (3) 
are removed). The resulting LP optimum y* is clearly a lower bound for y, the optimum of the (integral 
form of the) MIP. We will present a randomized rounding algorithm that exploits the dependencies among 
the rows of matrix A to find a good approximate solution for y. These dependencies are defined as follows. 
Definition 2. Given a MIP instance I as defined above, let the dependency graph Gz of I consist of the 
node set [m] and an edge set that contains edge (r, s) if and only if there are i,j,S so that ar,(i,j) 
> 0 and as,u,j,) > 0, and r ~ s or jCj'. Note that Gz may contain self-loops. This simplifies our proofs. 
However, it would also be possible to avoid them. Based on our LLL techniques, we will prove the following 
main result. THEOREM 1. Given a MIP instance I, let x* be its op- timal fractional solution and y* be 
the vector with y~ = ( Aw * )~ for all r. Consider the random experiment of setting xi,j to 1 and all 
other xi,j, 's to 0 with probability x~,j. For any vector c~ E IRr~, let Pr = e -'min[c~'' a,,.]y~/3 
 for all r E [m], where y~ = y*/(maxc at,c). (pr is a Chernoff bound for Pr[(Ax), _> (1 + mr)y;].) Let 
0 < ~ < 1 be a con- stant that is chosen such that ~ > max[(y~) -1, (y~)-(1-~)/2] for every r. Furthermore, 
let q, = e-In'P71 and let z~ = q~ be at most 1/3, where 6 is a su~ciently small positive con-stant. If 
it holds that q, < z, 1-I -z,) (1) (r,s)EGI for all r, then there is an algorithm that finds a vector 
m in polynomial time so that (Am)~ <_ (1 q- O(~))y; for all r. The main feature of this result is its 
great flexibility: It al- lows to exploit local dependencies among the rows to achieve approximate solutions 
that allow to distinguish between rows that require a very good or just a rough approximation. This is 
important, since MIPs may be --especially in real-world applications --composed of different classes 
of restrictions that have different dependency structures and that require different approximation ratios. 
In [21; 22] only the maxi- mum dependency of a row on other rows can be used. As we will demonstrate 
in Section 4, the property of be- ing able to exploit local dependencies and having different approximation 
ratios is vital to constructivize the job shop scheduling results presented in [12]. We also show how 
to apply our method to obtain the first approximation algo- rithm for MAX SAT problems that is able to 
exploit local dependencies. Finally, we note that the factor 2 in the exponent in (1) can be brought 
arbitrarily close to 1 if the z~'s are sufficiently small.   2. THE LOCAL LEMMA ALGORITHM We start 
with presenting a general approach to design poly- nomial-time algorithms for problems that require the 
general LLL. Let V dertote a set consisting of n independent, random vari- ables called trials. For each 
trial t E V, let f~t denote the set of its possible outcomes. Let ,4 be a set of m "bad" events, where 
each event A E A is determined by the outcome of the trials inTA C V. Let E= {TA ] Ae`4}. Two events 
A1,A2 E `4 are said to be neighbors if TA1 NTA2 # ~. For every A E .4, let N(A) denote the set of all 
neighbors of A. Clearly, event A is mutually independent of every subset of events A' with A' f~ N(A). 
We assume that for every event A and every set S C TA, the event A restricted to S (written as A[s) is 
well-defined. Let Pr[AIs ] denote the probability that AIs is true under the assumption that an outcome 
is chosen independently at random for each trial in S. Suppose that the following natural assumptions 
are valid: For every trial, a random outcome can be generated in polynomial time, and  it can be decided 
in polynomial time for every event A and set S C TA whether AIs is true.  Then the following theorem 
holds. THEOREM 2. There is a constant A > 0 so that for every 0 < 5 <_ A the following holds: Let At,... 
,Am be a set of "bad" events over a set V of independent random trials, IVI = n. Every trial t E V has 
a set of outcomes of size at most polylogarithmic in n+m. Let pi = e--[TAil for all i E [m], where 0 
< e < 1 is a constant away from O. Suppose that for all S C TAI it holds that ills I > ITA,] e then 
Pr[Ai[s] <_pi, and  ills I < ITA, I ~ then Pr[Ails ] -- 0.  Furthermore, assume that for q~ = e- ln~ 
PTI and x~ = q~ it holds that xi <_ 1/e and AjeN(Ai) for all i. Then there is a randomized algorithm 
that finds outcomes for the random trials in polynomial time (in n + m), w.h.p., such that for every 
event A, the set TA can be partitioned into at most some constant number of subsets $1,... ,Sk so that 
AIs j is false for all j E [k]. We first describe the algorithm underlying Theorem 2. Its analysis will 
be given in Section 2.2. 2.1 Description of the algorithm Consider any set of events that fulfills the 
conditions in The- orem 2. Our algorithm consists of four steps. Step 1: Choose a random outcome for 
every trial in V. Before we describe Step 2, we first introduce some notation and provide the ideas behind 
that step. Let the events in Theorem 2 be called basic events, and let any event defined by a basic event 
restricted to a subset of its trials be called a reduced event. As in Theorem 2, we assume the basic 
events to be numbered consecutively from A1 to Am. Given an event B representing an event Ai or any reduced 
event of Ai, we denote its index by i. A basic event A is called bad if its trial set cannot be decomposed 
into a constant number of subsets S such that A]s is false. Clearly, after Step 1 there might be many 
basic events left that are bad. In this case we have to redo certain trials covered by these events. 
The aim of Step 2.1 is to find a partition of the bad events into trial-disjoint groups. The key feature 
of our partitioning procedure is that we do not consider all the trials covered by the bad events. Instead, 
in the course of the algorithm we shall sometimes partition the basic events into several reduced events 
that are assigned to different groups. Step 2: Perform the following two substeps Step 2.1 and Step 2.2. 
Step 2.1: set R = V //R denotes the set of remaining trials fori=ltomdo: if AilTAInR is true then call 
Build_l-Component(A/ITAI NR)  Algorithm Build_l-Component (B): set i = 0 and E0 = {B} set R= R\TB repeat 
 set Ei+l = 0 set Ci+l = N(EO //Ci+l = all neighb, basic events for all events Aj E Ci+l in increasing 
j do: set S = TA~ N R // S: trials not selected yet if Aj Is is true then //if true, then ISI > ]TAj 
I e set R = R\S and add Aj]s to Ei+l seti=i+l until Ei = 0 Consider some fixed set E* = [Ji>0 El of (possibly 
reduced) events built in the algorithm Bu~d_l-Component. A basic event A is said to participate in it 
if more than ITAI ~ of its trials are covered by E*. (This clearly holds for those events that have a 
reduced event in E* .) The set of all participating events restricted to the trials covered by E* is 
called a 1-component. For every 1-component C, let Bc = E* denote its set of core events. Core events 
play an important role in the reselection process because, as we shall show, it is sufficient to reselect 
only the trials covered by the core events in order to find a good selection for all (possibly reduced) 
events in C. After performing Step 2.1, one might think that the 1-compo- nents can be solved independently. 
However, it could hap- pen that some basic event is partitioned into too many re- duced events that belong 
to different components. Theo-rem 2 requires for every event A to provide a decomposition of TA into 
a constant number of sets S such that AIs is false. Therefore, without a coordination among these components 
Theorem 2 might not hold at the end. Step 2.2 takes care of these events by combining components into 
one component if necessary. As we will see, the trials covered by the compo- nents constructed in Step 
2.2 can afterwards be reselected independently so that now Theorem 2 is ensured to hold at the end. Define 
g to be the set of all 1-components. Recall that .4 denotes the set of all basic events. The second substep 
works as follows. Step 2.2: set S ---A //S denotes the set of remaining events set R ----C //R denotes 
the set of remaining 1-components for all not yet taken 1-components C in R in increasing index of their 
initial events do: call Build_2-Component (C) Algorithm Build_2-Component (C): set i = 0 and E0 -- Be 
repeat set Ei+l = 0 //D = all neighb, basic events in S set D = UAeE, (N(A) A S) for all events Aj E 
D in increasing j do: set T to the set of trials in A i covered by outside 1-components if IT I > ITA~ 
I E then (.) for all 1-comp. C ~ in R that overlap with TAj: set Ei+l = Ei+l U Be, remove C t from R 
remove Aj from S set i = i + 1 until Ei = 0 In the following, a basic event A is called dangerous if 
condi- tion (.) above holds for it. Let the union of the 1-components built in the algorithm Build_2-Component 
be called a 2-component. For every 2-component C, let Be denote the union of the sets Be, of the 1-components 
C' it consists of and let Vc be the set of trials covered by the events in Be. Every basic event participating 
in 1-components in C is also said to participate in C, but only as a single event (by com- bining the 
reduced events from different 1-components into one event). Remark 1. Let us notice some important properties 
that we shall frequently use in our analysis and which follow im- mediately from our construction: 1. 
For every 2-component C, all events in Be are true. 2. For every 2-component C, the trial sets covered 
by the core events A' E Bc are disjoint and of size more than ITA I ~ of the corresponding basic event 
A. 3. Every basic event has at most one reduced event that is a core event. 4. For every basic event 
A, the part of A not covered by 2-components is false.  For the next step we use the following properties. 
LEMMA 3. 1. For every 2-component C, there are outcomes for the trials in Vc so that all events participating 
in C are false. 2. For every basic event A, A participates in at most one 2-component. 3. For every 
basic event A, the part of A not participating in any 2-component cannot become bad if only trials covered 
by the 2-components are redone.  PROOF. To prove (1), observe that for every 2-component C, every basic 
event participates as at most one (possibly reduced) event in C. Thus, one can easily verify that the 
probability bounds in Theorem 2 together with the LLL imply that there are outcomes for the trials in 
Vc so that all events participating in C are false. For (2), assume that there is a basic event A that 
partic- ipates in at least 2 different 2-components. In this case, A must have been dangerous from the 
viewpoint of every l-component. This, however, would have caused combin- ing all trials of A covered 
by l-components into a single 2-component at (*), contradicting the assumption. In order to prove (3), 
notice that, by Remark 1 (4), the part of an event not covered by 2-components is false. Fur- thermore, 
the part of an event covered by 2-components in which it does not participate is small enough that it 
can be divided into at most two parts that cannot become true. Otherwise, (.) of Step 2.2 would have 
been true. Hence, the part of an event not participating in any 2-component can- not become bad if only 
trials covered by the 2-components are redone. [] Remark 1 and Lemma 3 imply the following vital property. 
COROLLARY 1. The 2-components can be considered in- dependently of each other in order to fulfill Theorem 
2. Now we are ready to present Step 3. Step 3: For every 2-component C, apply independently Step 2 above 
until the 2-components resulting from C are sufficiently small. From Section 2.2 it follows that we can 
identify "sufficiently small" with 2-components covering at most O(ln 1/~lnm) trials. Since by Theorem 
2 every trial only has a set of outcomes that is polylogarithmic in n + m, we can use the following last 
step. Step 4: Find through exhaustive search outcomes for the trials in each 2-component so that all 
of its events are false.  2.2 Analysis of the algorithm The following lemma implies that the overall 
runtime of the algorithm presented above is polynomial in n + m. LEMMA 4. Let e and ~ be chosen as in 
Theorem 2. Then, 1. with probability at least 1 -1/m a for any con'stant (~ > 0 the number of trials 
covered by any P-component after Step 2 is at most O(((~lnm)l/~), and 2. for any 2-component C after 
Step 2 of size O(ln 1/~ m), with probability at least 1 - 1/In a m for any constant (~ > 0 the number 
of trials covered by any 2-component in C after applying Step 2 to C is at most O(((~+l/c). lnln m)l/~). 
 In order to prove the lemma, we intend to bound the ex-pected number of possibilities of choosing sets 
of core events that are able to establish a 2-component. We will do this by counting all possible sets 
of basic events that could repre- sent the core events of a 2-component. There are two main problems 
that have to be solved for the counting: 1. First of all, the counting has to provide an ordering for 
these basic events. The ordering must be able to uniquely determine how the 2-component must be con- 
structed by the algorithm. 2. A unique way of establishing a 2-component still does not uniquely determine 
the core events, since it can happen that there is a core event in this 2-component whose basic event 
has trials that are also covered by other 2-components.  These two problems are the main reasons why 
the analysis used in [9] does not apply here. There, the problem under consideration did not require 
to provide an ordering of the basic events, and a basic event corresponding to a core event cannot have 
trials belonging to two different 2-components. This made the proof significantly easier. In order to 
solve the problems above, we introduce the following structures. The 1-component witness Assume we aredgiven 
a 1-component C with a set of core events Be = Ui=o Ei, where E0 consists of the initial core event of 
C and each Ei with i >_ 1 represents the set of core events added to C in round i of Build_l-Component. 
A graph T = (BT, E) is called a 1-component witness of C if T is a directed tree with the property that 
 the node set BT of T is the set consisting of the basic events of all core events in Be, and  (A, B) 
E E for the basic events A and B if and only if  - there is an i >_ 0 such that, for the core events 
A' and B' of A and B, A' E E~ and B' E Ei+l, and - A is the event of smallest index with a core event 
in Ei that overlaps with B.  The 2-component witness Furthermore, assume we are given a 2-component 
D consist- ing of a set of 1-components So = Ui=od si, where So con- sists of the initial 1-component 
of D and each Si with i _> 1 represents the set of 1-components added to D in round i of Build_2-Component. 
A graph T = (BT,DT,E1,E2) is called a 2-component witness of D if T is a tree of directed edges with 
the property that the node set BT contains the basic events of all core events in D,  the node set 
DT of T contains the basic events of all dangerous events considered for D (see (*) of Build_2- Component), 
which may include some of the basic events in BT,  (A,B) E E1 for the basic events A,B E BT if and only 
if (A,B) is an edge of the 1-component witness of a 1-component in D, and  (A, B) E E2 for the basic 
events A, B E BT U DT if and only if the trial sets of A and B overlap, A and B do not have core events 
in a single 1-component, and one of the two cases holds: 1. A E BT and B E DT and A is the event of small- 
est index among the basic events that had a core event in D before B was discovered to be danger- ous, 
or 2. A E DT and B E BT and B is the event of small- est index in its 1-component, among those inter- 
secting A, that was added to D due to A.  Edge set E1 is called the set of l-component edges, and edge 
set E2 is called the set of 2-component edges. Due to the property that every basic event can only have 
one core event (see Remark 1 (3)), 1- and 2-component wit- nesses are always guaranteed to be trees. 
Obviously, every 2- component has a unique 2-component witness. On the other hand, every 2-component 
witness T implies a 2-component C that is unique concerning 1. its decomposition into 1-components, 
2. the order in which events are added to each of its 1- components in Build_l-Component, 3. the order 
in which its 1-components axe constructed, and 4. the round in which each 1-component is added to C 
in Build_2-Component.  Item (1) follows from the fact that the 1-component wit- nesses in T are separated 
via 2-component edges. Fur-thermore, item (2) holds because Build_l-Component en-sures that the order 
in which new events are added to a 1-component in one round is determined by their indices. Item (3) 
is true, since Step 2.1 ensures that the order in which the 1-components axe constructed is determined 
by the indices of their initial events. Finally, item (4) holds, because Step 2.2 ensures that the initial 
1-component of C is represented by the 1-component witness in T with the ini- tial event of minimum index, 
and the round in which every other 1-component C' is added to C is determined by the number of 1-component 
witnesses that have to be passed in T from the initial 1-component witness to the 1-component witness 
representing C I. However, a 2-component witness may not result in a 2-component with uniquely determined 
trial sets for the core events. The reason for this is that it can happen that a basic event can be a 
core event in one 2-component (say, C) and overlap with the trials of another 2-component (say, C'). 
This can happen if C' is constructed before C. Hence, in this case, a single 2-component witness is not 
able to uniquely specify the trials covered by the core events of a 2- component. Therefore, we also 
introduce 3-component wit- nesses.  The 3-component witness Given a 2-component D, the 3-component witness 
of D is a directed tree T = (BT, DT, El, E2, E3) that is iteratively constructed as follows: Initially, 
T is equal to the 2-component witness of D. As -long as there is some 2-component C not already in T 
that is in the neighborhood of an event in BT, we add a 3-component edge (A,B) to E3 for an arbitrary 
pair of in- tersecting basic events A in T and B having a core event in C, and we add the 2-component 
witness of C to T. Given a 3-component witness T, let VT denote the set of all trials covered by the 
basic events in BT. 3-component witnesses have the following important property. LEMMA 5. Every 3-component 
witness uniquely determi- nes the 2-component of its initial 2-component witness. PROOF. As noted above, 
a 2-component witness already completely specifies a 2-component up to the trial sets cov- ered by its 
core events. Since a 3-component witness provides a complete time or- dering in which events are added 
to the 2-components it is witnessing,  each time a core event is added to a 1-component, all trials 
of its basic event are taken that are not already covered by other 1-components, and  there are no more 
2-components that overlap with events in T,  a 3-component witness ensures the unique determination 
of the trials covered by the core events of the 2-component corresponding to its initial 2-component 
witness (and all other witnessed 2-components). [] This implies the following result. COROLLARY 2. For 
any 2-component C constructed by the algorithm, there is a 3-component witness that uniquely determines 
all core events in C. Corollary 2 implies that if there is no 3-component wit- ness including a given 
set of basic events B, then B can-not form the core of a 2-component. This does significantly help to 
bound the size of 2-components, since instead of counting combinatorial structures based on core events, 
we axe allowed to count combinatorial structures based on basic events, which is much easier. Before 
we do this, we introduce some notation. For every basic event A, let AA = ITA[ ~ be called the order 
of A and any of its reduced events. Recall that the probability of an event A or any of its reduced events 
to be true is at most PA <~ e -;~A (see Theorem 2). The order of a set E of events is defined as ~~AE£ 
)~A. Furthermore, for every event A in a 3-cornponent witness T, we define the witness order (or w-order 
in short) WA of A to be equal to ,~A if A E BT and otherwise to some value of at least A~. (We will leave 
out an exact definition of WA in this extended abstract.) The w-order WT of a 3-component witness T is 
the sum of the w-orders of all of its events in BT U DT. We intend to count the expected number of 3-component 
witnesses T that have some given w-order. Since WT > ABT., which is at least the sum of the orders of 
the core events of the 2-component represented by its initial 2-component witness, it holds: LEMMA 6. 
For any 2-component C, its corresponding 3- component witness T ensures that WT > AB e . Hence, in order 
to bound the expected number of 2-compo- nents of order at least A, it suffices to bound the expected 
number of 3-component witnesses of w-order at least I. This will enable us to prove Lemma 4. In order 
to count 3-component witnesses T, we start with the basic event that represents the initial event of 
the initial 1-component of T. Given any set of basic events, we count the possibilities of forming 1-, 
2-, and 3-component edges from the basic events to neighboring basic events. That is, we count 3-component 
witnesses level-wise. Given a set of basic events forming level i, we call the set of basic events forming 
level i + 1 its witness extension. Recall that the trial sets covered by the core events of a 2-component 
are disjoint. Therefore, the probability of a basic event to be true restricted to the trial set of its 
core event is indepen- dent of other events. This allows us to bound the expected number number of witness 
extensions )r of E of order w to at most e -~/~ e ~/s, using the following two vital claims that follow 
from the conditions of Theorem 2. CLAIM 1. For every event A and every k > 0 it holds [{B EN(A): ABE 
{1,... ,k}}[ <e ~'k~.A~4. CLAIM 2. Every event A fulfills AA > (1/6) 1/~. It can then be shown via standard 
calculations that the ex- pected number of 3-component witnesses T of w-order w is at most e -~/4s. On 
the other hand we know that AEBTUDT AEB T AEB T Since ~-~AeBT [TA[ ~ >-- (~AeST [TA[)~ and ~-~AeBT [TA[ 
> 1/e [VT[, it follows that [VT[ < w T . Together with Lemma 6, these facts can be used to prove Lemma 
4. 3. THE MIP ALGORITHM In this section we present an algorithm that fulfills Theo- rem 1. First we 
give a high-level description of the algo- rithm. Given an arbitrary MIP, we start with finding an optimal 
solution {x*,j : i E [n], j 6 [t?i]} to the LP relaxation of the MIP (i. e., the integrality constraints 
in Definition 1 (3) are removed). The resulting LP optimum y* is clearly a lower bound for y, the optimum 
of the (integral form of the) MIP. Afterwards, our algorithm works in 3 steps. Its structure is similar 
to the algorithm in [21], but the techniques used here are completely different. Step 1 rounds the x*j's 
to multi- ples of some number in f~(1/In m). Step 2, which contains the main novel contribution of our 
paper, rounds the xi,j* 's until all of them axe above some constant 7 < 1 or 0. Step 3 finally rounds 
the x~,j's to values in {0, 1}.  Step h Initial Rounding For every r 6 {1,... , m}, let w~ : maxc a 
.... First, we scale A to a matrix A O) = t(a (1)~r,c j by multiplying row r of A with w71 for all r. 
This ensures that maxc a (1) r~c : 1 for all r. Let the vector y(1) be equal to A(1)x *. For the remaining 
steps we will assume that the approximation vector a fulfills the following property for some 0 < e < 
1 (see Theorem 1): ~ > crT with cr~ = max [(y(1))-1 (y~1))-(1-~)/2] Q Let # : max~ r(6 in rn)/(min[a~, 
are]y(r))]. Since rain[c%, a2]. yO) > 1for every r, # < rClnm]. If# = 1, then we sim- ply use the randomized 
rounding strategy by Raghavan and Thompson [25] to transform w* into an integral vector x 0). _(1)That 
is, for every i ~ [hi we set the variable xi,j to 1 and the other x~,j,-(1) 's to 0 with probability 
x~,j. In this case, pr[(A(1)xO))~ > (1 + a~)y (1)] < e- mini .... 2]y(1)/3 < e-rain[ .... 2]y(1)/3 < 
e-2 Into 1 _ - m e for every row r. Hence, with high probability we obtain an integral solution w (1) 
with (A(1)x(1))~ < (1 + a~)y (1) for all r as proposed in Theorem 1. In case that # > 1, we select a 
matrix R = (ri,~)~e[~i,~[M uniformly at random out of [0, 1) nx'. For every i,j, we set X(1) j-1 * j 
* i,i ----I{k : ri,~ ~ [~=1 xi,t,~t=l xi,~)}l/#" Consider a system of bad events El,... , E,~ with E~ 
being true if and only if (A(1)w(1))r ~ (1+~)y(1). It is quite easy to see that for all i ~ In], the 
probabilities for the x!l)'s~,~ to have values > 0 are negatively correlated, and x~l ) is independent 
of any xl,~)j, with i ¢ i'. Thus, we can apply the Chernoff bounds to ~btain Pr[E~] < e -mini .... 2]y(1)l(3/u) 
< e_21nr n 1 _ -- m 2 for all r. Hence, with high probability none of the bad events occurs. Assume 
that the random experiment has been suc- cessful in avoiding all bad events (otherwise we repeat it). 
Then we replace x* by the m-vector x (1). In order to sim- plify the following calculations, we assume 
in the following as a worst case that, after Step 1, (A(i)x(i))r = (1 +c~i)y(~ 1) for all r. Step 2: 
Intermediate rounding This is the first step that requires the LLL (and the consid- eration of a dependency 
graph that may have already been significantly reduced by the previous step). Before we per- form any 
further rounding, we first transform x (~) into a vector x (~) that has only values in {0, 1/#}. This 
is simply _(1) _(1) many _(2) 's. done by expanding every xi, j > 1/# to #. xi, j xi, j, Let the matrix 
A (1) be expanded to A (2) in a similar way so that (A(2)w(z))~ = (A(~)~c(1))~ for all r ~ [m]. The aim 
of Step 2 is to round the _(2),~ from values in {0, 1/#} to ;hi, j values in {0, ~f} for some constant 
~, < 1. This will be done in several phases, starting with phase 1. Initially, we set z (°) equal to 
x (~) and lzo equal to #. The outcome vector of phase he _> 1 is defined by z (~). The final vector z 
(v) of Step 2 is set to x (3). The task of phase ~o is to round a vector z (~-1) of values in {0, 1/#~-1} 
to a vector z (~) of values in {0, 1/#~} with #v = [P~-I ln#~-l]. This is done as follows. For simplicity, 
set w = z (e-l), x' = z (v), # = #v-l, #' = #~, and A = A (e). The random experiment for applying the 
LLL is to select a vector S = (Si)ie[~] of sets of size #' uniformly at random out of J1 x ... x J,~, 
where Ji contains ! all j with xi,j > 0. Given a vector S, set xi,j = 1/#' for all i ~ In] with j ~ 
Si and the rest to 0. Consider a system of bad events El,.  , E,~ with E~ being true if and only if 
( min[a~, 11) . (Ax)~ . (Ax')~ > \~ + Let the dependency graph G of the E~'s be defined as in Def- inition 
2. Clearly, every event E~ is mutually independent of all events it is not connected with in G. Assume 
that the approximation vector a is chosen such that inequality (1) in Theorem 1 holds. As will be shown 
in the next subsec- tion, in this case Theorem 2 predicts that a vector x' can be provided in polynomial 
time so that for all r, (A~')~ < (l + c" min[a~' 1]) " for some constant c > 0. Step 2 ends with the 
first #~ for which #v+l > ~+(1-~)/2 , (1-~)/2 (i. e., phase ~ is the last phase). In this case, ~ < 
ln#~. In general, for all 0 < 5 < 1 it holds that ifx ~ < lnx then x < e 1/(1-~). Thus, in our case, 
~v < e 2/(~+¢), which is a constant. For all phases ~ of Step 2 it therefore #~-1 2/(1+~) This enables 
us to bound the holds that >_ ~v approximation factor obtained in Step 2. We distinguish between two 
cases. If ~r < 1 for some row r then ~=0 1 + ~/~.(~/(~+~))~ Thus, (A(2)z(v))~ _< (1 + O(a~))y(~ 1) for 
all r. If a~ >_ 1, then we also obtain an approximation ratio of (1 + O(a~)), since 1-Ij(1 + c/~/ln#j) 
is a constant. Applying Theorem 2 to Step 2 Our aim is to show that every phase of Step 2 can be made 
to match the requirements of Theorem 2. Let x, m',/~ and #' be chosen as above, and let A represent the 
matrix A (2). In order to avoid dealing with events of non-uniform weights, we use the following strategy. 
Let v be the minimum value of an ai,j in A. For every k E {0,... , [log vJ } and every row r in A, let 
the set Mr,k contain all c with a~,~ E (2 -(k+l), 2-k]. Ch(k)~ Clearly, IM~,~I < 2k+1# (Ax)r. Let the 
matrix Bk ---- ~ .... j represent A with zeroes at all entries a~,~ with c ~ Mr,k. Furthermore, let Y~,k 
be chosen so that (Bk x)~ ----y~,k. Set Y~ = ~k>o Yr,k. We define the event E~,k to be true if and only 
if (Bkw')r > yr,k + 6min[a~, 1] 'Y~ . ~/2(1-~) kin # Assume that all events E~,~ were false. Then it 
follows that as desired. By our choice of B~ and x', every factor b (k)~,c " (x')c has a value of at 
most (#')-12-k. Thus, according to the Chernoff bounds and since y~,k <_ yr, Pr[Er,k] is at most ( m~ot~., 
11 yr/(a. (#')-12-b  exp --6 \ x/2(~-~)~ l~. <_ exp (-2.2~k/~ ~ min[c~ 2, 1]y~) . Recall that IMr,kl 
<_ 2k+1# yr and, by our assumption, y~ _> (l+a~)y(1) >_ 1. Since a~ < 1 is only allowed for y(1) > 1, 
we obtain for ar < 1 that (~2y~ .> y~ and therefore 2eke 2 ,ek e mm[a~, 1]y~ >_ ~ # Yr >-- IMr,k] ~" 
2-~ Clearly, the same inequality holds for ~ > 1. Thus, we have Pr[E~,k] < e -IMÈ,~l . On the other 
h-and, the mini- mum number of nonzero xijt 1s necessary to obtain a value of6min[~r, 1]/(~¢/2(1-~)k 
ln#).y~ is > 6.2~k# ~ min[~, 1]yr, which is more than IM~,kl ~. Next we show how to transform inequality 
(1) in Theorem 1 into-a condition that matches the requirements of Theo- rem 2. First, we define the 
dependency graph. Suppose that we are dealing with a MIP instance I. Then we define the dependency graph 
G~ to contain the edge ((r, k), (s, g)) if and only if (r, s) E Gi and either r ¢ s or k ~/?. (Here we 
use the fact that Gz may have self-loops.) It is easy to check that G~ covers all dependencies among 
the E~,k events. Let p~, q~, and z~ be defined as in Theorem 1. That is, Pr = e-min[Otr'~2]y(rl)/3 --|n~: 
p~-I , qr = Pr , and zr : qr  Set P~,k = exp(-2 ~k+l mining, 1lye). Since we assumed at the end of Step 
1 that y, _> (1 +a~)y (~), it holds that PT,k _< 2 e k e --I . Let qr,k = e-in p, k , and z~,k = qSr, 
k. Then we have Pr , r, 2e2~ qr,k e-m~P-~ < e-~2~2k m~p:l --=q~ (2) Since z~ _< 1/3 by Theorem 1, 
~k>oz~ ~2k <- e-2z~/( 1 -z~) < 3/(2ee).z~ for all r. Thus, 1-I e .... ~ > e -3/~2~b'~ > (1 -z~) 3/~2~b 
. k>_0 On the other hand, (1 - z~,~) > e-(4/3)z-, k for all z~,k since e -(4/3)* < 1 - 4x/3 + 8x2/9 
< 1 - x for all x < 1/3. Hence, l-Ik>0(1 - z,,k) _> (1 - z~) 2/~2. Furthermore, due to (2) it holds for 
all (i, k) that qr,k 1-J 1-5 qr = q~,k <-- q~ = -- Zr,k Zr Thus, we obtain from inequality (1) in Theorem 
1 that < l-I (1 - ((~,~),(~,e))~G~ Next, we define the trials and events used in Theorem 2. The x~j's 
represent the trials tl,... , tn, and the E~,~'s rep- resent the events A1,... , Am. Obviously, every 
event E~,~ is determined by the outcome of the trials represented by Mr,k. Furthermore, let Tr,k represent 
the set of all xij' 's that have some x'i, j, counting for E~,~. As shown above, Pr,k ~ e -[MÈ'~I~. If 
a restricted set S of x~,j's is applied to an event E~,k, then we define E~,~[s to be true if and only 
if (S~.a:')~[s >y~,~[s+ 6rain[at, 11 "Y~. ~/2 (1-~)k ln# This ensures that, under the assumption that 
an event E~,~ can be decomposed into at most some constant number of events ET,k[s that are false (as 
claimed by Theorem 2), we obtain the desired asymptotic approximation ratio. We al- ready showed that 
if IS[ > [M~,k[ ~ then the above definition yields Pr[E~,~[s] _< pr,~. Otherwise, we can set Pr[E~,~[s] 
= 0, since more than [M~,~[ ~ trials are needed to violate our deviation hound. There is one aspect of 
our application that does not directly match the requirements of Theorem 2. We demand that if an event 
Er,kls is found bad in the LLL algorithm, then all of the x~,j's that have some x'i, j, counting for 
ET,kls have to be added to the 1-component. Hence, with regard to the in- sertion of an event, the set 
of trials that has to be considered is Tr,k instead of MT,k. This changes the number of trials covered 
by a 2-component C from at most ~](~,k)esc IMr,kl to at most ~--](~,k)~Bc ITr,kl ~ /-t ~(r,k)EBc IM~,kl 
 Thus, the size of the final 2-components changes to O(p( [ In In m)1/~). If # > x/]-og m, it may not 
be possible any more to find a final solution via exhaustive search. However, every event participating 
in this 2-component has a probability of at most pr,k _< e-" to be true. Furthermore, it follows from 
Claim 1 that a 2-component of q nodes can have at most q e 5"k~ events of order at most k, and each 
of these events has a probability of at most e -k to be true. Hence, an ex- pected constant number of 
random experiments suffice for each 2-component to obtain a final solution. Thus, Step 2 of the MIP algorithm 
can be done in polynomial time.   Step 3: Final rounding Recall that X (3) is the vector resulting 
from x (1) in Step 2. Let Bk, Y~,k, and y~ be defined as in the previous step. Step 3 works like a phase 
in Step 2 with the difference that now the random experiment simply consists of rounding x (3) to an 
integral vector x in a way that for every i, xi,j is set to 1 (and the other xl,j,'s to 0) with probability 
x~3). Assuming that yr _> (1 + ar)y (1) for all r it holds that Pr (Bka:)~ _> Yr,k + X/2(1_~) k In#~ 
"y~ < exp (--2-"~k #~ min[a 2, 1lye) _ z ifc _> 6#~ ln#v, which is a constant. Hence, as in Step 2, 
we can apply Theorem 2 to obtain altogether an approximation ratio of 1 + O(~) for every row r. This 
concludes the proof of Theorem 1. 4. APPLICATIONS In this section we present two applications of our 
method: job shop scheduling and MAX SAT. 4.1 Job Shop Scheduling In acyclic job shop scheduling problems 
there are n jobs and m machines. Each job is composed of a sequence of opera- tions to be performed on 
different machines. A legal schedule is one in which within each job, operations are carried out in order, 
and each machine performs at most one operation in any unit of time. If D denotes the length of the longest 
job and C denotes the number of time units requested by all jobs on the most loaded machine, then clearly 
lb = max[C, D] is a lower bound on the length of the shortest legal schedule. Let P denote the longest 
operation a job has on a machine. The best constructive upper bounds known for acyclic JSS are L = O(lb(log 
lb)2 / log log lb) by Shmoys, Stein and Wein [28], later improved by a log log lb factor by Goldberg, 
Pa- terson, Srinivasan and Sweedyk [16]. In [12] several non- constructive results have been shown about 
the makespan of job shop schedules. Our techniques constructivize these results with only a small loss 
(instead of 1, we obtain a 1 +e in the exponent of some terms). In particular, we will show that the 
following result holds. THEOREM 3. In polynomial time, acyclic job shop sched- ules can be computed with 
the following upper bounds on the makespan for any constant e > O: 1. For any acyelic JSS problem, [ 
(C+D(log log p)lTe) log P L = O \ log(min[C/m+(log log p)l+e,p])] , implying that L = O(Ib log lb(loglog 
Ib)l +~). Observe that if C > D . P6 for some constant 5 > O, then L = O(C). 2. If operation lengths 
depend only on the machine on which the operation is performed, then L = O(C + D(log log p)1+¢). 3. With 
preemption, L = O(C + D(loglogp)l+¢). In order to prove Theorem 3, we have to constructivize the existence 
proof given for an intermediate schedule in [12]. We do this with the help of Theorem 1 by refining subsched- 
ules of length a multiple of d = log 1/~ D instead of log D. The rest of the proof is omitted in this 
extended abstract. 4.2 Satisfiability Many approaches have already been presented that pro-vide good 
approximation algorithms for MAX SAT prob- lems. This was pioneered by Johnson [18] and Raghavan and 
Thompson [25] and further improved by Yannakakis [31], Goemans and Williamson [14; 15] and Asano et al. 
[5; 6]. Many of these approaches use at the end a simple random- ized rounding strategy. Our approach 
may help to improve this last step: Consider any boolean formula in CNF. We will give an ex- ample of 
how to express the MAX SAT problem for this formula as a reverse form of a MIP, called maximin integer 
program (MIP), that is defined as follows. Definition 3. A MIP has variables {xi,j : i [n], j [li]}, 
for some integers gi. Let N = ~-~e[~] ~i and let x denote the N-dimensional vector of the variables xi,j. 
A MIP seeks to maximize a real Y subject to: 1. a system of linear inequalities Ax > y, where A [0, 
1] m×N and y is the m-dimensional vector with the variable Y in each component, 2. for all i [n]: ~jc[~] 
xij = 1, and 3. for all i and j: xi,j {0, 1}.  Using the same dependency graph as for MIPs, the following 
result holds for MIPs. THEOREM 4. We are given any MIP instance I with an optimal fractional solution 
x*. Consider the random exper-iment of setting xi,j to 1 with probability x*,j (and in this case all 
other xi,j, 's to 0). Given any vector oL [0, 1] m, let 2 i pr -'-e -a'y~/2 ' * a Chernoff for allr 
 [m], where y~ = y~/(max.... ). (p~ is a bound for Pr[(Aw)~ _< (1 -a~)y;].) Let 0 < e < 1 be a constant 
that is chosen such that a~ >_min[1, (y~)-O-~)/2] for every r. Furthermore, let q~ = e -l~pT~ and let 
z~ = q~ be at most 1/3 for every r, where 5 is a suj~ciently small positive constant. If it holds that 
(r,s)EGz for all r, then there is an algorithm that finds a vector x in polynomial time so that (Ax)~ 
>_(1 - O(a~))y~ for all r. PROOF. The proof of the theorem is almost identical to the proof of Theorem 
1. Since the random variables con- sidered for the Chernoff bounds in Section 3 are weakly negatively 
correlated, techniques based on [27] have to be applied. Furthermore, we have to change to the rule that 
if at most IMp,k[ ~ many xi,j's in some set S are set to 0 (in- stead of 1/#' for MIPs), then it does 
not affect the deviation bound for E~,kls. [] An instance of MAX SAT is defined by (C,w), where g is 
a set of m boolean clauses such that each clause C E g is a disjunction of literals with a positive weight 
w(C). Let X = {xl,... ,xn} be the set of boolean variables in the clauses of C. For every i, x~ --- 1 
means xi is true and xi = 0 means x~ is false. A literal is a variable x E X or its negation = 1-x. For 
each xi, we define x~,o = 2i andxi,1 =--xi. We assume that no literals with the same variable appear 
more than once in a clause in g. Clause j in g is denoted by Cj ~-xij,l,lj, 1 V xij,2,1j, 2 V . . VXlj,kj,~j,kj 
 We would like to find an optimal solution to the following integer program: Maximize ~j~=l wjyj subject 
to kj E xi~,s,lj, > yj Vj (4) x~,~, y~ e {0, 1} (5) If we replace the conditions in' (5) by 0 <: xi,e,yj 
~_ 1, then an optimal solution x* of the LP can be found in polyno- mial time. In order to express this 
solution as a MIP, the variables xi,/ are the same as above, and every inequality in (4) is represented 
as a row in A with the property that (Ax*)j : 1 for every j. Obviously, the best possible Y that can 
be obtained for the LP relaxation of the MIP is at least 1, and the value of the objective function in 
this case is the same as for x*. Using our randomized rounding technique, our aim is to find integral 
xi,e's so that for some vector 04 (Ax)j ~ (1 - ou)Y for all j. If (~ is chosen such that the probabilities 
for this satisfy the conditions in Theorem 4, our techniques can be used to exploit non-uniform properties 
of the MIP (and therefore the corresponding MAX SAT problem) to find good approximate solutions in polynomial 
time. 5. CONCLUSIONS In this paper, we presented a powerful technique to exploit non-uniform properties 
in MIPs in order to find good ap- proximate solutions. We applied this technique to job shop scheduling 
and MAX SAT. The upper bounds we obtained for the job shop scheduling problems significantly improve 
previously known upper bounds. We expect our techniques to have many more applications to other types 
of integer programs and combinatorial optimization problems.  6. REFERENCES <RefA>[1] N. Alon. A parallel 
algorithmic version of the local lemma. Random Structures and Algorithms, 2(4):367-387, 1991. [2] N. 
Alon, A. Bar-Noy, N. Linial, and D. Peleg. A lower bound for radio broadcast. Journal on Computer and 
System Science, 43(2):290-298, 1991. [3] N. Alon, C. McDiarmid, and B. Reed. Acyclic col- oring of graphs. 
Random Structures and Algorithms, 2(3):277-288, 1991. [4] N. Alon, J. Spencer, and P. Erd6s. The Probabilistic 
Method. John Wiley &#38; Sons, New York, 1992. [5] T. Asano, K. Hori, T. Ono, and T. Hirata. A theoret- 
ical framework of hybrid approaches to MAX SAT. In Proc. of the 8th Symp. on Algorithms and Computation 
(ISAAC), pages 153-162, 1997. [6] T. Asano and D. Williamson. Improved approximation algorithms for MAX 
SAT. In Proc. of the 11th ACM Syrup. on Discrete Algorithms (SODA), pages 96-105, 2000. [7] J. Beck. 
An algorithmic approach to the Lov~sz local lemma. Random Structures and Algorithms, 2(4):343-365, 1991. 
[8] A. Z. Broder, A. M. Frieze, and E. Upfal. Static and dynamic path selection on expander graphs: A 
random walk approach. In Proc. of the 29th ACM Symp. on Theory of Computing (STOC), pages 531-539, 1997. 
[9] A. Czumaj and C. Scheideler. Coloring non-uniform hy- pergraphs: A new algorithmic approach to the 
general Lov£sz local lemma. In Proc. of the 11th ACM Symp. on Discrete Algorithms (SODA), pages 30-39, 
2000. [10] P. Erd6s and L. Lov~sz. Problems and results on 3-chromatic hypergraphs and some related 
questions. In A. Hajnal, R. Rado, and V. S6s, editors, Infinite and Finite Sets (to Paul Erd~s on his 
60th birthday), pages 609-627. North-Holland, Amsterdam, 1975.  [11] P. Erd6s and J. Spencer. Lopsided 
Lov£sz local lemma and latin transversals. Discrete Applied Mathematics, 30:151-154, 1991. [12] U. Feige 
and C. Scheideler. Improved bounds for acyclic job shop scheduling. In Proc. of the 30th A CM Syrup. 
on Theory of Computing (STOC), pages 624-633, 1998. [13] Z. Ffiredi and J. Kahn. On the dimensions of 
ordered sets of bounded degree. Order, 3:15-20, 1986. [14] M. Goemans and D. Williamson. New 3/4-approximation 
algorithms for the maximum sat-isfiability problem. SIAM Journal on Computing, 7:656-666, 1994. [15] 
M. Goemans and D. Williamson. Improved approxi- mation algorithms for maximum cut and satisfiability 
problems using semidefinite programming. Journal of the ACM, 42:1115-1145, 1995. [16] L. A. Goldberg, 
M. Paterson, A. Srinivasan, and E. Sweedyk. Better approximation guarantees for job- shop scheduling. 
In Proc. of the 8th ACM Symp. on Discrete Algorithms (SODA), pages 599-608, 1997.  [17] H. Hind, M. 
Molloy, and B. Reed. Colouring a graph frugally. Combinatorica, 17(4):469-482, 1997. [18] D. Johnson. 
Approximation algorithms for combinato- rial problems. Journal on Computer and System Sci- ence, 9:256-278, 
1974. [19] F. T. Leighton, B. M. Maggs, and S. B. Rao. Packet routing and job-shop scheduling in O(congestion 
+ di- lation) steps. Combinatorica, 14(2):167-186, 1994. [20] F. T. Leighton, B. M. Maggs, and A. W. 
Richa. Fast algorithms for finding o(congestion + dilation) packet routing schedules. Technical report, 
CMU-CS-96-152, School of Computer Science, Carnegie Mellon Univer- sity, Pittsburgh, PA, USA, 1996. [21] 
T. Leighton, S. Rao, and A. Srinivasan. New algorith- mic aspects of the Local Lemma with applications 
to routing and partitioning. In Proc. of the lOth ACM Symp. on Discrete Algorithms (SODA), pages 643-652, 
1999. [22] C.-J. Lu. A deterministic approximation algorithm for a minmax integer programming problem. 
In Proc. of the lOth ACM Symp. on Discrete Algorithms (SODA), pages 663-668, 1999. [23] M. Molloy and 
B. Reed. Further algorithmic aspects of the local lemma. In Proc. of the 30th ACM Symp. on Theory of 
Computing (STOC), pages 524-529, 1998. [24] J. Radhakrishnan and A. Srinivasan. Improved bounds and algorithms 
for hypergraph two-coloring. In Proc. of the 39th IEEE Symp. on Foundations of Computer Sci- ence (FOCS), 
pages 684-693, 1998. [25] P. Raghavan and C. Thompson. Randomized rounding: A technique for provably 
good algorithms and algorith- mic proofs. Combinatorica, 7:365-374, 1987. [26] B. Reed. w, A, and X. 
Journal of Graph Theory, 27(4):177-212, 1998. [27] J. Schmidt, A. Siegel, and A. Srinivasan. Chernoff-Hoeffding 
bounds for applications with limited inde- pendence. SIAM Journal on Discrete Mathematics, 8(2):223-250, 
1995. [28] D. Shmoys, C. Stein, and J. Wein. Improved approxi- mation algorithms for shop scheduling 
problems. SIAM Journal on Computing, 23(3):617-632, 1994. [29] J. Spencer. Probabilistic methods. In 
R. Graham, M. GrStschel, and L. Lov~ksz, editors, Handbook of Com- binatorics, chapter 33. Elsevier Science, 
Amsterdam, 1995. [30] A. Srinivasan. An extension of the Lov~sz local lemma, and its applications to 
integer programming. In Proc. of the 7th ACM Symp. on Discrete Algorithms (SODA), pages 6-15, 1996. 
[31] M. Yannakakis. On the approximation of maximum sat- isfiability. Journal of Algorithms, 17:475-502, 
1994.    </RefA>
			
