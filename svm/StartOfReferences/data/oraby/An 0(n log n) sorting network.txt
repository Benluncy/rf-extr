
 An O(n log n) Sorting [letwork t I . ~i. Ajtai, J. Komlos, E. Szemeredl Mathematical Institute of the 
Hunnarian Academy of Science, Dudapest University of California, San Diego University of South Carolina, 
Columbia Abstract The purpose of this paper is to describe a sorting network of size'O(nlogn) and depth 
O(log n). A natural way of sorting is through consecu-tive halvings: determine the upper and lower halves 
of the set, proceed similarly within the halves, and so on. Unfortunately, while one can halve a set 
using only O(n) comparisons, this can-not be done in less than Iogn (parallel) time, and it is known 
that a halving network needs (I/2)nlogn comparisons. It is possible, however, to construct a network 
of O(n) comparisons which halves in constant time with high accuracy. This procedure (~-halving) and 
a derived procedure (c-nearsort) are described below, and our sortin 9 network will be centered around 
these elementary steps. Introduction The network described in the abstract is a comparator network, i.e. 
a deterministic sequence of O(nlogn) switches, where a switch is a pair (i,j), l~i<j~n, and operates 
as follows: it compares the actual contents of the i th and jth registers, and (by switching if necessa~,) 
puts the larger one into the jth register and the Supported in part by the National Science Foundation 
under grant MCS-7906228. Permission to copy without fee all or part of this material is granted provided 
that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice 
and the title of the publication and its date appear, and notice is given that copying is by permission 
of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or 
specific permission. &#38;#169; 1983 ACM 0-89791-099-0/83/004/0001 $00.75 smaller one into the i th register. 
At the end of th the algorithm, the i smallest element is in the i th register for all i:I,2 ..... n; 
the elements are sorted. The algorithm works in O(loan) delay time (parallel time). We should mention, 
however, that the constant involved in 0(.) is very large and is determined by the size of certain expander 
graphs. This makes the algorithm unsuitable for actual implementation and much slower than, for example, 
Batcher's algorithm for "small" values of n. (The constants can drastically be reduced by generating 
random expander graphs, rather than usin~ the kno~m constructions.) rlo atte~ipt is made to get best 
possible constants. An earlier version of this algorithm has been submitted to Combinatorica. Although 
the main ideas are all contained in that paper, the two formal descriptions are so much different that 
we think it is ~vorth presenting this second version. We start with a short, informal description of 
our algorithm. This ~vas provided by Joel Spencer, and we are extremely grateful to him for it. It prompted 
us to vmite this paper without further delay. Spencer's outline will be followed by a brief review of 
expander ~raphs. The remainder of the paper will fill in the details of the informal description. Sections: 
I. Spencer's description 2. Expander graphs 3. c-halving 4. E-nearsort 5. Register assignment strategy 
 6. The partitions 7. A too formal description 8. The proof  I. Spencer's description There are three 
parts: e-halving, e-nearsort, and The Algorithm. Fortunately for each part we need only the results of 
the previous parts. e-halving. Here e is fixed, say e=lO -9. In fixed finite time (one time equals n/2 
comparisons) an e-halver on m registers will put the lower half ef the numbers in the lower half of the 
registers with at most em errors and for all k, l~k~m/2 the first k numbers are in the lower half of 
the registers with at most ek errors (and similarly for the top k numbers). These e-halvers are intimately 
connected to Expander Graphs, etc. As I think of it, each time unit takes a random l-factor between the 
LH and the UH of the registers and makes a probabilistic computation. e-nearsort. Here E is fixed, say 
e=lO -6 . In fixed finite time an e-nearsort will put contents l,...,m into registers l,...,m so that 
for any interval I of registers the proper numbers are in I (though not necessarily in order) with at 
most em mistakes and for all k, l <k<em the first k numbers are in the lower em registers with at most 
ek mistakes. To construct a nearsort apply an e-halver (e =lO -15) to the whole set of registers, then 
apply e-halvers to the top and bottom half of registers, then to each quarter, eighth, sixteenth, until 
the pieces have size mlO -9. One can show quickly that each piece of size w =mlO -9 has at most ew errors. 
The fringes are small since lO -6%>I0 -9 and this part of the argument is not too hard.,. The Algorithm. 
In one sense the algorithm is simple to describe. The n registers are bolted down and numbered l through 
n. For each i, 0< i < 31og n, a partition of the registers is given. These partitions are explicitly 
given and are completely independent of the contents of the registers. For a given i we perform an e-nearsort 
separately (and simultaneously) on each th set in the i partition. To descri be the partitions a binary 
tree format is used. Consider a binary planar tree. At time t we distribute the registers into the nodes 
as follows. No registers are at a level deeper than t. To each node there corresponds a natural interval 
of registers --for the root all of them, for the marked node (n/4+l, 2n/4). (This interval is interpreted 
as an estimate for the contents of registers assigned to the node.) A node at level t first appears at 
time t and then it has the whole natural interval except that already taken by nodes above it. (In one 
sense the higher nodes get higher priority.) A node at level t at time t+i will have two intervals of 
registers --on each edge A -i of its natural interval. (It will be as near to the edge as pos-sible but 
the higher nodes have priority.) (A=lO0 is an absolute constant. A << I/e.) Here is a triangle of nodes 
and the registers they represent. Each Y and Z interval is A =lO0 times as long as the X intervals (and 
the gap between the intervals is lO0 times as long as the intervals). W,. Y ". ; ~f ; Z : ~ Z iXi At 
time t we have placed (mentally) the registers in this binary tree. Partition the tree into triangles 
with the apexes of the nodes at even level. This is a partition of the registers, apply an E-nearsort 
to every triangle of registers. That is the Zig step. Now partition the tree into triangles with the 
apexes of the nodes at odd level. Apply the e-nearsort to each triangle. That is the Zag step, Go ZigZagZig. 
(We call the entire ZigZagZig step one time unit.) o ZIG ZAG Then time changes to t+l. We (mentally) 
move the registers about in the binary tree. (I think of that as the Bookkeeping step, but note that 
it takes no time and involves no comparisons.) Then we apply the next ZigZagZig step. We do this Iogn 
times. (We may think of all of the registers lying in the root at time 0o As time continues they filter 
down to the bottom of the tree, but if a register's position is near crn2 -s it will get held up at level 
s as an extreme register.) Now it is claimed that after this procedure the numbers have been sorted. 
To prove that this algorithm works an appro- priate induction hypothesis is required. At a given time 
a register R with content x lies on a node N. We define the wrongness w(R). For any node N there corresponds 
a natural interval of registers and hence of numbers I(N). If x ~ I(N) then w(R)=O, the register R has 
an appropriate number. Otherwise look at the node N l directly above N with interval I(Nl) double that 
of I(N). If x ~ I(N l) then w(R) =l. Otherwise, w(R) is the least w so that x ~ I(N w) where N w is the 
node directly w steps above N. Since the root node N has I(N) equal all numbers this is well defined. 
For example, let R be a register in the node marked in the diagram in the previous page. Let x be its 
content . (We may assume the contents are also l ..... n.) If O<x<n/4 then w(R) =l, if n/4<x<n/2 then 
w(R)=O, and if n/2<x<n then w(R)=2 since one must go to the root before having x correctly placed. Now 
for the induction hypothesis. Look at any node at any time and let r be any positive integer. The fraction 
of registers on that node with wrongness > r is at most cA -3r . At the end the only nodes with registers 
are at the bottom and contain only one register, and so none of the registers have any wrongness and 
so we would be done. The induction is on time t. A time unit consists of two parts. In the bookkeeping 
step wrongness is increased. The key step at time t is when at level t a node splits and sends its left 
and right sides to level t+l, saving the extremes for itself. The node had already had an e-nearsort 
applied to it but now a fraction e of registers that were correct (i.e. wrongness =0) have wrongness 
I. Also, all registers that had wrongness r>l now have wrongness r+l. This is natural since in the bookkeeping 
step we are claiming further refinement of the numbers and so wrongness will creep in. In the ZigZagZig 
step wrongness is decreased. (Actually --lets make it ZigZagZigZagZigZag since we are not worried about 
the constant.) Look at the XYZ triangle and con- sider the registers in Y with wrongness. If wrongness:l, 
they should be in Z and those numbers will move into registers in X or Z and decrease wrongness. If wrongness>l, 
say they should be less than I(X), then those elements form an initial segment, and thus E-nearsort will 
put almost all of them into X (being on the extreme left of the X-Y-Z picture), decreasing wrongness 
by I. If, say, wrongness is large then the number will be pushed up the tree by the alternate ZigZagZig 
step --X will be part of a bigger triangle A-X-V and it will go to apex A, etc. Note that in the X-Y-Z 
picture the leftmost X is only .01 of the picture but that is plenty of room since the errors are all 
on the order of one in a million. 2. Expander graphs Notation. Given a graph G= (V,E) and a set A c V 
of vertices, we write F(A) for the set of neighbors, i.e. F(A) = {v ~ V I (v,a) ~ E for some a E A} Given 
two sets VI,V2,1VII : IV21 =n, a bipar- tite graph G with vertex-sets VI,V 2 is called an empa~er graph 
with parameters (~,m,c) if for any set AcV I, IAl~n we have IF(A)I ~hA and the same holds for subsets 
of V 2, and yet the maximum valency in G does not exceed c (and hence G has only a linear number of edges). 
The above notion is slightly different from the usual one. The following theorem is the result of works 
by several authors: Theorem. For any X and ~, X>l, ~>0, X~<l, there is a c = c(X,~) such that there are 
expander graphs with parameters (X,a,c) for all n. Remark. The condition IAI= IBI in the definition was 
not essential, we will also use expanders with IKI= IBl+l. Comments. The first result on the existence 
of expanders is due to Pinsker (1973). The first explicit constructions were made by Margoulis (1973); 
and though the graphs themselves had a very simple structure, the proof of their expanding property involved 
group-representation theory. After this breakthrough, Gabber and Galil (1979) contributed by giving a 
more direct proof (using Fourier analysis), which led to effective estimates on the extent iX) of expanding. 
(The graph involved in the Gabber-Galil result --a modification of that of Margoulis --is worth mentioning 
for its simplicity: V 1 =V2= {(i,j), l~i,j~m} and there are (at most) five edges going out from each 
vertex(i,j) (in V l, say), namely to (i,j), (i,i+j), (i+j,j), (i,i+j+l), (i+j+l,j) (in V2), where addition 
is mod m. To understand why Margoulis used linear transformations on the torus rather than on the real 
line, read Klawe (1981). She proves that one-dimensional linear transformations (used for generating 
pseudo-random numbers) cannot work.) For more on expanders and superconcentrators read Valiant (1975), 
Paul-Tarjan-Celeni (1976), Pippinger (1977), Angluin (1979). The known constructions lead to expanders 
with arbitrary large X by forming high powers of the graph. The price, however, is high too. The above 
mentioned construction leads to an expander with X =2 (and ~=I/3, say) only with c of the order of magnitude 
2 lO0. Counting arguments show, however, that there is an expander with parameters (2,1/3,8). The astronomical 
gap 8-2 lO0 makes it questionable whether it is worth working with explicit constructions. (Simple counting 
shows that the graph consisting of c randomly chosen one-factors is, with high prob- ability, an expander 
with parameters (X,I/(X+l),c) if only C> 2[(~+I) log(X÷l)-XlogX] (X-l) log(X-l)+(X+l) log(X+l!-2XlogX 
= 2[XlnX+~+I/2+o(1)] while the above constructions give something like xIO0 Note that generating a random 
one-factor simply needs generating a random permutation.) 3. c-halving A permutation ~= (~I,~2 ..... 
~m) of (1,2 ..... m) is said to be E-/~:Ived if for any initial se~nent S = (l ..... k), k~Lm/2], ~i> 
Lm/2] occurs for at most clS I numbers i~S, and for any e~se~ment S = (k ..... m), k > Lm/2J, ~i~/m/2] 
occurs for at most ~Isl numbers i~S. (In other words, the two halves have very few errors and they are 
mostly concentrated "in the middle.") A procedure is called an ¢-~lver (of m elements) if it accepts 
permutations of l,...,m as inputs, and produces ~-halved permutations of l, .... m as outputs. We construct 
now a bounded depth E-~Iver ne~ork. Fit an even (bipartite) expander graph with parameters ((I-E)/E,~,C) 
to the register sets A =(l ..... Lm/2]), B=(Lm/2]+l .... ~m). Split the graph into c one-factors FI,...,F 
c. Interpreting the edges of a one-factor as switches (performing comparisons along the edges and always 
putting the smaller element to A), we obtain a network of depth c. We claim it is an E-halver. Note first 
that a comparator network defined by any bipartite graph has the property that if R l E A and R 2 c B 
are connected by an edge then the content of R l is less than that of R 2. Indeed, there was a stage 
(when we have just compared R l and R2) when this was true, and the content of R l decreases that of 
R 2 increases in time. Assume now that for some k~Lm/2], more than Ek of the elements of S = (l ..... 
k) end up in registers of B. Since more than ~k vertices in B are connected to at least (l-~)k vertices 
in A, we must have a pair of registers R l ~A, R 2~B, such that R 2 holds an element of S, R l holds 
an element outside S, and R l and R 2 are connected by an edge. This, however, contradicts the above 
mentioned property, since any element of S is less than any element outside S (initial segment). 4. c-nearsort 
Notation. Given a permutation = (~I,~2 ..... ~m) of (1,2 ..... m) and a subset S of [m] = {1,2 ..... 
m}, we write ~S ={~i I i~S} Given ~> O,S ~ denotes the blown-up set S~= {j ~ [m] I lJ-il~cm for some 
i c S} In particular, if S c M is a segment (a,a+l ..... b) then S c is the segment {a',a'+l ..... b'}, 
where a'= max(l,Fa-cml), b'= min(m, Lb+Em]) We say that a permutation ~= (~l ..... ~m) of (I ..... m) 
is ~-nearsorted if (i) Is-~s~l < ~Isl holds for all initial aegments S = (I ..... k) and e~se6~nents 
S : (k ..... m), (I < k < m). A procedure is called an c-ne~sort (of m elements) if it accepts permutations 
of 1 ..... m as inputs, and produces E-nearsorted permutations of l,...,m as outputs. Remarks. (i) clearly 
implies that (ii) Is-=s~l < 3cm holds for al~ segments S = (a, .... b). Note that for "middle" segments 
we only have an absolute bound 3em, while for initial (and end) segments (i) provides a relative bound, 
meaningful even for very short segments. This will enable us to obtain, by repeated applications of e-nearsort, 
an exponential decay of errors. E-nearsort and e-halving will usually be applied to a sequence al,a2,...,am, 
where a I ..... a m is a permutation of an ordered string e l<e 2< ...<e m. In this case we think of 
the elements e I ..... e m as identified by 1 ..... m. To construct an ~-nearsort network, apply an Cl-halver 
(network), with ~l<lo~' to the whole set of registers, then apply ~l-halvers to the top and bottom half 
of registers, then to each quarter, eighth, sixteenth, until the pieces have size w<Em. It is easy to 
see that the obtained network of bounded depth is an ~-nearsort. 5. Register assignment strategy We are 
going to use a paramenter A>IO, whose value will be discussed in the proof section. All logarithms are 
to the base 2, and we assume that n and A are powers of 2. Define the following numbers i Xt(i ) = Lcn2-tAi-tj, 
gt(i)=~"~ Xt(J) , j=l t=l,2 .... ,Iogn; i=l,2,...,t, where c=I/(2A), and we interpret Yt(i) as zero for 
i < O. At time t, the registers assigned to the jth node on the i th level, O<i<t-l, l<j<2 i, form two 
intervals Ji,J2 Jl = (J-l)n2-i+[Yt (i)+l' Yt (i+l)] and J2 " (J-1)n2-i+[n2-i-Yt (i+l)+l'n2-i] if j is 
odd Jl = (J-l)n2-i+[l' Yt (i+l)] and J2 : (J'l)n2-i+[n2-i-Yt(i+l)+l'n2-i-Yt(i)] if j is even. The registers 
assigned to the jth node on the t th level, l<j<2 t, form a single interval J, J = (j-l)n2-t+[Yt(t)+l,n2-t] 
if j is odd J = (j-l)n2-t+[l,n2-t-Yt(t)] if j is even Dynamics. As time increases from t to t+l, we reassign 
the registers to nodes. Analyzing the above formulas one sees that about I/A proportion of some intervals 
remain unchanged, but most registers of an interval move down one or two levels (about half-half). No 
register moves up, and A> 2 ensures that no register moves down more than two levels (Yt+l(i+3)>Yt(i+l)). 
These facts will be used in the proof. After Iogn/log(2A) steps the tree splits into two trees, and then 
disintegrates to small trees very fast. Since there is no movement of elements between the disjoint trees, 
by that time the corresponding intervals of elements should be (and will be) properly ordered among each 
other. 6. The partitions Although 3t+l partitions would suffice (three in each time-cycle m called ZigZagZig 
in Section l), in order to simplify the proof we define the algorithm using 2at+l partitions (ZigZag 
is repeated a times), where a<15. The 0 th partition consists of only one set [n] = {l ..... n}. For 
t>l, define the partitions Pti' l<i<2a, by using the register assignments at time t. The basic part --called 
cherry --is the set of registers assigned to a node and its two (possibly empty) children-nodes. Ptl 
consists of these parts for parent-nodes on even levels of the tree. Pt2 has parts with parents on odd 
levels, Pti is identical to Ptl or Pt2 according to the parity of i. (When the parent-nodes are on odd 
levels, the nodes assigned to the root, which has no parent, form a part themselves.) Since the registers 
assigned to a particular node form at most two intervals, the above parts consist of at most six intervals, 
but they actually melt together to at most three. Thus, this partition can be described explicitly, with-out 
referring to the tree structure, as it will be done in the next section. The tree structure shows, however, 
why the algorithm works: The two intervals of a parent-node frame the register-sets of the two children 
nodes. When applying ~-nearsort to these registers (sitting on parent and two children), the contents 
that are foreign elements m too large or too small m are forced to the edges, i.e. up to the parent-node. 
Thus, in the bookkeeping steps the elements move down with the registers but in the ~-nearsort steps 
most of the misplaced elements move up on the tree, and thus they have a chance of finding the proper 
place in O(logn) time. In other words, we have a correction mechanism for the errors E committed in each 
step that does not let the errors accumulate. I. A too formal description Having the ~-nearsort algorithm 
one can' describe the sorting network directly, without introducing the above tree structure. (The price 
paid is that this form is too mystic to under- stand.) For a given t, we define two partitions of the 
registers 1,2,...,n. Define the sets Tt(i,j) as follows: For O<i<t-2, l<j<2 i, Tt(i,j) consists of the 
following three intervals (for definition of Yt(i) see Section 5). (j-l)n2-i+[Yt(i)+l, Yt(i+2)] (j-1)n2-i+[n2-i-l-Yt(i+2)+l,n2-i-l+Yt(i+2)] 
(j-1)n2-i+[n2-i-Yt(i+2)+l,n2-i ] if j is odd, and (j-l)n2-i+[l,Yt(i+2)]  (j-1)n2-i+[n2-i-l-Yt(i+2)+l,n2-i-l+Yt(i+2)] 
(j-l)n2-i+[n2-i-Yt(i+2)+l,n2-i-Yt(i)] if j is even. For i=t-I or t, 1 <j<21 , Tt(i,j) consists of one 
single interval (j-l)n2-i+[Yt(i)+l,n2 -i] if j is odd (j-l)n2-i+[l,n2-i-Yt(i)] if j is even. We also 
define Tt(-l) as the union of the two intervals [l,Yt(O)] and [n-Yt(O)+l,n]. Now partition Ptl consists 
of the "even" sets 22i . Tt(2i,j), i=O,l ..... L(t-l)/2], j=1,2 ..... while partition Pt2 consists of 
Tt(-l) and the "odd" sets Tt(2i-l,j), i = 1,2 ..... Lt/2], j = 1,2 ..... 22i-I . Given these two partitions, 
the t-cycle of the algorithm consists of the application of ~-nearsort simultaneously within the parts 
of Ptl (Zig step), then the same for Pt2 (Zag step), then repeat this alternately for Ptl and Pt2' altogether 
a times each. The whole algorithm starts with an ~-nearsort applied to the whole set of registers 1,2,...,n, 
then the t-cycles are performed for t = 1,2, .... log n . 8. Proof Notations. Both the registers and 
the con-tents are identified by the numbers 1 .... ,n. We write R(i)=j if the content of the register 
i is j, and R(1) denotes U R(i). We will suppress iEl time t in the notations and resolve the missing 
notational problems verbally. The name "interval" and the letters J,K will stand for any one of the intervals 
defined in Section 5 (and called J,Ji,J2 there). The intervals J form a partition of (I ..... n); J' 
will denote the next interval on the right of J. The lower section L(J) consists of all intervals not 
exceeding J. The word "lower section" always means L(J) for some interval J. The definition of upper 
sections is similar. For an interval J and a lower section L =L(K), K<J, K'~J, we define the distance 
d(J,L) as the distance of the two nodes holding J and K on the tree. (Similar definition for upper section 
U.) Note that d might be O, namely if J and K are on the same node (siblings). We define the numbers 
~r(J) as the proportion of elements sitting in registers of J that are misplaced with r or more. More 
precisely, for a given J and r~O, find S 1 =suplR(j ) D L(K) I over all intervals K, K<J, K'~J, d(J,L(K))~r, 
and also S 2 =suplR(a) n U(K) I over all intervals K, K>J, K#J', d(J,U(K))~r. Then ~r(J) is defined by 
Ar(J ) : max(Si,S2)/Ial (when sup is taken over an empty set, we define it as 0). We also set 6(J) = 
IR(J)-JI/ IJl = sup 6(J), A r = s~p Ar(j ) r> 0 j , _ Throughout the proof we will assume that ~ is small 
(A is large), and c is sufficiently small in terms of ~ (actually, c is a small power of ~). The idea 
behind the proof is that in every step of the algorithm (~-nearsort within all parts of a partition) 
the elements (contents) not sitting on the right node of the tree are all moving to the right direction 
--except a negli-gible proportion. The following theorem clearly implies that the algorithm sorts (at 
time Iogn there are no errors since 6<I). Theorem. After every completed cycle, we have (I) Ar < 3r+40 
, r_>l and 3O (2) Proof. We use induction on time t. For t=O, 6=A =0. Assume the theorem holds for some 
r t and prove it for t+l. The cycle starts with changing the register assignments (pointers), thus redefining 
intervals, sections, A r and 6. This will greatly deteriorate the achieved accurateness; the following 
steps of the algorithm are supposed to decrease the errors again. Lemma I. If A r and 6 are the errors 
before the register reassignment, then for the corresponding numbers A' and 6' after reassignment we 
have the r estimates A'r < 6AAr -4 ' r _> 6 and 6' < 6A(6+~) We show now that a Zig (or a Zag) step cannot 
increase the errors too much, while a combined ZigZag step must decrease them. Lemma 2. Let A r and 6 
be the errors before a Zig I (or a Zag) step, and A r and 6' after it. If 6 < 2, then I A r < 8A(Ar+~Ar_2) 
r~3 A' < 8A(Ar+~) r = 1,2 r 6' < 4A(6+~) Lemma 3. If 6 < 4, then a ZigZag step will change the errors 
as follows A'r < 64A2(Ar+I+3EAr _ 4 ) r _> 5 A' < 64A2(Ar+l+3C) r = 1,2,3,4 r 6' < 16A2(~+2e) Starting 
from the inductive hypothesis, Lemma l and repeated applications of Lemma 3 show that after the new cycle 
the errors A r are as small as required. However, the error 6 seems to have increased through all steps. 
This is not so, as shown by the following lemma. Lemma 4. Aft~r.a completed cycle, we have (3) 6 < lO(A~+~(4A)rAr 
) < 30 r>l It remains to prove the lemmas. Proof of Lemma I. A new interval J' is the union of at most 
three segments, each one containpd in an old interval J, and IJ'l > ]JI~/2 for any one of them. Such 
a segment moved at most 2 levels on the tree. Similarly, a new section L(K') is contained in an old section 
L(K), and K' is at most two levels away from K on the tree. Hence the lemma follows. Proof of Lemma 2. 
We use the word near if referring to distances on the tree, and close for distances on the real line. 
Of the six (or less) intervals of a cherry, the closest one to but outside of a section L(K) is either 
the nearest one to L(K), or identical to K'. Most elements belonging to L will be forced to the left, 
and wander into L or to this closest interval. The number of exceptional elements cannot exceed proportion 
of the cherry, which is less than 8A~ proportion of any interval of the cherry. Since this closest interval 
is (one of the) nearest, too, the order of the error did not increase, except for at most the above 8A~ 
proportion, where it could increase at most two. Some elements of L might have come out from L, but they 
simply changed place with elements already belonging to L, not changing their number. Proof of Lemma 
3. The only extra remark we need here is that if an interval J, d(J,L)~l, was closest to L (outside L) 
in the Zig step, then it will not be a nearest one in the Zag step. Thus, not only errors will not increase 
(except a small proportion) but will strictly decrease. Remark. We always assumed that 6 < 2 to ensure 
that the extreme interval of the cherry (which represents about ~/4 proportion of the whole cherry) can 
accommodate all foreign elements. But this extreme interval might be empty. Then however, the total number 
of registers of the cherry is less than 4A, and 4A6<I, i.e. all registers of the cherry contain their 
own elements (R(i) =i). Proof of Lemma 4. We consider an interval J, and estimate the number x= IR(J) 
A U(J')I. It is certainly bounded by the number y = IR(L(J)) N U(J')I , which is of course equal to z 
= IR(U(J')) n L(J)I The equality y=z implies for the numbers x I = IR(J) N J'I and Yl = IR(J') n Jl 
the identity Yl-Xl = IR(J) A(U(j')-j')I-IR(J' ) ~(L(j)-J)I + IR(L(J)-J) ~U(J')I-IR(U(J')-J') ~k(a)l = 
x2-Y2+x3-Y 3 'Now we estimate the terms on the right-hand side; for reason of symmetry, it is enough 
to work with x 2 and x 3. Clearly, x2~AllJl, the hard part is estimating x 3. Let us partition L(J)-J 
into intervals. We may have an interval Jo among them which is at a distance 0 from J'. For this Jo IR(J 
O) AU(J')I ~ IR(J O) hu(a) I <A 1 Iaol <A 1 IJI The number of intervals of this partition that are at 
a distance r>l from J' is at most 2 r+l, and their size is at most (2A)rlJl . Thus (4) x 3 < IJl(Al+~'~22r+IAr~r 
) r>l We have shown now that lYl-Xll is small. Assume that the intervals J and J' are in the same cherry 
for a Zig step (if they are not in the same cherry neither in a Zig nor in a Zag step, then they belong 
to different components, and we have seen before that in this case 6(J) = 0). If the bound on the right-hand 
side of (4) is less than a351j I after a Zag step, then the next Zig step will exchange all foreign elements 
of J and J' except for at most Ixl-Y II+8A(AI+~)IJ 1 < (8A +20AA~+ ~"~,22r+IArA~)IJ 1 < a3OId 1 r>2 
We tacitly used the following trival Lemma 5. For any k, 1 <k<n, the numbers IR({I ..... k}) n {k+l ..... 
n} I and IR({k+l ..... n}) N {I ..... k} I change monotone decreasing throughout the algorithm. (This 
is clearly true for all comparator net- works in which switches place the larger element to the larger 
register.) References <RefA>I. D. Angluin, A note on a construction of Margulis, Information Processing Letters 
8(I), 1979, 17-19. 2, K. Batcher, Sorting networks and their applications, AFIPS Spring Joint Computer 
Conference 32(1968), 307-314. 3. O. Gabber and Z. Galil, Explicit constructions of linear size superconcentrators, 
Proc. 20th Ann. Symp. Found. Comp. Sci., 1 979, 364-370. 4. M. Klawe, Non-existence of one-dimensional 
expanding graphs, IEEE, 1981, 109-114. 5. D.E. Knuth, The Art of Computer Progra~ning, Addison-Wesley, 
Vol. 3, pp. 220-246 (and Exercises 49 and 51). 6. G. Margulis, Explicit constructions of concentrators, 
Problemy Peredachi Informatsii 9(4), 1973, 71-80 (English translation in Problems of Information Transmission, 
Plenum, N.Y., 1975. 7. W. Paul, R. Tarjan and J. Celoni, Space bounds for a game on graphs, Mathematical 
Systems Theory I0, 1979, 239-251. 8. M. Pinsker, On the complexity of a concentrator,  7th International 
Teletraffic Conference, Stockholm, June 1973, 318/I-318/4. 9, N. Pippenger, Superconcentrators, SIAM 
J. Computing 6(2), 1977, 298-304. I0. L.G. Valiant, On nonlinear lower bounds in computational complexity, 
Proo. ?th Annual AOM Symp. on Theory of Computing, Albuquerque, N.M., May 1975, 45-53. </RefA>
			
