
 A Differential Compiler for Computer Animation Michel J. Denber Xerox Corp. 800 Phillips Rd. Webster, 
N.Y. 14580 Paul M. Turner Consultant 287 Ravenwood Ave. Rochester, N.Y. 14619 Abstract A program for 
the real-time display of computer animation on a bit-mapped raster display is presented. The differential 
compiler performs temporal domain image data compression using frame replenishment coding on successive 
frames of animation stored in memory as bitmaps and saves only the differences. A small run-time interpreter 
then retrieves and displays the differences in real-time to create the animated effect. This results 
in a significant reduction in storage requirements, and allows animation on general purpose computers 
which would otherwise be too slow or have insufficient memory. Frame creation is both device and method 
independent. An animation environment supports interactive editing capabilities, reconstructing any arbitrary 
desired frame for later modification. Frames can be added, modified, or deleted, and the animated sequence 
can be viewed at any point during the session. The compiler is automatically called as needed; its operation 
is transparent to the user. The compiler is decribed in detail, both in terms of data compression and 
the requirements of interactive animation editing. CR Categories and Subject Headings: 1.3.2 [Graphics 
Systems]; 1.3.3 [Picture/Image Generation]; 1.3.4 [Graphics Utilities]; 1.4.2 [Image Processing -Compression 
(coding)]; J.5 [Arts and Humanitiesl General Terms: Computer Graphics, Animation Additional Keywords 
and Phrases: Real-time animation, Storage compression techniques, Interactive systems, Picture editing, 
Raster displays  Introduction Animation is the creation of the illusion of movement from a series of 
static images. If the images are displayed rapidly enough, the result is perceived as motion due to the 
factor of persistence of vision of the human eye. The smoothness of motion is determined both by the 
frame rate (the number of complete images presented to the viewer per second) and the relative velocity 
of objects moving in the image. Flicker results from an insufficient frame rate, although it can be aggravated 
by various psychological and physiological factors [7 I. Permission to copy without fee all or part of 
this material is granted provided that the copies are not made or distributed for direct commercial advantage, 
the ACM copyright notice and the title of the publication and its date appear, and notice is given that 
copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, 
requires a fee and/or specific permission. &#38;#169; 1986 ACM0-89791-196-2/86/008/0021 $00.75 Traditional 
animation was done by drawing and painting individual images on celluloid sheets or cels, and then photographing 
each drawing one at a time on movie film. The amount of effort that goes into the creation of a feature-length 
animated film is difficult to comprehend. The nature and number of repetitive tasks involved make animation 
an ideal application for computerization [31. However, although computers have eased the burden of certain 
administrative and bookkeeping tasks in traditional animation, animation by computer presents a variety 
of new problems related to image data storage requirements and user interaction [21]. There are two main 
types of 2-D computer animation: character for cartoon) animation, and modelled for object-oriented) 
animation [lll. Character animation systems duplicate traditional animation steps on a computer and provide 
in addition certain features such as editing and pencil test (animation preview) facilities. However, 
the actual creation of the animated movement is left up to the operator of the system, thus much of the 
work of traditional animation remains. Some systems have features to do automatic inbetweening (missing 
frame interpolation), given two key frames [17]. However, inbetweening is a difficult problem in itself 
and no fully general solution presently exists. Object-oriented systems shift the burden of motion generation 
to the computer itself. The animation is controlled in two main ways, either procedurally or "by example". 
In procedural animation, a program (often using a special animation language) generates the desired motion 
analytically. Although this eliminates the need for moving objects by hand, it requires the ability to 
describe the motion to the computer. The animator has in effect become a programmer. Many existing systems 
in fact take this approach [6, 9, 15, 19]. However, it is difficult to describe complex motions (such 
as human movement) procedurally. Animation by example is a technique in which the motion is shown to 
the computer, for example by tracing a trajectory with a mouse or touch screen [14]. The computer then 
stores the trajectory for subsequent playback. This method is limited by the amount of detail which can 
be presented to the computer. Real- ti me display Once an animated sequence has been created, it can 
be displayed either in real-time or off-line. [n off-line display, each frame is recorded on film or 
video tape for later viewing, in real-time display, the animation is shown directly by the computer. 
Real-time raster-based display creates space and time problems. Consider space: a modest one-minute film 
at 24 frames per second using 512 x 512 x 1 bit frames uses more than 47 million bytes of storage. Consider 
time: each frame (32K bytes) must be retrieved and displayed in 1/24th second (a data rate of over 6 
million bits per second). Even recently, real-time raster display has been considered infeasible [ 13 
]. A number of different approaches have been taken in the past to achieve reaLtime display. Generally, 
special-purpose hardware is used [2l, or some form of image data compression is performed in order to 
relieve the burden on memory size and bandwidth. Sometimes clever use can be made of hardware originally 
designed for other purposes to achieve limited animation as in the method of color-map cycling [20]. 
Some graphics terminals have features, such as pan or zoom registers that can be used for limited animation. 
Some systems use feature coding (higher level object-oriented representations which make use of knowledge 
about objects in a scene) [10]. Image data compression typically is performed on individual frames using 
techniques from the field of signal processing such as run coding, predictive coding, or pulse-code modulation 
[ 18]. Data compression is attractive in principle but is difficult to achieve in practice because it 
requires the system to restore each compressed frame in the available inter-frame interval (typically 
1/24th or 1/30th second). This is a serious problem because many image data compression algorithms are 
quite expensive computationally. The linear transformation with the smallest mean-square error of all 
(the Karhunen-Loeve transform), for instance, requires computation proportional to N 4 for an N x N image 
[16]. Faster transforms (such as the Fourier) operate at the expense of accuracy of image reconstruction, 
and still require considerable computation. Even simple techniques such as run-length encoding may be 
too slow, depending on image complexity. Our animation system is based on a temporal domain data compression 
technique known as frame replenishment coding [18]. Just as spatial image data compression takes advantage 
of the high degree of spatial similarity in local picture regions, we take advantage of the statistical 
redundancy between successive frames of animation. On average, this redundancy is high since it is a 
requirement for achieving the illusion of smooth movement. The system consists of three parts: an animation 
environment, the differential compiler and a real-time interpreter. The animation environment provides 
the user with a variety of paint and editing tools. The real-time interpreter displays the animation. 
The focus of this paper is on the operation of the remaining element, the differential compiler. Compiler 
Implementation To achieve sustained real-time raster display, our system uses a differential compiler 
to perform temporal domain encoding of animation image data. Rather than save each entire frame as it 
is created, the compiler saves only the differences between frames. Our system is unusual in that it 
implements frame difference (or inter-frame) compression on bitmap images using only bit-level operations. 
The entire system is written in [nterlisla-D (a window-based Interlisp impiementatlon developed by Xerox 
Corp.). The differential compiler does not compile a language, although it has features similar to a 
traditional compiler. The compiler consists of two main parts: a comparison phase and a merging phase. 
In the comparison phase, the differences between an input frame and its predecessor are computed. For 
example, Figure 1 shows a box which has moved down between frames 1 and 2. The differences between these 
two frames are shown in the frame labelled D. The open rectangle in D is an area which has been erased 
and the solid rectangle is an area which has been added. I ] m Figure l: The difference (D) between 
frames 1 and 2. Only the regions where the two frames differ are saved; the original frames are discarded. 
Then in the merging phase, the saved difference-regions are combined by merging adjacent regions into 
one. Merging these regions simplifies the operation of the run-Lime interpreter. Frame comparison In 
the comparison phase, the compiler traverses both frames looking for differences. (The compiler can deal 
with frames of any size, however, all frames in a given sequence must be the same size). The comparison 
is done in primitive blocks 16 bits wide to take advantage of the 16 bit word size of the machine. There 
is little point in trying to identify changes at the individual pixel level when we can process sixteen 
pixels at once for the same cost. Also, bitblt (the bit block transfer instruction, described in appendix 
B) works faster when transferring bltmaps to a destination that is aligned on full word boundaries. The 
height of each primitive block can in theory be set to any value; we use 16, producing square blocks. 
This simplifies the merging process described in the next section. When a difference is noted, the block 
and its location are added to a list of changes for thi~ frame. This process continues for the entire 
bitmap. At the end, this change list may contain hundreds of primitive blocks identifying the locations 
of changes in the two frames. This is sufficient to establish the frame differences, but it is inefficient 
to reconstruct the frame this way because the set-up time required by bitblt is independent of the size 
of the bitmap being transferred to the display. (Appendix B shows the relationship between bitmap size 
and display time.) To minimize overhead, the frame comparison is followed by a process that merges primitive 
blocks into larger bitmaps. Primary merging This merging operation is done in two steps: primary and 
secondary merging. Primary merging finds primitive blocks which are imrnediate[y adjacent to each other 
either horizontally or vertically; these are merged into one larger rectangular merged.block (Figure 
2). -L 2a. 2b. Figure 2: Primary merging of horizontally (2a) and vertically (2b) adjacent blocks. It 
is always advantageous to do this since displaying one rectangular bitrnap saves the additional overhead 
that would be required to display its component sub-rectangles individually. Figure 3: Merging order 
of primitive blocks. In actual operation, the compiler performs primary merging in two passes. It merges 
all horizontally adjacent blocks first. Then it merges as many blocks as possible vertically. In Figure 
3, blocks a and b would be merged first. This order minimizes bitblt scanline overhead. Bloek ab takes 
less time to display than block ac, although both eontain the same number of bits, because ab contains 
fewer scan lines. When as many adjacent primitive blocks as possible have been combined, the compiler 
does secondary merging. Figure 4 illustrates primary merging on a more complex image [51. To demonstrate 
the segmentation and merging process, we assume that the entire image in 4a is to be considered changed, 
as it would if it followed a blank frame, in 4b, the primitive blocks are identified. In 4c, they are 
merged horizontally, and in 4d they are merged vertically. 4a 4b 4c 4d Figure 4: Primary merging ofprimitve 
blocks. Secondary merging Primary merging greatly reduces the number of blocks on the change list (in 
Figure 4, it goes from 33 to 6). However, depending on the complexity of the image, we sometimes still 
end up with more blocks than could be displayed in the time available between frames. This is tested 
by doing a "trial bit" of the change list. The list is "displayed" to an off-screen buffer with a timer 
running. If the display time exceeds the inter-frame time, ~he change list contains too many bitmaps. 
Secondary merging is used in this case to further reduce the number of bitmaps which have to be displayed. 
This can only be done at the expense of including regions of no change between the two frames being compiled. 
For example, the three blocks from Figure 3 are reduced to two in primary merging. How could we merge 
these two blocks ab and c, into one new larger "superblock"? Recall that primitive blocks represent areas 
of change. If we are willing to include the corner represented by the dotted line in Figure 5b, we could 
produce a single block (bitmaps always have to be rectangular). The disadvantage is that this new block 
represents a decrease in compression efficiency, since one quarter of it (the dotted area) is a region 
in which no changes occurred between frames. .... 5a 5b Figure 5: Secondary merging ofah with e. To 
measure compression efficiency, we define an efficiency term e, as e = Np/S b , where Np is the number 
of primitive blocks in a block and S b is the total size (in multiples of primitive blocks) of the block. 
The primitive blocks a, b, and e from Figure 3 all have e = 1.0 by definition. Since merged-blocks are 
assumed to be maximally compressed (they are the result of directly combining primitive blocks), block 
ab also has an e of 1.0. Merging ab with c yields a superblock with an e of 0.75 {because it is made 
up of three primitive blocks, plus the space of a fourth). The efficiency factor e is used to control 
secondary merging, by indicating which blocks are suitable candidates for merging. The user can change 
the default threshold for e (0.5) at any time. Setting it to 1.0 results in no secondary merging taking 
place (since each block then has to be composed entirely of primitive blocks), while an e of 0.0 would 
cause every block to be merged together into one superblock equal to the original bitmap (all blocks 
are candidates for merging). Figures 6a and 6b show two frames of a more complex image. The animation 
involves Alice going "through the looking glass" [4] and in Figure 6b, her arm begins to fade from view 
as she reaches through the mirror. The differences (the compressed representation of 6b) are shown in 
Figure 6c. i.! ,:-,,,.,.-~,i .,~.,1~.,$( ~"/':J.i;.,~.X.. '~. ':. , ¢..rd'.iJ~ '1..-~. ~ iii,t?i'v_-2ll 
~"I '):~tl~t~,..:a,=.-:~..=.axe.~a t..~._~ I 6a  {:!. ' .", .gWTA'~,v,~ ) 'r, , ~........ ....... 
" . ' :"1% ._-. l ~,J~,~,e,..,..C(.I, 6b 6c Figure 6: The difference (6c) between frames 6a and 6b. 
How merging works Although it is conceptually simple, the actual process of finding an optimal secondary 
merging strategy for blocks is very difficult. It is in fact NP-complete, being an instance of the rectilinear 
picture compression problem [8]. Our approach to secondary merging is essentially a best-fit algorithm. 
Starting with the change list ofalt merged-blocks remaining after primary merging, each block is compared 
to every other block. The pair of blocks with the largest e value is selected for merging. The resulting 
superblock is added to the change list and the two original blocks are discarded. If no combination can 
be found satisfying the desired compression efficiency, the current block is assumed to be unmergable 
(isolated too far from any other block) and is removed from further consideration. As blocks are merged, 
the change list is updated to reflect the locations of the new superblocks formed, as well as their efficiencies 
for use in any further merging required. When no more merging is possible (because the e would drop too 
low), the compilation is complete. The compiler returns the change list which is saved along with a tag 
identifying the frame from which it was derived. Although secondary merging is the most time-consuming 
part of the compiler, the overhead is distributed by being performed immediately as each frame is completed. 
It can be further reduced by executing the merge phase as a separate process which proceeeds in parallel 
with picture editing, which is largely I/O bound. Playback When the user is ready to run the animation, 
the run-time interpreter is called. It uses the change list created by the compiler for each .successive 
frame, copying the appropriate superbloeks into the specified locations in the animation window. A delay 
mechanism is applied in the display process to those frames which are displayed in less time than the 
desired inter-frame interval, to obtain a constant frame rate. Optionally, the user may specify a greater 
or lesser delay for individual frames. This is used for example, to display a title frame where it is 
desired to leave a static image visible for several seconds or to allow time for a narration. In conventional 
animation, the same frame is repeated on the film as long as necessary. Since there are no differences, 
the run-time interpreter uses the delay loop to achieve the same effect. In this case the subsequent 
identical frames require no storage at all. There is a clear division of labor between compression and 
decompression in this system. Because of the real-time requirements of playback, the run-time interpreter 
is designed to be as simple as possible. All of the complexity is placed in the compiler, which has more 
time available for execution. Appendix A gives a pseudo-code description of the compiler in an Algol-like 
language. The Animation Environment The animation environment is the system's user interface. We discuss 
it here in the context of user interface problems in relation to the compiler. For example, it is important 
to maintain a "virtual frame illusion" for the user. That is, the system should present the illusion 
that the animated sequence being created consists of a series of full frames that can be accessed at 
any time for subsequent editing. Ofcourse, the system has really only saved lists of differences between 
frames. This creates a problem in reconstructing an arbitrary frame in a given sequence. Maintaining 
consistency Hew then, do we deal with editing operations? When the user wants to view a particular frame, 
the run-time interpreter is called. [t "plays back" the sequence from the beginning at high speed without 
displaying it, until the desired frame is reached. The user sees only one frame appear. It is now easy 
to modify that frame. Any of a number of different graphics tools can be called to make whatever changes 
are needed. When the changes are complete, the compiler takes the changed frame and its immediate predecessor 
and recompiles the changed frame, overwriting its previous definition. Then it recompiles the changed 
frame's successor. As an example of this process, consider Figure 7, which shows a simple five-frame 
animation sequence of a bouncing ball. Figure 7: A bouncing ball. Suppose we want this to be a rubber 
ball, and decide to change frame 3 to make the ball squash out on impact with the ground. Figure 8 shows 
the desired result. Figure 8: A bouncing ball, with squash.  Using the editor, we display frame 3 and 
modify the ball. The editor also saves adjacent frames 2 and 4 in their decompressed form. Frame 3 is 
then recompiled against frame 2 to link it back into the sequence. Now frame 4 is compiled against the 
modified frame 3, yielding a new change list for frame 4. It is important to note that the "buck stops 
here." It is not necessary to recompile frame 5 against frame 4. This is often a source of confusion 
to the casual observer, who might assume that since each frame is derived from its predecessor, a change 
in any frame means that all subsequent frames have to be recompiled. It is obvious that editing frame 
3 means that it will have to be recompiled. It is not so obvious that by then recompiling frame 4, the 
entire sequence is once again consistent. Because the time needed to recreate a frame is directly proportional 
to its distance from the start of a sequence, it is desirable in long sequences to cache certain checkpoint 
frames at regular intervals (for example, every 500 frames). This cache does not have to be saved with 
the sequence; it can be created at the beginning of an edit session and discarded upon completion. The 
system keeps the full definition of cached frames; then it is only necessary to back up to the last checkpoint 
to reconstruct a particular frame for editing. Frame editing The animation environment provides several 
different editing operations on frames. Frames may be changed, added, deleted, or copied. Copying is 
the default operation when creating a new frame. The system copies the previous frame so that the user 
can make changes while maintaining overall image registration. New frames can be created in any order. 
It is sometimes desirable to create frames in reverse order. For example, the user may want to show an 
object in the foreground moving to reveal a scene in the background. Rather than draw in the background 
elements revealed by the moving foreground object with each new frame, it is easier to draw the last 
frame containing the background first, and then move the foreground object backwards, gradually obscuring 
the background. Then each preceding frame can be generated simply by moving the object and erasing the 
part of the background newly obscured. Also, it is sometimes useful to be able to copy a previous frame 
into a later position. From the user's point of view, these operations are similar to those in text editing. 
The frame-difference technique lends itself well to modular design, In particular, the choice of input 
medium can be left up to the designer or user of the system. It makes no difference to the compiler 
how the image to be compiled was created, as long as it is placed in the frame window. In our system, 
the user can draw with the mouse or graphics tablet, or use images retrieved from an optical scanner. 
A good user interface should provide helpful feedback. This is especially important in animation where 
the motion must be built up from a series of static images. In our system, the user can do a pencil test 
at any time during the editing session (although only in a forward direction}. The entire sequence or 
any part of it can be played back, either once or in a loop. Results Frame rate One important measure 
of a real-time animation system is how fast it can display animation. Unfortunately the definition of 
"frame rate" becomes somewhat blurred in a system which uses data compression. A motion picture projector 
always displays discrete frames at a constant rate as complete images on film. However, our system uses 
a varying length of time to reconstruct frames, depending on the size of their change lists, and may 
leave each frame visible for varying times (as set by the user}. Our "frame rate" is bounded by the time 
needed for the interpreter to display change lists. For example, for a rate equivalent to 24 distinct 
frames per second, the interpreter has 1/24th second to put up all of the differences between the current 
frame and the next one. This "frame rate" can be changed by adjusting the delay between frames. Consider 
the technique of "shooting on twos", that is, repeating each frame twice to reduce the number of drawings 
that have to be made. In our system, this is accomplished simply by doubling the frame delay time. The 
frame rate of our system then depends on the complexity of the change-list plus the overhead of the run-time 
interpreter itself. This overhead is given by the formula: Ort i =(tf-N k*t k)/N k where tf is the inter-frame 
time, N k is the number of bitmaps of size k the interpreter can display in time tf, and t k is the time 
bitblt takes to display one bitmap of size k. For example, by running a timer we find that the interpreter 
can display 27 16 x 16 bitmaps in 1/24 sec. (41.7 msec.). Since bitblt alone takes 1.48 msec. to display 
a 16 x 16 bitmap, we find Ort i = (41.7 -27 * 1.48) / 27 --64.4/.tsec. per bitmap per frame. This figure 
is independent of the size or number of bitmaps in the change list. The general frame rate is then simply 
n r=l/(Orti*n+ ~ t i) i=] where n is the number of bitmaps in the change list and t i is the time it 
takes to bitblt the nth bitmap. From this one can find the upper bound on frame rate, which would result 
from displaying a change list containing a single 16 x I bitmap (the sma|lest possib|e block). Bitblt 
transfers this bitmap in 1.21 msec., therefore, rma x = 1 / (0.064 + 1.21) = 0.785, or 785 frames per 
second. A more typical example is Figure 8, which can be run at up to 139 frames per second. Data rate 
Also of interest is the maximum amount of data which can be displayed per frame. This depends not only 
on the number of bitmaps in the change list, but also on their shape, because of the set-up time per 
scan line associated with bitblt. Assuming as an average, a square shape, in 41.7 msec. the interpreter 
can display a bitmap of size 445 x 445. This is an area approximately 1/4 the size of the screen of an 
1108 (1024 x 808) and is the upper bound on frame size in the limit case where every pixel is changed 
between frames (a very rare occurrence}. In practice, the order of primary merging causes the compiler 
to generate more wide blocks (which can be displayed faster} than tall blocks. The exact amount of compression 
obtained depends on the complexity of the motion within the scene being animated. Complexity here refers 
to the degree of redundancy between successive frames, rather than the analytical description of the 
motion itself. On average, this complexity is directly proportional to the spatial complexity of the 
individual images, because of their lower spatial correlation. For simple animation the savings can be 
very large. For example, the original frame size of the five frame sequence in Figure 8. was 400 x 300 
pixels (15,000 bytes}, for a total size of 75,000 bytes. After compiling with a 0.5 efficiency, the compressed 
sequence used 15,255 bytes, an 80% reduction. For a longer sequence of 125 frames (frame size = 400 x 
225 pixels), the full frame representation used 1.4 megabytes; the compressed version (0.5 efficiency} 
used 85,636 bytes, a 94% savings. The total length of animation possible is limited by the hardware. 
For example, on an 1108 with 3.5 Mb. of real memory, assuming 500 bytes per frame, and aliowing 1/2 Mb. 
for system code, one could display 6291 frames, or 4.16 minutes at 24 fps. If we include the maximum 
virtual memory size of 32 Mb., less 8 Mb. for system code, we could display an additional 50,331 frames, 
although the maximum frame rate would be lower due to swapping considerations. Compilation time The length 
of time it takes to compile a frame is proportional to the complexity of the changes between the two 
frames being compared, not necessarily the complexity of the images themselves. For example, the bouncing 
ball in Figure 8 took 3.8 seconds per frame to compile. Figure 9 shows a six frame cycle of a dancer 
performing a fouette [ 12}. This used 25.9 seconds per frame. 2= Figure 9: A fouette turn. Conclusions 
 Frame difference data compression can be used to make animation possible on a general purpose computer. 
It is attractive because, by using bitblt, the total time for compression and decompression can be largely 
shifted to the compression phase, making it possible to restore the images quickly. This project demonstrates 
several basic programming principles of particular interest to graphics programmers: 1. When time is 
limited, shift the execution burden to a less critical section. The operation of the run-time interpreter 
was kept as simple as possible by having the compiler do the hard work. This leaves free cycles for the 
later addition of new features which might need to be done at run-time. 2. Use parallelism wisely. Bitblt 
is a powerful operation for moving large areas of memory within the computer with a single instruction. 
However, the straight-forward use of bitblt to do animation by superimposing whole frames is wasteful 
of space. 3. Demonstrating is better than calculating. To decide if secondary merging is required, we 
initially thought to calculate the time required from the change list to see if it is less than the interframe 
interval. However, it is simpler to actually do a trial display of the frame to a scratch area, running 
a timer on it. Having the computer do this work provides an existence proof that the frame can or cannot 
be reconstructed in the required interval, and is less work than calculating the expected display time 
from a formula. This method automatically accounts for different processor speeds, future mieroeode upgrades, 
etc.  The system is still under active development. Several optimizations can be made to the code. For 
example, bitblt is a complex instruction that handles many special cases, most of which are not used 
for animation display. We could increase the display rate by using a customized bitblt without the special 
ease code. Greater space compression could be obtained by applying simple image processing techniques 
to the images before being compared. For example, the inputs could be low-pass filtered, reducing the 
contribution of isolated pixels, Two new features need to be added to make the system easier to use. 
The first is to allow cels to exist on multiple planes, as in traditional animation. The second is to 
add object-oriented animation to the environment. Objects present a challenge to the animation compiler 
because they reflect a fundamentally different paradigm. The motion of an object in a scene is a property 
of the object itself. Objects move independently without regard to the fact that the system views the 
animation as a series of frames. In order to determine where an object would appear in a particular frame, 
the compiler has to execute the motion assigned to the object and calculate its location based on its 
speed, The system as currently implemented has been used successfully by professional technical illustrators 
who were not familiar with programming or with the Interlisp-D environment. They were able to produce 
drawings using the paint portion of the system after about a half hour of familiarization under the guidance 
of one of the implementors, and created simple animated sequences in less than two hours. Although the 
frame difference technique has disadvantages, we have shown that it can be used to create a viable animation 
tool on a computer which would otherwise be unable to support animation of such complexity. The versatile 
programming environment provided by Interlisp-D was of great help in programming our system. The use 
of powerful interactive languages including Lisp and Smalltalk is still relatively uncommon in computer 
animation [1, 18]. We believe that the advantages to the programmer are significant, especially where 
extensive user interaction is required. Acknowledgements The authors gratefully acknowledge the valuable 
technical comments and suggestions offered by Jules Bloomenthal, Xerox PARC, and the Siggraph reviewers. 
Bill Anderson, Xerox Webster, was of great help in editing the paper. Dave Ingalls, Xerox Webster, helped 
out with implementation issues and integrated the system into Trillium, a user-interfaCe design tool 
developed by Austin Henderson at PARC. I ~ant to thank Chris Brown, University of Rochester Computer 
Science Dept., for sharing his insight and enthusiasm for computer graphics. Finally, special thanks 
to Jim Iverson, Xerox Webster, for his continuing support and encouragement of our work, which made it 
all possible.  Appendix A: Compiler pseudocode The compiler takes two bitmaps BM l and BM 2, a maximum 
inter-frame time ti, and a user-set efficiency threshold u, and returns the compressed representation 
of BM 2 in the change list L. L ~-- empty-list ; begin "scan and save differences " for Y from 1 to bitmap-height 
by 16 (ie. block height} do for X from 1 to bitmap-width by 16 do get a 16 x 16 block B 1 at X,Y from 
BM l ; get a 16 x 16 block B 2 at X,Y from BM 2 ; if B l ~ B 2 then add B 2 to change list L ; end "for 
x" ; end "for y" ; end "scan and save differences" ; begin "combine regions (merge phase)" begin "primary 
merge" for i from 1 to length L - 1 do if block B i in L is horizontally adjacent to Bi+ 1 then replaceB 
iandB i+lwithB i~B i+B i+l; end "for i" ; for i from 1 to length L - 1 do if block B I in L is vertically 
adjacent to Bi+ 1 then replace B i and Bi+ l with B i ~-- B i + B i + t ; end "for i" ; end "primary 
merge" ; timer-count ~-- timer ("display" (off-screen} the blocks in L) ; if timer-count > inter-frame 
time t i then until no spaee-effieient blocks can be formed do "secondary merge" for i from 1 to length 
L do forj from 1 to length L when i ~ j do ifB i +Bj is the most efficient combination so far then Bma 
x ~-- B i + Bj ; end "for j" ; end "for i" ; ife (efficiency factor} ofBma x > user-set threshold u then 
replace B i and Bj in L with B i ~-- ~i + Bj ; end "secondary merge" ; end "combine regions (merge phase)"; 
Attach a default frame delay time (0) to L ; return L. Notes: The "sum" B i + Bj is the smallest bounding 
reetangular block around B i and Bj. Loop indices (such as "length L") are recomputed each time through 
the loop.   Appendix B: Speed of bitblt Bitblt is short for bit block transfer. This instruction treats 
memory as rectangular blocks and moves blocks of any size to any location. If the destination corresponds 
to the machine's display memory, an image will appear on the screen. The user can specify how the source 
and destination bits should be logically combined before being displayed. This gives the effect of replacement, 
erasure, addition, or inversion of pixels. The Xerox 1108 has a 17 inch bit-mapped raster display with 
a resolution of 1024 x 808 pixels. The following timings were obtained using bitblt to transfer various 
word-allgned bitmaps into a window on the display in replace mode: time (msec.) bitmap size (bits w x 
h) 0.70 0 x 0 1.48 16 x 16 3.01 64 x 64 16.9 256 x 256 1.21 16 x 1 1.26 256 x 1 4.95 1 x 256 1.54 32 
x 16 1.78 16 x 32  References <RefA>1. Baecker, Ronald M., "A conversational extensible system for the animation 
of shaded images", Computer Graphics (Proc. Siggraph 76), 10:2, 1976, 32-39 2. Baecker, Ronald M., "Digital 
video display systems and dynamic graphics", Computer Graphics (Proc. Siggraph 79), 13:2, Aug. 1979, 
48-56 3. Catmull, Edwin, "The problems of computer-assisted animation", Computer Graphics (Proc. Siggraph 
78), 12:3, July 1978, 348-353 4. Carroll, Lewis, The Annotated Alice, World Publishing Co., New York, 
1960, p. 184 (illustration hy John Tenniel). 5. Disney, Walter, Fantasia, 1940. Drawing digitized and 
then modified from a frame in the "Sorcerer's Apprentice" section of this film. 6. Feiner, S., D. Satesin, 
T. Banchoff, "Dial: a diagrammatic animation language", IEEE Computer Graphics and Applications, 2:7, 
Sept. 1982, 43-53 7. Fox, David, Mitchell Waite, Computer Animation Primer, McGraw-Hill, New York, 1984 
 8. Garey, Michael R., David S. Johnson, Computers and Intractability: A Guide to the Theory of NP-Completeness, 
W.H. Freeman &#38; Co., San Francisco, 1979 9. Kahn, Kenneth M., "An Actor-based computer animation 
language", Proc. ACM-SIGGRAPH Workshop on User-Oriented Design of Computer Graphics Systems, Pittsburgh, 
Pa., Oct. 1976 10. Karshmer, Arthur I., "A motion directed picture segmentation system to support network 
graphics applications", Proc. 1979 [EEE Computer Society Conference on Pattern Recognition and linage 
Processing, Aug. 1979, 630-637 11. Lansdown, R.J., "Computer animation: A concise review", Computer 
Graphics 82, Proceedings of the Online Conference, 1982, 279-290 12. Laws, Kenneth, "Physics and dance", 
American Scientist, 73:5, Sept.-Oct. 1985, 426-431. Drawings digitized from photographs by Martha Swope 
of Lisa de Ribere. 13. Magnenat-Thalmann, Nadia, Daniel Thalmann, Computer Animation: Theory and Practice, 
Springer-Verlag, Tokyo, 1985 14. Minsky, Margaret R., "Manipulating simulated objects with real-world 
gestures using a force and position sensitive screen", Computer Graphics (Proc. Stggraph $4), 18:3, July 
1984, 195-203 15. O'Donnell, T.J., Arthur J. Olson, "GRAMPS: A graphical language interpreter for real-tlme, 
interactive, three-dlmenslonal picture editing and animation", Computer Graphics (Proc. Siggraph 81), 
15:3, Aug. 1981, 133-142 16. Oppenheim, Alan V., ed., Applications of Digital Signal Processing, Prentice-Hall, 
Englewood Cliffs, N.J., 1978 17. Palyka, Duane M., "A brief description of an inbetween system (using 
drawings by Francis Glebas)", NYIT CGL, July 1983, in Siggraph 84 Animation Tutorial Notes, 82-87 18. 
Pratt, William K., Digital Image Processing, John Wiley &#38; Sons, New York, 1978 19. Reynolds, Craig 
W., "Computer animation with Scripts and  Actors", Computer Graphics (Proc. Siggraph 82), 16:3, July 
1982, 289-296 20. Sboup, Richard G., "Color table animation", Computer Graphics (Proc. Siggraph 79), 
13:2, Aug. 1979, 8-13 21. Thomas, Frank, "Can classic Disney animation be duplicated on the computer?", 
Computer Pictures, July/Aug. 1984, 20-26  </RefA>
			
