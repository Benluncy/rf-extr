
 Algorithms, Concurrent Processors, and Computer Science Education: or, "Think Concurrent or Capitulate?" 
 by Elliott I. Organick Department of Computer Science University of Utah Salt Lake City, UT 84112 Abstract 
 Concurrent processor supercomputers are yet another emerging reality to be reckoned with in preparing 
appropriate curricula for the remainder of the 1980s and beyond. Old problems need to be revisited and 
re-formulated for solution on concurrent computing systems. Such systems will soon be marketed. Fortunately 
for us, learning to exploit and teach the use of concurrent computer systems is exciting and rewarding 
work. At least one new book presenting concurrent algorithms for well known problems, aimed at graduate 
students and faculty, will be appearing shortly; it and similar books are likely to trigger a new wave 
of text and course development tailored for undergraduates. The conclusion is inescapable that concurrent 
computer systems and their use are another essential and attractive facet of the computer phenomenon, 
and another intellectual challenge set before us all. Fortunately, "thinking concurrently" is fun! key 
words: concurrent algorithms, concurrent processing, concurrent programming, hypercube architecture 
Introduction There can be no rest for us. Hardly have we finished upgrading our courses to reflect data 
abstraction, message-based communication, and objects for system modelling and problem solving; hardly 
have we adjusted to the Ada/Modula reality, to the new and more powerful dialects of Lisp, and to Prolog 
and other very high level languages; hardly have we adjusted to windows, rnouses, menus and spreadsheets; 
hardly have we come to understand how hardware and logic design systems can be driven by advances in 
specification and description methodologies, languages, and verification techniques; .... The pace of 
change is swift and leaves many of us breathless. So, what's next, assuming we are up to date with all 
of the above? My answer today is parallel processing. Actually, the more appropriate term is concurrent 
processing [8]. In case you haven't noticed, concurrent processing is about to leave the laboratory and 
enter the marketplace It is apt not only to revolutionize scientific computing, but may enter the world 
of everyday Permission to copy without fee all or part of this material is granted provided that the 
copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the 
title of the publication and its date appear, and notice is given that copying is by permission of the 
Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific 
permission. data processing as well ]. Concurrent processors embedded in personal supercomputers are 
yet another emerging reality that educators must come to terms with in preparing appropriate curricula 
for the remainder of the 1980s and beyond. Old and now uninteresting algorithms need to be revisited; 
the problems that gave rise to these algorithms need to be re-formulated in the framework of concurrent 
computing systems. Such systems will soon become new factors contending for attention in the supercomputer 
race and in the race to provide ever-more powerful workstations --and so, we must understand principles 
of their use, their scope of applicability (far wider than at first thought), and the art of "thinking 
concurrently" in approaching algorithm design for execution on concurrent processing systems. Fortunately 
for us, learning to exploit and teach the use of concurrent computer systems is pleasant and rewarding 
work, Soon there will be new textbooks (e.g., [2]) on concurrent algorithms aimed at graduate students 
and faculty. These will trigger, as did Knuth's "Art of Computer Programming", a new wave of text and 
course development tailored for undergraduates. Scope of Concurrent Processing One can easily be caught 
off guard, believing that concurrent processing is limited to a small and perhaps &#38;#169; 1985 ACMO-89791-152-O/85/O03/O001 
$00.75 1In some ways it has, e.g,, with distributed database systems, uninteresting class of problems. 
I now perceive otherwise. Certainly the field of scientific computing is poised for new advances by exploiting 
concurrent processors, especially ~sing systems which at the top level provide parallel concurrency and 
which at the next-to-top level exploit pipelined concurrency (eg, vector processing). There is much concurrency 
being "discovered" in problems originating in physical sciences, engineering, and numerical mathematics 
[1, 9]. Those working on improving performance of scientific applications are beginning to discover that 
models on which these computations are based are often most easily represented by regular structures 
of processes which interact through messages. Moreover, the same process structures can be viewed as 
regular geometric structures (as with systolic arrays [5]), within which the primary flow of messages 
is between "adjacent" processes. There is also much concurrency in problems that originate in computer 
science, such as string searching, sorting, and other problems that fall in the general category of "problem 
heap problems" [7] (which includes divide and conquer problems). Architectures and Models of Computation 
Design efforts leading to concurrent architectures and models of computation are numerous. Since I understand 
only two of them [4, 9), and then only moderately, these remarks are confined accordingly. Common to 
each approach is an ensemble of computers (or "nodes"), each with its own local memory and front-end 
communication processor (also referred to as message switch). Various node interconnection topologies 
have been considered, but hypercube connectivity has great appeal for use in a wide class of applications 
and range of cube degree [6]. Lang refers to this ensemble as a homogeneous computer. A hypercube of 
degree N (or N-cube) has 2**N nodes with each node having N nearest neighbors. The two computational 
models of interest here, both of which are supported by hypercube topology, exhibit extremes of binding 
strategies; from most static to most dynamic. The kernel initially implemented for the Caltech hypercube 
(called the Cosmic Kernel) supports only static program structures and (at least apparently) fixed workloads. 
That is, at the system level, processes are statically allocated to nodes. However, for at least some 
applications, the concurrent algorithms that a user constructs may (implicitly or explicitly) exhibit 
various degrees of load balancing -- through dynamic data distribution. 2. The kernel for the applicative 
multiprocessing system under development at Utah (Rediflow) will lead, when implemented, to dynamic load 
balancing and free migration of spawned tasks within the supporting concurrent processing structure --all 
this without overt management by the application programmer. The Rediflow approach also supports objects 
with system-wide unique names, thus permitting addressing across node boundaries. By contrast, each process' 
executing in each node in the Caltech model has its own separate, and hence more limited name (and address 
space). While the Rediflow approach has the appeal of greater generality and potentially greater ease 
of use, favorable performance for it has not yet been confirmed on a prototype By contrast, the Caltech 
cube with its Cosmic Kernel 2 exists and works; similar systems are being built and will soon be available 
for experimentation and use, especially within the scientific computing community. Because there have 
already been a number of successful computations executed on the Caltech Cube [1], the Cosmic Kernel 
model has more thoroughly demonstrated its appeal. In short, we need look no further to build the case 
for concurrent processing and the need to understand where it should fit in computer science education. 
Using a Cosmic Kernel-like executive, the simplest computation scenario appropriate for concurrent processing 
applications is as follows: A problem is formulated (in the abstract) as a graph structure of nodes such 
that each node holds an identical process (or set of processes) and arcs represent internode (and hence 
interprocess) communication paths. 3 The (abstract) graph is then embedded in the (concrete graph of 
the) hypercube. For example, ring, 2-d, or 3-d mesh problem graphs all embed nicely in a cube of degree 
6 or higher. Thus a ring of 64 nodes embeds in a 6-cube, but note that only two of the six interconnections 
from each node are put to active use as communication channels. [The graph-embedding process is surprisingly 
simple or at least surprisingly straightforward for many problems, especially so when using a well-chosen 
set of utility routines.] 2. Machine code for an abstract node is loaded into the corresponding concrete 
node --generated for each separately or independently compiled application process. Note that, in general, 
the (identical) application code is loaded into each participating node of the hypercube; the host computer 
serving (driving) the hypercube usually supplies this code. 3. The workspaces for each node are then 
initialized, usually via interaction with the host. 4. The application in each node is then invoked, 
thereby starting up to 2**N concurrent (and cooperative) computations. Cooperation is  2Both have been 
designed and implemented by Seitz end his students. 3One is free to replace "process" with "data abstraction" 
or "object" in this senter~ce. achieved via message exchange. Messages transmitted to distant neighbors 
are relatively costly in terms of system resources. For this reason, one tries to design algorithms which 
exhibit strong locality (i.e., messages are sent primarily to nearest neighbors). 5. Results, intermediate 
and final, as well as various pieces of status information useful for debugging or performance analysis, 
are sent from each node to the host.  6. A computation is usually terminated when the host determines 
that all processes in the cube have terminated or when the host chooses to kill the processes executing 
in the cube.  The choice of communication protocols by which messages are exchanged affects the simplicity, 
correctness, reliability, and speedup of a concurrent algorithm. By no means are all the interactions 
among these attributes vet well understood. Basically. there are two kinds of protocol style one can 
choose. (Both are available to the user of the Cosmic Kernel): 1. Fu/ly synchronous message passing. 
Here (i)a sender is blocked until a correspondin 9 receive request has been issued, and (ii) a process 
initiating a receive action waits until a message has been received. While this protocol appears simple 
to use and safe, it is, contrary to one's intuition, surprisingly easy to accidentally introduce a deadlock 
condition. For some applications, one may also find that the use of synchronous protocols leads to significan.tly 
slower performance. 2. Near/y asynchronous message passing. Here the sender is normally not blocked 
upon issue of a send request and can proceed to compute during the overlap with actual transmission and 
receipt of the message. Likewise the would-be receiver can issue a receive request, can proceed with 
computation that does not depend on the content of the to-be-received message, and can defer polling 
for actual receipt of a previous request until it is convenient to do so. When appropriate, deferred 
polling is a useful strategy to minimize waiting for messages and can add to the efficiency of the algorithm 
when executed on a system for which communication channels to and from the nodes are slow relative to 
computation speeds at the nodes. I have found that use of asynchronous protocols facilitates easily understood 
algorithms while eliminating deadlocks due to programmer errors in specifying synchronization for message 
sending/receiving. (There is still no guarantee, however, against possible system-induced deadlocks which 
may arise due to message  buffer storage limitations). Two concurrent algorithms with deferred polling 
illustrated Here we briefly sketch "textbook style" concurrent algorithms for two familiar problems In 
each case we illustrate the use of asynchronous protocols and introduce deferred polling to show how, 
if desired, message waiting time can be minimized, Our two exampres are: 1. Concurrent solution of LaPlace's 
equation in two dimensions, ~72(~) = 0, an "old saw", but definitely worth revisiting, and 2. Matrix 
multiplication, an even "older saw". Solution of Laplace's equation, Figure 1 shows a decomposition of 
a two-dimensional problem requiring solution of Laplace's equation. We don't show the boundary conditions, 
but rather focus on the 16 by 16 subgrid to be dealt with within a single node. Following the explanation 
in [1] we note "that the finite-difference approximation to Laplace's equation leads to a matrix inversion 
that can be solved iteratively. To each internal point (i, j), we successively apply the basic (element 
update) algorithm '~'central = [~teft + ~right + ~up + ~dowJ/4''4 Corresponding subgrid points in each 
node are updated concurrently, with points on problem boundaries dealt with as special cases. Elements 
that are internal to a subgrid can be computed immediately. Edge element computations require values 
of elements residing in adjacent nodes. (In particular, edge calculations require data from the edge 
of one adjacent node, whereas corner calculations require data from two adjacent edges of two adjacent 
nodes.) The algorithm followed for any node, and omitting steps required to test for termination, is 
given in Figure 2. The speedup of this concurrent Laplace equation solver over its sequential equivalent 
is close to 2**N. Multiplication of two conforming matrices. For simplicity here, let A and B both be 
n by n in shape, where n is large and divisible by 2**N. The computation is decomposed such that all 
of the 2~*N nodes are interconnected as a one way ring. Each node is initialized with matrix data as 
follows: The ith node is given the ith section of complete rows of A and the ith section of partial columns 
of B. Thus, node 1 has rows 1..n/(2*~N) of A and parts of all columns of B, namely, the elements in rows 
1..n/(2**N). The next node is initialized with the next row section of A and column parts of B. Thus 
initialized, each node can begin computing terms in the product C matrix for elements of C corresponding 
to each complete row of A supplied to the node Upon completion of this stage of the computation, the 
column parts of B residing in each 4Actually, slightly more complicated update rules, which lead to much 
faster rates of convergence, .are well known [3J Moreover, research continues for further improvements, 
especially in attempts to exploit concurrency. 1. Send row and column edges to adjacent nodes. 2. Receive 
row and column edges from adjacent nodes.  3. Complete update calculations for each inner element of 
the subgrid.  4, If an edge message has been received, update element values for that edge. 5. If 
adjacent edge messages have been received, update the corresponding corner element value. 6. If all 
elements of the subgrid have not been updated, return to step 4.  7. Start next iteration.  Figure 
2: Calculation and communication steps per iteration for zero-wait strategy. Node No I a 0 C  lSt row 
section &#38; T Node No 2 A 8 C [II,OI.II.II.II.III IIl,ll.ll,lm.ll.lll lma'ee'la'ae'te'lll Imo'el'oo'mm'm~'woJ 
 ='.~z-rc=CF 2ndrowsectJon Node No 3 1 A e C I , .................. ~.~.~.~...,.,..~-,.~.l . ................. 
. I n'n 'o o'o 'o m'l I ImHumlllwlu|ulwlulm~ I  'l 'e 'l 'l 'w J 3rd row s@otior~ l T'l 
Node NO 2**N i A a C I ',:1:1::11:11:1:~!11 ~ ~..................o'e i I le w'l l'w m'w g'l 2**N th 
row section I T Figure 3: Schematic of the concurrent algorithm for matrix multiplication. node are 
concurrently forwarded around the ring a "distance of" one node, Since the very first stage just completed 
serves to initia'lize each sum of terms which eventually become the elements Ci, j, during each successive 
stage, new terms are added to existing sums. Figure 3 offers a schematic for this algorithm. After completing 
a total of 2**N stages, each of the nodes holds a n/2**N-row section of the product matrix C, Computation 
is terminated when the row s~ctions of C are reported (sent) to the host, which then assembles C. In 
order not to incur delay by waiting for the next Problem Space: 16K elements distributed over 6q nodes. 
oOOO00OOOO0OOOOCOOOOO~ OO0000000000000C 00000 ooooooooooooooocooooo DOOOOOOOOOOOOOOC~OOOOgF oooooooooooooooo 
o~n# ~O00QOQOQQQUQQ 0 0 0 0 oooooooooo o~ 000000000000 o o o ~o~ 000000 000000 16K element problem 
space distributed over 64 nodes. 64 Nodes (8x8 2-d Model) iiiililiiiiiilililiiiiiiilili   iiiililili 
/ Individual Node: 16 x 16 elements. UUU ~O0000~O0000000000000C ~00000 I000000 ~000000 ~000000 ~000000 
~000000 ~000000 ~000000 ~ooo000  ~00000 ~ooooo ~oooo ~ooo ~o~ ~W Figure 1: O00000000000000C O00000000000000C 
OO0000000000000C OO0000000000000C O00000000000000C O00000000000000C O00000000000000C O00000000000000C 
 ' 000000~ 000000 000000( 0000000 0000000~ 00000~0, 0000000< 0000000( 0000000,  subsection of B, 
the deferred polling strategy can be implemented, i.e., as soon as a column subsection of B has been 
used at a node, it can be sent off (i.e., forwarded along the ring) as a message to the successor node 
and a corresponding column subsection received from the node immediately "upstream" of the node we focus 
on. Identical asynchronous send requests will be issued concurrently at all nodes (followed by identical 
receive requests), one send/receive pair per column subsection per node. Waiting time for completion 
of receives will overlap computation time required by use of column subsections of B to the right of 
column subsections (of B) that have already been "used up" and are "in flux" along data communication 
paths from and to each node along the ring. This concurrent matrix multiplication algorithm's speedup 
should be 2**N over the time required for the sequential matrix multiplication algorithm.  Conclusions 
 Although parallel processing is a relatively old field, new concurrent architectures exemplified by 
the Caltech Cube allow us to approach the subject as brand new and fertile. I have sketched, albeit very 
briefly, the general concepts including those from the viewpoint of an applications programmer. My own 
experience as a novice concurrent programmer has convinced me that the ideas are all simple or straightforward 
Very few if any new programming language constructs are required. The carrot for the computer scientist 
is that relatively simple but powerful concurrent algorithms will be eas~/ to develop for a growing variety 
of applications. The reward, for the computer science educator is the satisfaction, and indeed fun of 
discovering how exciting it is to teach concurrent processing. Acknowledgments I am indebted to a number 
of persons who are pioneering the area of concurrent processing architectures, operating systems, and 
algorithms. The list includes Professors Seitz and Fox at Caltech, Professors Keller and Lindstrom at 
the University of Utah, and Messrs. J. Rattner, C. B. Moler, P. Wiley, and J. Montague at Intel.  References 
<RefA>[1] G. Fox, S, Otto. Algorithms for Concurrent Processors. Physics Today :50-59, May, 1984. [2] G. Fox, 
G. Lyzenga, S. Otto. Solving Problems on Concurrent Processors. Book in Preparation, 1985. [3] E. Isaacson, 
H.B. Keller. Analysis of Numerical Methods. Wiley, New York, 1968. [4] R.M. Keller, C. H. Lin. Simulated 
Performance of a Reduction-Based Multiprocessor. Computer 17(7):70-82, July, 1984. [5] H.T. Kung. Why 
Systolic Arrays? IEEE Computer 15(1):37-46, Jan, 1982. [6] C.R. Lang, Jr. The Extension of Ob/ect-Oriented 
Languaages to a Homogenous Concurrent Architecture. Technical Report TR:5014:82, Department of Computer 
Science, California Institute of Technology, May, 1982. PhD Dissertation. [7] P. Moiler-Nielsen, J. Staunstrup. 
Experiments with a Multiprocessor. Technical Report DAIMI PB-185, Computer Science Department, Aarhus 
UniversitÂ¥, Nov, 1984. [8] C. Seitz. Concurrent VLSI Architecture. I EEE Trans Computers C-33(12): 1247-1265, 
Dec, 1984. [9] C. Seitz. The Cosmic Cube. Communications of the ACM 28(1), Jan, 1985</RefA>.  
			
