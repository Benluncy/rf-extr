
 A Doubly Logarithmic Communication Algorithm for the Completely Connected Optical Communication Parallel 
Computer Leslie Ann Goldberg, Sandia National Labs 1 Mark Jerrum, University of Edinburgh 2 Tom Leighton, 
MIT3 Satish Rae, NEC ABSTRACT In this paper we consider the prob­lem of interprocessor communication 
on a Compltd ely Connected Optical Communication Parallel Computer (OCPC). The particular problem we 
study is that of realizing an h-r-elation. In this problem, each processor has at most h messages to 
send and at most h mes­sages to receive. It is clear that any 1-relation can be realized in one communication 
step on an OCPC. However, the best known p-processor OCPC algorithm for realizing an arbitrary h-relation 
for h > 1 requires @(h + logp) expected communication steps. (This al­gorithm is due to Valiant and is 
based on earlier work of Anderson and Miller. ) Valiant s algorithm is op­timal only for h = f2(log p) 
and it is an open ques­tion of Ger6b-Graus and Tsantilas whether there is a faster algorithm for h = 
o(logp). In this paper we answer this question in the affirmative by presenting 1 Department 1423, Sandia 
National Labs, PO Box 5800, Al­ buquerque, NM 87185 USA, Bmail: lagoldb@cs.sandia.gov. This work was 
performed at %ndia National Laboratories and was support ed by the U.S. Department of Energy under contract 
DEAC04-76DPO0789. 2 Department of Computer Science, The University of Edin­burgh, The King s Buildings, 
Edinburgh EH9 3JZ United Kingdom, E-mail: mrj@dcs.ed.ac.uk. 3 Mathematics Department and Science, MIT, 
Cambridge, ftl@math.tit .edu. Supported AFOSR-F4962C-92-J-0125 and 91-J-1698 and NOO014-92-J-1799. 4 
Laboratory for Computer MA 02139 USA, E-mail: by Air Force contract DARPA contracts NooO14- NEC Research 
Institute, 4 Independence NJ 08540 USA, Email: satish@research.nj Permission to copy without fee all 
or part of granted provided that the copies are not made direct commercial advantage, the ACM copyright 
Way, Princeton, .nec.com. this material is or distributed for notica and the title of the publication 
and its date that copying is by permission of the Machinery. To copy otherwise, or and/or specific permission. 
ACM-SPAA 93-6/931Velen, Germany. @ 7993 ACM 0-8979 J-~99-2J93/0006 appear, and notice is given Association 
for Computing to republish, requires a fee /0300 . ..+J .5(3 Research Institute* a @(h + log log p) communication 
step randomized al­gorithm that realizes an arbitrary h-relation on a p­ processor OCPC. We show that 
if h < log p then the failure probability can be made as small as p- for any positive constant a. 1. 
Introduction The p-processor Completely Connected Optical Com­munication Parallel Computer (p-OCPC) consists 
of p processors, each of which haa its own local memory. The p processors can perform local computations 
and can communicate with each other by message passing. A computation on this computer consists of a 
sequence of communication steps. During each communication step each processor can perform some local 
computation and then send one message to any other processor. If a given processor is sent one message 
during a communication step then it receives this message successfully, but if it is sent more than one 
message then the transmissions are garbled and it does not receive any of the messages. The OCPC was 
first introduced as a model of com­putation by Anderson and Miller [AM 88], who called this model the 
Local Memory PRAM. Since then it has been studied by Valiant [Val 90] (who called the model the Y PRAM), 
by Ger6b-Graus and Tsantilas [GT 92], and by Gerbessiotis and Valiant [GV 92] (who also called the model 
the S* PRAM). A related model has been studied by Rao [Rao 92]. The feasibility of the OCPC from a engineering 
point of view is discussed in [AM 88, GT 92, and in [McC 92]. In this paper we cessor communication we 
study the problem relation (see [Val 90]) which each processor wishes to send to other Rao 92]. See also 
the references study the problem of interpro­ on an OCPC. In particular, of realizing h-relations. An 
h­is a communication problem in has up to h messages that it processors (assumed distinct). 300 The 
destinations of these messages can be arbitrary ex-in @(h + log p) expected communication steps. cept 
that each processor is the destination of at most h messages. The goal is to design a fast p-OCPC algo­rithm 
that can realize an arbitrary h-relation. Ander­son and Miller [AM 88] have observed that an h rela­tion 
can easily be realized in h communication steps if all of the processors are given toial information 
about the h-relation to be realized. (To see this, model the communications between the p processors 
viewed as sources, and the p processors viewed as destinations, as the edges of a bipartite graph of 
order 2p. Since the graph has maximum degree h, it is edge colorable with h colors, which can be interpreted 
as time steps.) A more interesting (and perhaps more realistic) situ­ation arises if we assume that initially 
each processor only knows about the messages that it wants to send and the processors learn about the 
h-relation only by receiving messages from other processors. This is the usual assumption, and the one 
that will be made here. An OCPC algorithm for realizing h-relations is said to be direct if it has the 
property that the only messages that are exchanged by the processors are the original messages of the 
h-relation and these messages are sent only to their destinations. In this paper we prove the following: 
 1. The expected number of communication steps taken by any direct algorithm for realizing h­relations 
on a p-OCPC is Q(h + log p). 2. An arbitrary h-relation can be realized on a p-OCPC in @(h+ log log 
p) communication steps. (Valiant has shown that an arbitrary h-relation can be realized in @(h+ logp) 
communication steps. In this paper we describe a t3(h + log log p) com­munication step randomized algorithm 
that realizes an arbitrary h-relation on a p-OCPC and we show that if h < log p then the failure probability 
can be made as small as p a for any positive constant a.)  It is easy to see that any l-relation can 
be realized in one communication step on an OCPC. Anderson and Miller [AM 88] were the first to consider 
the problem of realizing h-relations for h > 1. They discovered a direct P-OCPC algorithm that runs for 
~(h) commu­nication steps and delivers most of the messages in an arbitrary h-relation. In particular, 
the expected num­ber of messages remaining after Anderson and Miller s algorithm is run is O(p). Anderson 
and Miller were interested in the special class of h-relations in which each of the messages with a given 
destination has a unique label -? in the range 1 < 1 < h. For this class of h-relations Anderson and 
Miller also discov­ered a deterministic @(h + log ~) communication step algorithm that delivers all of 
the messages in any h­relation that cent ains only O(p) messages. Thus, their algorithms can be combined 
to obtain an algorithm that realizes an arbitrary h-relation from their special class Valiant [Val 90] 
considered the general problem of realizing h-relations for h > 1. He discovered a @(h + logp) expected 
communication step p-OCPC al­gorithm that realizes an arbitrary h-relation. Valiant s algorithm consists 
of the first phase of Anderson and Miller s algorithm followed by a second phase which re­distributes 
the remaining O(p) messages using parallel prefix, sorts them, and then sends them to the correct destinations. 
The second phase of Valiant s algorithm takes @(h + log p) communication steps. Prior to this work, Valiant 
s algorithm was the fastest known OCPC algorithm that can realize an ar­bitrary h-relation for h > 1. 
It is not direct, how­ever. The fastest known direct OCPC algorithm for re­alizing arbitrary h-relations 
is due to Ger6b-Graus and Tsantilas [GT 92] and runs in @(h+ logp log log p) ex­pected communication 
steps. In this paper we show that every direct algorithm for realizing h-relations takes Q(h + logp) 
expected communication steps. Further­more, we describe a @(h + log log p) communication step p-OCPC 
algorithm that can realize an arbitrary h-relation and we show that if h s logp then the fail­ure probability 
can be made as small as p-a for any positive constant CY. (The @ notation does not hide any large constants 
in the running time of our algorithm.) In order to motivate both our lower bound and our algorithm consider 
the following experiment. Suppose that two processors Pi and Pj are both trying to send messages to a 
third processor pd and that they adopt the following direct strategy. During each communica­tion step 
processors Pi and Pj both flip fair coins. If Pi s coin comes up heads then Pi sends its message to pd. 
Similarly, if Pj s coin comes UP heads then Pj sends its message to pd. On any given communication step 
pd has probability + of successfully receiving a message. Therefore the probability y that pd has not 
re­ ceived any messages after t communication steps is 2-~. Now suppose that we use a similar strategy 
to realize a 2-relation in which each processor is the destination of two messages. After t communication 
steps we will expect to have p 2-t processors that have received no messages at all. Therefore it will 
take fl(log p) commu­ nication steps to realize the 2-relation. Intuitively, the reason that so much 
time is needed is that the events are too independent . In particu­lar, the fact that most of the other 
messages are al­ready delivered will not make it easier for Pi and f j to send their messages to Pd. 
In order to obtain a sub­logarithmic algorithm we adopt the following strategy. We divide the set of 
p destinations into disjoint target groups . During the first part of our algorithm we send each message 
in the h-relation to a randomly chosen processor within the target group containing its desti­nation. 
As more and more messages are delivered to a given target group the probability that any remain­ing message 
is successfully delivered to the group in one communication step increases. Once all of the messages 
have been delivered to their target groups we solve the smaller problem of realizing an h-relation within 
each target group. The algorithm consists of four procedures. The first three procedures deliver the 
messages to their tar- get groups and the last procedure realizes smaller h­relations within the target 
groups. The methods that we use to deliver messages to target groups rely upon the fact that the number 
of messages being sent to each group is small compared to the size of the group. The first procedure 
of our algo­rithm (the thinning procedure) establishes this condi­tion by delivering most of the messages 
in the h-relation to their final destinations. The thinning procedure is a direct OCPC algorithm and 
it is based on Anderson and Miller s algorithm. Proving that it satisfies the appro­priate conditions 
requires a probabilistic analysis of de­pendent events. To do the analysis we use the method of bounded 
differences [McD 89, Bol 88]. After the thinning procedure has terminated the number of messages remaining 
will be O(p/h log log p) with high probability. The purpose of the second pro­cedure (the spreading procedure) 
is to re-distribute these messages so that each sender has at most 1 mes­sage to send. After the spreading 
procedure terminates the third procedure delivers the remaining messages to their target groups. The 
bulk of the messages are deliv­ered using a probabilistic tool called approximate com­paction . After 
the approximate compaction terminates the number of messages that have not been delivered to their target 
groups will be O(p/ log2 p) with high proba­ bility. Each remaining message is copied log p times and 
the processors are re-allocated so that log p processors can work together to send each message to its 
target group. (The approximate compaction technique and the copying technique were first used in PRAM 
alg~ rithms such as those described in [GM 91] and [MV 91]. In this work we require a smaller failure 
probability for approximate compaction than previous authors because our target groups are only polylogarithmic 
in size.) At the end of the third procedure the communi­cation problem that remains consists of one h-relation 
within each target group. These h-relations could be realized in @(h + log log p) communication steps 
by si­multaneously running the second phase of Valiant s al­gorithm within each target group, substituting 
a deter­ministic EREW sorting algorithm such as Cole s paral­lel merge sort (see [Col 88]) for the randomized 
sorting algorithm that Valiant uses. Our fourth procedure is an alternative algorithm for realizing the 
h-relations within the target groups. It does not rely on efficient deterministic O(logp) -time EREW 
sorting and it is therefore likely to be faster in practice. The algorithm is aa follows. Each tar­get 
group is sub-divided into disjoint sub-groups. Our thinning , spreading , and deliver to target group 
procedures are run simultaneously in each target group to deliver the messages in that group to the appropri­ate 
sub-groups. The communication problem remain­ing with each sub-group is an h-relation and this h­relation 
is realized using the second phase of Valiant s algorithm in which the sorting is done by Bitonic sort. 
With high probability the proportion of target groups for which this strategy delivers all of the messages 
is at least 1 1/ loge p for a sufficiently large con­stant c. The processors from these target groups 
are then re-allocated and used to help the unsuccessful target groups finish realizing their h-relations. 
After the processors are re-allocated each unsuccessful target group sorts its messages using an enumeration 
sort due to Muller and Preparata [MP 75] which is fact in prac­tice as well as in theory. The sorted 
messages are then delivered to their destinations. The structure of this paper is as follows. In sec­tion 
2 we describe the algorithm in detail. We demon­strate that it uses @(h + log log p) communication steps 
and we prove that if h < log p then the probability that any messages are left undelivered can be made 
as small as p-ff for any positive constant a. Section 3 contains the proof of the lower bound for direct 
algorithms. 2. The Algorithm Before we can define the algorithm we must describe the partition of the 
set of p processors into disjoint target groups . The size of each target group will be a polyno­mial 
in log(p). To be precise, let c1 denote a sufficiently large integer (the size of c1 will depend upon 
the fail­ure probability that we wish to obtain) and let k denote [log p] . We will divide the p processors 
into approxi­mately p/k target groups, each of size about k. To sim­plify the presentation we will assume 
that k divides p and we will define the tth target group, for Y in the range O < 1 < n/k, to be the set 
{P~L, . . .,l%+k-l}. (The case in which k does not divide p presents no real difficulty. In this case 
the target groups should be de­ fined in such a way that all but one of the groups has size k and the 
size of the remaining group is between k and 2k. ) We will define the target group of any given message 
to be the target group containing the destina­tion of the message and we will say that the message is 
destined for that target group. The algorithm consists of the following four proce­dures: Thinning. At 
the beginning of the algorithm the number of messages destined for any given tar­get group may be as 
high as hlc. The goal of the thinning procedure is to deliver most of the messages to their final destinations 
so that by the end of the procedure the number of undelivered messages destined for any given target 
group is at most k/h [C2 log log pl for a sufficiently large con­stant C2. If h ~ log p then this can 
be done in @(h + log(h) log log log(p)) steps with probability at least 1 p-a where the constant in 
the running time depends upon CYand C2. Spreading. At the end of the thinning proce­dure there will only 
be O(p/h log log p) undeliv­ered messages. However, some senders may have as many as h undelivered messages. 
The spread­ing procedure spreads these out so that each sender has at most one to send. This can be done 
in @(h + log logp) communication steps with proba­ bility at least 1 p-o where the constant in the running 
time depends upon CY. Deliver to Target Groups. This procedure de­livers all of the undelivered messages 
to their tar­get groups. After it terminates each sender will have at most 2 undelivered messages to 
send and the destination of each undelivered message will be within the target group cent aining its 
sender. The procedure can be implemented in @(log log p) com­ munication steps with probability at least 
1 p-a where the constant in the running time depends upon a. Deliver within Target Groups. This proce­dure 
delivers all mess;ges to th;ir final d&#38;tina­tions. It can be implemented deterministically in ~(h 
+ log logp) steps by running the second phase of Valiant s algorithm twice in each target group. However 
this implementation may be slow in prac­tice. In section 2.4 we describe an alternate im­plementation 
which runs in ~(h + log logp) com­munication steps and succeeds with probability at least 1 p-a. (The 
constant in the running time depends upon a.) We will use the following tool in the implementa­tion of 
our algorithm. (For similar tools see [GM 91] and [MV 91].) Definition 1. The (s, /3, A) approximate 
compaction problem is defined w follows. Given a P-OCPC in which at most s senders each have one message 
to send,  a set of ~s receivers which is known to all of the senders,  deliver all but up to A of the 
messages to the set of re­ ceivers in such a way that each receiver receives at most one message. (During 
the delivery messages may only be sent from the original senders to the ~s receivers.) Lemma 1. For any 
positive constant a there is a positive constant Cz such that the (s, [C2 log logpl, A) approximate compaction 
problem can be solved in O(log logp) communication steps with failure probabil­ ity at most a-~+ s &#38;(A+lJ 
. (In fact there is a positive constant C2 such that the (s, C2,A) approximate compaction problem can 
be solved in O(log logs) communication steps with small failure probability but lemma 1 is sufficient 
for our pur­poses.) Using the (s, P, A) approximate compaction algo­rithm we can accomplish a variety 
of tasks. For example (following Gil and Matias) we use the algorithm to allo­cate [log p] processors 
to each message once the number of undelivered messages is reduced to p/ [logp] 2. We use the following 
definition in the proof of lemma 1. Definition 2. The (s, /?, A) approximate collection problem is defined 
to be the same as the (s, ~, A) ap­proximate compaction problem except that we remove the requirement 
that each receiver receives at most one message. Lemma 2. For any positive constant a there is a pos­ 
itive constant CL such that the (s, 36, A) approximate collection problem can be solved in at most [c; 
log log pl communication steps with fw lure probability at most ~ ~ + S CY(A+l) . Proof of Lemma 1. Let 
Q be any positive constant and let C2 = 36c~ + 1, where c; is the constant as­sociated with a in Lemma 
2. Suppose that we are given an instance of the (s, [cz log log pl, A) approxi­mate compaction problem. 
Partition the set of receivers into [cL log log pl disjoint sets RI, R2, . . . . each of size at least 
36s. Since the (s, 36, A) approximate collec­tion problem can be solved in at most [c\ log log pl communication 
steps with failure probability at most ~-@ + S-a(A+l) , there is an algorithm with this fail­ure probability 
that delivers all but up to A of the messages to the receivers in RI in only [cL log log p~ steps. To 
solve the (s, [C2 log logpl, A) approximate compaction problem simply run this algorithm substi­tuting 
the set R~ for RI on the ith communication step of the algorithm. 0 Proof of Lemma 2. We say a sender 
is active ini­tially if it contains a message. Our algorithm proceeds in a number of similar communication 
steps, where in step i each active sender sends its message to a random location in the set of receivers. 
Each sender that suc­cessfully transmitted a message is considered inactive. Let m denote 36s. We must 
show that there are at most A active messages when the algorithm terminates. We use the following claim. 
Claim. Let c be a positive integer. If there at most m/r active senders left at step i, then the probability 
that there will be f = max{ [m/r312], A + 1} or more active senders left at step i + 2C is at most (2e/fi 
)Cf . We prove the claim by imagining that in a certain step the m/r active senders make their random 
choice of destination in some fixed order. For there to be f ac­tive senders that do not transmit their 
message, there must be [~/21 times at which a sender chooses the same receiver as one chosen by a previous 
sender in this or­der. The probability of choosing the same receiver as a previous sender is at most 
(m/r)/m = I/r. Thus, the probability of [.f/21 such events occurring is bounded above by (y;-)(:) s(+y 
2 (Y 2 2em ) f 2 Lr2max{[m/r3/21, A+ 1}) f/2 2em < ( r 2(m/? 3/2) ) ze j12 < () T We proceed by computing 
the probability that f active senders remain after 2C steps. It is easy to ver­ify that the probability 
that ~ senders remain active af­ter 2C steps in our algorithm is less than the probability that f senders 
remain active if each of the 2C successive steps is implemented by sending from all the processors that 
were active at the initial step. In this situation, the successive steps are independent thus the probabil­ity 
that there are f senders that never got a message through on any of the steps is at most the probability 
above raised to the 2Cth power. This proves the claim. Now we define r. = 36, = r~!~ , = max rj fj {(rn/r~~21, 
A+ I}, and t = min{j : ~j = A+ I}. The algorithm will run for t + 1 supersteps 0,1,.. ., t, each superstep 
consisting of 2C steps as described above, with c a constant to be chosen later, Observe that the number 
of supersteps, and hence the total number of steps, is O(log logs) and is therefore O(log logp). We say 
that superstep j is successful if, start­ing with at most m/rj active senders, it finishes with (strictly) 
fewer than ~j active senders. Note that if SU­persteps O, 1, . . . , j are all successful, then the number 
of active senders remaining at the end of superstep j is strictly less than fj . If all t + 1 supersteps 
are success­ful then the number of active senders remaining at the end is at most A, as required. Using 
the claim, we can bound the probability that some superstep fails by Notice that each term where rj < 
m~13 is at most (e/3) 6cfi, and every other term is at most (16e6/9s)CIA+lJi6. Thus the probability that 
some su­ perstep fails is at most e 6CG+( ~s (t+ Q{(j) )c(A+ ) 6}- Observe that t+ 1 = O(log logs) so 
if c is chosen to be big enough relative to CY this is at most ~-d + S-Q(A+l) as required. 0 We proceed 
by describing the implementation of the various steps of the algorithm. 2.1 Thinning The thinning procedure 
is a direct OCPC algo­rithm which is based on Anderson and Miller s algo­rithm [AM 88]. It consists of 
O(log h) phases. Intu­itively, the goal of the ith phase is to reduce the problem of realizing a h/2;-1-relation 
to the problem of realiz­ing a h/2i -relation. That is, the ith phase should get so many of the messages 
delivered that the remaining communicant ion problem is essent i all y a h/2i -relat ion. After the last 
phase the h-relation will be mostly real­ized except that there will be small number (at most k/h [C2 
log log pl ) of Undelivered messages destined for each target group. Let C3 be a sufficiently large constant 
(de­pending on c1 and C2 and the constant cr in the desired failure probability) and let ti denote C3[h/2i 
-1 + log h + log log logpl . (-ti denotes the num­ber of communication steps in phase i.) Before phase 
i it will be the case that each participating sender has at most h/2i -1 undelivered messages to send. 
During phase i each participating sender executes the following communication step t;times. Choose an 
integer j uniformly at random from the set {1, .,h/2;-1} If there are at least j undelivered msgs. to 
be sent Send the j th undelivered msg. to its destination After each communication step there is an 
acknowledg­ment step in which every receiver that receives a mes­sage sends an acknowledgment back to 
the sender indi­cating that the message was delivered successfully. At the end of phase i any sender 
that has more than h/2i undelivered messages left to send stops participating. We will prove the following 
theorem. Theorem 1. Suppose that h s log p. Then with probability at least I p-a the number of undeliv­ered 
messages destined for any given target group is at most k/h [C2 log log pl after the t~inning procedure 
terminates. In order to prove theorem 1 we will use the fol­lowing notation. We will say that a given 
message is participating at any point in time if it is undelivered at that time and its sender is participating. 
We will say that a receiver is overloaded in phase i if at the start of phase i the number of participating 
messages with that destination is more than h/2i -1. We will say that the receiver becomes overloaded 
in phase i if it is not overloaded in phases 1 through i but it is over­loaded in phase i + 1. We will 
say that a sender is good in phase i if it does not have a message to send to an overloaded receiver. 
For every target group T let S(T) denote the set containing all senders in the h­relation with messages 
destined for T and let IV(T) de­note the set containing all destinations of messages from processors 
in S(T). Finally, let S(IV(T)) be the set con­taining all senders with messages destined for members 
of IV(T). (Note that IS(T) I < h ITI, IIV(T)I ~ h21Tl, and \S(lV(Z )) I < h3 ITI. ) The theorem follows 
from the following lemma. Lemma 3. Suppose that h s log p. Let i bean arbi­trary phase of the thinning 
procedure and let T be any target group. With probability at least 1 p fa+l) 1. At most lN(T)l/h6 [C2 
log logpl receivers in IV(T) become overloaded in phase i 2. At most IS(T) I/hs [C2 log log PI good 
senders in S(T) stop participating at the end of phase i.  Proof of Theorem 1. To see that the theorem 
follows from lemma 3 note that the number of tar­get groups is at most p/k and the number of phases is 
O(log h) so with probability y at least 1 p-o (l.) and (2.) hold for all phases i and target groups 
T. Suppose that this is the case and consider any particular target group T. A message that is destined 
for 2 will be delivered by the thinning procedure unless either (1) there is a phase in which its sender 
is not good (in which case the sender could possibly stop participating) or (2] its sender stops participating 
even though it is good. The number of messages that are destined for T and are not delivered is therefore 
at most log(h) X ( )i2 \lV(T)\/h6 [c2 log logpl + hlS(T)l/h6 [C2 log logpl ) . This i~ at most lc/h 
[ca log log pl 0 The proof of lemma 3 will use the following in­dependent bounded differences inequality 
of McDi­armid [McD 89]. (The inequality is a development of the Azuma martingale inequality ; a similar 
formula­ tion was also derived by Bollobis as [Bol 88].) Theorem 2. [McDiarmid] Let Zl, . . . . Zn be 
inde­pendent random variables, with xi taking values in a set Ai for each i. Suppose that the (measurable) 
func­ tion f : ~ Ai ~ R satisfies \f(z) f(z?) \ ~ ci when­ ever the vectors Z and 3? differ only in 
the i th coor­dinate. Let Y be the random variable f(zl, . . . . Zn). Then for any t >0, Pr (IY E(Y) 
I ~ t) < 2exp ( 2t2/~~=1 c;). Proof of Lemma 3. Suppose that h < logp, let i be an arbitrary phase of 
the thinning procedure, and let T be any target group. Let xj denote the sequence of in­tegers randomly 
chosen by processor Pj during phase i. We will start by proving that with probability at least 1 p-(&#38;+2) 
at most liV(T)l/h6 ~c2 log logpl re­ceivers in IV(T) become overloaded in phase i. Let Y = ~({~j \ Pi 
G S(IV(T))}) be the number of receivers in IV(T) that become overloaded during phase i. Let R be any 
receiver in iV(Z ) that is not overloaded in phases 1 through i and let sj denote the number of participating 
messages that are destined for R at the j th communication step of phase i. (Note that these messages 
are not necessarily sent on the j th communication step. ) The probabiiit y that R receives a message 
on this step is sj (2 - /h) (1 -2i- /h)S - . There is a positive constant p such that this probabil­ity 
is greater than or equal to p for every Sj that is greater than or equal to h/2i. (Note that R cannot 
be­come overloaded in phase i if sj is ever less than h/2i.) Therefore, the probability that R becomes 
overloaded is at most h/2 -l .. . Furthermore, as long as C3 is sufficiently large (i.e., t~ is sufficiently 
large compared to j ) there is a con­stant C4 > 1 such that the above sum is at most c; t . Therefore 
the expected number of processors in IV(T) that become overloaded in phase i is at most lV(Z )c~~ which 
is at most [IV(T) I/2h6 [C2 log log pl as long as C3 is sufficiently large. If the value of XJ changes 
for any j then Y changes by at most h. Therefore, by the bounded differences in­equality of themern 2, 
the probability y that Y is greater than [IV(T) I/hG [CZlog logpl is at most 2exp( 2 lIV(T)12/4 h12 [C2 
loglogp12 IS(IV(T))I h2). This is at most p-fa+2j as long as the constant c1 is sufficiently large (i.e., 
the target groups are sufficiently large). (Here we use the fact that h ~ logp.) We now prove that with 
probability at least 1-p-(a+2J at most IS(T) \/h6 (cZ log Iogp] good senders in S(T) stop participating 
at the end of phase i. Let Y = ~({~j [ Pi E S(N(T))}) be the number of good senders in S(T) that stop 
participating at the end of phase i. Let S be any good sender in S(T) that participates in phase i and 
let sj denote the number of participating messages that S haa to send at the j th communication step 
of phase i. Let dl,j denote the number of partic­ipating messages at the j th communication step that 
have the same destination as the tth message that S has to send. (Since S is good each dl,j is less than 
or equal to h/2i -1.) The probability that S sends a message successfully on the j th communication step 
is X~Ll(2i- /h) (1 -2 -l/h)d -l , As before, there is a positive constant p such that this probability 
is greater than or equal to p for every sj that is greater than or equal to h/2i. Therefore, the probability 
that S stops participating is at most 2 -1 timj/(1 j=O -p) -~. As in the proof of the first part of 
the lemma, we conclude that the expected number of good senders in S(T) that stop participating at the 
end of phase i is at most IS(T) [/2h6 [C2 log log p] . If the value of Zj changes for any j then Y changes 
by at most h2. Therefore, by the bounded differ­ences inequality of theorem 2, the probability that Y 
is greater than IS(T) l/h6 (C2 log logp] is at most 2exp( 2 lS(T)12/4 h. 2 [ca loglogplz lS(lV(T))l h4) 
This is at most p-ta+2) as long as the constant c1 is sufficiently large (i.e., the target groups are 
sufficiently large). (Once again, we use the fact that h < Iogp.) 0 2.2 Spreading Let a be any positive 
constant and let C2 be the con­stant associated with a that is defined in lemma 1. At the end of the 
thinning procedure there will be at most p/h [C2 log log P1 undelivered messages. We wish to spread these 
out so that each sender has at most one to send. To do this we observe that there are at most p/h (C2log 
log pl senders with unde­livered messages. Suppose (without loss of generality) that h divides p and 
partition the set of p receivers into h disjoint sets R1, . . . . Rh of size p/h. Perform a (p/h [c2 
log log P1, [c2 log log PI, O) approximate com­paction to send the first message from each sender to 
a unique processor in RI. (The probability that this will succeed is at least 1 _ ~-~plh[cz loglogpl 
-(p/hl_c2 loglogp])-a.) Finally, send the remaining messages to R2, . . . . Rh in @(h) communication 
steps with no contention using the following strategy. If the 1st message of sender i was sent to the 
j th cell of RI by the approximate com­paction then send the lth message of sender i to the jth cell 
of RI for 1 <1< h.  2.3 Deliver to Target Groups Let a be any positive constant and let cz be the con­stant 
associated with a that is defined in lemma 1. At the end of the spreading procedure each sender will 
have at most one undelivered message to send and each target group will have at most k/h [C2 log log 
pl undelivered messages to receive. Our goal is to deliver the messages to the target groups. After this 
procedure terminates each processor will have at most 2 undelivered messages to send and the destination 
of each undelivered message will be within the target group containing its sender. We have two methods 
for implementing this proce­dure in @(log log p) communication steps. The simpler method (which we describe 
here) involves making copies of messages but the other method does not. The simpler of the two methods 
consists of two phases. We first describe phase 1. Consider any tar­get group T. At the start of the 
procedure there are at most k/ [C2 log log pl senders each of which has one message to send to the target 
group. Let t denote Llogpj . We send all but up to k/12 of these messages to T in O(log log p) steps 
by doing a (k/ [C2 log Iogpl, [C2 log logpl, k/.E2) approximate com­paction. We can do this in parallel 
for each target group and the probability that it fails for any target group is at most ;(@-<~@lOg}Ogpl 
+ (k/[cz loglogpl)-@lL2+l)) which is sufficiently small as long as the constant c1 in the definition 
of k is sufficiently large. We will use the phrase completely undelivered to describe all messages that 
were undelivered before phase 1 and were not delivered to their target groups during phase 1. At the 
end of phase 1 each sender has at most one completely undelivered message to send, each member of each 
target group has received at most one message, and the number of completely undeliv­ ered messages is 
at most p/t2. Choose 1 disjoint sets RI, . . . . Rl of size ~/l?j from the set of p receivers and let 
Qj denote the set consisting of the j th receiver from each of RI,..., RI. Next, send all of the com­pletely 
undelivered messages to RI by performing a (P/~2, [C2 @ @Pl, O) approximate compaction. (This fails 
with probability at most a ~+ (p/12 )-a.) Fi­nally (for each j in parallel) the processors in Qi copy 
the message received at the j th receiver in RI (if there is one) to the other processors in Qj . (This 
takes ~(log logp) communication steps.) At this point each completely undelivered message is stored at 
each of the / processors in Qj (for some j ) and each processor stores at most one completely unde­livered 
message. The following communication step is now performed in parallel by all processors. If the i th 
processor in Qj has a completely undelivered message to send then it chooses an integer uniformly at 
random from the set {y I (1 s 7< k) and (Ymod? = i)} and it sends the message to the ~ th processor in 
its target group. The probability that the i th processor in Qj is unsuccessful is at most l/~ and this 
proba­bility is independent of the probability that the other processors in Qj succeed so the probability 
that there is a completely undelivered message that is not delivered at least once to its target group 
in this communication step is at most pi-z which is sufficiently small. For each j in parallel the processors 
in Qj per­form parallel prefix to select one of the delivered copies. They then send messages canceling 
any other copies that were delivered to their target group. This takes El(log log p) communication steps. 
Note that each pro­cessor receives at most 2 messages during the procedure one in phase 1 and one in 
phase 2.  2.4 Deliver within Target Groups When this procedure begins each sender has at most 2 undelivered 
messages to send and the destination of each undelivered message is within the target group contain­ing 
its sender. Our goal is to deliver all of the undeliv­ered messages. This procedure can be implemented 
deterministi­cally in @(h. + log log p) steps by running the second phase of Valiant s algorithm [Val 
90] twice within each target group. The algorithm within each target group is as follows. First we consider 
only one undelivered message per sender. These messages are sorted by des­tination in @(log log p) communication 
steps using an EREW sorting algorithm such as Cole s parallel merge sort [Col 88], (Valiant uses a randomized 
parallel sort­ing algorithm indead of using parallel merge sort. We cannot do that here because we want 
to be able to claim that (with high probability) the messages are success­fully (and quickly) sorted 
in all of our target groups.) Then the sorted messages are delivered to their destina­tions without contention 
in E)(h) communication steps. Next the process is repeated for the remaining undeliv­ered messages. 
In this section we describe an alternative imple­ mentation of the procedure. It does not rely on effi­ 
cient deterministic O(log p) -time EREW sorting and it is therefore likely to be faster in practice. 
 The main idea is as follows. We start by sub­dividing each target group into target sub-groups. We then 
run the thinning , spreading , and deliver to target group procedures within each target group to deliver 
the messages to their target sub-groups. If these three procedures succeed within a target group then 
each sender in the group will have at most 2 undelivered messages to send and the destination of each 
undeliv­ered message will be within the target sub-group of its sender. We can now run the second phase 
of Valiant s algorithm twice within each target sub-group to deliver the messages in the target group 
to their final destina­tions. Since the sub-groups are very small we can use Bitonic sort (which is fast 
in practice) to do the sorting. With high probability the proportion of target groups for which the thinning 
, spreading , or deliver to target group procedures fail will be 0(k-3). We now allocate a group of k2 
extra processors to each of these taiget groups and we use these extra processors to sort the messages 
using a counting sort that is fast in prac­tice as well as in theory. We now describe the procedure in 
more detail. The communication problem within each target group can be viewed as the problem of realizing 
an h-relation on a k-OCPC. Therefore we can run the thinning , spreading , and deliver to target group 
procedures simultaneously within each target group. Before we can do that we must partition each target 
group into tar­get sub-groups. Let the size of the target sub-groups be k = (log kl where C5 is a constant 
that is suf­ficient large that the probability that the thinning , spreading , and deliver to target 
group procedures fail within a target group is at most k-3. (In order to simplify the presentation in 
this section we will as­sume that k divides k. The case in which k does not divide k is no more difficult 
-it is simply messier. Similarly, we will assume that k3 divides p.) After the deliver to target group 
procedure terminates within each target group run the second phase of Valiant s algo­rithm twice within 
each target sub-group, using Bitonic sort to do the sorting. (This takes @(h + log2 k ) com­munication 
steps.) If the thinning , spreading , and deliver to target group procedures succeeded within a target 
group then all of its messages are now deli~ered. (This will happen with probability at least 1-k- .) 
We now describe the second part of the proce­dure the allocation of extra processors to help target 
groups that have not finished, Partition the set of tar­get groups into p/k2 disjoint sets S1, . . . 
. SP/k2 . Each set St is called a target super-group. Each target group and each target super-group performs 
a parallel pre­fix to determine whether or not it has finished. (This takes @(log logp) communication 
steps.) Next each processor that is part of an un-finished target group attempts to find a finished target 
super-group. In par­ticular, if the processor is the j th member of the target group then it chooses 
an integer 1?uniformly at random from the set {4 I (1 ~ 1~ p/k2) and (1mod k = j)} and it sends a message 
to the first processor in super­group Se asking whether St is finished. The proba­bility that the j th 
member of a given unfinished target group fails to find a finished super-group is at most 2/k2 (the probability 
that the super-group chosen is not fin­ ished is at most l/k2 and the probability y that the query is 
sent to the same destination as some other query is at most l/k2 ). Furthermore the queries from any 
given target group are independent of each other so the probability that every processor in a given unfi­ 
 nished target group fails to find a finished super-group is at most (2/k2)k and the probability y that 
there exists an unfinished target group that fails to find a finished super-group is at most p(2/k2)k 
which is sufficiently small, Each unfinished target group then performs a parallel prefix to choose a 
single finished super-group. At this point each un-finished target group has identified a single finished 
super-group containing k2 processors. Consider the kz processors to be organized in a k by k matrix. 
We now run Valiant s algorithm twice in each un-finished target group. The message are sorted using Muller 
and Preparata s algorithm [MP 75] which works as follows. The i th processor of the un­finished target 
group sends its message (if it has one) to all of the processors in the ith row. (This takes @(log log 
p) communication steps.) If the processor in the i th row of the ith column gets a message then it sends 
this message to all of the processors in the i th column and the processors in the i th column perform 
parallel prefix to determine its rank. (Again, this takes @(log Iogp) communication steps.) Finally (in 
1 com­munication step) the message with rank i is sent to the ith processor in the un-finished target 
group. 3. A Lower Bound for Direct Algorithms The algorithm described in the previous section often sends 
a message to a processor other than its final des­tination, i.e., the algorithm is not direct. Using 
a non­direct strategy in a network that allows direct routing may seem strange at first, and one might 
question its necessity. In this section we prove a lower bound that demonstrates that any sublogarithmic 
algorithm must necessarily use non-direct routing. Theorem 3. Let A be any direct (randomized) OCPC algorithm 
that can realize any 2-relation with success probability at least ~. Then there is a 2-relation which 
A takes Q (log p) communication steps to realize. Proof of Theorem 3. Consider any direct random­ized 
algorithm that runs for t ~ [~ log p] steps. We shall construct a 2-relation p such that the probability 
that the algorithm successfully realizes p is exponen­tially small (in p). In the 2-relation p, each 
processor has at most one message to send. Consider a processor Pi that is not itself the des­tination 
of any messages and has a single message to send to pd, but is blocked every time it attempts to transmit. 
Since Pi receives no external stimulus, we can imagine that Pi selects its transmission strategy at random 
in advance of the first time step. A strategy for P; to transmit to pd (under the blocking regime) can 
be coded as a binary word of length t,where a 1in position t indicates that Pi is to attempt to send 
its message at time step t . For convenience, assume that p is divisible by 4. The 2-relation p is the 
union of p/4 subrelations, each consisting of a pair of sending processors attempting to send a single 
message each to a common destination. The 3p/4 processors in the 2-relation are distinct. The p/4 subrelations 
will be selected sequentially. Note that at any stage there will be ~ ~ p/4 free processors from which 
the next pair of senders may be selected. To make the selection, first choose a free destination processor 
pd. Observe that, since the number of pos­sible transmission strategies is 2t , there must exist a strategy 
a E {O, 1}* such that the expected number of free senders that choose strategy o to send to pd un­der 
the blocking regime is at least f2-t. Thus there is a free sender, say Pi, that chooses strategy u with 
probability at least 2-t ~ p-113; and a different free sender, say Pj , that chooses a with probability 
y at least (f2-~ 1)/($ 1) >2- f- > p- l 4p-1, which is at least ~p-1i3 for p ~ 24. Now add to p 
the subrelation that requires Pi and Pj each to send a single message to pd. Note that P, and Pj select 
strategies indepen­dently, so the probability that they both select u is 213 thus the probability that 
Pi and at le~t ~P Pj fail to get rid of their messages is also at least 1 2/3 7P . Since there are p/4 
subrelations forming p, the probability that p is successfully realized is at most (1 ~p 2j3)rf4, which 
is less than exp( p113/8). It may be observed from the proof that a direct algorithm requires a logarithmic 
number of steps to achieve even inverse polynomial success probability. References <RefA>[AM 88] R. J. Anderson 
and G. L. Miller, Optical Commu­nication for Pointer Based Algorithms, Technical Report CRI 88-14, Computer 
Science Department, University of Southern California, Los Angeles, CA 90089-0782 USA, 1988. [Bol 88] 
B. Bollobiis, Martingales, Isoperimetric Inequalities and Random Graphs, in Combinatom cs (eds A. Ha­jnal, 
L. Lov6sz, and V. T. S6s), Colloq. Math. Sot. ldnos Bolyai 52 (North Holland 1988) 113-139. [Col 88] 
R. Cole, Parallel Merge Sort, SIAM Journal of Computing 17(4) (1988) 770-785. [GT 92] M. Ger6b-Graus 
and T. Tsantilas, Efficient Optical Communication in Parallel Computers, Proceedings of the ACM Symposium 
On Parallel Algorithms and Architectures 4 (1992) 41 48. [GV 92] A. V. Gerbessiotis and L. G. Valiant, 
Direct Bulk-Synchronous Parallel Algorithms, Proceedings of the Scandinavian Workshop on Algorithm The­ory 
3 (1992). [GM 91] J. Gil and Y. Matias, Fast Hashing on a PRAM, Proceedings of the ACM-SIAM Symposium 
On Dis­crete Algorithms 2 (1991) 271 280. [MV 91] Y. Matias and U. Vishkin, Converting High Proba­bility 
into Nearly-Constant Time with Applica­tions to Parallel Hashing, Proceedings 0$ the ACM Symposium On 
Theory of Computing 23 (1991) 307-316. [McC 92] W. F. McCO1l. General Purpose Parallel Comput­ing, pre-print 
(1992). [McD 89] C. McDiarmid, On the Method of Bounded Differences, Surveys in Combinatorics, London 
Math. Sot. Lecture Notes Series 141 (Cambridge Univ. Press, 1989) 148-188. [MP 75] D. E. Muller and F 
. P. Preparata, Bounds to Com­plexities of Networks for Sorting and for Switching, Journal of i%e ACM 
22 (1975) 195-201. [Rao 92] S. B. Rae, Properties of an Interconnection Archi­tecture Based on Wavelength 
Division Multiplex­ing, Technical Report TR-92-O09-3 O054-2, NEC Research Institute, 4 Independence Way, 
Prince­ton, NJ 08540 USA, 1992. [Val 90] L. G. Valiant, General Purpose Parallel Architec­tures, Chapter 
18 of Handbook of Theoretical Com­puter Science, Edited by J. van Leeuwen (Elsevier 1990) (see especially 
p. 967) </RefA>
			
