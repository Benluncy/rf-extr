
 Extended Abstract Mesh-face / A Model for Multi-Grained Parallelism t John E. Savage Brown University 
Department of Computer Science, Box 1910 Providence, Rhode Island 02912 Phone: (401) 863-7642, Fax: (401) 
863-7657 Email: jes@cs. brown. edu Abstract Multi-grained parallel computers can be very effective on 
computationally intensive problems that have important se­rial and parallel components. We introduce 
the Mesh Su­perHet, a model of this type consisting of the close coupling of a d-dimensional toroidal 
mesh of coarse-grained proces­sors to a serial machine consisting of memory modules con­nected via a 
low-diameter network to a serial processor. We exhibit problems for which the Mesh SuperHet is superior 
to its serial or parallel components alone and develop tight performance bounds for sorting, the fast 
Fourier transform, and matrix multiplication. As multi-grained machines be­come more common, studies 
such as this will both reveal the fundamental limitations on such architectures and set the context for 
algorithm development. Introduction In this article we explore multi-grained parallel architec­ tures 
consisting of closely coupled high-performance serial and parallel machines. We approach this topic believing 
that while many important problems exhibit high-degrees of parallelism, they also have important serisl 
components which, if run on one processor of a parallel array, will run so slowly as to nullify the effect 
of parallelism. Anecdotal evidence suggests that multi-grained architec­ tures can greatly reduce computation 
time for large prob­ lems. It is reported that McRae solved a chemicaJ process resource-allocation problem 
40 times faster than he could on a supercomputer alone [1] using a CRAY Y-MP con­ nected via a high-speed 
link to a CM-2 Connection Machine. Others report a factor of 5 to 10 reduction in elapsed time [7] for 
such architectures. This evidence offers further support for multi-grained computers. Our model for multi-grained 
parallelism is the Mesh SW perllet. It consists of closely coupled serial and parallel ma­ chines, the 
former having a supercomputer-style memory system and the latter being a p-processor, m-word/processor, 
d-dimensional toroidal mesh. (A 2-D version of the machine is shown in Figure 1.) The memory on the serial 
machine consists of random access memory modules of unlimited size (simulated via a memory hierarchy) 
that are quickly accessi­ ble from the serial front-end processor via a small-diameter network. Although 
we assume the front-end processor has the same speed as a mesh processor, we find that for sorting and 
FFT its speed has a small effect on performance. We assume a mesh for the parallel machine because it 
offers the tThlS work Was supported in part by the Office of Naval Research under contract NO O014-91-J-4052, 
ARPA Order 8225 and NSF Grant MIP-9020570 Permission to copy without fee all or part of this material 
is granted provided that the copies are not made or distributed for direct commercial advantage, the 
ACM copyright notice and the title of the publication and its date smear, and notice is given t 1 1 I 
t t ~ Routing , Logic I :( CPU ) Figure 1: The two-dimensional Mesh SuperHet computer high-speed local 
communication needed for many large-scale computational problems. The two machines are coupled by connecting 
each pro­cessor on a face of the mesh with an individual memory module of the serial machine using channels 
of the same bandwidth as the mesh interprocessor bandwidth, This cou­pling offers a moderately generous 
bandwidth between the machines but requires that data generated on the mesh be organized carefully for 
front-end storage to avoid the cre­ation of serial bottlenecks. Thus, the Mesh SuperHet is a realistic 
model that provides a reasonable mix of serial and parallel computation together with a reasonable amount 
of structured communication between the two machines, We study close coupling between the serial and 
paral­lel machines in order to understand the effect of coupling on the amount of speedup attainable 
with the parallel ma­chine. As we show, for some problems, such as sorting and the FFT, the amount of 
coupling limits speedup whereas for others, such as matrix multiplication, a full speedup is possible 
with limited coupling if the amount of memory per parallel processor is sufficiently large. The front-end 
memory modules could also be connected via channels to processors distributed uniformly over the mesh. 
The effect is only a small change in our results be­cause data on the mesh can be permuted to arbitrary 
loca­ tions in O(mpl /d) steps, comparable to the time to load and unload the mesh memory. We could also 
connect fewer than P l 1 /d mesh processors to memory modules or reduce the bandwidth of the interface 
channels, These changes cause the data movement time between machines to grow in pro­portion to the decrease 
in cumulative bandwidth. This di­rectly affects the time for sorting and the FFT but only indirectly 
affects the time to multiply matrices if processors have enough memory to compensate for the reduced 
band­width. We make four kinds of contributions. First, we explore variations in the Mesh SuperHet architecture 
and argue that the model is robust under changes in the interconnection between the mesh and the serial 
machine. We also establish conditions under which the serial processor can be simulated by the mesh processors 
with a small loss in execution time. Second, we exhibit problems that are more quickly solved on the 
Mesh SuperHet than on either its serial or parallel component alone. that copying is by permission of 
the tiation of CompGting Machinery. To copy otherwise, or to republish, requires a fee and/or specific 
permission. SPAA 94-6/94 Cape May, N.J, USA G 1994 ACM 0-89791-671 -9194100cr6..W.5O  330 Our third 
contribution is to develop lower bounds on computation time for sorting and the FFT when the num­ber 
of inputs, n, is large. We show that a speedup of at most O(p l lJdlog(mp)) is possible on a ~-processor, 
d­ dimensional, m-word/processor Mesh SuperHet for these two problems. Fourth, we develop fast algorithms 
forrePre­sentative problems on this machine. We give algorithms for sorting and the FFT whose running 
time matches asymp­totically that of the lower bounds. We also examine matrix multiplication and show 
that a full speedup is attainable if m satisfies fib > cp [dizl Id for a constant c, where ~ is the number 
of mesh processors connected to front-end memo­ries. Our results are derived under the assumption that 
the problems are too large to store on the mesh. Data is stored on the front-end before and after a computation. 
 1.1 Related Computational Models The Mesh SuperHet model is a generalization of the model of Atallah 
and Tsay [2] and that of Dehne, Fabri and Rau-Chaplin [3]. Atallah and Tsay connect a serial front end 
to a p-processor, m-word/processor, d-dimensional, mesh for rJI-lld (thewhich m is constant. Thev . assume 
that at most \ number of processors on a mesh face) words can be trans­ferred between the mesh and the 
serial machine per unit time without any restrictions on how the data is moved. I-I /d log p) is possible 
for They show that a speedup of O(p sorting-related problems in computational geometry. Dehne, Fabri 
and Rau-Chaplin [3] consider a machine consisting of a d-dimensional. wrxocessor mesh in which the mem­ 
,.. ory is large enough to hold the data for an entire problem. When a sorting problem fits entirely 
on a two-dimensional mesh, thev show that a full sDeedutr is Dossible if m satisfies Like Dehne et al., 
we assume that each processor on the mesh can have a large amount of memory. Like Atallah and Tsay, we 
assume the problem is so large that it can­not fit entirely on the processor array. We severely restrict 
the Atallah-Tsay model by forcing data to move through individual channels bet we en machines. Related 
to this work is that of Kung [6] who has ex­plored conditions under which the computation and 1/0 time 
on serial machines are balanced for a number of prob­lems, such matrix multiplication, Gaussian elimination, 
the FFT, and sorting; he also comments on these issues for two­dimensional meshes. Finally, it should 
be noted that the Mesh SuperHet has much in common with the recently announced Cray Re­search, Inc. T3D 
machine. It has a supercomputer front-end and a three-dimensional mesh of fast processors with fast lo­cal 
communications. It differs from our model in that mesh processors are connected (in groups of 64) to 
the front-end via 1/0 channels, not directly to front-end memory modules. 2 Testing the Limits of the 
Mesh SuperHet In this section we consider two questions: Is the front-end processor always needed? and 
Are there problems solved more quickly on a Mesh SuperHet than on either the serial front end or the 
parallel computer alone? Figure 2: Computational graph computed more efficiently on the Mesh SuperHet 
than on the serial front end or parallel machine alone. 2.1 The Importance of the Serial Front End We 
note that although the large memory of the serial ma­chine is used heavily in the algorithms developed 
in Sec­tions 3, 4 and 5, the serial front-end processor is used only sparingly. This causes us to ask 
if this processor is always necessary. In practice it is necessary because, as explained earlier, many 
important computational problems have an important serial component. However, as the following the­orem 
demonstrates, the work of the serial processor can done by the parallel processors as long as the serial 
processor is used infrequently. Theorem 1 Let T.e,,aI be the number of steps executed by the serial front 
end of a d-dimensional, p-processor Mesh SuperHet in a T-step computation. Ij T.erta/ = o(T/pl d), the 
computation can be done without the front-end processor in O(T) steps. Proof We sketch a proof. The serial 
processor can access any front-end memory module in one unit of time. Thus, moving data between modules 
can be done quickly. Mesh processors connected to such modules can simulate the serial processor at the 
expense of mov­ ing data along a mesh face. Since the time to cross the face is no more than O(P1 /d), 
the result follows. 0  2.2 A Problem Better Solved on the Mesh SuperHet Figure 2 exhibits a graph that 
we show is better solved on the 2-D p-processor Mesh SuperHet than on either its se­rial component (the 
mesh is not used) or parallel component (front-end memory modules are connected to mesh-face pro­cessors 
but the front-end processor is disabled). Each circle vertex in the graph is associated with a unique 
variable. Each one-input solid vertex has two inputs, the output of its predecessor in the graph and 
the value of a unique variable not shown. Finally, each binary vertex has three inputs, outputs of the 
two graph vertices to which it is connected plus a unique variable, also not shown. This graph has p 
input variables and W output vertices. We assume that inrmt variables associated with the ith column 
are stored in tie jth memory module (See Figure-l). The graph can be computed level by level in @(fi) 
steps using the ~ mesh-face processors to produce column values and the serial processor to fetch remote 
values. However, if the rapid access to the front-end memory modules provided by the serial processor 
is not available, it must be simulated 331 A/ Ax3:x Jf 2_ 2_3> 2>3 4$----- X1: Xl:x + x3:x Xl:x %?iiiiht 
(a) (b) Figure 3: (a) Several levels of a parallel decision tree; (b) ~pl id levels of a large parallel 
decision tree. by mesh processors, introducing a factor of approximately @ in computation time for each 
parallel step, making the computation time comparable to the serial time. This argument fails when logically 
adjacent columns are stored in adjacent memory modules. A related graph with the same properties has 
the same structure except that a pa­rameter c,, unknown when columns are assigned to memory modules, 
specifies the third input to the vertex in row i col­umn t to be the value of the vertex in column c, 
of row z 1. This computation can be done in time O(@ on the Mesh SuperHet but only in time Q(p) on either 
submachine alone, whether column variables are stored in the same module or not. 3 Sorting on the Mesh 
SuperHet We derive a lower on the time to sort with comparison­based sorting algorithms on the Mesh SuperHet 
and sketch an asymptotically optimum sorting algorithm. 3.1 A Lower Bound on the Time to Sort Our lower 
bound on the time to sort uses the parallel deci­sion tree model of computation [5, pp. 185]. As suggested 
in Figure 3(a), a parallel decision tree performs multiple comparisons in each step and then executes 
one of several branches based on the outcomes. The leaves of a full par­allel decision tree to sort n 
words (see Figure 3(b)) identify the n! permutations of these words. We note that the paths through a 
parallel decision tree may not all have the same length and that several different paths may be associated 
with the same set of comparisons, although performed in different orders. While the following lower 
bound on sorting time is stated for the Mesh SuperHet, it applies to a more general class of machine, 
namely, those who~e front-end can do O(P1 1 /d log (rep)) comparisons per parallel machine step, whose 
parallel ma­chine has mp storage locations, and has a connecting channel that can pass words pi i/d per 
parallel step. Theorem z The number of steps, T..,t(rL, m,p, d), ~0 sort n elements, n > 4, on a p-processor, 
m-word[processor, d­ dimensional Mesh SuperHet satisfies the followzng anequal­ ity: n log n T...t(n, 
m,p, d) J 4(1 + 2pl-lldlog(2mp)) Proof We sketch a proof. Divide time up into intervals of rnpl ld steps. 
In any given interval let e be the number of words entering and 1 the number leaving the mesh. Clearly, 
e + 1< mp because at most p] 1 Id words can move across the mesh face in one step. Since at the beginning 
of the interval there are at most mp words in the array, at most e + 1 + mp < 2mp words can interact 
on the mesh during any interval. Thus, at most (2mp) ! permutations of these elements can be identified 
by the mesh during an interval. The serial processor can make at most one comparison on each step. Combining 
these facts we see that the number of distinguishable leaves of the subtree T in Figure 3(b\ is at most 
(2mo)!2mp1 d. Assigning a unique s~btree t o indistinguishable leaves of ~ we have at most   ((2mp)!2mp1/d)~sort( 
 m,m,~,d)/mp / leaves of the full tree which must be at least n! 0 This lower bound shows that a speedup 
of at most about 1 I /d log(mp) is possible. A full speedup of P k not achiev- P able because it takes 
time proportional to mpl d to move mp words on and off the mesh, which is comparable to the time to merge 
mp words on the mesh. It is interesting to note that, as shown by Dehne et al. [3], if all n words are 
uniformly distributed initially over the p processors in a 2-D mesh they can be sorted with a full speedup 
if m = 2n( fi). 3.2 An Asymptotically Optimal Sorting Algorithm In this section we generalize the Atallah-Tsay 
sorting algo­rithm [2] (see Figure 4) to the Mesh SuperHet. Our prin­cipal challenge is to manage data 
movement between the machines to avoid creating serial bottlenecks. The algorithm sorts a list of n = 
gk items by forming g = [(mP/3)1 lfd+l)l recursively sorted lists of n/q elements each. It finds the 
(mp/3)th smallest item in these lists and removes it and most smaller items from the lists. (The com­ 
plex details of this process are central to our extension of the Atallah-Tsay algorithm. ) This step 
is repeated with the next (rnp/3)th smallest element until all lists are ex­hausted. Identification, 
removal and sorting of these items is done through coordinated action by the serial front end and the 
mesh. Our extension to the Atallah-Tsay algorithm involves a careful implementation of the procedures 
Merge and Extract. We show that sublists can be merged in O(n/p - fd) steps which gives the following 
recurrence for T~~~t n, m, p, d), ( the number of steps used by our algorithm. sOTt(nmp )5gTs0+mp )+ 
0(+ Theorem 3 A set of n elements can be sorted on the p­processor, d-dimensional Mesh Super-Het with 
m words per mesh processor an TzO~~ n, m,p, d) steps where ( n log n T.o,,(n, m,p, d) = 0 pl-1/dlog(mp/3) 
+;lognz () The second term dominates when n = 2°(]0g(mp/3J 0g( m)/p ). Thus, a full speedup is possible 
when n = mk, k a con-Stantj and m = 2n(P d)/p. Otherwise the speedup as at most O(pl-l/d log(mp/3)). 
 332 Let g = ((rrzp/3)lftd+]Jl ; Sort(List ~ of n items) Mvidey into sublists {v),l < .I < g}; Recursively 
sort these into lists {WJ,l < j < q}; Merge sorted sublists {~jll < j < q}; Merge {wj, I<j <g} Until 
done Find (mp/3)th smallest item; Extract rep/3 smallest items from q lists; Shorten lists {wJ, 1 < j 
< 9} ; (update pointers) Move rep/3 smallest items to mesh; sort items on mesh; Move sorted items to 
front end;  Figure 4: A sketch of the Atallah-Tsay sorting algorithm, We now sketch a proof of this 
result. We begin by sketching an algorithm to sort a list of rep/3 items in time T,o,t((mp/3), m,p, d) 
= O(mp]ld + rnlog m) on the mesh without using the front-end. We first create p sorted lists of m/3 items 
by sorting them sequentially on each processor. They are then merged, We observe that a d-dimensional, 
p-processor mesh can merge p lists of w items per proces­ sor in O(wpl/d) steps, This can be shown by 
translating Batcher s bitonic merging algorithm from the hypercube to the mesh. (Ideas in [8] are used 
to map hypercube algo­rithms to meshes. ) We use the above procedure to sort 3n/mp lists of rep/3 items. 
Each unsorted list is moved from front-end memories to the mesh. sorted. and moved back. ImDose. a linear 
or­ deronthepl-lld mesh-face processors. Arrange the sorted items in each list into (rrzp/3)] d blocks 
of (mp/3)]-]id sorted items as follows: Items in the ith block are arranged in sorted order in the ith 
plane parallel to the mesh face, I-l/d items per processor. (The number of items Per (m/3) processor 
is all blocks is rrz/3.) The first plane is the mesh face itself. If necessary, route the rep/3 sorted 
items in O(rnp /d) steps (using Batcher s bitonic merging algorithm to route) to reach these final positions. 
In parallel, move blocks to the memories. This establishes the base case for the sorting algorithm. For 
the recurrence step, divide the n elements into q = [(mp/3)11(d+])l lists and sort them recursively. 
Using the efficient serial algorithm of Fredrickson and Johnson [4], the mp/3th element in these sorted 
lists is found in O(p] ld log mp) steps on the serial front-end. The Atallah-Tsay algorithm then removes 
the rep/3 smallest elements. We modify it to process only blocks, thereby avoiding memory fragmentation 
and serialization of the algorithm. The Extract procedure identifies blocks to be removed from the q 
sorted lists and merged. The mp/3th smallest element either exceeds all elements in these blocks or lies 
be­tween the largest and smallest items. There are at most mp items in these blocks and they can be identified 
in O(pl /d) steps (the first item in a block is in the first memory mod­ule, the last is in the last 
module). These blocks are moved to mesh planes. If some processor memory is not full, fill it with an 
item co larger than all other items. The first rep/3 smallest items are found by broadcast­ing on the 
mesh the (rep/3 )rd element to all processors in time O(pl d). Processors scan their elements serially 
and determine where this element fails. In O(p /d) steps the identities of the first item larger than 
the (rrzp/3)rd smallest in each block are found and sent to the front-end proces­sor in O(P1 I d ) steps. 
Also, the blocks cent aining elements larger than the (mp/3)rd are written back to the front-end memories 
in 0( mpl d ) steps, replacing the items less than or equal to the mp/3th with m. This action allows 
the re­moval of these smaller items when searching for the next rep/3 smallest items. Now a list of the 
rep/3 smallest items is formed by aa­signing value m to larger items, merging lists on individual processors, 
and merging these lists on the mesh in O(mplid) steps. When the merge is complete, the rep/3 smallest 
items are routed so they are stored in linear order on plid mesh l-1/d/3 items per plane planes parallel 
tothe mesh face, mp in O(mp]ld) steps and then moved to front-end memories in O(mp]ld) additional steps. 
It follows that the first rep/3 items can be identified, removed from the set of q lists, and stored 
in the front-end memory in O(mplld) steps. To find the next rep/3 smallest items, the serial machine 
shortens itsg lists using the locations transferred to the front end in the previous phase and then repeats 
the previous phase a total of at most of 3n/mp times, each globaJ step requiring O(mp]ld) time. Thus, 
the total time to merge the qsorted lists is O(n/pl-l/d). 4 The Fast Fourier Transform Algorithm The 
fast Fourier transform (FFT) algorithm on n inputs in time O(nlogn) computes the discrete Fourier transform 
(DFT)on aserial computer when n=2 forintegerk. We now develop a Mesh SuperHet algorithm for it, assuming 
m and v are Dowers of 2. OuraJ~orithm uses thewell-known decomposition of the FFTinto subFFTgraphs shown 
in Figure 5 [10]. Let F( ) bethe FFTgraph on2 inputs. It can be decomposed into 2 - = m top subgraphs 
{F \,~)} on 2 =p inputs and 2 bottom subgraphs {I&#38;-r)} on2 -r inputs. Edges between boxes identify 
common vertices. Since the Mesh SuperHet mesh canstoremp entries, we first consider the mesh computation 
of F( ) for a such that 2 = mp. As necessary, we assume the existence of addi­tional temporary storage 
for local computations and data movement. The bottom subgraphs of Figure 5 are computedin par­cdlel in 
O(m log m) steps on the p processors using the serial algorithm. The top FFT subgraphs are assigned to 
proces­sors with the first m/p subgraphs assigned to the first of the p processors, the second m/p subgraphs 
assigned to the second, etc, Then, the m outputs produced by the bottom subgraphs are divided into consecutive 
groups of m/p outputs and moved to the processors computing the top subgraphs. This step, which amounts 
to a matrix trans­pose, can be done parallel in O(mplid) steps using a rout­ing algorithm. The top subgraphs 
are computed serially in O((m/p)(plogp)) = O(mlogp) steps. It follows that F(a) for 2 = mp can be computed 
on the mesh in O(m log(mp)) steps. 333 Figure 5: Decomposition of the FFT graph F[5) on 32 in­puts. The 
decomposition lemma is also applied to the full FFT graph Ft~) on n = 2k inputs. Decompose F (k) into 
2 = mp top FFT subgraphs and 2k = n/rep bottom FFT sub­graphs on 2 = mp inputs. Inputs to the ith top 
sub­graph are ith outputs of the bottom subgraphs. We now show how to transpose the outputs of bottom 
subgraphs {~~~) [ 1< j < n/rep} and store them so they are avail­able in parallel to the top subgraphs. 
Transposition of Data Block the mp outputs of an FFT subgraph into p blocks of m words, {[1, 2, . . . 
. m], [m+l, m+ 2, ..., 2rn],. ... [(l) rn+l, (p(l) m+2,2, pm]}pm]}. Reorga­ nize blocks so the qth block 
contains the entries [g, p+ g, 2p+ q,.. .,(m l)P+ g] for 1 < g < m. The first P1 ] d such blocks are 
stored in parallel in the front-end memories, one block per memory. The second set of pl 1 td blocks 
are also stored one per memory. The second block in each memory unit is a block from the second set of 
pl 1 d blocks. This process is repeated until all mp results from the first bot­tom FFT subgraph are 
stored. For the second bottom FFT subgraph permute the blocks cyclically right one place. For the lth 
block we permute the blocks cyclically right 1 places. The data is now available in parallel to top subgraphs. 
All of these operations can be done in O(mpl /d) steps. Using this transposition and the decomposition 
described above, we establish a recurrence that provides the following upper bound on the time to compute 
the FFT on the Mesh SuperHet. Theorem 4 The FFT algorathm on n znputs can be com­puted on the p-processor, 
d-dimensional Mesh SuperHet with m words per mesh processor in TF,nT(n, m, p, d) steps where rrlog n 
T~~T(n, m, P> ~) = o P1-lld log(mp) ( [ +-1) I-I fd Iog(mp)) shown by this ~gO-The speedup of O(p rithm 
when m = 2°tr d J/p is best possible, as we now sketch. Wu and Feng [1 I] show that three back-to-back 
FFT graphs realize a permutation network when FFT nodes are replaced by switches. Thus, any straight-line 
program (SLP) for the FI?T on the Mesh SuperHet requires at least one third the number of operations 
required by an SLP for a permuta­tion network. Consider such an SLP on this machine. Let it be computed 
in T cr~u~e steps. By analogy with the lower bound of Theorem 2 divide the TP.,~ute steps into mpl d 
equal-length intervals during which at most mp elements can cross the mesh face between the front-end 
and the mesh. It follows that at most 2mp elements can interact on the mesh and at most (2mp)! permutations 
realized. During the same I/d interval, at most 2mp permutations can be realized on the front-end. Consequently, 
TP~,~tit, has the same lower bound as does Tso,, of Theorem 2. From this we have the following result. 
Theorem 5 The number of steps to compute the FFT on n elements on a p-processor, d-dimensional Mesh SuperHet 
with m words per processor, Tpmr(n, m,p, d), sattsjies the following inequality for n ~ 4: n log n TFFT(n, 
m,p, d) = ~ 1 + 2p1-lldlog(2mp) () 5 Matrix Multiplication Consider the multiplication of two n x n matrices 
on the p­processor, d-dimensional Mesh SuperHet when at most ~ < I/d Channel$ are u5ed) uniformly distributed 
over the mesh- P face. In this section we examine the effect of/3 by extending to d dimensions the standard 
s x s 2-D systolic-based matrix multiplication algorithm [9] of Figure 6. We show that if the storage 
capacity m per mesh processor is large enough relative to ~ and p, a full speedup is possible. Kung [6] 
has made similar observations for serial, 1/O-limited machines. The 2-D algorithm arranges the entries 
in the s x s ma­trices A = {A,,j} and B = {El,,j } in the s x s 2-D mesh, as shown in Figure 6, (The 
first row of A is placed in the first row of the mesh. The zth row of A is rotated left i I places before 
placing it in the tth row, A similar translation of columns of B generates the placement shown. ) The 
(z, ~) entry in the product matrix C = AB is formed by taking the product of entries of A and B in the 
(z, j ) mesh processor and adding it to the current value of C,,j, rotating all rows of A left one place 
and all columns of B up one place, and repeating the first two steps until all products are formed and 
accumulated. This algorithm executes O(s) steps, a speedup of order p = S2 over the classical seriaf 
algorithm. This algorithm can be extended in the obvious way to compute the product of two s-x s @ matrices 
entirely on a p-processor (p = S2 ), m-word/processor 2-D .­ mesh, multiplying _ x @ matrices serially. 
The running time of this algorithm, including time to move data between mesh processors, is 0(sm3t2 ). 
The algorithm can be further extended to run on the d­dimensional mesh, d even. We let A and B be sdf2.= 
x sd12m matrices and treat them as s x s rnatric;s whose entries are s(d-2)/2 m x .S~d- J12,~ matrices. 
We can view the mesh &#38; a two-dimensio~al mesh of (d 2)­ dimensional meshes. Products and sums of 
s(d-2)/2 ifi x s(d-2)12@ matri ces can be formed on these meshes and the two-dimensional algorithm used 
to direct these opera­ tions. This leads to a recursive matrix multiplication algo­ rithm that multiplies 
two s km x Skm matrices on the 2k-dimensional mesh in 0(s~m312) steps. 334 speedup is not possible with 
this architecture unless the de­gree of the mesh (and the bandwidth between it and the serial machine) 
is very large. We have examined matrix multiplication and shown that a full speedup for it is possi­ble 
even with a small number of channels if the amount of memory per mesh processor is large. As multi-grained 
computers become more prevalent, they will be used for a large variety of computationally intensive tasks. 
Thus, it is important to understand the opportu­nities and limit ations of this architecture. Not only 
must experiments be conducted to determine how best to exploit the architectural details of existing 
machines, studies such as this must be done to understand the fundamentzd limita­tions on such architectures 
and set the context for algorithm development. Bibliography <RefA>[1] Super Linkage, Science 251 (March 15, 
1991), 1311. Figure 6: Placement of matrix elements in a 2-D mesh be­ [2] M. J. Atallah and J. -J. Tsay, 
On the Parallel Decompo­fore starting the computation of the product C = A x B. sability of Geometric 
Problems, Algorithmic 8 (1992), Elements in C remain stationary while those in A and B 209-231. move 
horizontally and vertically, respectively. [3] F. Dehne, A. Fabri, and A. Rau-Chaplin, Scalable Parallel 
Geometric Algorithms for Coarse Grained Mul­ticomputers, Procs. ACM Symposium on Computa-The product 
W = UxVoftwo nxnmatrices Uand tional Geometry (1993), 298-307. V can be formed on the p = sol-processor 
2-D Mesh Super-IIet when n is divisible by p by treating each matrix as an [4] G. N. Fredrickson and 
D. B. Johnson, The Complex­ity of Selection and Ranking in X+Y and Matrices with (n/9) x (n/g) matrix, 
~ = Sd fi, and using the mesh as a co-processor to add and multiply q x g matrices. Tak-Sorted Columns 
, Journal of Computer and System ing care to move data between the processors to avoid serial Sciences 
24 (1982), 197-208. bottlenecks, we have the following result: [5] J. Jii.J&#38; in An Introduction 
to Parallel Algorithms, Addison-Wesley Publishing Company, Reading, Mas-Theorem 6 T~atv,z(n, m, p, d, 
/3), the number of steps to sachusetts, 1992. multiply two n x n matrices on the p-processor d-dimensional 
[6] H. T. Kung, Memory Requirements for BaJanced Com- Mesh SuperHet with ~ channels between the mesh 
and front puter Architectures, Journal of Complexity 1 (1985),end, satisjies the following upper bound: 
147-157. [7] H. Nicholas, G. Giras, V. Hartonas-Garmhausen, M. Kopko, C. Maher, and A. Ropelewski, Distributing 
  ma rtm(nmpd )=o($ [1+%1) the Comparison of DNA and Protein Sequences Across Heterogeneous Supercomputers, 
Procs. Supercomput- As this result demonstrates, a full speedup can be achieved ing 91 (Nov. 18-22, 
1991), 139 146. by matrix multiplication when only one channel (~ = 1) is available if the amount of 
local memory on each mesh pro-[8] F. P. Preparata and J. E. Vuillemin, The Cube-Connected­cessor grows 
approximately linearly with the number p of Cycles: A Versatile Network for Parallel Computation, processors. 
However, if the amount of local memory is con-Cornm. ACM 24 (May 1981), 300-309. stant, a large fraction 
of the channels is needed for two-F. and E. Area-Time [9] P. Preparata J. Vuillemin, Op­and three-dimensional 
meshes to achieve the same result. timal VLSI Networks for Multiplying Matrices, Inf. For higher dimensions, 
the number of channels must grow Proc. Let. 11 (1980), 77-80. approximately as ~. [10] J. E. Savage and 
S. Swamy, Space-Time Tradeoffs on the FFT Algorithm, IEEE Trans. on Info. Th. IT-24 6 Conclusion (Sept. 
1978), 563-568. [11] C. L. Wu and T. Y. Feng, The Universality of the We have used the p-processor, m-word/processor, 
d-dimensional Shuffle-Exchange Network, IEEE Trans. ComputingMesh SuperHet model to study multi-grained 
parallel com­ C-30 (May 1981), 324-332.</RefA> puters. We exhibit problems that execute more quickly (in time 
O(W)) on the Mesh SuperHet than an either of its components (in time Q(p)). However, we have also shown 
that the serial front end can be disabled without more than a constant-factor loss in the running time 
T for those prob­ lems that use the front end for fewer than O(T/pl / ) steps. We have derived lower 
bounds ou the time to sort and compute the FFT on the Mesh SuperHet and derived asymp­totically matching 
upper bounds. We have shown that a full 335 
			
