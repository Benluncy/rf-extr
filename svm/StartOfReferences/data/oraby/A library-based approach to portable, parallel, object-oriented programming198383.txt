
 A Library-based Approach to Portable, Parallel, Object-Oriented Programming% Interface, Implementation, 
and Application Steven Parkes John A. Chandy Prithviraj Banerjee parkeskicrhc. uiuc . edu j chandy@crhc. 
uluc . edu banerjee@crhc .uluc. edu Center for Reliable and High-Performance Computing University 1308 
W. Urbana, Abstract TbcuseofparalLJpla form.r, despite increasing avaiihbility, re­mains krge[yr estrictedto 
wel[-structured, numeric applications. We address the imue offaci[itating the use ofparaileip.ktj$orms 
on unstructured probkms through o~ect-oriented desip techniques and the actor mode[ of concurrent computation. 
We prese&#38; a madti-leve[ approach to eqwessing para[klkm for unstructured applications: a high-level 
interface based on the actor model of concurrent ob] ect-oriented programming and a low-level inter­ 
@ce whicbprovides an object-oriented inteface to system services across a wide range ofparalkl architectures. 
The high-and bJw­ leve[ interfaces are imp.kmentedmpart oftbe ProperCAD II C++ class library which supports 
shared-memory, message-passin~ and and hybrid architectures. l% demonstrate our approach tbrou~h .. a 
detaikd examination of the parallelizztion process for an ex­isting unstructured serial application, 
a state-of-the-art VLSI computer-aided design application. We compare and contrast the library-based 
actor approach to other methods for mpressin~ parallelism in C++ on a number of applications and kerne.k 
1. Introduction The goal of utilizing multiple processors to solve difficult problems has, to date, been 
largely unattainable for all but a set of restricted-though significant-problems, namely numeric and 
transaction-processing problems. While sub­stantial problems exist in other fields, the techniques used 
in scientific and database applications have not proven similarly effective on unstructured problems. 
The lack of structure in these problems reemphasizes floating point vector operations and emphasizes 
pointer-based data structures and operations comprising a mixture of integer and floating point arithmetic. 
Of particular interest is the issue of composability the ability to combine modules representing without 
deleterious inter-module In this work, we present a new tion for expressing parallelism, the istics of 
which are solutions to sub-problems interaction. interface and implementa­most significant character­ 
Statically-typed high-level interface based on Actors This researchwassupporred in part by the Semiconductor 
ResearchCorpo­rationundercontract93 DP lo9. of Illinois Main St. IL 61801 Class library-based interface 
and open implementation  Multiple levels of abstraction  Incremental parallelization  Many of the 
choices made in the development of these char­acteristics involved consideration of the needs and preferences 
of the target user communi~, a study of which is presented by Pancake and Bergmark [1]. Although their 
work focused on scientific applications, with a few exceptions their observa­tions and conclusions are 
equally applicable to other problem domains. We borrow from their discussion as we consider the characteristics 
of our work. Actor parallelism: Concurrency in an actor program is im­plicit in the semantics of the 
model. Interprocess communi­cation is expressed via continuation erecution, an extension of the member 
function execution mechanism of serial object­oriented languages. The use of explicit continuations makes 
the interface statically type-safe. Library-based interface: Pancake and Bergmark classi$ in­terfaces 
as concurrent kznguages, bzguage extensions, and run­time libraries. The interfaces in our approach are 
manifmt as C++ class libraries. A new concurrent language was rejected because it would require the sacrifice 
of a large investment in existing C and C++ code. An extension to C++ was consid­ered but rejected in 
favor of the class library approach because it was felt thk would maximally conserve our investment in 
tools, code, and user experience without seriously limiting expressibility. Multi-level abstraction: 
For portable, the infrastructure-the time support must insulate the underlying hardware. Yet a model 
of concurren~ to be interface, compiler, and run­the programmer from details of such insulation is not 
without COSGit may constrain the expressibility of the interface and/or the efficiency of implementation. 
As a result, most languages and libraries do not implement a single model of parallelism but instead 
provide primitives at various levels of abstraction. Unfortunately, this can lead to an interface which 
is confusing and dficuk to manage [1]. As a result, we have developed two models. The high-level model 
is capable of expressing actor operations which are easily implemented on any ma­chine, though at costs 
which will vary from architecture to 1063-9535/94 $4.0001994 IEEE architecture. The high-level interface 
is implemented via a well-defined low-level model which captures architectural de­tails such as address 
space dktribution. Use of the high-level interface face. does not preclude direct use of the low-level 
inter- Incremental tion of parallel parallelization: applications To from make existing feasible serial 
the genera­applications, not only must the cost of the parallelization process be min­imized, but the 
parallel application must be considered a variant of the serial application. In our approach, we have 
tar­geted incremental parallelism in two ways. First, the library approach implies that a serial program 
with no CAS to the library is a degenerate library clien~ incrementally greater de­grees of parallelism 
are expressed by adding additiomd library primitives. Second, we implement parallelism via derivation; 
in a real sense, the parallel application is derived from the serial application. By applying parallelism 
via derivation, it is possible to share both source and object code with only limited perturbation of 
the serial code. The ProperCAD project [2], of which this work is a part, has as its goal a suite of 
tools for solving the most computationally intensive problems in VLSI CAD. The ProperCAD II library provides 
the infrastructure that enables these tools to run unchanged across a range of parallel platforms. 2. 
An unstructured problem: test pattern generation assigning an input, for example a, to a value, say 1. 
The result of ~oin~ this ii a new expre~sion, b + c(dej + g) + h)d, so another input is chosen and the 
process repeated. If assign­ing an input results in t= 1, then that and the preceding assignments is 
a test. However, a result t = only Oindicatesthat the particular set of assignments chosen is not a test 
it does not indicate that there is no test. Each earlier assignment must be retried with the opposite 
value. This search process is performed once for each potential fault in a circuit. Generating a set 
of tests that ensure that no device contain­ing a defect is erroneously accepted is extremely difficult 
for contemporary chips containing millions of transistors. The problem of finding a set of inputs that 
yield a 1 is called ~at­i.s-ability, an NP-complete problem. This class of problems has the characteristic 
that the number of possible solutions grows exponentially with the number of inputs. Because naive enumeration 
is impossible, ~ew-i~tic~ are used to try to guess well. 2.2. Parallel test Given a set of heuristics, 
the process of parallelizing search is straightforward; multiple potential solutions are evaluated in 
parallel. Where no solution exists, near perfect speedup is observed. However, when a solution does exist, 
the effec­tiveness of the parallelization effort depends directly on how much extra search is done beyond 
that performed by a serial search. In particular, if the first candidate examined by the serial search 
is nearly always correct, little speedup will be To motivate the need for parallelization techniques 
for un­structured applications, we consider those characteristics which make an application unstructured 
and introduce test generation, an unstructured application drawn from the area of VLSI CAD. We define 
unstructured problems as those not naturally expressible as operations on arrays or through the iterative 
solution of a number of similar problems. The area of computer-aided design for VLSI circuits has many 
such aPPli=tic ns 2.1. Test generation: a search problem Most VLSI CAD problems can be viewed as either 
search or optimization problems. We use as an example test pattern generation, a search problem. Test 
patterns are stimulus sets that when applied to fabricated devices detect the presence of manufacturing 
defects. The output patterns produced by a device in response to a set of test patterns are compared 
against expected results and the device is rejected if the patterns do not match. We illustrate with 
an example. Test generation for a par­ticular fault on a simple chip might reduce to finding a set of 
assignments to inputs a through h such that a Boolean function t = CL(b+ c(dej + g) + h)d + be is 1. 
The simplest way to find such an input is to search for one. We start by observed. Two approaches to 
parallel test generation are fault­parallelism and decision-parailekrn (see Figure 1). In fauk­parallelism, 
a number if the many faul~ in a design are targeted in parallel. While this generally generates sufficient 
parallelism, it does not reduce the time required to find a test for an individual fault and can lead 
to decreased result quality. As a result, decision-parallelism can be used, in which case the PI Assignment 
Declslon Decislon- Targeted D-trontler Decision Sertal Figure 1 Parallelism in Test Generation search 
tree for a particular fault is searched in parallel. In the figure, the test generator makes two types 
of decisions, both of which maybe parallelized. 2.3. HITEC: a test generator for sequential circuits 
H ITEC [3] is a sequentkd test generation which exhibits per­formance and quality among the best known 
in the field. Ta­ble 1 gives a brief description of the major classes in HITEC. The original package 
comprises 25,000 lines of C++. To implement padel search in H ITEC, we create md­tiple search processes-in 
HITEC, Window objects-ither from the fault to be tested (fault parallel) or from the fault, the current 
state of the heuristics, and the set of previously as­signed inputs (decision parallel). We must also 
distribute the data represented by the other classes, e.g., the fault database. Although Conceptually 
straightforward, application of parallel search techniques to test generation on a real algorithm holds 
considerable implementation complexi~. We will see in the sequel how the actor interface is used to express 
parallelism in HITEC. 3. The actor interface The Actor Interface (AIF) provides a high-level interfam 
for expressing parallelism. The purpose of a high-level interface is to insulate the application-and 
by extension, the developer from the characteristics of the underly­ing hardware. The AIF is based closely 
on the Actor model [4] and supports such extensions as aggregates and meta-programmability. In this section 
we consider the basic actor interfa% in Section 4 we consider implementation and interface features which 
facilitate less abstract operations. 3.1. The actor model and continuation-passing style The fundamental 
object in the Actor model [4] is the actor, an object which communicates with other actors via messages. 
All actions an actor takes are in response to messages. There is a close relationship between sending 
messages in the Ac­tor model and calling remote procedures in the RPC model. Because messages automatically 
invoke a member function when received, the actor send operation more closely resem­bles a remote procedure 
call than it does a send operation in a send-receive programming model. The actor model lacks explicit 
sequencing primitives. Syn­ chronization is implicit and derives from the single-threaded Table 1 Classes 
in HITEC class Description I Vect orstat es database of test vectors ~ nature of individual actors. 
The return executed at the com­pletion of an actor method is an implicit wai~ the actor automatically 
becoming available for any pending method invocations. Since an actor can not suspend execution im­plicitly 
in the middle of a computation, continuation-passing s~k (CPS) [5] is used to express control and data 
dependen­cies. Figure 2 contrasts the RPC and Actor/CPS models. In the RPC model, a function f invokes 
a function g which executes on a remote thread. When the call to g oc­curs, the run-time system on the 
calling thread marshals any arguments, sends a message to the run-time system on the re­ mote thread, 
and blocks waiting for a response. On the remote thread, the run-time system receives the message, unmarshals 
arguments, and invokes g. When g executes a return, the run-time on the remote thread marshals the return 
value and sends a message to the local thread which unmarshals the return value and continues f. RPC 
represents distributed programming, a single thread of control in multiple address spaces; the RPC model 
does not express parallelism. The Actor/CPS model extends the RPC model to support object-oriented programming 
and parallelism. Functions are invoked with respect to an actor. The call of an actor method is non-blocking 
and consequently actor methods do not return values. In the figure, the f member of actor a calls the 
g member of b but does not wait. Instead, a continuation is passed as an argument to g. The continuation 
that f passes, a. f r, specifies that the object a and the method f will process the value returned by 
g. In this case, f will be that code in the RPC f which is dependent on the return value of g. g treats 
the continuation c as a function pointe~ once the return value x is computed, g calls c with that value, 
Continuation variables are the parallel, object-oriented extrapolation of serial, procedural function 
pointers. 3.2. Basic interface The library classes representing the fundamental abstractions in the actor 
model, are the Actor base class, ActorNames, ActorMethods, and Continuations. Act ors: Actor types are 
derived from the library-supplied class Act or. For example, to create a parallel test generator, we 
could modi$ HITEC to make the Window class an actor RPC fo Actors/CPS a f ( ) I call 9 ( )+.. . Caii 
b.g(a. f ) x,.. ..:f;~., and wait ?:;;::, no wait ..... 9,( ) b, J(c) return +i return x :::.:$:. y=return 
value ,,A$F . ... finish f () a.f (y) 1 I + Figure 2 RPC and Actors/CPS class: class Window : public 
Actor ( . . . }; This, however, requires an incompatible modification of the serial application. Instead, 
we choose to define a new type which is both an actor type and a Windovz class TestGenerator : public 
Actor, public Window { . . . }; Adding the Actor base to a class enables the creation of actor methods, 
actor names, and continuations, described below. Member flmction calls and access to data members ex­ 
pressed via CS+ pointers occur for actors as they do for any C+ objec~ parallelism and synchronization 
are expressed only through the use of continuations. This approach the addi­ tion of mechanisms for expressing 
parallelism without chang­ ing the meaning of native C++ constructs-is taken through­ out the interface. 
Rather than preclude constructs because they are potential~ unsafe, we choose a design style where parallel 
constructs are added in those situations where seman­ tics in a parallel environment are undefined. In 
this respect, we have tried to stay true to the spirit of C+ [6]. ActorNarnes: Actor names serve the 
role of pointers and references for instances of actor classes. Unlike C+ pointers, actor names are valid 
in the global namespace of a running program, independent of the number, type, and interconnec­tion of 
threads executing the application. Actor names, provided as a library-supplied template class, are jirst-cksss 
vahres, user-visible objects which possess a rich interface which facilitates parallel programming on 
contem­porary architectures. For example, as one might expec~ actor names may be created from a pointer 
to an acto~ TestGenerator* p = . . . ; ActorName<TestGenerator> name = p; p poinm to an actor which exists 
within the same address space as the object executing the code fragment; the pointer value is not necessarily 
valid on all threads. In contrast, name is valid on any thread. More interestingly, the inverse operation 
is also defined: ActorName<TestGenerator> name = . . . ; TestGenerator* p = name; In this case, we start 
with an actor name and try to create a local pointer. After execution of the fragment, p will point to 
the actor represented by name if that actor is in the ad­dress space of the executing rhrcad; otherwise 
it will be null. Other features of Act orNames are considered in the discus­sion of aggregates (Section 
3.3) and the Gauss-Jacobi example (Section 7. 1). Act orMethods and Cent inuat ions: Act orMe ­thods 
are member functions which may be invoked asyn­chronously and remotely. Act orMe t hods are executed 
via Cent i nuat i ons, the concurrent object-oriented ex­tension of member function pointers. An actor 
method is a member function of an actor class that is, by definition, callable through a continuation: 
class Vectors : public Actor, public VectorStates ( . . . void test ( const VectorList&#38; ) ; class 
test : public ActorMethod<VectorL ist> { ActorMethodOf (VectorList) ; } ; . . . J, The test member fimction 
of the vector database class Vectors, derived from the HITEC VectorStates class, is called by TestGenerator 
actors when a test se­quence is found. The declaration of the nested class test enables the creation 
of continuations that take a value of type Vect orLis t as a parameter. The templated base class Act 
orMethod and the macro Act orMethodO f are pro­vided by the Iibray together they define nested Con tinu 
 ation classes. A continuation is created by speci~ing an actor method Cent inuat ion class and an actor 
narnex Vectors : : test : :ContinuatiOn cent ( vectorDB ) ; This statement declares a continuation cent 
which when executed will call the member function test for the actor identified by the name ve c t orDB. 
Continuations are called using function-call syntax: cent ( list ); Here, the continuation cent is called 
with the vector list 1 is t as an argument. Execution via continuations is asyn­chronous with respect 
to the calleG actual execution of the member function occurs at some unspeci6ed time in the fu­ture. 
First-class continuations are necessary to write type-safe modular libraries. Code which calls a continuation 
is de­pendent ordy on the type of argument expected by the con­tinuation; it is independent of the actor 
type and individual method which was used to create the continuation. In a statically-typed actor model 
lacking first-class continuations, client code is dependent on the type of the actor to which the result 
is to be returned. This precludes type-safe class libraries and/or separate compilation.   3.3. Collection 
interface Ir&#38;-idual actors express neither internal parallelism nor data distribution. Collection 
types, based on aggregates with ex­plicit distributions, facilitate both object-internal parallelism 
and data distribution. Aggregates and Aggregat eNarnes: An aggregate is a collection of actors which 
share a common name [7]. When a continuation created from an aggregate name is executed, the appropriate 
actor method is executed by one or more representative actors. An example of the use of aggregates is 
a distributed array where non-overlapping ranges of elements are stored in indi­vidual actors. A distributed 
array can be represented as a set of actors or as an aggregate. In the actor implementation, a gateway 
actor redirects requests to the appropriate sub-range actor. In this case, an extra indirection is required 
and the indirection process is serialized. In the aggregate implemen­tation, there is no need to send 
all requests to a single actor; all representatives of the aggregate shar~ a common t&#38;e and a request 
to that name maybe sent dkectly to the appropriate representative. Serialization occurs only when two 
clients at­tempt to access simultaneously an element stored in the same representative. Aggregate types 
are derived from the library class Aggre ­gate. For example, the H ITEC fauk database class Fau 1 t becomes 
an aggregate in the parallel application: class FaultDataBase : public Aggregate, public Fault { . . 
. }; Cent inuat ions are created from Aggregat eNames just as they are for Act orNames; when such a 
continuation is called, a representative is selected by the run time system. The representative selection 
mechanism may be controlled by the application and supports broadcasting. The first-class nature of names 
has a significant impact on the conceptualization and implementation of aggregates; AggregateNames may 
be coerced into ActorNames without loss of representative-selection flexibility. Such con­version enables 
the writing of libraries for which client code is independent of the choice of actor or aggregate implementa­tion. 
Because the unicasdbroadcast nature of a continuation is inherited from the name used to create the continuation, 
the unica.dbroadcast choice can be made by the creator of the name. By contrast, in the original aggregate 
model the unicasdbroadcast choice must be made at the point where the continuation is called [71. The 
ability to encapsulate this information in continuation objects which are then passed to independent 
packages has considerable potential in the design of reusable library modules. Dist ribut ions: Distribution 
of aggregate representa­tives is specified via an optional argument supplied when creating aggregate 
instances. Distributions specifi the num­ber and location of representatives of an aggregate. The in­terface 
of the abstract Distribution library class is gen­eral enough to express both enumerated and algorithmically­ 
computed distributions. The library provides distribution classes which facilitate the most commonly 
used sharing ab­stractions found in medium-grain applications. 3.4. Advanced interface The run-time support 
of the actor interface is meta-circularly implemented via a number of library-supplied aggregates. Be­caisse 
the implementation is open, any of the system aggregate types may be used as a base class in an application, 
effectively customizing the run-time system in an application-specific manner. Although a detailed discussion 
of the advanced as­pects is beyond the scope of this paper, we summarize the most significant features 
below. Direct or: The main controller of each physical thread in an actor program is a representative 
of the Director aggre­gate. Director representatives are responsible for sending and receiving messages 
and for delivering messages to indi­vidual actors. The Direct or maintains all ready-to-run continuation 
invocations on a set of task queues, prioritizable via a application-extensible priori~ system. Name 
Server: The Name Server aggregate is responsi­ble for the allocation, distribution, and resolution of 
actor and aggregate names. NameS erver representatives allo­cate names which are unique across all processors, 
route actor method calls to the appropriate processor for execution, and maintain bindkg information 
as new actors and aggregates are created. The actor interface provides first-class message types; although 
messages are generally handled completely within the library, for reasons of efficiency, flexibility, 
and resource management, messages may be manipulated explicitly. The most common use of first-class messages 
is the minimization of the copying of data passed as parameters to continuations. 4. AIF implementation 
The set of target architectures for the AIF comprises existing platforms supporting medium-grain parallelism. 
The wide variety of extant medium-grain parallel architectures makes the issue of portability viral; 
an implementation that runs on a limited number of platforms or is dficuh to port would be of limited 
interest and have a short lifespan. The implementation of the actor interface is defined in terms of 
a low-level interface, the Abstract Parallel Architec­ture (APA), which provides an interface and implementation 
which can be used to describe and utilize resources needed by ~Y Parflel application aCrOSSa varietY 
Of architecmres. The APA is used by the AIF but may also be used by applications directly. The AIF has 
been carefully integrated with the APA so that common types of architectural tuning and incremental parallelization 
can be expressed in a systematic way, reducing the need for ad hoc combinations ofAIF and APA references. 
4.1. Abstract parallel architecture The Abstract Parallel Architecture (APA) comprises a model of a parallel 
computer and a set of objects that represent a reification of that model. The APA provides abstractions 
for thread, memory, and communication management and has been designed to provide a standard interface 
across all platforms with no extraneous overhead. The APA is self­sufficient and may be used apart from 
the actor interface. A unique characteristic of the APA is integrated support for parallel architectures 
which combine message-passing and shared-memory features, i.e., where some but not all threads share 
some part of an address space. This eliminates the need for separate shared-memory and message-passing 
imple­ mentations. It also facilitates specializing communication for different configurations; when 
running on a worksta­tion cluster which includes multiprocessors, communication occurs via shared-memory 
within a multiprocessor and via message-passing otherwise. The APA currently supports Sun and Encore 
multiprocessors and the Intel iPSC/2, iPSC/860, and Paragon multicomputers. The APA also supports IP­connected 
clusters of machines, where individual machines may be either uniprocessors or mulapromssors. Thread 
management The APA thread subsystem manages a set of processing elements, represented by instances of 
the Thread class. Thread objects are collected into sets charac­terized by the means by which they communicate. 
These sets are represented by the Iibraryclasses Process, Process Group, and Cluster. The relationships 
between the sets are depicted in Figure 3. Processes: Threads that share a complete address space are 
collected into a container called a Process. Since Threads in a Process share the same address space, 
pointers are always valid across Thread boundaries within a Process. This model is applicable, for example, 
to Mach threads. I 1 ~ - O -Add.. Spa.se lhmd mruld o nnoad o Ro.m. II Figure 3 APA Thread Management 
Classes ProcessGroups: Threads which share some portion of their address space are collected into a container 
called a ProcessGroup. Since Threads in a ProcessGroup may not have identical address spaces, pointers 
may not be valid across Process boundaries; addresses are guaranteed to be valid if and only if they 
point into a shared-memory segment. This model is applicable, for example, to Unix systems using remap 
( ) or System V IPC. c lust en AU threads in a program are collected into a Cluster object. Proces sGroups 
are interconnected by a completely-connected, reliable, unordered, daragram network. The expressibility 
of the APA is demonstrated by consider­ing the mapping of contemporary architectures to the model. Table 
2 shows the number of instances of each thread man­agement class for a number of parallel architectures. 
The first number in each triple represents the number of Process Groups per Cluster, the second the 
number of Pro­ cesses per ProcessGroup, and the last the number of Threads per F rotess; the total number 
of threads is the product of the triple elements. Of particular interest are the last two rows in the 
table, an IP-connected pair of Sun multi­processors and an Intel Paragon with~at ztode~. In contrast 
to most previous work, the APA represents machines which are hybrids of message-passing and shared-memory 
architectures. Resource management Free store (ma 11 OC) management is a common area of difficulty and 
little standardization. The APA provides a set of classes which implement free store man­agement on top 
of the primitive operating system calls, e.g., brk ( ) and n-map ( ) . In addition to providing mal 10C 
( ) functionality for both private and shared memory segments, the free store classes provide multiple 
interface levels, i.e., very fast operations for memory private to a thread and slower op­erations (i.e., 
requiring critical sections) for memory which may be shared among multiple threads. Communication managemenfi 
The final portion of the APA provides the interface necessary to support interprocess communication. 
Abstractions are provided for S emaphor es, Networks, and Datagrams. For network protocols where delivery 
is unreliable, e.g., UDP/IP, the APA imple­ments reliabili~. Table 2 APA triples for various machines 
Machine APA Configuration Figure 3 architecture 3/213 16 node Intel iPSC 16/1/1 4-processor Sun 41600MP 
1/4/1 Two 4-Pmc.ssor Sun 41600MPs 214/ 1 connected by Ethernet Intel 64 fat node Paragon 6411/4   4.2. 
AIF/APA interaction Many applications require or benefit from some amount of architecture-specific customization. 
While the complete APA interface is available to actor applications, it is desirable to provide high-level 
support for the most common architecture­specific optimi2ations. The most common type of architecture-specific 
tuning is the distribution of data, represented by aggregate distribu­tions. The AIF provides standard 
distributions based on the APA model: PerThread, Per Process, and PerPro ­c es sGroup. Aggregates created 
with these distributions have one representative for each of the related APA instances. Per Thread dktributions 
can be used to represent data pri­vate to individual threads; member functions and data maybe accessed 
by other actors allocated on the same thread without the possibility of a race. Perproces sGroup distributions 
can be used to represent read-only or, in concert with S ema phores, write-rarely data. 5. An unstructured 
parallel application: parallel test generation To evaluate the expressiveness the AIF and efficiency 
of the implementation, we developed ProperH ITEC, a parallel version of H ITEC. 5.1. Implementation 
Figure 4 shows the ProperHITEC classes which have a parzdlel semantics, along with the H ITEC and AIF 
objects from which they are derived. We give a brief description of the parallel types below; further 
details may be found in [8]. TestGenerator: TestGenerator instances are cre­ated from a specified fault 
to implement fault-parallelism and cloned from an existing instance and an assignment alternative to 
implement decision-parallelism. Each Tes t Gene rat or constructor continuation is assigned a priority; 
when run on a uniprocessor, TestGenerator objects are created in the same order as the serial algorithm. 
CircuitAggregate: The circuit is implemented as an aggregate with a PerThread distribution. Read-only 
shar­ing via Per Proces sGroup distribution was implemented but removed because the method of parallel 
fault-injection resulted in excessive overhead in the core H ITEC algorithm. FaultDataBase: The fault 
database is implemented as ~ %gregate with a Pe rThread distribution. Each rePre­~---~ ( n C+mISAW,egc4. 
--HITECdam P,oPuCAD II .1.s FTOFUHIIECda. a sentative stores the most recent status of the test generation 
process for each fault. Vectors: The Vectors object is essentially the same as the serial object but 
uses Act orMe t hods to record new vectors and to send resuhs to the Fault Database. UserInterface: The 
UserInterface object fills the role of the function main in a serial program. Tables 3 and 4 show the 
resuks of ProperH ITEC for a num­ber of standard benchmarks. The column labeled HITEC gives the results 
of the serial application. T is elapsed wall clock time in seconds. Fault efficiency, E, is a measure 
of the quality of results; higher efficiencies, up to 1007., represent higher quality solutions. Results 
for the parallel application, ProperH ITEC, is reported for several configurations. ProperH ITEC achieves 
consistent speedup with only a marginal effect on quality. The only cases for which the parallel algorithm 
does not achieve acceptable results are the benchmarks for which 100% efficiency is achieved within a 
few seconds. This is due to algorithmic issues and is not related to overhead within the run-time library. 
5.2. Experiences snd observations The most interesting characteristic of ProperH ITEC is the &#38;e of 
derivation to extend the serial application to a parallel version. The only fimdarnental change made 
to the serial code was the modification of several functions to use dynamic binding. Because dynamic 
dispatch involves an additional level of indirection, the impact on performance was studied but was found 
to have no measurable impact on performance. 6. Alternate approaches A signifkant amount of related work 
has been performed in concurrent object-oriented computing we concentrate on Table 3 ProperHITEC on Sun 
4/670MP Circuit HITEC ProperHITEC Processors 1I2 I4 TEl TEI TE] TE S344 I 369.4 95.9 I 374.3 95.9 I 
251.9 96.2 I 156.2 96.2 s820 435.9 99.3 396.8 99.3 225.4 99.3 140.3 99.1 s1238 13.13 100 21.64 100 15.15 
100 16.18 100 S1494 722.0 99.1 663.4 98.9 434.1 98.9 240.1 99.1 Table 4 ProperHITEC on the Intel iPSC/860 
Chair HITEC ProperHITEC Processors 148 TE TETE TE S344 481.4 94.2 485.8 94.2 194.5 96.8 142.1 96.5 s820 
438.3 99.3 440.8 99.3 158,0 99.3 108.0 98.9 s1238 14.15 100 23.29 100 12.16 100 11.12 100 S1494 819.8 
99.0 821.3 98.7 310.1 98.6 192.2 98.8 Figure 4 ProperHITEC OrgClniZCdiOn aPPr each= related to actors 
~d c+. A number of pure actor languages based on the Actor model have been implemented [9]. The majority 
of these ap­proaches have targeted fine-grain concurrency on massively­parallel MIMD architectures. Chien 
developed an actor lan­guage, Concurrent Aggregates (CA), which in addition to the features of actors 
and aggregates provides support for first-class continuations and messages [7, 101. Chmm ~d Charm++ [11, 
12] were developed to address the need for a portable, parallel language realizable across a wide variety 
of shared-memory and message-passing architectures. PC* [131 is an extension of C++ with support for 
distributed data struc­tures, in much the same manner as High-Performance FOR-TRAN. CC++ [14] extends 
C+ with support for task paral­lelism. We consider how the test generation problem might be expressed 
on some of these systems.  6.1. Data parallelism In data parallelism, a data structure, usually an 
array, is dis­tributed across processors in a regular manner. This type of expression is natural in scientific 
computing where many al­gorithms are described as operations on arrays. This type of parallelism is supported 
by pC++, an extension of C~ [13]. It is difficult to hypothesize how one would implement test generation 
on top ofa data parallel model. The majori~ of the work in test generation is incorporated in indkidual 
test gen­eration tasks, not in a distributed data structure. Moreover, the distributed data structures 
in Prope rH ITEC, e.g., the fault database, are replicated on each thread; data parallelism usually does 
not apply to duplicated distribution. The difficulty of expressing test generation in terms of data parallelism 
reinforces the need to choose the correct tool for the correct problem; in Section 7.1 we report an implemen­tation 
of the Jacobi computation using the actor model. It is clear that while the actor interface can be used 
to efficiently implement the Jacobi computation, data parallelism and pC* are the more intuitive ways 
of expressing the problem. 6.2. Task parallelism Task parallelism is usually interpreted to mean either 
a series of expressions to be evaluated in parallel or a loop for which individual iterations are to 
be performed in parallel. This type of parallelism is supported by another extension of C~, cc+ [14]. 
To express test generation in terms of task parallelism the problem is broken up into similarly sized 
tasks. The ap­proach taken in ProperH ITEC, in which a large number of variable-weight tasks are created, 
poses two problems. The greatest difficulty is guiding the ordering of task execution; if ordering of 
execution of test generator objects is not pre­served, processor efficiency and quality of results fall 
dramat­ ically. Task parallelism generally does not provide a priority mechanism. Further, many task 
parallelism implementations work best where the degree of parallelism expressed is on the order of the 
number of threads in the machine. On such a system, a master-slave model would be needed to maintain 
an ordered list of test generation tasks. This light-weight scheduling is implicit in the actor interface. 
Again, as is the case for data parallelism, there are problems for which task parallelism more closely 
matches the most intuitive solution. In particular, while features such as implicit barriers and futures 
can be expressed with continuations, such expression may be cumbersome. Applications which rely heavily 
on these operations could be tedious to translate to CPS. 6.3. Charm and Charm++ Charm and Charm+t are 
parallel programming environments, derived from C and C++ respectively, targeted at medium­grain architectures. 
The fimdarnental object of parallelism in Charm is the cbare, an object with characteristics very similar 
to actors. Charm supports collection types via r%-ancL­ o&#38;ce cbares which are aggregate-like objects 
with irnpkit per­thread distribution. The first phase of the ProperCAD project [2] used Charm as the 
parallel programming environment and many of the ex­ tensions to the actor model which exist in the AIF 
are drawn from or are extensions of features from Charm. The priority mechanism of Charm was determined 
in earlier work [2] to be crucial for efficient execution of CAD applications and was adopted and extended 
in the AIF. Features of the Ac­tor Interface which clifIer most significantly from Charm are support 
for static typing, as represented by first-class contin­uations, and composability, as represented by 
per-class meta­prograrnmability. Because Charm does not have first-class continuations, two techniques 
are used to represent method calls. A user defined continuation pair, <Entry Point ID, Chare ID>, can 
be used. However, because the pair is not a system object, static type checking cannot be done to be 
ensure that the elements of the pair are consistent. Derivation has also been proposed to express a type-safe 
alternative [12]. A &#38;are which will receive a message from another chare inherits from a receiver 
type defined by the sender. For example, the user interface in ProperH ITEC which uses barriers to synchronize 
aggregate creation could be defined in Charm~ as chare class User Interface : public BarrierReceiver 
{ . . . virtual void receiveBarrier ( ) ; ... ); However, this precludes the use of more than one barrier 
func­ tion in a class. The User Interface actor, which creates three barriers, would need to keep an 
extra state variable. By contrast, using first-class continuations a barrier actor is cre­ ated with 
a continuation representing the method to be called in the computation. Charm was one of the first actor 
models to implement advanced meta-prograrnmability features and the Actor In­terface incorporates extensions 
of many of the features in Charm. The most significant difference in the area of meta­programmability 
is the support for composability. In Charm, meta-programmability features, e.g. priority representations 
and queueing algorithms, are chosen at link time, implying that all classes which will be composed into 
an executable must use a uniform representation. In contrast, all meta­prograrnability features in the 
Actor Interface are specified on a per-cl~s bmis such that the use of a particular representation by 
one class does not preclude use of other representations by other classes. 7. kperiences and observations 
This section summarizes our experiences with other applica­tions using the actor interface as well as 
some observations on the effectiveness and efficiency of the library in general.  7.1. Other applications 
Gauss-Jacobi: An implementation of the Gauss-Jacobi iter­ation for partial differential equations was 
created to explore the performance of the library. Four implementations were created: serial, a serial 
implementation, shared-memory, a version for multiprocessors, message-passing, a version for muhicomputers, 
and actor, an implementation based on the AIF. With the exception of actor, all implementations were 
implemented in C using the most efficient algorithm for the target architecture. Figure 5 shows the performance 
of 100 iterations of the Jacobi computation across a range of problem sizes on a multicomputer, the Intel 
il?SC/2, and a mukipro­cessor, the Encore Multimax. The bulk of the dfierence between the custom C and 
actor C* implementation is due to overhead in the non-parallel elements of the C* library; the library 
is currently undergoing optimization of these ele­ments. The most interesting aspect of the Jacobi computation 
is the use of first-class names. An actor type representing each sub-block of the computation, Jacobi 
Block, was created. Each actor of this type must communicate boundary values with neighboring actors 
after each iteration. To do this, each actor requires the name of those neighbors. Difficulties arise 
if names cannot be created separately from the creation of the individual actors. This linkage of name 
creation with actor creation in Charm results in the Jacobi iteration being expressed as a branch-office 
chare. Because names in the AIF may be created in a separate operation, there is no need to use aggregates 
to implement Jacobi. Cell plaeementi When the logic design of a VLSI circuit has been completed, placement 
is performed to choose loca­tions for each component such that the area of the layout is minimized. We 
have implemented Proper PLACE, a par­allel version of TimberWol f 6.0, a simulated annealing I-9=-P-9% 
~ 16 ... ... .. . .--------------- *  -   Pmc-= 16 ,,­,a. . p .4 i: 1: :: -- -------- -  - - - 
- -  E==l 2 t . ... .. .-.... .-,.. -,,4 1 p .1 1001m2w2s0mo 3s0400 PrOblsm 924 Intel iPSC/2 4 . 
.. .. .... . ... .. . P .4 3 pm-=1 L-----d II 1Lv16020J2s08Lm Uamn Prew9m 21. Encore Multimax Figure 
5 Gauss-Jacobi speedup over serial placement application [15]. Results are shown in Table 5 for a sampling 
of circuits from a standard benchmark set. Times, reported in seconds, represent elapsed wall clock time. 
The wire-length measure, W, is a cost factor generated by Ti.mberWolf. Proper PLACE use-s actors, aggregates, 
first-class names, first-class messages, the ability to co-locate actors, and coercion of actor names 
to actor pointers. Logic synthesis: Logic synthesis is the process of optimizing a digital circuit to 
reduce area or delay. We have developed ProperMI S, a parallel implementation of the MIS synthesis .. 
apph~tlon [16]. The heart of the optimization procedure is rectangle covering, a standard search problem. 
Results of Prope rMI S are shown in Table 6 for standard benchmarks Table 5 ProperPLACE on Sun 4/670MP, 
Intel iPSC/860 Ckcuit Orieinal I Pror2erPLACE I fi Sun 4/6?OM Processors 14 Tw fraa 111779 433.1 21184 
265.1 22245 prirnaryl 1202241 1048 249982 607.2 274238 Strucc *2668124 2701 173730 1164 184613 D rimarv2 
7292946 3201 1410450 1945 1678079 Intel iPSC/8 1­ fract 179.6 22971 I primaryl 475.3 296624 struct 1164 
225248 I rimary2 1352 1727777 t1 Table 6 ProperMIS on Sun 4/670MP Ckcuir Original number of p rocesrors 
NameLit 1 4 I Lit r !-it C499 1032 77.96 678 37.27 684 aJu2 730 166.82 510 52.37 573 duke2 1746 83.36 
497 40.55 506 misex3 2692 157.67 427 74.81 503 on a Sun 4/670M P multiprocessor. Lit, the number of literals 
in the circuit, is a measure of cost. Prope rMI S uses actors. aggregates, first-class names, first-class 
messages, the ab~lty ~ co-locate actors, and coercion of actor names to actor pointers. 7.2. Strengths 
 First-class names and continuations: The ability to create, operate on, and interchange actor and ~regate 
names has turned out to be vital to the construction of an interface which has a high degree of expressibility 
but which supports staac type safety and separate compilation. We believe that the expressibility of 
names linked with statically-typed continu­ations will be key to implementing application libraries that 
can be effectively composed to create new applications. Derivation-based parallelixation: Our experience 
with par­allelization through derivation leads us to believe that this will be key to parallelizing 
existing codes without doubling devel­ opment and support costs. 7.3. w&#38;esses Remote procedure calls: 
The actor model provides no blocking operations; although specification and implementa­tion of blocking 
semanties may be diflicult, experience shows that users strongly desire blocking calls. 1/0 interface: 
The actor interface provides no direct support for 1/0; we are investigating the use of additional threads, 
i.e., more than the number of processors available, which would be used to execute only blocking 1/0 
operations. Performance interface: The logical model of the APA does not capture available processing 
poweu a measure of avail­able resources must be developed and incorporated into the scheduling mechanism. 
8. Summary We have presented a run-time library suitable for a statically­typed language which implements 
the actor model of concur­rency. We have demonstrated how a library-based implemen­tation ean be used 
to incrementally parallelize an existing serial code with only incremental increase in development cost. 
We demonstrated, through a large unstructured application, the efficaey of this approach on contemporary 
architectures We contrasted our approach with others and identified some of the most significant strengths 
and weaknesses. Acknowledgemen~ The authors gratefully acknowledge the support of the Argonne National 
Laboratory, the San Diego Supereomputer Center, and the National Center for Supercomputer Applications. 
References <RefA>[1] C. M. Pancake and D. Bergmark, Do parallel languages re­ spond to the needs of scientific 
program mers?, IEEE COm­puter, vol. 23, pp. 13-23, Dec. 1990. [2] B. I&#38;m&#38;mar and P. Banerjee, 
ProperCAD: A portable object-oriented parallel environment for VLSI CAD, IEEE Transaction on Computer 
AidedDesign, vol. 13, pp. 829 842, July 1994. [31 T. Niermann and J. Patel, HITEC: A test generation 
pack­age for sequential circuits, Proceedings of the European Design Automation Con@ence, pp. 214 218, 
Feb. 1991. [4] G. A. Agba, Acton: A Model of Concurrent Computation in Distributed Systezm. The MIT Press, 
1986. [5] A. W. Appel, Compiling with Continuations. Cambridge Uni­versity Press, 1992. [6] B. Stroostrup, 
The Design amfEvoktion of C++. Addison Wes­ley 1994. [7] A. A. Chien, Concurrent Aggregates: Supporting 
ModuLzrip in Massiv.~ ParaL!e[ Programr. The MIT Press, 1993. [8] S. Parkes, P. Banerjee, andJ. Pate], 
ProperHITEC: a portable, pardel, object-oriented approach to sequential test gerrera­tion, in Proceeding 
of the 31st Design A utonration Conference, pp. 717 721, Jun. 1994. [9] G. Agha, <An overview of actor 
languages, SIGPLAN No­tices, vol. 21, pp. 58-67, Oct. 1986. 10] V. Karamcheti and A. Chien, Concert 
efficient rurr­time support for concurrent object-oriented programming lan­guages on stock hardware, 
Proceedings ofSupercomputing 93, pp. 33 36, 1993. 11] W. Fenton, B. Ramkumar, V. A. Saletore, A. B. Sinba, 
and L. V. Kak, Supporting machine independent programming on diverse parallel arch itecturs, in Proceeding 
of tbe Interna­tional Conference on ParaLk[ Processing, Aug. 1991. [12] L. V. K&#38; and S. Krkhnan, 
CHARM++: A portable concur­rent object oriented system based on C*, in Proceedings of the 1993 Conference 
on Object-On entedProgmmming Systems, Languages, andApp[ications, pp. 91 108, Sept. 1993. [13] D. Gannon 
andJ. K. Lee, Object-oriented parallelism: pC++ ideas and experiments, in Proceeding of the Japan Socie~ 
for PamL!elProcessing, pp. 13-23, 1993. [14] K. M. Chandy and C. Kesselman, Compositional C++: Compositional 
parallel programming, in Proceedings oftbe Fz@ Workchop on Compikrs and Lzngrazges for Parallel Com­puting, 
pp. 79-93, 1992. 15] S. Kim, J. A. Chandy, S. Parkes, B. Ramkumar, and 1?Banerjee, ProperPIACE: A portable 
parallel algorithm for cell place­ment, in Proceedings of the International ParaUe[ Processing Symposium, 
pp. 932 941, Apr. 1994. 16] K. De, J. A. Chandy S. Parkes, and P. Banerjee, Portable pardel algorithms 
for factoring and simplitieation in logic synthesis. Submitted to the 1995 International Parallel Pro­cessing 
Symposium.  </RefA>
			
