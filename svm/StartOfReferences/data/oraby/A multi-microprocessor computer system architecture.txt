
 A MULTI-MICROPROCESSOR COMPUTER SYSTEM ARCHITECTURE* Bruce W. Arden and Alan D. Berenbaum Princeton 
University The development of microprocessors has suggested the design of distributed processing and 
multiprocessing computer architectures. A computer system design incorporating these ideas is proposed, 
along with its impact on memory manage- ment and process control aspects of the system's operating system. 
The key design feature is to identify system processes with microprocessors and inter- connect them in 
a hierarchy constructed to minimize intercommunication require- ments. Key words: computer networks, 
distributed processing, microprocessors, multiprocessing, parallel processing CR categories: 4.32, 4.35, 
6.21  Introduction. In the near future, computer system designers are expected to bring more computer 
power to more people than is currently feasible. At the same time, it is reasonable for the computer 
system designers to expect the computer architects to produce hardware structures incorporating system 
algorithms of greater complexity so that more of the general in- structions go to serving the users. 
In the past, the architect has delivered more computing to the user by constructing bigger and faster 
central processors, possibly connecting two or three CPUs to- gether. This approach, though, has its 
limitations, in cost, complexity and reli- ability. Instead of large, centralized processing systems, 
an increasingly attractive alternative is to employ multiple less-expensive processors. Present Systems. 
The simplest approach to distributing computing power is to set up independent minicomputers systems 
wherever there is a demand, as has been done in many universities and corporations. Although straightforward 
and inexpensive, this approach is limited with respect to information and resource sharing. Any common 
database is obviously difficult to implement and sharing of underutilized resources is limited. The problem 
of sharing is approached by net- working. Examples of such systems include UC Irvine's Distributed Computer 
System [6] and the ARPA net [18]. However, the restrictions on interprocessor communica- tion, necessary 
for a far-flung system *Research supported by NSF Grant No. DCR74-18655. like ARPA, may be unnecessarily 
severe for a more centralized system. It is possible to configure a mul- tiple processor system in a 
more "intimate" manner than a loosely-coupled network. If the computing system enables programs, which 
can be separated into parallel seg- ments, to be executed in parallel efficient- ly, it is possible for 
a collection of small, low-speed processors to do the work of one high-speed processor. This is the motivation 
behind Carnegie-Mellon's C.mmp system [21]. With this system the design- ers claim a set of DEC PDP-Ii's 
can do the work of an IBM 370/158 at a substantially lower cost [13]. Alternatively, tightly- coupled 
multi-miniprocessor systems can be used to effect a modular expansion of com- puter power. This technique 
has been employed for Bolt, Baranek and Newman's PLURIBUS system [8]. Although it permits a much higher 
degree of parallelism, C.mmp is similar to systems like MULTICS and those running on a 370/158. Despite 
their relative low cost, the PDP-11 processors on C.mmp are a valuable resource like the CPU of a 370/158, 
and so must be multiprogrammed in the interests of efficiency. The problems of process scheduling and 
resource allocation must still be handled in a centralized manner, at least in so far as only one set 
of system tables must be maintained [22]. By taking advantage of rapidly advancing technology and altering 
the system design philosophy, it should be possible to 114 construct a system that is at least as powerful, 
is highly modular in construc- tion, yet permits the use of much simpler operating system structures. 
The system described in the following sections is an attempt to meet these criteria, especially in the 
environment of a large, central time-sharing facility with shared data base. Microprocessor Potential. 
In a few years, microprocessors and semiconductor memories will become larger, faster and cheaper [20]. 
As block-oriented random access memories are developed, extremely large, if relatively slow, central 
memories are possible. As chips will still be con- strained by the number of pins available, the complexity 
of a system will come not in the logical design, but in the inter- connection of actibe components. 
A microprocessor cannot have both the computational power of a larger machine along with its input-output 
flexibility. However, it is possible to program special purpose microprocessors which can handle some 
aspect of resource manipulation. These processors can be tied together to form a distributed function 
processing complex [15], which has a broader capa- bility than any of its parts. Alternative- ly, one 
can consider operating system functions that were formerly implemented in software to be embodied in 
the hardware of the special-function processors. Memory Potential. It has been felt that reductions 
in operating system com- plexity can be achieved through the use of very large central memories [ll]. 
With the type of semiconductor central memory used now (MOS main store with a bipolar cache) it is unlikely 
that a central mem- ory will grow more than an order of magni- tude in size, so that a typical large 
system might have i0 or 20 megabytes of main store. However, it is unlikely that the need for sophisticated 
memory manage- ment techniques will vanish at that point, since increased processor technology (plus 
increased user sophistication) will probably increase demand on main memory at the same rate the memory 
size increases. It appears that a different type of organi- zation will be necessary to achieve much 
 larger memory sizes. Contemporary time- sharing computers use a drum to simulate a large random access 
memory. Future mini- mass memories, fabricated out of CCD's, for example, will be larger than contem- 
 porary drums, at similar prices but with lower access times. These memories are, of course, restricted 
in that data are organized in rings and an individual byte of memory may not be directly accessible. 
However, if the memory system is hierar- chical, this may not be a disadvantage. If a processor accesses 
memory only through a local memory or cache, then the main memory never need transfer data in groups 
 smaller than a cache block. If the ring size of main memory is the same as a cache block, and the 
transfer logic is sufficiently intelligent to start a trans- fer in the middle of a block, then informa- 
tion can be transferred between memory levels without rotational delay. There are tradeoffs in the design 
of a block size. The larger the wordsize, the shorter the ring size, and hence, the faster the transfer 
rate at the cost of greater inter- connection complexity. Likewise, a small blocksize leads to increased 
transfer rate, at the cost of greater memory management overhead [7]. Design concepts. The availability 
of sophisticated microprocessors suggests the design of inexpensive multiprocessor sys- tems [16]. 
It is proposed here that given a network composed of connection-limited, inexpensive microprocessors, 
the design challenge is to find a modular organiza- tion for a general system which permits the individual 
parts to operate quite independently with a minimum of overhead interference of the kind present in 
cur- rent multiprocessor systems. One approach is to identify the various processes in a process-structured 
system and implement each as a separate microprocessor. When a process included a critical region only 
a single microprocessor would be used (e.g., storage and device allocation). If more than one instance 
of a process were per- mitted (e.g., drivers for multiple print- ers), then several microprocessors dedi- 
 cated to that function would be included. Extending this construction to user- generated processes, 
as opposed to system processes, a single microprocessor would be dedicated to each user process. Since 
 a process is never swapped and a process identifier is equivalent to the processor identifier, there 
is no need for system- wide process queues. However, the question arises whether microprocessors dedicated 
 to a process for the lifetime of the pro- cess is the best approach. Considering the cost of microprocessors, 
the possibly large idle periods (or "busy waiting") are not strong counterarguments but, the in- creasing 
interconnection complexity with large numbers of microprocessors is a difficulty. This question is considered 
 in more detail with the aid of a simple queueing model later on. The large number of processors avail- 
able for assignment to processes is also suitable for parallel processing applica- tions. Process creation, 
termination and synchronization, although simple, are too time consuming with respect to primitive arithmetic 
operations to have concurrency at the arithmetic expression level. The parallelism would be at the more 
macro- scopic level of processes. By structuring a large program as a collection of inde- pendent processes, 
it should be possible for many large programs to execute in a reasonable time on relatively slow pro- 
cessors.  The identification of processes and processors is conceptually attractive and reduces, to 
some extent, process inter- action because process state is maintained in a hardware device and need 
not be trans- ferred to a process state table. However, the critical problem is how to make a user process 
execute for relatively long periods of time without the need for other system services. Stated differently, 
if the mem- ory associated with a user microprocessor is regarded as a cache or buffer store, the "hit 
ratio" or references to buffer addresses compared to external addresses must be very large. This much 
analyzed statistic can be increased by having a large buffer store, possibly relatively slow, or by increasing 
the fraction of the buffer store dedicated to data. The latter can be done by storing the instruc- tions 
in a succinct, high level language form (e.g., APL) and interpreting them by means of microinstructions 
obtained from a RQM. The design that follows is a pre- liminary version of a general system based on 
microprocessor implemented system and user processes with limited processor interconnections. Processor 
description. The basic service microprocessor, called a proqram processor, would be a $50 15-bit micro- 
processor attached to a high-speed buffer memory, a read-only control memory and a request bus connected 
to service pro- cessors. It has been shown that a small cache memory can effectively increase the speed 
of a larger, slower memory [i0], so it is likely that a relatively small buffer memory will be sufficient 
for maintaining the processing rate of a program processor. If the system contains a large amount of 
central memory, there will be little need for paging to the file system, the third level in the memory 
hierarchy. As block- oriented memory becomes inexpensive, it will be economically feasible to maintain 
at least iM byte of memory per process in the second level of central memory. This is as large a memory 
as that provided for a single process in many current, large scale multiprogramming systems. As has 
been mentioned above, the critical problem in a large system employ- ing a variety of small parts is 
that of interconnection. For a large system it would not be unreasonable to expect i00 program processors 
executing at once. It becomes difficult to connect those i00 processors to the various resources of a 
large computer system. It is therefore necessary to cluster program processors around a central service 
facility. A cluster is illustrated in figure i. The whole cluster can be seen as embodying a contemporary 
multiprogramming system in hardware. The microprocessors in a service center represent the system pro- 
cesses, while the program processors represent the active user processes. Unlike multiprogramming systems, 
however, there is no distinction between active and running processes. Figure i. S = service center 
 P = program processor The service center performs four functions: i) memory management, 2) process 
management, 3) file management (which includes all I/O) and 4) monitor- ing and protection. Each of 
these areas could be maintained by a separate micro- processor, called a service processor. Each of those 
microprocessors must have its own memory, both random access and control, and access to the status regis- 
ters of all of the program processors it serves. Only the service processors have facilities for manipulating 
the system's resources. Obviously, the service proces- sors may need to request service from each other. 
The four service processors and the program processors may be connected together by a single bus (see 
figure 2). Only one program processor may be receiv- ing service at a time. If the technology can support 
it, the service center may be a multiple bus system. A program proces- sor can request service only by 
interrupt- ing a service processor. Its request is posted on the bus and an equitable arbi- tration system, 
such as one described by Chen [3], decides which of the program processors to attach to the service pro- 
cessor. Figure 2. p = programmer s = service processor i l  To maintain efficiency and reduce interconnection 
complexity, it is desirable to limit the number of program processors attached to a service center. Since 
it is also desirable to have a very large number of program processors, a large-scale sys- tem must have 
a number of clusters. The simplest means for connecting an arbitrary number of clusters together seems 
to be the ring structure employed by the Distri- buted Computing System [6]. Since most processes are 
self-contained, the only need for communication between clusters is for multi-process tasks whose demand 
for processors cannot be met within the cluster that initiated the task. This sort of limited interaction 
is ideal for a ring system. The ring interface can be maintained by the process management processor 
in a service center. In addition to interprocess communication, there are other global system resources 
that must be shared by all clusters, namely the central memory and the input/output and file sys- tem. 
There is one bus connecting each cluster and the central memory. The memory manager controls all transactions. 
As in the UNIX system [18], for example, all input/output devices can be considered special cases of 
general files. Therefore, the file manager need only be concerned with one type of entity, a file. If 
the file system is maintained by a central file computer (like the PDP-10 serves the OCTOPUS system [2]) 
the operations of a file manager need be only formatting and posting requests to the file system com- 
puter. Memory description. In order to avoid memory interference, it is necessary that a large memory 
be broken up into many modules. An obvious problem exists in trying to connect possibly hundreds of processors 
with an equally large number of memory modules. There are three prin- cipal means of interconnecting 
processors and memories [4]. Multi-port modules, in which each processor has a direct connec- tion to 
every memory module, as in the IBM 360/370 series, is impractical be- cause of the large number of interconnec- 
tions and the inability to expand the number of modules beyond the number of ports. A crossbar system, 
in which each processor is connected to a switch which also connects all memories, as in C.mmp, has fewer 
connections, but expansion of the system is still difficult. A bus sys- tem, where processors and memories 
share (i.e., multiplex) one or more bus lines, seems the only practical means in terms of cost and expandability. 
To handle the large numbers of memory modules, a multi- bus system is proposed. The switching logic 
required for this sort of system could be implemented with a standardized switch module. With each module 
would be associated an address, corresponding to the identifier of a memory module. As the main memory 
in the proposed system is block-oriented, the memory module address would be the high order bits of the 
addresses of the words within the module. The switch would have four ports the width of the principal 
data path (see figure 3). A memory request would come in through one port. If the high order bits matched 
the address of the memory module, the switch would connect the input bus with memory module bus and the 
data transfer could proceed. If the addresses did not match, then the switch would connect the incoming 
bus with one of the two remaining busses: if the requested address was less than the assigned address, 
the left port is chosen; otherwise, the right port is chosen. Clearly, if the busses of the memory modules 
were organized in a binary search tree manner (figure 4), then a processor could be connected to any 
memory module in time log N, where N is the number of memory modules. If the time of switching is much 
less than the expected time of memory transfer, then the switching time could be ignored. Memory modules 
may be added simply by filling up or extending the tree at the cost of one switch per mem- ory module 
per processor. Likewise, each processor need be connected directly to only one switch to access the set 
of memory modules, and indirectly to at most 2**(N-i) switches, if there are 2**N memories. i.{ Figure 
3. FJ.g  The cost of the processor-memory interconnection would be prohibitive if every one of the 
program processors had direct access to the memory modules. How- ever, the program processors are constrain- 
ed to access main memory through the memory manaq~ment processor serving the relevant clusters. This 
reduces the number of inter- connections by an order of magnitude. A single bus connects all memories 
of the program and service processors of a cluster and the topmost switch of the memory- switch tree, 
including a connection to the memory management processor. All transfers along this bus consist of single 
blocks. Contention. The great problem with all multiprocessor systems is contention, and in a system 
with hundreds of proces- sors a great deal of architectural ingen- uity must be employed to circumvent 
it. There are basically two types of conten- tion: memory interference and system contention. Memory 
interference results when two or more processes attempt to share a common region of code or data. The 
problem is program-dependent, and the responsibility for avoiding it may rest with the programmer. Techniques 
for parallel programming that recognize this problem are just beginning to be developed [,12] . The 
situation is different with respect to system contention, which re- sults when two or more processors 
request common system routines or tables. In fact, a principle motivation for the pro- posed system is 
to reduce system conten- tion. By identifying the state of a pro- cess with a processor, a number of 
process control tables and task dispatcher routines are obviated. It is not immediately obvious that 
the memory management system can avoid the problems of contention for the global memory management system. 
 An extremely large central memory can simplify the problems of memory manage- ment and hence the problems 
of system con- tention in the memory management section of the operating system. Current systems like 
MULTICS, even with extended core storage, run out of primary memory space under heavy loads and must 
transfer pages to the file system. If the central mem- ory is large enough so that it is unlikely to 
run out of room page traffic to the file system will be reduced considerably. Although it is expected 
that there will be a large amount of sharing of data and procedures, there will always be a considerable 
amount of unshared code and data. For example, the runtime stack of an ALGOL-program or the code for 
any undebugged procedure are unlikely to be shared. If all segments, where a segment is a collection 
of pages, as in MULTICS, are declared to be either shared or non- shared when they are created, then 
a subset of the memory modules may be par- titioned among the clusters to that all nonshared segments 
associated with a pro- gram processor are assigned to memory modules allocated to the cluster containing 
the processor. No two clusters will have to access the same memory module within the subset (except in 
the case of message- passing between cooperating processes run- ning on separate clusters), so the page 
tables for those modules need be maintained only by the associated memory management processor. By the 
use of more LSI technology, it should be possible to construct associa- tive memories which can be attached 
as peripherals to the memory management processors. With these memories, it is possible to condense page 
tables, segment tables and unallocated area tables into a single table. Every segment created by the 
system can be assigned a unique identifier, like a capability identifier. In thls way, it is possible 
to reference any segment with a unique, invariant inte- ger. An entry in the associative memory would 
contain one bit for empty or full, enough bits to represent the unique seg- ment identifier, plus enough 
bits to rep- resent the number of pages in a segment. The associative memory can only assign an entry, 
test for match on segment identi- fier, test for match on segment identifier and page number, or test 
for empty. Each memory management processor has an attached associative memory with entries for all the 
pages in the memory modules assigned to that processor. There is one table for all shared memory modules. 
The latter table is "read-mostly" since, as mentioned above, the main memory size is sufficiently large 
to contain the full memory region of all running processes except under very heavy load and hence segment 
traffic is expected to be relatively stable. There- fore, it should be possible to implement this table 
by attaching a copy to each memory management processor. The memory management processors may read their 
tables independently, while all tables are locked when a processor attempts to write into one of the 
locations. Contention for system tables is thus reduced so that it can only occur when a segment is entering 
or leav- ing the system, i.e., the only occasions for writing. System Functions. It should be clear 
how the memory management system operates. A program processor generates a page fault whenever it attempts 
to reference a page that is external to its buffer memory, or, alternatively, when it attempts to write 
onto a shared page. The program processors post their request to the associated ser- vice processor. 
When service is granted, the service processor determines the page and segment number of the desired 
page, and whether a read or write is desired. Each program processor maintains a set of  registers, 
like base registers, which contain the unique segment identifiers of the segments referred to by the 
process running on the processor. A program pro- cessor address needs only enough bits to specify a segment 
register, a page within a segment and a displacement within a page. For buffer memory references, the 
segment register number can be used directly as an address to reduce the address space size in the program 
processor. The memory management processor concatenates the con- tents of the segment register and the 
dis- placements to form an integer which unique- ly identifies a word within a segment. It then checks 
the associative memories to see if the page is present. If it is, the associative memory returns the 
memory module location and the transfer can be initiated. If it is not, then the missing page can be 
transferred into memory from the file system (where the page will be located at a fixed displacement 
from the start of the segment). The service pro- cessor does this by transferring the page into the first 
available entry the associ- ative memory reports as empty. If there is no room, then some other page 
may be transferred to the file system. The concept of placing process queues in hardware also simplifies 
inter- process communication. The busy form of waiting is justified, since a program processor is necessarily 
idle whenever its associated process is waiting for a critical region. An operation analogous to an iterated 
TEST AND SET instruction, instead of a more complex queue implemen- tation of P and V, is sufficient 
for interprocess synchronization. Since pro- gram processors directly reference only buffer memories, 
the TEST AND SET must be an interrupt to the process management processor. Each time a program processor 
cycles on a shared variable, it joins the queue of program processors waiting for service. This is equivalent 
to a queue of processes waiting for a V operation, with the queue protocol built into the cluster's bus 
arbitration. Model. As an aid in understanding the effect in a cluster of maintaining process state 
in a processor, as opposed to shared system tables, an M/M/M//N queueing model has been used. That is, 
a closed system with N processes, sufficient queueing (i.e., storage) capacity and M program processors. 
The service time distributions are Markovian, i.e., exponential. (See figure 5). An Assumption has been 
made that clusters behave relatively independently. The model is'closed and has two nodes. The first 
node represents the central server of a cluster and is modeled by uniproces- sor. The other node has 
M processors, where M is the number of program proces- sors. Exponential service times are assumed throughout. 
 Fig e S = service center p = program processor  The service processor represents the "critical region" 
computation required for page faults and process state storage and initialization. It is assumed that 
the latter is, in time, about one half of the combined function and is modeled by the parameter ~. More 
specifically, when there is an idle program processor the process state manipulation need not be carried 
out. The parameter rho represents the ratio of the arrival rate of service requests (k) to the service 
time (~). Assuming a 98 per cent hit ratio and an average program instruction time of 3 microseconds, 
a program processor would, on the average, generate a request for service every (3 x 100/2)=150 microseconds. 
Assuming a 25 microsecond time for a page transfer (projected for imminent CCD storage devic?s), a value 
for rho would be 150/25=6. Figures 6 and 7 illustrate the expected queue length at the service node for 
the number of processes, N, equal to 8 and 16, with three values of rho. The independent variable M is 
the number of program processors in the cluster. If k, the number of processes waiting or in com- putation 
at the M processors is taken as the state of the system and Pk is the probability of the system being 
in state k, then, from standard queueing theory [9], the exponential parameter for the service processors 
and the program processors are respectively: kw Om k<M ~/k = M~//c~ k m M  and kk 0 ~ k < N kk = 0 
otherwise  For 0 ~ k ~ M-l, k k 1 , Pk = PO (7) " ~ " advantage of process-processor identifica- 
 and for M ~ k ~ N, tion.  Pk = P0 @ k . M'--l' . k-M Conclusions. The incorporation of connection-limited 
microprocessors in a general purpose network is a difficult where P0 may be computed in a straight- 
 design problem. The approach suggested forward manner. here is to identify system processes with microprocessors 
and interconnect them in a hierarchz constructed so that intercom- C~ munication requirements are small 
compared C~ to independent processing time. This US structure is examined in a preliminary way and needs 
a more elaborate quantitative N= :3, GLPHq=O~S analysis. Its feasibility depends criti- cally on a number 
of service time distribu- tions that can only be estimated at this z ~d + lqO=4 time. Specifically, 
the size and perfor- mance of future buffer and bulk memories are very important. Re ferences <RefA>i. Baum, 
A. and D. Senzig, Hardware considerations in a microcomputer multiprocessing system. IEEE COMPCON, Feb. 
1975, 27-30. 2. Burk, J.M. and J.E. Schoonover, Com- puter system maintainability at the Lawrence Livermore 
Laboratory. Proc. '-b, ti!O ~,J-"''J.'U 10. +'~ 15. +_.~_"-' AFIPS FJCC, part I, 1972, 263-272.  NUMBER 
OF PR@gF~%N PR@CE'~,S,JRS 3. Chen, R. C-H., Bus communication systems, Ph.D. Thesis, Carnegie- Mellon 
University, January 1974. Figure 6. CJ CJ 4. Enslow, P., ed., Multiprocessors and Parallel Processing. 
Wiley, New York, 1974,  5. Fabry, R.S., Capability-based address- ing, C. ACM, 17, 7, July 1974, 403- 
 412.  6. Farber, D., Data ring oriented com- puter networks, in Rustin, Randall, ed. Computer Networks, 
Prentice-Hall, Englewood Cliffs, N.J., 1972.  7. Gelenbe, E., P. Tiberio and J.C.A. Boekhorst. Page 
size in deman paging systems. Acta Informatica ~, i, 1973, 1-24.  8. Heart, F.E., S.M. Ornstein, W. 
R.  t -4----- Crowther, and W. Barker. A new "-['. C',O 5. C'.O i O. ['C 1 5 C'C minicomputer/multiprocessor 
for the NUM.~ER OF PR@CBRM PROCE<}SORS ARPA network. Proceedings AFIPS NCC 42, 1973, 529-537. Figure 
7. It can be seen that the reduction 9. Kleinrock, L. Queueing Systems, Vol.  of the need for the service 
processor to ~: Theory, Wiley, New York, 1975.  handle process state changes prevents the service node 
from overloading, as the i0. Liptay, J.S. The cache, II~4 Sys. ~. !, queue length decreases as the 
number of i, 1968, 15-21.  program processors goes up (once it is likely that there is an idle program 
pro- ii. Madnick, S. and T. Donovan. Operating cessor). In fact, because the curve is Systems, McGraw-Hill, 
New York, 1974.  relatively flat, it is reasonable to add processes to a cluster beyond the number 
12. Mason, P.H. The design of programs for of program processors without losing the asynchronous multiprocessors, 
technical  report, Department of Computer Science, Carnegie-Mellon University, March 1975. 13. Newell, 
A. and G. Robertson. Some issues in programming multi-mini- processors, Technical Report, Depart- ment 
of Computer Science, Carnegie- Mellon University, January 1975.  14. Organick, E.I. The Multics System: 
An Examination of Its Structure, MIT Press, Cambridge, Mass., 1972.  15. Raphael, H.A. Distributed intelligence 
microcomputer design, IEEE COMPCON, February 1975, 21-26.  16. Ravindran, V.K. and T. Thomas. Characterization 
of multiple micro- processor networks, IEEE COMPCON, March 1973, 133-137.  17. Ritchie, D.M. and K. 
Thompson. The UNIX time-sharing system, C. ACM 17, 7, July 1974, 365-375.  18. Roberts, L. and B. Wessler. 
Computer network development to achieve resource sharing, Proceedings AFIPS SJCC 36, 1970, 543-549. 
 19. Wensley, J.H. The impact of electronic disks on system architecture, Computer 8, 2, February 1975, 
44-48.  20. Withington, F.G. Beyond 1984: a technology forecast. Datamation 21, i, January 1975, 54-73. 
 21. Wulf, W.A. and C.G. Bell. C.mmp--a multi-mini processor, Proceedings AFIPS FJCC 41, 1972, 765-778. 
 22. Wulf, W., E. Cohen, W. Corwind, A. Jones, R. Levin, C. Pierson and F. Pollack. HYDRA: the kernel 
of a multiprocessor operating system, C__~ A(IM 17, 6, June 1975, 337-345</RefA>.   
			
