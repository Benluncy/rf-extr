
 A Demonstrational Technique For Developing Interfaces With Dynamically Created Objects David Wolber 
Gene Fisher Division of Computer Science Department of Computer Science University of California, Davis 
California Polytechnical University Davis, CA., 95616-8562 San Luis Obispo, CA., 93407 Abstract The 
development of user inteijaces is often facilitated by the use of a drawing editor. The user interface 
spe­cialist draws pictures of the different states of the inte~ace and passes these specifications on 
to the pro­grammer. The user interface specialist might also use the drawing editor to demonstrate to 
the programmer the interactive behavior that the interface should exhi­bit; that is, he might demonstrate 
to the programmer the actions that an end-user can pe~orm, and the graphical manner by which the application 
should respond to the end-user s stimuli. From the specljications, and the in-person demonstrations, 
the programmer implements a protoppe of the interface. DEMO is a User Interface Development System (UIDS) 
that eliminates the programmer from the above process. Using an enhanced drawing editor, the user interface 
specialist demonstrates the actions of the end-user and the system, just as he would if the pro­grammer 
were watching. However no programmer is necessary: DEMO recorak these demonstrations, makes generalizations 
from them, and automatically generates a prototype of the inte~ace. Key Phrases: User Interface Development 
System 1. Introduction DEMO is an experimental User Interface Development System (UIDS) that allows interface 
behavior to be specified by directly demonstrating the behavior of the end-user and the behavior of the 
com­puter. DEMO Research focuses on developing demonstrational methods of specifying interface Permission 
10 copy without fee all or part of this material is granted provided that the copies are not made or 
distributed for direct commercial advantage, the ACM copyright notice end the title of the publication 
and its date appear, and notice is given that copying is by permission of the Association for Computing 
Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. @ 1991 ACM 
0-89791 -451 -1/91 /0010 /0221 . ..$l .50 behavior that requires progmmming in other develop­ ment systems. 
This paper will emphasize a methodology for developing interfaces, such as graph editors, in which objects 
are dynamically created. With DEMO, a graph editor can be created in minutes by a developer with no programming 
skills. No system, besides DEMO, allows the creation of such an interface in a purely demonstrational 
manner. Stimulus-Response Specification The DEMO system is based on a role-playing methodology called 
stimulus-response demonstration. Under this methodology, the developer draws the inter­face using a drawing 
editor, and then, using the same drawing editor, specities the interactive behavior of the interface 
by directly demonstrating how the system should respond to stimuli from the end-user. To speci&#38; interactive 
behavior, the developer tirst plays the role of the end-user and performs some graphical action. He then 
plays the role of the system and per­forms the graphical actions that should be executed in response 
to the end-user s stimulus. The system gen­eralizes from stimulus-response demonstrations to gen­erate 
stimulus-response specifications that define the behavior of the interface. Dynamically Created Objects 
In the context of a demonstrational UIDS, dynamically created objects are graphical objects that do not 
appear at the beginning of execution, but appear only as a result of an end-user stimulus. For example, 
in a graph editor, the new nodes that appear when an end-user clicks the mouse on the screen arc dynami­cally 
created objects. A developer specifies that an object will be created dynamically during execution by 
creating it at development time during the demonstration of a system response to an end-user stimulus. 
When such a demonstration is performed, DEMO marks the created object as representing the class of all 
objects that might be created, in the same manner, during execution. For example, if the developer clicks 
the left-mouse-button as a stimulus, and draws an ellipse as a response, DEMO marks the ellipse object 
as representing all ellipses that might be created during execution as a result of a click of the left-mouse-button. 
Behavior demonstrations performed on represen­tative objects are interpreted differently than demons­trations 
performed on other objects. DEMO generalizes from demonstrations on representative objects and defines 
stimulus-response specifications that define the behavior for the class of all potential run-time objects 
that the development-time object represents. To aid in generalizing from demonstrations on representative 
objects, DEMO infers graphical condi­tions that distinguish subsets within the class that the object 
represents. For instance, in a graph editor, the lines that connect nodes are dynamically created objects. 
When the end-user moves a particular node, the system should respond by modifying all lines that are 
connected to the node so that the lines remain connected. DEMO infers distinguishing conditions, like 
connected to , directly from the developers stimulus­ response demonstration. The developer need only 
per­ form the demonstration on a representative object that exhibits the desired condition, and DEMO 
will generate a conditional behavior specification. For the graph editor example, the developer would 
move a node as the stimulus, and then, as the response, modify a line that was indeed comectcd to the 
node. From the demonstration, DEMO would infer graphical conditions so that, at run-time, only lines 
that are connected to a moved node will be modified. The ability to specify dynamically created objects 
and graphical conditions by demonstration significantly increases the variety of interfaces that a non­programmer 
can develop without the aid of a program­mer. A graph editor is just one example of the kind of ~aphic 
design system that can be created using the methodology described in this paper. The rest of this paper 
is organized as follows: Section 2 places DEMO in context to other graphical UIDS, Section 3 gives an 
overview of the DEMO sys­tem, and Section 4 provides a step-by-step description of the DEMO development 
of a graph editor. Section 5 describes the current status of the project, Section 6 discusses limitations 
and future work, and Section 7 provides final remarks. 2. Related Work The goals of DEMO are similar 
to the goals of the QUICK system [1]. The authors of QUICK cite the challenge of maximizing power and 
flexibility in an extremely simple environment , and the goal of pro­viding an interface design tool 
for non-programmers to build direct manipulation interfaces that go far beyond simply specifying windows, 
menus, and buttons . These goals are exactly the same as the developers of DEMO. However, whereas QUICK 
provides a special-purpose, high-level programming language for the specification of interactive behavior, 
DEMO allows such specification in a purely demonstrational manner. The rest of this section focuses on 
graphical UIDS that and require little or no programming. Widget placement systems such as the NeXT Interface 
Builder [8] and Druid [9] allow a developer to create interfaces by locating and sizing system-supplied 
interactive objects directly in the interface being developed. A system-supplied set of interactive objects 
 promotes the creation of interfaces with a uniform look and feel, but limits the kinds of interfaces 
that can be created to those that contain only standard interface objects. Without resorting to programming, 
developers cannot create new interactive objects because there is no way to graphically specify the low-level 
behavior of raw graphics objects. A number of systems have been developed that do allow developers to 
specify the low-level behavior of interface objects in a graphical, interactive manner. Systems such 
as Apogee [3] and Lapidary [6] allow development of interfaces using graphical constraint specification 
tools. Fabrik [4] and the Interface Con­struction Set [10] provide graphical data flow charts. Though 
these systems are graphical, they require a developer to specify behavior using abstractions, such as 
constraints and data flow, instead of specifying behavior by direct demonstration. DEMO was most heavily 
influenced by two sys­tems, Peridot [5,7] and Rehearsal World [2], that allow interface behavior to be 
specified in a programming­by-demonstration manner. Rehearsal World introduced the role-playing metaphor 
as a method of demonstrat­ing interactive behavior. Similar to DEMO, a developer demonstrates an end-user 
action, and then demonstrates how the system should respond to the end-user s action. However, Rehearsal 
World is not based on a drawing editor, but on a Smalltalk Browser. The only end-user action that can 
be demonstrated is a mouse click, and the system response to an end-user action is demonstrated by calling 
one of a fixed (to the non­programmer) set of Smailtalk functions. Furthermore, no generalization mechanism 
is used. The system records stimuli and response opera­tions, but does not record the pammeters of those 
operations in order to generalize from the demonstra­tion. Peridot provides a more graphical and more 
flexi­ble environment for the demonstration of behaviors than Rehearsal World. The most powerful feature 
of Peridot is that it aids in the drawing of an interface by inferring graphical constraints. For instance, 
if the developer draws a small rectangle within a larger rec­tangle, Peridot infers that the inner rectangle 
should be centered within the outer rectangle. The way in which Peridot infers graphical constraints 
in the context of drawing an interface led to the method by which DEMO infers graphical conditions in 
the context of specifying dynamically created objects. Unfortunately, the inference-based framework used 
to specify the presentation of Peridot interfaces is also used to specify the basic interactive behavior 
of an interface. After the interface is drawn, the developer demonstrates end-user stimuli using a simulated 
mouse. But instead of explicitly demonstrating how the system should respond to a stimulus, Peridot attempts 
to infer what the response should be. Such an inference-based framework restricts the kinds of behaviors 
that can be demonstrated in comparison to a framework that allows the developer to explicitly demonstrate 
both the end­user s actions and the system s responses to those actions. No UIDS besides DEMO provides 
a demonstra­tional methodology for developing interfaces with dynamically created objects. A Peridot 
developer can demonstrate that an object should appear, or pop-up, as a result of an end-user stimulus, 
but there is no method of specifying an interface, such as a graph editor, in which an unlimited number 
of instances of an object might appear. DEMO is also the only UIDS to automatically infer graphical conditions 
that constrain the behavior of an interface. Peridot does automatically infer graphical constraints, 
but not in the context of conditioning behaviors. 3. Overview of the DEMO System Figure 3.1 shows a DEMO 
development environ­ment in which a graph editor is being developed. The drawing palette (left), the 
menu bar (top), and the display canvas (middle) provide the components found in a standard drawing editor. 
The behavior palette (right) allows the developer to specify whether he is drawing the interface, demonstrating 
the actions of an end-user, demonstrating how the system should respond to the an end-user s stimulus, 
or testing an interface. Figure 3.1. The DEMO Environment Drawing the Interface A full drawing editor 
is provided in DEMO and is used both to draw an interface, and specify the interactive behavior of the 
objects in the interface. To draw the initial interface, the developer selects Draw Mode from the behavior 
palette, and uses the graphics operations in the drawing editor provided. When he is finished drawing 
the initial display, he selects Save Initial Interface , a snapshot of the disptay is saved, and the 
developer is free to demonstrate the interactive behavior of the interface without worrying about modi­fying 
the interface that will appear at the beginning of execution. Consider the rotating gauge shown below: 
l-uu-l 0 The developer draws the circle, the markers, the labels, and the gauge hand, and then draws 
a box and types in the text 0.000 . He then selects Save Initial Inter­face and the system records this 
initial configuration of the gauge. Demonstrating Stimulus-Response Behavior Stimulus-Response demonstmtions 
are used to specify both how end-user initiated mouse actions trigger graphical operations, and how the 
execution of graphical operations trigger the execution of other graphical operations. Physical rnggers 
allow the developer to demon­strate the mouse actions that should trigger each of the drawing editor 
operations. A Physical trigger is specified in the following manner. First, the developer selects Demonstrate 
Stimulus from the Behavior palette, Select from the drawing editor palette, and clicks on one of the 
objects in the interface. Because Select is the current drawing editor operation, DEMO infers that the 
developer is demonstrating a physical stimulus, and asks him the kind of physical mouse action that should 
be recorded as the stimulus: Execute resfmnse to this action when: QzF@O ~ - - d Button-Down refers 
to depression of a mouse button, Button-Up refers to release of a mouse button, Mouse-In refers to the 
mouse being dragged within the borders of an object, and Mouse-Out refers to the mouse being dragged 
outside of an object. After the developer chooses the mouse action, the mode is automatically set to 
Demonstrate Response mode, and the developer performs one of the operations available in the drawing 
editor. When a physical rngger demonstration is per­formed and the response operation is one in which 
the user manipulates the mouse in order to input the param­eters of the operation, DEMO asks the developer 
if, at run-time, the stimulus should just initiate the response operation, or if execution of the stimulus 
should result in execution of the response using the same parameters that were input during the demonstration: 
 I Simuld ttw muse actim imtiate the cwrat]on, w execute It? I II CIiiEEl m If the developer answers 
that the response operation should just be initiated by the stimulus, then, at run­time, the end-user 
will be allowed to complete the operation by graphically specifying the parameters. Consider, for example, 
the rotating gauge, and suppose that the developer wanted to specify a behavior such that a left-click 
on the gauge hand initiates the rotation operation. The developer first selects Demon­strate Stimulus 
from the behavior palette, and Select from the drawing palette, and clicks the left-button on the gauge 
hand. DEMO records the button that was clicked, and asks the developer what kind of stimulus should be 
recorded. The developer answers that the stimulus should be recorded as a Mouse-Down. Then, after the 
system automatically sets the mode to Demonstrate Response, the developer rotates the gauge hand some 
amount, say 30 degrees. DEMO then asks the developer if the Left-Down stimulus should just initiate the 
rotation, or if each time the end­user performs a Left-Down the hand should be rotated exactly 30 degrees, 
If the developer selects Initiate , the following specification is generated Stimulus GaugeHand.LeftDowno 
Response GaugeHand.InitiateRotateo and at run-time, when the end-user clicks the left­mouse button on 
the gauge hand, he is allowed to rotate it any amount. If the developer had answered that the response 
should be executed, not initiated, then the following specification would be generated: Stimulus GaugeHand.LeftDowno 
Response GaugeHand.Rotate(30) and each time the end-user clicked the left-button on the gauge hand, the 
system would rotate the hand exactly 30 degrees. The above behavior specifications, and the ones referred 
to in the rest of this paper, are kept in an inter­nal form throughout development. The text form of 
the specification is not actually generated until the interface is saved. By demonstrating physical triggers 
in the manner described above, a developer has complete control of how the operations provided in the 
drawing editor are accessed at run-time. Such a facility is essential because the drawing editor is only 
a development tool, and does not appear during execution. At run-time, the end-user will only be able 
to execute those drawing operations for which a trigger was defined during development. Besides being 
used to specify physical triggers, Stimulus-Response demonstrations are also used to specify logical 
triggers; that is, triggers that define how the execution of one graphics operations initiates another. 
After selecting Demonstrate Stimulus, the developer performs some drawing editor operation other than 
Select. DEMO records both the operation that is performed, and the parameters of the operation. After 
the stimulus demonstration, the system automati­cally sets the mode to Demonstrate Response , and the 
developer executes one or more graphical operations that are the response(s) to the stimulus. DEMO records 
the response operations and its parameters, and then generalizes from the demonstrations to create stimulus-response 
specifications. Consider once again the rotating gauge example. For a stimulus, the develo~r would rotate 
the gauge some amount, say 90 degrees: m .  l$==-* c ) 50 For a response, he would then edit the text 
view of the gauge by changing it from 0.0 , to 25.0: B 50 The system would record the parameter of 
the stimulus operation, 90 degrees, and the parameter of the text edit operation, 25.0, and create a 
stimulus-response specification, The Generalization Mechanism When a stimulus-response demonstration 
is com­pleted, the parameters of each response operation are related to the parameters of the stimulus 
operation in order to compute the proportional constants that relate a general stimulus to a general 
response. With the default generalization scheme, constants are computed that relate the ith parameter 
of each response parameter with the ith parameter of each stimulus parameter, using the following equation 
Ci=Ri IS, The constants Ci are then used to genemte stimulus­response specifications of the form: Stimulus 
obj.op(pl,p2) Response obj.op(C ~ * pl,C2 * p2)  For the rotating gauge example, C 1=25.0/90.0 so DEMO 
would generate the behavior specification Stimulus GaugeHand.Rotate (x) Response TextValue.RealEdit(25 
.0/90.0 *x)  Because these linear equations are automatically genemted directly from demonstrations, 
the developer is freed from dealing with mathematical computations containing low level parameters such 
as pixels and degrees. DEMO is the first system to provide a uniform generalization mechanism that generates 
linear equa­ tions relating any pair of graphical operations. Peridot provides a special linear interpolation 
mechanism, but it not part of a normal demonstration, and it pertains only to a move operation. Testing 
the Interface During development an internal form of the stimulus-response specifications is used to 
allow a developer to test an interface without leaving the DEMO environment. The developer simply selects 
Test Mode from the behavior palette and the system redraws the initial display. DEMO then processes the 
end-user stimuli in the manner specified during the stimulus-response demonstrations. If the developer 
entered Test Mode after developing the gauge, he would find that each time he clicked on the gauge hand, 
the rotate operation is ini­tiated, and that after he completed each rotation opera­tion, the text-view 
of the gauge is automatically updated. For instance, if the developer rotated the gauge 180 degrees, 
the text value would be incre­mented by Cl * 180= 25P0 * 180= 50. Saving an Interface A developer can 
save an interface by selecting Save Interface from the File menu. A module con­taining code in DEMO s 
own special-puqme language is generated. This module can be loaded into other interfaces using the Load 
Widget command, linked in with an application using the method described in the following section, or 
executed as a stand-alone graphi­cal program by starting DEMO up with the module file name as the command-line 
argument. When objects are loaded in with the Load Widget command, their appearance and behavior can 
be edited just as if they were drawn with the drawing editor. This facility promotes the reuse of work 
and the creation of reusable interface objects that can be cus­tomized by direct manipulation. Before 
saving an interface, a developer can use the Name Object operation so that the generated code will contain 
descriptive names for the graphics objects. Otherwise, each graphics object is given a system-generated 
name. In the example specifications given in this paper, it is assumed that the developer has given descriptive 
names to the graphics objects. Mapping To an Application When the developer saves an interface, the sys­tem 
generates a display module containing the stimulus-response specifications, along with code that, when 
executed, draws the objects of the interface. Where Interface Builder and Peridot generate pro­cedures 
that are linked with an application through function calls and active values, the display modules generated 
by DEMO can be linked with an application using the same stimulus-response demonstrational framework 
that is used to specify the graphical behavior of an interface. DEMO provides a Modula interpreter within 
the development environment that is used to demonstrate function calls to application rou­tines as both 
stimuli and responses. From the demons­trations, DEMO automatically generates the propor­tional equations 
that convert computational values (e.g. the value of an integer) into graphical vahres (e.g. the number 
of degrees to rotate a gauge). These equations, in the form of stimulus-response specifications, are 
grouped separately from the display module and all computational code, so that the computational part, 
graphical user interface part, and the application map­ ping parts of an application are kept totally 
separate. The DEMO application mapping mechanism is now under development, and will be described fully 
in an upcoming paper. Specifying Dynamically Created Objects During development, when an object is created 
as a response to some end-user stimulus, DEMO marks the object as representing a class of objects. At 
run­time, no instances of the class will appear in the inter­face until the end-user performs the required 
stimulus. Each time the end-user performs the stimulus, a new instance will appear, so it is conceivable 
that an infinite number of instances could be created. Because a single physical object, at development 
time, represents an unlimited number of objects that might appear at run-time, stimulus-response demonstra­tions 
concerning those objects are interpreted dif­ferently than other objects. DEMO generalizes from the demonstrations 
on the representative object, and generates a specification that defines the behavior for the entire 
class of potential run-time instances that the object represents. Demonstrations are generalized in the 
following mannec If a stimulus demonstration is performed on an object that represents a class, then 
during execution, a similar stimulus performed on any instance of the class will cause the response to 
be executed; if a response demonstration is performed on an object that represents a class, during execution 
the system will perform the response operation on a subset of the instances of the class, where the subset 
is distinguished by some graphi­cal conditions. The developer aides the system in determining the distinguishing 
graphical conditions by intelligently configuring the objats that are part of the demonstra­tion. The 
way in which DEMO identifies graphical conditions is explained below. Graphical Conditions Unlike any 
other UIDS, DEMO generates condi­tional statements directly from the behaviors demon­strated by the developer, 
thereby providing the demons­trational counterpart to the textual if-then statement. This condition generator 
is automatically executed when a response operation is demonstrated on a dynam­ically created object, 
but it can also be used to put con­ditions on stimulus-response specifications concerning objects that 
are not created dynamically. Generally, the condition generator seeks to iden­tify relationships between 
the object of the stimulus, and the object of the response. Since the condition gen­erator is not executed 
until after a stimulus-response demonstration is completed (e.g. after both the node is moved as the 
stimulus, and a line is modified as a response), the system must keep copies of the previous states of 
the objects modified by the stimulus and the response. After the demonstration, DEMO analyzes these copies 
to identify the graphical relationships between the objects before the demonstration. Sometimes, a stimulus-response 
should be condi­tioned upon something besides the relationship between the stimulus and response objects. 
In this case, the developer can specify that a more widespread search be made, that is, he can specify 
that the response object be compared pair-wise with every other object in the inter­face. This widespread 
search is not always executed, as it can lead to the developer having to accept or reject a number of 
conditions identified by the system, and because the relationship between the stimulus and response objects 
is often the distinguishing factor. However, the widespread search does allow the specification of conditions 
that otherwise would have to be specified by programming. DEMO identifies graphical conditions by testing 
a predefine list of possible relationships that could relate two objects. Thus, the developer must aid 
DEMO by configuring the objects in the interface so that DEMO will identify the proper conditions. The 
follow-ing is a list of the relational conditions that DEMO tests: Intersects LeftEndPointContained 
RightEndPointContained Contains ContainsLeftEndPoint ContainsRightEndPoint Left Right Top Bottom DistanceFrom(d) 
DirectionFrom(d)  4. An Example Development Session Following is a step-by-step description of how a 
DEMO developer would create the graph editor inter­face shown in the display canvas of Figure 3.1. The 
graph editor allows the end-user to place nodes (ellipses) on the screen using the left mouse button, 
to connect the nodes with lines using the middle mouse button, and to move nodes around the canvas using 
the right button. The graph editor exhibits five stimulus­response behaviors. First, clicking the left-mouse­button 
on the screen results in the appearance of a node. Second, clicking the middle-mouse-button on the screen 
results in the initiation of the line creation opera­tion. Third, clicking the right-mouse button on 
a node results in the initiation of a move operation. Fourth, when the end-user moves nodes, the system 
automati­cally modifies the lines connected to the node so that they remain connected. Fifth, when a 
connecting line is drawn, the system checks if the endpoints of the line are within a node, and if they 
are not, it deletes the newly created line. The description below shows how these behaviors can be specified 
in a completely demonstrational manner using DEMO s stimulus­response framework. Drawing the Initial 
Interface The first step in developing the graph editor inter­face is to draw the initial presentation 
that the end-user will see. In this case, the initial presentation is just a window with a header containing 
the directions, After drawing this header using the DEMO drawing editor, the developer selects Save Initial 
Interface from the behavior palette, When the graph editor is executed or tested, the display canvas 
wilt be blank except for the directions. Specifying Physical Triggers The first behavior demonstrated 
is that clicking the left-mouse-button on the display screen should result in the appearance of a node. 
The developer selects Demonstrate Stimulus from the behavior palette, chooses Select from the drawing 
palette, and clicks the left-mouse-button on the screen. The system then asks what kind of mouse action 
should be recorded as the stimulus, and the developer chooses Mouse Down . The mode is automatically 
set to Demonstrate Response after the stimulus demonstration. The developer then draws an ellipse, as 
the response to the stimulus. The system then asks if the Left-Down stimulus should initiate the drawing 
of the ellipse, or if it should result in it being drawn. The developer selects execute in this instance 
so that all nodes created will be of the same size as the node drawn during the demons­tration. From 
the demonstration, the following drawing code is generated Node = CreateEllipse (....).Dynamic and the 
foltowing stimulus-response specification is generated Stimulus Canvas.LeftDown(x,y) Response Node.Create(x,y) 
 Next, the deveIoper demonstrates how connect­ing lines are drawn. He selects Demonstrate Stimulus from 
the behavior palette, and clicks the middle­mouse-button on the screen. Then, while in Demon­strate Response 
mode, he draws a line as a response to the stimulus. Once again the system asks if the stimulus should 
initiate the creation of the line, or execute it. In this instance the developer chooses initiates so 
that the end-user will be able to draw lines of any size. From this demonstration, the following drawing 
code is gen­erated Line = CreateLineo:Dynamic and the following stimulus-response spccitication is generated: 
Stimulus Canvas. MiddleDown(x,y) Response Line.InitiateCreate(x,y) The third behavior specitied is that 
clicking the right button should initiate a move of a node. The developer selects Demonstrate Stimulus 
, and clicks on the node that was created in the demonstration above. Then, while in Demonstrate Response 
mode, the developer performs a Move operation. When asked, the developer chooses that the stimulus should 
only ini­tiate the move of the node. From this demonstration, the following specification is generated 
Stimulus Node.RightMouse(x,y) Response Node.InitiateMove(x,y) Specifying Complex Behaviors The next 
behavior specified is that a Move of a node should result in all lines connected to that node being Reshaped 
so that they remain connected. The developer first goes into Test mode and draws a node and two lines, 
one line connected to the node by its right endpoint, and the other line connected to the node by its 
left endpoint, as shown below Next, the developer selects Demonstrate Stimulus Mode. Then, using the 
drawing editor, he moves the node five grid lines up, and five grid lines to the right (optional grid 
lines are provided in the drawing editor): o For this example, grid lines are spaced 10 pixels apart, 
so the stimulus is recorded as a Move (50,50) After this stimulus demonstration, DEMO automatically sets 
the mode to Demonstrate Response Mode. The developer then selects Reshape from the drawing editor palette, 
and moves the right endpoint of the line whose right-endpoint is contained in the node five grid lines 
up, and five grid lines to the right, so that the right endpoint retains its relationship to the node 
This response operation is recorded as a RightReshape(50,50). DEMO relates the stimulus parameters to 
the response parameters to compute constants that relate a general move of a node to a general reshape 
of a right end-point. In this case, C ~=50/5&#38;l.O and c2=50/50=l. (). Because the node represents 
all nodes that might be dynamically created at run-time, and the line represents all lines that might 
be dynamically created during execution, DEMO generalizes from the demons­tration. First, DEMO makes 
the generalization that moving any node should result in the demonstrated response. Second, DEMO identities 
the graphical con­ditions that should distinguish which dynamically created lines should be modified 
in response to the stimulus. DEMO tests its list of possible conditions, finds that the condition RightEndPointContained 
is true, and asks the developer whether the condition should be used to distinguish which Line instances 
should be Reshaped: The condition RightEndFomtConta lned is valid m CEKl @EEKl The developer selects 
Use , and this condition is attached to the stimulus-response specification gen­erated: Stimulus Node.Move(x,y) 
Response ( Line.RightReshape( l.0 *x, 1.0 y) Where RightEndPointContained(Line,Node) ) Note that DEMO 
records the response as a RightReshape, instead of just a Reshape, even though the drawing editor operation 
that the developer demon­strates is a generic Reshape. Next, while remaining in Demonstrate Response 
mode, the developer Reshapes the line whose left end­point is contained in the node: After this LeftReshape, 
DEMO determines that the con­dition LeftEndPointContained is true, and asks the developer whether the 
condition should be used to dis­tinguish which Line instances should be Reshaped. The developer selects 
Use , and this condition is attached to the stimulus-response specification genemted: Stimulus Node.Move(x,y) 
Response { Line.LeftReshape( l.0 *x, 1.0 * y) Where LeftEndPointContained(Line,Node) Line.RightReshape( 
l.0 * x, 1.0* y) Where RightEndPointContained(LineNode) ) The final behavior specified is that if a 
line is created with neither endpoint contained in a node, then that line should be immediately deleted. 
For this demonstration, the condition (neither endpoint con­tained in) relates the response object, the 
line, to an object (a node) that is neither the stimulus or the response object. Thus, the option Global 
Conditions is selected so that DEMO will do a more general search in order to identify the proper condition. 
After setting the Global Conditions option, the developer enters test mode and draws two nodes. He then 
selects Demonstrate Stimulus Mode from the bhavior palette, and creates a connecting line, with one endpoint 
in one node, and the other endpoint within the other node. Note that the developer, in this case, configures 
the interface so that the negation of the desired condition is true. The system automatically sets the 
mode to Demonstrate Response Mode, and the developer deletes the line using the Delete function. Since 
the Line is a dynamically created object, DEMO searches for possi­ble conditions that might distinguish 
the Line instances that should be deleted at run-time. The first condition found is listed: r 1 The condition 
SaweInstancel% is valid mm- The developer chooses Use , because at run-time, only a Line instance that 
is the same instance as the instance of the stimulus should be deleted. That is, only the line created 
should be deleted. The next condition listed is: The credit 1cm LeftEncPoi ntContained IS valid During 
execution, the Line should only be deleted if LeftEndPointContained is false, that is, if the left end­point 
(and right endpoint) is not contained in any node. Thus, the developer selects Negate in the dialogue 
box. The final condition listed is RightEndPointCon­tained, and once again the developer selects Negate 
in the dialogue box. From this demonstration, DEMO generates the following stimulus-response specification: 
STIMULUS Line.Create () RESPONSE { Line.Deleteo WHERE SameInstanceAs(Line) AND NOT LeftEndPointContained(Line~ode) 
AND NOT RightEndPointContained(Line~ode) ) After the above demonstrations are performed, the developer 
saves the graph editor using the Save Interface command. He can execute the graph editor with the Load 
Widget command while in the DEMO environment, or by using a command-line argument when entering the DEMO 
environment. 5. Current Status DEMO can now be used to create standard inter­face objects such as sliders, 
gauges, dialogue boxes, and menus. These objects are created from scratch, so the developer has flexibility 
in specifying the graphical layout and low-level behavior of these objects. DEMO has also been used to 
create more com­plex interfaces. The graph editor interface described in this paper, and the Celsius-Fahrenheit 
interface described in [11] are just two examples of interfaces that have been developed. 6. Limitations 
and Future Work Currently, DEMO only generates linear equations when generalizing from a demonstration. 
This form of generalization covers many behaviors common to inter­faces, but not all of them. We are 
currently exploring new generalization schemes. For instance, an option is now being added so that snapping 
behaviors can be specified by demonstra­tion. Such a scheme would allow the developer to demonstrate, 
for example, that an endpoint of a line created near a node should snap to the center of the node. Adding 
new generalization schemes will give DEMO developers more flexibility in the kinds of inter­faces that 
can be developed, but it will also add to the complexity of the demonstrational framework, Another way 
to increase the flexibility of DEMO without adding to the demonstrational framework is to to allow a 
developer to textually insert general equations in the code generated by DEMO. Currently, the DEMO Load 
Widget facility only parses linear equations within a stimulus-response specification. Another limitation 
of the DEMO system is that responses are only executed after the completion of a stimulus operation, 
and not while the end-user is exe­cuting the operation. For example, there is no method of specifying 
that the connecting lines in a graph editor should be reshaped as a node is being moved by the end-user. 
The reason for this limitation is that the draw­ing editor that DEMO is based on only updates a mani­pulated 
object after the end-user finishes graphically entering the parameters of the operation. As the pamm­eters 
are being input, a bounding box, or handle, of the object moves around the screen. This limitation could 
be eliminated by updating objects incrementally as operations are being executed, and incrementally exe­cuting 
the responses to stimuli. The limits of the condition generator have yet to be identified. More research 
is necessary to obtain clo­sure on the list of possible graphical conditions, and to limit the set of 
conditions to a minimal set so that the developer need not spend an inordinate amount of time accepting 
or rejecting DEMO s inferences. Our future work also includes improving the ani­mation capabilities of 
DEMO by adding timing con­straints, Currently, a series of responses can be exe­cuted as a result of 
a stimulus, but they are executed as fast as the graphics system can modify the display. Timing constraints 
would allow the developer to con­trol when each response was executed. 7. Conclusions Our research has 
shown that the power of a demonstrational framework need not be restricted to locating and sizing system-supplied 
interactive objects, or to creating standard interface objects from scratch. With the stimulus-response 
framework, non­programming developers can create imaginative inter­faces in a manner almost as easy as 
using a drawing editor. The drawing editor basis of DEMO allows non-programmers to specify behavior at 
a level they are familiar with, but that is flexible enough to allow com­plex interfaces to be developed. 
The stimulus-rmponsc fmrrmwork has proved m be a strong basis for developing new demonstrational techniques. 
Dynamically created objects and condi­tional behaviors provide two examples of functionality that DEMO 
allows to be specified by demonstration, but that must be specitied by programming in other sys­tems. 
References <RefA>[1] Douglas, S., Eckehard, D., Novick, D., QUICK: A User-Inte#ace Design Kit for Non-Programmers, 
UIST Proceedings, October, 1990. [2] Finzer, W., Gould, L., Programming By Rehearsal World, BYTE, June 
1984. [3] Hudson, S., Graphical Specij ication of Flexible User Interface Displays, ACM Computer Graphics, 
Volume 23, Number 3, July 1989. pp. 105-115. [4] Ludolph, F., Chow, Y., Ingalls, D., Wallace, S., Doyle, 
K., The Fabrik Programming Environment IEEE Computers, 1988 [5] Myers, B., Creating User Inte~aces By 
Demonst­ration, Academic Press, San Diego, 1988. [6] Myers, B., Vander Zanden, B., Dannenberg, R., Creating 
Graphical Interactive Application Objects by Demonstration, ACM Computer Graphics, Volume 23, Number 
3, July 1989. [7] Myers, B., Creating User Inte@aces Using Pro­gramming By Example, Visual Programming, 
and Con­straints, ACM Tmnsactions of Programming Languages and Systems, Volume 12, No. 2, April 1990, 
pp. 143-177. [8] Interface Builder Next, Inc. Palo Alto, CA. [9] Singh, G., Kok, C., Ngan, T., Druid: 
A System for Demon~trationa[ Rapid User Interface Development, UIST Proceedings, October, 1990. [10] 
Smith,D., Visual Programming in the Interface Construction Set, IEEE Computer, 1988. [11] Wolber, D., 
Fisher, G., Developing User Interfaces By Stimulus-Response Demonstration, COMI?SAC Proceedings, September 
1991.</RefA> 
			
